<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240610.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MVGamba: Unify 3D Content Generation as State Space Sequence Modeling", "author": "Xuanyu Yi and Zike Wu and Qiuhong Shen and Qingshan Xu and Pan Zhou and Joo-Hwee Lim and Shuicheng Yan and Xinchao Wang and Hanwang Zhang", "abstract": "  Recent 3D large reconstruction models (LRMs) can generate high-quality 3D\ncontent in sub-seconds by integrating multi-view diffusion models with scalable\nmulti-view reconstructors. Current works further leverage 3D Gaussian Splatting\nas 3D representation for improved visual quality and rendering efficiency.\nHowever, we observe that existing Gaussian reconstruction models often suffer\nfrom multi-view inconsistency and blurred textures. We attribute this to the\ncompromise of multi-view information propagation in favor of adopting powerful\nyet computationally intensive architectures (\\eg, Transformers). To address\nthis issue, we introduce MVGamba, a general and lightweight Gaussian\nreconstruction model featuring a multi-view Gaussian reconstructor based on the\nRNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal\ncontext containing multi-view information for cross-view self-refinement while\ngenerating a long sequence of Gaussians for fine-detail modeling with linear\ncomplexity. With off-the-shelf multi-view diffusion models integrated, MVGamba\nunifies 3D generation tasks from a single image, sparse images, or text\nprompts. Extensive experiments demonstrate that MVGamba outperforms\nstate-of-the-art baselines in all 3D content generation scenarios with\napproximately only $0.1\\times$ of the model size.\n", "link": "http://arxiv.org/abs/2406.06367v1", "date": "2024-06-10", "relevancy": 3.3849, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6973}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.689}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVGamba%3A%20Unify%203D%20Content%20Generation%20as%20State%20Space%20Sequence%20Modeling&body=Title%3A%20MVGamba%3A%20Unify%203D%20Content%20Generation%20as%20State%20Space%20Sequence%20Modeling%0AAuthor%3A%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Qiuhong%20Shen%20and%20Qingshan%20Xu%20and%20Pan%20Zhou%20and%20Joo-Hwee%20Lim%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Recent%203D%20large%20reconstruction%20models%20%28LRMs%29%20can%20generate%20high-quality%203D%0Acontent%20in%20sub-seconds%20by%20integrating%20multi-view%20diffusion%20models%20with%20scalable%0Amulti-view%20reconstructors.%20Current%20works%20further%20leverage%203D%20Gaussian%20Splatting%0Aas%203D%20representation%20for%20improved%20visual%20quality%20and%20rendering%20efficiency.%0AHowever%2C%20we%20observe%20that%20existing%20Gaussian%20reconstruction%20models%20often%20suffer%0Afrom%20multi-view%20inconsistency%20and%20blurred%20textures.%20We%20attribute%20this%20to%20the%0Acompromise%20of%20multi-view%20information%20propagation%20in%20favor%20of%20adopting%20powerful%0Ayet%20computationally%20intensive%20architectures%20%28%5Ceg%2C%20Transformers%29.%20To%20address%0Athis%20issue%2C%20we%20introduce%20MVGamba%2C%20a%20general%20and%20lightweight%20Gaussian%0Areconstruction%20model%20featuring%20a%20multi-view%20Gaussian%20reconstructor%20based%20on%20the%0ARNN-like%20State%20Space%20Model%20%28SSM%29.%20Our%20Gaussian%20reconstructor%20propagates%20causal%0Acontext%20containing%20multi-view%20information%20for%20cross-view%20self-refinement%20while%0Agenerating%20a%20long%20sequence%20of%20Gaussians%20for%20fine-detail%20modeling%20with%20linear%0Acomplexity.%20With%20off-the-shelf%20multi-view%20diffusion%20models%20integrated%2C%20MVGamba%0Aunifies%203D%20generation%20tasks%20from%20a%20single%20image%2C%20sparse%20images%2C%20or%20text%0Aprompts.%20Extensive%20experiments%20demonstrate%20that%20MVGamba%20outperforms%0Astate-of-the-art%20baselines%20in%20all%203D%20content%20generation%20scenarios%20with%0Aapproximately%20only%20%240.1%5Ctimes%24%20of%20the%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVGamba%253A%2520Unify%25203D%2520Content%2520Generation%2520as%2520State%2520Space%2520Sequence%2520Modeling%26entry.906535625%3DXuanyu%2520Yi%2520and%2520Zike%2520Wu%2520and%2520Qiuhong%2520Shen%2520and%2520Qingshan%2520Xu%2520and%2520Pan%2520Zhou%2520and%2520Joo-Hwee%2520Lim%2520and%2520Shuicheng%2520Yan%2520and%2520Xinchao%2520Wang%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%25203D%2520large%2520reconstruction%2520models%2520%2528LRMs%2529%2520can%2520generate%2520high-quality%25203D%250Acontent%2520in%2520sub-seconds%2520by%2520integrating%2520multi-view%2520diffusion%2520models%2520with%2520scalable%250Amulti-view%2520reconstructors.%2520Current%2520works%2520further%2520leverage%25203D%2520Gaussian%2520Splatting%250Aas%25203D%2520representation%2520for%2520improved%2520visual%2520quality%2520and%2520rendering%2520efficiency.%250AHowever%252C%2520we%2520observe%2520that%2520existing%2520Gaussian%2520reconstruction%2520models%2520often%2520suffer%250Afrom%2520multi-view%2520inconsistency%2520and%2520blurred%2520textures.%2520We%2520attribute%2520this%2520to%2520the%250Acompromise%2520of%2520multi-view%2520information%2520propagation%2520in%2520favor%2520of%2520adopting%2520powerful%250Ayet%2520computationally%2520intensive%2520architectures%2520%2528%255Ceg%252C%2520Transformers%2529.%2520To%2520address%250Athis%2520issue%252C%2520we%2520introduce%2520MVGamba%252C%2520a%2520general%2520and%2520lightweight%2520Gaussian%250Areconstruction%2520model%2520featuring%2520a%2520multi-view%2520Gaussian%2520reconstructor%2520based%2520on%2520the%250ARNN-like%2520State%2520Space%2520Model%2520%2528SSM%2529.%2520Our%2520Gaussian%2520reconstructor%2520propagates%2520causal%250Acontext%2520containing%2520multi-view%2520information%2520for%2520cross-view%2520self-refinement%2520while%250Agenerating%2520a%2520long%2520sequence%2520of%2520Gaussians%2520for%2520fine-detail%2520modeling%2520with%2520linear%250Acomplexity.%2520With%2520off-the-shelf%2520multi-view%2520diffusion%2520models%2520integrated%252C%2520MVGamba%250Aunifies%25203D%2520generation%2520tasks%2520from%2520a%2520single%2520image%252C%2520sparse%2520images%252C%2520or%2520text%250Aprompts.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MVGamba%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520all%25203D%2520content%2520generation%2520scenarios%2520with%250Aapproximately%2520only%2520%25240.1%255Ctimes%2524%2520of%2520the%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVGamba%3A%20Unify%203D%20Content%20Generation%20as%20State%20Space%20Sequence%20Modeling&entry.906535625=Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Qiuhong%20Shen%20and%20Qingshan%20Xu%20and%20Pan%20Zhou%20and%20Joo-Hwee%20Lim%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Recent%203D%20large%20reconstruction%20models%20%28LRMs%29%20can%20generate%20high-quality%203D%0Acontent%20in%20sub-seconds%20by%20integrating%20multi-view%20diffusion%20models%20with%20scalable%0Amulti-view%20reconstructors.%20Current%20works%20further%20leverage%203D%20Gaussian%20Splatting%0Aas%203D%20representation%20for%20improved%20visual%20quality%20and%20rendering%20efficiency.%0AHowever%2C%20we%20observe%20that%20existing%20Gaussian%20reconstruction%20models%20often%20suffer%0Afrom%20multi-view%20inconsistency%20and%20blurred%20textures.%20We%20attribute%20this%20to%20the%0Acompromise%20of%20multi-view%20information%20propagation%20in%20favor%20of%20adopting%20powerful%0Ayet%20computationally%20intensive%20architectures%20%28%5Ceg%2C%20Transformers%29.%20To%20address%0Athis%20issue%2C%20we%20introduce%20MVGamba%2C%20a%20general%20and%20lightweight%20Gaussian%0Areconstruction%20model%20featuring%20a%20multi-view%20Gaussian%20reconstructor%20based%20on%20the%0ARNN-like%20State%20Space%20Model%20%28SSM%29.%20Our%20Gaussian%20reconstructor%20propagates%20causal%0Acontext%20containing%20multi-view%20information%20for%20cross-view%20self-refinement%20while%0Agenerating%20a%20long%20sequence%20of%20Gaussians%20for%20fine-detail%20modeling%20with%20linear%0Acomplexity.%20With%20off-the-shelf%20multi-view%20diffusion%20models%20integrated%2C%20MVGamba%0Aunifies%203D%20generation%20tasks%20from%20a%20single%20image%2C%20sparse%20images%2C%20or%20text%0Aprompts.%20Extensive%20experiments%20demonstrate%20that%20MVGamba%20outperforms%0Astate-of-the-art%20baselines%20in%20all%203D%20content%20generation%20scenarios%20with%0Aapproximately%20only%20%240.1%5Ctimes%24%20of%20the%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06367v1&entry.124074799=Read"},
{"title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity\n  Surface Reconstruction", "author": "Danpeng Chen and Hai Li and Weicai Ye and Yifan Wang and Weijian Xie and Shangjin Zhai and Nan Wang and Haomin Liu and Hujun Bao and Guofeng Zhang", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.\n", "link": "http://arxiv.org/abs/2406.06521v1", "date": "2024-06-10", "relevancy": 3.288, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7747}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6654}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction&body=Title%3A%20PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Danpeng%20Chen%20and%20Hai%20Li%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Weijian%20Xie%20and%20Shangjin%20Zhai%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20high-quality%20rendering%2C%20and%20ultra-fast%20training%20and%20rendering%20speed.%0AHowever%2C%20due%20to%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%2C%0Ait%20is%20difficult%20to%20guarantee%20geometric%20reconstruction%20accuracy%20and%20multi-view%0Aconsistency%20simply%20by%20relying%20on%20image%20reconstruction%20loss.%20Although%20many%0Astudies%20on%20surface%20reconstruction%20based%20on%203DGS%20have%20emerged%20recently%2C%20the%0Aquality%20of%20their%20meshes%20is%20generally%20unsatisfactory.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20fast%20planar-based%20Gaussian%20splatting%20reconstruction%20representation%0A%28PGSR%29%20to%20achieve%20high-fidelity%20surface%20reconstruction%20while%20ensuring%0Ahigh-quality%20rendering.%20Specifically%2C%20we%20first%20introduce%20an%20unbiased%20depth%0Arendering%20method%2C%20which%20directly%20renders%20the%20distance%20from%20the%20camera%20origin%20to%0Athe%20Gaussian%20plane%20and%20the%20corresponding%20normal%20map%20based%20on%20the%20Gaussian%0Adistribution%20of%20the%20point%20cloud%2C%20and%20divides%20the%20two%20to%20obtain%20the%20unbiased%0Adepth.%20We%20then%20introduce%20single-view%20geometric%2C%20multi-view%20photometric%2C%20and%0Ageometric%20regularization%20to%20preserve%20global%20geometric%20accuracy.%20We%20also%20propose%0Aa%20camera%20exposure%20compensation%20model%20to%20cope%20with%20scenes%20with%20large%0Aillumination%20variations.%20Experiments%20on%20indoor%20and%20outdoor%20scenes%20show%20that%20our%0Amethod%20achieves%20fast%20training%20and%20rendering%20while%20maintaining%20high-fidelity%0Arendering%20and%20geometric%20reconstruction%2C%20outperforming%203DGS-based%20and%20NeRF-based%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPGSR%253A%2520Planar-based%2520Gaussian%2520Splatting%2520for%2520Efficient%2520and%2520High-Fidelity%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DDanpeng%2520Chen%2520and%2520Hai%2520Li%2520and%2520Weicai%2520Ye%2520and%2520Yifan%2520Wang%2520and%2520Weijian%2520Xie%2520and%2520Shangjin%2520Zhai%2520and%2520Nan%2520Wang%2520and%2520Haomin%2520Liu%2520and%2520Hujun%2520Bao%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520attracted%2520widespread%2520attention%2520due%250Ato%2520its%2520high-quality%2520rendering%252C%2520and%2520ultra-fast%2520training%2520and%2520rendering%2520speed.%250AHowever%252C%2520due%2520to%2520the%2520unstructured%2520and%2520irregular%2520nature%2520of%2520Gaussian%2520point%2520clouds%252C%250Ait%2520is%2520difficult%2520to%2520guarantee%2520geometric%2520reconstruction%2520accuracy%2520and%2520multi-view%250Aconsistency%2520simply%2520by%2520relying%2520on%2520image%2520reconstruction%2520loss.%2520Although%2520many%250Astudies%2520on%2520surface%2520reconstruction%2520based%2520on%25203DGS%2520have%2520emerged%2520recently%252C%2520the%250Aquality%2520of%2520their%2520meshes%2520is%2520generally%2520unsatisfactory.%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520a%2520fast%2520planar-based%2520Gaussian%2520splatting%2520reconstruction%2520representation%250A%2528PGSR%2529%2520to%2520achieve%2520high-fidelity%2520surface%2520reconstruction%2520while%2520ensuring%250Ahigh-quality%2520rendering.%2520Specifically%252C%2520we%2520first%2520introduce%2520an%2520unbiased%2520depth%250Arendering%2520method%252C%2520which%2520directly%2520renders%2520the%2520distance%2520from%2520the%2520camera%2520origin%2520to%250Athe%2520Gaussian%2520plane%2520and%2520the%2520corresponding%2520normal%2520map%2520based%2520on%2520the%2520Gaussian%250Adistribution%2520of%2520the%2520point%2520cloud%252C%2520and%2520divides%2520the%2520two%2520to%2520obtain%2520the%2520unbiased%250Adepth.%2520We%2520then%2520introduce%2520single-view%2520geometric%252C%2520multi-view%2520photometric%252C%2520and%250Ageometric%2520regularization%2520to%2520preserve%2520global%2520geometric%2520accuracy.%2520We%2520also%2520propose%250Aa%2520camera%2520exposure%2520compensation%2520model%2520to%2520cope%2520with%2520scenes%2520with%2520large%250Aillumination%2520variations.%2520Experiments%2520on%2520indoor%2520and%2520outdoor%2520scenes%2520show%2520that%2520our%250Amethod%2520achieves%2520fast%2520training%2520and%2520rendering%2520while%2520maintaining%2520high-fidelity%250Arendering%2520and%2520geometric%2520reconstruction%252C%2520outperforming%25203DGS-based%2520and%2520NeRF-based%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction&entry.906535625=Danpeng%20Chen%20and%20Hai%20Li%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Weijian%20Xie%20and%20Shangjin%20Zhai%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20high-quality%20rendering%2C%20and%20ultra-fast%20training%20and%20rendering%20speed.%0AHowever%2C%20due%20to%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%2C%0Ait%20is%20difficult%20to%20guarantee%20geometric%20reconstruction%20accuracy%20and%20multi-view%0Aconsistency%20simply%20by%20relying%20on%20image%20reconstruction%20loss.%20Although%20many%0Astudies%20on%20surface%20reconstruction%20based%20on%203DGS%20have%20emerged%20recently%2C%20the%0Aquality%20of%20their%20meshes%20is%20generally%20unsatisfactory.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20fast%20planar-based%20Gaussian%20splatting%20reconstruction%20representation%0A%28PGSR%29%20to%20achieve%20high-fidelity%20surface%20reconstruction%20while%20ensuring%0Ahigh-quality%20rendering.%20Specifically%2C%20we%20first%20introduce%20an%20unbiased%20depth%0Arendering%20method%2C%20which%20directly%20renders%20the%20distance%20from%20the%20camera%20origin%20to%0Athe%20Gaussian%20plane%20and%20the%20corresponding%20normal%20map%20based%20on%20the%20Gaussian%0Adistribution%20of%20the%20point%20cloud%2C%20and%20divides%20the%20two%20to%20obtain%20the%20unbiased%0Adepth.%20We%20then%20introduce%20single-view%20geometric%2C%20multi-view%20photometric%2C%20and%0Ageometric%20regularization%20to%20preserve%20global%20geometric%20accuracy.%20We%20also%20propose%0Aa%20camera%20exposure%20compensation%20model%20to%20cope%20with%20scenes%20with%20large%0Aillumination%20variations.%20Experiments%20on%20indoor%20and%20outdoor%20scenes%20show%20that%20our%0Amethod%20achieves%20fast%20training%20and%20rendering%20while%20maintaining%20high-fidelity%0Arendering%20and%20geometric%20reconstruction%2C%20outperforming%203DGS-based%20and%20NeRF-based%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06521v1&entry.124074799=Read"},
{"title": "DreamGaussian4D: Generative 4D Gaussian Splatting", "author": "Jiawei Ren and Liang Pan and Jiaxiang Tang and Chi Zhang and Ang Cao and Gang Zeng and Ziwei Liu", "abstract": "  4D content generation has achieved remarkable progress recently. However,\nexisting methods suffer from long optimization times, a lack of motion\ncontrollability, and a low quality of details. In this paper, we introduce\nDreamGaussian4D (DG4D), an efficient 4D generation framework that builds on\nGaussian Splatting (GS). Our key insight is that combining explicit modeling of\nspatial transformations with static GS makes an efficient and powerful\nrepresentation for 4D generation. Moreover, video generation methods have the\npotential to offer valuable spatial-temporal priors, enhancing the high-quality\n4D generation. Specifically, we propose an integral framework with two major\nmodules: 1) Image-to-4D GS - we initially generate static GS with\nDreamGaussianHD, followed by HexPlane-based dynamic generation with Gaussian\ndeformation; and 2) Video-to-Video Texture Refinement - we refine the generated\nUV-space texture maps and meanwhile enhance their temporal consistency by\nutilizing a pre-trained image-to-video diffusion model. Notably, DG4D reduces\nthe optimization time from several hours to just a few minutes, allows the\ngenerated 3D motion to be visually controlled, and produces animated meshes\nthat can be realistically rendered in 3D engines.\n", "link": "http://arxiv.org/abs/2312.17142v3", "date": "2024-06-10", "relevancy": 3.2558, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6851}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6646}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamGaussian4D%3A%20Generative%204D%20Gaussian%20Splatting&body=Title%3A%20DreamGaussian4D%3A%20Generative%204D%20Gaussian%20Splatting%0AAuthor%3A%20Jiawei%20Ren%20and%20Liang%20Pan%20and%20Jiaxiang%20Tang%20and%20Chi%20Zhang%20and%20Ang%20Cao%20and%20Gang%20Zeng%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%204D%20content%20generation%20has%20achieved%20remarkable%20progress%20recently.%20However%2C%0Aexisting%20methods%20suffer%20from%20long%20optimization%20times%2C%20a%20lack%20of%20motion%0Acontrollability%2C%20and%20a%20low%20quality%20of%20details.%20In%20this%20paper%2C%20we%20introduce%0ADreamGaussian4D%20%28DG4D%29%2C%20an%20efficient%204D%20generation%20framework%20that%20builds%20on%0AGaussian%20Splatting%20%28GS%29.%20Our%20key%20insight%20is%20that%20combining%20explicit%20modeling%20of%0Aspatial%20transformations%20with%20static%20GS%20makes%20an%20efficient%20and%20powerful%0Arepresentation%20for%204D%20generation.%20Moreover%2C%20video%20generation%20methods%20have%20the%0Apotential%20to%20offer%20valuable%20spatial-temporal%20priors%2C%20enhancing%20the%20high-quality%0A4D%20generation.%20Specifically%2C%20we%20propose%20an%20integral%20framework%20with%20two%20major%0Amodules%3A%201%29%20Image-to-4D%20GS%20-%20we%20initially%20generate%20static%20GS%20with%0ADreamGaussianHD%2C%20followed%20by%20HexPlane-based%20dynamic%20generation%20with%20Gaussian%0Adeformation%3B%20and%202%29%20Video-to-Video%20Texture%20Refinement%20-%20we%20refine%20the%20generated%0AUV-space%20texture%20maps%20and%20meanwhile%20enhance%20their%20temporal%20consistency%20by%0Autilizing%20a%20pre-trained%20image-to-video%20diffusion%20model.%20Notably%2C%20DG4D%20reduces%0Athe%20optimization%20time%20from%20several%20hours%20to%20just%20a%20few%20minutes%2C%20allows%20the%0Agenerated%203D%20motion%20to%20be%20visually%20controlled%2C%20and%20produces%20animated%20meshes%0Athat%20can%20be%20realistically%20rendered%20in%203D%20engines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17142v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamGaussian4D%253A%2520Generative%25204D%2520Gaussian%2520Splatting%26entry.906535625%3DJiawei%2520Ren%2520and%2520Liang%2520Pan%2520and%2520Jiaxiang%2520Tang%2520and%2520Chi%2520Zhang%2520and%2520Ang%2520Cao%2520and%2520Gang%2520Zeng%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%25204D%2520content%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520recently.%2520However%252C%250Aexisting%2520methods%2520suffer%2520from%2520long%2520optimization%2520times%252C%2520a%2520lack%2520of%2520motion%250Acontrollability%252C%2520and%2520a%2520low%2520quality%2520of%2520details.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADreamGaussian4D%2520%2528DG4D%2529%252C%2520an%2520efficient%25204D%2520generation%2520framework%2520that%2520builds%2520on%250AGaussian%2520Splatting%2520%2528GS%2529.%2520Our%2520key%2520insight%2520is%2520that%2520combining%2520explicit%2520modeling%2520of%250Aspatial%2520transformations%2520with%2520static%2520GS%2520makes%2520an%2520efficient%2520and%2520powerful%250Arepresentation%2520for%25204D%2520generation.%2520Moreover%252C%2520video%2520generation%2520methods%2520have%2520the%250Apotential%2520to%2520offer%2520valuable%2520spatial-temporal%2520priors%252C%2520enhancing%2520the%2520high-quality%250A4D%2520generation.%2520Specifically%252C%2520we%2520propose%2520an%2520integral%2520framework%2520with%2520two%2520major%250Amodules%253A%25201%2529%2520Image-to-4D%2520GS%2520-%2520we%2520initially%2520generate%2520static%2520GS%2520with%250ADreamGaussianHD%252C%2520followed%2520by%2520HexPlane-based%2520dynamic%2520generation%2520with%2520Gaussian%250Adeformation%253B%2520and%25202%2529%2520Video-to-Video%2520Texture%2520Refinement%2520-%2520we%2520refine%2520the%2520generated%250AUV-space%2520texture%2520maps%2520and%2520meanwhile%2520enhance%2520their%2520temporal%2520consistency%2520by%250Autilizing%2520a%2520pre-trained%2520image-to-video%2520diffusion%2520model.%2520Notably%252C%2520DG4D%2520reduces%250Athe%2520optimization%2520time%2520from%2520several%2520hours%2520to%2520just%2520a%2520few%2520minutes%252C%2520allows%2520the%250Agenerated%25203D%2520motion%2520to%2520be%2520visually%2520controlled%252C%2520and%2520produces%2520animated%2520meshes%250Athat%2520can%2520be%2520realistically%2520rendered%2520in%25203D%2520engines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17142v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamGaussian4D%3A%20Generative%204D%20Gaussian%20Splatting&entry.906535625=Jiawei%20Ren%20and%20Liang%20Pan%20and%20Jiaxiang%20Tang%20and%20Chi%20Zhang%20and%20Ang%20Cao%20and%20Gang%20Zeng%20and%20Ziwei%20Liu&entry.1292438233=%20%204D%20content%20generation%20has%20achieved%20remarkable%20progress%20recently.%20However%2C%0Aexisting%20methods%20suffer%20from%20long%20optimization%20times%2C%20a%20lack%20of%20motion%0Acontrollability%2C%20and%20a%20low%20quality%20of%20details.%20In%20this%20paper%2C%20we%20introduce%0ADreamGaussian4D%20%28DG4D%29%2C%20an%20efficient%204D%20generation%20framework%20that%20builds%20on%0AGaussian%20Splatting%20%28GS%29.%20Our%20key%20insight%20is%20that%20combining%20explicit%20modeling%20of%0Aspatial%20transformations%20with%20static%20GS%20makes%20an%20efficient%20and%20powerful%0Arepresentation%20for%204D%20generation.%20Moreover%2C%20video%20generation%20methods%20have%20the%0Apotential%20to%20offer%20valuable%20spatial-temporal%20priors%2C%20enhancing%20the%20high-quality%0A4D%20generation.%20Specifically%2C%20we%20propose%20an%20integral%20framework%20with%20two%20major%0Amodules%3A%201%29%20Image-to-4D%20GS%20-%20we%20initially%20generate%20static%20GS%20with%0ADreamGaussianHD%2C%20followed%20by%20HexPlane-based%20dynamic%20generation%20with%20Gaussian%0Adeformation%3B%20and%202%29%20Video-to-Video%20Texture%20Refinement%20-%20we%20refine%20the%20generated%0AUV-space%20texture%20maps%20and%20meanwhile%20enhance%20their%20temporal%20consistency%20by%0Autilizing%20a%20pre-trained%20image-to-video%20diffusion%20model.%20Notably%2C%20DG4D%20reduces%0Athe%20optimization%20time%20from%20several%20hours%20to%20just%20a%20few%20minutes%2C%20allows%20the%0Agenerated%203D%20motion%20to%20be%20visually%20controlled%2C%20and%20produces%20animated%20meshes%0Athat%20can%20be%20realistically%20rendered%20in%203D%20engines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17142v3&entry.124074799=Read"},
{"title": "Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering\n  for HDR View Synthesis", "author": "Xin Jin and Pengyi Jiao and Zheng-Peng Duan and Xingchao Yang and Chun-Le Guo and Bo Ren and Chongyi Li", "abstract": "  Volumetric rendering based methods, like NeRF, excel in HDR view synthesis\nfrom RAWimages, especially for nighttime scenes. While, they suffer from long\ntraining times and cannot perform real-time rendering due to dense sampling\nrequirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time\nrendering and faster training. However, implementing RAW image-based view\nsynthesis directly using 3DGS is challenging due to its inherent drawbacks: 1)\nin nighttime scenes, extremely low SNR leads to poor structure-from-motion\n(SfM) estimation in distant views; 2) the limited representation capacity of\nspherical harmonics (SH) function is unsuitable for RAW linear color space; and\n3) inaccurate scene structure hampers downstream tasks such as refocusing. To\naddress these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our\nmethod proposes Cone Scatter Initialization to enrich the estimation of SfM,\nand replaces SH with a Color MLP to represent the RAW linear color space.\nAdditionally, we introduce depth distortion and near-far regularizations to\nimprove the accuracy of scene structure for downstream tasks. These designs\nenable LE3D to perform real-time novel view synthesis, HDR rendering,\nrefocusing, and tone-mapping changes. Compared to previous volumetric rendering\nbased methods, LE3D reduces training time to 1% and improves rendering speed by\nup to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can\nbe found in https://github.com/Srameo/LE3D .\n", "link": "http://arxiv.org/abs/2406.06216v1", "date": "2024-06-10", "relevancy": 3.24, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7152}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6165}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lighting%20Every%20Darkness%20with%203DGS%3A%20Fast%20Training%20and%20Real-Time%20Rendering%0A%20%20for%20HDR%20View%20Synthesis&body=Title%3A%20Lighting%20Every%20Darkness%20with%203DGS%3A%20Fast%20Training%20and%20Real-Time%20Rendering%0A%20%20for%20HDR%20View%20Synthesis%0AAuthor%3A%20Xin%20Jin%20and%20Pengyi%20Jiao%20and%20Zheng-Peng%20Duan%20and%20Xingchao%20Yang%20and%20Chun-Le%20Guo%20and%20Bo%20Ren%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Volumetric%20rendering%20based%20methods%2C%20like%20NeRF%2C%20excel%20in%20HDR%20view%20synthesis%0Afrom%20RAWimages%2C%20especially%20for%20nighttime%20scenes.%20While%2C%20they%20suffer%20from%20long%0Atraining%20times%20and%20cannot%20perform%20real-time%20rendering%20due%20to%20dense%20sampling%0Arequirements.%20The%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%0Arendering%20and%20faster%20training.%20However%2C%20implementing%20RAW%20image-based%20view%0Asynthesis%20directly%20using%203DGS%20is%20challenging%20due%20to%20its%20inherent%20drawbacks%3A%201%29%0Ain%20nighttime%20scenes%2C%20extremely%20low%20SNR%20leads%20to%20poor%20structure-from-motion%0A%28SfM%29%20estimation%20in%20distant%20views%3B%202%29%20the%20limited%20representation%20capacity%20of%0Aspherical%20harmonics%20%28SH%29%20function%20is%20unsuitable%20for%20RAW%20linear%20color%20space%3B%20and%0A3%29%20inaccurate%20scene%20structure%20hampers%20downstream%20tasks%20such%20as%20refocusing.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20LE3D%20%28Lighting%20Every%20darkness%20with%203DGS%29.%20Our%0Amethod%20proposes%20Cone%20Scatter%20Initialization%20to%20enrich%20the%20estimation%20of%20SfM%2C%0Aand%20replaces%20SH%20with%20a%20Color%20MLP%20to%20represent%20the%20RAW%20linear%20color%20space.%0AAdditionally%2C%20we%20introduce%20depth%20distortion%20and%20near-far%20regularizations%20to%0Aimprove%20the%20accuracy%20of%20scene%20structure%20for%20downstream%20tasks.%20These%20designs%0Aenable%20LE3D%20to%20perform%20real-time%20novel%20view%20synthesis%2C%20HDR%20rendering%2C%0Arefocusing%2C%20and%20tone-mapping%20changes.%20Compared%20to%20previous%20volumetric%20rendering%0Abased%20methods%2C%20LE3D%20reduces%20training%20time%20to%201%25%20and%20improves%20rendering%20speed%20by%0Aup%20to%204%2C000%20times%20for%202K%20resolution%20images%20in%20terms%20of%20FPS.%20Code%20and%20viewer%20can%0Abe%20found%20in%20https%3A//github.com/Srameo/LE3D%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLighting%2520Every%2520Darkness%2520with%25203DGS%253A%2520Fast%2520Training%2520and%2520Real-Time%2520Rendering%250A%2520%2520for%2520HDR%2520View%2520Synthesis%26entry.906535625%3DXin%2520Jin%2520and%2520Pengyi%2520Jiao%2520and%2520Zheng-Peng%2520Duan%2520and%2520Xingchao%2520Yang%2520and%2520Chun-Le%2520Guo%2520and%2520Bo%2520Ren%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Volumetric%2520rendering%2520based%2520methods%252C%2520like%2520NeRF%252C%2520excel%2520in%2520HDR%2520view%2520synthesis%250Afrom%2520RAWimages%252C%2520especially%2520for%2520nighttime%2520scenes.%2520While%252C%2520they%2520suffer%2520from%2520long%250Atraining%2520times%2520and%2520cannot%2520perform%2520real-time%2520rendering%2520due%2520to%2520dense%2520sampling%250Arequirements.%2520The%2520advent%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520real-time%250Arendering%2520and%2520faster%2520training.%2520However%252C%2520implementing%2520RAW%2520image-based%2520view%250Asynthesis%2520directly%2520using%25203DGS%2520is%2520challenging%2520due%2520to%2520its%2520inherent%2520drawbacks%253A%25201%2529%250Ain%2520nighttime%2520scenes%252C%2520extremely%2520low%2520SNR%2520leads%2520to%2520poor%2520structure-from-motion%250A%2528SfM%2529%2520estimation%2520in%2520distant%2520views%253B%25202%2529%2520the%2520limited%2520representation%2520capacity%2520of%250Aspherical%2520harmonics%2520%2528SH%2529%2520function%2520is%2520unsuitable%2520for%2520RAW%2520linear%2520color%2520space%253B%2520and%250A3%2529%2520inaccurate%2520scene%2520structure%2520hampers%2520downstream%2520tasks%2520such%2520as%2520refocusing.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520LE3D%2520%2528Lighting%2520Every%2520darkness%2520with%25203DGS%2529.%2520Our%250Amethod%2520proposes%2520Cone%2520Scatter%2520Initialization%2520to%2520enrich%2520the%2520estimation%2520of%2520SfM%252C%250Aand%2520replaces%2520SH%2520with%2520a%2520Color%2520MLP%2520to%2520represent%2520the%2520RAW%2520linear%2520color%2520space.%250AAdditionally%252C%2520we%2520introduce%2520depth%2520distortion%2520and%2520near-far%2520regularizations%2520to%250Aimprove%2520the%2520accuracy%2520of%2520scene%2520structure%2520for%2520downstream%2520tasks.%2520These%2520designs%250Aenable%2520LE3D%2520to%2520perform%2520real-time%2520novel%2520view%2520synthesis%252C%2520HDR%2520rendering%252C%250Arefocusing%252C%2520and%2520tone-mapping%2520changes.%2520Compared%2520to%2520previous%2520volumetric%2520rendering%250Abased%2520methods%252C%2520LE3D%2520reduces%2520training%2520time%2520to%25201%2525%2520and%2520improves%2520rendering%2520speed%2520by%250Aup%2520to%25204%252C000%2520times%2520for%25202K%2520resolution%2520images%2520in%2520terms%2520of%2520FPS.%2520Code%2520and%2520viewer%2520can%250Abe%2520found%2520in%2520https%253A//github.com/Srameo/LE3D%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lighting%20Every%20Darkness%20with%203DGS%3A%20Fast%20Training%20and%20Real-Time%20Rendering%0A%20%20for%20HDR%20View%20Synthesis&entry.906535625=Xin%20Jin%20and%20Pengyi%20Jiao%20and%20Zheng-Peng%20Duan%20and%20Xingchao%20Yang%20and%20Chun-Le%20Guo%20and%20Bo%20Ren%20and%20Chongyi%20Li&entry.1292438233=%20%20Volumetric%20rendering%20based%20methods%2C%20like%20NeRF%2C%20excel%20in%20HDR%20view%20synthesis%0Afrom%20RAWimages%2C%20especially%20for%20nighttime%20scenes.%20While%2C%20they%20suffer%20from%20long%0Atraining%20times%20and%20cannot%20perform%20real-time%20rendering%20due%20to%20dense%20sampling%0Arequirements.%20The%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%0Arendering%20and%20faster%20training.%20However%2C%20implementing%20RAW%20image-based%20view%0Asynthesis%20directly%20using%203DGS%20is%20challenging%20due%20to%20its%20inherent%20drawbacks%3A%201%29%0Ain%20nighttime%20scenes%2C%20extremely%20low%20SNR%20leads%20to%20poor%20structure-from-motion%0A%28SfM%29%20estimation%20in%20distant%20views%3B%202%29%20the%20limited%20representation%20capacity%20of%0Aspherical%20harmonics%20%28SH%29%20function%20is%20unsuitable%20for%20RAW%20linear%20color%20space%3B%20and%0A3%29%20inaccurate%20scene%20structure%20hampers%20downstream%20tasks%20such%20as%20refocusing.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20LE3D%20%28Lighting%20Every%20darkness%20with%203DGS%29.%20Our%0Amethod%20proposes%20Cone%20Scatter%20Initialization%20to%20enrich%20the%20estimation%20of%20SfM%2C%0Aand%20replaces%20SH%20with%20a%20Color%20MLP%20to%20represent%20the%20RAW%20linear%20color%20space.%0AAdditionally%2C%20we%20introduce%20depth%20distortion%20and%20near-far%20regularizations%20to%0Aimprove%20the%20accuracy%20of%20scene%20structure%20for%20downstream%20tasks.%20These%20designs%0Aenable%20LE3D%20to%20perform%20real-time%20novel%20view%20synthesis%2C%20HDR%20rendering%2C%0Arefocusing%2C%20and%20tone-mapping%20changes.%20Compared%20to%20previous%20volumetric%20rendering%0Abased%20methods%2C%20LE3D%20reduces%20training%20time%20to%201%25%20and%20improves%20rendering%20speed%20by%0Aup%20to%204%2C000%20times%20for%202K%20resolution%20images%20in%20terms%20of%20FPS.%20Code%20and%20viewer%20can%0Abe%20found%20in%20https%3A//github.com/Srameo/LE3D%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06216v1&entry.124074799=Read"},
{"title": "Loopy-SLAM: Dense Neural SLAM with Loop Closures", "author": "Lorenzo Liso and Erik Sandstr\u00f6m and Vladimir Yugay and Luc Van Gool and Martin R. Oswald", "abstract": "  Neural RGBD SLAM techniques have shown promise in dense Simultaneous\nLocalization And Mapping (SLAM), yet face challenges such as error accumulation\nduring camera tracking resulting in distorted maps. In response, we introduce\nLoopy-SLAM that globally optimizes poses and the dense 3D model. We use\nframe-to-model tracking using a data-driven point-based submap generation\nmethod and trigger loop closures online by performing global place recognition.\nRobust pose graph optimization is used to rigidly align the local submaps. As\nour representation is point based, map corrections can be performed efficiently\nwithout the need to store the entire history of input frames used for mapping\nas typically required by methods employing a grid based mapping structure.\nEvaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet\ndatasets demonstrate competitive or superior performance in tracking, mapping,\nand rendering accuracy when compared to existing dense neural RGBD SLAM\nmethods. Project page: notchla.github.io/Loopy-SLAM.\n", "link": "http://arxiv.org/abs/2402.09944v2", "date": "2024-06-10", "relevancy": 3.2256, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6921}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loopy-SLAM%3A%20Dense%20Neural%20SLAM%20with%20Loop%20Closures&body=Title%3A%20Loopy-SLAM%3A%20Dense%20Neural%20SLAM%20with%20Loop%20Closures%0AAuthor%3A%20Lorenzo%20Liso%20and%20Erik%20Sandstr%C3%B6m%20and%20Vladimir%20Yugay%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Neural%20RGBD%20SLAM%20techniques%20have%20shown%20promise%20in%20dense%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%2C%20yet%20face%20challenges%20such%20as%20error%20accumulation%0Aduring%20camera%20tracking%20resulting%20in%20distorted%20maps.%20In%20response%2C%20we%20introduce%0ALoopy-SLAM%20that%20globally%20optimizes%20poses%20and%20the%20dense%203D%20model.%20We%20use%0Aframe-to-model%20tracking%20using%20a%20data-driven%20point-based%20submap%20generation%0Amethod%20and%20trigger%20loop%20closures%20online%20by%20performing%20global%20place%20recognition.%0ARobust%20pose%20graph%20optimization%20is%20used%20to%20rigidly%20align%20the%20local%20submaps.%20As%0Aour%20representation%20is%20point%20based%2C%20map%20corrections%20can%20be%20performed%20efficiently%0Awithout%20the%20need%20to%20store%20the%20entire%20history%20of%20input%20frames%20used%20for%20mapping%0Aas%20typically%20required%20by%20methods%20employing%20a%20grid%20based%20mapping%20structure.%0AEvaluation%20on%20the%20synthetic%20Replica%20and%20real-world%20TUM-RGBD%20and%20ScanNet%0Adatasets%20demonstrate%20competitive%20or%20superior%20performance%20in%20tracking%2C%20mapping%2C%0Aand%20rendering%20accuracy%20when%20compared%20to%20existing%20dense%20neural%20RGBD%20SLAM%0Amethods.%20Project%20page%3A%20notchla.github.io/Loopy-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09944v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoopy-SLAM%253A%2520Dense%2520Neural%2520SLAM%2520with%2520Loop%2520Closures%26entry.906535625%3DLorenzo%2520Liso%2520and%2520Erik%2520Sandstr%25C3%25B6m%2520and%2520Vladimir%2520Yugay%2520and%2520Luc%2520Van%2520Gool%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Neural%2520RGBD%2520SLAM%2520techniques%2520have%2520shown%2520promise%2520in%2520dense%2520Simultaneous%250ALocalization%2520And%2520Mapping%2520%2528SLAM%2529%252C%2520yet%2520face%2520challenges%2520such%2520as%2520error%2520accumulation%250Aduring%2520camera%2520tracking%2520resulting%2520in%2520distorted%2520maps.%2520In%2520response%252C%2520we%2520introduce%250ALoopy-SLAM%2520that%2520globally%2520optimizes%2520poses%2520and%2520the%2520dense%25203D%2520model.%2520We%2520use%250Aframe-to-model%2520tracking%2520using%2520a%2520data-driven%2520point-based%2520submap%2520generation%250Amethod%2520and%2520trigger%2520loop%2520closures%2520online%2520by%2520performing%2520global%2520place%2520recognition.%250ARobust%2520pose%2520graph%2520optimization%2520is%2520used%2520to%2520rigidly%2520align%2520the%2520local%2520submaps.%2520As%250Aour%2520representation%2520is%2520point%2520based%252C%2520map%2520corrections%2520can%2520be%2520performed%2520efficiently%250Awithout%2520the%2520need%2520to%2520store%2520the%2520entire%2520history%2520of%2520input%2520frames%2520used%2520for%2520mapping%250Aas%2520typically%2520required%2520by%2520methods%2520employing%2520a%2520grid%2520based%2520mapping%2520structure.%250AEvaluation%2520on%2520the%2520synthetic%2520Replica%2520and%2520real-world%2520TUM-RGBD%2520and%2520ScanNet%250Adatasets%2520demonstrate%2520competitive%2520or%2520superior%2520performance%2520in%2520tracking%252C%2520mapping%252C%250Aand%2520rendering%2520accuracy%2520when%2520compared%2520to%2520existing%2520dense%2520neural%2520RGBD%2520SLAM%250Amethods.%2520Project%2520page%253A%2520notchla.github.io/Loopy-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09944v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loopy-SLAM%3A%20Dense%20Neural%20SLAM%20with%20Loop%20Closures&entry.906535625=Lorenzo%20Liso%20and%20Erik%20Sandstr%C3%B6m%20and%20Vladimir%20Yugay%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Neural%20RGBD%20SLAM%20techniques%20have%20shown%20promise%20in%20dense%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%2C%20yet%20face%20challenges%20such%20as%20error%20accumulation%0Aduring%20camera%20tracking%20resulting%20in%20distorted%20maps.%20In%20response%2C%20we%20introduce%0ALoopy-SLAM%20that%20globally%20optimizes%20poses%20and%20the%20dense%203D%20model.%20We%20use%0Aframe-to-model%20tracking%20using%20a%20data-driven%20point-based%20submap%20generation%0Amethod%20and%20trigger%20loop%20closures%20online%20by%20performing%20global%20place%20recognition.%0ARobust%20pose%20graph%20optimization%20is%20used%20to%20rigidly%20align%20the%20local%20submaps.%20As%0Aour%20representation%20is%20point%20based%2C%20map%20corrections%20can%20be%20performed%20efficiently%0Awithout%20the%20need%20to%20store%20the%20entire%20history%20of%20input%20frames%20used%20for%20mapping%0Aas%20typically%20required%20by%20methods%20employing%20a%20grid%20based%20mapping%20structure.%0AEvaluation%20on%20the%20synthetic%20Replica%20and%20real-world%20TUM-RGBD%20and%20ScanNet%0Adatasets%20demonstrate%20competitive%20or%20superior%20performance%20in%20tracking%2C%20mapping%2C%0Aand%20rendering%20accuracy%20when%20compared%20to%20existing%20dense%20neural%20RGBD%20SLAM%0Amethods.%20Project%20page%3A%20notchla.github.io/Loopy-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09944v2&entry.124074799=Read"},
{"title": "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery", "author": "Hongsheng Wang and Weiyue Zhang and Sihao Liu and Xinrui Zhou and Jing Li and Zhanyun Tang and Shengyu Zhang and Fei Wu and Feng Lin", "abstract": "  Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human\nreconstruction, it primarily relies on 2D pixel-level supervision, overlooking\nthe geometric complexity and topological relationships of different body parts.\nTo address this gap, we introduce the Hierarchical Graph Human Gaussian Control\n(HUGS) framework for achieving high-fidelity 3D human reconstruction. Our\napproach involves leveraging explicitly semantic priors of body parts to ensure\nthe consistency of geometric topology, thereby enabling the capture of the\ncomplex geometrical and topological associations among body parts.\nAdditionally, we disentangle high-frequency features from global human features\nto refine surface details in body parts. Extensive experiments demonstrate that\nour method exhibits superior performance in human body reconstruction,\nparticularly in enhancing surface details and accurately reconstructing body\npart junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.\n", "link": "http://arxiv.org/abs/2405.12477v2", "date": "2024-06-10", "relevancy": 3.1514, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6555}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6531}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Control%20with%20Hierarchical%20Semantic%20Graphs%20in%203D%20Human%20Recovery&body=Title%3A%20Gaussian%20Control%20with%20Hierarchical%20Semantic%20Graphs%20in%203D%20Human%20Recovery%0AAuthor%3A%20Hongsheng%20Wang%20and%20Weiyue%20Zhang%20and%20Sihao%20Liu%20and%20Xinrui%20Zhou%20and%20Jing%20Li%20and%20Zhanyun%20Tang%20and%20Shengyu%20Zhang%20and%20Fei%20Wu%20and%20Feng%20Lin%0AAbstract%3A%20%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20made%20progress%20in%203D%20human%0Areconstruction%2C%20it%20primarily%20relies%20on%202D%20pixel-level%20supervision%2C%20overlooking%0Athe%20geometric%20complexity%20and%20topological%20relationships%20of%20different%20body%20parts.%0ATo%20address%20this%20gap%2C%20we%20introduce%20the%20Hierarchical%20Graph%20Human%20Gaussian%20Control%0A%28HUGS%29%20framework%20for%20achieving%20high-fidelity%203D%20human%20reconstruction.%20Our%0Aapproach%20involves%20leveraging%20explicitly%20semantic%20priors%20of%20body%20parts%20to%20ensure%0Athe%20consistency%20of%20geometric%20topology%2C%20thereby%20enabling%20the%20capture%20of%20the%0Acomplex%20geometrical%20and%20topological%20associations%20among%20body%20parts.%0AAdditionally%2C%20we%20disentangle%20high-frequency%20features%20from%20global%20human%20features%0Ato%20refine%20surface%20details%20in%20body%20parts.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20exhibits%20superior%20performance%20in%20human%20body%20reconstruction%2C%0Aparticularly%20in%20enhancing%20surface%20details%20and%20accurately%20reconstructing%20body%0Apart%20junctions.%20Codes%20are%20available%20at%20https%3A//wanghongsheng01.github.io/HUGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Control%2520with%2520Hierarchical%2520Semantic%2520Graphs%2520in%25203D%2520Human%2520Recovery%26entry.906535625%3DHongsheng%2520Wang%2520and%2520Weiyue%2520Zhang%2520and%2520Sihao%2520Liu%2520and%2520Xinrui%2520Zhou%2520and%2520Jing%2520Li%2520and%2520Zhanyun%2520Tang%2520and%2520Shengyu%2520Zhang%2520and%2520Fei%2520Wu%2520and%2520Feng%2520Lin%26entry.1292438233%3D%2520%2520Although%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520made%2520progress%2520in%25203D%2520human%250Areconstruction%252C%2520it%2520primarily%2520relies%2520on%25202D%2520pixel-level%2520supervision%252C%2520overlooking%250Athe%2520geometric%2520complexity%2520and%2520topological%2520relationships%2520of%2520different%2520body%2520parts.%250ATo%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Hierarchical%2520Graph%2520Human%2520Gaussian%2520Control%250A%2528HUGS%2529%2520framework%2520for%2520achieving%2520high-fidelity%25203D%2520human%2520reconstruction.%2520Our%250Aapproach%2520involves%2520leveraging%2520explicitly%2520semantic%2520priors%2520of%2520body%2520parts%2520to%2520ensure%250Athe%2520consistency%2520of%2520geometric%2520topology%252C%2520thereby%2520enabling%2520the%2520capture%2520of%2520the%250Acomplex%2520geometrical%2520and%2520topological%2520associations%2520among%2520body%2520parts.%250AAdditionally%252C%2520we%2520disentangle%2520high-frequency%2520features%2520from%2520global%2520human%2520features%250Ato%2520refine%2520surface%2520details%2520in%2520body%2520parts.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520exhibits%2520superior%2520performance%2520in%2520human%2520body%2520reconstruction%252C%250Aparticularly%2520in%2520enhancing%2520surface%2520details%2520and%2520accurately%2520reconstructing%2520body%250Apart%2520junctions.%2520Codes%2520are%2520available%2520at%2520https%253A//wanghongsheng01.github.io/HUGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Control%20with%20Hierarchical%20Semantic%20Graphs%20in%203D%20Human%20Recovery&entry.906535625=Hongsheng%20Wang%20and%20Weiyue%20Zhang%20and%20Sihao%20Liu%20and%20Xinrui%20Zhou%20and%20Jing%20Li%20and%20Zhanyun%20Tang%20and%20Shengyu%20Zhang%20and%20Fei%20Wu%20and%20Feng%20Lin&entry.1292438233=%20%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20made%20progress%20in%203D%20human%0Areconstruction%2C%20it%20primarily%20relies%20on%202D%20pixel-level%20supervision%2C%20overlooking%0Athe%20geometric%20complexity%20and%20topological%20relationships%20of%20different%20body%20parts.%0ATo%20address%20this%20gap%2C%20we%20introduce%20the%20Hierarchical%20Graph%20Human%20Gaussian%20Control%0A%28HUGS%29%20framework%20for%20achieving%20high-fidelity%203D%20human%20reconstruction.%20Our%0Aapproach%20involves%20leveraging%20explicitly%20semantic%20priors%20of%20body%20parts%20to%20ensure%0Athe%20consistency%20of%20geometric%20topology%2C%20thereby%20enabling%20the%20capture%20of%20the%0Acomplex%20geometrical%20and%20topological%20associations%20among%20body%20parts.%0AAdditionally%2C%20we%20disentangle%20high-frequency%20features%20from%20global%20human%20features%0Ato%20refine%20surface%20details%20in%20body%20parts.%20Extensive%20experiments%20demonstrate%20that%0Aour%20method%20exhibits%20superior%20performance%20in%20human%20body%20reconstruction%2C%0Aparticularly%20in%20enhancing%20surface%20details%20and%20accurately%20reconstructing%20body%0Apart%20junctions.%20Codes%20are%20available%20at%20https%3A//wanghongsheng01.github.io/HUGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12477v2&entry.124074799=Read"},
{"title": "GaussianCity: Generative Gaussian Splatting for Unbounded 3D City\n  Generation", "author": "Haozhe Xie and Zhaoxi Chen and Fangzhou Hong and Ziwei Liu", "abstract": "  3D city generation with NeRF-based methods shows promising generation results\nbut is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has\nemerged as a highly efficient alternative for object-level 3D generation.\nHowever, adapting 3D-GS from finite-scale 3D objects and humans to\ninfinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails\nsignificant storage overhead (out-of-memory issues), arising from the need to\nexpand points to billions, often demanding hundreds of Gigabytes of VRAM for a\ncity scene spanning 10km^2. In this paper, we propose GaussianCity, a\ngenerative Gaussian Splatting framework dedicated to efficiently synthesizing\nunbounded 3D cities with a single feed-forward pass. Our key insights are\ntwo-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a\nhighly compact intermediate representation, ensuring that the growth in VRAM\nusage for unbounded scenes remains constant, thus enabling unbounded city\ngeneration. 2) Spatial-aware Gaussian Attribute Decoder: We present\nspatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which\nleverages Point Serializer to integrate the structural and contextual\ncharacteristics of BEV points. Extensive experiments demonstrate that\nGaussianCity achieves state-of-the-art results in both drone-view and\nstreet-view 3D city generation. Notably, compared to CityDreamer, GaussianCity\nexhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18\nFPS).\n", "link": "http://arxiv.org/abs/2406.06526v1", "date": "2024-06-10", "relevancy": 3.1239, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6875}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.627}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianCity%3A%20Generative%20Gaussian%20Splatting%20for%20Unbounded%203D%20City%0A%20%20Generation&body=Title%3A%20GaussianCity%3A%20Generative%20Gaussian%20Splatting%20for%20Unbounded%203D%20City%0A%20%20Generation%0AAuthor%3A%20Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%203D%20city%20generation%20with%20NeRF-based%20methods%20shows%20promising%20generation%20results%0Abut%20is%20computationally%20inefficient.%20Recently%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%0Aemerged%20as%20a%20highly%20efficient%20alternative%20for%20object-level%203D%20generation.%0AHowever%2C%20adapting%203D-GS%20from%20finite-scale%203D%20objects%20and%20humans%20to%0Ainfinite-scale%203D%20cities%20is%20non-trivial.%20Unbounded%203D%20city%20generation%20entails%0Asignificant%20storage%20overhead%20%28out-of-memory%20issues%29%2C%20arising%20from%20the%20need%20to%0Aexpand%20points%20to%20billions%2C%20often%20demanding%20hundreds%20of%20Gigabytes%20of%20VRAM%20for%20a%0Acity%20scene%20spanning%2010km%5E2.%20In%20this%20paper%2C%20we%20propose%20GaussianCity%2C%20a%0Agenerative%20Gaussian%20Splatting%20framework%20dedicated%20to%20efficiently%20synthesizing%0Aunbounded%203D%20cities%20with%20a%20single%20feed-forward%20pass.%20Our%20key%20insights%20are%0Atwo-fold%3A%201%29%20Compact%203D%20Scene%20Representation%3A%20We%20introduce%20BEV-Point%20as%20a%0Ahighly%20compact%20intermediate%20representation%2C%20ensuring%20that%20the%20growth%20in%20VRAM%0Ausage%20for%20unbounded%20scenes%20remains%20constant%2C%20thus%20enabling%20unbounded%20city%0Ageneration.%202%29%20Spatial-aware%20Gaussian%20Attribute%20Decoder%3A%20We%20present%0Aspatial-aware%20BEV-Point%20decoder%20to%20produce%203D%20Gaussian%20attributes%2C%20which%0Aleverages%20Point%20Serializer%20to%20integrate%20the%20structural%20and%20contextual%0Acharacteristics%20of%20BEV%20points.%20Extensive%20experiments%20demonstrate%20that%0AGaussianCity%20achieves%20state-of-the-art%20results%20in%20both%20drone-view%20and%0Astreet-view%203D%20city%20generation.%20Notably%2C%20compared%20to%20CityDreamer%2C%20GaussianCity%0Aexhibits%20superior%20performance%20with%20a%20speedup%20of%2060%20times%20%2810.72%20FPS%20v.s.%200.18%0AFPS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianCity%253A%2520Generative%2520Gaussian%2520Splatting%2520for%2520Unbounded%25203D%2520City%250A%2520%2520Generation%26entry.906535625%3DHaozhe%2520Xie%2520and%2520Zhaoxi%2520Chen%2520and%2520Fangzhou%2520Hong%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%25203D%2520city%2520generation%2520with%2520NeRF-based%2520methods%2520shows%2520promising%2520generation%2520results%250Abut%2520is%2520computationally%2520inefficient.%2520Recently%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520has%250Aemerged%2520as%2520a%2520highly%2520efficient%2520alternative%2520for%2520object-level%25203D%2520generation.%250AHowever%252C%2520adapting%25203D-GS%2520from%2520finite-scale%25203D%2520objects%2520and%2520humans%2520to%250Ainfinite-scale%25203D%2520cities%2520is%2520non-trivial.%2520Unbounded%25203D%2520city%2520generation%2520entails%250Asignificant%2520storage%2520overhead%2520%2528out-of-memory%2520issues%2529%252C%2520arising%2520from%2520the%2520need%2520to%250Aexpand%2520points%2520to%2520billions%252C%2520often%2520demanding%2520hundreds%2520of%2520Gigabytes%2520of%2520VRAM%2520for%2520a%250Acity%2520scene%2520spanning%252010km%255E2.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GaussianCity%252C%2520a%250Agenerative%2520Gaussian%2520Splatting%2520framework%2520dedicated%2520to%2520efficiently%2520synthesizing%250Aunbounded%25203D%2520cities%2520with%2520a%2520single%2520feed-forward%2520pass.%2520Our%2520key%2520insights%2520are%250Atwo-fold%253A%25201%2529%2520Compact%25203D%2520Scene%2520Representation%253A%2520We%2520introduce%2520BEV-Point%2520as%2520a%250Ahighly%2520compact%2520intermediate%2520representation%252C%2520ensuring%2520that%2520the%2520growth%2520in%2520VRAM%250Ausage%2520for%2520unbounded%2520scenes%2520remains%2520constant%252C%2520thus%2520enabling%2520unbounded%2520city%250Ageneration.%25202%2529%2520Spatial-aware%2520Gaussian%2520Attribute%2520Decoder%253A%2520We%2520present%250Aspatial-aware%2520BEV-Point%2520decoder%2520to%2520produce%25203D%2520Gaussian%2520attributes%252C%2520which%250Aleverages%2520Point%2520Serializer%2520to%2520integrate%2520the%2520structural%2520and%2520contextual%250Acharacteristics%2520of%2520BEV%2520points.%2520Extensive%2520experiments%2520demonstrate%2520that%250AGaussianCity%2520achieves%2520state-of-the-art%2520results%2520in%2520both%2520drone-view%2520and%250Astreet-view%25203D%2520city%2520generation.%2520Notably%252C%2520compared%2520to%2520CityDreamer%252C%2520GaussianCity%250Aexhibits%2520superior%2520performance%2520with%2520a%2520speedup%2520of%252060%2520times%2520%252810.72%2520FPS%2520v.s.%25200.18%250AFPS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianCity%3A%20Generative%20Gaussian%20Splatting%20for%20Unbounded%203D%20City%0A%20%20Generation&entry.906535625=Haozhe%20Xie%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu&entry.1292438233=%20%203D%20city%20generation%20with%20NeRF-based%20methods%20shows%20promising%20generation%20results%0Abut%20is%20computationally%20inefficient.%20Recently%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%0Aemerged%20as%20a%20highly%20efficient%20alternative%20for%20object-level%203D%20generation.%0AHowever%2C%20adapting%203D-GS%20from%20finite-scale%203D%20objects%20and%20humans%20to%0Ainfinite-scale%203D%20cities%20is%20non-trivial.%20Unbounded%203D%20city%20generation%20entails%0Asignificant%20storage%20overhead%20%28out-of-memory%20issues%29%2C%20arising%20from%20the%20need%20to%0Aexpand%20points%20to%20billions%2C%20often%20demanding%20hundreds%20of%20Gigabytes%20of%20VRAM%20for%20a%0Acity%20scene%20spanning%2010km%5E2.%20In%20this%20paper%2C%20we%20propose%20GaussianCity%2C%20a%0Agenerative%20Gaussian%20Splatting%20framework%20dedicated%20to%20efficiently%20synthesizing%0Aunbounded%203D%20cities%20with%20a%20single%20feed-forward%20pass.%20Our%20key%20insights%20are%0Atwo-fold%3A%201%29%20Compact%203D%20Scene%20Representation%3A%20We%20introduce%20BEV-Point%20as%20a%0Ahighly%20compact%20intermediate%20representation%2C%20ensuring%20that%20the%20growth%20in%20VRAM%0Ausage%20for%20unbounded%20scenes%20remains%20constant%2C%20thus%20enabling%20unbounded%20city%0Ageneration.%202%29%20Spatial-aware%20Gaussian%20Attribute%20Decoder%3A%20We%20present%0Aspatial-aware%20BEV-Point%20decoder%20to%20produce%203D%20Gaussian%20attributes%2C%20which%0Aleverages%20Point%20Serializer%20to%20integrate%20the%20structural%20and%20contextual%0Acharacteristics%20of%20BEV%20points.%20Extensive%20experiments%20demonstrate%20that%0AGaussianCity%20achieves%20state-of-the-art%20results%20in%20both%20drone-view%20and%0Astreet-view%203D%20city%20generation.%20Notably%2C%20compared%20to%20CityDreamer%2C%20GaussianCity%0Aexhibits%20superior%20performance%20with%20a%20speedup%20of%2060%20times%20%2810.72%20FPS%20v.s.%200.18%0AFPS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06526v1&entry.124074799=Read"},
{"title": "Active Neural 3D Reconstruction with Colorized Surface Voxel-based View\n  Selection", "author": "Hyunseo Kim and Hyeonseo Yang and Taekyung Kim and YoonSung Kim and Jin-Hwa Kim and Byoung-Tak Zhang", "abstract": "  Active view selection in 3D scene reconstruction has been widely studied\nsince training on informative views is critical for reconstruction. Recently,\nNeural Radiance Fields (NeRF) variants have shown promising results in active\n3D reconstruction using uncertainty-guided view selection. They utilize\nuncertainties estimated with neural networks that encode scene geometry and\nappearance. However, the choice of uncertainty integration methods, either\nvoxel-based or neural rendering, has conventionally depended on the types of\nscene uncertainty being estimated, whether geometric or appearance-related. In\nthis paper, we introduce Colorized Surface Voxel (CSV)-based view selection, a\nnew next-best view (NBV) selection method exploiting surface voxel-based\nmeasurement of uncertainty in scene appearance. CSV encapsulates the\nuncertainty of estimated scene appearance (e.g., color uncertainty) and\nestimated geometric information (e.g., surface). Using the geometry\ninformation, we interpret the uncertainty of scene appearance 3D-wise during\nthe aggregation of the per-voxel uncertainty. Consequently, the uncertainty\nfrom occluded and complex regions is recognized under challenging scenarios\nwith limited input data. Our method outperforms previous works on popular\ndatasets, DTU and Blender, and our new dataset with imbalanced viewpoints,\nshowing that the CSV-based view selection significantly improves performance by\nup to 30%.\n", "link": "http://arxiv.org/abs/2405.02568v2", "date": "2024-06-10", "relevancy": 3.0462, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6158}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6158}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Neural%203D%20Reconstruction%20with%20Colorized%20Surface%20Voxel-based%20View%0A%20%20Selection&body=Title%3A%20Active%20Neural%203D%20Reconstruction%20with%20Colorized%20Surface%20Voxel-based%20View%0A%20%20Selection%0AAuthor%3A%20Hyunseo%20Kim%20and%20Hyeonseo%20Yang%20and%20Taekyung%20Kim%20and%20YoonSung%20Kim%20and%20Jin-Hwa%20Kim%20and%20Byoung-Tak%20Zhang%0AAbstract%3A%20%20%20Active%20view%20selection%20in%203D%20scene%20reconstruction%20has%20been%20widely%20studied%0Asince%20training%20on%20informative%20views%20is%20critical%20for%20reconstruction.%20Recently%2C%0ANeural%20Radiance%20Fields%20%28NeRF%29%20variants%20have%20shown%20promising%20results%20in%20active%0A3D%20reconstruction%20using%20uncertainty-guided%20view%20selection.%20They%20utilize%0Auncertainties%20estimated%20with%20neural%20networks%20that%20encode%20scene%20geometry%20and%0Aappearance.%20However%2C%20the%20choice%20of%20uncertainty%20integration%20methods%2C%20either%0Avoxel-based%20or%20neural%20rendering%2C%20has%20conventionally%20depended%20on%20the%20types%20of%0Ascene%20uncertainty%20being%20estimated%2C%20whether%20geometric%20or%20appearance-related.%20In%0Athis%20paper%2C%20we%20introduce%20Colorized%20Surface%20Voxel%20%28CSV%29-based%20view%20selection%2C%20a%0Anew%20next-best%20view%20%28NBV%29%20selection%20method%20exploiting%20surface%20voxel-based%0Ameasurement%20of%20uncertainty%20in%20scene%20appearance.%20CSV%20encapsulates%20the%0Auncertainty%20of%20estimated%20scene%20appearance%20%28e.g.%2C%20color%20uncertainty%29%20and%0Aestimated%20geometric%20information%20%28e.g.%2C%20surface%29.%20Using%20the%20geometry%0Ainformation%2C%20we%20interpret%20the%20uncertainty%20of%20scene%20appearance%203D-wise%20during%0Athe%20aggregation%20of%20the%20per-voxel%20uncertainty.%20Consequently%2C%20the%20uncertainty%0Afrom%20occluded%20and%20complex%20regions%20is%20recognized%20under%20challenging%20scenarios%0Awith%20limited%20input%20data.%20Our%20method%20outperforms%20previous%20works%20on%20popular%0Adatasets%2C%20DTU%20and%20Blender%2C%20and%20our%20new%20dataset%20with%20imbalanced%20viewpoints%2C%0Ashowing%20that%20the%20CSV-based%20view%20selection%20significantly%20improves%20performance%20by%0Aup%20to%2030%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Neural%25203D%2520Reconstruction%2520with%2520Colorized%2520Surface%2520Voxel-based%2520View%250A%2520%2520Selection%26entry.906535625%3DHyunseo%2520Kim%2520and%2520Hyeonseo%2520Yang%2520and%2520Taekyung%2520Kim%2520and%2520YoonSung%2520Kim%2520and%2520Jin-Hwa%2520Kim%2520and%2520Byoung-Tak%2520Zhang%26entry.1292438233%3D%2520%2520Active%2520view%2520selection%2520in%25203D%2520scene%2520reconstruction%2520has%2520been%2520widely%2520studied%250Asince%2520training%2520on%2520informative%2520views%2520is%2520critical%2520for%2520reconstruction.%2520Recently%252C%250ANeural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520variants%2520have%2520shown%2520promising%2520results%2520in%2520active%250A3D%2520reconstruction%2520using%2520uncertainty-guided%2520view%2520selection.%2520They%2520utilize%250Auncertainties%2520estimated%2520with%2520neural%2520networks%2520that%2520encode%2520scene%2520geometry%2520and%250Aappearance.%2520However%252C%2520the%2520choice%2520of%2520uncertainty%2520integration%2520methods%252C%2520either%250Avoxel-based%2520or%2520neural%2520rendering%252C%2520has%2520conventionally%2520depended%2520on%2520the%2520types%2520of%250Ascene%2520uncertainty%2520being%2520estimated%252C%2520whether%2520geometric%2520or%2520appearance-related.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Colorized%2520Surface%2520Voxel%2520%2528CSV%2529-based%2520view%2520selection%252C%2520a%250Anew%2520next-best%2520view%2520%2528NBV%2529%2520selection%2520method%2520exploiting%2520surface%2520voxel-based%250Ameasurement%2520of%2520uncertainty%2520in%2520scene%2520appearance.%2520CSV%2520encapsulates%2520the%250Auncertainty%2520of%2520estimated%2520scene%2520appearance%2520%2528e.g.%252C%2520color%2520uncertainty%2529%2520and%250Aestimated%2520geometric%2520information%2520%2528e.g.%252C%2520surface%2529.%2520Using%2520the%2520geometry%250Ainformation%252C%2520we%2520interpret%2520the%2520uncertainty%2520of%2520scene%2520appearance%25203D-wise%2520during%250Athe%2520aggregation%2520of%2520the%2520per-voxel%2520uncertainty.%2520Consequently%252C%2520the%2520uncertainty%250Afrom%2520occluded%2520and%2520complex%2520regions%2520is%2520recognized%2520under%2520challenging%2520scenarios%250Awith%2520limited%2520input%2520data.%2520Our%2520method%2520outperforms%2520previous%2520works%2520on%2520popular%250Adatasets%252C%2520DTU%2520and%2520Blender%252C%2520and%2520our%2520new%2520dataset%2520with%2520imbalanced%2520viewpoints%252C%250Ashowing%2520that%2520the%2520CSV-based%2520view%2520selection%2520significantly%2520improves%2520performance%2520by%250Aup%2520to%252030%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Neural%203D%20Reconstruction%20with%20Colorized%20Surface%20Voxel-based%20View%0A%20%20Selection&entry.906535625=Hyunseo%20Kim%20and%20Hyeonseo%20Yang%20and%20Taekyung%20Kim%20and%20YoonSung%20Kim%20and%20Jin-Hwa%20Kim%20and%20Byoung-Tak%20Zhang&entry.1292438233=%20%20Active%20view%20selection%20in%203D%20scene%20reconstruction%20has%20been%20widely%20studied%0Asince%20training%20on%20informative%20views%20is%20critical%20for%20reconstruction.%20Recently%2C%0ANeural%20Radiance%20Fields%20%28NeRF%29%20variants%20have%20shown%20promising%20results%20in%20active%0A3D%20reconstruction%20using%20uncertainty-guided%20view%20selection.%20They%20utilize%0Auncertainties%20estimated%20with%20neural%20networks%20that%20encode%20scene%20geometry%20and%0Aappearance.%20However%2C%20the%20choice%20of%20uncertainty%20integration%20methods%2C%20either%0Avoxel-based%20or%20neural%20rendering%2C%20has%20conventionally%20depended%20on%20the%20types%20of%0Ascene%20uncertainty%20being%20estimated%2C%20whether%20geometric%20or%20appearance-related.%20In%0Athis%20paper%2C%20we%20introduce%20Colorized%20Surface%20Voxel%20%28CSV%29-based%20view%20selection%2C%20a%0Anew%20next-best%20view%20%28NBV%29%20selection%20method%20exploiting%20surface%20voxel-based%0Ameasurement%20of%20uncertainty%20in%20scene%20appearance.%20CSV%20encapsulates%20the%0Auncertainty%20of%20estimated%20scene%20appearance%20%28e.g.%2C%20color%20uncertainty%29%20and%0Aestimated%20geometric%20information%20%28e.g.%2C%20surface%29.%20Using%20the%20geometry%0Ainformation%2C%20we%20interpret%20the%20uncertainty%20of%20scene%20appearance%203D-wise%20during%0Athe%20aggregation%20of%20the%20per-voxel%20uncertainty.%20Consequently%2C%20the%20uncertainty%0Afrom%20occluded%20and%20complex%20regions%20is%20recognized%20under%20challenging%20scenarios%0Awith%20limited%20input%20data.%20Our%20method%20outperforms%20previous%20works%20on%20popular%0Adatasets%2C%20DTU%20and%20Blender%2C%20and%20our%20new%20dataset%20with%20imbalanced%20viewpoints%2C%0Ashowing%20that%20the%20CSV-based%20view%20selection%20significantly%20improves%20performance%20by%0Aup%20to%2030%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02568v2&entry.124074799=Read"},
{"title": "GenHeld: Generating and Editing Handheld Objects", "author": "Chaerin Min and Srinath Sridhar", "abstract": "  Grasping is an important human activity that has long been studied in\nrobotics, computer vision, and cognitive science. Most existing works study\ngrasping from the perspective of synthesizing hand poses conditioned on 3D or\n2D object representations. We propose GenHeld to address the inverse problem of\nsynthesizing held objects conditioned on 3D hand model or 2D image. Given a 3D\nmodel of hand, GenHeld 3D can select a plausible held object from a large\ndataset using compact object representations called object codes.The selected\nobject is then positioned and oriented to form a plausible grasp without\nchanging hand pose. If only a 2D hand image is available, GenHeld 2D can edit\nthis image to add or replace a held object. GenHeld 2D operates by combining\nthe abilities of GenHeld 3D with diffusion-based image editing. Results and\nexperiments show that we outperform baselines and can generate plausible held\nobjects in both 2D and 3D. Our experiments demonstrate that our method achieves\nhigh quality and plausibility of held object synthesis in both 3D and 2D.\n", "link": "http://arxiv.org/abs/2406.05059v2", "date": "2024-06-10", "relevancy": 3.0341, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6781}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5951}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects&body=Title%3A%20GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects%0AAuthor%3A%20Chaerin%20Min%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Grasping%20is%20an%20important%20human%20activity%20that%20has%20long%20been%20studied%20in%0Arobotics%2C%20computer%20vision%2C%20and%20cognitive%20science.%20Most%20existing%20works%20study%0Agrasping%20from%20the%20perspective%20of%20synthesizing%20hand%20poses%20conditioned%20on%203D%20or%0A2D%20object%20representations.%20We%20propose%20GenHeld%20to%20address%20the%20inverse%20problem%20of%0Asynthesizing%20held%20objects%20conditioned%20on%203D%20hand%20model%20or%202D%20image.%20Given%20a%203D%0Amodel%20of%20hand%2C%20GenHeld%203D%20can%20select%20a%20plausible%20held%20object%20from%20a%20large%0Adataset%20using%20compact%20object%20representations%20called%20object%20codes.The%20selected%0Aobject%20is%20then%20positioned%20and%20oriented%20to%20form%20a%20plausible%20grasp%20without%0Achanging%20hand%20pose.%20If%20only%20a%202D%20hand%20image%20is%20available%2C%20GenHeld%202D%20can%20edit%0Athis%20image%20to%20add%20or%20replace%20a%20held%20object.%20GenHeld%202D%20operates%20by%20combining%0Athe%20abilities%20of%20GenHeld%203D%20with%20diffusion-based%20image%20editing.%20Results%20and%0Aexperiments%20show%20that%20we%20outperform%20baselines%20and%20can%20generate%20plausible%20held%0Aobjects%20in%20both%202D%20and%203D.%20Our%20experiments%20demonstrate%20that%20our%20method%20achieves%0Ahigh%20quality%20and%20plausibility%20of%20held%20object%20synthesis%20in%20both%203D%20and%202D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenHeld%253A%2520Generating%2520and%2520Editing%2520Handheld%2520Objects%26entry.906535625%3DChaerin%2520Min%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3D%2520%2520Grasping%2520is%2520an%2520important%2520human%2520activity%2520that%2520has%2520long%2520been%2520studied%2520in%250Arobotics%252C%2520computer%2520vision%252C%2520and%2520cognitive%2520science.%2520Most%2520existing%2520works%2520study%250Agrasping%2520from%2520the%2520perspective%2520of%2520synthesizing%2520hand%2520poses%2520conditioned%2520on%25203D%2520or%250A2D%2520object%2520representations.%2520We%2520propose%2520GenHeld%2520to%2520address%2520the%2520inverse%2520problem%2520of%250Asynthesizing%2520held%2520objects%2520conditioned%2520on%25203D%2520hand%2520model%2520or%25202D%2520image.%2520Given%2520a%25203D%250Amodel%2520of%2520hand%252C%2520GenHeld%25203D%2520can%2520select%2520a%2520plausible%2520held%2520object%2520from%2520a%2520large%250Adataset%2520using%2520compact%2520object%2520representations%2520called%2520object%2520codes.The%2520selected%250Aobject%2520is%2520then%2520positioned%2520and%2520oriented%2520to%2520form%2520a%2520plausible%2520grasp%2520without%250Achanging%2520hand%2520pose.%2520If%2520only%2520a%25202D%2520hand%2520image%2520is%2520available%252C%2520GenHeld%25202D%2520can%2520edit%250Athis%2520image%2520to%2520add%2520or%2520replace%2520a%2520held%2520object.%2520GenHeld%25202D%2520operates%2520by%2520combining%250Athe%2520abilities%2520of%2520GenHeld%25203D%2520with%2520diffusion-based%2520image%2520editing.%2520Results%2520and%250Aexperiments%2520show%2520that%2520we%2520outperform%2520baselines%2520and%2520can%2520generate%2520plausible%2520held%250Aobjects%2520in%2520both%25202D%2520and%25203D.%2520Our%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Ahigh%2520quality%2520and%2520plausibility%2520of%2520held%2520object%2520synthesis%2520in%2520both%25203D%2520and%25202D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects&entry.906535625=Chaerin%20Min%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Grasping%20is%20an%20important%20human%20activity%20that%20has%20long%20been%20studied%20in%0Arobotics%2C%20computer%20vision%2C%20and%20cognitive%20science.%20Most%20existing%20works%20study%0Agrasping%20from%20the%20perspective%20of%20synthesizing%20hand%20poses%20conditioned%20on%203D%20or%0A2D%20object%20representations.%20We%20propose%20GenHeld%20to%20address%20the%20inverse%20problem%20of%0Asynthesizing%20held%20objects%20conditioned%20on%203D%20hand%20model%20or%202D%20image.%20Given%20a%203D%0Amodel%20of%20hand%2C%20GenHeld%203D%20can%20select%20a%20plausible%20held%20object%20from%20a%20large%0Adataset%20using%20compact%20object%20representations%20called%20object%20codes.The%20selected%0Aobject%20is%20then%20positioned%20and%20oriented%20to%20form%20a%20plausible%20grasp%20without%0Achanging%20hand%20pose.%20If%20only%20a%202D%20hand%20image%20is%20available%2C%20GenHeld%202D%20can%20edit%0Athis%20image%20to%20add%20or%20replace%20a%20held%20object.%20GenHeld%202D%20operates%20by%20combining%0Athe%20abilities%20of%20GenHeld%203D%20with%20diffusion-based%20image%20editing.%20Results%20and%0Aexperiments%20show%20that%20we%20outperform%20baselines%20and%20can%20generate%20plausible%20held%0Aobjects%20in%20both%202D%20and%203D.%20Our%20experiments%20demonstrate%20that%20our%20method%20achieves%0Ahigh%20quality%20and%20plausibility%20of%20held%20object%20synthesis%20in%20both%203D%20and%202D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05059v2&entry.124074799=Read"},
{"title": "Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era", "author": "Chenghao Li and Chaoning Zhang and Atish Waghwase and Lik-Hang Lee and Francois Rameau and Yang Yang and Sung-Ho Bae and Choong Seon Hong", "abstract": "  Generative AI (AIGC, a.k.a. AI generated content) has made significant\nprogress in recent years, with text-guided content generation being the most\npractical as it facilitates interaction between human instructions and AIGC.\nDue to advancements in text-to-image and 3D modeling technologies (like NeRF),\ntext-to-3D has emerged as a nascent yet highly active research field. Our work\nconducts the first comprehensive survey and follows up on subsequent research\nprogress in the overall field, aiming to help readers interested in this\ndirection quickly catch up with its rapid development. First, we introduce 3D\ndata representations, including both Euclidean and non-Euclidean data. Building\non this foundation, we introduce various foundational technologies and\nsummarize how recent work combines these foundational technologies to achieve\nsatisfactory text-to-3D results. Additionally, we present mainstream baselines\nand research directions in recent text-to-3D technology, including fidelity,\nefficiency, consistency, controllability, diversity, and applicability.\nFurthermore, we summarize the usage of text-to-3D technology in various\napplications, including avatar generation, texture generation, shape editing,\nand scene generation.\n", "link": "http://arxiv.org/abs/2305.06131v3", "date": "2024-06-10", "relevancy": 2.9795, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6254}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5843}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20meets%203D%3A%20A%20Survey%20on%20Text-to-3D%20in%20AIGC%20Era&body=Title%3A%20Generative%20AI%20meets%203D%3A%20A%20Survey%20on%20Text-to-3D%20in%20AIGC%20Era%0AAuthor%3A%20Chenghao%20Li%20and%20Chaoning%20Zhang%20and%20Atish%20Waghwase%20and%20Lik-Hang%20Lee%20and%20Francois%20Rameau%20and%20Yang%20Yang%20and%20Sung-Ho%20Bae%20and%20Choong%20Seon%20Hong%0AAbstract%3A%20%20%20Generative%20AI%20%28AIGC%2C%20a.k.a.%20AI%20generated%20content%29%20has%20made%20significant%0Aprogress%20in%20recent%20years%2C%20with%20text-guided%20content%20generation%20being%20the%20most%0Apractical%20as%20it%20facilitates%20interaction%20between%20human%20instructions%20and%20AIGC.%0ADue%20to%20advancements%20in%20text-to-image%20and%203D%20modeling%20technologies%20%28like%20NeRF%29%2C%0Atext-to-3D%20has%20emerged%20as%20a%20nascent%20yet%20highly%20active%20research%20field.%20Our%20work%0Aconducts%20the%20first%20comprehensive%20survey%20and%20follows%20up%20on%20subsequent%20research%0Aprogress%20in%20the%20overall%20field%2C%20aiming%20to%20help%20readers%20interested%20in%20this%0Adirection%20quickly%20catch%20up%20with%20its%20rapid%20development.%20First%2C%20we%20introduce%203D%0Adata%20representations%2C%20including%20both%20Euclidean%20and%20non-Euclidean%20data.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20various%20foundational%20technologies%20and%0Asummarize%20how%20recent%20work%20combines%20these%20foundational%20technologies%20to%20achieve%0Asatisfactory%20text-to-3D%20results.%20Additionally%2C%20we%20present%20mainstream%20baselines%0Aand%20research%20directions%20in%20recent%20text-to-3D%20technology%2C%20including%20fidelity%2C%0Aefficiency%2C%20consistency%2C%20controllability%2C%20diversity%2C%20and%20applicability.%0AFurthermore%2C%20we%20summarize%20the%20usage%20of%20text-to-3D%20technology%20in%20various%0Aapplications%2C%20including%20avatar%20generation%2C%20texture%20generation%2C%20shape%20editing%2C%0Aand%20scene%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06131v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520meets%25203D%253A%2520A%2520Survey%2520on%2520Text-to-3D%2520in%2520AIGC%2520Era%26entry.906535625%3DChenghao%2520Li%2520and%2520Chaoning%2520Zhang%2520and%2520Atish%2520Waghwase%2520and%2520Lik-Hang%2520Lee%2520and%2520Francois%2520Rameau%2520and%2520Yang%2520Yang%2520and%2520Sung-Ho%2520Bae%2520and%2520Choong%2520Seon%2520Hong%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528AIGC%252C%2520a.k.a.%2520AI%2520generated%2520content%2529%2520has%2520made%2520significant%250Aprogress%2520in%2520recent%2520years%252C%2520with%2520text-guided%2520content%2520generation%2520being%2520the%2520most%250Apractical%2520as%2520it%2520facilitates%2520interaction%2520between%2520human%2520instructions%2520and%2520AIGC.%250ADue%2520to%2520advancements%2520in%2520text-to-image%2520and%25203D%2520modeling%2520technologies%2520%2528like%2520NeRF%2529%252C%250Atext-to-3D%2520has%2520emerged%2520as%2520a%2520nascent%2520yet%2520highly%2520active%2520research%2520field.%2520Our%2520work%250Aconducts%2520the%2520first%2520comprehensive%2520survey%2520and%2520follows%2520up%2520on%2520subsequent%2520research%250Aprogress%2520in%2520the%2520overall%2520field%252C%2520aiming%2520to%2520help%2520readers%2520interested%2520in%2520this%250Adirection%2520quickly%2520catch%2520up%2520with%2520its%2520rapid%2520development.%2520First%252C%2520we%2520introduce%25203D%250Adata%2520representations%252C%2520including%2520both%2520Euclidean%2520and%2520non-Euclidean%2520data.%2520Building%250Aon%2520this%2520foundation%252C%2520we%2520introduce%2520various%2520foundational%2520technologies%2520and%250Asummarize%2520how%2520recent%2520work%2520combines%2520these%2520foundational%2520technologies%2520to%2520achieve%250Asatisfactory%2520text-to-3D%2520results.%2520Additionally%252C%2520we%2520present%2520mainstream%2520baselines%250Aand%2520research%2520directions%2520in%2520recent%2520text-to-3D%2520technology%252C%2520including%2520fidelity%252C%250Aefficiency%252C%2520consistency%252C%2520controllability%252C%2520diversity%252C%2520and%2520applicability.%250AFurthermore%252C%2520we%2520summarize%2520the%2520usage%2520of%2520text-to-3D%2520technology%2520in%2520various%250Aapplications%252C%2520including%2520avatar%2520generation%252C%2520texture%2520generation%252C%2520shape%2520editing%252C%250Aand%2520scene%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.06131v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20meets%203D%3A%20A%20Survey%20on%20Text-to-3D%20in%20AIGC%20Era&entry.906535625=Chenghao%20Li%20and%20Chaoning%20Zhang%20and%20Atish%20Waghwase%20and%20Lik-Hang%20Lee%20and%20Francois%20Rameau%20and%20Yang%20Yang%20and%20Sung-Ho%20Bae%20and%20Choong%20Seon%20Hong&entry.1292438233=%20%20Generative%20AI%20%28AIGC%2C%20a.k.a.%20AI%20generated%20content%29%20has%20made%20significant%0Aprogress%20in%20recent%20years%2C%20with%20text-guided%20content%20generation%20being%20the%20most%0Apractical%20as%20it%20facilitates%20interaction%20between%20human%20instructions%20and%20AIGC.%0ADue%20to%20advancements%20in%20text-to-image%20and%203D%20modeling%20technologies%20%28like%20NeRF%29%2C%0Atext-to-3D%20has%20emerged%20as%20a%20nascent%20yet%20highly%20active%20research%20field.%20Our%20work%0Aconducts%20the%20first%20comprehensive%20survey%20and%20follows%20up%20on%20subsequent%20research%0Aprogress%20in%20the%20overall%20field%2C%20aiming%20to%20help%20readers%20interested%20in%20this%0Adirection%20quickly%20catch%20up%20with%20its%20rapid%20development.%20First%2C%20we%20introduce%203D%0Adata%20representations%2C%20including%20both%20Euclidean%20and%20non-Euclidean%20data.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20various%20foundational%20technologies%20and%0Asummarize%20how%20recent%20work%20combines%20these%20foundational%20technologies%20to%20achieve%0Asatisfactory%20text-to-3D%20results.%20Additionally%2C%20we%20present%20mainstream%20baselines%0Aand%20research%20directions%20in%20recent%20text-to-3D%20technology%2C%20including%20fidelity%2C%0Aefficiency%2C%20consistency%2C%20controllability%2C%20diversity%2C%20and%20applicability.%0AFurthermore%2C%20we%20summarize%20the%20usage%20of%20text-to-3D%20technology%20in%20various%0Aapplications%2C%20including%20avatar%20generation%2C%20texture%20generation%2C%20shape%20editing%2C%0Aand%20scene%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06131v3&entry.124074799=Read"},
{"title": "SYM3D: Learning Symmetric Triplanes for Better 3D-Awareness of GANs", "author": "Jing Yang and Kyle Fogarty and Fangcheng Zhong and Cengiz Oztireli", "abstract": "  Despite the growing success of 3D-aware GANs, which can be trained on 2D\nimages to generate high-quality 3D assets, they still rely on multi-view images\nwith camera annotations to synthesize sufficient details from all viewing\ndirections. However, the scarce availability of calibrated multi-view image\ndatasets, especially in comparison to single-view images, has limited the\npotential of 3D GANs. Moreover, while bypassing camera pose annotations with a\ncamera distribution constraint reduces dependence on exact camera parameters,\nit still struggles to generate a consistent orientation of 3D assets. To this\nend, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent\nreflectional symmetry structure found in natural and man-made objects,\nalongside a proposed view-aware spatial attention mechanism in learning the 3D\nrepresentation. We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and\nAirplanes) and real-world datasets (ABO-Chair), demonstrating its superior\nperformance in capturing detailed geometry and texture, even when trained on\nonly single-view images. Finally, we demonstrate the effectiveness of\nincorporating symmetry regularization in helping reduce artifacts in the\nmodeling of 3D assets in the text-to-3D task.\n", "link": "http://arxiv.org/abs/2406.06432v1", "date": "2024-06-10", "relevancy": 2.9566, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6189}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs&body=Title%3A%20SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs%0AAuthor%3A%20Jing%20Yang%20and%20Kyle%20Fogarty%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20Despite%20the%20growing%20success%20of%203D-aware%20GANs%2C%20which%20can%20be%20trained%20on%202D%0Aimages%20to%20generate%20high-quality%203D%20assets%2C%20they%20still%20rely%20on%20multi-view%20images%0Awith%20camera%20annotations%20to%20synthesize%20sufficient%20details%20from%20all%20viewing%0Adirections.%20However%2C%20the%20scarce%20availability%20of%20calibrated%20multi-view%20image%0Adatasets%2C%20especially%20in%20comparison%20to%20single-view%20images%2C%20has%20limited%20the%0Apotential%20of%203D%20GANs.%20Moreover%2C%20while%20bypassing%20camera%20pose%20annotations%20with%20a%0Acamera%20distribution%20constraint%20reduces%20dependence%20on%20exact%20camera%20parameters%2C%0Ait%20still%20struggles%20to%20generate%20a%20consistent%20orientation%20of%203D%20assets.%20To%20this%0Aend%2C%20we%20propose%20SYM3D%2C%20a%20novel%203D-aware%20GAN%20designed%20to%20leverage%20the%20prevalent%0Areflectional%20symmetry%20structure%20found%20in%20natural%20and%20man-made%20objects%2C%0Aalongside%20a%20proposed%20view-aware%20spatial%20attention%20mechanism%20in%20learning%20the%203D%0Arepresentation.%20We%20evaluate%20SYM3D%20on%20both%20synthetic%20%28ShapeNet%20Chairs%2C%20Cars%2C%20and%0AAirplanes%29%20and%20real-world%20datasets%20%28ABO-Chair%29%2C%20demonstrating%20its%20superior%0Aperformance%20in%20capturing%20detailed%20geometry%20and%20texture%2C%20even%20when%20trained%20on%0Aonly%20single-view%20images.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aincorporating%20symmetry%20regularization%20in%20helping%20reduce%20artifacts%20in%20the%0Amodeling%20of%203D%20assets%20in%20the%20text-to-3D%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSYM3D%253A%2520Learning%2520Symmetric%2520Triplanes%2520for%2520Better%25203D-Awareness%2520of%2520GANs%26entry.906535625%3DJing%2520Yang%2520and%2520Kyle%2520Fogarty%2520and%2520Fangcheng%2520Zhong%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520Despite%2520the%2520growing%2520success%2520of%25203D-aware%2520GANs%252C%2520which%2520can%2520be%2520trained%2520on%25202D%250Aimages%2520to%2520generate%2520high-quality%25203D%2520assets%252C%2520they%2520still%2520rely%2520on%2520multi-view%2520images%250Awith%2520camera%2520annotations%2520to%2520synthesize%2520sufficient%2520details%2520from%2520all%2520viewing%250Adirections.%2520However%252C%2520the%2520scarce%2520availability%2520of%2520calibrated%2520multi-view%2520image%250Adatasets%252C%2520especially%2520in%2520comparison%2520to%2520single-view%2520images%252C%2520has%2520limited%2520the%250Apotential%2520of%25203D%2520GANs.%2520Moreover%252C%2520while%2520bypassing%2520camera%2520pose%2520annotations%2520with%2520a%250Acamera%2520distribution%2520constraint%2520reduces%2520dependence%2520on%2520exact%2520camera%2520parameters%252C%250Ait%2520still%2520struggles%2520to%2520generate%2520a%2520consistent%2520orientation%2520of%25203D%2520assets.%2520To%2520this%250Aend%252C%2520we%2520propose%2520SYM3D%252C%2520a%2520novel%25203D-aware%2520GAN%2520designed%2520to%2520leverage%2520the%2520prevalent%250Areflectional%2520symmetry%2520structure%2520found%2520in%2520natural%2520and%2520man-made%2520objects%252C%250Aalongside%2520a%2520proposed%2520view-aware%2520spatial%2520attention%2520mechanism%2520in%2520learning%2520the%25203D%250Arepresentation.%2520We%2520evaluate%2520SYM3D%2520on%2520both%2520synthetic%2520%2528ShapeNet%2520Chairs%252C%2520Cars%252C%2520and%250AAirplanes%2529%2520and%2520real-world%2520datasets%2520%2528ABO-Chair%2529%252C%2520demonstrating%2520its%2520superior%250Aperformance%2520in%2520capturing%2520detailed%2520geometry%2520and%2520texture%252C%2520even%2520when%2520trained%2520on%250Aonly%2520single-view%2520images.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%250Aincorporating%2520symmetry%2520regularization%2520in%2520helping%2520reduce%2520artifacts%2520in%2520the%250Amodeling%2520of%25203D%2520assets%2520in%2520the%2520text-to-3D%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs&entry.906535625=Jing%20Yang%20and%20Kyle%20Fogarty%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20Despite%20the%20growing%20success%20of%203D-aware%20GANs%2C%20which%20can%20be%20trained%20on%202D%0Aimages%20to%20generate%20high-quality%203D%20assets%2C%20they%20still%20rely%20on%20multi-view%20images%0Awith%20camera%20annotations%20to%20synthesize%20sufficient%20details%20from%20all%20viewing%0Adirections.%20However%2C%20the%20scarce%20availability%20of%20calibrated%20multi-view%20image%0Adatasets%2C%20especially%20in%20comparison%20to%20single-view%20images%2C%20has%20limited%20the%0Apotential%20of%203D%20GANs.%20Moreover%2C%20while%20bypassing%20camera%20pose%20annotations%20with%20a%0Acamera%20distribution%20constraint%20reduces%20dependence%20on%20exact%20camera%20parameters%2C%0Ait%20still%20struggles%20to%20generate%20a%20consistent%20orientation%20of%203D%20assets.%20To%20this%0Aend%2C%20we%20propose%20SYM3D%2C%20a%20novel%203D-aware%20GAN%20designed%20to%20leverage%20the%20prevalent%0Areflectional%20symmetry%20structure%20found%20in%20natural%20and%20man-made%20objects%2C%0Aalongside%20a%20proposed%20view-aware%20spatial%20attention%20mechanism%20in%20learning%20the%203D%0Arepresentation.%20We%20evaluate%20SYM3D%20on%20both%20synthetic%20%28ShapeNet%20Chairs%2C%20Cars%2C%20and%0AAirplanes%29%20and%20real-world%20datasets%20%28ABO-Chair%29%2C%20demonstrating%20its%20superior%0Aperformance%20in%20capturing%20detailed%20geometry%20and%20texture%2C%20even%20when%20trained%20on%0Aonly%20single-view%20images.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aincorporating%20symmetry%20regularization%20in%20helping%20reduce%20artifacts%20in%20the%0Amodeling%20of%203D%20assets%20in%20the%20text-to-3D%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06432v1&entry.124074799=Read"},
{"title": "Towards Transferable Targeted 3D Adversarial Attack in the Physical\n  World", "author": "Yao Huang and Yinpeng Dong and Shouwei Ruan and Xiao Yang and Hang Su and Xingxing Wei", "abstract": "  Compared with transferable untargeted attacks, transferable targeted\nadversarial attacks could specify the misclassification categories of\nadversarial samples, posing a greater threat to security-critical tasks. In the\nmeanwhile, 3D adversarial samples, due to their potential of multi-view\nrobustness, can more comprehensively identify weaknesses in existing deep\nlearning systems, possessing great application value. However, the field of\ntransferable targeted 3D adversarial attacks remains vacant. The goal of this\nwork is to develop a more effective technique that could generate transferable\ntargeted 3D adversarial examples, filling the gap in this field. To achieve\nthis goal, we design a novel framework named TT3D that could rapidly\nreconstruct from few multi-view images into Transferable Targeted 3D textured\nmeshes. While existing mesh-based texture optimization methods compute\ngradients in the high-dimensional mesh space and easily fall into local optima,\nleading to unsatisfactory transferability and distinct distortions, TT3D\ninnovatively performs dual optimization towards both feature grid and\nMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, which\nsignificantly enhances black-box transferability while enjoying naturalness.\nExperimental results show that TT3D not only exhibits superior cross-model\ntransferability but also maintains considerable adaptability across different\nrenders and vision tasks. More importantly, we produce 3D adversarial examples\nwith 3D printing techniques in the real world and verify their robust\nperformance under various scenarios.\n", "link": "http://arxiv.org/abs/2312.09558v3", "date": "2024-06-10", "relevancy": 2.9361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6192}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5712}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Transferable%20Targeted%203D%20Adversarial%20Attack%20in%20the%20Physical%0A%20%20World&body=Title%3A%20Towards%20Transferable%20Targeted%203D%20Adversarial%20Attack%20in%20the%20Physical%0A%20%20World%0AAuthor%3A%20Yao%20Huang%20and%20Yinpeng%20Dong%20and%20Shouwei%20Ruan%20and%20Xiao%20Yang%20and%20Hang%20Su%20and%20Xingxing%20Wei%0AAbstract%3A%20%20%20Compared%20with%20transferable%20untargeted%20attacks%2C%20transferable%20targeted%0Aadversarial%20attacks%20could%20specify%20the%20misclassification%20categories%20of%0Aadversarial%20samples%2C%20posing%20a%20greater%20threat%20to%20security-critical%20tasks.%20In%20the%0Ameanwhile%2C%203D%20adversarial%20samples%2C%20due%20to%20their%20potential%20of%20multi-view%0Arobustness%2C%20can%20more%20comprehensively%20identify%20weaknesses%20in%20existing%20deep%0Alearning%20systems%2C%20possessing%20great%20application%20value.%20However%2C%20the%20field%20of%0Atransferable%20targeted%203D%20adversarial%20attacks%20remains%20vacant.%20The%20goal%20of%20this%0Awork%20is%20to%20develop%20a%20more%20effective%20technique%20that%20could%20generate%20transferable%0Atargeted%203D%20adversarial%20examples%2C%20filling%20the%20gap%20in%20this%20field.%20To%20achieve%0Athis%20goal%2C%20we%20design%20a%20novel%20framework%20named%20TT3D%20that%20could%20rapidly%0Areconstruct%20from%20few%20multi-view%20images%20into%20Transferable%20Targeted%203D%20textured%0Ameshes.%20While%20existing%20mesh-based%20texture%20optimization%20methods%20compute%0Agradients%20in%20the%20high-dimensional%20mesh%20space%20and%20easily%20fall%20into%20local%20optima%2C%0Aleading%20to%20unsatisfactory%20transferability%20and%20distinct%20distortions%2C%20TT3D%0Ainnovatively%20performs%20dual%20optimization%20towards%20both%20feature%20grid%20and%0AMulti-layer%20Perceptron%20%28MLP%29%20parameters%20in%20the%20grid-based%20NeRF%20space%2C%20which%0Asignificantly%20enhances%20black-box%20transferability%20while%20enjoying%20naturalness.%0AExperimental%20results%20show%20that%20TT3D%20not%20only%20exhibits%20superior%20cross-model%0Atransferability%20but%20also%20maintains%20considerable%20adaptability%20across%20different%0Arenders%20and%20vision%20tasks.%20More%20importantly%2C%20we%20produce%203D%20adversarial%20examples%0Awith%203D%20printing%20techniques%20in%20the%20real%20world%20and%20verify%20their%20robust%0Aperformance%20under%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09558v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Transferable%2520Targeted%25203D%2520Adversarial%2520Attack%2520in%2520the%2520Physical%250A%2520%2520World%26entry.906535625%3DYao%2520Huang%2520and%2520Yinpeng%2520Dong%2520and%2520Shouwei%2520Ruan%2520and%2520Xiao%2520Yang%2520and%2520Hang%2520Su%2520and%2520Xingxing%2520Wei%26entry.1292438233%3D%2520%2520Compared%2520with%2520transferable%2520untargeted%2520attacks%252C%2520transferable%2520targeted%250Aadversarial%2520attacks%2520could%2520specify%2520the%2520misclassification%2520categories%2520of%250Aadversarial%2520samples%252C%2520posing%2520a%2520greater%2520threat%2520to%2520security-critical%2520tasks.%2520In%2520the%250Ameanwhile%252C%25203D%2520adversarial%2520samples%252C%2520due%2520to%2520their%2520potential%2520of%2520multi-view%250Arobustness%252C%2520can%2520more%2520comprehensively%2520identify%2520weaknesses%2520in%2520existing%2520deep%250Alearning%2520systems%252C%2520possessing%2520great%2520application%2520value.%2520However%252C%2520the%2520field%2520of%250Atransferable%2520targeted%25203D%2520adversarial%2520attacks%2520remains%2520vacant.%2520The%2520goal%2520of%2520this%250Awork%2520is%2520to%2520develop%2520a%2520more%2520effective%2520technique%2520that%2520could%2520generate%2520transferable%250Atargeted%25203D%2520adversarial%2520examples%252C%2520filling%2520the%2520gap%2520in%2520this%2520field.%2520To%2520achieve%250Athis%2520goal%252C%2520we%2520design%2520a%2520novel%2520framework%2520named%2520TT3D%2520that%2520could%2520rapidly%250Areconstruct%2520from%2520few%2520multi-view%2520images%2520into%2520Transferable%2520Targeted%25203D%2520textured%250Ameshes.%2520While%2520existing%2520mesh-based%2520texture%2520optimization%2520methods%2520compute%250Agradients%2520in%2520the%2520high-dimensional%2520mesh%2520space%2520and%2520easily%2520fall%2520into%2520local%2520optima%252C%250Aleading%2520to%2520unsatisfactory%2520transferability%2520and%2520distinct%2520distortions%252C%2520TT3D%250Ainnovatively%2520performs%2520dual%2520optimization%2520towards%2520both%2520feature%2520grid%2520and%250AMulti-layer%2520Perceptron%2520%2528MLP%2529%2520parameters%2520in%2520the%2520grid-based%2520NeRF%2520space%252C%2520which%250Asignificantly%2520enhances%2520black-box%2520transferability%2520while%2520enjoying%2520naturalness.%250AExperimental%2520results%2520show%2520that%2520TT3D%2520not%2520only%2520exhibits%2520superior%2520cross-model%250Atransferability%2520but%2520also%2520maintains%2520considerable%2520adaptability%2520across%2520different%250Arenders%2520and%2520vision%2520tasks.%2520More%2520importantly%252C%2520we%2520produce%25203D%2520adversarial%2520examples%250Awith%25203D%2520printing%2520techniques%2520in%2520the%2520real%2520world%2520and%2520verify%2520their%2520robust%250Aperformance%2520under%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09558v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Transferable%20Targeted%203D%20Adversarial%20Attack%20in%20the%20Physical%0A%20%20World&entry.906535625=Yao%20Huang%20and%20Yinpeng%20Dong%20and%20Shouwei%20Ruan%20and%20Xiao%20Yang%20and%20Hang%20Su%20and%20Xingxing%20Wei&entry.1292438233=%20%20Compared%20with%20transferable%20untargeted%20attacks%2C%20transferable%20targeted%0Aadversarial%20attacks%20could%20specify%20the%20misclassification%20categories%20of%0Aadversarial%20samples%2C%20posing%20a%20greater%20threat%20to%20security-critical%20tasks.%20In%20the%0Ameanwhile%2C%203D%20adversarial%20samples%2C%20due%20to%20their%20potential%20of%20multi-view%0Arobustness%2C%20can%20more%20comprehensively%20identify%20weaknesses%20in%20existing%20deep%0Alearning%20systems%2C%20possessing%20great%20application%20value.%20However%2C%20the%20field%20of%0Atransferable%20targeted%203D%20adversarial%20attacks%20remains%20vacant.%20The%20goal%20of%20this%0Awork%20is%20to%20develop%20a%20more%20effective%20technique%20that%20could%20generate%20transferable%0Atargeted%203D%20adversarial%20examples%2C%20filling%20the%20gap%20in%20this%20field.%20To%20achieve%0Athis%20goal%2C%20we%20design%20a%20novel%20framework%20named%20TT3D%20that%20could%20rapidly%0Areconstruct%20from%20few%20multi-view%20images%20into%20Transferable%20Targeted%203D%20textured%0Ameshes.%20While%20existing%20mesh-based%20texture%20optimization%20methods%20compute%0Agradients%20in%20the%20high-dimensional%20mesh%20space%20and%20easily%20fall%20into%20local%20optima%2C%0Aleading%20to%20unsatisfactory%20transferability%20and%20distinct%20distortions%2C%20TT3D%0Ainnovatively%20performs%20dual%20optimization%20towards%20both%20feature%20grid%20and%0AMulti-layer%20Perceptron%20%28MLP%29%20parameters%20in%20the%20grid-based%20NeRF%20space%2C%20which%0Asignificantly%20enhances%20black-box%20transferability%20while%20enjoying%20naturalness.%0AExperimental%20results%20show%20that%20TT3D%20not%20only%20exhibits%20superior%20cross-model%0Atransferability%20but%20also%20maintains%20considerable%20adaptability%20across%20different%0Arenders%20and%20vision%20tasks.%20More%20importantly%2C%20we%20produce%203D%20adversarial%20examples%0Awith%203D%20printing%20techniques%20in%20the%20real%20world%20and%20verify%20their%20robust%0Aperformance%20under%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09558v3&entry.124074799=Read"},
{"title": "An Effective-Efficient Approach for Dense Multi-Label Action Detection", "author": "Faegheh Sardari and Armin Mustafa and Philip J. B. Jackson and Adrian Hilton", "abstract": "  Unlike the sparse label action detection task, where a single action occurs\nin each timestamp of a video, in a dense multi-label scenario, actions can\noverlap. To address this challenging task, it is necessary to simultaneously\nlearn (i) temporal dependencies and (ii) co-occurrence action relationships.\nRecent approaches model temporal information by extracting multi-scale features\nthrough hierarchical transformer-based networks. However, the self-attention\nmechanism in transformers inherently loses temporal positional information. We\nargue that combining this with multiple sub-sampling processes in hierarchical\ndesigns can lead to further loss of positional information. Preserving this\ninformation is essential for accurate action detection. In this paper, we\naddress this issue by proposing a novel transformer-based network that (a)\nemploys a non-hierarchical structure when modelling different ranges of\ntemporal dependencies and (b) embeds relative positional encoding in its\ntransformer layers. Furthermore, to model co-occurrence action relationships,\ncurrent methods explicitly embed class relations into the transformer network.\nHowever, these approaches are not computationally efficient, as the network\nneeds to compute all possible pair action class relations. We also overcome\nthis challenge by introducing a novel learning paradigm that allows the network\nto benefit from explicitly modelling temporal co-occurrence action dependencies\nwithout imposing their additional computational costs during inference. We\nevaluate the performance of our proposed approach on two challenging dense\nmulti-label benchmark datasets and show that our method improves the current\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2406.06187v1", "date": "2024-06-10", "relevancy": 2.913, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Effective-Efficient%20Approach%20for%20Dense%20Multi-Label%20Action%20Detection&body=Title%3A%20An%20Effective-Efficient%20Approach%20for%20Dense%20Multi-Label%20Action%20Detection%0AAuthor%3A%20Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton%0AAbstract%3A%20%20%20Unlike%20the%20sparse%20label%20action%20detection%20task%2C%20where%20a%20single%20action%20occurs%0Ain%20each%20timestamp%20of%20a%20video%2C%20in%20a%20dense%20multi-label%20scenario%2C%20actions%20can%0Aoverlap.%20To%20address%20this%20challenging%20task%2C%20it%20is%20necessary%20to%20simultaneously%0Alearn%20%28i%29%20temporal%20dependencies%20and%20%28ii%29%20co-occurrence%20action%20relationships.%0ARecent%20approaches%20model%20temporal%20information%20by%20extracting%20multi-scale%20features%0Athrough%20hierarchical%20transformer-based%20networks.%20However%2C%20the%20self-attention%0Amechanism%20in%20transformers%20inherently%20loses%20temporal%20positional%20information.%20We%0Aargue%20that%20combining%20this%20with%20multiple%20sub-sampling%20processes%20in%20hierarchical%0Adesigns%20can%20lead%20to%20further%20loss%20of%20positional%20information.%20Preserving%20this%0Ainformation%20is%20essential%20for%20accurate%20action%20detection.%20In%20this%20paper%2C%20we%0Aaddress%20this%20issue%20by%20proposing%20a%20novel%20transformer-based%20network%20that%20%28a%29%0Aemploys%20a%20non-hierarchical%20structure%20when%20modelling%20different%20ranges%20of%0Atemporal%20dependencies%20and%20%28b%29%20embeds%20relative%20positional%20encoding%20in%20its%0Atransformer%20layers.%20Furthermore%2C%20to%20model%20co-occurrence%20action%20relationships%2C%0Acurrent%20methods%20explicitly%20embed%20class%20relations%20into%20the%20transformer%20network.%0AHowever%2C%20these%20approaches%20are%20not%20computationally%20efficient%2C%20as%20the%20network%0Aneeds%20to%20compute%20all%20possible%20pair%20action%20class%20relations.%20We%20also%20overcome%0Athis%20challenge%20by%20introducing%20a%20novel%20learning%20paradigm%20that%20allows%20the%20network%0Ato%20benefit%20from%20explicitly%20modelling%20temporal%20co-occurrence%20action%20dependencies%0Awithout%20imposing%20their%20additional%20computational%20costs%20during%20inference.%20We%0Aevaluate%20the%20performance%20of%20our%20proposed%20approach%20on%20two%20challenging%20dense%0Amulti-label%20benchmark%20datasets%20and%20show%20that%20our%20method%20improves%20the%20current%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Effective-Efficient%2520Approach%2520for%2520Dense%2520Multi-Label%2520Action%2520Detection%26entry.906535625%3DFaegheh%2520Sardari%2520and%2520Armin%2520Mustafa%2520and%2520Philip%2520J.%2520B.%2520Jackson%2520and%2520Adrian%2520Hilton%26entry.1292438233%3D%2520%2520Unlike%2520the%2520sparse%2520label%2520action%2520detection%2520task%252C%2520where%2520a%2520single%2520action%2520occurs%250Ain%2520each%2520timestamp%2520of%2520a%2520video%252C%2520in%2520a%2520dense%2520multi-label%2520scenario%252C%2520actions%2520can%250Aoverlap.%2520To%2520address%2520this%2520challenging%2520task%252C%2520it%2520is%2520necessary%2520to%2520simultaneously%250Alearn%2520%2528i%2529%2520temporal%2520dependencies%2520and%2520%2528ii%2529%2520co-occurrence%2520action%2520relationships.%250ARecent%2520approaches%2520model%2520temporal%2520information%2520by%2520extracting%2520multi-scale%2520features%250Athrough%2520hierarchical%2520transformer-based%2520networks.%2520However%252C%2520the%2520self-attention%250Amechanism%2520in%2520transformers%2520inherently%2520loses%2520temporal%2520positional%2520information.%2520We%250Aargue%2520that%2520combining%2520this%2520with%2520multiple%2520sub-sampling%2520processes%2520in%2520hierarchical%250Adesigns%2520can%2520lead%2520to%2520further%2520loss%2520of%2520positional%2520information.%2520Preserving%2520this%250Ainformation%2520is%2520essential%2520for%2520accurate%2520action%2520detection.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520this%2520issue%2520by%2520proposing%2520a%2520novel%2520transformer-based%2520network%2520that%2520%2528a%2529%250Aemploys%2520a%2520non-hierarchical%2520structure%2520when%2520modelling%2520different%2520ranges%2520of%250Atemporal%2520dependencies%2520and%2520%2528b%2529%2520embeds%2520relative%2520positional%2520encoding%2520in%2520its%250Atransformer%2520layers.%2520Furthermore%252C%2520to%2520model%2520co-occurrence%2520action%2520relationships%252C%250Acurrent%2520methods%2520explicitly%2520embed%2520class%2520relations%2520into%2520the%2520transformer%2520network.%250AHowever%252C%2520these%2520approaches%2520are%2520not%2520computationally%2520efficient%252C%2520as%2520the%2520network%250Aneeds%2520to%2520compute%2520all%2520possible%2520pair%2520action%2520class%2520relations.%2520We%2520also%2520overcome%250Athis%2520challenge%2520by%2520introducing%2520a%2520novel%2520learning%2520paradigm%2520that%2520allows%2520the%2520network%250Ato%2520benefit%2520from%2520explicitly%2520modelling%2520temporal%2520co-occurrence%2520action%2520dependencies%250Awithout%2520imposing%2520their%2520additional%2520computational%2520costs%2520during%2520inference.%2520We%250Aevaluate%2520the%2520performance%2520of%2520our%2520proposed%2520approach%2520on%2520two%2520challenging%2520dense%250Amulti-label%2520benchmark%2520datasets%2520and%2520show%2520that%2520our%2520method%2520improves%2520the%2520current%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Effective-Efficient%20Approach%20for%20Dense%20Multi-Label%20Action%20Detection&entry.906535625=Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton&entry.1292438233=%20%20Unlike%20the%20sparse%20label%20action%20detection%20task%2C%20where%20a%20single%20action%20occurs%0Ain%20each%20timestamp%20of%20a%20video%2C%20in%20a%20dense%20multi-label%20scenario%2C%20actions%20can%0Aoverlap.%20To%20address%20this%20challenging%20task%2C%20it%20is%20necessary%20to%20simultaneously%0Alearn%20%28i%29%20temporal%20dependencies%20and%20%28ii%29%20co-occurrence%20action%20relationships.%0ARecent%20approaches%20model%20temporal%20information%20by%20extracting%20multi-scale%20features%0Athrough%20hierarchical%20transformer-based%20networks.%20However%2C%20the%20self-attention%0Amechanism%20in%20transformers%20inherently%20loses%20temporal%20positional%20information.%20We%0Aargue%20that%20combining%20this%20with%20multiple%20sub-sampling%20processes%20in%20hierarchical%0Adesigns%20can%20lead%20to%20further%20loss%20of%20positional%20information.%20Preserving%20this%0Ainformation%20is%20essential%20for%20accurate%20action%20detection.%20In%20this%20paper%2C%20we%0Aaddress%20this%20issue%20by%20proposing%20a%20novel%20transformer-based%20network%20that%20%28a%29%0Aemploys%20a%20non-hierarchical%20structure%20when%20modelling%20different%20ranges%20of%0Atemporal%20dependencies%20and%20%28b%29%20embeds%20relative%20positional%20encoding%20in%20its%0Atransformer%20layers.%20Furthermore%2C%20to%20model%20co-occurrence%20action%20relationships%2C%0Acurrent%20methods%20explicitly%20embed%20class%20relations%20into%20the%20transformer%20network.%0AHowever%2C%20these%20approaches%20are%20not%20computationally%20efficient%2C%20as%20the%20network%0Aneeds%20to%20compute%20all%20possible%20pair%20action%20class%20relations.%20We%20also%20overcome%0Athis%20challenge%20by%20introducing%20a%20novel%20learning%20paradigm%20that%20allows%20the%20network%0Ato%20benefit%20from%20explicitly%20modelling%20temporal%20co-occurrence%20action%20dependencies%0Awithout%20imposing%20their%20additional%20computational%20costs%20during%20inference.%20We%0Aevaluate%20the%20performance%20of%20our%20proposed%20approach%20on%20two%20challenging%20dense%0Amulti-label%20benchmark%20datasets%20and%20show%20that%20our%20method%20improves%20the%20current%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06187v1&entry.124074799=Read"},
{"title": "IllumiNeRF: 3D Relighting without Inverse Rendering", "author": "Xiaoming Zhao and Pratul P. Srinivasan and Dor Verbin and Keunhong Park and Ricardo Martin Brualla and Philipp Henzler", "abstract": "  Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)\nwith these relit images, from which we render novel views under the target\nlighting. We demonstrate that this strategy is surprisingly competitive and\nachieves state-of-the-art results on multiple relighting benchmarks. Please see\nour project page at https://illuminerf.github.io/.\n", "link": "http://arxiv.org/abs/2406.06527v1", "date": "2024-06-10", "relevancy": 2.807, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.568}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.568}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IllumiNeRF%3A%203D%20Relighting%20without%20Inverse%20Rendering&body=Title%3A%20IllumiNeRF%3A%203D%20Relighting%20without%20Inverse%20Rendering%0AAuthor%3A%20Xiaoming%20Zhao%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin%20and%20Keunhong%20Park%20and%20Ricardo%20Martin%20Brualla%20and%20Philipp%20Henzler%0AAbstract%3A%20%20%20Existing%20methods%20for%20relightable%20view%20synthesis%20--%20using%20a%20set%20of%20images%20of%0Aan%20object%20under%20unknown%20lighting%20to%20recover%20a%203D%20representation%20that%20can%20be%0Arendered%20from%20novel%20viewpoints%20under%20a%20target%20illumination%20--%20are%20based%20on%0Ainverse%20rendering%2C%20and%20attempt%20to%20disentangle%20the%20object%20geometry%2C%20materials%2C%0Aand%20lighting%20that%20explain%20the%20input%20images.%20Furthermore%2C%20this%20typically%0Ainvolves%20optimization%20through%20differentiable%20Monte%20Carlo%20rendering%2C%20which%20is%0Abrittle%20and%20computationally-expensive.%20In%20this%20work%2C%20we%20propose%20a%20simpler%0Aapproach%3A%20we%20first%20relight%20each%20input%20image%20using%20an%20image%20diffusion%20model%0Aconditioned%20on%20lighting%20and%20then%20reconstruct%20a%20Neural%20Radiance%20Field%20%28NeRF%29%0Awith%20these%20relit%20images%2C%20from%20which%20we%20render%20novel%20views%20under%20the%20target%0Alighting.%20We%20demonstrate%20that%20this%20strategy%20is%20surprisingly%20competitive%20and%0Aachieves%20state-of-the-art%20results%20on%20multiple%20relighting%20benchmarks.%20Please%20see%0Aour%20project%20page%20at%20https%3A//illuminerf.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIllumiNeRF%253A%25203D%2520Relighting%2520without%2520Inverse%2520Rendering%26entry.906535625%3DXiaoming%2520Zhao%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Dor%2520Verbin%2520and%2520Keunhong%2520Park%2520and%2520Ricardo%2520Martin%2520Brualla%2520and%2520Philipp%2520Henzler%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520relightable%2520view%2520synthesis%2520--%2520using%2520a%2520set%2520of%2520images%2520of%250Aan%2520object%2520under%2520unknown%2520lighting%2520to%2520recover%2520a%25203D%2520representation%2520that%2520can%2520be%250Arendered%2520from%2520novel%2520viewpoints%2520under%2520a%2520target%2520illumination%2520--%2520are%2520based%2520on%250Ainverse%2520rendering%252C%2520and%2520attempt%2520to%2520disentangle%2520the%2520object%2520geometry%252C%2520materials%252C%250Aand%2520lighting%2520that%2520explain%2520the%2520input%2520images.%2520Furthermore%252C%2520this%2520typically%250Ainvolves%2520optimization%2520through%2520differentiable%2520Monte%2520Carlo%2520rendering%252C%2520which%2520is%250Abrittle%2520and%2520computationally-expensive.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simpler%250Aapproach%253A%2520we%2520first%2520relight%2520each%2520input%2520image%2520using%2520an%2520image%2520diffusion%2520model%250Aconditioned%2520on%2520lighting%2520and%2520then%2520reconstruct%2520a%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%250Awith%2520these%2520relit%2520images%252C%2520from%2520which%2520we%2520render%2520novel%2520views%2520under%2520the%2520target%250Alighting.%2520We%2520demonstrate%2520that%2520this%2520strategy%2520is%2520surprisingly%2520competitive%2520and%250Aachieves%2520state-of-the-art%2520results%2520on%2520multiple%2520relighting%2520benchmarks.%2520Please%2520see%250Aour%2520project%2520page%2520at%2520https%253A//illuminerf.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IllumiNeRF%3A%203D%20Relighting%20without%20Inverse%20Rendering&entry.906535625=Xiaoming%20Zhao%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin%20and%20Keunhong%20Park%20and%20Ricardo%20Martin%20Brualla%20and%20Philipp%20Henzler&entry.1292438233=%20%20Existing%20methods%20for%20relightable%20view%20synthesis%20--%20using%20a%20set%20of%20images%20of%0Aan%20object%20under%20unknown%20lighting%20to%20recover%20a%203D%20representation%20that%20can%20be%0Arendered%20from%20novel%20viewpoints%20under%20a%20target%20illumination%20--%20are%20based%20on%0Ainverse%20rendering%2C%20and%20attempt%20to%20disentangle%20the%20object%20geometry%2C%20materials%2C%0Aand%20lighting%20that%20explain%20the%20input%20images.%20Furthermore%2C%20this%20typically%0Ainvolves%20optimization%20through%20differentiable%20Monte%20Carlo%20rendering%2C%20which%20is%0Abrittle%20and%20computationally-expensive.%20In%20this%20work%2C%20we%20propose%20a%20simpler%0Aapproach%3A%20we%20first%20relight%20each%20input%20image%20using%20an%20image%20diffusion%20model%0Aconditioned%20on%20lighting%20and%20then%20reconstruct%20a%20Neural%20Radiance%20Field%20%28NeRF%29%0Awith%20these%20relit%20images%2C%20from%20which%20we%20render%20novel%20views%20under%20the%20target%0Alighting.%20We%20demonstrate%20that%20this%20strategy%20is%20surprisingly%20competitive%20and%0Aachieves%20state-of-the-art%20results%20on%20multiple%20relighting%20benchmarks.%20Please%20see%0Aour%20project%20page%20at%20https%3A//illuminerf.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06527v1&entry.124074799=Read"},
{"title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography", "author": "Louis Blankemeier and Joseph Paul Cohen and Ashwin Kumar and Dave Van Veen and Syed Jamal Safdar Gardezi and Magdalini Paschali and Zhihong Chen and Jean-Benoit Delbrouck and Eduardo Reis and Cesar Truyts and Christian Bluethgen and Malte Engmann Kjeldskov Jensen and Sophie Ostmeier and Maya Varma and Jeya Maria Jose Valanarasu and Zhongnan Fang and Zepeng Huo and Zaid Nabulsi and Diego Ardila and Wei-Hung Weng and Edson Amaro Junior and Neera Ahuja and Jason Fries and Nigam H. Shah and Andrew Johnston and Robert D. Boutin and Andrew Wentland and Curtis P. Langlotz and Jason Hom and Sergios Gatidis and Akshay S. Chaudhari", "abstract": "  Over 85 million computed tomography (CT) scans are performed annually in the\nUS, of which approximately one quarter focus on the abdomen. Given the current\nradiologist shortage, there is a large impetus to use artificial intelligence\nto alleviate the burden of interpreting these complex imaging studies. Prior\nstate-of-the-art approaches for automated medical image interpretation leverage\nvision language models (VLMs). However, current medical VLMs are generally\nlimited to 2D images and short reports, and do not leverage electronic health\nrecord (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train\nusing paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes\n(1.8+ million codes), and radiology reports (6+ million tokens). We evaluate\nMerlin on 6 task types and 752 individual tasks. The non-adapted\n(off-the-shelf) tasks include zero-shot findings classification (31 findings),\nphenotype classification (692 phenotypes), and zero-shot cross-modal retrieval\n(image to findings and image to impressions), while model adapted tasks include\n5-year disease prediction (6 diseases), radiology report generation, and 3D\nsemantic segmentation (20 organs). We perform internal validation on a test set\nof 5,137 CTs, and external validation on 7,000 clinical CTs and on two public\nCT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant\nevaluations, we assess the efficacy of various network architectures and\ntraining strategies to depict that Merlin has favorable performance to existing\ntask-specific baselines. We derive data scaling laws to empirically assess\ntraining data needs for requisite downstream task performance. Furthermore,\nunlike conventional VLMs that require hundreds of GPUs for training, we perform\nall training on a single GPU.\n", "link": "http://arxiv.org/abs/2406.06512v1", "date": "2024-06-10", "relevancy": 2.7402, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5333}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merlin%3A%20A%20Vision%20Language%20Foundation%20Model%20for%203D%20Computed%20Tomography&body=Title%3A%20Merlin%3A%20A%20Vision%20Language%20Foundation%20Model%20for%203D%20Computed%20Tomography%0AAuthor%3A%20Louis%20Blankemeier%20and%20Joseph%20Paul%20Cohen%20and%20Ashwin%20Kumar%20and%20Dave%20Van%20Veen%20and%20Syed%20Jamal%20Safdar%20Gardezi%20and%20Magdalini%20Paschali%20and%20Zhihong%20Chen%20and%20Jean-Benoit%20Delbrouck%20and%20Eduardo%20Reis%20and%20Cesar%20Truyts%20and%20Christian%20Bluethgen%20and%20Malte%20Engmann%20Kjeldskov%20Jensen%20and%20Sophie%20Ostmeier%20and%20Maya%20Varma%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Zhongnan%20Fang%20and%20Zepeng%20Huo%20and%20Zaid%20Nabulsi%20and%20Diego%20Ardila%20and%20Wei-Hung%20Weng%20and%20Edson%20Amaro%20Junior%20and%20Neera%20Ahuja%20and%20Jason%20Fries%20and%20Nigam%20H.%20Shah%20and%20Andrew%20Johnston%20and%20Robert%20D.%20Boutin%20and%20Andrew%20Wentland%20and%20Curtis%20P.%20Langlotz%20and%20Jason%20Hom%20and%20Sergios%20Gatidis%20and%20Akshay%20S.%20Chaudhari%0AAbstract%3A%20%20%20Over%2085%20million%20computed%20tomography%20%28CT%29%20scans%20are%20performed%20annually%20in%20the%0AUS%2C%20of%20which%20approximately%20one%20quarter%20focus%20on%20the%20abdomen.%20Given%20the%20current%0Aradiologist%20shortage%2C%20there%20is%20a%20large%20impetus%20to%20use%20artificial%20intelligence%0Ato%20alleviate%20the%20burden%20of%20interpreting%20these%20complex%20imaging%20studies.%20Prior%0Astate-of-the-art%20approaches%20for%20automated%20medical%20image%20interpretation%20leverage%0Avision%20language%20models%20%28VLMs%29.%20However%2C%20current%20medical%20VLMs%20are%20generally%0Alimited%20to%202D%20images%20and%20short%20reports%2C%20and%20do%20not%20leverage%20electronic%20health%0Arecord%20%28EHR%29%20data%20for%20supervision.%20We%20introduce%20Merlin%20-%20a%203D%20VLM%20that%20we%20train%0Ausing%20paired%20CT%20scans%20%286%2B%20million%20images%20from%2015%2C331%20CTs%29%2C%20EHR%20diagnosis%20codes%0A%281.8%2B%20million%20codes%29%2C%20and%20radiology%20reports%20%286%2B%20million%20tokens%29.%20We%20evaluate%0AMerlin%20on%206%20task%20types%20and%20752%20individual%20tasks.%20The%20non-adapted%0A%28off-the-shelf%29%20tasks%20include%20zero-shot%20findings%20classification%20%2831%20findings%29%2C%0Aphenotype%20classification%20%28692%20phenotypes%29%2C%20and%20zero-shot%20cross-modal%20retrieval%0A%28image%20to%20findings%20and%20image%20to%20impressions%29%2C%20while%20model%20adapted%20tasks%20include%0A5-year%20disease%20prediction%20%286%20diseases%29%2C%20radiology%20report%20generation%2C%20and%203D%0Asemantic%20segmentation%20%2820%20organs%29.%20We%20perform%20internal%20validation%20on%20a%20test%20set%0Aof%205%2C137%20CTs%2C%20and%20external%20validation%20on%207%2C000%20clinical%20CTs%20and%20on%20two%20public%0ACT%20datasets%20%28VerSe%2C%20TotalSegmentator%29.%20Beyond%20these%20clinically-relevant%0Aevaluations%2C%20we%20assess%20the%20efficacy%20of%20various%20network%20architectures%20and%0Atraining%20strategies%20to%20depict%20that%20Merlin%20has%20favorable%20performance%20to%20existing%0Atask-specific%20baselines.%20We%20derive%20data%20scaling%20laws%20to%20empirically%20assess%0Atraining%20data%20needs%20for%20requisite%20downstream%20task%20performance.%20Furthermore%2C%0Aunlike%20conventional%20VLMs%20that%20require%20hundreds%20of%20GPUs%20for%20training%2C%20we%20perform%0Aall%20training%20on%20a%20single%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerlin%253A%2520A%2520Vision%2520Language%2520Foundation%2520Model%2520for%25203D%2520Computed%2520Tomography%26entry.906535625%3DLouis%2520Blankemeier%2520and%2520Joseph%2520Paul%2520Cohen%2520and%2520Ashwin%2520Kumar%2520and%2520Dave%2520Van%2520Veen%2520and%2520Syed%2520Jamal%2520Safdar%2520Gardezi%2520and%2520Magdalini%2520Paschali%2520and%2520Zhihong%2520Chen%2520and%2520Jean-Benoit%2520Delbrouck%2520and%2520Eduardo%2520Reis%2520and%2520Cesar%2520Truyts%2520and%2520Christian%2520Bluethgen%2520and%2520Malte%2520Engmann%2520Kjeldskov%2520Jensen%2520and%2520Sophie%2520Ostmeier%2520and%2520Maya%2520Varma%2520and%2520Jeya%2520Maria%2520Jose%2520Valanarasu%2520and%2520Zhongnan%2520Fang%2520and%2520Zepeng%2520Huo%2520and%2520Zaid%2520Nabulsi%2520and%2520Diego%2520Ardila%2520and%2520Wei-Hung%2520Weng%2520and%2520Edson%2520Amaro%2520Junior%2520and%2520Neera%2520Ahuja%2520and%2520Jason%2520Fries%2520and%2520Nigam%2520H.%2520Shah%2520and%2520Andrew%2520Johnston%2520and%2520Robert%2520D.%2520Boutin%2520and%2520Andrew%2520Wentland%2520and%2520Curtis%2520P.%2520Langlotz%2520and%2520Jason%2520Hom%2520and%2520Sergios%2520Gatidis%2520and%2520Akshay%2520S.%2520Chaudhari%26entry.1292438233%3D%2520%2520Over%252085%2520million%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520are%2520performed%2520annually%2520in%2520the%250AUS%252C%2520of%2520which%2520approximately%2520one%2520quarter%2520focus%2520on%2520the%2520abdomen.%2520Given%2520the%2520current%250Aradiologist%2520shortage%252C%2520there%2520is%2520a%2520large%2520impetus%2520to%2520use%2520artificial%2520intelligence%250Ato%2520alleviate%2520the%2520burden%2520of%2520interpreting%2520these%2520complex%2520imaging%2520studies.%2520Prior%250Astate-of-the-art%2520approaches%2520for%2520automated%2520medical%2520image%2520interpretation%2520leverage%250Avision%2520language%2520models%2520%2528VLMs%2529.%2520However%252C%2520current%2520medical%2520VLMs%2520are%2520generally%250Alimited%2520to%25202D%2520images%2520and%2520short%2520reports%252C%2520and%2520do%2520not%2520leverage%2520electronic%2520health%250Arecord%2520%2528EHR%2529%2520data%2520for%2520supervision.%2520We%2520introduce%2520Merlin%2520-%2520a%25203D%2520VLM%2520that%2520we%2520train%250Ausing%2520paired%2520CT%2520scans%2520%25286%252B%2520million%2520images%2520from%252015%252C331%2520CTs%2529%252C%2520EHR%2520diagnosis%2520codes%250A%25281.8%252B%2520million%2520codes%2529%252C%2520and%2520radiology%2520reports%2520%25286%252B%2520million%2520tokens%2529.%2520We%2520evaluate%250AMerlin%2520on%25206%2520task%2520types%2520and%2520752%2520individual%2520tasks.%2520The%2520non-adapted%250A%2528off-the-shelf%2529%2520tasks%2520include%2520zero-shot%2520findings%2520classification%2520%252831%2520findings%2529%252C%250Aphenotype%2520classification%2520%2528692%2520phenotypes%2529%252C%2520and%2520zero-shot%2520cross-modal%2520retrieval%250A%2528image%2520to%2520findings%2520and%2520image%2520to%2520impressions%2529%252C%2520while%2520model%2520adapted%2520tasks%2520include%250A5-year%2520disease%2520prediction%2520%25286%2520diseases%2529%252C%2520radiology%2520report%2520generation%252C%2520and%25203D%250Asemantic%2520segmentation%2520%252820%2520organs%2529.%2520We%2520perform%2520internal%2520validation%2520on%2520a%2520test%2520set%250Aof%25205%252C137%2520CTs%252C%2520and%2520external%2520validation%2520on%25207%252C000%2520clinical%2520CTs%2520and%2520on%2520two%2520public%250ACT%2520datasets%2520%2528VerSe%252C%2520TotalSegmentator%2529.%2520Beyond%2520these%2520clinically-relevant%250Aevaluations%252C%2520we%2520assess%2520the%2520efficacy%2520of%2520various%2520network%2520architectures%2520and%250Atraining%2520strategies%2520to%2520depict%2520that%2520Merlin%2520has%2520favorable%2520performance%2520to%2520existing%250Atask-specific%2520baselines.%2520We%2520derive%2520data%2520scaling%2520laws%2520to%2520empirically%2520assess%250Atraining%2520data%2520needs%2520for%2520requisite%2520downstream%2520task%2520performance.%2520Furthermore%252C%250Aunlike%2520conventional%2520VLMs%2520that%2520require%2520hundreds%2520of%2520GPUs%2520for%2520training%252C%2520we%2520perform%250Aall%2520training%2520on%2520a%2520single%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merlin%3A%20A%20Vision%20Language%20Foundation%20Model%20for%203D%20Computed%20Tomography&entry.906535625=Louis%20Blankemeier%20and%20Joseph%20Paul%20Cohen%20and%20Ashwin%20Kumar%20and%20Dave%20Van%20Veen%20and%20Syed%20Jamal%20Safdar%20Gardezi%20and%20Magdalini%20Paschali%20and%20Zhihong%20Chen%20and%20Jean-Benoit%20Delbrouck%20and%20Eduardo%20Reis%20and%20Cesar%20Truyts%20and%20Christian%20Bluethgen%20and%20Malte%20Engmann%20Kjeldskov%20Jensen%20and%20Sophie%20Ostmeier%20and%20Maya%20Varma%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Zhongnan%20Fang%20and%20Zepeng%20Huo%20and%20Zaid%20Nabulsi%20and%20Diego%20Ardila%20and%20Wei-Hung%20Weng%20and%20Edson%20Amaro%20Junior%20and%20Neera%20Ahuja%20and%20Jason%20Fries%20and%20Nigam%20H.%20Shah%20and%20Andrew%20Johnston%20and%20Robert%20D.%20Boutin%20and%20Andrew%20Wentland%20and%20Curtis%20P.%20Langlotz%20and%20Jason%20Hom%20and%20Sergios%20Gatidis%20and%20Akshay%20S.%20Chaudhari&entry.1292438233=%20%20Over%2085%20million%20computed%20tomography%20%28CT%29%20scans%20are%20performed%20annually%20in%20the%0AUS%2C%20of%20which%20approximately%20one%20quarter%20focus%20on%20the%20abdomen.%20Given%20the%20current%0Aradiologist%20shortage%2C%20there%20is%20a%20large%20impetus%20to%20use%20artificial%20intelligence%0Ato%20alleviate%20the%20burden%20of%20interpreting%20these%20complex%20imaging%20studies.%20Prior%0Astate-of-the-art%20approaches%20for%20automated%20medical%20image%20interpretation%20leverage%0Avision%20language%20models%20%28VLMs%29.%20However%2C%20current%20medical%20VLMs%20are%20generally%0Alimited%20to%202D%20images%20and%20short%20reports%2C%20and%20do%20not%20leverage%20electronic%20health%0Arecord%20%28EHR%29%20data%20for%20supervision.%20We%20introduce%20Merlin%20-%20a%203D%20VLM%20that%20we%20train%0Ausing%20paired%20CT%20scans%20%286%2B%20million%20images%20from%2015%2C331%20CTs%29%2C%20EHR%20diagnosis%20codes%0A%281.8%2B%20million%20codes%29%2C%20and%20radiology%20reports%20%286%2B%20million%20tokens%29.%20We%20evaluate%0AMerlin%20on%206%20task%20types%20and%20752%20individual%20tasks.%20The%20non-adapted%0A%28off-the-shelf%29%20tasks%20include%20zero-shot%20findings%20classification%20%2831%20findings%29%2C%0Aphenotype%20classification%20%28692%20phenotypes%29%2C%20and%20zero-shot%20cross-modal%20retrieval%0A%28image%20to%20findings%20and%20image%20to%20impressions%29%2C%20while%20model%20adapted%20tasks%20include%0A5-year%20disease%20prediction%20%286%20diseases%29%2C%20radiology%20report%20generation%2C%20and%203D%0Asemantic%20segmentation%20%2820%20organs%29.%20We%20perform%20internal%20validation%20on%20a%20test%20set%0Aof%205%2C137%20CTs%2C%20and%20external%20validation%20on%207%2C000%20clinical%20CTs%20and%20on%20two%20public%0ACT%20datasets%20%28VerSe%2C%20TotalSegmentator%29.%20Beyond%20these%20clinically-relevant%0Aevaluations%2C%20we%20assess%20the%20efficacy%20of%20various%20network%20architectures%20and%0Atraining%20strategies%20to%20depict%20that%20Merlin%20has%20favorable%20performance%20to%20existing%0Atask-specific%20baselines.%20We%20derive%20data%20scaling%20laws%20to%20empirically%20assess%0Atraining%20data%20needs%20for%20requisite%20downstream%20task%20performance.%20Furthermore%2C%0Aunlike%20conventional%20VLMs%20that%20require%20hundreds%20of%20GPUs%20for%20training%2C%20we%20perform%0Aall%20training%20on%20a%20single%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06512v1&entry.124074799=Read"},
{"title": "Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\n  and Editing via Content-based Controls", "author": "Liwei Lin and Gus Xia and Yixiao Zhang and Junyan Jiang", "abstract": "  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n", "link": "http://arxiv.org/abs/2402.09508v2", "date": "2024-06-10", "relevancy": 2.6286, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5375}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arrange%2C%20Inpaint%2C%20and%20Refine%3A%20Steerable%20Long-term%20Music%20Audio%20Generation%0A%20%20and%20Editing%20via%20Content-based%20Controls&body=Title%3A%20Arrange%2C%20Inpaint%2C%20and%20Refine%3A%20Steerable%20Long-term%20Music%20Audio%20Generation%0A%20%20and%20Editing%20via%20Content-based%20Controls%0AAuthor%3A%20Liwei%20Lin%20and%20Gus%20Xia%20and%20Yixiao%20Zhang%20and%20Junyan%20Jiang%0AAbstract%3A%20%20%20Controllable%20music%20generation%20plays%20a%20vital%20role%20in%20human-AI%20music%0Aco-creation.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%0Agenerating%20high-quality%20music%2C%20their%20focus%20on%20autoregressive%20generation%20limits%0Atheir%20utility%20in%20music%20editing%20tasks.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%0Aapproach%20leveraging%20a%20parameter-efficient%20heterogeneous%20adapter%20combined%20with%20a%0Amasking%20training%20scheme.%20This%20approach%20enables%20autoregressive%20language%20models%0Ato%20seamlessly%20address%20music%20inpainting%20tasks.%20Additionally%2C%20our%20method%0Aintegrates%20frame-level%20content-based%20controls%2C%20facilitating%20track-conditioned%0Amusic%20refinement%20and%20score-conditioned%20music%20arrangement.%20We%20apply%20this%20method%0Ato%20fine-tune%20MusicGen%2C%20a%20leading%20autoregressive%20music%20generation%20model.%20Our%0Aexperiments%20demonstrate%20promising%20results%20across%20multiple%20music%20editing%20tasks%2C%0Aoffering%20more%20flexible%20controls%20for%20future%20AI-driven%20music%20editing%20tools.%20The%0Asource%20codes%20and%20a%20demo%20page%20showcasing%20our%20work%20are%20available%20at%0Ahttps%3A//kikyo-16.github.io/AIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArrange%252C%2520Inpaint%252C%2520and%2520Refine%253A%2520Steerable%2520Long-term%2520Music%2520Audio%2520Generation%250A%2520%2520and%2520Editing%2520via%2520Content-based%2520Controls%26entry.906535625%3DLiwei%2520Lin%2520and%2520Gus%2520Xia%2520and%2520Yixiao%2520Zhang%2520and%2520Junyan%2520Jiang%26entry.1292438233%3D%2520%2520Controllable%2520music%2520generation%2520plays%2520a%2520vital%2520role%2520in%2520human-AI%2520music%250Aco-creation.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%250Agenerating%2520high-quality%2520music%252C%2520their%2520focus%2520on%2520autoregressive%2520generation%2520limits%250Atheir%2520utility%2520in%2520music%2520editing%2520tasks.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520leveraging%2520a%2520parameter-efficient%2520heterogeneous%2520adapter%2520combined%2520with%2520a%250Amasking%2520training%2520scheme.%2520This%2520approach%2520enables%2520autoregressive%2520language%2520models%250Ato%2520seamlessly%2520address%2520music%2520inpainting%2520tasks.%2520Additionally%252C%2520our%2520method%250Aintegrates%2520frame-level%2520content-based%2520controls%252C%2520facilitating%2520track-conditioned%250Amusic%2520refinement%2520and%2520score-conditioned%2520music%2520arrangement.%2520We%2520apply%2520this%2520method%250Ato%2520fine-tune%2520MusicGen%252C%2520a%2520leading%2520autoregressive%2520music%2520generation%2520model.%2520Our%250Aexperiments%2520demonstrate%2520promising%2520results%2520across%2520multiple%2520music%2520editing%2520tasks%252C%250Aoffering%2520more%2520flexible%2520controls%2520for%2520future%2520AI-driven%2520music%2520editing%2520tools.%2520The%250Asource%2520codes%2520and%2520a%2520demo%2520page%2520showcasing%2520our%2520work%2520are%2520available%2520at%250Ahttps%253A//kikyo-16.github.io/AIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arrange%2C%20Inpaint%2C%20and%20Refine%3A%20Steerable%20Long-term%20Music%20Audio%20Generation%0A%20%20and%20Editing%20via%20Content-based%20Controls&entry.906535625=Liwei%20Lin%20and%20Gus%20Xia%20and%20Yixiao%20Zhang%20and%20Junyan%20Jiang&entry.1292438233=%20%20Controllable%20music%20generation%20plays%20a%20vital%20role%20in%20human-AI%20music%0Aco-creation.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%0Agenerating%20high-quality%20music%2C%20their%20focus%20on%20autoregressive%20generation%20limits%0Atheir%20utility%20in%20music%20editing%20tasks.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%0Aapproach%20leveraging%20a%20parameter-efficient%20heterogeneous%20adapter%20combined%20with%20a%0Amasking%20training%20scheme.%20This%20approach%20enables%20autoregressive%20language%20models%0Ato%20seamlessly%20address%20music%20inpainting%20tasks.%20Additionally%2C%20our%20method%0Aintegrates%20frame-level%20content-based%20controls%2C%20facilitating%20track-conditioned%0Amusic%20refinement%20and%20score-conditioned%20music%20arrangement.%20We%20apply%20this%20method%0Ato%20fine-tune%20MusicGen%2C%20a%20leading%20autoregressive%20music%20generation%20model.%20Our%0Aexperiments%20demonstrate%20promising%20results%20across%20multiple%20music%20editing%20tasks%2C%0Aoffering%20more%20flexible%20controls%20for%20future%20AI-driven%20music%20editing%20tools.%20The%0Asource%20codes%20and%20a%20demo%20page%20showcasing%20our%20work%20are%20available%20at%0Ahttps%3A//kikyo-16.github.io/AIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09508v2&entry.124074799=Read"},
{"title": "Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual\n  Localization and Navigation", "author": "Shenghao Li and Luchao Pang and Xianglong Hu", "abstract": "  This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.\n", "link": "http://arxiv.org/abs/2406.06374v1", "date": "2024-06-10", "relevancy": 2.6137, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation&body=Title%3A%20Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation%0AAuthor%3A%20Shenghao%20Li%20and%20Luchao%20Pang%20and%20Xianglong%20Hu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20visual%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20using%20multiple%20RGB-D%20cameras.%20The%20proposed%20method%2C%0AMulticam-SLAM%2C%20significantly%20enhances%20the%20robustness%20and%20accuracy%20of%20SLAM%0Asystems%20by%20capturing%20more%20comprehensive%20spatial%20information%20from%20various%0Aperspectives.%20This%20method%20enables%20the%20accurate%20determination%20of%20pose%0Arelationships%20among%20multiple%20cameras%20without%20the%20need%20for%20overlapping%20fields%20of%0Aview.%20The%20proposed%20Muticam-SLAM%20includes%20a%20unique%20multi-camera%20model%2C%20a%0Amulti-keyframes%20structure%2C%20and%20several%20parallel%20SLAM%20threads.%20The%20multi-camera%0Amodel%20allows%20for%20the%20integration%20of%20data%20from%20multiple%20cameras%2C%20while%20the%0Amulti-keyframes%20and%20parallel%20SLAM%20threads%20ensure%20efficient%20and%20accurate%20pose%0Aestimation%20and%20mapping.%20Extensive%20experiments%20in%20various%20environments%0Ademonstrate%20the%20superior%20accuracy%20and%20robustness%20of%20the%20proposed%20method%0Acompared%20to%20conventional%20single-camera%20SLAM%20systems.%20The%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20Multicam-SLAM%20for%20more%20complex%20and%20challenging%0Aapplications.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlterPang/Multi_ORB_SLAM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticam-SLAM%253A%2520Non-overlapping%2520Multi-camera%2520SLAM%2520for%2520Indirect%2520Visual%250A%2520%2520Localization%2520and%2520Navigation%26entry.906535625%3DShenghao%2520Li%2520and%2520Luchao%2520Pang%2520and%2520Xianglong%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520visual%2520simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%2520using%2520multiple%2520RGB-D%2520cameras.%2520The%2520proposed%2520method%252C%250AMulticam-SLAM%252C%2520significantly%2520enhances%2520the%2520robustness%2520and%2520accuracy%2520of%2520SLAM%250Asystems%2520by%2520capturing%2520more%2520comprehensive%2520spatial%2520information%2520from%2520various%250Aperspectives.%2520This%2520method%2520enables%2520the%2520accurate%2520determination%2520of%2520pose%250Arelationships%2520among%2520multiple%2520cameras%2520without%2520the%2520need%2520for%2520overlapping%2520fields%2520of%250Aview.%2520The%2520proposed%2520Muticam-SLAM%2520includes%2520a%2520unique%2520multi-camera%2520model%252C%2520a%250Amulti-keyframes%2520structure%252C%2520and%2520several%2520parallel%2520SLAM%2520threads.%2520The%2520multi-camera%250Amodel%2520allows%2520for%2520the%2520integration%2520of%2520data%2520from%2520multiple%2520cameras%252C%2520while%2520the%250Amulti-keyframes%2520and%2520parallel%2520SLAM%2520threads%2520ensure%2520efficient%2520and%2520accurate%2520pose%250Aestimation%2520and%2520mapping.%2520Extensive%2520experiments%2520in%2520various%2520environments%250Ademonstrate%2520the%2520superior%2520accuracy%2520and%2520robustness%2520of%2520the%2520proposed%2520method%250Acompared%2520to%2520conventional%2520single-camera%2520SLAM%2520systems.%2520The%2520results%2520highlight%2520the%250Apotential%2520of%2520the%2520proposed%2520Multicam-SLAM%2520for%2520more%2520complex%2520and%2520challenging%250Aapplications.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/AlterPang/Multi_ORB_SLAM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation&entry.906535625=Shenghao%20Li%20and%20Luchao%20Pang%20and%20Xianglong%20Hu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20visual%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20using%20multiple%20RGB-D%20cameras.%20The%20proposed%20method%2C%0AMulticam-SLAM%2C%20significantly%20enhances%20the%20robustness%20and%20accuracy%20of%20SLAM%0Asystems%20by%20capturing%20more%20comprehensive%20spatial%20information%20from%20various%0Aperspectives.%20This%20method%20enables%20the%20accurate%20determination%20of%20pose%0Arelationships%20among%20multiple%20cameras%20without%20the%20need%20for%20overlapping%20fields%20of%0Aview.%20The%20proposed%20Muticam-SLAM%20includes%20a%20unique%20multi-camera%20model%2C%20a%0Amulti-keyframes%20structure%2C%20and%20several%20parallel%20SLAM%20threads.%20The%20multi-camera%0Amodel%20allows%20for%20the%20integration%20of%20data%20from%20multiple%20cameras%2C%20while%20the%0Amulti-keyframes%20and%20parallel%20SLAM%20threads%20ensure%20efficient%20and%20accurate%20pose%0Aestimation%20and%20mapping.%20Extensive%20experiments%20in%20various%20environments%0Ademonstrate%20the%20superior%20accuracy%20and%20robustness%20of%20the%20proposed%20method%0Acompared%20to%20conventional%20single-camera%20SLAM%20systems.%20The%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20Multicam-SLAM%20for%20more%20complex%20and%20challenging%0Aapplications.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlterPang/Multi_ORB_SLAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06374v1&entry.124074799=Read"},
{"title": "UniVS: Unified and Universal Video Segmentation with Prompts as Queries", "author": "Minghan Li and Shuai Li and Xindong Zhang and Lei Zhang", "abstract": "  Despite the recent advances in unified image segmentation (IS), developing a\nunified video segmentation (VS) model remains a challenge. This is mainly\nbecause generic category-specified VS tasks need to detect all objects and\ntrack them across consecutive frames, while prompt-guided VS tasks require\nre-identifying the target with visual/text prompts throughout the entire video,\nmaking it hard to handle the different tasks with the same architecture. We\nmake an attempt to address these issues and present a novel unified VS\narchitecture, namely UniVS, by using prompts as queries. UniVS averages the\nprompt features of the target from previous frames as its initial query to\nexplicitly decode masks, and introduces a target-wise prompt cross-attention\nlayer in the mask decoder to integrate prompt features in the memory pool. By\ntaking the predicted masks of entities from previous frames as their visual\nprompts, UniVS converts different VS tasks into prompt-guided target\nsegmentation, eliminating the heuristic inter-frame matching process. Our\nframework not only unifies the different VS tasks but also naturally achieves\nuniversal training and testing, ensuring robust performance across different\nscenarios. UniVS shows a commendable balance between performance and\nuniversality on 10 challenging VS benchmarks, covering video instance,\nsemantic, panoptic, object, and referring segmentation tasks. Code can be found\nat \\url{https://github.com/MinghanLi/UniVS}.\n", "link": "http://arxiv.org/abs/2402.18115v2", "date": "2024-06-10", "relevancy": 2.6064, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVS%3A%20Unified%20and%20Universal%20Video%20Segmentation%20with%20Prompts%20as%20Queries&body=Title%3A%20UniVS%3A%20Unified%20and%20Universal%20Video%20Segmentation%20with%20Prompts%20as%20Queries%0AAuthor%3A%20Minghan%20Li%20and%20Shuai%20Li%20and%20Xindong%20Zhang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20recent%20advances%20in%20unified%20image%20segmentation%20%28IS%29%2C%20developing%20a%0Aunified%20video%20segmentation%20%28VS%29%20model%20remains%20a%20challenge.%20This%20is%20mainly%0Abecause%20generic%20category-specified%20VS%20tasks%20need%20to%20detect%20all%20objects%20and%0Atrack%20them%20across%20consecutive%20frames%2C%20while%20prompt-guided%20VS%20tasks%20require%0Are-identifying%20the%20target%20with%20visual/text%20prompts%20throughout%20the%20entire%20video%2C%0Amaking%20it%20hard%20to%20handle%20the%20different%20tasks%20with%20the%20same%20architecture.%20We%0Amake%20an%20attempt%20to%20address%20these%20issues%20and%20present%20a%20novel%20unified%20VS%0Aarchitecture%2C%20namely%20UniVS%2C%20by%20using%20prompts%20as%20queries.%20UniVS%20averages%20the%0Aprompt%20features%20of%20the%20target%20from%20previous%20frames%20as%20its%20initial%20query%20to%0Aexplicitly%20decode%20masks%2C%20and%20introduces%20a%20target-wise%20prompt%20cross-attention%0Alayer%20in%20the%20mask%20decoder%20to%20integrate%20prompt%20features%20in%20the%20memory%20pool.%20By%0Ataking%20the%20predicted%20masks%20of%20entities%20from%20previous%20frames%20as%20their%20visual%0Aprompts%2C%20UniVS%20converts%20different%20VS%20tasks%20into%20prompt-guided%20target%0Asegmentation%2C%20eliminating%20the%20heuristic%20inter-frame%20matching%20process.%20Our%0Aframework%20not%20only%20unifies%20the%20different%20VS%20tasks%20but%20also%20naturally%20achieves%0Auniversal%20training%20and%20testing%2C%20ensuring%20robust%20performance%20across%20different%0Ascenarios.%20UniVS%20shows%20a%20commendable%20balance%20between%20performance%20and%0Auniversality%20on%2010%20challenging%20VS%20benchmarks%2C%20covering%20video%20instance%2C%0Asemantic%2C%20panoptic%2C%20object%2C%20and%20referring%20segmentation%20tasks.%20Code%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/MinghanLi/UniVS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVS%253A%2520Unified%2520and%2520Universal%2520Video%2520Segmentation%2520with%2520Prompts%2520as%2520Queries%26entry.906535625%3DMinghan%2520Li%2520and%2520Shuai%2520Li%2520and%2520Xindong%2520Zhang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520advances%2520in%2520unified%2520image%2520segmentation%2520%2528IS%2529%252C%2520developing%2520a%250Aunified%2520video%2520segmentation%2520%2528VS%2529%2520model%2520remains%2520a%2520challenge.%2520This%2520is%2520mainly%250Abecause%2520generic%2520category-specified%2520VS%2520tasks%2520need%2520to%2520detect%2520all%2520objects%2520and%250Atrack%2520them%2520across%2520consecutive%2520frames%252C%2520while%2520prompt-guided%2520VS%2520tasks%2520require%250Are-identifying%2520the%2520target%2520with%2520visual/text%2520prompts%2520throughout%2520the%2520entire%2520video%252C%250Amaking%2520it%2520hard%2520to%2520handle%2520the%2520different%2520tasks%2520with%2520the%2520same%2520architecture.%2520We%250Amake%2520an%2520attempt%2520to%2520address%2520these%2520issues%2520and%2520present%2520a%2520novel%2520unified%2520VS%250Aarchitecture%252C%2520namely%2520UniVS%252C%2520by%2520using%2520prompts%2520as%2520queries.%2520UniVS%2520averages%2520the%250Aprompt%2520features%2520of%2520the%2520target%2520from%2520previous%2520frames%2520as%2520its%2520initial%2520query%2520to%250Aexplicitly%2520decode%2520masks%252C%2520and%2520introduces%2520a%2520target-wise%2520prompt%2520cross-attention%250Alayer%2520in%2520the%2520mask%2520decoder%2520to%2520integrate%2520prompt%2520features%2520in%2520the%2520memory%2520pool.%2520By%250Ataking%2520the%2520predicted%2520masks%2520of%2520entities%2520from%2520previous%2520frames%2520as%2520their%2520visual%250Aprompts%252C%2520UniVS%2520converts%2520different%2520VS%2520tasks%2520into%2520prompt-guided%2520target%250Asegmentation%252C%2520eliminating%2520the%2520heuristic%2520inter-frame%2520matching%2520process.%2520Our%250Aframework%2520not%2520only%2520unifies%2520the%2520different%2520VS%2520tasks%2520but%2520also%2520naturally%2520achieves%250Auniversal%2520training%2520and%2520testing%252C%2520ensuring%2520robust%2520performance%2520across%2520different%250Ascenarios.%2520UniVS%2520shows%2520a%2520commendable%2520balance%2520between%2520performance%2520and%250Auniversality%2520on%252010%2520challenging%2520VS%2520benchmarks%252C%2520covering%2520video%2520instance%252C%250Asemantic%252C%2520panoptic%252C%2520object%252C%2520and%2520referring%2520segmentation%2520tasks.%2520Code%2520can%2520be%2520found%250Aat%2520%255Curl%257Bhttps%253A//github.com/MinghanLi/UniVS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVS%3A%20Unified%20and%20Universal%20Video%20Segmentation%20with%20Prompts%20as%20Queries&entry.906535625=Minghan%20Li%20and%20Shuai%20Li%20and%20Xindong%20Zhang%20and%20Lei%20Zhang&entry.1292438233=%20%20Despite%20the%20recent%20advances%20in%20unified%20image%20segmentation%20%28IS%29%2C%20developing%20a%0Aunified%20video%20segmentation%20%28VS%29%20model%20remains%20a%20challenge.%20This%20is%20mainly%0Abecause%20generic%20category-specified%20VS%20tasks%20need%20to%20detect%20all%20objects%20and%0Atrack%20them%20across%20consecutive%20frames%2C%20while%20prompt-guided%20VS%20tasks%20require%0Are-identifying%20the%20target%20with%20visual/text%20prompts%20throughout%20the%20entire%20video%2C%0Amaking%20it%20hard%20to%20handle%20the%20different%20tasks%20with%20the%20same%20architecture.%20We%0Amake%20an%20attempt%20to%20address%20these%20issues%20and%20present%20a%20novel%20unified%20VS%0Aarchitecture%2C%20namely%20UniVS%2C%20by%20using%20prompts%20as%20queries.%20UniVS%20averages%20the%0Aprompt%20features%20of%20the%20target%20from%20previous%20frames%20as%20its%20initial%20query%20to%0Aexplicitly%20decode%20masks%2C%20and%20introduces%20a%20target-wise%20prompt%20cross-attention%0Alayer%20in%20the%20mask%20decoder%20to%20integrate%20prompt%20features%20in%20the%20memory%20pool.%20By%0Ataking%20the%20predicted%20masks%20of%20entities%20from%20previous%20frames%20as%20their%20visual%0Aprompts%2C%20UniVS%20converts%20different%20VS%20tasks%20into%20prompt-guided%20target%0Asegmentation%2C%20eliminating%20the%20heuristic%20inter-frame%20matching%20process.%20Our%0Aframework%20not%20only%20unifies%20the%20different%20VS%20tasks%20but%20also%20naturally%20achieves%0Auniversal%20training%20and%20testing%2C%20ensuring%20robust%20performance%20across%20different%0Ascenarios.%20UniVS%20shows%20a%20commendable%20balance%20between%20performance%20and%0Auniversality%20on%2010%20challenging%20VS%20benchmarks%2C%20covering%20video%20instance%2C%0Asemantic%2C%20panoptic%2C%20object%2C%20and%20referring%20segmentation%20tasks.%20Code%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/MinghanLi/UniVS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18115v2&entry.124074799=Read"},
{"title": "CountCLIP -- [Re] Teaching CLIP to Count to Ten", "author": "Harshvardhan Mestha and Tejas Agrawal and Karan Bania and Shreyas V and Yash Bhisikar", "abstract": "  Large vision-language models (VLMs) are shown to learn rich joint image-text\nrepresentations enabling high performances in relevant downstream tasks.\nHowever, they fail to showcase their quantitative understanding of objects, and\nthey lack good counting-aware representation. This paper conducts a\nreproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023),\nwhich presents a method to finetune a CLIP model (Radford et al., 2021) to\nimprove zero-shot counting accuracy in an image while maintaining the\nperformance for zero-shot classification by introducing a counting-contrastive\nloss term. We improve the model's performance on a smaller subset of their\ntraining data with lower computational resources. We verify these claims by\nreproducing their study with our own code. The implementation can be found at\nhttps://github.com/SforAiDl/CountCLIP.\n", "link": "http://arxiv.org/abs/2406.03586v2", "date": "2024-06-10", "relevancy": 2.5747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountCLIP%20--%20%5BRe%5D%20Teaching%20CLIP%20to%20Count%20to%20Ten&body=Title%3A%20CountCLIP%20--%20%5BRe%5D%20Teaching%20CLIP%20to%20Count%20to%20Ten%0AAuthor%3A%20Harshvardhan%20Mestha%20and%20Tejas%20Agrawal%20and%20Karan%20Bania%20and%20Shreyas%20V%20and%20Yash%20Bhisikar%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20are%20shown%20to%20learn%20rich%20joint%20image-text%0Arepresentations%20enabling%20high%20performances%20in%20relevant%20downstream%20tasks.%0AHowever%2C%20they%20fail%20to%20showcase%20their%20quantitative%20understanding%20of%20objects%2C%20and%0Athey%20lack%20good%20counting-aware%20representation.%20This%20paper%20conducts%20a%0Areproducibility%20study%20of%20%27Teaching%20CLIP%20to%20Count%20to%20Ten%27%20%28Paiss%20et%20al.%2C%202023%29%2C%0Awhich%20presents%20a%20method%20to%20finetune%20a%20CLIP%20model%20%28Radford%20et%20al.%2C%202021%29%20to%0Aimprove%20zero-shot%20counting%20accuracy%20in%20an%20image%20while%20maintaining%20the%0Aperformance%20for%20zero-shot%20classification%20by%20introducing%20a%20counting-contrastive%0Aloss%20term.%20We%20improve%20the%20model%27s%20performance%20on%20a%20smaller%20subset%20of%20their%0Atraining%20data%20with%20lower%20computational%20resources.%20We%20verify%20these%20claims%20by%0Areproducing%20their%20study%20with%20our%20own%20code.%20The%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/SforAiDl/CountCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountCLIP%2520--%2520%255BRe%255D%2520Teaching%2520CLIP%2520to%2520Count%2520to%2520Ten%26entry.906535625%3DHarshvardhan%2520Mestha%2520and%2520Tejas%2520Agrawal%2520and%2520Karan%2520Bania%2520and%2520Shreyas%2520V%2520and%2520Yash%2520Bhisikar%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520shown%2520to%2520learn%2520rich%2520joint%2520image-text%250Arepresentations%2520enabling%2520high%2520performances%2520in%2520relevant%2520downstream%2520tasks.%250AHowever%252C%2520they%2520fail%2520to%2520showcase%2520their%2520quantitative%2520understanding%2520of%2520objects%252C%2520and%250Athey%2520lack%2520good%2520counting-aware%2520representation.%2520This%2520paper%2520conducts%2520a%250Areproducibility%2520study%2520of%2520%2527Teaching%2520CLIP%2520to%2520Count%2520to%2520Ten%2527%2520%2528Paiss%2520et%2520al.%252C%25202023%2529%252C%250Awhich%2520presents%2520a%2520method%2520to%2520finetune%2520a%2520CLIP%2520model%2520%2528Radford%2520et%2520al.%252C%25202021%2529%2520to%250Aimprove%2520zero-shot%2520counting%2520accuracy%2520in%2520an%2520image%2520while%2520maintaining%2520the%250Aperformance%2520for%2520zero-shot%2520classification%2520by%2520introducing%2520a%2520counting-contrastive%250Aloss%2520term.%2520We%2520improve%2520the%2520model%2527s%2520performance%2520on%2520a%2520smaller%2520subset%2520of%2520their%250Atraining%2520data%2520with%2520lower%2520computational%2520resources.%2520We%2520verify%2520these%2520claims%2520by%250Areproducing%2520their%2520study%2520with%2520our%2520own%2520code.%2520The%2520implementation%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/SforAiDl/CountCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountCLIP%20--%20%5BRe%5D%20Teaching%20CLIP%20to%20Count%20to%20Ten&entry.906535625=Harshvardhan%20Mestha%20and%20Tejas%20Agrawal%20and%20Karan%20Bania%20and%20Shreyas%20V%20and%20Yash%20Bhisikar&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20are%20shown%20to%20learn%20rich%20joint%20image-text%0Arepresentations%20enabling%20high%20performances%20in%20relevant%20downstream%20tasks.%0AHowever%2C%20they%20fail%20to%20showcase%20their%20quantitative%20understanding%20of%20objects%2C%20and%0Athey%20lack%20good%20counting-aware%20representation.%20This%20paper%20conducts%20a%0Areproducibility%20study%20of%20%27Teaching%20CLIP%20to%20Count%20to%20Ten%27%20%28Paiss%20et%20al.%2C%202023%29%2C%0Awhich%20presents%20a%20method%20to%20finetune%20a%20CLIP%20model%20%28Radford%20et%20al.%2C%202021%29%20to%0Aimprove%20zero-shot%20counting%20accuracy%20in%20an%20image%20while%20maintaining%20the%0Aperformance%20for%20zero-shot%20classification%20by%20introducing%20a%20counting-contrastive%0Aloss%20term.%20We%20improve%20the%20model%27s%20performance%20on%20a%20smaller%20subset%20of%20their%0Atraining%20data%20with%20lower%20computational%20resources.%20We%20verify%20these%20claims%20by%0Areproducing%20their%20study%20with%20our%20own%20code.%20The%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/SforAiDl/CountCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03586v2&entry.124074799=Read"},
{"title": "From NeRFs to Gaussian Splats, and Back", "author": "Siming He and Zach Osman and Pratik Chaudhari", "abstract": "  For robotics applications where there is a limited number of (typically\nego-centric) views, parametric representations such as neural radiance fields\n(NeRFs) generalize better than non-parametric ones such as Gaussian splatting\n(GS) to views that are very different from those in the training data; GS\nhowever can render much faster than NeRFs. We develop a procedure to convert\nback and forth between the two. Our approach achieves the best of both NeRFs\n(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact\nrepresentation) and GS (real-time rendering and ability for easily modifying\nthe representation); the computational cost of these conversions is minor\ncompared to training the two from scratch.\n", "link": "http://arxiv.org/abs/2405.09717v2", "date": "2024-06-10", "relevancy": 2.5541, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.712}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back&body=Title%3A%20From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back%0AAuthor%3A%20Siming%20He%20and%20Zach%20Osman%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20For%20robotics%20applications%20where%20there%20is%20a%20limited%20number%20of%20%28typically%0Aego-centric%29%20views%2C%20parametric%20representations%20such%20as%20neural%20radiance%20fields%0A%28NeRFs%29%20generalize%20better%20than%20non-parametric%20ones%20such%20as%20Gaussian%20splatting%0A%28GS%29%20to%20views%20that%20are%20very%20different%20from%20those%20in%20the%20training%20data%3B%20GS%0Ahowever%20can%20render%20much%20faster%20than%20NeRFs.%20We%20develop%20a%20procedure%20to%20convert%0Aback%20and%20forth%20between%20the%20two.%20Our%20approach%20achieves%20the%20best%20of%20both%20NeRFs%0A%28superior%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20on%20dissimilar%20views%2C%20and%20a%20compact%0Arepresentation%29%20and%20GS%20%28real-time%20rendering%20and%20ability%20for%20easily%20modifying%0Athe%20representation%29%3B%20the%20computational%20cost%20of%20these%20conversions%20is%20minor%0Acompared%20to%20training%20the%20two%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520NeRFs%2520to%2520Gaussian%2520Splats%252C%2520and%2520Back%26entry.906535625%3DSiming%2520He%2520and%2520Zach%2520Osman%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520For%2520robotics%2520applications%2520where%2520there%2520is%2520a%2520limited%2520number%2520of%2520%2528typically%250Aego-centric%2529%2520views%252C%2520parametric%2520representations%2520such%2520as%2520neural%2520radiance%2520fields%250A%2528NeRFs%2529%2520generalize%2520better%2520than%2520non-parametric%2520ones%2520such%2520as%2520Gaussian%2520splatting%250A%2528GS%2529%2520to%2520views%2520that%2520are%2520very%2520different%2520from%2520those%2520in%2520the%2520training%2520data%253B%2520GS%250Ahowever%2520can%2520render%2520much%2520faster%2520than%2520NeRFs.%2520We%2520develop%2520a%2520procedure%2520to%2520convert%250Aback%2520and%2520forth%2520between%2520the%2520two.%2520Our%2520approach%2520achieves%2520the%2520best%2520of%2520both%2520NeRFs%250A%2528superior%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520on%2520dissimilar%2520views%252C%2520and%2520a%2520compact%250Arepresentation%2529%2520and%2520GS%2520%2528real-time%2520rendering%2520and%2520ability%2520for%2520easily%2520modifying%250Athe%2520representation%2529%253B%2520the%2520computational%2520cost%2520of%2520these%2520conversions%2520is%2520minor%250Acompared%2520to%2520training%2520the%2520two%2520from%2520scratch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20NeRFs%20to%20Gaussian%20Splats%2C%20and%20Back&entry.906535625=Siming%20He%20and%20Zach%20Osman%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20For%20robotics%20applications%20where%20there%20is%20a%20limited%20number%20of%20%28typically%0Aego-centric%29%20views%2C%20parametric%20representations%20such%20as%20neural%20radiance%20fields%0A%28NeRFs%29%20generalize%20better%20than%20non-parametric%20ones%20such%20as%20Gaussian%20splatting%0A%28GS%29%20to%20views%20that%20are%20very%20different%20from%20those%20in%20the%20training%20data%3B%20GS%0Ahowever%20can%20render%20much%20faster%20than%20NeRFs.%20We%20develop%20a%20procedure%20to%20convert%0Aback%20and%20forth%20between%20the%20two.%20Our%20approach%20achieves%20the%20best%20of%20both%20NeRFs%0A%28superior%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20on%20dissimilar%20views%2C%20and%20a%20compact%0Arepresentation%29%20and%20GS%20%28real-time%20rendering%20and%20ability%20for%20easily%20modifying%0Athe%20representation%29%3B%20the%20computational%20cost%20of%20these%20conversions%20is%20minor%0Acompared%20to%20training%20the%20two%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09717v2&entry.124074799=Read"},
{"title": "Coverage Axis++: Efficient Inner Point Selection for 3D Shape\n  Skeletonization", "author": "Zimeng Wang and Zhiyang Dou and Rui Xu and Cheng Lin and Yuan Liu and Xiaoxiao Long and Shiqing Xin and Taku Komura and Xiaoming Yuan and Wenping Wang", "abstract": "  We introduce Coverage Axis++, a novel and efficient approach to 3D shape\nskeletonization. The current state-of-the-art approaches for this task often\nrely on the watertightness of the input or suffer from substantial\ncomputational costs, thereby limiting their practicality. To address this\nchallenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal\npoints, offering a high-accuracy approximation of the Medial Axis Transform\n(MAT) while significantly mitigating computational intensity for various shape\nrepresentations. We introduce a simple yet effective strategy that considers\nshape coverage, uniformity, and centrality to derive skeletal points. The\nselection procedure enforces consistency with the shape structure while\nfavoring the dominant medial balls, which thus introduces a compact underlying\nshape representation in terms of MAT. As a result, Coverage Axis++ allows for\nskeletonization for various shape representations (e.g., water-tight meshes,\ntriangle soups, point clouds), specification of the number of skeletal points,\nfew hyperparameters, and highly efficient computation with improved\nreconstruction accuracy. Extensive experiments across a wide range of 3D shapes\nvalidate the efficiency and effectiveness of Coverage Axis++. Our codes are\navailable at https://github.com/Frank-ZY-Dou/Coverage_Axis.\n", "link": "http://arxiv.org/abs/2401.12946v7", "date": "2024-06-10", "relevancy": 2.5386, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coverage%20Axis%2B%2B%3A%20Efficient%20Inner%20Point%20Selection%20for%203D%20Shape%0A%20%20Skeletonization&body=Title%3A%20Coverage%20Axis%2B%2B%3A%20Efficient%20Inner%20Point%20Selection%20for%203D%20Shape%0A%20%20Skeletonization%0AAuthor%3A%20Zimeng%20Wang%20and%20Zhiyang%20Dou%20and%20Rui%20Xu%20and%20Cheng%20Lin%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Shiqing%20Xin%20and%20Taku%20Komura%20and%20Xiaoming%20Yuan%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Coverage%20Axis%2B%2B%2C%20a%20novel%20and%20efficient%20approach%20to%203D%20shape%0Askeletonization.%20The%20current%20state-of-the-art%20approaches%20for%20this%20task%20often%0Arely%20on%20the%20watertightness%20of%20the%20input%20or%20suffer%20from%20substantial%0Acomputational%20costs%2C%20thereby%20limiting%20their%20practicality.%20To%20address%20this%0Achallenge%2C%20Coverage%20Axis%2B%2B%20proposes%20a%20heuristic%20algorithm%20to%20select%20skeletal%0Apoints%2C%20offering%20a%20high-accuracy%20approximation%20of%20the%20Medial%20Axis%20Transform%0A%28MAT%29%20while%20significantly%20mitigating%20computational%20intensity%20for%20various%20shape%0Arepresentations.%20We%20introduce%20a%20simple%20yet%20effective%20strategy%20that%20considers%0Ashape%20coverage%2C%20uniformity%2C%20and%20centrality%20to%20derive%20skeletal%20points.%20The%0Aselection%20procedure%20enforces%20consistency%20with%20the%20shape%20structure%20while%0Afavoring%20the%20dominant%20medial%20balls%2C%20which%20thus%20introduces%20a%20compact%20underlying%0Ashape%20representation%20in%20terms%20of%20MAT.%20As%20a%20result%2C%20Coverage%20Axis%2B%2B%20allows%20for%0Askeletonization%20for%20various%20shape%20representations%20%28e.g.%2C%20water-tight%20meshes%2C%0Atriangle%20soups%2C%20point%20clouds%29%2C%20specification%20of%20the%20number%20of%20skeletal%20points%2C%0Afew%20hyperparameters%2C%20and%20highly%20efficient%20computation%20with%20improved%0Areconstruction%20accuracy.%20Extensive%20experiments%20across%20a%20wide%20range%20of%203D%20shapes%0Avalidate%20the%20efficiency%20and%20effectiveness%20of%20Coverage%20Axis%2B%2B.%20Our%20codes%20are%0Aavailable%20at%20https%3A//github.com/Frank-ZY-Dou/Coverage_Axis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12946v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoverage%2520Axis%252B%252B%253A%2520Efficient%2520Inner%2520Point%2520Selection%2520for%25203D%2520Shape%250A%2520%2520Skeletonization%26entry.906535625%3DZimeng%2520Wang%2520and%2520Zhiyang%2520Dou%2520and%2520Rui%2520Xu%2520and%2520Cheng%2520Lin%2520and%2520Yuan%2520Liu%2520and%2520Xiaoxiao%2520Long%2520and%2520Shiqing%2520Xin%2520and%2520Taku%2520Komura%2520and%2520Xiaoming%2520Yuan%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Coverage%2520Axis%252B%252B%252C%2520a%2520novel%2520and%2520efficient%2520approach%2520to%25203D%2520shape%250Askeletonization.%2520The%2520current%2520state-of-the-art%2520approaches%2520for%2520this%2520task%2520often%250Arely%2520on%2520the%2520watertightness%2520of%2520the%2520input%2520or%2520suffer%2520from%2520substantial%250Acomputational%2520costs%252C%2520thereby%2520limiting%2520their%2520practicality.%2520To%2520address%2520this%250Achallenge%252C%2520Coverage%2520Axis%252B%252B%2520proposes%2520a%2520heuristic%2520algorithm%2520to%2520select%2520skeletal%250Apoints%252C%2520offering%2520a%2520high-accuracy%2520approximation%2520of%2520the%2520Medial%2520Axis%2520Transform%250A%2528MAT%2529%2520while%2520significantly%2520mitigating%2520computational%2520intensity%2520for%2520various%2520shape%250Arepresentations.%2520We%2520introduce%2520a%2520simple%2520yet%2520effective%2520strategy%2520that%2520considers%250Ashape%2520coverage%252C%2520uniformity%252C%2520and%2520centrality%2520to%2520derive%2520skeletal%2520points.%2520The%250Aselection%2520procedure%2520enforces%2520consistency%2520with%2520the%2520shape%2520structure%2520while%250Afavoring%2520the%2520dominant%2520medial%2520balls%252C%2520which%2520thus%2520introduces%2520a%2520compact%2520underlying%250Ashape%2520representation%2520in%2520terms%2520of%2520MAT.%2520As%2520a%2520result%252C%2520Coverage%2520Axis%252B%252B%2520allows%2520for%250Askeletonization%2520for%2520various%2520shape%2520representations%2520%2528e.g.%252C%2520water-tight%2520meshes%252C%250Atriangle%2520soups%252C%2520point%2520clouds%2529%252C%2520specification%2520of%2520the%2520number%2520of%2520skeletal%2520points%252C%250Afew%2520hyperparameters%252C%2520and%2520highly%2520efficient%2520computation%2520with%2520improved%250Areconstruction%2520accuracy.%2520Extensive%2520experiments%2520across%2520a%2520wide%2520range%2520of%25203D%2520shapes%250Avalidate%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520Coverage%2520Axis%252B%252B.%2520Our%2520codes%2520are%250Aavailable%2520at%2520https%253A//github.com/Frank-ZY-Dou/Coverage_Axis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12946v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coverage%20Axis%2B%2B%3A%20Efficient%20Inner%20Point%20Selection%20for%203D%20Shape%0A%20%20Skeletonization&entry.906535625=Zimeng%20Wang%20and%20Zhiyang%20Dou%20and%20Rui%20Xu%20and%20Cheng%20Lin%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Shiqing%20Xin%20and%20Taku%20Komura%20and%20Xiaoming%20Yuan%20and%20Wenping%20Wang&entry.1292438233=%20%20We%20introduce%20Coverage%20Axis%2B%2B%2C%20a%20novel%20and%20efficient%20approach%20to%203D%20shape%0Askeletonization.%20The%20current%20state-of-the-art%20approaches%20for%20this%20task%20often%0Arely%20on%20the%20watertightness%20of%20the%20input%20or%20suffer%20from%20substantial%0Acomputational%20costs%2C%20thereby%20limiting%20their%20practicality.%20To%20address%20this%0Achallenge%2C%20Coverage%20Axis%2B%2B%20proposes%20a%20heuristic%20algorithm%20to%20select%20skeletal%0Apoints%2C%20offering%20a%20high-accuracy%20approximation%20of%20the%20Medial%20Axis%20Transform%0A%28MAT%29%20while%20significantly%20mitigating%20computational%20intensity%20for%20various%20shape%0Arepresentations.%20We%20introduce%20a%20simple%20yet%20effective%20strategy%20that%20considers%0Ashape%20coverage%2C%20uniformity%2C%20and%20centrality%20to%20derive%20skeletal%20points.%20The%0Aselection%20procedure%20enforces%20consistency%20with%20the%20shape%20structure%20while%0Afavoring%20the%20dominant%20medial%20balls%2C%20which%20thus%20introduces%20a%20compact%20underlying%0Ashape%20representation%20in%20terms%20of%20MAT.%20As%20a%20result%2C%20Coverage%20Axis%2B%2B%20allows%20for%0Askeletonization%20for%20various%20shape%20representations%20%28e.g.%2C%20water-tight%20meshes%2C%0Atriangle%20soups%2C%20point%20clouds%29%2C%20specification%20of%20the%20number%20of%20skeletal%20points%2C%0Afew%20hyperparameters%2C%20and%20highly%20efficient%20computation%20with%20improved%0Areconstruction%20accuracy.%20Extensive%20experiments%20across%20a%20wide%20range%20of%203D%20shapes%0Avalidate%20the%20efficiency%20and%20effectiveness%20of%20Coverage%20Axis%2B%2B.%20Our%20codes%20are%0Aavailable%20at%20https%3A//github.com/Frank-ZY-Dou/Coverage_Axis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12946v7&entry.124074799=Read"},
{"title": "Get rich quick: exact solutions reveal how unbalanced initializations\n  promote rapid feature learning", "author": "Daniel Kunin and Allan Ravent\u00f3s and Cl\u00e9mentine Domin\u00e9 and Feng Chen and David Klindt and Andrew Saxe and Surya Ganguli", "abstract": "  While the impressive performance of modern neural networks is often\nattributed to their capacity to efficiently extract task-relevant features from\ndata, the mechanisms underlying this rich feature learning regime remain\nelusive, with much of our theoretical understanding stemming from the opposing\nlazy regime. In this work, we derive exact solutions to a minimal model that\ntransitions between lazy and rich learning, precisely elucidating how\nunbalanced layer-specific initialization variances and learning rates determine\nthe degree of feature learning. Our analysis reveals that they conspire to\ninfluence the learning regime through a set of conserved quantities that\nconstrain and modify the geometry of learning trajectories in parameter and\nfunction space. We extend our analysis to more complex linear models with\nmultiple neurons, outputs, and layers and to shallow nonlinear networks with\npiecewise linear activation functions. In linear networks, rapid feature\nlearning only occurs with balanced initializations, where all layers learn at\nsimilar speeds. While in nonlinear networks, unbalanced initializations that\npromote faster learning in earlier layers can accelerate rich learning. Through\na series of experiments, we provide evidence that this unbalanced rich regime\ndrives feature learning in deep finite-width networks, promotes\ninterpretability of early layers in CNNs, reduces the sample complexity of\nlearning hierarchical data, and decreases the time to grokking in modular\narithmetic. Our theory motivates further exploration of unbalanced\ninitializations to enhance efficient feature learning.\n", "link": "http://arxiv.org/abs/2406.06158v1", "date": "2024-06-10", "relevancy": 2.5268, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Get%20rich%20quick%3A%20exact%20solutions%20reveal%20how%20unbalanced%20initializations%0A%20%20promote%20rapid%20feature%20learning&body=Title%3A%20Get%20rich%20quick%3A%20exact%20solutions%20reveal%20how%20unbalanced%20initializations%0A%20%20promote%20rapid%20feature%20learning%0AAuthor%3A%20Daniel%20Kunin%20and%20Allan%20Ravent%C3%B3s%20and%20Cl%C3%A9mentine%20Domin%C3%A9%20and%20Feng%20Chen%20and%20David%20Klindt%20and%20Andrew%20Saxe%20and%20Surya%20Ganguli%0AAbstract%3A%20%20%20While%20the%20impressive%20performance%20of%20modern%20neural%20networks%20is%20often%0Aattributed%20to%20their%20capacity%20to%20efficiently%20extract%20task-relevant%20features%20from%0Adata%2C%20the%20mechanisms%20underlying%20this%20rich%20feature%20learning%20regime%20remain%0Aelusive%2C%20with%20much%20of%20our%20theoretical%20understanding%20stemming%20from%20the%20opposing%0Alazy%20regime.%20In%20this%20work%2C%20we%20derive%20exact%20solutions%20to%20a%20minimal%20model%20that%0Atransitions%20between%20lazy%20and%20rich%20learning%2C%20precisely%20elucidating%20how%0Aunbalanced%20layer-specific%20initialization%20variances%20and%20learning%20rates%20determine%0Athe%20degree%20of%20feature%20learning.%20Our%20analysis%20reveals%20that%20they%20conspire%20to%0Ainfluence%20the%20learning%20regime%20through%20a%20set%20of%20conserved%20quantities%20that%0Aconstrain%20and%20modify%20the%20geometry%20of%20learning%20trajectories%20in%20parameter%20and%0Afunction%20space.%20We%20extend%20our%20analysis%20to%20more%20complex%20linear%20models%20with%0Amultiple%20neurons%2C%20outputs%2C%20and%20layers%20and%20to%20shallow%20nonlinear%20networks%20with%0Apiecewise%20linear%20activation%20functions.%20In%20linear%20networks%2C%20rapid%20feature%0Alearning%20only%20occurs%20with%20balanced%20initializations%2C%20where%20all%20layers%20learn%20at%0Asimilar%20speeds.%20While%20in%20nonlinear%20networks%2C%20unbalanced%20initializations%20that%0Apromote%20faster%20learning%20in%20earlier%20layers%20can%20accelerate%20rich%20learning.%20Through%0Aa%20series%20of%20experiments%2C%20we%20provide%20evidence%20that%20this%20unbalanced%20rich%20regime%0Adrives%20feature%20learning%20in%20deep%20finite-width%20networks%2C%20promotes%0Ainterpretability%20of%20early%20layers%20in%20CNNs%2C%20reduces%20the%20sample%20complexity%20of%0Alearning%20hierarchical%20data%2C%20and%20decreases%20the%20time%20to%20grokking%20in%20modular%0Aarithmetic.%20Our%20theory%20motivates%20further%20exploration%20of%20unbalanced%0Ainitializations%20to%20enhance%20efficient%20feature%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGet%2520rich%2520quick%253A%2520exact%2520solutions%2520reveal%2520how%2520unbalanced%2520initializations%250A%2520%2520promote%2520rapid%2520feature%2520learning%26entry.906535625%3DDaniel%2520Kunin%2520and%2520Allan%2520Ravent%25C3%25B3s%2520and%2520Cl%25C3%25A9mentine%2520Domin%25C3%25A9%2520and%2520Feng%2520Chen%2520and%2520David%2520Klindt%2520and%2520Andrew%2520Saxe%2520and%2520Surya%2520Ganguli%26entry.1292438233%3D%2520%2520While%2520the%2520impressive%2520performance%2520of%2520modern%2520neural%2520networks%2520is%2520often%250Aattributed%2520to%2520their%2520capacity%2520to%2520efficiently%2520extract%2520task-relevant%2520features%2520from%250Adata%252C%2520the%2520mechanisms%2520underlying%2520this%2520rich%2520feature%2520learning%2520regime%2520remain%250Aelusive%252C%2520with%2520much%2520of%2520our%2520theoretical%2520understanding%2520stemming%2520from%2520the%2520opposing%250Alazy%2520regime.%2520In%2520this%2520work%252C%2520we%2520derive%2520exact%2520solutions%2520to%2520a%2520minimal%2520model%2520that%250Atransitions%2520between%2520lazy%2520and%2520rich%2520learning%252C%2520precisely%2520elucidating%2520how%250Aunbalanced%2520layer-specific%2520initialization%2520variances%2520and%2520learning%2520rates%2520determine%250Athe%2520degree%2520of%2520feature%2520learning.%2520Our%2520analysis%2520reveals%2520that%2520they%2520conspire%2520to%250Ainfluence%2520the%2520learning%2520regime%2520through%2520a%2520set%2520of%2520conserved%2520quantities%2520that%250Aconstrain%2520and%2520modify%2520the%2520geometry%2520of%2520learning%2520trajectories%2520in%2520parameter%2520and%250Afunction%2520space.%2520We%2520extend%2520our%2520analysis%2520to%2520more%2520complex%2520linear%2520models%2520with%250Amultiple%2520neurons%252C%2520outputs%252C%2520and%2520layers%2520and%2520to%2520shallow%2520nonlinear%2520networks%2520with%250Apiecewise%2520linear%2520activation%2520functions.%2520In%2520linear%2520networks%252C%2520rapid%2520feature%250Alearning%2520only%2520occurs%2520with%2520balanced%2520initializations%252C%2520where%2520all%2520layers%2520learn%2520at%250Asimilar%2520speeds.%2520While%2520in%2520nonlinear%2520networks%252C%2520unbalanced%2520initializations%2520that%250Apromote%2520faster%2520learning%2520in%2520earlier%2520layers%2520can%2520accelerate%2520rich%2520learning.%2520Through%250Aa%2520series%2520of%2520experiments%252C%2520we%2520provide%2520evidence%2520that%2520this%2520unbalanced%2520rich%2520regime%250Adrives%2520feature%2520learning%2520in%2520deep%2520finite-width%2520networks%252C%2520promotes%250Ainterpretability%2520of%2520early%2520layers%2520in%2520CNNs%252C%2520reduces%2520the%2520sample%2520complexity%2520of%250Alearning%2520hierarchical%2520data%252C%2520and%2520decreases%2520the%2520time%2520to%2520grokking%2520in%2520modular%250Aarithmetic.%2520Our%2520theory%2520motivates%2520further%2520exploration%2520of%2520unbalanced%250Ainitializations%2520to%2520enhance%2520efficient%2520feature%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Get%20rich%20quick%3A%20exact%20solutions%20reveal%20how%20unbalanced%20initializations%0A%20%20promote%20rapid%20feature%20learning&entry.906535625=Daniel%20Kunin%20and%20Allan%20Ravent%C3%B3s%20and%20Cl%C3%A9mentine%20Domin%C3%A9%20and%20Feng%20Chen%20and%20David%20Klindt%20and%20Andrew%20Saxe%20and%20Surya%20Ganguli&entry.1292438233=%20%20While%20the%20impressive%20performance%20of%20modern%20neural%20networks%20is%20often%0Aattributed%20to%20their%20capacity%20to%20efficiently%20extract%20task-relevant%20features%20from%0Adata%2C%20the%20mechanisms%20underlying%20this%20rich%20feature%20learning%20regime%20remain%0Aelusive%2C%20with%20much%20of%20our%20theoretical%20understanding%20stemming%20from%20the%20opposing%0Alazy%20regime.%20In%20this%20work%2C%20we%20derive%20exact%20solutions%20to%20a%20minimal%20model%20that%0Atransitions%20between%20lazy%20and%20rich%20learning%2C%20precisely%20elucidating%20how%0Aunbalanced%20layer-specific%20initialization%20variances%20and%20learning%20rates%20determine%0Athe%20degree%20of%20feature%20learning.%20Our%20analysis%20reveals%20that%20they%20conspire%20to%0Ainfluence%20the%20learning%20regime%20through%20a%20set%20of%20conserved%20quantities%20that%0Aconstrain%20and%20modify%20the%20geometry%20of%20learning%20trajectories%20in%20parameter%20and%0Afunction%20space.%20We%20extend%20our%20analysis%20to%20more%20complex%20linear%20models%20with%0Amultiple%20neurons%2C%20outputs%2C%20and%20layers%20and%20to%20shallow%20nonlinear%20networks%20with%0Apiecewise%20linear%20activation%20functions.%20In%20linear%20networks%2C%20rapid%20feature%0Alearning%20only%20occurs%20with%20balanced%20initializations%2C%20where%20all%20layers%20learn%20at%0Asimilar%20speeds.%20While%20in%20nonlinear%20networks%2C%20unbalanced%20initializations%20that%0Apromote%20faster%20learning%20in%20earlier%20layers%20can%20accelerate%20rich%20learning.%20Through%0Aa%20series%20of%20experiments%2C%20we%20provide%20evidence%20that%20this%20unbalanced%20rich%20regime%0Adrives%20feature%20learning%20in%20deep%20finite-width%20networks%2C%20promotes%0Ainterpretability%20of%20early%20layers%20in%20CNNs%2C%20reduces%20the%20sample%20complexity%20of%0Alearning%20hierarchical%20data%2C%20and%20decreases%20the%20time%20to%20grokking%20in%20modular%0Aarithmetic.%20Our%20theory%20motivates%20further%20exploration%20of%20unbalanced%0Ainitializations%20to%20enhance%20efficient%20feature%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06158v1&entry.124074799=Read"},
{"title": "Data-Efficient Learning with Neural Programs", "author": "Alaia Solko-Breslin and Seewon Choi and Ziyang Li and Neelay Velingker and Rajeev Alur and Mayur Naik and Eric Wong", "abstract": "  Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymolic learning literature. Our evaluation shows that\nfor the latter benchmarks, ISED has comparable performance to state-of-the-art\nneurosymbolic frameworks. For the former, we use adaptations of prior work on\ngradient approximations of black-box components as a baseline, and show that\nISED achieves comparable accuracy but in a more data- and sample-efficient\nmanner.\n", "link": "http://arxiv.org/abs/2406.06246v1", "date": "2024-06-10", "relevancy": 2.5198, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5415}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4876}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Learning%20with%20Neural%20Programs&body=Title%3A%20Data-Efficient%20Learning%20with%20Neural%20Programs%0AAuthor%3A%20Alaia%20Solko-Breslin%20and%20Seewon%20Choi%20and%20Ziyang%20Li%20and%20Neelay%20Velingker%20and%20Rajeev%20Alur%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Many%20computational%20tasks%20can%20be%20naturally%20expressed%20as%20a%20composition%20of%20a%20DNN%0Afollowed%20by%20a%20program%20written%20in%20a%20traditional%20programming%20language%20or%20an%20API%0Acall%20to%20an%20LLM.%20We%20call%20such%20composites%20%22neural%20programs%22%20and%20focus%20on%20the%0Aproblem%20of%20learning%20the%20DNN%20parameters%20when%20the%20training%20data%20consist%20of%0Aend-to-end%20input-output%20labels%20for%20the%20composite.%20When%20the%20program%20is%20written%0Ain%20a%20differentiable%20logic%20programming%20language%2C%20techniques%20from%20neurosymbolic%0Alearning%20are%20applicable%2C%20but%20in%20general%2C%20the%20learning%20for%20neural%20programs%0Arequires%20estimating%20the%20gradients%20of%20black-box%20components.%20We%20present%20an%0Aalgorithm%20for%20learning%20neural%20programs%2C%20called%20ISED%2C%20that%20only%20relies%20on%0Ainput-output%20samples%20of%20black-box%20components.%20For%20evaluation%2C%20we%20introduce%20new%0Abenchmarks%20that%20involve%20calls%20to%20modern%20LLMs%20such%20as%20GPT-4%20and%20also%20consider%0Abenchmarks%20from%20the%20neurosymolic%20learning%20literature.%20Our%20evaluation%20shows%20that%0Afor%20the%20latter%20benchmarks%2C%20ISED%20has%20comparable%20performance%20to%20state-of-the-art%0Aneurosymbolic%20frameworks.%20For%20the%20former%2C%20we%20use%20adaptations%20of%20prior%20work%20on%0Agradient%20approximations%20of%20black-box%20components%20as%20a%20baseline%2C%20and%20show%20that%0AISED%20achieves%20comparable%20accuracy%20but%20in%20a%20more%20data-%20and%20sample-efficient%0Amanner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Learning%2520with%2520Neural%2520Programs%26entry.906535625%3DAlaia%2520Solko-Breslin%2520and%2520Seewon%2520Choi%2520and%2520Ziyang%2520Li%2520and%2520Neelay%2520Velingker%2520and%2520Rajeev%2520Alur%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Many%2520computational%2520tasks%2520can%2520be%2520naturally%2520expressed%2520as%2520a%2520composition%2520of%2520a%2520DNN%250Afollowed%2520by%2520a%2520program%2520written%2520in%2520a%2520traditional%2520programming%2520language%2520or%2520an%2520API%250Acall%2520to%2520an%2520LLM.%2520We%2520call%2520such%2520composites%2520%2522neural%2520programs%2522%2520and%2520focus%2520on%2520the%250Aproblem%2520of%2520learning%2520the%2520DNN%2520parameters%2520when%2520the%2520training%2520data%2520consist%2520of%250Aend-to-end%2520input-output%2520labels%2520for%2520the%2520composite.%2520When%2520the%2520program%2520is%2520written%250Ain%2520a%2520differentiable%2520logic%2520programming%2520language%252C%2520techniques%2520from%2520neurosymbolic%250Alearning%2520are%2520applicable%252C%2520but%2520in%2520general%252C%2520the%2520learning%2520for%2520neural%2520programs%250Arequires%2520estimating%2520the%2520gradients%2520of%2520black-box%2520components.%2520We%2520present%2520an%250Aalgorithm%2520for%2520learning%2520neural%2520programs%252C%2520called%2520ISED%252C%2520that%2520only%2520relies%2520on%250Ainput-output%2520samples%2520of%2520black-box%2520components.%2520For%2520evaluation%252C%2520we%2520introduce%2520new%250Abenchmarks%2520that%2520involve%2520calls%2520to%2520modern%2520LLMs%2520such%2520as%2520GPT-4%2520and%2520also%2520consider%250Abenchmarks%2520from%2520the%2520neurosymolic%2520learning%2520literature.%2520Our%2520evaluation%2520shows%2520that%250Afor%2520the%2520latter%2520benchmarks%252C%2520ISED%2520has%2520comparable%2520performance%2520to%2520state-of-the-art%250Aneurosymbolic%2520frameworks.%2520For%2520the%2520former%252C%2520we%2520use%2520adaptations%2520of%2520prior%2520work%2520on%250Agradient%2520approximations%2520of%2520black-box%2520components%2520as%2520a%2520baseline%252C%2520and%2520show%2520that%250AISED%2520achieves%2520comparable%2520accuracy%2520but%2520in%2520a%2520more%2520data-%2520and%2520sample-efficient%250Amanner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Learning%20with%20Neural%20Programs&entry.906535625=Alaia%20Solko-Breslin%20and%20Seewon%20Choi%20and%20Ziyang%20Li%20and%20Neelay%20Velingker%20and%20Rajeev%20Alur%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Many%20computational%20tasks%20can%20be%20naturally%20expressed%20as%20a%20composition%20of%20a%20DNN%0Afollowed%20by%20a%20program%20written%20in%20a%20traditional%20programming%20language%20or%20an%20API%0Acall%20to%20an%20LLM.%20We%20call%20such%20composites%20%22neural%20programs%22%20and%20focus%20on%20the%0Aproblem%20of%20learning%20the%20DNN%20parameters%20when%20the%20training%20data%20consist%20of%0Aend-to-end%20input-output%20labels%20for%20the%20composite.%20When%20the%20program%20is%20written%0Ain%20a%20differentiable%20logic%20programming%20language%2C%20techniques%20from%20neurosymbolic%0Alearning%20are%20applicable%2C%20but%20in%20general%2C%20the%20learning%20for%20neural%20programs%0Arequires%20estimating%20the%20gradients%20of%20black-box%20components.%20We%20present%20an%0Aalgorithm%20for%20learning%20neural%20programs%2C%20called%20ISED%2C%20that%20only%20relies%20on%0Ainput-output%20samples%20of%20black-box%20components.%20For%20evaluation%2C%20we%20introduce%20new%0Abenchmarks%20that%20involve%20calls%20to%20modern%20LLMs%20such%20as%20GPT-4%20and%20also%20consider%0Abenchmarks%20from%20the%20neurosymolic%20learning%20literature.%20Our%20evaluation%20shows%20that%0Afor%20the%20latter%20benchmarks%2C%20ISED%20has%20comparable%20performance%20to%20state-of-the-art%0Aneurosymbolic%20frameworks.%20For%20the%20former%2C%20we%20use%20adaptations%20of%20prior%20work%20on%0Agradient%20approximations%20of%20black-box%20components%20as%20a%20baseline%2C%20and%20show%20that%0AISED%20achieves%20comparable%20accuracy%20but%20in%20a%20more%20data-%20and%20sample-efficient%0Amanner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06246v1&entry.124074799=Read"},
{"title": "Decentralized Personalized Federated Learning", "author": "Salma Kharrat and Marco Canini and Samuel Horvath", "abstract": "  This work tackles the challenges of data heterogeneity and communication\nlimitations in decentralized federated learning. We focus on creating a\ncollaboration graph that guides each client in selecting suitable collaborators\nfor training personalized models that leverage their local data effectively.\nOur approach addresses these issues through a novel, communication-efficient\nstrategy that enhances resource efficiency. Unlike traditional methods, our\nformulation identifies collaborators at a granular level by considering\ncombinatorial relations of clients, enhancing personalization while minimizing\ncommunication overhead. We achieve this through a bi-level optimization\nframework that employs a constrained greedy algorithm, resulting in a\nresource-efficient collaboration graph for personalized learning. Extensive\nevaluation against various baselines across diverse datasets demonstrates the\nsuperiority of our method, named DPFL. DPFL consistently outperforms other\napproaches, showcasing its effectiveness in handling real-world data\nheterogeneity, minimizing communication overhead, enhancing resource\nefficiency, and building personalized models in decentralized federated\nlearning scenarios.\n", "link": "http://arxiv.org/abs/2406.06520v1", "date": "2024-06-10", "relevancy": 2.4669, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5116}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4957}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Personalized%20Federated%20Learning&body=Title%3A%20Decentralized%20Personalized%20Federated%20Learning%0AAuthor%3A%20Salma%20Kharrat%20and%20Marco%20Canini%20and%20Samuel%20Horvath%0AAbstract%3A%20%20%20This%20work%20tackles%20the%20challenges%20of%20data%20heterogeneity%20and%20communication%0Alimitations%20in%20decentralized%20federated%20learning.%20We%20focus%20on%20creating%20a%0Acollaboration%20graph%20that%20guides%20each%20client%20in%20selecting%20suitable%20collaborators%0Afor%20training%20personalized%20models%20that%20leverage%20their%20local%20data%20effectively.%0AOur%20approach%20addresses%20these%20issues%20through%20a%20novel%2C%20communication-efficient%0Astrategy%20that%20enhances%20resource%20efficiency.%20Unlike%20traditional%20methods%2C%20our%0Aformulation%20identifies%20collaborators%20at%20a%20granular%20level%20by%20considering%0Acombinatorial%20relations%20of%20clients%2C%20enhancing%20personalization%20while%20minimizing%0Acommunication%20overhead.%20We%20achieve%20this%20through%20a%20bi-level%20optimization%0Aframework%20that%20employs%20a%20constrained%20greedy%20algorithm%2C%20resulting%20in%20a%0Aresource-efficient%20collaboration%20graph%20for%20personalized%20learning.%20Extensive%0Aevaluation%20against%20various%20baselines%20across%20diverse%20datasets%20demonstrates%20the%0Asuperiority%20of%20our%20method%2C%20named%20DPFL.%20DPFL%20consistently%20outperforms%20other%0Aapproaches%2C%20showcasing%20its%20effectiveness%20in%20handling%20real-world%20data%0Aheterogeneity%2C%20minimizing%20communication%20overhead%2C%20enhancing%20resource%0Aefficiency%2C%20and%20building%20personalized%20models%20in%20decentralized%20federated%0Alearning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Personalized%2520Federated%2520Learning%26entry.906535625%3DSalma%2520Kharrat%2520and%2520Marco%2520Canini%2520and%2520Samuel%2520Horvath%26entry.1292438233%3D%2520%2520This%2520work%2520tackles%2520the%2520challenges%2520of%2520data%2520heterogeneity%2520and%2520communication%250Alimitations%2520in%2520decentralized%2520federated%2520learning.%2520We%2520focus%2520on%2520creating%2520a%250Acollaboration%2520graph%2520that%2520guides%2520each%2520client%2520in%2520selecting%2520suitable%2520collaborators%250Afor%2520training%2520personalized%2520models%2520that%2520leverage%2520their%2520local%2520data%2520effectively.%250AOur%2520approach%2520addresses%2520these%2520issues%2520through%2520a%2520novel%252C%2520communication-efficient%250Astrategy%2520that%2520enhances%2520resource%2520efficiency.%2520Unlike%2520traditional%2520methods%252C%2520our%250Aformulation%2520identifies%2520collaborators%2520at%2520a%2520granular%2520level%2520by%2520considering%250Acombinatorial%2520relations%2520of%2520clients%252C%2520enhancing%2520personalization%2520while%2520minimizing%250Acommunication%2520overhead.%2520We%2520achieve%2520this%2520through%2520a%2520bi-level%2520optimization%250Aframework%2520that%2520employs%2520a%2520constrained%2520greedy%2520algorithm%252C%2520resulting%2520in%2520a%250Aresource-efficient%2520collaboration%2520graph%2520for%2520personalized%2520learning.%2520Extensive%250Aevaluation%2520against%2520various%2520baselines%2520across%2520diverse%2520datasets%2520demonstrates%2520the%250Asuperiority%2520of%2520our%2520method%252C%2520named%2520DPFL.%2520DPFL%2520consistently%2520outperforms%2520other%250Aapproaches%252C%2520showcasing%2520its%2520effectiveness%2520in%2520handling%2520real-world%2520data%250Aheterogeneity%252C%2520minimizing%2520communication%2520overhead%252C%2520enhancing%2520resource%250Aefficiency%252C%2520and%2520building%2520personalized%2520models%2520in%2520decentralized%2520federated%250Alearning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Personalized%20Federated%20Learning&entry.906535625=Salma%20Kharrat%20and%20Marco%20Canini%20and%20Samuel%20Horvath&entry.1292438233=%20%20This%20work%20tackles%20the%20challenges%20of%20data%20heterogeneity%20and%20communication%0Alimitations%20in%20decentralized%20federated%20learning.%20We%20focus%20on%20creating%20a%0Acollaboration%20graph%20that%20guides%20each%20client%20in%20selecting%20suitable%20collaborators%0Afor%20training%20personalized%20models%20that%20leverage%20their%20local%20data%20effectively.%0AOur%20approach%20addresses%20these%20issues%20through%20a%20novel%2C%20communication-efficient%0Astrategy%20that%20enhances%20resource%20efficiency.%20Unlike%20traditional%20methods%2C%20our%0Aformulation%20identifies%20collaborators%20at%20a%20granular%20level%20by%20considering%0Acombinatorial%20relations%20of%20clients%2C%20enhancing%20personalization%20while%20minimizing%0Acommunication%20overhead.%20We%20achieve%20this%20through%20a%20bi-level%20optimization%0Aframework%20that%20employs%20a%20constrained%20greedy%20algorithm%2C%20resulting%20in%20a%0Aresource-efficient%20collaboration%20graph%20for%20personalized%20learning.%20Extensive%0Aevaluation%20against%20various%20baselines%20across%20diverse%20datasets%20demonstrates%20the%0Asuperiority%20of%20our%20method%2C%20named%20DPFL.%20DPFL%20consistently%20outperforms%20other%0Aapproaches%2C%20showcasing%20its%20effectiveness%20in%20handling%20real-world%20data%0Aheterogeneity%2C%20minimizing%20communication%20overhead%2C%20enhancing%20resource%0Aefficiency%2C%20and%20building%20personalized%20models%20in%20decentralized%20federated%0Alearning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06520v1&entry.124074799=Read"},
{"title": "Byzantine-Robust Federated Learning: Impact of Client Subsampling and\n  Local Updates", "author": "Youssef Allouah and Sadegh Farhadkhani and Rachid GuerraouI and Nirupam Gupta and Rafael Pinot and Geovani Rizk and Sasha Voitovych", "abstract": "  The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes\nfederated learning (FL) prone to arbitrary manipulation. The natural approach\nto robustify FL against adversarial clients is to replace the simple averaging\noperation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a\n\\emph{robust averaging rule}. While a significant amount of work has been\ndevoted to studying the convergence of federated {\\em robust averaging} (which\nwe denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of\n{\\em client subsampling} and {\\em local steps}, two fundamental FL\ncharacteristics. While client subsampling increases the effective fraction of\nByzantine clients, local steps increase the drift between the local updates\ncomputed by honest (i.e., non-Byzantine) clients. Consequently, a careless\ndeployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this\nobservation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly\nanalyzing the impact of client subsampling and local steps. Specifically, we\npresent a sufficient condition on client subsampling for nearly-optimal\nconvergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show\nthat the rate of improvement in learning accuracy {\\em diminishes} with respect\nto the number of clients subsampled, as soon as the sample size exceeds a\nthreshold value. Interestingly, we also observe that under a careful choice of\nstep-sizes, the learning error due to Byzantine clients decreases with the\nnumber of local steps. We validate our theory by experiments on the FEMNIST and\nCIFAR-$10$ image classification tasks.\n", "link": "http://arxiv.org/abs/2402.12780v2", "date": "2024-06-10", "relevancy": 2.45, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Byzantine-Robust%20Federated%20Learning%3A%20Impact%20of%20Client%20Subsampling%20and%0A%20%20Local%20Updates&body=Title%3A%20Byzantine-Robust%20Federated%20Learning%3A%20Impact%20of%20Client%20Subsampling%20and%0A%20%20Local%20Updates%0AAuthor%3A%20Youssef%20Allouah%20and%20Sadegh%20Farhadkhani%20and%20Rachid%20GuerraouI%20and%20Nirupam%20Gupta%20and%20Rafael%20Pinot%20and%20Geovani%20Rizk%20and%20Sasha%20Voitovych%0AAbstract%3A%20%20%20The%20possibility%20of%20adversarial%20%28a.k.a.%2C%20%7B%5Cem%20Byzantine%7D%29%20clients%20makes%0Afederated%20learning%20%28FL%29%20prone%20to%20arbitrary%20manipulation.%20The%20natural%20approach%0Ato%20robustify%20FL%20against%20adversarial%20clients%20is%20to%20replace%20the%20simple%20averaging%0Aoperation%20at%20the%20server%20in%20the%20standard%20%24%5Cmathsf%7BFedAvg%7D%24%20algorithm%20by%20a%0A%5Cemph%7Brobust%20averaging%20rule%7D.%20While%20a%20significant%20amount%20of%20work%20has%20been%0Adevoted%20to%20studying%20the%20convergence%20of%20federated%20%7B%5Cem%20robust%20averaging%7D%20%28which%0Awe%20denote%20by%20%24%5Cmathsf%7BFedRo%7D%24%29%2C%20prior%20work%20has%20largely%20ignored%20the%20impact%20of%0A%7B%5Cem%20client%20subsampling%7D%20and%20%7B%5Cem%20local%20steps%7D%2C%20two%20fundamental%20FL%0Acharacteristics.%20While%20client%20subsampling%20increases%20the%20effective%20fraction%20of%0AByzantine%20clients%2C%20local%20steps%20increase%20the%20drift%20between%20the%20local%20updates%0Acomputed%20by%20honest%20%28i.e.%2C%20non-Byzantine%29%20clients.%20Consequently%2C%20a%20careless%0Adeployment%20of%20%24%5Cmathsf%7BFedRo%7D%24%20could%20yield%20poor%20performance.%20We%20validate%20this%0Aobservation%20by%20presenting%20an%20in-depth%20analysis%20of%20%24%5Cmathsf%7BFedRo%7D%24%20tightly%0Aanalyzing%20the%20impact%20of%20client%20subsampling%20and%20local%20steps.%20Specifically%2C%20we%0Apresent%20a%20sufficient%20condition%20on%20client%20subsampling%20for%20nearly-optimal%0Aconvergence%20of%20%24%5Cmathsf%7BFedRo%7D%24%20%28for%20smooth%20non-convex%20loss%29.%20Also%2C%20we%20show%0Athat%20the%20rate%20of%20improvement%20in%20learning%20accuracy%20%7B%5Cem%20diminishes%7D%20with%20respect%0Ato%20the%20number%20of%20clients%20subsampled%2C%20as%20soon%20as%20the%20sample%20size%20exceeds%20a%0Athreshold%20value.%20Interestingly%2C%20we%20also%20observe%20that%20under%20a%20careful%20choice%20of%0Astep-sizes%2C%20the%20learning%20error%20due%20to%20Byzantine%20clients%20decreases%20with%20the%0Anumber%20of%20local%20steps.%20We%20validate%20our%20theory%20by%20experiments%20on%20the%20FEMNIST%20and%0ACIFAR-%2410%24%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByzantine-Robust%2520Federated%2520Learning%253A%2520Impact%2520of%2520Client%2520Subsampling%2520and%250A%2520%2520Local%2520Updates%26entry.906535625%3DYoussef%2520Allouah%2520and%2520Sadegh%2520Farhadkhani%2520and%2520Rachid%2520GuerraouI%2520and%2520Nirupam%2520Gupta%2520and%2520Rafael%2520Pinot%2520and%2520Geovani%2520Rizk%2520and%2520Sasha%2520Voitovych%26entry.1292438233%3D%2520%2520The%2520possibility%2520of%2520adversarial%2520%2528a.k.a.%252C%2520%257B%255Cem%2520Byzantine%257D%2529%2520clients%2520makes%250Afederated%2520learning%2520%2528FL%2529%2520prone%2520to%2520arbitrary%2520manipulation.%2520The%2520natural%2520approach%250Ato%2520robustify%2520FL%2520against%2520adversarial%2520clients%2520is%2520to%2520replace%2520the%2520simple%2520averaging%250Aoperation%2520at%2520the%2520server%2520in%2520the%2520standard%2520%2524%255Cmathsf%257BFedAvg%257D%2524%2520algorithm%2520by%2520a%250A%255Cemph%257Brobust%2520averaging%2520rule%257D.%2520While%2520a%2520significant%2520amount%2520of%2520work%2520has%2520been%250Adevoted%2520to%2520studying%2520the%2520convergence%2520of%2520federated%2520%257B%255Cem%2520robust%2520averaging%257D%2520%2528which%250Awe%2520denote%2520by%2520%2524%255Cmathsf%257BFedRo%257D%2524%2529%252C%2520prior%2520work%2520has%2520largely%2520ignored%2520the%2520impact%2520of%250A%257B%255Cem%2520client%2520subsampling%257D%2520and%2520%257B%255Cem%2520local%2520steps%257D%252C%2520two%2520fundamental%2520FL%250Acharacteristics.%2520While%2520client%2520subsampling%2520increases%2520the%2520effective%2520fraction%2520of%250AByzantine%2520clients%252C%2520local%2520steps%2520increase%2520the%2520drift%2520between%2520the%2520local%2520updates%250Acomputed%2520by%2520honest%2520%2528i.e.%252C%2520non-Byzantine%2529%2520clients.%2520Consequently%252C%2520a%2520careless%250Adeployment%2520of%2520%2524%255Cmathsf%257BFedRo%257D%2524%2520could%2520yield%2520poor%2520performance.%2520We%2520validate%2520this%250Aobservation%2520by%2520presenting%2520an%2520in-depth%2520analysis%2520of%2520%2524%255Cmathsf%257BFedRo%257D%2524%2520tightly%250Aanalyzing%2520the%2520impact%2520of%2520client%2520subsampling%2520and%2520local%2520steps.%2520Specifically%252C%2520we%250Apresent%2520a%2520sufficient%2520condition%2520on%2520client%2520subsampling%2520for%2520nearly-optimal%250Aconvergence%2520of%2520%2524%255Cmathsf%257BFedRo%257D%2524%2520%2528for%2520smooth%2520non-convex%2520loss%2529.%2520Also%252C%2520we%2520show%250Athat%2520the%2520rate%2520of%2520improvement%2520in%2520learning%2520accuracy%2520%257B%255Cem%2520diminishes%257D%2520with%2520respect%250Ato%2520the%2520number%2520of%2520clients%2520subsampled%252C%2520as%2520soon%2520as%2520the%2520sample%2520size%2520exceeds%2520a%250Athreshold%2520value.%2520Interestingly%252C%2520we%2520also%2520observe%2520that%2520under%2520a%2520careful%2520choice%2520of%250Astep-sizes%252C%2520the%2520learning%2520error%2520due%2520to%2520Byzantine%2520clients%2520decreases%2520with%2520the%250Anumber%2520of%2520local%2520steps.%2520We%2520validate%2520our%2520theory%2520by%2520experiments%2520on%2520the%2520FEMNIST%2520and%250ACIFAR-%252410%2524%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Byzantine-Robust%20Federated%20Learning%3A%20Impact%20of%20Client%20Subsampling%20and%0A%20%20Local%20Updates&entry.906535625=Youssef%20Allouah%20and%20Sadegh%20Farhadkhani%20and%20Rachid%20GuerraouI%20and%20Nirupam%20Gupta%20and%20Rafael%20Pinot%20and%20Geovani%20Rizk%20and%20Sasha%20Voitovych&entry.1292438233=%20%20The%20possibility%20of%20adversarial%20%28a.k.a.%2C%20%7B%5Cem%20Byzantine%7D%29%20clients%20makes%0Afederated%20learning%20%28FL%29%20prone%20to%20arbitrary%20manipulation.%20The%20natural%20approach%0Ato%20robustify%20FL%20against%20adversarial%20clients%20is%20to%20replace%20the%20simple%20averaging%0Aoperation%20at%20the%20server%20in%20the%20standard%20%24%5Cmathsf%7BFedAvg%7D%24%20algorithm%20by%20a%0A%5Cemph%7Brobust%20averaging%20rule%7D.%20While%20a%20significant%20amount%20of%20work%20has%20been%0Adevoted%20to%20studying%20the%20convergence%20of%20federated%20%7B%5Cem%20robust%20averaging%7D%20%28which%0Awe%20denote%20by%20%24%5Cmathsf%7BFedRo%7D%24%29%2C%20prior%20work%20has%20largely%20ignored%20the%20impact%20of%0A%7B%5Cem%20client%20subsampling%7D%20and%20%7B%5Cem%20local%20steps%7D%2C%20two%20fundamental%20FL%0Acharacteristics.%20While%20client%20subsampling%20increases%20the%20effective%20fraction%20of%0AByzantine%20clients%2C%20local%20steps%20increase%20the%20drift%20between%20the%20local%20updates%0Acomputed%20by%20honest%20%28i.e.%2C%20non-Byzantine%29%20clients.%20Consequently%2C%20a%20careless%0Adeployment%20of%20%24%5Cmathsf%7BFedRo%7D%24%20could%20yield%20poor%20performance.%20We%20validate%20this%0Aobservation%20by%20presenting%20an%20in-depth%20analysis%20of%20%24%5Cmathsf%7BFedRo%7D%24%20tightly%0Aanalyzing%20the%20impact%20of%20client%20subsampling%20and%20local%20steps.%20Specifically%2C%20we%0Apresent%20a%20sufficient%20condition%20on%20client%20subsampling%20for%20nearly-optimal%0Aconvergence%20of%20%24%5Cmathsf%7BFedRo%7D%24%20%28for%20smooth%20non-convex%20loss%29.%20Also%2C%20we%20show%0Athat%20the%20rate%20of%20improvement%20in%20learning%20accuracy%20%7B%5Cem%20diminishes%7D%20with%20respect%0Ato%20the%20number%20of%20clients%20subsampled%2C%20as%20soon%20as%20the%20sample%20size%20exceeds%20a%0Athreshold%20value.%20Interestingly%2C%20we%20also%20observe%20that%20under%20a%20careful%20choice%20of%0Astep-sizes%2C%20the%20learning%20error%20due%20to%20Byzantine%20clients%20decreases%20with%20the%0Anumber%20of%20local%20steps.%20We%20validate%20our%20theory%20by%20experiments%20on%20the%20FEMNIST%20and%0ACIFAR-%2410%24%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12780v2&entry.124074799=Read"},
{"title": "UnSupDLA: Towards Unsupervised Document Layout Analysis", "author": "Talha Uddin Sheikh and Tahira Shehzadi and Khurram Azeem Hashmi and Didier Stricker and Muhammad Zeshan Afzal", "abstract": "  Document layout analysis is a key area in document research, involving\ntechniques like text mining and visual analysis. Despite various methods\ndeveloped to tackle layout analysis, a critical but frequently overlooked\nproblem is the scarcity of labeled data needed for analyses. With the rise of\ninternet use, an overwhelming number of documents are now available online,\nmaking the process of accurately labeling them for research purposes\nincreasingly challenging and labor-intensive. Moreover, the diversity of\ndocuments online presents a unique set of challenges in maintaining the quality\nand consistency of these labels, further complicating document layout analysis\nin the digital era. To address this, we employ a vision-based approach for\nanalyzing document layouts designed to train a network without labels. Instead,\nwe focus on pre-training, initially generating simple object masks from the\nunlabeled document images. These masks are then used to train a detector,\nenhancing object detection and segmentation performance. The model's\neffectiveness is further amplified through several unsupervised training\niterations, continuously refining its performance. This approach significantly\nadvances document layout analysis, particularly precision and efficiency,\nwithout labels.\n", "link": "http://arxiv.org/abs/2406.06236v1", "date": "2024-06-10", "relevancy": 2.4473, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.703}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5661}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnSupDLA%3A%20Towards%20Unsupervised%20Document%20Layout%20Analysis&body=Title%3A%20UnSupDLA%3A%20Towards%20Unsupervised%20Document%20Layout%20Analysis%0AAuthor%3A%20Talha%20Uddin%20Sheikh%20and%20Tahira%20Shehzadi%20and%20Khurram%20Azeem%20Hashmi%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20Document%20layout%20analysis%20is%20a%20key%20area%20in%20document%20research%2C%20involving%0Atechniques%20like%20text%20mining%20and%20visual%20analysis.%20Despite%20various%20methods%0Adeveloped%20to%20tackle%20layout%20analysis%2C%20a%20critical%20but%20frequently%20overlooked%0Aproblem%20is%20the%20scarcity%20of%20labeled%20data%20needed%20for%20analyses.%20With%20the%20rise%20of%0Ainternet%20use%2C%20an%20overwhelming%20number%20of%20documents%20are%20now%20available%20online%2C%0Amaking%20the%20process%20of%20accurately%20labeling%20them%20for%20research%20purposes%0Aincreasingly%20challenging%20and%20labor-intensive.%20Moreover%2C%20the%20diversity%20of%0Adocuments%20online%20presents%20a%20unique%20set%20of%20challenges%20in%20maintaining%20the%20quality%0Aand%20consistency%20of%20these%20labels%2C%20further%20complicating%20document%20layout%20analysis%0Ain%20the%20digital%20era.%20To%20address%20this%2C%20we%20employ%20a%20vision-based%20approach%20for%0Aanalyzing%20document%20layouts%20designed%20to%20train%20a%20network%20without%20labels.%20Instead%2C%0Awe%20focus%20on%20pre-training%2C%20initially%20generating%20simple%20object%20masks%20from%20the%0Aunlabeled%20document%20images.%20These%20masks%20are%20then%20used%20to%20train%20a%20detector%2C%0Aenhancing%20object%20detection%20and%20segmentation%20performance.%20The%20model%27s%0Aeffectiveness%20is%20further%20amplified%20through%20several%20unsupervised%20training%0Aiterations%2C%20continuously%20refining%20its%20performance.%20This%20approach%20significantly%0Aadvances%20document%20layout%20analysis%2C%20particularly%20precision%20and%20efficiency%2C%0Awithout%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnSupDLA%253A%2520Towards%2520Unsupervised%2520Document%2520Layout%2520Analysis%26entry.906535625%3DTalha%2520Uddin%2520Sheikh%2520and%2520Tahira%2520Shehzadi%2520and%2520Khurram%2520Azeem%2520Hashmi%2520and%2520Didier%2520Stricker%2520and%2520Muhammad%2520Zeshan%2520Afzal%26entry.1292438233%3D%2520%2520Document%2520layout%2520analysis%2520is%2520a%2520key%2520area%2520in%2520document%2520research%252C%2520involving%250Atechniques%2520like%2520text%2520mining%2520and%2520visual%2520analysis.%2520Despite%2520various%2520methods%250Adeveloped%2520to%2520tackle%2520layout%2520analysis%252C%2520a%2520critical%2520but%2520frequently%2520overlooked%250Aproblem%2520is%2520the%2520scarcity%2520of%2520labeled%2520data%2520needed%2520for%2520analyses.%2520With%2520the%2520rise%2520of%250Ainternet%2520use%252C%2520an%2520overwhelming%2520number%2520of%2520documents%2520are%2520now%2520available%2520online%252C%250Amaking%2520the%2520process%2520of%2520accurately%2520labeling%2520them%2520for%2520research%2520purposes%250Aincreasingly%2520challenging%2520and%2520labor-intensive.%2520Moreover%252C%2520the%2520diversity%2520of%250Adocuments%2520online%2520presents%2520a%2520unique%2520set%2520of%2520challenges%2520in%2520maintaining%2520the%2520quality%250Aand%2520consistency%2520of%2520these%2520labels%252C%2520further%2520complicating%2520document%2520layout%2520analysis%250Ain%2520the%2520digital%2520era.%2520To%2520address%2520this%252C%2520we%2520employ%2520a%2520vision-based%2520approach%2520for%250Aanalyzing%2520document%2520layouts%2520designed%2520to%2520train%2520a%2520network%2520without%2520labels.%2520Instead%252C%250Awe%2520focus%2520on%2520pre-training%252C%2520initially%2520generating%2520simple%2520object%2520masks%2520from%2520the%250Aunlabeled%2520document%2520images.%2520These%2520masks%2520are%2520then%2520used%2520to%2520train%2520a%2520detector%252C%250Aenhancing%2520object%2520detection%2520and%2520segmentation%2520performance.%2520The%2520model%2527s%250Aeffectiveness%2520is%2520further%2520amplified%2520through%2520several%2520unsupervised%2520training%250Aiterations%252C%2520continuously%2520refining%2520its%2520performance.%2520This%2520approach%2520significantly%250Aadvances%2520document%2520layout%2520analysis%252C%2520particularly%2520precision%2520and%2520efficiency%252C%250Awithout%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnSupDLA%3A%20Towards%20Unsupervised%20Document%20Layout%20Analysis&entry.906535625=Talha%20Uddin%20Sheikh%20and%20Tahira%20Shehzadi%20and%20Khurram%20Azeem%20Hashmi%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20Document%20layout%20analysis%20is%20a%20key%20area%20in%20document%20research%2C%20involving%0Atechniques%20like%20text%20mining%20and%20visual%20analysis.%20Despite%20various%20methods%0Adeveloped%20to%20tackle%20layout%20analysis%2C%20a%20critical%20but%20frequently%20overlooked%0Aproblem%20is%20the%20scarcity%20of%20labeled%20data%20needed%20for%20analyses.%20With%20the%20rise%20of%0Ainternet%20use%2C%20an%20overwhelming%20number%20of%20documents%20are%20now%20available%20online%2C%0Amaking%20the%20process%20of%20accurately%20labeling%20them%20for%20research%20purposes%0Aincreasingly%20challenging%20and%20labor-intensive.%20Moreover%2C%20the%20diversity%20of%0Adocuments%20online%20presents%20a%20unique%20set%20of%20challenges%20in%20maintaining%20the%20quality%0Aand%20consistency%20of%20these%20labels%2C%20further%20complicating%20document%20layout%20analysis%0Ain%20the%20digital%20era.%20To%20address%20this%2C%20we%20employ%20a%20vision-based%20approach%20for%0Aanalyzing%20document%20layouts%20designed%20to%20train%20a%20network%20without%20labels.%20Instead%2C%0Awe%20focus%20on%20pre-training%2C%20initially%20generating%20simple%20object%20masks%20from%20the%0Aunlabeled%20document%20images.%20These%20masks%20are%20then%20used%20to%20train%20a%20detector%2C%0Aenhancing%20object%20detection%20and%20segmentation%20performance.%20The%20model%27s%0Aeffectiveness%20is%20further%20amplified%20through%20several%20unsupervised%20training%0Aiterations%2C%20continuously%20refining%20its%20performance.%20This%20approach%20significantly%0Aadvances%20document%20layout%20analysis%2C%20particularly%20precision%20and%20efficiency%2C%0Awithout%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06236v1&entry.124074799=Read"},
{"title": "Robust Distribution Learning with Local and Global Adversarial\n  Corruptions", "author": "Sloan Nietert and Ziv Goldfeld and Soroosh Shafiee", "abstract": "  We consider learning in an adversarial environment, where an\n$\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily\nmodified (*global* corruptions) and the remaining perturbations have average\nmagnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such\ncorrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$\nthat minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact,\nwe attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n,\n\\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$,\nwith performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to\naccount simultaneously for mean estimation ($k=1$), distribution estimation\n($k=d$), as well as the settings interpolating between these two extremes. We\ncharacterize the optimal population-limit risk for this task and then develop\nan efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon\nk} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order\n$2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded\ncovariance, our finite-sample bounds match the minimax population-level optimum\nfor large sample sizes. Our efficient procedure relies on a novel trace norm\napproximation of an ideal yet intractable 2-Wasserstein projection estimator.\nWe apply this algorithm to robust stochastic optimization, and, in the process,\nuncover a new method for overcoming the curse of dimensionality in Wasserstein\ndistributionally robust optimization.\n", "link": "http://arxiv.org/abs/2406.06509v1", "date": "2024-06-10", "relevancy": 2.4024, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4916}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Distribution%20Learning%20with%20Local%20and%20Global%20Adversarial%0A%20%20Corruptions&body=Title%3A%20Robust%20Distribution%20Learning%20with%20Local%20and%20Global%20Adversarial%0A%20%20Corruptions%0AAuthor%3A%20Sloan%20Nietert%20and%20Ziv%20Goldfeld%20and%20Soroosh%20Shafiee%0AAbstract%3A%20%20%20We%20consider%20learning%20in%20an%20adversarial%20environment%2C%20where%20an%0A%24%5Cvarepsilon%24-fraction%20of%20samples%20from%20a%20distribution%20%24P%24%20are%20arbitrarily%0Amodified%20%28%2Aglobal%2A%20corruptions%29%20and%20the%20remaining%20perturbations%20have%20average%0Amagnitude%20bounded%20by%20%24%5Crho%24%20%28%2Alocal%2A%20corruptions%29.%20Given%20access%20to%20%24n%24%20such%0Acorrupted%20samples%2C%20we%20seek%20a%20computationally%20efficient%20estimator%20%24%5Chat%7BP%7D_n%24%0Athat%20minimizes%20the%20Wasserstein%20distance%20%24%5Cmathsf%7BW%7D_1%28%5Chat%7BP%7D_n%2CP%29%24.%20In%20fact%2C%0Awe%20attack%20the%20fine-grained%20task%20of%20minimizing%20%24%5Cmathsf%7BW%7D_1%28%5CPi_%5C%23%20%5Chat%7BP%7D_n%2C%0A%5CPi_%5C%23%20P%29%24%20for%20all%20orthogonal%20projections%20%24%5CPi%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20d%7D%24%2C%0Awith%20performance%20scaling%20with%20%24%5Cmathrm%7Brank%7D%28%5CPi%29%20%3D%20k%24.%20This%20allows%20us%20to%0Aaccount%20simultaneously%20for%20mean%20estimation%20%28%24k%3D1%24%29%2C%20distribution%20estimation%0A%28%24k%3Dd%24%29%2C%20as%20well%20as%20the%20settings%20interpolating%20between%20these%20two%20extremes.%20We%0Acharacterize%20the%20optimal%20population-limit%20risk%20for%20this%20task%20and%20then%20develop%0Aan%20efficient%20finite-sample%20algorithm%20with%20error%20bounded%20by%20%24%5Csqrt%7B%5Cvarepsilon%0Ak%7D%20%2B%20%5Crho%20%2B%20d%5E%7BO%281%29%7D%5Ctilde%7BO%7D%28n%5E%7B-1/k%7D%29%24%20when%20%24P%24%20has%20bounded%20moments%20of%20order%0A%242%2B%5Cdelta%24%2C%20for%20constant%20%24%5Cdelta%20%3E%200%24.%20For%20data%20distributions%20with%20bounded%0Acovariance%2C%20our%20finite-sample%20bounds%20match%20the%20minimax%20population-level%20optimum%0Afor%20large%20sample%20sizes.%20Our%20efficient%20procedure%20relies%20on%20a%20novel%20trace%20norm%0Aapproximation%20of%20an%20ideal%20yet%20intractable%202-Wasserstein%20projection%20estimator.%0AWe%20apply%20this%20algorithm%20to%20robust%20stochastic%20optimization%2C%20and%2C%20in%20the%20process%2C%0Auncover%20a%20new%20method%20for%20overcoming%20the%20curse%20of%20dimensionality%20in%20Wasserstein%0Adistributionally%20robust%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Distribution%2520Learning%2520with%2520Local%2520and%2520Global%2520Adversarial%250A%2520%2520Corruptions%26entry.906535625%3DSloan%2520Nietert%2520and%2520Ziv%2520Goldfeld%2520and%2520Soroosh%2520Shafiee%26entry.1292438233%3D%2520%2520We%2520consider%2520learning%2520in%2520an%2520adversarial%2520environment%252C%2520where%2520an%250A%2524%255Cvarepsilon%2524-fraction%2520of%2520samples%2520from%2520a%2520distribution%2520%2524P%2524%2520are%2520arbitrarily%250Amodified%2520%2528%252Aglobal%252A%2520corruptions%2529%2520and%2520the%2520remaining%2520perturbations%2520have%2520average%250Amagnitude%2520bounded%2520by%2520%2524%255Crho%2524%2520%2528%252Alocal%252A%2520corruptions%2529.%2520Given%2520access%2520to%2520%2524n%2524%2520such%250Acorrupted%2520samples%252C%2520we%2520seek%2520a%2520computationally%2520efficient%2520estimator%2520%2524%255Chat%257BP%257D_n%2524%250Athat%2520minimizes%2520the%2520Wasserstein%2520distance%2520%2524%255Cmathsf%257BW%257D_1%2528%255Chat%257BP%257D_n%252CP%2529%2524.%2520In%2520fact%252C%250Awe%2520attack%2520the%2520fine-grained%2520task%2520of%2520minimizing%2520%2524%255Cmathsf%257BW%257D_1%2528%255CPi_%255C%2523%2520%255Chat%257BP%257D_n%252C%250A%255CPi_%255C%2523%2520P%2529%2524%2520for%2520all%2520orthogonal%2520projections%2520%2524%255CPi%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bd%2520%255Ctimes%2520d%257D%2524%252C%250Awith%2520performance%2520scaling%2520with%2520%2524%255Cmathrm%257Brank%257D%2528%255CPi%2529%2520%253D%2520k%2524.%2520This%2520allows%2520us%2520to%250Aaccount%2520simultaneously%2520for%2520mean%2520estimation%2520%2528%2524k%253D1%2524%2529%252C%2520distribution%2520estimation%250A%2528%2524k%253Dd%2524%2529%252C%2520as%2520well%2520as%2520the%2520settings%2520interpolating%2520between%2520these%2520two%2520extremes.%2520We%250Acharacterize%2520the%2520optimal%2520population-limit%2520risk%2520for%2520this%2520task%2520and%2520then%2520develop%250Aan%2520efficient%2520finite-sample%2520algorithm%2520with%2520error%2520bounded%2520by%2520%2524%255Csqrt%257B%255Cvarepsilon%250Ak%257D%2520%252B%2520%255Crho%2520%252B%2520d%255E%257BO%25281%2529%257D%255Ctilde%257BO%257D%2528n%255E%257B-1/k%257D%2529%2524%2520when%2520%2524P%2524%2520has%2520bounded%2520moments%2520of%2520order%250A%25242%252B%255Cdelta%2524%252C%2520for%2520constant%2520%2524%255Cdelta%2520%253E%25200%2524.%2520For%2520data%2520distributions%2520with%2520bounded%250Acovariance%252C%2520our%2520finite-sample%2520bounds%2520match%2520the%2520minimax%2520population-level%2520optimum%250Afor%2520large%2520sample%2520sizes.%2520Our%2520efficient%2520procedure%2520relies%2520on%2520a%2520novel%2520trace%2520norm%250Aapproximation%2520of%2520an%2520ideal%2520yet%2520intractable%25202-Wasserstein%2520projection%2520estimator.%250AWe%2520apply%2520this%2520algorithm%2520to%2520robust%2520stochastic%2520optimization%252C%2520and%252C%2520in%2520the%2520process%252C%250Auncover%2520a%2520new%2520method%2520for%2520overcoming%2520the%2520curse%2520of%2520dimensionality%2520in%2520Wasserstein%250Adistributionally%2520robust%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Distribution%20Learning%20with%20Local%20and%20Global%20Adversarial%0A%20%20Corruptions&entry.906535625=Sloan%20Nietert%20and%20Ziv%20Goldfeld%20and%20Soroosh%20Shafiee&entry.1292438233=%20%20We%20consider%20learning%20in%20an%20adversarial%20environment%2C%20where%20an%0A%24%5Cvarepsilon%24-fraction%20of%20samples%20from%20a%20distribution%20%24P%24%20are%20arbitrarily%0Amodified%20%28%2Aglobal%2A%20corruptions%29%20and%20the%20remaining%20perturbations%20have%20average%0Amagnitude%20bounded%20by%20%24%5Crho%24%20%28%2Alocal%2A%20corruptions%29.%20Given%20access%20to%20%24n%24%20such%0Acorrupted%20samples%2C%20we%20seek%20a%20computationally%20efficient%20estimator%20%24%5Chat%7BP%7D_n%24%0Athat%20minimizes%20the%20Wasserstein%20distance%20%24%5Cmathsf%7BW%7D_1%28%5Chat%7BP%7D_n%2CP%29%24.%20In%20fact%2C%0Awe%20attack%20the%20fine-grained%20task%20of%20minimizing%20%24%5Cmathsf%7BW%7D_1%28%5CPi_%5C%23%20%5Chat%7BP%7D_n%2C%0A%5CPi_%5C%23%20P%29%24%20for%20all%20orthogonal%20projections%20%24%5CPi%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20d%7D%24%2C%0Awith%20performance%20scaling%20with%20%24%5Cmathrm%7Brank%7D%28%5CPi%29%20%3D%20k%24.%20This%20allows%20us%20to%0Aaccount%20simultaneously%20for%20mean%20estimation%20%28%24k%3D1%24%29%2C%20distribution%20estimation%0A%28%24k%3Dd%24%29%2C%20as%20well%20as%20the%20settings%20interpolating%20between%20these%20two%20extremes.%20We%0Acharacterize%20the%20optimal%20population-limit%20risk%20for%20this%20task%20and%20then%20develop%0Aan%20efficient%20finite-sample%20algorithm%20with%20error%20bounded%20by%20%24%5Csqrt%7B%5Cvarepsilon%0Ak%7D%20%2B%20%5Crho%20%2B%20d%5E%7BO%281%29%7D%5Ctilde%7BO%7D%28n%5E%7B-1/k%7D%29%24%20when%20%24P%24%20has%20bounded%20moments%20of%20order%0A%242%2B%5Cdelta%24%2C%20for%20constant%20%24%5Cdelta%20%3E%200%24.%20For%20data%20distributions%20with%20bounded%0Acovariance%2C%20our%20finite-sample%20bounds%20match%20the%20minimax%20population-level%20optimum%0Afor%20large%20sample%20sizes.%20Our%20efficient%20procedure%20relies%20on%20a%20novel%20trace%20norm%0Aapproximation%20of%20an%20ideal%20yet%20intractable%202-Wasserstein%20projection%20estimator.%0AWe%20apply%20this%20algorithm%20to%20robust%20stochastic%20optimization%2C%20and%2C%20in%20the%20process%2C%0Auncover%20a%20new%20method%20for%20overcoming%20the%20curse%20of%20dimensionality%20in%20Wasserstein%0Adistributionally%20robust%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06509v1&entry.124074799=Read"},
{"title": "GKAN: Graph Kolmogorov-Arnold Networks", "author": "Mehrdad Kiamari and Mohammad Kiamari and Bhaskar Krishnamachari", "abstract": "  We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural\nnetwork architecture that extends the principles of the recently proposed\nKolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the\nunique characteristics of KANs, notably the use of learnable univariate\nfunctions instead of fixed linear weights, we develop a powerful model for\ngraph-based learning tasks. Unlike traditional Graph Convolutional Networks\n(GCNs) that rely on a fixed convolutional architecture, GKANs implement\nlearnable spline-based functions between layers, transforming the way\ninformation is processed across the graph structure. We present two different\nways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable\nfunctions are applied to input features after aggregation and architecture 2 --\nwhere the learnable functions are applied to input features before aggregation.\nWe evaluate GKAN empirically using a semi-supervised graph learning task on a\nreal-world dataset (Cora). We find that architecture generally performs better.\nWe find that GKANs achieve higher accuracy in semi-supervised learning tasks on\ngraphs compared to the traditional GCN model. For example, when considering 100\nfeatures, GCN provides an accuracy of 53.5 while a GKAN with a comparable\nnumber of parameters gives an accuracy of 61.76; with 200 features, GCN\nprovides an accuracy of 61.24 while a GKAN with a comparable number of\nparameters gives an accuracy of 67.66. We also present results on the impact of\nvarious parameters such as the number of hidden nodes, grid-size, and the\npolynomial-degree of the spline on the performance of GKAN.\n", "link": "http://arxiv.org/abs/2406.06470v1", "date": "2024-06-10", "relevancy": 2.394, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4722}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GKAN%3A%20Graph%20Kolmogorov-Arnold%20Networks&body=Title%3A%20GKAN%3A%20Graph%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Mehrdad%20Kiamari%20and%20Mohammad%20Kiamari%20and%20Bhaskar%20Krishnamachari%0AAbstract%3A%20%20%20We%20introduce%20Graph%20Kolmogorov-Arnold%20Networks%20%28GKAN%29%2C%20an%20innovative%20neural%0Anetwork%20architecture%20that%20extends%20the%20principles%20of%20the%20recently%20proposed%0AKolmogorov-Arnold%20Networks%20%28KAN%29%20to%20graph-structured%20data.%20By%20adopting%20the%0Aunique%20characteristics%20of%20KANs%2C%20notably%20the%20use%20of%20learnable%20univariate%0Afunctions%20instead%20of%20fixed%20linear%20weights%2C%20we%20develop%20a%20powerful%20model%20for%0Agraph-based%20learning%20tasks.%20Unlike%20traditional%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20that%20rely%20on%20a%20fixed%20convolutional%20architecture%2C%20GKANs%20implement%0Alearnable%20spline-based%20functions%20between%20layers%2C%20transforming%20the%20way%0Ainformation%20is%20processed%20across%20the%20graph%20structure.%20We%20present%20two%20different%0Aways%20to%20incorporate%20KAN%20layers%20into%20GKAN%3A%20architecture%201%20--%20where%20the%20learnable%0Afunctions%20are%20applied%20to%20input%20features%20after%20aggregation%20and%20architecture%202%20--%0Awhere%20the%20learnable%20functions%20are%20applied%20to%20input%20features%20before%20aggregation.%0AWe%20evaluate%20GKAN%20empirically%20using%20a%20semi-supervised%20graph%20learning%20task%20on%20a%0Areal-world%20dataset%20%28Cora%29.%20We%20find%20that%20architecture%20generally%20performs%20better.%0AWe%20find%20that%20GKANs%20achieve%20higher%20accuracy%20in%20semi-supervised%20learning%20tasks%20on%0Agraphs%20compared%20to%20the%20traditional%20GCN%20model.%20For%20example%2C%20when%20considering%20100%0Afeatures%2C%20GCN%20provides%20an%20accuracy%20of%2053.5%20while%20a%20GKAN%20with%20a%20comparable%0Anumber%20of%20parameters%20gives%20an%20accuracy%20of%2061.76%3B%20with%20200%20features%2C%20GCN%0Aprovides%20an%20accuracy%20of%2061.24%20while%20a%20GKAN%20with%20a%20comparable%20number%20of%0Aparameters%20gives%20an%20accuracy%20of%2067.66.%20We%20also%20present%20results%20on%20the%20impact%20of%0Avarious%20parameters%20such%20as%20the%20number%20of%20hidden%20nodes%2C%20grid-size%2C%20and%20the%0Apolynomial-degree%20of%20the%20spline%20on%20the%20performance%20of%20GKAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGKAN%253A%2520Graph%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DMehrdad%2520Kiamari%2520and%2520Mohammad%2520Kiamari%2520and%2520Bhaskar%2520Krishnamachari%26entry.1292438233%3D%2520%2520We%2520introduce%2520Graph%2520Kolmogorov-Arnold%2520Networks%2520%2528GKAN%2529%252C%2520an%2520innovative%2520neural%250Anetwork%2520architecture%2520that%2520extends%2520the%2520principles%2520of%2520the%2520recently%2520proposed%250AKolmogorov-Arnold%2520Networks%2520%2528KAN%2529%2520to%2520graph-structured%2520data.%2520By%2520adopting%2520the%250Aunique%2520characteristics%2520of%2520KANs%252C%2520notably%2520the%2520use%2520of%2520learnable%2520univariate%250Afunctions%2520instead%2520of%2520fixed%2520linear%2520weights%252C%2520we%2520develop%2520a%2520powerful%2520model%2520for%250Agraph-based%2520learning%2520tasks.%2520Unlike%2520traditional%2520Graph%2520Convolutional%2520Networks%250A%2528GCNs%2529%2520that%2520rely%2520on%2520a%2520fixed%2520convolutional%2520architecture%252C%2520GKANs%2520implement%250Alearnable%2520spline-based%2520functions%2520between%2520layers%252C%2520transforming%2520the%2520way%250Ainformation%2520is%2520processed%2520across%2520the%2520graph%2520structure.%2520We%2520present%2520two%2520different%250Aways%2520to%2520incorporate%2520KAN%2520layers%2520into%2520GKAN%253A%2520architecture%25201%2520--%2520where%2520the%2520learnable%250Afunctions%2520are%2520applied%2520to%2520input%2520features%2520after%2520aggregation%2520and%2520architecture%25202%2520--%250Awhere%2520the%2520learnable%2520functions%2520are%2520applied%2520to%2520input%2520features%2520before%2520aggregation.%250AWe%2520evaluate%2520GKAN%2520empirically%2520using%2520a%2520semi-supervised%2520graph%2520learning%2520task%2520on%2520a%250Areal-world%2520dataset%2520%2528Cora%2529.%2520We%2520find%2520that%2520architecture%2520generally%2520performs%2520better.%250AWe%2520find%2520that%2520GKANs%2520achieve%2520higher%2520accuracy%2520in%2520semi-supervised%2520learning%2520tasks%2520on%250Agraphs%2520compared%2520to%2520the%2520traditional%2520GCN%2520model.%2520For%2520example%252C%2520when%2520considering%2520100%250Afeatures%252C%2520GCN%2520provides%2520an%2520accuracy%2520of%252053.5%2520while%2520a%2520GKAN%2520with%2520a%2520comparable%250Anumber%2520of%2520parameters%2520gives%2520an%2520accuracy%2520of%252061.76%253B%2520with%2520200%2520features%252C%2520GCN%250Aprovides%2520an%2520accuracy%2520of%252061.24%2520while%2520a%2520GKAN%2520with%2520a%2520comparable%2520number%2520of%250Aparameters%2520gives%2520an%2520accuracy%2520of%252067.66.%2520We%2520also%2520present%2520results%2520on%2520the%2520impact%2520of%250Avarious%2520parameters%2520such%2520as%2520the%2520number%2520of%2520hidden%2520nodes%252C%2520grid-size%252C%2520and%2520the%250Apolynomial-degree%2520of%2520the%2520spline%2520on%2520the%2520performance%2520of%2520GKAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GKAN%3A%20Graph%20Kolmogorov-Arnold%20Networks&entry.906535625=Mehrdad%20Kiamari%20and%20Mohammad%20Kiamari%20and%20Bhaskar%20Krishnamachari&entry.1292438233=%20%20We%20introduce%20Graph%20Kolmogorov-Arnold%20Networks%20%28GKAN%29%2C%20an%20innovative%20neural%0Anetwork%20architecture%20that%20extends%20the%20principles%20of%20the%20recently%20proposed%0AKolmogorov-Arnold%20Networks%20%28KAN%29%20to%20graph-structured%20data.%20By%20adopting%20the%0Aunique%20characteristics%20of%20KANs%2C%20notably%20the%20use%20of%20learnable%20univariate%0Afunctions%20instead%20of%20fixed%20linear%20weights%2C%20we%20develop%20a%20powerful%20model%20for%0Agraph-based%20learning%20tasks.%20Unlike%20traditional%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20that%20rely%20on%20a%20fixed%20convolutional%20architecture%2C%20GKANs%20implement%0Alearnable%20spline-based%20functions%20between%20layers%2C%20transforming%20the%20way%0Ainformation%20is%20processed%20across%20the%20graph%20structure.%20We%20present%20two%20different%0Aways%20to%20incorporate%20KAN%20layers%20into%20GKAN%3A%20architecture%201%20--%20where%20the%20learnable%0Afunctions%20are%20applied%20to%20input%20features%20after%20aggregation%20and%20architecture%202%20--%0Awhere%20the%20learnable%20functions%20are%20applied%20to%20input%20features%20before%20aggregation.%0AWe%20evaluate%20GKAN%20empirically%20using%20a%20semi-supervised%20graph%20learning%20task%20on%20a%0Areal-world%20dataset%20%28Cora%29.%20We%20find%20that%20architecture%20generally%20performs%20better.%0AWe%20find%20that%20GKANs%20achieve%20higher%20accuracy%20in%20semi-supervised%20learning%20tasks%20on%0Agraphs%20compared%20to%20the%20traditional%20GCN%20model.%20For%20example%2C%20when%20considering%20100%0Afeatures%2C%20GCN%20provides%20an%20accuracy%20of%2053.5%20while%20a%20GKAN%20with%20a%20comparable%0Anumber%20of%20parameters%20gives%20an%20accuracy%20of%2061.76%3B%20with%20200%20features%2C%20GCN%0Aprovides%20an%20accuracy%20of%2061.24%20while%20a%20GKAN%20with%20a%20comparable%20number%20of%0Aparameters%20gives%20an%20accuracy%20of%2067.66.%20We%20also%20present%20results%20on%20the%20impact%20of%0Avarious%20parameters%20such%20as%20the%20number%20of%20hidden%20nodes%2C%20grid-size%2C%20and%20the%0Apolynomial-degree%20of%20the%20spline%20on%20the%20performance%20of%20GKAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06470v1&entry.124074799=Read"},
{"title": "Active Learning with Simple Questions", "author": "Vasilis Kontonis and Mingchen Ma and Christos Tzamos", "abstract": "  We consider an active learning setting where a learner is presented with a\npool S of n unlabeled examples belonging to a domain X and asks queries to find\nthe underlying labeling that agrees with a target concept h^* \\in H.\n  In contrast to traditional active learning that queries a single example for\nits label, we study more general region queries that allow the learner to pick\na subset of the domain T \\subset X and a target label y and ask a labeler\nwhether h^*(x) = y for every example in the set T \\cap S.\n  Such more powerful queries allow us to bypass the limitations of traditional\nactive learning and use significantly fewer rounds of interactions to learn but\ncan potentially lead to a significantly more complex query language. Our main\ncontribution is quantifying the trade-off between the number of queries and the\ncomplexity of the query language used by the learner.\n  We measure the complexity of the region queries via the VC dimension of the\nfamily of regions. We show that given any hypothesis class H with VC dimension\nd, one can design a region query family Q with VC dimension O(d) such that for\nevery set of n examples S \\subset X and every h^* \\in H, a learner can submit\nO(d log n) queries from Q to a labeler and perfectly label S. We show a\nmatching lower bound by designing a hypothesis class H with VC dimension d and\na dataset S \\subset X of size n such that any learning algorithm using any\nquery class with VC dimension less than O(d) must make poly(n) queries to label\nS perfectly.\n  Finally, we focus on well-studied hypothesis classes including unions of\nintervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain\nstronger results. In particular, we design learning algorithms that (i) are\ncomputationally efficient and (ii) work even when the queries are not answered\nbased on the learner's pool of examples S but on some unknown superset L of S\n", "link": "http://arxiv.org/abs/2405.07937v2", "date": "2024-06-10", "relevancy": 2.3923, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4923}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20with%20Simple%20Questions&body=Title%3A%20Active%20Learning%20with%20Simple%20Questions%0AAuthor%3A%20Vasilis%20Kontonis%20and%20Mingchen%20Ma%20and%20Christos%20Tzamos%0AAbstract%3A%20%20%20We%20consider%20an%20active%20learning%20setting%20where%20a%20learner%20is%20presented%20with%20a%0Apool%20S%20of%20n%20unlabeled%20examples%20belonging%20to%20a%20domain%20X%20and%20asks%20queries%20to%20find%0Athe%20underlying%20labeling%20that%20agrees%20with%20a%20target%20concept%20h%5E%2A%20%5Cin%20H.%0A%20%20In%20contrast%20to%20traditional%20active%20learning%20that%20queries%20a%20single%20example%20for%0Aits%20label%2C%20we%20study%20more%20general%20region%20queries%20that%20allow%20the%20learner%20to%20pick%0Aa%20subset%20of%20the%20domain%20T%20%5Csubset%20X%20and%20a%20target%20label%20y%20and%20ask%20a%20labeler%0Awhether%20h%5E%2A%28x%29%20%3D%20y%20for%20every%20example%20in%20the%20set%20T%20%5Ccap%20S.%0A%20%20Such%20more%20powerful%20queries%20allow%20us%20to%20bypass%20the%20limitations%20of%20traditional%0Aactive%20learning%20and%20use%20significantly%20fewer%20rounds%20of%20interactions%20to%20learn%20but%0Acan%20potentially%20lead%20to%20a%20significantly%20more%20complex%20query%20language.%20Our%20main%0Acontribution%20is%20quantifying%20the%20trade-off%20between%20the%20number%20of%20queries%20and%20the%0Acomplexity%20of%20the%20query%20language%20used%20by%20the%20learner.%0A%20%20We%20measure%20the%20complexity%20of%20the%20region%20queries%20via%20the%20VC%20dimension%20of%20the%0Afamily%20of%20regions.%20We%20show%20that%20given%20any%20hypothesis%20class%20H%20with%20VC%20dimension%0Ad%2C%20one%20can%20design%20a%20region%20query%20family%20Q%20with%20VC%20dimension%20O%28d%29%20such%20that%20for%0Aevery%20set%20of%20n%20examples%20S%20%5Csubset%20X%20and%20every%20h%5E%2A%20%5Cin%20H%2C%20a%20learner%20can%20submit%0AO%28d%20log%20n%29%20queries%20from%20Q%20to%20a%20labeler%20and%20perfectly%20label%20S.%20We%20show%20a%0Amatching%20lower%20bound%20by%20designing%20a%20hypothesis%20class%20H%20with%20VC%20dimension%20d%20and%0Aa%20dataset%20S%20%5Csubset%20X%20of%20size%20n%20such%20that%20any%20learning%20algorithm%20using%20any%0Aquery%20class%20with%20VC%20dimension%20less%20than%20O%28d%29%20must%20make%20poly%28n%29%20queries%20to%20label%0AS%20perfectly.%0A%20%20Finally%2C%20we%20focus%20on%20well-studied%20hypothesis%20classes%20including%20unions%20of%0Aintervals%2C%20high-dimensional%20boxes%2C%20and%20d-dimensional%20halfspaces%2C%20and%20obtain%0Astronger%20results.%20In%20particular%2C%20we%20design%20learning%20algorithms%20that%20%28i%29%20are%0Acomputationally%20efficient%20and%20%28ii%29%20work%20even%20when%20the%20queries%20are%20not%20answered%0Abased%20on%20the%20learner%27s%20pool%20of%20examples%20S%20but%20on%20some%20unknown%20superset%20L%20of%20S%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520with%2520Simple%2520Questions%26entry.906535625%3DVasilis%2520Kontonis%2520and%2520Mingchen%2520Ma%2520and%2520Christos%2520Tzamos%26entry.1292438233%3D%2520%2520We%2520consider%2520an%2520active%2520learning%2520setting%2520where%2520a%2520learner%2520is%2520presented%2520with%2520a%250Apool%2520S%2520of%2520n%2520unlabeled%2520examples%2520belonging%2520to%2520a%2520domain%2520X%2520and%2520asks%2520queries%2520to%2520find%250Athe%2520underlying%2520labeling%2520that%2520agrees%2520with%2520a%2520target%2520concept%2520h%255E%252A%2520%255Cin%2520H.%250A%2520%2520In%2520contrast%2520to%2520traditional%2520active%2520learning%2520that%2520queries%2520a%2520single%2520example%2520for%250Aits%2520label%252C%2520we%2520study%2520more%2520general%2520region%2520queries%2520that%2520allow%2520the%2520learner%2520to%2520pick%250Aa%2520subset%2520of%2520the%2520domain%2520T%2520%255Csubset%2520X%2520and%2520a%2520target%2520label%2520y%2520and%2520ask%2520a%2520labeler%250Awhether%2520h%255E%252A%2528x%2529%2520%253D%2520y%2520for%2520every%2520example%2520in%2520the%2520set%2520T%2520%255Ccap%2520S.%250A%2520%2520Such%2520more%2520powerful%2520queries%2520allow%2520us%2520to%2520bypass%2520the%2520limitations%2520of%2520traditional%250Aactive%2520learning%2520and%2520use%2520significantly%2520fewer%2520rounds%2520of%2520interactions%2520to%2520learn%2520but%250Acan%2520potentially%2520lead%2520to%2520a%2520significantly%2520more%2520complex%2520query%2520language.%2520Our%2520main%250Acontribution%2520is%2520quantifying%2520the%2520trade-off%2520between%2520the%2520number%2520of%2520queries%2520and%2520the%250Acomplexity%2520of%2520the%2520query%2520language%2520used%2520by%2520the%2520learner.%250A%2520%2520We%2520measure%2520the%2520complexity%2520of%2520the%2520region%2520queries%2520via%2520the%2520VC%2520dimension%2520of%2520the%250Afamily%2520of%2520regions.%2520We%2520show%2520that%2520given%2520any%2520hypothesis%2520class%2520H%2520with%2520VC%2520dimension%250Ad%252C%2520one%2520can%2520design%2520a%2520region%2520query%2520family%2520Q%2520with%2520VC%2520dimension%2520O%2528d%2529%2520such%2520that%2520for%250Aevery%2520set%2520of%2520n%2520examples%2520S%2520%255Csubset%2520X%2520and%2520every%2520h%255E%252A%2520%255Cin%2520H%252C%2520a%2520learner%2520can%2520submit%250AO%2528d%2520log%2520n%2529%2520queries%2520from%2520Q%2520to%2520a%2520labeler%2520and%2520perfectly%2520label%2520S.%2520We%2520show%2520a%250Amatching%2520lower%2520bound%2520by%2520designing%2520a%2520hypothesis%2520class%2520H%2520with%2520VC%2520dimension%2520d%2520and%250Aa%2520dataset%2520S%2520%255Csubset%2520X%2520of%2520size%2520n%2520such%2520that%2520any%2520learning%2520algorithm%2520using%2520any%250Aquery%2520class%2520with%2520VC%2520dimension%2520less%2520than%2520O%2528d%2529%2520must%2520make%2520poly%2528n%2529%2520queries%2520to%2520label%250AS%2520perfectly.%250A%2520%2520Finally%252C%2520we%2520focus%2520on%2520well-studied%2520hypothesis%2520classes%2520including%2520unions%2520of%250Aintervals%252C%2520high-dimensional%2520boxes%252C%2520and%2520d-dimensional%2520halfspaces%252C%2520and%2520obtain%250Astronger%2520results.%2520In%2520particular%252C%2520we%2520design%2520learning%2520algorithms%2520that%2520%2528i%2529%2520are%250Acomputationally%2520efficient%2520and%2520%2528ii%2529%2520work%2520even%2520when%2520the%2520queries%2520are%2520not%2520answered%250Abased%2520on%2520the%2520learner%2527s%2520pool%2520of%2520examples%2520S%2520but%2520on%2520some%2520unknown%2520superset%2520L%2520of%2520S%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20with%20Simple%20Questions&entry.906535625=Vasilis%20Kontonis%20and%20Mingchen%20Ma%20and%20Christos%20Tzamos&entry.1292438233=%20%20We%20consider%20an%20active%20learning%20setting%20where%20a%20learner%20is%20presented%20with%20a%0Apool%20S%20of%20n%20unlabeled%20examples%20belonging%20to%20a%20domain%20X%20and%20asks%20queries%20to%20find%0Athe%20underlying%20labeling%20that%20agrees%20with%20a%20target%20concept%20h%5E%2A%20%5Cin%20H.%0A%20%20In%20contrast%20to%20traditional%20active%20learning%20that%20queries%20a%20single%20example%20for%0Aits%20label%2C%20we%20study%20more%20general%20region%20queries%20that%20allow%20the%20learner%20to%20pick%0Aa%20subset%20of%20the%20domain%20T%20%5Csubset%20X%20and%20a%20target%20label%20y%20and%20ask%20a%20labeler%0Awhether%20h%5E%2A%28x%29%20%3D%20y%20for%20every%20example%20in%20the%20set%20T%20%5Ccap%20S.%0A%20%20Such%20more%20powerful%20queries%20allow%20us%20to%20bypass%20the%20limitations%20of%20traditional%0Aactive%20learning%20and%20use%20significantly%20fewer%20rounds%20of%20interactions%20to%20learn%20but%0Acan%20potentially%20lead%20to%20a%20significantly%20more%20complex%20query%20language.%20Our%20main%0Acontribution%20is%20quantifying%20the%20trade-off%20between%20the%20number%20of%20queries%20and%20the%0Acomplexity%20of%20the%20query%20language%20used%20by%20the%20learner.%0A%20%20We%20measure%20the%20complexity%20of%20the%20region%20queries%20via%20the%20VC%20dimension%20of%20the%0Afamily%20of%20regions.%20We%20show%20that%20given%20any%20hypothesis%20class%20H%20with%20VC%20dimension%0Ad%2C%20one%20can%20design%20a%20region%20query%20family%20Q%20with%20VC%20dimension%20O%28d%29%20such%20that%20for%0Aevery%20set%20of%20n%20examples%20S%20%5Csubset%20X%20and%20every%20h%5E%2A%20%5Cin%20H%2C%20a%20learner%20can%20submit%0AO%28d%20log%20n%29%20queries%20from%20Q%20to%20a%20labeler%20and%20perfectly%20label%20S.%20We%20show%20a%0Amatching%20lower%20bound%20by%20designing%20a%20hypothesis%20class%20H%20with%20VC%20dimension%20d%20and%0Aa%20dataset%20S%20%5Csubset%20X%20of%20size%20n%20such%20that%20any%20learning%20algorithm%20using%20any%0Aquery%20class%20with%20VC%20dimension%20less%20than%20O%28d%29%20must%20make%20poly%28n%29%20queries%20to%20label%0AS%20perfectly.%0A%20%20Finally%2C%20we%20focus%20on%20well-studied%20hypothesis%20classes%20including%20unions%20of%0Aintervals%2C%20high-dimensional%20boxes%2C%20and%20d-dimensional%20halfspaces%2C%20and%20obtain%0Astronger%20results.%20In%20particular%2C%20we%20design%20learning%20algorithms%20that%20%28i%29%20are%0Acomputationally%20efficient%20and%20%28ii%29%20work%20even%20when%20the%20queries%20are%20not%20answered%0Abased%20on%20the%20learner%27s%20pool%20of%20examples%20S%20but%20on%20some%20unknown%20superset%20L%20of%20S%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07937v2&entry.124074799=Read"},
{"title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion\n  Prior for Video Editing", "author": "Ting-Hsuan Chen and Jiewen Chan and Hau-Shiang Shiu and Shih-Han Yen and Chang-Han Yeh and Yu-Lun Liu", "abstract": "  We propose a video editing framework, NaRCan, which integrates a hybrid\ndeformation field and diffusion prior to generate high-quality natural\ncanonical images to represent the input video. Our approach utilizes homography\nto model global motion and employs multi-layer perceptrons (MLPs) to capture\nlocal residual deformations, enhancing the model's ability to handle complex\nvideo dynamics. By introducing a diffusion prior from the early stages of\ntraining, our model ensures that the generated images retain a high-quality\nnatural appearance, making the produced canonical images suitable for various\ndownstream tasks in video editing, a capability not achieved by current\ncanonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA)\nfine-tuning and introduce a noise and diffusion prior update scheduling\ntechnique that accelerates the training process by 14 times. Extensive\nexperimental results show that our method outperforms existing approaches in\nvarious video editing tasks and produces coherent and high-quality edited video\nsequences. See our project page for video results at\nhttps://koi953215.github.io/NaRCan_page/.\n", "link": "http://arxiv.org/abs/2406.06523v1", "date": "2024-06-10", "relevancy": 2.3806, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6289}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6066}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing&body=Title%3A%20NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing%0AAuthor%3A%20Ting-Hsuan%20Chen%20and%20Jiewen%20Chan%20and%20Hau-Shiang%20Shiu%20and%20Shih-Han%20Yen%20and%20Chang-Han%20Yeh%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20We%20propose%20a%20video%20editing%20framework%2C%20NaRCan%2C%20which%20integrates%20a%20hybrid%0Adeformation%20field%20and%20diffusion%20prior%20to%20generate%20high-quality%20natural%0Acanonical%20images%20to%20represent%20the%20input%20video.%20Our%20approach%20utilizes%20homography%0Ato%20model%20global%20motion%20and%20employs%20multi-layer%20perceptrons%20%28MLPs%29%20to%20capture%0Alocal%20residual%20deformations%2C%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%0Avideo%20dynamics.%20By%20introducing%20a%20diffusion%20prior%20from%20the%20early%20stages%20of%0Atraining%2C%20our%20model%20ensures%20that%20the%20generated%20images%20retain%20a%20high-quality%0Anatural%20appearance%2C%20making%20the%20produced%20canonical%20images%20suitable%20for%20various%0Adownstream%20tasks%20in%20video%20editing%2C%20a%20capability%20not%20achieved%20by%20current%0Acanonical-based%20methods.%20Furthermore%2C%20we%20incorporate%20low-rank%20adaptation%20%28LoRA%29%0Afine-tuning%20and%20introduce%20a%20noise%20and%20diffusion%20prior%20update%20scheduling%0Atechnique%20that%20accelerates%20the%20training%20process%20by%2014%20times.%20Extensive%0Aexperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Avarious%20video%20editing%20tasks%20and%20produces%20coherent%20and%20high-quality%20edited%20video%0Asequences.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//koi953215.github.io/NaRCan_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaRCan%253A%2520Natural%2520Refined%2520Canonical%2520Image%2520with%2520Integration%2520of%2520Diffusion%250A%2520%2520Prior%2520for%2520Video%2520Editing%26entry.906535625%3DTing-Hsuan%2520Chen%2520and%2520Jiewen%2520Chan%2520and%2520Hau-Shiang%2520Shiu%2520and%2520Shih-Han%2520Yen%2520and%2520Chang-Han%2520Yeh%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520video%2520editing%2520framework%252C%2520NaRCan%252C%2520which%2520integrates%2520a%2520hybrid%250Adeformation%2520field%2520and%2520diffusion%2520prior%2520to%2520generate%2520high-quality%2520natural%250Acanonical%2520images%2520to%2520represent%2520the%2520input%2520video.%2520Our%2520approach%2520utilizes%2520homography%250Ato%2520model%2520global%2520motion%2520and%2520employs%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520to%2520capture%250Alocal%2520residual%2520deformations%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520handle%2520complex%250Avideo%2520dynamics.%2520By%2520introducing%2520a%2520diffusion%2520prior%2520from%2520the%2520early%2520stages%2520of%250Atraining%252C%2520our%2520model%2520ensures%2520that%2520the%2520generated%2520images%2520retain%2520a%2520high-quality%250Anatural%2520appearance%252C%2520making%2520the%2520produced%2520canonical%2520images%2520suitable%2520for%2520various%250Adownstream%2520tasks%2520in%2520video%2520editing%252C%2520a%2520capability%2520not%2520achieved%2520by%2520current%250Acanonical-based%2520methods.%2520Furthermore%252C%2520we%2520incorporate%2520low-rank%2520adaptation%2520%2528LoRA%2529%250Afine-tuning%2520and%2520introduce%2520a%2520noise%2520and%2520diffusion%2520prior%2520update%2520scheduling%250Atechnique%2520that%2520accelerates%2520the%2520training%2520process%2520by%252014%2520times.%2520Extensive%250Aexperimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520in%250Avarious%2520video%2520editing%2520tasks%2520and%2520produces%2520coherent%2520and%2520high-quality%2520edited%2520video%250Asequences.%2520See%2520our%2520project%2520page%2520for%2520video%2520results%2520at%250Ahttps%253A//koi953215.github.io/NaRCan_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing&entry.906535625=Ting-Hsuan%20Chen%20and%20Jiewen%20Chan%20and%20Hau-Shiang%20Shiu%20and%20Shih-Han%20Yen%20and%20Chang-Han%20Yeh%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20We%20propose%20a%20video%20editing%20framework%2C%20NaRCan%2C%20which%20integrates%20a%20hybrid%0Adeformation%20field%20and%20diffusion%20prior%20to%20generate%20high-quality%20natural%0Acanonical%20images%20to%20represent%20the%20input%20video.%20Our%20approach%20utilizes%20homography%0Ato%20model%20global%20motion%20and%20employs%20multi-layer%20perceptrons%20%28MLPs%29%20to%20capture%0Alocal%20residual%20deformations%2C%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%0Avideo%20dynamics.%20By%20introducing%20a%20diffusion%20prior%20from%20the%20early%20stages%20of%0Atraining%2C%20our%20model%20ensures%20that%20the%20generated%20images%20retain%20a%20high-quality%0Anatural%20appearance%2C%20making%20the%20produced%20canonical%20images%20suitable%20for%20various%0Adownstream%20tasks%20in%20video%20editing%2C%20a%20capability%20not%20achieved%20by%20current%0Acanonical-based%20methods.%20Furthermore%2C%20we%20incorporate%20low-rank%20adaptation%20%28LoRA%29%0Afine-tuning%20and%20introduce%20a%20noise%20and%20diffusion%20prior%20update%20scheduling%0Atechnique%20that%20accelerates%20the%20training%20process%20by%2014%20times.%20Extensive%0Aexperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Avarious%20video%20editing%20tasks%20and%20produces%20coherent%20and%20high-quality%20edited%20video%0Asequences.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//koi953215.github.io/NaRCan_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06523v1&entry.124074799=Read"},
{"title": "STimage-1K4M: A histopathology image-gene expression dataset for spatial\n  transcriptomics", "author": "Jiawen Chen and Muqing Zhou and Wenrong Wu and Jinwei Zhang and Yun Li and Didong Li", "abstract": "  Recent advances in multi-modal algorithms have driven and been driven by the\nincreasing availability of large image-text datasets, leading to significant\nstrides in various fields, including computational pathology. However, in most\nexisting medical image-text datasets, the text typically provides high-level\nsummaries that may not sufficiently describe sub-tile regions within a large\npathology image. For example, an image might cover an extensive tissue area\ncontaining cancerous and healthy regions, but the accompanying text might only\nspecify that this image is a cancer slide, lacking the nuanced details needed\nfor in-depth analysis. In this study, we introduce STimage-1K4M, a novel\ndataset designed to bridge this gap by providing genomic features for sub-tile\nimages. STimage-1K4M contains 1,149 images derived from spatial transcriptomics\ndata, which captures gene expression information at the level of individual\nspatial spots within a pathology image. Specifically, each image in the dataset\nis broken down into smaller sub-image tiles, with each tile paired with\n15,000-30,000 dimensional gene expressions. With 4,293,195 pairs of sub-tile\nimages and gene expressions, STimage-1K4M offers unprecedented granularity,\npaving the way for a wide range of advanced research in multi-modal data\nanalysis an innovative applications in computational pathology, and beyond.\n", "link": "http://arxiv.org/abs/2406.06393v1", "date": "2024-06-10", "relevancy": 2.3435, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4636}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics&body=Title%3A%20STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics%0AAuthor%3A%20Jiawen%20Chen%20and%20Muqing%20Zhou%20and%20Wenrong%20Wu%20and%20Jinwei%20Zhang%20and%20Yun%20Li%20and%20Didong%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20multi-modal%20algorithms%20have%20driven%20and%20been%20driven%20by%20the%0Aincreasing%20availability%20of%20large%20image-text%20datasets%2C%20leading%20to%20significant%0Astrides%20in%20various%20fields%2C%20including%20computational%20pathology.%20However%2C%20in%20most%0Aexisting%20medical%20image-text%20datasets%2C%20the%20text%20typically%20provides%20high-level%0Asummaries%20that%20may%20not%20sufficiently%20describe%20sub-tile%20regions%20within%20a%20large%0Apathology%20image.%20For%20example%2C%20an%20image%20might%20cover%20an%20extensive%20tissue%20area%0Acontaining%20cancerous%20and%20healthy%20regions%2C%20but%20the%20accompanying%20text%20might%20only%0Aspecify%20that%20this%20image%20is%20a%20cancer%20slide%2C%20lacking%20the%20nuanced%20details%20needed%0Afor%20in-depth%20analysis.%20In%20this%20study%2C%20we%20introduce%20STimage-1K4M%2C%20a%20novel%0Adataset%20designed%20to%20bridge%20this%20gap%20by%20providing%20genomic%20features%20for%20sub-tile%0Aimages.%20STimage-1K4M%20contains%201%2C149%20images%20derived%20from%20spatial%20transcriptomics%0Adata%2C%20which%20captures%20gene%20expression%20information%20at%20the%20level%20of%20individual%0Aspatial%20spots%20within%20a%20pathology%20image.%20Specifically%2C%20each%20image%20in%20the%20dataset%0Ais%20broken%20down%20into%20smaller%20sub-image%20tiles%2C%20with%20each%20tile%20paired%20with%0A15%2C000-30%2C000%20dimensional%20gene%20expressions.%20With%204%2C293%2C195%20pairs%20of%20sub-tile%0Aimages%20and%20gene%20expressions%2C%20STimage-1K4M%20offers%20unprecedented%20granularity%2C%0Apaving%20the%20way%20for%20a%20wide%20range%20of%20advanced%20research%20in%20multi-modal%20data%0Aanalysis%20an%20innovative%20applications%20in%20computational%20pathology%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTimage-1K4M%253A%2520A%2520histopathology%2520image-gene%2520expression%2520dataset%2520for%2520spatial%250A%2520%2520transcriptomics%26entry.906535625%3DJiawen%2520Chen%2520and%2520Muqing%2520Zhou%2520and%2520Wenrong%2520Wu%2520and%2520Jinwei%2520Zhang%2520and%2520Yun%2520Li%2520and%2520Didong%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multi-modal%2520algorithms%2520have%2520driven%2520and%2520been%2520driven%2520by%2520the%250Aincreasing%2520availability%2520of%2520large%2520image-text%2520datasets%252C%2520leading%2520to%2520significant%250Astrides%2520in%2520various%2520fields%252C%2520including%2520computational%2520pathology.%2520However%252C%2520in%2520most%250Aexisting%2520medical%2520image-text%2520datasets%252C%2520the%2520text%2520typically%2520provides%2520high-level%250Asummaries%2520that%2520may%2520not%2520sufficiently%2520describe%2520sub-tile%2520regions%2520within%2520a%2520large%250Apathology%2520image.%2520For%2520example%252C%2520an%2520image%2520might%2520cover%2520an%2520extensive%2520tissue%2520area%250Acontaining%2520cancerous%2520and%2520healthy%2520regions%252C%2520but%2520the%2520accompanying%2520text%2520might%2520only%250Aspecify%2520that%2520this%2520image%2520is%2520a%2520cancer%2520slide%252C%2520lacking%2520the%2520nuanced%2520details%2520needed%250Afor%2520in-depth%2520analysis.%2520In%2520this%2520study%252C%2520we%2520introduce%2520STimage-1K4M%252C%2520a%2520novel%250Adataset%2520designed%2520to%2520bridge%2520this%2520gap%2520by%2520providing%2520genomic%2520features%2520for%2520sub-tile%250Aimages.%2520STimage-1K4M%2520contains%25201%252C149%2520images%2520derived%2520from%2520spatial%2520transcriptomics%250Adata%252C%2520which%2520captures%2520gene%2520expression%2520information%2520at%2520the%2520level%2520of%2520individual%250Aspatial%2520spots%2520within%2520a%2520pathology%2520image.%2520Specifically%252C%2520each%2520image%2520in%2520the%2520dataset%250Ais%2520broken%2520down%2520into%2520smaller%2520sub-image%2520tiles%252C%2520with%2520each%2520tile%2520paired%2520with%250A15%252C000-30%252C000%2520dimensional%2520gene%2520expressions.%2520With%25204%252C293%252C195%2520pairs%2520of%2520sub-tile%250Aimages%2520and%2520gene%2520expressions%252C%2520STimage-1K4M%2520offers%2520unprecedented%2520granularity%252C%250Apaving%2520the%2520way%2520for%2520a%2520wide%2520range%2520of%2520advanced%2520research%2520in%2520multi-modal%2520data%250Aanalysis%2520an%2520innovative%2520applications%2520in%2520computational%2520pathology%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics&entry.906535625=Jiawen%20Chen%20and%20Muqing%20Zhou%20and%20Wenrong%20Wu%20and%20Jinwei%20Zhang%20and%20Yun%20Li%20and%20Didong%20Li&entry.1292438233=%20%20Recent%20advances%20in%20multi-modal%20algorithms%20have%20driven%20and%20been%20driven%20by%20the%0Aincreasing%20availability%20of%20large%20image-text%20datasets%2C%20leading%20to%20significant%0Astrides%20in%20various%20fields%2C%20including%20computational%20pathology.%20However%2C%20in%20most%0Aexisting%20medical%20image-text%20datasets%2C%20the%20text%20typically%20provides%20high-level%0Asummaries%20that%20may%20not%20sufficiently%20describe%20sub-tile%20regions%20within%20a%20large%0Apathology%20image.%20For%20example%2C%20an%20image%20might%20cover%20an%20extensive%20tissue%20area%0Acontaining%20cancerous%20and%20healthy%20regions%2C%20but%20the%20accompanying%20text%20might%20only%0Aspecify%20that%20this%20image%20is%20a%20cancer%20slide%2C%20lacking%20the%20nuanced%20details%20needed%0Afor%20in-depth%20analysis.%20In%20this%20study%2C%20we%20introduce%20STimage-1K4M%2C%20a%20novel%0Adataset%20designed%20to%20bridge%20this%20gap%20by%20providing%20genomic%20features%20for%20sub-tile%0Aimages.%20STimage-1K4M%20contains%201%2C149%20images%20derived%20from%20spatial%20transcriptomics%0Adata%2C%20which%20captures%20gene%20expression%20information%20at%20the%20level%20of%20individual%0Aspatial%20spots%20within%20a%20pathology%20image.%20Specifically%2C%20each%20image%20in%20the%20dataset%0Ais%20broken%20down%20into%20smaller%20sub-image%20tiles%2C%20with%20each%20tile%20paired%20with%0A15%2C000-30%2C000%20dimensional%20gene%20expressions.%20With%204%2C293%2C195%20pairs%20of%20sub-tile%0Aimages%20and%20gene%20expressions%2C%20STimage-1K4M%20offers%20unprecedented%20granularity%2C%0Apaving%20the%20way%20for%20a%20wide%20range%20of%20advanced%20research%20in%20multi-modal%20data%0Aanalysis%20an%20innovative%20applications%20in%20computational%20pathology%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06393v1&entry.124074799=Read"},
{"title": "Meta Learning Text-to-Speech Synthesis in over 7000 Languages", "author": "Florian Lux and Sarina Meyer and Lyonel Behringer and Frank Zalkow and Phat Do and Matt Coler and Emanu\u00ebl A. P. Habets and Ngoc Thang Vu", "abstract": "  In this work, we take on the challenging task of building a single\ntext-to-speech synthesis system that is capable of generating speech in over\n7000 languages, many of which lack sufficient data for traditional TTS\ndevelopment. By leveraging a novel integration of massively multilingual\npretraining and meta learning to approximate language representations, our\napproach enables zero-shot speech synthesis in languages without any available\ndata. We validate our system's performance through objective measures and human\nevaluation across a diverse linguistic landscape. By releasing our code and\nmodels publicly, we aim to empower communities with limited linguistic\nresources and foster further innovation in the field of speech technology.\n", "link": "http://arxiv.org/abs/2406.06403v1", "date": "2024-06-10", "relevancy": 2.3359, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4725}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4718}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%20Learning%20Text-to-Speech%20Synthesis%20in%20over%207000%20Languages&body=Title%3A%20Meta%20Learning%20Text-to-Speech%20Synthesis%20in%20over%207000%20Languages%0AAuthor%3A%20Florian%20Lux%20and%20Sarina%20Meyer%20and%20Lyonel%20Behringer%20and%20Frank%20Zalkow%20and%20Phat%20Do%20and%20Matt%20Coler%20and%20Emanu%C3%ABl%20A.%20P.%20Habets%20and%20Ngoc%20Thang%20Vu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20take%20on%20the%20challenging%20task%20of%20building%20a%20single%0Atext-to-speech%20synthesis%20system%20that%20is%20capable%20of%20generating%20speech%20in%20over%0A7000%20languages%2C%20many%20of%20which%20lack%20sufficient%20data%20for%20traditional%20TTS%0Adevelopment.%20By%20leveraging%20a%20novel%20integration%20of%20massively%20multilingual%0Apretraining%20and%20meta%20learning%20to%20approximate%20language%20representations%2C%20our%0Aapproach%20enables%20zero-shot%20speech%20synthesis%20in%20languages%20without%20any%20available%0Adata.%20We%20validate%20our%20system%27s%20performance%20through%20objective%20measures%20and%20human%0Aevaluation%20across%20a%20diverse%20linguistic%20landscape.%20By%20releasing%20our%20code%20and%0Amodels%20publicly%2C%20we%20aim%20to%20empower%20communities%20with%20limited%20linguistic%0Aresources%20and%20foster%20further%20innovation%20in%20the%20field%20of%20speech%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%2520Learning%2520Text-to-Speech%2520Synthesis%2520in%2520over%25207000%2520Languages%26entry.906535625%3DFlorian%2520Lux%2520and%2520Sarina%2520Meyer%2520and%2520Lyonel%2520Behringer%2520and%2520Frank%2520Zalkow%2520and%2520Phat%2520Do%2520and%2520Matt%2520Coler%2520and%2520Emanu%25C3%25ABl%2520A.%2520P.%2520Habets%2520and%2520Ngoc%2520Thang%2520Vu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520take%2520on%2520the%2520challenging%2520task%2520of%2520building%2520a%2520single%250Atext-to-speech%2520synthesis%2520system%2520that%2520is%2520capable%2520of%2520generating%2520speech%2520in%2520over%250A7000%2520languages%252C%2520many%2520of%2520which%2520lack%2520sufficient%2520data%2520for%2520traditional%2520TTS%250Adevelopment.%2520By%2520leveraging%2520a%2520novel%2520integration%2520of%2520massively%2520multilingual%250Apretraining%2520and%2520meta%2520learning%2520to%2520approximate%2520language%2520representations%252C%2520our%250Aapproach%2520enables%2520zero-shot%2520speech%2520synthesis%2520in%2520languages%2520without%2520any%2520available%250Adata.%2520We%2520validate%2520our%2520system%2527s%2520performance%2520through%2520objective%2520measures%2520and%2520human%250Aevaluation%2520across%2520a%2520diverse%2520linguistic%2520landscape.%2520By%2520releasing%2520our%2520code%2520and%250Amodels%2520publicly%252C%2520we%2520aim%2520to%2520empower%2520communities%2520with%2520limited%2520linguistic%250Aresources%2520and%2520foster%2520further%2520innovation%2520in%2520the%2520field%2520of%2520speech%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20Learning%20Text-to-Speech%20Synthesis%20in%20over%207000%20Languages&entry.906535625=Florian%20Lux%20and%20Sarina%20Meyer%20and%20Lyonel%20Behringer%20and%20Frank%20Zalkow%20and%20Phat%20Do%20and%20Matt%20Coler%20and%20Emanu%C3%ABl%20A.%20P.%20Habets%20and%20Ngoc%20Thang%20Vu&entry.1292438233=%20%20In%20this%20work%2C%20we%20take%20on%20the%20challenging%20task%20of%20building%20a%20single%0Atext-to-speech%20synthesis%20system%20that%20is%20capable%20of%20generating%20speech%20in%20over%0A7000%20languages%2C%20many%20of%20which%20lack%20sufficient%20data%20for%20traditional%20TTS%0Adevelopment.%20By%20leveraging%20a%20novel%20integration%20of%20massively%20multilingual%0Apretraining%20and%20meta%20learning%20to%20approximate%20language%20representations%2C%20our%0Aapproach%20enables%20zero-shot%20speech%20synthesis%20in%20languages%20without%20any%20available%0Adata.%20We%20validate%20our%20system%27s%20performance%20through%20objective%20measures%20and%20human%0Aevaluation%20across%20a%20diverse%20linguistic%20landscape.%20By%20releasing%20our%20code%20and%0Amodels%20publicly%2C%20we%20aim%20to%20empower%20communities%20with%20limited%20linguistic%0Aresources%20and%20foster%20further%20innovation%20in%20the%20field%20of%20speech%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06403v1&entry.124074799=Read"},
{"title": "Notes on Various Errors and Jacobian Derivations for SLAM", "author": "Gyubeom Im", "abstract": "  This paper delves into critical concepts and meticulous calculations\npertinent to Simultaneous Localization and Mapping (SLAM), with a focus on\nerror analysis and Jacobian matrices. We introduce various types of errors\ncommonly encountered in SLAM, including reprojection error, photometric error,\nrelative pose error, and line reprojection error, alongside their mathematical\nformulations. The fundamental role of error as the discrepancy between observed\nand predicted values in SLAM optimization is examined, emphasizing non-linear\nleast squares methods for optimization.\n  We provide a detailed analysis of: - Reprojection Error: Including Jacobian\ncalculations for camera poses and map points, highlighting both theoretical\nunderpinnings and practical consequences. - Photometric Error: Addressing\nerrors from image intensity variations, essential for direct method-based SLAM.\n- Relative Pose Error: Discussing its significance in pose graph optimization,\nespecially in loop closure scenarios. The paper also presents extensive\nderivations of Jacobian matrices for various SLAM components such as camera\nposes, map points, and motion parameters. We explore the application of Lie\ntheory to optimize rotation representations and transformations, improving\ncomputational efficiency. Specific software implementations are referenced,\noffering practical insights into the real-world application of these theories\nin SLAM systems.\n  Additionally, advanced topics such as line reprojection errors and IMU\nmeasurement errors are explored, discussing their impact on SLAM accuracy and\nperformance. This comprehensive examination aims to enhance understanding and\nimplementation of error analysis and Jacobian derivation in SLAM, contributing\nto more accurate and efficient state estimation in complex environments.\n", "link": "http://arxiv.org/abs/2406.06422v1", "date": "2024-06-10", "relevancy": 2.3005, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6011}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Notes%20on%20Various%20Errors%20and%20Jacobian%20Derivations%20for%20SLAM&body=Title%3A%20Notes%20on%20Various%20Errors%20and%20Jacobian%20Derivations%20for%20SLAM%0AAuthor%3A%20Gyubeom%20Im%0AAbstract%3A%20%20%20This%20paper%20delves%20into%20critical%20concepts%20and%20meticulous%20calculations%0Apertinent%20to%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20with%20a%20focus%20on%0Aerror%20analysis%20and%20Jacobian%20matrices.%20We%20introduce%20various%20types%20of%20errors%0Acommonly%20encountered%20in%20SLAM%2C%20including%20reprojection%20error%2C%20photometric%20error%2C%0Arelative%20pose%20error%2C%20and%20line%20reprojection%20error%2C%20alongside%20their%20mathematical%0Aformulations.%20The%20fundamental%20role%20of%20error%20as%20the%20discrepancy%20between%20observed%0Aand%20predicted%20values%20in%20SLAM%20optimization%20is%20examined%2C%20emphasizing%20non-linear%0Aleast%20squares%20methods%20for%20optimization.%0A%20%20We%20provide%20a%20detailed%20analysis%20of%3A%20-%20Reprojection%20Error%3A%20Including%20Jacobian%0Acalculations%20for%20camera%20poses%20and%20map%20points%2C%20highlighting%20both%20theoretical%0Aunderpinnings%20and%20practical%20consequences.%20-%20Photometric%20Error%3A%20Addressing%0Aerrors%20from%20image%20intensity%20variations%2C%20essential%20for%20direct%20method-based%20SLAM.%0A-%20Relative%20Pose%20Error%3A%20Discussing%20its%20significance%20in%20pose%20graph%20optimization%2C%0Aespecially%20in%20loop%20closure%20scenarios.%20The%20paper%20also%20presents%20extensive%0Aderivations%20of%20Jacobian%20matrices%20for%20various%20SLAM%20components%20such%20as%20camera%0Aposes%2C%20map%20points%2C%20and%20motion%20parameters.%20We%20explore%20the%20application%20of%20Lie%0Atheory%20to%20optimize%20rotation%20representations%20and%20transformations%2C%20improving%0Acomputational%20efficiency.%20Specific%20software%20implementations%20are%20referenced%2C%0Aoffering%20practical%20insights%20into%20the%20real-world%20application%20of%20these%20theories%0Ain%20SLAM%20systems.%0A%20%20Additionally%2C%20advanced%20topics%20such%20as%20line%20reprojection%20errors%20and%20IMU%0Ameasurement%20errors%20are%20explored%2C%20discussing%20their%20impact%20on%20SLAM%20accuracy%20and%0Aperformance.%20This%20comprehensive%20examination%20aims%20to%20enhance%20understanding%20and%0Aimplementation%20of%20error%20analysis%20and%20Jacobian%20derivation%20in%20SLAM%2C%20contributing%0Ato%20more%20accurate%20and%20efficient%20state%20estimation%20in%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNotes%2520on%2520Various%2520Errors%2520and%2520Jacobian%2520Derivations%2520for%2520SLAM%26entry.906535625%3DGyubeom%2520Im%26entry.1292438233%3D%2520%2520This%2520paper%2520delves%2520into%2520critical%2520concepts%2520and%2520meticulous%2520calculations%250Apertinent%2520to%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520with%2520a%2520focus%2520on%250Aerror%2520analysis%2520and%2520Jacobian%2520matrices.%2520We%2520introduce%2520various%2520types%2520of%2520errors%250Acommonly%2520encountered%2520in%2520SLAM%252C%2520including%2520reprojection%2520error%252C%2520photometric%2520error%252C%250Arelative%2520pose%2520error%252C%2520and%2520line%2520reprojection%2520error%252C%2520alongside%2520their%2520mathematical%250Aformulations.%2520The%2520fundamental%2520role%2520of%2520error%2520as%2520the%2520discrepancy%2520between%2520observed%250Aand%2520predicted%2520values%2520in%2520SLAM%2520optimization%2520is%2520examined%252C%2520emphasizing%2520non-linear%250Aleast%2520squares%2520methods%2520for%2520optimization.%250A%2520%2520We%2520provide%2520a%2520detailed%2520analysis%2520of%253A%2520-%2520Reprojection%2520Error%253A%2520Including%2520Jacobian%250Acalculations%2520for%2520camera%2520poses%2520and%2520map%2520points%252C%2520highlighting%2520both%2520theoretical%250Aunderpinnings%2520and%2520practical%2520consequences.%2520-%2520Photometric%2520Error%253A%2520Addressing%250Aerrors%2520from%2520image%2520intensity%2520variations%252C%2520essential%2520for%2520direct%2520method-based%2520SLAM.%250A-%2520Relative%2520Pose%2520Error%253A%2520Discussing%2520its%2520significance%2520in%2520pose%2520graph%2520optimization%252C%250Aespecially%2520in%2520loop%2520closure%2520scenarios.%2520The%2520paper%2520also%2520presents%2520extensive%250Aderivations%2520of%2520Jacobian%2520matrices%2520for%2520various%2520SLAM%2520components%2520such%2520as%2520camera%250Aposes%252C%2520map%2520points%252C%2520and%2520motion%2520parameters.%2520We%2520explore%2520the%2520application%2520of%2520Lie%250Atheory%2520to%2520optimize%2520rotation%2520representations%2520and%2520transformations%252C%2520improving%250Acomputational%2520efficiency.%2520Specific%2520software%2520implementations%2520are%2520referenced%252C%250Aoffering%2520practical%2520insights%2520into%2520the%2520real-world%2520application%2520of%2520these%2520theories%250Ain%2520SLAM%2520systems.%250A%2520%2520Additionally%252C%2520advanced%2520topics%2520such%2520as%2520line%2520reprojection%2520errors%2520and%2520IMU%250Ameasurement%2520errors%2520are%2520explored%252C%2520discussing%2520their%2520impact%2520on%2520SLAM%2520accuracy%2520and%250Aperformance.%2520This%2520comprehensive%2520examination%2520aims%2520to%2520enhance%2520understanding%2520and%250Aimplementation%2520of%2520error%2520analysis%2520and%2520Jacobian%2520derivation%2520in%2520SLAM%252C%2520contributing%250Ato%2520more%2520accurate%2520and%2520efficient%2520state%2520estimation%2520in%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Notes%20on%20Various%20Errors%20and%20Jacobian%20Derivations%20for%20SLAM&entry.906535625=Gyubeom%20Im&entry.1292438233=%20%20This%20paper%20delves%20into%20critical%20concepts%20and%20meticulous%20calculations%0Apertinent%20to%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20with%20a%20focus%20on%0Aerror%20analysis%20and%20Jacobian%20matrices.%20We%20introduce%20various%20types%20of%20errors%0Acommonly%20encountered%20in%20SLAM%2C%20including%20reprojection%20error%2C%20photometric%20error%2C%0Arelative%20pose%20error%2C%20and%20line%20reprojection%20error%2C%20alongside%20their%20mathematical%0Aformulations.%20The%20fundamental%20role%20of%20error%20as%20the%20discrepancy%20between%20observed%0Aand%20predicted%20values%20in%20SLAM%20optimization%20is%20examined%2C%20emphasizing%20non-linear%0Aleast%20squares%20methods%20for%20optimization.%0A%20%20We%20provide%20a%20detailed%20analysis%20of%3A%20-%20Reprojection%20Error%3A%20Including%20Jacobian%0Acalculations%20for%20camera%20poses%20and%20map%20points%2C%20highlighting%20both%20theoretical%0Aunderpinnings%20and%20practical%20consequences.%20-%20Photometric%20Error%3A%20Addressing%0Aerrors%20from%20image%20intensity%20variations%2C%20essential%20for%20direct%20method-based%20SLAM.%0A-%20Relative%20Pose%20Error%3A%20Discussing%20its%20significance%20in%20pose%20graph%20optimization%2C%0Aespecially%20in%20loop%20closure%20scenarios.%20The%20paper%20also%20presents%20extensive%0Aderivations%20of%20Jacobian%20matrices%20for%20various%20SLAM%20components%20such%20as%20camera%0Aposes%2C%20map%20points%2C%20and%20motion%20parameters.%20We%20explore%20the%20application%20of%20Lie%0Atheory%20to%20optimize%20rotation%20representations%20and%20transformations%2C%20improving%0Acomputational%20efficiency.%20Specific%20software%20implementations%20are%20referenced%2C%0Aoffering%20practical%20insights%20into%20the%20real-world%20application%20of%20these%20theories%0Ain%20SLAM%20systems.%0A%20%20Additionally%2C%20advanced%20topics%20such%20as%20line%20reprojection%20errors%20and%20IMU%0Ameasurement%20errors%20are%20explored%2C%20discussing%20their%20impact%20on%20SLAM%20accuracy%20and%0Aperformance.%20This%20comprehensive%20examination%20aims%20to%20enhance%20understanding%20and%0Aimplementation%20of%20error%20analysis%20and%20Jacobian%20derivation%20in%20SLAM%2C%20contributing%0Ato%20more%20accurate%20and%20efficient%20state%20estimation%20in%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06422v1&entry.124074799=Read"},
{"title": "Vehicle Vectors and Traffic Patterns from Planet Imagery", "author": "Adam Van Etten", "abstract": "  We explore methods to detect automobiles in Planet imagery and build a large\nscale vector field for moving objects. Planet operates two distinct\nconstellations: high-resolution SkySat satellites as well as medium-resolution\nSuperDove satellites. We show that both static and moving cars can be\nidentified reliably in high-resolution SkySat imagery. We are able to estimate\nthe speed and heading of moving vehicles by leveraging the inter-band\ndisplacement (or \"rainbow\" effect) of moving objects. Identifying cars and\ntrucks in medium-resolution SuperDove imagery is far more difficult, though a\nsimilar rainbow effect is observed in these satellites and enables moving\nvehicles to be detected and vectorized. The frequent revisit of Planet\nsatellites enables the categorization of automobile and truck activity patterns\nover broad areas of interest and lengthy timeframes.\n", "link": "http://arxiv.org/abs/2406.06320v1", "date": "2024-06-10", "relevancy": 2.2964, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4893}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4443}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vehicle%20Vectors%20and%20Traffic%20Patterns%20from%20Planet%20Imagery&body=Title%3A%20Vehicle%20Vectors%20and%20Traffic%20Patterns%20from%20Planet%20Imagery%0AAuthor%3A%20Adam%20Van%20Etten%0AAbstract%3A%20%20%20We%20explore%20methods%20to%20detect%20automobiles%20in%20Planet%20imagery%20and%20build%20a%20large%0Ascale%20vector%20field%20for%20moving%20objects.%20Planet%20operates%20two%20distinct%0Aconstellations%3A%20high-resolution%20SkySat%20satellites%20as%20well%20as%20medium-resolution%0ASuperDove%20satellites.%20We%20show%20that%20both%20static%20and%20moving%20cars%20can%20be%0Aidentified%20reliably%20in%20high-resolution%20SkySat%20imagery.%20We%20are%20able%20to%20estimate%0Athe%20speed%20and%20heading%20of%20moving%20vehicles%20by%20leveraging%20the%20inter-band%0Adisplacement%20%28or%20%22rainbow%22%20effect%29%20of%20moving%20objects.%20Identifying%20cars%20and%0Atrucks%20in%20medium-resolution%20SuperDove%20imagery%20is%20far%20more%20difficult%2C%20though%20a%0Asimilar%20rainbow%20effect%20is%20observed%20in%20these%20satellites%20and%20enables%20moving%0Avehicles%20to%20be%20detected%20and%20vectorized.%20The%20frequent%20revisit%20of%20Planet%0Asatellites%20enables%20the%20categorization%20of%20automobile%20and%20truck%20activity%20patterns%0Aover%20broad%20areas%20of%20interest%20and%20lengthy%20timeframes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVehicle%2520Vectors%2520and%2520Traffic%2520Patterns%2520from%2520Planet%2520Imagery%26entry.906535625%3DAdam%2520Van%2520Etten%26entry.1292438233%3D%2520%2520We%2520explore%2520methods%2520to%2520detect%2520automobiles%2520in%2520Planet%2520imagery%2520and%2520build%2520a%2520large%250Ascale%2520vector%2520field%2520for%2520moving%2520objects.%2520Planet%2520operates%2520two%2520distinct%250Aconstellations%253A%2520high-resolution%2520SkySat%2520satellites%2520as%2520well%2520as%2520medium-resolution%250ASuperDove%2520satellites.%2520We%2520show%2520that%2520both%2520static%2520and%2520moving%2520cars%2520can%2520be%250Aidentified%2520reliably%2520in%2520high-resolution%2520SkySat%2520imagery.%2520We%2520are%2520able%2520to%2520estimate%250Athe%2520speed%2520and%2520heading%2520of%2520moving%2520vehicles%2520by%2520leveraging%2520the%2520inter-band%250Adisplacement%2520%2528or%2520%2522rainbow%2522%2520effect%2529%2520of%2520moving%2520objects.%2520Identifying%2520cars%2520and%250Atrucks%2520in%2520medium-resolution%2520SuperDove%2520imagery%2520is%2520far%2520more%2520difficult%252C%2520though%2520a%250Asimilar%2520rainbow%2520effect%2520is%2520observed%2520in%2520these%2520satellites%2520and%2520enables%2520moving%250Avehicles%2520to%2520be%2520detected%2520and%2520vectorized.%2520The%2520frequent%2520revisit%2520of%2520Planet%250Asatellites%2520enables%2520the%2520categorization%2520of%2520automobile%2520and%2520truck%2520activity%2520patterns%250Aover%2520broad%2520areas%2520of%2520interest%2520and%2520lengthy%2520timeframes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vehicle%20Vectors%20and%20Traffic%20Patterns%20from%20Planet%20Imagery&entry.906535625=Adam%20Van%20Etten&entry.1292438233=%20%20We%20explore%20methods%20to%20detect%20automobiles%20in%20Planet%20imagery%20and%20build%20a%20large%0Ascale%20vector%20field%20for%20moving%20objects.%20Planet%20operates%20two%20distinct%0Aconstellations%3A%20high-resolution%20SkySat%20satellites%20as%20well%20as%20medium-resolution%0ASuperDove%20satellites.%20We%20show%20that%20both%20static%20and%20moving%20cars%20can%20be%0Aidentified%20reliably%20in%20high-resolution%20SkySat%20imagery.%20We%20are%20able%20to%20estimate%0Athe%20speed%20and%20heading%20of%20moving%20vehicles%20by%20leveraging%20the%20inter-band%0Adisplacement%20%28or%20%22rainbow%22%20effect%29%20of%20moving%20objects.%20Identifying%20cars%20and%0Atrucks%20in%20medium-resolution%20SuperDove%20imagery%20is%20far%20more%20difficult%2C%20though%20a%0Asimilar%20rainbow%20effect%20is%20observed%20in%20these%20satellites%20and%20enables%20moving%0Avehicles%20to%20be%20detected%20and%20vectorized.%20The%20frequent%20revisit%20of%20Planet%0Asatellites%20enables%20the%20categorization%20of%20automobile%20and%20truck%20activity%20patterns%0Aover%20broad%20areas%20of%20interest%20and%20lengthy%20timeframes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06320v1&entry.124074799=Read"},
{"title": "I-MPN: Inductive Message Passing Network for Effective and Efficient\n  Human-in-the-Loop Annotation of Mobile Eye Tracking Data", "author": "Hoang H. Le and Duy M. H. Nguyen and Omair Shahzad Bhatti and Laszlo Kopacsi and Thinh P. Ngo and Binh T. Nguyen and Michael Barz and Daniel Sonntag", "abstract": "  Understanding human visual processing in dynamic environments is essential\nfor psychology and human-centered interaction design. Mobile eye-tracking\nsystems, combining egocentric video and gaze signals, offer valuable insights.\nHowever, manual analysis of these recordings is time-intensive. In this work,\nwe present a novel human-centered learning algorithm designed for automated\nobject recognition within mobile eye-tracking settings. Our approach seamlessly\nintegrates an object detector with an inductive message-passing network\ntechnique (I-MPN), harnessing node features such as node profile information\nand positions. This integration enables our algorithm to learn embedding\nfunctions capable of generalizing to new object angle views, thereby\nfacilitating rapid adaptation and efficient reasoning in dynamic contexts as\nusers navigate through their environment. Through experiments conducted on\nthree distinct video sequences, our \\textit{interactive-based method} showcases\nsignificant performance improvements over fixed training/testing algorithms,\neven when trained on considerably smaller annotated samples collected through\nuser feedback. Furthermore, we showcase exceptional efficiency in data\nannotation processes, surpassing approaches that use complete object detectors,\ncombine detectors with convolutional networks, or employ interactive video\nsegmentation.\n", "link": "http://arxiv.org/abs/2406.06239v1", "date": "2024-06-10", "relevancy": 2.2611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5789}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-MPN%3A%20Inductive%20Message%20Passing%20Network%20for%20Effective%20and%20Efficient%0A%20%20Human-in-the-Loop%20Annotation%20of%20Mobile%20Eye%20Tracking%20Data&body=Title%3A%20I-MPN%3A%20Inductive%20Message%20Passing%20Network%20for%20Effective%20and%20Efficient%0A%20%20Human-in-the-Loop%20Annotation%20of%20Mobile%20Eye%20Tracking%20Data%0AAuthor%3A%20Hoang%20H.%20Le%20and%20Duy%20M.%20H.%20Nguyen%20and%20Omair%20Shahzad%20Bhatti%20and%20Laszlo%20Kopacsi%20and%20Thinh%20P.%20Ngo%20and%20Binh%20T.%20Nguyen%20and%20Michael%20Barz%20and%20Daniel%20Sonntag%0AAbstract%3A%20%20%20Understanding%20human%20visual%20processing%20in%20dynamic%20environments%20is%20essential%0Afor%20psychology%20and%20human-centered%20interaction%20design.%20Mobile%20eye-tracking%0Asystems%2C%20combining%20egocentric%20video%20and%20gaze%20signals%2C%20offer%20valuable%20insights.%0AHowever%2C%20manual%20analysis%20of%20these%20recordings%20is%20time-intensive.%20In%20this%20work%2C%0Awe%20present%20a%20novel%20human-centered%20learning%20algorithm%20designed%20for%20automated%0Aobject%20recognition%20within%20mobile%20eye-tracking%20settings.%20Our%20approach%20seamlessly%0Aintegrates%20an%20object%20detector%20with%20an%20inductive%20message-passing%20network%0Atechnique%20%28I-MPN%29%2C%20harnessing%20node%20features%20such%20as%20node%20profile%20information%0Aand%20positions.%20This%20integration%20enables%20our%20algorithm%20to%20learn%20embedding%0Afunctions%20capable%20of%20generalizing%20to%20new%20object%20angle%20views%2C%20thereby%0Afacilitating%20rapid%20adaptation%20and%20efficient%20reasoning%20in%20dynamic%20contexts%20as%0Ausers%20navigate%20through%20their%20environment.%20Through%20experiments%20conducted%20on%0Athree%20distinct%20video%20sequences%2C%20our%20%5Ctextit%7Binteractive-based%20method%7D%20showcases%0Asignificant%20performance%20improvements%20over%20fixed%20training/testing%20algorithms%2C%0Aeven%20when%20trained%20on%20considerably%20smaller%20annotated%20samples%20collected%20through%0Auser%20feedback.%20Furthermore%2C%20we%20showcase%20exceptional%20efficiency%20in%20data%0Aannotation%20processes%2C%20surpassing%20approaches%20that%20use%20complete%20object%20detectors%2C%0Acombine%20detectors%20with%20convolutional%20networks%2C%20or%20employ%20interactive%20video%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-MPN%253A%2520Inductive%2520Message%2520Passing%2520Network%2520for%2520Effective%2520and%2520Efficient%250A%2520%2520Human-in-the-Loop%2520Annotation%2520of%2520Mobile%2520Eye%2520Tracking%2520Data%26entry.906535625%3DHoang%2520H.%2520Le%2520and%2520Duy%2520M.%2520H.%2520Nguyen%2520and%2520Omair%2520Shahzad%2520Bhatti%2520and%2520Laszlo%2520Kopacsi%2520and%2520Thinh%2520P.%2520Ngo%2520and%2520Binh%2520T.%2520Nguyen%2520and%2520Michael%2520Barz%2520and%2520Daniel%2520Sonntag%26entry.1292438233%3D%2520%2520Understanding%2520human%2520visual%2520processing%2520in%2520dynamic%2520environments%2520is%2520essential%250Afor%2520psychology%2520and%2520human-centered%2520interaction%2520design.%2520Mobile%2520eye-tracking%250Asystems%252C%2520combining%2520egocentric%2520video%2520and%2520gaze%2520signals%252C%2520offer%2520valuable%2520insights.%250AHowever%252C%2520manual%2520analysis%2520of%2520these%2520recordings%2520is%2520time-intensive.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520novel%2520human-centered%2520learning%2520algorithm%2520designed%2520for%2520automated%250Aobject%2520recognition%2520within%2520mobile%2520eye-tracking%2520settings.%2520Our%2520approach%2520seamlessly%250Aintegrates%2520an%2520object%2520detector%2520with%2520an%2520inductive%2520message-passing%2520network%250Atechnique%2520%2528I-MPN%2529%252C%2520harnessing%2520node%2520features%2520such%2520as%2520node%2520profile%2520information%250Aand%2520positions.%2520This%2520integration%2520enables%2520our%2520algorithm%2520to%2520learn%2520embedding%250Afunctions%2520capable%2520of%2520generalizing%2520to%2520new%2520object%2520angle%2520views%252C%2520thereby%250Afacilitating%2520rapid%2520adaptation%2520and%2520efficient%2520reasoning%2520in%2520dynamic%2520contexts%2520as%250Ausers%2520navigate%2520through%2520their%2520environment.%2520Through%2520experiments%2520conducted%2520on%250Athree%2520distinct%2520video%2520sequences%252C%2520our%2520%255Ctextit%257Binteractive-based%2520method%257D%2520showcases%250Asignificant%2520performance%2520improvements%2520over%2520fixed%2520training/testing%2520algorithms%252C%250Aeven%2520when%2520trained%2520on%2520considerably%2520smaller%2520annotated%2520samples%2520collected%2520through%250Auser%2520feedback.%2520Furthermore%252C%2520we%2520showcase%2520exceptional%2520efficiency%2520in%2520data%250Aannotation%2520processes%252C%2520surpassing%2520approaches%2520that%2520use%2520complete%2520object%2520detectors%252C%250Acombine%2520detectors%2520with%2520convolutional%2520networks%252C%2520or%2520employ%2520interactive%2520video%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-MPN%3A%20Inductive%20Message%20Passing%20Network%20for%20Effective%20and%20Efficient%0A%20%20Human-in-the-Loop%20Annotation%20of%20Mobile%20Eye%20Tracking%20Data&entry.906535625=Hoang%20H.%20Le%20and%20Duy%20M.%20H.%20Nguyen%20and%20Omair%20Shahzad%20Bhatti%20and%20Laszlo%20Kopacsi%20and%20Thinh%20P.%20Ngo%20and%20Binh%20T.%20Nguyen%20and%20Michael%20Barz%20and%20Daniel%20Sonntag&entry.1292438233=%20%20Understanding%20human%20visual%20processing%20in%20dynamic%20environments%20is%20essential%0Afor%20psychology%20and%20human-centered%20interaction%20design.%20Mobile%20eye-tracking%0Asystems%2C%20combining%20egocentric%20video%20and%20gaze%20signals%2C%20offer%20valuable%20insights.%0AHowever%2C%20manual%20analysis%20of%20these%20recordings%20is%20time-intensive.%20In%20this%20work%2C%0Awe%20present%20a%20novel%20human-centered%20learning%20algorithm%20designed%20for%20automated%0Aobject%20recognition%20within%20mobile%20eye-tracking%20settings.%20Our%20approach%20seamlessly%0Aintegrates%20an%20object%20detector%20with%20an%20inductive%20message-passing%20network%0Atechnique%20%28I-MPN%29%2C%20harnessing%20node%20features%20such%20as%20node%20profile%20information%0Aand%20positions.%20This%20integration%20enables%20our%20algorithm%20to%20learn%20embedding%0Afunctions%20capable%20of%20generalizing%20to%20new%20object%20angle%20views%2C%20thereby%0Afacilitating%20rapid%20adaptation%20and%20efficient%20reasoning%20in%20dynamic%20contexts%20as%0Ausers%20navigate%20through%20their%20environment.%20Through%20experiments%20conducted%20on%0Athree%20distinct%20video%20sequences%2C%20our%20%5Ctextit%7Binteractive-based%20method%7D%20showcases%0Asignificant%20performance%20improvements%20over%20fixed%20training/testing%20algorithms%2C%0Aeven%20when%20trained%20on%20considerably%20smaller%20annotated%20samples%20collected%20through%0Auser%20feedback.%20Furthermore%2C%20we%20showcase%20exceptional%20efficiency%20in%20data%0Aannotation%20processes%2C%20surpassing%20approaches%20that%20use%20complete%20object%20detectors%2C%0Acombine%20detectors%20with%20convolutional%20networks%2C%20or%20employ%20interactive%20video%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06239v1&entry.124074799=Read"},
{"title": "UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving", "author": "Daniel Bogdoll and No\u00ebl Ollick and Tim Joseph and J. Marius Z\u00f6llner", "abstract": "  Dealing with atypical traffic scenarios remains a challenging task in\nautonomous driving. However, most anomaly detection approaches cannot be\ntrained on raw sensor data but require exposure to outlier data and powerful\nsemantic segmentation models trained in a supervised fashion. This limits the\nrepresentation of normality to labeled data, which does not scale well. In this\nwork, we revisit unsupervised anomaly detection and present UMAD, leveraging\ngenerative world models and unsupervised image segmentation. Our method\noutperforms state-of-the-art unsupervised anomaly detection.\n", "link": "http://arxiv.org/abs/2406.06370v1", "date": "2024-06-10", "relevancy": 2.2373, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5604}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMAD%3A%20Unsupervised%20Mask-Level%20Anomaly%20Detection%20for%20Autonomous%20Driving&body=Title%3A%20UMAD%3A%20Unsupervised%20Mask-Level%20Anomaly%20Detection%20for%20Autonomous%20Driving%0AAuthor%3A%20Daniel%20Bogdoll%20and%20No%C3%ABl%20Ollick%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Dealing%20with%20atypical%20traffic%20scenarios%20remains%20a%20challenging%20task%20in%0Aautonomous%20driving.%20However%2C%20most%20anomaly%20detection%20approaches%20cannot%20be%0Atrained%20on%20raw%20sensor%20data%20but%20require%20exposure%20to%20outlier%20data%20and%20powerful%0Asemantic%20segmentation%20models%20trained%20in%20a%20supervised%20fashion.%20This%20limits%20the%0Arepresentation%20of%20normality%20to%20labeled%20data%2C%20which%20does%20not%20scale%20well.%20In%20this%0Awork%2C%20we%20revisit%20unsupervised%20anomaly%20detection%20and%20present%20UMAD%2C%20leveraging%0Agenerative%20world%20models%20and%20unsupervised%20image%20segmentation.%20Our%20method%0Aoutperforms%20state-of-the-art%20unsupervised%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMAD%253A%2520Unsupervised%2520Mask-Level%2520Anomaly%2520Detection%2520for%2520Autonomous%2520Driving%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520No%25C3%25ABl%2520Ollick%2520and%2520Tim%2520Joseph%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Dealing%2520with%2520atypical%2520traffic%2520scenarios%2520remains%2520a%2520challenging%2520task%2520in%250Aautonomous%2520driving.%2520However%252C%2520most%2520anomaly%2520detection%2520approaches%2520cannot%2520be%250Atrained%2520on%2520raw%2520sensor%2520data%2520but%2520require%2520exposure%2520to%2520outlier%2520data%2520and%2520powerful%250Asemantic%2520segmentation%2520models%2520trained%2520in%2520a%2520supervised%2520fashion.%2520This%2520limits%2520the%250Arepresentation%2520of%2520normality%2520to%2520labeled%2520data%252C%2520which%2520does%2520not%2520scale%2520well.%2520In%2520this%250Awork%252C%2520we%2520revisit%2520unsupervised%2520anomaly%2520detection%2520and%2520present%2520UMAD%252C%2520leveraging%250Agenerative%2520world%2520models%2520and%2520unsupervised%2520image%2520segmentation.%2520Our%2520method%250Aoutperforms%2520state-of-the-art%2520unsupervised%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMAD%3A%20Unsupervised%20Mask-Level%20Anomaly%20Detection%20for%20Autonomous%20Driving&entry.906535625=Daniel%20Bogdoll%20and%20No%C3%ABl%20Ollick%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Dealing%20with%20atypical%20traffic%20scenarios%20remains%20a%20challenging%20task%20in%0Aautonomous%20driving.%20However%2C%20most%20anomaly%20detection%20approaches%20cannot%20be%0Atrained%20on%20raw%20sensor%20data%20but%20require%20exposure%20to%20outlier%20data%20and%20powerful%0Asemantic%20segmentation%20models%20trained%20in%20a%20supervised%20fashion.%20This%20limits%20the%0Arepresentation%20of%20normality%20to%20labeled%20data%2C%20which%20does%20not%20scale%20well.%20In%20this%0Awork%2C%20we%20revisit%20unsupervised%20anomaly%20detection%20and%20present%20UMAD%2C%20leveraging%0Agenerative%20world%20models%20and%20unsupervised%20image%20segmentation.%20Our%20method%0Aoutperforms%20state-of-the-art%20unsupervised%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06370v1&entry.124074799=Read"},
{"title": "Outlier detection by ensembling uncertainty with negative objectness", "author": "Anja Deli\u0107 and Matej Grci\u0107 and Sini\u0161a \u0160egvi\u0107", "abstract": "  Outlier detection is an essential capability in safety-critical applications\nof supervised visual recognition. Most of the existing methods deliver best\nresults by encouraging standard closed-set models to produce low-confidence\npredictions in negative training data. However, that approach conflates\nprediction uncertainty with recognition of the negative class. We therefore\nreconsider direct prediction of K+1 logits that correspond to K groundtruth\nclasses and one outlier class. This setup allows us to formulate a novel\nanomaly score as an ensemble of in-distribution uncertainty and the posterior\nof the outlier class which we term negative objectness. Now outliers can be\nindependently detected due to i) high prediction uncertainty or ii) similarity\nwith negative data. We embed our method into a dense prediction architecture\nwith mask-level recognition over K+2 classes. The training procedure encourages\nthe novel K+2-th class to learn negative objectness at pasted negative\ninstances. Our models outperform the current state-of-the art on standard\nbenchmarks for image-wide and pixel-level outlier detection with and without\ntraining on real negative data.\n", "link": "http://arxiv.org/abs/2402.15374v2", "date": "2024-06-10", "relevancy": 2.2357, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5749}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5713}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness&body=Title%3A%20Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness%0AAuthor%3A%20Anja%20Deli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87%0AAbstract%3A%20%20%20Outlier%20detection%20is%20an%20essential%20capability%20in%20safety-critical%20applications%0Aof%20supervised%20visual%20recognition.%20Most%20of%20the%20existing%20methods%20deliver%20best%0Aresults%20by%20encouraging%20standard%20closed-set%20models%20to%20produce%20low-confidence%0Apredictions%20in%20negative%20training%20data.%20However%2C%20that%20approach%20conflates%0Aprediction%20uncertainty%20with%20recognition%20of%20the%20negative%20class.%20We%20therefore%0Areconsider%20direct%20prediction%20of%20K%2B1%20logits%20that%20correspond%20to%20K%20groundtruth%0Aclasses%20and%20one%20outlier%20class.%20This%20setup%20allows%20us%20to%20formulate%20a%20novel%0Aanomaly%20score%20as%20an%20ensemble%20of%20in-distribution%20uncertainty%20and%20the%20posterior%0Aof%20the%20outlier%20class%20which%20we%20term%20negative%20objectness.%20Now%20outliers%20can%20be%0Aindependently%20detected%20due%20to%20i%29%20high%20prediction%20uncertainty%20or%20ii%29%20similarity%0Awith%20negative%20data.%20We%20embed%20our%20method%20into%20a%20dense%20prediction%20architecture%0Awith%20mask-level%20recognition%20over%20K%2B2%20classes.%20The%20training%20procedure%20encourages%0Athe%20novel%20K%2B2-th%20class%20to%20learn%20negative%20objectness%20at%20pasted%20negative%0Ainstances.%20Our%20models%20outperform%20the%20current%20state-of-the%20art%20on%20standard%0Abenchmarks%20for%20image-wide%20and%20pixel-level%20outlier%20detection%20with%20and%20without%0Atraining%20on%20real%20negative%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutlier%2520detection%2520by%2520ensembling%2520uncertainty%2520with%2520negative%2520objectness%26entry.906535625%3DAnja%2520Deli%25C4%2587%2520and%2520Matej%2520Grci%25C4%2587%2520and%2520Sini%25C5%25A1a%2520%25C5%25A0egvi%25C4%2587%26entry.1292438233%3D%2520%2520Outlier%2520detection%2520is%2520an%2520essential%2520capability%2520in%2520safety-critical%2520applications%250Aof%2520supervised%2520visual%2520recognition.%2520Most%2520of%2520the%2520existing%2520methods%2520deliver%2520best%250Aresults%2520by%2520encouraging%2520standard%2520closed-set%2520models%2520to%2520produce%2520low-confidence%250Apredictions%2520in%2520negative%2520training%2520data.%2520However%252C%2520that%2520approach%2520conflates%250Aprediction%2520uncertainty%2520with%2520recognition%2520of%2520the%2520negative%2520class.%2520We%2520therefore%250Areconsider%2520direct%2520prediction%2520of%2520K%252B1%2520logits%2520that%2520correspond%2520to%2520K%2520groundtruth%250Aclasses%2520and%2520one%2520outlier%2520class.%2520This%2520setup%2520allows%2520us%2520to%2520formulate%2520a%2520novel%250Aanomaly%2520score%2520as%2520an%2520ensemble%2520of%2520in-distribution%2520uncertainty%2520and%2520the%2520posterior%250Aof%2520the%2520outlier%2520class%2520which%2520we%2520term%2520negative%2520objectness.%2520Now%2520outliers%2520can%2520be%250Aindependently%2520detected%2520due%2520to%2520i%2529%2520high%2520prediction%2520uncertainty%2520or%2520ii%2529%2520similarity%250Awith%2520negative%2520data.%2520We%2520embed%2520our%2520method%2520into%2520a%2520dense%2520prediction%2520architecture%250Awith%2520mask-level%2520recognition%2520over%2520K%252B2%2520classes.%2520The%2520training%2520procedure%2520encourages%250Athe%2520novel%2520K%252B2-th%2520class%2520to%2520learn%2520negative%2520objectness%2520at%2520pasted%2520negative%250Ainstances.%2520Our%2520models%2520outperform%2520the%2520current%2520state-of-the%2520art%2520on%2520standard%250Abenchmarks%2520for%2520image-wide%2520and%2520pixel-level%2520outlier%2520detection%2520with%2520and%2520without%250Atraining%2520on%2520real%2520negative%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness&entry.906535625=Anja%20Deli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87&entry.1292438233=%20%20Outlier%20detection%20is%20an%20essential%20capability%20in%20safety-critical%20applications%0Aof%20supervised%20visual%20recognition.%20Most%20of%20the%20existing%20methods%20deliver%20best%0Aresults%20by%20encouraging%20standard%20closed-set%20models%20to%20produce%20low-confidence%0Apredictions%20in%20negative%20training%20data.%20However%2C%20that%20approach%20conflates%0Aprediction%20uncertainty%20with%20recognition%20of%20the%20negative%20class.%20We%20therefore%0Areconsider%20direct%20prediction%20of%20K%2B1%20logits%20that%20correspond%20to%20K%20groundtruth%0Aclasses%20and%20one%20outlier%20class.%20This%20setup%20allows%20us%20to%20formulate%20a%20novel%0Aanomaly%20score%20as%20an%20ensemble%20of%20in-distribution%20uncertainty%20and%20the%20posterior%0Aof%20the%20outlier%20class%20which%20we%20term%20negative%20objectness.%20Now%20outliers%20can%20be%0Aindependently%20detected%20due%20to%20i%29%20high%20prediction%20uncertainty%20or%20ii%29%20similarity%0Awith%20negative%20data.%20We%20embed%20our%20method%20into%20a%20dense%20prediction%20architecture%0Awith%20mask-level%20recognition%20over%20K%2B2%20classes.%20The%20training%20procedure%20encourages%0Athe%20novel%20K%2B2-th%20class%20to%20learn%20negative%20objectness%20at%20pasted%20negative%0Ainstances.%20Our%20models%20outperform%20the%20current%20state-of-the%20art%20on%20standard%0Abenchmarks%20for%20image-wide%20and%20pixel-level%20outlier%20detection%20with%20and%20without%0Atraining%20on%20real%20negative%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15374v2&entry.124074799=Read"},
{"title": "Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak\n  Attacks", "author": "Zonghao Ying and Aishan Liu and Xianglong Liu and Dacheng Tao", "abstract": "  The recent release of GPT-4o has garnered widespread attention due to its\npowerful general capabilities. While its impressive performance is widely\nacknowledged, its safety aspects have not been sufficiently explored. Given the\npotential societal impact of risky content generated by advanced generative AI\nsuch as GPT-4o, it is crucial to rigorously evaluate its safety. In response to\nthis question, this paper for the first time conducts a rigorous evaluation of\nGPT-4o against jailbreak attacks. Specifically, this paper adopts a series of\nmulti-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks\nencompassing three modalities (\\ie, text, speech, and image), which involves\nthe optimization of over 4,000 initial text queries and the analysis and\nstatistical evaluation of nearly 8,000+ response on GPT-4o. Our extensive\nexperiments reveal several novel observations: (1) In contrast to the previous\nversion (such as GPT-4V), GPT-4o has enhanced safety in the context of text\nmodality jailbreak; (2) The newly introduced audio modality opens up new attack\nvectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal\njailbreak attack methods are largely ineffective against GPT-4o and GPT-4V.\nThese findings provide critical insights into the safety implications of GPT-4o\nand underscore the need for robust alignment guardrails in large models. Our\ncode is available at \\url{https://github.com/NY1024/Jailbreak_GPT4o}.\n", "link": "http://arxiv.org/abs/2406.06302v1", "date": "2024-06-10", "relevancy": 2.224, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4576}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4438}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Safety%20of%20GPT-4o%3A%20An%20Empirical%20Study%20using%20Jailbreak%0A%20%20Attacks&body=Title%3A%20Unveiling%20the%20Safety%20of%20GPT-4o%3A%20An%20Empirical%20Study%20using%20Jailbreak%0A%20%20Attacks%0AAuthor%3A%20Zonghao%20Ying%20and%20Aishan%20Liu%20and%20Xianglong%20Liu%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20recent%20release%20of%20GPT-4o%20has%20garnered%20widespread%20attention%20due%20to%20its%0Apowerful%20general%20capabilities.%20While%20its%20impressive%20performance%20is%20widely%0Aacknowledged%2C%20its%20safety%20aspects%20have%20not%20been%20sufficiently%20explored.%20Given%20the%0Apotential%20societal%20impact%20of%20risky%20content%20generated%20by%20advanced%20generative%20AI%0Asuch%20as%20GPT-4o%2C%20it%20is%20crucial%20to%20rigorously%20evaluate%20its%20safety.%20In%20response%20to%0Athis%20question%2C%20this%20paper%20for%20the%20first%20time%20conducts%20a%20rigorous%20evaluation%20of%0AGPT-4o%20against%20jailbreak%20attacks.%20Specifically%2C%20this%20paper%20adopts%20a%20series%20of%0Amulti-modal%20and%20uni-modal%20jailbreak%20attacks%20on%204%20commonly%20used%20benchmarks%0Aencompassing%20three%20modalities%20%28%5Cie%2C%20text%2C%20speech%2C%20and%20image%29%2C%20which%20involves%0Athe%20optimization%20of%20over%204%2C000%20initial%20text%20queries%20and%20the%20analysis%20and%0Astatistical%20evaluation%20of%20nearly%208%2C000%2B%20response%20on%20GPT-4o.%20Our%20extensive%0Aexperiments%20reveal%20several%20novel%20observations%3A%20%281%29%20In%20contrast%20to%20the%20previous%0Aversion%20%28such%20as%20GPT-4V%29%2C%20GPT-4o%20has%20enhanced%20safety%20in%20the%20context%20of%20text%0Amodality%20jailbreak%3B%20%282%29%20The%20newly%20introduced%20audio%20modality%20opens%20up%20new%20attack%0Avectors%20for%20jailbreak%20attacks%20on%20GPT-4o%3B%20%283%29%20Existing%20black-box%20multimodal%0Ajailbreak%20attack%20methods%20are%20largely%20ineffective%20against%20GPT-4o%20and%20GPT-4V.%0AThese%20findings%20provide%20critical%20insights%20into%20the%20safety%20implications%20of%20GPT-4o%0Aand%20underscore%20the%20need%20for%20robust%20alignment%20guardrails%20in%20large%20models.%20Our%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/NY1024/Jailbreak_GPT4o%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Safety%2520of%2520GPT-4o%253A%2520An%2520Empirical%2520Study%2520using%2520Jailbreak%250A%2520%2520Attacks%26entry.906535625%3DZonghao%2520Ying%2520and%2520Aishan%2520Liu%2520and%2520Xianglong%2520Liu%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520recent%2520release%2520of%2520GPT-4o%2520has%2520garnered%2520widespread%2520attention%2520due%2520to%2520its%250Apowerful%2520general%2520capabilities.%2520While%2520its%2520impressive%2520performance%2520is%2520widely%250Aacknowledged%252C%2520its%2520safety%2520aspects%2520have%2520not%2520been%2520sufficiently%2520explored.%2520Given%2520the%250Apotential%2520societal%2520impact%2520of%2520risky%2520content%2520generated%2520by%2520advanced%2520generative%2520AI%250Asuch%2520as%2520GPT-4o%252C%2520it%2520is%2520crucial%2520to%2520rigorously%2520evaluate%2520its%2520safety.%2520In%2520response%2520to%250Athis%2520question%252C%2520this%2520paper%2520for%2520the%2520first%2520time%2520conducts%2520a%2520rigorous%2520evaluation%2520of%250AGPT-4o%2520against%2520jailbreak%2520attacks.%2520Specifically%252C%2520this%2520paper%2520adopts%2520a%2520series%2520of%250Amulti-modal%2520and%2520uni-modal%2520jailbreak%2520attacks%2520on%25204%2520commonly%2520used%2520benchmarks%250Aencompassing%2520three%2520modalities%2520%2528%255Cie%252C%2520text%252C%2520speech%252C%2520and%2520image%2529%252C%2520which%2520involves%250Athe%2520optimization%2520of%2520over%25204%252C000%2520initial%2520text%2520queries%2520and%2520the%2520analysis%2520and%250Astatistical%2520evaluation%2520of%2520nearly%25208%252C000%252B%2520response%2520on%2520GPT-4o.%2520Our%2520extensive%250Aexperiments%2520reveal%2520several%2520novel%2520observations%253A%2520%25281%2529%2520In%2520contrast%2520to%2520the%2520previous%250Aversion%2520%2528such%2520as%2520GPT-4V%2529%252C%2520GPT-4o%2520has%2520enhanced%2520safety%2520in%2520the%2520context%2520of%2520text%250Amodality%2520jailbreak%253B%2520%25282%2529%2520The%2520newly%2520introduced%2520audio%2520modality%2520opens%2520up%2520new%2520attack%250Avectors%2520for%2520jailbreak%2520attacks%2520on%2520GPT-4o%253B%2520%25283%2529%2520Existing%2520black-box%2520multimodal%250Ajailbreak%2520attack%2520methods%2520are%2520largely%2520ineffective%2520against%2520GPT-4o%2520and%2520GPT-4V.%250AThese%2520findings%2520provide%2520critical%2520insights%2520into%2520the%2520safety%2520implications%2520of%2520GPT-4o%250Aand%2520underscore%2520the%2520need%2520for%2520robust%2520alignment%2520guardrails%2520in%2520large%2520models.%2520Our%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/NY1024/Jailbreak_GPT4o%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Safety%20of%20GPT-4o%3A%20An%20Empirical%20Study%20using%20Jailbreak%0A%20%20Attacks&entry.906535625=Zonghao%20Ying%20and%20Aishan%20Liu%20and%20Xianglong%20Liu%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20recent%20release%20of%20GPT-4o%20has%20garnered%20widespread%20attention%20due%20to%20its%0Apowerful%20general%20capabilities.%20While%20its%20impressive%20performance%20is%20widely%0Aacknowledged%2C%20its%20safety%20aspects%20have%20not%20been%20sufficiently%20explored.%20Given%20the%0Apotential%20societal%20impact%20of%20risky%20content%20generated%20by%20advanced%20generative%20AI%0Asuch%20as%20GPT-4o%2C%20it%20is%20crucial%20to%20rigorously%20evaluate%20its%20safety.%20In%20response%20to%0Athis%20question%2C%20this%20paper%20for%20the%20first%20time%20conducts%20a%20rigorous%20evaluation%20of%0AGPT-4o%20against%20jailbreak%20attacks.%20Specifically%2C%20this%20paper%20adopts%20a%20series%20of%0Amulti-modal%20and%20uni-modal%20jailbreak%20attacks%20on%204%20commonly%20used%20benchmarks%0Aencompassing%20three%20modalities%20%28%5Cie%2C%20text%2C%20speech%2C%20and%20image%29%2C%20which%20involves%0Athe%20optimization%20of%20over%204%2C000%20initial%20text%20queries%20and%20the%20analysis%20and%0Astatistical%20evaluation%20of%20nearly%208%2C000%2B%20response%20on%20GPT-4o.%20Our%20extensive%0Aexperiments%20reveal%20several%20novel%20observations%3A%20%281%29%20In%20contrast%20to%20the%20previous%0Aversion%20%28such%20as%20GPT-4V%29%2C%20GPT-4o%20has%20enhanced%20safety%20in%20the%20context%20of%20text%0Amodality%20jailbreak%3B%20%282%29%20The%20newly%20introduced%20audio%20modality%20opens%20up%20new%20attack%0Avectors%20for%20jailbreak%20attacks%20on%20GPT-4o%3B%20%283%29%20Existing%20black-box%20multimodal%0Ajailbreak%20attack%20methods%20are%20largely%20ineffective%20against%20GPT-4o%20and%20GPT-4V.%0AThese%20findings%20provide%20critical%20insights%20into%20the%20safety%20implications%20of%20GPT-4o%0Aand%20underscore%20the%20need%20for%20robust%20alignment%20guardrails%20in%20large%20models.%20Our%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/NY1024/Jailbreak_GPT4o%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06302v1&entry.124074799=Read"},
{"title": "Guided Diffusion for Fast Inverse Design of Density-based Mechanical\n  Metamaterials", "author": "Yanyan Yang and Lili Wang and Xiaoya Zhai and Kai Chen and Wenming Wu and Yunkai Zhao and Ligang Liu and Xiao-Ming Fu", "abstract": "  Mechanical metamaterial is a synthetic material that can possess\nextraordinary physical characteristics, such as abnormal elasticity, stiffness,\nand stability, by carefully designing its internal structure. To make\nmetamaterials contain delicate local structures with unique mechanical\nproperties, it is a potential method to represent them through high-resolution\nvoxels. However, it brings a substantial computational burden. To this end,\nthis paper proposes a fast inverse design method, whose core is an advanced\ndeep generative AI algorithm, to generate voxel-based mechanical metamaterials.\nSpecifically, we use the self-conditioned diffusion model, capable of\ngenerating a microstructure with a resolution of $128^3$ to approach the\nspecified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid\nreverse design tool facilitates the exploration of extreme metamaterials, the\nsequence interpolation in metamaterials, and the generation of diverse\nmicrostructures for multi-scale design. This flexible and adaptive generative\ntool is of great value in structural engineering or other mechanical systems\nand can stimulate more subsequent research.\n", "link": "http://arxiv.org/abs/2401.13570v2", "date": "2024-06-10", "relevancy": 2.2014, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5561}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Diffusion%20for%20Fast%20Inverse%20Design%20of%20Density-based%20Mechanical%0A%20%20Metamaterials&body=Title%3A%20Guided%20Diffusion%20for%20Fast%20Inverse%20Design%20of%20Density-based%20Mechanical%0A%20%20Metamaterials%0AAuthor%3A%20Yanyan%20Yang%20and%20Lili%20Wang%20and%20Xiaoya%20Zhai%20and%20Kai%20Chen%20and%20Wenming%20Wu%20and%20Yunkai%20Zhao%20and%20Ligang%20Liu%20and%20Xiao-Ming%20Fu%0AAbstract%3A%20%20%20Mechanical%20metamaterial%20is%20a%20synthetic%20material%20that%20can%20possess%0Aextraordinary%20physical%20characteristics%2C%20such%20as%20abnormal%20elasticity%2C%20stiffness%2C%0Aand%20stability%2C%20by%20carefully%20designing%20its%20internal%20structure.%20To%20make%0Ametamaterials%20contain%20delicate%20local%20structures%20with%20unique%20mechanical%0Aproperties%2C%20it%20is%20a%20potential%20method%20to%20represent%20them%20through%20high-resolution%0Avoxels.%20However%2C%20it%20brings%20a%20substantial%20computational%20burden.%20To%20this%20end%2C%0Athis%20paper%20proposes%20a%20fast%20inverse%20design%20method%2C%20whose%20core%20is%20an%20advanced%0Adeep%20generative%20AI%20algorithm%2C%20to%20generate%20voxel-based%20mechanical%20metamaterials.%0ASpecifically%2C%20we%20use%20the%20self-conditioned%20diffusion%20model%2C%20capable%20of%0Agenerating%20a%20microstructure%20with%20a%20resolution%20of%20%24128%5E3%24%20to%20approach%20the%0Aspecified%20homogenized%20tensor%20matrix%20in%20just%203%20seconds.%20Accordingly%2C%20this%20rapid%0Areverse%20design%20tool%20facilitates%20the%20exploration%20of%20extreme%20metamaterials%2C%20the%0Asequence%20interpolation%20in%20metamaterials%2C%20and%20the%20generation%20of%20diverse%0Amicrostructures%20for%20multi-scale%20design.%20This%20flexible%20and%20adaptive%20generative%0Atool%20is%20of%20great%20value%20in%20structural%20engineering%20or%20other%20mechanical%20systems%0Aand%20can%20stimulate%20more%20subsequent%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Diffusion%2520for%2520Fast%2520Inverse%2520Design%2520of%2520Density-based%2520Mechanical%250A%2520%2520Metamaterials%26entry.906535625%3DYanyan%2520Yang%2520and%2520Lili%2520Wang%2520and%2520Xiaoya%2520Zhai%2520and%2520Kai%2520Chen%2520and%2520Wenming%2520Wu%2520and%2520Yunkai%2520Zhao%2520and%2520Ligang%2520Liu%2520and%2520Xiao-Ming%2520Fu%26entry.1292438233%3D%2520%2520Mechanical%2520metamaterial%2520is%2520a%2520synthetic%2520material%2520that%2520can%2520possess%250Aextraordinary%2520physical%2520characteristics%252C%2520such%2520as%2520abnormal%2520elasticity%252C%2520stiffness%252C%250Aand%2520stability%252C%2520by%2520carefully%2520designing%2520its%2520internal%2520structure.%2520To%2520make%250Ametamaterials%2520contain%2520delicate%2520local%2520structures%2520with%2520unique%2520mechanical%250Aproperties%252C%2520it%2520is%2520a%2520potential%2520method%2520to%2520represent%2520them%2520through%2520high-resolution%250Avoxels.%2520However%252C%2520it%2520brings%2520a%2520substantial%2520computational%2520burden.%2520To%2520this%2520end%252C%250Athis%2520paper%2520proposes%2520a%2520fast%2520inverse%2520design%2520method%252C%2520whose%2520core%2520is%2520an%2520advanced%250Adeep%2520generative%2520AI%2520algorithm%252C%2520to%2520generate%2520voxel-based%2520mechanical%2520metamaterials.%250ASpecifically%252C%2520we%2520use%2520the%2520self-conditioned%2520diffusion%2520model%252C%2520capable%2520of%250Agenerating%2520a%2520microstructure%2520with%2520a%2520resolution%2520of%2520%2524128%255E3%2524%2520to%2520approach%2520the%250Aspecified%2520homogenized%2520tensor%2520matrix%2520in%2520just%25203%2520seconds.%2520Accordingly%252C%2520this%2520rapid%250Areverse%2520design%2520tool%2520facilitates%2520the%2520exploration%2520of%2520extreme%2520metamaterials%252C%2520the%250Asequence%2520interpolation%2520in%2520metamaterials%252C%2520and%2520the%2520generation%2520of%2520diverse%250Amicrostructures%2520for%2520multi-scale%2520design.%2520This%2520flexible%2520and%2520adaptive%2520generative%250Atool%2520is%2520of%2520great%2520value%2520in%2520structural%2520engineering%2520or%2520other%2520mechanical%2520systems%250Aand%2520can%2520stimulate%2520more%2520subsequent%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Diffusion%20for%20Fast%20Inverse%20Design%20of%20Density-based%20Mechanical%0A%20%20Metamaterials&entry.906535625=Yanyan%20Yang%20and%20Lili%20Wang%20and%20Xiaoya%20Zhai%20and%20Kai%20Chen%20and%20Wenming%20Wu%20and%20Yunkai%20Zhao%20and%20Ligang%20Liu%20and%20Xiao-Ming%20Fu&entry.1292438233=%20%20Mechanical%20metamaterial%20is%20a%20synthetic%20material%20that%20can%20possess%0Aextraordinary%20physical%20characteristics%2C%20such%20as%20abnormal%20elasticity%2C%20stiffness%2C%0Aand%20stability%2C%20by%20carefully%20designing%20its%20internal%20structure.%20To%20make%0Ametamaterials%20contain%20delicate%20local%20structures%20with%20unique%20mechanical%0Aproperties%2C%20it%20is%20a%20potential%20method%20to%20represent%20them%20through%20high-resolution%0Avoxels.%20However%2C%20it%20brings%20a%20substantial%20computational%20burden.%20To%20this%20end%2C%0Athis%20paper%20proposes%20a%20fast%20inverse%20design%20method%2C%20whose%20core%20is%20an%20advanced%0Adeep%20generative%20AI%20algorithm%2C%20to%20generate%20voxel-based%20mechanical%20metamaterials.%0ASpecifically%2C%20we%20use%20the%20self-conditioned%20diffusion%20model%2C%20capable%20of%0Agenerating%20a%20microstructure%20with%20a%20resolution%20of%20%24128%5E3%24%20to%20approach%20the%0Aspecified%20homogenized%20tensor%20matrix%20in%20just%203%20seconds.%20Accordingly%2C%20this%20rapid%0Areverse%20design%20tool%20facilitates%20the%20exploration%20of%20extreme%20metamaterials%2C%20the%0Asequence%20interpolation%20in%20metamaterials%2C%20and%20the%20generation%20of%20diverse%0Amicrostructures%20for%20multi-scale%20design.%20This%20flexible%20and%20adaptive%20generative%0Atool%20is%20of%20great%20value%20in%20structural%20engineering%20or%20other%20mechanical%20systems%0Aand%20can%20stimulate%20more%20subsequent%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13570v2&entry.124074799=Read"},
{"title": "Improving Deep Learning-based Automatic Cranial Defect Reconstruction by\n  Heavy Data Augmentation: From Image Registration to Latent Diffusion Models", "author": "Marek Wodzinski and Kamil Kwarciak and Mateusz Daniol and Daria Hemmerling", "abstract": "  Modeling and manufacturing of personalized cranial implants are important\nresearch areas that may decrease the waiting time for patients suffering from\ncranial damage. The modeling of personalized implants may be partially\nautomated by the use of deep learning-based methods. However, this task suffers\nfrom difficulties with generalizability into data from previously unseen\ndistributions that make it difficult to use the research outcomes in real\nclinical settings. Due to difficulties with acquiring ground-truth annotations,\ndifferent techniques to improve the heterogeneity of datasets used for training\nthe deep networks have to be considered and introduced. In this work, we\npresent a large-scale study of several augmentation techniques, varying from\nclassical geometric transformations, image registration, variational\nautoencoders, and generative adversarial networks, to the most recent advances\nin latent diffusion models. We show that the use of heavy data augmentation\nsignificantly increases both the quantitative and qualitative outcomes,\nresulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96\nfor the SkullFix datasets. Moreover, we show that the synthetically augmented\nnetwork successfully reconstructs real clinical defects. The work is a\nconsiderable contribution to the field of artificial intelligence in the\nautomatic modeling of personalized cranial implants.\n", "link": "http://arxiv.org/abs/2406.06372v1", "date": "2024-06-10", "relevancy": 2.1955, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5705}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5445}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Deep%20Learning-based%20Automatic%20Cranial%20Defect%20Reconstruction%20by%0A%20%20Heavy%20Data%20Augmentation%3A%20From%20Image%20Registration%20to%20Latent%20Diffusion%20Models&body=Title%3A%20Improving%20Deep%20Learning-based%20Automatic%20Cranial%20Defect%20Reconstruction%20by%0A%20%20Heavy%20Data%20Augmentation%3A%20From%20Image%20Registration%20to%20Latent%20Diffusion%20Models%0AAuthor%3A%20Marek%20Wodzinski%20and%20Kamil%20Kwarciak%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%0AAbstract%3A%20%20%20Modeling%20and%20manufacturing%20of%20personalized%20cranial%20implants%20are%20important%0Aresearch%20areas%20that%20may%20decrease%20the%20waiting%20time%20for%20patients%20suffering%20from%0Acranial%20damage.%20The%20modeling%20of%20personalized%20implants%20may%20be%20partially%0Aautomated%20by%20the%20use%20of%20deep%20learning-based%20methods.%20However%2C%20this%20task%20suffers%0Afrom%20difficulties%20with%20generalizability%20into%20data%20from%20previously%20unseen%0Adistributions%20that%20make%20it%20difficult%20to%20use%20the%20research%20outcomes%20in%20real%0Aclinical%20settings.%20Due%20to%20difficulties%20with%20acquiring%20ground-truth%20annotations%2C%0Adifferent%20techniques%20to%20improve%20the%20heterogeneity%20of%20datasets%20used%20for%20training%0Athe%20deep%20networks%20have%20to%20be%20considered%20and%20introduced.%20In%20this%20work%2C%20we%0Apresent%20a%20large-scale%20study%20of%20several%20augmentation%20techniques%2C%20varying%20from%0Aclassical%20geometric%20transformations%2C%20image%20registration%2C%20variational%0Aautoencoders%2C%20and%20generative%20adversarial%20networks%2C%20to%20the%20most%20recent%20advances%0Ain%20latent%20diffusion%20models.%20We%20show%20that%20the%20use%20of%20heavy%20data%20augmentation%0Asignificantly%20increases%20both%20the%20quantitative%20and%20qualitative%20outcomes%2C%0Aresulting%20in%20an%20average%20Dice%20Score%20above%200.94%20for%20the%20SkullBreak%20and%20above%200.96%0Afor%20the%20SkullFix%20datasets.%20Moreover%2C%20we%20show%20that%20the%20synthetically%20augmented%0Anetwork%20successfully%20reconstructs%20real%20clinical%20defects.%20The%20work%20is%20a%0Aconsiderable%20contribution%20to%20the%20field%20of%20artificial%20intelligence%20in%20the%0Aautomatic%20modeling%20of%20personalized%20cranial%20implants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Deep%2520Learning-based%2520Automatic%2520Cranial%2520Defect%2520Reconstruction%2520by%250A%2520%2520Heavy%2520Data%2520Augmentation%253A%2520From%2520Image%2520Registration%2520to%2520Latent%2520Diffusion%2520Models%26entry.906535625%3DMarek%2520Wodzinski%2520and%2520Kamil%2520Kwarciak%2520and%2520Mateusz%2520Daniol%2520and%2520Daria%2520Hemmerling%26entry.1292438233%3D%2520%2520Modeling%2520and%2520manufacturing%2520of%2520personalized%2520cranial%2520implants%2520are%2520important%250Aresearch%2520areas%2520that%2520may%2520decrease%2520the%2520waiting%2520time%2520for%2520patients%2520suffering%2520from%250Acranial%2520damage.%2520The%2520modeling%2520of%2520personalized%2520implants%2520may%2520be%2520partially%250Aautomated%2520by%2520the%2520use%2520of%2520deep%2520learning-based%2520methods.%2520However%252C%2520this%2520task%2520suffers%250Afrom%2520difficulties%2520with%2520generalizability%2520into%2520data%2520from%2520previously%2520unseen%250Adistributions%2520that%2520make%2520it%2520difficult%2520to%2520use%2520the%2520research%2520outcomes%2520in%2520real%250Aclinical%2520settings.%2520Due%2520to%2520difficulties%2520with%2520acquiring%2520ground-truth%2520annotations%252C%250Adifferent%2520techniques%2520to%2520improve%2520the%2520heterogeneity%2520of%2520datasets%2520used%2520for%2520training%250Athe%2520deep%2520networks%2520have%2520to%2520be%2520considered%2520and%2520introduced.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520large-scale%2520study%2520of%2520several%2520augmentation%2520techniques%252C%2520varying%2520from%250Aclassical%2520geometric%2520transformations%252C%2520image%2520registration%252C%2520variational%250Aautoencoders%252C%2520and%2520generative%2520adversarial%2520networks%252C%2520to%2520the%2520most%2520recent%2520advances%250Ain%2520latent%2520diffusion%2520models.%2520We%2520show%2520that%2520the%2520use%2520of%2520heavy%2520data%2520augmentation%250Asignificantly%2520increases%2520both%2520the%2520quantitative%2520and%2520qualitative%2520outcomes%252C%250Aresulting%2520in%2520an%2520average%2520Dice%2520Score%2520above%25200.94%2520for%2520the%2520SkullBreak%2520and%2520above%25200.96%250Afor%2520the%2520SkullFix%2520datasets.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520synthetically%2520augmented%250Anetwork%2520successfully%2520reconstructs%2520real%2520clinical%2520defects.%2520The%2520work%2520is%2520a%250Aconsiderable%2520contribution%2520to%2520the%2520field%2520of%2520artificial%2520intelligence%2520in%2520the%250Aautomatic%2520modeling%2520of%2520personalized%2520cranial%2520implants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Deep%20Learning-based%20Automatic%20Cranial%20Defect%20Reconstruction%20by%0A%20%20Heavy%20Data%20Augmentation%3A%20From%20Image%20Registration%20to%20Latent%20Diffusion%20Models&entry.906535625=Marek%20Wodzinski%20and%20Kamil%20Kwarciak%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling&entry.1292438233=%20%20Modeling%20and%20manufacturing%20of%20personalized%20cranial%20implants%20are%20important%0Aresearch%20areas%20that%20may%20decrease%20the%20waiting%20time%20for%20patients%20suffering%20from%0Acranial%20damage.%20The%20modeling%20of%20personalized%20implants%20may%20be%20partially%0Aautomated%20by%20the%20use%20of%20deep%20learning-based%20methods.%20However%2C%20this%20task%20suffers%0Afrom%20difficulties%20with%20generalizability%20into%20data%20from%20previously%20unseen%0Adistributions%20that%20make%20it%20difficult%20to%20use%20the%20research%20outcomes%20in%20real%0Aclinical%20settings.%20Due%20to%20difficulties%20with%20acquiring%20ground-truth%20annotations%2C%0Adifferent%20techniques%20to%20improve%20the%20heterogeneity%20of%20datasets%20used%20for%20training%0Athe%20deep%20networks%20have%20to%20be%20considered%20and%20introduced.%20In%20this%20work%2C%20we%0Apresent%20a%20large-scale%20study%20of%20several%20augmentation%20techniques%2C%20varying%20from%0Aclassical%20geometric%20transformations%2C%20image%20registration%2C%20variational%0Aautoencoders%2C%20and%20generative%20adversarial%20networks%2C%20to%20the%20most%20recent%20advances%0Ain%20latent%20diffusion%20models.%20We%20show%20that%20the%20use%20of%20heavy%20data%20augmentation%0Asignificantly%20increases%20both%20the%20quantitative%20and%20qualitative%20outcomes%2C%0Aresulting%20in%20an%20average%20Dice%20Score%20above%200.94%20for%20the%20SkullBreak%20and%20above%200.96%0Afor%20the%20SkullFix%20datasets.%20Moreover%2C%20we%20show%20that%20the%20synthetically%20augmented%0Anetwork%20successfully%20reconstructs%20real%20clinical%20defects.%20The%20work%20is%20a%0Aconsiderable%20contribution%20to%20the%20field%20of%20artificial%20intelligence%20in%20the%0Aautomatic%20modeling%20of%20personalized%20cranial%20implants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06372v1&entry.124074799=Read"},
{"title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples", "author": "Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee", "abstract": "  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n", "link": "http://arxiv.org/abs/2402.13254v3", "date": "2024-06-10", "relevancy": 2.1854, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5455}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&body=Title%3A%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples%0AAuthor%3A%20Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%20To%20facilitate%20future%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20benchmark%2C%0Aand%20checkpoints%20at%20https%3A//countercurate.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterCurate%253A%2520Enhancing%2520Physical%2520and%2520Semantic%2520Visio-Linguistic%250A%2520%2520Compositional%2520Reasoning%2520via%2520Counterfactual%2520Examples%26entry.906535625%3DJianrui%2520Zhang%2520and%2520Mu%2520Cai%2520and%2520Tengyang%2520Xie%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520We%2520propose%2520CounterCurate%252C%2520a%2520framework%2520to%2520comprehensively%2520improve%2520the%250Avisio-linguistic%2520compositional%2520reasoning%2520capability%2520for%2520both%2520contrastive%2520and%250Agenerative%2520multimodal%2520models.%2520In%2520particular%252C%2520we%2520identify%2520two%2520critical%250Aunder-explored%2520problems%253A%2520the%2520neglect%2520of%2520the%2520physically%2520grounded%2520reasoning%250A%2528counting%2520and%2520position%2520understanding%2529%2520and%2520the%2520potential%2520of%2520using%2520highly%2520capable%250Atext%2520and%2520image%2520generation%2520models%2520for%2520semantic%2520counterfactual%2520fine-tuning.%2520Our%250Awork%2520pioneers%2520an%2520approach%2520that%2520addresses%2520these%2520gaps.%2520We%2520first%2520spotlight%2520the%250Anear-chance%2520performance%2520of%2520multimodal%2520models%2520like%2520CLIP%2520and%2520LLaVA%2520in%2520physically%250Agrounded%2520compositional%2520reasoning.%2520We%2520then%2520apply%2520simple%2520data%2520augmentation%2520using%250Agrounded%2520image%2520generation%2520model%2520GLIGEN%2520to%2520generate%2520fine-tuning%2520data%252C%2520resulting%250Ain%2520significant%2520performance%2520improvements%253A%2520%252B33%2525%2520and%2520%252B37%2525%2520for%2520CLIP%2520and%2520LLaVA%252C%250Arespectively%252C%2520on%2520our%2520newly%2520curated%2520Flickr30k-Positions%2520benchmark.%2520Moreover%252C%2520we%250Aexploit%2520the%2520capabilities%2520of%2520high-performing%2520text%2520generation%2520and%2520image%250Ageneration%2520models%252C%2520specifically%2520GPT-4V%2520and%2520DALLE-3%252C%2520to%2520curate%2520challenging%250Asemantic%2520counterfactuals%252C%2520thereby%2520further%2520enhancing%2520compositional%2520reasoning%250Acapabilities%2520on%2520benchmarks%2520such%2520as%2520SugarCrepe%252C%2520where%2520CounterCurate%2520outperforms%250AGPT-4V.%2520To%2520facilitate%2520future%2520research%252C%2520we%2520release%2520our%2520code%252C%2520dataset%252C%2520benchmark%252C%250Aand%2520checkpoints%2520at%2520https%253A//countercurate.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&entry.906535625=Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%20To%20facilitate%20future%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20benchmark%2C%0Aand%20checkpoints%20at%20https%3A//countercurate.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13254v3&entry.124074799=Read"},
{"title": "Leveraging Neural Radiance Fields for Pose Estimation of an Unknown\n  Space Object during Proximity Operations", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  We address the estimation of the 6D pose of an unknown target spacecraft\nrelative to a monocular camera, a key step towards the autonomous rendezvous\nand proximity operations required by future Active Debris Removal missions. We\npresent a novel method that enables an \"off-the-shelf\" spacecraft pose\nestimator, which is supposed to known the target CAD model, to be applied on an\nunknown target. Our method relies on an in-the wild NeRF, i.e., a Neural\nRadiance Field that employs learnable appearance embeddings to represent\nvarying illumination conditions found in natural scenes. We train the NeRF\nmodel using a sparse collection of images that depict the target, and in turn\ngenerate a large dataset that is diverse both in terms of viewpoint and\nillumination. This dataset is then used to train the pose estimation network.\nWe validate our method on the Hardware-In-the-Loop images of SPEED+ that\nemulate lighting conditions close to those encountered on orbit. We demonstrate\nthat our method successfully enables the training of an off-the-shelf\nspacecraft pose estimation network from a sparse set of images. Furthermore, we\nshow that a network trained using our method performs similarly to a model\ntrained on synthetic images generated using the CAD model of the target.\n", "link": "http://arxiv.org/abs/2405.12728v2", "date": "2024-06-10", "relevancy": 2.1851, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations&body=Title%3A%20Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20We%20address%20the%20estimation%20of%20the%206D%20pose%20of%20an%20unknown%20target%20spacecraft%0Arelative%20to%20a%20monocular%20camera%2C%20a%20key%20step%20towards%20the%20autonomous%20rendezvous%0Aand%20proximity%20operations%20required%20by%20future%20Active%20Debris%20Removal%20missions.%20We%0Apresent%20a%20novel%20method%20that%20enables%20an%20%22off-the-shelf%22%20spacecraft%20pose%0Aestimator%2C%20which%20is%20supposed%20to%20known%20the%20target%20CAD%20model%2C%20to%20be%20applied%20on%20an%0Aunknown%20target.%20Our%20method%20relies%20on%20an%20in-the%20wild%20NeRF%2C%20i.e.%2C%20a%20Neural%0ARadiance%20Field%20that%20employs%20learnable%20appearance%20embeddings%20to%20represent%0Avarying%20illumination%20conditions%20found%20in%20natural%20scenes.%20We%20train%20the%20NeRF%0Amodel%20using%20a%20sparse%20collection%20of%20images%20that%20depict%20the%20target%2C%20and%20in%20turn%0Agenerate%20a%20large%20dataset%20that%20is%20diverse%20both%20in%20terms%20of%20viewpoint%20and%0Aillumination.%20This%20dataset%20is%20then%20used%20to%20train%20the%20pose%20estimation%20network.%0AWe%20validate%20our%20method%20on%20the%20Hardware-In-the-Loop%20images%20of%20SPEED%2B%20that%0Aemulate%20lighting%20conditions%20close%20to%20those%20encountered%20on%20orbit.%20We%20demonstrate%0Athat%20our%20method%20successfully%20enables%20the%20training%20of%20an%20off-the-shelf%0Aspacecraft%20pose%20estimation%20network%20from%20a%20sparse%20set%20of%20images.%20Furthermore%2C%20we%0Ashow%20that%20a%20network%20trained%20using%20our%20method%20performs%20similarly%20to%20a%20model%0Atrained%20on%20synthetic%20images%20generated%20using%20the%20CAD%20model%20of%20the%20target.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Neural%2520Radiance%2520Fields%2520for%2520Pose%2520Estimation%2520of%2520an%2520Unknown%250A%2520%2520Space%2520Object%2520during%2520Proximity%2520Operations%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520estimation%2520of%2520the%25206D%2520pose%2520of%2520an%2520unknown%2520target%2520spacecraft%250Arelative%2520to%2520a%2520monocular%2520camera%252C%2520a%2520key%2520step%2520towards%2520the%2520autonomous%2520rendezvous%250Aand%2520proximity%2520operations%2520required%2520by%2520future%2520Active%2520Debris%2520Removal%2520missions.%2520We%250Apresent%2520a%2520novel%2520method%2520that%2520enables%2520an%2520%2522off-the-shelf%2522%2520spacecraft%2520pose%250Aestimator%252C%2520which%2520is%2520supposed%2520to%2520known%2520the%2520target%2520CAD%2520model%252C%2520to%2520be%2520applied%2520on%2520an%250Aunknown%2520target.%2520Our%2520method%2520relies%2520on%2520an%2520in-the%2520wild%2520NeRF%252C%2520i.e.%252C%2520a%2520Neural%250ARadiance%2520Field%2520that%2520employs%2520learnable%2520appearance%2520embeddings%2520to%2520represent%250Avarying%2520illumination%2520conditions%2520found%2520in%2520natural%2520scenes.%2520We%2520train%2520the%2520NeRF%250Amodel%2520using%2520a%2520sparse%2520collection%2520of%2520images%2520that%2520depict%2520the%2520target%252C%2520and%2520in%2520turn%250Agenerate%2520a%2520large%2520dataset%2520that%2520is%2520diverse%2520both%2520in%2520terms%2520of%2520viewpoint%2520and%250Aillumination.%2520This%2520dataset%2520is%2520then%2520used%2520to%2520train%2520the%2520pose%2520estimation%2520network.%250AWe%2520validate%2520our%2520method%2520on%2520the%2520Hardware-In-the-Loop%2520images%2520of%2520SPEED%252B%2520that%250Aemulate%2520lighting%2520conditions%2520close%2520to%2520those%2520encountered%2520on%2520orbit.%2520We%2520demonstrate%250Athat%2520our%2520method%2520successfully%2520enables%2520the%2520training%2520of%2520an%2520off-the-shelf%250Aspacecraft%2520pose%2520estimation%2520network%2520from%2520a%2520sparse%2520set%2520of%2520images.%2520Furthermore%252C%2520we%250Ashow%2520that%2520a%2520network%2520trained%2520using%2520our%2520method%2520performs%2520similarly%2520to%2520a%2520model%250Atrained%2520on%2520synthetic%2520images%2520generated%2520using%2520the%2520CAD%2520model%2520of%2520the%2520target.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20We%20address%20the%20estimation%20of%20the%206D%20pose%20of%20an%20unknown%20target%20spacecraft%0Arelative%20to%20a%20monocular%20camera%2C%20a%20key%20step%20towards%20the%20autonomous%20rendezvous%0Aand%20proximity%20operations%20required%20by%20future%20Active%20Debris%20Removal%20missions.%20We%0Apresent%20a%20novel%20method%20that%20enables%20an%20%22off-the-shelf%22%20spacecraft%20pose%0Aestimator%2C%20which%20is%20supposed%20to%20known%20the%20target%20CAD%20model%2C%20to%20be%20applied%20on%20an%0Aunknown%20target.%20Our%20method%20relies%20on%20an%20in-the%20wild%20NeRF%2C%20i.e.%2C%20a%20Neural%0ARadiance%20Field%20that%20employs%20learnable%20appearance%20embeddings%20to%20represent%0Avarying%20illumination%20conditions%20found%20in%20natural%20scenes.%20We%20train%20the%20NeRF%0Amodel%20using%20a%20sparse%20collection%20of%20images%20that%20depict%20the%20target%2C%20and%20in%20turn%0Agenerate%20a%20large%20dataset%20that%20is%20diverse%20both%20in%20terms%20of%20viewpoint%20and%0Aillumination.%20This%20dataset%20is%20then%20used%20to%20train%20the%20pose%20estimation%20network.%0AWe%20validate%20our%20method%20on%20the%20Hardware-In-the-Loop%20images%20of%20SPEED%2B%20that%0Aemulate%20lighting%20conditions%20close%20to%20those%20encountered%20on%20orbit.%20We%20demonstrate%0Athat%20our%20method%20successfully%20enables%20the%20training%20of%20an%20off-the-shelf%0Aspacecraft%20pose%20estimation%20network%20from%20a%20sparse%20set%20of%20images.%20Furthermore%2C%20we%0Ashow%20that%20a%20network%20trained%20using%20our%20method%20performs%20similarly%20to%20a%20model%0Atrained%20on%20synthetic%20images%20generated%20using%20the%20CAD%20model%20of%20the%20target.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12728v2&entry.124074799=Read"},
{"title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification", "author": "Shan Yang and Yongfei Zhang", "abstract": "  Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of ReID (ReID) has not been\nexplored to date. This paper will investigate how to adapt them for the task of\nReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and\nthen use their visual encoder as a backbone for ReID. However, there still\nexist two apparent issues: (1) Designing instructions for ReID, MLLMs may\noverfit specific instructions, and designing a variety of instructions will\nlead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is\nnot trained synchronously with the ReID task. As a result, the effectiveness of\nthe visual encoder fine-tuning cannot be directly reflected in the performance\nof the ReID task. To address these problems, this paper proposes MLLMReID:\nMultimodal Large Language Model-based ReID. Firstly, we proposed Common\nInstruction, a simple approach that leverages the essence ability of LLMs to\ncontinue writing, avoiding complex and diverse instruction design. Secondly, we\npropose a multi-task learning-based synchronization module to ensure that the\nvisual encoder of the MLLM is trained synchronously with the ReID task. The\nexperimental results demonstrate the superiority of our method.\n", "link": "http://arxiv.org/abs/2401.13201v3", "date": "2024-06-10", "relevancy": 2.1711, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLLMReID%3A%20Multimodal%20Large%20Language%20Model-based%20Person%20Re-identification&body=Title%3A%20MLLMReID%3A%20Multimodal%20Large%20Language%20Model-based%20Person%20Re-identification%0AAuthor%3A%20Shan%20Yang%20and%20Yongfei%20Zhang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLM%29%20have%20achieved%20satisfactory%20results%20in%0Amany%20tasks.%20However%2C%20their%20performance%20in%20the%20task%20of%20ReID%20%28ReID%29%20has%20not%20been%0Aexplored%20to%20date.%20This%20paper%20will%20investigate%20how%20to%20adapt%20them%20for%20the%20task%20of%0AReID.%20An%20intuitive%20idea%20is%20to%20fine-tune%20MLLM%20with%20ReID%20image-text%20datasets%2C%20and%0Athen%20use%20their%20visual%20encoder%20as%20a%20backbone%20for%20ReID.%20However%2C%20there%20still%0Aexist%20two%20apparent%20issues%3A%20%281%29%20Designing%20instructions%20for%20ReID%2C%20MLLMs%20may%0Aoverfit%20specific%20instructions%2C%20and%20designing%20a%20variety%20of%20instructions%20will%0Alead%20to%20higher%20costs.%20%282%29%20When%20fine-tuning%20the%20visual%20encoder%20of%20a%20MLLM%2C%20it%20is%0Anot%20trained%20synchronously%20with%20the%20ReID%20task.%20As%20a%20result%2C%20the%20effectiveness%20of%0Athe%20visual%20encoder%20fine-tuning%20cannot%20be%20directly%20reflected%20in%20the%20performance%0Aof%20the%20ReID%20task.%20To%20address%20these%20problems%2C%20this%20paper%20proposes%20MLLMReID%3A%0AMultimodal%20Large%20Language%20Model-based%20ReID.%20Firstly%2C%20we%20proposed%20Common%0AInstruction%2C%20a%20simple%20approach%20that%20leverages%20the%20essence%20ability%20of%20LLMs%20to%0Acontinue%20writing%2C%20avoiding%20complex%20and%20diverse%20instruction%20design.%20Secondly%2C%20we%0Apropose%20a%20multi-task%20learning-based%20synchronization%20module%20to%20ensure%20that%20the%0Avisual%20encoder%20of%20the%20MLLM%20is%20trained%20synchronously%20with%20the%20ReID%20task.%20The%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13201v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLLMReID%253A%2520Multimodal%2520Large%2520Language%2520Model-based%2520Person%2520Re-identification%26entry.906535625%3DShan%2520Yang%2520and%2520Yongfei%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLM%2529%2520have%2520achieved%2520satisfactory%2520results%2520in%250Amany%2520tasks.%2520However%252C%2520their%2520performance%2520in%2520the%2520task%2520of%2520ReID%2520%2528ReID%2529%2520has%2520not%2520been%250Aexplored%2520to%2520date.%2520This%2520paper%2520will%2520investigate%2520how%2520to%2520adapt%2520them%2520for%2520the%2520task%2520of%250AReID.%2520An%2520intuitive%2520idea%2520is%2520to%2520fine-tune%2520MLLM%2520with%2520ReID%2520image-text%2520datasets%252C%2520and%250Athen%2520use%2520their%2520visual%2520encoder%2520as%2520a%2520backbone%2520for%2520ReID.%2520However%252C%2520there%2520still%250Aexist%2520two%2520apparent%2520issues%253A%2520%25281%2529%2520Designing%2520instructions%2520for%2520ReID%252C%2520MLLMs%2520may%250Aoverfit%2520specific%2520instructions%252C%2520and%2520designing%2520a%2520variety%2520of%2520instructions%2520will%250Alead%2520to%2520higher%2520costs.%2520%25282%2529%2520When%2520fine-tuning%2520the%2520visual%2520encoder%2520of%2520a%2520MLLM%252C%2520it%2520is%250Anot%2520trained%2520synchronously%2520with%2520the%2520ReID%2520task.%2520As%2520a%2520result%252C%2520the%2520effectiveness%2520of%250Athe%2520visual%2520encoder%2520fine-tuning%2520cannot%2520be%2520directly%2520reflected%2520in%2520the%2520performance%250Aof%2520the%2520ReID%2520task.%2520To%2520address%2520these%2520problems%252C%2520this%2520paper%2520proposes%2520MLLMReID%253A%250AMultimodal%2520Large%2520Language%2520Model-based%2520ReID.%2520Firstly%252C%2520we%2520proposed%2520Common%250AInstruction%252C%2520a%2520simple%2520approach%2520that%2520leverages%2520the%2520essence%2520ability%2520of%2520LLMs%2520to%250Acontinue%2520writing%252C%2520avoiding%2520complex%2520and%2520diverse%2520instruction%2520design.%2520Secondly%252C%2520we%250Apropose%2520a%2520multi-task%2520learning-based%2520synchronization%2520module%2520to%2520ensure%2520that%2520the%250Avisual%2520encoder%2520of%2520the%2520MLLM%2520is%2520trained%2520synchronously%2520with%2520the%2520ReID%2520task.%2520The%250Aexperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13201v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLLMReID%3A%20Multimodal%20Large%20Language%20Model-based%20Person%20Re-identification&entry.906535625=Shan%20Yang%20and%20Yongfei%20Zhang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLM%29%20have%20achieved%20satisfactory%20results%20in%0Amany%20tasks.%20However%2C%20their%20performance%20in%20the%20task%20of%20ReID%20%28ReID%29%20has%20not%20been%0Aexplored%20to%20date.%20This%20paper%20will%20investigate%20how%20to%20adapt%20them%20for%20the%20task%20of%0AReID.%20An%20intuitive%20idea%20is%20to%20fine-tune%20MLLM%20with%20ReID%20image-text%20datasets%2C%20and%0Athen%20use%20their%20visual%20encoder%20as%20a%20backbone%20for%20ReID.%20However%2C%20there%20still%0Aexist%20two%20apparent%20issues%3A%20%281%29%20Designing%20instructions%20for%20ReID%2C%20MLLMs%20may%0Aoverfit%20specific%20instructions%2C%20and%20designing%20a%20variety%20of%20instructions%20will%0Alead%20to%20higher%20costs.%20%282%29%20When%20fine-tuning%20the%20visual%20encoder%20of%20a%20MLLM%2C%20it%20is%0Anot%20trained%20synchronously%20with%20the%20ReID%20task.%20As%20a%20result%2C%20the%20effectiveness%20of%0Athe%20visual%20encoder%20fine-tuning%20cannot%20be%20directly%20reflected%20in%20the%20performance%0Aof%20the%20ReID%20task.%20To%20address%20these%20problems%2C%20this%20paper%20proposes%20MLLMReID%3A%0AMultimodal%20Large%20Language%20Model-based%20ReID.%20Firstly%2C%20we%20proposed%20Common%0AInstruction%2C%20a%20simple%20approach%20that%20leverages%20the%20essence%20ability%20of%20LLMs%20to%0Acontinue%20writing%2C%20avoiding%20complex%20and%20diverse%20instruction%20design.%20Secondly%2C%20we%0Apropose%20a%20multi-task%20learning-based%20synchronization%20module%20to%20ensure%20that%20the%0Avisual%20encoder%20of%20the%20MLLM%20is%20trained%20synchronously%20with%20the%20ReID%20task.%20The%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13201v3&entry.124074799=Read"},
{"title": "Self-Correcting Self-Consuming Loops for Generative Model Training", "author": "Nate Gillman and Michael Freeman and Daksh Aggarwal and Chia-Hong Hsu and Calvin Luo and Yonglong Tian and Chen Sun", "abstract": "  As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.\n", "link": "http://arxiv.org/abs/2402.07087v3", "date": "2024-06-10", "relevancy": 2.1698, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5775}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5524}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training&body=Title%3A%20Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training%0AAuthor%3A%20Nate%20Gillman%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Chia-Hong%20Hsu%20and%20Calvin%20Luo%20and%20Yonglong%20Tian%20and%20Chen%20Sun%0AAbstract%3A%20%20%20As%20synthetic%20data%20becomes%20higher%20quality%20and%20proliferates%20on%20the%20internet%2C%0Amachine%20learning%20models%20are%20increasingly%20trained%20on%20a%20mix%20of%20human-%20and%0Amachine-generated%20data.%20Despite%20the%20successful%20stories%20of%20using%20synthetic%20data%0Afor%20representation%20learning%2C%20using%20synthetic%20data%20for%20generative%20model%20training%0Acreates%20%22self-consuming%20loops%22%20which%20may%20lead%20to%20training%20instability%20or%20even%0Acollapse%2C%20unless%20certain%20conditions%20are%20met.%20Our%20paper%20aims%20to%20stabilize%0Aself-consuming%20generative%20model%20training.%20Our%20theoretical%20results%20demonstrate%0Athat%20by%20introducing%20an%20idealized%20correction%20function%2C%20which%20maps%20a%20data%20point%0Ato%20be%20more%20likely%20under%20the%20true%20data%20distribution%2C%20self-consuming%20loops%20can%20be%0Amade%20exponentially%20more%20stable.%20We%20then%20propose%20self-correction%20functions%2C%0Awhich%20rely%20on%20expert%20knowledge%20%28e.g.%20the%20laws%20of%20physics%20programmed%20in%20a%0Asimulator%29%2C%20and%20aim%20to%20approximate%20the%20idealized%20corrector%20automatically%20and%20at%0Ascale.%20We%20empirically%20validate%20the%20effectiveness%20of%20self-correcting%0Aself-consuming%20loops%20on%20the%20challenging%20human%20motion%20synthesis%20task%2C%20and%0Aobserve%20that%20it%20successfully%20avoids%20model%20collapse%2C%20even%20when%20the%20ratio%20of%0Asynthetic%20data%20to%20real%20data%20is%20as%20high%20as%20100%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07087v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Correcting%2520Self-Consuming%2520Loops%2520for%2520Generative%2520Model%2520Training%26entry.906535625%3DNate%2520Gillman%2520and%2520Michael%2520Freeman%2520and%2520Daksh%2520Aggarwal%2520and%2520Chia-Hong%2520Hsu%2520and%2520Calvin%2520Luo%2520and%2520Yonglong%2520Tian%2520and%2520Chen%2520Sun%26entry.1292438233%3D%2520%2520As%2520synthetic%2520data%2520becomes%2520higher%2520quality%2520and%2520proliferates%2520on%2520the%2520internet%252C%250Amachine%2520learning%2520models%2520are%2520increasingly%2520trained%2520on%2520a%2520mix%2520of%2520human-%2520and%250Amachine-generated%2520data.%2520Despite%2520the%2520successful%2520stories%2520of%2520using%2520synthetic%2520data%250Afor%2520representation%2520learning%252C%2520using%2520synthetic%2520data%2520for%2520generative%2520model%2520training%250Acreates%2520%2522self-consuming%2520loops%2522%2520which%2520may%2520lead%2520to%2520training%2520instability%2520or%2520even%250Acollapse%252C%2520unless%2520certain%2520conditions%2520are%2520met.%2520Our%2520paper%2520aims%2520to%2520stabilize%250Aself-consuming%2520generative%2520model%2520training.%2520Our%2520theoretical%2520results%2520demonstrate%250Athat%2520by%2520introducing%2520an%2520idealized%2520correction%2520function%252C%2520which%2520maps%2520a%2520data%2520point%250Ato%2520be%2520more%2520likely%2520under%2520the%2520true%2520data%2520distribution%252C%2520self-consuming%2520loops%2520can%2520be%250Amade%2520exponentially%2520more%2520stable.%2520We%2520then%2520propose%2520self-correction%2520functions%252C%250Awhich%2520rely%2520on%2520expert%2520knowledge%2520%2528e.g.%2520the%2520laws%2520of%2520physics%2520programmed%2520in%2520a%250Asimulator%2529%252C%2520and%2520aim%2520to%2520approximate%2520the%2520idealized%2520corrector%2520automatically%2520and%2520at%250Ascale.%2520We%2520empirically%2520validate%2520the%2520effectiveness%2520of%2520self-correcting%250Aself-consuming%2520loops%2520on%2520the%2520challenging%2520human%2520motion%2520synthesis%2520task%252C%2520and%250Aobserve%2520that%2520it%2520successfully%2520avoids%2520model%2520collapse%252C%2520even%2520when%2520the%2520ratio%2520of%250Asynthetic%2520data%2520to%2520real%2520data%2520is%2520as%2520high%2520as%2520100%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07087v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Correcting%20Self-Consuming%20Loops%20for%20Generative%20Model%20Training&entry.906535625=Nate%20Gillman%20and%20Michael%20Freeman%20and%20Daksh%20Aggarwal%20and%20Chia-Hong%20Hsu%20and%20Calvin%20Luo%20and%20Yonglong%20Tian%20and%20Chen%20Sun&entry.1292438233=%20%20As%20synthetic%20data%20becomes%20higher%20quality%20and%20proliferates%20on%20the%20internet%2C%0Amachine%20learning%20models%20are%20increasingly%20trained%20on%20a%20mix%20of%20human-%20and%0Amachine-generated%20data.%20Despite%20the%20successful%20stories%20of%20using%20synthetic%20data%0Afor%20representation%20learning%2C%20using%20synthetic%20data%20for%20generative%20model%20training%0Acreates%20%22self-consuming%20loops%22%20which%20may%20lead%20to%20training%20instability%20or%20even%0Acollapse%2C%20unless%20certain%20conditions%20are%20met.%20Our%20paper%20aims%20to%20stabilize%0Aself-consuming%20generative%20model%20training.%20Our%20theoretical%20results%20demonstrate%0Athat%20by%20introducing%20an%20idealized%20correction%20function%2C%20which%20maps%20a%20data%20point%0Ato%20be%20more%20likely%20under%20the%20true%20data%20distribution%2C%20self-consuming%20loops%20can%20be%0Amade%20exponentially%20more%20stable.%20We%20then%20propose%20self-correction%20functions%2C%0Awhich%20rely%20on%20expert%20knowledge%20%28e.g.%20the%20laws%20of%20physics%20programmed%20in%20a%0Asimulator%29%2C%20and%20aim%20to%20approximate%20the%20idealized%20corrector%20automatically%20and%20at%0Ascale.%20We%20empirically%20validate%20the%20effectiveness%20of%20self-correcting%0Aself-consuming%20loops%20on%20the%20challenging%20human%20motion%20synthesis%20task%2C%20and%0Aobserve%20that%20it%20successfully%20avoids%20model%20collapse%2C%20even%20when%20the%20ratio%20of%0Asynthetic%20data%20to%20real%20data%20is%20as%20high%20as%20100%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07087v3&entry.124074799=Read"},
{"title": "An Analysis of Driver-Initiated Takeovers during Assisted Driving and\n  their Effect on Driver Satisfaction", "author": "Robin Schwager and Michael Grimm and Xin Liu and Lukas Ewecker and Tim Bruehl and Tin Stribor Sohn and Soeren Hohmann", "abstract": "  During the use of Advanced Driver Assistance Systems (ADAS), drivers can\nintervene in the active function and take back control due to various reasons.\nHowever, the specific reasons for driver-initiated takeovers in naturalistic\ndriving are still not well understood. In order to get more information on the\nreasons behind these takeovers, a test group study was conducted. There, 17\nparticipants used a predictive longitudinal driving function for their daily\ncommutes and annotated the reasons for their takeovers during active function\nuse. In this paper, the recorded takeovers are analyzed and the different\nreasons for them are highlighted. The results show that the reasons can be\ndivided into three main categories. The most common category consists of\ntakeovers which aim to adjust the behavior of the ADAS within its Operational\nDesign Domain (ODD) in order to better match the drivers' personal preferences.\nOther reasons include takeovers due to leaving the ADAS's ODD and corrections\nof incorrect sensing state information. Using the questionnaire results of the\ntest group study, it was found that the number and frequency of takeovers\nespecially within the ADAS's ODD have a significant negative impact on driver\nsatisfaction. Therefore, the driver satisfaction with the ADAS could be\nincreased by adapting its behavior to the drivers' wishes and thereby lowering\nthe number of takeovers within the ODD. The information contained in the\ntakeover behavior of the drivers could be used as feedback for the ADAS.\nFinally, it is shown that there are considerable differences in the takeover\nbehavior of different drivers, which shows a need for ADAS individualization.\n", "link": "http://arxiv.org/abs/2404.13027v2", "date": "2024-06-10", "relevancy": 2.1659, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4603}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4348}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction&body=Title%3A%20An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction%0AAuthor%3A%20Robin%20Schwager%20and%20Michael%20Grimm%20and%20Xin%20Liu%20and%20Lukas%20Ewecker%20and%20Tim%20Bruehl%20and%20Tin%20Stribor%20Sohn%20and%20Soeren%20Hohmann%0AAbstract%3A%20%20%20During%20the%20use%20of%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20drivers%20can%0Aintervene%20in%20the%20active%20function%20and%20take%20back%20control%20due%20to%20various%20reasons.%0AHowever%2C%20the%20specific%20reasons%20for%20driver-initiated%20takeovers%20in%20naturalistic%0Adriving%20are%20still%20not%20well%20understood.%20In%20order%20to%20get%20more%20information%20on%20the%0Areasons%20behind%20these%20takeovers%2C%20a%20test%20group%20study%20was%20conducted.%20There%2C%2017%0Aparticipants%20used%20a%20predictive%20longitudinal%20driving%20function%20for%20their%20daily%0Acommutes%20and%20annotated%20the%20reasons%20for%20their%20takeovers%20during%20active%20function%0Ause.%20In%20this%20paper%2C%20the%20recorded%20takeovers%20are%20analyzed%20and%20the%20different%0Areasons%20for%20them%20are%20highlighted.%20The%20results%20show%20that%20the%20reasons%20can%20be%0Adivided%20into%20three%20main%20categories.%20The%20most%20common%20category%20consists%20of%0Atakeovers%20which%20aim%20to%20adjust%20the%20behavior%20of%20the%20ADAS%20within%20its%20Operational%0ADesign%20Domain%20%28ODD%29%20in%20order%20to%20better%20match%20the%20drivers%27%20personal%20preferences.%0AOther%20reasons%20include%20takeovers%20due%20to%20leaving%20the%20ADAS%27s%20ODD%20and%20corrections%0Aof%20incorrect%20sensing%20state%20information.%20Using%20the%20questionnaire%20results%20of%20the%0Atest%20group%20study%2C%20it%20was%20found%20that%20the%20number%20and%20frequency%20of%20takeovers%0Aespecially%20within%20the%20ADAS%27s%20ODD%20have%20a%20significant%20negative%20impact%20on%20driver%0Asatisfaction.%20Therefore%2C%20the%20driver%20satisfaction%20with%20the%20ADAS%20could%20be%0Aincreased%20by%20adapting%20its%20behavior%20to%20the%20drivers%27%20wishes%20and%20thereby%20lowering%0Athe%20number%20of%20takeovers%20within%20the%20ODD.%20The%20information%20contained%20in%20the%0Atakeover%20behavior%20of%20the%20drivers%20could%20be%20used%20as%20feedback%20for%20the%20ADAS.%0AFinally%2C%20it%20is%20shown%20that%20there%20are%20considerable%20differences%20in%20the%20takeover%0Abehavior%20of%20different%20drivers%2C%20which%20shows%20a%20need%20for%20ADAS%20individualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Analysis%2520of%2520Driver-Initiated%2520Takeovers%2520during%2520Assisted%2520Driving%2520and%250A%2520%2520their%2520Effect%2520on%2520Driver%2520Satisfaction%26entry.906535625%3DRobin%2520Schwager%2520and%2520Michael%2520Grimm%2520and%2520Xin%2520Liu%2520and%2520Lukas%2520Ewecker%2520and%2520Tim%2520Bruehl%2520and%2520Tin%2520Stribor%2520Sohn%2520and%2520Soeren%2520Hohmann%26entry.1292438233%3D%2520%2520During%2520the%2520use%2520of%2520Advanced%2520Driver%2520Assistance%2520Systems%2520%2528ADAS%2529%252C%2520drivers%2520can%250Aintervene%2520in%2520the%2520active%2520function%2520and%2520take%2520back%2520control%2520due%2520to%2520various%2520reasons.%250AHowever%252C%2520the%2520specific%2520reasons%2520for%2520driver-initiated%2520takeovers%2520in%2520naturalistic%250Adriving%2520are%2520still%2520not%2520well%2520understood.%2520In%2520order%2520to%2520get%2520more%2520information%2520on%2520the%250Areasons%2520behind%2520these%2520takeovers%252C%2520a%2520test%2520group%2520study%2520was%2520conducted.%2520There%252C%252017%250Aparticipants%2520used%2520a%2520predictive%2520longitudinal%2520driving%2520function%2520for%2520their%2520daily%250Acommutes%2520and%2520annotated%2520the%2520reasons%2520for%2520their%2520takeovers%2520during%2520active%2520function%250Ause.%2520In%2520this%2520paper%252C%2520the%2520recorded%2520takeovers%2520are%2520analyzed%2520and%2520the%2520different%250Areasons%2520for%2520them%2520are%2520highlighted.%2520The%2520results%2520show%2520that%2520the%2520reasons%2520can%2520be%250Adivided%2520into%2520three%2520main%2520categories.%2520The%2520most%2520common%2520category%2520consists%2520of%250Atakeovers%2520which%2520aim%2520to%2520adjust%2520the%2520behavior%2520of%2520the%2520ADAS%2520within%2520its%2520Operational%250ADesign%2520Domain%2520%2528ODD%2529%2520in%2520order%2520to%2520better%2520match%2520the%2520drivers%2527%2520personal%2520preferences.%250AOther%2520reasons%2520include%2520takeovers%2520due%2520to%2520leaving%2520the%2520ADAS%2527s%2520ODD%2520and%2520corrections%250Aof%2520incorrect%2520sensing%2520state%2520information.%2520Using%2520the%2520questionnaire%2520results%2520of%2520the%250Atest%2520group%2520study%252C%2520it%2520was%2520found%2520that%2520the%2520number%2520and%2520frequency%2520of%2520takeovers%250Aespecially%2520within%2520the%2520ADAS%2527s%2520ODD%2520have%2520a%2520significant%2520negative%2520impact%2520on%2520driver%250Asatisfaction.%2520Therefore%252C%2520the%2520driver%2520satisfaction%2520with%2520the%2520ADAS%2520could%2520be%250Aincreased%2520by%2520adapting%2520its%2520behavior%2520to%2520the%2520drivers%2527%2520wishes%2520and%2520thereby%2520lowering%250Athe%2520number%2520of%2520takeovers%2520within%2520the%2520ODD.%2520The%2520information%2520contained%2520in%2520the%250Atakeover%2520behavior%2520of%2520the%2520drivers%2520could%2520be%2520used%2520as%2520feedback%2520for%2520the%2520ADAS.%250AFinally%252C%2520it%2520is%2520shown%2520that%2520there%2520are%2520considerable%2520differences%2520in%2520the%2520takeover%250Abehavior%2520of%2520different%2520drivers%252C%2520which%2520shows%2520a%2520need%2520for%2520ADAS%2520individualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction&entry.906535625=Robin%20Schwager%20and%20Michael%20Grimm%20and%20Xin%20Liu%20and%20Lukas%20Ewecker%20and%20Tim%20Bruehl%20and%20Tin%20Stribor%20Sohn%20and%20Soeren%20Hohmann&entry.1292438233=%20%20During%20the%20use%20of%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20drivers%20can%0Aintervene%20in%20the%20active%20function%20and%20take%20back%20control%20due%20to%20various%20reasons.%0AHowever%2C%20the%20specific%20reasons%20for%20driver-initiated%20takeovers%20in%20naturalistic%0Adriving%20are%20still%20not%20well%20understood.%20In%20order%20to%20get%20more%20information%20on%20the%0Areasons%20behind%20these%20takeovers%2C%20a%20test%20group%20study%20was%20conducted.%20There%2C%2017%0Aparticipants%20used%20a%20predictive%20longitudinal%20driving%20function%20for%20their%20daily%0Acommutes%20and%20annotated%20the%20reasons%20for%20their%20takeovers%20during%20active%20function%0Ause.%20In%20this%20paper%2C%20the%20recorded%20takeovers%20are%20analyzed%20and%20the%20different%0Areasons%20for%20them%20are%20highlighted.%20The%20results%20show%20that%20the%20reasons%20can%20be%0Adivided%20into%20three%20main%20categories.%20The%20most%20common%20category%20consists%20of%0Atakeovers%20which%20aim%20to%20adjust%20the%20behavior%20of%20the%20ADAS%20within%20its%20Operational%0ADesign%20Domain%20%28ODD%29%20in%20order%20to%20better%20match%20the%20drivers%27%20personal%20preferences.%0AOther%20reasons%20include%20takeovers%20due%20to%20leaving%20the%20ADAS%27s%20ODD%20and%20corrections%0Aof%20incorrect%20sensing%20state%20information.%20Using%20the%20questionnaire%20results%20of%20the%0Atest%20group%20study%2C%20it%20was%20found%20that%20the%20number%20and%20frequency%20of%20takeovers%0Aespecially%20within%20the%20ADAS%27s%20ODD%20have%20a%20significant%20negative%20impact%20on%20driver%0Asatisfaction.%20Therefore%2C%20the%20driver%20satisfaction%20with%20the%20ADAS%20could%20be%0Aincreased%20by%20adapting%20its%20behavior%20to%20the%20drivers%27%20wishes%20and%20thereby%20lowering%0Athe%20number%20of%20takeovers%20within%20the%20ODD.%20The%20information%20contained%20in%20the%0Atakeover%20behavior%20of%20the%20drivers%20could%20be%20used%20as%20feedback%20for%20the%20ADAS.%0AFinally%2C%20it%20is%20shown%20that%20there%20are%20considerable%20differences%20in%20the%20takeover%0Abehavior%20of%20different%20drivers%2C%20which%20shows%20a%20need%20for%20ADAS%20individualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13027v2&entry.124074799=Read"},
{"title": "VCR: Visual Caption Restoration", "author": "Tianyu Zhang and Suyuchen Wang and Lu Li and Ge Zhang and Perouz Taslakian and Sai Rajeswar and Jie Fu and Bang Liu and Yoshua Bengio", "abstract": "  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n", "link": "http://arxiv.org/abs/2406.06462v1", "date": "2024-06-10", "relevancy": 2.1465, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5703}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5302}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCR%3A%20Visual%20Caption%20Restoration&body=Title%3A%20VCR%3A%20Visual%20Caption%20Restoration%0AAuthor%3A%20Tianyu%20Zhang%20and%20Suyuchen%20Wang%20and%20Lu%20Li%20and%20Ge%20Zhang%20and%20Perouz%20Taslakian%20and%20Sai%20Rajeswar%20and%20Jie%20Fu%20and%20Bang%20Liu%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20We%20introduce%20Visual%20Caption%20Restoration%20%28VCR%29%2C%20a%20novel%20vision-language%20task%0Athat%20challenges%20models%20to%20accurately%20restore%20partially%20obscured%20texts%20using%0Apixel-level%20hints%20within%20images.%20This%20task%20stems%20from%20the%20observation%20that%20text%0Aembedded%20in%20images%20is%20intrinsically%20different%20from%20common%20visual%20elements%20and%0Anatural%20language%20due%20to%20the%20need%20to%20align%20the%20modalities%20of%20vision%2C%20text%2C%20and%0Atext%20embedded%20in%20images.%20While%20numerous%20works%20have%20integrated%20text%20embedded%20in%0Aimages%20into%20visual%20question-answering%20tasks%2C%20approaches%20to%20these%20tasks%0Agenerally%20rely%20on%20optical%20character%20recognition%20or%20masked%20language%20modeling%2C%0Athus%20reducing%20the%20task%20to%20mainly%20text-based%20processing.%20However%2C%20text-based%0Aprocessing%20becomes%20ineffective%20in%20VCR%20as%20accurate%20text%20restoration%20depends%20on%0Athe%20combined%20information%20from%20provided%20images%2C%20context%2C%20and%20subtle%20cues%20from%0Athe%20tiny%20exposed%20areas%20of%20masked%20texts.%20We%20develop%20a%20pipeline%20to%20generate%0Asynthetic%20images%20for%20the%20VCR%20task%20using%20image-caption%20pairs%2C%20with%20adjustable%0Acaption%20visibility%20to%20control%20the%20task%20difficulty.%20With%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20for%20VCR%20called%20VCR-Wiki%20using%20images%20with%20captions%20from%0AWikipedia%2C%20comprising%202.11M%20English%20and%20346K%20Chinese%20entities%20in%20both%20easy%20and%0Ahard%20split%20variants.%20Our%20results%20reveal%20that%20current%20vision%20language%20models%0Asignificantly%20lag%20behind%20human%20performance%20in%20the%20VCR%20task%2C%20and%20merely%0Afine-tuning%20the%20models%20on%20our%20dataset%20does%20not%20lead%20to%20notable%20improvements.%20We%0Arelease%20VCR-Wiki%20and%20the%20data%20construction%20code%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCR%253A%2520Visual%2520Caption%2520Restoration%26entry.906535625%3DTianyu%2520Zhang%2520and%2520Suyuchen%2520Wang%2520and%2520Lu%2520Li%2520and%2520Ge%2520Zhang%2520and%2520Perouz%2520Taslakian%2520and%2520Sai%2520Rajeswar%2520and%2520Jie%2520Fu%2520and%2520Bang%2520Liu%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3D%2520%2520We%2520introduce%2520Visual%2520Caption%2520Restoration%2520%2528VCR%2529%252C%2520a%2520novel%2520vision-language%2520task%250Athat%2520challenges%2520models%2520to%2520accurately%2520restore%2520partially%2520obscured%2520texts%2520using%250Apixel-level%2520hints%2520within%2520images.%2520This%2520task%2520stems%2520from%2520the%2520observation%2520that%2520text%250Aembedded%2520in%2520images%2520is%2520intrinsically%2520different%2520from%2520common%2520visual%2520elements%2520and%250Anatural%2520language%2520due%2520to%2520the%2520need%2520to%2520align%2520the%2520modalities%2520of%2520vision%252C%2520text%252C%2520and%250Atext%2520embedded%2520in%2520images.%2520While%2520numerous%2520works%2520have%2520integrated%2520text%2520embedded%2520in%250Aimages%2520into%2520visual%2520question-answering%2520tasks%252C%2520approaches%2520to%2520these%2520tasks%250Agenerally%2520rely%2520on%2520optical%2520character%2520recognition%2520or%2520masked%2520language%2520modeling%252C%250Athus%2520reducing%2520the%2520task%2520to%2520mainly%2520text-based%2520processing.%2520However%252C%2520text-based%250Aprocessing%2520becomes%2520ineffective%2520in%2520VCR%2520as%2520accurate%2520text%2520restoration%2520depends%2520on%250Athe%2520combined%2520information%2520from%2520provided%2520images%252C%2520context%252C%2520and%2520subtle%2520cues%2520from%250Athe%2520tiny%2520exposed%2520areas%2520of%2520masked%2520texts.%2520We%2520develop%2520a%2520pipeline%2520to%2520generate%250Asynthetic%2520images%2520for%2520the%2520VCR%2520task%2520using%2520image-caption%2520pairs%252C%2520with%2520adjustable%250Acaption%2520visibility%2520to%2520control%2520the%2520task%2520difficulty.%2520With%2520this%2520pipeline%252C%2520we%250Aconstruct%2520a%2520dataset%2520for%2520VCR%2520called%2520VCR-Wiki%2520using%2520images%2520with%2520captions%2520from%250AWikipedia%252C%2520comprising%25202.11M%2520English%2520and%2520346K%2520Chinese%2520entities%2520in%2520both%2520easy%2520and%250Ahard%2520split%2520variants.%2520Our%2520results%2520reveal%2520that%2520current%2520vision%2520language%2520models%250Asignificantly%2520lag%2520behind%2520human%2520performance%2520in%2520the%2520VCR%2520task%252C%2520and%2520merely%250Afine-tuning%2520the%2520models%2520on%2520our%2520dataset%2520does%2520not%2520lead%2520to%2520notable%2520improvements.%2520We%250Arelease%2520VCR-Wiki%2520and%2520the%2520data%2520construction%2520code%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCR%3A%20Visual%20Caption%20Restoration&entry.906535625=Tianyu%20Zhang%20and%20Suyuchen%20Wang%20and%20Lu%20Li%20and%20Ge%20Zhang%20and%20Perouz%20Taslakian%20and%20Sai%20Rajeswar%20and%20Jie%20Fu%20and%20Bang%20Liu%20and%20Yoshua%20Bengio&entry.1292438233=%20%20We%20introduce%20Visual%20Caption%20Restoration%20%28VCR%29%2C%20a%20novel%20vision-language%20task%0Athat%20challenges%20models%20to%20accurately%20restore%20partially%20obscured%20texts%20using%0Apixel-level%20hints%20within%20images.%20This%20task%20stems%20from%20the%20observation%20that%20text%0Aembedded%20in%20images%20is%20intrinsically%20different%20from%20common%20visual%20elements%20and%0Anatural%20language%20due%20to%20the%20need%20to%20align%20the%20modalities%20of%20vision%2C%20text%2C%20and%0Atext%20embedded%20in%20images.%20While%20numerous%20works%20have%20integrated%20text%20embedded%20in%0Aimages%20into%20visual%20question-answering%20tasks%2C%20approaches%20to%20these%20tasks%0Agenerally%20rely%20on%20optical%20character%20recognition%20or%20masked%20language%20modeling%2C%0Athus%20reducing%20the%20task%20to%20mainly%20text-based%20processing.%20However%2C%20text-based%0Aprocessing%20becomes%20ineffective%20in%20VCR%20as%20accurate%20text%20restoration%20depends%20on%0Athe%20combined%20information%20from%20provided%20images%2C%20context%2C%20and%20subtle%20cues%20from%0Athe%20tiny%20exposed%20areas%20of%20masked%20texts.%20We%20develop%20a%20pipeline%20to%20generate%0Asynthetic%20images%20for%20the%20VCR%20task%20using%20image-caption%20pairs%2C%20with%20adjustable%0Acaption%20visibility%20to%20control%20the%20task%20difficulty.%20With%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20for%20VCR%20called%20VCR-Wiki%20using%20images%20with%20captions%20from%0AWikipedia%2C%20comprising%202.11M%20English%20and%20346K%20Chinese%20entities%20in%20both%20easy%20and%0Ahard%20split%20variants.%20Our%20results%20reveal%20that%20current%20vision%20language%20models%0Asignificantly%20lag%20behind%20human%20performance%20in%20the%20VCR%20task%2C%20and%20merely%0Afine-tuning%20the%20models%20on%20our%20dataset%20does%20not%20lead%20to%20notable%20improvements.%20We%0Arelease%20VCR-Wiki%20and%20the%20data%20construction%20code%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06462v1&entry.124074799=Read"},
{"title": "Beyond Strong labels: Weakly-supervised Learning Based on Gaussian\n  Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in\n  Non-contrast CTs", "author": "Qixiang Ma and Antoine \u0141ucas and Huazhong Shu and Adrien Kaladji and Pascal Haigron", "abstract": "  Deep-learning-based automated segmentation of vascular structures in\npreoperative CT scans contributes to computer-assisted diagnosis and\nintervention procedure in vascular diseases. While CT angiography (CTA) is the\ncommon standard, non-contrast CT imaging is significant as a contrast-risk-free\nalternative, avoiding complications associated with contrast agents. However,\nthe challenges of labor-intensive labeling and high labeling variability due to\nthe ambiguity of vascular boundaries hinder conventional strong-label-based,\nfully-supervised learning in non-contrast CTs. This paper introduces a\nweakly-supervised framework using ellipses' topology in slices, including 1) an\nefficient annotation process based on predefined standards, 2) ellipse-fitting\nprocessing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels,\n4) a training process through a combination of voxel reconstruction loss and\ndistribution loss with the pseudo labels. We assess the effectiveness of the\nproposed method on one local and two public datasets comprising non-contrast CT\nscans, particularly focusing on the abdominal aorta. On the local dataset, our\nweakly-supervised learning approach based on pseudo labels outperforms\nstrong-label-based fully-supervised learning (1.54\\% of Dice score on average),\nreducing labeling time by around 82.0\\%. The efficiency in generating pseudo\nlabels allows the inclusion of label-agnostic external data in the training\nset, leading to an additional improvement in performance (2.74\\% of Dice score\non average) with a reduction of 66.3\\% labeling time, where the labeling time\nremains considerably less than that of strong labels. On the public dataset,\nthe pseudo labels achieve an overall improvement of 1.95\\% in Dice score for 2D\nmodels while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D\nmodel.\n", "link": "http://arxiv.org/abs/2402.03492v2", "date": "2024-06-10", "relevancy": 2.1211, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5468}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5297}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Strong%20labels%3A%20Weakly-supervised%20Learning%20Based%20on%20Gaussian%0A%20%20Pseudo%20Labels%20for%20The%20Segmentation%20of%20Ellipse-like%20Vascular%20Structures%20in%0A%20%20Non-contrast%20CTs&body=Title%3A%20Beyond%20Strong%20labels%3A%20Weakly-supervised%20Learning%20Based%20on%20Gaussian%0A%20%20Pseudo%20Labels%20for%20The%20Segmentation%20of%20Ellipse-like%20Vascular%20Structures%20in%0A%20%20Non-contrast%20CTs%0AAuthor%3A%20Qixiang%20Ma%20and%20Antoine%20%C5%81ucas%20and%20Huazhong%20Shu%20and%20Adrien%20Kaladji%20and%20Pascal%20Haigron%0AAbstract%3A%20%20%20Deep-learning-based%20automated%20segmentation%20of%20vascular%20structures%20in%0Apreoperative%20CT%20scans%20contributes%20to%20computer-assisted%20diagnosis%20and%0Aintervention%20procedure%20in%20vascular%20diseases.%20While%20CT%20angiography%20%28CTA%29%20is%20the%0Acommon%20standard%2C%20non-contrast%20CT%20imaging%20is%20significant%20as%20a%20contrast-risk-free%0Aalternative%2C%20avoiding%20complications%20associated%20with%20contrast%20agents.%20However%2C%0Athe%20challenges%20of%20labor-intensive%20labeling%20and%20high%20labeling%20variability%20due%20to%0Athe%20ambiguity%20of%20vascular%20boundaries%20hinder%20conventional%20strong-label-based%2C%0Afully-supervised%20learning%20in%20non-contrast%20CTs.%20This%20paper%20introduces%20a%0Aweakly-supervised%20framework%20using%20ellipses%27%20topology%20in%20slices%2C%20including%201%29%20an%0Aefficient%20annotation%20process%20based%20on%20predefined%20standards%2C%202%29%20ellipse-fitting%0Aprocessing%2C%203%29%20the%20generation%20of%202D%20Gaussian%20heatmaps%20serving%20as%20pseudo%20labels%2C%0A4%29%20a%20training%20process%20through%20a%20combination%20of%20voxel%20reconstruction%20loss%20and%0Adistribution%20loss%20with%20the%20pseudo%20labels.%20We%20assess%20the%20effectiveness%20of%20the%0Aproposed%20method%20on%20one%20local%20and%20two%20public%20datasets%20comprising%20non-contrast%20CT%0Ascans%2C%20particularly%20focusing%20on%20the%20abdominal%20aorta.%20On%20the%20local%20dataset%2C%20our%0Aweakly-supervised%20learning%20approach%20based%20on%20pseudo%20labels%20outperforms%0Astrong-label-based%20fully-supervised%20learning%20%281.54%5C%25%20of%20Dice%20score%20on%20average%29%2C%0Areducing%20labeling%20time%20by%20around%2082.0%5C%25.%20The%20efficiency%20in%20generating%20pseudo%0Alabels%20allows%20the%20inclusion%20of%20label-agnostic%20external%20data%20in%20the%20training%0Aset%2C%20leading%20to%20an%20additional%20improvement%20in%20performance%20%282.74%5C%25%20of%20Dice%20score%0Aon%20average%29%20with%20a%20reduction%20of%2066.3%5C%25%20labeling%20time%2C%20where%20the%20labeling%20time%0Aremains%20considerably%20less%20than%20that%20of%20strong%20labels.%20On%20the%20public%20dataset%2C%0Athe%20pseudo%20labels%20achieve%20an%20overall%20improvement%20of%201.95%5C%25%20in%20Dice%20score%20for%202D%0Amodels%20while%20a%20reduction%20of%2011.65%20voxel%20spacing%20in%20Hausdorff%20distance%20for%203D%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Strong%2520labels%253A%2520Weakly-supervised%2520Learning%2520Based%2520on%2520Gaussian%250A%2520%2520Pseudo%2520Labels%2520for%2520The%2520Segmentation%2520of%2520Ellipse-like%2520Vascular%2520Structures%2520in%250A%2520%2520Non-contrast%2520CTs%26entry.906535625%3DQixiang%2520Ma%2520and%2520Antoine%2520%25C5%2581ucas%2520and%2520Huazhong%2520Shu%2520and%2520Adrien%2520Kaladji%2520and%2520Pascal%2520Haigron%26entry.1292438233%3D%2520%2520Deep-learning-based%2520automated%2520segmentation%2520of%2520vascular%2520structures%2520in%250Apreoperative%2520CT%2520scans%2520contributes%2520to%2520computer-assisted%2520diagnosis%2520and%250Aintervention%2520procedure%2520in%2520vascular%2520diseases.%2520While%2520CT%2520angiography%2520%2528CTA%2529%2520is%2520the%250Acommon%2520standard%252C%2520non-contrast%2520CT%2520imaging%2520is%2520significant%2520as%2520a%2520contrast-risk-free%250Aalternative%252C%2520avoiding%2520complications%2520associated%2520with%2520contrast%2520agents.%2520However%252C%250Athe%2520challenges%2520of%2520labor-intensive%2520labeling%2520and%2520high%2520labeling%2520variability%2520due%2520to%250Athe%2520ambiguity%2520of%2520vascular%2520boundaries%2520hinder%2520conventional%2520strong-label-based%252C%250Afully-supervised%2520learning%2520in%2520non-contrast%2520CTs.%2520This%2520paper%2520introduces%2520a%250Aweakly-supervised%2520framework%2520using%2520ellipses%2527%2520topology%2520in%2520slices%252C%2520including%25201%2529%2520an%250Aefficient%2520annotation%2520process%2520based%2520on%2520predefined%2520standards%252C%25202%2529%2520ellipse-fitting%250Aprocessing%252C%25203%2529%2520the%2520generation%2520of%25202D%2520Gaussian%2520heatmaps%2520serving%2520as%2520pseudo%2520labels%252C%250A4%2529%2520a%2520training%2520process%2520through%2520a%2520combination%2520of%2520voxel%2520reconstruction%2520loss%2520and%250Adistribution%2520loss%2520with%2520the%2520pseudo%2520labels.%2520We%2520assess%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520on%2520one%2520local%2520and%2520two%2520public%2520datasets%2520comprising%2520non-contrast%2520CT%250Ascans%252C%2520particularly%2520focusing%2520on%2520the%2520abdominal%2520aorta.%2520On%2520the%2520local%2520dataset%252C%2520our%250Aweakly-supervised%2520learning%2520approach%2520based%2520on%2520pseudo%2520labels%2520outperforms%250Astrong-label-based%2520fully-supervised%2520learning%2520%25281.54%255C%2525%2520of%2520Dice%2520score%2520on%2520average%2529%252C%250Areducing%2520labeling%2520time%2520by%2520around%252082.0%255C%2525.%2520The%2520efficiency%2520in%2520generating%2520pseudo%250Alabels%2520allows%2520the%2520inclusion%2520of%2520label-agnostic%2520external%2520data%2520in%2520the%2520training%250Aset%252C%2520leading%2520to%2520an%2520additional%2520improvement%2520in%2520performance%2520%25282.74%255C%2525%2520of%2520Dice%2520score%250Aon%2520average%2529%2520with%2520a%2520reduction%2520of%252066.3%255C%2525%2520labeling%2520time%252C%2520where%2520the%2520labeling%2520time%250Aremains%2520considerably%2520less%2520than%2520that%2520of%2520strong%2520labels.%2520On%2520the%2520public%2520dataset%252C%250Athe%2520pseudo%2520labels%2520achieve%2520an%2520overall%2520improvement%2520of%25201.95%255C%2525%2520in%2520Dice%2520score%2520for%25202D%250Amodels%2520while%2520a%2520reduction%2520of%252011.65%2520voxel%2520spacing%2520in%2520Hausdorff%2520distance%2520for%25203D%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Strong%20labels%3A%20Weakly-supervised%20Learning%20Based%20on%20Gaussian%0A%20%20Pseudo%20Labels%20for%20The%20Segmentation%20of%20Ellipse-like%20Vascular%20Structures%20in%0A%20%20Non-contrast%20CTs&entry.906535625=Qixiang%20Ma%20and%20Antoine%20%C5%81ucas%20and%20Huazhong%20Shu%20and%20Adrien%20Kaladji%20and%20Pascal%20Haigron&entry.1292438233=%20%20Deep-learning-based%20automated%20segmentation%20of%20vascular%20structures%20in%0Apreoperative%20CT%20scans%20contributes%20to%20computer-assisted%20diagnosis%20and%0Aintervention%20procedure%20in%20vascular%20diseases.%20While%20CT%20angiography%20%28CTA%29%20is%20the%0Acommon%20standard%2C%20non-contrast%20CT%20imaging%20is%20significant%20as%20a%20contrast-risk-free%0Aalternative%2C%20avoiding%20complications%20associated%20with%20contrast%20agents.%20However%2C%0Athe%20challenges%20of%20labor-intensive%20labeling%20and%20high%20labeling%20variability%20due%20to%0Athe%20ambiguity%20of%20vascular%20boundaries%20hinder%20conventional%20strong-label-based%2C%0Afully-supervised%20learning%20in%20non-contrast%20CTs.%20This%20paper%20introduces%20a%0Aweakly-supervised%20framework%20using%20ellipses%27%20topology%20in%20slices%2C%20including%201%29%20an%0Aefficient%20annotation%20process%20based%20on%20predefined%20standards%2C%202%29%20ellipse-fitting%0Aprocessing%2C%203%29%20the%20generation%20of%202D%20Gaussian%20heatmaps%20serving%20as%20pseudo%20labels%2C%0A4%29%20a%20training%20process%20through%20a%20combination%20of%20voxel%20reconstruction%20loss%20and%0Adistribution%20loss%20with%20the%20pseudo%20labels.%20We%20assess%20the%20effectiveness%20of%20the%0Aproposed%20method%20on%20one%20local%20and%20two%20public%20datasets%20comprising%20non-contrast%20CT%0Ascans%2C%20particularly%20focusing%20on%20the%20abdominal%20aorta.%20On%20the%20local%20dataset%2C%20our%0Aweakly-supervised%20learning%20approach%20based%20on%20pseudo%20labels%20outperforms%0Astrong-label-based%20fully-supervised%20learning%20%281.54%5C%25%20of%20Dice%20score%20on%20average%29%2C%0Areducing%20labeling%20time%20by%20around%2082.0%5C%25.%20The%20efficiency%20in%20generating%20pseudo%0Alabels%20allows%20the%20inclusion%20of%20label-agnostic%20external%20data%20in%20the%20training%0Aset%2C%20leading%20to%20an%20additional%20improvement%20in%20performance%20%282.74%5C%25%20of%20Dice%20score%0Aon%20average%29%20with%20a%20reduction%20of%2066.3%5C%25%20labeling%20time%2C%20where%20the%20labeling%20time%0Aremains%20considerably%20less%20than%20that%20of%20strong%20labels.%20On%20the%20public%20dataset%2C%0Athe%20pseudo%20labels%20achieve%20an%20overall%20improvement%20of%201.95%5C%25%20in%20Dice%20score%20for%202D%0Amodels%20while%20a%20reduction%20of%2011.65%20voxel%20spacing%20in%20Hausdorff%20distance%20for%203D%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03492v2&entry.124074799=Read"},
{"title": "Extending Segment Anything Model into Auditory and Temporal Dimensions\n  for Audio-Visual Segmentation", "author": "Juhyeong Seon and Woobin Im and Sebin Lee and Jumin Lee and Sung-Eui Yoon", "abstract": "  Audio-visual segmentation (AVS) aims to segment sound sources in the video\nsequence, requiring a pixel-level understanding of audio-visual correspondence.\nAs the Segment Anything Model (SAM) has strongly impacted extensive fields of\ndense prediction problems, prior works have investigated the introduction of\nSAM into AVS with audio as a new modality of the prompt. Nevertheless,\nconstrained by SAM's single-frame segmentation scheme, the temporal context\nacross multiple frames of audio-visual data remains insufficiently utilized. To\nthis end, we study the extension of SAM's capabilities to the sequence of\naudio-visual scenes by analyzing contextual cross-modal relationships across\nthe frames. To achieve this, we propose a Spatio-Temporal, Bidirectional\nAudio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's\nimage encoder and mask decoder. It adaptively updates the audio-visual features\nto convey the spatio-temporal correspondence between the video frames and audio\nstreams. Extensive experiments demonstrate that our proposed model outperforms\nthe state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU\ngain on a challenging multi-sources subset.\n", "link": "http://arxiv.org/abs/2406.06163v1", "date": "2024-06-10", "relevancy": 2.1185, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5145}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20Segment%20Anything%20Model%20into%20Auditory%20and%20Temporal%20Dimensions%0A%20%20for%20Audio-Visual%20Segmentation&body=Title%3A%20Extending%20Segment%20Anything%20Model%20into%20Auditory%20and%20Temporal%20Dimensions%0A%20%20for%20Audio-Visual%20Segmentation%0AAuthor%3A%20Juhyeong%20Seon%20and%20Woobin%20Im%20and%20Sebin%20Lee%20and%20Jumin%20Lee%20and%20Sung-Eui%20Yoon%0AAbstract%3A%20%20%20Audio-visual%20segmentation%20%28AVS%29%20aims%20to%20segment%20sound%20sources%20in%20the%20video%0Asequence%2C%20requiring%20a%20pixel-level%20understanding%20of%20audio-visual%20correspondence.%0AAs%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20strongly%20impacted%20extensive%20fields%20of%0Adense%20prediction%20problems%2C%20prior%20works%20have%20investigated%20the%20introduction%20of%0ASAM%20into%20AVS%20with%20audio%20as%20a%20new%20modality%20of%20the%20prompt.%20Nevertheless%2C%0Aconstrained%20by%20SAM%27s%20single-frame%20segmentation%20scheme%2C%20the%20temporal%20context%0Aacross%20multiple%20frames%20of%20audio-visual%20data%20remains%20insufficiently%20utilized.%20To%0Athis%20end%2C%20we%20study%20the%20extension%20of%20SAM%27s%20capabilities%20to%20the%20sequence%20of%0Aaudio-visual%20scenes%20by%20analyzing%20contextual%20cross-modal%20relationships%20across%0Athe%20frames.%20To%20achieve%20this%2C%20we%20propose%20a%20Spatio-Temporal%2C%20Bidirectional%0AAudio-Visual%20Attention%20%28ST-BAVA%29%20module%20integrated%20into%20the%20middle%20of%20SAM%27s%0Aimage%20encoder%20and%20mask%20decoder.%20It%20adaptively%20updates%20the%20audio-visual%20features%0Ato%20convey%20the%20spatio-temporal%20correspondence%20between%20the%20video%20frames%20and%20audio%0Astreams.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20model%20outperforms%0Athe%20state-of-the-art%20methods%20on%20AVS%20benchmarks%2C%20especially%20with%20an%208.3%25%20mIoU%0Again%20on%20a%20challenging%20multi-sources%20subset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520Segment%2520Anything%2520Model%2520into%2520Auditory%2520and%2520Temporal%2520Dimensions%250A%2520%2520for%2520Audio-Visual%2520Segmentation%26entry.906535625%3DJuhyeong%2520Seon%2520and%2520Woobin%2520Im%2520and%2520Sebin%2520Lee%2520and%2520Jumin%2520Lee%2520and%2520Sung-Eui%2520Yoon%26entry.1292438233%3D%2520%2520Audio-visual%2520segmentation%2520%2528AVS%2529%2520aims%2520to%2520segment%2520sound%2520sources%2520in%2520the%2520video%250Asequence%252C%2520requiring%2520a%2520pixel-level%2520understanding%2520of%2520audio-visual%2520correspondence.%250AAs%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520strongly%2520impacted%2520extensive%2520fields%2520of%250Adense%2520prediction%2520problems%252C%2520prior%2520works%2520have%2520investigated%2520the%2520introduction%2520of%250ASAM%2520into%2520AVS%2520with%2520audio%2520as%2520a%2520new%2520modality%2520of%2520the%2520prompt.%2520Nevertheless%252C%250Aconstrained%2520by%2520SAM%2527s%2520single-frame%2520segmentation%2520scheme%252C%2520the%2520temporal%2520context%250Aacross%2520multiple%2520frames%2520of%2520audio-visual%2520data%2520remains%2520insufficiently%2520utilized.%2520To%250Athis%2520end%252C%2520we%2520study%2520the%2520extension%2520of%2520SAM%2527s%2520capabilities%2520to%2520the%2520sequence%2520of%250Aaudio-visual%2520scenes%2520by%2520analyzing%2520contextual%2520cross-modal%2520relationships%2520across%250Athe%2520frames.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520Spatio-Temporal%252C%2520Bidirectional%250AAudio-Visual%2520Attention%2520%2528ST-BAVA%2529%2520module%2520integrated%2520into%2520the%2520middle%2520of%2520SAM%2527s%250Aimage%2520encoder%2520and%2520mask%2520decoder.%2520It%2520adaptively%2520updates%2520the%2520audio-visual%2520features%250Ato%2520convey%2520the%2520spatio-temporal%2520correspondence%2520between%2520the%2520video%2520frames%2520and%2520audio%250Astreams.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520model%2520outperforms%250Athe%2520state-of-the-art%2520methods%2520on%2520AVS%2520benchmarks%252C%2520especially%2520with%2520an%25208.3%2525%2520mIoU%250Again%2520on%2520a%2520challenging%2520multi-sources%2520subset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20Segment%20Anything%20Model%20into%20Auditory%20and%20Temporal%20Dimensions%0A%20%20for%20Audio-Visual%20Segmentation&entry.906535625=Juhyeong%20Seon%20and%20Woobin%20Im%20and%20Sebin%20Lee%20and%20Jumin%20Lee%20and%20Sung-Eui%20Yoon&entry.1292438233=%20%20Audio-visual%20segmentation%20%28AVS%29%20aims%20to%20segment%20sound%20sources%20in%20the%20video%0Asequence%2C%20requiring%20a%20pixel-level%20understanding%20of%20audio-visual%20correspondence.%0AAs%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20strongly%20impacted%20extensive%20fields%20of%0Adense%20prediction%20problems%2C%20prior%20works%20have%20investigated%20the%20introduction%20of%0ASAM%20into%20AVS%20with%20audio%20as%20a%20new%20modality%20of%20the%20prompt.%20Nevertheless%2C%0Aconstrained%20by%20SAM%27s%20single-frame%20segmentation%20scheme%2C%20the%20temporal%20context%0Aacross%20multiple%20frames%20of%20audio-visual%20data%20remains%20insufficiently%20utilized.%20To%0Athis%20end%2C%20we%20study%20the%20extension%20of%20SAM%27s%20capabilities%20to%20the%20sequence%20of%0Aaudio-visual%20scenes%20by%20analyzing%20contextual%20cross-modal%20relationships%20across%0Athe%20frames.%20To%20achieve%20this%2C%20we%20propose%20a%20Spatio-Temporal%2C%20Bidirectional%0AAudio-Visual%20Attention%20%28ST-BAVA%29%20module%20integrated%20into%20the%20middle%20of%20SAM%27s%0Aimage%20encoder%20and%20mask%20decoder.%20It%20adaptively%20updates%20the%20audio-visual%20features%0Ato%20convey%20the%20spatio-temporal%20correspondence%20between%20the%20video%20frames%20and%20audio%0Astreams.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20model%20outperforms%0Athe%20state-of-the-art%20methods%20on%20AVS%20benchmarks%2C%20especially%20with%20an%208.3%25%20mIoU%0Again%20on%20a%20challenging%20multi-sources%20subset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06163v1&entry.124074799=Read"},
{"title": "2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension\n  Method for Multimodal Moment Retrieval", "author": "Jiajun He and Tomoki Toda", "abstract": "  Moment retrieval aims to locate the most relevant moment in an untrimmed\nvideo based on a given natural language query. Existing solutions can be\nroughly categorized into moment-based and clip-based methods. The former often\ninvolves heavy computations, while the latter, due to overlooking\ncoarse-grained information, typically underperforms compared to moment-based\nmodels. Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine\nReading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address\nthe issue of imprecise localization in clip-based methods while maintaining\nlower computational complexity than moment-based methods. Specifically, we\nintroduce an AV-Encoder to capture coarse-grained information at moment and\nvideo levels. Additionally, a 2D pointer encoder module is introduced to\nfurther enhance boundary detection for target moment. Extensive experiments on\nthe HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing\nbaseline models.\n", "link": "http://arxiv.org/abs/2406.06201v1", "date": "2024-06-10", "relevancy": 2.1184, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202DP-2MRC%3A%202-Dimensional%20Pointer-based%20Machine%20Reading%20Comprehension%0A%20%20Method%20for%20Multimodal%20Moment%20Retrieval&body=Title%3A%202DP-2MRC%3A%202-Dimensional%20Pointer-based%20Machine%20Reading%20Comprehension%0A%20%20Method%20for%20Multimodal%20Moment%20Retrieval%0AAuthor%3A%20Jiajun%20He%20and%20Tomoki%20Toda%0AAbstract%3A%20%20%20Moment%20retrieval%20aims%20to%20locate%20the%20most%20relevant%20moment%20in%20an%20untrimmed%0Avideo%20based%20on%20a%20given%20natural%20language%20query.%20Existing%20solutions%20can%20be%0Aroughly%20categorized%20into%20moment-based%20and%20clip-based%20methods.%20The%20former%20often%0Ainvolves%20heavy%20computations%2C%20while%20the%20latter%2C%20due%20to%20overlooking%0Acoarse-grained%20information%2C%20typically%20underperforms%20compared%20to%20moment-based%0Amodels.%20Hence%2C%20this%20paper%20proposes%20a%20novel%202-Dimensional%20Pointer-based%20Machine%0AReading%20Comprehension%20for%20Moment%20Retrieval%20Choice%20%282DP-2MRC%29%20model%20to%20address%0Athe%20issue%20of%20imprecise%20localization%20in%20clip-based%20methods%20while%20maintaining%0Alower%20computational%20complexity%20than%20moment-based%20methods.%20Specifically%2C%20we%0Aintroduce%20an%20AV-Encoder%20to%20capture%20coarse-grained%20information%20at%20moment%20and%0Avideo%20levels.%20Additionally%2C%20a%202D%20pointer%20encoder%20module%20is%20introduced%20to%0Afurther%20enhance%20boundary%20detection%20for%20target%20moment.%20Extensive%20experiments%20on%0Athe%20HiREST%20dataset%20demonstrate%20that%202DP-2MRC%20significantly%20outperforms%20existing%0Abaseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2DP-2MRC%253A%25202-Dimensional%2520Pointer-based%2520Machine%2520Reading%2520Comprehension%250A%2520%2520Method%2520for%2520Multimodal%2520Moment%2520Retrieval%26entry.906535625%3DJiajun%2520He%2520and%2520Tomoki%2520Toda%26entry.1292438233%3D%2520%2520Moment%2520retrieval%2520aims%2520to%2520locate%2520the%2520most%2520relevant%2520moment%2520in%2520an%2520untrimmed%250Avideo%2520based%2520on%2520a%2520given%2520natural%2520language%2520query.%2520Existing%2520solutions%2520can%2520be%250Aroughly%2520categorized%2520into%2520moment-based%2520and%2520clip-based%2520methods.%2520The%2520former%2520often%250Ainvolves%2520heavy%2520computations%252C%2520while%2520the%2520latter%252C%2520due%2520to%2520overlooking%250Acoarse-grained%2520information%252C%2520typically%2520underperforms%2520compared%2520to%2520moment-based%250Amodels.%2520Hence%252C%2520this%2520paper%2520proposes%2520a%2520novel%25202-Dimensional%2520Pointer-based%2520Machine%250AReading%2520Comprehension%2520for%2520Moment%2520Retrieval%2520Choice%2520%25282DP-2MRC%2529%2520model%2520to%2520address%250Athe%2520issue%2520of%2520imprecise%2520localization%2520in%2520clip-based%2520methods%2520while%2520maintaining%250Alower%2520computational%2520complexity%2520than%2520moment-based%2520methods.%2520Specifically%252C%2520we%250Aintroduce%2520an%2520AV-Encoder%2520to%2520capture%2520coarse-grained%2520information%2520at%2520moment%2520and%250Avideo%2520levels.%2520Additionally%252C%2520a%25202D%2520pointer%2520encoder%2520module%2520is%2520introduced%2520to%250Afurther%2520enhance%2520boundary%2520detection%2520for%2520target%2520moment.%2520Extensive%2520experiments%2520on%250Athe%2520HiREST%2520dataset%2520demonstrate%2520that%25202DP-2MRC%2520significantly%2520outperforms%2520existing%250Abaseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2DP-2MRC%3A%202-Dimensional%20Pointer-based%20Machine%20Reading%20Comprehension%0A%20%20Method%20for%20Multimodal%20Moment%20Retrieval&entry.906535625=Jiajun%20He%20and%20Tomoki%20Toda&entry.1292438233=%20%20Moment%20retrieval%20aims%20to%20locate%20the%20most%20relevant%20moment%20in%20an%20untrimmed%0Avideo%20based%20on%20a%20given%20natural%20language%20query.%20Existing%20solutions%20can%20be%0Aroughly%20categorized%20into%20moment-based%20and%20clip-based%20methods.%20The%20former%20often%0Ainvolves%20heavy%20computations%2C%20while%20the%20latter%2C%20due%20to%20overlooking%0Acoarse-grained%20information%2C%20typically%20underperforms%20compared%20to%20moment-based%0Amodels.%20Hence%2C%20this%20paper%20proposes%20a%20novel%202-Dimensional%20Pointer-based%20Machine%0AReading%20Comprehension%20for%20Moment%20Retrieval%20Choice%20%282DP-2MRC%29%20model%20to%20address%0Athe%20issue%20of%20imprecise%20localization%20in%20clip-based%20methods%20while%20maintaining%0Alower%20computational%20complexity%20than%20moment-based%20methods.%20Specifically%2C%20we%0Aintroduce%20an%20AV-Encoder%20to%20capture%20coarse-grained%20information%20at%20moment%20and%0Avideo%20levels.%20Additionally%2C%20a%202D%20pointer%20encoder%20module%20is%20introduced%20to%0Afurther%20enhance%20boundary%20detection%20for%20target%20moment.%20Extensive%20experiments%20on%0Athe%20HiREST%20dataset%20demonstrate%20that%202DP-2MRC%20significantly%20outperforms%20existing%0Abaseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06201v1&entry.124074799=Read"},
{"title": "Efficient Neural Compression with Inference-time Decoding", "author": "C. Metz and O. Bichler and A. Dupret", "abstract": "  This paper explores the combination of neural network quantization and\nentropy coding for memory footprint minimization. Edge deployment of quantized\nmodels is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth\ntradeoff, causing dramatic accuracy loss below a certain bitwidth. This\naccuracy loss can be alleviated thanks to mixed precision quantization,\nallowing for more flexible bitwidth allocation. However, standard mixed\nprecision benefits remain limited due to the 1-bit frontier, that forces each\nparameter to be encoded on at least 1 bit of data. This paper introduces an\napproach that combines mixed precision, zero-point quantization and entropy\ncoding to push the compression boundary of Resnets beyond the 1-bit frontier\nwith an accuracy drop below 1% on the ImageNet benchmark. From an\nimplementation standpoint, a compact decoder architecture features reduced\nlatency, thus allowing for inference-compatible decoding.\n", "link": "http://arxiv.org/abs/2406.06237v1", "date": "2024-06-10", "relevancy": 2.1176, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Neural%20Compression%20with%20Inference-time%20Decoding&body=Title%3A%20Efficient%20Neural%20Compression%20with%20Inference-time%20Decoding%0AAuthor%3A%20C.%20Metz%20and%20O.%20Bichler%20and%20A.%20Dupret%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20combination%20of%20neural%20network%20quantization%20and%0Aentropy%20coding%20for%20memory%20footprint%20minimization.%20Edge%20deployment%20of%20quantized%0Amodels%20is%20hampered%20by%20the%20harsh%20Pareto%20frontier%20of%20the%20accuracy-to-bitwidth%0Atradeoff%2C%20causing%20dramatic%20accuracy%20loss%20below%20a%20certain%20bitwidth.%20This%0Aaccuracy%20loss%20can%20be%20alleviated%20thanks%20to%20mixed%20precision%20quantization%2C%0Aallowing%20for%20more%20flexible%20bitwidth%20allocation.%20However%2C%20standard%20mixed%0Aprecision%20benefits%20remain%20limited%20due%20to%20the%201-bit%20frontier%2C%20that%20forces%20each%0Aparameter%20to%20be%20encoded%20on%20at%20least%201%20bit%20of%20data.%20This%20paper%20introduces%20an%0Aapproach%20that%20combines%20mixed%20precision%2C%20zero-point%20quantization%20and%20entropy%0Acoding%20to%20push%20the%20compression%20boundary%20of%20Resnets%20beyond%20the%201-bit%20frontier%0Awith%20an%20accuracy%20drop%20below%201%25%20on%20the%20ImageNet%20benchmark.%20From%20an%0Aimplementation%20standpoint%2C%20a%20compact%20decoder%20architecture%20features%20reduced%0Alatency%2C%20thus%20allowing%20for%20inference-compatible%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Neural%2520Compression%2520with%2520Inference-time%2520Decoding%26entry.906535625%3DC.%2520Metz%2520and%2520O.%2520Bichler%2520and%2520A.%2520Dupret%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520combination%2520of%2520neural%2520network%2520quantization%2520and%250Aentropy%2520coding%2520for%2520memory%2520footprint%2520minimization.%2520Edge%2520deployment%2520of%2520quantized%250Amodels%2520is%2520hampered%2520by%2520the%2520harsh%2520Pareto%2520frontier%2520of%2520the%2520accuracy-to-bitwidth%250Atradeoff%252C%2520causing%2520dramatic%2520accuracy%2520loss%2520below%2520a%2520certain%2520bitwidth.%2520This%250Aaccuracy%2520loss%2520can%2520be%2520alleviated%2520thanks%2520to%2520mixed%2520precision%2520quantization%252C%250Aallowing%2520for%2520more%2520flexible%2520bitwidth%2520allocation.%2520However%252C%2520standard%2520mixed%250Aprecision%2520benefits%2520remain%2520limited%2520due%2520to%2520the%25201-bit%2520frontier%252C%2520that%2520forces%2520each%250Aparameter%2520to%2520be%2520encoded%2520on%2520at%2520least%25201%2520bit%2520of%2520data.%2520This%2520paper%2520introduces%2520an%250Aapproach%2520that%2520combines%2520mixed%2520precision%252C%2520zero-point%2520quantization%2520and%2520entropy%250Acoding%2520to%2520push%2520the%2520compression%2520boundary%2520of%2520Resnets%2520beyond%2520the%25201-bit%2520frontier%250Awith%2520an%2520accuracy%2520drop%2520below%25201%2525%2520on%2520the%2520ImageNet%2520benchmark.%2520From%2520an%250Aimplementation%2520standpoint%252C%2520a%2520compact%2520decoder%2520architecture%2520features%2520reduced%250Alatency%252C%2520thus%2520allowing%2520for%2520inference-compatible%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Neural%20Compression%20with%20Inference-time%20Decoding&entry.906535625=C.%20Metz%20and%20O.%20Bichler%20and%20A.%20Dupret&entry.1292438233=%20%20This%20paper%20explores%20the%20combination%20of%20neural%20network%20quantization%20and%0Aentropy%20coding%20for%20memory%20footprint%20minimization.%20Edge%20deployment%20of%20quantized%0Amodels%20is%20hampered%20by%20the%20harsh%20Pareto%20frontier%20of%20the%20accuracy-to-bitwidth%0Atradeoff%2C%20causing%20dramatic%20accuracy%20loss%20below%20a%20certain%20bitwidth.%20This%0Aaccuracy%20loss%20can%20be%20alleviated%20thanks%20to%20mixed%20precision%20quantization%2C%0Aallowing%20for%20more%20flexible%20bitwidth%20allocation.%20However%2C%20standard%20mixed%0Aprecision%20benefits%20remain%20limited%20due%20to%20the%201-bit%20frontier%2C%20that%20forces%20each%0Aparameter%20to%20be%20encoded%20on%20at%20least%201%20bit%20of%20data.%20This%20paper%20introduces%20an%0Aapproach%20that%20combines%20mixed%20precision%2C%20zero-point%20quantization%20and%20entropy%0Acoding%20to%20push%20the%20compression%20boundary%20of%20Resnets%20beyond%20the%201-bit%20frontier%0Awith%20an%20accuracy%20drop%20below%201%25%20on%20the%20ImageNet%20benchmark.%20From%20an%0Aimplementation%20standpoint%2C%20a%20compact%20decoder%20architecture%20features%20reduced%0Alatency%2C%20thus%20allowing%20for%20inference-compatible%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06237v1&entry.124074799=Read"},
{"title": "IReNe: Instant Recoloring of Neural Radiance Fields", "author": "Alessio Mazzucchelli and Adrian Garcia-Garcia and Elena Garces and Fernando Rivas-Manzaneque and Francesc Moreno-Noguer and Adrian Penate-Sanchez", "abstract": "  Advances in NERFs have allowed for 3D scene reconstructions and novel view\nsynthesis. Yet, efficiently editing these representations while retaining\nphotorealism is an emerging challenge. Recent methods face three primary\nlimitations: they're slow for interactive use, lack precision at object\nboundaries, and struggle to ensure multi-view consistency. We introduce IReNe\nto address these limitations, enabling swift, near real-time color editing in\nNeRF. Leveraging a pre-trained NeRF model and a single training image with\nuser-applied color edits, IReNe swiftly adjusts network parameters in seconds.\nThis adjustment allows the model to generate new scene views, accurately\nrepresenting the color changes from the training image while also controlling\nobject boundaries and view-specific effects. Object boundary control is\nachieved by integrating a trainable segmentation module into the model. The\nprocess gains efficiency by retraining only the weights of the last network\nlayer. We observed that neurons in this layer can be classified into those\nresponsible for view-dependent appearance and those contributing to diffuse\nappearance. We introduce an automated classification approach to identify these\nneuron types and exclusively fine-tune the weights of the diffuse neurons. This\nfurther accelerates training and ensures consistent color edits across\ndifferent views. A thorough validation on a new dataset, with edited object\ncolors, shows significant quantitative and qualitative advancements over\ncompetitors, accelerating speeds by 5x to 500x.\n", "link": "http://arxiv.org/abs/2405.19876v2", "date": "2024-06-10", "relevancy": 2.1138, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5388}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5265}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields&body=Title%3A%20IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields%0AAuthor%3A%20Alessio%20Mazzucchelli%20and%20Adrian%20Garcia-Garcia%20and%20Elena%20Garces%20and%20Fernando%20Rivas-Manzaneque%20and%20Francesc%20Moreno-Noguer%20and%20Adrian%20Penate-Sanchez%0AAbstract%3A%20%20%20Advances%20in%20NERFs%20have%20allowed%20for%203D%20scene%20reconstructions%20and%20novel%20view%0Asynthesis.%20Yet%2C%20efficiently%20editing%20these%20representations%20while%20retaining%0Aphotorealism%20is%20an%20emerging%20challenge.%20Recent%20methods%20face%20three%20primary%0Alimitations%3A%20they%27re%20slow%20for%20interactive%20use%2C%20lack%20precision%20at%20object%0Aboundaries%2C%20and%20struggle%20to%20ensure%20multi-view%20consistency.%20We%20introduce%20IReNe%0Ato%20address%20these%20limitations%2C%20enabling%20swift%2C%20near%20real-time%20color%20editing%20in%0ANeRF.%20Leveraging%20a%20pre-trained%20NeRF%20model%20and%20a%20single%20training%20image%20with%0Auser-applied%20color%20edits%2C%20IReNe%20swiftly%20adjusts%20network%20parameters%20in%20seconds.%0AThis%20adjustment%20allows%20the%20model%20to%20generate%20new%20scene%20views%2C%20accurately%0Arepresenting%20the%20color%20changes%20from%20the%20training%20image%20while%20also%20controlling%0Aobject%20boundaries%20and%20view-specific%20effects.%20Object%20boundary%20control%20is%0Aachieved%20by%20integrating%20a%20trainable%20segmentation%20module%20into%20the%20model.%20The%0Aprocess%20gains%20efficiency%20by%20retraining%20only%20the%20weights%20of%20the%20last%20network%0Alayer.%20We%20observed%20that%20neurons%20in%20this%20layer%20can%20be%20classified%20into%20those%0Aresponsible%20for%20view-dependent%20appearance%20and%20those%20contributing%20to%20diffuse%0Aappearance.%20We%20introduce%20an%20automated%20classification%20approach%20to%20identify%20these%0Aneuron%20types%20and%20exclusively%20fine-tune%20the%20weights%20of%20the%20diffuse%20neurons.%20This%0Afurther%20accelerates%20training%20and%20ensures%20consistent%20color%20edits%20across%0Adifferent%20views.%20A%20thorough%20validation%20on%20a%20new%20dataset%2C%20with%20edited%20object%0Acolors%2C%20shows%20significant%20quantitative%20and%20qualitative%20advancements%20over%0Acompetitors%2C%20accelerating%20speeds%20by%205x%20to%20500x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIReNe%253A%2520Instant%2520Recoloring%2520of%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DAlessio%2520Mazzucchelli%2520and%2520Adrian%2520Garcia-Garcia%2520and%2520Elena%2520Garces%2520and%2520Fernando%2520Rivas-Manzaneque%2520and%2520Francesc%2520Moreno-Noguer%2520and%2520Adrian%2520Penate-Sanchez%26entry.1292438233%3D%2520%2520Advances%2520in%2520NERFs%2520have%2520allowed%2520for%25203D%2520scene%2520reconstructions%2520and%2520novel%2520view%250Asynthesis.%2520Yet%252C%2520efficiently%2520editing%2520these%2520representations%2520while%2520retaining%250Aphotorealism%2520is%2520an%2520emerging%2520challenge.%2520Recent%2520methods%2520face%2520three%2520primary%250Alimitations%253A%2520they%2527re%2520slow%2520for%2520interactive%2520use%252C%2520lack%2520precision%2520at%2520object%250Aboundaries%252C%2520and%2520struggle%2520to%2520ensure%2520multi-view%2520consistency.%2520We%2520introduce%2520IReNe%250Ato%2520address%2520these%2520limitations%252C%2520enabling%2520swift%252C%2520near%2520real-time%2520color%2520editing%2520in%250ANeRF.%2520Leveraging%2520a%2520pre-trained%2520NeRF%2520model%2520and%2520a%2520single%2520training%2520image%2520with%250Auser-applied%2520color%2520edits%252C%2520IReNe%2520swiftly%2520adjusts%2520network%2520parameters%2520in%2520seconds.%250AThis%2520adjustment%2520allows%2520the%2520model%2520to%2520generate%2520new%2520scene%2520views%252C%2520accurately%250Arepresenting%2520the%2520color%2520changes%2520from%2520the%2520training%2520image%2520while%2520also%2520controlling%250Aobject%2520boundaries%2520and%2520view-specific%2520effects.%2520Object%2520boundary%2520control%2520is%250Aachieved%2520by%2520integrating%2520a%2520trainable%2520segmentation%2520module%2520into%2520the%2520model.%2520The%250Aprocess%2520gains%2520efficiency%2520by%2520retraining%2520only%2520the%2520weights%2520of%2520the%2520last%2520network%250Alayer.%2520We%2520observed%2520that%2520neurons%2520in%2520this%2520layer%2520can%2520be%2520classified%2520into%2520those%250Aresponsible%2520for%2520view-dependent%2520appearance%2520and%2520those%2520contributing%2520to%2520diffuse%250Aappearance.%2520We%2520introduce%2520an%2520automated%2520classification%2520approach%2520to%2520identify%2520these%250Aneuron%2520types%2520and%2520exclusively%2520fine-tune%2520the%2520weights%2520of%2520the%2520diffuse%2520neurons.%2520This%250Afurther%2520accelerates%2520training%2520and%2520ensures%2520consistent%2520color%2520edits%2520across%250Adifferent%2520views.%2520A%2520thorough%2520validation%2520on%2520a%2520new%2520dataset%252C%2520with%2520edited%2520object%250Acolors%252C%2520shows%2520significant%2520quantitative%2520and%2520qualitative%2520advancements%2520over%250Acompetitors%252C%2520accelerating%2520speeds%2520by%25205x%2520to%2520500x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields&entry.906535625=Alessio%20Mazzucchelli%20and%20Adrian%20Garcia-Garcia%20and%20Elena%20Garces%20and%20Fernando%20Rivas-Manzaneque%20and%20Francesc%20Moreno-Noguer%20and%20Adrian%20Penate-Sanchez&entry.1292438233=%20%20Advances%20in%20NERFs%20have%20allowed%20for%203D%20scene%20reconstructions%20and%20novel%20view%0Asynthesis.%20Yet%2C%20efficiently%20editing%20these%20representations%20while%20retaining%0Aphotorealism%20is%20an%20emerging%20challenge.%20Recent%20methods%20face%20three%20primary%0Alimitations%3A%20they%27re%20slow%20for%20interactive%20use%2C%20lack%20precision%20at%20object%0Aboundaries%2C%20and%20struggle%20to%20ensure%20multi-view%20consistency.%20We%20introduce%20IReNe%0Ato%20address%20these%20limitations%2C%20enabling%20swift%2C%20near%20real-time%20color%20editing%20in%0ANeRF.%20Leveraging%20a%20pre-trained%20NeRF%20model%20and%20a%20single%20training%20image%20with%0Auser-applied%20color%20edits%2C%20IReNe%20swiftly%20adjusts%20network%20parameters%20in%20seconds.%0AThis%20adjustment%20allows%20the%20model%20to%20generate%20new%20scene%20views%2C%20accurately%0Arepresenting%20the%20color%20changes%20from%20the%20training%20image%20while%20also%20controlling%0Aobject%20boundaries%20and%20view-specific%20effects.%20Object%20boundary%20control%20is%0Aachieved%20by%20integrating%20a%20trainable%20segmentation%20module%20into%20the%20model.%20The%0Aprocess%20gains%20efficiency%20by%20retraining%20only%20the%20weights%20of%20the%20last%20network%0Alayer.%20We%20observed%20that%20neurons%20in%20this%20layer%20can%20be%20classified%20into%20those%0Aresponsible%20for%20view-dependent%20appearance%20and%20those%20contributing%20to%20diffuse%0Aappearance.%20We%20introduce%20an%20automated%20classification%20approach%20to%20identify%20these%0Aneuron%20types%20and%20exclusively%20fine-tune%20the%20weights%20of%20the%20diffuse%20neurons.%20This%0Afurther%20accelerates%20training%20and%20ensures%20consistent%20color%20edits%20across%0Adifferent%20views.%20A%20thorough%20validation%20on%20a%20new%20dataset%2C%20with%20edited%20object%0Acolors%2C%20shows%20significant%20quantitative%20and%20qualitative%20advancements%20over%0Acompetitors%2C%20accelerating%20speeds%20by%205x%20to%20500x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19876v2&entry.124074799=Read"},
{"title": "Re.Dis.Cover Place with Generative AI: Exploring the Experience and\n  Design of City Wandering with Image-to-Image AI", "author": "Peng-Kai Hung and Janet Yi-Ching Huang and Stephan Wensveen and Rung-Huei Liang", "abstract": "  The HCI field has demonstrated a growing interest in leveraging emerging\ntechnologies to enrich urban experiences. However, insufficient studies\ninvestigate the experience and design space of AI image technology (AIGT)\napplications for playful urban interaction, despite its widespread adoption. To\nexplore this gap, we conducted an exploratory study involving four participants\nwho wandered and photographed within Eindhoven Centre and interacted with an\nimage-to-image AI. Preliminary findings present their observations, the effect\nof their familiarity with places, and how AIGT becomes an explorer's tool or\nco-speculator. We then highlight AIGT's capability of supporting playfulness,\nreimaginations, and rediscoveries of places through defamiliarizing and\nfamiliarizing cityscapes. Additionally, we propose the metaphor AIGT as a\n'tourist' to discuss its opportunities for engaging explorations and risks of\nstereotyping places. Collectively, our research provides initial empirical\ninsights and design considerations, inspiring future HCI endeavors for creating\nurban play with generative AI.\n", "link": "http://arxiv.org/abs/2406.06356v1", "date": "2024-06-10", "relevancy": 2.1097, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5478}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5472}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re.Dis.Cover%20Place%20with%20Generative%20AI%3A%20Exploring%20the%20Experience%20and%0A%20%20Design%20of%20City%20Wandering%20with%20Image-to-Image%20AI&body=Title%3A%20Re.Dis.Cover%20Place%20with%20Generative%20AI%3A%20Exploring%20the%20Experience%20and%0A%20%20Design%20of%20City%20Wandering%20with%20Image-to-Image%20AI%0AAuthor%3A%20Peng-Kai%20Hung%20and%20Janet%20Yi-Ching%20Huang%20and%20Stephan%20Wensveen%20and%20Rung-Huei%20Liang%0AAbstract%3A%20%20%20The%20HCI%20field%20has%20demonstrated%20a%20growing%20interest%20in%20leveraging%20emerging%0Atechnologies%20to%20enrich%20urban%20experiences.%20However%2C%20insufficient%20studies%0Ainvestigate%20the%20experience%20and%20design%20space%20of%20AI%20image%20technology%20%28AIGT%29%0Aapplications%20for%20playful%20urban%20interaction%2C%20despite%20its%20widespread%20adoption.%20To%0Aexplore%20this%20gap%2C%20we%20conducted%20an%20exploratory%20study%20involving%20four%20participants%0Awho%20wandered%20and%20photographed%20within%20Eindhoven%20Centre%20and%20interacted%20with%20an%0Aimage-to-image%20AI.%20Preliminary%20findings%20present%20their%20observations%2C%20the%20effect%0Aof%20their%20familiarity%20with%20places%2C%20and%20how%20AIGT%20becomes%20an%20explorer%27s%20tool%20or%0Aco-speculator.%20We%20then%20highlight%20AIGT%27s%20capability%20of%20supporting%20playfulness%2C%0Areimaginations%2C%20and%20rediscoveries%20of%20places%20through%20defamiliarizing%20and%0Afamiliarizing%20cityscapes.%20Additionally%2C%20we%20propose%20the%20metaphor%20AIGT%20as%20a%0A%27tourist%27%20to%20discuss%20its%20opportunities%20for%20engaging%20explorations%20and%20risks%20of%0Astereotyping%20places.%20Collectively%2C%20our%20research%20provides%20initial%20empirical%0Ainsights%20and%20design%20considerations%2C%20inspiring%20future%20HCI%20endeavors%20for%20creating%0Aurban%20play%20with%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe.Dis.Cover%2520Place%2520with%2520Generative%2520AI%253A%2520Exploring%2520the%2520Experience%2520and%250A%2520%2520Design%2520of%2520City%2520Wandering%2520with%2520Image-to-Image%2520AI%26entry.906535625%3DPeng-Kai%2520Hung%2520and%2520Janet%2520Yi-Ching%2520Huang%2520and%2520Stephan%2520Wensveen%2520and%2520Rung-Huei%2520Liang%26entry.1292438233%3D%2520%2520The%2520HCI%2520field%2520has%2520demonstrated%2520a%2520growing%2520interest%2520in%2520leveraging%2520emerging%250Atechnologies%2520to%2520enrich%2520urban%2520experiences.%2520However%252C%2520insufficient%2520studies%250Ainvestigate%2520the%2520experience%2520and%2520design%2520space%2520of%2520AI%2520image%2520technology%2520%2528AIGT%2529%250Aapplications%2520for%2520playful%2520urban%2520interaction%252C%2520despite%2520its%2520widespread%2520adoption.%2520To%250Aexplore%2520this%2520gap%252C%2520we%2520conducted%2520an%2520exploratory%2520study%2520involving%2520four%2520participants%250Awho%2520wandered%2520and%2520photographed%2520within%2520Eindhoven%2520Centre%2520and%2520interacted%2520with%2520an%250Aimage-to-image%2520AI.%2520Preliminary%2520findings%2520present%2520their%2520observations%252C%2520the%2520effect%250Aof%2520their%2520familiarity%2520with%2520places%252C%2520and%2520how%2520AIGT%2520becomes%2520an%2520explorer%2527s%2520tool%2520or%250Aco-speculator.%2520We%2520then%2520highlight%2520AIGT%2527s%2520capability%2520of%2520supporting%2520playfulness%252C%250Areimaginations%252C%2520and%2520rediscoveries%2520of%2520places%2520through%2520defamiliarizing%2520and%250Afamiliarizing%2520cityscapes.%2520Additionally%252C%2520we%2520propose%2520the%2520metaphor%2520AIGT%2520as%2520a%250A%2527tourist%2527%2520to%2520discuss%2520its%2520opportunities%2520for%2520engaging%2520explorations%2520and%2520risks%2520of%250Astereotyping%2520places.%2520Collectively%252C%2520our%2520research%2520provides%2520initial%2520empirical%250Ainsights%2520and%2520design%2520considerations%252C%2520inspiring%2520future%2520HCI%2520endeavors%2520for%2520creating%250Aurban%2520play%2520with%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re.Dis.Cover%20Place%20with%20Generative%20AI%3A%20Exploring%20the%20Experience%20and%0A%20%20Design%20of%20City%20Wandering%20with%20Image-to-Image%20AI&entry.906535625=Peng-Kai%20Hung%20and%20Janet%20Yi-Ching%20Huang%20and%20Stephan%20Wensveen%20and%20Rung-Huei%20Liang&entry.1292438233=%20%20The%20HCI%20field%20has%20demonstrated%20a%20growing%20interest%20in%20leveraging%20emerging%0Atechnologies%20to%20enrich%20urban%20experiences.%20However%2C%20insufficient%20studies%0Ainvestigate%20the%20experience%20and%20design%20space%20of%20AI%20image%20technology%20%28AIGT%29%0Aapplications%20for%20playful%20urban%20interaction%2C%20despite%20its%20widespread%20adoption.%20To%0Aexplore%20this%20gap%2C%20we%20conducted%20an%20exploratory%20study%20involving%20four%20participants%0Awho%20wandered%20and%20photographed%20within%20Eindhoven%20Centre%20and%20interacted%20with%20an%0Aimage-to-image%20AI.%20Preliminary%20findings%20present%20their%20observations%2C%20the%20effect%0Aof%20their%20familiarity%20with%20places%2C%20and%20how%20AIGT%20becomes%20an%20explorer%27s%20tool%20or%0Aco-speculator.%20We%20then%20highlight%20AIGT%27s%20capability%20of%20supporting%20playfulness%2C%0Areimaginations%2C%20and%20rediscoveries%20of%20places%20through%20defamiliarizing%20and%0Afamiliarizing%20cityscapes.%20Additionally%2C%20we%20propose%20the%20metaphor%20AIGT%20as%20a%0A%27tourist%27%20to%20discuss%20its%20opportunities%20for%20engaging%20explorations%20and%20risks%20of%0Astereotyping%20places.%20Collectively%2C%20our%20research%20provides%20initial%20empirical%0Ainsights%20and%20design%20considerations%2C%20inspiring%20future%20HCI%20endeavors%20for%20creating%0Aurban%20play%20with%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06356v1&entry.124074799=Read"},
{"title": "AI Cat Narrator: Designing an AI Tool for Exploring the Shared World and\n  Social Connection with a Cat", "author": "Zhenchi Lai and Janet Yi-Ching Huang and Rung-Huei Liang", "abstract": "  As technology continues to advance, the interaction between humans and cats\nis becoming more diverse. Our research introduces a new tool called the AI Cat\nNarrator, which offers a unique perspective on the shared lives of humans and\ncats. We combined the method of ethnography with fictional storytelling, using\na defamiliarization strategy to merge real-world data seen through the eyes of\ncats with excerpts from cat literature. This combination serves as the\nfoundation for a database to instruct the AI Cat Narrator in crafting\nalternative narrative. Our findings indicate that using defamiliarized data for\ntraining purposes significantly contributes to the development of characters\nthat are both more empathetic and individualized. The contributions of our\nstudy are twofold: 1) proposing an innovative approach to prompting a\nreevaluation of living alongside cats; 2) establishing a collaborative,\nexploratory tool developed by humans, cats, and AI together.\n", "link": "http://arxiv.org/abs/2406.06192v1", "date": "2024-06-10", "relevancy": 2.1082, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.531}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Cat%20Narrator%3A%20Designing%20an%20AI%20Tool%20for%20Exploring%20the%20Shared%20World%20and%0A%20%20Social%20Connection%20with%20a%20Cat&body=Title%3A%20AI%20Cat%20Narrator%3A%20Designing%20an%20AI%20Tool%20for%20Exploring%20the%20Shared%20World%20and%0A%20%20Social%20Connection%20with%20a%20Cat%0AAuthor%3A%20Zhenchi%20Lai%20and%20Janet%20Yi-Ching%20Huang%20and%20Rung-Huei%20Liang%0AAbstract%3A%20%20%20As%20technology%20continues%20to%20advance%2C%20the%20interaction%20between%20humans%20and%20cats%0Ais%20becoming%20more%20diverse.%20Our%20research%20introduces%20a%20new%20tool%20called%20the%20AI%20Cat%0ANarrator%2C%20which%20offers%20a%20unique%20perspective%20on%20the%20shared%20lives%20of%20humans%20and%0Acats.%20We%20combined%20the%20method%20of%20ethnography%20with%20fictional%20storytelling%2C%20using%0Aa%20defamiliarization%20strategy%20to%20merge%20real-world%20data%20seen%20through%20the%20eyes%20of%0Acats%20with%20excerpts%20from%20cat%20literature.%20This%20combination%20serves%20as%20the%0Afoundation%20for%20a%20database%20to%20instruct%20the%20AI%20Cat%20Narrator%20in%20crafting%0Aalternative%20narrative.%20Our%20findings%20indicate%20that%20using%20defamiliarized%20data%20for%0Atraining%20purposes%20significantly%20contributes%20to%20the%20development%20of%20characters%0Athat%20are%20both%20more%20empathetic%20and%20individualized.%20The%20contributions%20of%20our%0Astudy%20are%20twofold%3A%201%29%20proposing%20an%20innovative%20approach%20to%20prompting%20a%0Areevaluation%20of%20living%20alongside%20cats%3B%202%29%20establishing%20a%20collaborative%2C%0Aexploratory%20tool%20developed%20by%20humans%2C%20cats%2C%20and%20AI%20together.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Cat%2520Narrator%253A%2520Designing%2520an%2520AI%2520Tool%2520for%2520Exploring%2520the%2520Shared%2520World%2520and%250A%2520%2520Social%2520Connection%2520with%2520a%2520Cat%26entry.906535625%3DZhenchi%2520Lai%2520and%2520Janet%2520Yi-Ching%2520Huang%2520and%2520Rung-Huei%2520Liang%26entry.1292438233%3D%2520%2520As%2520technology%2520continues%2520to%2520advance%252C%2520the%2520interaction%2520between%2520humans%2520and%2520cats%250Ais%2520becoming%2520more%2520diverse.%2520Our%2520research%2520introduces%2520a%2520new%2520tool%2520called%2520the%2520AI%2520Cat%250ANarrator%252C%2520which%2520offers%2520a%2520unique%2520perspective%2520on%2520the%2520shared%2520lives%2520of%2520humans%2520and%250Acats.%2520We%2520combined%2520the%2520method%2520of%2520ethnography%2520with%2520fictional%2520storytelling%252C%2520using%250Aa%2520defamiliarization%2520strategy%2520to%2520merge%2520real-world%2520data%2520seen%2520through%2520the%2520eyes%2520of%250Acats%2520with%2520excerpts%2520from%2520cat%2520literature.%2520This%2520combination%2520serves%2520as%2520the%250Afoundation%2520for%2520a%2520database%2520to%2520instruct%2520the%2520AI%2520Cat%2520Narrator%2520in%2520crafting%250Aalternative%2520narrative.%2520Our%2520findings%2520indicate%2520that%2520using%2520defamiliarized%2520data%2520for%250Atraining%2520purposes%2520significantly%2520contributes%2520to%2520the%2520development%2520of%2520characters%250Athat%2520are%2520both%2520more%2520empathetic%2520and%2520individualized.%2520The%2520contributions%2520of%2520our%250Astudy%2520are%2520twofold%253A%25201%2529%2520proposing%2520an%2520innovative%2520approach%2520to%2520prompting%2520a%250Areevaluation%2520of%2520living%2520alongside%2520cats%253B%25202%2529%2520establishing%2520a%2520collaborative%252C%250Aexploratory%2520tool%2520developed%2520by%2520humans%252C%2520cats%252C%2520and%2520AI%2520together.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Cat%20Narrator%3A%20Designing%20an%20AI%20Tool%20for%20Exploring%20the%20Shared%20World%20and%0A%20%20Social%20Connection%20with%20a%20Cat&entry.906535625=Zhenchi%20Lai%20and%20Janet%20Yi-Ching%20Huang%20and%20Rung-Huei%20Liang&entry.1292438233=%20%20As%20technology%20continues%20to%20advance%2C%20the%20interaction%20between%20humans%20and%20cats%0Ais%20becoming%20more%20diverse.%20Our%20research%20introduces%20a%20new%20tool%20called%20the%20AI%20Cat%0ANarrator%2C%20which%20offers%20a%20unique%20perspective%20on%20the%20shared%20lives%20of%20humans%20and%0Acats.%20We%20combined%20the%20method%20of%20ethnography%20with%20fictional%20storytelling%2C%20using%0Aa%20defamiliarization%20strategy%20to%20merge%20real-world%20data%20seen%20through%20the%20eyes%20of%0Acats%20with%20excerpts%20from%20cat%20literature.%20This%20combination%20serves%20as%20the%0Afoundation%20for%20a%20database%20to%20instruct%20the%20AI%20Cat%20Narrator%20in%20crafting%0Aalternative%20narrative.%20Our%20findings%20indicate%20that%20using%20defamiliarized%20data%20for%0Atraining%20purposes%20significantly%20contributes%20to%20the%20development%20of%20characters%0Athat%20are%20both%20more%20empathetic%20and%20individualized.%20The%20contributions%20of%20our%0Astudy%20are%20twofold%3A%201%29%20proposing%20an%20innovative%20approach%20to%20prompting%20a%0Areevaluation%20of%20living%20alongside%20cats%3B%202%29%20establishing%20a%20collaborative%2C%0Aexploratory%20tool%20developed%20by%20humans%2C%20cats%2C%20and%20AI%20together.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06192v1&entry.124074799=Read"},
{"title": "FusionINN: Decomposable Image Fusion for Brain Tumor Monitoring", "author": "Nishant Kumar and Ziyan Tao and Jaikirat Singh and Yang Li and Peiwen Sun and Binghui Zhao and Stefan Gumhold", "abstract": "  Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.\n", "link": "http://arxiv.org/abs/2403.15769v3", "date": "2024-06-10", "relevancy": 2.1053, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5596}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5137}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionINN%3A%20Decomposable%20Image%20Fusion%20for%20Brain%20Tumor%20Monitoring&body=Title%3A%20FusionINN%3A%20Decomposable%20Image%20Fusion%20for%20Brain%20Tumor%20Monitoring%0AAuthor%3A%20Nishant%20Kumar%20and%20Ziyan%20Tao%20and%20Jaikirat%20Singh%20and%20Yang%20Li%20and%20Peiwen%20Sun%20and%20Binghui%20Zhao%20and%20Stefan%20Gumhold%0AAbstract%3A%20%20%20Image%20fusion%20typically%20employs%20non-invertible%20neural%20networks%20to%20merge%0Amultiple%20source%20images%20into%20a%20single%20fused%20image.%20However%2C%20for%20clinical%0Aexperts%2C%20solely%20relying%20on%20fused%20images%20may%20be%20insufficient%20for%20making%0Adiagnostic%20decisions%2C%20as%20the%20fusion%20mechanism%20blends%20features%20from%20source%0Aimages%2C%20thereby%20making%20it%20difficult%20to%20interpret%20the%20underlying%20tumor%0Apathology.%20We%20introduce%20FusionINN%2C%20a%20novel%20decomposable%20image%20fusion%20framework%2C%0Acapable%20of%20efficiently%20generating%20fused%20images%20and%20also%20decomposing%20them%20back%0Ato%20the%20source%20images.%20FusionINN%20is%20designed%20to%20be%20bijective%20by%20including%20a%0Alatent%20image%20alongside%20the%20fused%20image%2C%20while%20ensuring%20minimal%20transfer%20of%0Ainformation%20from%20the%20source%20images%20to%20the%20latent%20representation.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20investigate%20the%20decomposability%20of%20fused%0Aimages%2C%20which%20is%20particularly%20crucial%20for%20life-sensitive%20applications%20such%20as%0Amedical%20image%20fusion%20compared%20to%20other%20tasks%20like%20multi-focus%20or%20multi-exposure%0Aimage%20fusion.%20Our%20extensive%20experimentation%20validates%20FusionINN%20over%20existing%0Adiscriminative%20and%20generative%20fusion%20methods%2C%20both%20subjectively%20and%0Aobjectively.%20Moreover%2C%20compared%20to%20a%20recent%20denoising%20diffusion-based%20fusion%0Amodel%2C%20our%20approach%20offers%20faster%20and%20qualitatively%20better%20fusion%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15769v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionINN%253A%2520Decomposable%2520Image%2520Fusion%2520for%2520Brain%2520Tumor%2520Monitoring%26entry.906535625%3DNishant%2520Kumar%2520and%2520Ziyan%2520Tao%2520and%2520Jaikirat%2520Singh%2520and%2520Yang%2520Li%2520and%2520Peiwen%2520Sun%2520and%2520Binghui%2520Zhao%2520and%2520Stefan%2520Gumhold%26entry.1292438233%3D%2520%2520Image%2520fusion%2520typically%2520employs%2520non-invertible%2520neural%2520networks%2520to%2520merge%250Amultiple%2520source%2520images%2520into%2520a%2520single%2520fused%2520image.%2520However%252C%2520for%2520clinical%250Aexperts%252C%2520solely%2520relying%2520on%2520fused%2520images%2520may%2520be%2520insufficient%2520for%2520making%250Adiagnostic%2520decisions%252C%2520as%2520the%2520fusion%2520mechanism%2520blends%2520features%2520from%2520source%250Aimages%252C%2520thereby%2520making%2520it%2520difficult%2520to%2520interpret%2520the%2520underlying%2520tumor%250Apathology.%2520We%2520introduce%2520FusionINN%252C%2520a%2520novel%2520decomposable%2520image%2520fusion%2520framework%252C%250Acapable%2520of%2520efficiently%2520generating%2520fused%2520images%2520and%2520also%2520decomposing%2520them%2520back%250Ato%2520the%2520source%2520images.%2520FusionINN%2520is%2520designed%2520to%2520be%2520bijective%2520by%2520including%2520a%250Alatent%2520image%2520alongside%2520the%2520fused%2520image%252C%2520while%2520ensuring%2520minimal%2520transfer%2520of%250Ainformation%2520from%2520the%2520source%2520images%2520to%2520the%2520latent%2520representation.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520investigate%2520the%2520decomposability%2520of%2520fused%250Aimages%252C%2520which%2520is%2520particularly%2520crucial%2520for%2520life-sensitive%2520applications%2520such%2520as%250Amedical%2520image%2520fusion%2520compared%2520to%2520other%2520tasks%2520like%2520multi-focus%2520or%2520multi-exposure%250Aimage%2520fusion.%2520Our%2520extensive%2520experimentation%2520validates%2520FusionINN%2520over%2520existing%250Adiscriminative%2520and%2520generative%2520fusion%2520methods%252C%2520both%2520subjectively%2520and%250Aobjectively.%2520Moreover%252C%2520compared%2520to%2520a%2520recent%2520denoising%2520diffusion-based%2520fusion%250Amodel%252C%2520our%2520approach%2520offers%2520faster%2520and%2520qualitatively%2520better%2520fusion%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15769v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionINN%3A%20Decomposable%20Image%20Fusion%20for%20Brain%20Tumor%20Monitoring&entry.906535625=Nishant%20Kumar%20and%20Ziyan%20Tao%20and%20Jaikirat%20Singh%20and%20Yang%20Li%20and%20Peiwen%20Sun%20and%20Binghui%20Zhao%20and%20Stefan%20Gumhold&entry.1292438233=%20%20Image%20fusion%20typically%20employs%20non-invertible%20neural%20networks%20to%20merge%0Amultiple%20source%20images%20into%20a%20single%20fused%20image.%20However%2C%20for%20clinical%0Aexperts%2C%20solely%20relying%20on%20fused%20images%20may%20be%20insufficient%20for%20making%0Adiagnostic%20decisions%2C%20as%20the%20fusion%20mechanism%20blends%20features%20from%20source%0Aimages%2C%20thereby%20making%20it%20difficult%20to%20interpret%20the%20underlying%20tumor%0Apathology.%20We%20introduce%20FusionINN%2C%20a%20novel%20decomposable%20image%20fusion%20framework%2C%0Acapable%20of%20efficiently%20generating%20fused%20images%20and%20also%20decomposing%20them%20back%0Ato%20the%20source%20images.%20FusionINN%20is%20designed%20to%20be%20bijective%20by%20including%20a%0Alatent%20image%20alongside%20the%20fused%20image%2C%20while%20ensuring%20minimal%20transfer%20of%0Ainformation%20from%20the%20source%20images%20to%20the%20latent%20representation.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20investigate%20the%20decomposability%20of%20fused%0Aimages%2C%20which%20is%20particularly%20crucial%20for%20life-sensitive%20applications%20such%20as%0Amedical%20image%20fusion%20compared%20to%20other%20tasks%20like%20multi-focus%20or%20multi-exposure%0Aimage%20fusion.%20Our%20extensive%20experimentation%20validates%20FusionINN%20over%20existing%0Adiscriminative%20and%20generative%20fusion%20methods%2C%20both%20subjectively%20and%0Aobjectively.%20Moreover%2C%20compared%20to%20a%20recent%20denoising%20diffusion-based%20fusion%0Amodel%2C%20our%20approach%20offers%20faster%20and%20qualitatively%20better%20fusion%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15769v3&entry.124074799=Read"},
{"title": "iMotion-LLM: Motion Prediction Instruction Tuning", "author": "Abdulwahab Felemban and Eslam Mohamed Bakr and Xiaoqian Shen and Jian Ding and Abduallah Mohamed and Mohamed Elhoseiny", "abstract": "  We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with\ntrajectory prediction, tailored to guide interactive multi-agent scenarios.\nDifferent from conventional motion prediction approaches, iMotion-LLM\ncapitalizes on textual instructions as key inputs for generating contextually\nrelevant trajectories.By enriching the real-world driving scenarios in the\nWaymo Open Dataset with textual motion instructions, we created InstructWaymo.\nLeveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned\nwith LoRA, to translate scene features into the LLM input space. iMotion-LLM\noffers significant advantages over conventional motion prediction models.\nFirst, it can generate trajectories that align with the provided instructions\nif it is a feasible direction. Second, when given an infeasible direction, it\ncan reject the instruction, thereby enhancing safety. These findings act as\nmilestones in empowering autonomous navigation systems to interpret and predict\nthe dynamics of multi-agent environments, laying the groundwork for future\nadvancements in this field.\n", "link": "http://arxiv.org/abs/2406.06211v1", "date": "2024-06-10", "relevancy": 2.0956, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning&body=Title%3A%20iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning%0AAuthor%3A%20Abdulwahab%20Felemban%20and%20Eslam%20Mohamed%20Bakr%20and%20Xiaoqian%20Shen%20and%20Jian%20Ding%20and%20Abduallah%20Mohamed%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20We%20introduce%20iMotion-LLM%3A%20a%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20with%0Atrajectory%20prediction%2C%20tailored%20to%20guide%20interactive%20multi-agent%20scenarios.%0ADifferent%20from%20conventional%20motion%20prediction%20approaches%2C%20iMotion-LLM%0Acapitalizes%20on%20textual%20instructions%20as%20key%20inputs%20for%20generating%20contextually%0Arelevant%20trajectories.By%20enriching%20the%20real-world%20driving%20scenarios%20in%20the%0AWaymo%20Open%20Dataset%20with%20textual%20motion%20instructions%2C%20we%20created%20InstructWaymo.%0ALeveraging%20this%20dataset%2C%20iMotion-LLM%20integrates%20a%20pretrained%20LLM%2C%20fine-tuned%0Awith%20LoRA%2C%20to%20translate%20scene%20features%20into%20the%20LLM%20input%20space.%20iMotion-LLM%0Aoffers%20significant%20advantages%20over%20conventional%20motion%20prediction%20models.%0AFirst%2C%20it%20can%20generate%20trajectories%20that%20align%20with%20the%20provided%20instructions%0Aif%20it%20is%20a%20feasible%20direction.%20Second%2C%20when%20given%20an%20infeasible%20direction%2C%20it%0Acan%20reject%20the%20instruction%2C%20thereby%20enhancing%20safety.%20These%20findings%20act%20as%0Amilestones%20in%20empowering%20autonomous%20navigation%20systems%20to%20interpret%20and%20predict%0Athe%20dynamics%20of%20multi-agent%20environments%2C%20laying%20the%20groundwork%20for%20future%0Aadvancements%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMotion-LLM%253A%2520Motion%2520Prediction%2520Instruction%2520Tuning%26entry.906535625%3DAbdulwahab%2520Felemban%2520and%2520Eslam%2520Mohamed%2520Bakr%2520and%2520Xiaoqian%2520Shen%2520and%2520Jian%2520Ding%2520and%2520Abduallah%2520Mohamed%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520We%2520introduce%2520iMotion-LLM%253A%2520a%2520Multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%250Atrajectory%2520prediction%252C%2520tailored%2520to%2520guide%2520interactive%2520multi-agent%2520scenarios.%250ADifferent%2520from%2520conventional%2520motion%2520prediction%2520approaches%252C%2520iMotion-LLM%250Acapitalizes%2520on%2520textual%2520instructions%2520as%2520key%2520inputs%2520for%2520generating%2520contextually%250Arelevant%2520trajectories.By%2520enriching%2520the%2520real-world%2520driving%2520scenarios%2520in%2520the%250AWaymo%2520Open%2520Dataset%2520with%2520textual%2520motion%2520instructions%252C%2520we%2520created%2520InstructWaymo.%250ALeveraging%2520this%2520dataset%252C%2520iMotion-LLM%2520integrates%2520a%2520pretrained%2520LLM%252C%2520fine-tuned%250Awith%2520LoRA%252C%2520to%2520translate%2520scene%2520features%2520into%2520the%2520LLM%2520input%2520space.%2520iMotion-LLM%250Aoffers%2520significant%2520advantages%2520over%2520conventional%2520motion%2520prediction%2520models.%250AFirst%252C%2520it%2520can%2520generate%2520trajectories%2520that%2520align%2520with%2520the%2520provided%2520instructions%250Aif%2520it%2520is%2520a%2520feasible%2520direction.%2520Second%252C%2520when%2520given%2520an%2520infeasible%2520direction%252C%2520it%250Acan%2520reject%2520the%2520instruction%252C%2520thereby%2520enhancing%2520safety.%2520These%2520findings%2520act%2520as%250Amilestones%2520in%2520empowering%2520autonomous%2520navigation%2520systems%2520to%2520interpret%2520and%2520predict%250Athe%2520dynamics%2520of%2520multi-agent%2520environments%252C%2520laying%2520the%2520groundwork%2520for%2520future%250Aadvancements%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning&entry.906535625=Abdulwahab%20Felemban%20and%20Eslam%20Mohamed%20Bakr%20and%20Xiaoqian%20Shen%20and%20Jian%20Ding%20and%20Abduallah%20Mohamed%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20We%20introduce%20iMotion-LLM%3A%20a%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20with%0Atrajectory%20prediction%2C%20tailored%20to%20guide%20interactive%20multi-agent%20scenarios.%0ADifferent%20from%20conventional%20motion%20prediction%20approaches%2C%20iMotion-LLM%0Acapitalizes%20on%20textual%20instructions%20as%20key%20inputs%20for%20generating%20contextually%0Arelevant%20trajectories.By%20enriching%20the%20real-world%20driving%20scenarios%20in%20the%0AWaymo%20Open%20Dataset%20with%20textual%20motion%20instructions%2C%20we%20created%20InstructWaymo.%0ALeveraging%20this%20dataset%2C%20iMotion-LLM%20integrates%20a%20pretrained%20LLM%2C%20fine-tuned%0Awith%20LoRA%2C%20to%20translate%20scene%20features%20into%20the%20LLM%20input%20space.%20iMotion-LLM%0Aoffers%20significant%20advantages%20over%20conventional%20motion%20prediction%20models.%0AFirst%2C%20it%20can%20generate%20trajectories%20that%20align%20with%20the%20provided%20instructions%0Aif%20it%20is%20a%20feasible%20direction.%20Second%2C%20when%20given%20an%20infeasible%20direction%2C%20it%0Acan%20reject%20the%20instruction%2C%20thereby%20enhancing%20safety.%20These%20findings%20act%20as%0Amilestones%20in%20empowering%20autonomous%20navigation%20systems%20to%20interpret%20and%20predict%0Athe%20dynamics%20of%20multi-agent%20environments%2C%20laying%20the%20groundwork%20for%20future%0Aadvancements%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06211v1&entry.124074799=Read"},
{"title": "Benchmarking Counterfactual Image Generation", "author": "Thomas Melistas and Nikos Spyrou and Nefeli Gkouti and Pedro Sanchez and Athanasios Vlontzos and Yannis Panagakis and Giorgos Papanastasiou and Sotirios A. Tsaftaris", "abstract": "  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on.\n", "link": "http://arxiv.org/abs/2403.20287v2", "date": "2024-06-10", "relevancy": 2.0952, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5307}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5237}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Counterfactual%20Image%20Generation&body=Title%3A%20Benchmarking%20Counterfactual%20Image%20Generation%0AAuthor%3A%20Thomas%20Melistas%20and%20Nikos%20Spyrou%20and%20Nefeli%20Gkouti%20and%20Pedro%20Sanchez%20and%20Athanasios%20Vlontzos%20and%20Yannis%20Panagakis%20and%20Giorgos%20Papanastasiou%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Generative%20AI%20has%20revolutionised%20visual%20content%20editing%2C%20empowering%20users%20to%0Aeffortlessly%20modify%20images%20and%20videos.%20However%2C%20not%20all%20edits%20are%20equal.%20To%0Aperform%20realistic%20edits%20in%20domains%20such%20as%20natural%20image%20or%20medical%20imaging%2C%0Amodifications%20must%20respect%20causal%20relationships%20inherent%20to%20the%20data%20generation%0Aprocess.%20Such%20image%20editing%20falls%20into%20the%20counterfactual%20image%20generation%0Aregime.%20Evaluating%20counterfactual%20image%20generation%20is%20substantially%20complex%3A%0Anot%20only%20it%20lacks%20observable%20ground%20truths%2C%20but%20also%20requires%20adherence%20to%0Acausal%20constraints.%20Although%20several%20counterfactual%20image%20generation%20methods%0Aand%20evaluation%20metrics%20exist%2C%20a%20comprehensive%20comparison%20within%20a%20unified%0Asetting%20is%20lacking.%20We%20present%20a%20comparison%20framework%20to%20thoroughly%20benchmark%0Acounterfactual%20image%20generation%20methods.%20We%20integrate%20all%20models%20that%20have%20been%0Aused%20for%20the%20task%20at%20hand%20and%20expand%20them%20to%20novel%20datasets%20and%20causal%20graphs%2C%0Ademonstrating%20the%20superiority%20of%20Hierarchical%20VAEs%20across%20most%20datasets%20and%0Ametrics.%20Our%20framework%20is%20implemented%20in%20a%20user-friendly%20Python%20package%20that%0Acan%20be%20extended%20to%20incorporate%20additional%20SCMs%2C%20causal%20methods%2C%20generative%0Amodels%2C%20and%20datasets%20for%20the%20community%20to%20build%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Counterfactual%2520Image%2520Generation%26entry.906535625%3DThomas%2520Melistas%2520and%2520Nikos%2520Spyrou%2520and%2520Nefeli%2520Gkouti%2520and%2520Pedro%2520Sanchez%2520and%2520Athanasios%2520Vlontzos%2520and%2520Yannis%2520Panagakis%2520and%2520Giorgos%2520Papanastasiou%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520revolutionised%2520visual%2520content%2520editing%252C%2520empowering%2520users%2520to%250Aeffortlessly%2520modify%2520images%2520and%2520videos.%2520However%252C%2520not%2520all%2520edits%2520are%2520equal.%2520To%250Aperform%2520realistic%2520edits%2520in%2520domains%2520such%2520as%2520natural%2520image%2520or%2520medical%2520imaging%252C%250Amodifications%2520must%2520respect%2520causal%2520relationships%2520inherent%2520to%2520the%2520data%2520generation%250Aprocess.%2520Such%2520image%2520editing%2520falls%2520into%2520the%2520counterfactual%2520image%2520generation%250Aregime.%2520Evaluating%2520counterfactual%2520image%2520generation%2520is%2520substantially%2520complex%253A%250Anot%2520only%2520it%2520lacks%2520observable%2520ground%2520truths%252C%2520but%2520also%2520requires%2520adherence%2520to%250Acausal%2520constraints.%2520Although%2520several%2520counterfactual%2520image%2520generation%2520methods%250Aand%2520evaluation%2520metrics%2520exist%252C%2520a%2520comprehensive%2520comparison%2520within%2520a%2520unified%250Asetting%2520is%2520lacking.%2520We%2520present%2520a%2520comparison%2520framework%2520to%2520thoroughly%2520benchmark%250Acounterfactual%2520image%2520generation%2520methods.%2520We%2520integrate%2520all%2520models%2520that%2520have%2520been%250Aused%2520for%2520the%2520task%2520at%2520hand%2520and%2520expand%2520them%2520to%2520novel%2520datasets%2520and%2520causal%2520graphs%252C%250Ademonstrating%2520the%2520superiority%2520of%2520Hierarchical%2520VAEs%2520across%2520most%2520datasets%2520and%250Ametrics.%2520Our%2520framework%2520is%2520implemented%2520in%2520a%2520user-friendly%2520Python%2520package%2520that%250Acan%2520be%2520extended%2520to%2520incorporate%2520additional%2520SCMs%252C%2520causal%2520methods%252C%2520generative%250Amodels%252C%2520and%2520datasets%2520for%2520the%2520community%2520to%2520build%2520on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Counterfactual%20Image%20Generation&entry.906535625=Thomas%20Melistas%20and%20Nikos%20Spyrou%20and%20Nefeli%20Gkouti%20and%20Pedro%20Sanchez%20and%20Athanasios%20Vlontzos%20and%20Yannis%20Panagakis%20and%20Giorgos%20Papanastasiou%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Generative%20AI%20has%20revolutionised%20visual%20content%20editing%2C%20empowering%20users%20to%0Aeffortlessly%20modify%20images%20and%20videos.%20However%2C%20not%20all%20edits%20are%20equal.%20To%0Aperform%20realistic%20edits%20in%20domains%20such%20as%20natural%20image%20or%20medical%20imaging%2C%0Amodifications%20must%20respect%20causal%20relationships%20inherent%20to%20the%20data%20generation%0Aprocess.%20Such%20image%20editing%20falls%20into%20the%20counterfactual%20image%20generation%0Aregime.%20Evaluating%20counterfactual%20image%20generation%20is%20substantially%20complex%3A%0Anot%20only%20it%20lacks%20observable%20ground%20truths%2C%20but%20also%20requires%20adherence%20to%0Acausal%20constraints.%20Although%20several%20counterfactual%20image%20generation%20methods%0Aand%20evaluation%20metrics%20exist%2C%20a%20comprehensive%20comparison%20within%20a%20unified%0Asetting%20is%20lacking.%20We%20present%20a%20comparison%20framework%20to%20thoroughly%20benchmark%0Acounterfactual%20image%20generation%20methods.%20We%20integrate%20all%20models%20that%20have%20been%0Aused%20for%20the%20task%20at%20hand%20and%20expand%20them%20to%20novel%20datasets%20and%20causal%20graphs%2C%0Ademonstrating%20the%20superiority%20of%20Hierarchical%20VAEs%20across%20most%20datasets%20and%0Ametrics.%20Our%20framework%20is%20implemented%20in%20a%20user-friendly%20Python%20package%20that%0Acan%20be%20extended%20to%20incorporate%20additional%20SCMs%2C%20causal%20methods%2C%20generative%0Amodels%2C%20and%20datasets%20for%20the%20community%20to%20build%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20287v2&entry.124074799=Read"},
{"title": "FreeVA: Offline MLLM as Training-Free Video Assistant", "author": "Wenhao Wu", "abstract": "  This paper undertakes an empirical study to revisit the latest advancements\nin Multimodal Large Language Models (MLLMs): Video Assistant. This study,\nnamely FreeVA, aims to extend existing image-based MLLM to the video domain in\na training-free manner. The study provides an essential, yet must-know\nbaseline, and reveals several surprising findings: 1) FreeVA, leveraging only\noffline image-based MLLM without additional training, excels in zero-shot video\nquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even\nsurpassing state-of-the-art methods that involve video instruction tuning. 2)\nWhile mainstream video-based MLLMs typically initialize with an image-based\nMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study\nindicates that utilizing the widely adopted VideoInstruct-100K for video\ninstruction tuning doesn't actually lead to better performance compared to not\ntraining at all. 3) The commonly used evaluation metrics in existing works are\nsignificantly influenced by changes in the GPT API version over time. If\nignored, this could affect the fairness and uniformity of comparisons between\ndifferent methods and impact the analysis and judgment of researchers in the\nfield. The advancement of MLLMs is currently thriving, drawing numerous\nresearchers into the field. We aim for this work to serve as a plug-and-play,\nsimple yet effective baseline, encouraging the direct evaluation of existing\nMLLMs in video domain while also standardizing the field of video\nconversational models to a certain extent. Also, we encourage researchers to\nreconsider: Have current video MLLM methods truly acquired knowledge beyond\nimage MLLM? Code is available at https://github.com/whwu95/FreeVA\n", "link": "http://arxiv.org/abs/2405.07798v2", "date": "2024-06-10", "relevancy": 2.0945, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5318}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant&body=Title%3A%20FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant%0AAuthor%3A%20Wenhao%20Wu%0AAbstract%3A%20%20%20This%20paper%20undertakes%20an%20empirical%20study%20to%20revisit%20the%20latest%20advancements%0Ain%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%3A%20Video%20Assistant.%20This%20study%2C%0Anamely%20FreeVA%2C%20aims%20to%20extend%20existing%20image-based%20MLLM%20to%20the%20video%20domain%20in%0Aa%20training-free%20manner.%20The%20study%20provides%20an%20essential%2C%20yet%20must-know%0Abaseline%2C%20and%20reveals%20several%20surprising%20findings%3A%201%29%20FreeVA%2C%20leveraging%20only%0Aoffline%20image-based%20MLLM%20without%20additional%20training%2C%20excels%20in%20zero-shot%20video%0Aquestion-answering%20%28e.g.%2C%20MSVD-QA%2C%20ActivityNet-QA%2C%20and%20MSRVTT-QA%29%2C%20even%0Asurpassing%20state-of-the-art%20methods%20that%20involve%20video%20instruction%20tuning.%202%29%0AWhile%20mainstream%20video-based%20MLLMs%20typically%20initialize%20with%20an%20image-based%0AMLLM%20%28e.g.%2C%20LLaVA%29%20and%20then%20fine-tune%20using%20video%20instruction%20tuning%2C%20the%20study%0Aindicates%20that%20utilizing%20the%20widely%20adopted%20VideoInstruct-100K%20for%20video%0Ainstruction%20tuning%20doesn%27t%20actually%20lead%20to%20better%20performance%20compared%20to%20not%0Atraining%20at%20all.%203%29%20The%20commonly%20used%20evaluation%20metrics%20in%20existing%20works%20are%0Asignificantly%20influenced%20by%20changes%20in%20the%20GPT%20API%20version%20over%20time.%20If%0Aignored%2C%20this%20could%20affect%20the%20fairness%20and%20uniformity%20of%20comparisons%20between%0Adifferent%20methods%20and%20impact%20the%20analysis%20and%20judgment%20of%20researchers%20in%20the%0Afield.%20The%20advancement%20of%20MLLMs%20is%20currently%20thriving%2C%20drawing%20numerous%0Aresearchers%20into%20the%20field.%20We%20aim%20for%20this%20work%20to%20serve%20as%20a%20plug-and-play%2C%0Asimple%20yet%20effective%20baseline%2C%20encouraging%20the%20direct%20evaluation%20of%20existing%0AMLLMs%20in%20video%20domain%20while%20also%20standardizing%20the%20field%20of%20video%0Aconversational%20models%20to%20a%20certain%20extent.%20Also%2C%20we%20encourage%20researchers%20to%0Areconsider%3A%20Have%20current%20video%20MLLM%20methods%20truly%20acquired%20knowledge%20beyond%0Aimage%20MLLM%3F%20Code%20is%20available%20at%20https%3A//github.com/whwu95/FreeVA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07798v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeVA%253A%2520Offline%2520MLLM%2520as%2520Training-Free%2520Video%2520Assistant%26entry.906535625%3DWenhao%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520undertakes%2520an%2520empirical%2520study%2520to%2520revisit%2520the%2520latest%2520advancements%250Ain%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%253A%2520Video%2520Assistant.%2520This%2520study%252C%250Anamely%2520FreeVA%252C%2520aims%2520to%2520extend%2520existing%2520image-based%2520MLLM%2520to%2520the%2520video%2520domain%2520in%250Aa%2520training-free%2520manner.%2520The%2520study%2520provides%2520an%2520essential%252C%2520yet%2520must-know%250Abaseline%252C%2520and%2520reveals%2520several%2520surprising%2520findings%253A%25201%2529%2520FreeVA%252C%2520leveraging%2520only%250Aoffline%2520image-based%2520MLLM%2520without%2520additional%2520training%252C%2520excels%2520in%2520zero-shot%2520video%250Aquestion-answering%2520%2528e.g.%252C%2520MSVD-QA%252C%2520ActivityNet-QA%252C%2520and%2520MSRVTT-QA%2529%252C%2520even%250Asurpassing%2520state-of-the-art%2520methods%2520that%2520involve%2520video%2520instruction%2520tuning.%25202%2529%250AWhile%2520mainstream%2520video-based%2520MLLMs%2520typically%2520initialize%2520with%2520an%2520image-based%250AMLLM%2520%2528e.g.%252C%2520LLaVA%2529%2520and%2520then%2520fine-tune%2520using%2520video%2520instruction%2520tuning%252C%2520the%2520study%250Aindicates%2520that%2520utilizing%2520the%2520widely%2520adopted%2520VideoInstruct-100K%2520for%2520video%250Ainstruction%2520tuning%2520doesn%2527t%2520actually%2520lead%2520to%2520better%2520performance%2520compared%2520to%2520not%250Atraining%2520at%2520all.%25203%2529%2520The%2520commonly%2520used%2520evaluation%2520metrics%2520in%2520existing%2520works%2520are%250Asignificantly%2520influenced%2520by%2520changes%2520in%2520the%2520GPT%2520API%2520version%2520over%2520time.%2520If%250Aignored%252C%2520this%2520could%2520affect%2520the%2520fairness%2520and%2520uniformity%2520of%2520comparisons%2520between%250Adifferent%2520methods%2520and%2520impact%2520the%2520analysis%2520and%2520judgment%2520of%2520researchers%2520in%2520the%250Afield.%2520The%2520advancement%2520of%2520MLLMs%2520is%2520currently%2520thriving%252C%2520drawing%2520numerous%250Aresearchers%2520into%2520the%2520field.%2520We%2520aim%2520for%2520this%2520work%2520to%2520serve%2520as%2520a%2520plug-and-play%252C%250Asimple%2520yet%2520effective%2520baseline%252C%2520encouraging%2520the%2520direct%2520evaluation%2520of%2520existing%250AMLLMs%2520in%2520video%2520domain%2520while%2520also%2520standardizing%2520the%2520field%2520of%2520video%250Aconversational%2520models%2520to%2520a%2520certain%2520extent.%2520Also%252C%2520we%2520encourage%2520researchers%2520to%250Areconsider%253A%2520Have%2520current%2520video%2520MLLM%2520methods%2520truly%2520acquired%2520knowledge%2520beyond%250Aimage%2520MLLM%253F%2520Code%2520is%2520available%2520at%2520https%253A//github.com/whwu95/FreeVA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07798v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant&entry.906535625=Wenhao%20Wu&entry.1292438233=%20%20This%20paper%20undertakes%20an%20empirical%20study%20to%20revisit%20the%20latest%20advancements%0Ain%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%3A%20Video%20Assistant.%20This%20study%2C%0Anamely%20FreeVA%2C%20aims%20to%20extend%20existing%20image-based%20MLLM%20to%20the%20video%20domain%20in%0Aa%20training-free%20manner.%20The%20study%20provides%20an%20essential%2C%20yet%20must-know%0Abaseline%2C%20and%20reveals%20several%20surprising%20findings%3A%201%29%20FreeVA%2C%20leveraging%20only%0Aoffline%20image-based%20MLLM%20without%20additional%20training%2C%20excels%20in%20zero-shot%20video%0Aquestion-answering%20%28e.g.%2C%20MSVD-QA%2C%20ActivityNet-QA%2C%20and%20MSRVTT-QA%29%2C%20even%0Asurpassing%20state-of-the-art%20methods%20that%20involve%20video%20instruction%20tuning.%202%29%0AWhile%20mainstream%20video-based%20MLLMs%20typically%20initialize%20with%20an%20image-based%0AMLLM%20%28e.g.%2C%20LLaVA%29%20and%20then%20fine-tune%20using%20video%20instruction%20tuning%2C%20the%20study%0Aindicates%20that%20utilizing%20the%20widely%20adopted%20VideoInstruct-100K%20for%20video%0Ainstruction%20tuning%20doesn%27t%20actually%20lead%20to%20better%20performance%20compared%20to%20not%0Atraining%20at%20all.%203%29%20The%20commonly%20used%20evaluation%20metrics%20in%20existing%20works%20are%0Asignificantly%20influenced%20by%20changes%20in%20the%20GPT%20API%20version%20over%20time.%20If%0Aignored%2C%20this%20could%20affect%20the%20fairness%20and%20uniformity%20of%20comparisons%20between%0Adifferent%20methods%20and%20impact%20the%20analysis%20and%20judgment%20of%20researchers%20in%20the%0Afield.%20The%20advancement%20of%20MLLMs%20is%20currently%20thriving%2C%20drawing%20numerous%0Aresearchers%20into%20the%20field.%20We%20aim%20for%20this%20work%20to%20serve%20as%20a%20plug-and-play%2C%0Asimple%20yet%20effective%20baseline%2C%20encouraging%20the%20direct%20evaluation%20of%20existing%0AMLLMs%20in%20video%20domain%20while%20also%20standardizing%20the%20field%20of%20video%0Aconversational%20models%20to%20a%20certain%20extent.%20Also%2C%20we%20encourage%20researchers%20to%0Areconsider%3A%20Have%20current%20video%20MLLM%20methods%20truly%20acquired%20knowledge%20beyond%0Aimage%20MLLM%3F%20Code%20is%20available%20at%20https%3A//github.com/whwu95/FreeVA%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07798v2&entry.124074799=Read"},
{"title": "U-TELL: Unsupervised Task Expert Lifelong Learning", "author": "Indu Solomon and Aye Phyu Phyu Aung and Uttam Kumar and Senthilnath Jayavelu", "abstract": "  Continual learning (CL) models are designed to learn new tasks arriving\nsequentially without re-training the network. However, real-world ML\napplications have very limited label information and these models suffer from\ncatastrophic forgetting. To address these issues, we propose an unsupervised CL\nmodel with task experts called Unsupervised Task Expert Lifelong Learning\n(U-TELL) to continually learn the data arriving in a sequence addressing\ncatastrophic forgetting. During training of U-TELL, we introduce a new expert\non arrival of a new task. Our proposed architecture has task experts, a\nstructured data generator and a task assigner. Each task expert is composed of\n3 blocks; i) a variational autoencoder to capture the task distribution and\nperform data abstraction, ii) a k-means clustering module, and iii) a structure\nextractor to preserve latent task data signature. During testing, task assigner\nselects a suitable expert to perform clustering. U-TELL does not store or\nreplay task samples, instead, we use generated structured samples to train the\ntask assigner. We compared U-TELL with five SOTA unsupervised CL methods.\nU-TELL outperformed all baselines on seven benchmarks and one industry dataset\nfor various CL scenarios with a training time over 6 times faster than the best\nperforming baseline.\n", "link": "http://arxiv.org/abs/2405.14623v2", "date": "2024-06-10", "relevancy": 2.0933, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning&body=Title%3A%20U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0AAuthor%3A%20Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20models%20are%20designed%20to%20learn%20new%20tasks%20arriving%0Asequentially%20without%20re-training%20the%20network.%20However%2C%20real-world%20ML%0Aapplications%20have%20very%20limited%20label%20information%20and%20these%20models%20suffer%20from%0Acatastrophic%20forgetting.%20To%20address%20these%20issues%2C%20we%20propose%20an%20unsupervised%20CL%0Amodel%20with%20task%20experts%20called%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0A%28U-TELL%29%20to%20continually%20learn%20the%20data%20arriving%20in%20a%20sequence%20addressing%0Acatastrophic%20forgetting.%20During%20training%20of%20U-TELL%2C%20we%20introduce%20a%20new%20expert%0Aon%20arrival%20of%20a%20new%20task.%20Our%20proposed%20architecture%20has%20task%20experts%2C%20a%0Astructured%20data%20generator%20and%20a%20task%20assigner.%20Each%20task%20expert%20is%20composed%20of%0A3%20blocks%3B%20i%29%20a%20variational%20autoencoder%20to%20capture%20the%20task%20distribution%20and%0Aperform%20data%20abstraction%2C%20ii%29%20a%20k-means%20clustering%20module%2C%20and%20iii%29%20a%20structure%0Aextractor%20to%20preserve%20latent%20task%20data%20signature.%20During%20testing%2C%20task%20assigner%0Aselects%20a%20suitable%20expert%20to%20perform%20clustering.%20U-TELL%20does%20not%20store%20or%0Areplay%20task%20samples%2C%20instead%2C%20we%20use%20generated%20structured%20samples%20to%20train%20the%0Atask%20assigner.%20We%20compared%20U-TELL%20with%20five%20SOTA%20unsupervised%20CL%20methods.%0AU-TELL%20outperformed%20all%20baselines%20on%20seven%20benchmarks%20and%20one%20industry%20dataset%0Afor%20various%20CL%20scenarios%20with%20a%20training%20time%20over%206%20times%20faster%20than%20the%20best%0Aperforming%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-TELL%253A%2520Unsupervised%2520Task%2520Expert%2520Lifelong%2520Learning%26entry.906535625%3DIndu%2520Solomon%2520and%2520Aye%2520Phyu%2520Phyu%2520Aung%2520and%2520Uttam%2520Kumar%2520and%2520Senthilnath%2520Jayavelu%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520models%2520are%2520designed%2520to%2520learn%2520new%2520tasks%2520arriving%250Asequentially%2520without%2520re-training%2520the%2520network.%2520However%252C%2520real-world%2520ML%250Aapplications%2520have%2520very%2520limited%2520label%2520information%2520and%2520these%2520models%2520suffer%2520from%250Acatastrophic%2520forgetting.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%2520unsupervised%2520CL%250Amodel%2520with%2520task%2520experts%2520called%2520Unsupervised%2520Task%2520Expert%2520Lifelong%2520Learning%250A%2528U-TELL%2529%2520to%2520continually%2520learn%2520the%2520data%2520arriving%2520in%2520a%2520sequence%2520addressing%250Acatastrophic%2520forgetting.%2520During%2520training%2520of%2520U-TELL%252C%2520we%2520introduce%2520a%2520new%2520expert%250Aon%2520arrival%2520of%2520a%2520new%2520task.%2520Our%2520proposed%2520architecture%2520has%2520task%2520experts%252C%2520a%250Astructured%2520data%2520generator%2520and%2520a%2520task%2520assigner.%2520Each%2520task%2520expert%2520is%2520composed%2520of%250A3%2520blocks%253B%2520i%2529%2520a%2520variational%2520autoencoder%2520to%2520capture%2520the%2520task%2520distribution%2520and%250Aperform%2520data%2520abstraction%252C%2520ii%2529%2520a%2520k-means%2520clustering%2520module%252C%2520and%2520iii%2529%2520a%2520structure%250Aextractor%2520to%2520preserve%2520latent%2520task%2520data%2520signature.%2520During%2520testing%252C%2520task%2520assigner%250Aselects%2520a%2520suitable%2520expert%2520to%2520perform%2520clustering.%2520U-TELL%2520does%2520not%2520store%2520or%250Areplay%2520task%2520samples%252C%2520instead%252C%2520we%2520use%2520generated%2520structured%2520samples%2520to%2520train%2520the%250Atask%2520assigner.%2520We%2520compared%2520U-TELL%2520with%2520five%2520SOTA%2520unsupervised%2520CL%2520methods.%250AU-TELL%2520outperformed%2520all%2520baselines%2520on%2520seven%2520benchmarks%2520and%2520one%2520industry%2520dataset%250Afor%2520various%2520CL%2520scenarios%2520with%2520a%2520training%2520time%2520over%25206%2520times%2520faster%2520than%2520the%2520best%250Aperforming%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning&entry.906535625=Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20models%20are%20designed%20to%20learn%20new%20tasks%20arriving%0Asequentially%20without%20re-training%20the%20network.%20However%2C%20real-world%20ML%0Aapplications%20have%20very%20limited%20label%20information%20and%20these%20models%20suffer%20from%0Acatastrophic%20forgetting.%20To%20address%20these%20issues%2C%20we%20propose%20an%20unsupervised%20CL%0Amodel%20with%20task%20experts%20called%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0A%28U-TELL%29%20to%20continually%20learn%20the%20data%20arriving%20in%20a%20sequence%20addressing%0Acatastrophic%20forgetting.%20During%20training%20of%20U-TELL%2C%20we%20introduce%20a%20new%20expert%0Aon%20arrival%20of%20a%20new%20task.%20Our%20proposed%20architecture%20has%20task%20experts%2C%20a%0Astructured%20data%20generator%20and%20a%20task%20assigner.%20Each%20task%20expert%20is%20composed%20of%0A3%20blocks%3B%20i%29%20a%20variational%20autoencoder%20to%20capture%20the%20task%20distribution%20and%0Aperform%20data%20abstraction%2C%20ii%29%20a%20k-means%20clustering%20module%2C%20and%20iii%29%20a%20structure%0Aextractor%20to%20preserve%20latent%20task%20data%20signature.%20During%20testing%2C%20task%20assigner%0Aselects%20a%20suitable%20expert%20to%20perform%20clustering.%20U-TELL%20does%20not%20store%20or%0Areplay%20task%20samples%2C%20instead%2C%20we%20use%20generated%20structured%20samples%20to%20train%20the%0Atask%20assigner.%20We%20compared%20U-TELL%20with%20five%20SOTA%20unsupervised%20CL%20methods.%0AU-TELL%20outperformed%20all%20baselines%20on%20seven%20benchmarks%20and%20one%20industry%20dataset%0Afor%20various%20CL%20scenarios%20with%20a%20training%20time%20over%206%20times%20faster%20than%20the%20best%0Aperforming%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14623v2&entry.124074799=Read"},
{"title": "A quantitative investigation for deployment of mobile collaborative\n  robots in high-value manufacturing", "author": "Amine Hifi and W. Jackson and C. Loukas and M. Shields and A. Poole and E. Mohseni and C. N. MacLeod and G. Dobie and S. G. Pierce and T. O'Hare and G. Munro and J. O'Brian-O'Reilly and R. W. K. Vithanage", "abstract": "  Component inspection is often the bottleneck in high-value manufacturing,\ndriving industries like aerospace toward automated inspection technologies.\nCurrent systems often employ fixed arm robots, but they lack the flexibility in\nadapting to new components or orientations Advanced mobile robotic platforms\nwith updated sensor technologies and algorithms have improved localization and\npath planning capabilities, making them ideal for bringing inspection processes\ndirectly to parts. However, mobile platforms introduce challenges in\nlocalization and maneuverability, leading to potential errors. Their positional\nuncertainty is higher than fixed systems due to the lack of a fixed calibrated\nlocation, posing challenges for position-sensitive inspection sensors.\nTherefore, it's essential to assess the positional accuracy and repeatability\nof mobile manipulator platforms. The KUKA KMR iiwa was chosen for its\ncollaborative features, robust build, and scalability within the KUKA product\nrange. The accuracy and repeatability of the mobile platform were evaluated\nthrough a series of tests to evaluate the performance of its integrated feature\nmapping, the effect of various speeds on positional accuracy, and the\nefficiency of the omnidirectional wheels for a range of translation\norientations. Experimental evaluation revealed that enabling feature mapping\nsubstantially improves the KUKA KMR iiwa's performance, with accuracy gains and\nerror reductions exceeding 90%. Repeatability errors were under 7 mm with\nmapping activated and around 2.5 mm in practical scenarios, demonstrating that\nmobile manipulators, incorporating both the manipulator and platform, can\nfulfil the precise requirements of industries with high precision needs.\nProviding a highly diverse alternative to traditional fixed-base industrial\nmanipulators.\n", "link": "http://arxiv.org/abs/2406.06353v1", "date": "2024-06-10", "relevancy": 2.0902, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5233}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20quantitative%20investigation%20for%20deployment%20of%20mobile%20collaborative%0A%20%20robots%20in%20high-value%20manufacturing&body=Title%3A%20A%20quantitative%20investigation%20for%20deployment%20of%20mobile%20collaborative%0A%20%20robots%20in%20high-value%20manufacturing%0AAuthor%3A%20Amine%20Hifi%20and%20W.%20Jackson%20and%20C.%20Loukas%20and%20M.%20Shields%20and%20A.%20Poole%20and%20E.%20Mohseni%20and%20C.%20N.%20MacLeod%20and%20G.%20Dobie%20and%20S.%20G.%20Pierce%20and%20T.%20O%27Hare%20and%20G.%20Munro%20and%20J.%20O%27Brian-O%27Reilly%20and%20R.%20W.%20K.%20Vithanage%0AAbstract%3A%20%20%20Component%20inspection%20is%20often%20the%20bottleneck%20in%20high-value%20manufacturing%2C%0Adriving%20industries%20like%20aerospace%20toward%20automated%20inspection%20technologies.%0ACurrent%20systems%20often%20employ%20fixed%20arm%20robots%2C%20but%20they%20lack%20the%20flexibility%20in%0Aadapting%20to%20new%20components%20or%20orientations%20Advanced%20mobile%20robotic%20platforms%0Awith%20updated%20sensor%20technologies%20and%20algorithms%20have%20improved%20localization%20and%0Apath%20planning%20capabilities%2C%20making%20them%20ideal%20for%20bringing%20inspection%20processes%0Adirectly%20to%20parts.%20However%2C%20mobile%20platforms%20introduce%20challenges%20in%0Alocalization%20and%20maneuverability%2C%20leading%20to%20potential%20errors.%20Their%20positional%0Auncertainty%20is%20higher%20than%20fixed%20systems%20due%20to%20the%20lack%20of%20a%20fixed%20calibrated%0Alocation%2C%20posing%20challenges%20for%20position-sensitive%20inspection%20sensors.%0ATherefore%2C%20it%27s%20essential%20to%20assess%20the%20positional%20accuracy%20and%20repeatability%0Aof%20mobile%20manipulator%20platforms.%20The%20KUKA%20KMR%20iiwa%20was%20chosen%20for%20its%0Acollaborative%20features%2C%20robust%20build%2C%20and%20scalability%20within%20the%20KUKA%20product%0Arange.%20The%20accuracy%20and%20repeatability%20of%20the%20mobile%20platform%20were%20evaluated%0Athrough%20a%20series%20of%20tests%20to%20evaluate%20the%20performance%20of%20its%20integrated%20feature%0Amapping%2C%20the%20effect%20of%20various%20speeds%20on%20positional%20accuracy%2C%20and%20the%0Aefficiency%20of%20the%20omnidirectional%20wheels%20for%20a%20range%20of%20translation%0Aorientations.%20Experimental%20evaluation%20revealed%20that%20enabling%20feature%20mapping%0Asubstantially%20improves%20the%20KUKA%20KMR%20iiwa%27s%20performance%2C%20with%20accuracy%20gains%20and%0Aerror%20reductions%20exceeding%2090%25.%20Repeatability%20errors%20were%20under%207%20mm%20with%0Amapping%20activated%20and%20around%202.5%20mm%20in%20practical%20scenarios%2C%20demonstrating%20that%0Amobile%20manipulators%2C%20incorporating%20both%20the%20manipulator%20and%20platform%2C%20can%0Afulfil%20the%20precise%20requirements%20of%20industries%20with%20high%20precision%20needs.%0AProviding%20a%20highly%20diverse%20alternative%20to%20traditional%20fixed-base%20industrial%0Amanipulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520quantitative%2520investigation%2520for%2520deployment%2520of%2520mobile%2520collaborative%250A%2520%2520robots%2520in%2520high-value%2520manufacturing%26entry.906535625%3DAmine%2520Hifi%2520and%2520W.%2520Jackson%2520and%2520C.%2520Loukas%2520and%2520M.%2520Shields%2520and%2520A.%2520Poole%2520and%2520E.%2520Mohseni%2520and%2520C.%2520N.%2520MacLeod%2520and%2520G.%2520Dobie%2520and%2520S.%2520G.%2520Pierce%2520and%2520T.%2520O%2527Hare%2520and%2520G.%2520Munro%2520and%2520J.%2520O%2527Brian-O%2527Reilly%2520and%2520R.%2520W.%2520K.%2520Vithanage%26entry.1292438233%3D%2520%2520Component%2520inspection%2520is%2520often%2520the%2520bottleneck%2520in%2520high-value%2520manufacturing%252C%250Adriving%2520industries%2520like%2520aerospace%2520toward%2520automated%2520inspection%2520technologies.%250ACurrent%2520systems%2520often%2520employ%2520fixed%2520arm%2520robots%252C%2520but%2520they%2520lack%2520the%2520flexibility%2520in%250Aadapting%2520to%2520new%2520components%2520or%2520orientations%2520Advanced%2520mobile%2520robotic%2520platforms%250Awith%2520updated%2520sensor%2520technologies%2520and%2520algorithms%2520have%2520improved%2520localization%2520and%250Apath%2520planning%2520capabilities%252C%2520making%2520them%2520ideal%2520for%2520bringing%2520inspection%2520processes%250Adirectly%2520to%2520parts.%2520However%252C%2520mobile%2520platforms%2520introduce%2520challenges%2520in%250Alocalization%2520and%2520maneuverability%252C%2520leading%2520to%2520potential%2520errors.%2520Their%2520positional%250Auncertainty%2520is%2520higher%2520than%2520fixed%2520systems%2520due%2520to%2520the%2520lack%2520of%2520a%2520fixed%2520calibrated%250Alocation%252C%2520posing%2520challenges%2520for%2520position-sensitive%2520inspection%2520sensors.%250ATherefore%252C%2520it%2527s%2520essential%2520to%2520assess%2520the%2520positional%2520accuracy%2520and%2520repeatability%250Aof%2520mobile%2520manipulator%2520platforms.%2520The%2520KUKA%2520KMR%2520iiwa%2520was%2520chosen%2520for%2520its%250Acollaborative%2520features%252C%2520robust%2520build%252C%2520and%2520scalability%2520within%2520the%2520KUKA%2520product%250Arange.%2520The%2520accuracy%2520and%2520repeatability%2520of%2520the%2520mobile%2520platform%2520were%2520evaluated%250Athrough%2520a%2520series%2520of%2520tests%2520to%2520evaluate%2520the%2520performance%2520of%2520its%2520integrated%2520feature%250Amapping%252C%2520the%2520effect%2520of%2520various%2520speeds%2520on%2520positional%2520accuracy%252C%2520and%2520the%250Aefficiency%2520of%2520the%2520omnidirectional%2520wheels%2520for%2520a%2520range%2520of%2520translation%250Aorientations.%2520Experimental%2520evaluation%2520revealed%2520that%2520enabling%2520feature%2520mapping%250Asubstantially%2520improves%2520the%2520KUKA%2520KMR%2520iiwa%2527s%2520performance%252C%2520with%2520accuracy%2520gains%2520and%250Aerror%2520reductions%2520exceeding%252090%2525.%2520Repeatability%2520errors%2520were%2520under%25207%2520mm%2520with%250Amapping%2520activated%2520and%2520around%25202.5%2520mm%2520in%2520practical%2520scenarios%252C%2520demonstrating%2520that%250Amobile%2520manipulators%252C%2520incorporating%2520both%2520the%2520manipulator%2520and%2520platform%252C%2520can%250Afulfil%2520the%2520precise%2520requirements%2520of%2520industries%2520with%2520high%2520precision%2520needs.%250AProviding%2520a%2520highly%2520diverse%2520alternative%2520to%2520traditional%2520fixed-base%2520industrial%250Amanipulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20quantitative%20investigation%20for%20deployment%20of%20mobile%20collaborative%0A%20%20robots%20in%20high-value%20manufacturing&entry.906535625=Amine%20Hifi%20and%20W.%20Jackson%20and%20C.%20Loukas%20and%20M.%20Shields%20and%20A.%20Poole%20and%20E.%20Mohseni%20and%20C.%20N.%20MacLeod%20and%20G.%20Dobie%20and%20S.%20G.%20Pierce%20and%20T.%20O%27Hare%20and%20G.%20Munro%20and%20J.%20O%27Brian-O%27Reilly%20and%20R.%20W.%20K.%20Vithanage&entry.1292438233=%20%20Component%20inspection%20is%20often%20the%20bottleneck%20in%20high-value%20manufacturing%2C%0Adriving%20industries%20like%20aerospace%20toward%20automated%20inspection%20technologies.%0ACurrent%20systems%20often%20employ%20fixed%20arm%20robots%2C%20but%20they%20lack%20the%20flexibility%20in%0Aadapting%20to%20new%20components%20or%20orientations%20Advanced%20mobile%20robotic%20platforms%0Awith%20updated%20sensor%20technologies%20and%20algorithms%20have%20improved%20localization%20and%0Apath%20planning%20capabilities%2C%20making%20them%20ideal%20for%20bringing%20inspection%20processes%0Adirectly%20to%20parts.%20However%2C%20mobile%20platforms%20introduce%20challenges%20in%0Alocalization%20and%20maneuverability%2C%20leading%20to%20potential%20errors.%20Their%20positional%0Auncertainty%20is%20higher%20than%20fixed%20systems%20due%20to%20the%20lack%20of%20a%20fixed%20calibrated%0Alocation%2C%20posing%20challenges%20for%20position-sensitive%20inspection%20sensors.%0ATherefore%2C%20it%27s%20essential%20to%20assess%20the%20positional%20accuracy%20and%20repeatability%0Aof%20mobile%20manipulator%20platforms.%20The%20KUKA%20KMR%20iiwa%20was%20chosen%20for%20its%0Acollaborative%20features%2C%20robust%20build%2C%20and%20scalability%20within%20the%20KUKA%20product%0Arange.%20The%20accuracy%20and%20repeatability%20of%20the%20mobile%20platform%20were%20evaluated%0Athrough%20a%20series%20of%20tests%20to%20evaluate%20the%20performance%20of%20its%20integrated%20feature%0Amapping%2C%20the%20effect%20of%20various%20speeds%20on%20positional%20accuracy%2C%20and%20the%0Aefficiency%20of%20the%20omnidirectional%20wheels%20for%20a%20range%20of%20translation%0Aorientations.%20Experimental%20evaluation%20revealed%20that%20enabling%20feature%20mapping%0Asubstantially%20improves%20the%20KUKA%20KMR%20iiwa%27s%20performance%2C%20with%20accuracy%20gains%20and%0Aerror%20reductions%20exceeding%2090%25.%20Repeatability%20errors%20were%20under%207%20mm%20with%0Amapping%20activated%20and%20around%202.5%20mm%20in%20practical%20scenarios%2C%20demonstrating%20that%0Amobile%20manipulators%2C%20incorporating%20both%20the%20manipulator%20and%20platform%2C%20can%0Afulfil%20the%20precise%20requirements%20of%20industries%20with%20high%20precision%20needs.%0AProviding%20a%20highly%20diverse%20alternative%20to%20traditional%20fixed-base%20industrial%0Amanipulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06353v1&entry.124074799=Read"},
{"title": "Reproducibility study of FairAC", "author": "Gijs de Jong and Macha J. Meijer and Derck W. E. Prinzhorn and Harold Ruiter", "abstract": "  This work aims to reproduce the findings of the paper \"Fair Attribute\nCompletion on Graph with Missing Attributes\" written by Guo, Chu, and Li\narXiv:2302.12977 by investigating the claims made in the paper. This paper\nsuggests that the results of the original paper are reproducible and thus, the\nclaims hold. However, the claim that FairAC is a generic framework for many\ndownstream tasks is very broad and could therefore only be partially tested.\nMoreover, we show that FairAC is generalizable to various datasets and\nsensitive attributes and show evidence that the improvement in group fairness\nof the FairAC framework does not come at the expense of individual fairness.\nLastly, the codebase of FairAC has been refactored and is now easily applicable\nfor various datasets and models.\n", "link": "http://arxiv.org/abs/2406.03314v2", "date": "2024-06-10", "relevancy": 2.0865, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4221}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4169}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%20study%20of%20FairAC&body=Title%3A%20Reproducibility%20study%20of%20FairAC%0AAuthor%3A%20Gijs%20de%20Jong%20and%20Macha%20J.%20Meijer%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Harold%20Ruiter%0AAbstract%3A%20%20%20This%20work%20aims%20to%20reproduce%20the%20findings%20of%20the%20paper%20%22Fair%20Attribute%0ACompletion%20on%20Graph%20with%20Missing%20Attributes%22%20written%20by%20Guo%2C%20Chu%2C%20and%20Li%0AarXiv%3A2302.12977%20by%20investigating%20the%20claims%20made%20in%20the%20paper.%20This%20paper%0Asuggests%20that%20the%20results%20of%20the%20original%20paper%20are%20reproducible%20and%20thus%2C%20the%0Aclaims%20hold.%20However%2C%20the%20claim%20that%20FairAC%20is%20a%20generic%20framework%20for%20many%0Adownstream%20tasks%20is%20very%20broad%20and%20could%20therefore%20only%20be%20partially%20tested.%0AMoreover%2C%20we%20show%20that%20FairAC%20is%20generalizable%20to%20various%20datasets%20and%0Asensitive%20attributes%20and%20show%20evidence%20that%20the%20improvement%20in%20group%20fairness%0Aof%20the%20FairAC%20framework%20does%20not%20come%20at%20the%20expense%20of%20individual%20fairness.%0ALastly%2C%20the%20codebase%20of%20FairAC%20has%20been%20refactored%20and%20is%20now%20easily%20applicable%0Afor%20various%20datasets%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%2520study%2520of%2520FairAC%26entry.906535625%3DGijs%2520de%2520Jong%2520and%2520Macha%2520J.%2520Meijer%2520and%2520Derck%2520W.%2520E.%2520Prinzhorn%2520and%2520Harold%2520Ruiter%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520reproduce%2520the%2520findings%2520of%2520the%2520paper%2520%2522Fair%2520Attribute%250ACompletion%2520on%2520Graph%2520with%2520Missing%2520Attributes%2522%2520written%2520by%2520Guo%252C%2520Chu%252C%2520and%2520Li%250AarXiv%253A2302.12977%2520by%2520investigating%2520the%2520claims%2520made%2520in%2520the%2520paper.%2520This%2520paper%250Asuggests%2520that%2520the%2520results%2520of%2520the%2520original%2520paper%2520are%2520reproducible%2520and%2520thus%252C%2520the%250Aclaims%2520hold.%2520However%252C%2520the%2520claim%2520that%2520FairAC%2520is%2520a%2520generic%2520framework%2520for%2520many%250Adownstream%2520tasks%2520is%2520very%2520broad%2520and%2520could%2520therefore%2520only%2520be%2520partially%2520tested.%250AMoreover%252C%2520we%2520show%2520that%2520FairAC%2520is%2520generalizable%2520to%2520various%2520datasets%2520and%250Asensitive%2520attributes%2520and%2520show%2520evidence%2520that%2520the%2520improvement%2520in%2520group%2520fairness%250Aof%2520the%2520FairAC%2520framework%2520does%2520not%2520come%2520at%2520the%2520expense%2520of%2520individual%2520fairness.%250ALastly%252C%2520the%2520codebase%2520of%2520FairAC%2520has%2520been%2520refactored%2520and%2520is%2520now%2520easily%2520applicable%250Afor%2520various%2520datasets%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%20study%20of%20FairAC&entry.906535625=Gijs%20de%20Jong%20and%20Macha%20J.%20Meijer%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Harold%20Ruiter&entry.1292438233=%20%20This%20work%20aims%20to%20reproduce%20the%20findings%20of%20the%20paper%20%22Fair%20Attribute%0ACompletion%20on%20Graph%20with%20Missing%20Attributes%22%20written%20by%20Guo%2C%20Chu%2C%20and%20Li%0AarXiv%3A2302.12977%20by%20investigating%20the%20claims%20made%20in%20the%20paper.%20This%20paper%0Asuggests%20that%20the%20results%20of%20the%20original%20paper%20are%20reproducible%20and%20thus%2C%20the%0Aclaims%20hold.%20However%2C%20the%20claim%20that%20FairAC%20is%20a%20generic%20framework%20for%20many%0Adownstream%20tasks%20is%20very%20broad%20and%20could%20therefore%20only%20be%20partially%20tested.%0AMoreover%2C%20we%20show%20that%20FairAC%20is%20generalizable%20to%20various%20datasets%20and%0Asensitive%20attributes%20and%20show%20evidence%20that%20the%20improvement%20in%20group%20fairness%0Aof%20the%20FairAC%20framework%20does%20not%20come%20at%20the%20expense%20of%20individual%20fairness.%0ALastly%2C%20the%20codebase%20of%20FairAC%20has%20been%20refactored%20and%20is%20now%20easily%20applicable%0Afor%20various%20datasets%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03314v2&entry.124074799=Read"},
{"title": "Training Dynamics of Multi-Head Softmax Attention for In-Context\n  Learning: Emergence, Convergence, and Optimality", "author": "Siyu Chen and Heejune Sheen and Tianhao Wang and Zhuoran Yang", "abstract": "  We study the dynamics of gradient flow for training a multi-head softmax\nattention model for in-context learning of multi-task linear regression. We\nestablish the global convergence of gradient flow under suitable choices of\ninitialization. In addition, we prove that an interesting \"task allocation\"\nphenomenon emerges during the gradient flow dynamics, where each attention head\nfocuses on solving a single task of the multi-task model. Specifically, we\nprove that the gradient flow dynamics can be split into three phases -- a\nwarm-up phase where the loss decreases rather slowly and the attention heads\ngradually build up their inclination towards individual tasks, an emergence\nphase where each head selects a single task and the loss rapidly decreases, and\na convergence phase where the attention parameters converge to a limit.\nFurthermore, we prove the optimality of gradient flow in the sense that the\nlimiting model learned by gradient flow is on par with the best possible\nmulti-head softmax attention model up to a constant factor. Our analysis also\ndelineates a strict separation in terms of the prediction accuracy of ICL\nbetween single-head and multi-head attention models. The key technique for our\nconvergence analysis is to map the gradient flow dynamics in the parameter\nspace to a set of ordinary differential equations in the spectral domain, where\nthe relative magnitudes of the semi-singular values of the attention weights\ndetermines task allocation. To our best knowledge, our work provides the first\nconvergence result for the multi-head softmax attention model.\n", "link": "http://arxiv.org/abs/2402.19442v2", "date": "2024-06-10", "relevancy": 2.0787, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5253}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5202}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Dynamics%20of%20Multi-Head%20Softmax%20Attention%20for%20In-Context%0A%20%20Learning%3A%20Emergence%2C%20Convergence%2C%20and%20Optimality&body=Title%3A%20Training%20Dynamics%20of%20Multi-Head%20Softmax%20Attention%20for%20In-Context%0A%20%20Learning%3A%20Emergence%2C%20Convergence%2C%20and%20Optimality%0AAuthor%3A%20Siyu%20Chen%20and%20Heejune%20Sheen%20and%20Tianhao%20Wang%20and%20Zhuoran%20Yang%0AAbstract%3A%20%20%20We%20study%20the%20dynamics%20of%20gradient%20flow%20for%20training%20a%20multi-head%20softmax%0Aattention%20model%20for%20in-context%20learning%20of%20multi-task%20linear%20regression.%20We%0Aestablish%20the%20global%20convergence%20of%20gradient%20flow%20under%20suitable%20choices%20of%0Ainitialization.%20In%20addition%2C%20we%20prove%20that%20an%20interesting%20%22task%20allocation%22%0Aphenomenon%20emerges%20during%20the%20gradient%20flow%20dynamics%2C%20where%20each%20attention%20head%0Afocuses%20on%20solving%20a%20single%20task%20of%20the%20multi-task%20model.%20Specifically%2C%20we%0Aprove%20that%20the%20gradient%20flow%20dynamics%20can%20be%20split%20into%20three%20phases%20--%20a%0Awarm-up%20phase%20where%20the%20loss%20decreases%20rather%20slowly%20and%20the%20attention%20heads%0Agradually%20build%20up%20their%20inclination%20towards%20individual%20tasks%2C%20an%20emergence%0Aphase%20where%20each%20head%20selects%20a%20single%20task%20and%20the%20loss%20rapidly%20decreases%2C%20and%0Aa%20convergence%20phase%20where%20the%20attention%20parameters%20converge%20to%20a%20limit.%0AFurthermore%2C%20we%20prove%20the%20optimality%20of%20gradient%20flow%20in%20the%20sense%20that%20the%0Alimiting%20model%20learned%20by%20gradient%20flow%20is%20on%20par%20with%20the%20best%20possible%0Amulti-head%20softmax%20attention%20model%20up%20to%20a%20constant%20factor.%20Our%20analysis%20also%0Adelineates%20a%20strict%20separation%20in%20terms%20of%20the%20prediction%20accuracy%20of%20ICL%0Abetween%20single-head%20and%20multi-head%20attention%20models.%20The%20key%20technique%20for%20our%0Aconvergence%20analysis%20is%20to%20map%20the%20gradient%20flow%20dynamics%20in%20the%20parameter%0Aspace%20to%20a%20set%20of%20ordinary%20differential%20equations%20in%20the%20spectral%20domain%2C%20where%0Athe%20relative%20magnitudes%20of%20the%20semi-singular%20values%20of%20the%20attention%20weights%0Adetermines%20task%20allocation.%20To%20our%20best%20knowledge%2C%20our%20work%20provides%20the%20first%0Aconvergence%20result%20for%20the%20multi-head%20softmax%20attention%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Dynamics%2520of%2520Multi-Head%2520Softmax%2520Attention%2520for%2520In-Context%250A%2520%2520Learning%253A%2520Emergence%252C%2520Convergence%252C%2520and%2520Optimality%26entry.906535625%3DSiyu%2520Chen%2520and%2520Heejune%2520Sheen%2520and%2520Tianhao%2520Wang%2520and%2520Zhuoran%2520Yang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520dynamics%2520of%2520gradient%2520flow%2520for%2520training%2520a%2520multi-head%2520softmax%250Aattention%2520model%2520for%2520in-context%2520learning%2520of%2520multi-task%2520linear%2520regression.%2520We%250Aestablish%2520the%2520global%2520convergence%2520of%2520gradient%2520flow%2520under%2520suitable%2520choices%2520of%250Ainitialization.%2520In%2520addition%252C%2520we%2520prove%2520that%2520an%2520interesting%2520%2522task%2520allocation%2522%250Aphenomenon%2520emerges%2520during%2520the%2520gradient%2520flow%2520dynamics%252C%2520where%2520each%2520attention%2520head%250Afocuses%2520on%2520solving%2520a%2520single%2520task%2520of%2520the%2520multi-task%2520model.%2520Specifically%252C%2520we%250Aprove%2520that%2520the%2520gradient%2520flow%2520dynamics%2520can%2520be%2520split%2520into%2520three%2520phases%2520--%2520a%250Awarm-up%2520phase%2520where%2520the%2520loss%2520decreases%2520rather%2520slowly%2520and%2520the%2520attention%2520heads%250Agradually%2520build%2520up%2520their%2520inclination%2520towards%2520individual%2520tasks%252C%2520an%2520emergence%250Aphase%2520where%2520each%2520head%2520selects%2520a%2520single%2520task%2520and%2520the%2520loss%2520rapidly%2520decreases%252C%2520and%250Aa%2520convergence%2520phase%2520where%2520the%2520attention%2520parameters%2520converge%2520to%2520a%2520limit.%250AFurthermore%252C%2520we%2520prove%2520the%2520optimality%2520of%2520gradient%2520flow%2520in%2520the%2520sense%2520that%2520the%250Alimiting%2520model%2520learned%2520by%2520gradient%2520flow%2520is%2520on%2520par%2520with%2520the%2520best%2520possible%250Amulti-head%2520softmax%2520attention%2520model%2520up%2520to%2520a%2520constant%2520factor.%2520Our%2520analysis%2520also%250Adelineates%2520a%2520strict%2520separation%2520in%2520terms%2520of%2520the%2520prediction%2520accuracy%2520of%2520ICL%250Abetween%2520single-head%2520and%2520multi-head%2520attention%2520models.%2520The%2520key%2520technique%2520for%2520our%250Aconvergence%2520analysis%2520is%2520to%2520map%2520the%2520gradient%2520flow%2520dynamics%2520in%2520the%2520parameter%250Aspace%2520to%2520a%2520set%2520of%2520ordinary%2520differential%2520equations%2520in%2520the%2520spectral%2520domain%252C%2520where%250Athe%2520relative%2520magnitudes%2520of%2520the%2520semi-singular%2520values%2520of%2520the%2520attention%2520weights%250Adetermines%2520task%2520allocation.%2520To%2520our%2520best%2520knowledge%252C%2520our%2520work%2520provides%2520the%2520first%250Aconvergence%2520result%2520for%2520the%2520multi-head%2520softmax%2520attention%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Dynamics%20of%20Multi-Head%20Softmax%20Attention%20for%20In-Context%0A%20%20Learning%3A%20Emergence%2C%20Convergence%2C%20and%20Optimality&entry.906535625=Siyu%20Chen%20and%20Heejune%20Sheen%20and%20Tianhao%20Wang%20and%20Zhuoran%20Yang&entry.1292438233=%20%20We%20study%20the%20dynamics%20of%20gradient%20flow%20for%20training%20a%20multi-head%20softmax%0Aattention%20model%20for%20in-context%20learning%20of%20multi-task%20linear%20regression.%20We%0Aestablish%20the%20global%20convergence%20of%20gradient%20flow%20under%20suitable%20choices%20of%0Ainitialization.%20In%20addition%2C%20we%20prove%20that%20an%20interesting%20%22task%20allocation%22%0Aphenomenon%20emerges%20during%20the%20gradient%20flow%20dynamics%2C%20where%20each%20attention%20head%0Afocuses%20on%20solving%20a%20single%20task%20of%20the%20multi-task%20model.%20Specifically%2C%20we%0Aprove%20that%20the%20gradient%20flow%20dynamics%20can%20be%20split%20into%20three%20phases%20--%20a%0Awarm-up%20phase%20where%20the%20loss%20decreases%20rather%20slowly%20and%20the%20attention%20heads%0Agradually%20build%20up%20their%20inclination%20towards%20individual%20tasks%2C%20an%20emergence%0Aphase%20where%20each%20head%20selects%20a%20single%20task%20and%20the%20loss%20rapidly%20decreases%2C%20and%0Aa%20convergence%20phase%20where%20the%20attention%20parameters%20converge%20to%20a%20limit.%0AFurthermore%2C%20we%20prove%20the%20optimality%20of%20gradient%20flow%20in%20the%20sense%20that%20the%0Alimiting%20model%20learned%20by%20gradient%20flow%20is%20on%20par%20with%20the%20best%20possible%0Amulti-head%20softmax%20attention%20model%20up%20to%20a%20constant%20factor.%20Our%20analysis%20also%0Adelineates%20a%20strict%20separation%20in%20terms%20of%20the%20prediction%20accuracy%20of%20ICL%0Abetween%20single-head%20and%20multi-head%20attention%20models.%20The%20key%20technique%20for%20our%0Aconvergence%20analysis%20is%20to%20map%20the%20gradient%20flow%20dynamics%20in%20the%20parameter%0Aspace%20to%20a%20set%20of%20ordinary%20differential%20equations%20in%20the%20spectral%20domain%2C%20where%0Athe%20relative%20magnitudes%20of%20the%20semi-singular%20values%20of%20the%20attention%20weights%0Adetermines%20task%20allocation.%20To%20our%20best%20knowledge%2C%20our%20work%20provides%20the%20first%0Aconvergence%20result%20for%20the%20multi-head%20softmax%20attention%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19442v2&entry.124074799=Read"},
{"title": "Multifidelity digital twin for real-time monitoring of structural\n  dynamics in aquaculture net cages", "author": "Eirini Katsidoniotaki and Biao Su and Eleni Kelasidi and Themistoklis P. Sapsis", "abstract": "  As the global population grows and climate change intensifies, sustainable\nfood production is critical. Marine aquaculture offers a viable solution,\nproviding a sustainable protein source. However, the industry's expansion\nrequires novel technologies for remote management and autonomous operations.\nDigital twin technology can advance the aquaculture industry, but its adoption\nhas been limited. Fish net cages, which are flexible floating structures, are\ncritical yet vulnerable components of aquaculture farms. Exposed to harsh and\ndynamic marine environments, the cages experience significant loads and risk\ndamage, leading to fish escapes, environmental impacts, and financial losses.\nWe propose a multifidelity surrogate modeling framework for integration into a\ndigital twin for real-time monitoring of aquaculture net cage structural\ndynamics under stochastic marine conditions. Central to this framework is the\nnonlinear autoregressive Gaussian process method, which learns complex,\nnonlinear cross-correlations between models of varying fidelity. It combines\nlow-fidelity simulation data with a small set of high-fidelity field sensor\nmeasurements, which offer the real dynamics but are costly and spatially\nsparse. Validated at the SINTEF ACE fish farm in Norway, our digital twin\nreceives online metocean data and accurately predicts net cage displacements\nand mooring line loads, aligning closely with field measurements. The proposed\nframework is beneficial where application-specific data are scarce, offering\nrapid predictions and real-time system representation. The developed digital\ntwin prevents potential damages by assessing structural integrity and\nfacilitates remote operations with unmanned underwater vehicles. Our work also\ncompares GP and GCNs for predicting net cage deformation, highlighting the\nlatter's effectiveness in complex structural applications.\n", "link": "http://arxiv.org/abs/2406.04519v2", "date": "2024-06-10", "relevancy": 2.0642, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multifidelity%20digital%20twin%20for%20real-time%20monitoring%20of%20structural%0A%20%20dynamics%20in%20aquaculture%20net%20cages&body=Title%3A%20Multifidelity%20digital%20twin%20for%20real-time%20monitoring%20of%20structural%0A%20%20dynamics%20in%20aquaculture%20net%20cages%0AAuthor%3A%20Eirini%20Katsidoniotaki%20and%20Biao%20Su%20and%20Eleni%20Kelasidi%20and%20Themistoklis%20P.%20Sapsis%0AAbstract%3A%20%20%20As%20the%20global%20population%20grows%20and%20climate%20change%20intensifies%2C%20sustainable%0Afood%20production%20is%20critical.%20Marine%20aquaculture%20offers%20a%20viable%20solution%2C%0Aproviding%20a%20sustainable%20protein%20source.%20However%2C%20the%20industry%27s%20expansion%0Arequires%20novel%20technologies%20for%20remote%20management%20and%20autonomous%20operations.%0ADigital%20twin%20technology%20can%20advance%20the%20aquaculture%20industry%2C%20but%20its%20adoption%0Ahas%20been%20limited.%20Fish%20net%20cages%2C%20which%20are%20flexible%20floating%20structures%2C%20are%0Acritical%20yet%20vulnerable%20components%20of%20aquaculture%20farms.%20Exposed%20to%20harsh%20and%0Adynamic%20marine%20environments%2C%20the%20cages%20experience%20significant%20loads%20and%20risk%0Adamage%2C%20leading%20to%20fish%20escapes%2C%20environmental%20impacts%2C%20and%20financial%20losses.%0AWe%20propose%20a%20multifidelity%20surrogate%20modeling%20framework%20for%20integration%20into%20a%0Adigital%20twin%20for%20real-time%20monitoring%20of%20aquaculture%20net%20cage%20structural%0Adynamics%20under%20stochastic%20marine%20conditions.%20Central%20to%20this%20framework%20is%20the%0Anonlinear%20autoregressive%20Gaussian%20process%20method%2C%20which%20learns%20complex%2C%0Anonlinear%20cross-correlations%20between%20models%20of%20varying%20fidelity.%20It%20combines%0Alow-fidelity%20simulation%20data%20with%20a%20small%20set%20of%20high-fidelity%20field%20sensor%0Ameasurements%2C%20which%20offer%20the%20real%20dynamics%20but%20are%20costly%20and%20spatially%0Asparse.%20Validated%20at%20the%20SINTEF%20ACE%20fish%20farm%20in%20Norway%2C%20our%20digital%20twin%0Areceives%20online%20metocean%20data%20and%20accurately%20predicts%20net%20cage%20displacements%0Aand%20mooring%20line%20loads%2C%20aligning%20closely%20with%20field%20measurements.%20The%20proposed%0Aframework%20is%20beneficial%20where%20application-specific%20data%20are%20scarce%2C%20offering%0Arapid%20predictions%20and%20real-time%20system%20representation.%20The%20developed%20digital%0Atwin%20prevents%20potential%20damages%20by%20assessing%20structural%20integrity%20and%0Afacilitates%20remote%20operations%20with%20unmanned%20underwater%20vehicles.%20Our%20work%20also%0Acompares%20GP%20and%20GCNs%20for%20predicting%20net%20cage%20deformation%2C%20highlighting%20the%0Alatter%27s%20effectiveness%20in%20complex%20structural%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultifidelity%2520digital%2520twin%2520for%2520real-time%2520monitoring%2520of%2520structural%250A%2520%2520dynamics%2520in%2520aquaculture%2520net%2520cages%26entry.906535625%3DEirini%2520Katsidoniotaki%2520and%2520Biao%2520Su%2520and%2520Eleni%2520Kelasidi%2520and%2520Themistoklis%2520P.%2520Sapsis%26entry.1292438233%3D%2520%2520As%2520the%2520global%2520population%2520grows%2520and%2520climate%2520change%2520intensifies%252C%2520sustainable%250Afood%2520production%2520is%2520critical.%2520Marine%2520aquaculture%2520offers%2520a%2520viable%2520solution%252C%250Aproviding%2520a%2520sustainable%2520protein%2520source.%2520However%252C%2520the%2520industry%2527s%2520expansion%250Arequires%2520novel%2520technologies%2520for%2520remote%2520management%2520and%2520autonomous%2520operations.%250ADigital%2520twin%2520technology%2520can%2520advance%2520the%2520aquaculture%2520industry%252C%2520but%2520its%2520adoption%250Ahas%2520been%2520limited.%2520Fish%2520net%2520cages%252C%2520which%2520are%2520flexible%2520floating%2520structures%252C%2520are%250Acritical%2520yet%2520vulnerable%2520components%2520of%2520aquaculture%2520farms.%2520Exposed%2520to%2520harsh%2520and%250Adynamic%2520marine%2520environments%252C%2520the%2520cages%2520experience%2520significant%2520loads%2520and%2520risk%250Adamage%252C%2520leading%2520to%2520fish%2520escapes%252C%2520environmental%2520impacts%252C%2520and%2520financial%2520losses.%250AWe%2520propose%2520a%2520multifidelity%2520surrogate%2520modeling%2520framework%2520for%2520integration%2520into%2520a%250Adigital%2520twin%2520for%2520real-time%2520monitoring%2520of%2520aquaculture%2520net%2520cage%2520structural%250Adynamics%2520under%2520stochastic%2520marine%2520conditions.%2520Central%2520to%2520this%2520framework%2520is%2520the%250Anonlinear%2520autoregressive%2520Gaussian%2520process%2520method%252C%2520which%2520learns%2520complex%252C%250Anonlinear%2520cross-correlations%2520between%2520models%2520of%2520varying%2520fidelity.%2520It%2520combines%250Alow-fidelity%2520simulation%2520data%2520with%2520a%2520small%2520set%2520of%2520high-fidelity%2520field%2520sensor%250Ameasurements%252C%2520which%2520offer%2520the%2520real%2520dynamics%2520but%2520are%2520costly%2520and%2520spatially%250Asparse.%2520Validated%2520at%2520the%2520SINTEF%2520ACE%2520fish%2520farm%2520in%2520Norway%252C%2520our%2520digital%2520twin%250Areceives%2520online%2520metocean%2520data%2520and%2520accurately%2520predicts%2520net%2520cage%2520displacements%250Aand%2520mooring%2520line%2520loads%252C%2520aligning%2520closely%2520with%2520field%2520measurements.%2520The%2520proposed%250Aframework%2520is%2520beneficial%2520where%2520application-specific%2520data%2520are%2520scarce%252C%2520offering%250Arapid%2520predictions%2520and%2520real-time%2520system%2520representation.%2520The%2520developed%2520digital%250Atwin%2520prevents%2520potential%2520damages%2520by%2520assessing%2520structural%2520integrity%2520and%250Afacilitates%2520remote%2520operations%2520with%2520unmanned%2520underwater%2520vehicles.%2520Our%2520work%2520also%250Acompares%2520GP%2520and%2520GCNs%2520for%2520predicting%2520net%2520cage%2520deformation%252C%2520highlighting%2520the%250Alatter%2527s%2520effectiveness%2520in%2520complex%2520structural%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multifidelity%20digital%20twin%20for%20real-time%20monitoring%20of%20structural%0A%20%20dynamics%20in%20aquaculture%20net%20cages&entry.906535625=Eirini%20Katsidoniotaki%20and%20Biao%20Su%20and%20Eleni%20Kelasidi%20and%20Themistoklis%20P.%20Sapsis&entry.1292438233=%20%20As%20the%20global%20population%20grows%20and%20climate%20change%20intensifies%2C%20sustainable%0Afood%20production%20is%20critical.%20Marine%20aquaculture%20offers%20a%20viable%20solution%2C%0Aproviding%20a%20sustainable%20protein%20source.%20However%2C%20the%20industry%27s%20expansion%0Arequires%20novel%20technologies%20for%20remote%20management%20and%20autonomous%20operations.%0ADigital%20twin%20technology%20can%20advance%20the%20aquaculture%20industry%2C%20but%20its%20adoption%0Ahas%20been%20limited.%20Fish%20net%20cages%2C%20which%20are%20flexible%20floating%20structures%2C%20are%0Acritical%20yet%20vulnerable%20components%20of%20aquaculture%20farms.%20Exposed%20to%20harsh%20and%0Adynamic%20marine%20environments%2C%20the%20cages%20experience%20significant%20loads%20and%20risk%0Adamage%2C%20leading%20to%20fish%20escapes%2C%20environmental%20impacts%2C%20and%20financial%20losses.%0AWe%20propose%20a%20multifidelity%20surrogate%20modeling%20framework%20for%20integration%20into%20a%0Adigital%20twin%20for%20real-time%20monitoring%20of%20aquaculture%20net%20cage%20structural%0Adynamics%20under%20stochastic%20marine%20conditions.%20Central%20to%20this%20framework%20is%20the%0Anonlinear%20autoregressive%20Gaussian%20process%20method%2C%20which%20learns%20complex%2C%0Anonlinear%20cross-correlations%20between%20models%20of%20varying%20fidelity.%20It%20combines%0Alow-fidelity%20simulation%20data%20with%20a%20small%20set%20of%20high-fidelity%20field%20sensor%0Ameasurements%2C%20which%20offer%20the%20real%20dynamics%20but%20are%20costly%20and%20spatially%0Asparse.%20Validated%20at%20the%20SINTEF%20ACE%20fish%20farm%20in%20Norway%2C%20our%20digital%20twin%0Areceives%20online%20metocean%20data%20and%20accurately%20predicts%20net%20cage%20displacements%0Aand%20mooring%20line%20loads%2C%20aligning%20closely%20with%20field%20measurements.%20The%20proposed%0Aframework%20is%20beneficial%20where%20application-specific%20data%20are%20scarce%2C%20offering%0Arapid%20predictions%20and%20real-time%20system%20representation.%20The%20developed%20digital%0Atwin%20prevents%20potential%20damages%20by%20assessing%20structural%20integrity%20and%0Afacilitates%20remote%20operations%20with%20unmanned%20underwater%20vehicles.%20Our%20work%20also%0Acompares%20GP%20and%20GCNs%20for%20predicting%20net%20cage%20deformation%2C%20highlighting%20the%0Alatter%27s%20effectiveness%20in%20complex%20structural%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04519v2&entry.124074799=Read"},
{"title": "NarrativeBridge: Enhancing Video Captioning with Causal-Temporal\n  Narrative", "author": "Asmar Nadeem and Faegheh Sardari and Robert Dawes and Syed Sameed Husain and Adrian Hilton and Armin Mustafa", "abstract": "  Existing video captioning benchmarks and models lack coherent representations\nof causal-temporal narrative, which is sequences of events linked through cause\nand effect, unfolding over time and driven by characters or agents. This lack\nof narrative restricts models' ability to generate text descriptions that\ncapture the causal and temporal dynamics inherent in video content. To address\nthis gap, we propose NarrativeBridge, an approach comprising of: (1) a novel\nCausal-Temporal Narrative (CTN) captions benchmark generated using a large\nlanguage model and few-shot prompting, explicitly encoding cause-effect\ntemporal relationships in video descriptions, evaluated automatically to ensure\ncaption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN)\narchitecture with separate encoders for capturing cause and effect dynamics\nindependently, enabling effective learning and generation of captions with\ncausal-temporal narrative. Extensive experiments demonstrate that CEN is more\naccurate in articulating the causal and temporal aspects of video content than\nthe second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT\ndatasets, respectively. The proposed framework understands and generates\nnuanced text descriptions with intricate causal-temporal narrative structures\npresent in videos, addressing a critical limitation in video captioning. For\nproject details, visit https://narrativebridge.github.io/.\n", "link": "http://arxiv.org/abs/2406.06499v1", "date": "2024-06-10", "relevancy": 2.0461, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5221}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5114}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NarrativeBridge%3A%20Enhancing%20Video%20Captioning%20with%20Causal-Temporal%0A%20%20Narrative&body=Title%3A%20NarrativeBridge%3A%20Enhancing%20Video%20Captioning%20with%20Causal-Temporal%0A%20%20Narrative%0AAuthor%3A%20Asmar%20Nadeem%20and%20Faegheh%20Sardari%20and%20Robert%20Dawes%20and%20Syed%20Sameed%20Husain%20and%20Adrian%20Hilton%20and%20Armin%20Mustafa%0AAbstract%3A%20%20%20Existing%20video%20captioning%20benchmarks%20and%20models%20lack%20coherent%20representations%0Aof%20causal-temporal%20narrative%2C%20which%20is%20sequences%20of%20events%20linked%20through%20cause%0Aand%20effect%2C%20unfolding%20over%20time%20and%20driven%20by%20characters%20or%20agents.%20This%20lack%0Aof%20narrative%20restricts%20models%27%20ability%20to%20generate%20text%20descriptions%20that%0Acapture%20the%20causal%20and%20temporal%20dynamics%20inherent%20in%20video%20content.%20To%20address%0Athis%20gap%2C%20we%20propose%20NarrativeBridge%2C%20an%20approach%20comprising%20of%3A%20%281%29%20a%20novel%0ACausal-Temporal%20Narrative%20%28CTN%29%20captions%20benchmark%20generated%20using%20a%20large%0Alanguage%20model%20and%20few-shot%20prompting%2C%20explicitly%20encoding%20cause-effect%0Atemporal%20relationships%20in%20video%20descriptions%2C%20evaluated%20automatically%20to%20ensure%0Acaption%20quality%20and%20relevance%3B%20and%20%282%29%20a%20dedicated%20Cause-Effect%20Network%20%28CEN%29%0Aarchitecture%20with%20separate%20encoders%20for%20capturing%20cause%20and%20effect%20dynamics%0Aindependently%2C%20enabling%20effective%20learning%20and%20generation%20of%20captions%20with%0Acausal-temporal%20narrative.%20Extensive%20experiments%20demonstrate%20that%20CEN%20is%20more%0Aaccurate%20in%20articulating%20the%20causal%20and%20temporal%20aspects%20of%20video%20content%20than%0Athe%20second%20best%20model%20%28GIT%29%3A%2017.88%20and%2017.44%20CIDEr%20on%20the%20MSVD%20and%20MSR-VTT%0Adatasets%2C%20respectively.%20The%20proposed%20framework%20understands%20and%20generates%0Anuanced%20text%20descriptions%20with%20intricate%20causal-temporal%20narrative%20structures%0Apresent%20in%20videos%2C%20addressing%20a%20critical%20limitation%20in%20video%20captioning.%20For%0Aproject%20details%2C%20visit%20https%3A//narrativebridge.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNarrativeBridge%253A%2520Enhancing%2520Video%2520Captioning%2520with%2520Causal-Temporal%250A%2520%2520Narrative%26entry.906535625%3DAsmar%2520Nadeem%2520and%2520Faegheh%2520Sardari%2520and%2520Robert%2520Dawes%2520and%2520Syed%2520Sameed%2520Husain%2520and%2520Adrian%2520Hilton%2520and%2520Armin%2520Mustafa%26entry.1292438233%3D%2520%2520Existing%2520video%2520captioning%2520benchmarks%2520and%2520models%2520lack%2520coherent%2520representations%250Aof%2520causal-temporal%2520narrative%252C%2520which%2520is%2520sequences%2520of%2520events%2520linked%2520through%2520cause%250Aand%2520effect%252C%2520unfolding%2520over%2520time%2520and%2520driven%2520by%2520characters%2520or%2520agents.%2520This%2520lack%250Aof%2520narrative%2520restricts%2520models%2527%2520ability%2520to%2520generate%2520text%2520descriptions%2520that%250Acapture%2520the%2520causal%2520and%2520temporal%2520dynamics%2520inherent%2520in%2520video%2520content.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520NarrativeBridge%252C%2520an%2520approach%2520comprising%2520of%253A%2520%25281%2529%2520a%2520novel%250ACausal-Temporal%2520Narrative%2520%2528CTN%2529%2520captions%2520benchmark%2520generated%2520using%2520a%2520large%250Alanguage%2520model%2520and%2520few-shot%2520prompting%252C%2520explicitly%2520encoding%2520cause-effect%250Atemporal%2520relationships%2520in%2520video%2520descriptions%252C%2520evaluated%2520automatically%2520to%2520ensure%250Acaption%2520quality%2520and%2520relevance%253B%2520and%2520%25282%2529%2520a%2520dedicated%2520Cause-Effect%2520Network%2520%2528CEN%2529%250Aarchitecture%2520with%2520separate%2520encoders%2520for%2520capturing%2520cause%2520and%2520effect%2520dynamics%250Aindependently%252C%2520enabling%2520effective%2520learning%2520and%2520generation%2520of%2520captions%2520with%250Acausal-temporal%2520narrative.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CEN%2520is%2520more%250Aaccurate%2520in%2520articulating%2520the%2520causal%2520and%2520temporal%2520aspects%2520of%2520video%2520content%2520than%250Athe%2520second%2520best%2520model%2520%2528GIT%2529%253A%252017.88%2520and%252017.44%2520CIDEr%2520on%2520the%2520MSVD%2520and%2520MSR-VTT%250Adatasets%252C%2520respectively.%2520The%2520proposed%2520framework%2520understands%2520and%2520generates%250Anuanced%2520text%2520descriptions%2520with%2520intricate%2520causal-temporal%2520narrative%2520structures%250Apresent%2520in%2520videos%252C%2520addressing%2520a%2520critical%2520limitation%2520in%2520video%2520captioning.%2520For%250Aproject%2520details%252C%2520visit%2520https%253A//narrativebridge.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NarrativeBridge%3A%20Enhancing%20Video%20Captioning%20with%20Causal-Temporal%0A%20%20Narrative&entry.906535625=Asmar%20Nadeem%20and%20Faegheh%20Sardari%20and%20Robert%20Dawes%20and%20Syed%20Sameed%20Husain%20and%20Adrian%20Hilton%20and%20Armin%20Mustafa&entry.1292438233=%20%20Existing%20video%20captioning%20benchmarks%20and%20models%20lack%20coherent%20representations%0Aof%20causal-temporal%20narrative%2C%20which%20is%20sequences%20of%20events%20linked%20through%20cause%0Aand%20effect%2C%20unfolding%20over%20time%20and%20driven%20by%20characters%20or%20agents.%20This%20lack%0Aof%20narrative%20restricts%20models%27%20ability%20to%20generate%20text%20descriptions%20that%0Acapture%20the%20causal%20and%20temporal%20dynamics%20inherent%20in%20video%20content.%20To%20address%0Athis%20gap%2C%20we%20propose%20NarrativeBridge%2C%20an%20approach%20comprising%20of%3A%20%281%29%20a%20novel%0ACausal-Temporal%20Narrative%20%28CTN%29%20captions%20benchmark%20generated%20using%20a%20large%0Alanguage%20model%20and%20few-shot%20prompting%2C%20explicitly%20encoding%20cause-effect%0Atemporal%20relationships%20in%20video%20descriptions%2C%20evaluated%20automatically%20to%20ensure%0Acaption%20quality%20and%20relevance%3B%20and%20%282%29%20a%20dedicated%20Cause-Effect%20Network%20%28CEN%29%0Aarchitecture%20with%20separate%20encoders%20for%20capturing%20cause%20and%20effect%20dynamics%0Aindependently%2C%20enabling%20effective%20learning%20and%20generation%20of%20captions%20with%0Acausal-temporal%20narrative.%20Extensive%20experiments%20demonstrate%20that%20CEN%20is%20more%0Aaccurate%20in%20articulating%20the%20causal%20and%20temporal%20aspects%20of%20video%20content%20than%0Athe%20second%20best%20model%20%28GIT%29%3A%2017.88%20and%2017.44%20CIDEr%20on%20the%20MSVD%20and%20MSR-VTT%0Adatasets%2C%20respectively.%20The%20proposed%20framework%20understands%20and%20generates%0Anuanced%20text%20descriptions%20with%20intricate%20causal-temporal%20narrative%20structures%0Apresent%20in%20videos%2C%20addressing%20a%20critical%20limitation%20in%20video%20captioning.%20For%0Aproject%20details%2C%20visit%20https%3A//narrativebridge.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06499v1&entry.124074799=Read"},
{"title": "Point-VOS: Pointing Up Video Object Segmentation", "author": "Idil Esen Zulfikar and Sabarinath Mahadevan and Paul Voigtlaender and Bastian Leibe", "abstract": "  Current state-of-the-art Video Object Segmentation (VOS) methods rely on\ndense per-object mask annotations both during training and testing. This\nrequires time-consuming and costly video annotation mechanisms. We propose a\nnovel Point-VOS task with a spatio-temporally sparse point-wise annotation\nscheme that substantially reduces the annotation effort. We apply our\nannotation scheme to two large-scale video datasets with text descriptions and\nannotate over 19M points across 133K objects in 32K videos. Based on our\nannotations, we propose a new Point-VOS benchmark, and a corresponding\npoint-based training mechanism, which we use to establish strong baseline\nresults. We show that existing VOS methods can easily be adapted to leverage\nour point annotations during training, and can achieve results close to the\nfully-supervised performance when trained on pseudo-masks generated from these\npoints. In addition, we show that our data can be used to improve models that\nconnect vision and language, by evaluating it on the Video Narrative Grounding\n(VNG) task. We will make our code and annotations available at\nhttps://pointvos.github.io.\n", "link": "http://arxiv.org/abs/2402.05917v2", "date": "2024-06-10", "relevancy": 2.0448, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5219}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5095}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-VOS%3A%20Pointing%20Up%20Video%20Object%20Segmentation&body=Title%3A%20Point-VOS%3A%20Pointing%20Up%20Video%20Object%20Segmentation%0AAuthor%3A%20Idil%20Esen%20Zulfikar%20and%20Sabarinath%20Mahadevan%20and%20Paul%20Voigtlaender%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%20Current%20state-of-the-art%20Video%20Object%20Segmentation%20%28VOS%29%20methods%20rely%20on%0Adense%20per-object%20mask%20annotations%20both%20during%20training%20and%20testing.%20This%0Arequires%20time-consuming%20and%20costly%20video%20annotation%20mechanisms.%20We%20propose%20a%0Anovel%20Point-VOS%20task%20with%20a%20spatio-temporally%20sparse%20point-wise%20annotation%0Ascheme%20that%20substantially%20reduces%20the%20annotation%20effort.%20We%20apply%20our%0Aannotation%20scheme%20to%20two%20large-scale%20video%20datasets%20with%20text%20descriptions%20and%0Aannotate%20over%2019M%20points%20across%20133K%20objects%20in%2032K%20videos.%20Based%20on%20our%0Aannotations%2C%20we%20propose%20a%20new%20Point-VOS%20benchmark%2C%20and%20a%20corresponding%0Apoint-based%20training%20mechanism%2C%20which%20we%20use%20to%20establish%20strong%20baseline%0Aresults.%20We%20show%20that%20existing%20VOS%20methods%20can%20easily%20be%20adapted%20to%20leverage%0Aour%20point%20annotations%20during%20training%2C%20and%20can%20achieve%20results%20close%20to%20the%0Afully-supervised%20performance%20when%20trained%20on%20pseudo-masks%20generated%20from%20these%0Apoints.%20In%20addition%2C%20we%20show%20that%20our%20data%20can%20be%20used%20to%20improve%20models%20that%0Aconnect%20vision%20and%20language%2C%20by%20evaluating%20it%20on%20the%20Video%20Narrative%20Grounding%0A%28VNG%29%20task.%20We%20will%20make%20our%20code%20and%20annotations%20available%20at%0Ahttps%3A//pointvos.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-VOS%253A%2520Pointing%2520Up%2520Video%2520Object%2520Segmentation%26entry.906535625%3DIdil%2520Esen%2520Zulfikar%2520and%2520Sabarinath%2520Mahadevan%2520and%2520Paul%2520Voigtlaender%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520methods%2520rely%2520on%250Adense%2520per-object%2520mask%2520annotations%2520both%2520during%2520training%2520and%2520testing.%2520This%250Arequires%2520time-consuming%2520and%2520costly%2520video%2520annotation%2520mechanisms.%2520We%2520propose%2520a%250Anovel%2520Point-VOS%2520task%2520with%2520a%2520spatio-temporally%2520sparse%2520point-wise%2520annotation%250Ascheme%2520that%2520substantially%2520reduces%2520the%2520annotation%2520effort.%2520We%2520apply%2520our%250Aannotation%2520scheme%2520to%2520two%2520large-scale%2520video%2520datasets%2520with%2520text%2520descriptions%2520and%250Aannotate%2520over%252019M%2520points%2520across%2520133K%2520objects%2520in%252032K%2520videos.%2520Based%2520on%2520our%250Aannotations%252C%2520we%2520propose%2520a%2520new%2520Point-VOS%2520benchmark%252C%2520and%2520a%2520corresponding%250Apoint-based%2520training%2520mechanism%252C%2520which%2520we%2520use%2520to%2520establish%2520strong%2520baseline%250Aresults.%2520We%2520show%2520that%2520existing%2520VOS%2520methods%2520can%2520easily%2520be%2520adapted%2520to%2520leverage%250Aour%2520point%2520annotations%2520during%2520training%252C%2520and%2520can%2520achieve%2520results%2520close%2520to%2520the%250Afully-supervised%2520performance%2520when%2520trained%2520on%2520pseudo-masks%2520generated%2520from%2520these%250Apoints.%2520In%2520addition%252C%2520we%2520show%2520that%2520our%2520data%2520can%2520be%2520used%2520to%2520improve%2520models%2520that%250Aconnect%2520vision%2520and%2520language%252C%2520by%2520evaluating%2520it%2520on%2520the%2520Video%2520Narrative%2520Grounding%250A%2528VNG%2529%2520task.%2520We%2520will%2520make%2520our%2520code%2520and%2520annotations%2520available%2520at%250Ahttps%253A//pointvos.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-VOS%3A%20Pointing%20Up%20Video%20Object%20Segmentation&entry.906535625=Idil%20Esen%20Zulfikar%20and%20Sabarinath%20Mahadevan%20and%20Paul%20Voigtlaender%20and%20Bastian%20Leibe&entry.1292438233=%20%20Current%20state-of-the-art%20Video%20Object%20Segmentation%20%28VOS%29%20methods%20rely%20on%0Adense%20per-object%20mask%20annotations%20both%20during%20training%20and%20testing.%20This%0Arequires%20time-consuming%20and%20costly%20video%20annotation%20mechanisms.%20We%20propose%20a%0Anovel%20Point-VOS%20task%20with%20a%20spatio-temporally%20sparse%20point-wise%20annotation%0Ascheme%20that%20substantially%20reduces%20the%20annotation%20effort.%20We%20apply%20our%0Aannotation%20scheme%20to%20two%20large-scale%20video%20datasets%20with%20text%20descriptions%20and%0Aannotate%20over%2019M%20points%20across%20133K%20objects%20in%2032K%20videos.%20Based%20on%20our%0Aannotations%2C%20we%20propose%20a%20new%20Point-VOS%20benchmark%2C%20and%20a%20corresponding%0Apoint-based%20training%20mechanism%2C%20which%20we%20use%20to%20establish%20strong%20baseline%0Aresults.%20We%20show%20that%20existing%20VOS%20methods%20can%20easily%20be%20adapted%20to%20leverage%0Aour%20point%20annotations%20during%20training%2C%20and%20can%20achieve%20results%20close%20to%20the%0Afully-supervised%20performance%20when%20trained%20on%20pseudo-masks%20generated%20from%20these%0Apoints.%20In%20addition%2C%20we%20show%20that%20our%20data%20can%20be%20used%20to%20improve%20models%20that%0Aconnect%20vision%20and%20language%2C%20by%20evaluating%20it%20on%20the%20Video%20Narrative%20Grounding%0A%28VNG%29%20task.%20We%20will%20make%20our%20code%20and%20annotations%20available%20at%0Ahttps%3A//pointvos.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05917v2&entry.124074799=Read"},
{"title": "ProAct: Progressive Training for Hybrid Clipped Activation Function to\n  Enhance Resilience of DNNs", "author": "Seyedhamidreza Mousavi and Mohammad Hasan Ahmadilivani and Jaan Raik and Maksim Jenihhin and Masoud Daneshtalab", "abstract": "  Deep Neural Networks (DNNs) are extensively employed in safety-critical\napplications where ensuring hardware reliability is a primary concern. To\nenhance the reliability of DNNs against hardware faults, activation restriction\ntechniques significantly mitigate the fault effects at the DNN structure level,\nirrespective of accelerator architectures. State-of-the-art methods offer\neither neuron-wise or layer-wise clipping activation functions. They attempt to\ndetermine optimal clipping thresholds using heuristic and learning-based\napproaches. Layer-wise clipped activation functions cannot preserve DNNs\nresilience at high bit error rates. On the other hand, neuron-wise clipping\nactivation functions introduce considerable memory overhead due to the addition\nof parameters, which increases their vulnerability to faults. Moreover, the\nheuristic-based optimization approach demands numerous fault injections during\nthe search process, resulting in time-consuming threshold identification. On\nthe other hand, learning-based techniques that train thresholds for entire\nlayers concurrently often yield sub-optimal results. In this work, first, we\ndemonstrate that it is not essential to incorporate neuron-wise activation\nfunctions throughout all layers in DNNs. Then, we propose a hybrid clipped\nactivation function that integrates neuron-wise and layer-wise methods that\napply neuron-wise clipping only in the last layer of DNNs. Additionally, to\nattain optimal thresholds in the clipping activation function, we introduce\nProAct, a progressive training methodology. This approach iteratively trains\nthe thresholds on a layer-by-layer basis, aiming to obtain optimal threshold\nvalues in each layer separately.\n", "link": "http://arxiv.org/abs/2406.06313v1", "date": "2024-06-10", "relevancy": 2.0372, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5544}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.489}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProAct%3A%20Progressive%20Training%20for%20Hybrid%20Clipped%20Activation%20Function%20to%0A%20%20Enhance%20Resilience%20of%20DNNs&body=Title%3A%20ProAct%3A%20Progressive%20Training%20for%20Hybrid%20Clipped%20Activation%20Function%20to%0A%20%20Enhance%20Resilience%20of%20DNNs%0AAuthor%3A%20Seyedhamidreza%20Mousavi%20and%20Mohammad%20Hasan%20Ahmadilivani%20and%20Jaan%20Raik%20and%20Maksim%20Jenihhin%20and%20Masoud%20Daneshtalab%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20extensively%20employed%20in%20safety-critical%0Aapplications%20where%20ensuring%20hardware%20reliability%20is%20a%20primary%20concern.%20To%0Aenhance%20the%20reliability%20of%20DNNs%20against%20hardware%20faults%2C%20activation%20restriction%0Atechniques%20significantly%20mitigate%20the%20fault%20effects%20at%20the%20DNN%20structure%20level%2C%0Airrespective%20of%20accelerator%20architectures.%20State-of-the-art%20methods%20offer%0Aeither%20neuron-wise%20or%20layer-wise%20clipping%20activation%20functions.%20They%20attempt%20to%0Adetermine%20optimal%20clipping%20thresholds%20using%20heuristic%20and%20learning-based%0Aapproaches.%20Layer-wise%20clipped%20activation%20functions%20cannot%20preserve%20DNNs%0Aresilience%20at%20high%20bit%20error%20rates.%20On%20the%20other%20hand%2C%20neuron-wise%20clipping%0Aactivation%20functions%20introduce%20considerable%20memory%20overhead%20due%20to%20the%20addition%0Aof%20parameters%2C%20which%20increases%20their%20vulnerability%20to%20faults.%20Moreover%2C%20the%0Aheuristic-based%20optimization%20approach%20demands%20numerous%20fault%20injections%20during%0Athe%20search%20process%2C%20resulting%20in%20time-consuming%20threshold%20identification.%20On%0Athe%20other%20hand%2C%20learning-based%20techniques%20that%20train%20thresholds%20for%20entire%0Alayers%20concurrently%20often%20yield%20sub-optimal%20results.%20In%20this%20work%2C%20first%2C%20we%0Ademonstrate%20that%20it%20is%20not%20essential%20to%20incorporate%20neuron-wise%20activation%0Afunctions%20throughout%20all%20layers%20in%20DNNs.%20Then%2C%20we%20propose%20a%20hybrid%20clipped%0Aactivation%20function%20that%20integrates%20neuron-wise%20and%20layer-wise%20methods%20that%0Aapply%20neuron-wise%20clipping%20only%20in%20the%20last%20layer%20of%20DNNs.%20Additionally%2C%20to%0Aattain%20optimal%20thresholds%20in%20the%20clipping%20activation%20function%2C%20we%20introduce%0AProAct%2C%20a%20progressive%20training%20methodology.%20This%20approach%20iteratively%20trains%0Athe%20thresholds%20on%20a%20layer-by-layer%20basis%2C%20aiming%20to%20obtain%20optimal%20threshold%0Avalues%20in%20each%20layer%20separately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProAct%253A%2520Progressive%2520Training%2520for%2520Hybrid%2520Clipped%2520Activation%2520Function%2520to%250A%2520%2520Enhance%2520Resilience%2520of%2520DNNs%26entry.906535625%3DSeyedhamidreza%2520Mousavi%2520and%2520Mohammad%2520Hasan%2520Ahmadilivani%2520and%2520Jaan%2520Raik%2520and%2520Maksim%2520Jenihhin%2520and%2520Masoud%2520Daneshtalab%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520are%2520extensively%2520employed%2520in%2520safety-critical%250Aapplications%2520where%2520ensuring%2520hardware%2520reliability%2520is%2520a%2520primary%2520concern.%2520To%250Aenhance%2520the%2520reliability%2520of%2520DNNs%2520against%2520hardware%2520faults%252C%2520activation%2520restriction%250Atechniques%2520significantly%2520mitigate%2520the%2520fault%2520effects%2520at%2520the%2520DNN%2520structure%2520level%252C%250Airrespective%2520of%2520accelerator%2520architectures.%2520State-of-the-art%2520methods%2520offer%250Aeither%2520neuron-wise%2520or%2520layer-wise%2520clipping%2520activation%2520functions.%2520They%2520attempt%2520to%250Adetermine%2520optimal%2520clipping%2520thresholds%2520using%2520heuristic%2520and%2520learning-based%250Aapproaches.%2520Layer-wise%2520clipped%2520activation%2520functions%2520cannot%2520preserve%2520DNNs%250Aresilience%2520at%2520high%2520bit%2520error%2520rates.%2520On%2520the%2520other%2520hand%252C%2520neuron-wise%2520clipping%250Aactivation%2520functions%2520introduce%2520considerable%2520memory%2520overhead%2520due%2520to%2520the%2520addition%250Aof%2520parameters%252C%2520which%2520increases%2520their%2520vulnerability%2520to%2520faults.%2520Moreover%252C%2520the%250Aheuristic-based%2520optimization%2520approach%2520demands%2520numerous%2520fault%2520injections%2520during%250Athe%2520search%2520process%252C%2520resulting%2520in%2520time-consuming%2520threshold%2520identification.%2520On%250Athe%2520other%2520hand%252C%2520learning-based%2520techniques%2520that%2520train%2520thresholds%2520for%2520entire%250Alayers%2520concurrently%2520often%2520yield%2520sub-optimal%2520results.%2520In%2520this%2520work%252C%2520first%252C%2520we%250Ademonstrate%2520that%2520it%2520is%2520not%2520essential%2520to%2520incorporate%2520neuron-wise%2520activation%250Afunctions%2520throughout%2520all%2520layers%2520in%2520DNNs.%2520Then%252C%2520we%2520propose%2520a%2520hybrid%2520clipped%250Aactivation%2520function%2520that%2520integrates%2520neuron-wise%2520and%2520layer-wise%2520methods%2520that%250Aapply%2520neuron-wise%2520clipping%2520only%2520in%2520the%2520last%2520layer%2520of%2520DNNs.%2520Additionally%252C%2520to%250Aattain%2520optimal%2520thresholds%2520in%2520the%2520clipping%2520activation%2520function%252C%2520we%2520introduce%250AProAct%252C%2520a%2520progressive%2520training%2520methodology.%2520This%2520approach%2520iteratively%2520trains%250Athe%2520thresholds%2520on%2520a%2520layer-by-layer%2520basis%252C%2520aiming%2520to%2520obtain%2520optimal%2520threshold%250Avalues%2520in%2520each%2520layer%2520separately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProAct%3A%20Progressive%20Training%20for%20Hybrid%20Clipped%20Activation%20Function%20to%0A%20%20Enhance%20Resilience%20of%20DNNs&entry.906535625=Seyedhamidreza%20Mousavi%20and%20Mohammad%20Hasan%20Ahmadilivani%20and%20Jaan%20Raik%20and%20Maksim%20Jenihhin%20and%20Masoud%20Daneshtalab&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20extensively%20employed%20in%20safety-critical%0Aapplications%20where%20ensuring%20hardware%20reliability%20is%20a%20primary%20concern.%20To%0Aenhance%20the%20reliability%20of%20DNNs%20against%20hardware%20faults%2C%20activation%20restriction%0Atechniques%20significantly%20mitigate%20the%20fault%20effects%20at%20the%20DNN%20structure%20level%2C%0Airrespective%20of%20accelerator%20architectures.%20State-of-the-art%20methods%20offer%0Aeither%20neuron-wise%20or%20layer-wise%20clipping%20activation%20functions.%20They%20attempt%20to%0Adetermine%20optimal%20clipping%20thresholds%20using%20heuristic%20and%20learning-based%0Aapproaches.%20Layer-wise%20clipped%20activation%20functions%20cannot%20preserve%20DNNs%0Aresilience%20at%20high%20bit%20error%20rates.%20On%20the%20other%20hand%2C%20neuron-wise%20clipping%0Aactivation%20functions%20introduce%20considerable%20memory%20overhead%20due%20to%20the%20addition%0Aof%20parameters%2C%20which%20increases%20their%20vulnerability%20to%20faults.%20Moreover%2C%20the%0Aheuristic-based%20optimization%20approach%20demands%20numerous%20fault%20injections%20during%0Athe%20search%20process%2C%20resulting%20in%20time-consuming%20threshold%20identification.%20On%0Athe%20other%20hand%2C%20learning-based%20techniques%20that%20train%20thresholds%20for%20entire%0Alayers%20concurrently%20often%20yield%20sub-optimal%20results.%20In%20this%20work%2C%20first%2C%20we%0Ademonstrate%20that%20it%20is%20not%20essential%20to%20incorporate%20neuron-wise%20activation%0Afunctions%20throughout%20all%20layers%20in%20DNNs.%20Then%2C%20we%20propose%20a%20hybrid%20clipped%0Aactivation%20function%20that%20integrates%20neuron-wise%20and%20layer-wise%20methods%20that%0Aapply%20neuron-wise%20clipping%20only%20in%20the%20last%20layer%20of%20DNNs.%20Additionally%2C%20to%0Aattain%20optimal%20thresholds%20in%20the%20clipping%20activation%20function%2C%20we%20introduce%0AProAct%2C%20a%20progressive%20training%20methodology.%20This%20approach%20iteratively%20trains%0Athe%20thresholds%20on%20a%20layer-by-layer%20basis%2C%20aiming%20to%20obtain%20optimal%20threshold%0Avalues%20in%20each%20layer%20separately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06313v1&entry.124074799=Read"},
{"title": "Stabilized Adaptive Steering for 3D Sonar Microphone Arrays with IMU\n  Sensor Fusion", "author": "Wouter Jansen and Dennis Laurijssen and Jan Steckel", "abstract": "  This paper presents a novel software-based approach to stabilizing the\nacoustic images for in-air 3D sonars. Due to uneven terrain, traditional static\nbeamforming techniques can be misaligned, causing inaccurate measurements and\nimaging artifacts. Furthermore, mechanical stabilization can be more costly and\nprone to failure. We propose using an adaptive conventional beamforming\napproach by fusing it with real-time IMU data to adjust the sonar array's\nsteering matrix dynamically based on the elevation tilt angle caused by the\nuneven ground. Additionally, we propose gaining compensation to offset emission\nenergy loss due to the transducer's directivity pattern and validate our\napproach through various experiments, which show significant improvements in\ntemporal consistency in the acoustic images. We implemented a GPU-accelerated\nsoftware system that operates in real-time with an average execution time of\n210ms, meeting autonomous navigation requirements.\n", "link": "http://arxiv.org/abs/2406.06255v1", "date": "2024-06-10", "relevancy": 2.0278, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5097}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5074}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilized%20Adaptive%20Steering%20for%203D%20Sonar%20Microphone%20Arrays%20with%20IMU%0A%20%20Sensor%20Fusion&body=Title%3A%20Stabilized%20Adaptive%20Steering%20for%203D%20Sonar%20Microphone%20Arrays%20with%20IMU%0A%20%20Sensor%20Fusion%0AAuthor%3A%20Wouter%20Jansen%20and%20Dennis%20Laurijssen%20and%20Jan%20Steckel%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20software-based%20approach%20to%20stabilizing%20the%0Aacoustic%20images%20for%20in-air%203D%20sonars.%20Due%20to%20uneven%20terrain%2C%20traditional%20static%0Abeamforming%20techniques%20can%20be%20misaligned%2C%20causing%20inaccurate%20measurements%20and%0Aimaging%20artifacts.%20Furthermore%2C%20mechanical%20stabilization%20can%20be%20more%20costly%20and%0Aprone%20to%20failure.%20We%20propose%20using%20an%20adaptive%20conventional%20beamforming%0Aapproach%20by%20fusing%20it%20with%20real-time%20IMU%20data%20to%20adjust%20the%20sonar%20array%27s%0Asteering%20matrix%20dynamically%20based%20on%20the%20elevation%20tilt%20angle%20caused%20by%20the%0Auneven%20ground.%20Additionally%2C%20we%20propose%20gaining%20compensation%20to%20offset%20emission%0Aenergy%20loss%20due%20to%20the%20transducer%27s%20directivity%20pattern%20and%20validate%20our%0Aapproach%20through%20various%20experiments%2C%20which%20show%20significant%20improvements%20in%0Atemporal%20consistency%20in%20the%20acoustic%20images.%20We%20implemented%20a%20GPU-accelerated%0Asoftware%20system%20that%20operates%20in%20real-time%20with%20an%20average%20execution%20time%20of%0A210ms%2C%20meeting%20autonomous%20navigation%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilized%2520Adaptive%2520Steering%2520for%25203D%2520Sonar%2520Microphone%2520Arrays%2520with%2520IMU%250A%2520%2520Sensor%2520Fusion%26entry.906535625%3DWouter%2520Jansen%2520and%2520Dennis%2520Laurijssen%2520and%2520Jan%2520Steckel%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520software-based%2520approach%2520to%2520stabilizing%2520the%250Aacoustic%2520images%2520for%2520in-air%25203D%2520sonars.%2520Due%2520to%2520uneven%2520terrain%252C%2520traditional%2520static%250Abeamforming%2520techniques%2520can%2520be%2520misaligned%252C%2520causing%2520inaccurate%2520measurements%2520and%250Aimaging%2520artifacts.%2520Furthermore%252C%2520mechanical%2520stabilization%2520can%2520be%2520more%2520costly%2520and%250Aprone%2520to%2520failure.%2520We%2520propose%2520using%2520an%2520adaptive%2520conventional%2520beamforming%250Aapproach%2520by%2520fusing%2520it%2520with%2520real-time%2520IMU%2520data%2520to%2520adjust%2520the%2520sonar%2520array%2527s%250Asteering%2520matrix%2520dynamically%2520based%2520on%2520the%2520elevation%2520tilt%2520angle%2520caused%2520by%2520the%250Auneven%2520ground.%2520Additionally%252C%2520we%2520propose%2520gaining%2520compensation%2520to%2520offset%2520emission%250Aenergy%2520loss%2520due%2520to%2520the%2520transducer%2527s%2520directivity%2520pattern%2520and%2520validate%2520our%250Aapproach%2520through%2520various%2520experiments%252C%2520which%2520show%2520significant%2520improvements%2520in%250Atemporal%2520consistency%2520in%2520the%2520acoustic%2520images.%2520We%2520implemented%2520a%2520GPU-accelerated%250Asoftware%2520system%2520that%2520operates%2520in%2520real-time%2520with%2520an%2520average%2520execution%2520time%2520of%250A210ms%252C%2520meeting%2520autonomous%2520navigation%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilized%20Adaptive%20Steering%20for%203D%20Sonar%20Microphone%20Arrays%20with%20IMU%0A%20%20Sensor%20Fusion&entry.906535625=Wouter%20Jansen%20and%20Dennis%20Laurijssen%20and%20Jan%20Steckel&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20software-based%20approach%20to%20stabilizing%20the%0Aacoustic%20images%20for%20in-air%203D%20sonars.%20Due%20to%20uneven%20terrain%2C%20traditional%20static%0Abeamforming%20techniques%20can%20be%20misaligned%2C%20causing%20inaccurate%20measurements%20and%0Aimaging%20artifacts.%20Furthermore%2C%20mechanical%20stabilization%20can%20be%20more%20costly%20and%0Aprone%20to%20failure.%20We%20propose%20using%20an%20adaptive%20conventional%20beamforming%0Aapproach%20by%20fusing%20it%20with%20real-time%20IMU%20data%20to%20adjust%20the%20sonar%20array%27s%0Asteering%20matrix%20dynamically%20based%20on%20the%20elevation%20tilt%20angle%20caused%20by%20the%0Auneven%20ground.%20Additionally%2C%20we%20propose%20gaining%20compensation%20to%20offset%20emission%0Aenergy%20loss%20due%20to%20the%20transducer%27s%20directivity%20pattern%20and%20validate%20our%0Aapproach%20through%20various%20experiments%2C%20which%20show%20significant%20improvements%20in%0Atemporal%20consistency%20in%20the%20acoustic%20images.%20We%20implemented%20a%20GPU-accelerated%0Asoftware%20system%20that%20operates%20in%20real-time%20with%20an%20average%20execution%20time%20of%0A210ms%2C%20meeting%20autonomous%20navigation%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06255v1&entry.124074799=Read"},
{"title": "Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor\n  Microenvironment Subtype Prediction", "author": "Fangliangzi Meng and Hongrun Zhang and Ruodan Yan and Guohui Chuai and Chao Li and Qi Liu", "abstract": "  The characterization of Tumor MicroEnvironment (TME) is challenging due to\nits complexity and heterogeneity. Relatively consistent TME characteristics\nembedded within highly specific tissue features, render them difficult to\npredict. The capability to accurately classify TME subtypes is of critical\nsignificance for clinical tumor diagnosis and precision medicine. Based on the\nobservation that tumors with different origins share similar microenvironment\npatterns, we propose PathoTME, a genomics-guided Siamese representation\nlearning framework employing Whole Slide Image (WSI) for pan-cancer TME\nsubtypes prediction. Specifically, we utilize Siamese network to leverage\ngenomic information as a regularization factor to assist WSI embeddings\nlearning during the training phase. Additionally, we employ Domain Adversarial\nNeural Network (DANN) to mitigate the impact of tissue type variations. To\neliminate domain bias, a dynamic WSI prompt is designed to further unleash the\nmodel's capabilities. Our model achieves better performance than other\nstate-of-the-art methods across 23 cancer types on TCGA dataset. Our code is\navailable at https://github.com/Mengflz/PathoTME.\n", "link": "http://arxiv.org/abs/2406.06517v1", "date": "2024-06-10", "relevancy": 2.0252, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Genomics-guided%20Representation%20Learning%20for%20Pathologic%20Pan-cancer%20Tumor%0A%20%20Microenvironment%20Subtype%20Prediction&body=Title%3A%20Genomics-guided%20Representation%20Learning%20for%20Pathologic%20Pan-cancer%20Tumor%0A%20%20Microenvironment%20Subtype%20Prediction%0AAuthor%3A%20Fangliangzi%20Meng%20and%20Hongrun%20Zhang%20and%20Ruodan%20Yan%20and%20Guohui%20Chuai%20and%20Chao%20Li%20and%20Qi%20Liu%0AAbstract%3A%20%20%20The%20characterization%20of%20Tumor%20MicroEnvironment%20%28TME%29%20is%20challenging%20due%20to%0Aits%20complexity%20and%20heterogeneity.%20Relatively%20consistent%20TME%20characteristics%0Aembedded%20within%20highly%20specific%20tissue%20features%2C%20render%20them%20difficult%20to%0Apredict.%20The%20capability%20to%20accurately%20classify%20TME%20subtypes%20is%20of%20critical%0Asignificance%20for%20clinical%20tumor%20diagnosis%20and%20precision%20medicine.%20Based%20on%20the%0Aobservation%20that%20tumors%20with%20different%20origins%20share%20similar%20microenvironment%0Apatterns%2C%20we%20propose%20PathoTME%2C%20a%20genomics-guided%20Siamese%20representation%0Alearning%20framework%20employing%20Whole%20Slide%20Image%20%28WSI%29%20for%20pan-cancer%20TME%0Asubtypes%20prediction.%20Specifically%2C%20we%20utilize%20Siamese%20network%20to%20leverage%0Agenomic%20information%20as%20a%20regularization%20factor%20to%20assist%20WSI%20embeddings%0Alearning%20during%20the%20training%20phase.%20Additionally%2C%20we%20employ%20Domain%20Adversarial%0ANeural%20Network%20%28DANN%29%20to%20mitigate%20the%20impact%20of%20tissue%20type%20variations.%20To%0Aeliminate%20domain%20bias%2C%20a%20dynamic%20WSI%20prompt%20is%20designed%20to%20further%20unleash%20the%0Amodel%27s%20capabilities.%20Our%20model%20achieves%20better%20performance%20than%20other%0Astate-of-the-art%20methods%20across%2023%20cancer%20types%20on%20TCGA%20dataset.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Mengflz/PathoTME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenomics-guided%2520Representation%2520Learning%2520for%2520Pathologic%2520Pan-cancer%2520Tumor%250A%2520%2520Microenvironment%2520Subtype%2520Prediction%26entry.906535625%3DFangliangzi%2520Meng%2520and%2520Hongrun%2520Zhang%2520and%2520Ruodan%2520Yan%2520and%2520Guohui%2520Chuai%2520and%2520Chao%2520Li%2520and%2520Qi%2520Liu%26entry.1292438233%3D%2520%2520The%2520characterization%2520of%2520Tumor%2520MicroEnvironment%2520%2528TME%2529%2520is%2520challenging%2520due%2520to%250Aits%2520complexity%2520and%2520heterogeneity.%2520Relatively%2520consistent%2520TME%2520characteristics%250Aembedded%2520within%2520highly%2520specific%2520tissue%2520features%252C%2520render%2520them%2520difficult%2520to%250Apredict.%2520The%2520capability%2520to%2520accurately%2520classify%2520TME%2520subtypes%2520is%2520of%2520critical%250Asignificance%2520for%2520clinical%2520tumor%2520diagnosis%2520and%2520precision%2520medicine.%2520Based%2520on%2520the%250Aobservation%2520that%2520tumors%2520with%2520different%2520origins%2520share%2520similar%2520microenvironment%250Apatterns%252C%2520we%2520propose%2520PathoTME%252C%2520a%2520genomics-guided%2520Siamese%2520representation%250Alearning%2520framework%2520employing%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520for%2520pan-cancer%2520TME%250Asubtypes%2520prediction.%2520Specifically%252C%2520we%2520utilize%2520Siamese%2520network%2520to%2520leverage%250Agenomic%2520information%2520as%2520a%2520regularization%2520factor%2520to%2520assist%2520WSI%2520embeddings%250Alearning%2520during%2520the%2520training%2520phase.%2520Additionally%252C%2520we%2520employ%2520Domain%2520Adversarial%250ANeural%2520Network%2520%2528DANN%2529%2520to%2520mitigate%2520the%2520impact%2520of%2520tissue%2520type%2520variations.%2520To%250Aeliminate%2520domain%2520bias%252C%2520a%2520dynamic%2520WSI%2520prompt%2520is%2520designed%2520to%2520further%2520unleash%2520the%250Amodel%2527s%2520capabilities.%2520Our%2520model%2520achieves%2520better%2520performance%2520than%2520other%250Astate-of-the-art%2520methods%2520across%252023%2520cancer%2520types%2520on%2520TCGA%2520dataset.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Mengflz/PathoTME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genomics-guided%20Representation%20Learning%20for%20Pathologic%20Pan-cancer%20Tumor%0A%20%20Microenvironment%20Subtype%20Prediction&entry.906535625=Fangliangzi%20Meng%20and%20Hongrun%20Zhang%20and%20Ruodan%20Yan%20and%20Guohui%20Chuai%20and%20Chao%20Li%20and%20Qi%20Liu&entry.1292438233=%20%20The%20characterization%20of%20Tumor%20MicroEnvironment%20%28TME%29%20is%20challenging%20due%20to%0Aits%20complexity%20and%20heterogeneity.%20Relatively%20consistent%20TME%20characteristics%0Aembedded%20within%20highly%20specific%20tissue%20features%2C%20render%20them%20difficult%20to%0Apredict.%20The%20capability%20to%20accurately%20classify%20TME%20subtypes%20is%20of%20critical%0Asignificance%20for%20clinical%20tumor%20diagnosis%20and%20precision%20medicine.%20Based%20on%20the%0Aobservation%20that%20tumors%20with%20different%20origins%20share%20similar%20microenvironment%0Apatterns%2C%20we%20propose%20PathoTME%2C%20a%20genomics-guided%20Siamese%20representation%0Alearning%20framework%20employing%20Whole%20Slide%20Image%20%28WSI%29%20for%20pan-cancer%20TME%0Asubtypes%20prediction.%20Specifically%2C%20we%20utilize%20Siamese%20network%20to%20leverage%0Agenomic%20information%20as%20a%20regularization%20factor%20to%20assist%20WSI%20embeddings%0Alearning%20during%20the%20training%20phase.%20Additionally%2C%20we%20employ%20Domain%20Adversarial%0ANeural%20Network%20%28DANN%29%20to%20mitigate%20the%20impact%20of%20tissue%20type%20variations.%20To%0Aeliminate%20domain%20bias%2C%20a%20dynamic%20WSI%20prompt%20is%20designed%20to%20further%20unleash%20the%0Amodel%27s%20capabilities.%20Our%20model%20achieves%20better%20performance%20than%20other%0Astate-of-the-art%20methods%20across%2023%20cancer%20types%20on%20TCGA%20dataset.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Mengflz/PathoTME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06517v1&entry.124074799=Read"},
{"title": "Low-Rank Quantization-Aware Training for LLMs", "author": "Yelysei Bondarenko and Riccardo Del Chiaro and Markus Nagel", "abstract": "  Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model\nfamilies and validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory.\n", "link": "http://arxiv.org/abs/2406.06385v1", "date": "2024-06-10", "relevancy": 2.0246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5181}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5047}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&body=Title%3A%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs%0AAuthor%3A%20Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20the%20LLaMA-2/3%20and%20Mistral%20model%0Afamilies%20and%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Quantization-Aware%2520Training%2520for%2520LLMs%26entry.906535625%3DYelysei%2520Bondarenko%2520and%2520Riccardo%2520Del%2520Chiaro%2520and%2520Markus%2520Nagel%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520omnipresent%252C%2520however%2520their%2520practical%250Adeployment%2520is%2520challenging%2520due%2520to%2520their%2520ever%2520increasing%2520computational%2520and%2520memory%250Ademands.%2520Quantization%2520is%2520one%2520of%2520the%2520most%2520effective%2520ways%2520to%2520make%2520them%2520more%250Acompute%2520and%2520memory%2520efficient.%2520Quantization-aware%2520training%2520%2528QAT%2529%2520methods%252C%250Agenerally%2520produce%2520the%2520best%2520quantized%2520performance%252C%2520however%2520it%2520comes%2520at%2520the%2520cost%250Aof%2520potentially%2520long%2520training%2520time%2520and%2520excessive%2520memory%2520usage%252C%2520making%2520it%250Aimpractical%2520when%2520applying%2520for%2520LLMs.%2520Inspired%2520by%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520and%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520literature%252C%2520we%2520propose%2520LR-QAT%2520--%2520a%250Alightweight%2520and%2520memory-efficient%2520QAT%2520algorithm%2520for%2520LLMs.%2520LR-QAT%2520employs%2520several%250Acomponents%2520to%2520save%2520memory%2520without%2520sacrificing%2520predictive%2520performance%253A%2520%2528a%2529%250Alow-rank%2520auxiliary%2520weights%2520that%2520are%2520aware%2520of%2520the%2520quantization%2520grid%253B%2520%2528b%2529%2520a%250Adowncasting%2520operator%2520using%2520fixed-point%2520or%2520double-packed%2520integers%2520and%2520%2528c%2529%250Acheckpointing.%2520Unlike%2520most%2520related%2520work%252C%2520our%2520method%2520%2528i%2529%2520is%2520inference-efficient%252C%250Aleading%2520to%2520no%2520additional%2520overhead%2520compared%2520to%2520traditional%2520PTQ%253B%2520%2528ii%2529%2520can%2520be%2520seen%250Aas%2520a%2520general%2520extended%2520pretraining%2520framework%252C%2520meaning%2520that%2520the%2520resulting%2520model%250Acan%2520still%2520be%2520utilized%2520for%2520any%2520downstream%2520task%2520afterwards%253B%2520%2528iii%2529%2520can%2520be%2520applied%250Aacross%2520a%2520wide%2520range%2520of%2520quantization%2520settings%252C%2520such%2520as%2520different%2520choices%250Aquantization%2520granularity%252C%2520activation%2520quantization%252C%2520and%2520seamlessly%2520combined%2520with%250Amany%2520PTQ%2520techniques.%2520We%2520apply%2520LR-QAT%2520to%2520the%2520LLaMA-2/3%2520and%2520Mistral%2520model%250Afamilies%2520and%2520validate%2520its%2520effectiveness%2520on%2520several%2520downstream%2520tasks.%2520Our%2520method%250Aoutperforms%2520common%2520post-training%2520quantization%2520%2528PTQ%2529%2520approaches%2520and%2520reaches%2520the%250Asame%2520model%2520performance%2520as%2520full-model%2520QAT%2520at%2520the%2520fraction%2520of%2520its%2520memory%2520usage.%250ASpecifically%252C%2520we%2520can%2520train%2520a%25207B%2520LLM%2520on%2520a%2520single%2520consumer%2520grade%2520GPU%2520with%252024GB%2520of%250Amemory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&entry.906535625=Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20the%20LLaMA-2/3%20and%20Mistral%20model%0Afamilies%20and%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06385v1&entry.124074799=Read"},
{"title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for\n  Transformers", "author": "Reduan Achtibat and Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Aakriti Jain and Thomas Wiegand and Sebastian Lapuschkin and Wojciech Samek", "abstract": "  Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a single backward pass. Through extensive evaluations against existing\nmethods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,\nwe demonstrate that our proposed approach surpasses alternative methods in\nterms of faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an LRP library\nat https://github.com/rachtibat/LRP-eXplains-Transformers.\n", "link": "http://arxiv.org/abs/2402.05602v2", "date": "2024-06-10", "relevancy": 2.0235, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttnLRP%3A%20Attention-Aware%20Layer-Wise%20Relevance%20Propagation%20for%0A%20%20Transformers&body=Title%3A%20AttnLRP%3A%20Attention-Aware%20Layer-Wise%20Relevance%20Propagation%20for%0A%20%20Transformers%0AAuthor%3A%20Reduan%20Achtibat%20and%20Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Aakriti%20Jain%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20prone%20to%20biased%20predictions%20and%20hallucinations%2C%0Aunderlining%20the%20paramount%20importance%20of%20understanding%20their%20model-internal%0Areasoning%20process.%20However%2C%20achieving%20faithful%20attributions%20for%20the%20entirety%20of%0Aa%20black-box%20transformer%20model%20and%20maintaining%20computational%20efficiency%20is%20an%0Aunsolved%20challenge.%20By%20extending%20the%20Layer-wise%20Relevance%20Propagation%0Aattribution%20method%20to%20handle%20attention%20layers%2C%20we%20address%20these%20challenges%0Aeffectively.%20While%20partial%20solutions%20exist%2C%20our%20method%20is%20the%20first%20to%0Afaithfully%20and%20holistically%20attribute%20not%20only%20input%20but%20also%20latent%0Arepresentations%20of%20transformer%20models%20with%20the%20computational%20efficiency%20similar%0Ato%20a%20single%20backward%20pass.%20Through%20extensive%20evaluations%20against%20existing%0Amethods%20on%20LLaMa%202%2C%20Mixtral%208x7b%2C%20Flan-T5%20and%20vision%20transformer%20architectures%2C%0Awe%20demonstrate%20that%20our%20proposed%20approach%20surpasses%20alternative%20methods%20in%0Aterms%20of%20faithfulness%20and%20enables%20the%20understanding%20of%20latent%20representations%2C%0Aopening%20up%20the%20door%20for%20concept-based%20explanations.%20We%20provide%20an%20LRP%20library%0Aat%20https%3A//github.com/rachtibat/LRP-eXplains-Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttnLRP%253A%2520Attention-Aware%2520Layer-Wise%2520Relevance%2520Propagation%2520for%250A%2520%2520Transformers%26entry.906535625%3DReduan%2520Achtibat%2520and%2520Sayed%2520Mohammad%2520Vakilzadeh%2520Hatefi%2520and%2520Maximilian%2520Dreyer%2520and%2520Aakriti%2520Jain%2520and%2520Thomas%2520Wiegand%2520and%2520Sebastian%2520Lapuschkin%2520and%2520Wojciech%2520Samek%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520prone%2520to%2520biased%2520predictions%2520and%2520hallucinations%252C%250Aunderlining%2520the%2520paramount%2520importance%2520of%2520understanding%2520their%2520model-internal%250Areasoning%2520process.%2520However%252C%2520achieving%2520faithful%2520attributions%2520for%2520the%2520entirety%2520of%250Aa%2520black-box%2520transformer%2520model%2520and%2520maintaining%2520computational%2520efficiency%2520is%2520an%250Aunsolved%2520challenge.%2520By%2520extending%2520the%2520Layer-wise%2520Relevance%2520Propagation%250Aattribution%2520method%2520to%2520handle%2520attention%2520layers%252C%2520we%2520address%2520these%2520challenges%250Aeffectively.%2520While%2520partial%2520solutions%2520exist%252C%2520our%2520method%2520is%2520the%2520first%2520to%250Afaithfully%2520and%2520holistically%2520attribute%2520not%2520only%2520input%2520but%2520also%2520latent%250Arepresentations%2520of%2520transformer%2520models%2520with%2520the%2520computational%2520efficiency%2520similar%250Ato%2520a%2520single%2520backward%2520pass.%2520Through%2520extensive%2520evaluations%2520against%2520existing%250Amethods%2520on%2520LLaMa%25202%252C%2520Mixtral%25208x7b%252C%2520Flan-T5%2520and%2520vision%2520transformer%2520architectures%252C%250Awe%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520surpasses%2520alternative%2520methods%2520in%250Aterms%2520of%2520faithfulness%2520and%2520enables%2520the%2520understanding%2520of%2520latent%2520representations%252C%250Aopening%2520up%2520the%2520door%2520for%2520concept-based%2520explanations.%2520We%2520provide%2520an%2520LRP%2520library%250Aat%2520https%253A//github.com/rachtibat/LRP-eXplains-Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttnLRP%3A%20Attention-Aware%20Layer-Wise%20Relevance%20Propagation%20for%0A%20%20Transformers&entry.906535625=Reduan%20Achtibat%20and%20Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Aakriti%20Jain%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek&entry.1292438233=%20%20Large%20Language%20Models%20are%20prone%20to%20biased%20predictions%20and%20hallucinations%2C%0Aunderlining%20the%20paramount%20importance%20of%20understanding%20their%20model-internal%0Areasoning%20process.%20However%2C%20achieving%20faithful%20attributions%20for%20the%20entirety%20of%0Aa%20black-box%20transformer%20model%20and%20maintaining%20computational%20efficiency%20is%20an%0Aunsolved%20challenge.%20By%20extending%20the%20Layer-wise%20Relevance%20Propagation%0Aattribution%20method%20to%20handle%20attention%20layers%2C%20we%20address%20these%20challenges%0Aeffectively.%20While%20partial%20solutions%20exist%2C%20our%20method%20is%20the%20first%20to%0Afaithfully%20and%20holistically%20attribute%20not%20only%20input%20but%20also%20latent%0Arepresentations%20of%20transformer%20models%20with%20the%20computational%20efficiency%20similar%0Ato%20a%20single%20backward%20pass.%20Through%20extensive%20evaluations%20against%20existing%0Amethods%20on%20LLaMa%202%2C%20Mixtral%208x7b%2C%20Flan-T5%20and%20vision%20transformer%20architectures%2C%0Awe%20demonstrate%20that%20our%20proposed%20approach%20surpasses%20alternative%20methods%20in%0Aterms%20of%20faithfulness%20and%20enables%20the%20understanding%20of%20latent%20representations%2C%0Aopening%20up%20the%20door%20for%20concept-based%20explanations.%20We%20provide%20an%20LRP%20library%0Aat%20https%3A//github.com/rachtibat/LRP-eXplains-Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05602v2&entry.124074799=Read"},
{"title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction", "author": "Zhen Xing and Qi Dai and Zejia Weng and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.\n", "link": "http://arxiv.org/abs/2406.06465v1", "date": "2024-06-10", "relevancy": 2.0183, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7089}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6846}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AID%3A%20Adapting%20Image2Video%20Diffusion%20Models%20for%20Instruction-guided%20Video%0A%20%20Prediction&body=Title%3A%20AID%3A%20Adapting%20Image2Video%20Diffusion%20Models%20for%20Instruction-guided%20Video%0A%20%20Prediction%0AAuthor%3A%20Zhen%20Xing%20and%20Qi%20Dai%20and%20Zejia%20Weng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Text-guided%20video%20prediction%20%28TVP%29%20involves%20predicting%20the%20motion%20of%20future%0Aframes%20from%20the%20initial%20frame%20according%20to%20an%20instruction%2C%20which%20has%20wide%0Aapplications%20in%20virtual%20reality%2C%20robotics%2C%20and%20content%20creation.%20Previous%20TVP%0Amethods%20make%20significant%20breakthroughs%20by%20adapting%20Stable%20Diffusion%20for%20this%0Atask.%20However%2C%20they%20struggle%20with%20frame%20consistency%20and%20temporal%20stability%0Aprimarily%20due%20to%20the%20limited%20scale%20of%20video%20datasets.%20We%20observe%20that%0Apretrained%20Image2Video%20diffusion%20models%20possess%20good%20priors%20for%20video%20dynamics%0Abut%20they%20lack%20textual%20control.%20Hence%2C%20transferring%20Image2Video%20models%20to%0Aleverage%20their%20video%20dynamic%20priors%20while%20injecting%20instruction%20control%20to%0Agenerate%20controllable%20videos%20is%20both%20a%20meaningful%20and%20challenging%20task.%20To%0Aachieve%20this%2C%20we%20introduce%20the%20Multi-Modal%20Large%20Language%20Model%20%28MLLM%29%20to%0Apredict%20future%20video%20states%20based%20on%20initial%20frames%20and%20text%20instructions.%20More%0Aspecifically%2C%20we%20design%20a%20dual%20query%20transformer%20%28DQFormer%29%20architecture%2C%20which%0Aintegrates%20the%20instructions%20and%20frames%20into%20the%20conditional%20embeddings%20for%0Afuture%20frame%20prediction.%20Additionally%2C%20we%20develop%20Long-Short%20Term%20Temporal%0AAdapters%20and%20Spatial%20Adapters%20that%20can%20quickly%20transfer%20general%20video%20diffusion%0Amodels%20to%20specific%20scenarios%20with%20minimal%20training%20costs.%20Experimental%20results%0Ashow%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%20techniques%20on%0Afour%20datasets%3A%20Something%20Something%20V2%2C%20Epic%20Kitchen-100%2C%20Bridge%20Data%2C%20and%0AUCF-101.%20Notably%2C%20AID%20achieves%2091.2%25%20and%2055.5%25%20FVD%20improvements%20on%20Bridge%20and%0ASSv2%20respectively%2C%20demonstrating%20its%20effectiveness%20in%20various%20domains.%20More%0Aexamples%20can%20be%20found%20at%20our%20website%20https%3A//chenhsing.github.io/AID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAID%253A%2520Adapting%2520Image2Video%2520Diffusion%2520Models%2520for%2520Instruction-guided%2520Video%250A%2520%2520Prediction%26entry.906535625%3DZhen%2520Xing%2520and%2520Qi%2520Dai%2520and%2520Zejia%2520Weng%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Text-guided%2520video%2520prediction%2520%2528TVP%2529%2520involves%2520predicting%2520the%2520motion%2520of%2520future%250Aframes%2520from%2520the%2520initial%2520frame%2520according%2520to%2520an%2520instruction%252C%2520which%2520has%2520wide%250Aapplications%2520in%2520virtual%2520reality%252C%2520robotics%252C%2520and%2520content%2520creation.%2520Previous%2520TVP%250Amethods%2520make%2520significant%2520breakthroughs%2520by%2520adapting%2520Stable%2520Diffusion%2520for%2520this%250Atask.%2520However%252C%2520they%2520struggle%2520with%2520frame%2520consistency%2520and%2520temporal%2520stability%250Aprimarily%2520due%2520to%2520the%2520limited%2520scale%2520of%2520video%2520datasets.%2520We%2520observe%2520that%250Apretrained%2520Image2Video%2520diffusion%2520models%2520possess%2520good%2520priors%2520for%2520video%2520dynamics%250Abut%2520they%2520lack%2520textual%2520control.%2520Hence%252C%2520transferring%2520Image2Video%2520models%2520to%250Aleverage%2520their%2520video%2520dynamic%2520priors%2520while%2520injecting%2520instruction%2520control%2520to%250Agenerate%2520controllable%2520videos%2520is%2520both%2520a%2520meaningful%2520and%2520challenging%2520task.%2520To%250Aachieve%2520this%252C%2520we%2520introduce%2520the%2520Multi-Modal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520to%250Apredict%2520future%2520video%2520states%2520based%2520on%2520initial%2520frames%2520and%2520text%2520instructions.%2520More%250Aspecifically%252C%2520we%2520design%2520a%2520dual%2520query%2520transformer%2520%2528DQFormer%2529%2520architecture%252C%2520which%250Aintegrates%2520the%2520instructions%2520and%2520frames%2520into%2520the%2520conditional%2520embeddings%2520for%250Afuture%2520frame%2520prediction.%2520Additionally%252C%2520we%2520develop%2520Long-Short%2520Term%2520Temporal%250AAdapters%2520and%2520Spatial%2520Adapters%2520that%2520can%2520quickly%2520transfer%2520general%2520video%2520diffusion%250Amodels%2520to%2520specific%2520scenarios%2520with%2520minimal%2520training%2520costs.%2520Experimental%2520results%250Ashow%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%2520techniques%2520on%250Afour%2520datasets%253A%2520Something%2520Something%2520V2%252C%2520Epic%2520Kitchen-100%252C%2520Bridge%2520Data%252C%2520and%250AUCF-101.%2520Notably%252C%2520AID%2520achieves%252091.2%2525%2520and%252055.5%2525%2520FVD%2520improvements%2520on%2520Bridge%2520and%250ASSv2%2520respectively%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520various%2520domains.%2520More%250Aexamples%2520can%2520be%2520found%2520at%2520our%2520website%2520https%253A//chenhsing.github.io/AID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AID%3A%20Adapting%20Image2Video%20Diffusion%20Models%20for%20Instruction-guided%20Video%0A%20%20Prediction&entry.906535625=Zhen%20Xing%20and%20Qi%20Dai%20and%20Zejia%20Weng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Text-guided%20video%20prediction%20%28TVP%29%20involves%20predicting%20the%20motion%20of%20future%0Aframes%20from%20the%20initial%20frame%20according%20to%20an%20instruction%2C%20which%20has%20wide%0Aapplications%20in%20virtual%20reality%2C%20robotics%2C%20and%20content%20creation.%20Previous%20TVP%0Amethods%20make%20significant%20breakthroughs%20by%20adapting%20Stable%20Diffusion%20for%20this%0Atask.%20However%2C%20they%20struggle%20with%20frame%20consistency%20and%20temporal%20stability%0Aprimarily%20due%20to%20the%20limited%20scale%20of%20video%20datasets.%20We%20observe%20that%0Apretrained%20Image2Video%20diffusion%20models%20possess%20good%20priors%20for%20video%20dynamics%0Abut%20they%20lack%20textual%20control.%20Hence%2C%20transferring%20Image2Video%20models%20to%0Aleverage%20their%20video%20dynamic%20priors%20while%20injecting%20instruction%20control%20to%0Agenerate%20controllable%20videos%20is%20both%20a%20meaningful%20and%20challenging%20task.%20To%0Aachieve%20this%2C%20we%20introduce%20the%20Multi-Modal%20Large%20Language%20Model%20%28MLLM%29%20to%0Apredict%20future%20video%20states%20based%20on%20initial%20frames%20and%20text%20instructions.%20More%0Aspecifically%2C%20we%20design%20a%20dual%20query%20transformer%20%28DQFormer%29%20architecture%2C%20which%0Aintegrates%20the%20instructions%20and%20frames%20into%20the%20conditional%20embeddings%20for%0Afuture%20frame%20prediction.%20Additionally%2C%20we%20develop%20Long-Short%20Term%20Temporal%0AAdapters%20and%20Spatial%20Adapters%20that%20can%20quickly%20transfer%20general%20video%20diffusion%0Amodels%20to%20specific%20scenarios%20with%20minimal%20training%20costs.%20Experimental%20results%0Ashow%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%20techniques%20on%0Afour%20datasets%3A%20Something%20Something%20V2%2C%20Epic%20Kitchen-100%2C%20Bridge%20Data%2C%20and%0AUCF-101.%20Notably%2C%20AID%20achieves%2091.2%25%20and%2055.5%25%20FVD%20improvements%20on%20Bridge%20and%0ASSv2%20respectively%2C%20demonstrating%20its%20effectiveness%20in%20various%20domains.%20More%0Aexamples%20can%20be%20found%20at%20our%20website%20https%3A//chenhsing.github.io/AID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06465v1&entry.124074799=Read"},
{"title": "Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA\n  Benchmark", "author": "Evan M. Williams and Kathleen M. Carley", "abstract": "  We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.\n", "link": "http://arxiv.org/abs/2405.06634v2", "date": "2024-06-10", "relevancy": 2.0083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark&body=Title%3A%20Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark%0AAuthor%3A%20Evan%20M.%20Williams%20and%20Kathleen%20M.%20Carley%0AAbstract%3A%20%20%20We%20evaluate%20the%20zero-shot%20ability%20of%20GPT-4%20and%20LLaVa%20to%20perform%20simple%20Visual%0ANetwork%20Analysis%20%28VNA%29%20tasks%20on%20small-scale%20graphs.%20We%20evaluate%20the%20Vision%0ALanguage%20Models%20%28VLMs%29%20on%205%20tasks%20related%20to%20three%20foundational%20network%20science%0Aconcepts%3A%20identifying%20nodes%20of%20maximal%20degree%20on%20a%20rendered%20graph%2C%20identifying%0Awhether%20signed%20triads%20are%20balanced%20or%20unbalanced%2C%20and%20counting%20components.%20The%0Atasks%20are%20structured%20to%20be%20easy%20for%20a%20human%20who%20understands%20the%20underlying%0Agraph%20theoretic%20concepts%2C%20and%20can%20all%20be%20solved%20by%20counting%20the%20appropriate%0Aelements%20in%20graphs.%20We%20find%20that%20while%20GPT-4%20consistently%20outperforms%20LLaVa%2C%0Aboth%20models%20struggle%20with%20every%20visual%20network%20analysis%20task%20we%20propose.%20We%0Apublicly%20release%20the%20first%20benchmark%20for%20the%20evaluation%20of%20VLMs%20on%20foundational%0AVNA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520LLMs%2520Struggle%2520with%2520Basic%2520Visual%2520Network%2520Analysis%253A%2520a%2520VNA%250A%2520%2520Benchmark%26entry.906535625%3DEvan%2520M.%2520Williams%2520and%2520Kathleen%2520M.%2520Carley%26entry.1292438233%3D%2520%2520We%2520evaluate%2520the%2520zero-shot%2520ability%2520of%2520GPT-4%2520and%2520LLaVa%2520to%2520perform%2520simple%2520Visual%250ANetwork%2520Analysis%2520%2528VNA%2529%2520tasks%2520on%2520small-scale%2520graphs.%2520We%2520evaluate%2520the%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520on%25205%2520tasks%2520related%2520to%2520three%2520foundational%2520network%2520science%250Aconcepts%253A%2520identifying%2520nodes%2520of%2520maximal%2520degree%2520on%2520a%2520rendered%2520graph%252C%2520identifying%250Awhether%2520signed%2520triads%2520are%2520balanced%2520or%2520unbalanced%252C%2520and%2520counting%2520components.%2520The%250Atasks%2520are%2520structured%2520to%2520be%2520easy%2520for%2520a%2520human%2520who%2520understands%2520the%2520underlying%250Agraph%2520theoretic%2520concepts%252C%2520and%2520can%2520all%2520be%2520solved%2520by%2520counting%2520the%2520appropriate%250Aelements%2520in%2520graphs.%2520We%2520find%2520that%2520while%2520GPT-4%2520consistently%2520outperforms%2520LLaVa%252C%250Aboth%2520models%2520struggle%2520with%2520every%2520visual%2520network%2520analysis%2520task%2520we%2520propose.%2520We%250Apublicly%2520release%2520the%2520first%2520benchmark%2520for%2520the%2520evaluation%2520of%2520VLMs%2520on%2520foundational%250AVNA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark&entry.906535625=Evan%20M.%20Williams%20and%20Kathleen%20M.%20Carley&entry.1292438233=%20%20We%20evaluate%20the%20zero-shot%20ability%20of%20GPT-4%20and%20LLaVa%20to%20perform%20simple%20Visual%0ANetwork%20Analysis%20%28VNA%29%20tasks%20on%20small-scale%20graphs.%20We%20evaluate%20the%20Vision%0ALanguage%20Models%20%28VLMs%29%20on%205%20tasks%20related%20to%20three%20foundational%20network%20science%0Aconcepts%3A%20identifying%20nodes%20of%20maximal%20degree%20on%20a%20rendered%20graph%2C%20identifying%0Awhether%20signed%20triads%20are%20balanced%20or%20unbalanced%2C%20and%20counting%20components.%20The%0Atasks%20are%20structured%20to%20be%20easy%20for%20a%20human%20who%20understands%20the%20underlying%0Agraph%20theoretic%20concepts%2C%20and%20can%20all%20be%20solved%20by%20counting%20the%20appropriate%0Aelements%20in%20graphs.%20We%20find%20that%20while%20GPT-4%20consistently%20outperforms%20LLaVa%2C%0Aboth%20models%20struggle%20with%20every%20visual%20network%20analysis%20task%20we%20propose.%20We%0Apublicly%20release%20the%20first%20benchmark%20for%20the%20evaluation%20of%20VLMs%20on%20foundational%0AVNA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06634v2&entry.124074799=Read"},
{"title": "VS-PINN: A Fast and efficient training of physics-informed neural\n  networks using variable-scaling methods for solving PDEs with stiff behavior", "author": "Seungchan Ko and Sang Hyeon Park", "abstract": "  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n", "link": "http://arxiv.org/abs/2406.06287v1", "date": "2024-06-10", "relevancy": 2.0082, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5256}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4858}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VS-PINN%3A%20A%20Fast%20and%20efficient%20training%20of%20physics-informed%20neural%0A%20%20networks%20using%20variable-scaling%20methods%20for%20solving%20PDEs%20with%20stiff%20behavior&body=Title%3A%20VS-PINN%3A%20A%20Fast%20and%20efficient%20training%20of%20physics-informed%20neural%0A%20%20networks%20using%20variable-scaling%20methods%20for%20solving%20PDEs%20with%20stiff%20behavior%0AAuthor%3A%20Seungchan%20Ko%20and%20Sang%20Hyeon%20Park%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20promising%0Away%20to%20compute%20the%20solutions%20of%20partial%20differential%20equations%20%28PDEs%29%20using%0Adeep%20neural%20networks.%20However%2C%20despite%20their%20significant%20success%20in%20various%0Afields%2C%20it%20remains%20unclear%20in%20many%20aspects%20how%20to%20effectively%20train%20PINNs%20if%0Athe%20solutions%20of%20PDEs%20exhibit%20stiff%20behaviors%20or%20high%20frequencies.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20for%20training%20PINNs%20using%20variable-scaling%0Atechniques.%20This%20method%20is%20simple%20and%20it%20can%20be%20applied%20to%20a%20wide%20range%20of%0Aproblems%20including%20PDEs%20with%20rapidly-varying%20solutions.%20Throughout%20various%0Anumerical%20experiments%2C%20we%20will%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20for%20these%20problems%20and%20confirm%20that%20it%20can%20significantly%20improve%20the%0Atraining%20efficiency%20and%20performance%20of%20PINNs.%20Furthermore%2C%20based%20on%20the%0Aanalysis%20of%20the%20neural%20tangent%20kernel%20%28NTK%29%2C%20we%20will%20provide%20theoretical%0Aevidence%20for%20this%20phenomenon%20and%20show%20that%20our%20methods%20can%20indeed%20improve%20the%0Aperformance%20of%20PINNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVS-PINN%253A%2520A%2520Fast%2520and%2520efficient%2520training%2520of%2520physics-informed%2520neural%250A%2520%2520networks%2520using%2520variable-scaling%2520methods%2520for%2520solving%2520PDEs%2520with%2520stiff%2520behavior%26entry.906535625%3DSeungchan%2520Ko%2520and%2520Sang%2520Hyeon%2520Park%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520promising%250Away%2520to%2520compute%2520the%2520solutions%2520of%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520using%250Adeep%2520neural%2520networks.%2520However%252C%2520despite%2520their%2520significant%2520success%2520in%2520various%250Afields%252C%2520it%2520remains%2520unclear%2520in%2520many%2520aspects%2520how%2520to%2520effectively%2520train%2520PINNs%2520if%250Athe%2520solutions%2520of%2520PDEs%2520exhibit%2520stiff%2520behaviors%2520or%2520high%2520frequencies.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520method%2520for%2520training%2520PINNs%2520using%2520variable-scaling%250Atechniques.%2520This%2520method%2520is%2520simple%2520and%2520it%2520can%2520be%2520applied%2520to%2520a%2520wide%2520range%2520of%250Aproblems%2520including%2520PDEs%2520with%2520rapidly-varying%2520solutions.%2520Throughout%2520various%250Anumerical%2520experiments%252C%2520we%2520will%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%2520for%2520these%2520problems%2520and%2520confirm%2520that%2520it%2520can%2520significantly%2520improve%2520the%250Atraining%2520efficiency%2520and%2520performance%2520of%2520PINNs.%2520Furthermore%252C%2520based%2520on%2520the%250Aanalysis%2520of%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%252C%2520we%2520will%2520provide%2520theoretical%250Aevidence%2520for%2520this%2520phenomenon%2520and%2520show%2520that%2520our%2520methods%2520can%2520indeed%2520improve%2520the%250Aperformance%2520of%2520PINNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VS-PINN%3A%20A%20Fast%20and%20efficient%20training%20of%20physics-informed%20neural%0A%20%20networks%20using%20variable-scaling%20methods%20for%20solving%20PDEs%20with%20stiff%20behavior&entry.906535625=Seungchan%20Ko%20and%20Sang%20Hyeon%20Park&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20promising%0Away%20to%20compute%20the%20solutions%20of%20partial%20differential%20equations%20%28PDEs%29%20using%0Adeep%20neural%20networks.%20However%2C%20despite%20their%20significant%20success%20in%20various%0Afields%2C%20it%20remains%20unclear%20in%20many%20aspects%20how%20to%20effectively%20train%20PINNs%20if%0Athe%20solutions%20of%20PDEs%20exhibit%20stiff%20behaviors%20or%20high%20frequencies.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20for%20training%20PINNs%20using%20variable-scaling%0Atechniques.%20This%20method%20is%20simple%20and%20it%20can%20be%20applied%20to%20a%20wide%20range%20of%0Aproblems%20including%20PDEs%20with%20rapidly-varying%20solutions.%20Throughout%20various%0Anumerical%20experiments%2C%20we%20will%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20for%20these%20problems%20and%20confirm%20that%20it%20can%20significantly%20improve%20the%0Atraining%20efficiency%20and%20performance%20of%20PINNs.%20Furthermore%2C%20based%20on%20the%0Aanalysis%20of%20the%20neural%20tangent%20kernel%20%28NTK%29%2C%20we%20will%20provide%20theoretical%0Aevidence%20for%20this%20phenomenon%20and%20show%20that%20our%20methods%20can%20indeed%20improve%20the%0Aperformance%20of%20PINNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06287v1&entry.124074799=Read"},
{"title": "Towards Lifelong Learning of Large Language Models: A Survey", "author": "Junhao Zheng and Shengjie Qiu and Chengming Shi and Qianli Ma", "abstract": "  As the applications of large language models (LLMs) expand across diverse\nfields, the ability of these models to adapt to ongoing changes in data, tasks,\nand user preferences becomes crucial. Traditional training methods, relying on\nstatic datasets, are increasingly inadequate for coping with the dynamic nature\nof real-world information. Lifelong learning, also known as continual or\nincremental learning, addresses this challenge by enabling LLMs to learn\ncontinuously and adaptively over their operational lifetime, integrating new\nknowledge while retaining previously learned information and preventing\ncatastrophic forgetting. This survey delves into the sophisticated landscape of\nlifelong learning, categorizing strategies into two primary groups: Internal\nKnowledge and External Knowledge. Internal Knowledge includes continual\npretraining and continual finetuning, each enhancing the adaptability of LLMs\nin various scenarios. External Knowledge encompasses retrieval-based and\ntool-based lifelong learning, leveraging external data sources and\ncomputational tools to extend the model's capabilities without modifying core\nparameters. The key contributions of our survey are: (1) Introducing a novel\ntaxonomy categorizing the extensive literature of lifelong learning into 12\nscenarios; (2) Identifying common techniques across all lifelong learning\nscenarios and classifying existing literature into various technique groups\nwithin each scenario; (3) Highlighting emerging techniques such as model\nexpansion and data selection, which were less explored in the pre-LLM era.\nThrough a detailed examination of these groups and their respective categories,\nthis survey aims to enhance the adaptability, reliability, and overall\nperformance of LLMs in real-world applications.\n", "link": "http://arxiv.org/abs/2406.06391v1", "date": "2024-06-10", "relevancy": 1.9998, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5063}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Lifelong%20Learning%20of%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Towards%20Lifelong%20Learning%20of%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Chengming%20Shi%20and%20Qianli%20Ma%0AAbstract%3A%20%20%20As%20the%20applications%20of%20large%20language%20models%20%28LLMs%29%20expand%20across%20diverse%0Afields%2C%20the%20ability%20of%20these%20models%20to%20adapt%20to%20ongoing%20changes%20in%20data%2C%20tasks%2C%0Aand%20user%20preferences%20becomes%20crucial.%20Traditional%20training%20methods%2C%20relying%20on%0Astatic%20datasets%2C%20are%20increasingly%20inadequate%20for%20coping%20with%20the%20dynamic%20nature%0Aof%20real-world%20information.%20Lifelong%20learning%2C%20also%20known%20as%20continual%20or%0Aincremental%20learning%2C%20addresses%20this%20challenge%20by%20enabling%20LLMs%20to%20learn%0Acontinuously%20and%20adaptively%20over%20their%20operational%20lifetime%2C%20integrating%20new%0Aknowledge%20while%20retaining%20previously%20learned%20information%20and%20preventing%0Acatastrophic%20forgetting.%20This%20survey%20delves%20into%20the%20sophisticated%20landscape%20of%0Alifelong%20learning%2C%20categorizing%20strategies%20into%20two%20primary%20groups%3A%20Internal%0AKnowledge%20and%20External%20Knowledge.%20Internal%20Knowledge%20includes%20continual%0Apretraining%20and%20continual%20finetuning%2C%20each%20enhancing%20the%20adaptability%20of%20LLMs%0Ain%20various%20scenarios.%20External%20Knowledge%20encompasses%20retrieval-based%20and%0Atool-based%20lifelong%20learning%2C%20leveraging%20external%20data%20sources%20and%0Acomputational%20tools%20to%20extend%20the%20model%27s%20capabilities%20without%20modifying%20core%0Aparameters.%20The%20key%20contributions%20of%20our%20survey%20are%3A%20%281%29%20Introducing%20a%20novel%0Ataxonomy%20categorizing%20the%20extensive%20literature%20of%20lifelong%20learning%20into%2012%0Ascenarios%3B%20%282%29%20Identifying%20common%20techniques%20across%20all%20lifelong%20learning%0Ascenarios%20and%20classifying%20existing%20literature%20into%20various%20technique%20groups%0Awithin%20each%20scenario%3B%20%283%29%20Highlighting%20emerging%20techniques%20such%20as%20model%0Aexpansion%20and%20data%20selection%2C%20which%20were%20less%20explored%20in%20the%20pre-LLM%20era.%0AThrough%20a%20detailed%20examination%20of%20these%20groups%20and%20their%20respective%20categories%2C%0Athis%20survey%20aims%20to%20enhance%20the%20adaptability%2C%20reliability%2C%20and%20overall%0Aperformance%20of%20LLMs%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Lifelong%2520Learning%2520of%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DJunhao%2520Zheng%2520and%2520Shengjie%2520Qiu%2520and%2520Chengming%2520Shi%2520and%2520Qianli%2520Ma%26entry.1292438233%3D%2520%2520As%2520the%2520applications%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520expand%2520across%2520diverse%250Afields%252C%2520the%2520ability%2520of%2520these%2520models%2520to%2520adapt%2520to%2520ongoing%2520changes%2520in%2520data%252C%2520tasks%252C%250Aand%2520user%2520preferences%2520becomes%2520crucial.%2520Traditional%2520training%2520methods%252C%2520relying%2520on%250Astatic%2520datasets%252C%2520are%2520increasingly%2520inadequate%2520for%2520coping%2520with%2520the%2520dynamic%2520nature%250Aof%2520real-world%2520information.%2520Lifelong%2520learning%252C%2520also%2520known%2520as%2520continual%2520or%250Aincremental%2520learning%252C%2520addresses%2520this%2520challenge%2520by%2520enabling%2520LLMs%2520to%2520learn%250Acontinuously%2520and%2520adaptively%2520over%2520their%2520operational%2520lifetime%252C%2520integrating%2520new%250Aknowledge%2520while%2520retaining%2520previously%2520learned%2520information%2520and%2520preventing%250Acatastrophic%2520forgetting.%2520This%2520survey%2520delves%2520into%2520the%2520sophisticated%2520landscape%2520of%250Alifelong%2520learning%252C%2520categorizing%2520strategies%2520into%2520two%2520primary%2520groups%253A%2520Internal%250AKnowledge%2520and%2520External%2520Knowledge.%2520Internal%2520Knowledge%2520includes%2520continual%250Apretraining%2520and%2520continual%2520finetuning%252C%2520each%2520enhancing%2520the%2520adaptability%2520of%2520LLMs%250Ain%2520various%2520scenarios.%2520External%2520Knowledge%2520encompasses%2520retrieval-based%2520and%250Atool-based%2520lifelong%2520learning%252C%2520leveraging%2520external%2520data%2520sources%2520and%250Acomputational%2520tools%2520to%2520extend%2520the%2520model%2527s%2520capabilities%2520without%2520modifying%2520core%250Aparameters.%2520The%2520key%2520contributions%2520of%2520our%2520survey%2520are%253A%2520%25281%2529%2520Introducing%2520a%2520novel%250Ataxonomy%2520categorizing%2520the%2520extensive%2520literature%2520of%2520lifelong%2520learning%2520into%252012%250Ascenarios%253B%2520%25282%2529%2520Identifying%2520common%2520techniques%2520across%2520all%2520lifelong%2520learning%250Ascenarios%2520and%2520classifying%2520existing%2520literature%2520into%2520various%2520technique%2520groups%250Awithin%2520each%2520scenario%253B%2520%25283%2529%2520Highlighting%2520emerging%2520techniques%2520such%2520as%2520model%250Aexpansion%2520and%2520data%2520selection%252C%2520which%2520were%2520less%2520explored%2520in%2520the%2520pre-LLM%2520era.%250AThrough%2520a%2520detailed%2520examination%2520of%2520these%2520groups%2520and%2520their%2520respective%2520categories%252C%250Athis%2520survey%2520aims%2520to%2520enhance%2520the%2520adaptability%252C%2520reliability%252C%2520and%2520overall%250Aperformance%2520of%2520LLMs%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Lifelong%20Learning%20of%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Chengming%20Shi%20and%20Qianli%20Ma&entry.1292438233=%20%20As%20the%20applications%20of%20large%20language%20models%20%28LLMs%29%20expand%20across%20diverse%0Afields%2C%20the%20ability%20of%20these%20models%20to%20adapt%20to%20ongoing%20changes%20in%20data%2C%20tasks%2C%0Aand%20user%20preferences%20becomes%20crucial.%20Traditional%20training%20methods%2C%20relying%20on%0Astatic%20datasets%2C%20are%20increasingly%20inadequate%20for%20coping%20with%20the%20dynamic%20nature%0Aof%20real-world%20information.%20Lifelong%20learning%2C%20also%20known%20as%20continual%20or%0Aincremental%20learning%2C%20addresses%20this%20challenge%20by%20enabling%20LLMs%20to%20learn%0Acontinuously%20and%20adaptively%20over%20their%20operational%20lifetime%2C%20integrating%20new%0Aknowledge%20while%20retaining%20previously%20learned%20information%20and%20preventing%0Acatastrophic%20forgetting.%20This%20survey%20delves%20into%20the%20sophisticated%20landscape%20of%0Alifelong%20learning%2C%20categorizing%20strategies%20into%20two%20primary%20groups%3A%20Internal%0AKnowledge%20and%20External%20Knowledge.%20Internal%20Knowledge%20includes%20continual%0Apretraining%20and%20continual%20finetuning%2C%20each%20enhancing%20the%20adaptability%20of%20LLMs%0Ain%20various%20scenarios.%20External%20Knowledge%20encompasses%20retrieval-based%20and%0Atool-based%20lifelong%20learning%2C%20leveraging%20external%20data%20sources%20and%0Acomputational%20tools%20to%20extend%20the%20model%27s%20capabilities%20without%20modifying%20core%0Aparameters.%20The%20key%20contributions%20of%20our%20survey%20are%3A%20%281%29%20Introducing%20a%20novel%0Ataxonomy%20categorizing%20the%20extensive%20literature%20of%20lifelong%20learning%20into%2012%0Ascenarios%3B%20%282%29%20Identifying%20common%20techniques%20across%20all%20lifelong%20learning%0Ascenarios%20and%20classifying%20existing%20literature%20into%20various%20technique%20groups%0Awithin%20each%20scenario%3B%20%283%29%20Highlighting%20emerging%20techniques%20such%20as%20model%0Aexpansion%20and%20data%20selection%2C%20which%20were%20less%20explored%20in%20the%20pre-LLM%20era.%0AThrough%20a%20detailed%20examination%20of%20these%20groups%20and%20their%20respective%20categories%2C%0Athis%20survey%20aims%20to%20enhance%20the%20adaptability%2C%20reliability%2C%20and%20overall%0Aperformance%20of%20LLMs%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06391v1&entry.124074799=Read"},
{"title": "Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?", "author": "Letitia Parcalabescu and Anette Frank", "abstract": "  Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to predictions, they can also produce\nexplanations, either in post-hoc or CoT settings. However, it is not clear how\nmuch they use the vision and text modalities when generating predictions or\nexplanations. In this work, we investigate if VLMs rely on modalities\ndifferently when they produce explanations as opposed to providing answers. We\nalso evaluate the self-consistency of VLM decoders in both post-hoc and CoT\nexplanation settings, by extending existing unimodal tests and measures to VLM\ndecoders. We find that VLMs are less self-consistent than LLMs. Text\ncontributions in VL decoders are more important than image contributions in all\nexamined tasks. Moreover, the contributions of images are significantly\nstronger for explanation generation compared to answer generation. This\ndifference is even larger in CoT compared to post-hoc explanations. Lastly, we\nprovide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE\nbenchmark, which before only covered VL encoders. We find that VL decoders\nstill struggle with most phenomena tested by VALSE.\n", "link": "http://arxiv.org/abs/2404.18624v2", "date": "2024-06-10", "relevancy": 1.9996, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F&body=Title%3A%20Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F%0AAuthor%3A%20Letitia%20Parcalabescu%20and%20Anette%20Frank%0AAbstract%3A%20%20%20Vision%20and%20language%20model%20%28VLM%29%20decoders%20are%20currently%20the%20best-performing%0Aarchitectures%20on%20multimodal%20tasks.%20Next%20to%20predictions%2C%20they%20can%20also%20produce%0Aexplanations%2C%20either%20in%20post-hoc%20or%20CoT%20settings.%20However%2C%20it%20is%20not%20clear%20how%0Amuch%20they%20use%20the%20vision%20and%20text%20modalities%20when%20generating%20predictions%20or%0Aexplanations.%20In%20this%20work%2C%20we%20investigate%20if%20VLMs%20rely%20on%20modalities%0Adifferently%20when%20they%20produce%20explanations%20as%20opposed%20to%20providing%20answers.%20We%0Aalso%20evaluate%20the%20self-consistency%20of%20VLM%20decoders%20in%20both%20post-hoc%20and%20CoT%0Aexplanation%20settings%2C%20by%20extending%20existing%20unimodal%20tests%20and%20measures%20to%20VLM%0Adecoders.%20We%20find%20that%20VLMs%20are%20less%20self-consistent%20than%20LLMs.%20Text%0Acontributions%20in%20VL%20decoders%20are%20more%20important%20than%20image%20contributions%20in%20all%0Aexamined%20tasks.%20Moreover%2C%20the%20contributions%20of%20images%20are%20significantly%0Astronger%20for%20explanation%20generation%20compared%20to%20answer%20generation.%20This%0Adifference%20is%20even%20larger%20in%20CoT%20compared%20to%20post-hoc%20explanations.%20Lastly%2C%20we%0Aprovide%20an%20up-to-date%20benchmarking%20of%20state-of-the-art%20VL%20decoders%20on%20the%20VALSE%0Abenchmark%2C%20which%20before%20only%20covered%20VL%20encoders.%20We%20find%20that%20VL%20decoders%0Astill%20struggle%20with%20most%20phenomena%20tested%20by%20VALSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision%2520%2526%2520Language%2520Decoders%2520use%2520Images%2520and%2520Text%2520equally%253F%2520How%250A%2520%2520Self-consistent%2520are%2520their%2520Explanations%253F%26entry.906535625%3DLetitia%2520Parcalabescu%2520and%2520Anette%2520Frank%26entry.1292438233%3D%2520%2520Vision%2520and%2520language%2520model%2520%2528VLM%2529%2520decoders%2520are%2520currently%2520the%2520best-performing%250Aarchitectures%2520on%2520multimodal%2520tasks.%2520Next%2520to%2520predictions%252C%2520they%2520can%2520also%2520produce%250Aexplanations%252C%2520either%2520in%2520post-hoc%2520or%2520CoT%2520settings.%2520However%252C%2520it%2520is%2520not%2520clear%2520how%250Amuch%2520they%2520use%2520the%2520vision%2520and%2520text%2520modalities%2520when%2520generating%2520predictions%2520or%250Aexplanations.%2520In%2520this%2520work%252C%2520we%2520investigate%2520if%2520VLMs%2520rely%2520on%2520modalities%250Adifferently%2520when%2520they%2520produce%2520explanations%2520as%2520opposed%2520to%2520providing%2520answers.%2520We%250Aalso%2520evaluate%2520the%2520self-consistency%2520of%2520VLM%2520decoders%2520in%2520both%2520post-hoc%2520and%2520CoT%250Aexplanation%2520settings%252C%2520by%2520extending%2520existing%2520unimodal%2520tests%2520and%2520measures%2520to%2520VLM%250Adecoders.%2520We%2520find%2520that%2520VLMs%2520are%2520less%2520self-consistent%2520than%2520LLMs.%2520Text%250Acontributions%2520in%2520VL%2520decoders%2520are%2520more%2520important%2520than%2520image%2520contributions%2520in%2520all%250Aexamined%2520tasks.%2520Moreover%252C%2520the%2520contributions%2520of%2520images%2520are%2520significantly%250Astronger%2520for%2520explanation%2520generation%2520compared%2520to%2520answer%2520generation.%2520This%250Adifference%2520is%2520even%2520larger%2520in%2520CoT%2520compared%2520to%2520post-hoc%2520explanations.%2520Lastly%252C%2520we%250Aprovide%2520an%2520up-to-date%2520benchmarking%2520of%2520state-of-the-art%2520VL%2520decoders%2520on%2520the%2520VALSE%250Abenchmark%252C%2520which%2520before%2520only%2520covered%2520VL%2520encoders.%2520We%2520find%2520that%2520VL%2520decoders%250Astill%2520struggle%2520with%2520most%2520phenomena%2520tested%2520by%2520VALSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F&entry.906535625=Letitia%20Parcalabescu%20and%20Anette%20Frank&entry.1292438233=%20%20Vision%20and%20language%20model%20%28VLM%29%20decoders%20are%20currently%20the%20best-performing%0Aarchitectures%20on%20multimodal%20tasks.%20Next%20to%20predictions%2C%20they%20can%20also%20produce%0Aexplanations%2C%20either%20in%20post-hoc%20or%20CoT%20settings.%20However%2C%20it%20is%20not%20clear%20how%0Amuch%20they%20use%20the%20vision%20and%20text%20modalities%20when%20generating%20predictions%20or%0Aexplanations.%20In%20this%20work%2C%20we%20investigate%20if%20VLMs%20rely%20on%20modalities%0Adifferently%20when%20they%20produce%20explanations%20as%20opposed%20to%20providing%20answers.%20We%0Aalso%20evaluate%20the%20self-consistency%20of%20VLM%20decoders%20in%20both%20post-hoc%20and%20CoT%0Aexplanation%20settings%2C%20by%20extending%20existing%20unimodal%20tests%20and%20measures%20to%20VLM%0Adecoders.%20We%20find%20that%20VLMs%20are%20less%20self-consistent%20than%20LLMs.%20Text%0Acontributions%20in%20VL%20decoders%20are%20more%20important%20than%20image%20contributions%20in%20all%0Aexamined%20tasks.%20Moreover%2C%20the%20contributions%20of%20images%20are%20significantly%0Astronger%20for%20explanation%20generation%20compared%20to%20answer%20generation.%20This%0Adifference%20is%20even%20larger%20in%20CoT%20compared%20to%20post-hoc%20explanations.%20Lastly%2C%20we%0Aprovide%20an%20up-to-date%20benchmarking%20of%20state-of-the-art%20VL%20decoders%20on%20the%20VALSE%0Abenchmark%2C%20which%20before%20only%20covered%20VL%20encoders.%20We%20find%20that%20VL%20decoders%0Astill%20struggle%20with%20most%20phenomena%20tested%20by%20VALSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18624v2&entry.124074799=Read"},
{"title": "Improving Efficiency of Diffusion Models via Multi-Stage Framework and\n  Tailored Multi-Decoder Architectures", "author": "Huijie Zhang and Yifu Lu and Ismail Alkhouri and Saiprasad Ravishankar and Dogyoon Song and Qing Qu", "abstract": "  Diffusion models, emerging as powerful deep generative tools, excel in\nvarious applications. They operate through a two-steps process: introducing\nnoise into training samples and then employing a model to convert random noise\ninto new samples (e.g., images). However, their remarkable generative\nperformance is hindered by slow training and sampling. This is due to the\nnecessity of tracking extensive forward and reverse diffusion trajectories, and\nemploying a large model with numerous parameters across multiple timesteps\n(i.e., noise levels). To tackle these challenges, we present a multi-stage\nframework inspired by our empirical findings. These observations indicate the\nadvantages of employing distinct parameters tailored to each timestep while\nretaining universal parameters shared across all time steps. Our approach\ninvolves segmenting the time interval into multiple stages where we employ\ncustom multi-decoder U-net architecture that blends time-dependent models with\na universally shared encoder. Our framework enables the efficient distribution\nof computational resources and mitigates inter-stage interference, which\nsubstantially improves training efficiency. Extensive numerical experiments\naffirm the effectiveness of our framework, showcasing significant training and\nsampling efficiency enhancements on three state-of-the-art diffusion models,\nincluding large-scale latent diffusion models. Furthermore, our ablation\nstudies illustrate the impact of two important components in our framework: (i)\na novel timestep clustering algorithm for stage division, and (ii) an\ninnovative multi-decoder U-net architecture, seamlessly integrating universal\nand customized hyperparameters.\n", "link": "http://arxiv.org/abs/2312.09181v2", "date": "2024-06-10", "relevancy": 1.9896, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Efficiency%20of%20Diffusion%20Models%20via%20Multi-Stage%20Framework%20and%0A%20%20Tailored%20Multi-Decoder%20Architectures&body=Title%3A%20Improving%20Efficiency%20of%20Diffusion%20Models%20via%20Multi-Stage%20Framework%20and%0A%20%20Tailored%20Multi-Decoder%20Architectures%0AAuthor%3A%20Huijie%20Zhang%20and%20Yifu%20Lu%20and%20Ismail%20Alkhouri%20and%20Saiprasad%20Ravishankar%20and%20Dogyoon%20Song%20and%20Qing%20Qu%0AAbstract%3A%20%20%20Diffusion%20models%2C%20emerging%20as%20powerful%20deep%20generative%20tools%2C%20excel%20in%0Avarious%20applications.%20They%20operate%20through%20a%20two-steps%20process%3A%20introducing%0Anoise%20into%20training%20samples%20and%20then%20employing%20a%20model%20to%20convert%20random%20noise%0Ainto%20new%20samples%20%28e.g.%2C%20images%29.%20However%2C%20their%20remarkable%20generative%0Aperformance%20is%20hindered%20by%20slow%20training%20and%20sampling.%20This%20is%20due%20to%20the%0Anecessity%20of%20tracking%20extensive%20forward%20and%20reverse%20diffusion%20trajectories%2C%20and%0Aemploying%20a%20large%20model%20with%20numerous%20parameters%20across%20multiple%20timesteps%0A%28i.e.%2C%20noise%20levels%29.%20To%20tackle%20these%20challenges%2C%20we%20present%20a%20multi-stage%0Aframework%20inspired%20by%20our%20empirical%20findings.%20These%20observations%20indicate%20the%0Aadvantages%20of%20employing%20distinct%20parameters%20tailored%20to%20each%20timestep%20while%0Aretaining%20universal%20parameters%20shared%20across%20all%20time%20steps.%20Our%20approach%0Ainvolves%20segmenting%20the%20time%20interval%20into%20multiple%20stages%20where%20we%20employ%0Acustom%20multi-decoder%20U-net%20architecture%20that%20blends%20time-dependent%20models%20with%0Aa%20universally%20shared%20encoder.%20Our%20framework%20enables%20the%20efficient%20distribution%0Aof%20computational%20resources%20and%20mitigates%20inter-stage%20interference%2C%20which%0Asubstantially%20improves%20training%20efficiency.%20Extensive%20numerical%20experiments%0Aaffirm%20the%20effectiveness%20of%20our%20framework%2C%20showcasing%20significant%20training%20and%0Asampling%20efficiency%20enhancements%20on%20three%20state-of-the-art%20diffusion%20models%2C%0Aincluding%20large-scale%20latent%20diffusion%20models.%20Furthermore%2C%20our%20ablation%0Astudies%20illustrate%20the%20impact%20of%20two%20important%20components%20in%20our%20framework%3A%20%28i%29%0Aa%20novel%20timestep%20clustering%20algorithm%20for%20stage%20division%2C%20and%20%28ii%29%20an%0Ainnovative%20multi-decoder%20U-net%20architecture%2C%20seamlessly%20integrating%20universal%0Aand%20customized%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Efficiency%2520of%2520Diffusion%2520Models%2520via%2520Multi-Stage%2520Framework%2520and%250A%2520%2520Tailored%2520Multi-Decoder%2520Architectures%26entry.906535625%3DHuijie%2520Zhang%2520and%2520Yifu%2520Lu%2520and%2520Ismail%2520Alkhouri%2520and%2520Saiprasad%2520Ravishankar%2520and%2520Dogyoon%2520Song%2520and%2520Qing%2520Qu%26entry.1292438233%3D%2520%2520Diffusion%2520models%252C%2520emerging%2520as%2520powerful%2520deep%2520generative%2520tools%252C%2520excel%2520in%250Avarious%2520applications.%2520They%2520operate%2520through%2520a%2520two-steps%2520process%253A%2520introducing%250Anoise%2520into%2520training%2520samples%2520and%2520then%2520employing%2520a%2520model%2520to%2520convert%2520random%2520noise%250Ainto%2520new%2520samples%2520%2528e.g.%252C%2520images%2529.%2520However%252C%2520their%2520remarkable%2520generative%250Aperformance%2520is%2520hindered%2520by%2520slow%2520training%2520and%2520sampling.%2520This%2520is%2520due%2520to%2520the%250Anecessity%2520of%2520tracking%2520extensive%2520forward%2520and%2520reverse%2520diffusion%2520trajectories%252C%2520and%250Aemploying%2520a%2520large%2520model%2520with%2520numerous%2520parameters%2520across%2520multiple%2520timesteps%250A%2528i.e.%252C%2520noise%2520levels%2529.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520present%2520a%2520multi-stage%250Aframework%2520inspired%2520by%2520our%2520empirical%2520findings.%2520These%2520observations%2520indicate%2520the%250Aadvantages%2520of%2520employing%2520distinct%2520parameters%2520tailored%2520to%2520each%2520timestep%2520while%250Aretaining%2520universal%2520parameters%2520shared%2520across%2520all%2520time%2520steps.%2520Our%2520approach%250Ainvolves%2520segmenting%2520the%2520time%2520interval%2520into%2520multiple%2520stages%2520where%2520we%2520employ%250Acustom%2520multi-decoder%2520U-net%2520architecture%2520that%2520blends%2520time-dependent%2520models%2520with%250Aa%2520universally%2520shared%2520encoder.%2520Our%2520framework%2520enables%2520the%2520efficient%2520distribution%250Aof%2520computational%2520resources%2520and%2520mitigates%2520inter-stage%2520interference%252C%2520which%250Asubstantially%2520improves%2520training%2520efficiency.%2520Extensive%2520numerical%2520experiments%250Aaffirm%2520the%2520effectiveness%2520of%2520our%2520framework%252C%2520showcasing%2520significant%2520training%2520and%250Asampling%2520efficiency%2520enhancements%2520on%2520three%2520state-of-the-art%2520diffusion%2520models%252C%250Aincluding%2520large-scale%2520latent%2520diffusion%2520models.%2520Furthermore%252C%2520our%2520ablation%250Astudies%2520illustrate%2520the%2520impact%2520of%2520two%2520important%2520components%2520in%2520our%2520framework%253A%2520%2528i%2529%250Aa%2520novel%2520timestep%2520clustering%2520algorithm%2520for%2520stage%2520division%252C%2520and%2520%2528ii%2529%2520an%250Ainnovative%2520multi-decoder%2520U-net%2520architecture%252C%2520seamlessly%2520integrating%2520universal%250Aand%2520customized%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Efficiency%20of%20Diffusion%20Models%20via%20Multi-Stage%20Framework%20and%0A%20%20Tailored%20Multi-Decoder%20Architectures&entry.906535625=Huijie%20Zhang%20and%20Yifu%20Lu%20and%20Ismail%20Alkhouri%20and%20Saiprasad%20Ravishankar%20and%20Dogyoon%20Song%20and%20Qing%20Qu&entry.1292438233=%20%20Diffusion%20models%2C%20emerging%20as%20powerful%20deep%20generative%20tools%2C%20excel%20in%0Avarious%20applications.%20They%20operate%20through%20a%20two-steps%20process%3A%20introducing%0Anoise%20into%20training%20samples%20and%20then%20employing%20a%20model%20to%20convert%20random%20noise%0Ainto%20new%20samples%20%28e.g.%2C%20images%29.%20However%2C%20their%20remarkable%20generative%0Aperformance%20is%20hindered%20by%20slow%20training%20and%20sampling.%20This%20is%20due%20to%20the%0Anecessity%20of%20tracking%20extensive%20forward%20and%20reverse%20diffusion%20trajectories%2C%20and%0Aemploying%20a%20large%20model%20with%20numerous%20parameters%20across%20multiple%20timesteps%0A%28i.e.%2C%20noise%20levels%29.%20To%20tackle%20these%20challenges%2C%20we%20present%20a%20multi-stage%0Aframework%20inspired%20by%20our%20empirical%20findings.%20These%20observations%20indicate%20the%0Aadvantages%20of%20employing%20distinct%20parameters%20tailored%20to%20each%20timestep%20while%0Aretaining%20universal%20parameters%20shared%20across%20all%20time%20steps.%20Our%20approach%0Ainvolves%20segmenting%20the%20time%20interval%20into%20multiple%20stages%20where%20we%20employ%0Acustom%20multi-decoder%20U-net%20architecture%20that%20blends%20time-dependent%20models%20with%0Aa%20universally%20shared%20encoder.%20Our%20framework%20enables%20the%20efficient%20distribution%0Aof%20computational%20resources%20and%20mitigates%20inter-stage%20interference%2C%20which%0Asubstantially%20improves%20training%20efficiency.%20Extensive%20numerical%20experiments%0Aaffirm%20the%20effectiveness%20of%20our%20framework%2C%20showcasing%20significant%20training%20and%0Asampling%20efficiency%20enhancements%20on%20three%20state-of-the-art%20diffusion%20models%2C%0Aincluding%20large-scale%20latent%20diffusion%20models.%20Furthermore%2C%20our%20ablation%0Astudies%20illustrate%20the%20impact%20of%20two%20important%20components%20in%20our%20framework%3A%20%28i%29%0Aa%20novel%20timestep%20clustering%20algorithm%20for%20stage%20division%2C%20and%20%28ii%29%20an%0Ainnovative%20multi-decoder%20U-net%20architecture%2C%20seamlessly%20integrating%20universal%0Aand%20customized%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09181v2&entry.124074799=Read"},
{"title": "MinBackProp -- Backpropagating through Minimal Solvers", "author": "Diana Sungatullina and Tomas Pajdla", "abstract": "  We present an approach to backpropagating through minimal problem solvers in\nend-to-end neural network training. Traditional methods relying on manually\nconstructed formulas, finite differences, and autograd are laborious,\napproximate, and unstable for complex minimal problem solvers. We show that\nusing the Implicit function theorem (IFT) to calculate derivatives to\nbackpropagate through the solution of a minimal problem solver is simple, fast,\nand stable. We compare our approach to (i) using the standard autograd on\nminimal problem solvers and relate it to existing backpropagation formulas\nthrough SVD-based and Eig-based solvers and (ii) implementing the backprop with\nan existing PyTorch Deep Declarative Networks (DDN) framework. We demonstrate\nour technique on a toy example of training outlier-rejection weights for 3D\npoint registration and on a real application of training an outlier-rejection\nand RANSAC sampling network in image matching. Our method provides $100\\%$\nstability and is 10 times faster compared to autograd, which is unstable and\nslow, and compared to DDN, which is stable but also slow.\n", "link": "http://arxiv.org/abs/2404.17993v2", "date": "2024-06-10", "relevancy": 1.9797, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4964}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MinBackProp%20--%20Backpropagating%20through%20Minimal%20Solvers&body=Title%3A%20MinBackProp%20--%20Backpropagating%20through%20Minimal%20Solvers%0AAuthor%3A%20Diana%20Sungatullina%20and%20Tomas%20Pajdla%0AAbstract%3A%20%20%20We%20present%20an%20approach%20to%20backpropagating%20through%20minimal%20problem%20solvers%20in%0Aend-to-end%20neural%20network%20training.%20Traditional%20methods%20relying%20on%20manually%0Aconstructed%20formulas%2C%20finite%20differences%2C%20and%20autograd%20are%20laborious%2C%0Aapproximate%2C%20and%20unstable%20for%20complex%20minimal%20problem%20solvers.%20We%20show%20that%0Ausing%20the%20Implicit%20function%20theorem%20%28IFT%29%20to%20calculate%20derivatives%20to%0Abackpropagate%20through%20the%20solution%20of%20a%20minimal%20problem%20solver%20is%20simple%2C%20fast%2C%0Aand%20stable.%20We%20compare%20our%20approach%20to%20%28i%29%20using%20the%20standard%20autograd%20on%0Aminimal%20problem%20solvers%20and%20relate%20it%20to%20existing%20backpropagation%20formulas%0Athrough%20SVD-based%20and%20Eig-based%20solvers%20and%20%28ii%29%20implementing%20the%20backprop%20with%0Aan%20existing%20PyTorch%20Deep%20Declarative%20Networks%20%28DDN%29%20framework.%20We%20demonstrate%0Aour%20technique%20on%20a%20toy%20example%20of%20training%20outlier-rejection%20weights%20for%203D%0Apoint%20registration%20and%20on%20a%20real%20application%20of%20training%20an%20outlier-rejection%0Aand%20RANSAC%20sampling%20network%20in%20image%20matching.%20Our%20method%20provides%20%24100%5C%25%24%0Astability%20and%20is%2010%20times%20faster%20compared%20to%20autograd%2C%20which%20is%20unstable%20and%0Aslow%2C%20and%20compared%20to%20DDN%2C%20which%20is%20stable%20but%20also%20slow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinBackProp%2520--%2520Backpropagating%2520through%2520Minimal%2520Solvers%26entry.906535625%3DDiana%2520Sungatullina%2520and%2520Tomas%2520Pajdla%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520approach%2520to%2520backpropagating%2520through%2520minimal%2520problem%2520solvers%2520in%250Aend-to-end%2520neural%2520network%2520training.%2520Traditional%2520methods%2520relying%2520on%2520manually%250Aconstructed%2520formulas%252C%2520finite%2520differences%252C%2520and%2520autograd%2520are%2520laborious%252C%250Aapproximate%252C%2520and%2520unstable%2520for%2520complex%2520minimal%2520problem%2520solvers.%2520We%2520show%2520that%250Ausing%2520the%2520Implicit%2520function%2520theorem%2520%2528IFT%2529%2520to%2520calculate%2520derivatives%2520to%250Abackpropagate%2520through%2520the%2520solution%2520of%2520a%2520minimal%2520problem%2520solver%2520is%2520simple%252C%2520fast%252C%250Aand%2520stable.%2520We%2520compare%2520our%2520approach%2520to%2520%2528i%2529%2520using%2520the%2520standard%2520autograd%2520on%250Aminimal%2520problem%2520solvers%2520and%2520relate%2520it%2520to%2520existing%2520backpropagation%2520formulas%250Athrough%2520SVD-based%2520and%2520Eig-based%2520solvers%2520and%2520%2528ii%2529%2520implementing%2520the%2520backprop%2520with%250Aan%2520existing%2520PyTorch%2520Deep%2520Declarative%2520Networks%2520%2528DDN%2529%2520framework.%2520We%2520demonstrate%250Aour%2520technique%2520on%2520a%2520toy%2520example%2520of%2520training%2520outlier-rejection%2520weights%2520for%25203D%250Apoint%2520registration%2520and%2520on%2520a%2520real%2520application%2520of%2520training%2520an%2520outlier-rejection%250Aand%2520RANSAC%2520sampling%2520network%2520in%2520image%2520matching.%2520Our%2520method%2520provides%2520%2524100%255C%2525%2524%250Astability%2520and%2520is%252010%2520times%2520faster%2520compared%2520to%2520autograd%252C%2520which%2520is%2520unstable%2520and%250Aslow%252C%2520and%2520compared%2520to%2520DDN%252C%2520which%2520is%2520stable%2520but%2520also%2520slow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MinBackProp%20--%20Backpropagating%20through%20Minimal%20Solvers&entry.906535625=Diana%20Sungatullina%20and%20Tomas%20Pajdla&entry.1292438233=%20%20We%20present%20an%20approach%20to%20backpropagating%20through%20minimal%20problem%20solvers%20in%0Aend-to-end%20neural%20network%20training.%20Traditional%20methods%20relying%20on%20manually%0Aconstructed%20formulas%2C%20finite%20differences%2C%20and%20autograd%20are%20laborious%2C%0Aapproximate%2C%20and%20unstable%20for%20complex%20minimal%20problem%20solvers.%20We%20show%20that%0Ausing%20the%20Implicit%20function%20theorem%20%28IFT%29%20to%20calculate%20derivatives%20to%0Abackpropagate%20through%20the%20solution%20of%20a%20minimal%20problem%20solver%20is%20simple%2C%20fast%2C%0Aand%20stable.%20We%20compare%20our%20approach%20to%20%28i%29%20using%20the%20standard%20autograd%20on%0Aminimal%20problem%20solvers%20and%20relate%20it%20to%20existing%20backpropagation%20formulas%0Athrough%20SVD-based%20and%20Eig-based%20solvers%20and%20%28ii%29%20implementing%20the%20backprop%20with%0Aan%20existing%20PyTorch%20Deep%20Declarative%20Networks%20%28DDN%29%20framework.%20We%20demonstrate%0Aour%20technique%20on%20a%20toy%20example%20of%20training%20outlier-rejection%20weights%20for%203D%0Apoint%20registration%20and%20on%20a%20real%20application%20of%20training%20an%20outlier-rejection%0Aand%20RANSAC%20sampling%20network%20in%20image%20matching.%20Our%20method%20provides%20%24100%5C%25%24%0Astability%20and%20is%2010%20times%20faster%20compared%20to%20autograd%2C%20which%20is%20unstable%20and%0Aslow%2C%20and%20compared%20to%20DDN%2C%20which%20is%20stable%20but%20also%20slow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17993v2&entry.124074799=Read"},
{"title": "Tuning-Free Visual Customization via View Iterative Self-Attention\n  Control", "author": "Xiaojie Li and Chenghao Gu and Shuzhao Xie and Yunpeng Bai and Weixiang Zhang and Zhi Wang", "abstract": "  Fine-Tuning Diffusion Models enable a wide range of personalized generation\nand editing applications on diverse visual modalities. While Low-Rank\nAdaptation (LoRA) accelerates the fine-tuning process, it still requires\nmultiple reference images and time-consuming training, which constrains its\nscalability for large-scale and real-time applications. In this paper, we\npropose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this\nchallenge. Specifically, VisCtrl is a training-free method that injects the\nappearance and structure of a user-specified subject into another subject in\nthe target image, unlike previous approaches that require fine-tuning the\nmodel. Initially, we obtain the initial noise for both the reference and target\nimages through DDIM inversion. Then, during the denoising phase, features from\nthe reference image are injected into the target image via the self-attention\nmechanism. Notably, by iteratively performing this feature injection process,\nwe ensure that the reference image features are gradually integrated into the\ntarget image. This approach results in consistent and harmonious editing with\nonly one reference image in a few denoising steps. Moreover, benefiting from\nour plug-and-play architecture design and the proposed Feature Gradual Sampling\nstrategy for multi-view editing, our method can be easily extended to edit in\ncomplex visual domains. Extensive experiments show the efficacy of VisCtrl\nacross a spectrum of tasks, including personalized editing of images, videos,\nand 3D scenes.\n", "link": "http://arxiv.org/abs/2406.06258v1", "date": "2024-06-10", "relevancy": 1.9742, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6649}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6588}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Visual%20Customization%20via%20View%20Iterative%20Self-Attention%0A%20%20Control&body=Title%3A%20Tuning-Free%20Visual%20Customization%20via%20View%20Iterative%20Self-Attention%0A%20%20Control%0AAuthor%3A%20Xiaojie%20Li%20and%20Chenghao%20Gu%20and%20Shuzhao%20Xie%20and%20Yunpeng%20Bai%20and%20Weixiang%20Zhang%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Fine-Tuning%20Diffusion%20Models%20enable%20a%20wide%20range%20of%20personalized%20generation%0Aand%20editing%20applications%20on%20diverse%20visual%20modalities.%20While%20Low-Rank%0AAdaptation%20%28LoRA%29%20accelerates%20the%20fine-tuning%20process%2C%20it%20still%20requires%0Amultiple%20reference%20images%20and%20time-consuming%20training%2C%20which%20constrains%20its%0Ascalability%20for%20large-scale%20and%20real-time%20applications.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextit%7BView%20Iterative%20Self-Attention%20Control%20%28VisCtrl%29%7D%20to%20tackle%20this%0Achallenge.%20Specifically%2C%20VisCtrl%20is%20a%20training-free%20method%20that%20injects%20the%0Aappearance%20and%20structure%20of%20a%20user-specified%20subject%20into%20another%20subject%20in%0Athe%20target%20image%2C%20unlike%20previous%20approaches%20that%20require%20fine-tuning%20the%0Amodel.%20Initially%2C%20we%20obtain%20the%20initial%20noise%20for%20both%20the%20reference%20and%20target%0Aimages%20through%20DDIM%20inversion.%20Then%2C%20during%20the%20denoising%20phase%2C%20features%20from%0Athe%20reference%20image%20are%20injected%20into%20the%20target%20image%20via%20the%20self-attention%0Amechanism.%20Notably%2C%20by%20iteratively%20performing%20this%20feature%20injection%20process%2C%0Awe%20ensure%20that%20the%20reference%20image%20features%20are%20gradually%20integrated%20into%20the%0Atarget%20image.%20This%20approach%20results%20in%20consistent%20and%20harmonious%20editing%20with%0Aonly%20one%20reference%20image%20in%20a%20few%20denoising%20steps.%20Moreover%2C%20benefiting%20from%0Aour%20plug-and-play%20architecture%20design%20and%20the%20proposed%20Feature%20Gradual%20Sampling%0Astrategy%20for%20multi-view%20editing%2C%20our%20method%20can%20be%20easily%20extended%20to%20edit%20in%0Acomplex%20visual%20domains.%20Extensive%20experiments%20show%20the%20efficacy%20of%20VisCtrl%0Aacross%20a%20spectrum%20of%20tasks%2C%20including%20personalized%20editing%20of%20images%2C%20videos%2C%0Aand%203D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-Free%2520Visual%2520Customization%2520via%2520View%2520Iterative%2520Self-Attention%250A%2520%2520Control%26entry.906535625%3DXiaojie%2520Li%2520and%2520Chenghao%2520Gu%2520and%2520Shuzhao%2520Xie%2520and%2520Yunpeng%2520Bai%2520and%2520Weixiang%2520Zhang%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Fine-Tuning%2520Diffusion%2520Models%2520enable%2520a%2520wide%2520range%2520of%2520personalized%2520generation%250Aand%2520editing%2520applications%2520on%2520diverse%2520visual%2520modalities.%2520While%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%2520accelerates%2520the%2520fine-tuning%2520process%252C%2520it%2520still%2520requires%250Amultiple%2520reference%2520images%2520and%2520time-consuming%2520training%252C%2520which%2520constrains%2520its%250Ascalability%2520for%2520large-scale%2520and%2520real-time%2520applications.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520%255Ctextit%257BView%2520Iterative%2520Self-Attention%2520Control%2520%2528VisCtrl%2529%257D%2520to%2520tackle%2520this%250Achallenge.%2520Specifically%252C%2520VisCtrl%2520is%2520a%2520training-free%2520method%2520that%2520injects%2520the%250Aappearance%2520and%2520structure%2520of%2520a%2520user-specified%2520subject%2520into%2520another%2520subject%2520in%250Athe%2520target%2520image%252C%2520unlike%2520previous%2520approaches%2520that%2520require%2520fine-tuning%2520the%250Amodel.%2520Initially%252C%2520we%2520obtain%2520the%2520initial%2520noise%2520for%2520both%2520the%2520reference%2520and%2520target%250Aimages%2520through%2520DDIM%2520inversion.%2520Then%252C%2520during%2520the%2520denoising%2520phase%252C%2520features%2520from%250Athe%2520reference%2520image%2520are%2520injected%2520into%2520the%2520target%2520image%2520via%2520the%2520self-attention%250Amechanism.%2520Notably%252C%2520by%2520iteratively%2520performing%2520this%2520feature%2520injection%2520process%252C%250Awe%2520ensure%2520that%2520the%2520reference%2520image%2520features%2520are%2520gradually%2520integrated%2520into%2520the%250Atarget%2520image.%2520This%2520approach%2520results%2520in%2520consistent%2520and%2520harmonious%2520editing%2520with%250Aonly%2520one%2520reference%2520image%2520in%2520a%2520few%2520denoising%2520steps.%2520Moreover%252C%2520benefiting%2520from%250Aour%2520plug-and-play%2520architecture%2520design%2520and%2520the%2520proposed%2520Feature%2520Gradual%2520Sampling%250Astrategy%2520for%2520multi-view%2520editing%252C%2520our%2520method%2520can%2520be%2520easily%2520extended%2520to%2520edit%2520in%250Acomplex%2520visual%2520domains.%2520Extensive%2520experiments%2520show%2520the%2520efficacy%2520of%2520VisCtrl%250Aacross%2520a%2520spectrum%2520of%2520tasks%252C%2520including%2520personalized%2520editing%2520of%2520images%252C%2520videos%252C%250Aand%25203D%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Visual%20Customization%20via%20View%20Iterative%20Self-Attention%0A%20%20Control&entry.906535625=Xiaojie%20Li%20and%20Chenghao%20Gu%20and%20Shuzhao%20Xie%20and%20Yunpeng%20Bai%20and%20Weixiang%20Zhang%20and%20Zhi%20Wang&entry.1292438233=%20%20Fine-Tuning%20Diffusion%20Models%20enable%20a%20wide%20range%20of%20personalized%20generation%0Aand%20editing%20applications%20on%20diverse%20visual%20modalities.%20While%20Low-Rank%0AAdaptation%20%28LoRA%29%20accelerates%20the%20fine-tuning%20process%2C%20it%20still%20requires%0Amultiple%20reference%20images%20and%20time-consuming%20training%2C%20which%20constrains%20its%0Ascalability%20for%20large-scale%20and%20real-time%20applications.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextit%7BView%20Iterative%20Self-Attention%20Control%20%28VisCtrl%29%7D%20to%20tackle%20this%0Achallenge.%20Specifically%2C%20VisCtrl%20is%20a%20training-free%20method%20that%20injects%20the%0Aappearance%20and%20structure%20of%20a%20user-specified%20subject%20into%20another%20subject%20in%0Athe%20target%20image%2C%20unlike%20previous%20approaches%20that%20require%20fine-tuning%20the%0Amodel.%20Initially%2C%20we%20obtain%20the%20initial%20noise%20for%20both%20the%20reference%20and%20target%0Aimages%20through%20DDIM%20inversion.%20Then%2C%20during%20the%20denoising%20phase%2C%20features%20from%0Athe%20reference%20image%20are%20injected%20into%20the%20target%20image%20via%20the%20self-attention%0Amechanism.%20Notably%2C%20by%20iteratively%20performing%20this%20feature%20injection%20process%2C%0Awe%20ensure%20that%20the%20reference%20image%20features%20are%20gradually%20integrated%20into%20the%0Atarget%20image.%20This%20approach%20results%20in%20consistent%20and%20harmonious%20editing%20with%0Aonly%20one%20reference%20image%20in%20a%20few%20denoising%20steps.%20Moreover%2C%20benefiting%20from%0Aour%20plug-and-play%20architecture%20design%20and%20the%20proposed%20Feature%20Gradual%20Sampling%0Astrategy%20for%20multi-view%20editing%2C%20our%20method%20can%20be%20easily%20extended%20to%20edit%20in%0Acomplex%20visual%20domains.%20Extensive%20experiments%20show%20the%20efficacy%20of%20VisCtrl%0Aacross%20a%20spectrum%20of%20tasks%2C%20including%20personalized%20editing%20of%20images%2C%20videos%2C%0Aand%203D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06258v1&entry.124074799=Read"},
{"title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast\n  Decoding", "author": "Zhaorun Chen and Zhuokai Zhao and Hongyin Luo and Huaxiu Yao and Bo Li and Jiawei Zhou", "abstract": "  While large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in interpreting multi-modal contexts, they invariably suffer from\nobject hallucinations (OH). We introduce HALC, a novel decoding algorithm\ndesigned to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal\nvisual information in vision-language tasks and operates on both local and\nglobal contexts simultaneously. Specifically, HALC integrates a robust\nauto-focal grounding mechanism (locally) to correct hallucinated tokens on the\nfly, and a specialized beam search algorithm (globally) to significantly reduce\nOH while preserving text generation quality. Additionally, HALC can be\nintegrated into any LVLMs as a plug-and-play module without extra training.\nExtensive experimental studies demonstrate the effectiveness of HALC in\nreducing OH, outperforming state-of-the-arts across four benchmarks.\n", "link": "http://arxiv.org/abs/2403.00425v2", "date": "2024-06-10", "relevancy": 1.9692, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4842}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HALC%3A%20Object%20Hallucination%20Reduction%20via%20Adaptive%20Focal-Contrast%0A%20%20Decoding&body=Title%3A%20HALC%3A%20Object%20Hallucination%20Reduction%20via%20Adaptive%20Focal-Contrast%0A%20%20Decoding%0AAuthor%3A%20Zhaorun%20Chen%20and%20Zhuokai%20Zhao%20and%20Hongyin%20Luo%20and%20Huaxiu%20Yao%20and%20Bo%20Li%20and%20Jiawei%20Zhou%0AAbstract%3A%20%20%20While%20large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20interpreting%20multi-modal%20contexts%2C%20they%20invariably%20suffer%20from%0Aobject%20hallucinations%20%28OH%29.%20We%20introduce%20HALC%2C%20a%20novel%20decoding%20algorithm%0Adesigned%20to%20mitigate%20OH%20in%20LVLMs.%20HALC%20leverages%20distinct%20fine-grained%20optimal%0Avisual%20information%20in%20vision-language%20tasks%20and%20operates%20on%20both%20local%20and%0Aglobal%20contexts%20simultaneously.%20Specifically%2C%20HALC%20integrates%20a%20robust%0Aauto-focal%20grounding%20mechanism%20%28locally%29%20to%20correct%20hallucinated%20tokens%20on%20the%0Afly%2C%20and%20a%20specialized%20beam%20search%20algorithm%20%28globally%29%20to%20significantly%20reduce%0AOH%20while%20preserving%20text%20generation%20quality.%20Additionally%2C%20HALC%20can%20be%0Aintegrated%20into%20any%20LVLMs%20as%20a%20plug-and-play%20module%20without%20extra%20training.%0AExtensive%20experimental%20studies%20demonstrate%20the%20effectiveness%20of%20HALC%20in%0Areducing%20OH%2C%20outperforming%20state-of-the-arts%20across%20four%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHALC%253A%2520Object%2520Hallucination%2520Reduction%2520via%2520Adaptive%2520Focal-Contrast%250A%2520%2520Decoding%26entry.906535625%3DZhaorun%2520Chen%2520and%2520Zhuokai%2520Zhao%2520and%2520Hongyin%2520Luo%2520and%2520Huaxiu%2520Yao%2520and%2520Bo%2520Li%2520and%2520Jiawei%2520Zhou%26entry.1292438233%3D%2520%2520While%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520in%2520interpreting%2520multi-modal%2520contexts%252C%2520they%2520invariably%2520suffer%2520from%250Aobject%2520hallucinations%2520%2528OH%2529.%2520We%2520introduce%2520HALC%252C%2520a%2520novel%2520decoding%2520algorithm%250Adesigned%2520to%2520mitigate%2520OH%2520in%2520LVLMs.%2520HALC%2520leverages%2520distinct%2520fine-grained%2520optimal%250Avisual%2520information%2520in%2520vision-language%2520tasks%2520and%2520operates%2520on%2520both%2520local%2520and%250Aglobal%2520contexts%2520simultaneously.%2520Specifically%252C%2520HALC%2520integrates%2520a%2520robust%250Aauto-focal%2520grounding%2520mechanism%2520%2528locally%2529%2520to%2520correct%2520hallucinated%2520tokens%2520on%2520the%250Afly%252C%2520and%2520a%2520specialized%2520beam%2520search%2520algorithm%2520%2528globally%2529%2520to%2520significantly%2520reduce%250AOH%2520while%2520preserving%2520text%2520generation%2520quality.%2520Additionally%252C%2520HALC%2520can%2520be%250Aintegrated%2520into%2520any%2520LVLMs%2520as%2520a%2520plug-and-play%2520module%2520without%2520extra%2520training.%250AExtensive%2520experimental%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%2520HALC%2520in%250Areducing%2520OH%252C%2520outperforming%2520state-of-the-arts%2520across%2520four%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HALC%3A%20Object%20Hallucination%20Reduction%20via%20Adaptive%20Focal-Contrast%0A%20%20Decoding&entry.906535625=Zhaorun%20Chen%20and%20Zhuokai%20Zhao%20and%20Hongyin%20Luo%20and%20Huaxiu%20Yao%20and%20Bo%20Li%20and%20Jiawei%20Zhou&entry.1292438233=%20%20While%20large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20interpreting%20multi-modal%20contexts%2C%20they%20invariably%20suffer%20from%0Aobject%20hallucinations%20%28OH%29.%20We%20introduce%20HALC%2C%20a%20novel%20decoding%20algorithm%0Adesigned%20to%20mitigate%20OH%20in%20LVLMs.%20HALC%20leverages%20distinct%20fine-grained%20optimal%0Avisual%20information%20in%20vision-language%20tasks%20and%20operates%20on%20both%20local%20and%0Aglobal%20contexts%20simultaneously.%20Specifically%2C%20HALC%20integrates%20a%20robust%0Aauto-focal%20grounding%20mechanism%20%28locally%29%20to%20correct%20hallucinated%20tokens%20on%20the%0Afly%2C%20and%20a%20specialized%20beam%20search%20algorithm%20%28globally%29%20to%20significantly%20reduce%0AOH%20while%20preserving%20text%20generation%20quality.%20Additionally%2C%20HALC%20can%20be%0Aintegrated%20into%20any%20LVLMs%20as%20a%20plug-and-play%20module%20without%20extra%20training.%0AExtensive%20experimental%20studies%20demonstrate%20the%20effectiveness%20of%20HALC%20in%0Areducing%20OH%2C%20outperforming%20state-of-the-arts%20across%20four%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00425v2&entry.124074799=Read"},
{"title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural\n  language generation from feedback", "author": "Gaurav Pandey and Yatin Nandwani and Tahira Naseem and Mayank Mishra and Guangxuan Xu and Dinesh Raghu and Sachindra Joshi and Asim Munawar and Ram\u00f3n Fernandez Astudillo", "abstract": "  Distribution matching methods for language model alignment such as Generation\nwith Distributional Control (GDC) and Distributional Policy Gradient (DPG) have\nnot received the same level of attention in reinforcement learning from human\nfeedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration\n(SLiC), Direct Preference Optimization (DPO) and its variants. We identify high\nvariance of the gradient estimate as the primary reason for the lack of success\nof these methods and propose a self-normalized baseline to reduce the variance.\nWe further generalize the target distribution in DPG, GDC and DPO by using\nBayes' rule to define the reward-conditioned posterior. The resulting approach,\nreferred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as\na bridge between distribution matching methods and DPO and significantly\noutperforms prior art in summarization and Antropic HH tasks.\n", "link": "http://arxiv.org/abs/2402.02479v2", "date": "2024-06-10", "relevancy": 1.9678, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4827}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRAIn%3A%20Bayesian%20Reward-conditioned%20Amortized%20Inference%20for%20natural%0A%20%20language%20generation%20from%20feedback&body=Title%3A%20BRAIn%3A%20Bayesian%20Reward-conditioned%20Amortized%20Inference%20for%20natural%0A%20%20language%20generation%20from%20feedback%0AAuthor%3A%20Gaurav%20Pandey%20and%20Yatin%20Nandwani%20and%20Tahira%20Naseem%20and%20Mayank%20Mishra%20and%20Guangxuan%20Xu%20and%20Dinesh%20Raghu%20and%20Sachindra%20Joshi%20and%20Asim%20Munawar%20and%20Ram%C3%B3n%20Fernandez%20Astudillo%0AAbstract%3A%20%20%20Distribution%20matching%20methods%20for%20language%20model%20alignment%20such%20as%20Generation%0Awith%20Distributional%20Control%20%28GDC%29%20and%20Distributional%20Policy%20Gradient%20%28DPG%29%20have%0Anot%20received%20the%20same%20level%20of%20attention%20in%20reinforcement%20learning%20from%20human%0Afeedback%20%28RLHF%29%20as%20contrastive%20methods%20such%20as%20Sequence%20Likelihood%20Calibration%0A%28SLiC%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants.%20We%20identify%20high%0Avariance%20of%20the%20gradient%20estimate%20as%20the%20primary%20reason%20for%20the%20lack%20of%20success%0Aof%20these%20methods%20and%20propose%20a%20self-normalized%20baseline%20to%20reduce%20the%20variance.%0AWe%20further%20generalize%20the%20target%20distribution%20in%20DPG%2C%20GDC%20and%20DPO%20by%20using%0ABayes%27%20rule%20to%20define%20the%20reward-conditioned%20posterior.%20The%20resulting%20approach%2C%0Areferred%20to%20as%20BRAIn%20-%20Bayesian%20Reward-conditioned%20Amortized%20Inference%20acts%20as%0Aa%20bridge%20between%20distribution%20matching%20methods%20and%20DPO%20and%20significantly%0Aoutperforms%20prior%20art%20in%20summarization%20and%20Antropic%20HH%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRAIn%253A%2520Bayesian%2520Reward-conditioned%2520Amortized%2520Inference%2520for%2520natural%250A%2520%2520language%2520generation%2520from%2520feedback%26entry.906535625%3DGaurav%2520Pandey%2520and%2520Yatin%2520Nandwani%2520and%2520Tahira%2520Naseem%2520and%2520Mayank%2520Mishra%2520and%2520Guangxuan%2520Xu%2520and%2520Dinesh%2520Raghu%2520and%2520Sachindra%2520Joshi%2520and%2520Asim%2520Munawar%2520and%2520Ram%25C3%25B3n%2520Fernandez%2520Astudillo%26entry.1292438233%3D%2520%2520Distribution%2520matching%2520methods%2520for%2520language%2520model%2520alignment%2520such%2520as%2520Generation%250Awith%2520Distributional%2520Control%2520%2528GDC%2529%2520and%2520Distributional%2520Policy%2520Gradient%2520%2528DPG%2529%2520have%250Anot%2520received%2520the%2520same%2520level%2520of%2520attention%2520in%2520reinforcement%2520learning%2520from%2520human%250Afeedback%2520%2528RLHF%2529%2520as%2520contrastive%2520methods%2520such%2520as%2520Sequence%2520Likelihood%2520Calibration%250A%2528SLiC%2529%252C%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520its%2520variants.%2520We%2520identify%2520high%250Avariance%2520of%2520the%2520gradient%2520estimate%2520as%2520the%2520primary%2520reason%2520for%2520the%2520lack%2520of%2520success%250Aof%2520these%2520methods%2520and%2520propose%2520a%2520self-normalized%2520baseline%2520to%2520reduce%2520the%2520variance.%250AWe%2520further%2520generalize%2520the%2520target%2520distribution%2520in%2520DPG%252C%2520GDC%2520and%2520DPO%2520by%2520using%250ABayes%2527%2520rule%2520to%2520define%2520the%2520reward-conditioned%2520posterior.%2520The%2520resulting%2520approach%252C%250Areferred%2520to%2520as%2520BRAIn%2520-%2520Bayesian%2520Reward-conditioned%2520Amortized%2520Inference%2520acts%2520as%250Aa%2520bridge%2520between%2520distribution%2520matching%2520methods%2520and%2520DPO%2520and%2520significantly%250Aoutperforms%2520prior%2520art%2520in%2520summarization%2520and%2520Antropic%2520HH%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRAIn%3A%20Bayesian%20Reward-conditioned%20Amortized%20Inference%20for%20natural%0A%20%20language%20generation%20from%20feedback&entry.906535625=Gaurav%20Pandey%20and%20Yatin%20Nandwani%20and%20Tahira%20Naseem%20and%20Mayank%20Mishra%20and%20Guangxuan%20Xu%20and%20Dinesh%20Raghu%20and%20Sachindra%20Joshi%20and%20Asim%20Munawar%20and%20Ram%C3%B3n%20Fernandez%20Astudillo&entry.1292438233=%20%20Distribution%20matching%20methods%20for%20language%20model%20alignment%20such%20as%20Generation%0Awith%20Distributional%20Control%20%28GDC%29%20and%20Distributional%20Policy%20Gradient%20%28DPG%29%20have%0Anot%20received%20the%20same%20level%20of%20attention%20in%20reinforcement%20learning%20from%20human%0Afeedback%20%28RLHF%29%20as%20contrastive%20methods%20such%20as%20Sequence%20Likelihood%20Calibration%0A%28SLiC%29%2C%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants.%20We%20identify%20high%0Avariance%20of%20the%20gradient%20estimate%20as%20the%20primary%20reason%20for%20the%20lack%20of%20success%0Aof%20these%20methods%20and%20propose%20a%20self-normalized%20baseline%20to%20reduce%20the%20variance.%0AWe%20further%20generalize%20the%20target%20distribution%20in%20DPG%2C%20GDC%20and%20DPO%20by%20using%0ABayes%27%20rule%20to%20define%20the%20reward-conditioned%20posterior.%20The%20resulting%20approach%2C%0Areferred%20to%20as%20BRAIn%20-%20Bayesian%20Reward-conditioned%20Amortized%20Inference%20acts%20as%0Aa%20bridge%20between%20distribution%20matching%20methods%20and%20DPO%20and%20significantly%0Aoutperforms%20prior%20art%20in%20summarization%20and%20Antropic%20HH%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02479v2&entry.124074799=Read"},
{"title": "An Improved Empirical Fisher Approximation for Natural Gradient Descent", "author": "Xiaodong Wu and Wenyi Yu and Chao Zhang and Philip Woodland", "abstract": "  Approximate Natural Gradient Descent (NGD) methods are an important family of\noptimisers for deep learning models, which use approximate Fisher information\nmatrices to pre-condition gradients during training. The empirical Fisher (EF)\nmethod approximates the Fisher information matrix empirically by reusing the\nper-sample gradients collected during back-propagation. Despite its ease of\nimplementation, the EF approximation has its theoretical and practical\nlimitations. This paper first investigates the inversely-scaled projection\nissue of EF, which is shown to be a major cause of the poor empirical\napproximation quality. An improved empirical Fisher (iEF) method, motivated as\na generalised NGD method from a loss reduction perspective, is proposed to\naddress this issue, meanwhile retaining the practical convenience of EF. The\nexact iEF and EF methods are experimentally evaluated using practical deep\nlearning setups, including widely-used setups for parameter-efficient\nfine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE\ntasks, and ViT with LoRA for CIFAR100). Optimisation experiments show that\napplying exact iEF as an optimiser provides strong convergence and\ngeneralisation. It achieves the best test performance and the lowest training\nloss for majority of the tasks, even when compared with well-tuned\nAdamW/Adafactor baselines. Additionally, under a novel empirical evaluation\nframework, the proposed iEF method shows consistently better approximation\nquality to the exact Natural Gradient updates than both EF and the more\nexpensive sampled Fisher (SF). Further investigation also shows that the\nsuperior approximation quality of iEF is robust to damping across tasks and\ntraining stages. Improving existing approximate NGD optimisers with iEF is\nexpected to lead to better convergence ability and stronger robustness to\nchoice of damping.\n", "link": "http://arxiv.org/abs/2406.06420v1", "date": "2024-06-10", "relevancy": 1.9659, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5024}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.488}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Improved%20Empirical%20Fisher%20Approximation%20for%20Natural%20Gradient%20Descent&body=Title%3A%20An%20Improved%20Empirical%20Fisher%20Approximation%20for%20Natural%20Gradient%20Descent%0AAuthor%3A%20Xiaodong%20Wu%20and%20Wenyi%20Yu%20and%20Chao%20Zhang%20and%20Philip%20Woodland%0AAbstract%3A%20%20%20Approximate%20Natural%20Gradient%20Descent%20%28NGD%29%20methods%20are%20an%20important%20family%20of%0Aoptimisers%20for%20deep%20learning%20models%2C%20which%20use%20approximate%20Fisher%20information%0Amatrices%20to%20pre-condition%20gradients%20during%20training.%20The%20empirical%20Fisher%20%28EF%29%0Amethod%20approximates%20the%20Fisher%20information%20matrix%20empirically%20by%20reusing%20the%0Aper-sample%20gradients%20collected%20during%20back-propagation.%20Despite%20its%20ease%20of%0Aimplementation%2C%20the%20EF%20approximation%20has%20its%20theoretical%20and%20practical%0Alimitations.%20This%20paper%20first%20investigates%20the%20inversely-scaled%20projection%0Aissue%20of%20EF%2C%20which%20is%20shown%20to%20be%20a%20major%20cause%20of%20the%20poor%20empirical%0Aapproximation%20quality.%20An%20improved%20empirical%20Fisher%20%28iEF%29%20method%2C%20motivated%20as%0Aa%20generalised%20NGD%20method%20from%20a%20loss%20reduction%20perspective%2C%20is%20proposed%20to%0Aaddress%20this%20issue%2C%20meanwhile%20retaining%20the%20practical%20convenience%20of%20EF.%20The%0Aexact%20iEF%20and%20EF%20methods%20are%20experimentally%20evaluated%20using%20practical%20deep%0Alearning%20setups%2C%20including%20widely-used%20setups%20for%20parameter-efficient%0Afine-tuning%20of%20pre-trained%20models%20%28T5-base%20with%20LoRA%20and%20Prompt-Tuning%20on%20GLUE%0Atasks%2C%20and%20ViT%20with%20LoRA%20for%20CIFAR100%29.%20Optimisation%20experiments%20show%20that%0Aapplying%20exact%20iEF%20as%20an%20optimiser%20provides%20strong%20convergence%20and%0Ageneralisation.%20It%20achieves%20the%20best%20test%20performance%20and%20the%20lowest%20training%0Aloss%20for%20majority%20of%20the%20tasks%2C%20even%20when%20compared%20with%20well-tuned%0AAdamW/Adafactor%20baselines.%20Additionally%2C%20under%20a%20novel%20empirical%20evaluation%0Aframework%2C%20the%20proposed%20iEF%20method%20shows%20consistently%20better%20approximation%0Aquality%20to%20the%20exact%20Natural%20Gradient%20updates%20than%20both%20EF%20and%20the%20more%0Aexpensive%20sampled%20Fisher%20%28SF%29.%20Further%20investigation%20also%20shows%20that%20the%0Asuperior%20approximation%20quality%20of%20iEF%20is%20robust%20to%20damping%20across%20tasks%20and%0Atraining%20stages.%20Improving%20existing%20approximate%20NGD%20optimisers%20with%20iEF%20is%0Aexpected%20to%20lead%20to%20better%20convergence%20ability%20and%20stronger%20robustness%20to%0Achoice%20of%20damping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Improved%2520Empirical%2520Fisher%2520Approximation%2520for%2520Natural%2520Gradient%2520Descent%26entry.906535625%3DXiaodong%2520Wu%2520and%2520Wenyi%2520Yu%2520and%2520Chao%2520Zhang%2520and%2520Philip%2520Woodland%26entry.1292438233%3D%2520%2520Approximate%2520Natural%2520Gradient%2520Descent%2520%2528NGD%2529%2520methods%2520are%2520an%2520important%2520family%2520of%250Aoptimisers%2520for%2520deep%2520learning%2520models%252C%2520which%2520use%2520approximate%2520Fisher%2520information%250Amatrices%2520to%2520pre-condition%2520gradients%2520during%2520training.%2520The%2520empirical%2520Fisher%2520%2528EF%2529%250Amethod%2520approximates%2520the%2520Fisher%2520information%2520matrix%2520empirically%2520by%2520reusing%2520the%250Aper-sample%2520gradients%2520collected%2520during%2520back-propagation.%2520Despite%2520its%2520ease%2520of%250Aimplementation%252C%2520the%2520EF%2520approximation%2520has%2520its%2520theoretical%2520and%2520practical%250Alimitations.%2520This%2520paper%2520first%2520investigates%2520the%2520inversely-scaled%2520projection%250Aissue%2520of%2520EF%252C%2520which%2520is%2520shown%2520to%2520be%2520a%2520major%2520cause%2520of%2520the%2520poor%2520empirical%250Aapproximation%2520quality.%2520An%2520improved%2520empirical%2520Fisher%2520%2528iEF%2529%2520method%252C%2520motivated%2520as%250Aa%2520generalised%2520NGD%2520method%2520from%2520a%2520loss%2520reduction%2520perspective%252C%2520is%2520proposed%2520to%250Aaddress%2520this%2520issue%252C%2520meanwhile%2520retaining%2520the%2520practical%2520convenience%2520of%2520EF.%2520The%250Aexact%2520iEF%2520and%2520EF%2520methods%2520are%2520experimentally%2520evaluated%2520using%2520practical%2520deep%250Alearning%2520setups%252C%2520including%2520widely-used%2520setups%2520for%2520parameter-efficient%250Afine-tuning%2520of%2520pre-trained%2520models%2520%2528T5-base%2520with%2520LoRA%2520and%2520Prompt-Tuning%2520on%2520GLUE%250Atasks%252C%2520and%2520ViT%2520with%2520LoRA%2520for%2520CIFAR100%2529.%2520Optimisation%2520experiments%2520show%2520that%250Aapplying%2520exact%2520iEF%2520as%2520an%2520optimiser%2520provides%2520strong%2520convergence%2520and%250Ageneralisation.%2520It%2520achieves%2520the%2520best%2520test%2520performance%2520and%2520the%2520lowest%2520training%250Aloss%2520for%2520majority%2520of%2520the%2520tasks%252C%2520even%2520when%2520compared%2520with%2520well-tuned%250AAdamW/Adafactor%2520baselines.%2520Additionally%252C%2520under%2520a%2520novel%2520empirical%2520evaluation%250Aframework%252C%2520the%2520proposed%2520iEF%2520method%2520shows%2520consistently%2520better%2520approximation%250Aquality%2520to%2520the%2520exact%2520Natural%2520Gradient%2520updates%2520than%2520both%2520EF%2520and%2520the%2520more%250Aexpensive%2520sampled%2520Fisher%2520%2528SF%2529.%2520Further%2520investigation%2520also%2520shows%2520that%2520the%250Asuperior%2520approximation%2520quality%2520of%2520iEF%2520is%2520robust%2520to%2520damping%2520across%2520tasks%2520and%250Atraining%2520stages.%2520Improving%2520existing%2520approximate%2520NGD%2520optimisers%2520with%2520iEF%2520is%250Aexpected%2520to%2520lead%2520to%2520better%2520convergence%2520ability%2520and%2520stronger%2520robustness%2520to%250Achoice%2520of%2520damping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Improved%20Empirical%20Fisher%20Approximation%20for%20Natural%20Gradient%20Descent&entry.906535625=Xiaodong%20Wu%20and%20Wenyi%20Yu%20and%20Chao%20Zhang%20and%20Philip%20Woodland&entry.1292438233=%20%20Approximate%20Natural%20Gradient%20Descent%20%28NGD%29%20methods%20are%20an%20important%20family%20of%0Aoptimisers%20for%20deep%20learning%20models%2C%20which%20use%20approximate%20Fisher%20information%0Amatrices%20to%20pre-condition%20gradients%20during%20training.%20The%20empirical%20Fisher%20%28EF%29%0Amethod%20approximates%20the%20Fisher%20information%20matrix%20empirically%20by%20reusing%20the%0Aper-sample%20gradients%20collected%20during%20back-propagation.%20Despite%20its%20ease%20of%0Aimplementation%2C%20the%20EF%20approximation%20has%20its%20theoretical%20and%20practical%0Alimitations.%20This%20paper%20first%20investigates%20the%20inversely-scaled%20projection%0Aissue%20of%20EF%2C%20which%20is%20shown%20to%20be%20a%20major%20cause%20of%20the%20poor%20empirical%0Aapproximation%20quality.%20An%20improved%20empirical%20Fisher%20%28iEF%29%20method%2C%20motivated%20as%0Aa%20generalised%20NGD%20method%20from%20a%20loss%20reduction%20perspective%2C%20is%20proposed%20to%0Aaddress%20this%20issue%2C%20meanwhile%20retaining%20the%20practical%20convenience%20of%20EF.%20The%0Aexact%20iEF%20and%20EF%20methods%20are%20experimentally%20evaluated%20using%20practical%20deep%0Alearning%20setups%2C%20including%20widely-used%20setups%20for%20parameter-efficient%0Afine-tuning%20of%20pre-trained%20models%20%28T5-base%20with%20LoRA%20and%20Prompt-Tuning%20on%20GLUE%0Atasks%2C%20and%20ViT%20with%20LoRA%20for%20CIFAR100%29.%20Optimisation%20experiments%20show%20that%0Aapplying%20exact%20iEF%20as%20an%20optimiser%20provides%20strong%20convergence%20and%0Ageneralisation.%20It%20achieves%20the%20best%20test%20performance%20and%20the%20lowest%20training%0Aloss%20for%20majority%20of%20the%20tasks%2C%20even%20when%20compared%20with%20well-tuned%0AAdamW/Adafactor%20baselines.%20Additionally%2C%20under%20a%20novel%20empirical%20evaluation%0Aframework%2C%20the%20proposed%20iEF%20method%20shows%20consistently%20better%20approximation%0Aquality%20to%20the%20exact%20Natural%20Gradient%20updates%20than%20both%20EF%20and%20the%20more%0Aexpensive%20sampled%20Fisher%20%28SF%29.%20Further%20investigation%20also%20shows%20that%20the%0Asuperior%20approximation%20quality%20of%20iEF%20is%20robust%20to%20damping%20across%20tasks%20and%0Atraining%20stages.%20Improving%20existing%20approximate%20NGD%20optimisers%20with%20iEF%20is%0Aexpected%20to%20lead%20to%20better%20convergence%20ability%20and%20stronger%20robustness%20to%0Achoice%20of%20damping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06420v1&entry.124074799=Read"},
{"title": "Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous\n  Driving", "author": "Daniel Bogdoll and Jan Imhof and Tim Joseph and J. Marius Z\u00f6llner", "abstract": "  In autonomous driving, the most challenging scenarios are the ones that can\nonly be detected within their temporal context. Most video anomaly detection\napproaches focus either on surveillance or traffic accidents, which are only a\nsubfield of autonomous driving. In this work, we present HF$^2$-VAD$_{AD}$, a\nvariation of the HF$^2$-VAD surveillance video anomaly detection method for\nautonomous driving. We learn a representation of normality from a vehicle's ego\nperspective and evaluate pixel-wise anomaly detections in rare and critical\nscenarios.\n", "link": "http://arxiv.org/abs/2406.06423v1", "date": "2024-06-10", "relevancy": 1.9634, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4967}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4912}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving&body=Title%3A%20Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Daniel%20Bogdoll%20and%20Jan%20Imhof%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20the%20most%20challenging%20scenarios%20are%20the%20ones%20that%20can%0Aonly%20be%20detected%20within%20their%20temporal%20context.%20Most%20video%20anomaly%20detection%0Aapproaches%20focus%20either%20on%20surveillance%20or%20traffic%20accidents%2C%20which%20are%20only%20a%0Asubfield%20of%20autonomous%20driving.%20In%20this%20work%2C%20we%20present%20HF%24%5E2%24-VAD%24_%7BAD%7D%24%2C%20a%0Avariation%20of%20the%20HF%24%5E2%24-VAD%20surveillance%20video%20anomaly%20detection%20method%20for%0Aautonomous%20driving.%20We%20learn%20a%20representation%20of%20normality%20from%20a%20vehicle%27s%20ego%0Aperspective%20and%20evaluate%20pixel-wise%20anomaly%20detections%20in%20rare%20and%20critical%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Video%2520Anomaly%2520Detection%2520for%2520Anomalous%2520Scenarios%2520in%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520Jan%2520Imhof%2520and%2520Tim%2520Joseph%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520the%2520most%2520challenging%2520scenarios%2520are%2520the%2520ones%2520that%2520can%250Aonly%2520be%2520detected%2520within%2520their%2520temporal%2520context.%2520Most%2520video%2520anomaly%2520detection%250Aapproaches%2520focus%2520either%2520on%2520surveillance%2520or%2520traffic%2520accidents%252C%2520which%2520are%2520only%2520a%250Asubfield%2520of%2520autonomous%2520driving.%2520In%2520this%2520work%252C%2520we%2520present%2520HF%2524%255E2%2524-VAD%2524_%257BAD%257D%2524%252C%2520a%250Avariation%2520of%2520the%2520HF%2524%255E2%2524-VAD%2520surveillance%2520video%2520anomaly%2520detection%2520method%2520for%250Aautonomous%2520driving.%2520We%2520learn%2520a%2520representation%2520of%2520normality%2520from%2520a%2520vehicle%2527s%2520ego%250Aperspective%2520and%2520evaluate%2520pixel-wise%2520anomaly%2520detections%2520in%2520rare%2520and%2520critical%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving&entry.906535625=Daniel%20Bogdoll%20and%20Jan%20Imhof%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20In%20autonomous%20driving%2C%20the%20most%20challenging%20scenarios%20are%20the%20ones%20that%20can%0Aonly%20be%20detected%20within%20their%20temporal%20context.%20Most%20video%20anomaly%20detection%0Aapproaches%20focus%20either%20on%20surveillance%20or%20traffic%20accidents%2C%20which%20are%20only%20a%0Asubfield%20of%20autonomous%20driving.%20In%20this%20work%2C%20we%20present%20HF%24%5E2%24-VAD%24_%7BAD%7D%24%2C%20a%0Avariation%20of%20the%20HF%24%5E2%24-VAD%20surveillance%20video%20anomaly%20detection%20method%20for%0Aautonomous%20driving.%20We%20learn%20a%20representation%20of%20normality%20from%20a%20vehicle%27s%20ego%0Aperspective%20and%20evaluate%20pixel-wise%20anomaly%20detections%20in%20rare%20and%20critical%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06423v1&entry.124074799=Read"},
{"title": "Building Continuous Quantum-Classical Bayesian Neural Networks for a\n  Classical Clinical Dataset", "author": "Alona Sakhnenko and Julian Sikora and Jeanette Miriam Lorenz", "abstract": "  In this work, we are introducing a Quantum-Classical Bayesian Neural Network\n(QCBNN) that is capable to perform uncertainty-aware classification of\nclassical medical dataset. This model is a symbiosis of a classical\nConvolutional NN that performs ultra-sound image processing and a quantum\ncircuit that generates its stochastic weights, within a Bayesian learning\nframework. To test the utility of this idea for the possible future deployment\nin the medical sector we track multiple behavioral metrics that capture both\npredictive performance as well as model's uncertainty. It is our ambition to\ncreate a hybrid model that is capable to classify samples in a more uncertainty\naware fashion, which will advance the trustworthiness of these models and thus\nbring us step closer to utilizing them in the industry. We test multiple setups\nfor quantum circuit for this task, and our best architectures display bigger\nuncertainty gap between correctly and incorrectly identified samples than its\nclassical benchmark at an expense of a slight drop in predictive performance.\nThe innovation of this paper is two-fold: (1) combining of different approaches\nthat allow the stochastic weights from the quantum circuit to be continues thus\nallowing the model to classify application-driven dataset; (2) studying\narchitectural features of quantum circuit that make-or-break these models,\nwhich pave the way into further investigation of more informed architectural\ndesigns.\n", "link": "http://arxiv.org/abs/2406.06307v1", "date": "2024-06-10", "relevancy": 1.9604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Continuous%20Quantum-Classical%20Bayesian%20Neural%20Networks%20for%20a%0A%20%20Classical%20Clinical%20Dataset&body=Title%3A%20Building%20Continuous%20Quantum-Classical%20Bayesian%20Neural%20Networks%20for%20a%0A%20%20Classical%20Clinical%20Dataset%0AAuthor%3A%20Alona%20Sakhnenko%20and%20Julian%20Sikora%20and%20Jeanette%20Miriam%20Lorenz%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20are%20introducing%20a%20Quantum-Classical%20Bayesian%20Neural%20Network%0A%28QCBNN%29%20that%20is%20capable%20to%20perform%20uncertainty-aware%20classification%20of%0Aclassical%20medical%20dataset.%20This%20model%20is%20a%20symbiosis%20of%20a%20classical%0AConvolutional%20NN%20that%20performs%20ultra-sound%20image%20processing%20and%20a%20quantum%0Acircuit%20that%20generates%20its%20stochastic%20weights%2C%20within%20a%20Bayesian%20learning%0Aframework.%20To%20test%20the%20utility%20of%20this%20idea%20for%20the%20possible%20future%20deployment%0Ain%20the%20medical%20sector%20we%20track%20multiple%20behavioral%20metrics%20that%20capture%20both%0Apredictive%20performance%20as%20well%20as%20model%27s%20uncertainty.%20It%20is%20our%20ambition%20to%0Acreate%20a%20hybrid%20model%20that%20is%20capable%20to%20classify%20samples%20in%20a%20more%20uncertainty%0Aaware%20fashion%2C%20which%20will%20advance%20the%20trustworthiness%20of%20these%20models%20and%20thus%0Abring%20us%20step%20closer%20to%20utilizing%20them%20in%20the%20industry.%20We%20test%20multiple%20setups%0Afor%20quantum%20circuit%20for%20this%20task%2C%20and%20our%20best%20architectures%20display%20bigger%0Auncertainty%20gap%20between%20correctly%20and%20incorrectly%20identified%20samples%20than%20its%0Aclassical%20benchmark%20at%20an%20expense%20of%20a%20slight%20drop%20in%20predictive%20performance.%0AThe%20innovation%20of%20this%20paper%20is%20two-fold%3A%20%281%29%20combining%20of%20different%20approaches%0Athat%20allow%20the%20stochastic%20weights%20from%20the%20quantum%20circuit%20to%20be%20continues%20thus%0Aallowing%20the%20model%20to%20classify%20application-driven%20dataset%3B%20%282%29%20studying%0Aarchitectural%20features%20of%20quantum%20circuit%20that%20make-or-break%20these%20models%2C%0Awhich%20pave%20the%20way%20into%20further%20investigation%20of%20more%20informed%20architectural%0Adesigns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Continuous%2520Quantum-Classical%2520Bayesian%2520Neural%2520Networks%2520for%2520a%250A%2520%2520Classical%2520Clinical%2520Dataset%26entry.906535625%3DAlona%2520Sakhnenko%2520and%2520Julian%2520Sikora%2520and%2520Jeanette%2520Miriam%2520Lorenz%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520are%2520introducing%2520a%2520Quantum-Classical%2520Bayesian%2520Neural%2520Network%250A%2528QCBNN%2529%2520that%2520is%2520capable%2520to%2520perform%2520uncertainty-aware%2520classification%2520of%250Aclassical%2520medical%2520dataset.%2520This%2520model%2520is%2520a%2520symbiosis%2520of%2520a%2520classical%250AConvolutional%2520NN%2520that%2520performs%2520ultra-sound%2520image%2520processing%2520and%2520a%2520quantum%250Acircuit%2520that%2520generates%2520its%2520stochastic%2520weights%252C%2520within%2520a%2520Bayesian%2520learning%250Aframework.%2520To%2520test%2520the%2520utility%2520of%2520this%2520idea%2520for%2520the%2520possible%2520future%2520deployment%250Ain%2520the%2520medical%2520sector%2520we%2520track%2520multiple%2520behavioral%2520metrics%2520that%2520capture%2520both%250Apredictive%2520performance%2520as%2520well%2520as%2520model%2527s%2520uncertainty.%2520It%2520is%2520our%2520ambition%2520to%250Acreate%2520a%2520hybrid%2520model%2520that%2520is%2520capable%2520to%2520classify%2520samples%2520in%2520a%2520more%2520uncertainty%250Aaware%2520fashion%252C%2520which%2520will%2520advance%2520the%2520trustworthiness%2520of%2520these%2520models%2520and%2520thus%250Abring%2520us%2520step%2520closer%2520to%2520utilizing%2520them%2520in%2520the%2520industry.%2520We%2520test%2520multiple%2520setups%250Afor%2520quantum%2520circuit%2520for%2520this%2520task%252C%2520and%2520our%2520best%2520architectures%2520display%2520bigger%250Auncertainty%2520gap%2520between%2520correctly%2520and%2520incorrectly%2520identified%2520samples%2520than%2520its%250Aclassical%2520benchmark%2520at%2520an%2520expense%2520of%2520a%2520slight%2520drop%2520in%2520predictive%2520performance.%250AThe%2520innovation%2520of%2520this%2520paper%2520is%2520two-fold%253A%2520%25281%2529%2520combining%2520of%2520different%2520approaches%250Athat%2520allow%2520the%2520stochastic%2520weights%2520from%2520the%2520quantum%2520circuit%2520to%2520be%2520continues%2520thus%250Aallowing%2520the%2520model%2520to%2520classify%2520application-driven%2520dataset%253B%2520%25282%2529%2520studying%250Aarchitectural%2520features%2520of%2520quantum%2520circuit%2520that%2520make-or-break%2520these%2520models%252C%250Awhich%2520pave%2520the%2520way%2520into%2520further%2520investigation%2520of%2520more%2520informed%2520architectural%250Adesigns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Continuous%20Quantum-Classical%20Bayesian%20Neural%20Networks%20for%20a%0A%20%20Classical%20Clinical%20Dataset&entry.906535625=Alona%20Sakhnenko%20and%20Julian%20Sikora%20and%20Jeanette%20Miriam%20Lorenz&entry.1292438233=%20%20In%20this%20work%2C%20we%20are%20introducing%20a%20Quantum-Classical%20Bayesian%20Neural%20Network%0A%28QCBNN%29%20that%20is%20capable%20to%20perform%20uncertainty-aware%20classification%20of%0Aclassical%20medical%20dataset.%20This%20model%20is%20a%20symbiosis%20of%20a%20classical%0AConvolutional%20NN%20that%20performs%20ultra-sound%20image%20processing%20and%20a%20quantum%0Acircuit%20that%20generates%20its%20stochastic%20weights%2C%20within%20a%20Bayesian%20learning%0Aframework.%20To%20test%20the%20utility%20of%20this%20idea%20for%20the%20possible%20future%20deployment%0Ain%20the%20medical%20sector%20we%20track%20multiple%20behavioral%20metrics%20that%20capture%20both%0Apredictive%20performance%20as%20well%20as%20model%27s%20uncertainty.%20It%20is%20our%20ambition%20to%0Acreate%20a%20hybrid%20model%20that%20is%20capable%20to%20classify%20samples%20in%20a%20more%20uncertainty%0Aaware%20fashion%2C%20which%20will%20advance%20the%20trustworthiness%20of%20these%20models%20and%20thus%0Abring%20us%20step%20closer%20to%20utilizing%20them%20in%20the%20industry.%20We%20test%20multiple%20setups%0Afor%20quantum%20circuit%20for%20this%20task%2C%20and%20our%20best%20architectures%20display%20bigger%0Auncertainty%20gap%20between%20correctly%20and%20incorrectly%20identified%20samples%20than%20its%0Aclassical%20benchmark%20at%20an%20expense%20of%20a%20slight%20drop%20in%20predictive%20performance.%0AThe%20innovation%20of%20this%20paper%20is%20two-fold%3A%20%281%29%20combining%20of%20different%20approaches%0Athat%20allow%20the%20stochastic%20weights%20from%20the%20quantum%20circuit%20to%20be%20continues%20thus%0Aallowing%20the%20model%20to%20classify%20application-driven%20dataset%3B%20%282%29%20studying%0Aarchitectural%20features%20of%20quantum%20circuit%20that%20make-or-break%20these%20models%2C%0Awhich%20pave%20the%20way%20into%20further%20investigation%20of%20more%20informed%20architectural%0Adesigns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06307v1&entry.124074799=Read"},
{"title": "Cascading Unknown Detection with Known Classification for Open Set\n  Recognition", "author": "Daniel Brignac and Abhijit Mahalanobis", "abstract": "  Deep learners tend to perform well when trained under the closed set\nassumption but struggle when deployed under open set conditions. This motivates\nthe field of Open Set Recognition in which we seek to give deep learners the\nability to recognize whether a data sample belongs to the known classes trained\non or comes from the surrounding infinite world. Existing open set recognition\nmethods typically rely upon a single function for the dual task of\ndistinguishing between knowns and unknowns as well as making known class\ndistinction. This dual process leaves performance on the table as the function\nis not specialized for either task. In this work, we introduce Cascading\nUnknown Detection with Known Classification (Cas-DC), where we instead learn\nspecialized functions in a cascading fashion for both known/unknown detection\nand fine class classification amongst the world of knowns. Our experiments and\nanalysis demonstrate that Cas-DC handily outperforms modern methods in open set\nrecognition when compared using AUROC scores and correct classification rate at\nvarious true positive rates.\n", "link": "http://arxiv.org/abs/2406.06351v1", "date": "2024-06-10", "relevancy": 1.9563, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5045}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascading%20Unknown%20Detection%20with%20Known%20Classification%20for%20Open%20Set%0A%20%20Recognition&body=Title%3A%20Cascading%20Unknown%20Detection%20with%20Known%20Classification%20for%20Open%20Set%0A%20%20Recognition%0AAuthor%3A%20Daniel%20Brignac%20and%20Abhijit%20Mahalanobis%0AAbstract%3A%20%20%20Deep%20learners%20tend%20to%20perform%20well%20when%20trained%20under%20the%20closed%20set%0Aassumption%20but%20struggle%20when%20deployed%20under%20open%20set%20conditions.%20This%20motivates%0Athe%20field%20of%20Open%20Set%20Recognition%20in%20which%20we%20seek%20to%20give%20deep%20learners%20the%0Aability%20to%20recognize%20whether%20a%20data%20sample%20belongs%20to%20the%20known%20classes%20trained%0Aon%20or%20comes%20from%20the%20surrounding%20infinite%20world.%20Existing%20open%20set%20recognition%0Amethods%20typically%20rely%20upon%20a%20single%20function%20for%20the%20dual%20task%20of%0Adistinguishing%20between%20knowns%20and%20unknowns%20as%20well%20as%20making%20known%20class%0Adistinction.%20This%20dual%20process%20leaves%20performance%20on%20the%20table%20as%20the%20function%0Ais%20not%20specialized%20for%20either%20task.%20In%20this%20work%2C%20we%20introduce%20Cascading%0AUnknown%20Detection%20with%20Known%20Classification%20%28Cas-DC%29%2C%20where%20we%20instead%20learn%0Aspecialized%20functions%20in%20a%20cascading%20fashion%20for%20both%20known/unknown%20detection%0Aand%20fine%20class%20classification%20amongst%20the%20world%20of%20knowns.%20Our%20experiments%20and%0Aanalysis%20demonstrate%20that%20Cas-DC%20handily%20outperforms%20modern%20methods%20in%20open%20set%0Arecognition%20when%20compared%20using%20AUROC%20scores%20and%20correct%20classification%20rate%20at%0Avarious%20true%20positive%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascading%2520Unknown%2520Detection%2520with%2520Known%2520Classification%2520for%2520Open%2520Set%250A%2520%2520Recognition%26entry.906535625%3DDaniel%2520Brignac%2520and%2520Abhijit%2520Mahalanobis%26entry.1292438233%3D%2520%2520Deep%2520learners%2520tend%2520to%2520perform%2520well%2520when%2520trained%2520under%2520the%2520closed%2520set%250Aassumption%2520but%2520struggle%2520when%2520deployed%2520under%2520open%2520set%2520conditions.%2520This%2520motivates%250Athe%2520field%2520of%2520Open%2520Set%2520Recognition%2520in%2520which%2520we%2520seek%2520to%2520give%2520deep%2520learners%2520the%250Aability%2520to%2520recognize%2520whether%2520a%2520data%2520sample%2520belongs%2520to%2520the%2520known%2520classes%2520trained%250Aon%2520or%2520comes%2520from%2520the%2520surrounding%2520infinite%2520world.%2520Existing%2520open%2520set%2520recognition%250Amethods%2520typically%2520rely%2520upon%2520a%2520single%2520function%2520for%2520the%2520dual%2520task%2520of%250Adistinguishing%2520between%2520knowns%2520and%2520unknowns%2520as%2520well%2520as%2520making%2520known%2520class%250Adistinction.%2520This%2520dual%2520process%2520leaves%2520performance%2520on%2520the%2520table%2520as%2520the%2520function%250Ais%2520not%2520specialized%2520for%2520either%2520task.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Cascading%250AUnknown%2520Detection%2520with%2520Known%2520Classification%2520%2528Cas-DC%2529%252C%2520where%2520we%2520instead%2520learn%250Aspecialized%2520functions%2520in%2520a%2520cascading%2520fashion%2520for%2520both%2520known/unknown%2520detection%250Aand%2520fine%2520class%2520classification%2520amongst%2520the%2520world%2520of%2520knowns.%2520Our%2520experiments%2520and%250Aanalysis%2520demonstrate%2520that%2520Cas-DC%2520handily%2520outperforms%2520modern%2520methods%2520in%2520open%2520set%250Arecognition%2520when%2520compared%2520using%2520AUROC%2520scores%2520and%2520correct%2520classification%2520rate%2520at%250Avarious%2520true%2520positive%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascading%20Unknown%20Detection%20with%20Known%20Classification%20for%20Open%20Set%0A%20%20Recognition&entry.906535625=Daniel%20Brignac%20and%20Abhijit%20Mahalanobis&entry.1292438233=%20%20Deep%20learners%20tend%20to%20perform%20well%20when%20trained%20under%20the%20closed%20set%0Aassumption%20but%20struggle%20when%20deployed%20under%20open%20set%20conditions.%20This%20motivates%0Athe%20field%20of%20Open%20Set%20Recognition%20in%20which%20we%20seek%20to%20give%20deep%20learners%20the%0Aability%20to%20recognize%20whether%20a%20data%20sample%20belongs%20to%20the%20known%20classes%20trained%0Aon%20or%20comes%20from%20the%20surrounding%20infinite%20world.%20Existing%20open%20set%20recognition%0Amethods%20typically%20rely%20upon%20a%20single%20function%20for%20the%20dual%20task%20of%0Adistinguishing%20between%20knowns%20and%20unknowns%20as%20well%20as%20making%20known%20class%0Adistinction.%20This%20dual%20process%20leaves%20performance%20on%20the%20table%20as%20the%20function%0Ais%20not%20specialized%20for%20either%20task.%20In%20this%20work%2C%20we%20introduce%20Cascading%0AUnknown%20Detection%20with%20Known%20Classification%20%28Cas-DC%29%2C%20where%20we%20instead%20learn%0Aspecialized%20functions%20in%20a%20cascading%20fashion%20for%20both%20known/unknown%20detection%0Aand%20fine%20class%20classification%20amongst%20the%20world%20of%20knowns.%20Our%20experiments%20and%0Aanalysis%20demonstrate%20that%20Cas-DC%20handily%20outperforms%20modern%20methods%20in%20open%20set%0Arecognition%20when%20compared%20using%20AUROC%20scores%20and%20correct%20classification%20rate%20at%0Avarious%20true%20positive%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06351v1&entry.124074799=Read"},
{"title": "Modular Growth of Hierarchical Networks: Efficient, General, and Robust\n  Curriculum Learning", "author": "Mani Hamidi and Sina Khajehabdollahi and Emmanouil Giannakakis and Tim Sch\u00e4fer and Anna Levina and Charley M. Wu", "abstract": "  Structural modularity is a pervasive feature of biological neural networks,\nwhich have been linked to several functional and computational advantages. Yet,\nthe use of modular architectures in artificial neural networks has been\nrelatively limited despite early successes. Here, we explore the performance\nand functional dynamics of a modular network trained on a memory task via an\niterative growth curriculum. We find that for a given classical, non-modular\nrecurrent neural network (RNN), an equivalent modular network will perform\nbetter across multiple metrics, including training time, generalizability, and\nrobustness to some perturbations. We further examine how different aspects of a\nmodular network's connectivity contribute to its computational capability. We\nthen demonstrate that the inductive bias introduced by the modular topology is\nstrong enough for the network to perform well even when the connectivity within\nmodules is fixed and only the connections between modules are trained. Our\nfindings suggest that gradual modular growth of RNNs could provide advantages\nfor learning increasingly complex tasks on evolutionary timescales, and help\nbuild more scalable and compressible artificial networks.\n", "link": "http://arxiv.org/abs/2406.06262v1", "date": "2024-06-10", "relevancy": 1.9437, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4711}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Growth%20of%20Hierarchical%20Networks%3A%20Efficient%2C%20General%2C%20and%20Robust%0A%20%20Curriculum%20Learning&body=Title%3A%20Modular%20Growth%20of%20Hierarchical%20Networks%3A%20Efficient%2C%20General%2C%20and%20Robust%0A%20%20Curriculum%20Learning%0AAuthor%3A%20Mani%20Hamidi%20and%20Sina%20Khajehabdollahi%20and%20Emmanouil%20Giannakakis%20and%20Tim%20Sch%C3%A4fer%20and%20Anna%20Levina%20and%20Charley%20M.%20Wu%0AAbstract%3A%20%20%20Structural%20modularity%20is%20a%20pervasive%20feature%20of%20biological%20neural%20networks%2C%0Awhich%20have%20been%20linked%20to%20several%20functional%20and%20computational%20advantages.%20Yet%2C%0Athe%20use%20of%20modular%20architectures%20in%20artificial%20neural%20networks%20has%20been%0Arelatively%20limited%20despite%20early%20successes.%20Here%2C%20we%20explore%20the%20performance%0Aand%20functional%20dynamics%20of%20a%20modular%20network%20trained%20on%20a%20memory%20task%20via%20an%0Aiterative%20growth%20curriculum.%20We%20find%20that%20for%20a%20given%20classical%2C%20non-modular%0Arecurrent%20neural%20network%20%28RNN%29%2C%20an%20equivalent%20modular%20network%20will%20perform%0Abetter%20across%20multiple%20metrics%2C%20including%20training%20time%2C%20generalizability%2C%20and%0Arobustness%20to%20some%20perturbations.%20We%20further%20examine%20how%20different%20aspects%20of%20a%0Amodular%20network%27s%20connectivity%20contribute%20to%20its%20computational%20capability.%20We%0Athen%20demonstrate%20that%20the%20inductive%20bias%20introduced%20by%20the%20modular%20topology%20is%0Astrong%20enough%20for%20the%20network%20to%20perform%20well%20even%20when%20the%20connectivity%20within%0Amodules%20is%20fixed%20and%20only%20the%20connections%20between%20modules%20are%20trained.%20Our%0Afindings%20suggest%20that%20gradual%20modular%20growth%20of%20RNNs%20could%20provide%20advantages%0Afor%20learning%20increasingly%20complex%20tasks%20on%20evolutionary%20timescales%2C%20and%20help%0Abuild%20more%20scalable%20and%20compressible%20artificial%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Growth%2520of%2520Hierarchical%2520Networks%253A%2520Efficient%252C%2520General%252C%2520and%2520Robust%250A%2520%2520Curriculum%2520Learning%26entry.906535625%3DMani%2520Hamidi%2520and%2520Sina%2520Khajehabdollahi%2520and%2520Emmanouil%2520Giannakakis%2520and%2520Tim%2520Sch%25C3%25A4fer%2520and%2520Anna%2520Levina%2520and%2520Charley%2520M.%2520Wu%26entry.1292438233%3D%2520%2520Structural%2520modularity%2520is%2520a%2520pervasive%2520feature%2520of%2520biological%2520neural%2520networks%252C%250Awhich%2520have%2520been%2520linked%2520to%2520several%2520functional%2520and%2520computational%2520advantages.%2520Yet%252C%250Athe%2520use%2520of%2520modular%2520architectures%2520in%2520artificial%2520neural%2520networks%2520has%2520been%250Arelatively%2520limited%2520despite%2520early%2520successes.%2520Here%252C%2520we%2520explore%2520the%2520performance%250Aand%2520functional%2520dynamics%2520of%2520a%2520modular%2520network%2520trained%2520on%2520a%2520memory%2520task%2520via%2520an%250Aiterative%2520growth%2520curriculum.%2520We%2520find%2520that%2520for%2520a%2520given%2520classical%252C%2520non-modular%250Arecurrent%2520neural%2520network%2520%2528RNN%2529%252C%2520an%2520equivalent%2520modular%2520network%2520will%2520perform%250Abetter%2520across%2520multiple%2520metrics%252C%2520including%2520training%2520time%252C%2520generalizability%252C%2520and%250Arobustness%2520to%2520some%2520perturbations.%2520We%2520further%2520examine%2520how%2520different%2520aspects%2520of%2520a%250Amodular%2520network%2527s%2520connectivity%2520contribute%2520to%2520its%2520computational%2520capability.%2520We%250Athen%2520demonstrate%2520that%2520the%2520inductive%2520bias%2520introduced%2520by%2520the%2520modular%2520topology%2520is%250Astrong%2520enough%2520for%2520the%2520network%2520to%2520perform%2520well%2520even%2520when%2520the%2520connectivity%2520within%250Amodules%2520is%2520fixed%2520and%2520only%2520the%2520connections%2520between%2520modules%2520are%2520trained.%2520Our%250Afindings%2520suggest%2520that%2520gradual%2520modular%2520growth%2520of%2520RNNs%2520could%2520provide%2520advantages%250Afor%2520learning%2520increasingly%2520complex%2520tasks%2520on%2520evolutionary%2520timescales%252C%2520and%2520help%250Abuild%2520more%2520scalable%2520and%2520compressible%2520artificial%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Growth%20of%20Hierarchical%20Networks%3A%20Efficient%2C%20General%2C%20and%20Robust%0A%20%20Curriculum%20Learning&entry.906535625=Mani%20Hamidi%20and%20Sina%20Khajehabdollahi%20and%20Emmanouil%20Giannakakis%20and%20Tim%20Sch%C3%A4fer%20and%20Anna%20Levina%20and%20Charley%20M.%20Wu&entry.1292438233=%20%20Structural%20modularity%20is%20a%20pervasive%20feature%20of%20biological%20neural%20networks%2C%0Awhich%20have%20been%20linked%20to%20several%20functional%20and%20computational%20advantages.%20Yet%2C%0Athe%20use%20of%20modular%20architectures%20in%20artificial%20neural%20networks%20has%20been%0Arelatively%20limited%20despite%20early%20successes.%20Here%2C%20we%20explore%20the%20performance%0Aand%20functional%20dynamics%20of%20a%20modular%20network%20trained%20on%20a%20memory%20task%20via%20an%0Aiterative%20growth%20curriculum.%20We%20find%20that%20for%20a%20given%20classical%2C%20non-modular%0Arecurrent%20neural%20network%20%28RNN%29%2C%20an%20equivalent%20modular%20network%20will%20perform%0Abetter%20across%20multiple%20metrics%2C%20including%20training%20time%2C%20generalizability%2C%20and%0Arobustness%20to%20some%20perturbations.%20We%20further%20examine%20how%20different%20aspects%20of%20a%0Amodular%20network%27s%20connectivity%20contribute%20to%20its%20computational%20capability.%20We%0Athen%20demonstrate%20that%20the%20inductive%20bias%20introduced%20by%20the%20modular%20topology%20is%0Astrong%20enough%20for%20the%20network%20to%20perform%20well%20even%20when%20the%20connectivity%20within%0Amodules%20is%20fixed%20and%20only%20the%20connections%20between%20modules%20are%20trained.%20Our%0Afindings%20suggest%20that%20gradual%20modular%20growth%20of%20RNNs%20could%20provide%20advantages%0Afor%20learning%20increasingly%20complex%20tasks%20on%20evolutionary%20timescales%2C%20and%20help%0Abuild%20more%20scalable%20and%20compressible%20artificial%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06262v1&entry.124074799=Read"},
{"title": "Self-explainable Graph Neural Network for Alzheimer's Disease And\n  Related Dementias Risk Prediction", "author": "Xinyue Hu and Zenan Sun and Yi Nian and Yichen Wang and Yifang Dang and Fang Li and Jingna Feng and Evan Yu and Cui Tao", "abstract": "  Background:\n  Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading\ncause of death in the US, underlining the importance of accurate ADRD risk\nprediction. While recent advancement in ADRD risk prediction have primarily\nrelied on imaging analysis, yet not all patients undergo medical imaging before\nan ADRD diagnosis. Merging machine learning with claims data can reveal\nadditional risk factors and uncover interconnections among diverse medical\ncodes.\n  Objective:\n  Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD\nrisk prediction. Addressing the lack of human-interpretable reasons behind\nthese predictions, we introduce an innovative method to evaluate relationship\nimportance and its influence on ADRD risk prediction, ensuring comprehensive\ninterpretation.\n  Methods:\n  We employed Variationally Regularized Encoder-decoder Graph Neural Network\n(VGNN) for estimating ADRD likelihood. We created three scenarios to assess the\nmodel's efficiency, using Random Forest and Light Gradient Boost Machine as\nbaselines. We further used our relation importance method to clarify the key\nrelationships for ADRD risk prediction.\n  Results:\n  VGNN surpassed other baseline models by 10% in the area under the receiver\noperating characteristic. The integration of the GNN model and relation\nimportance interpretation could potentially play an essential role in providing\nvaluable insight into factors that may contribute to or delay ADRD progression.\n  Conclusions:\n  Employing a GNN approach with claims data enhances ADRD risk prediction and\nprovides insights into the impact of interconnected medical code relationships.\nThis methodology not only enables ADRD risk modeling but also shows potential\nfor other image analysis predictions using claims data.\n", "link": "http://arxiv.org/abs/2309.06584v4", "date": "2024-06-10", "relevancy": 1.9413, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-explainable%20Graph%20Neural%20Network%20for%20Alzheimer%27s%20Disease%20And%0A%20%20Related%20Dementias%20Risk%20Prediction&body=Title%3A%20Self-explainable%20Graph%20Neural%20Network%20for%20Alzheimer%27s%20Disease%20And%0A%20%20Related%20Dementias%20Risk%20Prediction%0AAuthor%3A%20Xinyue%20Hu%20and%20Zenan%20Sun%20and%20Yi%20Nian%20and%20Yichen%20Wang%20and%20Yifang%20Dang%20and%20Fang%20Li%20and%20Jingna%20Feng%20and%20Evan%20Yu%20and%20Cui%20Tao%0AAbstract%3A%20%20%20Background%3A%0A%20%20Alzheimer%27s%20disease%20and%20related%20dementias%20%28ADRD%29%20ranks%20as%20the%20sixth%20leading%0Acause%20of%20death%20in%20the%20US%2C%20underlining%20the%20importance%20of%20accurate%20ADRD%20risk%0Aprediction.%20While%20recent%20advancement%20in%20ADRD%20risk%20prediction%20have%20primarily%0Arelied%20on%20imaging%20analysis%2C%20yet%20not%20all%20patients%20undergo%20medical%20imaging%20before%0Aan%20ADRD%20diagnosis.%20Merging%20machine%20learning%20with%20claims%20data%20can%20reveal%0Aadditional%20risk%20factors%20and%20uncover%20interconnections%20among%20diverse%20medical%0Acodes.%0A%20%20Objective%3A%0A%20%20Our%20goal%20is%20to%20utilize%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20claims%20data%20for%20ADRD%0Arisk%20prediction.%20Addressing%20the%20lack%20of%20human-interpretable%20reasons%20behind%0Athese%20predictions%2C%20we%20introduce%20an%20innovative%20method%20to%20evaluate%20relationship%0Aimportance%20and%20its%20influence%20on%20ADRD%20risk%20prediction%2C%20ensuring%20comprehensive%0Ainterpretation.%0A%20%20Methods%3A%0A%20%20We%20employed%20Variationally%20Regularized%20Encoder-decoder%20Graph%20Neural%20Network%0A%28VGNN%29%20for%20estimating%20ADRD%20likelihood.%20We%20created%20three%20scenarios%20to%20assess%20the%0Amodel%27s%20efficiency%2C%20using%20Random%20Forest%20and%20Light%20Gradient%20Boost%20Machine%20as%0Abaselines.%20We%20further%20used%20our%20relation%20importance%20method%20to%20clarify%20the%20key%0Arelationships%20for%20ADRD%20risk%20prediction.%0A%20%20Results%3A%0A%20%20VGNN%20surpassed%20other%20baseline%20models%20by%2010%25%20in%20the%20area%20under%20the%20receiver%0Aoperating%20characteristic.%20The%20integration%20of%20the%20GNN%20model%20and%20relation%0Aimportance%20interpretation%20could%20potentially%20play%20an%20essential%20role%20in%20providing%0Avaluable%20insight%20into%20factors%20that%20may%20contribute%20to%20or%20delay%20ADRD%20progression.%0A%20%20Conclusions%3A%0A%20%20Employing%20a%20GNN%20approach%20with%20claims%20data%20enhances%20ADRD%20risk%20prediction%20and%0Aprovides%20insights%20into%20the%20impact%20of%20interconnected%20medical%20code%20relationships.%0AThis%20methodology%20not%20only%20enables%20ADRD%20risk%20modeling%20but%20also%20shows%20potential%0Afor%20other%20image%20analysis%20predictions%20using%20claims%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06584v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-explainable%2520Graph%2520Neural%2520Network%2520for%2520Alzheimer%2527s%2520Disease%2520And%250A%2520%2520Related%2520Dementias%2520Risk%2520Prediction%26entry.906535625%3DXinyue%2520Hu%2520and%2520Zenan%2520Sun%2520and%2520Yi%2520Nian%2520and%2520Yichen%2520Wang%2520and%2520Yifang%2520Dang%2520and%2520Fang%2520Li%2520and%2520Jingna%2520Feng%2520and%2520Evan%2520Yu%2520and%2520Cui%2520Tao%26entry.1292438233%3D%2520%2520Background%253A%250A%2520%2520Alzheimer%2527s%2520disease%2520and%2520related%2520dementias%2520%2528ADRD%2529%2520ranks%2520as%2520the%2520sixth%2520leading%250Acause%2520of%2520death%2520in%2520the%2520US%252C%2520underlining%2520the%2520importance%2520of%2520accurate%2520ADRD%2520risk%250Aprediction.%2520While%2520recent%2520advancement%2520in%2520ADRD%2520risk%2520prediction%2520have%2520primarily%250Arelied%2520on%2520imaging%2520analysis%252C%2520yet%2520not%2520all%2520patients%2520undergo%2520medical%2520imaging%2520before%250Aan%2520ADRD%2520diagnosis.%2520Merging%2520machine%2520learning%2520with%2520claims%2520data%2520can%2520reveal%250Aadditional%2520risk%2520factors%2520and%2520uncover%2520interconnections%2520among%2520diverse%2520medical%250Acodes.%250A%2520%2520Objective%253A%250A%2520%2520Our%2520goal%2520is%2520to%2520utilize%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520with%2520claims%2520data%2520for%2520ADRD%250Arisk%2520prediction.%2520Addressing%2520the%2520lack%2520of%2520human-interpretable%2520reasons%2520behind%250Athese%2520predictions%252C%2520we%2520introduce%2520an%2520innovative%2520method%2520to%2520evaluate%2520relationship%250Aimportance%2520and%2520its%2520influence%2520on%2520ADRD%2520risk%2520prediction%252C%2520ensuring%2520comprehensive%250Ainterpretation.%250A%2520%2520Methods%253A%250A%2520%2520We%2520employed%2520Variationally%2520Regularized%2520Encoder-decoder%2520Graph%2520Neural%2520Network%250A%2528VGNN%2529%2520for%2520estimating%2520ADRD%2520likelihood.%2520We%2520created%2520three%2520scenarios%2520to%2520assess%2520the%250Amodel%2527s%2520efficiency%252C%2520using%2520Random%2520Forest%2520and%2520Light%2520Gradient%2520Boost%2520Machine%2520as%250Abaselines.%2520We%2520further%2520used%2520our%2520relation%2520importance%2520method%2520to%2520clarify%2520the%2520key%250Arelationships%2520for%2520ADRD%2520risk%2520prediction.%250A%2520%2520Results%253A%250A%2520%2520VGNN%2520surpassed%2520other%2520baseline%2520models%2520by%252010%2525%2520in%2520the%2520area%2520under%2520the%2520receiver%250Aoperating%2520characteristic.%2520The%2520integration%2520of%2520the%2520GNN%2520model%2520and%2520relation%250Aimportance%2520interpretation%2520could%2520potentially%2520play%2520an%2520essential%2520role%2520in%2520providing%250Avaluable%2520insight%2520into%2520factors%2520that%2520may%2520contribute%2520to%2520or%2520delay%2520ADRD%2520progression.%250A%2520%2520Conclusions%253A%250A%2520%2520Employing%2520a%2520GNN%2520approach%2520with%2520claims%2520data%2520enhances%2520ADRD%2520risk%2520prediction%2520and%250Aprovides%2520insights%2520into%2520the%2520impact%2520of%2520interconnected%2520medical%2520code%2520relationships.%250AThis%2520methodology%2520not%2520only%2520enables%2520ADRD%2520risk%2520modeling%2520but%2520also%2520shows%2520potential%250Afor%2520other%2520image%2520analysis%2520predictions%2520using%2520claims%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06584v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-explainable%20Graph%20Neural%20Network%20for%20Alzheimer%27s%20Disease%20And%0A%20%20Related%20Dementias%20Risk%20Prediction&entry.906535625=Xinyue%20Hu%20and%20Zenan%20Sun%20and%20Yi%20Nian%20and%20Yichen%20Wang%20and%20Yifang%20Dang%20and%20Fang%20Li%20and%20Jingna%20Feng%20and%20Evan%20Yu%20and%20Cui%20Tao&entry.1292438233=%20%20Background%3A%0A%20%20Alzheimer%27s%20disease%20and%20related%20dementias%20%28ADRD%29%20ranks%20as%20the%20sixth%20leading%0Acause%20of%20death%20in%20the%20US%2C%20underlining%20the%20importance%20of%20accurate%20ADRD%20risk%0Aprediction.%20While%20recent%20advancement%20in%20ADRD%20risk%20prediction%20have%20primarily%0Arelied%20on%20imaging%20analysis%2C%20yet%20not%20all%20patients%20undergo%20medical%20imaging%20before%0Aan%20ADRD%20diagnosis.%20Merging%20machine%20learning%20with%20claims%20data%20can%20reveal%0Aadditional%20risk%20factors%20and%20uncover%20interconnections%20among%20diverse%20medical%0Acodes.%0A%20%20Objective%3A%0A%20%20Our%20goal%20is%20to%20utilize%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20claims%20data%20for%20ADRD%0Arisk%20prediction.%20Addressing%20the%20lack%20of%20human-interpretable%20reasons%20behind%0Athese%20predictions%2C%20we%20introduce%20an%20innovative%20method%20to%20evaluate%20relationship%0Aimportance%20and%20its%20influence%20on%20ADRD%20risk%20prediction%2C%20ensuring%20comprehensive%0Ainterpretation.%0A%20%20Methods%3A%0A%20%20We%20employed%20Variationally%20Regularized%20Encoder-decoder%20Graph%20Neural%20Network%0A%28VGNN%29%20for%20estimating%20ADRD%20likelihood.%20We%20created%20three%20scenarios%20to%20assess%20the%0Amodel%27s%20efficiency%2C%20using%20Random%20Forest%20and%20Light%20Gradient%20Boost%20Machine%20as%0Abaselines.%20We%20further%20used%20our%20relation%20importance%20method%20to%20clarify%20the%20key%0Arelationships%20for%20ADRD%20risk%20prediction.%0A%20%20Results%3A%0A%20%20VGNN%20surpassed%20other%20baseline%20models%20by%2010%25%20in%20the%20area%20under%20the%20receiver%0Aoperating%20characteristic.%20The%20integration%20of%20the%20GNN%20model%20and%20relation%0Aimportance%20interpretation%20could%20potentially%20play%20an%20essential%20role%20in%20providing%0Avaluable%20insight%20into%20factors%20that%20may%20contribute%20to%20or%20delay%20ADRD%20progression.%0A%20%20Conclusions%3A%0A%20%20Employing%20a%20GNN%20approach%20with%20claims%20data%20enhances%20ADRD%20risk%20prediction%20and%0Aprovides%20insights%20into%20the%20impact%20of%20interconnected%20medical%20code%20relationships.%0AThis%20methodology%20not%20only%20enables%20ADRD%20risk%20modeling%20but%20also%20shows%20potential%0Afor%20other%20image%20analysis%20predictions%20using%20claims%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06584v4&entry.124074799=Read"},
{"title": "Quantized Approximately Orthogonal Recurrent Neural Networks", "author": "Armand Foucault and Franck Mamalet and Fran\u00e7ois Malgouyres", "abstract": "  In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained\npopularity due to their ability to manage tasks involving long-term\ndependencies, such as the copy-task, and their linear complexity. However,\nexisting ORNNs utilize full precision weights and activations, which prevents\ntheir deployment on compact devices.In this paper, we explore the quantization\nof the weight matrices in ORNNs, leading to Quantized approximately Orthogonal\nRNNs (QORNNs). The construction of such networks remained an open problem,\nacknowledged for its inherent instability. We propose and investigate two\nstrategies to learn QORNN by combining quantization-aware training (QAT) and\northogonal projections. We also study post-training quantization of the\nactivations for pure integer computation of the recurrent loop. The most\nefficient models achieve results similar to state-of-the-art full-precision\nORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 4-bits\nquantization.\n", "link": "http://arxiv.org/abs/2402.04012v2", "date": "2024-06-10", "relevancy": 1.9334, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5162}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantized%20Approximately%20Orthogonal%20Recurrent%20Neural%20Networks&body=Title%3A%20Quantized%20Approximately%20Orthogonal%20Recurrent%20Neural%20Networks%0AAuthor%3A%20Armand%20Foucault%20and%20Franck%20Mamalet%20and%20Fran%C3%A7ois%20Malgouyres%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Orthogonal%20Recurrent%20Neural%20Networks%20%28ORNNs%29%20have%20gained%0Apopularity%20due%20to%20their%20ability%20to%20manage%20tasks%20involving%20long-term%0Adependencies%2C%20such%20as%20the%20copy-task%2C%20and%20their%20linear%20complexity.%20However%2C%0Aexisting%20ORNNs%20utilize%20full%20precision%20weights%20and%20activations%2C%20which%20prevents%0Atheir%20deployment%20on%20compact%20devices.In%20this%20paper%2C%20we%20explore%20the%20quantization%0Aof%20the%20weight%20matrices%20in%20ORNNs%2C%20leading%20to%20Quantized%20approximately%20Orthogonal%0ARNNs%20%28QORNNs%29.%20The%20construction%20of%20such%20networks%20remained%20an%20open%20problem%2C%0Aacknowledged%20for%20its%20inherent%20instability.%20We%20propose%20and%20investigate%20two%0Astrategies%20to%20learn%20QORNN%20by%20combining%20quantization-aware%20training%20%28QAT%29%20and%0Aorthogonal%20projections.%20We%20also%20study%20post-training%20quantization%20of%20the%0Aactivations%20for%20pure%20integer%20computation%20of%20the%20recurrent%20loop.%20The%20most%0Aefficient%20models%20achieve%20results%20similar%20to%20state-of-the-art%20full-precision%0AORNN%2C%20LSTM%20and%20FastRNN%20on%20a%20variety%20of%20standard%20benchmarks%2C%20even%20with%204-bits%0Aquantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantized%2520Approximately%2520Orthogonal%2520Recurrent%2520Neural%2520Networks%26entry.906535625%3DArmand%2520Foucault%2520and%2520Franck%2520Mamalet%2520and%2520Fran%25C3%25A7ois%2520Malgouyres%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Orthogonal%2520Recurrent%2520Neural%2520Networks%2520%2528ORNNs%2529%2520have%2520gained%250Apopularity%2520due%2520to%2520their%2520ability%2520to%2520manage%2520tasks%2520involving%2520long-term%250Adependencies%252C%2520such%2520as%2520the%2520copy-task%252C%2520and%2520their%2520linear%2520complexity.%2520However%252C%250Aexisting%2520ORNNs%2520utilize%2520full%2520precision%2520weights%2520and%2520activations%252C%2520which%2520prevents%250Atheir%2520deployment%2520on%2520compact%2520devices.In%2520this%2520paper%252C%2520we%2520explore%2520the%2520quantization%250Aof%2520the%2520weight%2520matrices%2520in%2520ORNNs%252C%2520leading%2520to%2520Quantized%2520approximately%2520Orthogonal%250ARNNs%2520%2528QORNNs%2529.%2520The%2520construction%2520of%2520such%2520networks%2520remained%2520an%2520open%2520problem%252C%250Aacknowledged%2520for%2520its%2520inherent%2520instability.%2520We%2520propose%2520and%2520investigate%2520two%250Astrategies%2520to%2520learn%2520QORNN%2520by%2520combining%2520quantization-aware%2520training%2520%2528QAT%2529%2520and%250Aorthogonal%2520projections.%2520We%2520also%2520study%2520post-training%2520quantization%2520of%2520the%250Aactivations%2520for%2520pure%2520integer%2520computation%2520of%2520the%2520recurrent%2520loop.%2520The%2520most%250Aefficient%2520models%2520achieve%2520results%2520similar%2520to%2520state-of-the-art%2520full-precision%250AORNN%252C%2520LSTM%2520and%2520FastRNN%2520on%2520a%2520variety%2520of%2520standard%2520benchmarks%252C%2520even%2520with%25204-bits%250Aquantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantized%20Approximately%20Orthogonal%20Recurrent%20Neural%20Networks&entry.906535625=Armand%20Foucault%20and%20Franck%20Mamalet%20and%20Fran%C3%A7ois%20Malgouyres&entry.1292438233=%20%20In%20recent%20years%2C%20Orthogonal%20Recurrent%20Neural%20Networks%20%28ORNNs%29%20have%20gained%0Apopularity%20due%20to%20their%20ability%20to%20manage%20tasks%20involving%20long-term%0Adependencies%2C%20such%20as%20the%20copy-task%2C%20and%20their%20linear%20complexity.%20However%2C%0Aexisting%20ORNNs%20utilize%20full%20precision%20weights%20and%20activations%2C%20which%20prevents%0Atheir%20deployment%20on%20compact%20devices.In%20this%20paper%2C%20we%20explore%20the%20quantization%0Aof%20the%20weight%20matrices%20in%20ORNNs%2C%20leading%20to%20Quantized%20approximately%20Orthogonal%0ARNNs%20%28QORNNs%29.%20The%20construction%20of%20such%20networks%20remained%20an%20open%20problem%2C%0Aacknowledged%20for%20its%20inherent%20instability.%20We%20propose%20and%20investigate%20two%0Astrategies%20to%20learn%20QORNN%20by%20combining%20quantization-aware%20training%20%28QAT%29%20and%0Aorthogonal%20projections.%20We%20also%20study%20post-training%20quantization%20of%20the%0Aactivations%20for%20pure%20integer%20computation%20of%20the%20recurrent%20loop.%20The%20most%0Aefficient%20models%20achieve%20results%20similar%20to%20state-of-the-art%20full-precision%0AORNN%2C%20LSTM%20and%20FastRNN%20on%20a%20variety%20of%20standard%20benchmarks%2C%20even%20with%204-bits%0Aquantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04012v2&entry.124074799=Read"},
{"title": "PAC-Bayes Analysis for Recalibration in Classification", "author": "Masahiro Fujisawa and Futoshi Futami", "abstract": "  Nonparametric estimation with binning is widely employed in the calibration\nerror evaluation and the recalibration of machine learning models. Recently,\ntheoretical analyses of the bias induced by this estimation approach have been\nactively pursued; however, the understanding of the generalization of the\ncalibration error to unknown data remains limited. In addition, although many\nrecalibration algorithms have been proposed, their generalization performance\nlacks theoretical guarantees. To address this problem, we conduct a\ngeneralization analysis of the calibration error under the probably\napproximately correct (PAC) Bayes framework. This approach enables us to derive\na first optimizable upper bound for the generalization error in the calibration\ncontext. We then propose a generalization-aware recalibration algorithm based\non our generalization theory. Numerical experiments show that our algorithm\nimproves the Gaussian-process-based recalibration performance on various\nbenchmark datasets and models.\n", "link": "http://arxiv.org/abs/2406.06227v1", "date": "2024-06-10", "relevancy": 1.9257, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC-Bayes%20Analysis%20for%20Recalibration%20in%20Classification&body=Title%3A%20PAC-Bayes%20Analysis%20for%20Recalibration%20in%20Classification%0AAuthor%3A%20Masahiro%20Fujisawa%20and%20Futoshi%20Futami%0AAbstract%3A%20%20%20Nonparametric%20estimation%20with%20binning%20is%20widely%20employed%20in%20the%20calibration%0Aerror%20evaluation%20and%20the%20recalibration%20of%20machine%20learning%20models.%20Recently%2C%0Atheoretical%20analyses%20of%20the%20bias%20induced%20by%20this%20estimation%20approach%20have%20been%0Aactively%20pursued%3B%20however%2C%20the%20understanding%20of%20the%20generalization%20of%20the%0Acalibration%20error%20to%20unknown%20data%20remains%20limited.%20In%20addition%2C%20although%20many%0Arecalibration%20algorithms%20have%20been%20proposed%2C%20their%20generalization%20performance%0Alacks%20theoretical%20guarantees.%20To%20address%20this%20problem%2C%20we%20conduct%20a%0Ageneralization%20analysis%20of%20the%20calibration%20error%20under%20the%20probably%0Aapproximately%20correct%20%28PAC%29%20Bayes%20framework.%20This%20approach%20enables%20us%20to%20derive%0Aa%20first%20optimizable%20upper%20bound%20for%20the%20generalization%20error%20in%20the%20calibration%0Acontext.%20We%20then%20propose%20a%20generalization-aware%20recalibration%20algorithm%20based%0Aon%20our%20generalization%20theory.%20Numerical%20experiments%20show%20that%20our%20algorithm%0Aimproves%20the%20Gaussian-process-based%20recalibration%20performance%20on%20various%0Abenchmark%20datasets%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC-Bayes%2520Analysis%2520for%2520Recalibration%2520in%2520Classification%26entry.906535625%3DMasahiro%2520Fujisawa%2520and%2520Futoshi%2520Futami%26entry.1292438233%3D%2520%2520Nonparametric%2520estimation%2520with%2520binning%2520is%2520widely%2520employed%2520in%2520the%2520calibration%250Aerror%2520evaluation%2520and%2520the%2520recalibration%2520of%2520machine%2520learning%2520models.%2520Recently%252C%250Atheoretical%2520analyses%2520of%2520the%2520bias%2520induced%2520by%2520this%2520estimation%2520approach%2520have%2520been%250Aactively%2520pursued%253B%2520however%252C%2520the%2520understanding%2520of%2520the%2520generalization%2520of%2520the%250Acalibration%2520error%2520to%2520unknown%2520data%2520remains%2520limited.%2520In%2520addition%252C%2520although%2520many%250Arecalibration%2520algorithms%2520have%2520been%2520proposed%252C%2520their%2520generalization%2520performance%250Alacks%2520theoretical%2520guarantees.%2520To%2520address%2520this%2520problem%252C%2520we%2520conduct%2520a%250Ageneralization%2520analysis%2520of%2520the%2520calibration%2520error%2520under%2520the%2520probably%250Aapproximately%2520correct%2520%2528PAC%2529%2520Bayes%2520framework.%2520This%2520approach%2520enables%2520us%2520to%2520derive%250Aa%2520first%2520optimizable%2520upper%2520bound%2520for%2520the%2520generalization%2520error%2520in%2520the%2520calibration%250Acontext.%2520We%2520then%2520propose%2520a%2520generalization-aware%2520recalibration%2520algorithm%2520based%250Aon%2520our%2520generalization%2520theory.%2520Numerical%2520experiments%2520show%2520that%2520our%2520algorithm%250Aimproves%2520the%2520Gaussian-process-based%2520recalibration%2520performance%2520on%2520various%250Abenchmark%2520datasets%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-Bayes%20Analysis%20for%20Recalibration%20in%20Classification&entry.906535625=Masahiro%20Fujisawa%20and%20Futoshi%20Futami&entry.1292438233=%20%20Nonparametric%20estimation%20with%20binning%20is%20widely%20employed%20in%20the%20calibration%0Aerror%20evaluation%20and%20the%20recalibration%20of%20machine%20learning%20models.%20Recently%2C%0Atheoretical%20analyses%20of%20the%20bias%20induced%20by%20this%20estimation%20approach%20have%20been%0Aactively%20pursued%3B%20however%2C%20the%20understanding%20of%20the%20generalization%20of%20the%0Acalibration%20error%20to%20unknown%20data%20remains%20limited.%20In%20addition%2C%20although%20many%0Arecalibration%20algorithms%20have%20been%20proposed%2C%20their%20generalization%20performance%0Alacks%20theoretical%20guarantees.%20To%20address%20this%20problem%2C%20we%20conduct%20a%0Ageneralization%20analysis%20of%20the%20calibration%20error%20under%20the%20probably%0Aapproximately%20correct%20%28PAC%29%20Bayes%20framework.%20This%20approach%20enables%20us%20to%20derive%0Aa%20first%20optimizable%20upper%20bound%20for%20the%20generalization%20error%20in%20the%20calibration%0Acontext.%20We%20then%20propose%20a%20generalization-aware%20recalibration%20algorithm%20based%0Aon%20our%20generalization%20theory.%20Numerical%20experiments%20show%20that%20our%20algorithm%0Aimproves%20the%20Gaussian-process-based%20recalibration%20performance%20on%20various%0Abenchmark%20datasets%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06227v1&entry.124074799=Read"},
{"title": "Sparsity regularization via tree-structured environments for\n  disentangled representations", "author": "Elliot Layne and Jason Hartford and S\u00e9bastien Lachapelle and Mathieu Blanchette and Dhanya Sridhar", "abstract": "  Many causal systems such as biological processes in cells can only be\nobserved indirectly via measurements, such as gene expression. Causal\nrepresentation learning -- the task of correctly mapping low-level observations\nto latent causal variables -- could advance scientific understanding by\nenabling inference of latent variables such as pathway activation. In this\npaper, we develop methods for inferring latent variables from multiple related\ndatasets (environments) and tasks. As a running example, we consider the task\nof predicting a phenotype from gene expression, where we often collect data\nfrom multiple cell types or organisms that are related in known ways. The key\ninsight is that the mapping from latent variables driven by gene expression to\nthe phenotype of interest changes sparsely across closely related environments.\nTo model sparse changes, we introduce Tree-Based Regularization (TBR), an\nobjective that minimizes both prediction error and regularizes closely related\nenvironments to learn similar predictors. We prove that under assumptions about\nthe degree of sparse changes, TBR identifies the true latent variables up to\nsome simple transformations. We evaluate the theory empirically with both\nsimulations and ground-truth gene expression data. We find that TBR recovers\nthe latent causal variables better than related methods across these settings,\neven under settings that violate some assumptions of the theory.\n", "link": "http://arxiv.org/abs/2405.20482v2", "date": "2024-06-10", "relevancy": 1.9228, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4828}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsity%20regularization%20via%20tree-structured%20environments%20for%0A%20%20disentangled%20representations&body=Title%3A%20Sparsity%20regularization%20via%20tree-structured%20environments%20for%0A%20%20disentangled%20representations%0AAuthor%3A%20Elliot%20Layne%20and%20Jason%20Hartford%20and%20S%C3%A9bastien%20Lachapelle%20and%20Mathieu%20Blanchette%20and%20Dhanya%20Sridhar%0AAbstract%3A%20%20%20Many%20causal%20systems%20such%20as%20biological%20processes%20in%20cells%20can%20only%20be%0Aobserved%20indirectly%20via%20measurements%2C%20such%20as%20gene%20expression.%20Causal%0Arepresentation%20learning%20--%20the%20task%20of%20correctly%20mapping%20low-level%20observations%0Ato%20latent%20causal%20variables%20--%20could%20advance%20scientific%20understanding%20by%0Aenabling%20inference%20of%20latent%20variables%20such%20as%20pathway%20activation.%20In%20this%0Apaper%2C%20we%20develop%20methods%20for%20inferring%20latent%20variables%20from%20multiple%20related%0Adatasets%20%28environments%29%20and%20tasks.%20As%20a%20running%20example%2C%20we%20consider%20the%20task%0Aof%20predicting%20a%20phenotype%20from%20gene%20expression%2C%20where%20we%20often%20collect%20data%0Afrom%20multiple%20cell%20types%20or%20organisms%20that%20are%20related%20in%20known%20ways.%20The%20key%0Ainsight%20is%20that%20the%20mapping%20from%20latent%20variables%20driven%20by%20gene%20expression%20to%0Athe%20phenotype%20of%20interest%20changes%20sparsely%20across%20closely%20related%20environments.%0ATo%20model%20sparse%20changes%2C%20we%20introduce%20Tree-Based%20Regularization%20%28TBR%29%2C%20an%0Aobjective%20that%20minimizes%20both%20prediction%20error%20and%20regularizes%20closely%20related%0Aenvironments%20to%20learn%20similar%20predictors.%20We%20prove%20that%20under%20assumptions%20about%0Athe%20degree%20of%20sparse%20changes%2C%20TBR%20identifies%20the%20true%20latent%20variables%20up%20to%0Asome%20simple%20transformations.%20We%20evaluate%20the%20theory%20empirically%20with%20both%0Asimulations%20and%20ground-truth%20gene%20expression%20data.%20We%20find%20that%20TBR%20recovers%0Athe%20latent%20causal%20variables%20better%20than%20related%20methods%20across%20these%20settings%2C%0Aeven%20under%20settings%20that%20violate%20some%20assumptions%20of%20the%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsity%2520regularization%2520via%2520tree-structured%2520environments%2520for%250A%2520%2520disentangled%2520representations%26entry.906535625%3DElliot%2520Layne%2520and%2520Jason%2520Hartford%2520and%2520S%25C3%25A9bastien%2520Lachapelle%2520and%2520Mathieu%2520Blanchette%2520and%2520Dhanya%2520Sridhar%26entry.1292438233%3D%2520%2520Many%2520causal%2520systems%2520such%2520as%2520biological%2520processes%2520in%2520cells%2520can%2520only%2520be%250Aobserved%2520indirectly%2520via%2520measurements%252C%2520such%2520as%2520gene%2520expression.%2520Causal%250Arepresentation%2520learning%2520--%2520the%2520task%2520of%2520correctly%2520mapping%2520low-level%2520observations%250Ato%2520latent%2520causal%2520variables%2520--%2520could%2520advance%2520scientific%2520understanding%2520by%250Aenabling%2520inference%2520of%2520latent%2520variables%2520such%2520as%2520pathway%2520activation.%2520In%2520this%250Apaper%252C%2520we%2520develop%2520methods%2520for%2520inferring%2520latent%2520variables%2520from%2520multiple%2520related%250Adatasets%2520%2528environments%2529%2520and%2520tasks.%2520As%2520a%2520running%2520example%252C%2520we%2520consider%2520the%2520task%250Aof%2520predicting%2520a%2520phenotype%2520from%2520gene%2520expression%252C%2520where%2520we%2520often%2520collect%2520data%250Afrom%2520multiple%2520cell%2520types%2520or%2520organisms%2520that%2520are%2520related%2520in%2520known%2520ways.%2520The%2520key%250Ainsight%2520is%2520that%2520the%2520mapping%2520from%2520latent%2520variables%2520driven%2520by%2520gene%2520expression%2520to%250Athe%2520phenotype%2520of%2520interest%2520changes%2520sparsely%2520across%2520closely%2520related%2520environments.%250ATo%2520model%2520sparse%2520changes%252C%2520we%2520introduce%2520Tree-Based%2520Regularization%2520%2528TBR%2529%252C%2520an%250Aobjective%2520that%2520minimizes%2520both%2520prediction%2520error%2520and%2520regularizes%2520closely%2520related%250Aenvironments%2520to%2520learn%2520similar%2520predictors.%2520We%2520prove%2520that%2520under%2520assumptions%2520about%250Athe%2520degree%2520of%2520sparse%2520changes%252C%2520TBR%2520identifies%2520the%2520true%2520latent%2520variables%2520up%2520to%250Asome%2520simple%2520transformations.%2520We%2520evaluate%2520the%2520theory%2520empirically%2520with%2520both%250Asimulations%2520and%2520ground-truth%2520gene%2520expression%2520data.%2520We%2520find%2520that%2520TBR%2520recovers%250Athe%2520latent%2520causal%2520variables%2520better%2520than%2520related%2520methods%2520across%2520these%2520settings%252C%250Aeven%2520under%2520settings%2520that%2520violate%2520some%2520assumptions%2520of%2520the%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsity%20regularization%20via%20tree-structured%20environments%20for%0A%20%20disentangled%20representations&entry.906535625=Elliot%20Layne%20and%20Jason%20Hartford%20and%20S%C3%A9bastien%20Lachapelle%20and%20Mathieu%20Blanchette%20and%20Dhanya%20Sridhar&entry.1292438233=%20%20Many%20causal%20systems%20such%20as%20biological%20processes%20in%20cells%20can%20only%20be%0Aobserved%20indirectly%20via%20measurements%2C%20such%20as%20gene%20expression.%20Causal%0Arepresentation%20learning%20--%20the%20task%20of%20correctly%20mapping%20low-level%20observations%0Ato%20latent%20causal%20variables%20--%20could%20advance%20scientific%20understanding%20by%0Aenabling%20inference%20of%20latent%20variables%20such%20as%20pathway%20activation.%20In%20this%0Apaper%2C%20we%20develop%20methods%20for%20inferring%20latent%20variables%20from%20multiple%20related%0Adatasets%20%28environments%29%20and%20tasks.%20As%20a%20running%20example%2C%20we%20consider%20the%20task%0Aof%20predicting%20a%20phenotype%20from%20gene%20expression%2C%20where%20we%20often%20collect%20data%0Afrom%20multiple%20cell%20types%20or%20organisms%20that%20are%20related%20in%20known%20ways.%20The%20key%0Ainsight%20is%20that%20the%20mapping%20from%20latent%20variables%20driven%20by%20gene%20expression%20to%0Athe%20phenotype%20of%20interest%20changes%20sparsely%20across%20closely%20related%20environments.%0ATo%20model%20sparse%20changes%2C%20we%20introduce%20Tree-Based%20Regularization%20%28TBR%29%2C%20an%0Aobjective%20that%20minimizes%20both%20prediction%20error%20and%20regularizes%20closely%20related%0Aenvironments%20to%20learn%20similar%20predictors.%20We%20prove%20that%20under%20assumptions%20about%0Athe%20degree%20of%20sparse%20changes%2C%20TBR%20identifies%20the%20true%20latent%20variables%20up%20to%0Asome%20simple%20transformations.%20We%20evaluate%20the%20theory%20empirically%20with%20both%0Asimulations%20and%20ground-truth%20gene%20expression%20data.%20We%20find%20that%20TBR%20recovers%0Athe%20latent%20causal%20variables%20better%20than%20related%20methods%20across%20these%20settings%2C%0Aeven%20under%20settings%20that%20violate%20some%20assumptions%20of%20the%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20482v2&entry.124074799=Read"},
{"title": "Survey for Landing Generative AI in Social and E-commerce Recsys -- the\n  Industry Perspectives", "author": "Da Xu and Danqing Zhang and Guangyu Yang and Bo Yang and Shuyuan Xu and Lingling Zheng and Cindy Liang", "abstract": "  Recently, generative AI (GAI), with their emerging capabilities, have\npresented unique opportunities for augmenting and revolutionizing industrial\nrecommender systems (Recsys). Despite growing research efforts at the\nintersection of these fields, the integration of GAI into industrial Recsys\nremains in its infancy, largely due to the intricate nature of modern\nindustrial Recsys infrastructure, operations, and product sophistication.\nDrawing upon our experiences in successfully integrating GAI into several major\nsocial and e-commerce platforms, this survey aims to comprehensively examine\nthe underlying system and AI foundations, solution frameworks, connections to\nkey research advancements, as well as summarize the practical insights and\nchallenges encountered in the endeavor to integrate GAI into industrial Recsys.\nAs pioneering work in this domain, we hope outline the representative\ndevelopments of relevant fields, shed lights on practical GAI adoptions in the\nindustry, and motivate future research.\n", "link": "http://arxiv.org/abs/2406.06475v1", "date": "2024-06-10", "relevancy": 1.9224, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4912}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4755}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20for%20Landing%20Generative%20AI%20in%20Social%20and%20E-commerce%20Recsys%20--%20the%0A%20%20Industry%20Perspectives&body=Title%3A%20Survey%20for%20Landing%20Generative%20AI%20in%20Social%20and%20E-commerce%20Recsys%20--%20the%0A%20%20Industry%20Perspectives%0AAuthor%3A%20Da%20Xu%20and%20Danqing%20Zhang%20and%20Guangyu%20Yang%20and%20Bo%20Yang%20and%20Shuyuan%20Xu%20and%20Lingling%20Zheng%20and%20Cindy%20Liang%0AAbstract%3A%20%20%20Recently%2C%20generative%20AI%20%28GAI%29%2C%20with%20their%20emerging%20capabilities%2C%20have%0Apresented%20unique%20opportunities%20for%20augmenting%20and%20revolutionizing%20industrial%0Arecommender%20systems%20%28Recsys%29.%20Despite%20growing%20research%20efforts%20at%20the%0Aintersection%20of%20these%20fields%2C%20the%20integration%20of%20GAI%20into%20industrial%20Recsys%0Aremains%20in%20its%20infancy%2C%20largely%20due%20to%20the%20intricate%20nature%20of%20modern%0Aindustrial%20Recsys%20infrastructure%2C%20operations%2C%20and%20product%20sophistication.%0ADrawing%20upon%20our%20experiences%20in%20successfully%20integrating%20GAI%20into%20several%20major%0Asocial%20and%20e-commerce%20platforms%2C%20this%20survey%20aims%20to%20comprehensively%20examine%0Athe%20underlying%20system%20and%20AI%20foundations%2C%20solution%20frameworks%2C%20connections%20to%0Akey%20research%20advancements%2C%20as%20well%20as%20summarize%20the%20practical%20insights%20and%0Achallenges%20encountered%20in%20the%20endeavor%20to%20integrate%20GAI%20into%20industrial%20Recsys.%0AAs%20pioneering%20work%20in%20this%20domain%2C%20we%20hope%20outline%20the%20representative%0Adevelopments%20of%20relevant%20fields%2C%20shed%20lights%20on%20practical%20GAI%20adoptions%20in%20the%0Aindustry%2C%20and%20motivate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520for%2520Landing%2520Generative%2520AI%2520in%2520Social%2520and%2520E-commerce%2520Recsys%2520--%2520the%250A%2520%2520Industry%2520Perspectives%26entry.906535625%3DDa%2520Xu%2520and%2520Danqing%2520Zhang%2520and%2520Guangyu%2520Yang%2520and%2520Bo%2520Yang%2520and%2520Shuyuan%2520Xu%2520and%2520Lingling%2520Zheng%2520and%2520Cindy%2520Liang%26entry.1292438233%3D%2520%2520Recently%252C%2520generative%2520AI%2520%2528GAI%2529%252C%2520with%2520their%2520emerging%2520capabilities%252C%2520have%250Apresented%2520unique%2520opportunities%2520for%2520augmenting%2520and%2520revolutionizing%2520industrial%250Arecommender%2520systems%2520%2528Recsys%2529.%2520Despite%2520growing%2520research%2520efforts%2520at%2520the%250Aintersection%2520of%2520these%2520fields%252C%2520the%2520integration%2520of%2520GAI%2520into%2520industrial%2520Recsys%250Aremains%2520in%2520its%2520infancy%252C%2520largely%2520due%2520to%2520the%2520intricate%2520nature%2520of%2520modern%250Aindustrial%2520Recsys%2520infrastructure%252C%2520operations%252C%2520and%2520product%2520sophistication.%250ADrawing%2520upon%2520our%2520experiences%2520in%2520successfully%2520integrating%2520GAI%2520into%2520several%2520major%250Asocial%2520and%2520e-commerce%2520platforms%252C%2520this%2520survey%2520aims%2520to%2520comprehensively%2520examine%250Athe%2520underlying%2520system%2520and%2520AI%2520foundations%252C%2520solution%2520frameworks%252C%2520connections%2520to%250Akey%2520research%2520advancements%252C%2520as%2520well%2520as%2520summarize%2520the%2520practical%2520insights%2520and%250Achallenges%2520encountered%2520in%2520the%2520endeavor%2520to%2520integrate%2520GAI%2520into%2520industrial%2520Recsys.%250AAs%2520pioneering%2520work%2520in%2520this%2520domain%252C%2520we%2520hope%2520outline%2520the%2520representative%250Adevelopments%2520of%2520relevant%2520fields%252C%2520shed%2520lights%2520on%2520practical%2520GAI%2520adoptions%2520in%2520the%250Aindustry%252C%2520and%2520motivate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20for%20Landing%20Generative%20AI%20in%20Social%20and%20E-commerce%20Recsys%20--%20the%0A%20%20Industry%20Perspectives&entry.906535625=Da%20Xu%20and%20Danqing%20Zhang%20and%20Guangyu%20Yang%20and%20Bo%20Yang%20and%20Shuyuan%20Xu%20and%20Lingling%20Zheng%20and%20Cindy%20Liang&entry.1292438233=%20%20Recently%2C%20generative%20AI%20%28GAI%29%2C%20with%20their%20emerging%20capabilities%2C%20have%0Apresented%20unique%20opportunities%20for%20augmenting%20and%20revolutionizing%20industrial%0Arecommender%20systems%20%28Recsys%29.%20Despite%20growing%20research%20efforts%20at%20the%0Aintersection%20of%20these%20fields%2C%20the%20integration%20of%20GAI%20into%20industrial%20Recsys%0Aremains%20in%20its%20infancy%2C%20largely%20due%20to%20the%20intricate%20nature%20of%20modern%0Aindustrial%20Recsys%20infrastructure%2C%20operations%2C%20and%20product%20sophistication.%0ADrawing%20upon%20our%20experiences%20in%20successfully%20integrating%20GAI%20into%20several%20major%0Asocial%20and%20e-commerce%20platforms%2C%20this%20survey%20aims%20to%20comprehensively%20examine%0Athe%20underlying%20system%20and%20AI%20foundations%2C%20solution%20frameworks%2C%20connections%20to%0Akey%20research%20advancements%2C%20as%20well%20as%20summarize%20the%20practical%20insights%20and%0Achallenges%20encountered%20in%20the%20endeavor%20to%20integrate%20GAI%20into%20industrial%20Recsys.%0AAs%20pioneering%20work%20in%20this%20domain%2C%20we%20hope%20outline%20the%20representative%0Adevelopments%20of%20relevant%20fields%2C%20shed%20lights%20on%20practical%20GAI%20adoptions%20in%20the%0Aindustry%2C%20and%20motivate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06475v1&entry.124074799=Read"},
{"title": "Random Features Approximation for Control-Affine Systems", "author": "Kimia Kazemian and Yahya Sattar and Sarah Dean", "abstract": "  Modern data-driven control applications call for flexible nonlinear models\nthat are amenable to principled controller synthesis and realtime feedback.\nMany nonlinear dynamical systems of interest are control affine. We propose two\nnovel classes of nonlinear feature representations which capture control affine\nstructure while allowing for arbitrary complexity in the state dependence. Our\nmethods make use of random features (RF) approximations, inheriting the\nexpressiveness of kernel methods at a lower computational cost. We formalize\nthe representational capabilities of our methods by showing their relationship\nto the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021)\nand a novel Affine Dense (AD) kernel that we introduce. We further illustrate\nthe utility by presenting a case study of data-driven optimization-based\ncontrol using control certificate functions (CCF). Simulation experiments on a\ndouble pendulum empirically demonstrate the advantages of our methods.\n", "link": "http://arxiv.org/abs/2406.06514v1", "date": "2024-06-10", "relevancy": 1.9096, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4707}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Features%20Approximation%20for%20Control-Affine%20Systems&body=Title%3A%20Random%20Features%20Approximation%20for%20Control-Affine%20Systems%0AAuthor%3A%20Kimia%20Kazemian%20and%20Yahya%20Sattar%20and%20Sarah%20Dean%0AAbstract%3A%20%20%20Modern%20data-driven%20control%20applications%20call%20for%20flexible%20nonlinear%20models%0Athat%20are%20amenable%20to%20principled%20controller%20synthesis%20and%20realtime%20feedback.%0AMany%20nonlinear%20dynamical%20systems%20of%20interest%20are%20control%20affine.%20We%20propose%20two%0Anovel%20classes%20of%20nonlinear%20feature%20representations%20which%20capture%20control%20affine%0Astructure%20while%20allowing%20for%20arbitrary%20complexity%20in%20the%20state%20dependence.%20Our%0Amethods%20make%20use%20of%20random%20features%20%28RF%29%20approximations%2C%20inheriting%20the%0Aexpressiveness%20of%20kernel%20methods%20at%20a%20lower%20computational%20cost.%20We%20formalize%0Athe%20representational%20capabilities%20of%20our%20methods%20by%20showing%20their%20relationship%0Ato%20the%20Affine%20Dot%20Product%20%28ADP%29%20kernel%20proposed%20by%20Casta%5C~neda%20et%20al.%20%282021%29%0Aand%20a%20novel%20Affine%20Dense%20%28AD%29%20kernel%20that%20we%20introduce.%20We%20further%20illustrate%0Athe%20utility%20by%20presenting%20a%20case%20study%20of%20data-driven%20optimization-based%0Acontrol%20using%20control%20certificate%20functions%20%28CCF%29.%20Simulation%20experiments%20on%20a%0Adouble%20pendulum%20empirically%20demonstrate%20the%20advantages%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Features%2520Approximation%2520for%2520Control-Affine%2520Systems%26entry.906535625%3DKimia%2520Kazemian%2520and%2520Yahya%2520Sattar%2520and%2520Sarah%2520Dean%26entry.1292438233%3D%2520%2520Modern%2520data-driven%2520control%2520applications%2520call%2520for%2520flexible%2520nonlinear%2520models%250Athat%2520are%2520amenable%2520to%2520principled%2520controller%2520synthesis%2520and%2520realtime%2520feedback.%250AMany%2520nonlinear%2520dynamical%2520systems%2520of%2520interest%2520are%2520control%2520affine.%2520We%2520propose%2520two%250Anovel%2520classes%2520of%2520nonlinear%2520feature%2520representations%2520which%2520capture%2520control%2520affine%250Astructure%2520while%2520allowing%2520for%2520arbitrary%2520complexity%2520in%2520the%2520state%2520dependence.%2520Our%250Amethods%2520make%2520use%2520of%2520random%2520features%2520%2528RF%2529%2520approximations%252C%2520inheriting%2520the%250Aexpressiveness%2520of%2520kernel%2520methods%2520at%2520a%2520lower%2520computational%2520cost.%2520We%2520formalize%250Athe%2520representational%2520capabilities%2520of%2520our%2520methods%2520by%2520showing%2520their%2520relationship%250Ato%2520the%2520Affine%2520Dot%2520Product%2520%2528ADP%2529%2520kernel%2520proposed%2520by%2520Casta%255C~neda%2520et%2520al.%2520%25282021%2529%250Aand%2520a%2520novel%2520Affine%2520Dense%2520%2528AD%2529%2520kernel%2520that%2520we%2520introduce.%2520We%2520further%2520illustrate%250Athe%2520utility%2520by%2520presenting%2520a%2520case%2520study%2520of%2520data-driven%2520optimization-based%250Acontrol%2520using%2520control%2520certificate%2520functions%2520%2528CCF%2529.%2520Simulation%2520experiments%2520on%2520a%250Adouble%2520pendulum%2520empirically%2520demonstrate%2520the%2520advantages%2520of%2520our%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Features%20Approximation%20for%20Control-Affine%20Systems&entry.906535625=Kimia%20Kazemian%20and%20Yahya%20Sattar%20and%20Sarah%20Dean&entry.1292438233=%20%20Modern%20data-driven%20control%20applications%20call%20for%20flexible%20nonlinear%20models%0Athat%20are%20amenable%20to%20principled%20controller%20synthesis%20and%20realtime%20feedback.%0AMany%20nonlinear%20dynamical%20systems%20of%20interest%20are%20control%20affine.%20We%20propose%20two%0Anovel%20classes%20of%20nonlinear%20feature%20representations%20which%20capture%20control%20affine%0Astructure%20while%20allowing%20for%20arbitrary%20complexity%20in%20the%20state%20dependence.%20Our%0Amethods%20make%20use%20of%20random%20features%20%28RF%29%20approximations%2C%20inheriting%20the%0Aexpressiveness%20of%20kernel%20methods%20at%20a%20lower%20computational%20cost.%20We%20formalize%0Athe%20representational%20capabilities%20of%20our%20methods%20by%20showing%20their%20relationship%0Ato%20the%20Affine%20Dot%20Product%20%28ADP%29%20kernel%20proposed%20by%20Casta%5C~neda%20et%20al.%20%282021%29%0Aand%20a%20novel%20Affine%20Dense%20%28AD%29%20kernel%20that%20we%20introduce.%20We%20further%20illustrate%0Athe%20utility%20by%20presenting%20a%20case%20study%20of%20data-driven%20optimization-based%0Acontrol%20using%20control%20certificate%20functions%20%28CCF%29.%20Simulation%20experiments%20on%20a%0Adouble%20pendulum%20empirically%20demonstrate%20the%20advantages%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06514v1&entry.124074799=Read"},
{"title": "Can I understand what I create? Self-Knowledge Evaluation of Large\n  Language Models", "author": "Zhiquan Tan and Lai Wei and Jindong Wang and Xing Xie and Weiran Huang", "abstract": "  Large language models (LLMs) have achieved remarkable progress in linguistic\ntasks, necessitating robust evaluation frameworks to understand their\ncapabilities and limitations. Inspired by Feynman's principle of understanding\nthrough creation, we introduce a self-knowledge evaluation framework that is\neasy to implement, evaluating models on their ability to comprehend and respond\nto self-generated questions. Our findings, based on testing multiple models\nacross diverse tasks, reveal significant gaps in the model's self-knowledge\nability. Further analysis indicates these gaps may be due to misalignment with\nhuman attention mechanisms. Additionally, fine-tuning on self-generated math\ntask may enhance the model's math performance, highlighting the potential of\nthe framework for efficient and insightful model evaluation and may also\ncontribute to the improvement of LLMs.\n", "link": "http://arxiv.org/abs/2406.06140v1", "date": "2024-06-10", "relevancy": 1.4217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4794}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20I%20understand%20what%20I%20create%3F%20Self-Knowledge%20Evaluation%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Can%20I%20understand%20what%20I%20create%3F%20Self-Knowledge%20Evaluation%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Zhiquan%20Tan%20and%20Lai%20Wei%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Weiran%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20linguistic%0Atasks%2C%20necessitating%20robust%20evaluation%20frameworks%20to%20understand%20their%0Acapabilities%20and%20limitations.%20Inspired%20by%20Feynman%27s%20principle%20of%20understanding%0Athrough%20creation%2C%20we%20introduce%20a%20self-knowledge%20evaluation%20framework%20that%20is%0Aeasy%20to%20implement%2C%20evaluating%20models%20on%20their%20ability%20to%20comprehend%20and%20respond%0Ato%20self-generated%20questions.%20Our%20findings%2C%20based%20on%20testing%20multiple%20models%0Aacross%20diverse%20tasks%2C%20reveal%20significant%20gaps%20in%20the%20model%27s%20self-knowledge%0Aability.%20Further%20analysis%20indicates%20these%20gaps%20may%20be%20due%20to%20misalignment%20with%0Ahuman%20attention%20mechanisms.%20Additionally%2C%20fine-tuning%20on%20self-generated%20math%0Atask%20may%20enhance%20the%20model%27s%20math%20performance%2C%20highlighting%20the%20potential%20of%0Athe%20framework%20for%20efficient%20and%20insightful%20model%20evaluation%20and%20may%20also%0Acontribute%20to%20the%20improvement%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520I%2520understand%2520what%2520I%2520create%253F%2520Self-Knowledge%2520Evaluation%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DZhiquan%2520Tan%2520and%2520Lai%2520Wei%2520and%2520Jindong%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Weiran%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520linguistic%250Atasks%252C%2520necessitating%2520robust%2520evaluation%2520frameworks%2520to%2520understand%2520their%250Acapabilities%2520and%2520limitations.%2520Inspired%2520by%2520Feynman%2527s%2520principle%2520of%2520understanding%250Athrough%2520creation%252C%2520we%2520introduce%2520a%2520self-knowledge%2520evaluation%2520framework%2520that%2520is%250Aeasy%2520to%2520implement%252C%2520evaluating%2520models%2520on%2520their%2520ability%2520to%2520comprehend%2520and%2520respond%250Ato%2520self-generated%2520questions.%2520Our%2520findings%252C%2520based%2520on%2520testing%2520multiple%2520models%250Aacross%2520diverse%2520tasks%252C%2520reveal%2520significant%2520gaps%2520in%2520the%2520model%2527s%2520self-knowledge%250Aability.%2520Further%2520analysis%2520indicates%2520these%2520gaps%2520may%2520be%2520due%2520to%2520misalignment%2520with%250Ahuman%2520attention%2520mechanisms.%2520Additionally%252C%2520fine-tuning%2520on%2520self-generated%2520math%250Atask%2520may%2520enhance%2520the%2520model%2527s%2520math%2520performance%252C%2520highlighting%2520the%2520potential%2520of%250Athe%2520framework%2520for%2520efficient%2520and%2520insightful%2520model%2520evaluation%2520and%2520may%2520also%250Acontribute%2520to%2520the%2520improvement%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20I%20understand%20what%20I%20create%3F%20Self-Knowledge%20Evaluation%20of%20Large%0A%20%20Language%20Models&entry.906535625=Zhiquan%20Tan%20and%20Lai%20Wei%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Weiran%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20linguistic%0Atasks%2C%20necessitating%20robust%20evaluation%20frameworks%20to%20understand%20their%0Acapabilities%20and%20limitations.%20Inspired%20by%20Feynman%27s%20principle%20of%20understanding%0Athrough%20creation%2C%20we%20introduce%20a%20self-knowledge%20evaluation%20framework%20that%20is%0Aeasy%20to%20implement%2C%20evaluating%20models%20on%20their%20ability%20to%20comprehend%20and%20respond%0Ato%20self-generated%20questions.%20Our%20findings%2C%20based%20on%20testing%20multiple%20models%0Aacross%20diverse%20tasks%2C%20reveal%20significant%20gaps%20in%20the%20model%27s%20self-knowledge%0Aability.%20Further%20analysis%20indicates%20these%20gaps%20may%20be%20due%20to%20misalignment%20with%0Ahuman%20attention%20mechanisms.%20Additionally%2C%20fine-tuning%20on%20self-generated%20math%0Atask%20may%20enhance%20the%20model%27s%20math%20performance%2C%20highlighting%20the%20potential%20of%0Athe%20framework%20for%20efficient%20and%20insightful%20model%20evaluation%20and%20may%20also%0Acontribute%20to%20the%20improvement%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06140v1&entry.124074799=Read"},
{"title": "Demonstration-Regularized RL", "author": "Daniil Tiapkin and Denis Belomestny and Daniele Calandriello and Eric Moulines and Alexey Naumov and Pierre Perrault and Michal Valko and Pierre Menard", "abstract": "  Incorporating expert demonstrations has empirically helped to improve the\nsample efficiency of reinforcement learning (RL). This paper quantifies\ntheoretically to what extent this extra information reduces RL's sample\ncomplexity. In particular, we study the demonstration-regularized reinforcement\nlearning that leverages the expert demonstrations by KL-regularization for a\npolicy learned by behavior cloning. Our findings reveal that using\n$N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal\npolicy at a sample complexity of order\n$\\widetilde{O}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite\nand $\\widetilde{O}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in\nlinear Markov decision processes, where $\\varepsilon$ is the target precision,\n$H$ the horizon, $A$ the number of action, $S$ the number of states in the\nfinite case and $d$ the dimension of the feature space in the linear case. As a\nby-product, we provide tight convergence guarantees for the behaviour cloning\nprocedure under general assumptions on the policy classes. Additionally, we\nestablish that demonstration-regularized methods are provably efficient for\nreinforcement learning from human feedback (RLHF). In this respect, we provide\ntheoretical evidence showing the benefits of KL-regularization for RLHF in\ntabular and linear MDPs. Interestingly, we avoid pessimism injection by\nemploying computationally feasible regularization to handle reward estimation\nuncertainty, thus setting our approach apart from the prior works.\n", "link": "http://arxiv.org/abs/2310.17303v2", "date": "2024-06-10", "relevancy": 0.9251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demonstration-Regularized%20RL&body=Title%3A%20Demonstration-Regularized%20RL%0AAuthor%3A%20Daniil%20Tiapkin%20and%20Denis%20Belomestny%20and%20Daniele%20Calandriello%20and%20Eric%20Moulines%20and%20Alexey%20Naumov%20and%20Pierre%20Perrault%20and%20Michal%20Valko%20and%20Pierre%20Menard%0AAbstract%3A%20%20%20Incorporating%20expert%20demonstrations%20has%20empirically%20helped%20to%20improve%20the%0Asample%20efficiency%20of%20reinforcement%20learning%20%28RL%29.%20This%20paper%20quantifies%0Atheoretically%20to%20what%20extent%20this%20extra%20information%20reduces%20RL%27s%20sample%0Acomplexity.%20In%20particular%2C%20we%20study%20the%20demonstration-regularized%20reinforcement%0Alearning%20that%20leverages%20the%20expert%20demonstrations%20by%20KL-regularization%20for%20a%0Apolicy%20learned%20by%20behavior%20cloning.%20Our%20findings%20reveal%20that%20using%0A%24N%5E%7B%5Cmathrm%7BE%7D%7D%24%20expert%20demonstrations%20enables%20the%20identification%20of%20an%20optimal%0Apolicy%20at%20a%20sample%20complexity%20of%20order%0A%24%5Cwidetilde%7BO%7D%28%5Cmathrm%7BPoly%7D%28S%2CA%2CH%29/%28%5Cvarepsilon%5E2%20N%5E%7B%5Cmathrm%7BE%7D%7D%29%29%24%20in%20finite%0Aand%20%24%5Cwidetilde%7BO%7D%28%5Cmathrm%7BPoly%7D%28d%2CH%29/%28%5Cvarepsilon%5E2%20N%5E%7B%5Cmathrm%7BE%7D%7D%29%29%24%20in%0Alinear%20Markov%20decision%20processes%2C%20where%20%24%5Cvarepsilon%24%20is%20the%20target%20precision%2C%0A%24H%24%20the%20horizon%2C%20%24A%24%20the%20number%20of%20action%2C%20%24S%24%20the%20number%20of%20states%20in%20the%0Afinite%20case%20and%20%24d%24%20the%20dimension%20of%20the%20feature%20space%20in%20the%20linear%20case.%20As%20a%0Aby-product%2C%20we%20provide%20tight%20convergence%20guarantees%20for%20the%20behaviour%20cloning%0Aprocedure%20under%20general%20assumptions%20on%20the%20policy%20classes.%20Additionally%2C%20we%0Aestablish%20that%20demonstration-regularized%20methods%20are%20provably%20efficient%20for%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29.%20In%20this%20respect%2C%20we%20provide%0Atheoretical%20evidence%20showing%20the%20benefits%20of%20KL-regularization%20for%20RLHF%20in%0Atabular%20and%20linear%20MDPs.%20Interestingly%2C%20we%20avoid%20pessimism%20injection%20by%0Aemploying%20computationally%20feasible%20regularization%20to%20handle%20reward%20estimation%0Auncertainty%2C%20thus%20setting%20our%20approach%20apart%20from%20the%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemonstration-Regularized%2520RL%26entry.906535625%3DDaniil%2520Tiapkin%2520and%2520Denis%2520Belomestny%2520and%2520Daniele%2520Calandriello%2520and%2520Eric%2520Moulines%2520and%2520Alexey%2520Naumov%2520and%2520Pierre%2520Perrault%2520and%2520Michal%2520Valko%2520and%2520Pierre%2520Menard%26entry.1292438233%3D%2520%2520Incorporating%2520expert%2520demonstrations%2520has%2520empirically%2520helped%2520to%2520improve%2520the%250Asample%2520efficiency%2520of%2520reinforcement%2520learning%2520%2528RL%2529.%2520This%2520paper%2520quantifies%250Atheoretically%2520to%2520what%2520extent%2520this%2520extra%2520information%2520reduces%2520RL%2527s%2520sample%250Acomplexity.%2520In%2520particular%252C%2520we%2520study%2520the%2520demonstration-regularized%2520reinforcement%250Alearning%2520that%2520leverages%2520the%2520expert%2520demonstrations%2520by%2520KL-regularization%2520for%2520a%250Apolicy%2520learned%2520by%2520behavior%2520cloning.%2520Our%2520findings%2520reveal%2520that%2520using%250A%2524N%255E%257B%255Cmathrm%257BE%257D%257D%2524%2520expert%2520demonstrations%2520enables%2520the%2520identification%2520of%2520an%2520optimal%250Apolicy%2520at%2520a%2520sample%2520complexity%2520of%2520order%250A%2524%255Cwidetilde%257BO%257D%2528%255Cmathrm%257BPoly%257D%2528S%252CA%252CH%2529/%2528%255Cvarepsilon%255E2%2520N%255E%257B%255Cmathrm%257BE%257D%257D%2529%2529%2524%2520in%2520finite%250Aand%2520%2524%255Cwidetilde%257BO%257D%2528%255Cmathrm%257BPoly%257D%2528d%252CH%2529/%2528%255Cvarepsilon%255E2%2520N%255E%257B%255Cmathrm%257BE%257D%257D%2529%2529%2524%2520in%250Alinear%2520Markov%2520decision%2520processes%252C%2520where%2520%2524%255Cvarepsilon%2524%2520is%2520the%2520target%2520precision%252C%250A%2524H%2524%2520the%2520horizon%252C%2520%2524A%2524%2520the%2520number%2520of%2520action%252C%2520%2524S%2524%2520the%2520number%2520of%2520states%2520in%2520the%250Afinite%2520case%2520and%2520%2524d%2524%2520the%2520dimension%2520of%2520the%2520feature%2520space%2520in%2520the%2520linear%2520case.%2520As%2520a%250Aby-product%252C%2520we%2520provide%2520tight%2520convergence%2520guarantees%2520for%2520the%2520behaviour%2520cloning%250Aprocedure%2520under%2520general%2520assumptions%2520on%2520the%2520policy%2520classes.%2520Additionally%252C%2520we%250Aestablish%2520that%2520demonstration-regularized%2520methods%2520are%2520provably%2520efficient%2520for%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529.%2520In%2520this%2520respect%252C%2520we%2520provide%250Atheoretical%2520evidence%2520showing%2520the%2520benefits%2520of%2520KL-regularization%2520for%2520RLHF%2520in%250Atabular%2520and%2520linear%2520MDPs.%2520Interestingly%252C%2520we%2520avoid%2520pessimism%2520injection%2520by%250Aemploying%2520computationally%2520feasible%2520regularization%2520to%2520handle%2520reward%2520estimation%250Auncertainty%252C%2520thus%2520setting%2520our%2520approach%2520apart%2520from%2520the%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demonstration-Regularized%20RL&entry.906535625=Daniil%20Tiapkin%20and%20Denis%20Belomestny%20and%20Daniele%20Calandriello%20and%20Eric%20Moulines%20and%20Alexey%20Naumov%20and%20Pierre%20Perrault%20and%20Michal%20Valko%20and%20Pierre%20Menard&entry.1292438233=%20%20Incorporating%20expert%20demonstrations%20has%20empirically%20helped%20to%20improve%20the%0Asample%20efficiency%20of%20reinforcement%20learning%20%28RL%29.%20This%20paper%20quantifies%0Atheoretically%20to%20what%20extent%20this%20extra%20information%20reduces%20RL%27s%20sample%0Acomplexity.%20In%20particular%2C%20we%20study%20the%20demonstration-regularized%20reinforcement%0Alearning%20that%20leverages%20the%20expert%20demonstrations%20by%20KL-regularization%20for%20a%0Apolicy%20learned%20by%20behavior%20cloning.%20Our%20findings%20reveal%20that%20using%0A%24N%5E%7B%5Cmathrm%7BE%7D%7D%24%20expert%20demonstrations%20enables%20the%20identification%20of%20an%20optimal%0Apolicy%20at%20a%20sample%20complexity%20of%20order%0A%24%5Cwidetilde%7BO%7D%28%5Cmathrm%7BPoly%7D%28S%2CA%2CH%29/%28%5Cvarepsilon%5E2%20N%5E%7B%5Cmathrm%7BE%7D%7D%29%29%24%20in%20finite%0Aand%20%24%5Cwidetilde%7BO%7D%28%5Cmathrm%7BPoly%7D%28d%2CH%29/%28%5Cvarepsilon%5E2%20N%5E%7B%5Cmathrm%7BE%7D%7D%29%29%24%20in%0Alinear%20Markov%20decision%20processes%2C%20where%20%24%5Cvarepsilon%24%20is%20the%20target%20precision%2C%0A%24H%24%20the%20horizon%2C%20%24A%24%20the%20number%20of%20action%2C%20%24S%24%20the%20number%20of%20states%20in%20the%0Afinite%20case%20and%20%24d%24%20the%20dimension%20of%20the%20feature%20space%20in%20the%20linear%20case.%20As%20a%0Aby-product%2C%20we%20provide%20tight%20convergence%20guarantees%20for%20the%20behaviour%20cloning%0Aprocedure%20under%20general%20assumptions%20on%20the%20policy%20classes.%20Additionally%2C%20we%0Aestablish%20that%20demonstration-regularized%20methods%20are%20provably%20efficient%20for%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29.%20In%20this%20respect%2C%20we%20provide%0Atheoretical%20evidence%20showing%20the%20benefits%20of%20KL-regularization%20for%20RLHF%20in%0Atabular%20and%20linear%20MDPs.%20Interestingly%2C%20we%20avoid%20pessimism%20injection%20by%0Aemploying%20computationally%20feasible%20regularization%20to%20handle%20reward%20estimation%0Auncertainty%2C%20thus%20setting%20our%20approach%20apart%20from%20the%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17303v2&entry.124074799=Read"},
{"title": "The fast committor machine: Interpretable prediction with kernels", "author": "D. Aristoff and M. Johnson and G. Simpson and R. J. Webber", "abstract": "  In the study of stochastic systems, the committor function describes the\nprobability that a system starting from an initial configuration $x$ will reach\na set $B$ before a set $A$. This paper introduces an efficient and\ninterpretable algorithm for approximating the committor, called the \"fast\ncommittor machine\" (FCM). The FCM uses simulated trajectory data to build a\nkernel-based model of the committor. The kernel function is constructed to\nemphasize low-dimensional subspaces which optimally describe the $A$ to $B$\ntransitions. The coefficients in the kernel model are determined using\nrandomized linear algebra, leading to a runtime that scales linearly in the\nnumber of data points. In numerical experiments involving a triple-well\npotential and alanine dipeptide, the FCM yields higher accuracy and trains more\nquickly than a neural network with the same number of parameters. The FCM is\nalso more interpretable than the neural net.\n", "link": "http://arxiv.org/abs/2405.10410v2", "date": "2024-06-10", "relevancy": 1.3097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4279}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20fast%20committor%20machine%3A%20Interpretable%20prediction%20with%20kernels&body=Title%3A%20The%20fast%20committor%20machine%3A%20Interpretable%20prediction%20with%20kernels%0AAuthor%3A%20D.%20Aristoff%20and%20M.%20Johnson%20and%20G.%20Simpson%20and%20R.%20J.%20Webber%0AAbstract%3A%20%20%20In%20the%20study%20of%20stochastic%20systems%2C%20the%20committor%20function%20describes%20the%0Aprobability%20that%20a%20system%20starting%20from%20an%20initial%20configuration%20%24x%24%20will%20reach%0Aa%20set%20%24B%24%20before%20a%20set%20%24A%24.%20This%20paper%20introduces%20an%20efficient%20and%0Ainterpretable%20algorithm%20for%20approximating%20the%20committor%2C%20called%20the%20%22fast%0Acommittor%20machine%22%20%28FCM%29.%20The%20FCM%20uses%20simulated%20trajectory%20data%20to%20build%20a%0Akernel-based%20model%20of%20the%20committor.%20The%20kernel%20function%20is%20constructed%20to%0Aemphasize%20low-dimensional%20subspaces%20which%20optimally%20describe%20the%20%24A%24%20to%20%24B%24%0Atransitions.%20The%20coefficients%20in%20the%20kernel%20model%20are%20determined%20using%0Arandomized%20linear%20algebra%2C%20leading%20to%20a%20runtime%20that%20scales%20linearly%20in%20the%0Anumber%20of%20data%20points.%20In%20numerical%20experiments%20involving%20a%20triple-well%0Apotential%20and%20alanine%20dipeptide%2C%20the%20FCM%20yields%20higher%20accuracy%20and%20trains%20more%0Aquickly%20than%20a%20neural%20network%20with%20the%20same%20number%20of%20parameters.%20The%20FCM%20is%0Aalso%20more%20interpretable%20than%20the%20neural%20net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520fast%2520committor%2520machine%253A%2520Interpretable%2520prediction%2520with%2520kernels%26entry.906535625%3DD.%2520Aristoff%2520and%2520M.%2520Johnson%2520and%2520G.%2520Simpson%2520and%2520R.%2520J.%2520Webber%26entry.1292438233%3D%2520%2520In%2520the%2520study%2520of%2520stochastic%2520systems%252C%2520the%2520committor%2520function%2520describes%2520the%250Aprobability%2520that%2520a%2520system%2520starting%2520from%2520an%2520initial%2520configuration%2520%2524x%2524%2520will%2520reach%250Aa%2520set%2520%2524B%2524%2520before%2520a%2520set%2520%2524A%2524.%2520This%2520paper%2520introduces%2520an%2520efficient%2520and%250Ainterpretable%2520algorithm%2520for%2520approximating%2520the%2520committor%252C%2520called%2520the%2520%2522fast%250Acommittor%2520machine%2522%2520%2528FCM%2529.%2520The%2520FCM%2520uses%2520simulated%2520trajectory%2520data%2520to%2520build%2520a%250Akernel-based%2520model%2520of%2520the%2520committor.%2520The%2520kernel%2520function%2520is%2520constructed%2520to%250Aemphasize%2520low-dimensional%2520subspaces%2520which%2520optimally%2520describe%2520the%2520%2524A%2524%2520to%2520%2524B%2524%250Atransitions.%2520The%2520coefficients%2520in%2520the%2520kernel%2520model%2520are%2520determined%2520using%250Arandomized%2520linear%2520algebra%252C%2520leading%2520to%2520a%2520runtime%2520that%2520scales%2520linearly%2520in%2520the%250Anumber%2520of%2520data%2520points.%2520In%2520numerical%2520experiments%2520involving%2520a%2520triple-well%250Apotential%2520and%2520alanine%2520dipeptide%252C%2520the%2520FCM%2520yields%2520higher%2520accuracy%2520and%2520trains%2520more%250Aquickly%2520than%2520a%2520neural%2520network%2520with%2520the%2520same%2520number%2520of%2520parameters.%2520The%2520FCM%2520is%250Aalso%2520more%2520interpretable%2520than%2520the%2520neural%2520net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20fast%20committor%20machine%3A%20Interpretable%20prediction%20with%20kernels&entry.906535625=D.%20Aristoff%20and%20M.%20Johnson%20and%20G.%20Simpson%20and%20R.%20J.%20Webber&entry.1292438233=%20%20In%20the%20study%20of%20stochastic%20systems%2C%20the%20committor%20function%20describes%20the%0Aprobability%20that%20a%20system%20starting%20from%20an%20initial%20configuration%20%24x%24%20will%20reach%0Aa%20set%20%24B%24%20before%20a%20set%20%24A%24.%20This%20paper%20introduces%20an%20efficient%20and%0Ainterpretable%20algorithm%20for%20approximating%20the%20committor%2C%20called%20the%20%22fast%0Acommittor%20machine%22%20%28FCM%29.%20The%20FCM%20uses%20simulated%20trajectory%20data%20to%20build%20a%0Akernel-based%20model%20of%20the%20committor.%20The%20kernel%20function%20is%20constructed%20to%0Aemphasize%20low-dimensional%20subspaces%20which%20optimally%20describe%20the%20%24A%24%20to%20%24B%24%0Atransitions.%20The%20coefficients%20in%20the%20kernel%20model%20are%20determined%20using%0Arandomized%20linear%20algebra%2C%20leading%20to%20a%20runtime%20that%20scales%20linearly%20in%20the%0Anumber%20of%20data%20points.%20In%20numerical%20experiments%20involving%20a%20triple-well%0Apotential%20and%20alanine%20dipeptide%2C%20the%20FCM%20yields%20higher%20accuracy%20and%20trains%20more%0Aquickly%20than%20a%20neural%20network%20with%20the%20same%20number%20of%20parameters.%20The%20FCM%20is%0Aalso%20more%20interpretable%20than%20the%20neural%20net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10410v2&entry.124074799=Read"},
{"title": "Deep Generative Modeling Reshapes Compression and Transmission: From\n  Efficiency to Resiliency", "author": "Jincheng Dai and Xiaoqi Qin and Sixian Wang and Lexi Xu and Kai Niu and Ping Zhang", "abstract": "  Information theory and machine learning are inextricably linked and have even\nbeen referred to as \"two sides of the same coin\". One particularly elegant\nconnection is the essential equivalence between probabilistic generative\nmodeling and data compression or transmission. In this article, we reveal the\ndual-functionality of deep generative models that reshapes both data\ncompression for efficiency and transmission error concealment for resiliency.\nWe present how the contextual predictive capabilities of powerful generative\nmodels can be well positioned to be strong compressors and estimators. In this\nsense, we advocate for viewing the deep generative modeling problem through the\nlens of end-to-end communications, and evaluate the compression and error\nrestoration capabilities of foundation generative models. We show that the\nkernel of many large generative models is powerful predictor that can capture\ncomplex relationships among semantic latent variables, and the communication\nviewpoints provide novel insights into semantic feature tokenization,\ncontextual learning, and usage of deep generative models. In summary, our\narticle highlights the essential connections of generative AI to source and\nchannel coding techniques, and motivates researchers to make further\nexplorations in this emerging topic.\n", "link": "http://arxiv.org/abs/2406.06446v1", "date": "2024-06-10", "relevancy": 1.6021, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5226}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Modeling%20Reshapes%20Compression%20and%20Transmission%3A%20From%0A%20%20Efficiency%20to%20Resiliency&body=Title%3A%20Deep%20Generative%20Modeling%20Reshapes%20Compression%20and%20Transmission%3A%20From%0A%20%20Efficiency%20to%20Resiliency%0AAuthor%3A%20Jincheng%20Dai%20and%20Xiaoqi%20Qin%20and%20Sixian%20Wang%20and%20Lexi%20Xu%20and%20Kai%20Niu%20and%20Ping%20Zhang%0AAbstract%3A%20%20%20Information%20theory%20and%20machine%20learning%20are%20inextricably%20linked%20and%20have%20even%0Abeen%20referred%20to%20as%20%22two%20sides%20of%20the%20same%20coin%22.%20One%20particularly%20elegant%0Aconnection%20is%20the%20essential%20equivalence%20between%20probabilistic%20generative%0Amodeling%20and%20data%20compression%20or%20transmission.%20In%20this%20article%2C%20we%20reveal%20the%0Adual-functionality%20of%20deep%20generative%20models%20that%20reshapes%20both%20data%0Acompression%20for%20efficiency%20and%20transmission%20error%20concealment%20for%20resiliency.%0AWe%20present%20how%20the%20contextual%20predictive%20capabilities%20of%20powerful%20generative%0Amodels%20can%20be%20well%20positioned%20to%20be%20strong%20compressors%20and%20estimators.%20In%20this%0Asense%2C%20we%20advocate%20for%20viewing%20the%20deep%20generative%20modeling%20problem%20through%20the%0Alens%20of%20end-to-end%20communications%2C%20and%20evaluate%20the%20compression%20and%20error%0Arestoration%20capabilities%20of%20foundation%20generative%20models.%20We%20show%20that%20the%0Akernel%20of%20many%20large%20generative%20models%20is%20powerful%20predictor%20that%20can%20capture%0Acomplex%20relationships%20among%20semantic%20latent%20variables%2C%20and%20the%20communication%0Aviewpoints%20provide%20novel%20insights%20into%20semantic%20feature%20tokenization%2C%0Acontextual%20learning%2C%20and%20usage%20of%20deep%20generative%20models.%20In%20summary%2C%20our%0Aarticle%20highlights%20the%20essential%20connections%20of%20generative%20AI%20to%20source%20and%0Achannel%20coding%20techniques%2C%20and%20motivates%20researchers%20to%20make%20further%0Aexplorations%20in%20this%20emerging%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Modeling%2520Reshapes%2520Compression%2520and%2520Transmission%253A%2520From%250A%2520%2520Efficiency%2520to%2520Resiliency%26entry.906535625%3DJincheng%2520Dai%2520and%2520Xiaoqi%2520Qin%2520and%2520Sixian%2520Wang%2520and%2520Lexi%2520Xu%2520and%2520Kai%2520Niu%2520and%2520Ping%2520Zhang%26entry.1292438233%3D%2520%2520Information%2520theory%2520and%2520machine%2520learning%2520are%2520inextricably%2520linked%2520and%2520have%2520even%250Abeen%2520referred%2520to%2520as%2520%2522two%2520sides%2520of%2520the%2520same%2520coin%2522.%2520One%2520particularly%2520elegant%250Aconnection%2520is%2520the%2520essential%2520equivalence%2520between%2520probabilistic%2520generative%250Amodeling%2520and%2520data%2520compression%2520or%2520transmission.%2520In%2520this%2520article%252C%2520we%2520reveal%2520the%250Adual-functionality%2520of%2520deep%2520generative%2520models%2520that%2520reshapes%2520both%2520data%250Acompression%2520for%2520efficiency%2520and%2520transmission%2520error%2520concealment%2520for%2520resiliency.%250AWe%2520present%2520how%2520the%2520contextual%2520predictive%2520capabilities%2520of%2520powerful%2520generative%250Amodels%2520can%2520be%2520well%2520positioned%2520to%2520be%2520strong%2520compressors%2520and%2520estimators.%2520In%2520this%250Asense%252C%2520we%2520advocate%2520for%2520viewing%2520the%2520deep%2520generative%2520modeling%2520problem%2520through%2520the%250Alens%2520of%2520end-to-end%2520communications%252C%2520and%2520evaluate%2520the%2520compression%2520and%2520error%250Arestoration%2520capabilities%2520of%2520foundation%2520generative%2520models.%2520We%2520show%2520that%2520the%250Akernel%2520of%2520many%2520large%2520generative%2520models%2520is%2520powerful%2520predictor%2520that%2520can%2520capture%250Acomplex%2520relationships%2520among%2520semantic%2520latent%2520variables%252C%2520and%2520the%2520communication%250Aviewpoints%2520provide%2520novel%2520insights%2520into%2520semantic%2520feature%2520tokenization%252C%250Acontextual%2520learning%252C%2520and%2520usage%2520of%2520deep%2520generative%2520models.%2520In%2520summary%252C%2520our%250Aarticle%2520highlights%2520the%2520essential%2520connections%2520of%2520generative%2520AI%2520to%2520source%2520and%250Achannel%2520coding%2520techniques%252C%2520and%2520motivates%2520researchers%2520to%2520make%2520further%250Aexplorations%2520in%2520this%2520emerging%2520topic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Modeling%20Reshapes%20Compression%20and%20Transmission%3A%20From%0A%20%20Efficiency%20to%20Resiliency&entry.906535625=Jincheng%20Dai%20and%20Xiaoqi%20Qin%20and%20Sixian%20Wang%20and%20Lexi%20Xu%20and%20Kai%20Niu%20and%20Ping%20Zhang&entry.1292438233=%20%20Information%20theory%20and%20machine%20learning%20are%20inextricably%20linked%20and%20have%20even%0Abeen%20referred%20to%20as%20%22two%20sides%20of%20the%20same%20coin%22.%20One%20particularly%20elegant%0Aconnection%20is%20the%20essential%20equivalence%20between%20probabilistic%20generative%0Amodeling%20and%20data%20compression%20or%20transmission.%20In%20this%20article%2C%20we%20reveal%20the%0Adual-functionality%20of%20deep%20generative%20models%20that%20reshapes%20both%20data%0Acompression%20for%20efficiency%20and%20transmission%20error%20concealment%20for%20resiliency.%0AWe%20present%20how%20the%20contextual%20predictive%20capabilities%20of%20powerful%20generative%0Amodels%20can%20be%20well%20positioned%20to%20be%20strong%20compressors%20and%20estimators.%20In%20this%0Asense%2C%20we%20advocate%20for%20viewing%20the%20deep%20generative%20modeling%20problem%20through%20the%0Alens%20of%20end-to-end%20communications%2C%20and%20evaluate%20the%20compression%20and%20error%0Arestoration%20capabilities%20of%20foundation%20generative%20models.%20We%20show%20that%20the%0Akernel%20of%20many%20large%20generative%20models%20is%20powerful%20predictor%20that%20can%20capture%0Acomplex%20relationships%20among%20semantic%20latent%20variables%2C%20and%20the%20communication%0Aviewpoints%20provide%20novel%20insights%20into%20semantic%20feature%20tokenization%2C%0Acontextual%20learning%2C%20and%20usage%20of%20deep%20generative%20models.%20In%20summary%2C%20our%0Aarticle%20highlights%20the%20essential%20connections%20of%20generative%20AI%20to%20source%20and%0Achannel%20coding%20techniques%2C%20and%20motivates%20researchers%20to%20make%20further%0Aexplorations%20in%20this%20emerging%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06446v1&entry.124074799=Read"},
{"title": "Data Augmentation for Multivariate Time Series Classification: An\n  Experimental Study", "author": "Romain Ilbert and Thai V. Hoang and Zonghua Zhang", "abstract": "  Our study investigates the impact of data augmentation on the performance of\nmultivariate time series models, focusing on datasets from the UCR archive.\nDespite the limited size of these datasets, we achieved classification accuracy\nimprovements in 10 out of 13 datasets using the Rocket and InceptionTime\nmodels. This highlights the essential role of sufficient data in training\neffective models, paralleling the advancements seen in computer vision. Our\nwork delves into adapting and applying existing methods in innovative ways to\nthe domain of multivariate time series classification. Our comprehensive\nexploration of these techniques sets a new standard for addressing data\nscarcity in time series analysis, emphasizing that diverse augmentation\nstrategies are crucial for unlocking the potential of both traditional and deep\nlearning models. Moreover, by meticulously analyzing and applying a variety of\naugmentation techniques, we demonstrate that strategic data enrichment can\nenhance model accuracy. This not only establishes a benchmark for future\nresearch in time series analysis but also underscores the importance of\nadopting varied augmentation approaches to improve model performance in the\nface of limited data availability.\n", "link": "http://arxiv.org/abs/2406.06518v1", "date": "2024-06-10", "relevancy": 1.9071, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4652}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Multivariate%20Time%20Series%20Classification%3A%20An%0A%20%20Experimental%20Study&body=Title%3A%20Data%20Augmentation%20for%20Multivariate%20Time%20Series%20Classification%3A%20An%0A%20%20Experimental%20Study%0AAuthor%3A%20Romain%20Ilbert%20and%20Thai%20V.%20Hoang%20and%20Zonghua%20Zhang%0AAbstract%3A%20%20%20Our%20study%20investigates%20the%20impact%20of%20data%20augmentation%20on%20the%20performance%20of%0Amultivariate%20time%20series%20models%2C%20focusing%20on%20datasets%20from%20the%20UCR%20archive.%0ADespite%20the%20limited%20size%20of%20these%20datasets%2C%20we%20achieved%20classification%20accuracy%0Aimprovements%20in%2010%20out%20of%2013%20datasets%20using%20the%20Rocket%20and%20InceptionTime%0Amodels.%20This%20highlights%20the%20essential%20role%20of%20sufficient%20data%20in%20training%0Aeffective%20models%2C%20paralleling%20the%20advancements%20seen%20in%20computer%20vision.%20Our%0Awork%20delves%20into%20adapting%20and%20applying%20existing%20methods%20in%20innovative%20ways%20to%0Athe%20domain%20of%20multivariate%20time%20series%20classification.%20Our%20comprehensive%0Aexploration%20of%20these%20techniques%20sets%20a%20new%20standard%20for%20addressing%20data%0Ascarcity%20in%20time%20series%20analysis%2C%20emphasizing%20that%20diverse%20augmentation%0Astrategies%20are%20crucial%20for%20unlocking%20the%20potential%20of%20both%20traditional%20and%20deep%0Alearning%20models.%20Moreover%2C%20by%20meticulously%20analyzing%20and%20applying%20a%20variety%20of%0Aaugmentation%20techniques%2C%20we%20demonstrate%20that%20strategic%20data%20enrichment%20can%0Aenhance%20model%20accuracy.%20This%20not%20only%20establishes%20a%20benchmark%20for%20future%0Aresearch%20in%20time%20series%20analysis%20but%20also%20underscores%20the%20importance%20of%0Aadopting%20varied%20augmentation%20approaches%20to%20improve%20model%20performance%20in%20the%0Aface%20of%20limited%20data%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520for%2520Multivariate%2520Time%2520Series%2520Classification%253A%2520An%250A%2520%2520Experimental%2520Study%26entry.906535625%3DRomain%2520Ilbert%2520and%2520Thai%2520V.%2520Hoang%2520and%2520Zonghua%2520Zhang%26entry.1292438233%3D%2520%2520Our%2520study%2520investigates%2520the%2520impact%2520of%2520data%2520augmentation%2520on%2520the%2520performance%2520of%250Amultivariate%2520time%2520series%2520models%252C%2520focusing%2520on%2520datasets%2520from%2520the%2520UCR%2520archive.%250ADespite%2520the%2520limited%2520size%2520of%2520these%2520datasets%252C%2520we%2520achieved%2520classification%2520accuracy%250Aimprovements%2520in%252010%2520out%2520of%252013%2520datasets%2520using%2520the%2520Rocket%2520and%2520InceptionTime%250Amodels.%2520This%2520highlights%2520the%2520essential%2520role%2520of%2520sufficient%2520data%2520in%2520training%250Aeffective%2520models%252C%2520paralleling%2520the%2520advancements%2520seen%2520in%2520computer%2520vision.%2520Our%250Awork%2520delves%2520into%2520adapting%2520and%2520applying%2520existing%2520methods%2520in%2520innovative%2520ways%2520to%250Athe%2520domain%2520of%2520multivariate%2520time%2520series%2520classification.%2520Our%2520comprehensive%250Aexploration%2520of%2520these%2520techniques%2520sets%2520a%2520new%2520standard%2520for%2520addressing%2520data%250Ascarcity%2520in%2520time%2520series%2520analysis%252C%2520emphasizing%2520that%2520diverse%2520augmentation%250Astrategies%2520are%2520crucial%2520for%2520unlocking%2520the%2520potential%2520of%2520both%2520traditional%2520and%2520deep%250Alearning%2520models.%2520Moreover%252C%2520by%2520meticulously%2520analyzing%2520and%2520applying%2520a%2520variety%2520of%250Aaugmentation%2520techniques%252C%2520we%2520demonstrate%2520that%2520strategic%2520data%2520enrichment%2520can%250Aenhance%2520model%2520accuracy.%2520This%2520not%2520only%2520establishes%2520a%2520benchmark%2520for%2520future%250Aresearch%2520in%2520time%2520series%2520analysis%2520but%2520also%2520underscores%2520the%2520importance%2520of%250Aadopting%2520varied%2520augmentation%2520approaches%2520to%2520improve%2520model%2520performance%2520in%2520the%250Aface%2520of%2520limited%2520data%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Multivariate%20Time%20Series%20Classification%3A%20An%0A%20%20Experimental%20Study&entry.906535625=Romain%20Ilbert%20and%20Thai%20V.%20Hoang%20and%20Zonghua%20Zhang&entry.1292438233=%20%20Our%20study%20investigates%20the%20impact%20of%20data%20augmentation%20on%20the%20performance%20of%0Amultivariate%20time%20series%20models%2C%20focusing%20on%20datasets%20from%20the%20UCR%20archive.%0ADespite%20the%20limited%20size%20of%20these%20datasets%2C%20we%20achieved%20classification%20accuracy%0Aimprovements%20in%2010%20out%20of%2013%20datasets%20using%20the%20Rocket%20and%20InceptionTime%0Amodels.%20This%20highlights%20the%20essential%20role%20of%20sufficient%20data%20in%20training%0Aeffective%20models%2C%20paralleling%20the%20advancements%20seen%20in%20computer%20vision.%20Our%0Awork%20delves%20into%20adapting%20and%20applying%20existing%20methods%20in%20innovative%20ways%20to%0Athe%20domain%20of%20multivariate%20time%20series%20classification.%20Our%20comprehensive%0Aexploration%20of%20these%20techniques%20sets%20a%20new%20standard%20for%20addressing%20data%0Ascarcity%20in%20time%20series%20analysis%2C%20emphasizing%20that%20diverse%20augmentation%0Astrategies%20are%20crucial%20for%20unlocking%20the%20potential%20of%20both%20traditional%20and%20deep%0Alearning%20models.%20Moreover%2C%20by%20meticulously%20analyzing%20and%20applying%20a%20variety%20of%0Aaugmentation%20techniques%2C%20we%20demonstrate%20that%20strategic%20data%20enrichment%20can%0Aenhance%20model%20accuracy.%20This%20not%20only%20establishes%20a%20benchmark%20for%20future%0Aresearch%20in%20time%20series%20analysis%20but%20also%20underscores%20the%20importance%20of%0Aadopting%20varied%20augmentation%20approaches%20to%20improve%20model%20performance%20in%20the%0Aface%20of%20limited%20data%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06518v1&entry.124074799=Read"},
{"title": "A Guide to Stochastic Optimisation for Large-Scale Inverse Problems", "author": "Matthias J. Ehrhardt and Zeljko Kereta and Jingwei Liang and Junqi Tang", "abstract": "  Stochastic optimisation algorithms are the de facto standard for machine\nlearning with large amounts of data. Handling only a subset of available data\nin each optimisation step dramatically reduces the per-iteration computational\ncosts, while still ensuring significant progress towards the solution. Driven\nby the need to solve large-scale optimisation problems as efficiently as\npossible, the last decade has witnessed an explosion of research in this area.\nLeveraging the parallels between machine learning and inverse problems has\nallowed harnessing the power of this research wave for solving inverse\nproblems. In this survey, we provide a comprehensive account of the\nstate-of-the-art in stochastic optimisation from the viewpoint of inverse\nproblems. We present algorithms with diverse modalities of problem\nrandomisation and discuss the roles of variance reduction, acceleration,\nhigher-order methods, and other algorithmic modifications, and compare\ntheoretical results with practical behaviour. We focus on the potential and the\nchallenges for stochastic optimisation that are unique to inverse imaging\nproblems and are not commonly encountered in machine learning. We conclude the\nsurvey with illustrative examples from imaging problems to examine the\nadvantages and disadvantages that this new generation of algorithms bring to\nthe field of inverse problems.\n", "link": "http://arxiv.org/abs/2406.06342v1", "date": "2024-06-10", "relevancy": 1.425, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.478}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4754}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Guide%20to%20Stochastic%20Optimisation%20for%20Large-Scale%20Inverse%20Problems&body=Title%3A%20A%20Guide%20to%20Stochastic%20Optimisation%20for%20Large-Scale%20Inverse%20Problems%0AAuthor%3A%20Matthias%20J.%20Ehrhardt%20and%20Zeljko%20Kereta%20and%20Jingwei%20Liang%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Stochastic%20optimisation%20algorithms%20are%20the%20de%20facto%20standard%20for%20machine%0Alearning%20with%20large%20amounts%20of%20data.%20Handling%20only%20a%20subset%20of%20available%20data%0Ain%20each%20optimisation%20step%20dramatically%20reduces%20the%20per-iteration%20computational%0Acosts%2C%20while%20still%20ensuring%20significant%20progress%20towards%20the%20solution.%20Driven%0Aby%20the%20need%20to%20solve%20large-scale%20optimisation%20problems%20as%20efficiently%20as%0Apossible%2C%20the%20last%20decade%20has%20witnessed%20an%20explosion%20of%20research%20in%20this%20area.%0ALeveraging%20the%20parallels%20between%20machine%20learning%20and%20inverse%20problems%20has%0Aallowed%20harnessing%20the%20power%20of%20this%20research%20wave%20for%20solving%20inverse%0Aproblems.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20account%20of%20the%0Astate-of-the-art%20in%20stochastic%20optimisation%20from%20the%20viewpoint%20of%20inverse%0Aproblems.%20We%20present%20algorithms%20with%20diverse%20modalities%20of%20problem%0Arandomisation%20and%20discuss%20the%20roles%20of%20variance%20reduction%2C%20acceleration%2C%0Ahigher-order%20methods%2C%20and%20other%20algorithmic%20modifications%2C%20and%20compare%0Atheoretical%20results%20with%20practical%20behaviour.%20We%20focus%20on%20the%20potential%20and%20the%0Achallenges%20for%20stochastic%20optimisation%20that%20are%20unique%20to%20inverse%20imaging%0Aproblems%20and%20are%20not%20commonly%20encountered%20in%20machine%20learning.%20We%20conclude%20the%0Asurvey%20with%20illustrative%20examples%20from%20imaging%20problems%20to%20examine%20the%0Aadvantages%20and%20disadvantages%20that%20this%20new%20generation%20of%20algorithms%20bring%20to%0Athe%20field%20of%20inverse%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Guide%2520to%2520Stochastic%2520Optimisation%2520for%2520Large-Scale%2520Inverse%2520Problems%26entry.906535625%3DMatthias%2520J.%2520Ehrhardt%2520and%2520Zeljko%2520Kereta%2520and%2520Jingwei%2520Liang%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Stochastic%2520optimisation%2520algorithms%2520are%2520the%2520de%2520facto%2520standard%2520for%2520machine%250Alearning%2520with%2520large%2520amounts%2520of%2520data.%2520Handling%2520only%2520a%2520subset%2520of%2520available%2520data%250Ain%2520each%2520optimisation%2520step%2520dramatically%2520reduces%2520the%2520per-iteration%2520computational%250Acosts%252C%2520while%2520still%2520ensuring%2520significant%2520progress%2520towards%2520the%2520solution.%2520Driven%250Aby%2520the%2520need%2520to%2520solve%2520large-scale%2520optimisation%2520problems%2520as%2520efficiently%2520as%250Apossible%252C%2520the%2520last%2520decade%2520has%2520witnessed%2520an%2520explosion%2520of%2520research%2520in%2520this%2520area.%250ALeveraging%2520the%2520parallels%2520between%2520machine%2520learning%2520and%2520inverse%2520problems%2520has%250Aallowed%2520harnessing%2520the%2520power%2520of%2520this%2520research%2520wave%2520for%2520solving%2520inverse%250Aproblems.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520account%2520of%2520the%250Astate-of-the-art%2520in%2520stochastic%2520optimisation%2520from%2520the%2520viewpoint%2520of%2520inverse%250Aproblems.%2520We%2520present%2520algorithms%2520with%2520diverse%2520modalities%2520of%2520problem%250Arandomisation%2520and%2520discuss%2520the%2520roles%2520of%2520variance%2520reduction%252C%2520acceleration%252C%250Ahigher-order%2520methods%252C%2520and%2520other%2520algorithmic%2520modifications%252C%2520and%2520compare%250Atheoretical%2520results%2520with%2520practical%2520behaviour.%2520We%2520focus%2520on%2520the%2520potential%2520and%2520the%250Achallenges%2520for%2520stochastic%2520optimisation%2520that%2520are%2520unique%2520to%2520inverse%2520imaging%250Aproblems%2520and%2520are%2520not%2520commonly%2520encountered%2520in%2520machine%2520learning.%2520We%2520conclude%2520the%250Asurvey%2520with%2520illustrative%2520examples%2520from%2520imaging%2520problems%2520to%2520examine%2520the%250Aadvantages%2520and%2520disadvantages%2520that%2520this%2520new%2520generation%2520of%2520algorithms%2520bring%2520to%250Athe%2520field%2520of%2520inverse%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Guide%20to%20Stochastic%20Optimisation%20for%20Large-Scale%20Inverse%20Problems&entry.906535625=Matthias%20J.%20Ehrhardt%20and%20Zeljko%20Kereta%20and%20Jingwei%20Liang%20and%20Junqi%20Tang&entry.1292438233=%20%20Stochastic%20optimisation%20algorithms%20are%20the%20de%20facto%20standard%20for%20machine%0Alearning%20with%20large%20amounts%20of%20data.%20Handling%20only%20a%20subset%20of%20available%20data%0Ain%20each%20optimisation%20step%20dramatically%20reduces%20the%20per-iteration%20computational%0Acosts%2C%20while%20still%20ensuring%20significant%20progress%20towards%20the%20solution.%20Driven%0Aby%20the%20need%20to%20solve%20large-scale%20optimisation%20problems%20as%20efficiently%20as%0Apossible%2C%20the%20last%20decade%20has%20witnessed%20an%20explosion%20of%20research%20in%20this%20area.%0ALeveraging%20the%20parallels%20between%20machine%20learning%20and%20inverse%20problems%20has%0Aallowed%20harnessing%20the%20power%20of%20this%20research%20wave%20for%20solving%20inverse%0Aproblems.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20account%20of%20the%0Astate-of-the-art%20in%20stochastic%20optimisation%20from%20the%20viewpoint%20of%20inverse%0Aproblems.%20We%20present%20algorithms%20with%20diverse%20modalities%20of%20problem%0Arandomisation%20and%20discuss%20the%20roles%20of%20variance%20reduction%2C%20acceleration%2C%0Ahigher-order%20methods%2C%20and%20other%20algorithmic%20modifications%2C%20and%20compare%0Atheoretical%20results%20with%20practical%20behaviour.%20We%20focus%20on%20the%20potential%20and%20the%0Achallenges%20for%20stochastic%20optimisation%20that%20are%20unique%20to%20inverse%20imaging%0Aproblems%20and%20are%20not%20commonly%20encountered%20in%20machine%20learning.%20We%20conclude%20the%0Asurvey%20with%20illustrative%20examples%20from%20imaging%20problems%20to%20examine%20the%0Aadvantages%20and%20disadvantages%20that%20this%20new%20generation%20of%20algorithms%20bring%20to%0Athe%20field%20of%20inverse%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06342v1&entry.124074799=Read"},
{"title": "Differentially Private Best-Arm Identification", "author": "Achraf Azize and Marc Jourdan and Aymen Al Marjani and Debabrota Basu", "abstract": "  Best Arm Identification (BAI) problems are progressively used for\ndata-sensitive applications, such as designing adaptive clinical trials, tuning\nhyper-parameters, and conducting user studies. Motivated by the data privacy\nconcerns invoked by these applications, we study the problem of BAI with fixed\nconfidence in both the local and central models, i.e. $\\epsilon$-local and\n$\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of\nprivacy, we derive lower bounds on the sample complexity of any\n$\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or\n$\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy\nregimes. In the high-privacy regime, the hardness depends on a coupled effect\nof privacy and novel information-theoretic quantities involving the Total\nVariation. In the low-privacy regime, the lower bounds reduce to the\nnon-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global\nDP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively.\nFor $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a\nprivate estimator of the means based on Randomised Response. For\n$\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent\nadaptive episodes and adds Laplace noise to ensure a good privacy-utility\ntrade-off. By adapting the transportation costs, the expected sample complexity\nof AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.\n", "link": "http://arxiv.org/abs/2406.06408v1", "date": "2024-06-10", "relevancy": 1.3266, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Best-Arm%20Identification&body=Title%3A%20Differentially%20Private%20Best-Arm%20Identification%0AAuthor%3A%20Achraf%20Azize%20and%20Marc%20Jourdan%20and%20Aymen%20Al%20Marjani%20and%20Debabrota%20Basu%0AAbstract%3A%20%20%20Best%20Arm%20Identification%20%28BAI%29%20problems%20are%20progressively%20used%20for%0Adata-sensitive%20applications%2C%20such%20as%20designing%20adaptive%20clinical%20trials%2C%20tuning%0Ahyper-parameters%2C%20and%20conducting%20user%20studies.%20Motivated%20by%20the%20data%20privacy%0Aconcerns%20invoked%20by%20these%20applications%2C%20we%20study%20the%20problem%20of%20BAI%20with%20fixed%0Aconfidence%20in%20both%20the%20local%20and%20central%20models%2C%20i.e.%20%24%5Cepsilon%24-local%20and%0A%24%5Cepsilon%24-global%20Differential%20Privacy%20%28DP%29.%20First%2C%20to%20quantify%20the%20cost%20of%0Aprivacy%2C%20we%20derive%20lower%20bounds%20on%20the%20sample%20complexity%20of%20any%0A%24%5Cdelta%24-correct%20BAI%20algorithm%20satisfying%20%24%5Cepsilon%24-global%20DP%20or%0A%24%5Cepsilon%24-local%20DP.%20Our%20lower%20bounds%20suggest%20the%20existence%20of%20two%20privacy%0Aregimes.%20In%20the%20high-privacy%20regime%2C%20the%20hardness%20depends%20on%20a%20coupled%20effect%0Aof%20privacy%20and%20novel%20information-theoretic%20quantities%20involving%20the%20Total%0AVariation.%20In%20the%20low-privacy%20regime%2C%20the%20lower%20bounds%20reduce%20to%20the%0Anon-private%20lower%20bounds.%20We%20propose%20%24%5Cepsilon%24-local%20DP%20and%20%24%5Cepsilon%24-global%0ADP%20variants%20of%20a%20Top%20Two%20algorithm%2C%20namely%20CTB-TT%20and%20AdaP-TT%2A%2C%20respectively.%0AFor%20%24%5Cepsilon%24-local%20DP%2C%20CTB-TT%20is%20asymptotically%20optimal%20by%20plugging%20in%20a%0Aprivate%20estimator%20of%20the%20means%20based%20on%20Randomised%20Response.%20For%0A%24%5Cepsilon%24-global%20DP%2C%20our%20private%20estimator%20of%20the%20mean%20runs%20in%20arm-dependent%0Aadaptive%20episodes%20and%20adds%20Laplace%20noise%20to%20ensure%20a%20good%20privacy-utility%0Atrade-off.%20By%20adapting%20the%20transportation%20costs%2C%20the%20expected%20sample%20complexity%0Aof%20AdaP-TT%2A%20reaches%20the%20asymptotic%20lower%20bound%20up%20to%20multiplicative%20constants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%2520Best-Arm%2520Identification%26entry.906535625%3DAchraf%2520Azize%2520and%2520Marc%2520Jourdan%2520and%2520Aymen%2520Al%2520Marjani%2520and%2520Debabrota%2520Basu%26entry.1292438233%3D%2520%2520Best%2520Arm%2520Identification%2520%2528BAI%2529%2520problems%2520are%2520progressively%2520used%2520for%250Adata-sensitive%2520applications%252C%2520such%2520as%2520designing%2520adaptive%2520clinical%2520trials%252C%2520tuning%250Ahyper-parameters%252C%2520and%2520conducting%2520user%2520studies.%2520Motivated%2520by%2520the%2520data%2520privacy%250Aconcerns%2520invoked%2520by%2520these%2520applications%252C%2520we%2520study%2520the%2520problem%2520of%2520BAI%2520with%2520fixed%250Aconfidence%2520in%2520both%2520the%2520local%2520and%2520central%2520models%252C%2520i.e.%2520%2524%255Cepsilon%2524-local%2520and%250A%2524%255Cepsilon%2524-global%2520Differential%2520Privacy%2520%2528DP%2529.%2520First%252C%2520to%2520quantify%2520the%2520cost%2520of%250Aprivacy%252C%2520we%2520derive%2520lower%2520bounds%2520on%2520the%2520sample%2520complexity%2520of%2520any%250A%2524%255Cdelta%2524-correct%2520BAI%2520algorithm%2520satisfying%2520%2524%255Cepsilon%2524-global%2520DP%2520or%250A%2524%255Cepsilon%2524-local%2520DP.%2520Our%2520lower%2520bounds%2520suggest%2520the%2520existence%2520of%2520two%2520privacy%250Aregimes.%2520In%2520the%2520high-privacy%2520regime%252C%2520the%2520hardness%2520depends%2520on%2520a%2520coupled%2520effect%250Aof%2520privacy%2520and%2520novel%2520information-theoretic%2520quantities%2520involving%2520the%2520Total%250AVariation.%2520In%2520the%2520low-privacy%2520regime%252C%2520the%2520lower%2520bounds%2520reduce%2520to%2520the%250Anon-private%2520lower%2520bounds.%2520We%2520propose%2520%2524%255Cepsilon%2524-local%2520DP%2520and%2520%2524%255Cepsilon%2524-global%250ADP%2520variants%2520of%2520a%2520Top%2520Two%2520algorithm%252C%2520namely%2520CTB-TT%2520and%2520AdaP-TT%252A%252C%2520respectively.%250AFor%2520%2524%255Cepsilon%2524-local%2520DP%252C%2520CTB-TT%2520is%2520asymptotically%2520optimal%2520by%2520plugging%2520in%2520a%250Aprivate%2520estimator%2520of%2520the%2520means%2520based%2520on%2520Randomised%2520Response.%2520For%250A%2524%255Cepsilon%2524-global%2520DP%252C%2520our%2520private%2520estimator%2520of%2520the%2520mean%2520runs%2520in%2520arm-dependent%250Aadaptive%2520episodes%2520and%2520adds%2520Laplace%2520noise%2520to%2520ensure%2520a%2520good%2520privacy-utility%250Atrade-off.%2520By%2520adapting%2520the%2520transportation%2520costs%252C%2520the%2520expected%2520sample%2520complexity%250Aof%2520AdaP-TT%252A%2520reaches%2520the%2520asymptotic%2520lower%2520bound%2520up%2520to%2520multiplicative%2520constants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Best-Arm%20Identification&entry.906535625=Achraf%20Azize%20and%20Marc%20Jourdan%20and%20Aymen%20Al%20Marjani%20and%20Debabrota%20Basu&entry.1292438233=%20%20Best%20Arm%20Identification%20%28BAI%29%20problems%20are%20progressively%20used%20for%0Adata-sensitive%20applications%2C%20such%20as%20designing%20adaptive%20clinical%20trials%2C%20tuning%0Ahyper-parameters%2C%20and%20conducting%20user%20studies.%20Motivated%20by%20the%20data%20privacy%0Aconcerns%20invoked%20by%20these%20applications%2C%20we%20study%20the%20problem%20of%20BAI%20with%20fixed%0Aconfidence%20in%20both%20the%20local%20and%20central%20models%2C%20i.e.%20%24%5Cepsilon%24-local%20and%0A%24%5Cepsilon%24-global%20Differential%20Privacy%20%28DP%29.%20First%2C%20to%20quantify%20the%20cost%20of%0Aprivacy%2C%20we%20derive%20lower%20bounds%20on%20the%20sample%20complexity%20of%20any%0A%24%5Cdelta%24-correct%20BAI%20algorithm%20satisfying%20%24%5Cepsilon%24-global%20DP%20or%0A%24%5Cepsilon%24-local%20DP.%20Our%20lower%20bounds%20suggest%20the%20existence%20of%20two%20privacy%0Aregimes.%20In%20the%20high-privacy%20regime%2C%20the%20hardness%20depends%20on%20a%20coupled%20effect%0Aof%20privacy%20and%20novel%20information-theoretic%20quantities%20involving%20the%20Total%0AVariation.%20In%20the%20low-privacy%20regime%2C%20the%20lower%20bounds%20reduce%20to%20the%0Anon-private%20lower%20bounds.%20We%20propose%20%24%5Cepsilon%24-local%20DP%20and%20%24%5Cepsilon%24-global%0ADP%20variants%20of%20a%20Top%20Two%20algorithm%2C%20namely%20CTB-TT%20and%20AdaP-TT%2A%2C%20respectively.%0AFor%20%24%5Cepsilon%24-local%20DP%2C%20CTB-TT%20is%20asymptotically%20optimal%20by%20plugging%20in%20a%0Aprivate%20estimator%20of%20the%20means%20based%20on%20Randomised%20Response.%20For%0A%24%5Cepsilon%24-global%20DP%2C%20our%20private%20estimator%20of%20the%20mean%20runs%20in%20arm-dependent%0Aadaptive%20episodes%20and%20adds%20Laplace%20noise%20to%20ensure%20a%20good%20privacy-utility%0Atrade-off.%20By%20adapting%20the%20transportation%20costs%2C%20the%20expected%20sample%20complexity%0Aof%20AdaP-TT%2A%20reaches%20the%20asymptotic%20lower%20bound%20up%20to%20multiplicative%20constants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06408v1&entry.124074799=Read"},
{"title": "When is Multicalibration Post-Processing Necessary?", "author": "Dutch Hansen and Siddartha Devic and Preetum Nakkiran and Vatsal Sharan", "abstract": "  Calibration is a well-studied property of predictors which guarantees\nmeaningful uncertainty estimates. Multicalibration is a related notion --\noriginating in algorithmic fairness -- which requires predictors to be\nsimultaneously calibrated over a potentially complex and overlapping collection\nof protected subpopulations (such as groups defined by ethnicity, race, or\nincome). We conduct the first comprehensive study evaluating the usefulness of\nmulticalibration post-processing across a broad set of tabular, image, and\nlanguage datasets for models spanning from simple decision trees to 90 million\nparameter fine-tuned LLMs. Our findings can be summarized as follows: (1)\nmodels which are calibrated out of the box tend to be relatively\nmulticalibrated without any additional post-processing; (2) multicalibration\npost-processing can help inherently uncalibrated models; and (3) traditional\ncalibration measures may sometimes provide multicalibration implicitly. More\ngenerally, we also distill many independent observations which may be useful\nfor practical and effective applications of multicalibration post-processing in\nreal-world contexts.\n", "link": "http://arxiv.org/abs/2406.06487v1", "date": "2024-06-10", "relevancy": 1.4083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4714}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20is%20Multicalibration%20Post-Processing%20Necessary%3F&body=Title%3A%20When%20is%20Multicalibration%20Post-Processing%20Necessary%3F%0AAuthor%3A%20Dutch%20Hansen%20and%20Siddartha%20Devic%20and%20Preetum%20Nakkiran%20and%20Vatsal%20Sharan%0AAbstract%3A%20%20%20Calibration%20is%20a%20well-studied%20property%20of%20predictors%20which%20guarantees%0Ameaningful%20uncertainty%20estimates.%20Multicalibration%20is%20a%20related%20notion%20--%0Aoriginating%20in%20algorithmic%20fairness%20--%20which%20requires%20predictors%20to%20be%0Asimultaneously%20calibrated%20over%20a%20potentially%20complex%20and%20overlapping%20collection%0Aof%20protected%20subpopulations%20%28such%20as%20groups%20defined%20by%20ethnicity%2C%20race%2C%20or%0Aincome%29.%20We%20conduct%20the%20first%20comprehensive%20study%20evaluating%20the%20usefulness%20of%0Amulticalibration%20post-processing%20across%20a%20broad%20set%20of%20tabular%2C%20image%2C%20and%0Alanguage%20datasets%20for%20models%20spanning%20from%20simple%20decision%20trees%20to%2090%20million%0Aparameter%20fine-tuned%20LLMs.%20Our%20findings%20can%20be%20summarized%20as%20follows%3A%20%281%29%0Amodels%20which%20are%20calibrated%20out%20of%20the%20box%20tend%20to%20be%20relatively%0Amulticalibrated%20without%20any%20additional%20post-processing%3B%20%282%29%20multicalibration%0Apost-processing%20can%20help%20inherently%20uncalibrated%20models%3B%20and%20%283%29%20traditional%0Acalibration%20measures%20may%20sometimes%20provide%20multicalibration%20implicitly.%20More%0Agenerally%2C%20we%20also%20distill%20many%20independent%20observations%20which%20may%20be%20useful%0Afor%20practical%20and%20effective%20applications%20of%20multicalibration%20post-processing%20in%0Areal-world%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520is%2520Multicalibration%2520Post-Processing%2520Necessary%253F%26entry.906535625%3DDutch%2520Hansen%2520and%2520Siddartha%2520Devic%2520and%2520Preetum%2520Nakkiran%2520and%2520Vatsal%2520Sharan%26entry.1292438233%3D%2520%2520Calibration%2520is%2520a%2520well-studied%2520property%2520of%2520predictors%2520which%2520guarantees%250Ameaningful%2520uncertainty%2520estimates.%2520Multicalibration%2520is%2520a%2520related%2520notion%2520--%250Aoriginating%2520in%2520algorithmic%2520fairness%2520--%2520which%2520requires%2520predictors%2520to%2520be%250Asimultaneously%2520calibrated%2520over%2520a%2520potentially%2520complex%2520and%2520overlapping%2520collection%250Aof%2520protected%2520subpopulations%2520%2528such%2520as%2520groups%2520defined%2520by%2520ethnicity%252C%2520race%252C%2520or%250Aincome%2529.%2520We%2520conduct%2520the%2520first%2520comprehensive%2520study%2520evaluating%2520the%2520usefulness%2520of%250Amulticalibration%2520post-processing%2520across%2520a%2520broad%2520set%2520of%2520tabular%252C%2520image%252C%2520and%250Alanguage%2520datasets%2520for%2520models%2520spanning%2520from%2520simple%2520decision%2520trees%2520to%252090%2520million%250Aparameter%2520fine-tuned%2520LLMs.%2520Our%2520findings%2520can%2520be%2520summarized%2520as%2520follows%253A%2520%25281%2529%250Amodels%2520which%2520are%2520calibrated%2520out%2520of%2520the%2520box%2520tend%2520to%2520be%2520relatively%250Amulticalibrated%2520without%2520any%2520additional%2520post-processing%253B%2520%25282%2529%2520multicalibration%250Apost-processing%2520can%2520help%2520inherently%2520uncalibrated%2520models%253B%2520and%2520%25283%2529%2520traditional%250Acalibration%2520measures%2520may%2520sometimes%2520provide%2520multicalibration%2520implicitly.%2520More%250Agenerally%252C%2520we%2520also%2520distill%2520many%2520independent%2520observations%2520which%2520may%2520be%2520useful%250Afor%2520practical%2520and%2520effective%2520applications%2520of%2520multicalibration%2520post-processing%2520in%250Areal-world%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20is%20Multicalibration%20Post-Processing%20Necessary%3F&entry.906535625=Dutch%20Hansen%20and%20Siddartha%20Devic%20and%20Preetum%20Nakkiran%20and%20Vatsal%20Sharan&entry.1292438233=%20%20Calibration%20is%20a%20well-studied%20property%20of%20predictors%20which%20guarantees%0Ameaningful%20uncertainty%20estimates.%20Multicalibration%20is%20a%20related%20notion%20--%0Aoriginating%20in%20algorithmic%20fairness%20--%20which%20requires%20predictors%20to%20be%0Asimultaneously%20calibrated%20over%20a%20potentially%20complex%20and%20overlapping%20collection%0Aof%20protected%20subpopulations%20%28such%20as%20groups%20defined%20by%20ethnicity%2C%20race%2C%20or%0Aincome%29.%20We%20conduct%20the%20first%20comprehensive%20study%20evaluating%20the%20usefulness%20of%0Amulticalibration%20post-processing%20across%20a%20broad%20set%20of%20tabular%2C%20image%2C%20and%0Alanguage%20datasets%20for%20models%20spanning%20from%20simple%20decision%20trees%20to%2090%20million%0Aparameter%20fine-tuned%20LLMs.%20Our%20findings%20can%20be%20summarized%20as%20follows%3A%20%281%29%0Amodels%20which%20are%20calibrated%20out%20of%20the%20box%20tend%20to%20be%20relatively%0Amulticalibrated%20without%20any%20additional%20post-processing%3B%20%282%29%20multicalibration%0Apost-processing%20can%20help%20inherently%20uncalibrated%20models%3B%20and%20%283%29%20traditional%0Acalibration%20measures%20may%20sometimes%20provide%20multicalibration%20implicitly.%20More%0Agenerally%2C%20we%20also%20distill%20many%20independent%20observations%20which%20may%20be%20useful%0Afor%20practical%20and%20effective%20applications%20of%20multicalibration%20post-processing%20in%0Areal-world%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06487v1&entry.124074799=Read"},
{"title": "Thunder : Unified Regression-Diffusion Speech Enhancement with a Single\n  Reverse Step using Brownian Bridge", "author": "Thanapat Trachu and Chawan Piansaddhayanon and Ekapol Chuangsuwanich", "abstract": "  Diffusion-based speech enhancement has shown promising results, but can\nsuffer from a slower inference time. Initializing the diffusion process with\nthe enhanced audio generated by a regression-based model can be used to reduce\nthe computational steps required. However, these approaches often necessitate a\nregression model, further increasing the system's complexity. We propose\nThunder, a unified regression-diffusion model that utilizes the Brownian bridge\nprocess which can allow the model to act in both modes. The regression mode can\nbe accessed by setting the diffusion time step closed to 1. However, the\nstandard score-based diffusion modeling does not perform well in this setup due\nto gradient instability. To mitigate this problem, we modify the diffusion\nmodel to predict the clean speech instead of the score function, achieving\ncompetitive performance with a more compact model size and fewer reverse steps.\n", "link": "http://arxiv.org/abs/2406.06139v1", "date": "2024-06-10", "relevancy": 1.6001, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5753}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5328}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thunder%20%3A%20Unified%20Regression-Diffusion%20Speech%20Enhancement%20with%20a%20Single%0A%20%20Reverse%20Step%20using%20Brownian%20Bridge&body=Title%3A%20Thunder%20%3A%20Unified%20Regression-Diffusion%20Speech%20Enhancement%20with%20a%20Single%0A%20%20Reverse%20Step%20using%20Brownian%20Bridge%0AAuthor%3A%20Thanapat%20Trachu%20and%20Chawan%20Piansaddhayanon%20and%20Ekapol%20Chuangsuwanich%0AAbstract%3A%20%20%20Diffusion-based%20speech%20enhancement%20has%20shown%20promising%20results%2C%20but%20can%0Asuffer%20from%20a%20slower%20inference%20time.%20Initializing%20the%20diffusion%20process%20with%0Athe%20enhanced%20audio%20generated%20by%20a%20regression-based%20model%20can%20be%20used%20to%20reduce%0Athe%20computational%20steps%20required.%20However%2C%20these%20approaches%20often%20necessitate%20a%0Aregression%20model%2C%20further%20increasing%20the%20system%27s%20complexity.%20We%20propose%0AThunder%2C%20a%20unified%20regression-diffusion%20model%20that%20utilizes%20the%20Brownian%20bridge%0Aprocess%20which%20can%20allow%20the%20model%20to%20act%20in%20both%20modes.%20The%20regression%20mode%20can%0Abe%20accessed%20by%20setting%20the%20diffusion%20time%20step%20closed%20to%201.%20However%2C%20the%0Astandard%20score-based%20diffusion%20modeling%20does%20not%20perform%20well%20in%20this%20setup%20due%0Ato%20gradient%20instability.%20To%20mitigate%20this%20problem%2C%20we%20modify%20the%20diffusion%0Amodel%20to%20predict%20the%20clean%20speech%20instead%20of%20the%20score%20function%2C%20achieving%0Acompetitive%20performance%20with%20a%20more%20compact%20model%20size%20and%20fewer%20reverse%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThunder%2520%253A%2520Unified%2520Regression-Diffusion%2520Speech%2520Enhancement%2520with%2520a%2520Single%250A%2520%2520Reverse%2520Step%2520using%2520Brownian%2520Bridge%26entry.906535625%3DThanapat%2520Trachu%2520and%2520Chawan%2520Piansaddhayanon%2520and%2520Ekapol%2520Chuangsuwanich%26entry.1292438233%3D%2520%2520Diffusion-based%2520speech%2520enhancement%2520has%2520shown%2520promising%2520results%252C%2520but%2520can%250Asuffer%2520from%2520a%2520slower%2520inference%2520time.%2520Initializing%2520the%2520diffusion%2520process%2520with%250Athe%2520enhanced%2520audio%2520generated%2520by%2520a%2520regression-based%2520model%2520can%2520be%2520used%2520to%2520reduce%250Athe%2520computational%2520steps%2520required.%2520However%252C%2520these%2520approaches%2520often%2520necessitate%2520a%250Aregression%2520model%252C%2520further%2520increasing%2520the%2520system%2527s%2520complexity.%2520We%2520propose%250AThunder%252C%2520a%2520unified%2520regression-diffusion%2520model%2520that%2520utilizes%2520the%2520Brownian%2520bridge%250Aprocess%2520which%2520can%2520allow%2520the%2520model%2520to%2520act%2520in%2520both%2520modes.%2520The%2520regression%2520mode%2520can%250Abe%2520accessed%2520by%2520setting%2520the%2520diffusion%2520time%2520step%2520closed%2520to%25201.%2520However%252C%2520the%250Astandard%2520score-based%2520diffusion%2520modeling%2520does%2520not%2520perform%2520well%2520in%2520this%2520setup%2520due%250Ato%2520gradient%2520instability.%2520To%2520mitigate%2520this%2520problem%252C%2520we%2520modify%2520the%2520diffusion%250Amodel%2520to%2520predict%2520the%2520clean%2520speech%2520instead%2520of%2520the%2520score%2520function%252C%2520achieving%250Acompetitive%2520performance%2520with%2520a%2520more%2520compact%2520model%2520size%2520and%2520fewer%2520reverse%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thunder%20%3A%20Unified%20Regression-Diffusion%20Speech%20Enhancement%20with%20a%20Single%0A%20%20Reverse%20Step%20using%20Brownian%20Bridge&entry.906535625=Thanapat%20Trachu%20and%20Chawan%20Piansaddhayanon%20and%20Ekapol%20Chuangsuwanich&entry.1292438233=%20%20Diffusion-based%20speech%20enhancement%20has%20shown%20promising%20results%2C%20but%20can%0Asuffer%20from%20a%20slower%20inference%20time.%20Initializing%20the%20diffusion%20process%20with%0Athe%20enhanced%20audio%20generated%20by%20a%20regression-based%20model%20can%20be%20used%20to%20reduce%0Athe%20computational%20steps%20required.%20However%2C%20these%20approaches%20often%20necessitate%20a%0Aregression%20model%2C%20further%20increasing%20the%20system%27s%20complexity.%20We%20propose%0AThunder%2C%20a%20unified%20regression-diffusion%20model%20that%20utilizes%20the%20Brownian%20bridge%0Aprocess%20which%20can%20allow%20the%20model%20to%20act%20in%20both%20modes.%20The%20regression%20mode%20can%0Abe%20accessed%20by%20setting%20the%20diffusion%20time%20step%20closed%20to%201.%20However%2C%20the%0Astandard%20score-based%20diffusion%20modeling%20does%20not%20perform%20well%20in%20this%20setup%20due%0Ato%20gradient%20instability.%20To%20mitigate%20this%20problem%2C%20we%20modify%20the%20diffusion%0Amodel%20to%20predict%20the%20clean%20speech%20instead%20of%20the%20score%20function%2C%20achieving%0Acompetitive%20performance%20with%20a%20more%20compact%20model%20size%20and%20fewer%20reverse%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06139v1&entry.124074799=Read"},
{"title": "Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against\n  Personalized Federated Learning", "author": "Xiaoting Lyu and Yufei Han and Wei Wang and Jingkai Liu and Yongsheng Zhu and Guangquan Xu and Jiqiang Liu and Xiangliang Zhang", "abstract": "  Federated Learning (FL) is a collaborative machine learning technique where\nmultiple clients work together with a central server to train a global model\nwithout sharing their private data. However, the distribution shift across\nnon-IID datasets of clients poses a challenge to this one-model-fits-all method\nhindering the ability of the global model to effectively adapt to each client's\nunique local data. To echo this challenge, personalized FL (PFL) is designed to\nallow each client to create personalized local models tailored to their private\ndata. While extensive research has scrutinized backdoor risks in FL, it has\nremained underexplored in PFL applications. In this study, we delve deep into\nthe vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale\nof two cities. On the one hand, the personalization process in PFL can dilute\nthe backdoor poisoning effects injected into the personalized local models.\nFurthermore, PFL systems can also deploy both server-end and client-end defense\nmechanisms to strengthen the barrier against backdoor attacks. On the other\nhand, our study shows that PFL fortified with these defense methods may offer a\nfalse sense of security. We propose \\textit{PFedBA}, a stealthy and effective\nbackdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously\naligns the backdoor learning task with the main learning task of PFL by\noptimizing the trigger generation process. Our comprehensive experiments\ndemonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding\ntriggers into personalized local models. \\textit{PFedBA} yields outstanding\nattack performance across 10 state-of-the-art PFL algorithms, defeating the\nexisting 6 defense mechanisms. Our study sheds light on the subtle yet potent\nbackdoor threats to PFL systems, urging the community to bolster defenses\nagainst emerging backdoor challenges.\n", "link": "http://arxiv.org/abs/2406.06207v1", "date": "2024-06-10", "relevancy": 1.8572, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4796}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4564}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lurking%20in%20the%20shadows%3A%20Unveiling%20Stealthy%20Backdoor%20Attacks%20against%0A%20%20Personalized%20Federated%20Learning&body=Title%3A%20Lurking%20in%20the%20shadows%3A%20Unveiling%20Stealthy%20Backdoor%20Attacks%20against%0A%20%20Personalized%20Federated%20Learning%0AAuthor%3A%20Xiaoting%20Lyu%20and%20Yufei%20Han%20and%20Wei%20Wang%20and%20Jingkai%20Liu%20and%20Yongsheng%20Zhu%20and%20Guangquan%20Xu%20and%20Jiqiang%20Liu%20and%20Xiangliang%20Zhang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20machine%20learning%20technique%20where%0Amultiple%20clients%20work%20together%20with%20a%20central%20server%20to%20train%20a%20global%20model%0Awithout%20sharing%20their%20private%20data.%20However%2C%20the%20distribution%20shift%20across%0Anon-IID%20datasets%20of%20clients%20poses%20a%20challenge%20to%20this%20one-model-fits-all%20method%0Ahindering%20the%20ability%20of%20the%20global%20model%20to%20effectively%20adapt%20to%20each%20client%27s%0Aunique%20local%20data.%20To%20echo%20this%20challenge%2C%20personalized%20FL%20%28PFL%29%20is%20designed%20to%0Aallow%20each%20client%20to%20create%20personalized%20local%20models%20tailored%20to%20their%20private%0Adata.%20While%20extensive%20research%20has%20scrutinized%20backdoor%20risks%20in%20FL%2C%20it%20has%0Aremained%20underexplored%20in%20PFL%20applications.%20In%20this%20study%2C%20we%20delve%20deep%20into%0Athe%20vulnerabilities%20of%20PFL%20to%20backdoor%20attacks.%20Our%20analysis%20showcases%20a%20tale%0Aof%20two%20cities.%20On%20the%20one%20hand%2C%20the%20personalization%20process%20in%20PFL%20can%20dilute%0Athe%20backdoor%20poisoning%20effects%20injected%20into%20the%20personalized%20local%20models.%0AFurthermore%2C%20PFL%20systems%20can%20also%20deploy%20both%20server-end%20and%20client-end%20defense%0Amechanisms%20to%20strengthen%20the%20barrier%20against%20backdoor%20attacks.%20On%20the%20other%0Ahand%2C%20our%20study%20shows%20that%20PFL%20fortified%20with%20these%20defense%20methods%20may%20offer%20a%0Afalse%20sense%20of%20security.%20We%20propose%20%5Ctextit%7BPFedBA%7D%2C%20a%20stealthy%20and%20effective%0Abackdoor%20attack%20strategy%20applicable%20to%20PFL%20systems.%20%5Ctextit%7BPFedBA%7D%20ingeniously%0Aaligns%20the%20backdoor%20learning%20task%20with%20the%20main%20learning%20task%20of%20PFL%20by%0Aoptimizing%20the%20trigger%20generation%20process.%20Our%20comprehensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20%5Ctextit%7BPFedBA%7D%20in%20seamlessly%20embedding%0Atriggers%20into%20personalized%20local%20models.%20%5Ctextit%7BPFedBA%7D%20yields%20outstanding%0Aattack%20performance%20across%2010%20state-of-the-art%20PFL%20algorithms%2C%20defeating%20the%0Aexisting%206%20defense%20mechanisms.%20Our%20study%20sheds%20light%20on%20the%20subtle%20yet%20potent%0Abackdoor%20threats%20to%20PFL%20systems%2C%20urging%20the%20community%20to%20bolster%20defenses%0Aagainst%20emerging%20backdoor%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLurking%2520in%2520the%2520shadows%253A%2520Unveiling%2520Stealthy%2520Backdoor%2520Attacks%2520against%250A%2520%2520Personalized%2520Federated%2520Learning%26entry.906535625%3DXiaoting%2520Lyu%2520and%2520Yufei%2520Han%2520and%2520Wei%2520Wang%2520and%2520Jingkai%2520Liu%2520and%2520Yongsheng%2520Zhu%2520and%2520Guangquan%2520Xu%2520and%2520Jiqiang%2520Liu%2520and%2520Xiangliang%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520collaborative%2520machine%2520learning%2520technique%2520where%250Amultiple%2520clients%2520work%2520together%2520with%2520a%2520central%2520server%2520to%2520train%2520a%2520global%2520model%250Awithout%2520sharing%2520their%2520private%2520data.%2520However%252C%2520the%2520distribution%2520shift%2520across%250Anon-IID%2520datasets%2520of%2520clients%2520poses%2520a%2520challenge%2520to%2520this%2520one-model-fits-all%2520method%250Ahindering%2520the%2520ability%2520of%2520the%2520global%2520model%2520to%2520effectively%2520adapt%2520to%2520each%2520client%2527s%250Aunique%2520local%2520data.%2520To%2520echo%2520this%2520challenge%252C%2520personalized%2520FL%2520%2528PFL%2529%2520is%2520designed%2520to%250Aallow%2520each%2520client%2520to%2520create%2520personalized%2520local%2520models%2520tailored%2520to%2520their%2520private%250Adata.%2520While%2520extensive%2520research%2520has%2520scrutinized%2520backdoor%2520risks%2520in%2520FL%252C%2520it%2520has%250Aremained%2520underexplored%2520in%2520PFL%2520applications.%2520In%2520this%2520study%252C%2520we%2520delve%2520deep%2520into%250Athe%2520vulnerabilities%2520of%2520PFL%2520to%2520backdoor%2520attacks.%2520Our%2520analysis%2520showcases%2520a%2520tale%250Aof%2520two%2520cities.%2520On%2520the%2520one%2520hand%252C%2520the%2520personalization%2520process%2520in%2520PFL%2520can%2520dilute%250Athe%2520backdoor%2520poisoning%2520effects%2520injected%2520into%2520the%2520personalized%2520local%2520models.%250AFurthermore%252C%2520PFL%2520systems%2520can%2520also%2520deploy%2520both%2520server-end%2520and%2520client-end%2520defense%250Amechanisms%2520to%2520strengthen%2520the%2520barrier%2520against%2520backdoor%2520attacks.%2520On%2520the%2520other%250Ahand%252C%2520our%2520study%2520shows%2520that%2520PFL%2520fortified%2520with%2520these%2520defense%2520methods%2520may%2520offer%2520a%250Afalse%2520sense%2520of%2520security.%2520We%2520propose%2520%255Ctextit%257BPFedBA%257D%252C%2520a%2520stealthy%2520and%2520effective%250Abackdoor%2520attack%2520strategy%2520applicable%2520to%2520PFL%2520systems.%2520%255Ctextit%257BPFedBA%257D%2520ingeniously%250Aaligns%2520the%2520backdoor%2520learning%2520task%2520with%2520the%2520main%2520learning%2520task%2520of%2520PFL%2520by%250Aoptimizing%2520the%2520trigger%2520generation%2520process.%2520Our%2520comprehensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520%255Ctextit%257BPFedBA%257D%2520in%2520seamlessly%2520embedding%250Atriggers%2520into%2520personalized%2520local%2520models.%2520%255Ctextit%257BPFedBA%257D%2520yields%2520outstanding%250Aattack%2520performance%2520across%252010%2520state-of-the-art%2520PFL%2520algorithms%252C%2520defeating%2520the%250Aexisting%25206%2520defense%2520mechanisms.%2520Our%2520study%2520sheds%2520light%2520on%2520the%2520subtle%2520yet%2520potent%250Abackdoor%2520threats%2520to%2520PFL%2520systems%252C%2520urging%2520the%2520community%2520to%2520bolster%2520defenses%250Aagainst%2520emerging%2520backdoor%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lurking%20in%20the%20shadows%3A%20Unveiling%20Stealthy%20Backdoor%20Attacks%20against%0A%20%20Personalized%20Federated%20Learning&entry.906535625=Xiaoting%20Lyu%20and%20Yufei%20Han%20and%20Wei%20Wang%20and%20Jingkai%20Liu%20and%20Yongsheng%20Zhu%20and%20Guangquan%20Xu%20and%20Jiqiang%20Liu%20and%20Xiangliang%20Zhang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20machine%20learning%20technique%20where%0Amultiple%20clients%20work%20together%20with%20a%20central%20server%20to%20train%20a%20global%20model%0Awithout%20sharing%20their%20private%20data.%20However%2C%20the%20distribution%20shift%20across%0Anon-IID%20datasets%20of%20clients%20poses%20a%20challenge%20to%20this%20one-model-fits-all%20method%0Ahindering%20the%20ability%20of%20the%20global%20model%20to%20effectively%20adapt%20to%20each%20client%27s%0Aunique%20local%20data.%20To%20echo%20this%20challenge%2C%20personalized%20FL%20%28PFL%29%20is%20designed%20to%0Aallow%20each%20client%20to%20create%20personalized%20local%20models%20tailored%20to%20their%20private%0Adata.%20While%20extensive%20research%20has%20scrutinized%20backdoor%20risks%20in%20FL%2C%20it%20has%0Aremained%20underexplored%20in%20PFL%20applications.%20In%20this%20study%2C%20we%20delve%20deep%20into%0Athe%20vulnerabilities%20of%20PFL%20to%20backdoor%20attacks.%20Our%20analysis%20showcases%20a%20tale%0Aof%20two%20cities.%20On%20the%20one%20hand%2C%20the%20personalization%20process%20in%20PFL%20can%20dilute%0Athe%20backdoor%20poisoning%20effects%20injected%20into%20the%20personalized%20local%20models.%0AFurthermore%2C%20PFL%20systems%20can%20also%20deploy%20both%20server-end%20and%20client-end%20defense%0Amechanisms%20to%20strengthen%20the%20barrier%20against%20backdoor%20attacks.%20On%20the%20other%0Ahand%2C%20our%20study%20shows%20that%20PFL%20fortified%20with%20these%20defense%20methods%20may%20offer%20a%0Afalse%20sense%20of%20security.%20We%20propose%20%5Ctextit%7BPFedBA%7D%2C%20a%20stealthy%20and%20effective%0Abackdoor%20attack%20strategy%20applicable%20to%20PFL%20systems.%20%5Ctextit%7BPFedBA%7D%20ingeniously%0Aaligns%20the%20backdoor%20learning%20task%20with%20the%20main%20learning%20task%20of%20PFL%20by%0Aoptimizing%20the%20trigger%20generation%20process.%20Our%20comprehensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20%5Ctextit%7BPFedBA%7D%20in%20seamlessly%20embedding%0Atriggers%20into%20personalized%20local%20models.%20%5Ctextit%7BPFedBA%7D%20yields%20outstanding%0Aattack%20performance%20across%2010%20state-of-the-art%20PFL%20algorithms%2C%20defeating%20the%0Aexisting%206%20defense%20mechanisms.%20Our%20study%20sheds%20light%20on%20the%20subtle%20yet%20potent%0Abackdoor%20threats%20to%20PFL%20systems%2C%20urging%20the%20community%20to%20bolster%20defenses%0Aagainst%20emerging%20backdoor%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06207v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


