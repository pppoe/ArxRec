<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240422.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Scene Coordinate Reconstruction: Posing of Image Collections via\n  Incremental Learning of a Relocalizer", "author": "Eric Brachmann and Jamie Wynn and Shuai Chen and Tommaso Cavallari and \u00c1ron Monszpart and Daniyar Turmukhambetov and Victor Adrian Prisacariu", "abstract": "  We address the task of estimating camera parameters from a set of images\ndepicting a scene. Popular feature-based structure-from-motion (SfM) tools\nsolve this task by incremental reconstruction: they repeat triangulation of\nsparse 3D points and registration of more camera views to the sparse point\ncloud. We re-interpret incremental structure-from-motion as an iterated\napplication and refinement of a visual relocalizer, that is, of a method that\nregisters new views to the current state of the reconstruction. This\nperspective allows us to investigate alternative visual relocalizers that are\nnot rooted in local feature matching. We show that scene coordinate regression,\na learning-based relocalization approach, allows us to build implicit, neural\nscene representations from unposed images. Different from other learning-based\nreconstruction methods, we do not require pose priors nor sequential inputs,\nand we optimize efficiently over thousands of images. Our method, ACE0 (ACE\nZero), estimates camera poses to an accuracy comparable to feature-based SfM,\nas demonstrated by novel view synthesis. Project page:\nhttps://nianticlabs.github.io/acezero/\n", "link": "http://arxiv.org/abs/2404.14351v1", "date": "2024-04-22", "relevancy": 2.8496, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5721}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer&body=Title%3A%20Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer%0AAuthor%3A%20Eric%20Brachmann%20and%20Jamie%20Wynn%20and%20Shuai%20Chen%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Daniyar%20Turmukhambetov%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20address%20the%20task%20of%20estimating%20camera%20parameters%20from%20a%20set%20of%20images%0Adepicting%20a%20scene.%20Popular%20feature-based%20structure-from-motion%20%28SfM%29%20tools%0Asolve%20this%20task%20by%20incremental%20reconstruction%3A%20they%20repeat%20triangulation%20of%0Asparse%203D%20points%20and%20registration%20of%20more%20camera%20views%20to%20the%20sparse%20point%0Acloud.%20We%20re-interpret%20incremental%20structure-from-motion%20as%20an%20iterated%0Aapplication%20and%20refinement%20of%20a%20visual%20relocalizer%2C%20that%20is%2C%20of%20a%20method%20that%0Aregisters%20new%20views%20to%20the%20current%20state%20of%20the%20reconstruction.%20This%0Aperspective%20allows%20us%20to%20investigate%20alternative%20visual%20relocalizers%20that%20are%0Anot%20rooted%20in%20local%20feature%20matching.%20We%20show%20that%20scene%20coordinate%20regression%2C%0Aa%20learning-based%20relocalization%20approach%2C%20allows%20us%20to%20build%20implicit%2C%20neural%0Ascene%20representations%20from%20unposed%20images.%20Different%20from%20other%20learning-based%0Areconstruction%20methods%2C%20we%20do%20not%20require%20pose%20priors%20nor%20sequential%20inputs%2C%0Aand%20we%20optimize%20efficiently%20over%20thousands%20of%20images.%20Our%20method%2C%20ACE0%20%28ACE%0AZero%29%2C%20estimates%20camera%20poses%20to%20an%20accuracy%20comparable%20to%20feature-based%20SfM%2C%0Aas%20demonstrated%20by%20novel%20view%20synthesis.%20Project%20page%3A%0Ahttps%3A//nianticlabs.github.io/acezero/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14351v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer&entry.906535625=Eric%20Brachmann%20and%20Jamie%20Wynn%20and%20Shuai%20Chen%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Daniyar%20Turmukhambetov%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20address%20the%20task%20of%20estimating%20camera%20parameters%20from%20a%20set%20of%20images%0Adepicting%20a%20scene.%20Popular%20feature-based%20structure-from-motion%20%28SfM%29%20tools%0Asolve%20this%20task%20by%20incremental%20reconstruction%3A%20they%20repeat%20triangulation%20of%0Asparse%203D%20points%20and%20registration%20of%20more%20camera%20views%20to%20the%20sparse%20point%0Acloud.%20We%20re-interpret%20incremental%20structure-from-motion%20as%20an%20iterated%0Aapplication%20and%20refinement%20of%20a%20visual%20relocalizer%2C%20that%20is%2C%20of%20a%20method%20that%0Aregisters%20new%20views%20to%20the%20current%20state%20of%20the%20reconstruction.%20This%0Aperspective%20allows%20us%20to%20investigate%20alternative%20visual%20relocalizers%20that%20are%0Anot%20rooted%20in%20local%20feature%20matching.%20We%20show%20that%20scene%20coordinate%20regression%2C%0Aa%20learning-based%20relocalization%20approach%2C%20allows%20us%20to%20build%20implicit%2C%20neural%0Ascene%20representations%20from%20unposed%20images.%20Different%20from%20other%20learning-based%0Areconstruction%20methods%2C%20we%20do%20not%20require%20pose%20priors%20nor%20sequential%20inputs%2C%0Aand%20we%20optimize%20efficiently%20over%20thousands%20of%20images.%20Our%20method%2C%20ACE0%20%28ACE%0AZero%29%2C%20estimates%20camera%20poses%20to%20an%20accuracy%20comparable%20to%20feature-based%20SfM%2C%0Aas%20demonstrated%20by%20novel%20view%20synthesis.%20Project%20page%3A%0Ahttps%3A//nianticlabs.github.io/acezero/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14351v1&entry.124074799=Read"},
{"title": "Seeing Text in the Dark: Algorithm and Benchmark", "author": "Chengpei Xu and Hao Fu and Long Ma and Wenjing Jia and Chengqi Zhang and Feng Xia and Xiaoyu Ai and Binghao Li and Wenjie Zhang", "abstract": "  Localizing text in low-light environments is challenging due to visual\ndegradations. Although a straightforward solution involves a two-stage pipeline\nwith low-light image enhancement (LLE) as the initial step followed by\ndetector, LLE is primarily designed for human vision instead of machine and can\naccumulate errors. In this work, we propose an efficient and effective\nsingle-stage approach for localizing text in dark that circumvents the need for\nLLE. We introduce a constrained learning module as an auxiliary mechanism\nduring the training stage of the text detector. This module is designed to\nguide the text detector in preserving textual spatial features amidst feature\nmap resizing, thus minimizing the loss of spatial information in texts under\nlow-light visual degradations. Specifically, we incorporate spatial\nreconstruction and spatial semantic constraints within this module to ensure\nthe text detector acquires essential positional and contextual range knowledge.\nOur approach enhances the original text detector's ability to identify text's\nlocal topological features using a dynamic snake feature pyramid network and\nadopts a bottom-up contour shaping strategy with a novel rectangular\naccumulation technique for accurate delineation of streamlined text features.\nIn addition, we present a comprehensive low-light dataset for arbitrary-shaped\ntext, encompassing diverse scenes and languages. Notably, our method achieves\nstate-of-the-art results on this low-light dataset and exhibits comparable\nperformance on standard normal light datasets. The code and dataset will be\nreleased.\n", "link": "http://arxiv.org/abs/2404.08965v2", "date": "2024-04-22", "relevancy": 2.838, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5638}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Seeing%20Text%20in%20the%20Dark%3A%20Algorithm%20and%20Benchmark&body=Title%3A%20Seeing%20Text%20in%20the%20Dark%3A%20Algorithm%20and%20Benchmark%0AAuthor%3A%20Chengpei%20Xu%20and%20Hao%20Fu%20and%20Long%20Ma%20and%20Wenjing%20Jia%20and%20Chengqi%20Zhang%20and%20Feng%20Xia%20and%20Xiaoyu%20Ai%20and%20Binghao%20Li%20and%20Wenjie%20Zhang%0AAbstract%3A%20%20%20Localizing%20text%20in%20low-light%20environments%20is%20challenging%20due%20to%20visual%0Adegradations.%20Although%20a%20straightforward%20solution%20involves%20a%20two-stage%20pipeline%0Awith%20low-light%20image%20enhancement%20%28LLE%29%20as%20the%20initial%20step%20followed%20by%0Adetector%2C%20LLE%20is%20primarily%20designed%20for%20human%20vision%20instead%20of%20machine%20and%20can%0Aaccumulate%20errors.%20In%20this%20work%2C%20we%20propose%20an%20efficient%20and%20effective%0Asingle-stage%20approach%20for%20localizing%20text%20in%20dark%20that%20circumvents%20the%20need%20for%0ALLE.%20We%20introduce%20a%20constrained%20learning%20module%20as%20an%20auxiliary%20mechanism%0Aduring%20the%20training%20stage%20of%20the%20text%20detector.%20This%20module%20is%20designed%20to%0Aguide%20the%20text%20detector%20in%20preserving%20textual%20spatial%20features%20amidst%20feature%0Amap%20resizing%2C%20thus%20minimizing%20the%20loss%20of%20spatial%20information%20in%20texts%20under%0Alow-light%20visual%20degradations.%20Specifically%2C%20we%20incorporate%20spatial%0Areconstruction%20and%20spatial%20semantic%20constraints%20within%20this%20module%20to%20ensure%0Athe%20text%20detector%20acquires%20essential%20positional%20and%20contextual%20range%20knowledge.%0AOur%20approach%20enhances%20the%20original%20text%20detector%27s%20ability%20to%20identify%20text%27s%0Alocal%20topological%20features%20using%20a%20dynamic%20snake%20feature%20pyramid%20network%20and%0Aadopts%20a%20bottom-up%20contour%20shaping%20strategy%20with%20a%20novel%20rectangular%0Aaccumulation%20technique%20for%20accurate%20delineation%20of%20streamlined%20text%20features.%0AIn%20addition%2C%20we%20present%20a%20comprehensive%20low-light%20dataset%20for%20arbitrary-shaped%0Atext%2C%20encompassing%20diverse%20scenes%20and%20languages.%20Notably%2C%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20this%20low-light%20dataset%20and%20exhibits%20comparable%0Aperformance%20on%20standard%20normal%20light%20datasets.%20The%20code%20and%20dataset%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08965v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Text%20in%20the%20Dark%3A%20Algorithm%20and%20Benchmark&entry.906535625=Chengpei%20Xu%20and%20Hao%20Fu%20and%20Long%20Ma%20and%20Wenjing%20Jia%20and%20Chengqi%20Zhang%20and%20Feng%20Xia%20and%20Xiaoyu%20Ai%20and%20Binghao%20Li%20and%20Wenjie%20Zhang&entry.1292438233=%20%20Localizing%20text%20in%20low-light%20environments%20is%20challenging%20due%20to%20visual%0Adegradations.%20Although%20a%20straightforward%20solution%20involves%20a%20two-stage%20pipeline%0Awith%20low-light%20image%20enhancement%20%28LLE%29%20as%20the%20initial%20step%20followed%20by%0Adetector%2C%20LLE%20is%20primarily%20designed%20for%20human%20vision%20instead%20of%20machine%20and%20can%0Aaccumulate%20errors.%20In%20this%20work%2C%20we%20propose%20an%20efficient%20and%20effective%0Asingle-stage%20approach%20for%20localizing%20text%20in%20dark%20that%20circumvents%20the%20need%20for%0ALLE.%20We%20introduce%20a%20constrained%20learning%20module%20as%20an%20auxiliary%20mechanism%0Aduring%20the%20training%20stage%20of%20the%20text%20detector.%20This%20module%20is%20designed%20to%0Aguide%20the%20text%20detector%20in%20preserving%20textual%20spatial%20features%20amidst%20feature%0Amap%20resizing%2C%20thus%20minimizing%20the%20loss%20of%20spatial%20information%20in%20texts%20under%0Alow-light%20visual%20degradations.%20Specifically%2C%20we%20incorporate%20spatial%0Areconstruction%20and%20spatial%20semantic%20constraints%20within%20this%20module%20to%20ensure%0Athe%20text%20detector%20acquires%20essential%20positional%20and%20contextual%20range%20knowledge.%0AOur%20approach%20enhances%20the%20original%20text%20detector%27s%20ability%20to%20identify%20text%27s%0Alocal%20topological%20features%20using%20a%20dynamic%20snake%20feature%20pyramid%20network%20and%0Aadopts%20a%20bottom-up%20contour%20shaping%20strategy%20with%20a%20novel%20rectangular%0Aaccumulation%20technique%20for%20accurate%20delineation%20of%20streamlined%20text%20features.%0AIn%20addition%2C%20we%20present%20a%20comprehensive%20low-light%20dataset%20for%20arbitrary-shaped%0Atext%2C%20encompassing%20diverse%20scenes%20and%20languages.%20Notably%2C%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20this%20low-light%20dataset%20and%20exhibits%20comparable%0Aperformance%20on%20standard%20normal%20light%20datasets.%20The%20code%20and%20dataset%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08965v2&entry.124074799=Read"},
{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "author": "Isabella Liu and Hao Su and Xiaolong Wang", "abstract": "  Modern 3D engines and graphics pipelines require mesh as a memory-efficient\nrepresentation, which allows efficient rendering, geometry processing, texture\nediting, and many other downstream operations. However, it is still highly\ndifficult to obtain high-quality mesh in terms of structure and detail from\nmonocular visual observations. The problem becomes even more challenging for\ndynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh\n(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh\ngiven a single monocular video. Our work leverages the recent advancement in 3D\nGaussian Splatting to construct the mesh sequence with temporal consistency\nfrom a video. Building on top of this representation, DG-Mesh recovers\nhigh-quality meshes from the Gaussian points and can track the mesh vertices\nover time, which enables applications such as texture editing on dynamic\nobjects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly\ndistributed Gaussians, resulting better mesh reconstruction through mesh-guided\ndensification and pruning on the deformed Gaussians. By applying\ncycle-consistent deformation between the canonical and the deformed space, we\ncan project the anchored Gaussian back to the canonical space and optimize\nGaussians across all time frames. During the evaluation on different datasets,\nDG-Mesh provides significantly better mesh reconstruction and rendering than\nbaselines. Project page: https://www.liuisabella.com/DG-Mesh/\n", "link": "http://arxiv.org/abs/2404.12379v2", "date": "2024-04-22", "relevancy": 2.6552, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5314}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5312}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5305}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos&body=Title%3A%20Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos%0AAuthor%3A%20Isabella%20Liu%20and%20Hao%20Su%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Modern%203D%20engines%20and%20graphics%20pipelines%20require%20mesh%20as%20a%20memory-efficient%0Arepresentation%2C%20which%20allows%20efficient%20rendering%2C%20geometry%20processing%2C%20texture%0Aediting%2C%20and%20many%20other%20downstream%20operations.%20However%2C%20it%20is%20still%20highly%0Adifficult%20to%20obtain%20high-quality%20mesh%20in%20terms%20of%20structure%20and%20detail%20from%0Amonocular%20visual%20observations.%20The%20problem%20becomes%20even%20more%20challenging%20for%0Adynamic%20scenes%20and%20objects.%20To%20this%20end%2C%20we%20introduce%20Dynamic%20Gaussians%20Mesh%0A%28DG-Mesh%29%2C%20a%20framework%20to%20reconstruct%20a%20high-fidelity%20and%20time-consistent%20mesh%0Agiven%20a%20single%20monocular%20video.%20Our%20work%20leverages%20the%20recent%20advancement%20in%203D%0AGaussian%20Splatting%20to%20construct%20the%20mesh%20sequence%20with%20temporal%20consistency%0Afrom%20a%20video.%20Building%20on%20top%20of%20this%20representation%2C%20DG-Mesh%20recovers%0Ahigh-quality%20meshes%20from%20the%20Gaussian%20points%20and%20can%20track%20the%20mesh%20vertices%0Aover%20time%2C%20which%20enables%20applications%20such%20as%20texture%20editing%20on%20dynamic%0Aobjects.%20We%20introduce%20the%20Gaussian-Mesh%20Anchoring%2C%20which%20encourages%20evenly%0Adistributed%20Gaussians%2C%20resulting%20better%20mesh%20reconstruction%20through%20mesh-guided%0Adensification%20and%20pruning%20on%20the%20deformed%20Gaussians.%20By%20applying%0Acycle-consistent%20deformation%20between%20the%20canonical%20and%20the%20deformed%20space%2C%20we%0Acan%20project%20the%20anchored%20Gaussian%20back%20to%20the%20canonical%20space%20and%20optimize%0AGaussians%20across%20all%20time%20frames.%20During%20the%20evaluation%20on%20different%20datasets%2C%0ADG-Mesh%20provides%20significantly%20better%20mesh%20reconstruction%20and%20rendering%20than%0Abaselines.%20Project%20page%3A%20https%3A//www.liuisabella.com/DG-Mesh/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12379v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos&entry.906535625=Isabella%20Liu%20and%20Hao%20Su%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Modern%203D%20engines%20and%20graphics%20pipelines%20require%20mesh%20as%20a%20memory-efficient%0Arepresentation%2C%20which%20allows%20efficient%20rendering%2C%20geometry%20processing%2C%20texture%0Aediting%2C%20and%20many%20other%20downstream%20operations.%20However%2C%20it%20is%20still%20highly%0Adifficult%20to%20obtain%20high-quality%20mesh%20in%20terms%20of%20structure%20and%20detail%20from%0Amonocular%20visual%20observations.%20The%20problem%20becomes%20even%20more%20challenging%20for%0Adynamic%20scenes%20and%20objects.%20To%20this%20end%2C%20we%20introduce%20Dynamic%20Gaussians%20Mesh%0A%28DG-Mesh%29%2C%20a%20framework%20to%20reconstruct%20a%20high-fidelity%20and%20time-consistent%20mesh%0Agiven%20a%20single%20monocular%20video.%20Our%20work%20leverages%20the%20recent%20advancement%20in%203D%0AGaussian%20Splatting%20to%20construct%20the%20mesh%20sequence%20with%20temporal%20consistency%0Afrom%20a%20video.%20Building%20on%20top%20of%20this%20representation%2C%20DG-Mesh%20recovers%0Ahigh-quality%20meshes%20from%20the%20Gaussian%20points%20and%20can%20track%20the%20mesh%20vertices%0Aover%20time%2C%20which%20enables%20applications%20such%20as%20texture%20editing%20on%20dynamic%0Aobjects.%20We%20introduce%20the%20Gaussian-Mesh%20Anchoring%2C%20which%20encourages%20evenly%0Adistributed%20Gaussians%2C%20resulting%20better%20mesh%20reconstruction%20through%20mesh-guided%0Adensification%20and%20pruning%20on%20the%20deformed%20Gaussians.%20By%20applying%0Acycle-consistent%20deformation%20between%20the%20canonical%20and%20the%20deformed%20space%2C%20we%0Acan%20project%20the%20anchored%20Gaussian%20back%20to%20the%20canonical%20space%20and%20optimize%0AGaussians%20across%20all%20time%20frames.%20During%20the%20evaluation%20on%20different%20datasets%2C%0ADG-Mesh%20provides%20significantly%20better%20mesh%20reconstruction%20and%20rendering%20than%0Abaselines.%20Project%20page%3A%20https%3A//www.liuisabella.com/DG-Mesh/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12379v2&entry.124074799=Read"},
{"title": "NeLF-Pro: Neural Light Field Probes for Multi-Scale Novel View Synthesis", "author": "Zinuo You and Andreas Geiger and Anpei Chen", "abstract": "  We present NeLF-Pro, a novel representation to model and reconstruct light\nfields in diverse natural scenes that vary in extent and spatial granularity.\nIn contrast to previous fast reconstruction methods that represent the 3D scene\nglobally, we model the light field of a scene as a set of local light field\nfeature probes, parameterized with position and multi-channel 2D feature maps.\nOur central idea is to bake the scene's light field into spatially varying\nlearnable representations and to query point features by weighted blending of\nprobes close to the camera - allowing for mipmap representation and rendering.\nWe introduce a novel vector-matrix-matrix (VMM) factorization technique that\neffectively represents the light field feature probes as products of core\nfactors (i.e., VM) shared among local feature probes, and a basis factor (i.e.,\nM) - efficiently encoding internal relationships and patterns within the scene.\nExperimentally, we demonstrate that NeLF-Pro significantly boosts the\nperformance of feature grid-based representations, and achieves fast\nreconstruction with better rendering quality while maintaining compact\nmodeling. Project webpage https://sinoyou.github.io/nelf-pro/.\n", "link": "http://arxiv.org/abs/2312.13328v2", "date": "2024-04-22", "relevancy": 2.6309, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5136}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeLF-Pro%3A%20Neural%20Light%20Field%20Probes%20for%20Multi-Scale%20Novel%20View%20Synthesis&body=Title%3A%20NeLF-Pro%3A%20Neural%20Light%20Field%20Probes%20for%20Multi-Scale%20Novel%20View%20Synthesis%0AAuthor%3A%20Zinuo%20You%20and%20Andreas%20Geiger%20and%20Anpei%20Chen%0AAbstract%3A%20%20%20We%20present%20NeLF-Pro%2C%20a%20novel%20representation%20to%20model%20and%20reconstruct%20light%0Afields%20in%20diverse%20natural%20scenes%20that%20vary%20in%20extent%20and%20spatial%20granularity.%0AIn%20contrast%20to%20previous%20fast%20reconstruction%20methods%20that%20represent%20the%203D%20scene%0Aglobally%2C%20we%20model%20the%20light%20field%20of%20a%20scene%20as%20a%20set%20of%20local%20light%20field%0Afeature%20probes%2C%20parameterized%20with%20position%20and%20multi-channel%202D%20feature%20maps.%0AOur%20central%20idea%20is%20to%20bake%20the%20scene%27s%20light%20field%20into%20spatially%20varying%0Alearnable%20representations%20and%20to%20query%20point%20features%20by%20weighted%20blending%20of%0Aprobes%20close%20to%20the%20camera%20-%20allowing%20for%20mipmap%20representation%20and%20rendering.%0AWe%20introduce%20a%20novel%20vector-matrix-matrix%20%28VMM%29%20factorization%20technique%20that%0Aeffectively%20represents%20the%20light%20field%20feature%20probes%20as%20products%20of%20core%0Afactors%20%28i.e.%2C%20VM%29%20shared%20among%20local%20feature%20probes%2C%20and%20a%20basis%20factor%20%28i.e.%2C%0AM%29%20-%20efficiently%20encoding%20internal%20relationships%20and%20patterns%20within%20the%20scene.%0AExperimentally%2C%20we%20demonstrate%20that%20NeLF-Pro%20significantly%20boosts%20the%0Aperformance%20of%20feature%20grid-based%20representations%2C%20and%20achieves%20fast%0Areconstruction%20with%20better%20rendering%20quality%20while%20maintaining%20compact%0Amodeling.%20Project%20webpage%20https%3A//sinoyou.github.io/nelf-pro/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13328v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeLF-Pro%3A%20Neural%20Light%20Field%20Probes%20for%20Multi-Scale%20Novel%20View%20Synthesis&entry.906535625=Zinuo%20You%20and%20Andreas%20Geiger%20and%20Anpei%20Chen&entry.1292438233=%20%20We%20present%20NeLF-Pro%2C%20a%20novel%20representation%20to%20model%20and%20reconstruct%20light%0Afields%20in%20diverse%20natural%20scenes%20that%20vary%20in%20extent%20and%20spatial%20granularity.%0AIn%20contrast%20to%20previous%20fast%20reconstruction%20methods%20that%20represent%20the%203D%20scene%0Aglobally%2C%20we%20model%20the%20light%20field%20of%20a%20scene%20as%20a%20set%20of%20local%20light%20field%0Afeature%20probes%2C%20parameterized%20with%20position%20and%20multi-channel%202D%20feature%20maps.%0AOur%20central%20idea%20is%20to%20bake%20the%20scene%27s%20light%20field%20into%20spatially%20varying%0Alearnable%20representations%20and%20to%20query%20point%20features%20by%20weighted%20blending%20of%0Aprobes%20close%20to%20the%20camera%20-%20allowing%20for%20mipmap%20representation%20and%20rendering.%0AWe%20introduce%20a%20novel%20vector-matrix-matrix%20%28VMM%29%20factorization%20technique%20that%0Aeffectively%20represents%20the%20light%20field%20feature%20probes%20as%20products%20of%20core%0Afactors%20%28i.e.%2C%20VM%29%20shared%20among%20local%20feature%20probes%2C%20and%20a%20basis%20factor%20%28i.e.%2C%0AM%29%20-%20efficiently%20encoding%20internal%20relationships%20and%20patterns%20within%20the%20scene.%0AExperimentally%2C%20we%20demonstrate%20that%20NeLF-Pro%20significantly%20boosts%20the%0Aperformance%20of%20feature%20grid-based%20representations%2C%20and%20achieves%20fast%0Areconstruction%20with%20better%20rendering%20quality%20while%20maintaining%20compact%0Amodeling.%20Project%20webpage%20https%3A//sinoyou.github.io/nelf-pro/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13328v2&entry.124074799=Read"},
{"title": "Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and\n  Attention Mechanism Approach for Heterogeneous Graph-Structured Data", "author": "Jinghan Huang and Qiufeng Chen and Yijun Bian and Pengli Zhu and Nanguang Chen and Moo K. Chung and Anqi Qiu", "abstract": "  Graph neural networks (GNNs) have proven effective in capturing relationships\namong nodes in a graph. This study introduces a novel perspective by\nconsidering a graph as a simplicial complex, encompassing nodes, edges,\ntriangles, and $k$-simplices, enabling the definition of graph-structured data\non any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous\ngraph attention network (HL-HGAT), designed to learn heterogeneous signal\nrepresentations across $k$-simplices. The HL-HGAT incorporates three key\ncomponents: HL convolutional filters (HL-filters), simplicial projection (SP),\nand simplicial attention pooling (SAP) operators, applied to $k$-simplices.\nHL-filters leverage the unique topology of $k$-simplices encoded by the\nHodge-Laplacian (HL) operator, operating within the spectral domain of the\n$k$-th HL operator. To address computation challenges, we introduce a\npolynomial approximation for HL-filters, exhibiting spatial localization\nproperties. Additionally, we propose a pooling operator to coarsen\n$k$-simplices, combining features through simplicial attention mechanisms of\nself-attention and cross-attention via transformers and SP operators, capturing\ntopological interconnections across multiple dimensions of simplices. The\nHL-HGAT is comprehensively evaluated across diverse graph applications,\nincluding NP-hard problems, graph multi-label and classification challenges,\nand graph regression tasks in logistics, computer vision, biology, chemistry,\nand neuroscience. The results demonstrate the model's efficacy and versatility\nin handling a wide range of graph-based scenarios.\n", "link": "http://arxiv.org/abs/2403.06687v2", "date": "2024-04-22", "relevancy": 2.6265, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5034}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data&body=Title%3A%20Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data%0AAuthor%3A%20Jinghan%20Huang%20and%20Qiufeng%20Chen%20and%20Yijun%20Bian%20and%20Pengli%20Zhu%20and%20Nanguang%20Chen%20and%20Moo%20K.%20Chung%20and%20Anqi%20Qiu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20proven%20effective%20in%20capturing%20relationships%0Aamong%20nodes%20in%20a%20graph.%20This%20study%20introduces%20a%20novel%20perspective%20by%0Aconsidering%20a%20graph%20as%20a%20simplicial%20complex%2C%20encompassing%20nodes%2C%20edges%2C%0Atriangles%2C%20and%20%24k%24-simplices%2C%20enabling%20the%20definition%20of%20graph-structured%20data%0Aon%20any%20%24k%24-simplices.%20Our%20contribution%20is%20the%20Hodge-Laplacian%20heterogeneous%0Agraph%20attention%20network%20%28HL-HGAT%29%2C%20designed%20to%20learn%20heterogeneous%20signal%0Arepresentations%20across%20%24k%24-simplices.%20The%20HL-HGAT%20incorporates%20three%20key%0Acomponents%3A%20HL%20convolutional%20filters%20%28HL-filters%29%2C%20simplicial%20projection%20%28SP%29%2C%0Aand%20simplicial%20attention%20pooling%20%28SAP%29%20operators%2C%20applied%20to%20%24k%24-simplices.%0AHL-filters%20leverage%20the%20unique%20topology%20of%20%24k%24-simplices%20encoded%20by%20the%0AHodge-Laplacian%20%28HL%29%20operator%2C%20operating%20within%20the%20spectral%20domain%20of%20the%0A%24k%24-th%20HL%20operator.%20To%20address%20computation%20challenges%2C%20we%20introduce%20a%0Apolynomial%20approximation%20for%20HL-filters%2C%20exhibiting%20spatial%20localization%0Aproperties.%20Additionally%2C%20we%20propose%20a%20pooling%20operator%20to%20coarsen%0A%24k%24-simplices%2C%20combining%20features%20through%20simplicial%20attention%20mechanisms%20of%0Aself-attention%20and%20cross-attention%20via%20transformers%20and%20SP%20operators%2C%20capturing%0Atopological%20interconnections%20across%20multiple%20dimensions%20of%20simplices.%20The%0AHL-HGAT%20is%20comprehensively%20evaluated%20across%20diverse%20graph%20applications%2C%0Aincluding%20NP-hard%20problems%2C%20graph%20multi-label%20and%20classification%20challenges%2C%0Aand%20graph%20regression%20tasks%20in%20logistics%2C%20computer%20vision%2C%20biology%2C%20chemistry%2C%0Aand%20neuroscience.%20The%20results%20demonstrate%20the%20model%27s%20efficacy%20and%20versatility%0Ain%20handling%20a%20wide%20range%20of%20graph-based%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06687v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data&entry.906535625=Jinghan%20Huang%20and%20Qiufeng%20Chen%20and%20Yijun%20Bian%20and%20Pengli%20Zhu%20and%20Nanguang%20Chen%20and%20Moo%20K.%20Chung%20and%20Anqi%20Qiu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20proven%20effective%20in%20capturing%20relationships%0Aamong%20nodes%20in%20a%20graph.%20This%20study%20introduces%20a%20novel%20perspective%20by%0Aconsidering%20a%20graph%20as%20a%20simplicial%20complex%2C%20encompassing%20nodes%2C%20edges%2C%0Atriangles%2C%20and%20%24k%24-simplices%2C%20enabling%20the%20definition%20of%20graph-structured%20data%0Aon%20any%20%24k%24-simplices.%20Our%20contribution%20is%20the%20Hodge-Laplacian%20heterogeneous%0Agraph%20attention%20network%20%28HL-HGAT%29%2C%20designed%20to%20learn%20heterogeneous%20signal%0Arepresentations%20across%20%24k%24-simplices.%20The%20HL-HGAT%20incorporates%20three%20key%0Acomponents%3A%20HL%20convolutional%20filters%20%28HL-filters%29%2C%20simplicial%20projection%20%28SP%29%2C%0Aand%20simplicial%20attention%20pooling%20%28SAP%29%20operators%2C%20applied%20to%20%24k%24-simplices.%0AHL-filters%20leverage%20the%20unique%20topology%20of%20%24k%24-simplices%20encoded%20by%20the%0AHodge-Laplacian%20%28HL%29%20operator%2C%20operating%20within%20the%20spectral%20domain%20of%20the%0A%24k%24-th%20HL%20operator.%20To%20address%20computation%20challenges%2C%20we%20introduce%20a%0Apolynomial%20approximation%20for%20HL-filters%2C%20exhibiting%20spatial%20localization%0Aproperties.%20Additionally%2C%20we%20propose%20a%20pooling%20operator%20to%20coarsen%0A%24k%24-simplices%2C%20combining%20features%20through%20simplicial%20attention%20mechanisms%20of%0Aself-attention%20and%20cross-attention%20via%20transformers%20and%20SP%20operators%2C%20capturing%0Atopological%20interconnections%20across%20multiple%20dimensions%20of%20simplices.%20The%0AHL-HGAT%20is%20comprehensively%20evaluated%20across%20diverse%20graph%20applications%2C%0Aincluding%20NP-hard%20problems%2C%20graph%20multi-label%20and%20classification%20challenges%2C%0Aand%20graph%20regression%20tasks%20in%20logistics%2C%20computer%20vision%2C%20biology%2C%20chemistry%2C%0Aand%20neuroscience.%20The%20results%20demonstrate%20the%20model%27s%20efficacy%20and%20versatility%0Ain%20handling%20a%20wide%20range%20of%20graph-based%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06687v2&entry.124074799=Read"},
{"title": "PLUTO: Pushing the Limit of Imitation Learning-based Planning for\n  Autonomous Driving", "author": "Jie Cheng and Yingbing Chen and Qifeng Chen", "abstract": "  We present PLUTO, a powerful framework that pushes the limit of imitation\nlearning-based planning for autonomous driving. Our improvements stem from\nthree pivotal aspects: a longitudinal-lateral aware model architecture that\nenables flexible and diverse driving behaviors; An innovative auxiliary loss\ncomputation method that is broadly applicable and efficient for batch-wise\ncalculation; A novel training framework that leverages contrastive learning,\naugmented by a suite of new data augmentations to regulate driving behaviors\nand facilitate the understanding of underlying interactions. We assessed our\nframework using the large-scale real-world nuPlan dataset and its associated\nstandardized planning benchmark. Impressively, PLUTO achieves state-of-the-art\nclosed-loop performance, beating other competing learning-based methods and\nsurpassing the current top-performed rule-based planner for the first time.\nResults and code are available at https://jchengai.github.io/pluto.\n", "link": "http://arxiv.org/abs/2404.14327v1", "date": "2024-04-22", "relevancy": 2.5823, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4993}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4864}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PLUTO%3A%20Pushing%20the%20Limit%20of%20Imitation%20Learning-based%20Planning%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20PLUTO%3A%20Pushing%20the%20Limit%20of%20Imitation%20Learning-based%20Planning%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Jie%20Cheng%20and%20Yingbing%20Chen%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20We%20present%20PLUTO%2C%20a%20powerful%20framework%20that%20pushes%20the%20limit%20of%20imitation%0Alearning-based%20planning%20for%20autonomous%20driving.%20Our%20improvements%20stem%20from%0Athree%20pivotal%20aspects%3A%20a%20longitudinal-lateral%20aware%20model%20architecture%20that%0Aenables%20flexible%20and%20diverse%20driving%20behaviors%3B%20An%20innovative%20auxiliary%20loss%0Acomputation%20method%20that%20is%20broadly%20applicable%20and%20efficient%20for%20batch-wise%0Acalculation%3B%20A%20novel%20training%20framework%20that%20leverages%20contrastive%20learning%2C%0Aaugmented%20by%20a%20suite%20of%20new%20data%20augmentations%20to%20regulate%20driving%20behaviors%0Aand%20facilitate%20the%20understanding%20of%20underlying%20interactions.%20We%20assessed%20our%0Aframework%20using%20the%20large-scale%20real-world%20nuPlan%20dataset%20and%20its%20associated%0Astandardized%20planning%20benchmark.%20Impressively%2C%20PLUTO%20achieves%20state-of-the-art%0Aclosed-loop%20performance%2C%20beating%20other%20competing%20learning-based%20methods%20and%0Asurpassing%20the%20current%20top-performed%20rule-based%20planner%20for%20the%20first%20time.%0AResults%20and%20code%20are%20available%20at%20https%3A//jchengai.github.io/pluto.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14327v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLUTO%3A%20Pushing%20the%20Limit%20of%20Imitation%20Learning-based%20Planning%20for%0A%20%20Autonomous%20Driving&entry.906535625=Jie%20Cheng%20and%20Yingbing%20Chen%20and%20Qifeng%20Chen&entry.1292438233=%20%20We%20present%20PLUTO%2C%20a%20powerful%20framework%20that%20pushes%20the%20limit%20of%20imitation%0Alearning-based%20planning%20for%20autonomous%20driving.%20Our%20improvements%20stem%20from%0Athree%20pivotal%20aspects%3A%20a%20longitudinal-lateral%20aware%20model%20architecture%20that%0Aenables%20flexible%20and%20diverse%20driving%20behaviors%3B%20An%20innovative%20auxiliary%20loss%0Acomputation%20method%20that%20is%20broadly%20applicable%20and%20efficient%20for%20batch-wise%0Acalculation%3B%20A%20novel%20training%20framework%20that%20leverages%20contrastive%20learning%2C%0Aaugmented%20by%20a%20suite%20of%20new%20data%20augmentations%20to%20regulate%20driving%20behaviors%0Aand%20facilitate%20the%20understanding%20of%20underlying%20interactions.%20We%20assessed%20our%0Aframework%20using%20the%20large-scale%20real-world%20nuPlan%20dataset%20and%20its%20associated%0Astandardized%20planning%20benchmark.%20Impressively%2C%20PLUTO%20achieves%20state-of-the-art%0Aclosed-loop%20performance%2C%20beating%20other%20competing%20learning-based%20methods%20and%0Asurpassing%20the%20current%20top-performed%20rule-based%20planner%20for%20the%20first%20time.%0AResults%20and%20code%20are%20available%20at%20https%3A//jchengai.github.io/pluto.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14327v1&entry.124074799=Read"},
{"title": "CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against\n  Backdoor Attacks via Spatial Partitioning and Ensemble Prediction", "author": "Wenhao Lan and Yijun Yang and Haihua Shen and Shan Li", "abstract": "  The increasing adoption of 3D point cloud data in various applications, such\nas autonomous vehicles, robotics, and virtual reality, has brought about\nsignificant advancements in object recognition and scene understanding.\nHowever, this progress is accompanied by new security challenges, particularly\nin the form of backdoor attacks. These attacks involve inserting malicious\ninformation into the training data of machine learning models, potentially\ncompromising the model's behavior. In this paper, we propose CloudFort, a novel\ndefense mechanism designed to enhance the robustness of 3D point cloud\nclassifiers against backdoor attacks. CloudFort leverages spatial partitioning\nand ensemble prediction techniques to effectively mitigate the impact of\nbackdoor triggers while preserving the model's performance on clean data. We\nevaluate the effectiveness of CloudFort through extensive experiments,\ndemonstrating its strong resilience against the Point Cloud Backdoor Attack\n(PCBA). Our results show that CloudFort significantly enhances the security of\n3D point cloud classification models without compromising their accuracy on\nbenign samples. Furthermore, we explore the limitations of CloudFort and\ndiscuss potential avenues for future research in the field of 3D point cloud\nsecurity. The proposed defense mechanism represents a significant step towards\nensuring the trustworthiness and reliability of point-cloud-based systems in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2404.14042v1", "date": "2024-04-22", "relevancy": 2.5307, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.498}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CloudFort%3A%20Enhancing%20Robustness%20of%203D%20Point%20Cloud%20Classification%20Against%0A%20%20Backdoor%20Attacks%20via%20Spatial%20Partitioning%20and%20Ensemble%20Prediction&body=Title%3A%20CloudFort%3A%20Enhancing%20Robustness%20of%203D%20Point%20Cloud%20Classification%20Against%0A%20%20Backdoor%20Attacks%20via%20Spatial%20Partitioning%20and%20Ensemble%20Prediction%0AAuthor%3A%20Wenhao%20Lan%20and%20Yijun%20Yang%20and%20Haihua%20Shen%20and%20Shan%20Li%0AAbstract%3A%20%20%20The%20increasing%20adoption%20of%203D%20point%20cloud%20data%20in%20various%20applications%2C%20such%0Aas%20autonomous%20vehicles%2C%20robotics%2C%20and%20virtual%20reality%2C%20has%20brought%20about%0Asignificant%20advancements%20in%20object%20recognition%20and%20scene%20understanding.%0AHowever%2C%20this%20progress%20is%20accompanied%20by%20new%20security%20challenges%2C%20particularly%0Ain%20the%20form%20of%20backdoor%20attacks.%20These%20attacks%20involve%20inserting%20malicious%0Ainformation%20into%20the%20training%20data%20of%20machine%20learning%20models%2C%20potentially%0Acompromising%20the%20model%27s%20behavior.%20In%20this%20paper%2C%20we%20propose%20CloudFort%2C%20a%20novel%0Adefense%20mechanism%20designed%20to%20enhance%20the%20robustness%20of%203D%20point%20cloud%0Aclassifiers%20against%20backdoor%20attacks.%20CloudFort%20leverages%20spatial%20partitioning%0Aand%20ensemble%20prediction%20techniques%20to%20effectively%20mitigate%20the%20impact%20of%0Abackdoor%20triggers%20while%20preserving%20the%20model%27s%20performance%20on%20clean%20data.%20We%0Aevaluate%20the%20effectiveness%20of%20CloudFort%20through%20extensive%20experiments%2C%0Ademonstrating%20its%20strong%20resilience%20against%20the%20Point%20Cloud%20Backdoor%20Attack%0A%28PCBA%29.%20Our%20results%20show%20that%20CloudFort%20significantly%20enhances%20the%20security%20of%0A3D%20point%20cloud%20classification%20models%20without%20compromising%20their%20accuracy%20on%0Abenign%20samples.%20Furthermore%2C%20we%20explore%20the%20limitations%20of%20CloudFort%20and%0Adiscuss%20potential%20avenues%20for%20future%20research%20in%20the%20field%20of%203D%20point%20cloud%0Asecurity.%20The%20proposed%20defense%20mechanism%20represents%20a%20significant%20step%20towards%0Aensuring%20the%20trustworthiness%20and%20reliability%20of%20point-cloud-based%20systems%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14042v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CloudFort%3A%20Enhancing%20Robustness%20of%203D%20Point%20Cloud%20Classification%20Against%0A%20%20Backdoor%20Attacks%20via%20Spatial%20Partitioning%20and%20Ensemble%20Prediction&entry.906535625=Wenhao%20Lan%20and%20Yijun%20Yang%20and%20Haihua%20Shen%20and%20Shan%20Li&entry.1292438233=%20%20The%20increasing%20adoption%20of%203D%20point%20cloud%20data%20in%20various%20applications%2C%20such%0Aas%20autonomous%20vehicles%2C%20robotics%2C%20and%20virtual%20reality%2C%20has%20brought%20about%0Asignificant%20advancements%20in%20object%20recognition%20and%20scene%20understanding.%0AHowever%2C%20this%20progress%20is%20accompanied%20by%20new%20security%20challenges%2C%20particularly%0Ain%20the%20form%20of%20backdoor%20attacks.%20These%20attacks%20involve%20inserting%20malicious%0Ainformation%20into%20the%20training%20data%20of%20machine%20learning%20models%2C%20potentially%0Acompromising%20the%20model%27s%20behavior.%20In%20this%20paper%2C%20we%20propose%20CloudFort%2C%20a%20novel%0Adefense%20mechanism%20designed%20to%20enhance%20the%20robustness%20of%203D%20point%20cloud%0Aclassifiers%20against%20backdoor%20attacks.%20CloudFort%20leverages%20spatial%20partitioning%0Aand%20ensemble%20prediction%20techniques%20to%20effectively%20mitigate%20the%20impact%20of%0Abackdoor%20triggers%20while%20preserving%20the%20model%27s%20performance%20on%20clean%20data.%20We%0Aevaluate%20the%20effectiveness%20of%20CloudFort%20through%20extensive%20experiments%2C%0Ademonstrating%20its%20strong%20resilience%20against%20the%20Point%20Cloud%20Backdoor%20Attack%0A%28PCBA%29.%20Our%20results%20show%20that%20CloudFort%20significantly%20enhances%20the%20security%20of%0A3D%20point%20cloud%20classification%20models%20without%20compromising%20their%20accuracy%20on%0Abenign%20samples.%20Furthermore%2C%20we%20explore%20the%20limitations%20of%20CloudFort%20and%0Adiscuss%20potential%20avenues%20for%20future%20research%20in%20the%20field%20of%203D%20point%20cloud%0Asecurity.%20The%20proposed%20defense%20mechanism%20represents%20a%20significant%20step%20towards%0Aensuring%20the%20trustworthiness%20and%20reliability%20of%20point-cloud-based%20systems%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14042v1&entry.124074799=Read"},
{"title": "CKD: Contrastive Knowledge Distillation from A Sample-wise Perspective", "author": "Wencheng Zhu and Xin Zhou and Pengfei Zhu and Yu Wang and Qinghua Hu", "abstract": "  In this paper, we present a simple yet effective contrastive knowledge\ndistillation approach, which can be formulated as a sample-wise alignment\nproblem with intra- and inter-sample constraints. Unlike traditional knowledge\ndistillation methods that concentrate on maximizing feature similarities or\npreserving class-wise semantic correlations between teacher and student\nfeatures, our method attempts to recover the \"dark knowledge\" by aligning\nsample-wise teacher and student logits. Specifically, our method first\nminimizes logit differences within the same sample by considering their\nnumerical values, thus preserving intra-sample similarities. Next, we bridge\nsemantic disparities by leveraging dissimilarities across different samples.\nNote that constraints on intra-sample similarities and inter-sample\ndissimilarities can be efficiently and effectively reformulated into a\ncontrastive learning framework with newly designed positive and negative pairs.\nThe positive pair consists of the teacher's and student's logits derived from\nan identical sample, while the negative pairs are formed by using logits from\ndifferent samples. With this formulation, our method benefits from the\nsimplicity and efficiency of contrastive learning through the optimization of\nInfoNCE, yielding a run-time complexity that is far less than $O(n^2)$, where\n$n$ represents the total number of training samples. Furthermore, our method\ncan eliminate the need for hyperparameter tuning, particularly related to\ntemperature parameters and large batch sizes. We conduct comprehensive\nexperiments on three datasets including CIFAR-100, ImageNet-1K, and MS COCO.\nExperimental results clearly confirm the effectiveness of the proposed method\non both image classification and object detection tasks. Our source codes will\nbe publicly available at https://github.com/wencheng-zhu/CKD.\n", "link": "http://arxiv.org/abs/2404.14109v1", "date": "2024-04-22", "relevancy": 2.498, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4917}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CKD%3A%20Contrastive%20Knowledge%20Distillation%20from%20A%20Sample-wise%20Perspective&body=Title%3A%20CKD%3A%20Contrastive%20Knowledge%20Distillation%20from%20A%20Sample-wise%20Perspective%0AAuthor%3A%20Wencheng%20Zhu%20and%20Xin%20Zhou%20and%20Pengfei%20Zhu%20and%20Yu%20Wang%20and%20Qinghua%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20simple%20yet%20effective%20contrastive%20knowledge%0Adistillation%20approach%2C%20which%20can%20be%20formulated%20as%20a%20sample-wise%20alignment%0Aproblem%20with%20intra-%20and%20inter-sample%20constraints.%20Unlike%20traditional%20knowledge%0Adistillation%20methods%20that%20concentrate%20on%20maximizing%20feature%20similarities%20or%0Apreserving%20class-wise%20semantic%20correlations%20between%20teacher%20and%20student%0Afeatures%2C%20our%20method%20attempts%20to%20recover%20the%20%22dark%20knowledge%22%20by%20aligning%0Asample-wise%20teacher%20and%20student%20logits.%20Specifically%2C%20our%20method%20first%0Aminimizes%20logit%20differences%20within%20the%20same%20sample%20by%20considering%20their%0Anumerical%20values%2C%20thus%20preserving%20intra-sample%20similarities.%20Next%2C%20we%20bridge%0Asemantic%20disparities%20by%20leveraging%20dissimilarities%20across%20different%20samples.%0ANote%20that%20constraints%20on%20intra-sample%20similarities%20and%20inter-sample%0Adissimilarities%20can%20be%20efficiently%20and%20effectively%20reformulated%20into%20a%0Acontrastive%20learning%20framework%20with%20newly%20designed%20positive%20and%20negative%20pairs.%0AThe%20positive%20pair%20consists%20of%20the%20teacher%27s%20and%20student%27s%20logits%20derived%20from%0Aan%20identical%20sample%2C%20while%20the%20negative%20pairs%20are%20formed%20by%20using%20logits%20from%0Adifferent%20samples.%20With%20this%20formulation%2C%20our%20method%20benefits%20from%20the%0Asimplicity%20and%20efficiency%20of%20contrastive%20learning%20through%20the%20optimization%20of%0AInfoNCE%2C%20yielding%20a%20run-time%20complexity%20that%20is%20far%20less%20than%20%24O%28n%5E2%29%24%2C%20where%0A%24n%24%20represents%20the%20total%20number%20of%20training%20samples.%20Furthermore%2C%20our%20method%0Acan%20eliminate%20the%20need%20for%20hyperparameter%20tuning%2C%20particularly%20related%20to%0Atemperature%20parameters%20and%20large%20batch%20sizes.%20We%20conduct%20comprehensive%0Aexperiments%20on%20three%20datasets%20including%20CIFAR-100%2C%20ImageNet-1K%2C%20and%20MS%20COCO.%0AExperimental%20results%20clearly%20confirm%20the%20effectiveness%20of%20the%20proposed%20method%0Aon%20both%20image%20classification%20and%20object%20detection%20tasks.%20Our%20source%20codes%20will%0Abe%20publicly%20available%20at%20https%3A//github.com/wencheng-zhu/CKD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14109v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CKD%3A%20Contrastive%20Knowledge%20Distillation%20from%20A%20Sample-wise%20Perspective&entry.906535625=Wencheng%20Zhu%20and%20Xin%20Zhou%20and%20Pengfei%20Zhu%20and%20Yu%20Wang%20and%20Qinghua%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20simple%20yet%20effective%20contrastive%20knowledge%0Adistillation%20approach%2C%20which%20can%20be%20formulated%20as%20a%20sample-wise%20alignment%0Aproblem%20with%20intra-%20and%20inter-sample%20constraints.%20Unlike%20traditional%20knowledge%0Adistillation%20methods%20that%20concentrate%20on%20maximizing%20feature%20similarities%20or%0Apreserving%20class-wise%20semantic%20correlations%20between%20teacher%20and%20student%0Afeatures%2C%20our%20method%20attempts%20to%20recover%20the%20%22dark%20knowledge%22%20by%20aligning%0Asample-wise%20teacher%20and%20student%20logits.%20Specifically%2C%20our%20method%20first%0Aminimizes%20logit%20differences%20within%20the%20same%20sample%20by%20considering%20their%0Anumerical%20values%2C%20thus%20preserving%20intra-sample%20similarities.%20Next%2C%20we%20bridge%0Asemantic%20disparities%20by%20leveraging%20dissimilarities%20across%20different%20samples.%0ANote%20that%20constraints%20on%20intra-sample%20similarities%20and%20inter-sample%0Adissimilarities%20can%20be%20efficiently%20and%20effectively%20reformulated%20into%20a%0Acontrastive%20learning%20framework%20with%20newly%20designed%20positive%20and%20negative%20pairs.%0AThe%20positive%20pair%20consists%20of%20the%20teacher%27s%20and%20student%27s%20logits%20derived%20from%0Aan%20identical%20sample%2C%20while%20the%20negative%20pairs%20are%20formed%20by%20using%20logits%20from%0Adifferent%20samples.%20With%20this%20formulation%2C%20our%20method%20benefits%20from%20the%0Asimplicity%20and%20efficiency%20of%20contrastive%20learning%20through%20the%20optimization%20of%0AInfoNCE%2C%20yielding%20a%20run-time%20complexity%20that%20is%20far%20less%20than%20%24O%28n%5E2%29%24%2C%20where%0A%24n%24%20represents%20the%20total%20number%20of%20training%20samples.%20Furthermore%2C%20our%20method%0Acan%20eliminate%20the%20need%20for%20hyperparameter%20tuning%2C%20particularly%20related%20to%0Atemperature%20parameters%20and%20large%20batch%20sizes.%20We%20conduct%20comprehensive%0Aexperiments%20on%20three%20datasets%20including%20CIFAR-100%2C%20ImageNet-1K%2C%20and%20MS%20COCO.%0AExperimental%20results%20clearly%20confirm%20the%20effectiveness%20of%20the%20proposed%20method%0Aon%20both%20image%20classification%20and%20object%20detection%20tasks.%20Our%20source%20codes%20will%0Abe%20publicly%20available%20at%20https%3A//github.com/wencheng-zhu/CKD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14109v1&entry.124074799=Read"},
{"title": "LVNS-RAVE: Diversified audio generation with RAVE and Latent Vector\n  Novelty Search", "author": "Jinyue Guo and Anna-Maria Christodoulou and Balint Laczko and Kyrre Glette", "abstract": "  Evolutionary Algorithms and Generative Deep Learning have been two of the\nmost powerful tools for sound generation tasks. However, they have limitations:\nEvolutionary Algorithms require complicated designs, posing challenges in\ncontrol and achieving realistic sound generation. Generative Deep Learning\nmodels often copy from the dataset and lack creativity. In this paper, we\npropose LVNS-RAVE, a method to combine Evolutionary Algorithms and Generative\nDeep Learning to produce realistic and novel sounds. We use the RAVE model as\nthe sound generator and the VGGish model as a novelty evaluator in the Latent\nVector Novelty Search (LVNS) algorithm. The reported experiments show that the\nmethod can successfully generate diversified, novel audio samples under\ndifferent mutation setups using different pre-trained RAVE models. The\ncharacteristics of the generation process can be easily controlled with the\nmutation parameters. The proposed algorithm can be a creative tool for sound\nartists and musicians.\n", "link": "http://arxiv.org/abs/2404.14063v1", "date": "2024-04-22", "relevancy": 2.4918, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5114}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4936}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4901}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LVNS-RAVE%3A%20Diversified%20audio%20generation%20with%20RAVE%20and%20Latent%20Vector%0A%20%20Novelty%20Search&body=Title%3A%20LVNS-RAVE%3A%20Diversified%20audio%20generation%20with%20RAVE%20and%20Latent%20Vector%0A%20%20Novelty%20Search%0AAuthor%3A%20Jinyue%20Guo%20and%20Anna-Maria%20Christodoulou%20and%20Balint%20Laczko%20and%20Kyrre%20Glette%0AAbstract%3A%20%20%20Evolutionary%20Algorithms%20and%20Generative%20Deep%20Learning%20have%20been%20two%20of%20the%0Amost%20powerful%20tools%20for%20sound%20generation%20tasks.%20However%2C%20they%20have%20limitations%3A%0AEvolutionary%20Algorithms%20require%20complicated%20designs%2C%20posing%20challenges%20in%0Acontrol%20and%20achieving%20realistic%20sound%20generation.%20Generative%20Deep%20Learning%0Amodels%20often%20copy%20from%20the%20dataset%20and%20lack%20creativity.%20In%20this%20paper%2C%20we%0Apropose%20LVNS-RAVE%2C%20a%20method%20to%20combine%20Evolutionary%20Algorithms%20and%20Generative%0ADeep%20Learning%20to%20produce%20realistic%20and%20novel%20sounds.%20We%20use%20the%20RAVE%20model%20as%0Athe%20sound%20generator%20and%20the%20VGGish%20model%20as%20a%20novelty%20evaluator%20in%20the%20Latent%0AVector%20Novelty%20Search%20%28LVNS%29%20algorithm.%20The%20reported%20experiments%20show%20that%20the%0Amethod%20can%20successfully%20generate%20diversified%2C%20novel%20audio%20samples%20under%0Adifferent%20mutation%20setups%20using%20different%20pre-trained%20RAVE%20models.%20The%0Acharacteristics%20of%20the%20generation%20process%20can%20be%20easily%20controlled%20with%20the%0Amutation%20parameters.%20The%20proposed%20algorithm%20can%20be%20a%20creative%20tool%20for%20sound%0Aartists%20and%20musicians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14063v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVNS-RAVE%3A%20Diversified%20audio%20generation%20with%20RAVE%20and%20Latent%20Vector%0A%20%20Novelty%20Search&entry.906535625=Jinyue%20Guo%20and%20Anna-Maria%20Christodoulou%20and%20Balint%20Laczko%20and%20Kyrre%20Glette&entry.1292438233=%20%20Evolutionary%20Algorithms%20and%20Generative%20Deep%20Learning%20have%20been%20two%20of%20the%0Amost%20powerful%20tools%20for%20sound%20generation%20tasks.%20However%2C%20they%20have%20limitations%3A%0AEvolutionary%20Algorithms%20require%20complicated%20designs%2C%20posing%20challenges%20in%0Acontrol%20and%20achieving%20realistic%20sound%20generation.%20Generative%20Deep%20Learning%0Amodels%20often%20copy%20from%20the%20dataset%20and%20lack%20creativity.%20In%20this%20paper%2C%20we%0Apropose%20LVNS-RAVE%2C%20a%20method%20to%20combine%20Evolutionary%20Algorithms%20and%20Generative%0ADeep%20Learning%20to%20produce%20realistic%20and%20novel%20sounds.%20We%20use%20the%20RAVE%20model%20as%0Athe%20sound%20generator%20and%20the%20VGGish%20model%20as%20a%20novelty%20evaluator%20in%20the%20Latent%0AVector%20Novelty%20Search%20%28LVNS%29%20algorithm.%20The%20reported%20experiments%20show%20that%20the%0Amethod%20can%20successfully%20generate%20diversified%2C%20novel%20audio%20samples%20under%0Adifferent%20mutation%20setups%20using%20different%20pre-trained%20RAVE%20models.%20The%0Acharacteristics%20of%20the%20generation%20process%20can%20be%20easily%20controlled%20with%20the%0Amutation%20parameters.%20The%20proposed%20algorithm%20can%20be%20a%20creative%20tool%20for%20sound%0Aartists%20and%20musicians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14063v1&entry.124074799=Read"},
{"title": "GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text\n  Recognition System", "author": "Lalita Kumari and Sukhdeep Singh and Vaibhav Varish Singh Rathore and Anuj Sharma", "abstract": "  The Handwritten Text Recognition problem has been a challenge for researchers\nfor the last few decades, especially in the domain of computer vision, a\nsubdomain of pattern recognition. Variability of texts amongst writers,\ncursiveness, and different font styles of handwritten texts with degradation of\nhistorical text images make it a challenging problem. Recognizing scanned\ndocument images in neural network-based systems typically involves a two-step\napproach: segmentation and recognition. However, this method has several\ndrawbacks. These shortcomings encompass challenges in identifying text regions,\nanalyzing layout diversity within pages, and establishing accurate ground truth\nsegmentation. Consequently, these processes are prone to errors, leading to\nbottlenecks in achieving high recognition accuracies. Thus, in this study, we\npresent an end-to-end paragraph recognition system that incorporates internal\nline segmentation and gated convolutional layers based encoder. The gating is a\nmechanism that controls the flow of information and allows to adaptively\nselection of the more relevant features in handwritten text recognition models.\nThe attention module plays an important role in performing internal line\nsegmentation, allowing the page to be processed line-by-line. During the\ndecoding step, we have integrated a connectionist temporal classification-based\nword beam search decoder as a post-processing step. In this work, we have\nextended existing LexiconNet by carefully applying and utilizing gated\nconvolutional layers in the existing deep neural network. Our results at line\nand page levels also favour our new GatedLexiconNet. This study reported\ncharacter error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and\nword error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016\ndatasets.\n", "link": "http://arxiv.org/abs/2404.14062v1", "date": "2024-04-22", "relevancy": 2.4854, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GatedLexiconNet%3A%20A%20Comprehensive%20End-to-End%20Handwritten%20Paragraph%20Text%0A%20%20Recognition%20System&body=Title%3A%20GatedLexiconNet%3A%20A%20Comprehensive%20End-to-End%20Handwritten%20Paragraph%20Text%0A%20%20Recognition%20System%0AAuthor%3A%20Lalita%20Kumari%20and%20Sukhdeep%20Singh%20and%20Vaibhav%20Varish%20Singh%20Rathore%20and%20Anuj%20Sharma%0AAbstract%3A%20%20%20The%20Handwritten%20Text%20Recognition%20problem%20has%20been%20a%20challenge%20for%20researchers%0Afor%20the%20last%20few%20decades%2C%20especially%20in%20the%20domain%20of%20computer%20vision%2C%20a%0Asubdomain%20of%20pattern%20recognition.%20Variability%20of%20texts%20amongst%20writers%2C%0Acursiveness%2C%20and%20different%20font%20styles%20of%20handwritten%20texts%20with%20degradation%20of%0Ahistorical%20text%20images%20make%20it%20a%20challenging%20problem.%20Recognizing%20scanned%0Adocument%20images%20in%20neural%20network-based%20systems%20typically%20involves%20a%20two-step%0Aapproach%3A%20segmentation%20and%20recognition.%20However%2C%20this%20method%20has%20several%0Adrawbacks.%20These%20shortcomings%20encompass%20challenges%20in%20identifying%20text%20regions%2C%0Aanalyzing%20layout%20diversity%20within%20pages%2C%20and%20establishing%20accurate%20ground%20truth%0Asegmentation.%20Consequently%2C%20these%20processes%20are%20prone%20to%20errors%2C%20leading%20to%0Abottlenecks%20in%20achieving%20high%20recognition%20accuracies.%20Thus%2C%20in%20this%20study%2C%20we%0Apresent%20an%20end-to-end%20paragraph%20recognition%20system%20that%20incorporates%20internal%0Aline%20segmentation%20and%20gated%20convolutional%20layers%20based%20encoder.%20The%20gating%20is%20a%0Amechanism%20that%20controls%20the%20flow%20of%20information%20and%20allows%20to%20adaptively%0Aselection%20of%20the%20more%20relevant%20features%20in%20handwritten%20text%20recognition%20models.%0AThe%20attention%20module%20plays%20an%20important%20role%20in%20performing%20internal%20line%0Asegmentation%2C%20allowing%20the%20page%20to%20be%20processed%20line-by-line.%20During%20the%0Adecoding%20step%2C%20we%20have%20integrated%20a%20connectionist%20temporal%20classification-based%0Aword%20beam%20search%20decoder%20as%20a%20post-processing%20step.%20In%20this%20work%2C%20we%20have%0Aextended%20existing%20LexiconNet%20by%20carefully%20applying%20and%20utilizing%20gated%0Aconvolutional%20layers%20in%20the%20existing%20deep%20neural%20network.%20Our%20results%20at%20line%0Aand%20page%20levels%20also%20favour%20our%20new%20GatedLexiconNet.%20This%20study%20reported%0Acharacter%20error%20rates%20of%202.27%25%20on%20IAM%2C%200.9%25%20on%20RIMES%2C%20and%202.13%25%20on%20READ-16%2C%20and%0Aword%20error%20rates%20of%205.73%25%20on%20IAM%2C%202.76%25%20on%20RIMES%2C%20and%206.52%25%20on%20READ-2016%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14062v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GatedLexiconNet%3A%20A%20Comprehensive%20End-to-End%20Handwritten%20Paragraph%20Text%0A%20%20Recognition%20System&entry.906535625=Lalita%20Kumari%20and%20Sukhdeep%20Singh%20and%20Vaibhav%20Varish%20Singh%20Rathore%20and%20Anuj%20Sharma&entry.1292438233=%20%20The%20Handwritten%20Text%20Recognition%20problem%20has%20been%20a%20challenge%20for%20researchers%0Afor%20the%20last%20few%20decades%2C%20especially%20in%20the%20domain%20of%20computer%20vision%2C%20a%0Asubdomain%20of%20pattern%20recognition.%20Variability%20of%20texts%20amongst%20writers%2C%0Acursiveness%2C%20and%20different%20font%20styles%20of%20handwritten%20texts%20with%20degradation%20of%0Ahistorical%20text%20images%20make%20it%20a%20challenging%20problem.%20Recognizing%20scanned%0Adocument%20images%20in%20neural%20network-based%20systems%20typically%20involves%20a%20two-step%0Aapproach%3A%20segmentation%20and%20recognition.%20However%2C%20this%20method%20has%20several%0Adrawbacks.%20These%20shortcomings%20encompass%20challenges%20in%20identifying%20text%20regions%2C%0Aanalyzing%20layout%20diversity%20within%20pages%2C%20and%20establishing%20accurate%20ground%20truth%0Asegmentation.%20Consequently%2C%20these%20processes%20are%20prone%20to%20errors%2C%20leading%20to%0Abottlenecks%20in%20achieving%20high%20recognition%20accuracies.%20Thus%2C%20in%20this%20study%2C%20we%0Apresent%20an%20end-to-end%20paragraph%20recognition%20system%20that%20incorporates%20internal%0Aline%20segmentation%20and%20gated%20convolutional%20layers%20based%20encoder.%20The%20gating%20is%20a%0Amechanism%20that%20controls%20the%20flow%20of%20information%20and%20allows%20to%20adaptively%0Aselection%20of%20the%20more%20relevant%20features%20in%20handwritten%20text%20recognition%20models.%0AThe%20attention%20module%20plays%20an%20important%20role%20in%20performing%20internal%20line%0Asegmentation%2C%20allowing%20the%20page%20to%20be%20processed%20line-by-line.%20During%20the%0Adecoding%20step%2C%20we%20have%20integrated%20a%20connectionist%20temporal%20classification-based%0Aword%20beam%20search%20decoder%20as%20a%20post-processing%20step.%20In%20this%20work%2C%20we%20have%0Aextended%20existing%20LexiconNet%20by%20carefully%20applying%20and%20utilizing%20gated%0Aconvolutional%20layers%20in%20the%20existing%20deep%20neural%20network.%20Our%20results%20at%20line%0Aand%20page%20levels%20also%20favour%20our%20new%20GatedLexiconNet.%20This%20study%20reported%0Acharacter%20error%20rates%20of%202.27%25%20on%20IAM%2C%200.9%25%20on%20RIMES%2C%20and%202.13%25%20on%20READ-16%2C%20and%0Aword%20error%20rates%20of%205.73%25%20on%20IAM%2C%202.76%25%20on%20RIMES%2C%20and%206.52%25%20on%20READ-2016%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14062v1&entry.124074799=Read"},
{"title": "Hierarchical localization with panoramic views and triplet loss\n  functions", "author": "Marcos Alfaro and Juan Jos\u00e9 Cabrera and Luis Miguel Jim\u00e9nez and \u00d3scar Reinoso and Luis Pay\u00e1", "abstract": "  The main objective of this paper is to address the mobile robot localization\nproblem with Triplet Convolutional Neural Networks and test their robustness\nagainst changes of the lighting conditions. We have used omnidirectional images\nfrom real indoor environments captured in dynamic conditions that have been\nconverted to panoramic format. Two approaches are proposed to address\nlocalization by means of triplet neural networks. First, hierarchical\nlocalization, which consists in estimating the robot position in two stages: a\ncoarse localization, which involves a room retrieval task, and a fine\nlocalization is addressed by means of image retrieval in the previously\nselected room. Second, global localization, which consists in estimating the\nposition of the robot inside the entire map in a unique step. Besides, an\nexhaustive study of the loss function influence on the network learning process\nhas been made. The experimental section proves that triplet neural networks are\nan efficient and robust tool to address the localization of mobile robots in\nindoor environments, considering real operation conditions.\n", "link": "http://arxiv.org/abs/2404.14117v1", "date": "2024-04-22", "relevancy": 2.4777, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6338}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions&body=Title%3A%20Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions%0AAuthor%3A%20Marcos%20Alfaro%20and%20Juan%20Jos%C3%A9%20Cabrera%20and%20Luis%20Miguel%20Jim%C3%A9nez%20and%20%C3%93scar%20Reinoso%20and%20Luis%20Pay%C3%A1%0AAbstract%3A%20%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20address%20the%20mobile%20robot%20localization%0Aproblem%20with%20Triplet%20Convolutional%20Neural%20Networks%20and%20test%20their%20robustness%0Aagainst%20changes%20of%20the%20lighting%20conditions.%20We%20have%20used%20omnidirectional%20images%0Afrom%20real%20indoor%20environments%20captured%20in%20dynamic%20conditions%20that%20have%20been%0Aconverted%20to%20panoramic%20format.%20Two%20approaches%20are%20proposed%20to%20address%0Alocalization%20by%20means%20of%20triplet%20neural%20networks.%20First%2C%20hierarchical%0Alocalization%2C%20which%20consists%20in%20estimating%20the%20robot%20position%20in%20two%20stages%3A%20a%0Acoarse%20localization%2C%20which%20involves%20a%20room%20retrieval%20task%2C%20and%20a%20fine%0Alocalization%20is%20addressed%20by%20means%20of%20image%20retrieval%20in%20the%20previously%0Aselected%20room.%20Second%2C%20global%20localization%2C%20which%20consists%20in%20estimating%20the%0Aposition%20of%20the%20robot%20inside%20the%20entire%20map%20in%20a%20unique%20step.%20Besides%2C%20an%0Aexhaustive%20study%20of%20the%20loss%20function%20influence%20on%20the%20network%20learning%20process%0Ahas%20been%20made.%20The%20experimental%20section%20proves%20that%20triplet%20neural%20networks%20are%0Aan%20efficient%20and%20robust%20tool%20to%20address%20the%20localization%20of%20mobile%20robots%20in%0Aindoor%20environments%2C%20considering%20real%20operation%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14117v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions&entry.906535625=Marcos%20Alfaro%20and%20Juan%20Jos%C3%A9%20Cabrera%20and%20Luis%20Miguel%20Jim%C3%A9nez%20and%20%C3%93scar%20Reinoso%20and%20Luis%20Pay%C3%A1&entry.1292438233=%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20address%20the%20mobile%20robot%20localization%0Aproblem%20with%20Triplet%20Convolutional%20Neural%20Networks%20and%20test%20their%20robustness%0Aagainst%20changes%20of%20the%20lighting%20conditions.%20We%20have%20used%20omnidirectional%20images%0Afrom%20real%20indoor%20environments%20captured%20in%20dynamic%20conditions%20that%20have%20been%0Aconverted%20to%20panoramic%20format.%20Two%20approaches%20are%20proposed%20to%20address%0Alocalization%20by%20means%20of%20triplet%20neural%20networks.%20First%2C%20hierarchical%0Alocalization%2C%20which%20consists%20in%20estimating%20the%20robot%20position%20in%20two%20stages%3A%20a%0Acoarse%20localization%2C%20which%20involves%20a%20room%20retrieval%20task%2C%20and%20a%20fine%0Alocalization%20is%20addressed%20by%20means%20of%20image%20retrieval%20in%20the%20previously%0Aselected%20room.%20Second%2C%20global%20localization%2C%20which%20consists%20in%20estimating%20the%0Aposition%20of%20the%20robot%20inside%20the%20entire%20map%20in%20a%20unique%20step.%20Besides%2C%20an%0Aexhaustive%20study%20of%20the%20loss%20function%20influence%20on%20the%20network%20learning%20process%0Ahas%20been%20made.%20The%20experimental%20section%20proves%20that%20triplet%20neural%20networks%20are%0Aan%20efficient%20and%20robust%20tool%20to%20address%20the%20localization%20of%20mobile%20robots%20in%0Aindoor%20environments%2C%20considering%20real%20operation%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14117v1&entry.124074799=Read"},
{"title": "CrossScore: Towards Multi-View Image Evaluation and Scoring", "author": "Zirui Wang and Wenjing Bian and Omkar Parkhi and Yuheng Ren and Victor Adrian Prisacariu", "abstract": "  We introduce a novel cross-reference image quality assessment method that\neffectively fills the gap in the image assessment landscape, complementing the\narray of established evaluation schemes -- ranging from full-reference metrics\nlike SSIM, no-reference metrics such as NIQE, to general-reference metrics\nincluding FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a\nneural network with the cross-attention mechanism and a unique data collection\npipeline from NVS optimisation, our method enables accurate image quality\nassessment without requiring ground truth references. By comparing a query\nimage against multiple views of the same scene, our method addresses the\nlimitations of existing metrics in novel view synthesis (NVS) and similar tasks\nwhere direct reference images are unavailable. Experimental results show that\nour method is closely correlated to the full-reference metric SSIM, while not\nrequiring ground truth references.\n", "link": "http://arxiv.org/abs/2404.14409v1", "date": "2024-04-22", "relevancy": 2.4616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5004}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4912}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4853}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CrossScore%3A%20Towards%20Multi-View%20Image%20Evaluation%20and%20Scoring&body=Title%3A%20CrossScore%3A%20Towards%20Multi-View%20Image%20Evaluation%20and%20Scoring%0AAuthor%3A%20Zirui%20Wang%20and%20Wenjing%20Bian%20and%20Omkar%20Parkhi%20and%20Yuheng%20Ren%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20cross-reference%20image%20quality%20assessment%20method%20that%0Aeffectively%20fills%20the%20gap%20in%20the%20image%20assessment%20landscape%2C%20complementing%20the%0Aarray%20of%20established%20evaluation%20schemes%20--%20ranging%20from%20full-reference%20metrics%0Alike%20SSIM%2C%20no-reference%20metrics%20such%20as%20NIQE%2C%20to%20general-reference%20metrics%0Aincluding%20FID%2C%20and%20Multi-modal-reference%20metrics%2C%20e.g.%2C%20CLIPScore.%20Utilising%20a%0Aneural%20network%20with%20the%20cross-attention%20mechanism%20and%20a%20unique%20data%20collection%0Apipeline%20from%20NVS%20optimisation%2C%20our%20method%20enables%20accurate%20image%20quality%0Aassessment%20without%20requiring%20ground%20truth%20references.%20By%20comparing%20a%20query%0Aimage%20against%20multiple%20views%20of%20the%20same%20scene%2C%20our%20method%20addresses%20the%0Alimitations%20of%20existing%20metrics%20in%20novel%20view%20synthesis%20%28NVS%29%20and%20similar%20tasks%0Awhere%20direct%20reference%20images%20are%20unavailable.%20Experimental%20results%20show%20that%0Aour%20method%20is%20closely%20correlated%20to%20the%20full-reference%20metric%20SSIM%2C%20while%20not%0Arequiring%20ground%20truth%20references.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14409v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossScore%3A%20Towards%20Multi-View%20Image%20Evaluation%20and%20Scoring&entry.906535625=Zirui%20Wang%20and%20Wenjing%20Bian%20and%20Omkar%20Parkhi%20and%20Yuheng%20Ren%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20introduce%20a%20novel%20cross-reference%20image%20quality%20assessment%20method%20that%0Aeffectively%20fills%20the%20gap%20in%20the%20image%20assessment%20landscape%2C%20complementing%20the%0Aarray%20of%20established%20evaluation%20schemes%20--%20ranging%20from%20full-reference%20metrics%0Alike%20SSIM%2C%20no-reference%20metrics%20such%20as%20NIQE%2C%20to%20general-reference%20metrics%0Aincluding%20FID%2C%20and%20Multi-modal-reference%20metrics%2C%20e.g.%2C%20CLIPScore.%20Utilising%20a%0Aneural%20network%20with%20the%20cross-attention%20mechanism%20and%20a%20unique%20data%20collection%0Apipeline%20from%20NVS%20optimisation%2C%20our%20method%20enables%20accurate%20image%20quality%0Aassessment%20without%20requiring%20ground%20truth%20references.%20By%20comparing%20a%20query%0Aimage%20against%20multiple%20views%20of%20the%20same%20scene%2C%20our%20method%20addresses%20the%0Alimitations%20of%20existing%20metrics%20in%20novel%20view%20synthesis%20%28NVS%29%20and%20similar%20tasks%0Awhere%20direct%20reference%20images%20are%20unavailable.%20Experimental%20results%20show%20that%0Aour%20method%20is%20closely%20correlated%20to%20the%20full-reference%20metric%20SSIM%2C%20while%20not%0Arequiring%20ground%20truth%20references.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14409v1&entry.124074799=Read"},
{"title": "Heterogeneous Face Recognition Using Domain Invariant Units", "author": "Anjith George and Sebastien Marcel", "abstract": "  Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face\nRecognition (FR) systems to challenging scenarios, enabling the matching of\nface images across different domains, such as matching thermal images to\nvisible spectra. However, the development of HFR systems is challenging because\nof the significant domain gap between modalities and the lack of availability\nof large-scale paired multi-channel data. In this work, we leverage a\npretrained face recognition model as a teacher network to learn domaininvariant\nnetwork layers called Domain-Invariant Units (DIU) to reduce the domain gap.\nThe proposed DIU can be trained effectively even with a limited amount of\npaired training data, in a contrastive distillation framework. This proposed\napproach has the potential to enhance pretrained models, making them more\nadaptable to a wider range of variations in data. We extensively evaluate our\napproach on multiple challenging benchmarks, demonstrating superior performance\ncompared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2404.14343v1", "date": "2024-04-22", "relevancy": 2.4387, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4974}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4864}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4794}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Face%20Recognition%20Using%20Domain%20Invariant%20Units&body=Title%3A%20Heterogeneous%20Face%20Recognition%20Using%20Domain%20Invariant%20Units%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20aims%20to%20expand%20the%20applicability%20of%20Face%0ARecognition%20%28FR%29%20systems%20to%20challenging%20scenarios%2C%20enabling%20the%20matching%20of%0Aface%20images%20across%20different%20domains%2C%20such%20as%20matching%20thermal%20images%20to%0Avisible%20spectra.%20However%2C%20the%20development%20of%20HFR%20systems%20is%20challenging%20because%0Aof%20the%20significant%20domain%20gap%20between%20modalities%20and%20the%20lack%20of%20availability%0Aof%20large-scale%20paired%20multi-channel%20data.%20In%20this%20work%2C%20we%20leverage%20a%0Apretrained%20face%20recognition%20model%20as%20a%20teacher%20network%20to%20learn%20domaininvariant%0Anetwork%20layers%20called%20Domain-Invariant%20Units%20%28DIU%29%20to%20reduce%20the%20domain%20gap.%0AThe%20proposed%20DIU%20can%20be%20trained%20effectively%20even%20with%20a%20limited%20amount%20of%0Apaired%20training%20data%2C%20in%20a%20contrastive%20distillation%20framework.%20This%20proposed%0Aapproach%20has%20the%20potential%20to%20enhance%20pretrained%20models%2C%20making%20them%20more%0Aadaptable%20to%20a%20wider%20range%20of%20variations%20in%20data.%20We%20extensively%20evaluate%20our%0Aapproach%20on%20multiple%20challenging%20benchmarks%2C%20demonstrating%20superior%20performance%0Acompared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14343v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Face%20Recognition%20Using%20Domain%20Invariant%20Units&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20aims%20to%20expand%20the%20applicability%20of%20Face%0ARecognition%20%28FR%29%20systems%20to%20challenging%20scenarios%2C%20enabling%20the%20matching%20of%0Aface%20images%20across%20different%20domains%2C%20such%20as%20matching%20thermal%20images%20to%0Avisible%20spectra.%20However%2C%20the%20development%20of%20HFR%20systems%20is%20challenging%20because%0Aof%20the%20significant%20domain%20gap%20between%20modalities%20and%20the%20lack%20of%20availability%0Aof%20large-scale%20paired%20multi-channel%20data.%20In%20this%20work%2C%20we%20leverage%20a%0Apretrained%20face%20recognition%20model%20as%20a%20teacher%20network%20to%20learn%20domaininvariant%0Anetwork%20layers%20called%20Domain-Invariant%20Units%20%28DIU%29%20to%20reduce%20the%20domain%20gap.%0AThe%20proposed%20DIU%20can%20be%20trained%20effectively%20even%20with%20a%20limited%20amount%20of%0Apaired%20training%20data%2C%20in%20a%20contrastive%20distillation%20framework.%20This%20proposed%0Aapproach%20has%20the%20potential%20to%20enhance%20pretrained%20models%2C%20making%20them%20more%0Aadaptable%20to%20a%20wider%20range%20of%20variations%20in%20data.%20We%20extensively%20evaluate%20our%0Aapproach%20on%20multiple%20challenging%20benchmarks%2C%20demonstrating%20superior%20performance%0Acompared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14343v1&entry.124074799=Read"},
{"title": "SEED-X: Multimodal Models with Unified Multi-granularity Comprehension\n  and Generation", "author": "Yuying Ge and Sijie Zhao and Jinguo Zhu and Yixiao Ge and Kun Yi and Lin Song and Chen Li and Xiaohan Ding and Ying Shan", "abstract": "  The rapid evolution of multimodal foundation model has demonstrated\nsignificant progresses in vision-language understanding and generation, e.g.,\nour previous work SEED-LLaMA. However, there remains a gap between its\ncapability and the real-world applicability, primarily due to the model's\nlimited capacity to effectively respond to various user instructions and\ninteract with diverse visual data. In this work, we focus on bridging this gap\nthrough integrating two enhanced features: (1) comprehending images of\narbitrary sizes and ratios, and (2) enabling multi-granularity image\ngeneration. We present a unified and versatile foundation model, namely,\nSEED-X, which is able to model multi-granularity visual semantics for\ncomprehension and generation tasks. Besides the competitive results on public\nbenchmarks, SEED-X demonstrates its effectiveness in handling real-world\napplications across various domains after instruction tuning. We hope that our\nwork will inspire future research into what can be achieved by versatile\nmultimodal foundation models in real-world applications. The models, codes, and\ndatasets will be released in https://github.com/AILab-CVC/SEED-X.\n", "link": "http://arxiv.org/abs/2404.14396v1", "date": "2024-04-22", "relevancy": 2.3132, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5811}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5593}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SEED-X%3A%20Multimodal%20Models%20with%20Unified%20Multi-granularity%20Comprehension%0A%20%20and%20Generation&body=Title%3A%20SEED-X%3A%20Multimodal%20Models%20with%20Unified%20Multi-granularity%20Comprehension%0A%20%20and%20Generation%0AAuthor%3A%20Yuying%20Ge%20and%20Sijie%20Zhao%20and%20Jinguo%20Zhu%20and%20Yixiao%20Ge%20and%20Kun%20Yi%20and%20Lin%20Song%20and%20Chen%20Li%20and%20Xiaohan%20Ding%20and%20Ying%20Shan%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20multimodal%20foundation%20model%20has%20demonstrated%0Asignificant%20progresses%20in%20vision-language%20understanding%20and%20generation%2C%20e.g.%2C%0Aour%20previous%20work%20SEED-LLaMA.%20However%2C%20there%20remains%20a%20gap%20between%20its%0Acapability%20and%20the%20real-world%20applicability%2C%20primarily%20due%20to%20the%20model%27s%0Alimited%20capacity%20to%20effectively%20respond%20to%20various%20user%20instructions%20and%0Ainteract%20with%20diverse%20visual%20data.%20In%20this%20work%2C%20we%20focus%20on%20bridging%20this%20gap%0Athrough%20integrating%20two%20enhanced%20features%3A%20%281%29%20comprehending%20images%20of%0Aarbitrary%20sizes%20and%20ratios%2C%20and%20%282%29%20enabling%20multi-granularity%20image%0Ageneration.%20We%20present%20a%20unified%20and%20versatile%20foundation%20model%2C%20namely%2C%0ASEED-X%2C%20which%20is%20able%20to%20model%20multi-granularity%20visual%20semantics%20for%0Acomprehension%20and%20generation%20tasks.%20Besides%20the%20competitive%20results%20on%20public%0Abenchmarks%2C%20SEED-X%20demonstrates%20its%20effectiveness%20in%20handling%20real-world%0Aapplications%20across%20various%20domains%20after%20instruction%20tuning.%20We%20hope%20that%20our%0Awork%20will%20inspire%20future%20research%20into%20what%20can%20be%20achieved%20by%20versatile%0Amultimodal%20foundation%20models%20in%20real-world%20applications.%20The%20models%2C%20codes%2C%20and%0Adatasets%20will%20be%20released%20in%20https%3A//github.com/AILab-CVC/SEED-X.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14396v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEED-X%3A%20Multimodal%20Models%20with%20Unified%20Multi-granularity%20Comprehension%0A%20%20and%20Generation&entry.906535625=Yuying%20Ge%20and%20Sijie%20Zhao%20and%20Jinguo%20Zhu%20and%20Yixiao%20Ge%20and%20Kun%20Yi%20and%20Lin%20Song%20and%20Chen%20Li%20and%20Xiaohan%20Ding%20and%20Ying%20Shan&entry.1292438233=%20%20The%20rapid%20evolution%20of%20multimodal%20foundation%20model%20has%20demonstrated%0Asignificant%20progresses%20in%20vision-language%20understanding%20and%20generation%2C%20e.g.%2C%0Aour%20previous%20work%20SEED-LLaMA.%20However%2C%20there%20remains%20a%20gap%20between%20its%0Acapability%20and%20the%20real-world%20applicability%2C%20primarily%20due%20to%20the%20model%27s%0Alimited%20capacity%20to%20effectively%20respond%20to%20various%20user%20instructions%20and%0Ainteract%20with%20diverse%20visual%20data.%20In%20this%20work%2C%20we%20focus%20on%20bridging%20this%20gap%0Athrough%20integrating%20two%20enhanced%20features%3A%20%281%29%20comprehending%20images%20of%0Aarbitrary%20sizes%20and%20ratios%2C%20and%20%282%29%20enabling%20multi-granularity%20image%0Ageneration.%20We%20present%20a%20unified%20and%20versatile%20foundation%20model%2C%20namely%2C%0ASEED-X%2C%20which%20is%20able%20to%20model%20multi-granularity%20visual%20semantics%20for%0Acomprehension%20and%20generation%20tasks.%20Besides%20the%20competitive%20results%20on%20public%0Abenchmarks%2C%20SEED-X%20demonstrates%20its%20effectiveness%20in%20handling%20real-world%0Aapplications%20across%20various%20domains%20after%20instruction%20tuning.%20We%20hope%20that%20our%0Awork%20will%20inspire%20future%20research%20into%20what%20can%20be%20achieved%20by%20versatile%0Amultimodal%20foundation%20models%20in%20real-world%20applications.%20The%20models%2C%20codes%2C%20and%0Adatasets%20will%20be%20released%20in%20https%3A//github.com/AILab-CVC/SEED-X.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14396v1&entry.124074799=Read"},
{"title": "PointDifformer: Robust Point Cloud Registration With Neural Diffusion\n  and Transformer", "author": "Rui She and Qiyu Kang and Sijie Wang and Wee Peng Tay and Kai Zhao and Yang Song and Tianyu Geng and Yi Xu and Diego Navarro Navarro and Andreas Hartmannsgruber", "abstract": "  Point cloud registration is a fundamental technique in 3-D computer vision\nwith applications in graphics, autonomous driving, and robotics. However,\nregistration tasks under challenging conditions, under which noise or\nperturbations are prevalent, can be difficult. We propose a robust point cloud\nregistration approach that leverages graph neural partial differential\nequations (PDEs) and heat kernel signatures. Our method first uses graph neural\nPDE modules to extract high dimensional features from point clouds by\naggregating information from the 3-D point neighborhood, thereby enhancing the\nrobustness of the feature representations. Then, we incorporate heat kernel\nsignatures into an attention mechanism to efficiently obtain corresponding\nkeypoints. Finally, a singular value decomposition (SVD) module with learnable\nweights is used to predict the transformation between two point clouds.\nEmpirical experiments on a 3-D point cloud dataset demonstrate that our\napproach not only achieves state-of-the-art performance for point cloud\nregistration but also exhibits better robustness to additive noise or 3-D shape\nperturbations.\n", "link": "http://arxiv.org/abs/2404.14034v1", "date": "2024-04-22", "relevancy": 2.3122, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.604}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PointDifformer%3A%20Robust%20Point%20Cloud%20Registration%20With%20Neural%20Diffusion%0A%20%20and%20Transformer&body=Title%3A%20PointDifformer%3A%20Robust%20Point%20Cloud%20Registration%20With%20Neural%20Diffusion%0A%20%20and%20Transformer%0AAuthor%3A%20Rui%20She%20and%20Qiyu%20Kang%20and%20Sijie%20Wang%20and%20Wee%20Peng%20Tay%20and%20Kai%20Zhao%20and%20Yang%20Song%20and%20Tianyu%20Geng%20and%20Yi%20Xu%20and%20Diego%20Navarro%20Navarro%20and%20Andreas%20Hartmannsgruber%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20fundamental%20technique%20in%203-D%20computer%20vision%0Awith%20applications%20in%20graphics%2C%20autonomous%20driving%2C%20and%20robotics.%20However%2C%0Aregistration%20tasks%20under%20challenging%20conditions%2C%20under%20which%20noise%20or%0Aperturbations%20are%20prevalent%2C%20can%20be%20difficult.%20We%20propose%20a%20robust%20point%20cloud%0Aregistration%20approach%20that%20leverages%20graph%20neural%20partial%20differential%0Aequations%20%28PDEs%29%20and%20heat%20kernel%20signatures.%20Our%20method%20first%20uses%20graph%20neural%0APDE%20modules%20to%20extract%20high%20dimensional%20features%20from%20point%20clouds%20by%0Aaggregating%20information%20from%20the%203-D%20point%20neighborhood%2C%20thereby%20enhancing%20the%0Arobustness%20of%20the%20feature%20representations.%20Then%2C%20we%20incorporate%20heat%20kernel%0Asignatures%20into%20an%20attention%20mechanism%20to%20efficiently%20obtain%20corresponding%0Akeypoints.%20Finally%2C%20a%20singular%20value%20decomposition%20%28SVD%29%20module%20with%20learnable%0Aweights%20is%20used%20to%20predict%20the%20transformation%20between%20two%20point%20clouds.%0AEmpirical%20experiments%20on%20a%203-D%20point%20cloud%20dataset%20demonstrate%20that%20our%0Aapproach%20not%20only%20achieves%20state-of-the-art%20performance%20for%20point%20cloud%0Aregistration%20but%20also%20exhibits%20better%20robustness%20to%20additive%20noise%20or%203-D%20shape%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14034v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointDifformer%3A%20Robust%20Point%20Cloud%20Registration%20With%20Neural%20Diffusion%0A%20%20and%20Transformer&entry.906535625=Rui%20She%20and%20Qiyu%20Kang%20and%20Sijie%20Wang%20and%20Wee%20Peng%20Tay%20and%20Kai%20Zhao%20and%20Yang%20Song%20and%20Tianyu%20Geng%20and%20Yi%20Xu%20and%20Diego%20Navarro%20Navarro%20and%20Andreas%20Hartmannsgruber&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20fundamental%20technique%20in%203-D%20computer%20vision%0Awith%20applications%20in%20graphics%2C%20autonomous%20driving%2C%20and%20robotics.%20However%2C%0Aregistration%20tasks%20under%20challenging%20conditions%2C%20under%20which%20noise%20or%0Aperturbations%20are%20prevalent%2C%20can%20be%20difficult.%20We%20propose%20a%20robust%20point%20cloud%0Aregistration%20approach%20that%20leverages%20graph%20neural%20partial%20differential%0Aequations%20%28PDEs%29%20and%20heat%20kernel%20signatures.%20Our%20method%20first%20uses%20graph%20neural%0APDE%20modules%20to%20extract%20high%20dimensional%20features%20from%20point%20clouds%20by%0Aaggregating%20information%20from%20the%203-D%20point%20neighborhood%2C%20thereby%20enhancing%20the%0Arobustness%20of%20the%20feature%20representations.%20Then%2C%20we%20incorporate%20heat%20kernel%0Asignatures%20into%20an%20attention%20mechanism%20to%20efficiently%20obtain%20corresponding%0Akeypoints.%20Finally%2C%20a%20singular%20value%20decomposition%20%28SVD%29%20module%20with%20learnable%0Aweights%20is%20used%20to%20predict%20the%20transformation%20between%20two%20point%20clouds.%0AEmpirical%20experiments%20on%20a%203-D%20point%20cloud%20dataset%20demonstrate%20that%20our%0Aapproach%20not%20only%20achieves%20state-of-the-art%20performance%20for%20point%20cloud%0Aregistration%20but%20also%20exhibits%20better%20robustness%20to%20additive%20noise%20or%203-D%20shape%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14034v1&entry.124074799=Read"},
{"title": "Harnessing Orthogonality to Train Low-Rank Neural Networks", "author": "Daniel Coquelin and Katharina Fl\u00fcgel and Marie Weiel and Nicholas Kiefer and Charlotte Debus and Achim Streit and Markus G\u00f6tz", "abstract": "  This study explores the learning dynamics of neural networks by analyzing the\nsingular value decomposition (SVD) of their weights throughout training. Our\ninvestigation reveals that an orthogonal basis within each multidimensional\nweight's SVD representation stabilizes during training. Building upon this, we\nintroduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel\ntraining method exploiting the intrinsic orthogonality of neural networks.\nOIALR seamlessly integrates into existing training workflows with minimal\naccuracy loss, as demonstrated by benchmarking on various datasets and\nwell-established network architectures. With appropriate hyperparameter tuning,\nOIALR can surpass conventional training setups, including those of\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2401.08505v2", "date": "2024-04-22", "relevancy": 2.3091, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Orthogonality%20to%20Train%20Low-Rank%20Neural%20Networks&body=Title%3A%20Harnessing%20Orthogonality%20to%20Train%20Low-Rank%20Neural%20Networks%0AAuthor%3A%20Daniel%20Coquelin%20and%20Katharina%20Fl%C3%BCgel%20and%20Marie%20Weiel%20and%20Nicholas%20Kiefer%20and%20Charlotte%20Debus%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz%0AAbstract%3A%20%20%20This%20study%20explores%20the%20learning%20dynamics%20of%20neural%20networks%20by%20analyzing%20the%0Asingular%20value%20decomposition%20%28SVD%29%20of%20their%20weights%20throughout%20training.%20Our%0Ainvestigation%20reveals%20that%20an%20orthogonal%20basis%20within%20each%20multidimensional%0Aweight%27s%20SVD%20representation%20stabilizes%20during%20training.%20Building%20upon%20this%2C%20we%0Aintroduce%20Orthogonality-Informed%20Adaptive%20Low-Rank%20%28OIALR%29%20training%2C%20a%20novel%0Atraining%20method%20exploiting%20the%20intrinsic%20orthogonality%20of%20neural%20networks.%0AOIALR%20seamlessly%20integrates%20into%20existing%20training%20workflows%20with%20minimal%0Aaccuracy%20loss%2C%20as%20demonstrated%20by%20benchmarking%20on%20various%20datasets%20and%0Awell-established%20network%20architectures.%20With%20appropriate%20hyperparameter%20tuning%2C%0AOIALR%20can%20surpass%20conventional%20training%20setups%2C%20including%20those%20of%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08505v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Orthogonality%20to%20Train%20Low-Rank%20Neural%20Networks&entry.906535625=Daniel%20Coquelin%20and%20Katharina%20Fl%C3%BCgel%20and%20Marie%20Weiel%20and%20Nicholas%20Kiefer%20and%20Charlotte%20Debus%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz&entry.1292438233=%20%20This%20study%20explores%20the%20learning%20dynamics%20of%20neural%20networks%20by%20analyzing%20the%0Asingular%20value%20decomposition%20%28SVD%29%20of%20their%20weights%20throughout%20training.%20Our%0Ainvestigation%20reveals%20that%20an%20orthogonal%20basis%20within%20each%20multidimensional%0Aweight%27s%20SVD%20representation%20stabilizes%20during%20training.%20Building%20upon%20this%2C%20we%0Aintroduce%20Orthogonality-Informed%20Adaptive%20Low-Rank%20%28OIALR%29%20training%2C%20a%20novel%0Atraining%20method%20exploiting%20the%20intrinsic%20orthogonality%20of%20neural%20networks.%0AOIALR%20seamlessly%20integrates%20into%20existing%20training%20workflows%20with%20minimal%0Aaccuracy%20loss%2C%20as%20demonstrated%20by%20benchmarking%20on%20various%20datasets%20and%0Awell-established%20network%20architectures.%20With%20appropriate%20hyperparameter%20tuning%2C%0AOIALR%20can%20surpass%20conventional%20training%20setups%2C%20including%20those%20of%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08505v2&entry.124074799=Read"},
{"title": "Empowering Diffusion Models on the Embedding Space for Text Generation", "author": "Zhujin Gao and Junliang Guo and Xu Tan and Yongxin Zhu and Fang Zhang and Jiang Bian and Linli Xu", "abstract": "  Diffusion models have achieved state-of-the-art synthesis quality on both\nvisual and audio tasks, and recent works further adapt them to textual data by\ndiffusing on the embedding space. In this paper, we conduct systematic studies\nof the optimization challenges encountered with both the embedding space and\nthe denoising model, which have not been carefully explored. Firstly, the data\ndistribution is learnable for embeddings, which may lead to the collapse of the\nembedding space and unstable training. To alleviate this problem, we propose a\nnew objective called the anchor loss which is more efficient than previous\nmethods. Secondly, we find the noise levels of conventional schedules are\ninsufficient for training a desirable denoising model while introducing varying\ndegrees of degeneration in consequence. To address this challenge, we propose a\nnovel framework called noise rescaling. Based on the above analysis, we propose\nDifformer, an embedding diffusion model based on Transformer. Experiments on\nvarieties of seminal text generation tasks show the effectiveness of the\nproposed methods and the superiority of Difformer over previous\nstate-of-the-art embedding diffusion baselines.\n", "link": "http://arxiv.org/abs/2212.09412v3", "date": "2024-04-22", "relevancy": 2.3037, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6521}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5617}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Empowering%20Diffusion%20Models%20on%20the%20Embedding%20Space%20for%20Text%20Generation&body=Title%3A%20Empowering%20Diffusion%20Models%20on%20the%20Embedding%20Space%20for%20Text%20Generation%0AAuthor%3A%20Zhujin%20Gao%20and%20Junliang%20Guo%20and%20Xu%20Tan%20and%20Yongxin%20Zhu%20and%20Fang%20Zhang%20and%20Jiang%20Bian%20and%20Linli%20Xu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20synthesis%20quality%20on%20both%0Avisual%20and%20audio%20tasks%2C%20and%20recent%20works%20further%20adapt%20them%20to%20textual%20data%20by%0Adiffusing%20on%20the%20embedding%20space.%20In%20this%20paper%2C%20we%20conduct%20systematic%20studies%0Aof%20the%20optimization%20challenges%20encountered%20with%20both%20the%20embedding%20space%20and%0Athe%20denoising%20model%2C%20which%20have%20not%20been%20carefully%20explored.%20Firstly%2C%20the%20data%0Adistribution%20is%20learnable%20for%20embeddings%2C%20which%20may%20lead%20to%20the%20collapse%20of%20the%0Aembedding%20space%20and%20unstable%20training.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%0Anew%20objective%20called%20the%20anchor%20loss%20which%20is%20more%20efficient%20than%20previous%0Amethods.%20Secondly%2C%20we%20find%20the%20noise%20levels%20of%20conventional%20schedules%20are%0Ainsufficient%20for%20training%20a%20desirable%20denoising%20model%20while%20introducing%20varying%0Adegrees%20of%20degeneration%20in%20consequence.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20framework%20called%20noise%20rescaling.%20Based%20on%20the%20above%20analysis%2C%20we%20propose%0ADifformer%2C%20an%20embedding%20diffusion%20model%20based%20on%20Transformer.%20Experiments%20on%0Avarieties%20of%20seminal%20text%20generation%20tasks%20show%20the%20effectiveness%20of%20the%0Aproposed%20methods%20and%20the%20superiority%20of%20Difformer%20over%20previous%0Astate-of-the-art%20embedding%20diffusion%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.09412v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Diffusion%20Models%20on%20the%20Embedding%20Space%20for%20Text%20Generation&entry.906535625=Zhujin%20Gao%20and%20Junliang%20Guo%20and%20Xu%20Tan%20and%20Yongxin%20Zhu%20and%20Fang%20Zhang%20and%20Jiang%20Bian%20and%20Linli%20Xu&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20synthesis%20quality%20on%20both%0Avisual%20and%20audio%20tasks%2C%20and%20recent%20works%20further%20adapt%20them%20to%20textual%20data%20by%0Adiffusing%20on%20the%20embedding%20space.%20In%20this%20paper%2C%20we%20conduct%20systematic%20studies%0Aof%20the%20optimization%20challenges%20encountered%20with%20both%20the%20embedding%20space%20and%0Athe%20denoising%20model%2C%20which%20have%20not%20been%20carefully%20explored.%20Firstly%2C%20the%20data%0Adistribution%20is%20learnable%20for%20embeddings%2C%20which%20may%20lead%20to%20the%20collapse%20of%20the%0Aembedding%20space%20and%20unstable%20training.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%0Anew%20objective%20called%20the%20anchor%20loss%20which%20is%20more%20efficient%20than%20previous%0Amethods.%20Secondly%2C%20we%20find%20the%20noise%20levels%20of%20conventional%20schedules%20are%0Ainsufficient%20for%20training%20a%20desirable%20denoising%20model%20while%20introducing%20varying%0Adegrees%20of%20degeneration%20in%20consequence.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20framework%20called%20noise%20rescaling.%20Based%20on%20the%20above%20analysis%2C%20we%20propose%0ADifformer%2C%20an%20embedding%20diffusion%20model%20based%20on%20Transformer.%20Experiments%20on%0Avarieties%20of%20seminal%20text%20generation%20tasks%20show%20the%20effectiveness%20of%20the%0Aproposed%20methods%20and%20the%20superiority%20of%20Difformer%20over%20previous%0Astate-of-the-art%20embedding%20diffusion%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.09412v3&entry.124074799=Read"},
{"title": "Graphic Design with Large Multimodal Model", "author": "Yutao Cheng and Zhao Zhang and Maoke Yang and Hui Nie and Chunyuan Li and Xinglong Wu and Jie Shao", "abstract": "  In the field of graphic design, automating the integration of design elements\ninto a cohesive multi-layered artwork not only boosts productivity but also\npaves the way for the democratization of graphic design. One existing practice\nis Graphic Layout Generation (GLG), which aims to layout sequential design\nelements. It has been constrained by the necessity for a predefined correct\nsequence of layers, thus limiting creative potential and increasing user\nworkload. In this paper, we present Hierarchical Layout Generation (HLG) as a\nmore flexible and pragmatic setup, which creates graphic composition from\nunordered sets of design elements. To tackle the HLG task, we introduce\nGraphist, the first layout generation model based on large multimodal models.\nGraphist efficiently reframes the HLG as a sequence generation problem,\nutilizing RGB-A images as input, outputs a JSON draft protocol, indicating the\ncoordinates, size, and order of each element. We develop new evaluation metrics\nfor HLG. Graphist outperforms prior arts and establishes a strong baseline for\nthis field. Project homepage: https://github.com/graphic-design-ai/graphist\n", "link": "http://arxiv.org/abs/2404.14368v1", "date": "2024-04-22", "relevancy": 2.2992, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5987}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5699}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5528}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graphic%20Design%20with%20Large%20Multimodal%20Model&body=Title%3A%20Graphic%20Design%20with%20Large%20Multimodal%20Model%0AAuthor%3A%20Yutao%20Cheng%20and%20Zhao%20Zhang%20and%20Maoke%20Yang%20and%20Hui%20Nie%20and%20Chunyuan%20Li%20and%20Xinglong%20Wu%20and%20Jie%20Shao%0AAbstract%3A%20%20%20In%20the%20field%20of%20graphic%20design%2C%20automating%20the%20integration%20of%20design%20elements%0Ainto%20a%20cohesive%20multi-layered%20artwork%20not%20only%20boosts%20productivity%20but%20also%0Apaves%20the%20way%20for%20the%20democratization%20of%20graphic%20design.%20One%20existing%20practice%0Ais%20Graphic%20Layout%20Generation%20%28GLG%29%2C%20which%20aims%20to%20layout%20sequential%20design%0Aelements.%20It%20has%20been%20constrained%20by%20the%20necessity%20for%20a%20predefined%20correct%0Asequence%20of%20layers%2C%20thus%20limiting%20creative%20potential%20and%20increasing%20user%0Aworkload.%20In%20this%20paper%2C%20we%20present%20Hierarchical%20Layout%20Generation%20%28HLG%29%20as%20a%0Amore%20flexible%20and%20pragmatic%20setup%2C%20which%20creates%20graphic%20composition%20from%0Aunordered%20sets%20of%20design%20elements.%20To%20tackle%20the%20HLG%20task%2C%20we%20introduce%0AGraphist%2C%20the%20first%20layout%20generation%20model%20based%20on%20large%20multimodal%20models.%0AGraphist%20efficiently%20reframes%20the%20HLG%20as%20a%20sequence%20generation%20problem%2C%0Autilizing%20RGB-A%20images%20as%20input%2C%20outputs%20a%20JSON%20draft%20protocol%2C%20indicating%20the%0Acoordinates%2C%20size%2C%20and%20order%20of%20each%20element.%20We%20develop%20new%20evaluation%20metrics%0Afor%20HLG.%20Graphist%20outperforms%20prior%20arts%20and%20establishes%20a%20strong%20baseline%20for%0Athis%20field.%20Project%20homepage%3A%20https%3A//github.com/graphic-design-ai/graphist%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14368v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graphic%20Design%20with%20Large%20Multimodal%20Model&entry.906535625=Yutao%20Cheng%20and%20Zhao%20Zhang%20and%20Maoke%20Yang%20and%20Hui%20Nie%20and%20Chunyuan%20Li%20and%20Xinglong%20Wu%20and%20Jie%20Shao&entry.1292438233=%20%20In%20the%20field%20of%20graphic%20design%2C%20automating%20the%20integration%20of%20design%20elements%0Ainto%20a%20cohesive%20multi-layered%20artwork%20not%20only%20boosts%20productivity%20but%20also%0Apaves%20the%20way%20for%20the%20democratization%20of%20graphic%20design.%20One%20existing%20practice%0Ais%20Graphic%20Layout%20Generation%20%28GLG%29%2C%20which%20aims%20to%20layout%20sequential%20design%0Aelements.%20It%20has%20been%20constrained%20by%20the%20necessity%20for%20a%20predefined%20correct%0Asequence%20of%20layers%2C%20thus%20limiting%20creative%20potential%20and%20increasing%20user%0Aworkload.%20In%20this%20paper%2C%20we%20present%20Hierarchical%20Layout%20Generation%20%28HLG%29%20as%20a%0Amore%20flexible%20and%20pragmatic%20setup%2C%20which%20creates%20graphic%20composition%20from%0Aunordered%20sets%20of%20design%20elements.%20To%20tackle%20the%20HLG%20task%2C%20we%20introduce%0AGraphist%2C%20the%20first%20layout%20generation%20model%20based%20on%20large%20multimodal%20models.%0AGraphist%20efficiently%20reframes%20the%20HLG%20as%20a%20sequence%20generation%20problem%2C%0Autilizing%20RGB-A%20images%20as%20input%2C%20outputs%20a%20JSON%20draft%20protocol%2C%20indicating%20the%0Acoordinates%2C%20size%2C%20and%20order%20of%20each%20element.%20We%20develop%20new%20evaluation%20metrics%0Afor%20HLG.%20Graphist%20outperforms%20prior%20arts%20and%20establishes%20a%20strong%20baseline%20for%0Athis%20field.%20Project%20homepage%3A%20https%3A//github.com/graphic-design-ai/graphist%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14368v1&entry.124074799=Read"},
{"title": "Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\n  Propagation", "author": "Paulo Yanez Sarmiento and Simon Witzke and Nadja Klein and Bernhard Y. Renard", "abstract": "  Explainability is a key component in many applications involving deep neural\nnetworks (DNNs). However, current explanation methods for DNNs commonly leave\nit to the human observer to distinguish relevant explanations from spurious\nnoise. This is not feasible anymore when going from easily human-accessible\ndata such as images to more complex data such as genome sequences. To\nfacilitate the accessibility of DNN outputs from such complex data and to\nincrease explainability, we present a modification of the widely used\nexplanation method layer-wise relevance propagation. Our approach enforces\nsparsity directly by pruning the relevance propagation for the different\nlayers. Thereby, we achieve sparser relevance attributions for the input\nfeatures as well as for the intermediate layers. As the relevance propagation\nis input-specific, we aim to prune the relevance propagation rather than the\nunderlying model architecture. This allows to prune different neurons for\ndifferent inputs and hence, might be more appropriate to the local nature of\nexplanation methods. To demonstrate the efficacy of our method, we evaluate it\non two types of data, images and genomic sequences. We show that our\nmodification indeed leads to noise reduction and concentrates relevance on the\nmost important features compared to the baseline.\n", "link": "http://arxiv.org/abs/2404.14271v1", "date": "2024-04-22", "relevancy": 2.2697, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4545}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4528}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation&body=Title%3A%20Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation%0AAuthor%3A%20Paulo%20Yanez%20Sarmiento%20and%20Simon%20Witzke%20and%20Nadja%20Klein%20and%20Bernhard%20Y.%20Renard%0AAbstract%3A%20%20%20Explainability%20is%20a%20key%20component%20in%20many%20applications%20involving%20deep%20neural%0Anetworks%20%28DNNs%29.%20However%2C%20current%20explanation%20methods%20for%20DNNs%20commonly%20leave%0Ait%20to%20the%20human%20observer%20to%20distinguish%20relevant%20explanations%20from%20spurious%0Anoise.%20This%20is%20not%20feasible%20anymore%20when%20going%20from%20easily%20human-accessible%0Adata%20such%20as%20images%20to%20more%20complex%20data%20such%20as%20genome%20sequences.%20To%0Afacilitate%20the%20accessibility%20of%20DNN%20outputs%20from%20such%20complex%20data%20and%20to%0Aincrease%20explainability%2C%20we%20present%20a%20modification%20of%20the%20widely%20used%0Aexplanation%20method%20layer-wise%20relevance%20propagation.%20Our%20approach%20enforces%0Asparsity%20directly%20by%20pruning%20the%20relevance%20propagation%20for%20the%20different%0Alayers.%20Thereby%2C%20we%20achieve%20sparser%20relevance%20attributions%20for%20the%20input%0Afeatures%20as%20well%20as%20for%20the%20intermediate%20layers.%20As%20the%20relevance%20propagation%0Ais%20input-specific%2C%20we%20aim%20to%20prune%20the%20relevance%20propagation%20rather%20than%20the%0Aunderlying%20model%20architecture.%20This%20allows%20to%20prune%20different%20neurons%20for%0Adifferent%20inputs%20and%20hence%2C%20might%20be%20more%20appropriate%20to%20the%20local%20nature%20of%0Aexplanation%20methods.%20To%20demonstrate%20the%20efficacy%20of%20our%20method%2C%20we%20evaluate%20it%0Aon%20two%20types%20of%20data%2C%20images%20and%20genomic%20sequences.%20We%20show%20that%20our%0Amodification%20indeed%20leads%20to%20noise%20reduction%20and%20concentrates%20relevance%20on%20the%0Amost%20important%20features%20compared%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14271v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation&entry.906535625=Paulo%20Yanez%20Sarmiento%20and%20Simon%20Witzke%20and%20Nadja%20Klein%20and%20Bernhard%20Y.%20Renard&entry.1292438233=%20%20Explainability%20is%20a%20key%20component%20in%20many%20applications%20involving%20deep%20neural%0Anetworks%20%28DNNs%29.%20However%2C%20current%20explanation%20methods%20for%20DNNs%20commonly%20leave%0Ait%20to%20the%20human%20observer%20to%20distinguish%20relevant%20explanations%20from%20spurious%0Anoise.%20This%20is%20not%20feasible%20anymore%20when%20going%20from%20easily%20human-accessible%0Adata%20such%20as%20images%20to%20more%20complex%20data%20such%20as%20genome%20sequences.%20To%0Afacilitate%20the%20accessibility%20of%20DNN%20outputs%20from%20such%20complex%20data%20and%20to%0Aincrease%20explainability%2C%20we%20present%20a%20modification%20of%20the%20widely%20used%0Aexplanation%20method%20layer-wise%20relevance%20propagation.%20Our%20approach%20enforces%0Asparsity%20directly%20by%20pruning%20the%20relevance%20propagation%20for%20the%20different%0Alayers.%20Thereby%2C%20we%20achieve%20sparser%20relevance%20attributions%20for%20the%20input%0Afeatures%20as%20well%20as%20for%20the%20intermediate%20layers.%20As%20the%20relevance%20propagation%0Ais%20input-specific%2C%20we%20aim%20to%20prune%20the%20relevance%20propagation%20rather%20than%20the%0Aunderlying%20model%20architecture.%20This%20allows%20to%20prune%20different%20neurons%20for%0Adifferent%20inputs%20and%20hence%2C%20might%20be%20more%20appropriate%20to%20the%20local%20nature%20of%0Aexplanation%20methods.%20To%20demonstrate%20the%20efficacy%20of%20our%20method%2C%20we%20evaluate%20it%0Aon%20two%20types%20of%20data%2C%20images%20and%20genomic%20sequences.%20We%20show%20that%20our%0Amodification%20indeed%20leads%20to%20noise%20reduction%20and%20concentrates%20relevance%20on%20the%0Amost%20important%20features%20compared%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14271v1&entry.124074799=Read"},
{"title": "Multilevel Geometric Optimization for Regularised Constrained Linear\n  Inverse Problems", "author": "Sebastian M\u00fcller and Stefania Petra and Matthias Zisler", "abstract": "  We present a geometric multilevel optimization approach that smoothly\nincorporates box constraints. Given a box constrained optimization problem, we\nconsider a hierarchy of models with varying discretization levels. Finer models\nare accurate but expensive to compute, while coarser models are less accurate\nbut cheaper to compute. When working at the fine level, multilevel optimisation\ncomputes the search direction based on a coarser model which speeds up updates\nat the fine level. Moreover, exploiting geometry induced by the hierarchy the\nfeasibility of the updates is preserved. In particular, our approach extends\nclassical components of multigrid methods like restriction and prolongation to\nthe Riemannian structure of our constraints.\n", "link": "http://arxiv.org/abs/2207.04934v3", "date": "2024-04-22", "relevancy": 2.2157, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4537}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.437}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multilevel%20Geometric%20Optimization%20for%20Regularised%20Constrained%20Linear%0A%20%20Inverse%20Problems&body=Title%3A%20Multilevel%20Geometric%20Optimization%20for%20Regularised%20Constrained%20Linear%0A%20%20Inverse%20Problems%0AAuthor%3A%20Sebastian%20M%C3%BCller%20and%20Stefania%20Petra%20and%20Matthias%20Zisler%0AAbstract%3A%20%20%20We%20present%20a%20geometric%20multilevel%20optimization%20approach%20that%20smoothly%0Aincorporates%20box%20constraints.%20Given%20a%20box%20constrained%20optimization%20problem%2C%20we%0Aconsider%20a%20hierarchy%20of%20models%20with%20varying%20discretization%20levels.%20Finer%20models%0Aare%20accurate%20but%20expensive%20to%20compute%2C%20while%20coarser%20models%20are%20less%20accurate%0Abut%20cheaper%20to%20compute.%20When%20working%20at%20the%20fine%20level%2C%20multilevel%20optimisation%0Acomputes%20the%20search%20direction%20based%20on%20a%20coarser%20model%20which%20speeds%20up%20updates%0Aat%20the%20fine%20level.%20Moreover%2C%20exploiting%20geometry%20induced%20by%20the%20hierarchy%20the%0Afeasibility%20of%20the%20updates%20is%20preserved.%20In%20particular%2C%20our%20approach%20extends%0Aclassical%20components%20of%20multigrid%20methods%20like%20restriction%20and%20prolongation%20to%0Athe%20Riemannian%20structure%20of%20our%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.04934v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilevel%20Geometric%20Optimization%20for%20Regularised%20Constrained%20Linear%0A%20%20Inverse%20Problems&entry.906535625=Sebastian%20M%C3%BCller%20and%20Stefania%20Petra%20and%20Matthias%20Zisler&entry.1292438233=%20%20We%20present%20a%20geometric%20multilevel%20optimization%20approach%20that%20smoothly%0Aincorporates%20box%20constraints.%20Given%20a%20box%20constrained%20optimization%20problem%2C%20we%0Aconsider%20a%20hierarchy%20of%20models%20with%20varying%20discretization%20levels.%20Finer%20models%0Aare%20accurate%20but%20expensive%20to%20compute%2C%20while%20coarser%20models%20are%20less%20accurate%0Abut%20cheaper%20to%20compute.%20When%20working%20at%20the%20fine%20level%2C%20multilevel%20optimisation%0Acomputes%20the%20search%20direction%20based%20on%20a%20coarser%20model%20which%20speeds%20up%20updates%0Aat%20the%20fine%20level.%20Moreover%2C%20exploiting%20geometry%20induced%20by%20the%20hierarchy%20the%0Afeasibility%20of%20the%20updates%20is%20preserved.%20In%20particular%2C%20our%20approach%20extends%0Aclassical%20components%20of%20multigrid%20methods%20like%20restriction%20and%20prolongation%20to%0Athe%20Riemannian%20structure%20of%20our%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.04934v3&entry.124074799=Read"},
{"title": "Think Twice Before Selection: Federated Evidential Active Learning for\n  Medical Image Analysis with Domain Shifts", "author": "Jiayi Chen and Benteng Ma and Hengfei Cui and Yong Xia", "abstract": "  Federated learning facilitates the collaborative learning of a global model\nacross multiple distributed medical institutions without centralizing data.\nNevertheless, the expensive cost of annotation on local clients remains an\nobstacle to effectively utilizing local data. To mitigate this issue, federated\nactive learning methods suggest leveraging local and global model predictions\nto select a relatively small amount of informative local data for annotation.\nHowever, existing methods mainly focus on all local data sampled from the same\ndomain, making them unreliable in realistic medical scenarios with domain\nshifts among different clients. In this paper, we make the first attempt to\nassess the informativeness of local data derived from diverse domains and\npropose a novel methodology termed Federated Evidential Active Learning (FEAL)\nto calibrate the data evaluation under domain shift. Specifically, we introduce\na Dirichlet prior distribution in both local and global models to treat the\nprediction as a distribution over the probability simplex and capture both\naleatoric and epistemic uncertainties by using the Dirichlet-based evidential\nmodel. Then we employ the epistemic uncertainty to calibrate the aleatoric\nuncertainty. Afterward, we design a diversity relaxation strategy to reduce\ndata redundancy and maintain data diversity. Extensive experiments and analysis\non five real multi-center medical image datasets demonstrate the superiority of\nFEAL over the state-of-the-art active learning methods in federated scenarios\nwith domain shifts. The code will be available at\nhttps://github.com/JiayiChen815/FEAL.\n", "link": "http://arxiv.org/abs/2312.02567v2", "date": "2024-04-22", "relevancy": 2.2151, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6213}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5523}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Think%20Twice%20Before%20Selection%3A%20Federated%20Evidential%20Active%20Learning%20for%0A%20%20Medical%20Image%20Analysis%20with%20Domain%20Shifts&body=Title%3A%20Think%20Twice%20Before%20Selection%3A%20Federated%20Evidential%20Active%20Learning%20for%0A%20%20Medical%20Image%20Analysis%20with%20Domain%20Shifts%0AAuthor%3A%20Jiayi%20Chen%20and%20Benteng%20Ma%20and%20Hengfei%20Cui%20and%20Yong%20Xia%0AAbstract%3A%20%20%20Federated%20learning%20facilitates%20the%20collaborative%20learning%20of%20a%20global%20model%0Aacross%20multiple%20distributed%20medical%20institutions%20without%20centralizing%20data.%0ANevertheless%2C%20the%20expensive%20cost%20of%20annotation%20on%20local%20clients%20remains%20an%0Aobstacle%20to%20effectively%20utilizing%20local%20data.%20To%20mitigate%20this%20issue%2C%20federated%0Aactive%20learning%20methods%20suggest%20leveraging%20local%20and%20global%20model%20predictions%0Ato%20select%20a%20relatively%20small%20amount%20of%20informative%20local%20data%20for%20annotation.%0AHowever%2C%20existing%20methods%20mainly%20focus%20on%20all%20local%20data%20sampled%20from%20the%20same%0Adomain%2C%20making%20them%20unreliable%20in%20realistic%20medical%20scenarios%20with%20domain%0Ashifts%20among%20different%20clients.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%20to%0Aassess%20the%20informativeness%20of%20local%20data%20derived%20from%20diverse%20domains%20and%0Apropose%20a%20novel%20methodology%20termed%20Federated%20Evidential%20Active%20Learning%20%28FEAL%29%0Ato%20calibrate%20the%20data%20evaluation%20under%20domain%20shift.%20Specifically%2C%20we%20introduce%0Aa%20Dirichlet%20prior%20distribution%20in%20both%20local%20and%20global%20models%20to%20treat%20the%0Aprediction%20as%20a%20distribution%20over%20the%20probability%20simplex%20and%20capture%20both%0Aaleatoric%20and%20epistemic%20uncertainties%20by%20using%20the%20Dirichlet-based%20evidential%0Amodel.%20Then%20we%20employ%20the%20epistemic%20uncertainty%20to%20calibrate%20the%20aleatoric%0Auncertainty.%20Afterward%2C%20we%20design%20a%20diversity%20relaxation%20strategy%20to%20reduce%0Adata%20redundancy%20and%20maintain%20data%20diversity.%20Extensive%20experiments%20and%20analysis%0Aon%20five%20real%20multi-center%20medical%20image%20datasets%20demonstrate%20the%20superiority%20of%0AFEAL%20over%20the%20state-of-the-art%20active%20learning%20methods%20in%20federated%20scenarios%0Awith%20domain%20shifts.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/JiayiChen815/FEAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02567v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Twice%20Before%20Selection%3A%20Federated%20Evidential%20Active%20Learning%20for%0A%20%20Medical%20Image%20Analysis%20with%20Domain%20Shifts&entry.906535625=Jiayi%20Chen%20and%20Benteng%20Ma%20and%20Hengfei%20Cui%20and%20Yong%20Xia&entry.1292438233=%20%20Federated%20learning%20facilitates%20the%20collaborative%20learning%20of%20a%20global%20model%0Aacross%20multiple%20distributed%20medical%20institutions%20without%20centralizing%20data.%0ANevertheless%2C%20the%20expensive%20cost%20of%20annotation%20on%20local%20clients%20remains%20an%0Aobstacle%20to%20effectively%20utilizing%20local%20data.%20To%20mitigate%20this%20issue%2C%20federated%0Aactive%20learning%20methods%20suggest%20leveraging%20local%20and%20global%20model%20predictions%0Ato%20select%20a%20relatively%20small%20amount%20of%20informative%20local%20data%20for%20annotation.%0AHowever%2C%20existing%20methods%20mainly%20focus%20on%20all%20local%20data%20sampled%20from%20the%20same%0Adomain%2C%20making%20them%20unreliable%20in%20realistic%20medical%20scenarios%20with%20domain%0Ashifts%20among%20different%20clients.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%20to%0Aassess%20the%20informativeness%20of%20local%20data%20derived%20from%20diverse%20domains%20and%0Apropose%20a%20novel%20methodology%20termed%20Federated%20Evidential%20Active%20Learning%20%28FEAL%29%0Ato%20calibrate%20the%20data%20evaluation%20under%20domain%20shift.%20Specifically%2C%20we%20introduce%0Aa%20Dirichlet%20prior%20distribution%20in%20both%20local%20and%20global%20models%20to%20treat%20the%0Aprediction%20as%20a%20distribution%20over%20the%20probability%20simplex%20and%20capture%20both%0Aaleatoric%20and%20epistemic%20uncertainties%20by%20using%20the%20Dirichlet-based%20evidential%0Amodel.%20Then%20we%20employ%20the%20epistemic%20uncertainty%20to%20calibrate%20the%20aleatoric%0Auncertainty.%20Afterward%2C%20we%20design%20a%20diversity%20relaxation%20strategy%20to%20reduce%0Adata%20redundancy%20and%20maintain%20data%20diversity.%20Extensive%20experiments%20and%20analysis%0Aon%20five%20real%20multi-center%20medical%20image%20datasets%20demonstrate%20the%20superiority%20of%0AFEAL%20over%20the%20state-of-the-art%20active%20learning%20methods%20in%20federated%20scenarios%0Awith%20domain%20shifts.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/JiayiChen815/FEAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02567v2&entry.124074799=Read"},
{"title": "CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and\n  View-consistent 3D Semantic Understanding", "author": "Guibiao Liao and Jiankun Li and Zhenyu Bao and Xiaoqing Ye and Jingdong Wang and Qing Li and Kanglin Liu", "abstract": "  The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time\nsynthesis of novel views in 3D scenes. Currently, it primarily focuses on\ngeometry and appearance modeling, while lacking the semantic understanding of\nscenes. To bridge this gap, we present CLIP-GS, which integrates semantics from\nContrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to\nefficiently comprehend 3D environments without annotated semantic data. In\nspecific, rather than straightforwardly learning and rendering high-dimensional\nsemantic features of 3D Gaussians, which significantly diminishes the\nefficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC\nexploits the inherent unified semantics within objects to learn compact yet\neffective semantic representations of 3D Gaussians, enabling highly efficient\nrendering (>100 FPS). Additionally, to address the semantic ambiguity, caused\nby utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we\nintroduce a 3D Coherent Self-training (3DCS) strategy, resorting to the\nmulti-view consistency originated from the 3D model. 3DCS imposes cross-view\nsemantic consistency constraints by leveraging refined, self-predicted\npseudo-labels derived from the trained 3D Gaussian model, thereby enhancing\nprecise and view-consistent segmentation results. Extensive experiments\ndemonstrate that our method remarkably outperforms existing state-of-the-art\napproaches, achieving improvements of 17.29% and 20.81% in mIoU metric on\nReplica and ScanNet datasets, respectively, while maintaining real-time\nrendering speed. Furthermore, our approach exhibits superior performance even\nwith sparse input data, verifying the robustness of our method.\n", "link": "http://arxiv.org/abs/2404.14249v1", "date": "2024-04-22", "relevancy": 2.2019, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5894}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5312}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLIP-GS%3A%20CLIP-Informed%20Gaussian%20Splatting%20for%20Real-time%20and%0A%20%20View-consistent%203D%20Semantic%20Understanding&body=Title%3A%20CLIP-GS%3A%20CLIP-Informed%20Gaussian%20Splatting%20for%20Real-time%20and%0A%20%20View-consistent%203D%20Semantic%20Understanding%0AAuthor%3A%20Guibiao%20Liao%20and%20Jiankun%20Li%20and%20Zhenyu%20Bao%20and%20Xiaoqing%20Ye%20and%20Jingdong%20Wang%20and%20Qing%20Li%20and%20Kanglin%20Liu%0AAbstract%3A%20%20%20The%20recent%203D%20Gaussian%20Splatting%20%28GS%29%20exhibits%20high-quality%20and%20real-time%0Asynthesis%20of%20novel%20views%20in%203D%20scenes.%20Currently%2C%20it%20primarily%20focuses%20on%0Ageometry%20and%20appearance%20modeling%2C%20while%20lacking%20the%20semantic%20understanding%20of%0Ascenes.%20To%20bridge%20this%20gap%2C%20we%20present%20CLIP-GS%2C%20which%20integrates%20semantics%20from%0AContrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20into%20Gaussian%20Splatting%20to%0Aefficiently%20comprehend%203D%20environments%20without%20annotated%20semantic%20data.%20In%0Aspecific%2C%20rather%20than%20straightforwardly%20learning%20and%20rendering%20high-dimensional%0Asemantic%20features%20of%203D%20Gaussians%2C%20which%20significantly%20diminishes%20the%0Aefficiency%2C%20we%20propose%20a%20Semantic%20Attribute%20Compactness%20%28SAC%29%20approach.%20SAC%0Aexploits%20the%20inherent%20unified%20semantics%20within%20objects%20to%20learn%20compact%20yet%0Aeffective%20semantic%20representations%20of%203D%20Gaussians%2C%20enabling%20highly%20efficient%0Arendering%20%28%3E100%20FPS%29.%20Additionally%2C%20to%20address%20the%20semantic%20ambiguity%2C%20caused%0Aby%20utilizing%20view-inconsistent%202D%20CLIP%20semantics%20to%20supervise%20Gaussians%2C%20we%0Aintroduce%20a%203D%20Coherent%20Self-training%20%283DCS%29%20strategy%2C%20resorting%20to%20the%0Amulti-view%20consistency%20originated%20from%20the%203D%20model.%203DCS%20imposes%20cross-view%0Asemantic%20consistency%20constraints%20by%20leveraging%20refined%2C%20self-predicted%0Apseudo-labels%20derived%20from%20the%20trained%203D%20Gaussian%20model%2C%20thereby%20enhancing%0Aprecise%20and%20view-consistent%20segmentation%20results.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20remarkably%20outperforms%20existing%20state-of-the-art%0Aapproaches%2C%20achieving%20improvements%20of%2017.29%25%20and%2020.81%25%20in%20mIoU%20metric%20on%0AReplica%20and%20ScanNet%20datasets%2C%20respectively%2C%20while%20maintaining%20real-time%0Arendering%20speed.%20Furthermore%2C%20our%20approach%20exhibits%20superior%20performance%20even%0Awith%20sparse%20input%20data%2C%20verifying%20the%20robustness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14249v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-GS%3A%20CLIP-Informed%20Gaussian%20Splatting%20for%20Real-time%20and%0A%20%20View-consistent%203D%20Semantic%20Understanding&entry.906535625=Guibiao%20Liao%20and%20Jiankun%20Li%20and%20Zhenyu%20Bao%20and%20Xiaoqing%20Ye%20and%20Jingdong%20Wang%20and%20Qing%20Li%20and%20Kanglin%20Liu&entry.1292438233=%20%20The%20recent%203D%20Gaussian%20Splatting%20%28GS%29%20exhibits%20high-quality%20and%20real-time%0Asynthesis%20of%20novel%20views%20in%203D%20scenes.%20Currently%2C%20it%20primarily%20focuses%20on%0Ageometry%20and%20appearance%20modeling%2C%20while%20lacking%20the%20semantic%20understanding%20of%0Ascenes.%20To%20bridge%20this%20gap%2C%20we%20present%20CLIP-GS%2C%20which%20integrates%20semantics%20from%0AContrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20into%20Gaussian%20Splatting%20to%0Aefficiently%20comprehend%203D%20environments%20without%20annotated%20semantic%20data.%20In%0Aspecific%2C%20rather%20than%20straightforwardly%20learning%20and%20rendering%20high-dimensional%0Asemantic%20features%20of%203D%20Gaussians%2C%20which%20significantly%20diminishes%20the%0Aefficiency%2C%20we%20propose%20a%20Semantic%20Attribute%20Compactness%20%28SAC%29%20approach.%20SAC%0Aexploits%20the%20inherent%20unified%20semantics%20within%20objects%20to%20learn%20compact%20yet%0Aeffective%20semantic%20representations%20of%203D%20Gaussians%2C%20enabling%20highly%20efficient%0Arendering%20%28%3E100%20FPS%29.%20Additionally%2C%20to%20address%20the%20semantic%20ambiguity%2C%20caused%0Aby%20utilizing%20view-inconsistent%202D%20CLIP%20semantics%20to%20supervise%20Gaussians%2C%20we%0Aintroduce%20a%203D%20Coherent%20Self-training%20%283DCS%29%20strategy%2C%20resorting%20to%20the%0Amulti-view%20consistency%20originated%20from%20the%203D%20model.%203DCS%20imposes%20cross-view%0Asemantic%20consistency%20constraints%20by%20leveraging%20refined%2C%20self-predicted%0Apseudo-labels%20derived%20from%20the%20trained%203D%20Gaussian%20model%2C%20thereby%20enhancing%0Aprecise%20and%20view-consistent%20segmentation%20results.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20remarkably%20outperforms%20existing%20state-of-the-art%0Aapproaches%2C%20achieving%20improvements%20of%2017.29%25%20and%2020.81%25%20in%20mIoU%20metric%20on%0AReplica%20and%20ScanNet%20datasets%2C%20respectively%2C%20while%20maintaining%20real-time%0Arendering%20speed.%20Furthermore%2C%20our%20approach%20exhibits%20superior%20performance%20even%0Awith%20sparse%20input%20data%2C%20verifying%20the%20robustness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14249v1&entry.124074799=Read"},
{"title": "AutoAD III: The Prequel -- Back to the Pixels", "author": "Tengda Han and Max Bain and Arsha Nagrani and G\u00fcl Varol and Weidi Xie and Andrew Zisserman", "abstract": "  Generating Audio Description (AD) for movies is a challenging task that\nrequires fine-grained visual understanding and an awareness of the characters\nand their names. Currently, visual language models for AD generation are\nlimited by a lack of suitable training data, and also their evaluation is\nhampered by using performance measures not specialized to the AD domain. In\nthis paper, we make three contributions: (i) We propose two approaches for\nconstructing AD datasets with aligned video data, and build training and\nevaluation datasets using these. These datasets will be publicly released; (ii)\nWe develop a Q-former-based architecture which ingests raw video and generates\nAD, using frozen pre-trained visual encoders and large language models; and\n(iii) We provide new evaluation metrics to benchmark AD quality that are\nwell-matched to human performance. Taken together, we improve the state of the\nart on AD generation.\n", "link": "http://arxiv.org/abs/2404.14412v1", "date": "2024-04-22", "relevancy": 2.1945, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5675}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.538}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5281}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AutoAD%20III%3A%20The%20Prequel%20--%20Back%20to%20the%20Pixels&body=Title%3A%20AutoAD%20III%3A%20The%20Prequel%20--%20Back%20to%20the%20Pixels%0AAuthor%3A%20Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20G%C3%BCl%20Varol%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Generating%20Audio%20Description%20%28AD%29%20for%20movies%20is%20a%20challenging%20task%20that%0Arequires%20fine-grained%20visual%20understanding%20and%20an%20awareness%20of%20the%20characters%0Aand%20their%20names.%20Currently%2C%20visual%20language%20models%20for%20AD%20generation%20are%0Alimited%20by%20a%20lack%20of%20suitable%20training%20data%2C%20and%20also%20their%20evaluation%20is%0Ahampered%20by%20using%20performance%20measures%20not%20specialized%20to%20the%20AD%20domain.%20In%0Athis%20paper%2C%20we%20make%20three%20contributions%3A%20%28i%29%20We%20propose%20two%20approaches%20for%0Aconstructing%20AD%20datasets%20with%20aligned%20video%20data%2C%20and%20build%20training%20and%0Aevaluation%20datasets%20using%20these.%20These%20datasets%20will%20be%20publicly%20released%3B%20%28ii%29%0AWe%20develop%20a%20Q-former-based%20architecture%20which%20ingests%20raw%20video%20and%20generates%0AAD%2C%20using%20frozen%20pre-trained%20visual%20encoders%20and%20large%20language%20models%3B%20and%0A%28iii%29%20We%20provide%20new%20evaluation%20metrics%20to%20benchmark%20AD%20quality%20that%20are%0Awell-matched%20to%20human%20performance.%20Taken%20together%2C%20we%20improve%20the%20state%20of%20the%0Aart%20on%20AD%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoAD%20III%3A%20The%20Prequel%20--%20Back%20to%20the%20Pixels&entry.906535625=Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20G%C3%BCl%20Varol%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Generating%20Audio%20Description%20%28AD%29%20for%20movies%20is%20a%20challenging%20task%20that%0Arequires%20fine-grained%20visual%20understanding%20and%20an%20awareness%20of%20the%20characters%0Aand%20their%20names.%20Currently%2C%20visual%20language%20models%20for%20AD%20generation%20are%0Alimited%20by%20a%20lack%20of%20suitable%20training%20data%2C%20and%20also%20their%20evaluation%20is%0Ahampered%20by%20using%20performance%20measures%20not%20specialized%20to%20the%20AD%20domain.%20In%0Athis%20paper%2C%20we%20make%20three%20contributions%3A%20%28i%29%20We%20propose%20two%20approaches%20for%0Aconstructing%20AD%20datasets%20with%20aligned%20video%20data%2C%20and%20build%20training%20and%0Aevaluation%20datasets%20using%20these.%20These%20datasets%20will%20be%20publicly%20released%3B%20%28ii%29%0AWe%20develop%20a%20Q-former-based%20architecture%20which%20ingests%20raw%20video%20and%20generates%0AAD%2C%20using%20frozen%20pre-trained%20visual%20encoders%20and%20large%20language%20models%3B%20and%0A%28iii%29%20We%20provide%20new%20evaluation%20metrics%20to%20benchmark%20AD%20quality%20that%20are%0Awell-matched%20to%20human%20performance.%20Taken%20together%2C%20we%20improve%20the%20state%20of%20the%0Aart%20on%20AD%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14412v1&entry.124074799=Read"},
{"title": "From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous\n  Face Recognition", "author": "Anjith George and Sebastien Marcel", "abstract": "  Heterogeneous Face Recognition (HFR) focuses on matching faces from different\ndomains, for instance, thermal to visible images, making Face Recognition (FR)\nsystems more versatile for challenging scenarios. However, the domain gap\nbetween these domains and the limited large-scale datasets in the target HFR\nmodalities make it challenging to develop robust HFR models from scratch. In\nour work, we view different modalities as distinct styles and propose a method\nto modulate feature maps of the target modality to address the domain gap. We\npresent a new Conditional Adaptive Instance Modulation (CAIM ) module that\nseamlessly fits into existing FR networks, turning them into HFR-ready systems.\nThe CAIM block modulates intermediate feature maps, efficiently adapting to the\nstyle of the source modality and bridging the domain gap. Our method enables\nend-to-end training using a small set of paired samples. We extensively\nevaluate the proposed approach on various challenging HFR benchmarks, showing\nthat it outperforms state-of-the-art methods. The source code and protocols for\nreproducing the findings will be made publicly available\n", "link": "http://arxiv.org/abs/2404.14247v1", "date": "2024-04-22", "relevancy": 2.1897, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Modalities%20to%20Styles%3A%20Rethinking%20the%20Domain%20Gap%20in%20Heterogeneous%0A%20%20Face%20Recognition&body=Title%3A%20From%20Modalities%20to%20Styles%3A%20Rethinking%20the%20Domain%20Gap%20in%20Heterogeneous%0A%20%20Face%20Recognition%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20focuses%20on%20matching%20faces%20from%20different%0Adomains%2C%20for%20instance%2C%20thermal%20to%20visible%20images%2C%20making%20Face%20Recognition%20%28FR%29%0Asystems%20more%20versatile%20for%20challenging%20scenarios.%20However%2C%20the%20domain%20gap%0Abetween%20these%20domains%20and%20the%20limited%20large-scale%20datasets%20in%20the%20target%20HFR%0Amodalities%20make%20it%20challenging%20to%20develop%20robust%20HFR%20models%20from%20scratch.%20In%0Aour%20work%2C%20we%20view%20different%20modalities%20as%20distinct%20styles%20and%20propose%20a%20method%0Ato%20modulate%20feature%20maps%20of%20the%20target%20modality%20to%20address%20the%20domain%20gap.%20We%0Apresent%20a%20new%20Conditional%20Adaptive%20Instance%20Modulation%20%28CAIM%20%29%20module%20that%0Aseamlessly%20fits%20into%20existing%20FR%20networks%2C%20turning%20them%20into%20HFR-ready%20systems.%0AThe%20CAIM%20block%20modulates%20intermediate%20feature%20maps%2C%20efficiently%20adapting%20to%20the%0Astyle%20of%20the%20source%20modality%20and%20bridging%20the%20domain%20gap.%20Our%20method%20enables%0Aend-to-end%20training%20using%20a%20small%20set%20of%20paired%20samples.%20We%20extensively%0Aevaluate%20the%20proposed%20approach%20on%20various%20challenging%20HFR%20benchmarks%2C%20showing%0Athat%20it%20outperforms%20state-of-the-art%20methods.%20The%20source%20code%20and%20protocols%20for%0Areproducing%20the%20findings%20will%20be%20made%20publicly%20available%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14247v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Modalities%20to%20Styles%3A%20Rethinking%20the%20Domain%20Gap%20in%20Heterogeneous%0A%20%20Face%20Recognition&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20focuses%20on%20matching%20faces%20from%20different%0Adomains%2C%20for%20instance%2C%20thermal%20to%20visible%20images%2C%20making%20Face%20Recognition%20%28FR%29%0Asystems%20more%20versatile%20for%20challenging%20scenarios.%20However%2C%20the%20domain%20gap%0Abetween%20these%20domains%20and%20the%20limited%20large-scale%20datasets%20in%20the%20target%20HFR%0Amodalities%20make%20it%20challenging%20to%20develop%20robust%20HFR%20models%20from%20scratch.%20In%0Aour%20work%2C%20we%20view%20different%20modalities%20as%20distinct%20styles%20and%20propose%20a%20method%0Ato%20modulate%20feature%20maps%20of%20the%20target%20modality%20to%20address%20the%20domain%20gap.%20We%0Apresent%20a%20new%20Conditional%20Adaptive%20Instance%20Modulation%20%28CAIM%20%29%20module%20that%0Aseamlessly%20fits%20into%20existing%20FR%20networks%2C%20turning%20them%20into%20HFR-ready%20systems.%0AThe%20CAIM%20block%20modulates%20intermediate%20feature%20maps%2C%20efficiently%20adapting%20to%20the%0Astyle%20of%20the%20source%20modality%20and%20bridging%20the%20domain%20gap.%20Our%20method%20enables%0Aend-to-end%20training%20using%20a%20small%20set%20of%20paired%20samples.%20We%20extensively%0Aevaluate%20the%20proposed%20approach%20on%20various%20challenging%20HFR%20benchmarks%2C%20showing%0Athat%20it%20outperforms%20state-of-the-art%20methods.%20The%20source%20code%20and%20protocols%20for%0Areproducing%20the%20findings%20will%20be%20made%20publicly%20available%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14247v1&entry.124074799=Read"},
{"title": "UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain\n  Adaptation", "author": "Siru Zhong and Xixuan Hao and Yibo Yan and Ying Zhang and Yangqiu Song and Yuxuan Liang", "abstract": "  Urbanization challenges underscore the necessity for effective satellite\nimage-text retrieval methods to swiftly access specific information enriched\nwith geographic semantics for urban applications. However, existing methods\noften overlook significant domain gaps across diverse urban landscapes,\nprimarily focusing on enhancing retrieval performance within single domains. To\ntackle this issue, we present UrbanCross, a new framework for cross-domain\nsatellite image-text retrieval. UrbanCross leverages a high-quality,\ncross-domain dataset enriched with extensive geo-tags from three countries to\nhighlight domain diversity. It employs the Large Multimodal Model (LMM) for\ntextual refinement and the Segment Anything Model (SAM) for visual\naugmentation, achieving a fine-grained alignment of images, segments and texts,\nyielding a 10% improvement in retrieval performance. Additionally, UrbanCross\nincorporates an adaptive curriculum-based source sampler and a weighted\nadversarial cross-domain fine-tuning module, progressively enhancing\nadaptability across various domains. Extensive experiments confirm UrbanCross's\nsuperior efficiency in retrieval and adaptation to new urban environments,\ndemonstrating an average performance increase of 15% over its version without\ndomain adaptation mechanisms, effectively bridging the domain gap.\n", "link": "http://arxiv.org/abs/2404.14241v1", "date": "2024-04-22", "relevancy": 2.1833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5693}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5332}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5187}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UrbanCross%3A%20Enhancing%20Satellite%20Image-Text%20Retrieval%20with%20Cross-Domain%0A%20%20Adaptation&body=Title%3A%20UrbanCross%3A%20Enhancing%20Satellite%20Image-Text%20Retrieval%20with%20Cross-Domain%0A%20%20Adaptation%0AAuthor%3A%20Siru%20Zhong%20and%20Xixuan%20Hao%20and%20Yibo%20Yan%20and%20Ying%20Zhang%20and%20Yangqiu%20Song%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Urbanization%20challenges%20underscore%20the%20necessity%20for%20effective%20satellite%0Aimage-text%20retrieval%20methods%20to%20swiftly%20access%20specific%20information%20enriched%0Awith%20geographic%20semantics%20for%20urban%20applications.%20However%2C%20existing%20methods%0Aoften%20overlook%20significant%20domain%20gaps%20across%20diverse%20urban%20landscapes%2C%0Aprimarily%20focusing%20on%20enhancing%20retrieval%20performance%20within%20single%20domains.%20To%0Atackle%20this%20issue%2C%20we%20present%20UrbanCross%2C%20a%20new%20framework%20for%20cross-domain%0Asatellite%20image-text%20retrieval.%20UrbanCross%20leverages%20a%20high-quality%2C%0Across-domain%20dataset%20enriched%20with%20extensive%20geo-tags%20from%20three%20countries%20to%0Ahighlight%20domain%20diversity.%20It%20employs%20the%20Large%20Multimodal%20Model%20%28LMM%29%20for%0Atextual%20refinement%20and%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20visual%0Aaugmentation%2C%20achieving%20a%20fine-grained%20alignment%20of%20images%2C%20segments%20and%20texts%2C%0Ayielding%20a%2010%25%20improvement%20in%20retrieval%20performance.%20Additionally%2C%20UrbanCross%0Aincorporates%20an%20adaptive%20curriculum-based%20source%20sampler%20and%20a%20weighted%0Aadversarial%20cross-domain%20fine-tuning%20module%2C%20progressively%20enhancing%0Aadaptability%20across%20various%20domains.%20Extensive%20experiments%20confirm%20UrbanCross%27s%0Asuperior%20efficiency%20in%20retrieval%20and%20adaptation%20to%20new%20urban%20environments%2C%0Ademonstrating%20an%20average%20performance%20increase%20of%2015%25%20over%20its%20version%20without%0Adomain%20adaptation%20mechanisms%2C%20effectively%20bridging%20the%20domain%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14241v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanCross%3A%20Enhancing%20Satellite%20Image-Text%20Retrieval%20with%20Cross-Domain%0A%20%20Adaptation&entry.906535625=Siru%20Zhong%20and%20Xixuan%20Hao%20and%20Yibo%20Yan%20and%20Ying%20Zhang%20and%20Yangqiu%20Song%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Urbanization%20challenges%20underscore%20the%20necessity%20for%20effective%20satellite%0Aimage-text%20retrieval%20methods%20to%20swiftly%20access%20specific%20information%20enriched%0Awith%20geographic%20semantics%20for%20urban%20applications.%20However%2C%20existing%20methods%0Aoften%20overlook%20significant%20domain%20gaps%20across%20diverse%20urban%20landscapes%2C%0Aprimarily%20focusing%20on%20enhancing%20retrieval%20performance%20within%20single%20domains.%20To%0Atackle%20this%20issue%2C%20we%20present%20UrbanCross%2C%20a%20new%20framework%20for%20cross-domain%0Asatellite%20image-text%20retrieval.%20UrbanCross%20leverages%20a%20high-quality%2C%0Across-domain%20dataset%20enriched%20with%20extensive%20geo-tags%20from%20three%20countries%20to%0Ahighlight%20domain%20diversity.%20It%20employs%20the%20Large%20Multimodal%20Model%20%28LMM%29%20for%0Atextual%20refinement%20and%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20visual%0Aaugmentation%2C%20achieving%20a%20fine-grained%20alignment%20of%20images%2C%20segments%20and%20texts%2C%0Ayielding%20a%2010%25%20improvement%20in%20retrieval%20performance.%20Additionally%2C%20UrbanCross%0Aincorporates%20an%20adaptive%20curriculum-based%20source%20sampler%20and%20a%20weighted%0Aadversarial%20cross-domain%20fine-tuning%20module%2C%20progressively%20enhancing%0Aadaptability%20across%20various%20domains.%20Extensive%20experiments%20confirm%20UrbanCross%27s%0Asuperior%20efficiency%20in%20retrieval%20and%20adaptation%20to%20new%20urban%20environments%2C%0Ademonstrating%20an%20average%20performance%20increase%20of%2015%25%20over%20its%20version%20without%0Adomain%20adaptation%20mechanisms%2C%20effectively%20bridging%20the%20domain%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14241v1&entry.124074799=Read"},
{"title": "AccidentBlip2: Accident Detection With Multi-View MotionBlip2", "author": "Yihua Shao and Hongyi Cai and Xinwei Long and Weiyi Lang and Zhe Wang and Haoran Wu and Yan Wang and Jiayi Yin and Yang Yang and Zhen Lei", "abstract": "  Intelligent vehicles have demonstrated excellent capabilities in many\ntransportation scenarios, but the complex on-board sensors and the inference\ncapabilities of on-board neural networks limit the accuracy of intelligent\nvehicles for accident detection in complex transportation systems. In this\npaper, we present AccidentBlip2, a pure vision-based multimodal large model\nBlip2 accident detection method. Our method first processes the multi-view\nthrough ViT-14g and inputs the multi-view features into the cross attention\nlayer of the Qformer, while our self-designed Motion Qformer replaces the\nself-attention layer in Blip2's Qformer with the Temporal Attention layer in\nthe In the inference process, the query generated in the previous frame is\ninput into the Temporal Attention layer to realize the inference for temporal\ninformation. Then we detect whether there is an accident in the surrounding\nenvironment by performing autoregressive inference on the query input to the\nMLP. We also extend our approach to a multi-vehicle cooperative system by\ndeploying Motion Qformer on each vehicle and simultaneously inputting the\ninference-generated query into the MLP for autoregressive inference. Our\napproach detects the accuracy of existing video large language models and also\nadapts to multi-vehicle systems, making it more applicable to intelligent\ntransportation scenarios.\n", "link": "http://arxiv.org/abs/2404.12149v3", "date": "2024-04-22", "relevancy": 2.1793, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2&body=Title%3A%20AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2%0AAuthor%3A%20Yihua%20Shao%20and%20Hongyi%20Cai%20and%20Xinwei%20Long%20and%20Weiyi%20Lang%20and%20Zhe%20Wang%20and%20Haoran%20Wu%20and%20Yan%20Wang%20and%20Jiayi%20Yin%20and%20Yang%20Yang%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Intelligent%20vehicles%20have%20demonstrated%20excellent%20capabilities%20in%20many%0Atransportation%20scenarios%2C%20but%20the%20complex%20on-board%20sensors%20and%20the%20inference%0Acapabilities%20of%20on-board%20neural%20networks%20limit%20the%20accuracy%20of%20intelligent%0Avehicles%20for%20accident%20detection%20in%20complex%20transportation%20systems.%20In%20this%0Apaper%2C%20we%20present%20AccidentBlip2%2C%20a%20pure%20vision-based%20multimodal%20large%20model%0ABlip2%20accident%20detection%20method.%20Our%20method%20first%20processes%20the%20multi-view%0Athrough%20ViT-14g%20and%20inputs%20the%20multi-view%20features%20into%20the%20cross%20attention%0Alayer%20of%20the%20Qformer%2C%20while%20our%20self-designed%20Motion%20Qformer%20replaces%20the%0Aself-attention%20layer%20in%20Blip2%27s%20Qformer%20with%20the%20Temporal%20Attention%20layer%20in%0Athe%20In%20the%20inference%20process%2C%20the%20query%20generated%20in%20the%20previous%20frame%20is%0Ainput%20into%20the%20Temporal%20Attention%20layer%20to%20realize%20the%20inference%20for%20temporal%0Ainformation.%20Then%20we%20detect%20whether%20there%20is%20an%20accident%20in%20the%20surrounding%0Aenvironment%20by%20performing%20autoregressive%20inference%20on%20the%20query%20input%20to%20the%0AMLP.%20We%20also%20extend%20our%20approach%20to%20a%20multi-vehicle%20cooperative%20system%20by%0Adeploying%20Motion%20Qformer%20on%20each%20vehicle%20and%20simultaneously%20inputting%20the%0Ainference-generated%20query%20into%20the%20MLP%20for%20autoregressive%20inference.%20Our%0Aapproach%20detects%20the%20accuracy%20of%20existing%20video%20large%20language%20models%20and%20also%0Aadapts%20to%20multi-vehicle%20systems%2C%20making%20it%20more%20applicable%20to%20intelligent%0Atransportation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12149v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AccidentBlip2%3A%20Accident%20Detection%20With%20Multi-View%20MotionBlip2&entry.906535625=Yihua%20Shao%20and%20Hongyi%20Cai%20and%20Xinwei%20Long%20and%20Weiyi%20Lang%20and%20Zhe%20Wang%20and%20Haoran%20Wu%20and%20Yan%20Wang%20and%20Jiayi%20Yin%20and%20Yang%20Yang%20and%20Zhen%20Lei&entry.1292438233=%20%20Intelligent%20vehicles%20have%20demonstrated%20excellent%20capabilities%20in%20many%0Atransportation%20scenarios%2C%20but%20the%20complex%20on-board%20sensors%20and%20the%20inference%0Acapabilities%20of%20on-board%20neural%20networks%20limit%20the%20accuracy%20of%20intelligent%0Avehicles%20for%20accident%20detection%20in%20complex%20transportation%20systems.%20In%20this%0Apaper%2C%20we%20present%20AccidentBlip2%2C%20a%20pure%20vision-based%20multimodal%20large%20model%0ABlip2%20accident%20detection%20method.%20Our%20method%20first%20processes%20the%20multi-view%0Athrough%20ViT-14g%20and%20inputs%20the%20multi-view%20features%20into%20the%20cross%20attention%0Alayer%20of%20the%20Qformer%2C%20while%20our%20self-designed%20Motion%20Qformer%20replaces%20the%0Aself-attention%20layer%20in%20Blip2%27s%20Qformer%20with%20the%20Temporal%20Attention%20layer%20in%0Athe%20In%20the%20inference%20process%2C%20the%20query%20generated%20in%20the%20previous%20frame%20is%0Ainput%20into%20the%20Temporal%20Attention%20layer%20to%20realize%20the%20inference%20for%20temporal%0Ainformation.%20Then%20we%20detect%20whether%20there%20is%20an%20accident%20in%20the%20surrounding%0Aenvironment%20by%20performing%20autoregressive%20inference%20on%20the%20query%20input%20to%20the%0AMLP.%20We%20also%20extend%20our%20approach%20to%20a%20multi-vehicle%20cooperative%20system%20by%0Adeploying%20Motion%20Qformer%20on%20each%20vehicle%20and%20simultaneously%20inputting%20the%0Ainference-generated%20query%20into%20the%20MLP%20for%20autoregressive%20inference.%20Our%0Aapproach%20detects%20the%20accuracy%20of%20existing%20video%20large%20language%20models%20and%20also%0Aadapts%20to%20multi-vehicle%20systems%2C%20making%20it%20more%20applicable%20to%20intelligent%0Atransportation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12149v3&entry.124074799=Read"},
{"title": "Unsupervised Learning of the Total Variation Flow", "author": "Tamara G. Grossmann and S\u00f6ren Dittmer and Yury Korolev and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  The total variation (TV) flow generates a scale-space representation of an\nimage based on the TV functional. This gradient flow observes desirable\nfeatures for images, such as sharp edges and enables spectral, scale, and\ntexture analysis. Solving the TV flow is challenging; one reason is the the\nnon-uniqueness of the subgradients. The standard numerical approach for TV flow\nrequires solving multiple non-smooth optimisation problems. Even with\nstate-of-the-art convex optimisation techniques, this is often prohibitively\nexpensive and strongly motivates the use of alternative, faster approaches.\nInspired by and extending the framework of physics-informed neural networks\n(PINNs), we propose the TVflowNET, an unsupervised neural network approach, to\napproximate the solution of the TV flow given an initial image and a time\ninstance. The TVflowNET requires no ground truth data but rather makes use of\nthe PDE for optimisation of the network parameters. We circumvent the\nchallenges related to the non-uniqueness of the subgradients by additionally\nlearning the related diffusivity term. Our approach significantly speeds up the\ncomputation time and we show that the TVflowNET approximates the TV flow\nsolution with high fidelity for different image sizes and image types.\nAdditionally, we give a full comparison of different network architecture\ndesigns as well as training regimes to underscore the effectiveness of our\napproach.\n", "link": "http://arxiv.org/abs/2206.04406v2", "date": "2024-04-22", "relevancy": 2.1715, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6055}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5324}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5283}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20of%20the%20Total%20Variation%20Flow&body=Title%3A%20Unsupervised%20Learning%20of%20the%20Total%20Variation%20Flow%0AAuthor%3A%20Tamara%20G.%20Grossmann%20and%20S%C3%B6ren%20Dittmer%20and%20Yury%20Korolev%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20The%20total%20variation%20%28TV%29%20flow%20generates%20a%20scale-space%20representation%20of%20an%0Aimage%20based%20on%20the%20TV%20functional.%20This%20gradient%20flow%20observes%20desirable%0Afeatures%20for%20images%2C%20such%20as%20sharp%20edges%20and%20enables%20spectral%2C%20scale%2C%20and%0Atexture%20analysis.%20Solving%20the%20TV%20flow%20is%20challenging%3B%20one%20reason%20is%20the%20the%0Anon-uniqueness%20of%20the%20subgradients.%20The%20standard%20numerical%20approach%20for%20TV%20flow%0Arequires%20solving%20multiple%20non-smooth%20optimisation%20problems.%20Even%20with%0Astate-of-the-art%20convex%20optimisation%20techniques%2C%20this%20is%20often%20prohibitively%0Aexpensive%20and%20strongly%20motivates%20the%20use%20of%20alternative%2C%20faster%20approaches.%0AInspired%20by%20and%20extending%20the%20framework%20of%20physics-informed%20neural%20networks%0A%28PINNs%29%2C%20we%20propose%20the%20TVflowNET%2C%20an%20unsupervised%20neural%20network%20approach%2C%20to%0Aapproximate%20the%20solution%20of%20the%20TV%20flow%20given%20an%20initial%20image%20and%20a%20time%0Ainstance.%20The%20TVflowNET%20requires%20no%20ground%20truth%20data%20but%20rather%20makes%20use%20of%0Athe%20PDE%20for%20optimisation%20of%20the%20network%20parameters.%20We%20circumvent%20the%0Achallenges%20related%20to%20the%20non-uniqueness%20of%20the%20subgradients%20by%20additionally%0Alearning%20the%20related%20diffusivity%20term.%20Our%20approach%20significantly%20speeds%20up%20the%0Acomputation%20time%20and%20we%20show%20that%20the%20TVflowNET%20approximates%20the%20TV%20flow%0Asolution%20with%20high%20fidelity%20for%20different%20image%20sizes%20and%20image%20types.%0AAdditionally%2C%20we%20give%20a%20full%20comparison%20of%20different%20network%20architecture%0Adesigns%20as%20well%20as%20training%20regimes%20to%20underscore%20the%20effectiveness%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.04406v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20of%20the%20Total%20Variation%20Flow&entry.906535625=Tamara%20G.%20Grossmann%20and%20S%C3%B6ren%20Dittmer%20and%20Yury%20Korolev%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20The%20total%20variation%20%28TV%29%20flow%20generates%20a%20scale-space%20representation%20of%20an%0Aimage%20based%20on%20the%20TV%20functional.%20This%20gradient%20flow%20observes%20desirable%0Afeatures%20for%20images%2C%20such%20as%20sharp%20edges%20and%20enables%20spectral%2C%20scale%2C%20and%0Atexture%20analysis.%20Solving%20the%20TV%20flow%20is%20challenging%3B%20one%20reason%20is%20the%20the%0Anon-uniqueness%20of%20the%20subgradients.%20The%20standard%20numerical%20approach%20for%20TV%20flow%0Arequires%20solving%20multiple%20non-smooth%20optimisation%20problems.%20Even%20with%0Astate-of-the-art%20convex%20optimisation%20techniques%2C%20this%20is%20often%20prohibitively%0Aexpensive%20and%20strongly%20motivates%20the%20use%20of%20alternative%2C%20faster%20approaches.%0AInspired%20by%20and%20extending%20the%20framework%20of%20physics-informed%20neural%20networks%0A%28PINNs%29%2C%20we%20propose%20the%20TVflowNET%2C%20an%20unsupervised%20neural%20network%20approach%2C%20to%0Aapproximate%20the%20solution%20of%20the%20TV%20flow%20given%20an%20initial%20image%20and%20a%20time%0Ainstance.%20The%20TVflowNET%20requires%20no%20ground%20truth%20data%20but%20rather%20makes%20use%20of%0Athe%20PDE%20for%20optimisation%20of%20the%20network%20parameters.%20We%20circumvent%20the%0Achallenges%20related%20to%20the%20non-uniqueness%20of%20the%20subgradients%20by%20additionally%0Alearning%20the%20related%20diffusivity%20term.%20Our%20approach%20significantly%20speeds%20up%20the%0Acomputation%20time%20and%20we%20show%20that%20the%20TVflowNET%20approximates%20the%20TV%20flow%0Asolution%20with%20high%20fidelity%20for%20different%20image%20sizes%20and%20image%20types.%0AAdditionally%2C%20we%20give%20a%20full%20comparison%20of%20different%20network%20architecture%0Adesigns%20as%20well%20as%20training%20regimes%20to%20underscore%20the%20effectiveness%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.04406v2&entry.124074799=Read"},
{"title": "SE(3)-Equivariant and Noise-Invariant 3D Rigid Motion Tracking in Brain\n  MRI", "author": "Benjamin Billot and Neel Dey and Daniel Moyer and Malte Hoffmann and Esra Abaci Turk and Borjan Gagoski and Ellen Grant and Polina Golland", "abstract": "  Rigid motion tracking is paramount in many medical imaging applications where\nmovements need to be detected, corrected, or accounted for. Modern strategies\nrely on convolutional neural networks (CNN) and pose this problem as rigid\nregistration. Yet, CNNs do not exploit natural symmetries in this task, as they\nare equivariant to translations (their outputs shift with their inputs) but not\nto rotations. Here we propose EquiTrack, the first method that uses recent\nsteerable SE(3)-equivariant CNNs (E-CNN) for motion tracking. While steerable\nE-CNNs can extract corresponding features across different poses, testing them\non noisy medical images reveals that they do not have enough learning capacity\nto learn noise invariance. Thus, we introduce a hybrid architecture that pairs\na denoiser with an E-CNN to decouple the processing of anatomically irrelevant\nintensity features from the extraction of equivariant spatial features. Rigid\ntransforms are then estimated in closed-form. EquiTrack outperforms\nstate-of-the-art learning and optimisation methods for motion tracking in adult\nbrain MRI and fetal MRI time series. Our code is available at\nhttps://github.com/BBillot/EquiTrack.\n", "link": "http://arxiv.org/abs/2312.13534v2", "date": "2024-04-22", "relevancy": 2.1709, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5367}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.524}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI&body=Title%3A%20SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI%0AAuthor%3A%20Benjamin%20Billot%20and%20Neel%20Dey%20and%20Daniel%20Moyer%20and%20Malte%20Hoffmann%20and%20Esra%20Abaci%20Turk%20and%20Borjan%20Gagoski%20and%20Ellen%20Grant%20and%20Polina%20Golland%0AAbstract%3A%20%20%20Rigid%20motion%20tracking%20is%20paramount%20in%20many%20medical%20imaging%20applications%20where%0Amovements%20need%20to%20be%20detected%2C%20corrected%2C%20or%20accounted%20for.%20Modern%20strategies%0Arely%20on%20convolutional%20neural%20networks%20%28CNN%29%20and%20pose%20this%20problem%20as%20rigid%0Aregistration.%20Yet%2C%20CNNs%20do%20not%20exploit%20natural%20symmetries%20in%20this%20task%2C%20as%20they%0Aare%20equivariant%20to%20translations%20%28their%20outputs%20shift%20with%20their%20inputs%29%20but%20not%0Ato%20rotations.%20Here%20we%20propose%20EquiTrack%2C%20the%20first%20method%20that%20uses%20recent%0Asteerable%20SE%283%29-equivariant%20CNNs%20%28E-CNN%29%20for%20motion%20tracking.%20While%20steerable%0AE-CNNs%20can%20extract%20corresponding%20features%20across%20different%20poses%2C%20testing%20them%0Aon%20noisy%20medical%20images%20reveals%20that%20they%20do%20not%20have%20enough%20learning%20capacity%0Ato%20learn%20noise%20invariance.%20Thus%2C%20we%20introduce%20a%20hybrid%20architecture%20that%20pairs%0Aa%20denoiser%20with%20an%20E-CNN%20to%20decouple%20the%20processing%20of%20anatomically%20irrelevant%0Aintensity%20features%20from%20the%20extraction%20of%20equivariant%20spatial%20features.%20Rigid%0Atransforms%20are%20then%20estimated%20in%20closed-form.%20EquiTrack%20outperforms%0Astate-of-the-art%20learning%20and%20optimisation%20methods%20for%20motion%20tracking%20in%20adult%0Abrain%20MRI%20and%20fetal%20MRI%20time%20series.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BBillot/EquiTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13534v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI&entry.906535625=Benjamin%20Billot%20and%20Neel%20Dey%20and%20Daniel%20Moyer%20and%20Malte%20Hoffmann%20and%20Esra%20Abaci%20Turk%20and%20Borjan%20Gagoski%20and%20Ellen%20Grant%20and%20Polina%20Golland&entry.1292438233=%20%20Rigid%20motion%20tracking%20is%20paramount%20in%20many%20medical%20imaging%20applications%20where%0Amovements%20need%20to%20be%20detected%2C%20corrected%2C%20or%20accounted%20for.%20Modern%20strategies%0Arely%20on%20convolutional%20neural%20networks%20%28CNN%29%20and%20pose%20this%20problem%20as%20rigid%0Aregistration.%20Yet%2C%20CNNs%20do%20not%20exploit%20natural%20symmetries%20in%20this%20task%2C%20as%20they%0Aare%20equivariant%20to%20translations%20%28their%20outputs%20shift%20with%20their%20inputs%29%20but%20not%0Ato%20rotations.%20Here%20we%20propose%20EquiTrack%2C%20the%20first%20method%20that%20uses%20recent%0Asteerable%20SE%283%29-equivariant%20CNNs%20%28E-CNN%29%20for%20motion%20tracking.%20While%20steerable%0AE-CNNs%20can%20extract%20corresponding%20features%20across%20different%20poses%2C%20testing%20them%0Aon%20noisy%20medical%20images%20reveals%20that%20they%20do%20not%20have%20enough%20learning%20capacity%0Ato%20learn%20noise%20invariance.%20Thus%2C%20we%20introduce%20a%20hybrid%20architecture%20that%20pairs%0Aa%20denoiser%20with%20an%20E-CNN%20to%20decouple%20the%20processing%20of%20anatomically%20irrelevant%0Aintensity%20features%20from%20the%20extraction%20of%20equivariant%20spatial%20features.%20Rigid%0Atransforms%20are%20then%20estimated%20in%20closed-form.%20EquiTrack%20outperforms%0Astate-of-the-art%20learning%20and%20optimisation%20methods%20for%20motion%20tracking%20in%20adult%0Abrain%20MRI%20and%20fetal%20MRI%20time%20series.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BBillot/EquiTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13534v2&entry.124074799=Read"},
{"title": "Learning H-Infinity Locomotion Control", "author": "Junfeng Long and Wenye Yu and Quanyi Li and Zirui Wang and Dahua Lin and Jiangmiao Pang", "abstract": "  Stable locomotion in precipitous environments is an essential capability of\nquadruped robots, demanding the ability to resist various external\ndisturbances. However, recent learning-based policies only use basic domain\nrandomization to improve the robustness of learned policies, which cannot\nguarantee that the robot has adequate disturbance resistance capabilities. In\nthis paper, we propose to model the learning process as an adversarial\ninteraction between the actor and a newly introduced disturber and ensure their\noptimization with $H_{\\infty}$ constraint. In contrast to the actor that\nmaximizes the discounted overall reward, the disturber is responsible for\ngenerating effective external forces and is optimized by maximizing the error\nbetween the task reward and its oracle, i.e., \"cost\" in each iteration. To keep\njoint optimization between the actor and the disturber stable, our $H_{\\infty}$\nconstraint mandates the bound of ratio between the cost to the intensity of the\nexternal forces. Through reciprocal interaction throughout the training phase,\nthe actor can acquire the capability to navigate increasingly complex physical\ndisturbances. We verify the robustness of our approach on quadrupedal\nlocomotion tasks with Unitree Aliengo robot, and also a more challenging task\nwith Unitree A1 robot, where the quadruped is expected to perform locomotion\nmerely on its hind legs as if it is a bipedal robot. The simulated quantitative\nresults show improvement against baselines, demonstrating the effectiveness of\nthe method and each design choice. On the other hand, real-robot experiments\nqualitatively exhibit how robust the policy is when interfering with various\ndisturbances on various terrains, including stairs, high platforms, slopes, and\nslippery terrains. All code, checkpoints, and real-world deployment guidance\nwill be made public.\n", "link": "http://arxiv.org/abs/2404.14405v1", "date": "2024-04-22", "relevancy": 2.1684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5547}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5196}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20H-Infinity%20Locomotion%20Control&body=Title%3A%20Learning%20H-Infinity%20Locomotion%20Control%0AAuthor%3A%20Junfeng%20Long%20and%20Wenye%20Yu%20and%20Quanyi%20Li%20and%20Zirui%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Stable%20locomotion%20in%20precipitous%20environments%20is%20an%20essential%20capability%20of%0Aquadruped%20robots%2C%20demanding%20the%20ability%20to%20resist%20various%20external%0Adisturbances.%20However%2C%20recent%20learning-based%20policies%20only%20use%20basic%20domain%0Arandomization%20to%20improve%20the%20robustness%20of%20learned%20policies%2C%20which%20cannot%0Aguarantee%20that%20the%20robot%20has%20adequate%20disturbance%20resistance%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20to%20model%20the%20learning%20process%20as%20an%20adversarial%0Ainteraction%20between%20the%20actor%20and%20a%20newly%20introduced%20disturber%20and%20ensure%20their%0Aoptimization%20with%20%24H_%7B%5Cinfty%7D%24%20constraint.%20In%20contrast%20to%20the%20actor%20that%0Amaximizes%20the%20discounted%20overall%20reward%2C%20the%20disturber%20is%20responsible%20for%0Agenerating%20effective%20external%20forces%20and%20is%20optimized%20by%20maximizing%20the%20error%0Abetween%20the%20task%20reward%20and%20its%20oracle%2C%20i.e.%2C%20%22cost%22%20in%20each%20iteration.%20To%20keep%0Ajoint%20optimization%20between%20the%20actor%20and%20the%20disturber%20stable%2C%20our%20%24H_%7B%5Cinfty%7D%24%0Aconstraint%20mandates%20the%20bound%20of%20ratio%20between%20the%20cost%20to%20the%20intensity%20of%20the%0Aexternal%20forces.%20Through%20reciprocal%20interaction%20throughout%20the%20training%20phase%2C%0Athe%20actor%20can%20acquire%20the%20capability%20to%20navigate%20increasingly%20complex%20physical%0Adisturbances.%20We%20verify%20the%20robustness%20of%20our%20approach%20on%20quadrupedal%0Alocomotion%20tasks%20with%20Unitree%20Aliengo%20robot%2C%20and%20also%20a%20more%20challenging%20task%0Awith%20Unitree%20A1%20robot%2C%20where%20the%20quadruped%20is%20expected%20to%20perform%20locomotion%0Amerely%20on%20its%20hind%20legs%20as%20if%20it%20is%20a%20bipedal%20robot.%20The%20simulated%20quantitative%0Aresults%20show%20improvement%20against%20baselines%2C%20demonstrating%20the%20effectiveness%20of%0Athe%20method%20and%20each%20design%20choice.%20On%20the%20other%20hand%2C%20real-robot%20experiments%0Aqualitatively%20exhibit%20how%20robust%20the%20policy%20is%20when%20interfering%20with%20various%0Adisturbances%20on%20various%20terrains%2C%20including%20stairs%2C%20high%20platforms%2C%20slopes%2C%20and%0Aslippery%20terrains.%20All%20code%2C%20checkpoints%2C%20and%20real-world%20deployment%20guidance%0Awill%20be%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14405v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20H-Infinity%20Locomotion%20Control&entry.906535625=Junfeng%20Long%20and%20Wenye%20Yu%20and%20Quanyi%20Li%20and%20Zirui%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Stable%20locomotion%20in%20precipitous%20environments%20is%20an%20essential%20capability%20of%0Aquadruped%20robots%2C%20demanding%20the%20ability%20to%20resist%20various%20external%0Adisturbances.%20However%2C%20recent%20learning-based%20policies%20only%20use%20basic%20domain%0Arandomization%20to%20improve%20the%20robustness%20of%20learned%20policies%2C%20which%20cannot%0Aguarantee%20that%20the%20robot%20has%20adequate%20disturbance%20resistance%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20to%20model%20the%20learning%20process%20as%20an%20adversarial%0Ainteraction%20between%20the%20actor%20and%20a%20newly%20introduced%20disturber%20and%20ensure%20their%0Aoptimization%20with%20%24H_%7B%5Cinfty%7D%24%20constraint.%20In%20contrast%20to%20the%20actor%20that%0Amaximizes%20the%20discounted%20overall%20reward%2C%20the%20disturber%20is%20responsible%20for%0Agenerating%20effective%20external%20forces%20and%20is%20optimized%20by%20maximizing%20the%20error%0Abetween%20the%20task%20reward%20and%20its%20oracle%2C%20i.e.%2C%20%22cost%22%20in%20each%20iteration.%20To%20keep%0Ajoint%20optimization%20between%20the%20actor%20and%20the%20disturber%20stable%2C%20our%20%24H_%7B%5Cinfty%7D%24%0Aconstraint%20mandates%20the%20bound%20of%20ratio%20between%20the%20cost%20to%20the%20intensity%20of%20the%0Aexternal%20forces.%20Through%20reciprocal%20interaction%20throughout%20the%20training%20phase%2C%0Athe%20actor%20can%20acquire%20the%20capability%20to%20navigate%20increasingly%20complex%20physical%0Adisturbances.%20We%20verify%20the%20robustness%20of%20our%20approach%20on%20quadrupedal%0Alocomotion%20tasks%20with%20Unitree%20Aliengo%20robot%2C%20and%20also%20a%20more%20challenging%20task%0Awith%20Unitree%20A1%20robot%2C%20where%20the%20quadruped%20is%20expected%20to%20perform%20locomotion%0Amerely%20on%20its%20hind%20legs%20as%20if%20it%20is%20a%20bipedal%20robot.%20The%20simulated%20quantitative%0Aresults%20show%20improvement%20against%20baselines%2C%20demonstrating%20the%20effectiveness%20of%0Athe%20method%20and%20each%20design%20choice.%20On%20the%20other%20hand%2C%20real-robot%20experiments%0Aqualitatively%20exhibit%20how%20robust%20the%20policy%20is%20when%20interfering%20with%20various%0Adisturbances%20on%20various%20terrains%2C%20including%20stairs%2C%20high%20platforms%2C%20slopes%2C%20and%0Aslippery%20terrains.%20All%20code%2C%20checkpoints%2C%20and%20real-world%20deployment%20guidance%0Awill%20be%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14405v1&entry.124074799=Read"},
{"title": "TAVGBench: Benchmarking Text to Audible-Video Generation", "author": "Yuxin Mao and Xuyang Shen and Jing Zhang and Zhen Qin and Jinxing Zhou and Mochu Xiang and Yiran Zhong and Yuchao Dai", "abstract": "  The Text to Audible-Video Generation (TAVG) task involves generating videos\nwith accompanying audio based on text descriptions. Achieving this requires\nskillful alignment of both audio and video elements. To support research in\nthis field, we have developed a comprehensive Text to Audible-Video Generation\nBenchmark (TAVGBench), which contains over 1.7 million clips with a total\nduration of 11.8 thousand hours. We propose an automatic annotation pipeline to\nensure each audible video has detailed descriptions for both its audio and\nvideo contents. We also introduce the Audio-Visual Harmoni score (AVHScore) to\nprovide a quantitative measure of the alignment between the generated audio and\nvideo modalities. Additionally, we present a baseline model for TAVG called\nTAVDiffusion, which uses a two-stream latent diffusion model to provide a\nfundamental starting point for further research in this area. We achieve the\nalignment of audio and video by employing cross-attention and contrastive\nlearning. Through extensive experiments and evaluations on TAVGBench, we\ndemonstrate the effectiveness of our proposed model under both conventional\nmetrics and our proposed metrics.\n", "link": "http://arxiv.org/abs/2404.14381v1", "date": "2024-04-22", "relevancy": 2.1657, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.54}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TAVGBench%3A%20Benchmarking%20Text%20to%20Audible-Video%20Generation&body=Title%3A%20TAVGBench%3A%20Benchmarking%20Text%20to%20Audible-Video%20Generation%0AAuthor%3A%20Yuxin%20Mao%20and%20Xuyang%20Shen%20and%20Jing%20Zhang%20and%20Zhen%20Qin%20and%20Jinxing%20Zhou%20and%20Mochu%20Xiang%20and%20Yiran%20Zhong%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20The%20Text%20to%20Audible-Video%20Generation%20%28TAVG%29%20task%20involves%20generating%20videos%0Awith%20accompanying%20audio%20based%20on%20text%20descriptions.%20Achieving%20this%20requires%0Askillful%20alignment%20of%20both%20audio%20and%20video%20elements.%20To%20support%20research%20in%0Athis%20field%2C%20we%20have%20developed%20a%20comprehensive%20Text%20to%20Audible-Video%20Generation%0ABenchmark%20%28TAVGBench%29%2C%20which%20contains%20over%201.7%20million%20clips%20with%20a%20total%0Aduration%20of%2011.8%20thousand%20hours.%20We%20propose%20an%20automatic%20annotation%20pipeline%20to%0Aensure%20each%20audible%20video%20has%20detailed%20descriptions%20for%20both%20its%20audio%20and%0Avideo%20contents.%20We%20also%20introduce%20the%20Audio-Visual%20Harmoni%20score%20%28AVHScore%29%20to%0Aprovide%20a%20quantitative%20measure%20of%20the%20alignment%20between%20the%20generated%20audio%20and%0Avideo%20modalities.%20Additionally%2C%20we%20present%20a%20baseline%20model%20for%20TAVG%20called%0ATAVDiffusion%2C%20which%20uses%20a%20two-stream%20latent%20diffusion%20model%20to%20provide%20a%0Afundamental%20starting%20point%20for%20further%20research%20in%20this%20area.%20We%20achieve%20the%0Aalignment%20of%20audio%20and%20video%20by%20employing%20cross-attention%20and%20contrastive%0Alearning.%20Through%20extensive%20experiments%20and%20evaluations%20on%20TAVGBench%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20model%20under%20both%20conventional%0Ametrics%20and%20our%20proposed%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14381v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAVGBench%3A%20Benchmarking%20Text%20to%20Audible-Video%20Generation&entry.906535625=Yuxin%20Mao%20and%20Xuyang%20Shen%20and%20Jing%20Zhang%20and%20Zhen%20Qin%20and%20Jinxing%20Zhou%20and%20Mochu%20Xiang%20and%20Yiran%20Zhong%20and%20Yuchao%20Dai&entry.1292438233=%20%20The%20Text%20to%20Audible-Video%20Generation%20%28TAVG%29%20task%20involves%20generating%20videos%0Awith%20accompanying%20audio%20based%20on%20text%20descriptions.%20Achieving%20this%20requires%0Askillful%20alignment%20of%20both%20audio%20and%20video%20elements.%20To%20support%20research%20in%0Athis%20field%2C%20we%20have%20developed%20a%20comprehensive%20Text%20to%20Audible-Video%20Generation%0ABenchmark%20%28TAVGBench%29%2C%20which%20contains%20over%201.7%20million%20clips%20with%20a%20total%0Aduration%20of%2011.8%20thousand%20hours.%20We%20propose%20an%20automatic%20annotation%20pipeline%20to%0Aensure%20each%20audible%20video%20has%20detailed%20descriptions%20for%20both%20its%20audio%20and%0Avideo%20contents.%20We%20also%20introduce%20the%20Audio-Visual%20Harmoni%20score%20%28AVHScore%29%20to%0Aprovide%20a%20quantitative%20measure%20of%20the%20alignment%20between%20the%20generated%20audio%20and%0Avideo%20modalities.%20Additionally%2C%20we%20present%20a%20baseline%20model%20for%20TAVG%20called%0ATAVDiffusion%2C%20which%20uses%20a%20two-stream%20latent%20diffusion%20model%20to%20provide%20a%0Afundamental%20starting%20point%20for%20further%20research%20in%20this%20area.%20We%20achieve%20the%0Aalignment%20of%20audio%20and%20video%20by%20employing%20cross-attention%20and%20contrastive%0Alearning.%20Through%20extensive%20experiments%20and%20evaluations%20on%20TAVGBench%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20model%20under%20both%20conventional%0Ametrics%20and%20our%20proposed%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14381v1&entry.124074799=Read"},
{"title": "Large Language Model as a Policy Teacher for Training Reinforcement\n  Learning Agents", "author": "Zihao Zhou and Bin Hu and Chenyang Zhao and Pu Zhang and Bin Liu", "abstract": "  Recent studies have uncovered the potential of Large Language Models (LLMs)\nin addressing complex sequential decision-making tasks through the provision of\nhigh-level instructions. However, LLM-based agents lack specialization in\ntackling specific target problems, particularly in real-time dynamic\nenvironments. Additionally, deploying an LLM-based agent in practical scenarios\ncan be both costly and time-consuming. On the other hand, reinforcement\nlearning (RL) approaches train agents that specialize in the target task but\noften suffer from low sampling efficiency and high exploration costs. In this\npaper, we introduce a novel framework that addresses these challenges by\ntraining a smaller, specialized student RL agent using instructions from an\nLLM-based teacher agent. By incorporating the guidance from the teacher agent,\nthe student agent can distill the prior knowledge of the LLM into its own\nmodel. Consequently, the student agent can be trained with significantly less\ndata. Moreover, through further training with environment feedback, the student\nagent surpasses the capabilities of its teacher for completing the target task.\nWe conducted experiments on challenging MiniGrid and Habitat environments,\nspecifically designed for embodied AI research, to evaluate the effectiveness\nof our framework. The results clearly demonstrate that our approach achieves\nsuperior performance compared to strong baseline methods. Our code is available\nat https://github.com/ZJLAB-AMMI/LLM4Teach.\n", "link": "http://arxiv.org/abs/2311.13373v5", "date": "2024-04-22", "relevancy": 2.1647, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.585}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5123}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents&body=Title%3A%20Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents%0AAuthor%3A%20Zihao%20Zhou%20and%20Bin%20Hu%20and%20Chenyang%20Zhao%20and%20Pu%20Zhang%20and%20Bin%20Liu%0AAbstract%3A%20%20%20Recent%20studies%20have%20uncovered%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%0Ain%20addressing%20complex%20sequential%20decision-making%20tasks%20through%20the%20provision%20of%0Ahigh-level%20instructions.%20However%2C%20LLM-based%20agents%20lack%20specialization%20in%0Atackling%20specific%20target%20problems%2C%20particularly%20in%20real-time%20dynamic%0Aenvironments.%20Additionally%2C%20deploying%20an%20LLM-based%20agent%20in%20practical%20scenarios%0Acan%20be%20both%20costly%20and%20time-consuming.%20On%20the%20other%20hand%2C%20reinforcement%0Alearning%20%28RL%29%20approaches%20train%20agents%20that%20specialize%20in%20the%20target%20task%20but%0Aoften%20suffer%20from%20low%20sampling%20efficiency%20and%20high%20exploration%20costs.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20framework%20that%20addresses%20these%20challenges%20by%0Atraining%20a%20smaller%2C%20specialized%20student%20RL%20agent%20using%20instructions%20from%20an%0ALLM-based%20teacher%20agent.%20By%20incorporating%20the%20guidance%20from%20the%20teacher%20agent%2C%0Athe%20student%20agent%20can%20distill%20the%20prior%20knowledge%20of%20the%20LLM%20into%20its%20own%0Amodel.%20Consequently%2C%20the%20student%20agent%20can%20be%20trained%20with%20significantly%20less%0Adata.%20Moreover%2C%20through%20further%20training%20with%20environment%20feedback%2C%20the%20student%0Aagent%20surpasses%20the%20capabilities%20of%20its%20teacher%20for%20completing%20the%20target%20task.%0AWe%20conducted%20experiments%20on%20challenging%20MiniGrid%20and%20Habitat%20environments%2C%0Aspecifically%20designed%20for%20embodied%20AI%20research%2C%20to%20evaluate%20the%20effectiveness%0Aof%20our%20framework.%20The%20results%20clearly%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20compared%20to%20strong%20baseline%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ZJLAB-AMMI/LLM4Teach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13373v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents&entry.906535625=Zihao%20Zhou%20and%20Bin%20Hu%20and%20Chenyang%20Zhao%20and%20Pu%20Zhang%20and%20Bin%20Liu&entry.1292438233=%20%20Recent%20studies%20have%20uncovered%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%0Ain%20addressing%20complex%20sequential%20decision-making%20tasks%20through%20the%20provision%20of%0Ahigh-level%20instructions.%20However%2C%20LLM-based%20agents%20lack%20specialization%20in%0Atackling%20specific%20target%20problems%2C%20particularly%20in%20real-time%20dynamic%0Aenvironments.%20Additionally%2C%20deploying%20an%20LLM-based%20agent%20in%20practical%20scenarios%0Acan%20be%20both%20costly%20and%20time-consuming.%20On%20the%20other%20hand%2C%20reinforcement%0Alearning%20%28RL%29%20approaches%20train%20agents%20that%20specialize%20in%20the%20target%20task%20but%0Aoften%20suffer%20from%20low%20sampling%20efficiency%20and%20high%20exploration%20costs.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20framework%20that%20addresses%20these%20challenges%20by%0Atraining%20a%20smaller%2C%20specialized%20student%20RL%20agent%20using%20instructions%20from%20an%0ALLM-based%20teacher%20agent.%20By%20incorporating%20the%20guidance%20from%20the%20teacher%20agent%2C%0Athe%20student%20agent%20can%20distill%20the%20prior%20knowledge%20of%20the%20LLM%20into%20its%20own%0Amodel.%20Consequently%2C%20the%20student%20agent%20can%20be%20trained%20with%20significantly%20less%0Adata.%20Moreover%2C%20through%20further%20training%20with%20environment%20feedback%2C%20the%20student%0Aagent%20surpasses%20the%20capabilities%20of%20its%20teacher%20for%20completing%20the%20target%20task.%0AWe%20conducted%20experiments%20on%20challenging%20MiniGrid%20and%20Habitat%20environments%2C%0Aspecifically%20designed%20for%20embodied%20AI%20research%2C%20to%20evaluate%20the%20effectiveness%0Aof%20our%20framework.%20The%20results%20clearly%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20compared%20to%20strong%20baseline%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ZJLAB-AMMI/LLM4Teach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13373v5&entry.124074799=Read"},
{"title": "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D\n  Glimpses", "author": "Inhee Lee and Byungjun Kim and Hanbyul Joo", "abstract": "  In this paper, we present a method to reconstruct the world and multiple\ndynamic humans in 3D from a monocular video input. As a key idea, we represent\nboth the world and multiple humans via the recently emerging 3D Gaussian\nSplatting (3D-GS) representation, enabling to conveniently and efficiently\ncompose and render them together. In particular, we address the scenarios with\nseverely limited and sparse observations in 3D human reconstruction, a common\nchallenge encountered in the real world. To tackle this challenge, we introduce\na novel approach to optimize the 3D-GS representation in a canonical space by\nfusing the sparse cues in the common space, where we leverage a pre-trained 2D\ndiffusion model to synthesize unseen views while keeping the consistency with\nthe observed 2D appearances. We demonstrate our method can reconstruct\nhigh-quality animatable 3D humans in various challenging examples, in the\npresence of occlusion, image crops, few-shot, and extremely sparse\nobservations. After reconstruction, our method is capable of not only rendering\nthe scene in any novel views at arbitrary time instances, but also editing the\n3D scene by removing individual humans or applying different motions for each\nhuman. Through various experiments, we demonstrate the quality and efficiency\nof our methods over alternative existing approaches.\n", "link": "http://arxiv.org/abs/2404.14410v1", "date": "2024-04-22", "relevancy": 2.1625, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5415}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5409}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.54}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guess%20The%20Unseen%3A%20Dynamic%203D%20Scene%20Reconstruction%20from%20Partial%202D%0A%20%20Glimpses&body=Title%3A%20Guess%20The%20Unseen%3A%20Dynamic%203D%20Scene%20Reconstruction%20from%20Partial%202D%0A%20%20Glimpses%0AAuthor%3A%20Inhee%20Lee%20and%20Byungjun%20Kim%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reconstruct%20the%20world%20and%20multiple%0Adynamic%20humans%20in%203D%20from%20a%20monocular%20video%20input.%20As%20a%20key%20idea%2C%20we%20represent%0Aboth%20the%20world%20and%20multiple%20humans%20via%20the%20recently%20emerging%203D%20Gaussian%0ASplatting%20%283D-GS%29%20representation%2C%20enabling%20to%20conveniently%20and%20efficiently%0Acompose%20and%20render%20them%20together.%20In%20particular%2C%20we%20address%20the%20scenarios%20with%0Aseverely%20limited%20and%20sparse%20observations%20in%203D%20human%20reconstruction%2C%20a%20common%0Achallenge%20encountered%20in%20the%20real%20world.%20To%20tackle%20this%20challenge%2C%20we%20introduce%0Aa%20novel%20approach%20to%20optimize%20the%203D-GS%20representation%20in%20a%20canonical%20space%20by%0Afusing%20the%20sparse%20cues%20in%20the%20common%20space%2C%20where%20we%20leverage%20a%20pre-trained%202D%0Adiffusion%20model%20to%20synthesize%20unseen%20views%20while%20keeping%20the%20consistency%20with%0Athe%20observed%202D%20appearances.%20We%20demonstrate%20our%20method%20can%20reconstruct%0Ahigh-quality%20animatable%203D%20humans%20in%20various%20challenging%20examples%2C%20in%20the%0Apresence%20of%20occlusion%2C%20image%20crops%2C%20few-shot%2C%20and%20extremely%20sparse%0Aobservations.%20After%20reconstruction%2C%20our%20method%20is%20capable%20of%20not%20only%20rendering%0Athe%20scene%20in%20any%20novel%20views%20at%20arbitrary%20time%20instances%2C%20but%20also%20editing%20the%0A3D%20scene%20by%20removing%20individual%20humans%20or%20applying%20different%20motions%20for%20each%0Ahuman.%20Through%20various%20experiments%2C%20we%20demonstrate%20the%20quality%20and%20efficiency%0Aof%20our%20methods%20over%20alternative%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14410v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guess%20The%20Unseen%3A%20Dynamic%203D%20Scene%20Reconstruction%20from%20Partial%202D%0A%20%20Glimpses&entry.906535625=Inhee%20Lee%20and%20Byungjun%20Kim%20and%20Hanbyul%20Joo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reconstruct%20the%20world%20and%20multiple%0Adynamic%20humans%20in%203D%20from%20a%20monocular%20video%20input.%20As%20a%20key%20idea%2C%20we%20represent%0Aboth%20the%20world%20and%20multiple%20humans%20via%20the%20recently%20emerging%203D%20Gaussian%0ASplatting%20%283D-GS%29%20representation%2C%20enabling%20to%20conveniently%20and%20efficiently%0Acompose%20and%20render%20them%20together.%20In%20particular%2C%20we%20address%20the%20scenarios%20with%0Aseverely%20limited%20and%20sparse%20observations%20in%203D%20human%20reconstruction%2C%20a%20common%0Achallenge%20encountered%20in%20the%20real%20world.%20To%20tackle%20this%20challenge%2C%20we%20introduce%0Aa%20novel%20approach%20to%20optimize%20the%203D-GS%20representation%20in%20a%20canonical%20space%20by%0Afusing%20the%20sparse%20cues%20in%20the%20common%20space%2C%20where%20we%20leverage%20a%20pre-trained%202D%0Adiffusion%20model%20to%20synthesize%20unseen%20views%20while%20keeping%20the%20consistency%20with%0Athe%20observed%202D%20appearances.%20We%20demonstrate%20our%20method%20can%20reconstruct%0Ahigh-quality%20animatable%203D%20humans%20in%20various%20challenging%20examples%2C%20in%20the%0Apresence%20of%20occlusion%2C%20image%20crops%2C%20few-shot%2C%20and%20extremely%20sparse%0Aobservations.%20After%20reconstruction%2C%20our%20method%20is%20capable%20of%20not%20only%20rendering%0Athe%20scene%20in%20any%20novel%20views%20at%20arbitrary%20time%20instances%2C%20but%20also%20editing%20the%0A3D%20scene%20by%20removing%20individual%20humans%20or%20applying%20different%20motions%20for%20each%0Ahuman.%20Through%20various%20experiments%2C%20we%20demonstrate%20the%20quality%20and%20efficiency%0Aof%20our%20methods%20over%20alternative%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14410v1&entry.124074799=Read"},
{"title": "RESFM: Robust Equivariant Multiview Structure from Motion", "author": "Fadi Khatib and Yoni Kasten and Dror Moran and Meirav Galun and Ronen Basri", "abstract": "  Multiview Structure from Motion is a fundamental and challenging computer\nvision problem. A recent deep-based approach was proposed utilizing matrix\nequivariant architectures for the simultaneous recovery of camera pose and 3D\nscene structure from large image collections. This work however made the\nunrealistic assumption that the point tracks given as input are clean of\noutliers. Here we propose an architecture suited to dealing with outliers by\nadding an inlier/outlier classifying module that respects the model\nequivariance and by adding a robust bundle adjustment step. Experiments\ndemonstrate that our method can be successfully applied in realistic settings\nthat include large image collections and point tracks extracted with common\nheuristics and include many outliers.\n", "link": "http://arxiv.org/abs/2404.14280v1", "date": "2024-04-22", "relevancy": 2.1384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RESFM%3A%20Robust%20Equivariant%20Multiview%20Structure%20from%20Motion&body=Title%3A%20RESFM%3A%20Robust%20Equivariant%20Multiview%20Structure%20from%20Motion%0AAuthor%3A%20Fadi%20Khatib%20and%20Yoni%20Kasten%20and%20Dror%20Moran%20and%20Meirav%20Galun%20and%20Ronen%20Basri%0AAbstract%3A%20%20%20Multiview%20Structure%20from%20Motion%20is%20a%20fundamental%20and%20challenging%20computer%0Avision%20problem.%20A%20recent%20deep-based%20approach%20was%20proposed%20utilizing%20matrix%0Aequivariant%20architectures%20for%20the%20simultaneous%20recovery%20of%20camera%20pose%20and%203D%0Ascene%20structure%20from%20large%20image%20collections.%20This%20work%20however%20made%20the%0Aunrealistic%20assumption%20that%20the%20point%20tracks%20given%20as%20input%20are%20clean%20of%0Aoutliers.%20Here%20we%20propose%20an%20architecture%20suited%20to%20dealing%20with%20outliers%20by%0Aadding%20an%20inlier/outlier%20classifying%20module%20that%20respects%20the%20model%0Aequivariance%20and%20by%20adding%20a%20robust%20bundle%20adjustment%20step.%20Experiments%0Ademonstrate%20that%20our%20method%20can%20be%20successfully%20applied%20in%20realistic%20settings%0Athat%20include%20large%20image%20collections%20and%20point%20tracks%20extracted%20with%20common%0Aheuristics%20and%20include%20many%20outliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14280v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RESFM%3A%20Robust%20Equivariant%20Multiview%20Structure%20from%20Motion&entry.906535625=Fadi%20Khatib%20and%20Yoni%20Kasten%20and%20Dror%20Moran%20and%20Meirav%20Galun%20and%20Ronen%20Basri&entry.1292438233=%20%20Multiview%20Structure%20from%20Motion%20is%20a%20fundamental%20and%20challenging%20computer%0Avision%20problem.%20A%20recent%20deep-based%20approach%20was%20proposed%20utilizing%20matrix%0Aequivariant%20architectures%20for%20the%20simultaneous%20recovery%20of%20camera%20pose%20and%203D%0Ascene%20structure%20from%20large%20image%20collections.%20This%20work%20however%20made%20the%0Aunrealistic%20assumption%20that%20the%20point%20tracks%20given%20as%20input%20are%20clean%20of%0Aoutliers.%20Here%20we%20propose%20an%20architecture%20suited%20to%20dealing%20with%20outliers%20by%0Aadding%20an%20inlier/outlier%20classifying%20module%20that%20respects%20the%20model%0Aequivariance%20and%20by%20adding%20a%20robust%20bundle%20adjustment%20step.%20Experiments%0Ademonstrate%20that%20our%20method%20can%20be%20successfully%20applied%20in%20realistic%20settings%0Athat%20include%20large%20image%20collections%20and%20point%20tracks%20extracted%20with%20common%0Aheuristics%20and%20include%20many%20outliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14280v1&entry.124074799=Read"},
{"title": "CRNet: A Detail-Preserving Network for Unified Image Restoration and\n  Enhancement Task", "author": "Kangzhen Yang and Tao Hu and Kexin Dai and Genggeng Chen and Yu Cao and Wei Dong and Peng Wu and Yanning Zhang and Qingsen Yan", "abstract": "  In real-world scenarios, images captured often suffer from blurring, noise,\nand other forms of image degradation, and due to sensor limitations, people\nusually can only obtain low dynamic range images. To achieve high-quality\nimages, researchers have attempted various image restoration and enhancement\noperations on photographs, including denoising, deblurring, and high dynamic\nrange imaging. However, merely performing a single type of image enhancement\nstill cannot yield satisfactory images. In this paper, to deal with the\nchallenge above, we propose the Composite Refinement Network (CRNet) to address\nthis issue using multiple exposure images. By fully integrating\ninformation-rich multiple exposure inputs, CRNet can perform unified image\nrestoration and enhancement. To improve the quality of image details, CRNet\nexplicitly separates and strengthens high and low-frequency information through\npooling layers, using specially designed Multi-Branch Blocks for effective\nfusion of these frequencies. To increase the receptive field and fully\nintegrate input features, CRNet employs the High-Frequency Enhancement Module,\nwhich includes large kernel convolutions and an inverted bottleneck ConvFFN.\nOur model secured third place in the first track of the Bracketing Image\nRestoration and Enhancement Challenge, surpassing previous SOTA models in both\ntesting metrics and visual quality.\n", "link": "http://arxiv.org/abs/2404.14132v1", "date": "2024-04-22", "relevancy": 2.1366, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5548}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5202}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CRNet%3A%20A%20Detail-Preserving%20Network%20for%20Unified%20Image%20Restoration%20and%0A%20%20Enhancement%20Task&body=Title%3A%20CRNet%3A%20A%20Detail-Preserving%20Network%20for%20Unified%20Image%20Restoration%20and%0A%20%20Enhancement%20Task%0AAuthor%3A%20Kangzhen%20Yang%20and%20Tao%20Hu%20and%20Kexin%20Dai%20and%20Genggeng%20Chen%20and%20Yu%20Cao%20and%20Wei%20Dong%20and%20Peng%20Wu%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20images%20captured%20often%20suffer%20from%20blurring%2C%20noise%2C%0Aand%20other%20forms%20of%20image%20degradation%2C%20and%20due%20to%20sensor%20limitations%2C%20people%0Ausually%20can%20only%20obtain%20low%20dynamic%20range%20images.%20To%20achieve%20high-quality%0Aimages%2C%20researchers%20have%20attempted%20various%20image%20restoration%20and%20enhancement%0Aoperations%20on%20photographs%2C%20including%20denoising%2C%20deblurring%2C%20and%20high%20dynamic%0Arange%20imaging.%20However%2C%20merely%20performing%20a%20single%20type%20of%20image%20enhancement%0Astill%20cannot%20yield%20satisfactory%20images.%20In%20this%20paper%2C%20to%20deal%20with%20the%0Achallenge%20above%2C%20we%20propose%20the%20Composite%20Refinement%20Network%20%28CRNet%29%20to%20address%0Athis%20issue%20using%20multiple%20exposure%20images.%20By%20fully%20integrating%0Ainformation-rich%20multiple%20exposure%20inputs%2C%20CRNet%20can%20perform%20unified%20image%0Arestoration%20and%20enhancement.%20To%20improve%20the%20quality%20of%20image%20details%2C%20CRNet%0Aexplicitly%20separates%20and%20strengthens%20high%20and%20low-frequency%20information%20through%0Apooling%20layers%2C%20using%20specially%20designed%20Multi-Branch%20Blocks%20for%20effective%0Afusion%20of%20these%20frequencies.%20To%20increase%20the%20receptive%20field%20and%20fully%0Aintegrate%20input%20features%2C%20CRNet%20employs%20the%20High-Frequency%20Enhancement%20Module%2C%0Awhich%20includes%20large%20kernel%20convolutions%20and%20an%20inverted%20bottleneck%20ConvFFN.%0AOur%20model%20secured%20third%20place%20in%20the%20first%20track%20of%20the%20Bracketing%20Image%0ARestoration%20and%20Enhancement%20Challenge%2C%20surpassing%20previous%20SOTA%20models%20in%20both%0Atesting%20metrics%20and%20visual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14132v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRNet%3A%20A%20Detail-Preserving%20Network%20for%20Unified%20Image%20Restoration%20and%0A%20%20Enhancement%20Task&entry.906535625=Kangzhen%20Yang%20and%20Tao%20Hu%20and%20Kexin%20Dai%20and%20Genggeng%20Chen%20and%20Yu%20Cao%20and%20Wei%20Dong%20and%20Peng%20Wu%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20images%20captured%20often%20suffer%20from%20blurring%2C%20noise%2C%0Aand%20other%20forms%20of%20image%20degradation%2C%20and%20due%20to%20sensor%20limitations%2C%20people%0Ausually%20can%20only%20obtain%20low%20dynamic%20range%20images.%20To%20achieve%20high-quality%0Aimages%2C%20researchers%20have%20attempted%20various%20image%20restoration%20and%20enhancement%0Aoperations%20on%20photographs%2C%20including%20denoising%2C%20deblurring%2C%20and%20high%20dynamic%0Arange%20imaging.%20However%2C%20merely%20performing%20a%20single%20type%20of%20image%20enhancement%0Astill%20cannot%20yield%20satisfactory%20images.%20In%20this%20paper%2C%20to%20deal%20with%20the%0Achallenge%20above%2C%20we%20propose%20the%20Composite%20Refinement%20Network%20%28CRNet%29%20to%20address%0Athis%20issue%20using%20multiple%20exposure%20images.%20By%20fully%20integrating%0Ainformation-rich%20multiple%20exposure%20inputs%2C%20CRNet%20can%20perform%20unified%20image%0Arestoration%20and%20enhancement.%20To%20improve%20the%20quality%20of%20image%20details%2C%20CRNet%0Aexplicitly%20separates%20and%20strengthens%20high%20and%20low-frequency%20information%20through%0Apooling%20layers%2C%20using%20specially%20designed%20Multi-Branch%20Blocks%20for%20effective%0Afusion%20of%20these%20frequencies.%20To%20increase%20the%20receptive%20field%20and%20fully%0Aintegrate%20input%20features%2C%20CRNet%20employs%20the%20High-Frequency%20Enhancement%20Module%2C%0Awhich%20includes%20large%20kernel%20convolutions%20and%20an%20inverted%20bottleneck%20ConvFFN.%0AOur%20model%20secured%20third%20place%20in%20the%20first%20track%20of%20the%20Bracketing%20Image%0ARestoration%20and%20Enhancement%20Challenge%2C%20surpassing%20previous%20SOTA%20models%20in%20both%0Atesting%20metrics%20and%20visual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14132v1&entry.124074799=Read"},
{"title": "Dynamic Cross Attention for Audio-Visual Person Verification", "author": "R. Gnana Praveen and Jahangir Alam", "abstract": "  Although person or identity verification has been predominantly explored\nusing individual modalities such as face and voice, audio-visual fusion has\nrecently shown immense potential to outperform unimodal approaches. Audio and\nvisual modalities are often expected to pose strong complementary\nrelationships, which plays a crucial role in effective audio-visual fusion.\nHowever, they may not always strongly complement each other, they may also\nexhibit weak complementary relationships, resulting in poor audio-visual\nfeature representations. In this paper, we propose a Dynamic Cross-Attention\n(DCA) model that can dynamically select the cross-attended or unattended\nfeatures on the fly based on the strong or weak complementary relationships,\nrespectively, across audio and visual modalities. In particular, a conditional\ngating layer is designed to evaluate the contribution of the cross-attention\nmechanism and choose cross-attended features only when they exhibit strong\ncomplementary relationships, otherwise unattended features. Extensive\nexperiments are conducted on the Voxceleb1 dataset to demonstrate the\nrobustness of the proposed model. Results indicate that the proposed model\nconsistently improves the performance on multiple variants of cross-attention\nwhile outperforming the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.04661v3", "date": "2024-04-22", "relevancy": 2.1354, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.556}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification&body=Title%3A%20Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification%0AAuthor%3A%20R.%20Gnana%20Praveen%20and%20Jahangir%20Alam%0AAbstract%3A%20%20%20Although%20person%20or%20identity%20verification%20has%20been%20predominantly%20explored%0Ausing%20individual%20modalities%20such%20as%20face%20and%20voice%2C%20audio-visual%20fusion%20has%0Arecently%20shown%20immense%20potential%20to%20outperform%20unimodal%20approaches.%20Audio%20and%0Avisual%20modalities%20are%20often%20expected%20to%20pose%20strong%20complementary%0Arelationships%2C%20which%20plays%20a%20crucial%20role%20in%20effective%20audio-visual%20fusion.%0AHowever%2C%20they%20may%20not%20always%20strongly%20complement%20each%20other%2C%20they%20may%20also%0Aexhibit%20weak%20complementary%20relationships%2C%20resulting%20in%20poor%20audio-visual%0Afeature%20representations.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Cross-Attention%0A%28DCA%29%20model%20that%20can%20dynamically%20select%20the%20cross-attended%20or%20unattended%0Afeatures%20on%20the%20fly%20based%20on%20the%20strong%20or%20weak%20complementary%20relationships%2C%0Arespectively%2C%20across%20audio%20and%20visual%20modalities.%20In%20particular%2C%20a%20conditional%0Agating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%20cross-attention%0Amechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%20exhibit%20strong%0Acomplementary%20relationships%2C%20otherwise%20unattended%20features.%20Extensive%0Aexperiments%20are%20conducted%20on%20the%20Voxceleb1%20dataset%20to%20demonstrate%20the%0Arobustness%20of%20the%20proposed%20model.%20Results%20indicate%20that%20the%20proposed%20model%0Aconsistently%20improves%20the%20performance%20on%20multiple%20variants%20of%20cross-attention%0Awhile%20outperforming%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04661v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Cross%20Attention%20for%20Audio-Visual%20Person%20Verification&entry.906535625=R.%20Gnana%20Praveen%20and%20Jahangir%20Alam&entry.1292438233=%20%20Although%20person%20or%20identity%20verification%20has%20been%20predominantly%20explored%0Ausing%20individual%20modalities%20such%20as%20face%20and%20voice%2C%20audio-visual%20fusion%20has%0Arecently%20shown%20immense%20potential%20to%20outperform%20unimodal%20approaches.%20Audio%20and%0Avisual%20modalities%20are%20often%20expected%20to%20pose%20strong%20complementary%0Arelationships%2C%20which%20plays%20a%20crucial%20role%20in%20effective%20audio-visual%20fusion.%0AHowever%2C%20they%20may%20not%20always%20strongly%20complement%20each%20other%2C%20they%20may%20also%0Aexhibit%20weak%20complementary%20relationships%2C%20resulting%20in%20poor%20audio-visual%0Afeature%20representations.%20In%20this%20paper%2C%20we%20propose%20a%20Dynamic%20Cross-Attention%0A%28DCA%29%20model%20that%20can%20dynamically%20select%20the%20cross-attended%20or%20unattended%0Afeatures%20on%20the%20fly%20based%20on%20the%20strong%20or%20weak%20complementary%20relationships%2C%0Arespectively%2C%20across%20audio%20and%20visual%20modalities.%20In%20particular%2C%20a%20conditional%0Agating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%20cross-attention%0Amechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%20exhibit%20strong%0Acomplementary%20relationships%2C%20otherwise%20unattended%20features.%20Extensive%0Aexperiments%20are%20conducted%20on%20the%20Voxceleb1%20dataset%20to%20demonstrate%20the%0Arobustness%20of%20the%20proposed%20model.%20Results%20indicate%20that%20the%20proposed%20model%0Aconsistently%20improves%20the%20performance%20on%20multiple%20variants%20of%20cross-attention%0Awhile%20outperforming%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04661v3&entry.124074799=Read"},
{"title": "Neural Control: Concurrent System Identification and Control Learning\n  with Neural ODE", "author": "Cheng Chi", "abstract": "  Controlling continuous-time dynamical systems is generally a two step\nprocess: first, identify or model the system dynamics with differential\nequations, then, minimize the control objectives to achieve optimal control\nfunction and optimal state trajectories. However, any inaccuracy in dynamics\nmodeling will lead to sub-optimality in the resulting control function. To\naddress this, we propose a neural ODE based method for controlling unknown\ndynamical systems, denoted as Neural Control (NC), which combines dynamics\nidentification and optimal control learning using a coupled neural ODE. Through\nan intriguing interplay between the two neural networks in coupled neural ODE\nstructure, our model concurrently learns system dynamics as well as optimal\ncontrols that guides towards target states. Our experiments demonstrate the\neffectiveness of our model for learning optimal control of unknown dynamical\nsystems. Codes available at\nhttps://github.com/chichengmessi/neural_ode_control/tree/main\n", "link": "http://arxiv.org/abs/2401.01836v4", "date": "2024-04-22", "relevancy": 2.1328, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5471}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4915}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Control%3A%20Concurrent%20System%20Identification%20and%20Control%20Learning%0A%20%20with%20Neural%20ODE&body=Title%3A%20Neural%20Control%3A%20Concurrent%20System%20Identification%20and%20Control%20Learning%0A%20%20with%20Neural%20ODE%0AAuthor%3A%20Cheng%20Chi%0AAbstract%3A%20%20%20Controlling%20continuous-time%20dynamical%20systems%20is%20generally%20a%20two%20step%0Aprocess%3A%20first%2C%20identify%20or%20model%20the%20system%20dynamics%20with%20differential%0Aequations%2C%20then%2C%20minimize%20the%20control%20objectives%20to%20achieve%20optimal%20control%0Afunction%20and%20optimal%20state%20trajectories.%20However%2C%20any%20inaccuracy%20in%20dynamics%0Amodeling%20will%20lead%20to%20sub-optimality%20in%20the%20resulting%20control%20function.%20To%0Aaddress%20this%2C%20we%20propose%20a%20neural%20ODE%20based%20method%20for%20controlling%20unknown%0Adynamical%20systems%2C%20denoted%20as%20Neural%20Control%20%28NC%29%2C%20which%20combines%20dynamics%0Aidentification%20and%20optimal%20control%20learning%20using%20a%20coupled%20neural%20ODE.%20Through%0Aan%20intriguing%20interplay%20between%20the%20two%20neural%20networks%20in%20coupled%20neural%20ODE%0Astructure%2C%20our%20model%20concurrently%20learns%20system%20dynamics%20as%20well%20as%20optimal%0Acontrols%20that%20guides%20towards%20target%20states.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20for%20learning%20optimal%20control%20of%20unknown%20dynamical%0Asystems.%20Codes%20available%20at%0Ahttps%3A//github.com/chichengmessi/neural_ode_control/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01836v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Control%3A%20Concurrent%20System%20Identification%20and%20Control%20Learning%0A%20%20with%20Neural%20ODE&entry.906535625=Cheng%20Chi&entry.1292438233=%20%20Controlling%20continuous-time%20dynamical%20systems%20is%20generally%20a%20two%20step%0Aprocess%3A%20first%2C%20identify%20or%20model%20the%20system%20dynamics%20with%20differential%0Aequations%2C%20then%2C%20minimize%20the%20control%20objectives%20to%20achieve%20optimal%20control%0Afunction%20and%20optimal%20state%20trajectories.%20However%2C%20any%20inaccuracy%20in%20dynamics%0Amodeling%20will%20lead%20to%20sub-optimality%20in%20the%20resulting%20control%20function.%20To%0Aaddress%20this%2C%20we%20propose%20a%20neural%20ODE%20based%20method%20for%20controlling%20unknown%0Adynamical%20systems%2C%20denoted%20as%20Neural%20Control%20%28NC%29%2C%20which%20combines%20dynamics%0Aidentification%20and%20optimal%20control%20learning%20using%20a%20coupled%20neural%20ODE.%20Through%0Aan%20intriguing%20interplay%20between%20the%20two%20neural%20networks%20in%20coupled%20neural%20ODE%0Astructure%2C%20our%20model%20concurrently%20learns%20system%20dynamics%20as%20well%20as%20optimal%0Acontrols%20that%20guides%20towards%20target%20states.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20for%20learning%20optimal%20control%20of%20unknown%20dynamical%0Asystems.%20Codes%20available%20at%0Ahttps%3A//github.com/chichengmessi/neural_ode_control/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01836v4&entry.124074799=Read"},
{"title": "A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid\n  Transformer and Contrastive Learning", "author": "Yuelin Zhang and Pengyu Zheng and Wanquan Yan and Chengyu Fang and Shing Shin Cheng", "abstract": "  Defocus blur is a persistent problem in microscope imaging that poses harm to\npathology interpretation and medical intervention in cell microscopy and\nmicroscope surgery. To address this problem, a unified framework including the\nmulti-pyramid transformer (MPT) and extended frequency contrastive\nregularization (EFCR) is proposed to tackle two outstanding challenges in\nmicroscopy deblur: longer attention span and data deficiency. The MPT employs\nan explicit pyramid structure at each network stage that integrates the\ncross-scale window attention (CSWA), the intra-scale channel attention (ISCA),\nand the feature-enhancing feed-forward network (FEFN) to capture long-range\ncross-scale spatial interaction and global channel context. The EFCR addresses\nthe data deficiency problem by exploring latent deblur signals from different\nfrequency bands. It also enables deblur knowledge transfer to learn\ncross-domain information from extra data, improving deblur performance for\nlabeled and unlabeled data. Extensive experiments and downstream task\nvalidation show the framework achieves state-of-the-art performance across\nmultiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.\n", "link": "http://arxiv.org/abs/2403.02611v2", "date": "2024-04-22", "relevancy": 2.1263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5314}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Microscopy%20Defocus%20Deblur%20with%20Multi-Pyramid%0A%20%20Transformer%20and%20Contrastive%20Learning&body=Title%3A%20A%20Unified%20Framework%20for%20Microscopy%20Defocus%20Deblur%20with%20Multi-Pyramid%0A%20%20Transformer%20and%20Contrastive%20Learning%0AAuthor%3A%20Yuelin%20Zhang%20and%20Pengyu%20Zheng%20and%20Wanquan%20Yan%20and%20Chengyu%20Fang%20and%20Shing%20Shin%20Cheng%0AAbstract%3A%20%20%20Defocus%20blur%20is%20a%20persistent%20problem%20in%20microscope%20imaging%20that%20poses%20harm%20to%0Apathology%20interpretation%20and%20medical%20intervention%20in%20cell%20microscopy%20and%0Amicroscope%20surgery.%20To%20address%20this%20problem%2C%20a%20unified%20framework%20including%20the%0Amulti-pyramid%20transformer%20%28MPT%29%20and%20extended%20frequency%20contrastive%0Aregularization%20%28EFCR%29%20is%20proposed%20to%20tackle%20two%20outstanding%20challenges%20in%0Amicroscopy%20deblur%3A%20longer%20attention%20span%20and%20data%20deficiency.%20The%20MPT%20employs%0Aan%20explicit%20pyramid%20structure%20at%20each%20network%20stage%20that%20integrates%20the%0Across-scale%20window%20attention%20%28CSWA%29%2C%20the%20intra-scale%20channel%20attention%20%28ISCA%29%2C%0Aand%20the%20feature-enhancing%20feed-forward%20network%20%28FEFN%29%20to%20capture%20long-range%0Across-scale%20spatial%20interaction%20and%20global%20channel%20context.%20The%20EFCR%20addresses%0Athe%20data%20deficiency%20problem%20by%20exploring%20latent%20deblur%20signals%20from%20different%0Afrequency%20bands.%20It%20also%20enables%20deblur%20knowledge%20transfer%20to%20learn%0Across-domain%20information%20from%20extra%20data%2C%20improving%20deblur%20performance%20for%0Alabeled%20and%20unlabeled%20data.%20Extensive%20experiments%20and%20downstream%20task%0Avalidation%20show%20the%20framework%20achieves%20state-of-the-art%20performance%20across%0Amultiple%20datasets.%20Project%20page%3A%20https%3A//github.com/PieceZhang/MPT-CataBlur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02611v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Microscopy%20Defocus%20Deblur%20with%20Multi-Pyramid%0A%20%20Transformer%20and%20Contrastive%20Learning&entry.906535625=Yuelin%20Zhang%20and%20Pengyu%20Zheng%20and%20Wanquan%20Yan%20and%20Chengyu%20Fang%20and%20Shing%20Shin%20Cheng&entry.1292438233=%20%20Defocus%20blur%20is%20a%20persistent%20problem%20in%20microscope%20imaging%20that%20poses%20harm%20to%0Apathology%20interpretation%20and%20medical%20intervention%20in%20cell%20microscopy%20and%0Amicroscope%20surgery.%20To%20address%20this%20problem%2C%20a%20unified%20framework%20including%20the%0Amulti-pyramid%20transformer%20%28MPT%29%20and%20extended%20frequency%20contrastive%0Aregularization%20%28EFCR%29%20is%20proposed%20to%20tackle%20two%20outstanding%20challenges%20in%0Amicroscopy%20deblur%3A%20longer%20attention%20span%20and%20data%20deficiency.%20The%20MPT%20employs%0Aan%20explicit%20pyramid%20structure%20at%20each%20network%20stage%20that%20integrates%20the%0Across-scale%20window%20attention%20%28CSWA%29%2C%20the%20intra-scale%20channel%20attention%20%28ISCA%29%2C%0Aand%20the%20feature-enhancing%20feed-forward%20network%20%28FEFN%29%20to%20capture%20long-range%0Across-scale%20spatial%20interaction%20and%20global%20channel%20context.%20The%20EFCR%20addresses%0Athe%20data%20deficiency%20problem%20by%20exploring%20latent%20deblur%20signals%20from%20different%0Afrequency%20bands.%20It%20also%20enables%20deblur%20knowledge%20transfer%20to%20learn%0Across-domain%20information%20from%20extra%20data%2C%20improving%20deblur%20performance%20for%0Alabeled%20and%20unlabeled%20data.%20Extensive%20experiments%20and%20downstream%20task%0Avalidation%20show%20the%20framework%20achieves%20state-of-the-art%20performance%20across%0Amultiple%20datasets.%20Project%20page%3A%20https%3A//github.com/PieceZhang/MPT-CataBlur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02611v2&entry.124074799=Read"},
{"title": "Towards Better Adversarial Purification via Adversarial Denoising\n  Diffusion Training", "author": "Yiming Liu and Kezhao Liu and Yao Xiao and Ziyi Dong and Xiaogang Xu and Pengxu Wei and Liang Lin", "abstract": "  Recently, diffusion-based purification (DBP) has emerged as a promising\napproach for defending against adversarial attacks. However, previous studies\nhave used questionable methods to evaluate the robustness of DBP models, their\nexplanations of DBP robustness also lack experimental support. We re-examine\nDBP robustness using precise gradient, and discuss the impact of stochasticity\non DBP robustness. To better explain DBP robustness, we assess DBP robustness\nunder a novel attack setting, Deterministic White-box, and pinpoint\nstochasticity as the main factor in DBP robustness. Our results suggest that\nDBP models rely on stochasticity to evade the most effective attack direction,\nrather than directly countering adversarial perturbations. To improve the\nrobustness of DBP models, we propose Adversarial Denoising Diffusion Training\n(ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO)\nto generate adversarial perturbation through guidance from a pre-trained\nclassifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial\npertubation into a normal Gaussian distribution. Empirical results show that\nADDT improves the robustness of DBP models. Further experiments confirm that\nADDT equips DBP models with the ability to directly counter adversarial\nperturbations.\n", "link": "http://arxiv.org/abs/2404.14309v1", "date": "2024-04-22", "relevancy": 2.1076, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5268}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Better%20Adversarial%20Purification%20via%20Adversarial%20Denoising%0A%20%20Diffusion%20Training&body=Title%3A%20Towards%20Better%20Adversarial%20Purification%20via%20Adversarial%20Denoising%0A%20%20Diffusion%20Training%0AAuthor%3A%20Yiming%20Liu%20and%20Kezhao%20Liu%20and%20Yao%20Xiao%20and%20Ziyi%20Dong%20and%20Xiaogang%20Xu%20and%20Pengxu%20Wei%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Recently%2C%20diffusion-based%20purification%20%28DBP%29%20has%20emerged%20as%20a%20promising%0Aapproach%20for%20defending%20against%20adversarial%20attacks.%20However%2C%20previous%20studies%0Ahave%20used%20questionable%20methods%20to%20evaluate%20the%20robustness%20of%20DBP%20models%2C%20their%0Aexplanations%20of%20DBP%20robustness%20also%20lack%20experimental%20support.%20We%20re-examine%0ADBP%20robustness%20using%20precise%20gradient%2C%20and%20discuss%20the%20impact%20of%20stochasticity%0Aon%20DBP%20robustness.%20To%20better%20explain%20DBP%20robustness%2C%20we%20assess%20DBP%20robustness%0Aunder%20a%20novel%20attack%20setting%2C%20Deterministic%20White-box%2C%20and%20pinpoint%0Astochasticity%20as%20the%20main%20factor%20in%20DBP%20robustness.%20Our%20results%20suggest%20that%0ADBP%20models%20rely%20on%20stochasticity%20to%20evade%20the%20most%20effective%20attack%20direction%2C%0Arather%20than%20directly%20countering%20adversarial%20perturbations.%20To%20improve%20the%0Arobustness%20of%20DBP%20models%2C%20we%20propose%20Adversarial%20Denoising%20Diffusion%20Training%0A%28ADDT%29.%20This%20technique%20uses%20Classifier-Guided%20Perturbation%20Optimization%20%28CGPO%29%0Ato%20generate%20adversarial%20perturbation%20through%20guidance%20from%20a%20pre-trained%0Aclassifier%2C%20and%20uses%20Rank-Based%20Gaussian%20Mapping%20%28RBGM%29%20to%20convert%20adversarial%0Apertubation%20into%20a%20normal%20Gaussian%20distribution.%20Empirical%20results%20show%20that%0AADDT%20improves%20the%20robustness%20of%20DBP%20models.%20Further%20experiments%20confirm%20that%0AADDT%20equips%20DBP%20models%20with%20the%20ability%20to%20directly%20counter%20adversarial%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Better%20Adversarial%20Purification%20via%20Adversarial%20Denoising%0A%20%20Diffusion%20Training&entry.906535625=Yiming%20Liu%20and%20Kezhao%20Liu%20and%20Yao%20Xiao%20and%20Ziyi%20Dong%20and%20Xiaogang%20Xu%20and%20Pengxu%20Wei%20and%20Liang%20Lin&entry.1292438233=%20%20Recently%2C%20diffusion-based%20purification%20%28DBP%29%20has%20emerged%20as%20a%20promising%0Aapproach%20for%20defending%20against%20adversarial%20attacks.%20However%2C%20previous%20studies%0Ahave%20used%20questionable%20methods%20to%20evaluate%20the%20robustness%20of%20DBP%20models%2C%20their%0Aexplanations%20of%20DBP%20robustness%20also%20lack%20experimental%20support.%20We%20re-examine%0ADBP%20robustness%20using%20precise%20gradient%2C%20and%20discuss%20the%20impact%20of%20stochasticity%0Aon%20DBP%20robustness.%20To%20better%20explain%20DBP%20robustness%2C%20we%20assess%20DBP%20robustness%0Aunder%20a%20novel%20attack%20setting%2C%20Deterministic%20White-box%2C%20and%20pinpoint%0Astochasticity%20as%20the%20main%20factor%20in%20DBP%20robustness.%20Our%20results%20suggest%20that%0ADBP%20models%20rely%20on%20stochasticity%20to%20evade%20the%20most%20effective%20attack%20direction%2C%0Arather%20than%20directly%20countering%20adversarial%20perturbations.%20To%20improve%20the%0Arobustness%20of%20DBP%20models%2C%20we%20propose%20Adversarial%20Denoising%20Diffusion%20Training%0A%28ADDT%29.%20This%20technique%20uses%20Classifier-Guided%20Perturbation%20Optimization%20%28CGPO%29%0Ato%20generate%20adversarial%20perturbation%20through%20guidance%20from%20a%20pre-trained%0Aclassifier%2C%20and%20uses%20Rank-Based%20Gaussian%20Mapping%20%28RBGM%29%20to%20convert%20adversarial%0Apertubation%20into%20a%20normal%20Gaussian%20distribution.%20Empirical%20results%20show%20that%0AADDT%20improves%20the%20robustness%20of%20DBP%20models.%20Further%20experiments%20confirm%20that%0AADDT%20equips%20DBP%20models%20with%20the%20ability%20to%20directly%20counter%20adversarial%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14309v1&entry.124074799=Read"},
{"title": "Experimental Validation of Ultrasound Beamforming with End-to-End Deep\n  Learning for Single Plane Wave Imaging", "author": "Ryan A. L. Schoop and Gijs Hendriks and Tristan van Leeuwen and Chris L. de Korte and Felix Lucka", "abstract": "  Ultrafast ultrasound imaging insonifies a medium with one or a combination of\na few plane waves at different beam-steered angles instead of many focused\nwaves. It can achieve much higher frame rates, but often at the cost of reduced\nimage quality. Deep learning approaches have been proposed to mitigate this\ndisadvantage, in particular for single plane wave imaging. Predominantly,\nimage-to-image post-processing networks or fully learned data-to-image neural\nnetworks are used. Both construct their mapping purely data-driven and require\nexpressive networks and large amounts of training data to perform well. In\ncontrast, we consider data-to-image networks which incorporate a conventional\nimage formation techniques as differentiable layers in the network\narchitecture. This allows for end-to-end training with small amounts of\ntraining data. In this work, using f-k migration as an image formation layer is\nevaluated in-depth with experimental data. We acquired a data collection\ndesigned for benchmarking data-driven plane wave imaging approaches using a\nrealistic breast mimicking phantom and an ultrasound calibration phantom. The\nevaluation considers global and local image similarity measures and contrast,\nresolution and lesion detectability analysis. The results show that the\nproposed network architecture is capable of improving the image quality of\nsingle plane wave images on all evaluation metrics. Furthermore, these image\nquality improvements can be achieved with surprisingly little amounts of\ntraining data.\n", "link": "http://arxiv.org/abs/2404.14188v1", "date": "2024-04-22", "relevancy": 2.1058, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5373}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5337}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5149}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Experimental%20Validation%20of%20Ultrasound%20Beamforming%20with%20End-to-End%20Deep%0A%20%20Learning%20for%20Single%20Plane%20Wave%20Imaging&body=Title%3A%20Experimental%20Validation%20of%20Ultrasound%20Beamforming%20with%20End-to-End%20Deep%0A%20%20Learning%20for%20Single%20Plane%20Wave%20Imaging%0AAuthor%3A%20Ryan%20A.%20L.%20Schoop%20and%20Gijs%20Hendriks%20and%20Tristan%20van%20Leeuwen%20and%20Chris%20L.%20de%20Korte%20and%20Felix%20Lucka%0AAbstract%3A%20%20%20Ultrafast%20ultrasound%20imaging%20insonifies%20a%20medium%20with%20one%20or%20a%20combination%20of%0Aa%20few%20plane%20waves%20at%20different%20beam-steered%20angles%20instead%20of%20many%20focused%0Awaves.%20It%20can%20achieve%20much%20higher%20frame%20rates%2C%20but%20often%20at%20the%20cost%20of%20reduced%0Aimage%20quality.%20Deep%20learning%20approaches%20have%20been%20proposed%20to%20mitigate%20this%0Adisadvantage%2C%20in%20particular%20for%20single%20plane%20wave%20imaging.%20Predominantly%2C%0Aimage-to-image%20post-processing%20networks%20or%20fully%20learned%20data-to-image%20neural%0Anetworks%20are%20used.%20Both%20construct%20their%20mapping%20purely%20data-driven%20and%20require%0Aexpressive%20networks%20and%20large%20amounts%20of%20training%20data%20to%20perform%20well.%20In%0Acontrast%2C%20we%20consider%20data-to-image%20networks%20which%20incorporate%20a%20conventional%0Aimage%20formation%20techniques%20as%20differentiable%20layers%20in%20the%20network%0Aarchitecture.%20This%20allows%20for%20end-to-end%20training%20with%20small%20amounts%20of%0Atraining%20data.%20In%20this%20work%2C%20using%20f-k%20migration%20as%20an%20image%20formation%20layer%20is%0Aevaluated%20in-depth%20with%20experimental%20data.%20We%20acquired%20a%20data%20collection%0Adesigned%20for%20benchmarking%20data-driven%20plane%20wave%20imaging%20approaches%20using%20a%0Arealistic%20breast%20mimicking%20phantom%20and%20an%20ultrasound%20calibration%20phantom.%20The%0Aevaluation%20considers%20global%20and%20local%20image%20similarity%20measures%20and%20contrast%2C%0Aresolution%20and%20lesion%20detectability%20analysis.%20The%20results%20show%20that%20the%0Aproposed%20network%20architecture%20is%20capable%20of%20improving%20the%20image%20quality%20of%0Asingle%20plane%20wave%20images%20on%20all%20evaluation%20metrics.%20Furthermore%2C%20these%20image%0Aquality%20improvements%20can%20be%20achieved%20with%20surprisingly%20little%20amounts%20of%0Atraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14188v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experimental%20Validation%20of%20Ultrasound%20Beamforming%20with%20End-to-End%20Deep%0A%20%20Learning%20for%20Single%20Plane%20Wave%20Imaging&entry.906535625=Ryan%20A.%20L.%20Schoop%20and%20Gijs%20Hendriks%20and%20Tristan%20van%20Leeuwen%20and%20Chris%20L.%20de%20Korte%20and%20Felix%20Lucka&entry.1292438233=%20%20Ultrafast%20ultrasound%20imaging%20insonifies%20a%20medium%20with%20one%20or%20a%20combination%20of%0Aa%20few%20plane%20waves%20at%20different%20beam-steered%20angles%20instead%20of%20many%20focused%0Awaves.%20It%20can%20achieve%20much%20higher%20frame%20rates%2C%20but%20often%20at%20the%20cost%20of%20reduced%0Aimage%20quality.%20Deep%20learning%20approaches%20have%20been%20proposed%20to%20mitigate%20this%0Adisadvantage%2C%20in%20particular%20for%20single%20plane%20wave%20imaging.%20Predominantly%2C%0Aimage-to-image%20post-processing%20networks%20or%20fully%20learned%20data-to-image%20neural%0Anetworks%20are%20used.%20Both%20construct%20their%20mapping%20purely%20data-driven%20and%20require%0Aexpressive%20networks%20and%20large%20amounts%20of%20training%20data%20to%20perform%20well.%20In%0Acontrast%2C%20we%20consider%20data-to-image%20networks%20which%20incorporate%20a%20conventional%0Aimage%20formation%20techniques%20as%20differentiable%20layers%20in%20the%20network%0Aarchitecture.%20This%20allows%20for%20end-to-end%20training%20with%20small%20amounts%20of%0Atraining%20data.%20In%20this%20work%2C%20using%20f-k%20migration%20as%20an%20image%20formation%20layer%20is%0Aevaluated%20in-depth%20with%20experimental%20data.%20We%20acquired%20a%20data%20collection%0Adesigned%20for%20benchmarking%20data-driven%20plane%20wave%20imaging%20approaches%20using%20a%0Arealistic%20breast%20mimicking%20phantom%20and%20an%20ultrasound%20calibration%20phantom.%20The%0Aevaluation%20considers%20global%20and%20local%20image%20similarity%20measures%20and%20contrast%2C%0Aresolution%20and%20lesion%20detectability%20analysis.%20The%20results%20show%20that%20the%0Aproposed%20network%20architecture%20is%20capable%20of%20improving%20the%20image%20quality%20of%0Asingle%20plane%20wave%20images%20on%20all%20evaluation%20metrics.%20Furthermore%2C%20these%20image%0Aquality%20improvements%20can%20be%20achieved%20with%20surprisingly%20little%20amounts%20of%0Atraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14188v1&entry.124074799=Read"},
{"title": "Generalizable Neural Human Renderer", "author": "Mana Masuda and Jinhyung Park and Shun Iwase and Rawal Khirodkar and Kris Kitani", "abstract": "  While recent advancements in animatable human rendering have achieved\nremarkable results, they require test-time optimization for each subject which\ncan be a significant limitation for real-world applications. To address this,\nwe tackle the challenging task of learning a Generalizable Neural Human\nRenderer (GNH), a novel method for rendering animatable humans from monocular\nvideo without any test-time optimization. Our core method focuses on\ntransferring appearance information from the input video to the output image\nplane by utilizing explicit body priors and multi-view geometry. To render the\nsubject in the intended pose, we utilize a straightforward CNN-based image\nrenderer, foregoing the more common ray-sampling or rasterizing-based rendering\nmodules. Our GNH achieves remarkable generalizable, photorealistic rendering\nwith unseen subjects with a three-stage process. We quantitatively and\nqualitatively demonstrate that GNH significantly surpasses current\nstate-of-the-art methods, notably achieving a 31.3% improvement in LPIPS.\n", "link": "http://arxiv.org/abs/2404.14199v1", "date": "2024-04-22", "relevancy": 2.1051, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5445}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5155}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5077}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Neural%20Human%20Renderer&body=Title%3A%20Generalizable%20Neural%20Human%20Renderer%0AAuthor%3A%20Mana%20Masuda%20and%20Jinhyung%20Park%20and%20Shun%20Iwase%20and%20Rawal%20Khirodkar%20and%20Kris%20Kitani%0AAbstract%3A%20%20%20While%20recent%20advancements%20in%20animatable%20human%20rendering%20have%20achieved%0Aremarkable%20results%2C%20they%20require%20test-time%20optimization%20for%20each%20subject%20which%0Acan%20be%20a%20significant%20limitation%20for%20real-world%20applications.%20To%20address%20this%2C%0Awe%20tackle%20the%20challenging%20task%20of%20learning%20a%20Generalizable%20Neural%20Human%0ARenderer%20%28GNH%29%2C%20a%20novel%20method%20for%20rendering%20animatable%20humans%20from%20monocular%0Avideo%20without%20any%20test-time%20optimization.%20Our%20core%20method%20focuses%20on%0Atransferring%20appearance%20information%20from%20the%20input%20video%20to%20the%20output%20image%0Aplane%20by%20utilizing%20explicit%20body%20priors%20and%20multi-view%20geometry.%20To%20render%20the%0Asubject%20in%20the%20intended%20pose%2C%20we%20utilize%20a%20straightforward%20CNN-based%20image%0Arenderer%2C%20foregoing%20the%20more%20common%20ray-sampling%20or%20rasterizing-based%20rendering%0Amodules.%20Our%20GNH%20achieves%20remarkable%20generalizable%2C%20photorealistic%20rendering%0Awith%20unseen%20subjects%20with%20a%20three-stage%20process.%20We%20quantitatively%20and%0Aqualitatively%20demonstrate%20that%20GNH%20significantly%20surpasses%20current%0Astate-of-the-art%20methods%2C%20notably%20achieving%20a%2031.3%25%20improvement%20in%20LPIPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14199v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Neural%20Human%20Renderer&entry.906535625=Mana%20Masuda%20and%20Jinhyung%20Park%20and%20Shun%20Iwase%20and%20Rawal%20Khirodkar%20and%20Kris%20Kitani&entry.1292438233=%20%20While%20recent%20advancements%20in%20animatable%20human%20rendering%20have%20achieved%0Aremarkable%20results%2C%20they%20require%20test-time%20optimization%20for%20each%20subject%20which%0Acan%20be%20a%20significant%20limitation%20for%20real-world%20applications.%20To%20address%20this%2C%0Awe%20tackle%20the%20challenging%20task%20of%20learning%20a%20Generalizable%20Neural%20Human%0ARenderer%20%28GNH%29%2C%20a%20novel%20method%20for%20rendering%20animatable%20humans%20from%20monocular%0Avideo%20without%20any%20test-time%20optimization.%20Our%20core%20method%20focuses%20on%0Atransferring%20appearance%20information%20from%20the%20input%20video%20to%20the%20output%20image%0Aplane%20by%20utilizing%20explicit%20body%20priors%20and%20multi-view%20geometry.%20To%20render%20the%0Asubject%20in%20the%20intended%20pose%2C%20we%20utilize%20a%20straightforward%20CNN-based%20image%0Arenderer%2C%20foregoing%20the%20more%20common%20ray-sampling%20or%20rasterizing-based%20rendering%0Amodules.%20Our%20GNH%20achieves%20remarkable%20generalizable%2C%20photorealistic%20rendering%0Awith%20unseen%20subjects%20with%20a%20three-stage%20process.%20We%20quantitatively%20and%0Aqualitatively%20demonstrate%20that%20GNH%20significantly%20surpasses%20current%0Astate-of-the-art%20methods%2C%20notably%20achieving%20a%2031.3%25%20improvement%20in%20LPIPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14199v1&entry.124074799=Read"},
{"title": "Immersive Rover Control and Obstacle Detection based on Extended Reality\n  and Artificial Intelligence", "author": "Sof\u00eda Coloma and Alexandre Frantz and Dave van der Meer and Ernest Skrzypczyk and Andrej Orsula and Miguel Olivares-Mendez", "abstract": "  Lunar exploration has become a key focus, driving scientific and\ntechnological advances. Ongoing missions are deploying rovers to the surface of\nthe Moon, targeting the far side and south pole. However, these terrains pose\nchallenges, emphasizing the need for precise obstacles and resource detection\nto avoid mission risks. This work proposes a novel system that integrates\neXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar\nrovers. It is capable of autonomously detecting rocks and recreating an\nimmersive 3D virtual environment of the location of the robot. This system has\nbeen validated in a lunar laboratory to observe its advantages over traditional\n2D-based teleoperation approaches\n", "link": "http://arxiv.org/abs/2404.14095v1", "date": "2024-04-22", "relevancy": 2.1026, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5546}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Immersive%20Rover%20Control%20and%20Obstacle%20Detection%20based%20on%20Extended%20Reality%0A%20%20and%20Artificial%20Intelligence&body=Title%3A%20Immersive%20Rover%20Control%20and%20Obstacle%20Detection%20based%20on%20Extended%20Reality%0A%20%20and%20Artificial%20Intelligence%0AAuthor%3A%20Sof%C3%ADa%20Coloma%20and%20Alexandre%20Frantz%20and%20Dave%20van%20der%20Meer%20and%20Ernest%20Skrzypczyk%20and%20Andrej%20Orsula%20and%20Miguel%20Olivares-Mendez%0AAbstract%3A%20%20%20Lunar%20exploration%20has%20become%20a%20key%20focus%2C%20driving%20scientific%20and%0Atechnological%20advances.%20Ongoing%20missions%20are%20deploying%20rovers%20to%20the%20surface%20of%0Athe%20Moon%2C%20targeting%20the%20far%20side%20and%20south%20pole.%20However%2C%20these%20terrains%20pose%0Achallenges%2C%20emphasizing%20the%20need%20for%20precise%20obstacles%20and%20resource%20detection%0Ato%20avoid%20mission%20risks.%20This%20work%20proposes%20a%20novel%20system%20that%20integrates%0AeXtended%20Reality%20%28XR%29%20and%20Artificial%20Intelligence%20%28AI%29%20to%20teleoperate%20lunar%0Arovers.%20It%20is%20capable%20of%20autonomously%20detecting%20rocks%20and%20recreating%20an%0Aimmersive%203D%20virtual%20environment%20of%20the%20location%20of%20the%20robot.%20This%20system%20has%0Abeen%20validated%20in%20a%20lunar%20laboratory%20to%20observe%20its%20advantages%20over%20traditional%0A2D-based%20teleoperation%20approaches%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14095v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Immersive%20Rover%20Control%20and%20Obstacle%20Detection%20based%20on%20Extended%20Reality%0A%20%20and%20Artificial%20Intelligence&entry.906535625=Sof%C3%ADa%20Coloma%20and%20Alexandre%20Frantz%20and%20Dave%20van%20der%20Meer%20and%20Ernest%20Skrzypczyk%20and%20Andrej%20Orsula%20and%20Miguel%20Olivares-Mendez&entry.1292438233=%20%20Lunar%20exploration%20has%20become%20a%20key%20focus%2C%20driving%20scientific%20and%0Atechnological%20advances.%20Ongoing%20missions%20are%20deploying%20rovers%20to%20the%20surface%20of%0Athe%20Moon%2C%20targeting%20the%20far%20side%20and%20south%20pole.%20However%2C%20these%20terrains%20pose%0Achallenges%2C%20emphasizing%20the%20need%20for%20precise%20obstacles%20and%20resource%20detection%0Ato%20avoid%20mission%20risks.%20This%20work%20proposes%20a%20novel%20system%20that%20integrates%0AeXtended%20Reality%20%28XR%29%20and%20Artificial%20Intelligence%20%28AI%29%20to%20teleoperate%20lunar%0Arovers.%20It%20is%20capable%20of%20autonomously%20detecting%20rocks%20and%20recreating%20an%0Aimmersive%203D%20virtual%20environment%20of%20the%20location%20of%20the%20robot.%20This%20system%20has%0Abeen%20validated%20in%20a%20lunar%20laboratory%20to%20observe%20its%20advantages%20over%20traditional%0A2D-based%20teleoperation%20approaches%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14095v1&entry.124074799=Read"},
{"title": "A Multimodal Automated Interpretability Agent", "author": "Tamar Rott Shaham and Sarah Schwettmann and Franklin Wang and Achyuta Rajaram and Evan Hernandez and Jacob Andreas and Antonio Torralba", "abstract": "  This paper describes MAIA, a Multimodal Automated Interpretability Agent.\nMAIA is a system that uses neural models to automate neural model understanding\ntasks like feature interpretation and failure mode discovery. It equips a\npre-trained vision-language model with a set of tools that support iterative\nexperimentation on subcomponents of other models to explain their behavior.\nThese include tools commonly used by human interpretability researchers: for\nsynthesizing and editing inputs, computing maximally activating exemplars from\nreal-world datasets, and summarizing and describing experimental results.\nInterpretability experiments proposed by MAIA compose these tools to describe\nand explain system behavior. We evaluate applications of MAIA to computer\nvision models. We first characterize MAIA's ability to describe (neuron-level)\nfeatures in learned representations of images. Across several trained models\nand a novel dataset of synthetic vision neurons with paired ground-truth\ndescriptions, MAIA produces descriptions comparable to those generated by\nexpert human experimenters. We then show that MAIA can aid in two additional\ninterpretability tasks: reducing sensitivity to spurious features, and\nautomatically identifying inputs likely to be mis-classified.\n", "link": "http://arxiv.org/abs/2404.14394v1", "date": "2024-04-22", "relevancy": 2.0944, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5589}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5146}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Automated%20Interpretability%20Agent&body=Title%3A%20A%20Multimodal%20Automated%20Interpretability%20Agent%0AAuthor%3A%20Tamar%20Rott%20Shaham%20and%20Sarah%20Schwettmann%20and%20Franklin%20Wang%20and%20Achyuta%20Rajaram%20and%20Evan%20Hernandez%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba%0AAbstract%3A%20%20%20This%20paper%20describes%20MAIA%2C%20a%20Multimodal%20Automated%20Interpretability%20Agent.%0AMAIA%20is%20a%20system%20that%20uses%20neural%20models%20to%20automate%20neural%20model%20understanding%0Atasks%20like%20feature%20interpretation%20and%20failure%20mode%20discovery.%20It%20equips%20a%0Apre-trained%20vision-language%20model%20with%20a%20set%20of%20tools%20that%20support%20iterative%0Aexperimentation%20on%20subcomponents%20of%20other%20models%20to%20explain%20their%20behavior.%0AThese%20include%20tools%20commonly%20used%20by%20human%20interpretability%20researchers%3A%20for%0Asynthesizing%20and%20editing%20inputs%2C%20computing%20maximally%20activating%20exemplars%20from%0Areal-world%20datasets%2C%20and%20summarizing%20and%20describing%20experimental%20results.%0AInterpretability%20experiments%20proposed%20by%20MAIA%20compose%20these%20tools%20to%20describe%0Aand%20explain%20system%20behavior.%20We%20evaluate%20applications%20of%20MAIA%20to%20computer%0Avision%20models.%20We%20first%20characterize%20MAIA%27s%20ability%20to%20describe%20%28neuron-level%29%0Afeatures%20in%20learned%20representations%20of%20images.%20Across%20several%20trained%20models%0Aand%20a%20novel%20dataset%20of%20synthetic%20vision%20neurons%20with%20paired%20ground-truth%0Adescriptions%2C%20MAIA%20produces%20descriptions%20comparable%20to%20those%20generated%20by%0Aexpert%20human%20experimenters.%20We%20then%20show%20that%20MAIA%20can%20aid%20in%20two%20additional%0Ainterpretability%20tasks%3A%20reducing%20sensitivity%20to%20spurious%20features%2C%20and%0Aautomatically%20identifying%20inputs%20likely%20to%20be%20mis-classified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14394v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Automated%20Interpretability%20Agent&entry.906535625=Tamar%20Rott%20Shaham%20and%20Sarah%20Schwettmann%20and%20Franklin%20Wang%20and%20Achyuta%20Rajaram%20and%20Evan%20Hernandez%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba&entry.1292438233=%20%20This%20paper%20describes%20MAIA%2C%20a%20Multimodal%20Automated%20Interpretability%20Agent.%0AMAIA%20is%20a%20system%20that%20uses%20neural%20models%20to%20automate%20neural%20model%20understanding%0Atasks%20like%20feature%20interpretation%20and%20failure%20mode%20discovery.%20It%20equips%20a%0Apre-trained%20vision-language%20model%20with%20a%20set%20of%20tools%20that%20support%20iterative%0Aexperimentation%20on%20subcomponents%20of%20other%20models%20to%20explain%20their%20behavior.%0AThese%20include%20tools%20commonly%20used%20by%20human%20interpretability%20researchers%3A%20for%0Asynthesizing%20and%20editing%20inputs%2C%20computing%20maximally%20activating%20exemplars%20from%0Areal-world%20datasets%2C%20and%20summarizing%20and%20describing%20experimental%20results.%0AInterpretability%20experiments%20proposed%20by%20MAIA%20compose%20these%20tools%20to%20describe%0Aand%20explain%20system%20behavior.%20We%20evaluate%20applications%20of%20MAIA%20to%20computer%0Avision%20models.%20We%20first%20characterize%20MAIA%27s%20ability%20to%20describe%20%28neuron-level%29%0Afeatures%20in%20learned%20representations%20of%20images.%20Across%20several%20trained%20models%0Aand%20a%20novel%20dataset%20of%20synthetic%20vision%20neurons%20with%20paired%20ground-truth%0Adescriptions%2C%20MAIA%20produces%20descriptions%20comparable%20to%20those%20generated%20by%0Aexpert%20human%20experimenters.%20We%20then%20show%20that%20MAIA%20can%20aid%20in%20two%20additional%0Ainterpretability%20tasks%3A%20reducing%20sensitivity%20to%20spurious%20features%2C%20and%0Aautomatically%20identifying%20inputs%20likely%20to%20be%20mis-classified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14394v1&entry.124074799=Read"},
{"title": "YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel\n  Class Discovery", "author": "Qian Wan and Xiang Xiang and Qinhao Zhou", "abstract": "  Because of its use in practice, open-world object detection (OWOD) has gotten\na lot of attention recently. The challenge is how can a model detect novel\nclasses and then incrementally learn them without forgetting previously known\nclasses. Previous approaches hinge on strongly-supervised or weakly-supervised\nnovel-class data for novel-class detection, which may not apply to real\napplications. We construct a new benchmark that novel classes are only\nencountered at the inference stage. And we propose a new OWOD detector YOLOOC,\nbased on the YOLO architecture yet for the Open-Class setup. We introduce label\nsmoothing to prevent the detector from over-confidently mapping novel classes\nto known classes and to discover novel classes. Extensive experiments conducted\non our more realistic setup demonstrate the effectiveness of our method for\ndiscovering novel classes in our new benchmark.\n", "link": "http://arxiv.org/abs/2404.00257v2", "date": "2024-04-22", "relevancy": 2.0844, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5326}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5183}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5107}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20YOLOOC%3A%20YOLO-based%20Open-Class%20Incremental%20Object%20Detection%20with%20Novel%0A%20%20Class%20Discovery&body=Title%3A%20YOLOOC%3A%20YOLO-based%20Open-Class%20Incremental%20Object%20Detection%20with%20Novel%0A%20%20Class%20Discovery%0AAuthor%3A%20Qian%20Wan%20and%20Xiang%20Xiang%20and%20Qinhao%20Zhou%0AAbstract%3A%20%20%20Because%20of%20its%20use%20in%20practice%2C%20open-world%20object%20detection%20%28OWOD%29%20has%20gotten%0Aa%20lot%20of%20attention%20recently.%20The%20challenge%20is%20how%20can%20a%20model%20detect%20novel%0Aclasses%20and%20then%20incrementally%20learn%20them%20without%20forgetting%20previously%20known%0Aclasses.%20Previous%20approaches%20hinge%20on%20strongly-supervised%20or%20weakly-supervised%0Anovel-class%20data%20for%20novel-class%20detection%2C%20which%20may%20not%20apply%20to%20real%0Aapplications.%20We%20construct%20a%20new%20benchmark%20that%20novel%20classes%20are%20only%0Aencountered%20at%20the%20inference%20stage.%20And%20we%20propose%20a%20new%20OWOD%20detector%20YOLOOC%2C%0Abased%20on%20the%20YOLO%20architecture%20yet%20for%20the%20Open-Class%20setup.%20We%20introduce%20label%0Asmoothing%20to%20prevent%20the%20detector%20from%20over-confidently%20mapping%20novel%20classes%0Ato%20known%20classes%20and%20to%20discover%20novel%20classes.%20Extensive%20experiments%20conducted%0Aon%20our%20more%20realistic%20setup%20demonstrate%20the%20effectiveness%20of%20our%20method%20for%0Adiscovering%20novel%20classes%20in%20our%20new%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00257v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOOC%3A%20YOLO-based%20Open-Class%20Incremental%20Object%20Detection%20with%20Novel%0A%20%20Class%20Discovery&entry.906535625=Qian%20Wan%20and%20Xiang%20Xiang%20and%20Qinhao%20Zhou&entry.1292438233=%20%20Because%20of%20its%20use%20in%20practice%2C%20open-world%20object%20detection%20%28OWOD%29%20has%20gotten%0Aa%20lot%20of%20attention%20recently.%20The%20challenge%20is%20how%20can%20a%20model%20detect%20novel%0Aclasses%20and%20then%20incrementally%20learn%20them%20without%20forgetting%20previously%20known%0Aclasses.%20Previous%20approaches%20hinge%20on%20strongly-supervised%20or%20weakly-supervised%0Anovel-class%20data%20for%20novel-class%20detection%2C%20which%20may%20not%20apply%20to%20real%0Aapplications.%20We%20construct%20a%20new%20benchmark%20that%20novel%20classes%20are%20only%0Aencountered%20at%20the%20inference%20stage.%20And%20we%20propose%20a%20new%20OWOD%20detector%20YOLOOC%2C%0Abased%20on%20the%20YOLO%20architecture%20yet%20for%20the%20Open-Class%20setup.%20We%20introduce%20label%0Asmoothing%20to%20prevent%20the%20detector%20from%20over-confidently%20mapping%20novel%20classes%0Ato%20known%20classes%20and%20to%20discover%20novel%20classes.%20Extensive%20experiments%20conducted%0Aon%20our%20more%20realistic%20setup%20demonstrate%20the%20effectiveness%20of%20our%20method%20for%0Adiscovering%20novel%20classes%20in%20our%20new%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00257v2&entry.124074799=Read"},
{"title": "Automatic Discovery of Visual Circuits", "author": "Achyuta Rajaram and Neil Chowdhury and Antonio Torralba and Jacob Andreas and Sarah Schwettmann", "abstract": "  To date, most discoveries of network subcomponents that implement\nhuman-interpretable computations in deep vision models have involved close\nstudy of single units and large amounts of human labor. We explore scalable\nmethods for extracting the subgraph of a vision model's computational graph\nthat underlies recognition of a specific visual concept. We introduce a new\nmethod for identifying these subgraphs: specifying a visual concept using a few\nexamples, and then tracing the interdependence of neuron activations across\nlayers, or their functional connectivity. We find that our approach extracts\ncircuits that causally affect model output, and that editing these circuits can\ndefend large pretrained models from adversarial attacks.\n", "link": "http://arxiv.org/abs/2404.14349v1", "date": "2024-04-22", "relevancy": 2.0829, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5377}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5099}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5081}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Discovery%20of%20Visual%20Circuits&body=Title%3A%20Automatic%20Discovery%20of%20Visual%20Circuits%0AAuthor%3A%20Achyuta%20Rajaram%20and%20Neil%20Chowdhury%20and%20Antonio%20Torralba%20and%20Jacob%20Andreas%20and%20Sarah%20Schwettmann%0AAbstract%3A%20%20%20To%20date%2C%20most%20discoveries%20of%20network%20subcomponents%20that%20implement%0Ahuman-interpretable%20computations%20in%20deep%20vision%20models%20have%20involved%20close%0Astudy%20of%20single%20units%20and%20large%20amounts%20of%20human%20labor.%20We%20explore%20scalable%0Amethods%20for%20extracting%20the%20subgraph%20of%20a%20vision%20model%27s%20computational%20graph%0Athat%20underlies%20recognition%20of%20a%20specific%20visual%20concept.%20We%20introduce%20a%20new%0Amethod%20for%20identifying%20these%20subgraphs%3A%20specifying%20a%20visual%20concept%20using%20a%20few%0Aexamples%2C%20and%20then%20tracing%20the%20interdependence%20of%20neuron%20activations%20across%0Alayers%2C%20or%20their%20functional%20connectivity.%20We%20find%20that%20our%20approach%20extracts%0Acircuits%20that%20causally%20affect%20model%20output%2C%20and%20that%20editing%20these%20circuits%20can%0Adefend%20large%20pretrained%20models%20from%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14349v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Discovery%20of%20Visual%20Circuits&entry.906535625=Achyuta%20Rajaram%20and%20Neil%20Chowdhury%20and%20Antonio%20Torralba%20and%20Jacob%20Andreas%20and%20Sarah%20Schwettmann&entry.1292438233=%20%20To%20date%2C%20most%20discoveries%20of%20network%20subcomponents%20that%20implement%0Ahuman-interpretable%20computations%20in%20deep%20vision%20models%20have%20involved%20close%0Astudy%20of%20single%20units%20and%20large%20amounts%20of%20human%20labor.%20We%20explore%20scalable%0Amethods%20for%20extracting%20the%20subgraph%20of%20a%20vision%20model%27s%20computational%20graph%0Athat%20underlies%20recognition%20of%20a%20specific%20visual%20concept.%20We%20introduce%20a%20new%0Amethod%20for%20identifying%20these%20subgraphs%3A%20specifying%20a%20visual%20concept%20using%20a%20few%0Aexamples%2C%20and%20then%20tracing%20the%20interdependence%20of%20neuron%20activations%20across%0Alayers%2C%20or%20their%20functional%20connectivity.%20We%20find%20that%20our%20approach%20extracts%0Acircuits%20that%20causally%20affect%20model%20output%2C%20and%20that%20editing%20these%20circuits%20can%0Adefend%20large%20pretrained%20models%20from%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14349v1&entry.124074799=Read"},
{"title": "Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation", "author": "G\u00e1bor Antal and Rich\u00e1rd Voz\u00e1r and Rudolf Ferenc", "abstract": "  The emergence of advanced neural networks has opened up new ways in automated\ncode generation from conceptual models, promising to enhance software\ndevelopment processes. This paper presents a preliminary evaluation of\nGPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in\ntransforming Unified Modeling Language (UML) class diagrams into fully\noperating Java class files. In our study, we used exported images of 18 class\ndiagrams comprising 10 single-class and 8 multi-class diagrams. We used 3\ndifferent prompts for each input, and we manually evaluated the results. We\ncreated a scoring system in which we scored the occurrence of elements found in\nthe diagram within the source code. On average, the model was able to generate\nsource code for 88% of the elements shown in the diagrams. Our results indicate\nthat GPT-4-Vision exhibits proficiency in handling single-class UML diagrams,\nsuccessfully transforming them into syntactically correct class files. However,\nfor multi-class UML diagrams, the model's performance is weaker compared to\nsingle-class diagrams. In summary, further investigations are necessary to\nexploit the model's potential completely.\n", "link": "http://arxiv.org/abs/2404.14370v1", "date": "2024-04-22", "relevancy": 2.0489, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5543}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessing%20GPT-4-Vision%27s%20Capabilities%20in%20UML-Based%20Code%20Generation&body=Title%3A%20Assessing%20GPT-4-Vision%27s%20Capabilities%20in%20UML-Based%20Code%20Generation%0AAuthor%3A%20G%C3%A1bor%20Antal%20and%20Rich%C3%A1rd%20Voz%C3%A1r%20and%20Rudolf%20Ferenc%0AAbstract%3A%20%20%20The%20emergence%20of%20advanced%20neural%20networks%20has%20opened%20up%20new%20ways%20in%20automated%0Acode%20generation%20from%20conceptual%20models%2C%20promising%20to%20enhance%20software%0Adevelopment%20processes.%20This%20paper%20presents%20a%20preliminary%20evaluation%20of%0AGPT-4-Vision%2C%20a%20state-of-the-art%20deep%20learning%20model%2C%20and%20its%20capabilities%20in%0Atransforming%20Unified%20Modeling%20Language%20%28UML%29%20class%20diagrams%20into%20fully%0Aoperating%20Java%20class%20files.%20In%20our%20study%2C%20we%20used%20exported%20images%20of%2018%20class%0Adiagrams%20comprising%2010%20single-class%20and%208%20multi-class%20diagrams.%20We%20used%203%0Adifferent%20prompts%20for%20each%20input%2C%20and%20we%20manually%20evaluated%20the%20results.%20We%0Acreated%20a%20scoring%20system%20in%20which%20we%20scored%20the%20occurrence%20of%20elements%20found%20in%0Athe%20diagram%20within%20the%20source%20code.%20On%20average%2C%20the%20model%20was%20able%20to%20generate%0Asource%20code%20for%2088%25%20of%20the%20elements%20shown%20in%20the%20diagrams.%20Our%20results%20indicate%0Athat%20GPT-4-Vision%20exhibits%20proficiency%20in%20handling%20single-class%20UML%20diagrams%2C%0Asuccessfully%20transforming%20them%20into%20syntactically%20correct%20class%20files.%20However%2C%0Afor%20multi-class%20UML%20diagrams%2C%20the%20model%27s%20performance%20is%20weaker%20compared%20to%0Asingle-class%20diagrams.%20In%20summary%2C%20further%20investigations%20are%20necessary%20to%0Aexploit%20the%20model%27s%20potential%20completely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14370v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20GPT-4-Vision%27s%20Capabilities%20in%20UML-Based%20Code%20Generation&entry.906535625=G%C3%A1bor%20Antal%20and%20Rich%C3%A1rd%20Voz%C3%A1r%20and%20Rudolf%20Ferenc&entry.1292438233=%20%20The%20emergence%20of%20advanced%20neural%20networks%20has%20opened%20up%20new%20ways%20in%20automated%0Acode%20generation%20from%20conceptual%20models%2C%20promising%20to%20enhance%20software%0Adevelopment%20processes.%20This%20paper%20presents%20a%20preliminary%20evaluation%20of%0AGPT-4-Vision%2C%20a%20state-of-the-art%20deep%20learning%20model%2C%20and%20its%20capabilities%20in%0Atransforming%20Unified%20Modeling%20Language%20%28UML%29%20class%20diagrams%20into%20fully%0Aoperating%20Java%20class%20files.%20In%20our%20study%2C%20we%20used%20exported%20images%20of%2018%20class%0Adiagrams%20comprising%2010%20single-class%20and%208%20multi-class%20diagrams.%20We%20used%203%0Adifferent%20prompts%20for%20each%20input%2C%20and%20we%20manually%20evaluated%20the%20results.%20We%0Acreated%20a%20scoring%20system%20in%20which%20we%20scored%20the%20occurrence%20of%20elements%20found%20in%0Athe%20diagram%20within%20the%20source%20code.%20On%20average%2C%20the%20model%20was%20able%20to%20generate%0Asource%20code%20for%2088%25%20of%20the%20elements%20shown%20in%20the%20diagrams.%20Our%20results%20indicate%0Athat%20GPT-4-Vision%20exhibits%20proficiency%20in%20handling%20single-class%20UML%20diagrams%2C%0Asuccessfully%20transforming%20them%20into%20syntactically%20correct%20class%20files.%20However%2C%0Afor%20multi-class%20UML%20diagrams%2C%20the%20model%27s%20performance%20is%20weaker%20compared%20to%0Asingle-class%20diagrams.%20In%20summary%2C%20further%20investigations%20are%20necessary%20to%0Aexploit%20the%20model%27s%20potential%20completely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14370v1&entry.124074799=Read"},
{"title": "Co-designing a Sub-millisecond Latency Event-based Eye Tracking System\n  with Submanifold Sparse CNN", "author": "Baoheng Zhang and Yizhao Gao and Jingyuan Li and Hayden Kwok-Hay So", "abstract": "  Eye-tracking technology is integral to numerous consumer electronics\napplications, particularly in the realm of virtual and augmented reality\n(VR/AR). These applications demand solutions that excel in three crucial\naspects: low-latency, low-power consumption, and precision. Yet, achieving\noptimal performance across all these fronts presents a formidable challenge,\nnecessitating a balance between sophisticated algorithms and efficient backend\nhardware implementations. In this study, we tackle this challenge through a\nsynergistic software/hardware co-design of the system with an event camera.\nLeveraging the inherent sparsity of event-based input data, we integrate a\nnovel sparse FPGA dataflow accelerator customized for submanifold sparse\nconvolution neural networks (SCNN). The SCNN implemented on the accelerator can\nefficiently extract the embedding feature vector from each representation of\nevent slices by only processing the non-zero activations. Subsequently, these\nvectors undergo further processing by a gated recurrent unit (GRU) and a fully\nconnected layer on the host CPU to generate the eye centers. Deployment and\nevaluation of our system reveal outstanding performance metrics. On the\nEvent-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy,\n99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while\nonly consuming 2.29 mJ per inference. Notably, our solution opens up\nopportunities for future eye-tracking systems. Code is available at\nhttps://github.com/CASR-HKU/ESDA/tree/eye_tracking.\n", "link": "http://arxiv.org/abs/2404.14279v1", "date": "2024-04-22", "relevancy": 2.039, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4962}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Co-designing%20a%20Sub-millisecond%20Latency%20Event-based%20Eye%20Tracking%20System%0A%20%20with%20Submanifold%20Sparse%20CNN&body=Title%3A%20Co-designing%20a%20Sub-millisecond%20Latency%20Event-based%20Eye%20Tracking%20System%0A%20%20with%20Submanifold%20Sparse%20CNN%0AAuthor%3A%20Baoheng%20Zhang%20and%20Yizhao%20Gao%20and%20Jingyuan%20Li%20and%20Hayden%20Kwok-Hay%20So%0AAbstract%3A%20%20%20Eye-tracking%20technology%20is%20integral%20to%20numerous%20consumer%20electronics%0Aapplications%2C%20particularly%20in%20the%20realm%20of%20virtual%20and%20augmented%20reality%0A%28VR/AR%29.%20These%20applications%20demand%20solutions%20that%20excel%20in%20three%20crucial%0Aaspects%3A%20low-latency%2C%20low-power%20consumption%2C%20and%20precision.%20Yet%2C%20achieving%0Aoptimal%20performance%20across%20all%20these%20fronts%20presents%20a%20formidable%20challenge%2C%0Anecessitating%20a%20balance%20between%20sophisticated%20algorithms%20and%20efficient%20backend%0Ahardware%20implementations.%20In%20this%20study%2C%20we%20tackle%20this%20challenge%20through%20a%0Asynergistic%20software/hardware%20co-design%20of%20the%20system%20with%20an%20event%20camera.%0ALeveraging%20the%20inherent%20sparsity%20of%20event-based%20input%20data%2C%20we%20integrate%20a%0Anovel%20sparse%20FPGA%20dataflow%20accelerator%20customized%20for%20submanifold%20sparse%0Aconvolution%20neural%20networks%20%28SCNN%29.%20The%20SCNN%20implemented%20on%20the%20accelerator%20can%0Aefficiently%20extract%20the%20embedding%20feature%20vector%20from%20each%20representation%20of%0Aevent%20slices%20by%20only%20processing%20the%20non-zero%20activations.%20Subsequently%2C%20these%0Avectors%20undergo%20further%20processing%20by%20a%20gated%20recurrent%20unit%20%28GRU%29%20and%20a%20fully%0Aconnected%20layer%20on%20the%20host%20CPU%20to%20generate%20the%20eye%20centers.%20Deployment%20and%0Aevaluation%20of%20our%20system%20reveal%20outstanding%20performance%20metrics.%20On%20the%0AEvent-based%20Eye-Tracking-AIS2024%20dataset%2C%20our%20system%20achieves%2081%25%20p5%20accuracy%2C%0A99.5%25%20p10%20accuracy%2C%20and%203.71%20Mean%20Euclidean%20Distance%20with%200.7%20ms%20latency%20while%0Aonly%20consuming%202.29%20mJ%20per%20inference.%20Notably%2C%20our%20solution%20opens%20up%0Aopportunities%20for%20future%20eye-tracking%20systems.%20Code%20is%20available%20at%0Ahttps%3A//github.com/CASR-HKU/ESDA/tree/eye_tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14279v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-designing%20a%20Sub-millisecond%20Latency%20Event-based%20Eye%20Tracking%20System%0A%20%20with%20Submanifold%20Sparse%20CNN&entry.906535625=Baoheng%20Zhang%20and%20Yizhao%20Gao%20and%20Jingyuan%20Li%20and%20Hayden%20Kwok-Hay%20So&entry.1292438233=%20%20Eye-tracking%20technology%20is%20integral%20to%20numerous%20consumer%20electronics%0Aapplications%2C%20particularly%20in%20the%20realm%20of%20virtual%20and%20augmented%20reality%0A%28VR/AR%29.%20These%20applications%20demand%20solutions%20that%20excel%20in%20three%20crucial%0Aaspects%3A%20low-latency%2C%20low-power%20consumption%2C%20and%20precision.%20Yet%2C%20achieving%0Aoptimal%20performance%20across%20all%20these%20fronts%20presents%20a%20formidable%20challenge%2C%0Anecessitating%20a%20balance%20between%20sophisticated%20algorithms%20and%20efficient%20backend%0Ahardware%20implementations.%20In%20this%20study%2C%20we%20tackle%20this%20challenge%20through%20a%0Asynergistic%20software/hardware%20co-design%20of%20the%20system%20with%20an%20event%20camera.%0ALeveraging%20the%20inherent%20sparsity%20of%20event-based%20input%20data%2C%20we%20integrate%20a%0Anovel%20sparse%20FPGA%20dataflow%20accelerator%20customized%20for%20submanifold%20sparse%0Aconvolution%20neural%20networks%20%28SCNN%29.%20The%20SCNN%20implemented%20on%20the%20accelerator%20can%0Aefficiently%20extract%20the%20embedding%20feature%20vector%20from%20each%20representation%20of%0Aevent%20slices%20by%20only%20processing%20the%20non-zero%20activations.%20Subsequently%2C%20these%0Avectors%20undergo%20further%20processing%20by%20a%20gated%20recurrent%20unit%20%28GRU%29%20and%20a%20fully%0Aconnected%20layer%20on%20the%20host%20CPU%20to%20generate%20the%20eye%20centers.%20Deployment%20and%0Aevaluation%20of%20our%20system%20reveal%20outstanding%20performance%20metrics.%20On%20the%0AEvent-based%20Eye-Tracking-AIS2024%20dataset%2C%20our%20system%20achieves%2081%25%20p5%20accuracy%2C%0A99.5%25%20p10%20accuracy%2C%20and%203.71%20Mean%20Euclidean%20Distance%20with%200.7%20ms%20latency%20while%0Aonly%20consuming%202.29%20mJ%20per%20inference.%20Notably%2C%20our%20solution%20opens%20up%0Aopportunities%20for%20future%20eye-tracking%20systems.%20Code%20is%20available%20at%0Ahttps%3A//github.com/CASR-HKU/ESDA/tree/eye_tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14279v1&entry.124074799=Read"},
{"title": "Key ingredients for effective zero-shot cross-lingual knowledge transfer\n  in generative tasks", "author": "Nadezhda Chirkova and Vassilina Nikoulina", "abstract": "  Zero-shot cross-lingual knowledge transfer enables a multilingual pretrained\nlanguage model, finetuned on a task in one language, make predictions for this\ntask in other languages. While being broadly studied for natural language\nunderstanding tasks, the described setting is understudied for generation.\nPrevious works notice a frequent problem of generation in a wrong language and\npropose approaches to address it, usually using mT5 as a backbone model. In\nthis work we compare various approaches proposed from the literature in unified\nsettings, also including alternative backbone models, namely mBART and\nNLLB-200. We first underline the importance of tuning learning rate used for\nfinetuning, which helps to substantially alleviate the problem of generation in\nthe wrong language. Then, we show that with careful learning rate tuning, the\nsimple full finetuning of the model acts as a very strong baseline and\nalternative approaches bring only marginal improvements. Finally, we find that\nmBART performs similarly to mT5 of the same size, and NLLB-200 can be\ncompetitive in some cases. Our final zero-shot models reach the performance of\nthe approach based on data translation which is usually considered as an upper\nbaseline for zero-shot cross-lingual transfer in generation.\n", "link": "http://arxiv.org/abs/2402.12279v2", "date": "2024-04-22", "relevancy": 2.0368, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5141}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4988}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Key%20ingredients%20for%20effective%20zero-shot%20cross-lingual%20knowledge%20transfer%0A%20%20in%20generative%20tasks&body=Title%3A%20Key%20ingredients%20for%20effective%20zero-shot%20cross-lingual%20knowledge%20transfer%0A%20%20in%20generative%20tasks%0AAuthor%3A%20Nadezhda%20Chirkova%20and%20Vassilina%20Nikoulina%0AAbstract%3A%20%20%20Zero-shot%20cross-lingual%20knowledge%20transfer%20enables%20a%20multilingual%20pretrained%0Alanguage%20model%2C%20finetuned%20on%20a%20task%20in%20one%20language%2C%20make%20predictions%20for%20this%0Atask%20in%20other%20languages.%20While%20being%20broadly%20studied%20for%20natural%20language%0Aunderstanding%20tasks%2C%20the%20described%20setting%20is%20understudied%20for%20generation.%0APrevious%20works%20notice%20a%20frequent%20problem%20of%20generation%20in%20a%20wrong%20language%20and%0Apropose%20approaches%20to%20address%20it%2C%20usually%20using%20mT5%20as%20a%20backbone%20model.%20In%0Athis%20work%20we%20compare%20various%20approaches%20proposed%20from%20the%20literature%20in%20unified%0Asettings%2C%20also%20including%20alternative%20backbone%20models%2C%20namely%20mBART%20and%0ANLLB-200.%20We%20first%20underline%20the%20importance%20of%20tuning%20learning%20rate%20used%20for%0Afinetuning%2C%20which%20helps%20to%20substantially%20alleviate%20the%20problem%20of%20generation%20in%0Athe%20wrong%20language.%20Then%2C%20we%20show%20that%20with%20careful%20learning%20rate%20tuning%2C%20the%0Asimple%20full%20finetuning%20of%20the%20model%20acts%20as%20a%20very%20strong%20baseline%20and%0Aalternative%20approaches%20bring%20only%20marginal%20improvements.%20Finally%2C%20we%20find%20that%0AmBART%20performs%20similarly%20to%20mT5%20of%20the%20same%20size%2C%20and%20NLLB-200%20can%20be%0Acompetitive%20in%20some%20cases.%20Our%20final%20zero-shot%20models%20reach%20the%20performance%20of%0Athe%20approach%20based%20on%20data%20translation%20which%20is%20usually%20considered%20as%20an%20upper%0Abaseline%20for%20zero-shot%20cross-lingual%20transfer%20in%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12279v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Key%20ingredients%20for%20effective%20zero-shot%20cross-lingual%20knowledge%20transfer%0A%20%20in%20generative%20tasks&entry.906535625=Nadezhda%20Chirkova%20and%20Vassilina%20Nikoulina&entry.1292438233=%20%20Zero-shot%20cross-lingual%20knowledge%20transfer%20enables%20a%20multilingual%20pretrained%0Alanguage%20model%2C%20finetuned%20on%20a%20task%20in%20one%20language%2C%20make%20predictions%20for%20this%0Atask%20in%20other%20languages.%20While%20being%20broadly%20studied%20for%20natural%20language%0Aunderstanding%20tasks%2C%20the%20described%20setting%20is%20understudied%20for%20generation.%0APrevious%20works%20notice%20a%20frequent%20problem%20of%20generation%20in%20a%20wrong%20language%20and%0Apropose%20approaches%20to%20address%20it%2C%20usually%20using%20mT5%20as%20a%20backbone%20model.%20In%0Athis%20work%20we%20compare%20various%20approaches%20proposed%20from%20the%20literature%20in%20unified%0Asettings%2C%20also%20including%20alternative%20backbone%20models%2C%20namely%20mBART%20and%0ANLLB-200.%20We%20first%20underline%20the%20importance%20of%20tuning%20learning%20rate%20used%20for%0Afinetuning%2C%20which%20helps%20to%20substantially%20alleviate%20the%20problem%20of%20generation%20in%0Athe%20wrong%20language.%20Then%2C%20we%20show%20that%20with%20careful%20learning%20rate%20tuning%2C%20the%0Asimple%20full%20finetuning%20of%20the%20model%20acts%20as%20a%20very%20strong%20baseline%20and%0Aalternative%20approaches%20bring%20only%20marginal%20improvements.%20Finally%2C%20we%20find%20that%0AmBART%20performs%20similarly%20to%20mT5%20of%20the%20same%20size%2C%20and%20NLLB-200%20can%20be%0Acompetitive%20in%20some%20cases.%20Our%20final%20zero-shot%20models%20reach%20the%20performance%20of%0Athe%20approach%20based%20on%20data%20translation%20which%20is%20usually%20considered%20as%20an%20upper%0Abaseline%20for%20zero-shot%20cross-lingual%20transfer%20in%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12279v2&entry.124074799=Read"},
{"title": "CoGS: Controllable Gaussian Splatting", "author": "Heng Yu and Joel Julin and Zolt\u00e1n \u00c1. Milacski and Koichiro Niinuma and L\u00e1szl\u00f3 A. Jeni", "abstract": "  Capturing and re-animating the 3D structure of articulated objects present\nsignificant barriers. On one hand, methods requiring extensively calibrated\nmulti-view setups are prohibitively complex and resource-intensive, limiting\ntheir practical applicability. On the other hand, while single-camera Neural\nRadiance Fields (NeRFs) offer a more streamlined approach, they have excessive\ntraining and rendering costs. 3D Gaussian Splatting would be a suitable\nalternative but for two reasons. Firstly, existing methods for 3D dynamic\nGaussians require synchronized multi-view cameras, and secondly, the lack of\ncontrollability in dynamic scenarios. We present CoGS, a method for\nControllable Gaussian Splatting, that enables the direct manipulation of scene\nelements, offering real-time control of dynamic scenes without the prerequisite\nof pre-computing control signals. We evaluated CoGS using both synthetic and\nreal-world datasets that include dynamic objects that differ in degree of\ndifficulty. In our evaluations, CoGS consistently outperformed existing dynamic\nand controllable neural representations in terms of visual fidelity.\n", "link": "http://arxiv.org/abs/2312.05664v2", "date": "2024-04-22", "relevancy": 2.0368, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5365}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5107}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4968}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoGS%3A%20Controllable%20Gaussian%20Splatting&body=Title%3A%20CoGS%3A%20Controllable%20Gaussian%20Splatting%0AAuthor%3A%20Heng%20Yu%20and%20Joel%20Julin%20and%20Zolt%C3%A1n%20%C3%81.%20Milacski%20and%20Koichiro%20Niinuma%20and%20L%C3%A1szl%C3%B3%20A.%20Jeni%0AAbstract%3A%20%20%20Capturing%20and%20re-animating%20the%203D%20structure%20of%20articulated%20objects%20present%0Asignificant%20barriers.%20On%20one%20hand%2C%20methods%20requiring%20extensively%20calibrated%0Amulti-view%20setups%20are%20prohibitively%20complex%20and%20resource-intensive%2C%20limiting%0Atheir%20practical%20applicability.%20On%20the%20other%20hand%2C%20while%20single-camera%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20offer%20a%20more%20streamlined%20approach%2C%20they%20have%20excessive%0Atraining%20and%20rendering%20costs.%203D%20Gaussian%20Splatting%20would%20be%20a%20suitable%0Aalternative%20but%20for%20two%20reasons.%20Firstly%2C%20existing%20methods%20for%203D%20dynamic%0AGaussians%20require%20synchronized%20multi-view%20cameras%2C%20and%20secondly%2C%20the%20lack%20of%0Acontrollability%20in%20dynamic%20scenarios.%20We%20present%20CoGS%2C%20a%20method%20for%0AControllable%20Gaussian%20Splatting%2C%20that%20enables%20the%20direct%20manipulation%20of%20scene%0Aelements%2C%20offering%20real-time%20control%20of%20dynamic%20scenes%20without%20the%20prerequisite%0Aof%20pre-computing%20control%20signals.%20We%20evaluated%20CoGS%20using%20both%20synthetic%20and%0Areal-world%20datasets%20that%20include%20dynamic%20objects%20that%20differ%20in%20degree%20of%0Adifficulty.%20In%20our%20evaluations%2C%20CoGS%20consistently%20outperformed%20existing%20dynamic%0Aand%20controllable%20neural%20representations%20in%20terms%20of%20visual%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05664v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoGS%3A%20Controllable%20Gaussian%20Splatting&entry.906535625=Heng%20Yu%20and%20Joel%20Julin%20and%20Zolt%C3%A1n%20%C3%81.%20Milacski%20and%20Koichiro%20Niinuma%20and%20L%C3%A1szl%C3%B3%20A.%20Jeni&entry.1292438233=%20%20Capturing%20and%20re-animating%20the%203D%20structure%20of%20articulated%20objects%20present%0Asignificant%20barriers.%20On%20one%20hand%2C%20methods%20requiring%20extensively%20calibrated%0Amulti-view%20setups%20are%20prohibitively%20complex%20and%20resource-intensive%2C%20limiting%0Atheir%20practical%20applicability.%20On%20the%20other%20hand%2C%20while%20single-camera%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20offer%20a%20more%20streamlined%20approach%2C%20they%20have%20excessive%0Atraining%20and%20rendering%20costs.%203D%20Gaussian%20Splatting%20would%20be%20a%20suitable%0Aalternative%20but%20for%20two%20reasons.%20Firstly%2C%20existing%20methods%20for%203D%20dynamic%0AGaussians%20require%20synchronized%20multi-view%20cameras%2C%20and%20secondly%2C%20the%20lack%20of%0Acontrollability%20in%20dynamic%20scenarios.%20We%20present%20CoGS%2C%20a%20method%20for%0AControllable%20Gaussian%20Splatting%2C%20that%20enables%20the%20direct%20manipulation%20of%20scene%0Aelements%2C%20offering%20real-time%20control%20of%20dynamic%20scenes%20without%20the%20prerequisite%0Aof%20pre-computing%20control%20signals.%20We%20evaluated%20CoGS%20using%20both%20synthetic%20and%0Areal-world%20datasets%20that%20include%20dynamic%20objects%20that%20differ%20in%20degree%20of%0Adifficulty.%20In%20our%20evaluations%2C%20CoGS%20consistently%20outperformed%20existing%20dynamic%0Aand%20controllable%20neural%20representations%20in%20terms%20of%20visual%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05664v2&entry.124074799=Read"},
{"title": "BCFPL: Binary classification ConvNet based Fast Parking space\n  recognition with Low resolution image", "author": "Shuo Zhang and Xin Chen and Zixuan Wang", "abstract": "  The automobile plays an important role in the economic activities of mankind,\nespecially in the metropolis. Under the circumstances, the demand of quick\nsearch for available parking spaces has become a major concern for the\nautomobile drivers. Meanwhile, the public sense of privacy is also awaking, the\nimage-based parking space recognition methods lack the attention of privacy\nprotection. In this paper, we proposed a binary convolutional neural network\nwith lightweight design structure named BCFPL, which can be used to train with\nlow-resolution parking space images and offer a reasonable recognition result.\nThe images of parking space were collected from various complex environments,\nincluding different weather, occlusion conditions, and various camera angles.\nWe conducted the training and testing progresses among different datasets and\npartial subsets. The experimental results show that the accuracy of BCFPL does\nnot decrease compared with the original resolution image directly, and can\nreach the average level of the existing mainstream method. BCFPL also has low\nhardware requirements and fast recognition speed while meeting the privacy\nrequirements, so it has application potential in intelligent city construction\nand automatic driving field.\n", "link": "http://arxiv.org/abs/2404.14198v1", "date": "2024-04-22", "relevancy": 2.0283, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BCFPL%3A%20Binary%20classification%20ConvNet%20based%20Fast%20Parking%20space%0A%20%20recognition%20with%20Low%20resolution%20image&body=Title%3A%20BCFPL%3A%20Binary%20classification%20ConvNet%20based%20Fast%20Parking%20space%0A%20%20recognition%20with%20Low%20resolution%20image%0AAuthor%3A%20Shuo%20Zhang%20and%20Xin%20Chen%20and%20Zixuan%20Wang%0AAbstract%3A%20%20%20The%20automobile%20plays%20an%20important%20role%20in%20the%20economic%20activities%20of%20mankind%2C%0Aespecially%20in%20the%20metropolis.%20Under%20the%20circumstances%2C%20the%20demand%20of%20quick%0Asearch%20for%20available%20parking%20spaces%20has%20become%20a%20major%20concern%20for%20the%0Aautomobile%20drivers.%20Meanwhile%2C%20the%20public%20sense%20of%20privacy%20is%20also%20awaking%2C%20the%0Aimage-based%20parking%20space%20recognition%20methods%20lack%20the%20attention%20of%20privacy%0Aprotection.%20In%20this%20paper%2C%20we%20proposed%20a%20binary%20convolutional%20neural%20network%0Awith%20lightweight%20design%20structure%20named%20BCFPL%2C%20which%20can%20be%20used%20to%20train%20with%0Alow-resolution%20parking%20space%20images%20and%20offer%20a%20reasonable%20recognition%20result.%0AThe%20images%20of%20parking%20space%20were%20collected%20from%20various%20complex%20environments%2C%0Aincluding%20different%20weather%2C%20occlusion%20conditions%2C%20and%20various%20camera%20angles.%0AWe%20conducted%20the%20training%20and%20testing%20progresses%20among%20different%20datasets%20and%0Apartial%20subsets.%20The%20experimental%20results%20show%20that%20the%20accuracy%20of%20BCFPL%20does%0Anot%20decrease%20compared%20with%20the%20original%20resolution%20image%20directly%2C%20and%20can%0Areach%20the%20average%20level%20of%20the%20existing%20mainstream%20method.%20BCFPL%20also%20has%20low%0Ahardware%20requirements%20and%20fast%20recognition%20speed%20while%20meeting%20the%20privacy%0Arequirements%2C%20so%20it%20has%20application%20potential%20in%20intelligent%20city%20construction%0Aand%20automatic%20driving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14198v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BCFPL%3A%20Binary%20classification%20ConvNet%20based%20Fast%20Parking%20space%0A%20%20recognition%20with%20Low%20resolution%20image&entry.906535625=Shuo%20Zhang%20and%20Xin%20Chen%20and%20Zixuan%20Wang&entry.1292438233=%20%20The%20automobile%20plays%20an%20important%20role%20in%20the%20economic%20activities%20of%20mankind%2C%0Aespecially%20in%20the%20metropolis.%20Under%20the%20circumstances%2C%20the%20demand%20of%20quick%0Asearch%20for%20available%20parking%20spaces%20has%20become%20a%20major%20concern%20for%20the%0Aautomobile%20drivers.%20Meanwhile%2C%20the%20public%20sense%20of%20privacy%20is%20also%20awaking%2C%20the%0Aimage-based%20parking%20space%20recognition%20methods%20lack%20the%20attention%20of%20privacy%0Aprotection.%20In%20this%20paper%2C%20we%20proposed%20a%20binary%20convolutional%20neural%20network%0Awith%20lightweight%20design%20structure%20named%20BCFPL%2C%20which%20can%20be%20used%20to%20train%20with%0Alow-resolution%20parking%20space%20images%20and%20offer%20a%20reasonable%20recognition%20result.%0AThe%20images%20of%20parking%20space%20were%20collected%20from%20various%20complex%20environments%2C%0Aincluding%20different%20weather%2C%20occlusion%20conditions%2C%20and%20various%20camera%20angles.%0AWe%20conducted%20the%20training%20and%20testing%20progresses%20among%20different%20datasets%20and%0Apartial%20subsets.%20The%20experimental%20results%20show%20that%20the%20accuracy%20of%20BCFPL%20does%0Anot%20decrease%20compared%20with%20the%20original%20resolution%20image%20directly%2C%20and%20can%0Areach%20the%20average%20level%20of%20the%20existing%20mainstream%20method.%20BCFPL%20also%20has%20low%0Ahardware%20requirements%20and%20fast%20recognition%20speed%20while%20meeting%20the%20privacy%0Arequirements%2C%20so%20it%20has%20application%20potential%20in%20intelligent%20city%20construction%0Aand%20automatic%20driving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14198v1&entry.124074799=Read"},
{"title": "Empowering Molecule Discovery for Molecule-Caption Translation with\n  Large Language Models: A ChatGPT Perspective", "author": "Jiatong Li and Yunqing Liu and Wenqi Fan and Xiao-Yong Wei and Hui Liu and Jiliang Tang and Qing Li", "abstract": "  Molecule discovery plays a crucial role in various scientific fields,\nadvancing the design of tailored materials and drugs. However, most of the\nexisting methods heavily rely on domain experts, require excessive\ncomputational cost, or suffer from sub-optimal performance. On the other hand,\nLarge Language Models (LLMs), like ChatGPT, have shown remarkable performance\nin various cross-modal tasks due to their powerful capabilities in natural\nlanguage understanding, generalization, and in-context learning (ICL), which\nprovides unprecedented opportunities to advance molecule discovery. Despite\nseveral previous works trying to apply LLMs in this task, the lack of\ndomain-specific corpus and difficulties in training specialized LLMs still\nremain challenges. In this work, we propose a novel LLM-based framework\n(MolReGPT) for molecule-caption translation, where an In-Context Few-Shot\nMolecule Learning paradigm is introduced to empower molecule discovery with\nLLMs like ChatGPT to perform their in-context learning capability without\ndomain-specific pre-training and fine-tuning. MolReGPT leverages the principle\nof molecular similarity to retrieve similar molecules and their text\ndescriptions from a local database to enable LLMs to learn the task knowledge\nfrom context examples. We evaluate the effectiveness of MolReGPT on\nmolecule-caption translation, including molecule understanding and text-based\nmolecule generation. Experimental results show that compared to fine-tuned\nmodels, MolReGPT outperforms MolT5-base and is comparable to MolT5-large\nwithout additional training. To the best of our knowledge, MolReGPT is the\nfirst work to leverage LLMs via in-context learning in molecule-caption\ntranslation for advancing molecule discovery. Our work expands the scope of LLM\napplications, as well as providing a new paradigm for molecule discovery and\ndesign.\n", "link": "http://arxiv.org/abs/2306.06615v2", "date": "2024-04-22", "relevancy": 2.0233, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4858}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4848}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Empowering%20Molecule%20Discovery%20for%20Molecule-Caption%20Translation%20with%0A%20%20Large%20Language%20Models%3A%20A%20ChatGPT%20Perspective&body=Title%3A%20Empowering%20Molecule%20Discovery%20for%20Molecule-Caption%20Translation%20with%0A%20%20Large%20Language%20Models%3A%20A%20ChatGPT%20Perspective%0AAuthor%3A%20Jiatong%20Li%20and%20Yunqing%20Liu%20and%20Wenqi%20Fan%20and%20Xiao-Yong%20Wei%20and%20Hui%20Liu%20and%20Jiliang%20Tang%20and%20Qing%20Li%0AAbstract%3A%20%20%20Molecule%20discovery%20plays%20a%20crucial%20role%20in%20various%20scientific%20fields%2C%0Aadvancing%20the%20design%20of%20tailored%20materials%20and%20drugs.%20However%2C%20most%20of%20the%0Aexisting%20methods%20heavily%20rely%20on%20domain%20experts%2C%20require%20excessive%0Acomputational%20cost%2C%20or%20suffer%20from%20sub-optimal%20performance.%20On%20the%20other%20hand%2C%0ALarge%20Language%20Models%20%28LLMs%29%2C%20like%20ChatGPT%2C%20have%20shown%20remarkable%20performance%0Ain%20various%20cross-modal%20tasks%20due%20to%20their%20powerful%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20generalization%2C%20and%20in-context%20learning%20%28ICL%29%2C%20which%0Aprovides%20unprecedented%20opportunities%20to%20advance%20molecule%20discovery.%20Despite%0Aseveral%20previous%20works%20trying%20to%20apply%20LLMs%20in%20this%20task%2C%20the%20lack%20of%0Adomain-specific%20corpus%20and%20difficulties%20in%20training%20specialized%20LLMs%20still%0Aremain%20challenges.%20In%20this%20work%2C%20we%20propose%20a%20novel%20LLM-based%20framework%0A%28MolReGPT%29%20for%20molecule-caption%20translation%2C%20where%20an%20In-Context%20Few-Shot%0AMolecule%20Learning%20paradigm%20is%20introduced%20to%20empower%20molecule%20discovery%20with%0ALLMs%20like%20ChatGPT%20to%20perform%20their%20in-context%20learning%20capability%20without%0Adomain-specific%20pre-training%20and%20fine-tuning.%20MolReGPT%20leverages%20the%20principle%0Aof%20molecular%20similarity%20to%20retrieve%20similar%20molecules%20and%20their%20text%0Adescriptions%20from%20a%20local%20database%20to%20enable%20LLMs%20to%20learn%20the%20task%20knowledge%0Afrom%20context%20examples.%20We%20evaluate%20the%20effectiveness%20of%20MolReGPT%20on%0Amolecule-caption%20translation%2C%20including%20molecule%20understanding%20and%20text-based%0Amolecule%20generation.%20Experimental%20results%20show%20that%20compared%20to%20fine-tuned%0Amodels%2C%20MolReGPT%20outperforms%20MolT5-base%20and%20is%20comparable%20to%20MolT5-large%0Awithout%20additional%20training.%20To%20the%20best%20of%20our%20knowledge%2C%20MolReGPT%20is%20the%0Afirst%20work%20to%20leverage%20LLMs%20via%20in-context%20learning%20in%20molecule-caption%0Atranslation%20for%20advancing%20molecule%20discovery.%20Our%20work%20expands%20the%20scope%20of%20LLM%0Aapplications%2C%20as%20well%20as%20providing%20a%20new%20paradigm%20for%20molecule%20discovery%20and%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06615v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Molecule%20Discovery%20for%20Molecule-Caption%20Translation%20with%0A%20%20Large%20Language%20Models%3A%20A%20ChatGPT%20Perspective&entry.906535625=Jiatong%20Li%20and%20Yunqing%20Liu%20and%20Wenqi%20Fan%20and%20Xiao-Yong%20Wei%20and%20Hui%20Liu%20and%20Jiliang%20Tang%20and%20Qing%20Li&entry.1292438233=%20%20Molecule%20discovery%20plays%20a%20crucial%20role%20in%20various%20scientific%20fields%2C%0Aadvancing%20the%20design%20of%20tailored%20materials%20and%20drugs.%20However%2C%20most%20of%20the%0Aexisting%20methods%20heavily%20rely%20on%20domain%20experts%2C%20require%20excessive%0Acomputational%20cost%2C%20or%20suffer%20from%20sub-optimal%20performance.%20On%20the%20other%20hand%2C%0ALarge%20Language%20Models%20%28LLMs%29%2C%20like%20ChatGPT%2C%20have%20shown%20remarkable%20performance%0Ain%20various%20cross-modal%20tasks%20due%20to%20their%20powerful%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20generalization%2C%20and%20in-context%20learning%20%28ICL%29%2C%20which%0Aprovides%20unprecedented%20opportunities%20to%20advance%20molecule%20discovery.%20Despite%0Aseveral%20previous%20works%20trying%20to%20apply%20LLMs%20in%20this%20task%2C%20the%20lack%20of%0Adomain-specific%20corpus%20and%20difficulties%20in%20training%20specialized%20LLMs%20still%0Aremain%20challenges.%20In%20this%20work%2C%20we%20propose%20a%20novel%20LLM-based%20framework%0A%28MolReGPT%29%20for%20molecule-caption%20translation%2C%20where%20an%20In-Context%20Few-Shot%0AMolecule%20Learning%20paradigm%20is%20introduced%20to%20empower%20molecule%20discovery%20with%0ALLMs%20like%20ChatGPT%20to%20perform%20their%20in-context%20learning%20capability%20without%0Adomain-specific%20pre-training%20and%20fine-tuning.%20MolReGPT%20leverages%20the%20principle%0Aof%20molecular%20similarity%20to%20retrieve%20similar%20molecules%20and%20their%20text%0Adescriptions%20from%20a%20local%20database%20to%20enable%20LLMs%20to%20learn%20the%20task%20knowledge%0Afrom%20context%20examples.%20We%20evaluate%20the%20effectiveness%20of%20MolReGPT%20on%0Amolecule-caption%20translation%2C%20including%20molecule%20understanding%20and%20text-based%0Amolecule%20generation.%20Experimental%20results%20show%20that%20compared%20to%20fine-tuned%0Amodels%2C%20MolReGPT%20outperforms%20MolT5-base%20and%20is%20comparable%20to%20MolT5-large%0Awithout%20additional%20training.%20To%20the%20best%20of%20our%20knowledge%2C%20MolReGPT%20is%20the%0Afirst%20work%20to%20leverage%20LLMs%20via%20in-context%20learning%20in%20molecule-caption%0Atranslation%20for%20advancing%20molecule%20discovery.%20Our%20work%20expands%20the%20scope%20of%20LLM%0Aapplications%2C%20as%20well%20as%20providing%20a%20new%20paradigm%20for%20molecule%20discovery%20and%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06615v2&entry.124074799=Read"},
{"title": "Hyp-OC: Hyperbolic One Class Classification for Face Anti-Spoofing", "author": "Kartik Narayan and Vishal M. Patel", "abstract": "  Face recognition technology has become an integral part of modern security\nsystems and user authentication processes. However, these systems are\nvulnerable to spoofing attacks and can easily be circumvented. Most prior\nresearch in face anti-spoofing (FAS) approaches it as a two-class\nclassification task where models are trained on real samples and known spoof\nattacks and tested for detection performance on unknown spoof attacks. However,\nin practice, FAS should be treated as a one-class classification task where,\nwhile training, one cannot assume any knowledge regarding the spoof samples a\npriori. In this paper, we reformulate the face anti-spoofing task from a\none-class perspective and propose a novel hyperbolic one-class classification\nframework. To train our network, we use a pseudo-negative class sampled from\nthe Gaussian distribution with a weighted running mean and propose two novel\nloss functions: (1) Hyp-PC: Hyperbolic Pairwise Confusion loss, and (2) Hyp-CE:\nHyperbolic Cross Entropy loss, which operate in the hyperbolic space.\nAdditionally, we employ Euclidean feature clipping and gradient clipping to\nstabilize the training in the hyperbolic space. To the best of our knowledge,\nthis is the first work extending hyperbolic embeddings for face anti-spoofing\nin a one-class manner. With extensive experiments on five benchmark datasets:\nRose-Youtu, MSU-MFSD, CASIA-MFSD, Idiap Replay-Attack, and OULU-NPU, we\ndemonstrate that our method significantly outperforms the state-of-the-art,\nachieving better spoof detection performance.\n", "link": "http://arxiv.org/abs/2404.14406v1", "date": "2024-04-22", "relevancy": 2.0178, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5218}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5008}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4702}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyp-OC%3A%20Hyperbolic%20One%20Class%20Classification%20for%20Face%20Anti-Spoofing&body=Title%3A%20Hyp-OC%3A%20Hyperbolic%20One%20Class%20Classification%20for%20Face%20Anti-Spoofing%0AAuthor%3A%20Kartik%20Narayan%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Face%20recognition%20technology%20has%20become%20an%20integral%20part%20of%20modern%20security%0Asystems%20and%20user%20authentication%20processes.%20However%2C%20these%20systems%20are%0Avulnerable%20to%20spoofing%20attacks%20and%20can%20easily%20be%20circumvented.%20Most%20prior%0Aresearch%20in%20face%20anti-spoofing%20%28FAS%29%20approaches%20it%20as%20a%20two-class%0Aclassification%20task%20where%20models%20are%20trained%20on%20real%20samples%20and%20known%20spoof%0Aattacks%20and%20tested%20for%20detection%20performance%20on%20unknown%20spoof%20attacks.%20However%2C%0Ain%20practice%2C%20FAS%20should%20be%20treated%20as%20a%20one-class%20classification%20task%20where%2C%0Awhile%20training%2C%20one%20cannot%20assume%20any%20knowledge%20regarding%20the%20spoof%20samples%20a%0Apriori.%20In%20this%20paper%2C%20we%20reformulate%20the%20face%20anti-spoofing%20task%20from%20a%0Aone-class%20perspective%20and%20propose%20a%20novel%20hyperbolic%20one-class%20classification%0Aframework.%20To%20train%20our%20network%2C%20we%20use%20a%20pseudo-negative%20class%20sampled%20from%0Athe%20Gaussian%20distribution%20with%20a%20weighted%20running%20mean%20and%20propose%20two%20novel%0Aloss%20functions%3A%20%281%29%20Hyp-PC%3A%20Hyperbolic%20Pairwise%20Confusion%20loss%2C%20and%20%282%29%20Hyp-CE%3A%0AHyperbolic%20Cross%20Entropy%20loss%2C%20which%20operate%20in%20the%20hyperbolic%20space.%0AAdditionally%2C%20we%20employ%20Euclidean%20feature%20clipping%20and%20gradient%20clipping%20to%0Astabilize%20the%20training%20in%20the%20hyperbolic%20space.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20work%20extending%20hyperbolic%20embeddings%20for%20face%20anti-spoofing%0Ain%20a%20one-class%20manner.%20With%20extensive%20experiments%20on%20five%20benchmark%20datasets%3A%0ARose-Youtu%2C%20MSU-MFSD%2C%20CASIA-MFSD%2C%20Idiap%20Replay-Attack%2C%20and%20OULU-NPU%2C%20we%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%2C%0Aachieving%20better%20spoof%20detection%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14406v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyp-OC%3A%20Hyperbolic%20One%20Class%20Classification%20for%20Face%20Anti-Spoofing&entry.906535625=Kartik%20Narayan%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Face%20recognition%20technology%20has%20become%20an%20integral%20part%20of%20modern%20security%0Asystems%20and%20user%20authentication%20processes.%20However%2C%20these%20systems%20are%0Avulnerable%20to%20spoofing%20attacks%20and%20can%20easily%20be%20circumvented.%20Most%20prior%0Aresearch%20in%20face%20anti-spoofing%20%28FAS%29%20approaches%20it%20as%20a%20two-class%0Aclassification%20task%20where%20models%20are%20trained%20on%20real%20samples%20and%20known%20spoof%0Aattacks%20and%20tested%20for%20detection%20performance%20on%20unknown%20spoof%20attacks.%20However%2C%0Ain%20practice%2C%20FAS%20should%20be%20treated%20as%20a%20one-class%20classification%20task%20where%2C%0Awhile%20training%2C%20one%20cannot%20assume%20any%20knowledge%20regarding%20the%20spoof%20samples%20a%0Apriori.%20In%20this%20paper%2C%20we%20reformulate%20the%20face%20anti-spoofing%20task%20from%20a%0Aone-class%20perspective%20and%20propose%20a%20novel%20hyperbolic%20one-class%20classification%0Aframework.%20To%20train%20our%20network%2C%20we%20use%20a%20pseudo-negative%20class%20sampled%20from%0Athe%20Gaussian%20distribution%20with%20a%20weighted%20running%20mean%20and%20propose%20two%20novel%0Aloss%20functions%3A%20%281%29%20Hyp-PC%3A%20Hyperbolic%20Pairwise%20Confusion%20loss%2C%20and%20%282%29%20Hyp-CE%3A%0AHyperbolic%20Cross%20Entropy%20loss%2C%20which%20operate%20in%20the%20hyperbolic%20space.%0AAdditionally%2C%20we%20employ%20Euclidean%20feature%20clipping%20and%20gradient%20clipping%20to%0Astabilize%20the%20training%20in%20the%20hyperbolic%20space.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20work%20extending%20hyperbolic%20embeddings%20for%20face%20anti-spoofing%0Ain%20a%20one-class%20manner.%20With%20extensive%20experiments%20on%20five%20benchmark%20datasets%3A%0ARose-Youtu%2C%20MSU-MFSD%2C%20CASIA-MFSD%2C%20Idiap%20Replay-Attack%2C%20and%20OULU-NPU%2C%20we%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%2C%0Aachieving%20better%20spoof%20detection%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14406v1&entry.124074799=Read"},
{"title": "A Novel Approach to Chest X-ray Lung Segmentation Using U-net and\n  Modified Convolutional Block Attention Module", "author": "Mohammad Ali Labbaf Khaniki and Mohammad Manthouri", "abstract": "  Lung segmentation in chest X-ray images is of paramount importance as it\nplays a crucial role in the diagnosis and treatment of various lung diseases.\nThis paper presents a novel approach for lung segmentation in chest X-ray\nimages by integrating U-net with attention mechanisms. The proposed method\nenhances the U-net architecture by incorporating a Convolutional Block\nAttention Module (CBAM), which unifies three distinct attention mechanisms:\nchannel attention, spatial attention, and pixel attention. The channel\nattention mechanism enables the model to concentrate on the most informative\nfeatures across various channels. The spatial attention mechanism enhances the\nmodel's precision in localization by focusing on significant spatial locations.\nLastly, the pixel attention mechanism empowers the model to focus on individual\npixels, further refining the model's focus and thereby improving the accuracy\nof segmentation. The adoption of the proposed CBAM in conjunction with the\nU-net architecture marks a significant advancement in the field of medical\nimaging, with potential implications for improving diagnostic precision and\npatient outcomes. The efficacy of this method is validated against contemporary\nstate-of-the-art techniques, showcasing its superiority in segmentation\nperformance.\n", "link": "http://arxiv.org/abs/2404.14322v1", "date": "2024-04-22", "relevancy": 2.0109, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4941}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module&body=Title%3A%20A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module%0AAuthor%3A%20Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Mohammad%20Manthouri%0AAbstract%3A%20%20%20Lung%20segmentation%20in%20chest%20X-ray%20images%20is%20of%20paramount%20importance%20as%20it%0Aplays%20a%20crucial%20role%20in%20the%20diagnosis%20and%20treatment%20of%20various%20lung%20diseases.%0AThis%20paper%20presents%20a%20novel%20approach%20for%20lung%20segmentation%20in%20chest%20X-ray%0Aimages%20by%20integrating%20U-net%20with%20attention%20mechanisms.%20The%20proposed%20method%0Aenhances%20the%20U-net%20architecture%20by%20incorporating%20a%20Convolutional%20Block%0AAttention%20Module%20%28CBAM%29%2C%20which%20unifies%20three%20distinct%20attention%20mechanisms%3A%0Achannel%20attention%2C%20spatial%20attention%2C%20and%20pixel%20attention.%20The%20channel%0Aattention%20mechanism%20enables%20the%20model%20to%20concentrate%20on%20the%20most%20informative%0Afeatures%20across%20various%20channels.%20The%20spatial%20attention%20mechanism%20enhances%20the%0Amodel%27s%20precision%20in%20localization%20by%20focusing%20on%20significant%20spatial%20locations.%0ALastly%2C%20the%20pixel%20attention%20mechanism%20empowers%20the%20model%20to%20focus%20on%20individual%0Apixels%2C%20further%20refining%20the%20model%27s%20focus%20and%20thereby%20improving%20the%20accuracy%0Aof%20segmentation.%20The%20adoption%20of%20the%20proposed%20CBAM%20in%20conjunction%20with%20the%0AU-net%20architecture%20marks%20a%20significant%20advancement%20in%20the%20field%20of%20medical%0Aimaging%2C%20with%20potential%20implications%20for%20improving%20diagnostic%20precision%20and%0Apatient%20outcomes.%20The%20efficacy%20of%20this%20method%20is%20validated%20against%20contemporary%0Astate-of-the-art%20techniques%2C%20showcasing%20its%20superiority%20in%20segmentation%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14322v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20Chest%20X-ray%20Lung%20Segmentation%20Using%20U-net%20and%0A%20%20Modified%20Convolutional%20Block%20Attention%20Module&entry.906535625=Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Mohammad%20Manthouri&entry.1292438233=%20%20Lung%20segmentation%20in%20chest%20X-ray%20images%20is%20of%20paramount%20importance%20as%20it%0Aplays%20a%20crucial%20role%20in%20the%20diagnosis%20and%20treatment%20of%20various%20lung%20diseases.%0AThis%20paper%20presents%20a%20novel%20approach%20for%20lung%20segmentation%20in%20chest%20X-ray%0Aimages%20by%20integrating%20U-net%20with%20attention%20mechanisms.%20The%20proposed%20method%0Aenhances%20the%20U-net%20architecture%20by%20incorporating%20a%20Convolutional%20Block%0AAttention%20Module%20%28CBAM%29%2C%20which%20unifies%20three%20distinct%20attention%20mechanisms%3A%0Achannel%20attention%2C%20spatial%20attention%2C%20and%20pixel%20attention.%20The%20channel%0Aattention%20mechanism%20enables%20the%20model%20to%20concentrate%20on%20the%20most%20informative%0Afeatures%20across%20various%20channels.%20The%20spatial%20attention%20mechanism%20enhances%20the%0Amodel%27s%20precision%20in%20localization%20by%20focusing%20on%20significant%20spatial%20locations.%0ALastly%2C%20the%20pixel%20attention%20mechanism%20empowers%20the%20model%20to%20focus%20on%20individual%0Apixels%2C%20further%20refining%20the%20model%27s%20focus%20and%20thereby%20improving%20the%20accuracy%0Aof%20segmentation.%20The%20adoption%20of%20the%20proposed%20CBAM%20in%20conjunction%20with%20the%0AU-net%20architecture%20marks%20a%20significant%20advancement%20in%20the%20field%20of%20medical%0Aimaging%2C%20with%20potential%20implications%20for%20improving%20diagnostic%20precision%20and%0Apatient%20outcomes.%20The%20efficacy%20of%20this%20method%20is%20validated%20against%20contemporary%0Astate-of-the-art%20techniques%2C%20showcasing%20its%20superiority%20in%20segmentation%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14322v1&entry.124074799=Read"},
{"title": "STROOBnet Optimization via GPU-Accelerated Proximal Recurrence\n  Strategies", "author": "Ted Edward Holmberg and Mahdi Abdelguerfi and Elias Ioup", "abstract": "  Spatiotemporal networks' observational capabilities are crucial for accurate\ndata gathering and informed decisions across multiple sectors. This study\nfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network\n(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events\nwithin defined geographical regions, enabling efficient monitoring. Using data\nfrom Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New\nOrleans, where RTCC combats rising crime amidst reduced police presence, we\naddress the network's initial observational imbalances. Aiming for uniform\nobservational efficacy, we propose the Proximal Recurrence approach. It\noutperformed traditional clustering methods like k-means and DBSCAN by offering\nholistic event frequency and spatial consideration, enhancing observational\ncoverage.\n", "link": "http://arxiv.org/abs/2404.14388v1", "date": "2024-04-22", "relevancy": 1.9979, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4943}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies&body=Title%3A%20STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies%0AAuthor%3A%20Ted%20Edward%20Holmberg%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup%0AAbstract%3A%20%20%20Spatiotemporal%20networks%27%20observational%20capabilities%20are%20crucial%20for%20accurate%0Adata%20gathering%20and%20informed%20decisions%20across%20multiple%20sectors.%20This%20study%0Afocuses%20on%20the%20Spatiotemporal%20Ranged%20Observer-Observable%20Bipartite%20Network%0A%28STROOBnet%29%2C%20linking%20observational%20nodes%20%28e.g.%2C%20surveillance%20cameras%29%20to%20events%0Awithin%20defined%20geographical%20regions%2C%20enabling%20efficient%20monitoring.%20Using%20data%0Afrom%20Real-Time%20Crime%20Camera%20%28RTCC%29%20systems%20and%20Calls%20for%20Service%20%28CFS%29%20in%20New%0AOrleans%2C%20where%20RTCC%20combats%20rising%20crime%20amidst%20reduced%20police%20presence%2C%20we%0Aaddress%20the%20network%27s%20initial%20observational%20imbalances.%20Aiming%20for%20uniform%0Aobservational%20efficacy%2C%20we%20propose%20the%20Proximal%20Recurrence%20approach.%20It%0Aoutperformed%20traditional%20clustering%20methods%20like%20k-means%20and%20DBSCAN%20by%20offering%0Aholistic%20event%20frequency%20and%20spatial%20consideration%2C%20enhancing%20observational%0Acoverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14388v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies&entry.906535625=Ted%20Edward%20Holmberg%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup&entry.1292438233=%20%20Spatiotemporal%20networks%27%20observational%20capabilities%20are%20crucial%20for%20accurate%0Adata%20gathering%20and%20informed%20decisions%20across%20multiple%20sectors.%20This%20study%0Afocuses%20on%20the%20Spatiotemporal%20Ranged%20Observer-Observable%20Bipartite%20Network%0A%28STROOBnet%29%2C%20linking%20observational%20nodes%20%28e.g.%2C%20surveillance%20cameras%29%20to%20events%0Awithin%20defined%20geographical%20regions%2C%20enabling%20efficient%20monitoring.%20Using%20data%0Afrom%20Real-Time%20Crime%20Camera%20%28RTCC%29%20systems%20and%20Calls%20for%20Service%20%28CFS%29%20in%20New%0AOrleans%2C%20where%20RTCC%20combats%20rising%20crime%20amidst%20reduced%20police%20presence%2C%20we%0Aaddress%20the%20network%27s%20initial%20observational%20imbalances.%20Aiming%20for%20uniform%0Aobservational%20efficacy%2C%20we%20propose%20the%20Proximal%20Recurrence%20approach.%20It%0Aoutperformed%20traditional%20clustering%20methods%20like%20k-means%20and%20DBSCAN%20by%20offering%0Aholistic%20event%20frequency%20and%20spatial%20consideration%2C%20enhancing%20observational%0Acoverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14388v1&entry.124074799=Read"},
{"title": "Fast and Robust Normal Estimation for Sparse LiDAR Scans", "author": "Igor Bogoslavskyi and Konstantinos Zampogiannis and Raymond Phan", "abstract": "  Light Detection and Ranging (LiDAR) technology has proven to be an important\npart of many robotics systems. Surface normals estimated from LiDAR data are\ncommonly used for a variety of tasks in such systems. As most of the today's\nmechanical LiDAR sensors produce sparse data, estimating normals from a single\nscan in a robust manner poses difficulties.\n  In this paper, we address the problem of estimating normals for sparse LiDAR\ndata avoiding the typical issues of smoothing out the normals in high curvature\nareas.\n  Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of such\na set of lasers produces an array of points where each point's neighbor is\nknown due to the known firing pattern of the scanner. We use this knowledge to\nconnect these points to their neighbors and label them using the angles of the\nlines connecting them. When estimating normals at these points, we only\nconsider points with the same label as neighbors. This allows us to avoid\nestimating normals in high curvature areas.\n  We evaluate our approach on various data, both self-recorded and publicly\navailable, acquired using various sparse LiDAR sensors. We show that using our\nmethod for normal estimation leads to normals that are more robust in areas\nwith high curvature which leads to maps of higher quality. We also show that\nour method only incurs a constant factor runtime overhead with respect to a\nlightweight baseline normal estimation procedure and is therefore suited for\noperation in computationally demanding environments.\n", "link": "http://arxiv.org/abs/2404.14281v1", "date": "2024-04-22", "relevancy": 1.9965, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5282}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Robust%20Normal%20Estimation%20for%20Sparse%20LiDAR%20Scans&body=Title%3A%20Fast%20and%20Robust%20Normal%20Estimation%20for%20Sparse%20LiDAR%20Scans%0AAuthor%3A%20Igor%20Bogoslavskyi%20and%20Konstantinos%20Zampogiannis%20and%20Raymond%20Phan%0AAbstract%3A%20%20%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20technology%20has%20proven%20to%20be%20an%20important%0Apart%20of%20many%20robotics%20systems.%20Surface%20normals%20estimated%20from%20LiDAR%20data%20are%0Acommonly%20used%20for%20a%20variety%20of%20tasks%20in%20such%20systems.%20As%20most%20of%20the%20today%27s%0Amechanical%20LiDAR%20sensors%20produce%20sparse%20data%2C%20estimating%20normals%20from%20a%20single%0Ascan%20in%20a%20robust%20manner%20poses%20difficulties.%0A%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20estimating%20normals%20for%20sparse%20LiDAR%0Adata%20avoiding%20the%20typical%20issues%20of%20smoothing%20out%20the%20normals%20in%20high%20curvature%0Aareas.%0A%20%20Mechanical%20LiDARs%20rotate%20a%20set%20of%20rigidly%20mounted%20lasers.%20One%20firing%20of%20such%0Aa%20set%20of%20lasers%20produces%20an%20array%20of%20points%20where%20each%20point%27s%20neighbor%20is%0Aknown%20due%20to%20the%20known%20firing%20pattern%20of%20the%20scanner.%20We%20use%20this%20knowledge%20to%0Aconnect%20these%20points%20to%20their%20neighbors%20and%20label%20them%20using%20the%20angles%20of%20the%0Alines%20connecting%20them.%20When%20estimating%20normals%20at%20these%20points%2C%20we%20only%0Aconsider%20points%20with%20the%20same%20label%20as%20neighbors.%20This%20allows%20us%20to%20avoid%0Aestimating%20normals%20in%20high%20curvature%20areas.%0A%20%20We%20evaluate%20our%20approach%20on%20various%20data%2C%20both%20self-recorded%20and%20publicly%0Aavailable%2C%20acquired%20using%20various%20sparse%20LiDAR%20sensors.%20We%20show%20that%20using%20our%0Amethod%20for%20normal%20estimation%20leads%20to%20normals%20that%20are%20more%20robust%20in%20areas%0Awith%20high%20curvature%20which%20leads%20to%20maps%20of%20higher%20quality.%20We%20also%20show%20that%0Aour%20method%20only%20incurs%20a%20constant%20factor%20runtime%20overhead%20with%20respect%20to%20a%0Alightweight%20baseline%20normal%20estimation%20procedure%20and%20is%20therefore%20suited%20for%0Aoperation%20in%20computationally%20demanding%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14281v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Robust%20Normal%20Estimation%20for%20Sparse%20LiDAR%20Scans&entry.906535625=Igor%20Bogoslavskyi%20and%20Konstantinos%20Zampogiannis%20and%20Raymond%20Phan&entry.1292438233=%20%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20technology%20has%20proven%20to%20be%20an%20important%0Apart%20of%20many%20robotics%20systems.%20Surface%20normals%20estimated%20from%20LiDAR%20data%20are%0Acommonly%20used%20for%20a%20variety%20of%20tasks%20in%20such%20systems.%20As%20most%20of%20the%20today%27s%0Amechanical%20LiDAR%20sensors%20produce%20sparse%20data%2C%20estimating%20normals%20from%20a%20single%0Ascan%20in%20a%20robust%20manner%20poses%20difficulties.%0A%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20estimating%20normals%20for%20sparse%20LiDAR%0Adata%20avoiding%20the%20typical%20issues%20of%20smoothing%20out%20the%20normals%20in%20high%20curvature%0Aareas.%0A%20%20Mechanical%20LiDARs%20rotate%20a%20set%20of%20rigidly%20mounted%20lasers.%20One%20firing%20of%20such%0Aa%20set%20of%20lasers%20produces%20an%20array%20of%20points%20where%20each%20point%27s%20neighbor%20is%0Aknown%20due%20to%20the%20known%20firing%20pattern%20of%20the%20scanner.%20We%20use%20this%20knowledge%20to%0Aconnect%20these%20points%20to%20their%20neighbors%20and%20label%20them%20using%20the%20angles%20of%20the%0Alines%20connecting%20them.%20When%20estimating%20normals%20at%20these%20points%2C%20we%20only%0Aconsider%20points%20with%20the%20same%20label%20as%20neighbors.%20This%20allows%20us%20to%20avoid%0Aestimating%20normals%20in%20high%20curvature%20areas.%0A%20%20We%20evaluate%20our%20approach%20on%20various%20data%2C%20both%20self-recorded%20and%20publicly%0Aavailable%2C%20acquired%20using%20various%20sparse%20LiDAR%20sensors.%20We%20show%20that%20using%20our%0Amethod%20for%20normal%20estimation%20leads%20to%20normals%20that%20are%20more%20robust%20in%20areas%0Awith%20high%20curvature%20which%20leads%20to%20maps%20of%20higher%20quality.%20We%20also%20show%20that%0Aour%20method%20only%20incurs%20a%20constant%20factor%20runtime%20overhead%20with%20respect%20to%20a%0Alightweight%20baseline%20normal%20estimation%20procedure%20and%20is%20therefore%20suited%20for%0Aoperation%20in%20computationally%20demanding%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14281v1&entry.124074799=Read"},
{"title": "X-Ray: A Sequential 3D Representation for Generation", "author": "Tao Hu and Wenhang Ge and Yuyang Zhao and Gim Hee Lee", "abstract": "  In this paper, we introduce X-Ray, an innovative approach to 3D generation\nthat employs a new sequential representation, drawing inspiration from the\ndepth-revealing capabilities of X-Ray scans to meticulously capture both the\nexternal and internal features of objects. Central to our method is the\nutilization of ray casting techniques originating from the camera's viewpoint,\nmeticulously recording the geometric and textural details encountered across\nall intersected surfaces. This process efficiently condenses complete objects\nor scenes into a multi-frame format, just like videos. Such a structure ensures\nthe 3D representation is composed solely of critical surface information.\nHighlighting the practicality and adaptability of our X-Ray representation, we\nshowcase its utility in synthesizing 3D objects, employing a network\narchitecture akin to that used in video diffusion models. The outcomes reveal\nour representation's superior performance in enhancing both the accuracy and\nefficiency of 3D synthesis, heralding new directions for ongoing research and\npractical implementations in the field.\n", "link": "http://arxiv.org/abs/2404.14329v1", "date": "2024-04-22", "relevancy": 1.9903, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5027}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4937}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20X-Ray%3A%20A%20Sequential%203D%20Representation%20for%20Generation&body=Title%3A%20X-Ray%3A%20A%20Sequential%203D%20Representation%20for%20Generation%0AAuthor%3A%20Tao%20Hu%20and%20Wenhang%20Ge%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20X-Ray%2C%20an%20innovative%20approach%20to%203D%20generation%0Athat%20employs%20a%20new%20sequential%20representation%2C%20drawing%20inspiration%20from%20the%0Adepth-revealing%20capabilities%20of%20X-Ray%20scans%20to%20meticulously%20capture%20both%20the%0Aexternal%20and%20internal%20features%20of%20objects.%20Central%20to%20our%20method%20is%20the%0Autilization%20of%20ray%20casting%20techniques%20originating%20from%20the%20camera%27s%20viewpoint%2C%0Ameticulously%20recording%20the%20geometric%20and%20textural%20details%20encountered%20across%0Aall%20intersected%20surfaces.%20This%20process%20efficiently%20condenses%20complete%20objects%0Aor%20scenes%20into%20a%20multi-frame%20format%2C%20just%20like%20videos.%20Such%20a%20structure%20ensures%0Athe%203D%20representation%20is%20composed%20solely%20of%20critical%20surface%20information.%0AHighlighting%20the%20practicality%20and%20adaptability%20of%20our%20X-Ray%20representation%2C%20we%0Ashowcase%20its%20utility%20in%20synthesizing%203D%20objects%2C%20employing%20a%20network%0Aarchitecture%20akin%20to%20that%20used%20in%20video%20diffusion%20models.%20The%20outcomes%20reveal%0Aour%20representation%27s%20superior%20performance%20in%20enhancing%20both%20the%20accuracy%20and%0Aefficiency%20of%203D%20synthesis%2C%20heralding%20new%20directions%20for%20ongoing%20research%20and%0Apractical%20implementations%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14329v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Ray%3A%20A%20Sequential%203D%20Representation%20for%20Generation&entry.906535625=Tao%20Hu%20and%20Wenhang%20Ge%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20X-Ray%2C%20an%20innovative%20approach%20to%203D%20generation%0Athat%20employs%20a%20new%20sequential%20representation%2C%20drawing%20inspiration%20from%20the%0Adepth-revealing%20capabilities%20of%20X-Ray%20scans%20to%20meticulously%20capture%20both%20the%0Aexternal%20and%20internal%20features%20of%20objects.%20Central%20to%20our%20method%20is%20the%0Autilization%20of%20ray%20casting%20techniques%20originating%20from%20the%20camera%27s%20viewpoint%2C%0Ameticulously%20recording%20the%20geometric%20and%20textural%20details%20encountered%20across%0Aall%20intersected%20surfaces.%20This%20process%20efficiently%20condenses%20complete%20objects%0Aor%20scenes%20into%20a%20multi-frame%20format%2C%20just%20like%20videos.%20Such%20a%20structure%20ensures%0Athe%203D%20representation%20is%20composed%20solely%20of%20critical%20surface%20information.%0AHighlighting%20the%20practicality%20and%20adaptability%20of%20our%20X-Ray%20representation%2C%20we%0Ashowcase%20its%20utility%20in%20synthesizing%203D%20objects%2C%20employing%20a%20network%0Aarchitecture%20akin%20to%20that%20used%20in%20video%20diffusion%20models.%20The%20outcomes%20reveal%0Aour%20representation%27s%20superior%20performance%20in%20enhancing%20both%20the%20accuracy%20and%0Aefficiency%20of%203D%20synthesis%2C%20heralding%20new%20directions%20for%20ongoing%20research%20and%0Apractical%20implementations%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14329v1&entry.124074799=Read"},
{"title": "SHE-Net: Syntax-Hierarchy-Enhanced Text-Video Retrieval", "author": "Xuzheng Yu and Chen Jiang and Xingning Dong and Tian Gan and Ming Yang and Qingpei Guo", "abstract": "  The user base of short video apps has experienced unprecedented growth in\nrecent years, resulting in a significant demand for video content analysis. In\nparticular, text-video retrieval, which aims to find the top matching videos\ngiven text descriptions from a vast video corpus, is an essential function, the\nprimary challenge of which is to bridge the modality gap. Nevertheless, most\nexisting approaches treat texts merely as discrete tokens and neglect their\nsyntax structures. Moreover, the abundant spatial and temporal clues in videos\nare often underutilized due to the lack of interaction with text. To address\nthese issues, we argue that using texts as guidance to focus on relevant\ntemporal frames and spatial regions within videos is beneficial. In this paper,\nwe propose a novel Syntax-Hierarchy-Enhanced text-video retrieval method\n(SHE-Net) that exploits the inherent semantic and syntax hierarchy of texts to\nbridge the modality gap from two perspectives. First, to facilitate a more\nfine-grained integration of visual content, we employ the text syntax\nhierarchy, which reveals the grammatical structure of text descriptions, to\nguide the visual representations. Second, to further enhance the multi-modal\ninteraction and alignment, we also utilize the syntax hierarchy to guide the\nsimilarity calculation. We evaluated our method on four public text-video\nretrieval datasets of MSR-VTT, MSVD, DiDeMo, and ActivityNet. The experimental\nresults and ablation studies confirm the advantages of our proposed method.\n", "link": "http://arxiv.org/abs/2404.14066v1", "date": "2024-04-22", "relevancy": 1.9888, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5172}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5067}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval&body=Title%3A%20SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval%0AAuthor%3A%20Xuzheng%20Yu%20and%20Chen%20Jiang%20and%20Xingning%20Dong%20and%20Tian%20Gan%20and%20Ming%20Yang%20and%20Qingpei%20Guo%0AAbstract%3A%20%20%20The%20user%20base%20of%20short%20video%20apps%20has%20experienced%20unprecedented%20growth%20in%0Arecent%20years%2C%20resulting%20in%20a%20significant%20demand%20for%20video%20content%20analysis.%20In%0Aparticular%2C%20text-video%20retrieval%2C%20which%20aims%20to%20find%20the%20top%20matching%20videos%0Agiven%20text%20descriptions%20from%20a%20vast%20video%20corpus%2C%20is%20an%20essential%20function%2C%20the%0Aprimary%20challenge%20of%20which%20is%20to%20bridge%20the%20modality%20gap.%20Nevertheless%2C%20most%0Aexisting%20approaches%20treat%20texts%20merely%20as%20discrete%20tokens%20and%20neglect%20their%0Asyntax%20structures.%20Moreover%2C%20the%20abundant%20spatial%20and%20temporal%20clues%20in%20videos%0Aare%20often%20underutilized%20due%20to%20the%20lack%20of%20interaction%20with%20text.%20To%20address%0Athese%20issues%2C%20we%20argue%20that%20using%20texts%20as%20guidance%20to%20focus%20on%20relevant%0Atemporal%20frames%20and%20spatial%20regions%20within%20videos%20is%20beneficial.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Syntax-Hierarchy-Enhanced%20text-video%20retrieval%20method%0A%28SHE-Net%29%20that%20exploits%20the%20inherent%20semantic%20and%20syntax%20hierarchy%20of%20texts%20to%0Abridge%20the%20modality%20gap%20from%20two%20perspectives.%20First%2C%20to%20facilitate%20a%20more%0Afine-grained%20integration%20of%20visual%20content%2C%20we%20employ%20the%20text%20syntax%0Ahierarchy%2C%20which%20reveals%20the%20grammatical%20structure%20of%20text%20descriptions%2C%20to%0Aguide%20the%20visual%20representations.%20Second%2C%20to%20further%20enhance%20the%20multi-modal%0Ainteraction%20and%20alignment%2C%20we%20also%20utilize%20the%20syntax%20hierarchy%20to%20guide%20the%0Asimilarity%20calculation.%20We%20evaluated%20our%20method%20on%20four%20public%20text-video%0Aretrieval%20datasets%20of%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20and%20ActivityNet.%20The%20experimental%0Aresults%20and%20ablation%20studies%20confirm%20the%20advantages%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14066v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval&entry.906535625=Xuzheng%20Yu%20and%20Chen%20Jiang%20and%20Xingning%20Dong%20and%20Tian%20Gan%20and%20Ming%20Yang%20and%20Qingpei%20Guo&entry.1292438233=%20%20The%20user%20base%20of%20short%20video%20apps%20has%20experienced%20unprecedented%20growth%20in%0Arecent%20years%2C%20resulting%20in%20a%20significant%20demand%20for%20video%20content%20analysis.%20In%0Aparticular%2C%20text-video%20retrieval%2C%20which%20aims%20to%20find%20the%20top%20matching%20videos%0Agiven%20text%20descriptions%20from%20a%20vast%20video%20corpus%2C%20is%20an%20essential%20function%2C%20the%0Aprimary%20challenge%20of%20which%20is%20to%20bridge%20the%20modality%20gap.%20Nevertheless%2C%20most%0Aexisting%20approaches%20treat%20texts%20merely%20as%20discrete%20tokens%20and%20neglect%20their%0Asyntax%20structures.%20Moreover%2C%20the%20abundant%20spatial%20and%20temporal%20clues%20in%20videos%0Aare%20often%20underutilized%20due%20to%20the%20lack%20of%20interaction%20with%20text.%20To%20address%0Athese%20issues%2C%20we%20argue%20that%20using%20texts%20as%20guidance%20to%20focus%20on%20relevant%0Atemporal%20frames%20and%20spatial%20regions%20within%20videos%20is%20beneficial.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Syntax-Hierarchy-Enhanced%20text-video%20retrieval%20method%0A%28SHE-Net%29%20that%20exploits%20the%20inherent%20semantic%20and%20syntax%20hierarchy%20of%20texts%20to%0Abridge%20the%20modality%20gap%20from%20two%20perspectives.%20First%2C%20to%20facilitate%20a%20more%0Afine-grained%20integration%20of%20visual%20content%2C%20we%20employ%20the%20text%20syntax%0Ahierarchy%2C%20which%20reveals%20the%20grammatical%20structure%20of%20text%20descriptions%2C%20to%0Aguide%20the%20visual%20representations.%20Second%2C%20to%20further%20enhance%20the%20multi-modal%0Ainteraction%20and%20alignment%2C%20we%20also%20utilize%20the%20syntax%20hierarchy%20to%20guide%20the%0Asimilarity%20calculation.%20We%20evaluated%20our%20method%20on%20four%20public%20text-video%0Aretrieval%20datasets%20of%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20and%20ActivityNet.%20The%20experimental%0Aresults%20and%20ablation%20studies%20confirm%20the%20advantages%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14066v1&entry.124074799=Read"},
{"title": "Deep Learning as Ricci Flow", "author": "Anthony Baptista and Alessandro Barp and Tapabrata Chakraborti and Chris Harbron and Ben D. MacArthur and Christopher R. S. Banerji", "abstract": "  Deep neural networks (DNNs) are powerful tools for approximating the\ndistribution of complex data. It is known that data passing through a trained\nDNN classifier undergoes a series of geometric and topological simplifications.\nWhile some progress has been made toward understanding these transformations in\nneural networks with smooth activation functions, an understanding in the more\ngeneral setting of non-smooth activation functions, such as the rectified\nlinear unit (ReLU), which tend to perform better, is required. Here we propose\nthat the geometric transformations performed by DNNs during classification\ntasks have parallels to those expected under Hamilton's Ricci flow - a tool\nfrom differential geometry that evolves a manifold by smoothing its curvature,\nin order to identify its topology. To illustrate this idea, we present a\ncomputational framework to quantify the geometric changes that occur as data\npasses through successive layers of a DNN, and use this framework to motivate a\nnotion of `global Ricci network flow' that can be used to assess a DNN's\nability to disentangle complex data geometries to solve classification\nproblems. By training more than $1,500$ DNN classifiers of different widths and\ndepths on synthetic and real-world data, we show that the strength of global\nRicci network flow-like behaviour correlates with accuracy for well-trained\nDNNs, independently of depth, width and data set. Our findings motivate the use\nof tools from differential and discrete geometry to the problem of\nexplainability in deep learning.\n", "link": "http://arxiv.org/abs/2404.14265v1", "date": "2024-04-22", "relevancy": 1.9861, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4613}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20as%20Ricci%20Flow&body=Title%3A%20Deep%20Learning%20as%20Ricci%20Flow%0AAuthor%3A%20Anthony%20Baptista%20and%20Alessandro%20Barp%20and%20Tapabrata%20Chakraborti%20and%20Chris%20Harbron%20and%20Ben%20D.%20MacArthur%20and%20Christopher%20R.%20S.%20Banerji%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20powerful%20tools%20for%20approximating%20the%0Adistribution%20of%20complex%20data.%20It%20is%20known%20that%20data%20passing%20through%20a%20trained%0ADNN%20classifier%20undergoes%20a%20series%20of%20geometric%20and%20topological%20simplifications.%0AWhile%20some%20progress%20has%20been%20made%20toward%20understanding%20these%20transformations%20in%0Aneural%20networks%20with%20smooth%20activation%20functions%2C%20an%20understanding%20in%20the%20more%0Ageneral%20setting%20of%20non-smooth%20activation%20functions%2C%20such%20as%20the%20rectified%0Alinear%20unit%20%28ReLU%29%2C%20which%20tend%20to%20perform%20better%2C%20is%20required.%20Here%20we%20propose%0Athat%20the%20geometric%20transformations%20performed%20by%20DNNs%20during%20classification%0Atasks%20have%20parallels%20to%20those%20expected%20under%20Hamilton%27s%20Ricci%20flow%20-%20a%20tool%0Afrom%20differential%20geometry%20that%20evolves%20a%20manifold%20by%20smoothing%20its%20curvature%2C%0Ain%20order%20to%20identify%20its%20topology.%20To%20illustrate%20this%20idea%2C%20we%20present%20a%0Acomputational%20framework%20to%20quantify%20the%20geometric%20changes%20that%20occur%20as%20data%0Apasses%20through%20successive%20layers%20of%20a%20DNN%2C%20and%20use%20this%20framework%20to%20motivate%20a%0Anotion%20of%20%60global%20Ricci%20network%20flow%27%20that%20can%20be%20used%20to%20assess%20a%20DNN%27s%0Aability%20to%20disentangle%20complex%20data%20geometries%20to%20solve%20classification%0Aproblems.%20By%20training%20more%20than%20%241%2C500%24%20DNN%20classifiers%20of%20different%20widths%20and%0Adepths%20on%20synthetic%20and%20real-world%20data%2C%20we%20show%20that%20the%20strength%20of%20global%0ARicci%20network%20flow-like%20behaviour%20correlates%20with%20accuracy%20for%20well-trained%0ADNNs%2C%20independently%20of%20depth%2C%20width%20and%20data%20set.%20Our%20findings%20motivate%20the%20use%0Aof%20tools%20from%20differential%20and%20discrete%20geometry%20to%20the%20problem%20of%0Aexplainability%20in%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14265v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20as%20Ricci%20Flow&entry.906535625=Anthony%20Baptista%20and%20Alessandro%20Barp%20and%20Tapabrata%20Chakraborti%20and%20Chris%20Harbron%20and%20Ben%20D.%20MacArthur%20and%20Christopher%20R.%20S.%20Banerji&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20powerful%20tools%20for%20approximating%20the%0Adistribution%20of%20complex%20data.%20It%20is%20known%20that%20data%20passing%20through%20a%20trained%0ADNN%20classifier%20undergoes%20a%20series%20of%20geometric%20and%20topological%20simplifications.%0AWhile%20some%20progress%20has%20been%20made%20toward%20understanding%20these%20transformations%20in%0Aneural%20networks%20with%20smooth%20activation%20functions%2C%20an%20understanding%20in%20the%20more%0Ageneral%20setting%20of%20non-smooth%20activation%20functions%2C%20such%20as%20the%20rectified%0Alinear%20unit%20%28ReLU%29%2C%20which%20tend%20to%20perform%20better%2C%20is%20required.%20Here%20we%20propose%0Athat%20the%20geometric%20transformations%20performed%20by%20DNNs%20during%20classification%0Atasks%20have%20parallels%20to%20those%20expected%20under%20Hamilton%27s%20Ricci%20flow%20-%20a%20tool%0Afrom%20differential%20geometry%20that%20evolves%20a%20manifold%20by%20smoothing%20its%20curvature%2C%0Ain%20order%20to%20identify%20its%20topology.%20To%20illustrate%20this%20idea%2C%20we%20present%20a%0Acomputational%20framework%20to%20quantify%20the%20geometric%20changes%20that%20occur%20as%20data%0Apasses%20through%20successive%20layers%20of%20a%20DNN%2C%20and%20use%20this%20framework%20to%20motivate%20a%0Anotion%20of%20%60global%20Ricci%20network%20flow%27%20that%20can%20be%20used%20to%20assess%20a%20DNN%27s%0Aability%20to%20disentangle%20complex%20data%20geometries%20to%20solve%20classification%0Aproblems.%20By%20training%20more%20than%20%241%2C500%24%20DNN%20classifiers%20of%20different%20widths%20and%0Adepths%20on%20synthetic%20and%20real-world%20data%2C%20we%20show%20that%20the%20strength%20of%20global%0ARicci%20network%20flow-like%20behaviour%20correlates%20with%20accuracy%20for%20well-trained%0ADNNs%2C%20independently%20of%20depth%2C%20width%20and%20data%20set.%20Our%20findings%20motivate%20the%20use%0Aof%20tools%20from%20differential%20and%20discrete%20geometry%20to%20the%20problem%20of%0Aexplainability%20in%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14265v1&entry.124074799=Read"},
{"title": "An Adversarial Approach to Evaluating the Robustness of Event\n  Identification Models", "author": "Obai Bahwal and Oliver Kosut and Lalitha Sankar", "abstract": "  Intelligent machine learning approaches are finding active use for event\ndetection and identification that allow real-time situational awareness. Yet,\nsuch machine learning algorithms have been shown to be susceptible to\nadversarial attacks on the incoming telemetry data. This paper considers a\nphysics-based modal decomposition method to extract features for event\nclassification and focuses on interpretable classifiers including logistic\nregression and gradient boosting to distinguish two types of events: load loss\nand generation loss. The resulting classifiers are then tested against an\nadversarial algorithm to evaluate their robustness. The adversarial attack is\ntested in two settings: the white box setting, wherein the attacker knows\nexactly the classification model; and the gray box setting, wherein the\nattacker has access to historical data from the same network as was used to\ntrain the classifier, but does not know the classification model. Thorough\nexperiments on the synthetic South Carolina 500-bus system highlight that a\nrelatively simpler model such as logistic regression is more susceptible to\nadversarial attacks than gradient boosting.\n", "link": "http://arxiv.org/abs/2402.12338v2", "date": "2024-04-22", "relevancy": 1.972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Adversarial%20Approach%20to%20Evaluating%20the%20Robustness%20of%20Event%0A%20%20Identification%20Models&body=Title%3A%20An%20Adversarial%20Approach%20to%20Evaluating%20the%20Robustness%20of%20Event%0A%20%20Identification%20Models%0AAuthor%3A%20Obai%20Bahwal%20and%20Oliver%20Kosut%20and%20Lalitha%20Sankar%0AAbstract%3A%20%20%20Intelligent%20machine%20learning%20approaches%20are%20finding%20active%20use%20for%20event%0Adetection%20and%20identification%20that%20allow%20real-time%20situational%20awareness.%20Yet%2C%0Asuch%20machine%20learning%20algorithms%20have%20been%20shown%20to%20be%20susceptible%20to%0Aadversarial%20attacks%20on%20the%20incoming%20telemetry%20data.%20This%20paper%20considers%20a%0Aphysics-based%20modal%20decomposition%20method%20to%20extract%20features%20for%20event%0Aclassification%20and%20focuses%20on%20interpretable%20classifiers%20including%20logistic%0Aregression%20and%20gradient%20boosting%20to%20distinguish%20two%20types%20of%20events%3A%20load%20loss%0Aand%20generation%20loss.%20The%20resulting%20classifiers%20are%20then%20tested%20against%20an%0Aadversarial%20algorithm%20to%20evaluate%20their%20robustness.%20The%20adversarial%20attack%20is%0Atested%20in%20two%20settings%3A%20the%20white%20box%20setting%2C%20wherein%20the%20attacker%20knows%0Aexactly%20the%20classification%20model%3B%20and%20the%20gray%20box%20setting%2C%20wherein%20the%0Aattacker%20has%20access%20to%20historical%20data%20from%20the%20same%20network%20as%20was%20used%20to%0Atrain%20the%20classifier%2C%20but%20does%20not%20know%20the%20classification%20model.%20Thorough%0Aexperiments%20on%20the%20synthetic%20South%20Carolina%20500-bus%20system%20highlight%20that%20a%0Arelatively%20simpler%20model%20such%20as%20logistic%20regression%20is%20more%20susceptible%20to%0Aadversarial%20attacks%20than%20gradient%20boosting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12338v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adversarial%20Approach%20to%20Evaluating%20the%20Robustness%20of%20Event%0A%20%20Identification%20Models&entry.906535625=Obai%20Bahwal%20and%20Oliver%20Kosut%20and%20Lalitha%20Sankar&entry.1292438233=%20%20Intelligent%20machine%20learning%20approaches%20are%20finding%20active%20use%20for%20event%0Adetection%20and%20identification%20that%20allow%20real-time%20situational%20awareness.%20Yet%2C%0Asuch%20machine%20learning%20algorithms%20have%20been%20shown%20to%20be%20susceptible%20to%0Aadversarial%20attacks%20on%20the%20incoming%20telemetry%20data.%20This%20paper%20considers%20a%0Aphysics-based%20modal%20decomposition%20method%20to%20extract%20features%20for%20event%0Aclassification%20and%20focuses%20on%20interpretable%20classifiers%20including%20logistic%0Aregression%20and%20gradient%20boosting%20to%20distinguish%20two%20types%20of%20events%3A%20load%20loss%0Aand%20generation%20loss.%20The%20resulting%20classifiers%20are%20then%20tested%20against%20an%0Aadversarial%20algorithm%20to%20evaluate%20their%20robustness.%20The%20adversarial%20attack%20is%0Atested%20in%20two%20settings%3A%20the%20white%20box%20setting%2C%20wherein%20the%20attacker%20knows%0Aexactly%20the%20classification%20model%3B%20and%20the%20gray%20box%20setting%2C%20wherein%20the%0Aattacker%20has%20access%20to%20historical%20data%20from%20the%20same%20network%20as%20was%20used%20to%0Atrain%20the%20classifier%2C%20but%20does%20not%20know%20the%20classification%20model.%20Thorough%0Aexperiments%20on%20the%20synthetic%20South%20Carolina%20500-bus%20system%20highlight%20that%20a%0Arelatively%20simpler%20model%20such%20as%20logistic%20regression%20is%20more%20susceptible%20to%0Aadversarial%20attacks%20than%20gradient%20boosting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12338v2&entry.124074799=Read"},
{"title": "1st Place Solution to the 1st SkatingVerse Challenge", "author": "Tao Sun and Yuanzi Fu and Kaicheng Yang and Jian Wu and Ziyong Feng", "abstract": "  This paper presents the winning solution for the 1st SkatingVerse Challenge.\nWe propose a method that involves several steps. To begin, we leverage the DINO\nframework to extract the Region of Interest (ROI) and perform precise cropping\nof the raw video footage. Subsequently, we employ three distinct models, namely\nUnmasked Teacher, UniformerV2, and InfoGCN, to capture different aspects of the\ndata. By ensembling the prediction results based on logits, our solution\nattains an impressive leaderboard score of 95.73%.\n", "link": "http://arxiv.org/abs/2404.14032v1", "date": "2024-04-22", "relevancy": 1.9639, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4965}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4903}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%201st%20Place%20Solution%20to%20the%201st%20SkatingVerse%20Challenge&body=Title%3A%201st%20Place%20Solution%20to%20the%201st%20SkatingVerse%20Challenge%0AAuthor%3A%20Tao%20Sun%20and%20Yuanzi%20Fu%20and%20Kaicheng%20Yang%20and%20Jian%20Wu%20and%20Ziyong%20Feng%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20winning%20solution%20for%20the%201st%20SkatingVerse%20Challenge.%0AWe%20propose%20a%20method%20that%20involves%20several%20steps.%20To%20begin%2C%20we%20leverage%20the%20DINO%0Aframework%20to%20extract%20the%20Region%20of%20Interest%20%28ROI%29%20and%20perform%20precise%20cropping%0Aof%20the%20raw%20video%20footage.%20Subsequently%2C%20we%20employ%20three%20distinct%20models%2C%20namely%0AUnmasked%20Teacher%2C%20UniformerV2%2C%20and%20InfoGCN%2C%20to%20capture%20different%20aspects%20of%20the%0Adata.%20By%20ensembling%20the%20prediction%20results%20based%20on%20logits%2C%20our%20solution%0Aattains%20an%20impressive%20leaderboard%20score%20of%2095.73%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14032v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1st%20Place%20Solution%20to%20the%201st%20SkatingVerse%20Challenge&entry.906535625=Tao%20Sun%20and%20Yuanzi%20Fu%20and%20Kaicheng%20Yang%20and%20Jian%20Wu%20and%20Ziyong%20Feng&entry.1292438233=%20%20This%20paper%20presents%20the%20winning%20solution%20for%20the%201st%20SkatingVerse%20Challenge.%0AWe%20propose%20a%20method%20that%20involves%20several%20steps.%20To%20begin%2C%20we%20leverage%20the%20DINO%0Aframework%20to%20extract%20the%20Region%20of%20Interest%20%28ROI%29%20and%20perform%20precise%20cropping%0Aof%20the%20raw%20video%20footage.%20Subsequently%2C%20we%20employ%20three%20distinct%20models%2C%20namely%0AUnmasked%20Teacher%2C%20UniformerV2%2C%20and%20InfoGCN%2C%20to%20capture%20different%20aspects%20of%20the%0Adata.%20By%20ensembling%20the%20prediction%20results%20based%20on%20logits%2C%20our%20solution%0Aattains%20an%20impressive%20leaderboard%20score%20of%2095.73%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14032v1&entry.124074799=Read"},
{"title": "Differentially Private Kernel Inducing Points using features from\n  ScatterNets (DP-KIP-ScatterNet) for Privacy Preserving Data Distillation", "author": "Margarita Vinaroz and Mi Jung Park", "abstract": "  Data distillation aims to generate a small data set that closely mimics the\nperformance of a given learning algorithm on the original data set. The\ndistilled dataset is hence useful to simplify the training process thanks to\nits small data size. However, distilled data samples are not necessarily\nprivacy-preserving, even if they are generally humanly indiscernible. To\naddress this limitation, we introduce differentially private kernel inducing\npoints (DP-KIP) for privacy-preserving data distillation. Unlike our original\nintention to simply apply DP-SGD to the framework of KIP, we find that KIP\nusing infinitely-wide convolutional neural tangent kernels (conv-NTKs) performs\nbetter compared to KIP using fully-connected NTKs. However, KIP with conv-NTKs,\ndue to its convolutional and pooling operations, introduces an unbearable\ncomputational complexity, requiring hundreds of V100 GPUs in parallel to train,\nwhich is impractical and more importantly, such computational resources are\ninaccessible to many. To overcome this issue, we propose an alternative that\ndoes not require pre-training (to avoid a privacy loss) and can well capture\ncomplex information on images, as those features from conv-NKTs do, while the\ncomputational cost is manageable by a single V100 GPU. To this end, we propose\nDP-KIP-ScatterNet, which uses the wavelet features from Scattering networks\n(ScatterNet) instead of those from conv-NTKs, to perform DP-KIP at a reasonable\ncomputational cost. We implement DP-KIP-ScatterNet in -- computationally\nefficient -- JAX and test on several popular image datasets to show its\nefficacy and its superior performance compared to state-of-the art methods in\nimage data distillation with differential privacy guarantees.\n", "link": "http://arxiv.org/abs/2301.13389v2", "date": "2024-04-22", "relevancy": 1.9627, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Kernel%20Inducing%20Points%20using%20features%20from%0A%20%20ScatterNets%20%28DP-KIP-ScatterNet%29%20for%20Privacy%20Preserving%20Data%20Distillation&body=Title%3A%20Differentially%20Private%20Kernel%20Inducing%20Points%20using%20features%20from%0A%20%20ScatterNets%20%28DP-KIP-ScatterNet%29%20for%20Privacy%20Preserving%20Data%20Distillation%0AAuthor%3A%20Margarita%20Vinaroz%20and%20Mi%20Jung%20Park%0AAbstract%3A%20%20%20Data%20distillation%20aims%20to%20generate%20a%20small%20data%20set%20that%20closely%20mimics%20the%0Aperformance%20of%20a%20given%20learning%20algorithm%20on%20the%20original%20data%20set.%20The%0Adistilled%20dataset%20is%20hence%20useful%20to%20simplify%20the%20training%20process%20thanks%20to%0Aits%20small%20data%20size.%20However%2C%20distilled%20data%20samples%20are%20not%20necessarily%0Aprivacy-preserving%2C%20even%20if%20they%20are%20generally%20humanly%20indiscernible.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20differentially%20private%20kernel%20inducing%0Apoints%20%28DP-KIP%29%20for%20privacy-preserving%20data%20distillation.%20Unlike%20our%20original%0Aintention%20to%20simply%20apply%20DP-SGD%20to%20the%20framework%20of%20KIP%2C%20we%20find%20that%20KIP%0Ausing%20infinitely-wide%20convolutional%20neural%20tangent%20kernels%20%28conv-NTKs%29%20performs%0Abetter%20compared%20to%20KIP%20using%20fully-connected%20NTKs.%20However%2C%20KIP%20with%20conv-NTKs%2C%0Adue%20to%20its%20convolutional%20and%20pooling%20operations%2C%20introduces%20an%20unbearable%0Acomputational%20complexity%2C%20requiring%20hundreds%20of%20V100%20GPUs%20in%20parallel%20to%20train%2C%0Awhich%20is%20impractical%20and%20more%20importantly%2C%20such%20computational%20resources%20are%0Ainaccessible%20to%20many.%20To%20overcome%20this%20issue%2C%20we%20propose%20an%20alternative%20that%0Adoes%20not%20require%20pre-training%20%28to%20avoid%20a%20privacy%20loss%29%20and%20can%20well%20capture%0Acomplex%20information%20on%20images%2C%20as%20those%20features%20from%20conv-NKTs%20do%2C%20while%20the%0Acomputational%20cost%20is%20manageable%20by%20a%20single%20V100%20GPU.%20To%20this%20end%2C%20we%20propose%0ADP-KIP-ScatterNet%2C%20which%20uses%20the%20wavelet%20features%20from%20Scattering%20networks%0A%28ScatterNet%29%20instead%20of%20those%20from%20conv-NTKs%2C%20to%20perform%20DP-KIP%20at%20a%20reasonable%0Acomputational%20cost.%20We%20implement%20DP-KIP-ScatterNet%20in%20--%20computationally%0Aefficient%20--%20JAX%20and%20test%20on%20several%20popular%20image%20datasets%20to%20show%20its%0Aefficacy%20and%20its%20superior%20performance%20compared%20to%20state-of-the%20art%20methods%20in%0Aimage%20data%20distillation%20with%20differential%20privacy%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13389v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Kernel%20Inducing%20Points%20using%20features%20from%0A%20%20ScatterNets%20%28DP-KIP-ScatterNet%29%20for%20Privacy%20Preserving%20Data%20Distillation&entry.906535625=Margarita%20Vinaroz%20and%20Mi%20Jung%20Park&entry.1292438233=%20%20Data%20distillation%20aims%20to%20generate%20a%20small%20data%20set%20that%20closely%20mimics%20the%0Aperformance%20of%20a%20given%20learning%20algorithm%20on%20the%20original%20data%20set.%20The%0Adistilled%20dataset%20is%20hence%20useful%20to%20simplify%20the%20training%20process%20thanks%20to%0Aits%20small%20data%20size.%20However%2C%20distilled%20data%20samples%20are%20not%20necessarily%0Aprivacy-preserving%2C%20even%20if%20they%20are%20generally%20humanly%20indiscernible.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20differentially%20private%20kernel%20inducing%0Apoints%20%28DP-KIP%29%20for%20privacy-preserving%20data%20distillation.%20Unlike%20our%20original%0Aintention%20to%20simply%20apply%20DP-SGD%20to%20the%20framework%20of%20KIP%2C%20we%20find%20that%20KIP%0Ausing%20infinitely-wide%20convolutional%20neural%20tangent%20kernels%20%28conv-NTKs%29%20performs%0Abetter%20compared%20to%20KIP%20using%20fully-connected%20NTKs.%20However%2C%20KIP%20with%20conv-NTKs%2C%0Adue%20to%20its%20convolutional%20and%20pooling%20operations%2C%20introduces%20an%20unbearable%0Acomputational%20complexity%2C%20requiring%20hundreds%20of%20V100%20GPUs%20in%20parallel%20to%20train%2C%0Awhich%20is%20impractical%20and%20more%20importantly%2C%20such%20computational%20resources%20are%0Ainaccessible%20to%20many.%20To%20overcome%20this%20issue%2C%20we%20propose%20an%20alternative%20that%0Adoes%20not%20require%20pre-training%20%28to%20avoid%20a%20privacy%20loss%29%20and%20can%20well%20capture%0Acomplex%20information%20on%20images%2C%20as%20those%20features%20from%20conv-NKTs%20do%2C%20while%20the%0Acomputational%20cost%20is%20manageable%20by%20a%20single%20V100%20GPU.%20To%20this%20end%2C%20we%20propose%0ADP-KIP-ScatterNet%2C%20which%20uses%20the%20wavelet%20features%20from%20Scattering%20networks%0A%28ScatterNet%29%20instead%20of%20those%20from%20conv-NTKs%2C%20to%20perform%20DP-KIP%20at%20a%20reasonable%0Acomputational%20cost.%20We%20implement%20DP-KIP-ScatterNet%20in%20--%20computationally%0Aefficient%20--%20JAX%20and%20test%20on%20several%20popular%20image%20datasets%20to%20show%20its%0Aefficacy%20and%20its%20superior%20performance%20compared%20to%20state-of-the%20art%20methods%20in%0Aimage%20data%20distillation%20with%20differential%20privacy%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13389v2&entry.124074799=Read"},
{"title": "NTIRE 2024 Challenge on Low Light Image Enhancement: Methods and Results", "author": "Xiaoning Liu and Zongwei Wu and Ao Li and Florin-Alexandru Vasluianu and Yulun Zhang and Shuhang Gu and Le Zhang and Ce Zhu and Radu Timofte and Zhi Jin and Hongjun Wu and Chenxi Wang and Haitao Ling and Yuanhao Cai and Hao Bian and Yuxin Zheng and Jing Lin and Alan Yuille and Ben Shao and Jin Guo and Tianli Liu and Mohao Wu and Yixu Feng and Shuo Hou and Haotian Lin and Yu Zhu and Peng Wu and Wei Dong and Jinqiu Sun and Yanning Zhang and Qingsen Yan and Wenbin Zou and Weipeng Yang and Yunxiang Li and Qiaomu Wei and Tian Ye and Sixiang Chen and Zhao Zhang and Suiyi Zhao and Bo Wang and Yan Luo and Zhichao Zuo and Mingshen Wang and Junhu Wang and Yanyan Wei and Xiaopeng Sun and Yu Gao and Jiancheng Huang and Hongming Chen and Xiang Chen and Hui Tang and Yuanbin Chen and Yuanbo Zhou and Xinwei Dai and Xintao Qiu and Wei Deng and Qinquan Gao and Tong Tong and Mingjia Li and Jin Hu and Xinyu He and Xiaojie Guo and  Sabarinathan and K Uma and A Sasithradevi and B Sathya Bama and S. Mohamed Mansoor Roomi and V. Srivatsav and Jinjuan Wang and Long Sun and Qiuying Chen and Jiahong Shao and Yizhi Zhang and Marcos V. Conde and Daniel Feijoo and Juan C. Benito and Alvaro Garc\u00eda and Jaeho Lee and Seongwan Kim and Sharif S M A and Nodirkhuja Khujaev and Roman Tsoy and Ali Murtaza and Uswah Khairuddin and Ahmad 'Athif Mohd Faudzi and Sampada Malagi and Amogh Joshi and Nikhil Akalwadi and Chaitra Desai and Ramesh Ashok Tabib and Uma Mudenagudi and Wenyi Lian and Wenjing Lian and Jagadeesh Kalyanshetti and Vijayalaxmi Ashok Aralikatti and Palani Yashaswini and Nitish Upasi and Dikshit Hegde and Ujwala Patil and Sujata C and Xingzhuo Yan and Wei Hao and Minghan Fu and Pooja choksy and Anjali Sarvaiya and Kishor Upla and Kiran Raja and Hailong Yan and Yunkai Zhang and Baiang Li and Jingyi Zhang and Huan Zheng", "abstract": "  This paper reviews the NTIRE 2024 low light image enhancement challenge,\nhighlighting the proposed solutions and results. The aim of this challenge is\nto discover an effective network design or solution capable of generating\nbrighter, clearer, and visually appealing results when dealing with a variety\nof conditions, including ultra-high resolution (4K and beyond), non-uniform\nillumination, backlighting, extreme darkness, and night scenes. A notable total\nof 428 participants registered for the challenge, with 22 teams ultimately\nmaking valid submissions. This paper meticulously evaluates the\nstate-of-the-art advancements in enhancing low-light images, reflecting the\nsignificant progress and creativity in this field.\n", "link": "http://arxiv.org/abs/2404.14248v1", "date": "2024-04-22", "relevancy": 1.9612, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.508}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4886}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NTIRE%202024%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results&body=Title%3A%20NTIRE%202024%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results%0AAuthor%3A%20Xiaoning%20Liu%20and%20Zongwei%20Wu%20and%20Ao%20Li%20and%20Florin-Alexandru%20Vasluianu%20and%20Yulun%20Zhang%20and%20Shuhang%20Gu%20and%20Le%20Zhang%20and%20Ce%20Zhu%20and%20Radu%20Timofte%20and%20Zhi%20Jin%20and%20Hongjun%20Wu%20and%20Chenxi%20Wang%20and%20Haitao%20Ling%20and%20Yuanhao%20Cai%20and%20Hao%20Bian%20and%20Yuxin%20Zheng%20and%20Jing%20Lin%20and%20Alan%20Yuille%20and%20Ben%20Shao%20and%20Jin%20Guo%20and%20Tianli%20Liu%20and%20Mohao%20Wu%20and%20Yixu%20Feng%20and%20Shuo%20Hou%20and%20Haotian%20Lin%20and%20Yu%20Zhu%20and%20Peng%20Wu%20and%20Wei%20Dong%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan%20and%20Wenbin%20Zou%20and%20Weipeng%20Yang%20and%20Yunxiang%20Li%20and%20Qiaomu%20Wei%20and%20Tian%20Ye%20and%20Sixiang%20Chen%20and%20Zhao%20Zhang%20and%20Suiyi%20Zhao%20and%20Bo%20Wang%20and%20Yan%20Luo%20and%20Zhichao%20Zuo%20and%20Mingshen%20Wang%20and%20Junhu%20Wang%20and%20Yanyan%20Wei%20and%20Xiaopeng%20Sun%20and%20Yu%20Gao%20and%20Jiancheng%20Huang%20and%20Hongming%20Chen%20and%20Xiang%20Chen%20and%20Hui%20Tang%20and%20Yuanbin%20Chen%20and%20Yuanbo%20Zhou%20and%20Xinwei%20Dai%20and%20Xintao%20Qiu%20and%20Wei%20Deng%20and%20Qinquan%20Gao%20and%20Tong%20Tong%20and%20Mingjia%20Li%20and%20Jin%20Hu%20and%20Xinyu%20He%20and%20Xiaojie%20Guo%20and%20%20Sabarinathan%20and%20K%20Uma%20and%20A%20Sasithradevi%20and%20B%20Sathya%20Bama%20and%20S.%20Mohamed%20Mansoor%20Roomi%20and%20V.%20Srivatsav%20and%20Jinjuan%20Wang%20and%20Long%20Sun%20and%20Qiuying%20Chen%20and%20Jiahong%20Shao%20and%20Yizhi%20Zhang%20and%20Marcos%20V.%20Conde%20and%20Daniel%20Feijoo%20and%20Juan%20C.%20Benito%20and%20Alvaro%20Garc%C3%ADa%20and%20Jaeho%20Lee%20and%20Seongwan%20Kim%20and%20Sharif%20S%20M%20A%20and%20Nodirkhuja%20Khujaev%20and%20Roman%20Tsoy%20and%20Ali%20Murtaza%20and%20Uswah%20Khairuddin%20and%20Ahmad%20%27Athif%20Mohd%20Faudzi%20and%20Sampada%20Malagi%20and%20Amogh%20Joshi%20and%20Nikhil%20Akalwadi%20and%20Chaitra%20Desai%20and%20Ramesh%20Ashok%20Tabib%20and%20Uma%20Mudenagudi%20and%20Wenyi%20Lian%20and%20Wenjing%20Lian%20and%20Jagadeesh%20Kalyanshetti%20and%20Vijayalaxmi%20Ashok%20Aralikatti%20and%20Palani%20Yashaswini%20and%20Nitish%20Upasi%20and%20Dikshit%20Hegde%20and%20Ujwala%20Patil%20and%20Sujata%20C%20and%20Xingzhuo%20Yan%20and%20Wei%20Hao%20and%20Minghan%20Fu%20and%20Pooja%20choksy%20and%20Anjali%20Sarvaiya%20and%20Kishor%20Upla%20and%20Kiran%20Raja%20and%20Hailong%20Yan%20and%20Yunkai%20Zhang%20and%20Baiang%20Li%20and%20Jingyi%20Zhang%20and%20Huan%20Zheng%0AAbstract%3A%20%20%20This%20paper%20reviews%20the%20NTIRE%202024%20low%20light%20image%20enhancement%20challenge%2C%0Ahighlighting%20the%20proposed%20solutions%20and%20results.%20The%20aim%20of%20this%20challenge%20is%0Ato%20discover%20an%20effective%20network%20design%20or%20solution%20capable%20of%20generating%0Abrighter%2C%20clearer%2C%20and%20visually%20appealing%20results%20when%20dealing%20with%20a%20variety%0Aof%20conditions%2C%20including%20ultra-high%20resolution%20%284K%20and%20beyond%29%2C%20non-uniform%0Aillumination%2C%20backlighting%2C%20extreme%20darkness%2C%20and%20night%20scenes.%20A%20notable%20total%0Aof%20428%20participants%20registered%20for%20the%20challenge%2C%20with%2022%20teams%20ultimately%0Amaking%20valid%20submissions.%20This%20paper%20meticulously%20evaluates%20the%0Astate-of-the-art%20advancements%20in%20enhancing%20low-light%20images%2C%20reflecting%20the%0Asignificant%20progress%20and%20creativity%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14248v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202024%20Challenge%20on%20Low%20Light%20Image%20Enhancement%3A%20Methods%20and%20Results&entry.906535625=Xiaoning%20Liu%20and%20Zongwei%20Wu%20and%20Ao%20Li%20and%20Florin-Alexandru%20Vasluianu%20and%20Yulun%20Zhang%20and%20Shuhang%20Gu%20and%20Le%20Zhang%20and%20Ce%20Zhu%20and%20Radu%20Timofte%20and%20Zhi%20Jin%20and%20Hongjun%20Wu%20and%20Chenxi%20Wang%20and%20Haitao%20Ling%20and%20Yuanhao%20Cai%20and%20Hao%20Bian%20and%20Yuxin%20Zheng%20and%20Jing%20Lin%20and%20Alan%20Yuille%20and%20Ben%20Shao%20and%20Jin%20Guo%20and%20Tianli%20Liu%20and%20Mohao%20Wu%20and%20Yixu%20Feng%20and%20Shuo%20Hou%20and%20Haotian%20Lin%20and%20Yu%20Zhu%20and%20Peng%20Wu%20and%20Wei%20Dong%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Qingsen%20Yan%20and%20Wenbin%20Zou%20and%20Weipeng%20Yang%20and%20Yunxiang%20Li%20and%20Qiaomu%20Wei%20and%20Tian%20Ye%20and%20Sixiang%20Chen%20and%20Zhao%20Zhang%20and%20Suiyi%20Zhao%20and%20Bo%20Wang%20and%20Yan%20Luo%20and%20Zhichao%20Zuo%20and%20Mingshen%20Wang%20and%20Junhu%20Wang%20and%20Yanyan%20Wei%20and%20Xiaopeng%20Sun%20and%20Yu%20Gao%20and%20Jiancheng%20Huang%20and%20Hongming%20Chen%20and%20Xiang%20Chen%20and%20Hui%20Tang%20and%20Yuanbin%20Chen%20and%20Yuanbo%20Zhou%20and%20Xinwei%20Dai%20and%20Xintao%20Qiu%20and%20Wei%20Deng%20and%20Qinquan%20Gao%20and%20Tong%20Tong%20and%20Mingjia%20Li%20and%20Jin%20Hu%20and%20Xinyu%20He%20and%20Xiaojie%20Guo%20and%20%20Sabarinathan%20and%20K%20Uma%20and%20A%20Sasithradevi%20and%20B%20Sathya%20Bama%20and%20S.%20Mohamed%20Mansoor%20Roomi%20and%20V.%20Srivatsav%20and%20Jinjuan%20Wang%20and%20Long%20Sun%20and%20Qiuying%20Chen%20and%20Jiahong%20Shao%20and%20Yizhi%20Zhang%20and%20Marcos%20V.%20Conde%20and%20Daniel%20Feijoo%20and%20Juan%20C.%20Benito%20and%20Alvaro%20Garc%C3%ADa%20and%20Jaeho%20Lee%20and%20Seongwan%20Kim%20and%20Sharif%20S%20M%20A%20and%20Nodirkhuja%20Khujaev%20and%20Roman%20Tsoy%20and%20Ali%20Murtaza%20and%20Uswah%20Khairuddin%20and%20Ahmad%20%27Athif%20Mohd%20Faudzi%20and%20Sampada%20Malagi%20and%20Amogh%20Joshi%20and%20Nikhil%20Akalwadi%20and%20Chaitra%20Desai%20and%20Ramesh%20Ashok%20Tabib%20and%20Uma%20Mudenagudi%20and%20Wenyi%20Lian%20and%20Wenjing%20Lian%20and%20Jagadeesh%20Kalyanshetti%20and%20Vijayalaxmi%20Ashok%20Aralikatti%20and%20Palani%20Yashaswini%20and%20Nitish%20Upasi%20and%20Dikshit%20Hegde%20and%20Ujwala%20Patil%20and%20Sujata%20C%20and%20Xingzhuo%20Yan%20and%20Wei%20Hao%20and%20Minghan%20Fu%20and%20Pooja%20choksy%20and%20Anjali%20Sarvaiya%20and%20Kishor%20Upla%20and%20Kiran%20Raja%20and%20Hailong%20Yan%20and%20Yunkai%20Zhang%20and%20Baiang%20Li%20and%20Jingyi%20Zhang%20and%20Huan%20Zheng&entry.1292438233=%20%20This%20paper%20reviews%20the%20NTIRE%202024%20low%20light%20image%20enhancement%20challenge%2C%0Ahighlighting%20the%20proposed%20solutions%20and%20results.%20The%20aim%20of%20this%20challenge%20is%0Ato%20discover%20an%20effective%20network%20design%20or%20solution%20capable%20of%20generating%0Abrighter%2C%20clearer%2C%20and%20visually%20appealing%20results%20when%20dealing%20with%20a%20variety%0Aof%20conditions%2C%20including%20ultra-high%20resolution%20%284K%20and%20beyond%29%2C%20non-uniform%0Aillumination%2C%20backlighting%2C%20extreme%20darkness%2C%20and%20night%20scenes.%20A%20notable%20total%0Aof%20428%20participants%20registered%20for%20the%20challenge%2C%20with%2022%20teams%20ultimately%0Amaking%20valid%20submissions.%20This%20paper%20meticulously%20evaluates%20the%0Astate-of-the-art%20advancements%20in%20enhancing%20low-light%20images%2C%20reflecting%20the%0Asignificant%20progress%20and%20creativity%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14248v1&entry.124074799=Read"},
{"title": "FedTAD: Topology-aware Data-free Knowledge Distillation for Subgraph\n  Federated Learning", "author": "Yinlin Zhu and Xunkai Li and Zhengyu Wu and Di Wu and Miao Hu and Rong-Hua Li", "abstract": "  Subgraph federated learning (subgraph-FL) is a new distributed paradigm that\nfacilitates the collaborative training of graph neural networks (GNNs) by\nmulti-client subgraphs. Unfortunately, a significant challenge of subgraph-FL\narises from subgraph heterogeneity, which stems from node and topology\nvariation, causing the impaired performance of the global GNN. Despite various\nstudies, they have not yet thoroughly investigated the impact mechanism of\nsubgraph heterogeneity. To this end, we decouple node and topology variation,\nrevealing that they correspond to differences in label distribution and\nstructure homophily. Remarkably, these variations lead to significant\ndifferences in the class-wise knowledge reliability of multiple local GNNs,\nmisguiding the model aggregation with varying degrees. Building on this\ninsight, we propose topology-aware data-free knowledge distillation technology\n(FedTAD), enhancing reliable knowledge transfer from the local model to the\nglobal model. Extensive experiments on six public datasets consistently\ndemonstrate the superiority of FedTAD over state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2404.14061v1", "date": "2024-04-22", "relevancy": 1.9457, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4816}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedTAD%3A%20Topology-aware%20Data-free%20Knowledge%20Distillation%20for%20Subgraph%0A%20%20Federated%20Learning&body=Title%3A%20FedTAD%3A%20Topology-aware%20Data-free%20Knowledge%20Distillation%20for%20Subgraph%0A%20%20Federated%20Learning%0AAuthor%3A%20Yinlin%20Zhu%20and%20Xunkai%20Li%20and%20Zhengyu%20Wu%20and%20Di%20Wu%20and%20Miao%20Hu%20and%20Rong-Hua%20Li%0AAbstract%3A%20%20%20Subgraph%20federated%20learning%20%28subgraph-FL%29%20is%20a%20new%20distributed%20paradigm%20that%0Afacilitates%20the%20collaborative%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20by%0Amulti-client%20subgraphs.%20Unfortunately%2C%20a%20significant%20challenge%20of%20subgraph-FL%0Aarises%20from%20subgraph%20heterogeneity%2C%20which%20stems%20from%20node%20and%20topology%0Avariation%2C%20causing%20the%20impaired%20performance%20of%20the%20global%20GNN.%20Despite%20various%0Astudies%2C%20they%20have%20not%20yet%20thoroughly%20investigated%20the%20impact%20mechanism%20of%0Asubgraph%20heterogeneity.%20To%20this%20end%2C%20we%20decouple%20node%20and%20topology%20variation%2C%0Arevealing%20that%20they%20correspond%20to%20differences%20in%20label%20distribution%20and%0Astructure%20homophily.%20Remarkably%2C%20these%20variations%20lead%20to%20significant%0Adifferences%20in%20the%20class-wise%20knowledge%20reliability%20of%20multiple%20local%20GNNs%2C%0Amisguiding%20the%20model%20aggregation%20with%20varying%20degrees.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20topology-aware%20data-free%20knowledge%20distillation%20technology%0A%28FedTAD%29%2C%20enhancing%20reliable%20knowledge%20transfer%20from%20the%20local%20model%20to%20the%0Aglobal%20model.%20Extensive%20experiments%20on%20six%20public%20datasets%20consistently%0Ademonstrate%20the%20superiority%20of%20FedTAD%20over%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14061v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedTAD%3A%20Topology-aware%20Data-free%20Knowledge%20Distillation%20for%20Subgraph%0A%20%20Federated%20Learning&entry.906535625=Yinlin%20Zhu%20and%20Xunkai%20Li%20and%20Zhengyu%20Wu%20and%20Di%20Wu%20and%20Miao%20Hu%20and%20Rong-Hua%20Li&entry.1292438233=%20%20Subgraph%20federated%20learning%20%28subgraph-FL%29%20is%20a%20new%20distributed%20paradigm%20that%0Afacilitates%20the%20collaborative%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20by%0Amulti-client%20subgraphs.%20Unfortunately%2C%20a%20significant%20challenge%20of%20subgraph-FL%0Aarises%20from%20subgraph%20heterogeneity%2C%20which%20stems%20from%20node%20and%20topology%0Avariation%2C%20causing%20the%20impaired%20performance%20of%20the%20global%20GNN.%20Despite%20various%0Astudies%2C%20they%20have%20not%20yet%20thoroughly%20investigated%20the%20impact%20mechanism%20of%0Asubgraph%20heterogeneity.%20To%20this%20end%2C%20we%20decouple%20node%20and%20topology%20variation%2C%0Arevealing%20that%20they%20correspond%20to%20differences%20in%20label%20distribution%20and%0Astructure%20homophily.%20Remarkably%2C%20these%20variations%20lead%20to%20significant%0Adifferences%20in%20the%20class-wise%20knowledge%20reliability%20of%20multiple%20local%20GNNs%2C%0Amisguiding%20the%20model%20aggregation%20with%20varying%20degrees.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20topology-aware%20data-free%20knowledge%20distillation%20technology%0A%28FedTAD%29%2C%20enhancing%20reliable%20knowledge%20transfer%20from%20the%20local%20model%20to%20the%0Aglobal%20model.%20Extensive%20experiments%20on%20six%20public%20datasets%20consistently%0Ademonstrate%20the%20superiority%20of%20FedTAD%20over%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14061v1&entry.124074799=Read"},
{"title": "AI-Generated Faces in the Real World: A Large-Scale Case Study of\n  Twitter Profile Images", "author": "Jonas Ricker and Dennis Assenmacher and Thorsten Holz and Asja Fischer and Erwin Quiring", "abstract": "  Recent advances in the field of generative artificial intelligence (AI) have\nblurred the lines between authentic and machine-generated content, making it\nalmost impossible for humans to distinguish between such media. One notable\nconsequence is the use of AI-generated images for fake profiles on social\nmedia. While several types of disinformation campaigns and similar incidents\nhave been reported in the past, a systematic analysis has been lacking. In this\nwork, we conduct the first large-scale investigation of the prevalence of\nAI-generated profile pictures on Twitter. We tackle the challenges of a\nreal-world measurement study by carefully integrating various data sources and\ndesigning a multi-stage detection pipeline. Our analysis of nearly 15 million\nTwitter profile pictures shows that 0.052% were artificially generated,\nconfirming their notable presence on the platform. We comprehensively examine\nthe characteristics of these accounts and their tweet content, and uncover\npatterns of coordinated inauthentic behavior. The results also reveal several\nmotives, including spamming and political amplification campaigns. Our research\nreaffirms the need for effective detection and mitigation strategies to cope\nwith the potential negative effects of generative AI in the future.\n", "link": "http://arxiv.org/abs/2404.14244v1", "date": "2024-04-22", "relevancy": 1.9349, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5068}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4902}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AI-Generated%20Faces%20in%20the%20Real%20World%3A%20A%20Large-Scale%20Case%20Study%20of%0A%20%20Twitter%20Profile%20Images&body=Title%3A%20AI-Generated%20Faces%20in%20the%20Real%20World%3A%20A%20Large-Scale%20Case%20Study%20of%0A%20%20Twitter%20Profile%20Images%0AAuthor%3A%20Jonas%20Ricker%20and%20Dennis%20Assenmacher%20and%20Thorsten%20Holz%20and%20Asja%20Fischer%20and%20Erwin%20Quiring%0AAbstract%3A%20%20%20Recent%20advances%20in%20the%20field%20of%20generative%20artificial%20intelligence%20%28AI%29%20have%0Ablurred%20the%20lines%20between%20authentic%20and%20machine-generated%20content%2C%20making%20it%0Aalmost%20impossible%20for%20humans%20to%20distinguish%20between%20such%20media.%20One%20notable%0Aconsequence%20is%20the%20use%20of%20AI-generated%20images%20for%20fake%20profiles%20on%20social%0Amedia.%20While%20several%20types%20of%20disinformation%20campaigns%20and%20similar%20incidents%0Ahave%20been%20reported%20in%20the%20past%2C%20a%20systematic%20analysis%20has%20been%20lacking.%20In%20this%0Awork%2C%20we%20conduct%20the%20first%20large-scale%20investigation%20of%20the%20prevalence%20of%0AAI-generated%20profile%20pictures%20on%20Twitter.%20We%20tackle%20the%20challenges%20of%20a%0Areal-world%20measurement%20study%20by%20carefully%20integrating%20various%20data%20sources%20and%0Adesigning%20a%20multi-stage%20detection%20pipeline.%20Our%20analysis%20of%20nearly%2015%20million%0ATwitter%20profile%20pictures%20shows%20that%200.052%25%20were%20artificially%20generated%2C%0Aconfirming%20their%20notable%20presence%20on%20the%20platform.%20We%20comprehensively%20examine%0Athe%20characteristics%20of%20these%20accounts%20and%20their%20tweet%20content%2C%20and%20uncover%0Apatterns%20of%20coordinated%20inauthentic%20behavior.%20The%20results%20also%20reveal%20several%0Amotives%2C%20including%20spamming%20and%20political%20amplification%20campaigns.%20Our%20research%0Areaffirms%20the%20need%20for%20effective%20detection%20and%20mitigation%20strategies%20to%20cope%0Awith%20the%20potential%20negative%20effects%20of%20generative%20AI%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14244v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Generated%20Faces%20in%20the%20Real%20World%3A%20A%20Large-Scale%20Case%20Study%20of%0A%20%20Twitter%20Profile%20Images&entry.906535625=Jonas%20Ricker%20and%20Dennis%20Assenmacher%20and%20Thorsten%20Holz%20and%20Asja%20Fischer%20and%20Erwin%20Quiring&entry.1292438233=%20%20Recent%20advances%20in%20the%20field%20of%20generative%20artificial%20intelligence%20%28AI%29%20have%0Ablurred%20the%20lines%20between%20authentic%20and%20machine-generated%20content%2C%20making%20it%0Aalmost%20impossible%20for%20humans%20to%20distinguish%20between%20such%20media.%20One%20notable%0Aconsequence%20is%20the%20use%20of%20AI-generated%20images%20for%20fake%20profiles%20on%20social%0Amedia.%20While%20several%20types%20of%20disinformation%20campaigns%20and%20similar%20incidents%0Ahave%20been%20reported%20in%20the%20past%2C%20a%20systematic%20analysis%20has%20been%20lacking.%20In%20this%0Awork%2C%20we%20conduct%20the%20first%20large-scale%20investigation%20of%20the%20prevalence%20of%0AAI-generated%20profile%20pictures%20on%20Twitter.%20We%20tackle%20the%20challenges%20of%20a%0Areal-world%20measurement%20study%20by%20carefully%20integrating%20various%20data%20sources%20and%0Adesigning%20a%20multi-stage%20detection%20pipeline.%20Our%20analysis%20of%20nearly%2015%20million%0ATwitter%20profile%20pictures%20shows%20that%200.052%25%20were%20artificially%20generated%2C%0Aconfirming%20their%20notable%20presence%20on%20the%20platform.%20We%20comprehensively%20examine%0Athe%20characteristics%20of%20these%20accounts%20and%20their%20tweet%20content%2C%20and%20uncover%0Apatterns%20of%20coordinated%20inauthentic%20behavior.%20The%20results%20also%20reveal%20several%0Amotives%2C%20including%20spamming%20and%20political%20amplification%20campaigns.%20Our%20research%0Areaffirms%20the%20need%20for%20effective%20detection%20and%20mitigation%20strategies%20to%20cope%0Awith%20the%20potential%20negative%20effects%20of%20generative%20AI%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14244v1&entry.124074799=Read"},
{"title": "Machine Learning Techniques for MRI Data Processing at Expanding Scale", "author": "Taro Langner", "abstract": "  Imaging sites around the world generate growing amounts of medical scan data\nwith ever more versatile and affordable technology. Large-scale studies acquire\nMRI for tens of thousands of participants, together with metadata ranging from\nlifestyle questionnaires to biochemical assays, genetic analyses and more.\nThese large datasets encode substantial information about human health and hold\nconsiderable potential for machine learning training and analysis. This chapter\nexamines ongoing large-scale studies and the challenge of distribution shifts\nbetween them. Transfer learning for overcoming such shifts is discussed,\ntogether with federated learning for safe access to distributed training data\nsecurely held at multiple institutions. Finally, representation learning is\nreviewed as a methodology for encoding embeddings that express abstract\nrelationships in multi-modal input formats.\n", "link": "http://arxiv.org/abs/2404.14326v1", "date": "2024-04-22", "relevancy": 1.9254, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4743}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Techniques%20for%20MRI%20Data%20Processing%20at%20Expanding%20Scale&body=Title%3A%20Machine%20Learning%20Techniques%20for%20MRI%20Data%20Processing%20at%20Expanding%20Scale%0AAuthor%3A%20Taro%20Langner%0AAbstract%3A%20%20%20Imaging%20sites%20around%20the%20world%20generate%20growing%20amounts%20of%20medical%20scan%20data%0Awith%20ever%20more%20versatile%20and%20affordable%20technology.%20Large-scale%20studies%20acquire%0AMRI%20for%20tens%20of%20thousands%20of%20participants%2C%20together%20with%20metadata%20ranging%20from%0Alifestyle%20questionnaires%20to%20biochemical%20assays%2C%20genetic%20analyses%20and%20more.%0AThese%20large%20datasets%20encode%20substantial%20information%20about%20human%20health%20and%20hold%0Aconsiderable%20potential%20for%20machine%20learning%20training%20and%20analysis.%20This%20chapter%0Aexamines%20ongoing%20large-scale%20studies%20and%20the%20challenge%20of%20distribution%20shifts%0Abetween%20them.%20Transfer%20learning%20for%20overcoming%20such%20shifts%20is%20discussed%2C%0Atogether%20with%20federated%20learning%20for%20safe%20access%20to%20distributed%20training%20data%0Asecurely%20held%20at%20multiple%20institutions.%20Finally%2C%20representation%20learning%20is%0Areviewed%20as%20a%20methodology%20for%20encoding%20embeddings%20that%20express%20abstract%0Arelationships%20in%20multi-modal%20input%20formats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14326v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Techniques%20for%20MRI%20Data%20Processing%20at%20Expanding%20Scale&entry.906535625=Taro%20Langner&entry.1292438233=%20%20Imaging%20sites%20around%20the%20world%20generate%20growing%20amounts%20of%20medical%20scan%20data%0Awith%20ever%20more%20versatile%20and%20affordable%20technology.%20Large-scale%20studies%20acquire%0AMRI%20for%20tens%20of%20thousands%20of%20participants%2C%20together%20with%20metadata%20ranging%20from%0Alifestyle%20questionnaires%20to%20biochemical%20assays%2C%20genetic%20analyses%20and%20more.%0AThese%20large%20datasets%20encode%20substantial%20information%20about%20human%20health%20and%20hold%0Aconsiderable%20potential%20for%20machine%20learning%20training%20and%20analysis.%20This%20chapter%0Aexamines%20ongoing%20large-scale%20studies%20and%20the%20challenge%20of%20distribution%20shifts%0Abetween%20them.%20Transfer%20learning%20for%20overcoming%20such%20shifts%20is%20discussed%2C%0Atogether%20with%20federated%20learning%20for%20safe%20access%20to%20distributed%20training%20data%0Asecurely%20held%20at%20multiple%20institutions.%20Finally%2C%20representation%20learning%20is%0Areviewed%20as%20a%20methodology%20for%20encoding%20embeddings%20that%20express%20abstract%0Arelationships%20in%20multi-modal%20input%20formats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14326v1&entry.124074799=Read"},
{"title": "HashPoint: Accelerated Point Searching and Sampling for Neural Rendering", "author": "Jiahao Ma and Miaomiao Liu and David Ahmedt-Aristizaba and Chuong Nguyen", "abstract": "  In this paper, we address the problem of efficient point searching and\nsampling for volume neural rendering. Within this realm, two typical approaches\nare employed: rasterization and ray tracing. The rasterization-based methods\nenable real-time rendering at the cost of increased memory and lower fidelity.\nIn contrast, the ray-tracing-based methods yield superior quality but demand\nlonger rendering time. We solve this problem by our HashPoint method combining\nthese two strategies, leveraging rasterization for efficient point searching\nand sampling, and ray marching for rendering. Our method optimizes point\nsearching by rasterizing points within the camera's view, organizing them in a\nhash table, and facilitating rapid searches. Notably, we accelerate the\nrendering process by adaptive sampling on the primary surface encountered by\nthe ray. Our approach yields substantial speed-up for a range of\nstate-of-the-art ray-tracing-based methods, maintaining equivalent or superior\naccuracy across synthetic and real test datasets. The code will be available at\nhttps://jiahao-ma.github.io/hashpoint/.\n", "link": "http://arxiv.org/abs/2404.14044v1", "date": "2024-04-22", "relevancy": 1.9248, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4777}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4769}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HashPoint%3A%20Accelerated%20Point%20Searching%20and%20Sampling%20for%20Neural%20Rendering&body=Title%3A%20HashPoint%3A%20Accelerated%20Point%20Searching%20and%20Sampling%20for%20Neural%20Rendering%0AAuthor%3A%20Jiahao%20Ma%20and%20Miaomiao%20Liu%20and%20David%20Ahmedt-Aristizaba%20and%20Chuong%20Nguyen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20efficient%20point%20searching%20and%0Asampling%20for%20volume%20neural%20rendering.%20Within%20this%20realm%2C%20two%20typical%20approaches%0Aare%20employed%3A%20rasterization%20and%20ray%20tracing.%20The%20rasterization-based%20methods%0Aenable%20real-time%20rendering%20at%20the%20cost%20of%20increased%20memory%20and%20lower%20fidelity.%0AIn%20contrast%2C%20the%20ray-tracing-based%20methods%20yield%20superior%20quality%20but%20demand%0Alonger%20rendering%20time.%20We%20solve%20this%20problem%20by%20our%20HashPoint%20method%20combining%0Athese%20two%20strategies%2C%20leveraging%20rasterization%20for%20efficient%20point%20searching%0Aand%20sampling%2C%20and%20ray%20marching%20for%20rendering.%20Our%20method%20optimizes%20point%0Asearching%20by%20rasterizing%20points%20within%20the%20camera%27s%20view%2C%20organizing%20them%20in%20a%0Ahash%20table%2C%20and%20facilitating%20rapid%20searches.%20Notably%2C%20we%20accelerate%20the%0Arendering%20process%20by%20adaptive%20sampling%20on%20the%20primary%20surface%20encountered%20by%0Athe%20ray.%20Our%20approach%20yields%20substantial%20speed-up%20for%20a%20range%20of%0Astate-of-the-art%20ray-tracing-based%20methods%2C%20maintaining%20equivalent%20or%20superior%0Aaccuracy%20across%20synthetic%20and%20real%20test%20datasets.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//jiahao-ma.github.io/hashpoint/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14044v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HashPoint%3A%20Accelerated%20Point%20Searching%20and%20Sampling%20for%20Neural%20Rendering&entry.906535625=Jiahao%20Ma%20and%20Miaomiao%20Liu%20and%20David%20Ahmedt-Aristizaba%20and%20Chuong%20Nguyen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20efficient%20point%20searching%20and%0Asampling%20for%20volume%20neural%20rendering.%20Within%20this%20realm%2C%20two%20typical%20approaches%0Aare%20employed%3A%20rasterization%20and%20ray%20tracing.%20The%20rasterization-based%20methods%0Aenable%20real-time%20rendering%20at%20the%20cost%20of%20increased%20memory%20and%20lower%20fidelity.%0AIn%20contrast%2C%20the%20ray-tracing-based%20methods%20yield%20superior%20quality%20but%20demand%0Alonger%20rendering%20time.%20We%20solve%20this%20problem%20by%20our%20HashPoint%20method%20combining%0Athese%20two%20strategies%2C%20leveraging%20rasterization%20for%20efficient%20point%20searching%0Aand%20sampling%2C%20and%20ray%20marching%20for%20rendering.%20Our%20method%20optimizes%20point%0Asearching%20by%20rasterizing%20points%20within%20the%20camera%27s%20view%2C%20organizing%20them%20in%20a%0Ahash%20table%2C%20and%20facilitating%20rapid%20searches.%20Notably%2C%20we%20accelerate%20the%0Arendering%20process%20by%20adaptive%20sampling%20on%20the%20primary%20surface%20encountered%20by%0Athe%20ray.%20Our%20approach%20yields%20substantial%20speed-up%20for%20a%20range%20of%0Astate-of-the-art%20ray-tracing-based%20methods%2C%20maintaining%20equivalent%20or%20superior%0Aaccuracy%20across%20synthetic%20and%20real%20test%20datasets.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//jiahao-ma.github.io/hashpoint/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14044v1&entry.124074799=Read"},
{"title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy\n  Data", "author": "Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar", "abstract": "  Learning from preference labels plays a crucial role in fine-tuning large\nlanguage models. There are several distinct approaches for preference\nfine-tuning, including supervised learning, on-policy reinforcement learning\n(RL), and contrastive learning. Different methods come with different\nimplementation tradeoffs and performance differences, and existing empirical\nfindings present different conclusions, for instance, some results show that\nonline RL is quite important to attain good fine-tuning results, while others\nfind (offline) contrastive or even purely supervised methods sufficient. This\nraises a natural question: what kind of approaches are important for\nfine-tuning with preference data and why? In this paper, we answer this\nquestion by performing a rigorous analysis of a number of fine-tuning\ntechniques on didactic and full-scale LLM problems. Our main finding is that,\nin general, approaches that use on-policy sampling or attempt to push down the\nlikelihood on certain responses (i.e., employ a \"negative gradient\") outperform\noffline and maximum likelihood objectives. We conceptualize our insights and\nunify methods that use on-policy sampling or negative gradient under a notion\nof mode-seeking objectives for categorical distributions. Mode-seeking\nobjectives are able to alter probability mass on specific bins of a categorical\ndistribution at a fast rate compared to maximum likelihood, allowing them to\nrelocate masses across bins more effectively. Our analysis prescribes\nactionable insights for preference fine-tuning of LLMs and informs how data\nshould be collected for maximal improvement.\n", "link": "http://arxiv.org/abs/2404.14367v1", "date": "2024-04-22", "relevancy": 1.9061, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4741}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Preference%20Fine-Tuning%20of%20LLMs%20Should%20Leverage%20Suboptimal%2C%20On-Policy%0A%20%20Data&body=Title%3A%20Preference%20Fine-Tuning%20of%20LLMs%20Should%20Leverage%20Suboptimal%2C%20On-Policy%0A%20%20Data%0AAuthor%3A%20Fahim%20Tajwar%20and%20Anikait%20Singh%20and%20Archit%20Sharma%20and%20Rafael%20Rafailov%20and%20Jeff%20Schneider%20and%20Tengyang%20Xie%20and%20Stefano%20Ermon%20and%20Chelsea%20Finn%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Learning%20from%20preference%20labels%20plays%20a%20crucial%20role%20in%20fine-tuning%20large%0Alanguage%20models.%20There%20are%20several%20distinct%20approaches%20for%20preference%0Afine-tuning%2C%20including%20supervised%20learning%2C%20on-policy%20reinforcement%20learning%0A%28RL%29%2C%20and%20contrastive%20learning.%20Different%20methods%20come%20with%20different%0Aimplementation%20tradeoffs%20and%20performance%20differences%2C%20and%20existing%20empirical%0Afindings%20present%20different%20conclusions%2C%20for%20instance%2C%20some%20results%20show%20that%0Aonline%20RL%20is%20quite%20important%20to%20attain%20good%20fine-tuning%20results%2C%20while%20others%0Afind%20%28offline%29%20contrastive%20or%20even%20purely%20supervised%20methods%20sufficient.%20This%0Araises%20a%20natural%20question%3A%20what%20kind%20of%20approaches%20are%20important%20for%0Afine-tuning%20with%20preference%20data%20and%20why%3F%20In%20this%20paper%2C%20we%20answer%20this%0Aquestion%20by%20performing%20a%20rigorous%20analysis%20of%20a%20number%20of%20fine-tuning%0Atechniques%20on%20didactic%20and%20full-scale%20LLM%20problems.%20Our%20main%20finding%20is%20that%2C%0Ain%20general%2C%20approaches%20that%20use%20on-policy%20sampling%20or%20attempt%20to%20push%20down%20the%0Alikelihood%20on%20certain%20responses%20%28i.e.%2C%20employ%20a%20%22negative%20gradient%22%29%20outperform%0Aoffline%20and%20maximum%20likelihood%20objectives.%20We%20conceptualize%20our%20insights%20and%0Aunify%20methods%20that%20use%20on-policy%20sampling%20or%20negative%20gradient%20under%20a%20notion%0Aof%20mode-seeking%20objectives%20for%20categorical%20distributions.%20Mode-seeking%0Aobjectives%20are%20able%20to%20alter%20probability%20mass%20on%20specific%20bins%20of%20a%20categorical%0Adistribution%20at%20a%20fast%20rate%20compared%20to%20maximum%20likelihood%2C%20allowing%20them%20to%0Arelocate%20masses%20across%20bins%20more%20effectively.%20Our%20analysis%20prescribes%0Aactionable%20insights%20for%20preference%20fine-tuning%20of%20LLMs%20and%20informs%20how%20data%0Ashould%20be%20collected%20for%20maximal%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14367v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference%20Fine-Tuning%20of%20LLMs%20Should%20Leverage%20Suboptimal%2C%20On-Policy%0A%20%20Data&entry.906535625=Fahim%20Tajwar%20and%20Anikait%20Singh%20and%20Archit%20Sharma%20and%20Rafael%20Rafailov%20and%20Jeff%20Schneider%20and%20Tengyang%20Xie%20and%20Stefano%20Ermon%20and%20Chelsea%20Finn%20and%20Aviral%20Kumar&entry.1292438233=%20%20Learning%20from%20preference%20labels%20plays%20a%20crucial%20role%20in%20fine-tuning%20large%0Alanguage%20models.%20There%20are%20several%20distinct%20approaches%20for%20preference%0Afine-tuning%2C%20including%20supervised%20learning%2C%20on-policy%20reinforcement%20learning%0A%28RL%29%2C%20and%20contrastive%20learning.%20Different%20methods%20come%20with%20different%0Aimplementation%20tradeoffs%20and%20performance%20differences%2C%20and%20existing%20empirical%0Afindings%20present%20different%20conclusions%2C%20for%20instance%2C%20some%20results%20show%20that%0Aonline%20RL%20is%20quite%20important%20to%20attain%20good%20fine-tuning%20results%2C%20while%20others%0Afind%20%28offline%29%20contrastive%20or%20even%20purely%20supervised%20methods%20sufficient.%20This%0Araises%20a%20natural%20question%3A%20what%20kind%20of%20approaches%20are%20important%20for%0Afine-tuning%20with%20preference%20data%20and%20why%3F%20In%20this%20paper%2C%20we%20answer%20this%0Aquestion%20by%20performing%20a%20rigorous%20analysis%20of%20a%20number%20of%20fine-tuning%0Atechniques%20on%20didactic%20and%20full-scale%20LLM%20problems.%20Our%20main%20finding%20is%20that%2C%0Ain%20general%2C%20approaches%20that%20use%20on-policy%20sampling%20or%20attempt%20to%20push%20down%20the%0Alikelihood%20on%20certain%20responses%20%28i.e.%2C%20employ%20a%20%22negative%20gradient%22%29%20outperform%0Aoffline%20and%20maximum%20likelihood%20objectives.%20We%20conceptualize%20our%20insights%20and%0Aunify%20methods%20that%20use%20on-policy%20sampling%20or%20negative%20gradient%20under%20a%20notion%0Aof%20mode-seeking%20objectives%20for%20categorical%20distributions.%20Mode-seeking%0Aobjectives%20are%20able%20to%20alter%20probability%20mass%20on%20specific%20bins%20of%20a%20categorical%0Adistribution%20at%20a%20fast%20rate%20compared%20to%20maximum%20likelihood%2C%20allowing%20them%20to%0Arelocate%20masses%20across%20bins%20more%20effectively.%20Our%20analysis%20prescribes%0Aactionable%20insights%20for%20preference%20fine-tuning%20of%20LLMs%20and%20informs%20how%20data%0Ashould%20be%20collected%20for%20maximal%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14367v1&entry.124074799=Read"},
{"title": "SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images\n  using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation", "author": "Hendrik M\u00f6ller and Robert Graf and Joachim Schmitt and Benjamin Keinert and Matan Atad and Anjany Sekuboyina and Felix Streckenbach and Hanna Sch\u00f6n and Florian Kofler and Thomas Kroencke and Stefanie Bette and Stefan Willich and Thomas Keil and Thoralf Niendorf and Tobias Pischon and Beate Endemann and Bjoern Menze and Daniel Rueckert and Jan S. Kirschke", "abstract": "  Purpose. To present SPINEPS, an open-source deep learning approach for\nsemantic and instance segmentation of 14 spinal structures (ten vertebra\nsubstructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in\nwhole body T2w MRI.\n  Methods. During this HIPPA-compliant, retrospective study, we utilized the\npublic SPIDER dataset (218 subjects, 63% female) and a subset of the German\nNational Cohort (1423 subjects, mean age 53, 49% female) for training and\nevaluation. We combined CT and T2w segmentations to train models that segment\n14 spinal structures in T2w sagittal scans both semantically and instance-wise.\nPerformance evaluation metrics included Dice similarity coefficient, average\nsymmetrical surface distance, panoptic quality, segmentation quality, and\nrecognition quality. Statistical significance was assessed using the Wilcoxon\nsigned-rank test. An in-house dataset was used to qualitatively evaluate\nout-of-distribution samples.\n  Results. On the public dataset, our approach outperformed the baseline\n(instance-wise vertebra dice score 0.929 vs. 0.907, p-value<0.001). Training on\nauto-generated annotations and evaluating on manually corrected test data from\nthe GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for\nintervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER\ndataset during training increased these scores to 0.920, 0.967, 0.958,\nrespectively.\n  Conclusions. The proposed segmentation approach offers robust segmentation of\n14 spinal structures in T2w sagittal images, including the spinal cord, spinal\ncanal, intervertebral discs, endplate, sacrum, and vertebrae. The approach\nyields both a semantic and instance mask as output, thus being easy to utilize.\nThis marks the first publicly available algorithm for whole spine segmentation\nin sagittal T2w MR imaging.\n", "link": "http://arxiv.org/abs/2402.16368v2", "date": "2024-04-22", "relevancy": 1.8622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4581}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4387}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPINEPS%20--%20Automatic%20Whole%20Spine%20Segmentation%20of%20T2-weighted%20MR%20images%0A%20%20using%20a%20Two-Phase%20Approach%20to%20Multi-class%20Semantic%20and%20Instance%20Segmentation&body=Title%3A%20SPINEPS%20--%20Automatic%20Whole%20Spine%20Segmentation%20of%20T2-weighted%20MR%20images%0A%20%20using%20a%20Two-Phase%20Approach%20to%20Multi-class%20Semantic%20and%20Instance%20Segmentation%0AAuthor%3A%20Hendrik%20M%C3%B6ller%20and%20Robert%20Graf%20and%20Joachim%20Schmitt%20and%20Benjamin%20Keinert%20and%20Matan%20Atad%20and%20Anjany%20Sekuboyina%20and%20Felix%20Streckenbach%20and%20Hanna%20Sch%C3%B6n%20and%20Florian%20Kofler%20and%20Thomas%20Kroencke%20and%20Stefanie%20Bette%20and%20Stefan%20Willich%20and%20Thomas%20Keil%20and%20Thoralf%20Niendorf%20and%20Tobias%20Pischon%20and%20Beate%20Endemann%20and%20Bjoern%20Menze%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke%0AAbstract%3A%20%20%20Purpose.%20To%20present%20SPINEPS%2C%20an%20open-source%20deep%20learning%20approach%20for%0Asemantic%20and%20instance%20segmentation%20of%2014%20spinal%20structures%20%28ten%20vertebra%0Asubstructures%2C%20intervertebral%20discs%2C%20spinal%20cord%2C%20spinal%20canal%2C%20and%20sacrum%29%20in%0Awhole%20body%20T2w%20MRI.%0A%20%20Methods.%20During%20this%20HIPPA-compliant%2C%20retrospective%20study%2C%20we%20utilized%20the%0Apublic%20SPIDER%20dataset%20%28218%20subjects%2C%2063%25%20female%29%20and%20a%20subset%20of%20the%20German%0ANational%20Cohort%20%281423%20subjects%2C%20mean%20age%2053%2C%2049%25%20female%29%20for%20training%20and%0Aevaluation.%20We%20combined%20CT%20and%20T2w%20segmentations%20to%20train%20models%20that%20segment%0A14%20spinal%20structures%20in%20T2w%20sagittal%20scans%20both%20semantically%20and%20instance-wise.%0APerformance%20evaluation%20metrics%20included%20Dice%20similarity%20coefficient%2C%20average%0Asymmetrical%20surface%20distance%2C%20panoptic%20quality%2C%20segmentation%20quality%2C%20and%0Arecognition%20quality.%20Statistical%20significance%20was%20assessed%20using%20the%20Wilcoxon%0Asigned-rank%20test.%20An%20in-house%20dataset%20was%20used%20to%20qualitatively%20evaluate%0Aout-of-distribution%20samples.%0A%20%20Results.%20On%20the%20public%20dataset%2C%20our%20approach%20outperformed%20the%20baseline%0A%28instance-wise%20vertebra%20dice%20score%200.929%20vs.%200.907%2C%20p-value%3C0.001%29.%20Training%20on%0Aauto-generated%20annotations%20and%20evaluating%20on%20manually%20corrected%20test%20data%20from%0Athe%20GNC%20yielded%20global%20dice%20scores%20of%200.900%20for%20vertebrae%2C%200.960%20for%0Aintervertebral%20discs%2C%20and%200.947%20for%20the%20spinal%20canal.%20Incorporating%20the%20SPIDER%0Adataset%20during%20training%20increased%20these%20scores%20to%200.920%2C%200.967%2C%200.958%2C%0Arespectively.%0A%20%20Conclusions.%20The%20proposed%20segmentation%20approach%20offers%20robust%20segmentation%20of%0A14%20spinal%20structures%20in%20T2w%20sagittal%20images%2C%20including%20the%20spinal%20cord%2C%20spinal%0Acanal%2C%20intervertebral%20discs%2C%20endplate%2C%20sacrum%2C%20and%20vertebrae.%20The%20approach%0Ayields%20both%20a%20semantic%20and%20instance%20mask%20as%20output%2C%20thus%20being%20easy%20to%20utilize.%0AThis%20marks%20the%20first%20publicly%20available%20algorithm%20for%20whole%20spine%20segmentation%0Ain%20sagittal%20T2w%20MR%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16368v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPINEPS%20--%20Automatic%20Whole%20Spine%20Segmentation%20of%20T2-weighted%20MR%20images%0A%20%20using%20a%20Two-Phase%20Approach%20to%20Multi-class%20Semantic%20and%20Instance%20Segmentation&entry.906535625=Hendrik%20M%C3%B6ller%20and%20Robert%20Graf%20and%20Joachim%20Schmitt%20and%20Benjamin%20Keinert%20and%20Matan%20Atad%20and%20Anjany%20Sekuboyina%20and%20Felix%20Streckenbach%20and%20Hanna%20Sch%C3%B6n%20and%20Florian%20Kofler%20and%20Thomas%20Kroencke%20and%20Stefanie%20Bette%20and%20Stefan%20Willich%20and%20Thomas%20Keil%20and%20Thoralf%20Niendorf%20and%20Tobias%20Pischon%20and%20Beate%20Endemann%20and%20Bjoern%20Menze%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke&entry.1292438233=%20%20Purpose.%20To%20present%20SPINEPS%2C%20an%20open-source%20deep%20learning%20approach%20for%0Asemantic%20and%20instance%20segmentation%20of%2014%20spinal%20structures%20%28ten%20vertebra%0Asubstructures%2C%20intervertebral%20discs%2C%20spinal%20cord%2C%20spinal%20canal%2C%20and%20sacrum%29%20in%0Awhole%20body%20T2w%20MRI.%0A%20%20Methods.%20During%20this%20HIPPA-compliant%2C%20retrospective%20study%2C%20we%20utilized%20the%0Apublic%20SPIDER%20dataset%20%28218%20subjects%2C%2063%25%20female%29%20and%20a%20subset%20of%20the%20German%0ANational%20Cohort%20%281423%20subjects%2C%20mean%20age%2053%2C%2049%25%20female%29%20for%20training%20and%0Aevaluation.%20We%20combined%20CT%20and%20T2w%20segmentations%20to%20train%20models%20that%20segment%0A14%20spinal%20structures%20in%20T2w%20sagittal%20scans%20both%20semantically%20and%20instance-wise.%0APerformance%20evaluation%20metrics%20included%20Dice%20similarity%20coefficient%2C%20average%0Asymmetrical%20surface%20distance%2C%20panoptic%20quality%2C%20segmentation%20quality%2C%20and%0Arecognition%20quality.%20Statistical%20significance%20was%20assessed%20using%20the%20Wilcoxon%0Asigned-rank%20test.%20An%20in-house%20dataset%20was%20used%20to%20qualitatively%20evaluate%0Aout-of-distribution%20samples.%0A%20%20Results.%20On%20the%20public%20dataset%2C%20our%20approach%20outperformed%20the%20baseline%0A%28instance-wise%20vertebra%20dice%20score%200.929%20vs.%200.907%2C%20p-value%3C0.001%29.%20Training%20on%0Aauto-generated%20annotations%20and%20evaluating%20on%20manually%20corrected%20test%20data%20from%0Athe%20GNC%20yielded%20global%20dice%20scores%20of%200.900%20for%20vertebrae%2C%200.960%20for%0Aintervertebral%20discs%2C%20and%200.947%20for%20the%20spinal%20canal.%20Incorporating%20the%20SPIDER%0Adataset%20during%20training%20increased%20these%20scores%20to%200.920%2C%200.967%2C%200.958%2C%0Arespectively.%0A%20%20Conclusions.%20The%20proposed%20segmentation%20approach%20offers%20robust%20segmentation%20of%0A14%20spinal%20structures%20in%20T2w%20sagittal%20images%2C%20including%20the%20spinal%20cord%2C%20spinal%0Acanal%2C%20intervertebral%20discs%2C%20endplate%2C%20sacrum%2C%20and%20vertebrae.%20The%20approach%0Ayields%20both%20a%20semantic%20and%20instance%20mask%20as%20output%2C%20thus%20being%20easy%20to%20utilize.%0AThis%20marks%20the%20first%20publicly%20available%20algorithm%20for%20whole%20spine%20segmentation%0Ain%20sagittal%20T2w%20MR%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16368v2&entry.124074799=Read"},
{"title": "GeoAI Reproducibility and Replicability: a computational and spatial\n  perspective", "author": "Wenwen Li and Chia-Yu Hsu and Sizhe Wang and Peter Kedron", "abstract": "  GeoAI has emerged as an exciting interdisciplinary research area that\ncombines spatial theories and data with cutting-edge AI models to address\ngeospatial problems in a novel, data-driven manner. While GeoAI research has\nflourished in the GIScience literature, its reproducibility and replicability\n(R&R), fundamental principles that determine the reusability, reliability, and\nscientific rigor of research findings, have rarely been discussed. This paper\naims to provide an in-depth analysis of this topic from both computational and\nspatial perspectives. We first categorize the major goals for reproducing GeoAI\nresearch, namely, validation (repeatability), learning and adapting the method\nfor solving a similar or new problem (reproducibility), and examining the\ngeneralizability of the research findings (replicability). Each of these goals\nrequires different levels of understanding of GeoAI, as well as different\nmethods to ensure its success. We then discuss the factors that may cause the\nlack of R&R in GeoAI research, with an emphasis on (1) the selection and use of\ntraining data; (2) the uncertainty that resides in the GeoAI model design,\ntraining, deployment, and inference processes; and more importantly (3) the\ninherent spatial heterogeneity of geospatial data and processes. We use a deep\nlearning-based image analysis task as an example to demonstrate the results'\nuncertainty and spatial variance caused by different factors. The findings\nreiterate the importance of knowledge sharing, as well as the generation of a\n\"replicability map\" that incorporates spatial autocorrelation and spatial\nheterogeneity into consideration in quantifying the spatial replicability of\nGeoAI research.\n", "link": "http://arxiv.org/abs/2404.10108v2", "date": "2024-04-22", "relevancy": 1.862, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GeoAI%20Reproducibility%20and%20Replicability%3A%20a%20computational%20and%20spatial%0A%20%20perspective&body=Title%3A%20GeoAI%20Reproducibility%20and%20Replicability%3A%20a%20computational%20and%20spatial%0A%20%20perspective%0AAuthor%3A%20Wenwen%20Li%20and%20Chia-Yu%20Hsu%20and%20Sizhe%20Wang%20and%20Peter%20Kedron%0AAbstract%3A%20%20%20GeoAI%20has%20emerged%20as%20an%20exciting%20interdisciplinary%20research%20area%20that%0Acombines%20spatial%20theories%20and%20data%20with%20cutting-edge%20AI%20models%20to%20address%0Ageospatial%20problems%20in%20a%20novel%2C%20data-driven%20manner.%20While%20GeoAI%20research%20has%0Aflourished%20in%20the%20GIScience%20literature%2C%20its%20reproducibility%20and%20replicability%0A%28R%26R%29%2C%20fundamental%20principles%20that%20determine%20the%20reusability%2C%20reliability%2C%20and%0Ascientific%20rigor%20of%20research%20findings%2C%20have%20rarely%20been%20discussed.%20This%20paper%0Aaims%20to%20provide%20an%20in-depth%20analysis%20of%20this%20topic%20from%20both%20computational%20and%0Aspatial%20perspectives.%20We%20first%20categorize%20the%20major%20goals%20for%20reproducing%20GeoAI%0Aresearch%2C%20namely%2C%20validation%20%28repeatability%29%2C%20learning%20and%20adapting%20the%20method%0Afor%20solving%20a%20similar%20or%20new%20problem%20%28reproducibility%29%2C%20and%20examining%20the%0Ageneralizability%20of%20the%20research%20findings%20%28replicability%29.%20Each%20of%20these%20goals%0Arequires%20different%20levels%20of%20understanding%20of%20GeoAI%2C%20as%20well%20as%20different%0Amethods%20to%20ensure%20its%20success.%20We%20then%20discuss%20the%20factors%20that%20may%20cause%20the%0Alack%20of%20R%26R%20in%20GeoAI%20research%2C%20with%20an%20emphasis%20on%20%281%29%20the%20selection%20and%20use%20of%0Atraining%20data%3B%20%282%29%20the%20uncertainty%20that%20resides%20in%20the%20GeoAI%20model%20design%2C%0Atraining%2C%20deployment%2C%20and%20inference%20processes%3B%20and%20more%20importantly%20%283%29%20the%0Ainherent%20spatial%20heterogeneity%20of%20geospatial%20data%20and%20processes.%20We%20use%20a%20deep%0Alearning-based%20image%20analysis%20task%20as%20an%20example%20to%20demonstrate%20the%20results%27%0Auncertainty%20and%20spatial%20variance%20caused%20by%20different%20factors.%20The%20findings%0Areiterate%20the%20importance%20of%20knowledge%20sharing%2C%20as%20well%20as%20the%20generation%20of%20a%0A%22replicability%20map%22%20that%20incorporates%20spatial%20autocorrelation%20and%20spatial%0Aheterogeneity%20into%20consideration%20in%20quantifying%20the%20spatial%20replicability%20of%0AGeoAI%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10108v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoAI%20Reproducibility%20and%20Replicability%3A%20a%20computational%20and%20spatial%0A%20%20perspective&entry.906535625=Wenwen%20Li%20and%20Chia-Yu%20Hsu%20and%20Sizhe%20Wang%20and%20Peter%20Kedron&entry.1292438233=%20%20GeoAI%20has%20emerged%20as%20an%20exciting%20interdisciplinary%20research%20area%20that%0Acombines%20spatial%20theories%20and%20data%20with%20cutting-edge%20AI%20models%20to%20address%0Ageospatial%20problems%20in%20a%20novel%2C%20data-driven%20manner.%20While%20GeoAI%20research%20has%0Aflourished%20in%20the%20GIScience%20literature%2C%20its%20reproducibility%20and%20replicability%0A%28R%26R%29%2C%20fundamental%20principles%20that%20determine%20the%20reusability%2C%20reliability%2C%20and%0Ascientific%20rigor%20of%20research%20findings%2C%20have%20rarely%20been%20discussed.%20This%20paper%0Aaims%20to%20provide%20an%20in-depth%20analysis%20of%20this%20topic%20from%20both%20computational%20and%0Aspatial%20perspectives.%20We%20first%20categorize%20the%20major%20goals%20for%20reproducing%20GeoAI%0Aresearch%2C%20namely%2C%20validation%20%28repeatability%29%2C%20learning%20and%20adapting%20the%20method%0Afor%20solving%20a%20similar%20or%20new%20problem%20%28reproducibility%29%2C%20and%20examining%20the%0Ageneralizability%20of%20the%20research%20findings%20%28replicability%29.%20Each%20of%20these%20goals%0Arequires%20different%20levels%20of%20understanding%20of%20GeoAI%2C%20as%20well%20as%20different%0Amethods%20to%20ensure%20its%20success.%20We%20then%20discuss%20the%20factors%20that%20may%20cause%20the%0Alack%20of%20R%26R%20in%20GeoAI%20research%2C%20with%20an%20emphasis%20on%20%281%29%20the%20selection%20and%20use%20of%0Atraining%20data%3B%20%282%29%20the%20uncertainty%20that%20resides%20in%20the%20GeoAI%20model%20design%2C%0Atraining%2C%20deployment%2C%20and%20inference%20processes%3B%20and%20more%20importantly%20%283%29%20the%0Ainherent%20spatial%20heterogeneity%20of%20geospatial%20data%20and%20processes.%20We%20use%20a%20deep%0Alearning-based%20image%20analysis%20task%20as%20an%20example%20to%20demonstrate%20the%20results%27%0Auncertainty%20and%20spatial%20variance%20caused%20by%20different%20factors.%20The%20findings%0Areiterate%20the%20importance%20of%20knowledge%20sharing%2C%20as%20well%20as%20the%20generation%20of%20a%0A%22replicability%20map%22%20that%20incorporates%20spatial%20autocorrelation%20and%20spatial%0Aheterogeneity%20into%20consideration%20in%20quantifying%20the%20spatial%20replicability%20of%0AGeoAI%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10108v2&entry.124074799=Read"},
{"title": "A Survey on Self-Evolution of Large Language Models", "author": "Zhengwei Tao and Ting-En Lin and Xiancai Chen and Hangyu Li and Yuchuan Wu and Yongbin Li and Zhi Jin and Fei Huang and Dacheng Tao and Jingren Zhou", "abstract": "  Large language models (LLMs) have significantly advanced in various fields\nand intelligent agent applications. However, current LLMs that learn from human\nor external model supervision are costly and may face performance ceilings as\ntask complexity and diversity increase. To address this issue, self-evolution\napproaches that enable LLM to autonomously acquire, refine, and learn from\nexperiences generated by the model itself are rapidly growing. This new\ntraining paradigm inspired by the human experiential learning process offers\nthe potential to scale LLMs towards superintelligence. In this work, we present\na comprehensive survey of self-evolution approaches in LLMs. We first propose a\nconceptual framework for self-evolution and outline the evolving process as\niterative cycles composed of four phases: experience acquisition, experience\nrefinement, updating, and evaluation. Second, we categorize the evolution\nobjectives of LLMs and LLM-based agents; then, we summarize the literature and\nprovide taxonomy and insights for each module. Lastly, we pinpoint existing\nchallenges and propose future directions to improve self-evolution frameworks,\nequipping researchers with critical insights to fast-track the development of\nself-evolving LLMs.\n", "link": "http://arxiv.org/abs/2404.14387v1", "date": "2024-04-22", "relevancy": 1.8534, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.468}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4607}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4582}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models%0AAuthor%3A%20Zhengwei%20Tao%20and%20Ting-En%20Lin%20and%20Xiancai%20Chen%20and%20Hangyu%20Li%20and%20Yuchuan%20Wu%20and%20Yongbin%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Dacheng%20Tao%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20in%20various%20fields%0Aand%20intelligent%20agent%20applications.%20However%2C%20current%20LLMs%20that%20learn%20from%20human%0Aor%20external%20model%20supervision%20are%20costly%20and%20may%20face%20performance%20ceilings%20as%0Atask%20complexity%20and%20diversity%20increase.%20To%20address%20this%20issue%2C%20self-evolution%0Aapproaches%20that%20enable%20LLM%20to%20autonomously%20acquire%2C%20refine%2C%20and%20learn%20from%0Aexperiences%20generated%20by%20the%20model%20itself%20are%20rapidly%20growing.%20This%20new%0Atraining%20paradigm%20inspired%20by%20the%20human%20experiential%20learning%20process%20offers%0Athe%20potential%20to%20scale%20LLMs%20towards%20superintelligence.%20In%20this%20work%2C%20we%20present%0Aa%20comprehensive%20survey%20of%20self-evolution%20approaches%20in%20LLMs.%20We%20first%20propose%20a%0Aconceptual%20framework%20for%20self-evolution%20and%20outline%20the%20evolving%20process%20as%0Aiterative%20cycles%20composed%20of%20four%20phases%3A%20experience%20acquisition%2C%20experience%0Arefinement%2C%20updating%2C%20and%20evaluation.%20Second%2C%20we%20categorize%20the%20evolution%0Aobjectives%20of%20LLMs%20and%20LLM-based%20agents%3B%20then%2C%20we%20summarize%20the%20literature%20and%0Aprovide%20taxonomy%20and%20insights%20for%20each%20module.%20Lastly%2C%20we%20pinpoint%20existing%0Achallenges%20and%20propose%20future%20directions%20to%20improve%20self-evolution%20frameworks%2C%0Aequipping%20researchers%20with%20critical%20insights%20to%20fast-track%20the%20development%20of%0Aself-evolving%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14387v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Self-Evolution%20of%20Large%20Language%20Models&entry.906535625=Zhengwei%20Tao%20and%20Ting-En%20Lin%20and%20Xiancai%20Chen%20and%20Hangyu%20Li%20and%20Yuchuan%20Wu%20and%20Yongbin%20Li%20and%20Zhi%20Jin%20and%20Fei%20Huang%20and%20Dacheng%20Tao%20and%20Jingren%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20in%20various%20fields%0Aand%20intelligent%20agent%20applications.%20However%2C%20current%20LLMs%20that%20learn%20from%20human%0Aor%20external%20model%20supervision%20are%20costly%20and%20may%20face%20performance%20ceilings%20as%0Atask%20complexity%20and%20diversity%20increase.%20To%20address%20this%20issue%2C%20self-evolution%0Aapproaches%20that%20enable%20LLM%20to%20autonomously%20acquire%2C%20refine%2C%20and%20learn%20from%0Aexperiences%20generated%20by%20the%20model%20itself%20are%20rapidly%20growing.%20This%20new%0Atraining%20paradigm%20inspired%20by%20the%20human%20experiential%20learning%20process%20offers%0Athe%20potential%20to%20scale%20LLMs%20towards%20superintelligence.%20In%20this%20work%2C%20we%20present%0Aa%20comprehensive%20survey%20of%20self-evolution%20approaches%20in%20LLMs.%20We%20first%20propose%20a%0Aconceptual%20framework%20for%20self-evolution%20and%20outline%20the%20evolving%20process%20as%0Aiterative%20cycles%20composed%20of%20four%20phases%3A%20experience%20acquisition%2C%20experience%0Arefinement%2C%20updating%2C%20and%20evaluation.%20Second%2C%20we%20categorize%20the%20evolution%0Aobjectives%20of%20LLMs%20and%20LLM-based%20agents%3B%20then%2C%20we%20summarize%20the%20literature%20and%0Aprovide%20taxonomy%20and%20insights%20for%20each%20module.%20Lastly%2C%20we%20pinpoint%20existing%0Achallenges%20and%20propose%20future%20directions%20to%20improve%20self-evolution%20frameworks%2C%0Aequipping%20researchers%20with%20critical%20insights%20to%20fast-track%20the%20development%20of%0Aself-evolving%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14387v1&entry.124074799=Read"},
{"title": "Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting\n  and Cognitive Load on User Attention and Saliency Prediction", "author": "Anwesha Das and Zekun Wu and Iza \u0160krjanec and Anna Maria Feit", "abstract": "  Visual highlighting can guide user attention in complex interfaces. However,\nits effectiveness under limited attentional capacities is underexplored. This\npaper examines the joint impact of visual highlighting (permanent and dynamic)\nand dual-task-induced cognitive load on gaze behaviour. Our analysis, using\neye-movement data from 27 participants viewing 150 unique webpages reveals that\nwhile participants' ability to attend to UI elements decreases with increasing\ncognitive load, dynamic adaptations (i.e., highlighting) remain\nattention-grabbing. The presence of these factors significantly alters what\npeople attend to and thus what is salient. Accordingly, we show that\nstate-of-the-art saliency models increase their performance when accounting for\ndifferent cognitive loads. Our empirical insights, along with our openly\navailable dataset, enhance our understanding of attentional processes in UIs\nunder varying cognitive (and perceptual) loads and open the door for new models\nthat can predict user attention while multitasking.\n", "link": "http://arxiv.org/abs/2404.14232v1", "date": "2024-04-22", "relevancy": 1.8451, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4879}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4601}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction&body=Title%3A%20Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction%0AAuthor%3A%20Anwesha%20Das%20and%20Zekun%20Wu%20and%20Iza%20%C5%A0krjanec%20and%20Anna%20Maria%20Feit%0AAbstract%3A%20%20%20Visual%20highlighting%20can%20guide%20user%20attention%20in%20complex%20interfaces.%20However%2C%0Aits%20effectiveness%20under%20limited%20attentional%20capacities%20is%20underexplored.%20This%0Apaper%20examines%20the%20joint%20impact%20of%20visual%20highlighting%20%28permanent%20and%20dynamic%29%0Aand%20dual-task-induced%20cognitive%20load%20on%20gaze%20behaviour.%20Our%20analysis%2C%20using%0Aeye-movement%20data%20from%2027%20participants%20viewing%20150%20unique%20webpages%20reveals%20that%0Awhile%20participants%27%20ability%20to%20attend%20to%20UI%20elements%20decreases%20with%20increasing%0Acognitive%20load%2C%20dynamic%20adaptations%20%28i.e.%2C%20highlighting%29%20remain%0Aattention-grabbing.%20The%20presence%20of%20these%20factors%20significantly%20alters%20what%0Apeople%20attend%20to%20and%20thus%20what%20is%20salient.%20Accordingly%2C%20we%20show%20that%0Astate-of-the-art%20saliency%20models%20increase%20their%20performance%20when%20accounting%20for%0Adifferent%20cognitive%20loads.%20Our%20empirical%20insights%2C%20along%20with%20our%20openly%0Aavailable%20dataset%2C%20enhance%20our%20understanding%20of%20attentional%20processes%20in%20UIs%0Aunder%20varying%20cognitive%20%28and%20perceptual%29%20loads%20and%20open%20the%20door%20for%20new%20models%0Athat%20can%20predict%20user%20attention%20while%20multitasking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14232v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction&entry.906535625=Anwesha%20Das%20and%20Zekun%20Wu%20and%20Iza%20%C5%A0krjanec%20and%20Anna%20Maria%20Feit&entry.1292438233=%20%20Visual%20highlighting%20can%20guide%20user%20attention%20in%20complex%20interfaces.%20However%2C%0Aits%20effectiveness%20under%20limited%20attentional%20capacities%20is%20underexplored.%20This%0Apaper%20examines%20the%20joint%20impact%20of%20visual%20highlighting%20%28permanent%20and%20dynamic%29%0Aand%20dual-task-induced%20cognitive%20load%20on%20gaze%20behaviour.%20Our%20analysis%2C%20using%0Aeye-movement%20data%20from%2027%20participants%20viewing%20150%20unique%20webpages%20reveals%20that%0Awhile%20participants%27%20ability%20to%20attend%20to%20UI%20elements%20decreases%20with%20increasing%0Acognitive%20load%2C%20dynamic%20adaptations%20%28i.e.%2C%20highlighting%29%20remain%0Aattention-grabbing.%20The%20presence%20of%20these%20factors%20significantly%20alters%20what%0Apeople%20attend%20to%20and%20thus%20what%20is%20salient.%20Accordingly%2C%20we%20show%20that%0Astate-of-the-art%20saliency%20models%20increase%20their%20performance%20when%20accounting%20for%0Adifferent%20cognitive%20loads.%20Our%20empirical%20insights%2C%20along%20with%20our%20openly%0Aavailable%20dataset%2C%20enhance%20our%20understanding%20of%20attentional%20processes%20in%20UIs%0Aunder%20varying%20cognitive%20%28and%20perceptual%29%20loads%20and%20open%20the%20door%20for%20new%20models%0Athat%20can%20predict%20user%20attention%20while%20multitasking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14232v1&entry.124074799=Read"},
{"title": "Structure-preserving neural networks for the regularzied entropy-based\n  closure of the Boltzmann moment system", "author": "Steffen Schotth\u00f6fer and M. Paul Laiu and Martin Frank and Cory D. Hauck", "abstract": "  The main challenge of large-scale numerical simulation of radiation transport\nis the high memory and computation time requirements of discretization methods\nfor kinetic equations. In this work, we derive and investigate a neural\nnetwork-based approximation to the entropy closure method to accurately compute\nthe solution of the multi-dimensional moment system with a low memory footprint\nand competitive computational time. We extend methods developed for the\nstandard entropy-based closure to the context of regularized entropy-based\nclosures. The main idea is to interpret structure-preserving neural network\napproximations of the regularized entropy closure as a two-stage approximation\nto the original entropy closure. We conduct a numerical analysis of this\napproximation and investigate optimal parameter choices. Our numerical\nexperiments demonstrate that the method has a much lower memory footprint than\ntraditional methods with competitive computation times and simulation accuracy.\nThe code and all trained networks are provided on\nGitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}.\n", "link": "http://arxiv.org/abs/2404.14312v1", "date": "2024-04-22", "relevancy": 1.8368, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4581}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20neural%20networks%20for%20the%20regularzied%20entropy-based%0A%20%20closure%20of%20the%20Boltzmann%20moment%20system&body=Title%3A%20Structure-preserving%20neural%20networks%20for%20the%20regularzied%20entropy-based%0A%20%20closure%20of%20the%20Boltzmann%20moment%20system%0AAuthor%3A%20Steffen%20Schotth%C3%B6fer%20and%20M.%20Paul%20Laiu%20and%20Martin%20Frank%20and%20Cory%20D.%20Hauck%0AAbstract%3A%20%20%20The%20main%20challenge%20of%20large-scale%20numerical%20simulation%20of%20radiation%20transport%0Ais%20the%20high%20memory%20and%20computation%20time%20requirements%20of%20discretization%20methods%0Afor%20kinetic%20equations.%20In%20this%20work%2C%20we%20derive%20and%20investigate%20a%20neural%0Anetwork-based%20approximation%20to%20the%20entropy%20closure%20method%20to%20accurately%20compute%0Athe%20solution%20of%20the%20multi-dimensional%20moment%20system%20with%20a%20low%20memory%20footprint%0Aand%20competitive%20computational%20time.%20We%20extend%20methods%20developed%20for%20the%0Astandard%20entropy-based%20closure%20to%20the%20context%20of%20regularized%20entropy-based%0Aclosures.%20The%20main%20idea%20is%20to%20interpret%20structure-preserving%20neural%20network%0Aapproximations%20of%20the%20regularized%20entropy%20closure%20as%20a%20two-stage%20approximation%0Ato%20the%20original%20entropy%20closure.%20We%20conduct%20a%20numerical%20analysis%20of%20this%0Aapproximation%20and%20investigate%20optimal%20parameter%20choices.%20Our%20numerical%0Aexperiments%20demonstrate%20that%20the%20method%20has%20a%20much%20lower%20memory%20footprint%20than%0Atraditional%20methods%20with%20competitive%20computation%20times%20and%20simulation%20accuracy.%0AThe%20code%20and%20all%20trained%20networks%20are%20provided%20on%0AGitHub%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/ScSteffen/neuralEntropyClosures%7D%7D%24%5E%2C%24%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/CSMMLab/KiT-RT%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14312v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20neural%20networks%20for%20the%20regularzied%20entropy-based%0A%20%20closure%20of%20the%20Boltzmann%20moment%20system&entry.906535625=Steffen%20Schotth%C3%B6fer%20and%20M.%20Paul%20Laiu%20and%20Martin%20Frank%20and%20Cory%20D.%20Hauck&entry.1292438233=%20%20The%20main%20challenge%20of%20large-scale%20numerical%20simulation%20of%20radiation%20transport%0Ais%20the%20high%20memory%20and%20computation%20time%20requirements%20of%20discretization%20methods%0Afor%20kinetic%20equations.%20In%20this%20work%2C%20we%20derive%20and%20investigate%20a%20neural%0Anetwork-based%20approximation%20to%20the%20entropy%20closure%20method%20to%20accurately%20compute%0Athe%20solution%20of%20the%20multi-dimensional%20moment%20system%20with%20a%20low%20memory%20footprint%0Aand%20competitive%20computational%20time.%20We%20extend%20methods%20developed%20for%20the%0Astandard%20entropy-based%20closure%20to%20the%20context%20of%20regularized%20entropy-based%0Aclosures.%20The%20main%20idea%20is%20to%20interpret%20structure-preserving%20neural%20network%0Aapproximations%20of%20the%20regularized%20entropy%20closure%20as%20a%20two-stage%20approximation%0Ato%20the%20original%20entropy%20closure.%20We%20conduct%20a%20numerical%20analysis%20of%20this%0Aapproximation%20and%20investigate%20optimal%20parameter%20choices.%20Our%20numerical%0Aexperiments%20demonstrate%20that%20the%20method%20has%20a%20much%20lower%20memory%20footprint%20than%0Atraditional%20methods%20with%20competitive%20computation%20times%20and%20simulation%20accuracy.%0AThe%20code%20and%20all%20trained%20networks%20are%20provided%20on%0AGitHub%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/ScSteffen/neuralEntropyClosures%7D%7D%24%5E%2C%24%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/CSMMLab/KiT-RT%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14312v1&entry.124074799=Read"},
{"title": "PARAMANU-GANITA: Language Model with Mathematical Capabilities", "author": "Mitodru Niyogi and Arnab Bhattacharya", "abstract": "  In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto\nRegressive (AR) decoder based language model on mathematics. The model is\npretrained from scratch at context size of 4096 on our curated mixed\nmathematical corpus. We evaluate our model on both perplexity metric and GSM8k\nmathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B\nLLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2\n7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and\nmath specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0%\npoints in GSM8k test accuracy metric respectively. Paramanu-Ganita also\noutperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8%\npoints, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively.\nThe large significant margin improvement in performance of our math model over\nthe existing LLMs signifies that reasoning capabilities of language model are\njust not restricted to LLMs with humongous number of parameters.\nParamanu-Ganita took 146 hours of A100 training whereas math specialised LLM,\nLLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our\napproach of pretraining powerful domain specialised language models from\nscratch for domain adaptation is much more cost-effective than performing\ncontinual training of LLMs for domain adaptation. Hence, we conclude that for\nstrong mathematical reasoning abilities of language model, we do not need giant\nLLMs and immense computing power to our end. In the end, we want to point out\nthat we have only trained Paramanu-Ganita only on a part of our entire\nmathematical corpus and yet to explore the full potential of our model.\n", "link": "http://arxiv.org/abs/2404.14395v1", "date": "2024-04-22", "relevancy": 1.8328, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PARAMANU-GANITA%3A%20Language%20Model%20with%20Mathematical%20Capabilities&body=Title%3A%20PARAMANU-GANITA%3A%20Language%20Model%20with%20Mathematical%20Capabilities%0AAuthor%3A%20Mitodru%20Niyogi%20and%20Arnab%20Bhattacharya%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20Paramanu-Ganita%2C%20a%20208%20million%20parameter%20novel%20Auto%0ARegressive%20%28AR%29%20decoder%20based%20language%20model%20on%20mathematics.%20The%20model%20is%0Apretrained%20from%20scratch%20at%20context%20size%20of%204096%20on%20our%20curated%20mixed%0Amathematical%20corpus.%20We%20evaluate%20our%20model%20on%20both%20perplexity%20metric%20and%20GSM8k%0Amathematical%20benchmark.%20Paramanu-Ganita%20despite%20being%2035%20times%20smaller%20than%207B%0ALLMs%2C%20outperformed%20generalist%20LLMs%20such%20as%20LLaMa-1%207B%20by%2028.4%25%20points%2C%20LLaMa-2%0A7B%20by%2027.6%25%20points%2C%20Falcon%207B%20by%2032.6%25%20points%2C%20PaLM%208B%20by%2035.3%25%20points%2C%20and%0Amath%20specialised%20LLMs%20such%20as%20Minerva%208B%20by%2023.2%25%20points%2C%20and%20LLEMMA-7B%20by%203.0%25%0Apoints%20in%20GSM8k%20test%20accuracy%20metric%20respectively.%20Paramanu-Ganita%20also%0Aoutperformed%20giant%20LLMs%20like%20PaLM%2062B%20by%206.4%25%20points%2C%20Falcon%2040B%20by%2019.8%25%0Apoints%2C%20LLaMa-1%2033B%20by%203.8%25%20points%20and%20Vicuna%2013B%20by%2011.8%25%20points%20respectively.%0AThe%20large%20significant%20margin%20improvement%20in%20performance%20of%20our%20math%20model%20over%0Athe%20existing%20LLMs%20signifies%20that%20reasoning%20capabilities%20of%20language%20model%20are%0Ajust%20not%20restricted%20to%20LLMs%20with%20humongous%20number%20of%20parameters.%0AParamanu-Ganita%20took%20146%20hours%20of%20A100%20training%20whereas%20math%20specialised%20LLM%2C%0ALLEMMA%207B%2C%20was%20trained%20for%2023%2C000%20A100%20hours%20of%20training%20equivalent.%20Thus%2C%20our%0Aapproach%20of%20pretraining%20powerful%20domain%20specialised%20language%20models%20from%0Ascratch%20for%20domain%20adaptation%20is%20much%20more%20cost-effective%20than%20performing%0Acontinual%20training%20of%20LLMs%20for%20domain%20adaptation.%20Hence%2C%20we%20conclude%20that%20for%0Astrong%20mathematical%20reasoning%20abilities%20of%20language%20model%2C%20we%20do%20not%20need%20giant%0ALLMs%20and%20immense%20computing%20power%20to%20our%20end.%20In%20the%20end%2C%20we%20want%20to%20point%20out%0Athat%20we%20have%20only%20trained%20Paramanu-Ganita%20only%20on%20a%20part%20of%20our%20entire%0Amathematical%20corpus%20and%20yet%20to%20explore%20the%20full%20potential%20of%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14395v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARAMANU-GANITA%3A%20Language%20Model%20with%20Mathematical%20Capabilities&entry.906535625=Mitodru%20Niyogi%20and%20Arnab%20Bhattacharya&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20Paramanu-Ganita%2C%20a%20208%20million%20parameter%20novel%20Auto%0ARegressive%20%28AR%29%20decoder%20based%20language%20model%20on%20mathematics.%20The%20model%20is%0Apretrained%20from%20scratch%20at%20context%20size%20of%204096%20on%20our%20curated%20mixed%0Amathematical%20corpus.%20We%20evaluate%20our%20model%20on%20both%20perplexity%20metric%20and%20GSM8k%0Amathematical%20benchmark.%20Paramanu-Ganita%20despite%20being%2035%20times%20smaller%20than%207B%0ALLMs%2C%20outperformed%20generalist%20LLMs%20such%20as%20LLaMa-1%207B%20by%2028.4%25%20points%2C%20LLaMa-2%0A7B%20by%2027.6%25%20points%2C%20Falcon%207B%20by%2032.6%25%20points%2C%20PaLM%208B%20by%2035.3%25%20points%2C%20and%0Amath%20specialised%20LLMs%20such%20as%20Minerva%208B%20by%2023.2%25%20points%2C%20and%20LLEMMA-7B%20by%203.0%25%0Apoints%20in%20GSM8k%20test%20accuracy%20metric%20respectively.%20Paramanu-Ganita%20also%0Aoutperformed%20giant%20LLMs%20like%20PaLM%2062B%20by%206.4%25%20points%2C%20Falcon%2040B%20by%2019.8%25%0Apoints%2C%20LLaMa-1%2033B%20by%203.8%25%20points%20and%20Vicuna%2013B%20by%2011.8%25%20points%20respectively.%0AThe%20large%20significant%20margin%20improvement%20in%20performance%20of%20our%20math%20model%20over%0Athe%20existing%20LLMs%20signifies%20that%20reasoning%20capabilities%20of%20language%20model%20are%0Ajust%20not%20restricted%20to%20LLMs%20with%20humongous%20number%20of%20parameters.%0AParamanu-Ganita%20took%20146%20hours%20of%20A100%20training%20whereas%20math%20specialised%20LLM%2C%0ALLEMMA%207B%2C%20was%20trained%20for%2023%2C000%20A100%20hours%20of%20training%20equivalent.%20Thus%2C%20our%0Aapproach%20of%20pretraining%20powerful%20domain%20specialised%20language%20models%20from%0Ascratch%20for%20domain%20adaptation%20is%20much%20more%20cost-effective%20than%20performing%0Acontinual%20training%20of%20LLMs%20for%20domain%20adaptation.%20Hence%2C%20we%20conclude%20that%20for%0Astrong%20mathematical%20reasoning%20abilities%20of%20language%20model%2C%20we%20do%20not%20need%20giant%0ALLMs%20and%20immense%20computing%20power%20to%20our%20end.%20In%20the%20end%2C%20we%20want%20to%20point%20out%0Athat%20we%20have%20only%20trained%20Paramanu-Ganita%20only%20on%20a%20part%20of%20our%20entire%0Amathematical%20corpus%20and%20yet%20to%20explore%20the%20full%20potential%20of%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14395v1&entry.124074799=Read"},
{"title": "Two-timescale Extragradient for Finding Local Minimax Points", "author": "Jiseok Chae and Kyuwon Kim and Donghwan Kim", "abstract": "  Minimax problems are notoriously challenging to optimize. However, we present\nthat the two-timescale extragradient method can be a viable solution. By\nutilizing dynamical systems theory, we show that it converges to points that\nsatisfy the second-order necessary condition of local minimax points, under\nmild conditions that the two-timescale gradient descent ascent fails to work.\nThis work provably improves upon all previous results on finding local minimax\npoints, by eliminating a crucial assumption that the Hessian with respect to\nthe maximization variable is nondegenerate.\n", "link": "http://arxiv.org/abs/2305.16242v2", "date": "2024-04-22", "relevancy": 1.8209, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4604}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4458}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two-timescale%20Extragradient%20for%20Finding%20Local%20Minimax%20Points&body=Title%3A%20Two-timescale%20Extragradient%20for%20Finding%20Local%20Minimax%20Points%0AAuthor%3A%20Jiseok%20Chae%20and%20Kyuwon%20Kim%20and%20Donghwan%20Kim%0AAbstract%3A%20%20%20Minimax%20problems%20are%20notoriously%20challenging%20to%20optimize.%20However%2C%20we%20present%0Athat%20the%20two-timescale%20extragradient%20method%20can%20be%20a%20viable%20solution.%20By%0Autilizing%20dynamical%20systems%20theory%2C%20we%20show%20that%20it%20converges%20to%20points%20that%0Asatisfy%20the%20second-order%20necessary%20condition%20of%20local%20minimax%20points%2C%20under%0Amild%20conditions%20that%20the%20two-timescale%20gradient%20descent%20ascent%20fails%20to%20work.%0AThis%20work%20provably%20improves%20upon%20all%20previous%20results%20on%20finding%20local%20minimax%0Apoints%2C%20by%20eliminating%20a%20crucial%20assumption%20that%20the%20Hessian%20with%20respect%20to%0Athe%20maximization%20variable%20is%20nondegenerate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16242v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-timescale%20Extragradient%20for%20Finding%20Local%20Minimax%20Points&entry.906535625=Jiseok%20Chae%20and%20Kyuwon%20Kim%20and%20Donghwan%20Kim&entry.1292438233=%20%20Minimax%20problems%20are%20notoriously%20challenging%20to%20optimize.%20However%2C%20we%20present%0Athat%20the%20two-timescale%20extragradient%20method%20can%20be%20a%20viable%20solution.%20By%0Autilizing%20dynamical%20systems%20theory%2C%20we%20show%20that%20it%20converges%20to%20points%20that%0Asatisfy%20the%20second-order%20necessary%20condition%20of%20local%20minimax%20points%2C%20under%0Amild%20conditions%20that%20the%20two-timescale%20gradient%20descent%20ascent%20fails%20to%20work.%0AThis%20work%20provably%20improves%20upon%20all%20previous%20results%20on%20finding%20local%20minimax%0Apoints%2C%20by%20eliminating%20a%20crucial%20assumption%20that%20the%20Hessian%20with%20respect%20to%0Athe%20maximization%20variable%20is%20nondegenerate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16242v2&entry.124074799=Read"},
{"title": "A mean curvature flow arising in adversarial training", "author": "Leon Bungert and Tim Laux and Kerrek Stinson", "abstract": "  We connect adversarial training for binary classification to a geometric\nevolution equation for the decision boundary. Relying on a perspective that\nrecasts adversarial training as a regularization problem, we introduce a\nmodified training scheme that constitutes a minimizing movements scheme for a\nnonlocal perimeter functional. We prove that the scheme is monotone and\nconsistent as the adversarial budget vanishes and the perimeter localizes, and\nas a consequence we rigorously show that the scheme approximates a weighted\nmean curvature flow. This highlights that the efficacy of adversarial training\nmay be due to locally minimizing the length of the decision boundary. In our\nanalysis, we introduce a variety of tools for working with the subdifferential\nof a supremal-type nonlocal total variation and its regularity properties.\n", "link": "http://arxiv.org/abs/2404.14402v1", "date": "2024-04-22", "relevancy": 1.8001, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4321}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20mean%20curvature%20flow%20arising%20in%20adversarial%20training&body=Title%3A%20A%20mean%20curvature%20flow%20arising%20in%20adversarial%20training%0AAuthor%3A%20Leon%20Bungert%20and%20Tim%20Laux%20and%20Kerrek%20Stinson%0AAbstract%3A%20%20%20We%20connect%20adversarial%20training%20for%20binary%20classification%20to%20a%20geometric%0Aevolution%20equation%20for%20the%20decision%20boundary.%20Relying%20on%20a%20perspective%20that%0Arecasts%20adversarial%20training%20as%20a%20regularization%20problem%2C%20we%20introduce%20a%0Amodified%20training%20scheme%20that%20constitutes%20a%20minimizing%20movements%20scheme%20for%20a%0Anonlocal%20perimeter%20functional.%20We%20prove%20that%20the%20scheme%20is%20monotone%20and%0Aconsistent%20as%20the%20adversarial%20budget%20vanishes%20and%20the%20perimeter%20localizes%2C%20and%0Aas%20a%20consequence%20we%20rigorously%20show%20that%20the%20scheme%20approximates%20a%20weighted%0Amean%20curvature%20flow.%20This%20highlights%20that%20the%20efficacy%20of%20adversarial%20training%0Amay%20be%20due%20to%20locally%20minimizing%20the%20length%20of%20the%20decision%20boundary.%20In%20our%0Aanalysis%2C%20we%20introduce%20a%20variety%20of%20tools%20for%20working%20with%20the%20subdifferential%0Aof%20a%20supremal-type%20nonlocal%20total%20variation%20and%20its%20regularity%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14402v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20mean%20curvature%20flow%20arising%20in%20adversarial%20training&entry.906535625=Leon%20Bungert%20and%20Tim%20Laux%20and%20Kerrek%20Stinson&entry.1292438233=%20%20We%20connect%20adversarial%20training%20for%20binary%20classification%20to%20a%20geometric%0Aevolution%20equation%20for%20the%20decision%20boundary.%20Relying%20on%20a%20perspective%20that%0Arecasts%20adversarial%20training%20as%20a%20regularization%20problem%2C%20we%20introduce%20a%0Amodified%20training%20scheme%20that%20constitutes%20a%20minimizing%20movements%20scheme%20for%20a%0Anonlocal%20perimeter%20functional.%20We%20prove%20that%20the%20scheme%20is%20monotone%20and%0Aconsistent%20as%20the%20adversarial%20budget%20vanishes%20and%20the%20perimeter%20localizes%2C%20and%0Aas%20a%20consequence%20we%20rigorously%20show%20that%20the%20scheme%20approximates%20a%20weighted%0Amean%20curvature%20flow.%20This%20highlights%20that%20the%20efficacy%20of%20adversarial%20training%0Amay%20be%20due%20to%20locally%20minimizing%20the%20length%20of%20the%20decision%20boundary.%20In%20our%0Aanalysis%2C%20we%20introduce%20a%20variety%20of%20tools%20for%20working%20with%20the%20subdifferential%0Aof%20a%20supremal-type%20nonlocal%20total%20variation%20and%20its%20regularity%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14402v1&entry.124074799=Read"},
{"title": "Ultra-short-term multi-step wind speed prediction for wind farms based\n  on adaptive noise reduction technology and temporal convolutional network", "author": "Haojian Huang", "abstract": "  As an important clean and renewable kind of energy, wind power plays an\nimportant role in coping with energy crisis and environmental pollution.\nHowever, the volatility and intermittency of wind speed restrict the\ndevelopment of wind power. To improve the utilization of wind power, this study\nproposes a new wind speed prediction model based on data noise reduction\ntechnology, temporal convolutional network (TCN), and gated recurrent unit\n(GRU). Firstly, an adaptive data noise reduction algorithm P-SSA is proposed\nbased on singular spectrum analysis (SSA) and Pearson correlation coefficient.\nThe original wind speed is decomposed into multiple subsequences by SSA and\nthen reconstructed. When the Pearson correlation coefficient between the\nreconstructed sequence and the original sequence is greater than 0.99, other\nnoise subsequences are deleted to complete the data denoising. Then, the\nreceptive field of the samples is expanded through the causal convolution and\ndilated convolution of TCN, and the characteristics of wind speed change are\nextracted. Then, the time feature information of the sequence is extracted by\nGRU, and then the wind speed is predicted to form the wind speed sequence\nprediction model of P-SSA-TCN-GRU. The proposed model was validated on three\nwind farms in Shandong Province. The experimental results show that the\nprediction performance of the proposed model is better than that of the\ntraditional model and other models based on TCN, and the wind speed prediction\nof wind farms with high precision and strong stability is realized. The wind\nspeed predictions of this model have the potential to become the data that\nsupport the operation and management of wind farms. The code is available at\nhttps://github.com/JethroJames/Wind-Speed-Forecast-TCN_GRU\n", "link": "http://arxiv.org/abs/2311.16198v2", "date": "2024-04-22", "relevancy": 1.7945, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4609}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4474}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ultra-short-term%20multi-step%20wind%20speed%20prediction%20for%20wind%20farms%20based%0A%20%20on%20adaptive%20noise%20reduction%20technology%20and%20temporal%20convolutional%20network&body=Title%3A%20Ultra-short-term%20multi-step%20wind%20speed%20prediction%20for%20wind%20farms%20based%0A%20%20on%20adaptive%20noise%20reduction%20technology%20and%20temporal%20convolutional%20network%0AAuthor%3A%20Haojian%20Huang%0AAbstract%3A%20%20%20As%20an%20important%20clean%20and%20renewable%20kind%20of%20energy%2C%20wind%20power%20plays%20an%0Aimportant%20role%20in%20coping%20with%20energy%20crisis%20and%20environmental%20pollution.%0AHowever%2C%20the%20volatility%20and%20intermittency%20of%20wind%20speed%20restrict%20the%0Adevelopment%20of%20wind%20power.%20To%20improve%20the%20utilization%20of%20wind%20power%2C%20this%20study%0Aproposes%20a%20new%20wind%20speed%20prediction%20model%20based%20on%20data%20noise%20reduction%0Atechnology%2C%20temporal%20convolutional%20network%20%28TCN%29%2C%20and%20gated%20recurrent%20unit%0A%28GRU%29.%20Firstly%2C%20an%20adaptive%20data%20noise%20reduction%20algorithm%20P-SSA%20is%20proposed%0Abased%20on%20singular%20spectrum%20analysis%20%28SSA%29%20and%20Pearson%20correlation%20coefficient.%0AThe%20original%20wind%20speed%20is%20decomposed%20into%20multiple%20subsequences%20by%20SSA%20and%0Athen%20reconstructed.%20When%20the%20Pearson%20correlation%20coefficient%20between%20the%0Areconstructed%20sequence%20and%20the%20original%20sequence%20is%20greater%20than%200.99%2C%20other%0Anoise%20subsequences%20are%20deleted%20to%20complete%20the%20data%20denoising.%20Then%2C%20the%0Areceptive%20field%20of%20the%20samples%20is%20expanded%20through%20the%20causal%20convolution%20and%0Adilated%20convolution%20of%20TCN%2C%20and%20the%20characteristics%20of%20wind%20speed%20change%20are%0Aextracted.%20Then%2C%20the%20time%20feature%20information%20of%20the%20sequence%20is%20extracted%20by%0AGRU%2C%20and%20then%20the%20wind%20speed%20is%20predicted%20to%20form%20the%20wind%20speed%20sequence%0Aprediction%20model%20of%20P-SSA-TCN-GRU.%20The%20proposed%20model%20was%20validated%20on%20three%0Awind%20farms%20in%20Shandong%20Province.%20The%20experimental%20results%20show%20that%20the%0Aprediction%20performance%20of%20the%20proposed%20model%20is%20better%20than%20that%20of%20the%0Atraditional%20model%20and%20other%20models%20based%20on%20TCN%2C%20and%20the%20wind%20speed%20prediction%0Aof%20wind%20farms%20with%20high%20precision%20and%20strong%20stability%20is%20realized.%20The%20wind%0Aspeed%20predictions%20of%20this%20model%20have%20the%20potential%20to%20become%20the%20data%20that%0Asupport%20the%20operation%20and%20management%20of%20wind%20farms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/JethroJames/Wind-Speed-Forecast-TCN_GRU%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16198v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-short-term%20multi-step%20wind%20speed%20prediction%20for%20wind%20farms%20based%0A%20%20on%20adaptive%20noise%20reduction%20technology%20and%20temporal%20convolutional%20network&entry.906535625=Haojian%20Huang&entry.1292438233=%20%20As%20an%20important%20clean%20and%20renewable%20kind%20of%20energy%2C%20wind%20power%20plays%20an%0Aimportant%20role%20in%20coping%20with%20energy%20crisis%20and%20environmental%20pollution.%0AHowever%2C%20the%20volatility%20and%20intermittency%20of%20wind%20speed%20restrict%20the%0Adevelopment%20of%20wind%20power.%20To%20improve%20the%20utilization%20of%20wind%20power%2C%20this%20study%0Aproposes%20a%20new%20wind%20speed%20prediction%20model%20based%20on%20data%20noise%20reduction%0Atechnology%2C%20temporal%20convolutional%20network%20%28TCN%29%2C%20and%20gated%20recurrent%20unit%0A%28GRU%29.%20Firstly%2C%20an%20adaptive%20data%20noise%20reduction%20algorithm%20P-SSA%20is%20proposed%0Abased%20on%20singular%20spectrum%20analysis%20%28SSA%29%20and%20Pearson%20correlation%20coefficient.%0AThe%20original%20wind%20speed%20is%20decomposed%20into%20multiple%20subsequences%20by%20SSA%20and%0Athen%20reconstructed.%20When%20the%20Pearson%20correlation%20coefficient%20between%20the%0Areconstructed%20sequence%20and%20the%20original%20sequence%20is%20greater%20than%200.99%2C%20other%0Anoise%20subsequences%20are%20deleted%20to%20complete%20the%20data%20denoising.%20Then%2C%20the%0Areceptive%20field%20of%20the%20samples%20is%20expanded%20through%20the%20causal%20convolution%20and%0Adilated%20convolution%20of%20TCN%2C%20and%20the%20characteristics%20of%20wind%20speed%20change%20are%0Aextracted.%20Then%2C%20the%20time%20feature%20information%20of%20the%20sequence%20is%20extracted%20by%0AGRU%2C%20and%20then%20the%20wind%20speed%20is%20predicted%20to%20form%20the%20wind%20speed%20sequence%0Aprediction%20model%20of%20P-SSA-TCN-GRU.%20The%20proposed%20model%20was%20validated%20on%20three%0Awind%20farms%20in%20Shandong%20Province.%20The%20experimental%20results%20show%20that%20the%0Aprediction%20performance%20of%20the%20proposed%20model%20is%20better%20than%20that%20of%20the%0Atraditional%20model%20and%20other%20models%20based%20on%20TCN%2C%20and%20the%20wind%20speed%20prediction%0Aof%20wind%20farms%20with%20high%20precision%20and%20strong%20stability%20is%20realized.%20The%20wind%0Aspeed%20predictions%20of%20this%20model%20have%20the%20potential%20to%20become%20the%20data%20that%0Asupport%20the%20operation%20and%20management%20of%20wind%20farms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/JethroJames/Wind-Speed-Forecast-TCN_GRU%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16198v2&entry.124074799=Read"},
{"title": "Implicit and Explicit Language Guidance for Diffusion-based Visual\n  Perception", "author": "Hefeng Wang and Jiale Cao and Jin Xie and Aiping Yang and Yanwei Pang", "abstract": "  Text-to-image diffusion models have shown powerful ability on conditional\nimage synthesis. With large-scale vision-language pre-training, diffusion\nmodels are able to generate high-quality images with rich texture and\nreasonable structure under different text prompts. However, it is an open\nproblem to adapt the pre-trained diffusion model for visual perception. In this\npaper, we propose an implicit and explicit language guidance framework for\ndiffusion-based perception, named IEDP. Our IEDP comprises an implicit language\nguidance branch and an explicit language guidance branch. The implicit branch\nemploys frozen CLIP image encoder to directly generate implicit text embeddings\nthat are fed to diffusion model, without using explicit text prompts. The\nexplicit branch utilizes the ground-truth labels of corresponding images as\ntext prompts to condition feature extraction of diffusion model. During\ntraining, we jointly train diffusion model by sharing the model weights of\nthese two branches. As a result, implicit and explicit branches can jointly\nguide feature learning. During inference, we only employ implicit branch for\nfinal prediction, which does not require any ground-truth labels. Experiments\nare performed on two typical perception tasks, including semantic segmentation\nand depth estimation. Our IEDP achieves promising performance on both tasks.\nFor semantic segmentation, our IEDP has the mIoU$^\\text{ss}$ score of 55.9% on\nAD20K validation set, which outperforms the baseline method VPD by 2.2%. For\ndepth estimation, our IEDP outperforms the baseline method VPD with a relative\ngain of 11.0%.\n", "link": "http://arxiv.org/abs/2404.07600v2", "date": "2024-04-22", "relevancy": 1.793, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6236}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5928}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Implicit%20and%20Explicit%20Language%20Guidance%20for%20Diffusion-based%20Visual%0A%20%20Perception&body=Title%3A%20Implicit%20and%20Explicit%20Language%20Guidance%20for%20Diffusion-based%20Visual%0A%20%20Perception%0AAuthor%3A%20Hefeng%20Wang%20and%20Jiale%20Cao%20and%20Jin%20Xie%20and%20Aiping%20Yang%20and%20Yanwei%20Pang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20shown%20powerful%20ability%20on%20conditional%0Aimage%20synthesis.%20With%20large-scale%20vision-language%20pre-training%2C%20diffusion%0Amodels%20are%20able%20to%20generate%20high-quality%20images%20with%20rich%20texture%20and%0Areasonable%20structure%20under%20different%20text%20prompts.%20However%2C%20it%20is%20an%20open%0Aproblem%20to%20adapt%20the%20pre-trained%20diffusion%20model%20for%20visual%20perception.%20In%20this%0Apaper%2C%20we%20propose%20an%20implicit%20and%20explicit%20language%20guidance%20framework%20for%0Adiffusion-based%20perception%2C%20named%20IEDP.%20Our%20IEDP%20comprises%20an%20implicit%20language%0Aguidance%20branch%20and%20an%20explicit%20language%20guidance%20branch.%20The%20implicit%20branch%0Aemploys%20frozen%20CLIP%20image%20encoder%20to%20directly%20generate%20implicit%20text%20embeddings%0Athat%20are%20fed%20to%20diffusion%20model%2C%20without%20using%20explicit%20text%20prompts.%20The%0Aexplicit%20branch%20utilizes%20the%20ground-truth%20labels%20of%20corresponding%20images%20as%0Atext%20prompts%20to%20condition%20feature%20extraction%20of%20diffusion%20model.%20During%0Atraining%2C%20we%20jointly%20train%20diffusion%20model%20by%20sharing%20the%20model%20weights%20of%0Athese%20two%20branches.%20As%20a%20result%2C%20implicit%20and%20explicit%20branches%20can%20jointly%0Aguide%20feature%20learning.%20During%20inference%2C%20we%20only%20employ%20implicit%20branch%20for%0Afinal%20prediction%2C%20which%20does%20not%20require%20any%20ground-truth%20labels.%20Experiments%0Aare%20performed%20on%20two%20typical%20perception%20tasks%2C%20including%20semantic%20segmentation%0Aand%20depth%20estimation.%20Our%20IEDP%20achieves%20promising%20performance%20on%20both%20tasks.%0AFor%20semantic%20segmentation%2C%20our%20IEDP%20has%20the%20mIoU%24%5E%5Ctext%7Bss%7D%24%20score%20of%2055.9%25%20on%0AAD20K%20validation%20set%2C%20which%20outperforms%20the%20baseline%20method%20VPD%20by%202.2%25.%20For%0Adepth%20estimation%2C%20our%20IEDP%20outperforms%20the%20baseline%20method%20VPD%20with%20a%20relative%0Again%20of%2011.0%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07600v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20and%20Explicit%20Language%20Guidance%20for%20Diffusion-based%20Visual%0A%20%20Perception&entry.906535625=Hefeng%20Wang%20and%20Jiale%20Cao%20and%20Jin%20Xie%20and%20Aiping%20Yang%20and%20Yanwei%20Pang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20shown%20powerful%20ability%20on%20conditional%0Aimage%20synthesis.%20With%20large-scale%20vision-language%20pre-training%2C%20diffusion%0Amodels%20are%20able%20to%20generate%20high-quality%20images%20with%20rich%20texture%20and%0Areasonable%20structure%20under%20different%20text%20prompts.%20However%2C%20it%20is%20an%20open%0Aproblem%20to%20adapt%20the%20pre-trained%20diffusion%20model%20for%20visual%20perception.%20In%20this%0Apaper%2C%20we%20propose%20an%20implicit%20and%20explicit%20language%20guidance%20framework%20for%0Adiffusion-based%20perception%2C%20named%20IEDP.%20Our%20IEDP%20comprises%20an%20implicit%20language%0Aguidance%20branch%20and%20an%20explicit%20language%20guidance%20branch.%20The%20implicit%20branch%0Aemploys%20frozen%20CLIP%20image%20encoder%20to%20directly%20generate%20implicit%20text%20embeddings%0Athat%20are%20fed%20to%20diffusion%20model%2C%20without%20using%20explicit%20text%20prompts.%20The%0Aexplicit%20branch%20utilizes%20the%20ground-truth%20labels%20of%20corresponding%20images%20as%0Atext%20prompts%20to%20condition%20feature%20extraction%20of%20diffusion%20model.%20During%0Atraining%2C%20we%20jointly%20train%20diffusion%20model%20by%20sharing%20the%20model%20weights%20of%0Athese%20two%20branches.%20As%20a%20result%2C%20implicit%20and%20explicit%20branches%20can%20jointly%0Aguide%20feature%20learning.%20During%20inference%2C%20we%20only%20employ%20implicit%20branch%20for%0Afinal%20prediction%2C%20which%20does%20not%20require%20any%20ground-truth%20labels.%20Experiments%0Aare%20performed%20on%20two%20typical%20perception%20tasks%2C%20including%20semantic%20segmentation%0Aand%20depth%20estimation.%20Our%20IEDP%20achieves%20promising%20performance%20on%20both%20tasks.%0AFor%20semantic%20segmentation%2C%20our%20IEDP%20has%20the%20mIoU%24%5E%5Ctext%7Bss%7D%24%20score%20of%2055.9%25%20on%0AAD20K%20validation%20set%2C%20which%20outperforms%20the%20baseline%20method%20VPD%20by%202.2%25.%20For%0Adepth%20estimation%2C%20our%20IEDP%20outperforms%20the%20baseline%20method%20VPD%20with%20a%20relative%0Again%20of%2011.0%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07600v2&entry.124074799=Read"},
{"title": "FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on", "author": "Chenhui Wang and Tao Chen and Zhihao Chen and Zhizhong Huang and Taoran Jiang and Qi Wang and Hongming Shan", "abstract": "  Despite their impressive generative performance, latent diffusion model-based\nvirtual try-on (VTON) methods lack faithfulness to crucial details of the\nclothes, such as style, pattern, and text. To alleviate these issues caused by\nthe diffusion stochastic nature and latent supervision, we propose a novel\nFaithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves\nthe conventional latent diffusion process in three major aspects. First, we\npropose incorporating warped clothes as both the starting point and local\ncondition, supplying the model with faithful clothes priors. Second, we\nintroduce a novel clothes flattening network to constrain generated try-on\nimages, providing clothes-consistent faithful supervision. Third, we devise a\nclothes-posterior sampling for faithful inference, further enhancing the model\nperformance over conventional clothes-agnostic Gaussian sampling. Extensive\nexperimental results on the benchmark VITON-HD and Dress Code datasets\ndemonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is\nable to generate photo-realistic try-on images with faithful clothing details.\n", "link": "http://arxiv.org/abs/2404.14162v1", "date": "2024-04-22", "relevancy": 1.7898, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7047}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5686}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5646}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FLDM-VTON%3A%20Faithful%20Latent%20Diffusion%20Model%20for%20Virtual%20Try-on&body=Title%3A%20FLDM-VTON%3A%20Faithful%20Latent%20Diffusion%20Model%20for%20Virtual%20Try-on%0AAuthor%3A%20Chenhui%20Wang%20and%20Tao%20Chen%20and%20Zhihao%20Chen%20and%20Zhizhong%20Huang%20and%20Taoran%20Jiang%20and%20Qi%20Wang%20and%20Hongming%20Shan%0AAbstract%3A%20%20%20Despite%20their%20impressive%20generative%20performance%2C%20latent%20diffusion%20model-based%0Avirtual%20try-on%20%28VTON%29%20methods%20lack%20faithfulness%20to%20crucial%20details%20of%20the%0Aclothes%2C%20such%20as%20style%2C%20pattern%2C%20and%20text.%20To%20alleviate%20these%20issues%20caused%20by%0Athe%20diffusion%20stochastic%20nature%20and%20latent%20supervision%2C%20we%20propose%20a%20novel%0AFaithful%20Latent%20Diffusion%20Model%20for%20VTON%2C%20termed%20FLDM-VTON.%20FLDM-VTON%20improves%0Athe%20conventional%20latent%20diffusion%20process%20in%20three%20major%20aspects.%20First%2C%20we%0Apropose%20incorporating%20warped%20clothes%20as%20both%20the%20starting%20point%20and%20local%0Acondition%2C%20supplying%20the%20model%20with%20faithful%20clothes%20priors.%20Second%2C%20we%0Aintroduce%20a%20novel%20clothes%20flattening%20network%20to%20constrain%20generated%20try-on%0Aimages%2C%20providing%20clothes-consistent%20faithful%20supervision.%20Third%2C%20we%20devise%20a%0Aclothes-posterior%20sampling%20for%20faithful%20inference%2C%20further%20enhancing%20the%20model%0Aperformance%20over%20conventional%20clothes-agnostic%20Gaussian%20sampling.%20Extensive%0Aexperimental%20results%20on%20the%20benchmark%20VITON-HD%20and%20Dress%20Code%20datasets%0Ademonstrate%20that%20our%20FLDM-VTON%20outperforms%20state-of-the-art%20baselines%20and%20is%0Aable%20to%20generate%20photo-realistic%20try-on%20images%20with%20faithful%20clothing%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14162v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLDM-VTON%3A%20Faithful%20Latent%20Diffusion%20Model%20for%20Virtual%20Try-on&entry.906535625=Chenhui%20Wang%20and%20Tao%20Chen%20and%20Zhihao%20Chen%20and%20Zhizhong%20Huang%20and%20Taoran%20Jiang%20and%20Qi%20Wang%20and%20Hongming%20Shan&entry.1292438233=%20%20Despite%20their%20impressive%20generative%20performance%2C%20latent%20diffusion%20model-based%0Avirtual%20try-on%20%28VTON%29%20methods%20lack%20faithfulness%20to%20crucial%20details%20of%20the%0Aclothes%2C%20such%20as%20style%2C%20pattern%2C%20and%20text.%20To%20alleviate%20these%20issues%20caused%20by%0Athe%20diffusion%20stochastic%20nature%20and%20latent%20supervision%2C%20we%20propose%20a%20novel%0AFaithful%20Latent%20Diffusion%20Model%20for%20VTON%2C%20termed%20FLDM-VTON.%20FLDM-VTON%20improves%0Athe%20conventional%20latent%20diffusion%20process%20in%20three%20major%20aspects.%20First%2C%20we%0Apropose%20incorporating%20warped%20clothes%20as%20both%20the%20starting%20point%20and%20local%0Acondition%2C%20supplying%20the%20model%20with%20faithful%20clothes%20priors.%20Second%2C%20we%0Aintroduce%20a%20novel%20clothes%20flattening%20network%20to%20constrain%20generated%20try-on%0Aimages%2C%20providing%20clothes-consistent%20faithful%20supervision.%20Third%2C%20we%20devise%20a%0Aclothes-posterior%20sampling%20for%20faithful%20inference%2C%20further%20enhancing%20the%20model%0Aperformance%20over%20conventional%20clothes-agnostic%20Gaussian%20sampling.%20Extensive%0Aexperimental%20results%20on%20the%20benchmark%20VITON-HD%20and%20Dress%20Code%20datasets%0Ademonstrate%20that%20our%20FLDM-VTON%20outperforms%20state-of-the-art%20baselines%20and%20is%0Aable%20to%20generate%20photo-realistic%20try-on%20images%20with%20faithful%20clothing%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14162v1&entry.124074799=Read"},
{"title": "Zero-shot cross-lingual transfer in instruction tuning of large language\n  models", "author": "Nadezhda Chirkova and Vassilina Nikoulina", "abstract": "  Instruction tuning (IT) is widely used to teach pretrained large language\nmodels (LLMs) to follow arbitrary instructions, but is under-studied in\nmultilingual settings. In this work, we conduct a systematic study of zero-shot\ncross-lingual transfer in IT, when an LLM is instruction-tuned on English-only\ndata and then tested on user prompts in other languages. We advocate for the\nimportance of evaluating various aspects of model responses in multilingual\ninstruction following and investigate the influence of different model\nconfiguration choices. We find that cross-lingual transfer does happen\nsuccessfully in IT even if all stages of model training are English-centric,\nbut only if multiliguality is taken into account in hyperparameter tuning and\nwith large enough IT data. English-trained LLMs are capable of generating\ncorrect-language, comprehensive and helpful responses in other languages, but\nsuffer from low factuality and may occasionally have fluency errors.\n", "link": "http://arxiv.org/abs/2402.14778v2", "date": "2024-04-22", "relevancy": 1.7826, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4231}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20cross-lingual%20transfer%20in%20instruction%20tuning%20of%20large%20language%0A%20%20models&body=Title%3A%20Zero-shot%20cross-lingual%20transfer%20in%20instruction%20tuning%20of%20large%20language%0A%20%20models%0AAuthor%3A%20Nadezhda%20Chirkova%20and%20Vassilina%20Nikoulina%0AAbstract%3A%20%20%20Instruction%20tuning%20%28IT%29%20is%20widely%20used%20to%20teach%20pretrained%20large%20language%0Amodels%20%28LLMs%29%20to%20follow%20arbitrary%20instructions%2C%20but%20is%20under-studied%20in%0Amultilingual%20settings.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20zero-shot%0Across-lingual%20transfer%20in%20IT%2C%20when%20an%20LLM%20is%20instruction-tuned%20on%20English-only%0Adata%20and%20then%20tested%20on%20user%20prompts%20in%20other%20languages.%20We%20advocate%20for%20the%0Aimportance%20of%20evaluating%20various%20aspects%20of%20model%20responses%20in%20multilingual%0Ainstruction%20following%20and%20investigate%20the%20influence%20of%20different%20model%0Aconfiguration%20choices.%20We%20find%20that%20cross-lingual%20transfer%20does%20happen%0Asuccessfully%20in%20IT%20even%20if%20all%20stages%20of%20model%20training%20are%20English-centric%2C%0Abut%20only%20if%20multiliguality%20is%20taken%20into%20account%20in%20hyperparameter%20tuning%20and%0Awith%20large%20enough%20IT%20data.%20English-trained%20LLMs%20are%20capable%20of%20generating%0Acorrect-language%2C%20comprehensive%20and%20helpful%20responses%20in%20other%20languages%2C%20but%0Asuffer%20from%20low%20factuality%20and%20may%20occasionally%20have%20fluency%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14778v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20cross-lingual%20transfer%20in%20instruction%20tuning%20of%20large%20language%0A%20%20models&entry.906535625=Nadezhda%20Chirkova%20and%20Vassilina%20Nikoulina&entry.1292438233=%20%20Instruction%20tuning%20%28IT%29%20is%20widely%20used%20to%20teach%20pretrained%20large%20language%0Amodels%20%28LLMs%29%20to%20follow%20arbitrary%20instructions%2C%20but%20is%20under-studied%20in%0Amultilingual%20settings.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20zero-shot%0Across-lingual%20transfer%20in%20IT%2C%20when%20an%20LLM%20is%20instruction-tuned%20on%20English-only%0Adata%20and%20then%20tested%20on%20user%20prompts%20in%20other%20languages.%20We%20advocate%20for%20the%0Aimportance%20of%20evaluating%20various%20aspects%20of%20model%20responses%20in%20multilingual%0Ainstruction%20following%20and%20investigate%20the%20influence%20of%20different%20model%0Aconfiguration%20choices.%20We%20find%20that%20cross-lingual%20transfer%20does%20happen%0Asuccessfully%20in%20IT%20even%20if%20all%20stages%20of%20model%20training%20are%20English-centric%2C%0Abut%20only%20if%20multiliguality%20is%20taken%20into%20account%20in%20hyperparameter%20tuning%20and%0Awith%20large%20enough%20IT%20data.%20English-trained%20LLMs%20are%20capable%20of%20generating%0Acorrect-language%2C%20comprehensive%20and%20helpful%20responses%20in%20other%20languages%2C%20but%0Asuffer%20from%20low%20factuality%20and%20may%20occasionally%20have%20fluency%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14778v2&entry.124074799=Read"},
{"title": "New Solutions Based on the Generalized Eigenvalue Problem for the Data\n  Collaboration Analysis", "author": "Yuta Kawakami and Yuichi Takano and Akira Imakura", "abstract": "  In recent years, the accumulation of data across various institutions has\ngarnered attention for the technology of confidential data analysis, which\nimproves analytical accuracy by sharing data between multiple institutions\nwhile protecting sensitive information. Among these methods, Data Collaboration\nAnalysis (DCA) is noted for its efficiency in terms of computational cost and\ncommunication load, facilitating data sharing and analysis across different\ninstitutions while safeguarding confidential information. However, existing\noptimization problems for determining the necessary collaborative functions\nhave faced challenges, such as the optimal solution for the collaborative\nrepresentation often being a zero matrix and the difficulty in understanding\nthe process of deriving solutions. This research addresses these issues by\nformulating the optimization problem through the segmentation of matrices into\ncolumn vectors and proposing a solution method based on the generalized\neigenvalue problem. Additionally, we demonstrate methods for constructing\ncollaborative functions more effectively through weighting and the selection of\nefficient algorithms suited to specific situations. Experiments using\nreal-world datasets have shown that our proposed formulation and solution for\nthe collaborative function optimization problem achieve superior predictive\naccuracy compared to existing methods.\n", "link": "http://arxiv.org/abs/2404.14164v1", "date": "2024-04-22", "relevancy": 1.7819, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4483}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4404}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20New%20Solutions%20Based%20on%20the%20Generalized%20Eigenvalue%20Problem%20for%20the%20Data%0A%20%20Collaboration%20Analysis&body=Title%3A%20New%20Solutions%20Based%20on%20the%20Generalized%20Eigenvalue%20Problem%20for%20the%20Data%0A%20%20Collaboration%20Analysis%0AAuthor%3A%20Yuta%20Kawakami%20and%20Yuichi%20Takano%20and%20Akira%20Imakura%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20accumulation%20of%20data%20across%20various%20institutions%20has%0Agarnered%20attention%20for%20the%20technology%20of%20confidential%20data%20analysis%2C%20which%0Aimproves%20analytical%20accuracy%20by%20sharing%20data%20between%20multiple%20institutions%0Awhile%20protecting%20sensitive%20information.%20Among%20these%20methods%2C%20Data%20Collaboration%0AAnalysis%20%28DCA%29%20is%20noted%20for%20its%20efficiency%20in%20terms%20of%20computational%20cost%20and%0Acommunication%20load%2C%20facilitating%20data%20sharing%20and%20analysis%20across%20different%0Ainstitutions%20while%20safeguarding%20confidential%20information.%20However%2C%20existing%0Aoptimization%20problems%20for%20determining%20the%20necessary%20collaborative%20functions%0Ahave%20faced%20challenges%2C%20such%20as%20the%20optimal%20solution%20for%20the%20collaborative%0Arepresentation%20often%20being%20a%20zero%20matrix%20and%20the%20difficulty%20in%20understanding%0Athe%20process%20of%20deriving%20solutions.%20This%20research%20addresses%20these%20issues%20by%0Aformulating%20the%20optimization%20problem%20through%20the%20segmentation%20of%20matrices%20into%0Acolumn%20vectors%20and%20proposing%20a%20solution%20method%20based%20on%20the%20generalized%0Aeigenvalue%20problem.%20Additionally%2C%20we%20demonstrate%20methods%20for%20constructing%0Acollaborative%20functions%20more%20effectively%20through%20weighting%20and%20the%20selection%20of%0Aefficient%20algorithms%20suited%20to%20specific%20situations.%20Experiments%20using%0Areal-world%20datasets%20have%20shown%20that%20our%20proposed%20formulation%20and%20solution%20for%0Athe%20collaborative%20function%20optimization%20problem%20achieve%20superior%20predictive%0Aaccuracy%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14164v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Solutions%20Based%20on%20the%20Generalized%20Eigenvalue%20Problem%20for%20the%20Data%0A%20%20Collaboration%20Analysis&entry.906535625=Yuta%20Kawakami%20and%20Yuichi%20Takano%20and%20Akira%20Imakura&entry.1292438233=%20%20In%20recent%20years%2C%20the%20accumulation%20of%20data%20across%20various%20institutions%20has%0Agarnered%20attention%20for%20the%20technology%20of%20confidential%20data%20analysis%2C%20which%0Aimproves%20analytical%20accuracy%20by%20sharing%20data%20between%20multiple%20institutions%0Awhile%20protecting%20sensitive%20information.%20Among%20these%20methods%2C%20Data%20Collaboration%0AAnalysis%20%28DCA%29%20is%20noted%20for%20its%20efficiency%20in%20terms%20of%20computational%20cost%20and%0Acommunication%20load%2C%20facilitating%20data%20sharing%20and%20analysis%20across%20different%0Ainstitutions%20while%20safeguarding%20confidential%20information.%20However%2C%20existing%0Aoptimization%20problems%20for%20determining%20the%20necessary%20collaborative%20functions%0Ahave%20faced%20challenges%2C%20such%20as%20the%20optimal%20solution%20for%20the%20collaborative%0Arepresentation%20often%20being%20a%20zero%20matrix%20and%20the%20difficulty%20in%20understanding%0Athe%20process%20of%20deriving%20solutions.%20This%20research%20addresses%20these%20issues%20by%0Aformulating%20the%20optimization%20problem%20through%20the%20segmentation%20of%20matrices%20into%0Acolumn%20vectors%20and%20proposing%20a%20solution%20method%20based%20on%20the%20generalized%0Aeigenvalue%20problem.%20Additionally%2C%20we%20demonstrate%20methods%20for%20constructing%0Acollaborative%20functions%20more%20effectively%20through%20weighting%20and%20the%20selection%20of%0Aefficient%20algorithms%20suited%20to%20specific%20situations.%20Experiments%20using%0Areal-world%20datasets%20have%20shown%20that%20our%20proposed%20formulation%20and%20solution%20for%0Athe%20collaborative%20function%20optimization%20problem%20achieve%20superior%20predictive%0Aaccuracy%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14164v1&entry.124074799=Read"},
{"title": "Poisoning Attacks on Federated Learning-based Wireless Traffic\n  Prediction", "author": "Zifan Zhang and Minghong Fang and Jiayuan Huang and Yuchen Liu", "abstract": "  Federated Learning (FL) offers a distributed framework to train a global\ncontrol model across multiple base stations without compromising the privacy of\ntheir local network data. This makes it ideal for applications like wireless\ntraffic prediction (WTP), which plays a crucial role in optimizing network\nresources, enabling proactive traffic flow management, and enhancing the\nreliability of downstream communication-aided applications, such as IoT\ndevices, autonomous vehicles, and industrial automation systems. Despite its\npromise, the security aspects of FL-based distributed wireless systems,\nparticularly in regression-based WTP problems, remain inadequately\ninvestigated. In this paper, we introduce a novel fake traffic injection (FTI)\nattack, designed to undermine the FL-based WTP system by injecting fabricated\ntraffic distributions with minimal knowledge. We further propose a defense\nmechanism, termed global-local inconsistency detection (GLID), which\nstrategically removes abnormal model parameters that deviate beyond a specific\npercentile range estimated through statistical methods in each dimension.\nExtensive experimental evaluations, performed on real-world wireless traffic\ndatasets, demonstrate that both our attack and defense strategies significantly\noutperform existing baselines.\n", "link": "http://arxiv.org/abs/2404.14389v1", "date": "2024-04-22", "relevancy": 1.779, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4514}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4425}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Poisoning%20Attacks%20on%20Federated%20Learning-based%20Wireless%20Traffic%0A%20%20Prediction&body=Title%3A%20Poisoning%20Attacks%20on%20Federated%20Learning-based%20Wireless%20Traffic%0A%20%20Prediction%0AAuthor%3A%20Zifan%20Zhang%20and%20Minghong%20Fang%20and%20Jiayuan%20Huang%20and%20Yuchen%20Liu%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20distributed%20framework%20to%20train%20a%20global%0Acontrol%20model%20across%20multiple%20base%20stations%20without%20compromising%20the%20privacy%20of%0Atheir%20local%20network%20data.%20This%20makes%20it%20ideal%20for%20applications%20like%20wireless%0Atraffic%20prediction%20%28WTP%29%2C%20which%20plays%20a%20crucial%20role%20in%20optimizing%20network%0Aresources%2C%20enabling%20proactive%20traffic%20flow%20management%2C%20and%20enhancing%20the%0Areliability%20of%20downstream%20communication-aided%20applications%2C%20such%20as%20IoT%0Adevices%2C%20autonomous%20vehicles%2C%20and%20industrial%20automation%20systems.%20Despite%20its%0Apromise%2C%20the%20security%20aspects%20of%20FL-based%20distributed%20wireless%20systems%2C%0Aparticularly%20in%20regression-based%20WTP%20problems%2C%20remain%20inadequately%0Ainvestigated.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20fake%20traffic%20injection%20%28FTI%29%0Aattack%2C%20designed%20to%20undermine%20the%20FL-based%20WTP%20system%20by%20injecting%20fabricated%0Atraffic%20distributions%20with%20minimal%20knowledge.%20We%20further%20propose%20a%20defense%0Amechanism%2C%20termed%20global-local%20inconsistency%20detection%20%28GLID%29%2C%20which%0Astrategically%20removes%20abnormal%20model%20parameters%20that%20deviate%20beyond%20a%20specific%0Apercentile%20range%20estimated%20through%20statistical%20methods%20in%20each%20dimension.%0AExtensive%20experimental%20evaluations%2C%20performed%20on%20real-world%20wireless%20traffic%0Adatasets%2C%20demonstrate%20that%20both%20our%20attack%20and%20defense%20strategies%20significantly%0Aoutperform%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14389v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poisoning%20Attacks%20on%20Federated%20Learning-based%20Wireless%20Traffic%0A%20%20Prediction&entry.906535625=Zifan%20Zhang%20and%20Minghong%20Fang%20and%20Jiayuan%20Huang%20and%20Yuchen%20Liu&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20distributed%20framework%20to%20train%20a%20global%0Acontrol%20model%20across%20multiple%20base%20stations%20without%20compromising%20the%20privacy%20of%0Atheir%20local%20network%20data.%20This%20makes%20it%20ideal%20for%20applications%20like%20wireless%0Atraffic%20prediction%20%28WTP%29%2C%20which%20plays%20a%20crucial%20role%20in%20optimizing%20network%0Aresources%2C%20enabling%20proactive%20traffic%20flow%20management%2C%20and%20enhancing%20the%0Areliability%20of%20downstream%20communication-aided%20applications%2C%20such%20as%20IoT%0Adevices%2C%20autonomous%20vehicles%2C%20and%20industrial%20automation%20systems.%20Despite%20its%0Apromise%2C%20the%20security%20aspects%20of%20FL-based%20distributed%20wireless%20systems%2C%0Aparticularly%20in%20regression-based%20WTP%20problems%2C%20remain%20inadequately%0Ainvestigated.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20fake%20traffic%20injection%20%28FTI%29%0Aattack%2C%20designed%20to%20undermine%20the%20FL-based%20WTP%20system%20by%20injecting%20fabricated%0Atraffic%20distributions%20with%20minimal%20knowledge.%20We%20further%20propose%20a%20defense%0Amechanism%2C%20termed%20global-local%20inconsistency%20detection%20%28GLID%29%2C%20which%0Astrategically%20removes%20abnormal%20model%20parameters%20that%20deviate%20beyond%20a%20specific%0Apercentile%20range%20estimated%20through%20statistical%20methods%20in%20each%20dimension.%0AExtensive%20experimental%20evaluations%2C%20performed%20on%20real-world%20wireless%20traffic%0Adatasets%2C%20demonstrate%20that%20both%20our%20attack%20and%20defense%20strategies%20significantly%0Aoutperform%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14389v1&entry.124074799=Read"},
{"title": "A Bayesian Approach for Prioritising Driving Behaviour Investigations in\n  Telematic Auto Insurance Policies", "author": "Mark McLeod and Bernardo Perez-Orozco and Nika Lee and Davide Zilli", "abstract": "  Automotive insurers increasingly have access to telematic information via\nblack-box recorders installed in the insured vehicle, and wish to identify\nundesirable behaviour which may signify increased risk or uninsured activities.\nHowever, identification of such behaviour with machine learning is non-trivial,\nand results are far from perfect, requiring human investigation to verify\nsuspected cases. An appropriately formed priority score, generated by automated\nanalysis of GPS data, allows underwriters to make more efficient use of their\ntime, improving detection of the behaviour under investigation.\n  An example of such behaviour is the use of a privately insured vehicle for\ncommercial purposes, such as delivering meals and parcels. We first make use of\ntrip GPS and accelerometer data, augmented by geospatial information, to train\nan imperfect classifier for delivery driving on a per-trip basis. We make use\nof a mixture of Beta-Binomial distributions to model the propensity of a\npolicyholder to undertake trips which result in a positive classification as\nbeing drawn from either a rare high-scoring or common low-scoring group, and\nlearn the parameters of this model using MCMC. This model provides us with a\nposterior probability that any policyholder will be a regular generator of\nautomated alerts given any number of trips and alerts. This posterior\nprobability is converted to a priority score, which was used to select the most\nvaluable candidates for manual investigation.\n  Testing over a 1-year period ranked policyholders by likelihood of commercial\ndriving activity on a weekly basis. The top 0.9% have been reviewed at least\nonce by the underwriters at the time of writing, and of those 99.4% have been\nconfirmed as correctly identified, showing the approach has achieved a\nsignificant improvement in efficiency of human resource allocation compared to\nmanual searching.\n", "link": "http://arxiv.org/abs/2404.14276v1", "date": "2024-04-22", "relevancy": 1.7759, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5179}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4584}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20Approach%20for%20Prioritising%20Driving%20Behaviour%20Investigations%20in%0A%20%20Telematic%20Auto%20Insurance%20Policies&body=Title%3A%20A%20Bayesian%20Approach%20for%20Prioritising%20Driving%20Behaviour%20Investigations%20in%0A%20%20Telematic%20Auto%20Insurance%20Policies%0AAuthor%3A%20Mark%20McLeod%20and%20Bernardo%20Perez-Orozco%20and%20Nika%20Lee%20and%20Davide%20Zilli%0AAbstract%3A%20%20%20Automotive%20insurers%20increasingly%20have%20access%20to%20telematic%20information%20via%0Ablack-box%20recorders%20installed%20in%20the%20insured%20vehicle%2C%20and%20wish%20to%20identify%0Aundesirable%20behaviour%20which%20may%20signify%20increased%20risk%20or%20uninsured%20activities.%0AHowever%2C%20identification%20of%20such%20behaviour%20with%20machine%20learning%20is%20non-trivial%2C%0Aand%20results%20are%20far%20from%20perfect%2C%20requiring%20human%20investigation%20to%20verify%0Asuspected%20cases.%20An%20appropriately%20formed%20priority%20score%2C%20generated%20by%20automated%0Aanalysis%20of%20GPS%20data%2C%20allows%20underwriters%20to%20make%20more%20efficient%20use%20of%20their%0Atime%2C%20improving%20detection%20of%20the%20behaviour%20under%20investigation.%0A%20%20An%20example%20of%20such%20behaviour%20is%20the%20use%20of%20a%20privately%20insured%20vehicle%20for%0Acommercial%20purposes%2C%20such%20as%20delivering%20meals%20and%20parcels.%20We%20first%20make%20use%20of%0Atrip%20GPS%20and%20accelerometer%20data%2C%20augmented%20by%20geospatial%20information%2C%20to%20train%0Aan%20imperfect%20classifier%20for%20delivery%20driving%20on%20a%20per-trip%20basis.%20We%20make%20use%0Aof%20a%20mixture%20of%20Beta-Binomial%20distributions%20to%20model%20the%20propensity%20of%20a%0Apolicyholder%20to%20undertake%20trips%20which%20result%20in%20a%20positive%20classification%20as%0Abeing%20drawn%20from%20either%20a%20rare%20high-scoring%20or%20common%20low-scoring%20group%2C%20and%0Alearn%20the%20parameters%20of%20this%20model%20using%20MCMC.%20This%20model%20provides%20us%20with%20a%0Aposterior%20probability%20that%20any%20policyholder%20will%20be%20a%20regular%20generator%20of%0Aautomated%20alerts%20given%20any%20number%20of%20trips%20and%20alerts.%20This%20posterior%0Aprobability%20is%20converted%20to%20a%20priority%20score%2C%20which%20was%20used%20to%20select%20the%20most%0Avaluable%20candidates%20for%20manual%20investigation.%0A%20%20Testing%20over%20a%201-year%20period%20ranked%20policyholders%20by%20likelihood%20of%20commercial%0Adriving%20activity%20on%20a%20weekly%20basis.%20The%20top%200.9%25%20have%20been%20reviewed%20at%20least%0Aonce%20by%20the%20underwriters%20at%20the%20time%20of%20writing%2C%20and%20of%20those%2099.4%25%20have%20been%0Aconfirmed%20as%20correctly%20identified%2C%20showing%20the%20approach%20has%20achieved%20a%0Asignificant%20improvement%20in%20efficiency%20of%20human%20resource%20allocation%20compared%20to%0Amanual%20searching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14276v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20Approach%20for%20Prioritising%20Driving%20Behaviour%20Investigations%20in%0A%20%20Telematic%20Auto%20Insurance%20Policies&entry.906535625=Mark%20McLeod%20and%20Bernardo%20Perez-Orozco%20and%20Nika%20Lee%20and%20Davide%20Zilli&entry.1292438233=%20%20Automotive%20insurers%20increasingly%20have%20access%20to%20telematic%20information%20via%0Ablack-box%20recorders%20installed%20in%20the%20insured%20vehicle%2C%20and%20wish%20to%20identify%0Aundesirable%20behaviour%20which%20may%20signify%20increased%20risk%20or%20uninsured%20activities.%0AHowever%2C%20identification%20of%20such%20behaviour%20with%20machine%20learning%20is%20non-trivial%2C%0Aand%20results%20are%20far%20from%20perfect%2C%20requiring%20human%20investigation%20to%20verify%0Asuspected%20cases.%20An%20appropriately%20formed%20priority%20score%2C%20generated%20by%20automated%0Aanalysis%20of%20GPS%20data%2C%20allows%20underwriters%20to%20make%20more%20efficient%20use%20of%20their%0Atime%2C%20improving%20detection%20of%20the%20behaviour%20under%20investigation.%0A%20%20An%20example%20of%20such%20behaviour%20is%20the%20use%20of%20a%20privately%20insured%20vehicle%20for%0Acommercial%20purposes%2C%20such%20as%20delivering%20meals%20and%20parcels.%20We%20first%20make%20use%20of%0Atrip%20GPS%20and%20accelerometer%20data%2C%20augmented%20by%20geospatial%20information%2C%20to%20train%0Aan%20imperfect%20classifier%20for%20delivery%20driving%20on%20a%20per-trip%20basis.%20We%20make%20use%0Aof%20a%20mixture%20of%20Beta-Binomial%20distributions%20to%20model%20the%20propensity%20of%20a%0Apolicyholder%20to%20undertake%20trips%20which%20result%20in%20a%20positive%20classification%20as%0Abeing%20drawn%20from%20either%20a%20rare%20high-scoring%20or%20common%20low-scoring%20group%2C%20and%0Alearn%20the%20parameters%20of%20this%20model%20using%20MCMC.%20This%20model%20provides%20us%20with%20a%0Aposterior%20probability%20that%20any%20policyholder%20will%20be%20a%20regular%20generator%20of%0Aautomated%20alerts%20given%20any%20number%20of%20trips%20and%20alerts.%20This%20posterior%0Aprobability%20is%20converted%20to%20a%20priority%20score%2C%20which%20was%20used%20to%20select%20the%20most%0Avaluable%20candidates%20for%20manual%20investigation.%0A%20%20Testing%20over%20a%201-year%20period%20ranked%20policyholders%20by%20likelihood%20of%20commercial%0Adriving%20activity%20on%20a%20weekly%20basis.%20The%20top%200.9%25%20have%20been%20reviewed%20at%20least%0Aonce%20by%20the%20underwriters%20at%20the%20time%20of%20writing%2C%20and%20of%20those%2099.4%25%20have%20been%0Aconfirmed%20as%20correctly%20identified%2C%20showing%20the%20approach%20has%20achieved%20a%0Asignificant%20improvement%20in%20efficiency%20of%20human%20resource%20allocation%20compared%20to%0Amanual%20searching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14276v1&entry.124074799=Read"},
{"title": "RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key\n  Identification", "author": "Hai Ci and Pei Yang and Yiren Song and Mike Zheng Shou", "abstract": "  We revisit Tree-Ring Watermarking, a recent diffusion model watermarking\nmethod that demonstrates great robustness to various attacks. We conduct an\nin-depth study on it and reveal that the distribution shift unintentionally\nintroduced by the watermarking process, apart from watermark pattern matching,\ncontributes to its exceptional robustness. Our investigation further exposes\ninherent flaws in its original design, particularly in its ability to identify\nmultiple distinct keys, where distribution shift offers no assistance. Based on\nthese findings and analysis, we present RingID for enhanced multi-key\nidentification. It consists of a novel multi-channel heterogeneous watermarking\napproach designed to seamlessly amalgamate distinctive advantages from diverse\nwatermarks. Coupled with a series of suggested enhancements, RingID exhibits\nsubstantial advancements in multi-key identification.\n", "link": "http://arxiv.org/abs/2404.14055v1", "date": "2024-04-22", "relevancy": 1.7741, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4776}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4235}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4085}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&body=Title%3A%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification%0AAuthor%3A%20Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14055v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&entry.906535625=Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14055v1&entry.124074799=Read"},
{"title": "Investigation of the effectiveness of applying ChatGPT in Dialogic\n  Teaching Using Electroencephalography", "author": "Jiayue Zhang and Yiheng Liu and Wenqi Cai and Lanlan Wu and Yali Peng and Jingjing Yu and Senqing Qi and Taotao Long and Bao Ge", "abstract": "  In recent years, the rapid development of artificial intelligence technology,\nespecially the emergence of large language models (LLMs) such as ChatGPT, has\npresented significant prospects for application in the field of education. LLMs\npossess the capability to interpret knowledge, answer questions, and consider\ncontext, thus providing support for dialogic teaching to students. Therefore,\nan examination of the capacity of LLMs to effectively fulfill instructional\nroles, thereby facilitating student learning akin to human educators within\ndialogic teaching scenarios, is an exceptionally valuable research topic. This\nresearch recruited 34 undergraduate students as participants, who were randomly\ndivided into two groups. The experimental group engaged in dialogic teaching\nusing ChatGPT, while the control group interacted with human teachers. Both\ngroups learned the histogram equalization unit in the information-related\ncourse \"Digital Image Processing\". The research findings show comparable scores\nbetween the two groups on the retention test. However, students who engaged in\ndialogue with ChatGPT exhibited lower performance on the transfer test.\nElectroencephalography data revealed that students who interacted with ChatGPT\nexhibited higher levels of cognitive activity, suggesting that ChatGPT could\nhelp students establish a knowledge foundation and stimulate cognitive\nactivity. However, its strengths on promoting students. knowledge application\nand creativity were insignificant. Based upon the research findings, it is\nevident that ChatGPT cannot fully excel in fulfilling teaching tasks in the\ndialogue teaching in information related courses. Combining ChatGPT with\ntraditional human teachers might be a more ideal approach. The synergistic use\nof both can provide students with more comprehensive learning support, thus\ncontributing to enhancing the quality of teaching.\n", "link": "http://arxiv.org/abs/2403.16687v3", "date": "2024-04-22", "relevancy": 1.7698, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4419}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4326}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography&body=Title%3A%20Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography%0AAuthor%3A%20Jiayue%20Zhang%20and%20Yiheng%20Liu%20and%20Wenqi%20Cai%20and%20Lanlan%20Wu%20and%20Yali%20Peng%20and%20Jingjing%20Yu%20and%20Senqing%20Qi%20and%20Taotao%20Long%20and%20Bao%20Ge%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20artificial%20intelligence%20technology%2C%0Aespecially%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%2C%20has%0Apresented%20significant%20prospects%20for%20application%20in%20the%20field%20of%20education.%20LLMs%0Apossess%20the%20capability%20to%20interpret%20knowledge%2C%20answer%20questions%2C%20and%20consider%0Acontext%2C%20thus%20providing%20support%20for%20dialogic%20teaching%20to%20students.%20Therefore%2C%0Aan%20examination%20of%20the%20capacity%20of%20LLMs%20to%20effectively%20fulfill%20instructional%0Aroles%2C%20thereby%20facilitating%20student%20learning%20akin%20to%20human%20educators%20within%0Adialogic%20teaching%20scenarios%2C%20is%20an%20exceptionally%20valuable%20research%20topic.%20This%0Aresearch%20recruited%2034%20undergraduate%20students%20as%20participants%2C%20who%20were%20randomly%0Adivided%20into%20two%20groups.%20The%20experimental%20group%20engaged%20in%20dialogic%20teaching%0Ausing%20ChatGPT%2C%20while%20the%20control%20group%20interacted%20with%20human%20teachers.%20Both%0Agroups%20learned%20the%20histogram%20equalization%20unit%20in%20the%20information-related%0Acourse%20%22Digital%20Image%20Processing%22.%20The%20research%20findings%20show%20comparable%20scores%0Abetween%20the%20two%20groups%20on%20the%20retention%20test.%20However%2C%20students%20who%20engaged%20in%0Adialogue%20with%20ChatGPT%20exhibited%20lower%20performance%20on%20the%20transfer%20test.%0AElectroencephalography%20data%20revealed%20that%20students%20who%20interacted%20with%20ChatGPT%0Aexhibited%20higher%20levels%20of%20cognitive%20activity%2C%20suggesting%20that%20ChatGPT%20could%0Ahelp%20students%20establish%20a%20knowledge%20foundation%20and%20stimulate%20cognitive%0Aactivity.%20However%2C%20its%20strengths%20on%20promoting%20students.%20knowledge%20application%0Aand%20creativity%20were%20insignificant.%20Based%20upon%20the%20research%20findings%2C%20it%20is%0Aevident%20that%20ChatGPT%20cannot%20fully%20excel%20in%20fulfilling%20teaching%20tasks%20in%20the%0Adialogue%20teaching%20in%20information%20related%20courses.%20Combining%20ChatGPT%20with%0Atraditional%20human%20teachers%20might%20be%20a%20more%20ideal%20approach.%20The%20synergistic%20use%0Aof%20both%20can%20provide%20students%20with%20more%20comprehensive%20learning%20support%2C%20thus%0Acontributing%20to%20enhancing%20the%20quality%20of%20teaching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16687v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography&entry.906535625=Jiayue%20Zhang%20and%20Yiheng%20Liu%20and%20Wenqi%20Cai%20and%20Lanlan%20Wu%20and%20Yali%20Peng%20and%20Jingjing%20Yu%20and%20Senqing%20Qi%20and%20Taotao%20Long%20and%20Bao%20Ge&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20artificial%20intelligence%20technology%2C%0Aespecially%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%2C%20has%0Apresented%20significant%20prospects%20for%20application%20in%20the%20field%20of%20education.%20LLMs%0Apossess%20the%20capability%20to%20interpret%20knowledge%2C%20answer%20questions%2C%20and%20consider%0Acontext%2C%20thus%20providing%20support%20for%20dialogic%20teaching%20to%20students.%20Therefore%2C%0Aan%20examination%20of%20the%20capacity%20of%20LLMs%20to%20effectively%20fulfill%20instructional%0Aroles%2C%20thereby%20facilitating%20student%20learning%20akin%20to%20human%20educators%20within%0Adialogic%20teaching%20scenarios%2C%20is%20an%20exceptionally%20valuable%20research%20topic.%20This%0Aresearch%20recruited%2034%20undergraduate%20students%20as%20participants%2C%20who%20were%20randomly%0Adivided%20into%20two%20groups.%20The%20experimental%20group%20engaged%20in%20dialogic%20teaching%0Ausing%20ChatGPT%2C%20while%20the%20control%20group%20interacted%20with%20human%20teachers.%20Both%0Agroups%20learned%20the%20histogram%20equalization%20unit%20in%20the%20information-related%0Acourse%20%22Digital%20Image%20Processing%22.%20The%20research%20findings%20show%20comparable%20scores%0Abetween%20the%20two%20groups%20on%20the%20retention%20test.%20However%2C%20students%20who%20engaged%20in%0Adialogue%20with%20ChatGPT%20exhibited%20lower%20performance%20on%20the%20transfer%20test.%0AElectroencephalography%20data%20revealed%20that%20students%20who%20interacted%20with%20ChatGPT%0Aexhibited%20higher%20levels%20of%20cognitive%20activity%2C%20suggesting%20that%20ChatGPT%20could%0Ahelp%20students%20establish%20a%20knowledge%20foundation%20and%20stimulate%20cognitive%0Aactivity.%20However%2C%20its%20strengths%20on%20promoting%20students.%20knowledge%20application%0Aand%20creativity%20were%20insignificant.%20Based%20upon%20the%20research%20findings%2C%20it%20is%0Aevident%20that%20ChatGPT%20cannot%20fully%20excel%20in%20fulfilling%20teaching%20tasks%20in%20the%0Adialogue%20teaching%20in%20information%20related%20courses.%20Combining%20ChatGPT%20with%0Atraditional%20human%20teachers%20might%20be%20a%20more%20ideal%20approach.%20The%20synergistic%20use%0Aof%20both%20can%20provide%20students%20with%20more%20comprehensive%20learning%20support%2C%20thus%0Acontributing%20to%20enhancing%20the%20quality%20of%20teaching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16687v3&entry.124074799=Read"},
{"title": "On Prediction Feature Assignment in the Heckman Selection Model", "author": "Huy Mai and Xintao Wu", "abstract": "  Under missing-not-at-random (MNAR) sample selection bias, the performance of\na prediction model is often degraded. This paper focuses on one classic\ninstance of MNAR sample selection bias where a subset of samples have\nnon-randomly missing outcomes. The Heckman selection model and its variants\nhave commonly been used to handle this type of sample selection bias. The\nHeckman model uses two separate equations to model the prediction and selection\nof samples, where the selection features include all prediction features. When\nusing the Heckman model, the prediction features must be properly chosen from\nthe set of selection features. However, choosing the proper prediction features\nis a challenging task for the Heckman model. This is especially the case when\nthe number of selection features is large. Existing approaches that use the\nHeckman model often provide a manually chosen set of prediction features. In\nthis paper, we propose Heckman-FA as a novel data-driven framework for\nobtaining prediction features for the Heckman model. Heckman-FA first trains an\nassignment function that determines whether or not a selection feature is\nassigned as a prediction feature. Using the parameters of the trained function,\nthe framework extracts a suitable set of prediction features based on the\ngoodness-of-fit of the prediction model given the chosen prediction features\nand the correlation between noise terms of the prediction and selection\nequations. Experimental results on real-world datasets show that Heckman-FA\nproduces a robust regression model under MNAR sample selection bias.\n", "link": "http://arxiv.org/abs/2309.08043v2", "date": "2024-04-22", "relevancy": 1.7641, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4441}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.426}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Prediction%20Feature%20Assignment%20in%20the%20Heckman%20Selection%20Model&body=Title%3A%20On%20Prediction%20Feature%20Assignment%20in%20the%20Heckman%20Selection%20Model%0AAuthor%3A%20Huy%20Mai%20and%20Xintao%20Wu%0AAbstract%3A%20%20%20Under%20missing-not-at-random%20%28MNAR%29%20sample%20selection%20bias%2C%20the%20performance%20of%0Aa%20prediction%20model%20is%20often%20degraded.%20This%20paper%20focuses%20on%20one%20classic%0Ainstance%20of%20MNAR%20sample%20selection%20bias%20where%20a%20subset%20of%20samples%20have%0Anon-randomly%20missing%20outcomes.%20The%20Heckman%20selection%20model%20and%20its%20variants%0Ahave%20commonly%20been%20used%20to%20handle%20this%20type%20of%20sample%20selection%20bias.%20The%0AHeckman%20model%20uses%20two%20separate%20equations%20to%20model%20the%20prediction%20and%20selection%0Aof%20samples%2C%20where%20the%20selection%20features%20include%20all%20prediction%20features.%20When%0Ausing%20the%20Heckman%20model%2C%20the%20prediction%20features%20must%20be%20properly%20chosen%20from%0Athe%20set%20of%20selection%20features.%20However%2C%20choosing%20the%20proper%20prediction%20features%0Ais%20a%20challenging%20task%20for%20the%20Heckman%20model.%20This%20is%20especially%20the%20case%20when%0Athe%20number%20of%20selection%20features%20is%20large.%20Existing%20approaches%20that%20use%20the%0AHeckman%20model%20often%20provide%20a%20manually%20chosen%20set%20of%20prediction%20features.%20In%0Athis%20paper%2C%20we%20propose%20Heckman-FA%20as%20a%20novel%20data-driven%20framework%20for%0Aobtaining%20prediction%20features%20for%20the%20Heckman%20model.%20Heckman-FA%20first%20trains%20an%0Aassignment%20function%20that%20determines%20whether%20or%20not%20a%20selection%20feature%20is%0Aassigned%20as%20a%20prediction%20feature.%20Using%20the%20parameters%20of%20the%20trained%20function%2C%0Athe%20framework%20extracts%20a%20suitable%20set%20of%20prediction%20features%20based%20on%20the%0Agoodness-of-fit%20of%20the%20prediction%20model%20given%20the%20chosen%20prediction%20features%0Aand%20the%20correlation%20between%20noise%20terms%20of%20the%20prediction%20and%20selection%0Aequations.%20Experimental%20results%20on%20real-world%20datasets%20show%20that%20Heckman-FA%0Aproduces%20a%20robust%20regression%20model%20under%20MNAR%20sample%20selection%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08043v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Prediction%20Feature%20Assignment%20in%20the%20Heckman%20Selection%20Model&entry.906535625=Huy%20Mai%20and%20Xintao%20Wu&entry.1292438233=%20%20Under%20missing-not-at-random%20%28MNAR%29%20sample%20selection%20bias%2C%20the%20performance%20of%0Aa%20prediction%20model%20is%20often%20degraded.%20This%20paper%20focuses%20on%20one%20classic%0Ainstance%20of%20MNAR%20sample%20selection%20bias%20where%20a%20subset%20of%20samples%20have%0Anon-randomly%20missing%20outcomes.%20The%20Heckman%20selection%20model%20and%20its%20variants%0Ahave%20commonly%20been%20used%20to%20handle%20this%20type%20of%20sample%20selection%20bias.%20The%0AHeckman%20model%20uses%20two%20separate%20equations%20to%20model%20the%20prediction%20and%20selection%0Aof%20samples%2C%20where%20the%20selection%20features%20include%20all%20prediction%20features.%20When%0Ausing%20the%20Heckman%20model%2C%20the%20prediction%20features%20must%20be%20properly%20chosen%20from%0Athe%20set%20of%20selection%20features.%20However%2C%20choosing%20the%20proper%20prediction%20features%0Ais%20a%20challenging%20task%20for%20the%20Heckman%20model.%20This%20is%20especially%20the%20case%20when%0Athe%20number%20of%20selection%20features%20is%20large.%20Existing%20approaches%20that%20use%20the%0AHeckman%20model%20often%20provide%20a%20manually%20chosen%20set%20of%20prediction%20features.%20In%0Athis%20paper%2C%20we%20propose%20Heckman-FA%20as%20a%20novel%20data-driven%20framework%20for%0Aobtaining%20prediction%20features%20for%20the%20Heckman%20model.%20Heckman-FA%20first%20trains%20an%0Aassignment%20function%20that%20determines%20whether%20or%20not%20a%20selection%20feature%20is%0Aassigned%20as%20a%20prediction%20feature.%20Using%20the%20parameters%20of%20the%20trained%20function%2C%0Athe%20framework%20extracts%20a%20suitable%20set%20of%20prediction%20features%20based%20on%20the%0Agoodness-of-fit%20of%20the%20prediction%20model%20given%20the%20chosen%20prediction%20features%0Aand%20the%20correlation%20between%20noise%20terms%20of%20the%20prediction%20and%20selection%0Aequations.%20Experimental%20results%20on%20real-world%20datasets%20show%20that%20Heckman-FA%0Aproduces%20a%20robust%20regression%20model%20under%20MNAR%20sample%20selection%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08043v2&entry.124074799=Read"},
{"title": "Using Graph Neural Networks to Predict Local Culture", "author": "Thiago H Silva and Daniel Silver", "abstract": "  Urban research has long recognized that neighbourhoods are dynamic and\nrelational. However, lack of data, methodologies, and computer processing power\nhave hampered a formal quantitative examination of neighbourhood relational\ndynamics. To make progress on this issue, this study proposes a graph neural\nnetwork (GNN) approach that permits combining and evaluating multiple sources\nof information about internal characteristics of neighbourhoods, their past\ncharacteristics, and flows of groups among them, potentially providing greater\nexpressive power in predictive models. By exploring a public large-scale\ndataset from Yelp, we show the potential of our approach for considering\nstructural connectedness in predicting neighbourhood attributes, specifically\nto predict local culture. Results are promising from a substantive and\nmethodologically point of view. Substantively, we find that either local area\ninformation (e.g. area demographics) or group profiles (tastes of Yelp\nreviewers) give the best results in predicting local culture, and they are\nnearly equivalent in all studied cases. Methodologically, exploring group\nprofiles could be a helpful alternative where finding local information for\nspecific areas is challenging, since they can be extracted automatically from\nmany forms of online data. Thus, our approach could empower researchers and\npolicy-makers to use a range of data sources when other local area information\nis lacking.\n", "link": "http://arxiv.org/abs/2402.17905v3", "date": "2024-04-22", "relevancy": 1.7638, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4388}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture&body=Title%3A%20Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture%0AAuthor%3A%20Thiago%20H%20Silva%20and%20Daniel%20Silver%0AAbstract%3A%20%20%20Urban%20research%20has%20long%20recognized%20that%20neighbourhoods%20are%20dynamic%20and%0Arelational.%20However%2C%20lack%20of%20data%2C%20methodologies%2C%20and%20computer%20processing%20power%0Ahave%20hampered%20a%20formal%20quantitative%20examination%20of%20neighbourhood%20relational%0Adynamics.%20To%20make%20progress%20on%20this%20issue%2C%20this%20study%20proposes%20a%20graph%20neural%0Anetwork%20%28GNN%29%20approach%20that%20permits%20combining%20and%20evaluating%20multiple%20sources%0Aof%20information%20about%20internal%20characteristics%20of%20neighbourhoods%2C%20their%20past%0Acharacteristics%2C%20and%20flows%20of%20groups%20among%20them%2C%20potentially%20providing%20greater%0Aexpressive%20power%20in%20predictive%20models.%20By%20exploring%20a%20public%20large-scale%0Adataset%20from%20Yelp%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20considering%0Astructural%20connectedness%20in%20predicting%20neighbourhood%20attributes%2C%20specifically%0Ato%20predict%20local%20culture.%20Results%20are%20promising%20from%20a%20substantive%20and%0Amethodologically%20point%20of%20view.%20Substantively%2C%20we%20find%20that%20either%20local%20area%0Ainformation%20%28e.g.%20area%20demographics%29%20or%20group%20profiles%20%28tastes%20of%20Yelp%0Areviewers%29%20give%20the%20best%20results%20in%20predicting%20local%20culture%2C%20and%20they%20are%0Anearly%20equivalent%20in%20all%20studied%20cases.%20Methodologically%2C%20exploring%20group%0Aprofiles%20could%20be%20a%20helpful%20alternative%20where%20finding%20local%20information%20for%0Aspecific%20areas%20is%20challenging%2C%20since%20they%20can%20be%20extracted%20automatically%20from%0Amany%20forms%20of%20online%20data.%20Thus%2C%20our%20approach%20could%20empower%20researchers%20and%0Apolicy-makers%20to%20use%20a%20range%20of%20data%20sources%20when%20other%20local%20area%20information%0Ais%20lacking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17905v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture&entry.906535625=Thiago%20H%20Silva%20and%20Daniel%20Silver&entry.1292438233=%20%20Urban%20research%20has%20long%20recognized%20that%20neighbourhoods%20are%20dynamic%20and%0Arelational.%20However%2C%20lack%20of%20data%2C%20methodologies%2C%20and%20computer%20processing%20power%0Ahave%20hampered%20a%20formal%20quantitative%20examination%20of%20neighbourhood%20relational%0Adynamics.%20To%20make%20progress%20on%20this%20issue%2C%20this%20study%20proposes%20a%20graph%20neural%0Anetwork%20%28GNN%29%20approach%20that%20permits%20combining%20and%20evaluating%20multiple%20sources%0Aof%20information%20about%20internal%20characteristics%20of%20neighbourhoods%2C%20their%20past%0Acharacteristics%2C%20and%20flows%20of%20groups%20among%20them%2C%20potentially%20providing%20greater%0Aexpressive%20power%20in%20predictive%20models.%20By%20exploring%20a%20public%20large-scale%0Adataset%20from%20Yelp%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20considering%0Astructural%20connectedness%20in%20predicting%20neighbourhood%20attributes%2C%20specifically%0Ato%20predict%20local%20culture.%20Results%20are%20promising%20from%20a%20substantive%20and%0Amethodologically%20point%20of%20view.%20Substantively%2C%20we%20find%20that%20either%20local%20area%0Ainformation%20%28e.g.%20area%20demographics%29%20or%20group%20profiles%20%28tastes%20of%20Yelp%0Areviewers%29%20give%20the%20best%20results%20in%20predicting%20local%20culture%2C%20and%20they%20are%0Anearly%20equivalent%20in%20all%20studied%20cases.%20Methodologically%2C%20exploring%20group%0Aprofiles%20could%20be%20a%20helpful%20alternative%20where%20finding%20local%20information%20for%0Aspecific%20areas%20is%20challenging%2C%20since%20they%20can%20be%20extracted%20automatically%20from%0Amany%20forms%20of%20online%20data.%20Thus%2C%20our%20approach%20could%20empower%20researchers%20and%0Apolicy-makers%20to%20use%20a%20range%20of%20data%20sources%20when%20other%20local%20area%20information%0Ais%20lacking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17905v3&entry.124074799=Read"},
{"title": "3D Uncertain Implicit Surface Mapping using GMM and GP", "author": "Qianqian Zou and Monika Sester", "abstract": "  In this study, we address the challenge of constructing continuous\nthree-dimensional (3D) models that accurately represent uncertain surfaces,\nderived from noisy and incomplete LiDAR scanning data. Building upon our prior\nwork, which utilized the Gaussian Process (GP) and Gaussian Mixture Model (GMM)\nfor structured building models, we introduce a more generalized approach\ntailored for complex surfaces in urban scenes, where GMM Regression and GP with\nderivative observations are applied. A Hierarchical GMM (HGMM) is employed to\noptimize the number of GMM components and speed up the GMM training. With the\nprior map obtained from HGMM, GP inference is followed for the refinement of\nthe final map. Our approach models the implicit surface of the geo-object and\nenables the inference of the regions that are not completely covered by\nmeasurements. The integration of GMM and GP yields well-calibrated uncertainty\nestimates alongside the surface model, enhancing both accuracy and reliability.\nThe proposed method is evaluated on real data collected by a mobile mapping\nsystem. Compared to the performance in mapping accuracy and uncertainty\nquantification of other methods, such as Gaussian Process Implicit Surface map\n(GPIS) and log-Gaussian Process Implicit Surface map (Log-GPIS), the proposed\nmethod achieves lower RMSEs, higher log-likelihood values and lower\ncomputational costs for the evaluated datasets.\n", "link": "http://arxiv.org/abs/2403.07223v2", "date": "2024-04-22", "relevancy": 1.7483, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5361}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP&body=Title%3A%203D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP%0AAuthor%3A%20Qianqian%20Zou%20and%20Monika%20Sester%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20constructing%20continuous%0Athree-dimensional%20%283D%29%20models%20that%20accurately%20represent%20uncertain%20surfaces%2C%0Aderived%20from%20noisy%20and%20incomplete%20LiDAR%20scanning%20data.%20Building%20upon%20our%20prior%0Awork%2C%20which%20utilized%20the%20Gaussian%20Process%20%28GP%29%20and%20Gaussian%20Mixture%20Model%20%28GMM%29%0Afor%20structured%20building%20models%2C%20we%20introduce%20a%20more%20generalized%20approach%0Atailored%20for%20complex%20surfaces%20in%20urban%20scenes%2C%20where%20GMM%20Regression%20and%20GP%20with%0Aderivative%20observations%20are%20applied.%20A%20Hierarchical%20GMM%20%28HGMM%29%20is%20employed%20to%0Aoptimize%20the%20number%20of%20GMM%20components%20and%20speed%20up%20the%20GMM%20training.%20With%20the%0Aprior%20map%20obtained%20from%20HGMM%2C%20GP%20inference%20is%20followed%20for%20the%20refinement%20of%0Athe%20final%20map.%20Our%20approach%20models%20the%20implicit%20surface%20of%20the%20geo-object%20and%0Aenables%20the%20inference%20of%20the%20regions%20that%20are%20not%20completely%20covered%20by%0Ameasurements.%20The%20integration%20of%20GMM%20and%20GP%20yields%20well-calibrated%20uncertainty%0Aestimates%20alongside%20the%20surface%20model%2C%20enhancing%20both%20accuracy%20and%20reliability.%0AThe%20proposed%20method%20is%20evaluated%20on%20real%20data%20collected%20by%20a%20mobile%20mapping%0Asystem.%20Compared%20to%20the%20performance%20in%20mapping%20accuracy%20and%20uncertainty%0Aquantification%20of%20other%20methods%2C%20such%20as%20Gaussian%20Process%20Implicit%20Surface%20map%0A%28GPIS%29%20and%20log-Gaussian%20Process%20Implicit%20Surface%20map%20%28Log-GPIS%29%2C%20the%20proposed%0Amethod%20achieves%20lower%20RMSEs%2C%20higher%20log-likelihood%20values%20and%20lower%0Acomputational%20costs%20for%20the%20evaluated%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07223v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP&entry.906535625=Qianqian%20Zou%20and%20Monika%20Sester&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20constructing%20continuous%0Athree-dimensional%20%283D%29%20models%20that%20accurately%20represent%20uncertain%20surfaces%2C%0Aderived%20from%20noisy%20and%20incomplete%20LiDAR%20scanning%20data.%20Building%20upon%20our%20prior%0Awork%2C%20which%20utilized%20the%20Gaussian%20Process%20%28GP%29%20and%20Gaussian%20Mixture%20Model%20%28GMM%29%0Afor%20structured%20building%20models%2C%20we%20introduce%20a%20more%20generalized%20approach%0Atailored%20for%20complex%20surfaces%20in%20urban%20scenes%2C%20where%20GMM%20Regression%20and%20GP%20with%0Aderivative%20observations%20are%20applied.%20A%20Hierarchical%20GMM%20%28HGMM%29%20is%20employed%20to%0Aoptimize%20the%20number%20of%20GMM%20components%20and%20speed%20up%20the%20GMM%20training.%20With%20the%0Aprior%20map%20obtained%20from%20HGMM%2C%20GP%20inference%20is%20followed%20for%20the%20refinement%20of%0Athe%20final%20map.%20Our%20approach%20models%20the%20implicit%20surface%20of%20the%20geo-object%20and%0Aenables%20the%20inference%20of%20the%20regions%20that%20are%20not%20completely%20covered%20by%0Ameasurements.%20The%20integration%20of%20GMM%20and%20GP%20yields%20well-calibrated%20uncertainty%0Aestimates%20alongside%20the%20surface%20model%2C%20enhancing%20both%20accuracy%20and%20reliability.%0AThe%20proposed%20method%20is%20evaluated%20on%20real%20data%20collected%20by%20a%20mobile%20mapping%0Asystem.%20Compared%20to%20the%20performance%20in%20mapping%20accuracy%20and%20uncertainty%0Aquantification%20of%20other%20methods%2C%20such%20as%20Gaussian%20Process%20Implicit%20Surface%20map%0A%28GPIS%29%20and%20log-Gaussian%20Process%20Implicit%20Surface%20map%20%28Log-GPIS%29%2C%20the%20proposed%0Amethod%20achieves%20lower%20RMSEs%2C%20higher%20log-likelihood%20values%20and%20lower%0Acomputational%20costs%20for%20the%20evaluated%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07223v2&entry.124074799=Read"},
{"title": "Found in the Middle: Permutation Self-Consistency Improves Listwise\n  Ranking in Large Language Models", "author": "Raphael Tang and Xinyu Zhang and Xueguang Ma and Jimmy Lin and Ferhan Ture", "abstract": "  Large language models (LLMs) exhibit positional bias in how they use context,\nwhich especially complicates listwise ranking. To address this, we propose\npermutation self-consistency, a form of self-consistency over ranking list\noutputs of black-box LLMs. Our key idea is to marginalize out different list\norders in the prompt to produce an order-independent ranking with less\npositional bias. First, given some input prompt, we repeatedly shuffle the list\nin the prompt and pass it through the LLM while holding the instructions the\nsame. Next, we aggregate the resulting sample of rankings by computing the\ncentral ranking closest in distance to all of them, marginalizing out prompt\norder biases in the process. Theoretically, we prove the robustness of our\nmethod, showing convergence to the true ranking in the presence of random\nperturbations. Empirically, on five list-ranking datasets in sorting and\npassage reranking, our approach improves scores from conventional inference by\nup to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous\nstate of the art in passage reranking. Our code is at\nhttps://github.com/castorini/perm-sc.\n", "link": "http://arxiv.org/abs/2310.07712v2", "date": "2024-04-22", "relevancy": 1.741, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.425}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Found%20in%20the%20Middle%3A%20Permutation%20Self-Consistency%20Improves%20Listwise%0A%20%20Ranking%20in%20Large%20Language%20Models&body=Title%3A%20Found%20in%20the%20Middle%3A%20Permutation%20Self-Consistency%20Improves%20Listwise%0A%20%20Ranking%20in%20Large%20Language%20Models%0AAuthor%3A%20Raphael%20Tang%20and%20Xinyu%20Zhang%20and%20Xueguang%20Ma%20and%20Jimmy%20Lin%20and%20Ferhan%20Ture%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20positional%20bias%20in%20how%20they%20use%20context%2C%0Awhich%20especially%20complicates%20listwise%20ranking.%20To%20address%20this%2C%20we%20propose%0Apermutation%20self-consistency%2C%20a%20form%20of%20self-consistency%20over%20ranking%20list%0Aoutputs%20of%20black-box%20LLMs.%20Our%20key%20idea%20is%20to%20marginalize%20out%20different%20list%0Aorders%20in%20the%20prompt%20to%20produce%20an%20order-independent%20ranking%20with%20less%0Apositional%20bias.%20First%2C%20given%20some%20input%20prompt%2C%20we%20repeatedly%20shuffle%20the%20list%0Ain%20the%20prompt%20and%20pass%20it%20through%20the%20LLM%20while%20holding%20the%20instructions%20the%0Asame.%20Next%2C%20we%20aggregate%20the%20resulting%20sample%20of%20rankings%20by%20computing%20the%0Acentral%20ranking%20closest%20in%20distance%20to%20all%20of%20them%2C%20marginalizing%20out%20prompt%0Aorder%20biases%20in%20the%20process.%20Theoretically%2C%20we%20prove%20the%20robustness%20of%20our%0Amethod%2C%20showing%20convergence%20to%20the%20true%20ranking%20in%20the%20presence%20of%20random%0Aperturbations.%20Empirically%2C%20on%20five%20list-ranking%20datasets%20in%20sorting%20and%0Apassage%20reranking%2C%20our%20approach%20improves%20scores%20from%20conventional%20inference%20by%0Aup%20to%207-18%25%20for%20GPT-3.5%20and%208-16%25%20for%20LLaMA%20v2%20%2870B%29%2C%20surpassing%20the%20previous%0Astate%20of%20the%20art%20in%20passage%20reranking.%20Our%20code%20is%20at%0Ahttps%3A//github.com/castorini/perm-sc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07712v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Found%20in%20the%20Middle%3A%20Permutation%20Self-Consistency%20Improves%20Listwise%0A%20%20Ranking%20in%20Large%20Language%20Models&entry.906535625=Raphael%20Tang%20and%20Xinyu%20Zhang%20and%20Xueguang%20Ma%20and%20Jimmy%20Lin%20and%20Ferhan%20Ture&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20positional%20bias%20in%20how%20they%20use%20context%2C%0Awhich%20especially%20complicates%20listwise%20ranking.%20To%20address%20this%2C%20we%20propose%0Apermutation%20self-consistency%2C%20a%20form%20of%20self-consistency%20over%20ranking%20list%0Aoutputs%20of%20black-box%20LLMs.%20Our%20key%20idea%20is%20to%20marginalize%20out%20different%20list%0Aorders%20in%20the%20prompt%20to%20produce%20an%20order-independent%20ranking%20with%20less%0Apositional%20bias.%20First%2C%20given%20some%20input%20prompt%2C%20we%20repeatedly%20shuffle%20the%20list%0Ain%20the%20prompt%20and%20pass%20it%20through%20the%20LLM%20while%20holding%20the%20instructions%20the%0Asame.%20Next%2C%20we%20aggregate%20the%20resulting%20sample%20of%20rankings%20by%20computing%20the%0Acentral%20ranking%20closest%20in%20distance%20to%20all%20of%20them%2C%20marginalizing%20out%20prompt%0Aorder%20biases%20in%20the%20process.%20Theoretically%2C%20we%20prove%20the%20robustness%20of%20our%0Amethod%2C%20showing%20convergence%20to%20the%20true%20ranking%20in%20the%20presence%20of%20random%0Aperturbations.%20Empirically%2C%20on%20five%20list-ranking%20datasets%20in%20sorting%20and%0Apassage%20reranking%2C%20our%20approach%20improves%20scores%20from%20conventional%20inference%20by%0Aup%20to%207-18%25%20for%20GPT-3.5%20and%208-16%25%20for%20LLaMA%20v2%20%2870B%29%2C%20surpassing%20the%20previous%0Astate%20of%20the%20art%20in%20passage%20reranking.%20Our%20code%20is%20at%0Ahttps%3A//github.com/castorini/perm-sc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07712v2&entry.124074799=Read"},
{"title": "DynaMMo: Dynamic Model Merging for Efficient Class Incremental Learning\n  for Medical Images", "author": "Mohammad Areeb Qazi and Ibrahim Almakky and Anees Ur Rehman Hashmi and Santosh Sanjeev and Mohammad Yaqub", "abstract": "  Continual learning, the ability to acquire knowledge from new data while\nretaining previously learned information, is a fundamental challenge in machine\nlearning. Various approaches, including memory replay, knowledge distillation,\nmodel regularization, and dynamic network expansion, have been proposed to\naddress this issue. Thus far, dynamic network expansion methods have achieved\nstate-of-the-art performance at the cost of incurring significant computational\noverhead. This is due to the need for additional model buffers, which makes it\nless feasible in resource-constrained settings, particularly in the medical\ndomain. To overcome this challenge, we propose Dynamic Model Merging, DynaMMo,\na method that merges multiple networks at different stages of model training to\nachieve better computational efficiency. Specifically, we employ lightweight\nlearnable modules for each task and combine them into a unified model to\nminimize computational overhead. DynaMMo achieves this without compromising\nperformance, offering a cost-effective solution for continual learning in\nmedical applications. We evaluate DynaMMo on three publicly available datasets,\ndemonstrating its effectiveness compared to existing approaches. DynaMMo offers\naround 10-fold reduction in GFLOPS with a small drop of 2.76 in average\naccuracy when compared to state-of-the-art dynamic-based approaches. The code\nimplementation of this work will be available upon the acceptance of this work\nat https://github.com/BioMedIA-MBZUAI/DynaMMo.\n", "link": "http://arxiv.org/abs/2404.14099v1", "date": "2024-04-22", "relevancy": 1.7246, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5982}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5438}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DynaMMo%3A%20Dynamic%20Model%20Merging%20for%20Efficient%20Class%20Incremental%20Learning%0A%20%20for%20Medical%20Images&body=Title%3A%20DynaMMo%3A%20Dynamic%20Model%20Merging%20for%20Efficient%20Class%20Incremental%20Learning%0A%20%20for%20Medical%20Images%0AAuthor%3A%20Mohammad%20Areeb%20Qazi%20and%20Ibrahim%20Almakky%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Santosh%20Sanjeev%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Continual%20learning%2C%20the%20ability%20to%20acquire%20knowledge%20from%20new%20data%20while%0Aretaining%20previously%20learned%20information%2C%20is%20a%20fundamental%20challenge%20in%20machine%0Alearning.%20Various%20approaches%2C%20including%20memory%20replay%2C%20knowledge%20distillation%2C%0Amodel%20regularization%2C%20and%20dynamic%20network%20expansion%2C%20have%20been%20proposed%20to%0Aaddress%20this%20issue.%20Thus%20far%2C%20dynamic%20network%20expansion%20methods%20have%20achieved%0Astate-of-the-art%20performance%20at%20the%20cost%20of%20incurring%20significant%20computational%0Aoverhead.%20This%20is%20due%20to%20the%20need%20for%20additional%20model%20buffers%2C%20which%20makes%20it%0Aless%20feasible%20in%20resource-constrained%20settings%2C%20particularly%20in%20the%20medical%0Adomain.%20To%20overcome%20this%20challenge%2C%20we%20propose%20Dynamic%20Model%20Merging%2C%20DynaMMo%2C%0Aa%20method%20that%20merges%20multiple%20networks%20at%20different%20stages%20of%20model%20training%20to%0Aachieve%20better%20computational%20efficiency.%20Specifically%2C%20we%20employ%20lightweight%0Alearnable%20modules%20for%20each%20task%20and%20combine%20them%20into%20a%20unified%20model%20to%0Aminimize%20computational%20overhead.%20DynaMMo%20achieves%20this%20without%20compromising%0Aperformance%2C%20offering%20a%20cost-effective%20solution%20for%20continual%20learning%20in%0Amedical%20applications.%20We%20evaluate%20DynaMMo%20on%20three%20publicly%20available%20datasets%2C%0Ademonstrating%20its%20effectiveness%20compared%20to%20existing%20approaches.%20DynaMMo%20offers%0Aaround%2010-fold%20reduction%20in%20GFLOPS%20with%20a%20small%20drop%20of%202.76%20in%20average%0Aaccuracy%20when%20compared%20to%20state-of-the-art%20dynamic-based%20approaches.%20The%20code%0Aimplementation%20of%20this%20work%20will%20be%20available%20upon%20the%20acceptance%20of%20this%20work%0Aat%20https%3A//github.com/BioMedIA-MBZUAI/DynaMMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14099v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMMo%3A%20Dynamic%20Model%20Merging%20for%20Efficient%20Class%20Incremental%20Learning%0A%20%20for%20Medical%20Images&entry.906535625=Mohammad%20Areeb%20Qazi%20and%20Ibrahim%20Almakky%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Santosh%20Sanjeev%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Continual%20learning%2C%20the%20ability%20to%20acquire%20knowledge%20from%20new%20data%20while%0Aretaining%20previously%20learned%20information%2C%20is%20a%20fundamental%20challenge%20in%20machine%0Alearning.%20Various%20approaches%2C%20including%20memory%20replay%2C%20knowledge%20distillation%2C%0Amodel%20regularization%2C%20and%20dynamic%20network%20expansion%2C%20have%20been%20proposed%20to%0Aaddress%20this%20issue.%20Thus%20far%2C%20dynamic%20network%20expansion%20methods%20have%20achieved%0Astate-of-the-art%20performance%20at%20the%20cost%20of%20incurring%20significant%20computational%0Aoverhead.%20This%20is%20due%20to%20the%20need%20for%20additional%20model%20buffers%2C%20which%20makes%20it%0Aless%20feasible%20in%20resource-constrained%20settings%2C%20particularly%20in%20the%20medical%0Adomain.%20To%20overcome%20this%20challenge%2C%20we%20propose%20Dynamic%20Model%20Merging%2C%20DynaMMo%2C%0Aa%20method%20that%20merges%20multiple%20networks%20at%20different%20stages%20of%20model%20training%20to%0Aachieve%20better%20computational%20efficiency.%20Specifically%2C%20we%20employ%20lightweight%0Alearnable%20modules%20for%20each%20task%20and%20combine%20them%20into%20a%20unified%20model%20to%0Aminimize%20computational%20overhead.%20DynaMMo%20achieves%20this%20without%20compromising%0Aperformance%2C%20offering%20a%20cost-effective%20solution%20for%20continual%20learning%20in%0Amedical%20applications.%20We%20evaluate%20DynaMMo%20on%20three%20publicly%20available%20datasets%2C%0Ademonstrating%20its%20effectiveness%20compared%20to%20existing%20approaches.%20DynaMMo%20offers%0Aaround%2010-fold%20reduction%20in%20GFLOPS%20with%20a%20small%20drop%20of%202.76%20in%20average%0Aaccuracy%20when%20compared%20to%20state-of-the-art%20dynamic-based%20approaches.%20The%20code%0Aimplementation%20of%20this%20work%20will%20be%20available%20upon%20the%20acceptance%20of%20this%20work%0Aat%20https%3A//github.com/BioMedIA-MBZUAI/DynaMMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14099v1&entry.124074799=Read"},
{"title": "What do Transformers Know about Government?", "author": "Jue Hou and Anisia Katinskaia and Lari Kotilainen and Sathianpong Trangcasanchai and Anh-Duc Vu and Roman Yangarber", "abstract": "  This paper investigates what insights about linguistic features and what\nknowledge about the structure of natural language can be obtained from the\nencodings in transformer language models.In particular, we explore how BERT\nencodes the government relation between constituents in a sentence. We use\nseveral probing classifiers, and data from two morphologically rich languages.\nOur experiments show that information about government is encoded across all\ntransformer layers, but predominantly in the early layers of the model. We find\nthat, for both languages, a small number of attention heads encode enough\ninformation about the government relations to enable us to train a classifier\ncapable of discovering new, previously unknown types of government, never seen\nin the training data. Currently, data is lacking for the research community\nworking on grammatical constructions, and government in particular. We release\nthe Government Bank -- a dataset defining the government relations for\nthousands of lemmas in the languages in our experiments.\n", "link": "http://arxiv.org/abs/2404.14270v1", "date": "2024-04-22", "relevancy": 1.7114, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4535}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20What%20do%20Transformers%20Know%20about%20Government%3F&body=Title%3A%20What%20do%20Transformers%20Know%20about%20Government%3F%0AAuthor%3A%20Jue%20Hou%20and%20Anisia%20Katinskaia%20and%20Lari%20Kotilainen%20and%20Sathianpong%20Trangcasanchai%20and%20Anh-Duc%20Vu%20and%20Roman%20Yangarber%0AAbstract%3A%20%20%20This%20paper%20investigates%20what%20insights%20about%20linguistic%20features%20and%20what%0Aknowledge%20about%20the%20structure%20of%20natural%20language%20can%20be%20obtained%20from%20the%0Aencodings%20in%20transformer%20language%20models.In%20particular%2C%20we%20explore%20how%20BERT%0Aencodes%20the%20government%20relation%20between%20constituents%20in%20a%20sentence.%20We%20use%0Aseveral%20probing%20classifiers%2C%20and%20data%20from%20two%20morphologically%20rich%20languages.%0AOur%20experiments%20show%20that%20information%20about%20government%20is%20encoded%20across%20all%0Atransformer%20layers%2C%20but%20predominantly%20in%20the%20early%20layers%20of%20the%20model.%20We%20find%0Athat%2C%20for%20both%20languages%2C%20a%20small%20number%20of%20attention%20heads%20encode%20enough%0Ainformation%20about%20the%20government%20relations%20to%20enable%20us%20to%20train%20a%20classifier%0Acapable%20of%20discovering%20new%2C%20previously%20unknown%20types%20of%20government%2C%20never%20seen%0Ain%20the%20training%20data.%20Currently%2C%20data%20is%20lacking%20for%20the%20research%20community%0Aworking%20on%20grammatical%20constructions%2C%20and%20government%20in%20particular.%20We%20release%0Athe%20Government%20Bank%20--%20a%20dataset%20defining%20the%20government%20relations%20for%0Athousands%20of%20lemmas%20in%20the%20languages%20in%20our%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14270v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20do%20Transformers%20Know%20about%20Government%3F&entry.906535625=Jue%20Hou%20and%20Anisia%20Katinskaia%20and%20Lari%20Kotilainen%20and%20Sathianpong%20Trangcasanchai%20and%20Anh-Duc%20Vu%20and%20Roman%20Yangarber&entry.1292438233=%20%20This%20paper%20investigates%20what%20insights%20about%20linguistic%20features%20and%20what%0Aknowledge%20about%20the%20structure%20of%20natural%20language%20can%20be%20obtained%20from%20the%0Aencodings%20in%20transformer%20language%20models.In%20particular%2C%20we%20explore%20how%20BERT%0Aencodes%20the%20government%20relation%20between%20constituents%20in%20a%20sentence.%20We%20use%0Aseveral%20probing%20classifiers%2C%20and%20data%20from%20two%20morphologically%20rich%20languages.%0AOur%20experiments%20show%20that%20information%20about%20government%20is%20encoded%20across%20all%0Atransformer%20layers%2C%20but%20predominantly%20in%20the%20early%20layers%20of%20the%20model.%20We%20find%0Athat%2C%20for%20both%20languages%2C%20a%20small%20number%20of%20attention%20heads%20encode%20enough%0Ainformation%20about%20the%20government%20relations%20to%20enable%20us%20to%20train%20a%20classifier%0Acapable%20of%20discovering%20new%2C%20previously%20unknown%20types%20of%20government%2C%20never%20seen%0Ain%20the%20training%20data.%20Currently%2C%20data%20is%20lacking%20for%20the%20research%20community%0Aworking%20on%20grammatical%20constructions%2C%20and%20government%20in%20particular.%20We%20release%0Athe%20Government%20Bank%20--%20a%20dataset%20defining%20the%20government%20relations%20for%0Athousands%20of%20lemmas%20in%20the%20languages%20in%20our%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14270v1&entry.124074799=Read"},
{"title": "OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework\n  for 3D Occupancy Prediction", "author": "Zhenxing Ming and Julie Stephany Berrio and Mao Shan and Stewart Worrall", "abstract": "  This paper introduces OccFusion, a straightforward and efficient sensor\nfusion framework for predicting 3D occupancy. A comprehensive understanding of\n3D scenes is crucial in autonomous driving, and recent models for 3D semantic\noccupancy prediction have successfully addressed the challenge of describing\nreal-world objects with varied shapes and classes. However, existing methods\nfor 3D occupancy prediction heavily rely on surround-view camera images, making\nthem susceptible to changes in lighting and weather conditions. By integrating\nfeatures from additional sensors, such as lidar and surround view radars, our\nframework enhances the accuracy and robustness of occupancy prediction,\nresulting in top-tier performance on the nuScenes benchmark. Furthermore,\nextensive experiments conducted on the nuScenes dataset, including challenging\nnight and rainy scenarios, confirm the superior performance of our sensor\nfusion strategy across various perception ranges. The code for this framework\nwill be made available at https://github.com/DanielMing123/OCCFusion.\n", "link": "http://arxiv.org/abs/2403.01644v3", "date": "2024-04-22", "relevancy": 1.6938, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6012}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OccFusion%3A%20A%20Straightforward%20and%20Effective%20Multi-Sensor%20Fusion%20Framework%0A%20%20for%203D%20Occupancy%20Prediction&body=Title%3A%20OccFusion%3A%20A%20Straightforward%20and%20Effective%20Multi-Sensor%20Fusion%20Framework%0A%20%20for%203D%20Occupancy%20Prediction%0AAuthor%3A%20Zhenxing%20Ming%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Stewart%20Worrall%0AAbstract%3A%20%20%20This%20paper%20introduces%20OccFusion%2C%20a%20straightforward%20and%20efficient%20sensor%0Afusion%20framework%20for%20predicting%203D%20occupancy.%20A%20comprehensive%20understanding%20of%0A3D%20scenes%20is%20crucial%20in%20autonomous%20driving%2C%20and%20recent%20models%20for%203D%20semantic%0Aoccupancy%20prediction%20have%20successfully%20addressed%20the%20challenge%20of%20describing%0Areal-world%20objects%20with%20varied%20shapes%20and%20classes.%20However%2C%20existing%20methods%0Afor%203D%20occupancy%20prediction%20heavily%20rely%20on%20surround-view%20camera%20images%2C%20making%0Athem%20susceptible%20to%20changes%20in%20lighting%20and%20weather%20conditions.%20By%20integrating%0Afeatures%20from%20additional%20sensors%2C%20such%20as%20lidar%20and%20surround%20view%20radars%2C%20our%0Aframework%20enhances%20the%20accuracy%20and%20robustness%20of%20occupancy%20prediction%2C%0Aresulting%20in%20top-tier%20performance%20on%20the%20nuScenes%20benchmark.%20Furthermore%2C%0Aextensive%20experiments%20conducted%20on%20the%20nuScenes%20dataset%2C%20including%20challenging%0Anight%20and%20rainy%20scenarios%2C%20confirm%20the%20superior%20performance%20of%20our%20sensor%0Afusion%20strategy%20across%20various%20perception%20ranges.%20The%20code%20for%20this%20framework%0Awill%20be%20made%20available%20at%20https%3A//github.com/DanielMing123/OCCFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01644v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccFusion%3A%20A%20Straightforward%20and%20Effective%20Multi-Sensor%20Fusion%20Framework%0A%20%20for%203D%20Occupancy%20Prediction&entry.906535625=Zhenxing%20Ming%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Stewart%20Worrall&entry.1292438233=%20%20This%20paper%20introduces%20OccFusion%2C%20a%20straightforward%20and%20efficient%20sensor%0Afusion%20framework%20for%20predicting%203D%20occupancy.%20A%20comprehensive%20understanding%20of%0A3D%20scenes%20is%20crucial%20in%20autonomous%20driving%2C%20and%20recent%20models%20for%203D%20semantic%0Aoccupancy%20prediction%20have%20successfully%20addressed%20the%20challenge%20of%20describing%0Areal-world%20objects%20with%20varied%20shapes%20and%20classes.%20However%2C%20existing%20methods%0Afor%203D%20occupancy%20prediction%20heavily%20rely%20on%20surround-view%20camera%20images%2C%20making%0Athem%20susceptible%20to%20changes%20in%20lighting%20and%20weather%20conditions.%20By%20integrating%0Afeatures%20from%20additional%20sensors%2C%20such%20as%20lidar%20and%20surround%20view%20radars%2C%20our%0Aframework%20enhances%20the%20accuracy%20and%20robustness%20of%20occupancy%20prediction%2C%0Aresulting%20in%20top-tier%20performance%20on%20the%20nuScenes%20benchmark.%20Furthermore%2C%0Aextensive%20experiments%20conducted%20on%20the%20nuScenes%20dataset%2C%20including%20challenging%0Anight%20and%20rainy%20scenarios%2C%20confirm%20the%20superior%20performance%20of%20our%20sensor%0Afusion%20strategy%20across%20various%20perception%20ranges.%20The%20code%20for%20this%20framework%0Awill%20be%20made%20available%20at%20https%3A//github.com/DanielMing123/OCCFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01644v3&entry.124074799=Read"},
{"title": "TWIMP: Two-Wheel Inverted Musculoskeletal Pendulum as a Learning Control\n  Platform in the Real World with Environmental Physical Contact", "author": "Kento Kawaharazuka and Tasuku Makabe and Shogo Makino and Kei Tsuzuki and Yuya Nagamatsu and Yuki Asano and Takuma Shirai and Fumihito Sugai and Kei Okada and Koji Kawasaki and Masayuki Inaba", "abstract": "  By the recent spread of machine learning in the robotics field, a humanoid\nthat can act, perceive, and learn in the real world through contact with the\nenvironment needs to be developed. In this study, as one of the choices, we\npropose a novel humanoid TWIMP, which combines a human mimetic musculoskeletal\nupper limb with a two-wheel inverted pendulum. By combining the benefit of a\nmusculoskeletal humanoid, which can achieve soft contact with the external\nenvironment, and the benefit of a two-wheel inverted pendulum with a small\nfootprint and high mobility, we can easily investigate learning control systems\nin environments with contact and sudden impact. We reveal our whole concept and\nsystem details of TWIMP, and execute several preliminary experiments to show\nits potential ability.\n", "link": "http://arxiv.org/abs/2404.14080v1", "date": "2024-04-22", "relevancy": 1.6752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5809}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5772}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TWIMP%3A%20Two-Wheel%20Inverted%20Musculoskeletal%20Pendulum%20as%20a%20Learning%20Control%0A%20%20Platform%20in%20the%20Real%20World%20with%20Environmental%20Physical%20Contact&body=Title%3A%20TWIMP%3A%20Two-Wheel%20Inverted%20Musculoskeletal%20Pendulum%20as%20a%20Learning%20Control%0A%20%20Platform%20in%20the%20Real%20World%20with%20Environmental%20Physical%20Contact%0AAuthor%3A%20Kento%20Kawaharazuka%20and%20Tasuku%20Makabe%20and%20Shogo%20Makino%20and%20Kei%20Tsuzuki%20and%20Yuya%20Nagamatsu%20and%20Yuki%20Asano%20and%20Takuma%20Shirai%20and%20Fumihito%20Sugai%20and%20Kei%20Okada%20and%20Koji%20Kawasaki%20and%20Masayuki%20Inaba%0AAbstract%3A%20%20%20By%20the%20recent%20spread%20of%20machine%20learning%20in%20the%20robotics%20field%2C%20a%20humanoid%0Athat%20can%20act%2C%20perceive%2C%20and%20learn%20in%20the%20real%20world%20through%20contact%20with%20the%0Aenvironment%20needs%20to%20be%20developed.%20In%20this%20study%2C%20as%20one%20of%20the%20choices%2C%20we%0Apropose%20a%20novel%20humanoid%20TWIMP%2C%20which%20combines%20a%20human%20mimetic%20musculoskeletal%0Aupper%20limb%20with%20a%20two-wheel%20inverted%20pendulum.%20By%20combining%20the%20benefit%20of%20a%0Amusculoskeletal%20humanoid%2C%20which%20can%20achieve%20soft%20contact%20with%20the%20external%0Aenvironment%2C%20and%20the%20benefit%20of%20a%20two-wheel%20inverted%20pendulum%20with%20a%20small%0Afootprint%20and%20high%20mobility%2C%20we%20can%20easily%20investigate%20learning%20control%20systems%0Ain%20environments%20with%20contact%20and%20sudden%20impact.%20We%20reveal%20our%20whole%20concept%20and%0Asystem%20details%20of%20TWIMP%2C%20and%20execute%20several%20preliminary%20experiments%20to%20show%0Aits%20potential%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14080v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TWIMP%3A%20Two-Wheel%20Inverted%20Musculoskeletal%20Pendulum%20as%20a%20Learning%20Control%0A%20%20Platform%20in%20the%20Real%20World%20with%20Environmental%20Physical%20Contact&entry.906535625=Kento%20Kawaharazuka%20and%20Tasuku%20Makabe%20and%20Shogo%20Makino%20and%20Kei%20Tsuzuki%20and%20Yuya%20Nagamatsu%20and%20Yuki%20Asano%20and%20Takuma%20Shirai%20and%20Fumihito%20Sugai%20and%20Kei%20Okada%20and%20Koji%20Kawasaki%20and%20Masayuki%20Inaba&entry.1292438233=%20%20By%20the%20recent%20spread%20of%20machine%20learning%20in%20the%20robotics%20field%2C%20a%20humanoid%0Athat%20can%20act%2C%20perceive%2C%20and%20learn%20in%20the%20real%20world%20through%20contact%20with%20the%0Aenvironment%20needs%20to%20be%20developed.%20In%20this%20study%2C%20as%20one%20of%20the%20choices%2C%20we%0Apropose%20a%20novel%20humanoid%20TWIMP%2C%20which%20combines%20a%20human%20mimetic%20musculoskeletal%0Aupper%20limb%20with%20a%20two-wheel%20inverted%20pendulum.%20By%20combining%20the%20benefit%20of%20a%0Amusculoskeletal%20humanoid%2C%20which%20can%20achieve%20soft%20contact%20with%20the%20external%0Aenvironment%2C%20and%20the%20benefit%20of%20a%20two-wheel%20inverted%20pendulum%20with%20a%20small%0Afootprint%20and%20high%20mobility%2C%20we%20can%20easily%20investigate%20learning%20control%20systems%0Ain%20environments%20with%20contact%20and%20sudden%20impact.%20We%20reveal%20our%20whole%20concept%20and%0Asystem%20details%20of%20TWIMP%2C%20and%20execute%20several%20preliminary%20experiments%20to%20show%0Aits%20potential%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14080v1&entry.124074799=Read"},
{"title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?", "author": "Adrian de Wynter and Ishaan Watts and Nektar Ege Alt\u0131ntoprak and Tua Wongsangaroonsri and Minghui Zhang and Noura Farra and Lena Baur and Samantha Claudet and Pavel Gajdusek and Can G\u00f6ren and Qilong Gu and Anna Kaminska and Tomasz Kaminski and Ruby Kuo and Akiko Kyuba and Jongho Lee and Kartik Mathur and Petter Merok and Ivana Milovanovi\u0107 and Nani Paananen and Vesa-Matti Paananen and Anna Pavlenko and Bruno Pereira Vidal and Luciano Strika and Yueh Tsao and Davide Turcato and Oleksandr Vakhno and Judit Velcsov and Anna Vickers and St\u00e9phanie Visser and Herdyan Widarmanto and Andrey Zaikin and Si-Qing Chen", "abstract": "  Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate seven S/LLMs on their ability to detect toxic\ncontent in a culturally-sensitive, multilingual scenario. We find that,\nalthough they typically score acceptably in terms of accuracy, they have low\nagreement with human judges when judging holistically the toxicity of a prompt,\nand have difficulty discerning harm in context-dependent scenarios,\nparticularly with subtle-yet-harmful content (e.g. microagressions, bias). We\nrelease of this dataset to contribute to further reduce harmful uses of these\nmodels and improve their safe deployment.\n", "link": "http://arxiv.org/abs/2404.14397v1", "date": "2024-04-22", "relevancy": 1.3269, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4459}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4223}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F&body=Title%3A%20RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F%0AAuthor%3A%20Adrian%20de%20Wynter%20and%20Ishaan%20Watts%20and%20Nektar%20Ege%20Alt%C4%B1ntoprak%20and%20Tua%20Wongsangaroonsri%20and%20Minghui%20Zhang%20and%20Noura%20Farra%20and%20Lena%20Baur%20and%20Samantha%20Claudet%20and%20Pavel%20Gajdusek%20and%20Can%20G%C3%B6ren%20and%20Qilong%20Gu%20and%20Anna%20Kaminska%20and%20Tomasz%20Kaminski%20and%20Ruby%20Kuo%20and%20Akiko%20Kyuba%20and%20Jongho%20Lee%20and%20Kartik%20Mathur%20and%20Petter%20Merok%20and%20Ivana%20Milovanovi%C4%87%20and%20Nani%20Paananen%20and%20Vesa-Matti%20Paananen%20and%20Anna%20Pavlenko%20and%20Bruno%20Pereira%20Vidal%20and%20Luciano%20Strika%20and%20Yueh%20Tsao%20and%20Davide%20Turcato%20and%20Oleksandr%20Vakhno%20and%20Judit%20Velcsov%20and%20Anna%20Vickers%20and%20St%C3%A9phanie%20Visser%20and%20Herdyan%20Widarmanto%20and%20Andrey%20Zaikin%20and%20Si-Qing%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20small%20language%20models%20%28SLMs%29%20are%20being%0Aadopted%20at%20remarkable%20speed%2C%20although%20their%20safety%20still%20remains%20a%20serious%0Aconcern.%20With%20the%20advent%20of%20multilingual%20S/LLMs%2C%20the%20question%20now%20becomes%20a%0Amatter%20of%20scale%3A%20can%20we%20expand%20multilingual%20safety%20evaluations%20of%20these%20models%0Awith%20the%20same%20velocity%20at%20which%20they%20are%20deployed%3F%20To%20this%20end%20we%20introduce%0ARTP-LX%2C%20a%20human-transcreated%20and%20human-annotated%20corpus%20of%20toxic%20prompts%20and%0Aoutputs%20in%2028%20languages.%20RTP-LX%20follows%20participatory%20design%20practices%2C%20and%20a%0Aportion%20of%20the%20corpus%20is%20especially%20designed%20to%20detect%20culturally-specific%0Atoxic%20language.%20We%20evaluate%20seven%20S/LLMs%20on%20their%20ability%20to%20detect%20toxic%0Acontent%20in%20a%20culturally-sensitive%2C%20multilingual%20scenario.%20We%20find%20that%2C%0Aalthough%20they%20typically%20score%20acceptably%20in%20terms%20of%20accuracy%2C%20they%20have%20low%0Aagreement%20with%20human%20judges%20when%20judging%20holistically%20the%20toxicity%20of%20a%20prompt%2C%0Aand%20have%20difficulty%20discerning%20harm%20in%20context-dependent%20scenarios%2C%0Aparticularly%20with%20subtle-yet-harmful%20content%20%28e.g.%20microagressions%2C%20bias%29.%20We%0Arelease%20of%20this%20dataset%20to%20contribute%20to%20further%20reduce%20harmful%20uses%20of%20these%0Amodels%20and%20improve%20their%20safe%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14397v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTP-LX%3A%20Can%20LLMs%20Evaluate%20Toxicity%20in%20Multilingual%20Scenarios%3F&entry.906535625=Adrian%20de%20Wynter%20and%20Ishaan%20Watts%20and%20Nektar%20Ege%20Alt%C4%B1ntoprak%20and%20Tua%20Wongsangaroonsri%20and%20Minghui%20Zhang%20and%20Noura%20Farra%20and%20Lena%20Baur%20and%20Samantha%20Claudet%20and%20Pavel%20Gajdusek%20and%20Can%20G%C3%B6ren%20and%20Qilong%20Gu%20and%20Anna%20Kaminska%20and%20Tomasz%20Kaminski%20and%20Ruby%20Kuo%20and%20Akiko%20Kyuba%20and%20Jongho%20Lee%20and%20Kartik%20Mathur%20and%20Petter%20Merok%20and%20Ivana%20Milovanovi%C4%87%20and%20Nani%20Paananen%20and%20Vesa-Matti%20Paananen%20and%20Anna%20Pavlenko%20and%20Bruno%20Pereira%20Vidal%20and%20Luciano%20Strika%20and%20Yueh%20Tsao%20and%20Davide%20Turcato%20and%20Oleksandr%20Vakhno%20and%20Judit%20Velcsov%20and%20Anna%20Vickers%20and%20St%C3%A9phanie%20Visser%20and%20Herdyan%20Widarmanto%20and%20Andrey%20Zaikin%20and%20Si-Qing%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20small%20language%20models%20%28SLMs%29%20are%20being%0Aadopted%20at%20remarkable%20speed%2C%20although%20their%20safety%20still%20remains%20a%20serious%0Aconcern.%20With%20the%20advent%20of%20multilingual%20S/LLMs%2C%20the%20question%20now%20becomes%20a%0Amatter%20of%20scale%3A%20can%20we%20expand%20multilingual%20safety%20evaluations%20of%20these%20models%0Awith%20the%20same%20velocity%20at%20which%20they%20are%20deployed%3F%20To%20this%20end%20we%20introduce%0ARTP-LX%2C%20a%20human-transcreated%20and%20human-annotated%20corpus%20of%20toxic%20prompts%20and%0Aoutputs%20in%2028%20languages.%20RTP-LX%20follows%20participatory%20design%20practices%2C%20and%20a%0Aportion%20of%20the%20corpus%20is%20especially%20designed%20to%20detect%20culturally-specific%0Atoxic%20language.%20We%20evaluate%20seven%20S/LLMs%20on%20their%20ability%20to%20detect%20toxic%0Acontent%20in%20a%20culturally-sensitive%2C%20multilingual%20scenario.%20We%20find%20that%2C%0Aalthough%20they%20typically%20score%20acceptably%20in%20terms%20of%20accuracy%2C%20they%20have%20low%0Aagreement%20with%20human%20judges%20when%20judging%20holistically%20the%20toxicity%20of%20a%20prompt%2C%0Aand%20have%20difficulty%20discerning%20harm%20in%20context-dependent%20scenarios%2C%0Aparticularly%20with%20subtle-yet-harmful%20content%20%28e.g.%20microagressions%2C%20bias%29.%20We%0Arelease%20of%20this%20dataset%20to%20contribute%20to%20further%20reduce%20harmful%20uses%20of%20these%0Amodels%20and%20improve%20their%20safe%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14397v1&entry.124074799=Read"},
{"title": "Does Your Neural Code Completion Model Use My Code? A Membership\n  Inference Approach", "author": "Yao Wan and Guanghua Wan and Shijie Zhang and Hongyu Zhang and Yulei Sui and Pan Zhou and Hai Jin and Lichao Sun", "abstract": "  Recent years have witnessed significant progress in developing deep\nlearning-based models for automated code completion. Although using source code\nin GitHub has been a common practice for training deep-learning-based models\nfor code completion, it may induce some legal and ethical issues such as\ncopyright infringement. In this paper, we investigate the legal and ethical\nissues of current neural code completion models by answering the following\nquestion: Is my code used to train your neural code completion model? To this\nend, we tailor a membership inference approach (termed CodeMI) that was\noriginally crafted for classification tasks to a more challenging task of code\ncompletion. In particular, since the target code completion models perform as\nopaque black boxes, preventing access to their training data and parameters, we\nopt to train multiple shadow models to mimic their behavior. The acquired\nposteriors from these shadow models are subsequently employed to train a\nmembership classifier. Subsequently, the membership classifier can be\neffectively employed to deduce the membership status of a given code sample\nbased on the output of a target code completion model. We comprehensively\nevaluate the effectiveness of this adapted approach across a diverse array of\nneural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and\nStarCoder). Experimental results reveal that the LSTM-based and CodeGPT models\nsuffer the membership leakage issue, which can be easily detected by our\nproposed membership inference approach with an accuracy of 0.842, and 0.730,\nrespectively. Interestingly, our experiments also show that the data membership\nof current large language models of code, e.g., CodeGen and StarCoder, is\ndifficult to detect, leaving amper space for further improvement. Finally, we\nalso try to explain the findings from the perspective of model memorization.\n", "link": "http://arxiv.org/abs/2404.14296v1", "date": "2024-04-22", "relevancy": 1.3972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4606}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Does%20Your%20Neural%20Code%20Completion%20Model%20Use%20My%20Code%3F%20A%20Membership%0A%20%20Inference%20Approach&body=Title%3A%20Does%20Your%20Neural%20Code%20Completion%20Model%20Use%20My%20Code%3F%20A%20Membership%0A%20%20Inference%20Approach%0AAuthor%3A%20Yao%20Wan%20and%20Guanghua%20Wan%20and%20Shijie%20Zhang%20and%20Hongyu%20Zhang%20and%20Yulei%20Sui%20and%20Pan%20Zhou%20and%20Hai%20Jin%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20developing%20deep%0Alearning-based%20models%20for%20automated%20code%20completion.%20Although%20using%20source%20code%0Ain%20GitHub%20has%20been%20a%20common%20practice%20for%20training%20deep-learning-based%20models%0Afor%20code%20completion%2C%20it%20may%20induce%20some%20legal%20and%20ethical%20issues%20such%20as%0Acopyright%20infringement.%20In%20this%20paper%2C%20we%20investigate%20the%20legal%20and%20ethical%0Aissues%20of%20current%20neural%20code%20completion%20models%20by%20answering%20the%20following%0Aquestion%3A%20Is%20my%20code%20used%20to%20train%20your%20neural%20code%20completion%20model%3F%20To%20this%0Aend%2C%20we%20tailor%20a%20membership%20inference%20approach%20%28termed%20CodeMI%29%20that%20was%0Aoriginally%20crafted%20for%20classification%20tasks%20to%20a%20more%20challenging%20task%20of%20code%0Acompletion.%20In%20particular%2C%20since%20the%20target%20code%20completion%20models%20perform%20as%0Aopaque%20black%20boxes%2C%20preventing%20access%20to%20their%20training%20data%20and%20parameters%2C%20we%0Aopt%20to%20train%20multiple%20shadow%20models%20to%20mimic%20their%20behavior.%20The%20acquired%0Aposteriors%20from%20these%20shadow%20models%20are%20subsequently%20employed%20to%20train%20a%0Amembership%20classifier.%20Subsequently%2C%20the%20membership%20classifier%20can%20be%0Aeffectively%20employed%20to%20deduce%20the%20membership%20status%20of%20a%20given%20code%20sample%0Abased%20on%20the%20output%20of%20a%20target%20code%20completion%20model.%20We%20comprehensively%0Aevaluate%20the%20effectiveness%20of%20this%20adapted%20approach%20across%20a%20diverse%20array%20of%0Aneural%20code%20completion%20models%2C%20%28i.e.%2C%20LSTM-based%2C%20CodeGPT%2C%20CodeGen%2C%20and%0AStarCoder%29.%20Experimental%20results%20reveal%20that%20the%20LSTM-based%20and%20CodeGPT%20models%0Asuffer%20the%20membership%20leakage%20issue%2C%20which%20can%20be%20easily%20detected%20by%20our%0Aproposed%20membership%20inference%20approach%20with%20an%20accuracy%20of%200.842%2C%20and%200.730%2C%0Arespectively.%20Interestingly%2C%20our%20experiments%20also%20show%20that%20the%20data%20membership%0Aof%20current%20large%20language%20models%20of%20code%2C%20e.g.%2C%20CodeGen%20and%20StarCoder%2C%20is%0Adifficult%20to%20detect%2C%20leaving%20amper%20space%20for%20further%20improvement.%20Finally%2C%20we%0Aalso%20try%20to%20explain%20the%20findings%20from%20the%20perspective%20of%20model%20memorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14296v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Your%20Neural%20Code%20Completion%20Model%20Use%20My%20Code%3F%20A%20Membership%0A%20%20Inference%20Approach&entry.906535625=Yao%20Wan%20and%20Guanghua%20Wan%20and%20Shijie%20Zhang%20and%20Hongyu%20Zhang%20and%20Yulei%20Sui%20and%20Pan%20Zhou%20and%20Hai%20Jin%20and%20Lichao%20Sun&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20developing%20deep%0Alearning-based%20models%20for%20automated%20code%20completion.%20Although%20using%20source%20code%0Ain%20GitHub%20has%20been%20a%20common%20practice%20for%20training%20deep-learning-based%20models%0Afor%20code%20completion%2C%20it%20may%20induce%20some%20legal%20and%20ethical%20issues%20such%20as%0Acopyright%20infringement.%20In%20this%20paper%2C%20we%20investigate%20the%20legal%20and%20ethical%0Aissues%20of%20current%20neural%20code%20completion%20models%20by%20answering%20the%20following%0Aquestion%3A%20Is%20my%20code%20used%20to%20train%20your%20neural%20code%20completion%20model%3F%20To%20this%0Aend%2C%20we%20tailor%20a%20membership%20inference%20approach%20%28termed%20CodeMI%29%20that%20was%0Aoriginally%20crafted%20for%20classification%20tasks%20to%20a%20more%20challenging%20task%20of%20code%0Acompletion.%20In%20particular%2C%20since%20the%20target%20code%20completion%20models%20perform%20as%0Aopaque%20black%20boxes%2C%20preventing%20access%20to%20their%20training%20data%20and%20parameters%2C%20we%0Aopt%20to%20train%20multiple%20shadow%20models%20to%20mimic%20their%20behavior.%20The%20acquired%0Aposteriors%20from%20these%20shadow%20models%20are%20subsequently%20employed%20to%20train%20a%0Amembership%20classifier.%20Subsequently%2C%20the%20membership%20classifier%20can%20be%0Aeffectively%20employed%20to%20deduce%20the%20membership%20status%20of%20a%20given%20code%20sample%0Abased%20on%20the%20output%20of%20a%20target%20code%20completion%20model.%20We%20comprehensively%0Aevaluate%20the%20effectiveness%20of%20this%20adapted%20approach%20across%20a%20diverse%20array%20of%0Aneural%20code%20completion%20models%2C%20%28i.e.%2C%20LSTM-based%2C%20CodeGPT%2C%20CodeGen%2C%20and%0AStarCoder%29.%20Experimental%20results%20reveal%20that%20the%20LSTM-based%20and%20CodeGPT%20models%0Asuffer%20the%20membership%20leakage%20issue%2C%20which%20can%20be%20easily%20detected%20by%20our%0Aproposed%20membership%20inference%20approach%20with%20an%20accuracy%20of%200.842%2C%20and%200.730%2C%0Arespectively.%20Interestingly%2C%20our%20experiments%20also%20show%20that%20the%20data%20membership%0Aof%20current%20large%20language%20models%20of%20code%2C%20e.g.%2C%20CodeGen%20and%20StarCoder%2C%20is%0Adifficult%20to%20detect%2C%20leaving%20amper%20space%20for%20further%20improvement.%20Finally%2C%20we%0Aalso%20try%20to%20explain%20the%20findings%20from%20the%20perspective%20of%20model%20memorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14296v1&entry.124074799=Read"},
{"title": "Scoring Intervals using Non-Hierarchical Transformer For Automatic Piano\n  Transcription", "author": "Yujia Yan and Zhiyao Duan", "abstract": "  The neural semi-Markov Conditional Random Field (semi-CRF) framework has\ndemonstrated promise for event-based piano transcription. In this framework,\nall events (notes or pedals) are represented as closed intervals tied to\nspecific event types. The neural semi-CRF approach requires an interval scoring\nmatrix that assigns a score for every candidate interval. However, designing an\nefficient and expressive architecture for scoring intervals is not trivial. In\nthis paper, we introduce a simple method for scoring intervals using scaled\ninner product operations that resemble how attention scoring is done in\ntransformers. We show theoretically that, due to the special structure from\nencoding the non-overlapping intervals, under a mild condition, the inner\nproduct operations are expressive enough to represent an ideal scoring matrix\nthat can yield the correct transcription result. We then demonstrate that an\nencoder-only non-hierarchical transformer backbone, operating only on a\nlow-time-resolution feature map, is capable of transcribing piano notes and\npedals with high accuracy and time precision. The experiment shows that our\napproach achieves the new state-of-the-art performance across all subtasks in\nterms of the F1 measure on the Maestro dataset.\n", "link": "http://arxiv.org/abs/2404.09466v3", "date": "2024-04-22", "relevancy": 1.3445, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4335}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scoring%20Intervals%20using%20Non-Hierarchical%20Transformer%20For%20Automatic%20Piano%0A%20%20Transcription&body=Title%3A%20Scoring%20Intervals%20using%20Non-Hierarchical%20Transformer%20For%20Automatic%20Piano%0A%20%20Transcription%0AAuthor%3A%20Yujia%20Yan%20and%20Zhiyao%20Duan%0AAbstract%3A%20%20%20The%20neural%20semi-Markov%20Conditional%20Random%20Field%20%28semi-CRF%29%20framework%20has%0Ademonstrated%20promise%20for%20event-based%20piano%20transcription.%20In%20this%20framework%2C%0Aall%20events%20%28notes%20or%20pedals%29%20are%20represented%20as%20closed%20intervals%20tied%20to%0Aspecific%20event%20types.%20The%20neural%20semi-CRF%20approach%20requires%20an%20interval%20scoring%0Amatrix%20that%20assigns%20a%20score%20for%20every%20candidate%20interval.%20However%2C%20designing%20an%0Aefficient%20and%20expressive%20architecture%20for%20scoring%20intervals%20is%20not%20trivial.%20In%0Athis%20paper%2C%20we%20introduce%20a%20simple%20method%20for%20scoring%20intervals%20using%20scaled%0Ainner%20product%20operations%20that%20resemble%20how%20attention%20scoring%20is%20done%20in%0Atransformers.%20We%20show%20theoretically%20that%2C%20due%20to%20the%20special%20structure%20from%0Aencoding%20the%20non-overlapping%20intervals%2C%20under%20a%20mild%20condition%2C%20the%20inner%0Aproduct%20operations%20are%20expressive%20enough%20to%20represent%20an%20ideal%20scoring%20matrix%0Athat%20can%20yield%20the%20correct%20transcription%20result.%20We%20then%20demonstrate%20that%20an%0Aencoder-only%20non-hierarchical%20transformer%20backbone%2C%20operating%20only%20on%20a%0Alow-time-resolution%20feature%20map%2C%20is%20capable%20of%20transcribing%20piano%20notes%20and%0Apedals%20with%20high%20accuracy%20and%20time%20precision.%20The%20experiment%20shows%20that%20our%0Aapproach%20achieves%20the%20new%20state-of-the-art%20performance%20across%20all%20subtasks%20in%0Aterms%20of%20the%20F1%20measure%20on%20the%20Maestro%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09466v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scoring%20Intervals%20using%20Non-Hierarchical%20Transformer%20For%20Automatic%20Piano%0A%20%20Transcription&entry.906535625=Yujia%20Yan%20and%20Zhiyao%20Duan&entry.1292438233=%20%20The%20neural%20semi-Markov%20Conditional%20Random%20Field%20%28semi-CRF%29%20framework%20has%0Ademonstrated%20promise%20for%20event-based%20piano%20transcription.%20In%20this%20framework%2C%0Aall%20events%20%28notes%20or%20pedals%29%20are%20represented%20as%20closed%20intervals%20tied%20to%0Aspecific%20event%20types.%20The%20neural%20semi-CRF%20approach%20requires%20an%20interval%20scoring%0Amatrix%20that%20assigns%20a%20score%20for%20every%20candidate%20interval.%20However%2C%20designing%20an%0Aefficient%20and%20expressive%20architecture%20for%20scoring%20intervals%20is%20not%20trivial.%20In%0Athis%20paper%2C%20we%20introduce%20a%20simple%20method%20for%20scoring%20intervals%20using%20scaled%0Ainner%20product%20operations%20that%20resemble%20how%20attention%20scoring%20is%20done%20in%0Atransformers.%20We%20show%20theoretically%20that%2C%20due%20to%20the%20special%20structure%20from%0Aencoding%20the%20non-overlapping%20intervals%2C%20under%20a%20mild%20condition%2C%20the%20inner%0Aproduct%20operations%20are%20expressive%20enough%20to%20represent%20an%20ideal%20scoring%20matrix%0Athat%20can%20yield%20the%20correct%20transcription%20result.%20We%20then%20demonstrate%20that%20an%0Aencoder-only%20non-hierarchical%20transformer%20backbone%2C%20operating%20only%20on%20a%0Alow-time-resolution%20feature%20map%2C%20is%20capable%20of%20transcribing%20piano%20notes%20and%0Apedals%20with%20high%20accuracy%20and%20time%20precision.%20The%20experiment%20shows%20that%20our%0Aapproach%20achieves%20the%20new%20state-of-the-art%20performance%20across%20all%20subtasks%20in%0Aterms%20of%20the%20F1%20measure%20on%20the%20Maestro%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09466v3&entry.124074799=Read"},
{"title": "Multidimensional Interpolants", "author": "Dohoon Lee and Kyogu Lee", "abstract": "  In the domain of differential equation-based generative modeling,\nconventional approaches often rely on single-dimensional scalar values as\ninterpolation coefficients during both training and inference phases. In this\nwork, we introduce, for the first time, a multidimensional interpolant that\nextends these coefficients into multiple dimensions, leveraging the stochastic\ninterpolant framework. Additionally, we propose a novel path optimization\nproblem tailored to adaptively determine multidimensional inference\ntrajectories, with a predetermined differential equation solver and a fixed\nnumber of function evaluations. Our solution involves simulation dynamics\ncoupled with adversarial training to optimize the inference path. Notably,\nemploying a multidimensional interpolant during training improves the model's\ninference performance, even in the absence of path optimization. When the\nadaptive, multidimensional path derived from our optimization process is\nemployed, it yields further performance gains, even with fixed solver\nconfigurations. The introduction of multidimensional interpolants not only\nenhances the efficacy of models but also opens up a new domain for exploration\nin training and inference methodologies, emphasizing the potential of\nmultidimensional paths as an untapped frontier.\n", "link": "http://arxiv.org/abs/2404.14161v1", "date": "2024-04-22", "relevancy": 1.0008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5035}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4866}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multidimensional%20Interpolants&body=Title%3A%20Multidimensional%20Interpolants%0AAuthor%3A%20Dohoon%20Lee%20and%20Kyogu%20Lee%0AAbstract%3A%20%20%20In%20the%20domain%20of%20differential%20equation-based%20generative%20modeling%2C%0Aconventional%20approaches%20often%20rely%20on%20single-dimensional%20scalar%20values%20as%0Ainterpolation%20coefficients%20during%20both%20training%20and%20inference%20phases.%20In%20this%0Awork%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20a%20multidimensional%20interpolant%20that%0Aextends%20these%20coefficients%20into%20multiple%20dimensions%2C%20leveraging%20the%20stochastic%0Ainterpolant%20framework.%20Additionally%2C%20we%20propose%20a%20novel%20path%20optimization%0Aproblem%20tailored%20to%20adaptively%20determine%20multidimensional%20inference%0Atrajectories%2C%20with%20a%20predetermined%20differential%20equation%20solver%20and%20a%20fixed%0Anumber%20of%20function%20evaluations.%20Our%20solution%20involves%20simulation%20dynamics%0Acoupled%20with%20adversarial%20training%20to%20optimize%20the%20inference%20path.%20Notably%2C%0Aemploying%20a%20multidimensional%20interpolant%20during%20training%20improves%20the%20model%27s%0Ainference%20performance%2C%20even%20in%20the%20absence%20of%20path%20optimization.%20When%20the%0Aadaptive%2C%20multidimensional%20path%20derived%20from%20our%20optimization%20process%20is%0Aemployed%2C%20it%20yields%20further%20performance%20gains%2C%20even%20with%20fixed%20solver%0Aconfigurations.%20The%20introduction%20of%20multidimensional%20interpolants%20not%20only%0Aenhances%20the%20efficacy%20of%20models%20but%20also%20opens%20up%20a%20new%20domain%20for%20exploration%0Ain%20training%20and%20inference%20methodologies%2C%20emphasizing%20the%20potential%20of%0Amultidimensional%20paths%20as%20an%20untapped%20frontier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14161v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multidimensional%20Interpolants&entry.906535625=Dohoon%20Lee%20and%20Kyogu%20Lee&entry.1292438233=%20%20In%20the%20domain%20of%20differential%20equation-based%20generative%20modeling%2C%0Aconventional%20approaches%20often%20rely%20on%20single-dimensional%20scalar%20values%20as%0Ainterpolation%20coefficients%20during%20both%20training%20and%20inference%20phases.%20In%20this%0Awork%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20a%20multidimensional%20interpolant%20that%0Aextends%20these%20coefficients%20into%20multiple%20dimensions%2C%20leveraging%20the%20stochastic%0Ainterpolant%20framework.%20Additionally%2C%20we%20propose%20a%20novel%20path%20optimization%0Aproblem%20tailored%20to%20adaptively%20determine%20multidimensional%20inference%0Atrajectories%2C%20with%20a%20predetermined%20differential%20equation%20solver%20and%20a%20fixed%0Anumber%20of%20function%20evaluations.%20Our%20solution%20involves%20simulation%20dynamics%0Acoupled%20with%20adversarial%20training%20to%20optimize%20the%20inference%20path.%20Notably%2C%0Aemploying%20a%20multidimensional%20interpolant%20during%20training%20improves%20the%20model%27s%0Ainference%20performance%2C%20even%20in%20the%20absence%20of%20path%20optimization.%20When%20the%0Aadaptive%2C%20multidimensional%20path%20derived%20from%20our%20optimization%20process%20is%0Aemployed%2C%20it%20yields%20further%20performance%20gains%2C%20even%20with%20fixed%20solver%0Aconfigurations.%20The%20introduction%20of%20multidimensional%20interpolants%20not%20only%0Aenhances%20the%20efficacy%20of%20models%20but%20also%20opens%20up%20a%20new%20domain%20for%20exploration%0Ain%20training%20and%20inference%20methodologies%2C%20emphasizing%20the%20potential%20of%0Amultidimensional%20paths%20as%20an%20untapped%20frontier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14161v1&entry.124074799=Read"},
{"title": "How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study", "author": "Wei Huang and Xudong Ma and Haotong Qin and Xingyu Zheng and Chengtao Lv and Hong Chen and Jie Luo and Xiaojuan Qi and Xianglong Liu and Michele Magno", "abstract": "  Meta's LLaMA family has become one of the most powerful open-source Large\nLanguage Model (LLM) series. Notably, LLaMA3 models have recently been released\nand achieve impressive performance across various with super-large scale\npre-training on over 15T tokens of data. Given the wide application of low-bit\nquantization for LLMs in resource-limited scenarios, we explore LLaMA3's\ncapabilities when quantized to low bit-width. This exploration holds the\npotential to unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we evaluate\nthe 10 existing post-training quantization and LoRA-finetuning methods of\nLLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's\nlow-bit quantization performance. Our experiment results indicate that LLaMA3\nstill suffers non-negligent degradation in these scenarios, especially in\nultra-low bit-width. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, pushing the\nLLMs to lower bit-width with higher accuracy for being practical. Our project\nis released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized\nLLaMA3 models are released in https://huggingface.co/LLMQ.\n", "link": "http://arxiv.org/abs/2404.14047v1", "date": "2024-04-22", "relevancy": 1.4011, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4894}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4644}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4512}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20Good%20Are%20Low-bit%20Quantized%20LLaMA3%20Models%3F%20An%20Empirical%20Study&body=Title%3A%20How%20Good%20Are%20Low-bit%20Quantized%20LLaMA3%20Models%3F%20An%20Empirical%20Study%0AAuthor%3A%20Wei%20Huang%20and%20Xudong%20Ma%20and%20Haotong%20Qin%20and%20Xingyu%20Zheng%20and%20Chengtao%20Lv%20and%20Hong%20Chen%20and%20Jie%20Luo%20and%20Xiaojuan%20Qi%20and%20Xianglong%20Liu%20and%20Michele%20Magno%0AAbstract%3A%20%20%20Meta%27s%20LLaMA%20family%20has%20become%20one%20of%20the%20most%20powerful%20open-source%20Large%0ALanguage%20Model%20%28LLM%29%20series.%20Notably%2C%20LLaMA3%20models%20have%20recently%20been%20released%0Aand%20achieve%20impressive%20performance%20across%20various%20with%20super-large%20scale%0Apre-training%20on%20over%2015T%20tokens%20of%20data.%20Given%20the%20wide%20application%20of%20low-bit%0Aquantization%20for%20LLMs%20in%20resource-limited%20scenarios%2C%20we%20explore%20LLaMA3%27s%0Acapabilities%20when%20quantized%20to%20low%20bit-width.%20This%20exploration%20holds%20the%0Apotential%20to%20unveil%20new%20insights%20and%20challenges%20for%20low-bit%20quantization%20of%0ALLaMA3%20and%20other%20forthcoming%20LLMs%2C%20especially%20in%20addressing%20performance%0Adegradation%20problems%20that%20suffer%20in%20LLM%20compression.%20Specifically%2C%20we%20evaluate%0Athe%2010%20existing%20post-training%20quantization%20and%20LoRA-finetuning%20methods%20of%0ALLaMA3%20on%201-8%20bits%20and%20diverse%20datasets%20to%20comprehensively%20reveal%20LLaMA3%27s%0Alow-bit%20quantization%20performance.%20Our%20experiment%20results%20indicate%20that%20LLaMA3%0Astill%20suffers%20non-negligent%20degradation%20in%20these%20scenarios%2C%20especially%20in%0Aultra-low%20bit-width.%20This%20highlights%20the%20significant%20performance%20gap%20under%20low%0Abit-width%20that%20needs%20to%20be%20bridged%20in%20future%20developments.%20We%20expect%20that%20this%0Aempirical%20study%20will%20prove%20valuable%20in%20advancing%20future%20models%2C%20pushing%20the%0ALLMs%20to%20lower%20bit-width%20with%20higher%20accuracy%20for%20being%20practical.%20Our%20project%0Ais%20released%20on%20https%3A//github.com/Macaronlin/LLaMA3-Quantization%20and%20quantized%0ALLaMA3%20models%20are%20released%20in%20https%3A//huggingface.co/LLMQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14047v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Good%20Are%20Low-bit%20Quantized%20LLaMA3%20Models%3F%20An%20Empirical%20Study&entry.906535625=Wei%20Huang%20and%20Xudong%20Ma%20and%20Haotong%20Qin%20and%20Xingyu%20Zheng%20and%20Chengtao%20Lv%20and%20Hong%20Chen%20and%20Jie%20Luo%20and%20Xiaojuan%20Qi%20and%20Xianglong%20Liu%20and%20Michele%20Magno&entry.1292438233=%20%20Meta%27s%20LLaMA%20family%20has%20become%20one%20of%20the%20most%20powerful%20open-source%20Large%0ALanguage%20Model%20%28LLM%29%20series.%20Notably%2C%20LLaMA3%20models%20have%20recently%20been%20released%0Aand%20achieve%20impressive%20performance%20across%20various%20with%20super-large%20scale%0Apre-training%20on%20over%2015T%20tokens%20of%20data.%20Given%20the%20wide%20application%20of%20low-bit%0Aquantization%20for%20LLMs%20in%20resource-limited%20scenarios%2C%20we%20explore%20LLaMA3%27s%0Acapabilities%20when%20quantized%20to%20low%20bit-width.%20This%20exploration%20holds%20the%0Apotential%20to%20unveil%20new%20insights%20and%20challenges%20for%20low-bit%20quantization%20of%0ALLaMA3%20and%20other%20forthcoming%20LLMs%2C%20especially%20in%20addressing%20performance%0Adegradation%20problems%20that%20suffer%20in%20LLM%20compression.%20Specifically%2C%20we%20evaluate%0Athe%2010%20existing%20post-training%20quantization%20and%20LoRA-finetuning%20methods%20of%0ALLaMA3%20on%201-8%20bits%20and%20diverse%20datasets%20to%20comprehensively%20reveal%20LLaMA3%27s%0Alow-bit%20quantization%20performance.%20Our%20experiment%20results%20indicate%20that%20LLaMA3%0Astill%20suffers%20non-negligent%20degradation%20in%20these%20scenarios%2C%20especially%20in%0Aultra-low%20bit-width.%20This%20highlights%20the%20significant%20performance%20gap%20under%20low%0Abit-width%20that%20needs%20to%20be%20bridged%20in%20future%20developments.%20We%20expect%20that%20this%0Aempirical%20study%20will%20prove%20valuable%20in%20advancing%20future%20models%2C%20pushing%20the%0ALLMs%20to%20lower%20bit-width%20with%20higher%20accuracy%20for%20being%20practical.%20Our%20project%0Ais%20released%20on%20https%3A//github.com/Macaronlin/LLaMA3-Quantization%20and%20quantized%0ALLaMA3%20models%20are%20released%20in%20https%3A//huggingface.co/LLMQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14047v1&entry.124074799=Read"},
{"title": "Unlawful Proxy Discrimination: A Framework for Challenging Inherently\n  Discriminatory Algorithms", "author": "Hilde Weerts and Aislinn Kelly-Lyth and Reuben Binns and Jeremias Adams-Prassl", "abstract": "  Emerging scholarship suggests that the EU legal concept of direct\ndiscrimination - where a person is given different treatment on grounds of a\nprotected characteristic - may apply to various algorithmic decision-making\ncontexts. This has important implications: unlike indirect discrimination,\nthere is generally no 'objective justification' stage in the direct\ndiscrimination framework, which means that the deployment of directly\ndiscriminatory algorithms will usually be unlawful per se. In this paper, we\nfocus on the most likely candidate for direct discrimination in the algorithmic\ncontext, termed inherent direct discrimination, where a proxy is inextricably\nlinked to a protected characteristic. We draw on computer science literature to\nsuggest that, in the algorithmic context, 'treatment on the grounds of' needs\nto be understood in terms of two steps: proxy capacity and proxy use. Only\nwhere both elements can be made out can direct discrimination be said to be `on\ngrounds of' a protected characteristic. We analyse the legal conditions of our\nproposed proxy capacity and proxy use tests. Based on this analysis, we discuss\ntechnical approaches and metrics that could be developed or applied to identify\ninherent direct discrimination in algorithmic decision-making.\n", "link": "http://arxiv.org/abs/2404.14050v1", "date": "2024-04-22", "relevancy": 1.2067, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4088}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4009}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4001}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unlawful%20Proxy%20Discrimination%3A%20A%20Framework%20for%20Challenging%20Inherently%0A%20%20Discriminatory%20Algorithms&body=Title%3A%20Unlawful%20Proxy%20Discrimination%3A%20A%20Framework%20for%20Challenging%20Inherently%0A%20%20Discriminatory%20Algorithms%0AAuthor%3A%20Hilde%20Weerts%20and%20Aislinn%20Kelly-Lyth%20and%20Reuben%20Binns%20and%20Jeremias%20Adams-Prassl%0AAbstract%3A%20%20%20Emerging%20scholarship%20suggests%20that%20the%20EU%20legal%20concept%20of%20direct%0Adiscrimination%20-%20where%20a%20person%20is%20given%20different%20treatment%20on%20grounds%20of%20a%0Aprotected%20characteristic%20-%20may%20apply%20to%20various%20algorithmic%20decision-making%0Acontexts.%20This%20has%20important%20implications%3A%20unlike%20indirect%20discrimination%2C%0Athere%20is%20generally%20no%20%27objective%20justification%27%20stage%20in%20the%20direct%0Adiscrimination%20framework%2C%20which%20means%20that%20the%20deployment%20of%20directly%0Adiscriminatory%20algorithms%20will%20usually%20be%20unlawful%20per%20se.%20In%20this%20paper%2C%20we%0Afocus%20on%20the%20most%20likely%20candidate%20for%20direct%20discrimination%20in%20the%20algorithmic%0Acontext%2C%20termed%20inherent%20direct%20discrimination%2C%20where%20a%20proxy%20is%20inextricably%0Alinked%20to%20a%20protected%20characteristic.%20We%20draw%20on%20computer%20science%20literature%20to%0Asuggest%20that%2C%20in%20the%20algorithmic%20context%2C%20%27treatment%20on%20the%20grounds%20of%27%20needs%0Ato%20be%20understood%20in%20terms%20of%20two%20steps%3A%20proxy%20capacity%20and%20proxy%20use.%20Only%0Awhere%20both%20elements%20can%20be%20made%20out%20can%20direct%20discrimination%20be%20said%20to%20be%20%60on%0Agrounds%20of%27%20a%20protected%20characteristic.%20We%20analyse%20the%20legal%20conditions%20of%20our%0Aproposed%20proxy%20capacity%20and%20proxy%20use%20tests.%20Based%20on%20this%20analysis%2C%20we%20discuss%0Atechnical%20approaches%20and%20metrics%20that%20could%20be%20developed%20or%20applied%20to%20identify%0Ainherent%20direct%20discrimination%20in%20algorithmic%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14050v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlawful%20Proxy%20Discrimination%3A%20A%20Framework%20for%20Challenging%20Inherently%0A%20%20Discriminatory%20Algorithms&entry.906535625=Hilde%20Weerts%20and%20Aislinn%20Kelly-Lyth%20and%20Reuben%20Binns%20and%20Jeremias%20Adams-Prassl&entry.1292438233=%20%20Emerging%20scholarship%20suggests%20that%20the%20EU%20legal%20concept%20of%20direct%0Adiscrimination%20-%20where%20a%20person%20is%20given%20different%20treatment%20on%20grounds%20of%20a%0Aprotected%20characteristic%20-%20may%20apply%20to%20various%20algorithmic%20decision-making%0Acontexts.%20This%20has%20important%20implications%3A%20unlike%20indirect%20discrimination%2C%0Athere%20is%20generally%20no%20%27objective%20justification%27%20stage%20in%20the%20direct%0Adiscrimination%20framework%2C%20which%20means%20that%20the%20deployment%20of%20directly%0Adiscriminatory%20algorithms%20will%20usually%20be%20unlawful%20per%20se.%20In%20this%20paper%2C%20we%0Afocus%20on%20the%20most%20likely%20candidate%20for%20direct%20discrimination%20in%20the%20algorithmic%0Acontext%2C%20termed%20inherent%20direct%20discrimination%2C%20where%20a%20proxy%20is%20inextricably%0Alinked%20to%20a%20protected%20characteristic.%20We%20draw%20on%20computer%20science%20literature%20to%0Asuggest%20that%2C%20in%20the%20algorithmic%20context%2C%20%27treatment%20on%20the%20grounds%20of%27%20needs%0Ato%20be%20understood%20in%20terms%20of%20two%20steps%3A%20proxy%20capacity%20and%20proxy%20use.%20Only%0Awhere%20both%20elements%20can%20be%20made%20out%20can%20direct%20discrimination%20be%20said%20to%20be%20%60on%0Agrounds%20of%27%20a%20protected%20characteristic.%20We%20analyse%20the%20legal%20conditions%20of%20our%0Aproposed%20proxy%20capacity%20and%20proxy%20use%20tests.%20Based%20on%20this%20analysis%2C%20we%20discuss%0Atechnical%20approaches%20and%20metrics%20that%20could%20be%20developed%20or%20applied%20to%20identify%0Ainherent%20direct%20discrimination%20in%20algorithmic%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14050v1&entry.124074799=Read"},
{"title": "GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian\n  Splatting", "author": "Hongyun Yu and Zhan Qu and Qihang Yu and Jianchuan Chen and Zhonghua Jiang and Zhiwen Chen and Shengyu Zhang and Jimin Xu and Fei Wu and Chengfei Lv and Gang Yu", "abstract": "  Recent works on audio-driven talking head synthesis using Neural Radiance\nFields (NeRF) have achieved impressive results. However, due to inadequate pose\nand expression control caused by NeRF implicit representation, these methods\nstill have some limitations, such as unsynchronized or unnatural lip movements,\nand visual jitter and artifacts. In this paper, we propose GaussianTalker, a\nnovel method for audio-driven talking head synthesis based on 3D Gaussian\nSplatting. With the explicit representation property of 3D Gaussians, intuitive\ncontrol of the facial motion is achieved by binding Gaussians to 3D facial\nmodels. GaussianTalker consists of two modules, Speaker-specific Motion\nTranslator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator\nachieves accurate lip movements specific to the target speaker through\nuniversalized audio feature extraction and customized lip motion generation.\nDynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance\nfacial detail representation via a latent pose, delivering stable and realistic\nrendered videos. Extensive experimental results suggest that GaussianTalker\noutperforms existing state-of-the-art methods in talking head synthesis,\ndelivering precise lip synchronization and exceptional visual quality. Our\nmethod achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU,\nsignificantly exceeding the threshold for real-time rendering performance, and\ncan potentially be deployed on other hardware platforms.\n", "link": "http://arxiv.org/abs/2404.14037v1", "date": "2024-04-22", "relevancy": 1.4881, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5084}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.505}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4875}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GaussianTalker%3A%20Speaker-specific%20Talking%20Head%20Synthesis%20via%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20GaussianTalker%3A%20Speaker-specific%20Talking%20Head%20Synthesis%20via%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Hongyun%20Yu%20and%20Zhan%20Qu%20and%20Qihang%20Yu%20and%20Jianchuan%20Chen%20and%20Zhonghua%20Jiang%20and%20Zhiwen%20Chen%20and%20Shengyu%20Zhang%20and%20Jimin%20Xu%20and%20Fei%20Wu%20and%20Chengfei%20Lv%20and%20Gang%20Yu%0AAbstract%3A%20%20%20Recent%20works%20on%20audio-driven%20talking%20head%20synthesis%20using%20Neural%20Radiance%0AFields%20%28NeRF%29%20have%20achieved%20impressive%20results.%20However%2C%20due%20to%20inadequate%20pose%0Aand%20expression%20control%20caused%20by%20NeRF%20implicit%20representation%2C%20these%20methods%0Astill%20have%20some%20limitations%2C%20such%20as%20unsynchronized%20or%20unnatural%20lip%20movements%2C%0Aand%20visual%20jitter%20and%20artifacts.%20In%20this%20paper%2C%20we%20propose%20GaussianTalker%2C%20a%0Anovel%20method%20for%20audio-driven%20talking%20head%20synthesis%20based%20on%203D%20Gaussian%0ASplatting.%20With%20the%20explicit%20representation%20property%20of%203D%20Gaussians%2C%20intuitive%0Acontrol%20of%20the%20facial%20motion%20is%20achieved%20by%20binding%20Gaussians%20to%203D%20facial%0Amodels.%20GaussianTalker%20consists%20of%20two%20modules%2C%20Speaker-specific%20Motion%0ATranslator%20and%20Dynamic%20Gaussian%20Renderer.%20Speaker-specific%20Motion%20Translator%0Aachieves%20accurate%20lip%20movements%20specific%20to%20the%20target%20speaker%20through%0Auniversalized%20audio%20feature%20extraction%20and%20customized%20lip%20motion%20generation.%0ADynamic%20Gaussian%20Renderer%20introduces%20Speaker-specific%20BlendShapes%20to%20enhance%0Afacial%20detail%20representation%20via%20a%20latent%20pose%2C%20delivering%20stable%20and%20realistic%0Arendered%20videos.%20Extensive%20experimental%20results%20suggest%20that%20GaussianTalker%0Aoutperforms%20existing%20state-of-the-art%20methods%20in%20talking%20head%20synthesis%2C%0Adelivering%20precise%20lip%20synchronization%20and%20exceptional%20visual%20quality.%20Our%0Amethod%20achieves%20rendering%20speeds%20of%20130%20FPS%20on%20NVIDIA%20RTX4090%20GPU%2C%0Asignificantly%20exceeding%20the%20threshold%20for%20real-time%20rendering%20performance%2C%20and%0Acan%20potentially%20be%20deployed%20on%20other%20hardware%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14037v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianTalker%3A%20Speaker-specific%20Talking%20Head%20Synthesis%20via%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Hongyun%20Yu%20and%20Zhan%20Qu%20and%20Qihang%20Yu%20and%20Jianchuan%20Chen%20and%20Zhonghua%20Jiang%20and%20Zhiwen%20Chen%20and%20Shengyu%20Zhang%20and%20Jimin%20Xu%20and%20Fei%20Wu%20and%20Chengfei%20Lv%20and%20Gang%20Yu&entry.1292438233=%20%20Recent%20works%20on%20audio-driven%20talking%20head%20synthesis%20using%20Neural%20Radiance%0AFields%20%28NeRF%29%20have%20achieved%20impressive%20results.%20However%2C%20due%20to%20inadequate%20pose%0Aand%20expression%20control%20caused%20by%20NeRF%20implicit%20representation%2C%20these%20methods%0Astill%20have%20some%20limitations%2C%20such%20as%20unsynchronized%20or%20unnatural%20lip%20movements%2C%0Aand%20visual%20jitter%20and%20artifacts.%20In%20this%20paper%2C%20we%20propose%20GaussianTalker%2C%20a%0Anovel%20method%20for%20audio-driven%20talking%20head%20synthesis%20based%20on%203D%20Gaussian%0ASplatting.%20With%20the%20explicit%20representation%20property%20of%203D%20Gaussians%2C%20intuitive%0Acontrol%20of%20the%20facial%20motion%20is%20achieved%20by%20binding%20Gaussians%20to%203D%20facial%0Amodels.%20GaussianTalker%20consists%20of%20two%20modules%2C%20Speaker-specific%20Motion%0ATranslator%20and%20Dynamic%20Gaussian%20Renderer.%20Speaker-specific%20Motion%20Translator%0Aachieves%20accurate%20lip%20movements%20specific%20to%20the%20target%20speaker%20through%0Auniversalized%20audio%20feature%20extraction%20and%20customized%20lip%20motion%20generation.%0ADynamic%20Gaussian%20Renderer%20introduces%20Speaker-specific%20BlendShapes%20to%20enhance%0Afacial%20detail%20representation%20via%20a%20latent%20pose%2C%20delivering%20stable%20and%20realistic%0Arendered%20videos.%20Extensive%20experimental%20results%20suggest%20that%20GaussianTalker%0Aoutperforms%20existing%20state-of-the-art%20methods%20in%20talking%20head%20synthesis%2C%0Adelivering%20precise%20lip%20synchronization%20and%20exceptional%20visual%20quality.%20Our%0Amethod%20achieves%20rendering%20speeds%20of%20130%20FPS%20on%20NVIDIA%20RTX4090%20GPU%2C%0Asignificantly%20exceeding%20the%20threshold%20for%20real-time%20rendering%20performance%2C%20and%0Acan%20potentially%20be%20deployed%20on%20other%20hardware%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14037v1&entry.124074799=Read"},
{"title": "Mechanistic Interpretability for AI Safety -- A Review", "author": "Leonard Bereska and Efstratios Gavves", "abstract": "  Understanding AI systems' inner workings is critical for ensuring value\nalignment and safety. This review explores mechanistic interpretability:\nreverse-engineering the computational mechanisms and representations learned by\nneural networks into human-understandable algorithms and concepts to provide a\ngranular, causal understanding. We establish foundational concepts such as\nfeatures encoding knowledge within neural activations and hypotheses about\ntheir representation and computation. We survey methodologies for causally\ndissecting model behaviors and assess the relevance of mechanistic\ninterpretability to AI safety. We investigate challenges surrounding\nscalability, automation, and comprehensive interpretation. We advocate for\nclarifying concepts, setting standards, and scaling techniques to handle\ncomplex models and behaviors and expand to domains such as vision and\nreinforcement learning. Mechanistic interpretability could help prevent\ncatastrophic outcomes as AI systems become more powerful and inscrutable.\n", "link": "http://arxiv.org/abs/2404.14082v1", "date": "2024-04-22", "relevancy": 1.3809, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4615}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Interpretability%20for%20AI%20Safety%20--%20A%20Review&body=Title%3A%20Mechanistic%20Interpretability%20for%20AI%20Safety%20--%20A%20Review%0AAuthor%3A%20Leonard%20Bereska%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20Understanding%20AI%20systems%27%20inner%20workings%20is%20critical%20for%20ensuring%20value%0Aalignment%20and%20safety.%20This%20review%20explores%20mechanistic%20interpretability%3A%0Areverse-engineering%20the%20computational%20mechanisms%20and%20representations%20learned%20by%0Aneural%20networks%20into%20human-understandable%20algorithms%20and%20concepts%20to%20provide%20a%0Agranular%2C%20causal%20understanding.%20We%20establish%20foundational%20concepts%20such%20as%0Afeatures%20encoding%20knowledge%20within%20neural%20activations%20and%20hypotheses%20about%0Atheir%20representation%20and%20computation.%20We%20survey%20methodologies%20for%20causally%0Adissecting%20model%20behaviors%20and%20assess%20the%20relevance%20of%20mechanistic%0Ainterpretability%20to%20AI%20safety.%20We%20investigate%20challenges%20surrounding%0Ascalability%2C%20automation%2C%20and%20comprehensive%20interpretation.%20We%20advocate%20for%0Aclarifying%20concepts%2C%20setting%20standards%2C%20and%20scaling%20techniques%20to%20handle%0Acomplex%20models%20and%20behaviors%20and%20expand%20to%20domains%20such%20as%20vision%20and%0Areinforcement%20learning.%20Mechanistic%20interpretability%20could%20help%20prevent%0Acatastrophic%20outcomes%20as%20AI%20systems%20become%20more%20powerful%20and%20inscrutable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14082v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Interpretability%20for%20AI%20Safety%20--%20A%20Review&entry.906535625=Leonard%20Bereska%20and%20Efstratios%20Gavves&entry.1292438233=%20%20Understanding%20AI%20systems%27%20inner%20workings%20is%20critical%20for%20ensuring%20value%0Aalignment%20and%20safety.%20This%20review%20explores%20mechanistic%20interpretability%3A%0Areverse-engineering%20the%20computational%20mechanisms%20and%20representations%20learned%20by%0Aneural%20networks%20into%20human-understandable%20algorithms%20and%20concepts%20to%20provide%20a%0Agranular%2C%20causal%20understanding.%20We%20establish%20foundational%20concepts%20such%20as%0Afeatures%20encoding%20knowledge%20within%20neural%20activations%20and%20hypotheses%20about%0Atheir%20representation%20and%20computation.%20We%20survey%20methodologies%20for%20causally%0Adissecting%20model%20behaviors%20and%20assess%20the%20relevance%20of%20mechanistic%0Ainterpretability%20to%20AI%20safety.%20We%20investigate%20challenges%20surrounding%0Ascalability%2C%20automation%2C%20and%20comprehensive%20interpretation.%20We%20advocate%20for%0Aclarifying%20concepts%2C%20setting%20standards%2C%20and%20scaling%20techniques%20to%20handle%0Acomplex%20models%20and%20behaviors%20and%20expand%20to%20domains%20such%20as%20vision%20and%0Areinforcement%20learning.%20Mechanistic%20interpretability%20could%20help%20prevent%0Acatastrophic%20outcomes%20as%20AI%20systems%20become%20more%20powerful%20and%20inscrutable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14082v1&entry.124074799=Read"},
{"title": "Text in the Dark: Extremely Low-Light Text Image Enhancement", "author": "Che-Tsung Lin and Chun Chet Ng and Zhi Qin Tan and Wan Jun Nah and Xinyu Wang and Jie Long Kew and Pohao Hsu and Shang Hong Lai and Chee Seng Chan and Christopher Zach", "abstract": "  Extremely low-light text images are common in natural scenes, making scene\ntext detection and recognition challenging. One solution is to enhance these\nimages using low-light image enhancement methods before text extraction.\nHowever, previous methods often do not try to particularly address the\nsignificance of low-level features, which are crucial for optimal performance\non downstream scene text tasks. Further research is also hindered by the lack\nof extremely low-light text datasets. To address these limitations, we propose\na novel encoder-decoder framework with an edge-aware attention module to focus\non scene text regions during enhancement. Our proposed method uses novel text\ndetection and edge reconstruction losses to emphasize low-level scene text\nfeatures, leading to successful text extraction. Additionally, we present a\nSupervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely\nlow-light images based on publicly available scene text datasets such as\nICDAR15 (IC15). We also labeled texts in the extremely low-light See In the\nDark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective\nassessment of extremely low-light image enhancement through scene text tasks.\nExtensive experiments show that our model outperforms state-of-the-art methods\nin terms of both image quality and scene text metrics on the widely-used LOL,\nSID, and synthetic IC15 datasets. Code and dataset will be released publicly at\nhttps://github.com/chunchet-ng/Text-in-the-Dark.\n", "link": "http://arxiv.org/abs/2404.14135v1", "date": "2024-04-22", "relevancy": 1.0403, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5251}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5197}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Text%20in%20the%20Dark%3A%20Extremely%20Low-Light%20Text%20Image%20Enhancement&body=Title%3A%20Text%20in%20the%20Dark%3A%20Extremely%20Low-Light%20Text%20Image%20Enhancement%0AAuthor%3A%20Che-Tsung%20Lin%20and%20Chun%20Chet%20Ng%20and%20Zhi%20Qin%20Tan%20and%20Wan%20Jun%20Nah%20and%20Xinyu%20Wang%20and%20Jie%20Long%20Kew%20and%20Pohao%20Hsu%20and%20Shang%20Hong%20Lai%20and%20Chee%20Seng%20Chan%20and%20Christopher%20Zach%0AAbstract%3A%20%20%20Extremely%20low-light%20text%20images%20are%20common%20in%20natural%20scenes%2C%20making%20scene%0Atext%20detection%20and%20recognition%20challenging.%20One%20solution%20is%20to%20enhance%20these%0Aimages%20using%20low-light%20image%20enhancement%20methods%20before%20text%20extraction.%0AHowever%2C%20previous%20methods%20often%20do%20not%20try%20to%20particularly%20address%20the%0Asignificance%20of%20low-level%20features%2C%20which%20are%20crucial%20for%20optimal%20performance%0Aon%20downstream%20scene%20text%20tasks.%20Further%20research%20is%20also%20hindered%20by%20the%20lack%0Aof%20extremely%20low-light%20text%20datasets.%20To%20address%20these%20limitations%2C%20we%20propose%0Aa%20novel%20encoder-decoder%20framework%20with%20an%20edge-aware%20attention%20module%20to%20focus%0Aon%20scene%20text%20regions%20during%20enhancement.%20Our%20proposed%20method%20uses%20novel%20text%0Adetection%20and%20edge%20reconstruction%20losses%20to%20emphasize%20low-level%20scene%20text%0Afeatures%2C%20leading%20to%20successful%20text%20extraction.%20Additionally%2C%20we%20present%20a%0ASupervised%20Deep%20Curve%20Estimation%20%28Supervised-DCE%29%20model%20to%20synthesize%20extremely%0Alow-light%20images%20based%20on%20publicly%20available%20scene%20text%20datasets%20such%20as%0AICDAR15%20%28IC15%29.%20We%20also%20labeled%20texts%20in%20the%20extremely%20low-light%20See%20In%20the%0ADark%20%28SID%29%20and%20ordinary%20LOw-Light%20%28LOL%29%20datasets%20to%20allow%20for%20objective%0Aassessment%20of%20extremely%20low-light%20image%20enhancement%20through%20scene%20text%20tasks.%0AExtensive%20experiments%20show%20that%20our%20model%20outperforms%20state-of-the-art%20methods%0Ain%20terms%20of%20both%20image%20quality%20and%20scene%20text%20metrics%20on%20the%20widely-used%20LOL%2C%0ASID%2C%20and%20synthetic%20IC15%20datasets.%20Code%20and%20dataset%20will%20be%20released%20publicly%20at%0Ahttps%3A//github.com/chunchet-ng/Text-in-the-Dark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14135v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20in%20the%20Dark%3A%20Extremely%20Low-Light%20Text%20Image%20Enhancement&entry.906535625=Che-Tsung%20Lin%20and%20Chun%20Chet%20Ng%20and%20Zhi%20Qin%20Tan%20and%20Wan%20Jun%20Nah%20and%20Xinyu%20Wang%20and%20Jie%20Long%20Kew%20and%20Pohao%20Hsu%20and%20Shang%20Hong%20Lai%20and%20Chee%20Seng%20Chan%20and%20Christopher%20Zach&entry.1292438233=%20%20Extremely%20low-light%20text%20images%20are%20common%20in%20natural%20scenes%2C%20making%20scene%0Atext%20detection%20and%20recognition%20challenging.%20One%20solution%20is%20to%20enhance%20these%0Aimages%20using%20low-light%20image%20enhancement%20methods%20before%20text%20extraction.%0AHowever%2C%20previous%20methods%20often%20do%20not%20try%20to%20particularly%20address%20the%0Asignificance%20of%20low-level%20features%2C%20which%20are%20crucial%20for%20optimal%20performance%0Aon%20downstream%20scene%20text%20tasks.%20Further%20research%20is%20also%20hindered%20by%20the%20lack%0Aof%20extremely%20low-light%20text%20datasets.%20To%20address%20these%20limitations%2C%20we%20propose%0Aa%20novel%20encoder-decoder%20framework%20with%20an%20edge-aware%20attention%20module%20to%20focus%0Aon%20scene%20text%20regions%20during%20enhancement.%20Our%20proposed%20method%20uses%20novel%20text%0Adetection%20and%20edge%20reconstruction%20losses%20to%20emphasize%20low-level%20scene%20text%0Afeatures%2C%20leading%20to%20successful%20text%20extraction.%20Additionally%2C%20we%20present%20a%0ASupervised%20Deep%20Curve%20Estimation%20%28Supervised-DCE%29%20model%20to%20synthesize%20extremely%0Alow-light%20images%20based%20on%20publicly%20available%20scene%20text%20datasets%20such%20as%0AICDAR15%20%28IC15%29.%20We%20also%20labeled%20texts%20in%20the%20extremely%20low-light%20See%20In%20the%0ADark%20%28SID%29%20and%20ordinary%20LOw-Light%20%28LOL%29%20datasets%20to%20allow%20for%20objective%0Aassessment%20of%20extremely%20low-light%20image%20enhancement%20through%20scene%20text%20tasks.%0AExtensive%20experiments%20show%20that%20our%20model%20outperforms%20state-of-the-art%20methods%0Ain%20terms%20of%20both%20image%20quality%20and%20scene%20text%20metrics%20on%20the%20widely-used%20LOL%2C%0ASID%2C%20and%20synthetic%20IC15%20datasets.%20Code%20and%20dataset%20will%20be%20released%20publicly%20at%0Ahttps%3A//github.com/chunchet-ng/Text-in-the-Dark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14135v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


