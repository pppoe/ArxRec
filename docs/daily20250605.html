<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250604.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D\n  Gaussian Splatting", "author": "Hengyu Liu and Yuehao Wang and Chenxin Li and Ruisi Cai and Kevin Wang and Wuyang Li and Pavlo Molchanov and Peihao Wang and Zhangyang Wang", "abstract": "  3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io.\n", "link": "http://arxiv.org/abs/2506.04174v1", "date": "2025-06-04", "relevancy": 3.4638, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7202}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6991}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexGS%3A%20Train%20Once%2C%20Deploy%20Everywhere%20with%20Many-in-One%20Flexible%203D%0A%20%20Gaussian%20Splatting&body=Title%3A%20FlexGS%3A%20Train%20Once%2C%20Deploy%20Everywhere%20with%20Many-in-One%20Flexible%203D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Hengyu%20Liu%20and%20Yuehao%20Wang%20and%20Chenxin%20Li%20and%20Ruisi%20Cai%20and%20Kevin%20Wang%20and%20Wuyang%20Li%20and%20Pavlo%20Molchanov%20and%20Peihao%20Wang%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20enabled%20various%20applications%20in%203D%20scene%0Arepresentation%20and%20novel%20view%20synthesis%20due%20to%20its%20efficient%20rendering%0Acapabilities.%20However%2C%203DGS%20demands%20relatively%20significant%20GPU%20memory%2C%20limiting%0Aits%20use%20on%20devices%20with%20restricted%20computational%20resources.%20Previous%20approaches%0Ahave%20focused%20on%20pruning%20less%20important%20Gaussians%2C%20effectively%20compressing%203DGS%0Abut%20often%20requiring%20a%20fine-tuning%20stage%20and%20lacking%20adaptability%20for%20the%0Aspecific%20memory%20needs%20of%20different%20devices.%20In%20this%20work%2C%20we%20present%20an%20elastic%0Ainference%20method%20for%203DGS.%20Given%20an%20input%20for%20the%20desired%20model%20size%2C%20our%0Amethod%20selects%20and%20transforms%20a%20subset%20of%20Gaussians%2C%20achieving%20substantial%0Arendering%20performance%20without%20additional%20fine-tuning.%20We%20introduce%20a%20tiny%0Alearnable%20module%20that%20controls%20Gaussian%20selection%20based%20on%20the%20input%0Apercentage%2C%20along%20with%20a%20transformation%20module%20that%20adjusts%20the%20selected%0AGaussians%20to%20complement%20the%20performance%20of%20the%20reduced%20model.%20Comprehensive%0Aexperiments%20on%20ZipNeRF%2C%20MipNeRF%20and%20Tanks%5C%26Temples%20scenes%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Code%20is%20available%20at%20https%3A//flexgs.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexGS%253A%2520Train%2520Once%252C%2520Deploy%2520Everywhere%2520with%2520Many-in-One%2520Flexible%25203D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DHengyu%2520Liu%2520and%2520Yuehao%2520Wang%2520and%2520Chenxin%2520Li%2520and%2520Ruisi%2520Cai%2520and%2520Kevin%2520Wang%2520and%2520Wuyang%2520Li%2520and%2520Pavlo%2520Molchanov%2520and%2520Peihao%2520Wang%2520and%2520Zhangyang%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520enabled%2520various%2520applications%2520in%25203D%2520scene%250Arepresentation%2520and%2520novel%2520view%2520synthesis%2520due%2520to%2520its%2520efficient%2520rendering%250Acapabilities.%2520However%252C%25203DGS%2520demands%2520relatively%2520significant%2520GPU%2520memory%252C%2520limiting%250Aits%2520use%2520on%2520devices%2520with%2520restricted%2520computational%2520resources.%2520Previous%2520approaches%250Ahave%2520focused%2520on%2520pruning%2520less%2520important%2520Gaussians%252C%2520effectively%2520compressing%25203DGS%250Abut%2520often%2520requiring%2520a%2520fine-tuning%2520stage%2520and%2520lacking%2520adaptability%2520for%2520the%250Aspecific%2520memory%2520needs%2520of%2520different%2520devices.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520elastic%250Ainference%2520method%2520for%25203DGS.%2520Given%2520an%2520input%2520for%2520the%2520desired%2520model%2520size%252C%2520our%250Amethod%2520selects%2520and%2520transforms%2520a%2520subset%2520of%2520Gaussians%252C%2520achieving%2520substantial%250Arendering%2520performance%2520without%2520additional%2520fine-tuning.%2520We%2520introduce%2520a%2520tiny%250Alearnable%2520module%2520that%2520controls%2520Gaussian%2520selection%2520based%2520on%2520the%2520input%250Apercentage%252C%2520along%2520with%2520a%2520transformation%2520module%2520that%2520adjusts%2520the%2520selected%250AGaussians%2520to%2520complement%2520the%2520performance%2520of%2520the%2520reduced%2520model.%2520Comprehensive%250Aexperiments%2520on%2520ZipNeRF%252C%2520MipNeRF%2520and%2520Tanks%255C%2526Temples%2520scenes%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach.%2520Code%2520is%2520available%2520at%2520https%253A//flexgs.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexGS%3A%20Train%20Once%2C%20Deploy%20Everywhere%20with%20Many-in-One%20Flexible%203D%0A%20%20Gaussian%20Splatting&entry.906535625=Hengyu%20Liu%20and%20Yuehao%20Wang%20and%20Chenxin%20Li%20and%20Ruisi%20Cai%20and%20Kevin%20Wang%20and%20Wuyang%20Li%20and%20Pavlo%20Molchanov%20and%20Peihao%20Wang%20and%20Zhangyang%20Wang&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20enabled%20various%20applications%20in%203D%20scene%0Arepresentation%20and%20novel%20view%20synthesis%20due%20to%20its%20efficient%20rendering%0Acapabilities.%20However%2C%203DGS%20demands%20relatively%20significant%20GPU%20memory%2C%20limiting%0Aits%20use%20on%20devices%20with%20restricted%20computational%20resources.%20Previous%20approaches%0Ahave%20focused%20on%20pruning%20less%20important%20Gaussians%2C%20effectively%20compressing%203DGS%0Abut%20often%20requiring%20a%20fine-tuning%20stage%20and%20lacking%20adaptability%20for%20the%0Aspecific%20memory%20needs%20of%20different%20devices.%20In%20this%20work%2C%20we%20present%20an%20elastic%0Ainference%20method%20for%203DGS.%20Given%20an%20input%20for%20the%20desired%20model%20size%2C%20our%0Amethod%20selects%20and%20transforms%20a%20subset%20of%20Gaussians%2C%20achieving%20substantial%0Arendering%20performance%20without%20additional%20fine-tuning.%20We%20introduce%20a%20tiny%0Alearnable%20module%20that%20controls%20Gaussian%20selection%20based%20on%20the%20input%0Apercentage%2C%20along%20with%20a%20transformation%20module%20that%20adjusts%20the%20selected%0AGaussians%20to%20complement%20the%20performance%20of%20the%20reduced%20model.%20Comprehensive%0Aexperiments%20on%20ZipNeRF%2C%20MipNeRF%20and%20Tanks%5C%26Temples%20scenes%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Code%20is%20available%20at%20https%3A//flexgs.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04174v1&entry.124074799=Read"},
{"title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View\n  Gaussian Splatting", "author": "Yang Xiao and Guoan Xu and Qiang Wu and Wenjing Jia", "abstract": "  Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge\nwith wide applications. Recent advances in feed-forward 3D Gaussian sparse-view\nreconstruction methods provide an efficient solution for real-time novel view\nsynthesis by leveraging geometric priors learned from large-scale multi-view\ndatasets and computing 3D Gaussian centers via back-projection. Despite\noffering strong geometric cues, both feed-forward multi-view depth estimation\nand flow-depth joint estimation face key limitations: the former suffers from\nmislocation and artifact issues in low-texture or repetitive regions, while the\nlatter is prone to local noise and global inconsistency due to unreliable\nmatches when ground-truth flow supervision is unavailable. To overcome this, we\npropose JointSplat, a unified framework that leverages the complementarity\nbetween optical flow and depth via a novel probabilistic optimization\nmechanism. Specifically, this pixel-level mechanism scales the information\nfusion between depth and flow based on the matching probability of optical flow\nduring training. Building upon the above mechanism, we further propose a novel\nmulti-view depth-consistency loss to leverage the reliability of supervision\nwhile suppressing misleading gradients in uncertain areas. Evaluated on\nRealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art\n(SOTA) methods, demonstrating the effectiveness and robustness of our proposed\nprobabilistic joint flow-depth optimization approach for high-fidelity\nsparse-view 3D reconstruction.\n", "link": "http://arxiv.org/abs/2506.03872v1", "date": "2025-06-04", "relevancy": 3.2614, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6902}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6535}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointSplat%3A%20Probabilistic%20Joint%20Flow-Depth%20Optimization%20for%20Sparse-View%0A%20%20Gaussian%20Splatting&body=Title%3A%20JointSplat%3A%20Probabilistic%20Joint%20Flow-Depth%20Optimization%20for%20Sparse-View%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Yang%20Xiao%20and%20Guoan%20Xu%20and%20Qiang%20Wu%20and%20Wenjing%20Jia%0AAbstract%3A%20%20%20Reconstructing%203D%20scenes%20from%20sparse%20viewpoints%20is%20a%20long-standing%20challenge%0Awith%20wide%20applications.%20Recent%20advances%20in%20feed-forward%203D%20Gaussian%20sparse-view%0Areconstruction%20methods%20provide%20an%20efficient%20solution%20for%20real-time%20novel%20view%0Asynthesis%20by%20leveraging%20geometric%20priors%20learned%20from%20large-scale%20multi-view%0Adatasets%20and%20computing%203D%20Gaussian%20centers%20via%20back-projection.%20Despite%0Aoffering%20strong%20geometric%20cues%2C%20both%20feed-forward%20multi-view%20depth%20estimation%0Aand%20flow-depth%20joint%20estimation%20face%20key%20limitations%3A%20the%20former%20suffers%20from%0Amislocation%20and%20artifact%20issues%20in%20low-texture%20or%20repetitive%20regions%2C%20while%20the%0Alatter%20is%20prone%20to%20local%20noise%20and%20global%20inconsistency%20due%20to%20unreliable%0Amatches%20when%20ground-truth%20flow%20supervision%20is%20unavailable.%20To%20overcome%20this%2C%20we%0Apropose%20JointSplat%2C%20a%20unified%20framework%20that%20leverages%20the%20complementarity%0Abetween%20optical%20flow%20and%20depth%20via%20a%20novel%20probabilistic%20optimization%0Amechanism.%20Specifically%2C%20this%20pixel-level%20mechanism%20scales%20the%20information%0Afusion%20between%20depth%20and%20flow%20based%20on%20the%20matching%20probability%20of%20optical%20flow%0Aduring%20training.%20Building%20upon%20the%20above%20mechanism%2C%20we%20further%20propose%20a%20novel%0Amulti-view%20depth-consistency%20loss%20to%20leverage%20the%20reliability%20of%20supervision%0Awhile%20suppressing%20misleading%20gradients%20in%20uncertain%20areas.%20Evaluated%20on%0ARealEstate10K%20and%20ACID%2C%20JointSplat%20consistently%20outperforms%20state-of-the-art%0A%28SOTA%29%20methods%2C%20demonstrating%20the%20effectiveness%20and%20robustness%20of%20our%20proposed%0Aprobabilistic%20joint%20flow-depth%20optimization%20approach%20for%20high-fidelity%0Asparse-view%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointSplat%253A%2520Probabilistic%2520Joint%2520Flow-Depth%2520Optimization%2520for%2520Sparse-View%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DYang%2520Xiao%2520and%2520Guoan%2520Xu%2520and%2520Qiang%2520Wu%2520and%2520Wenjing%2520Jia%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520scenes%2520from%2520sparse%2520viewpoints%2520is%2520a%2520long-standing%2520challenge%250Awith%2520wide%2520applications.%2520Recent%2520advances%2520in%2520feed-forward%25203D%2520Gaussian%2520sparse-view%250Areconstruction%2520methods%2520provide%2520an%2520efficient%2520solution%2520for%2520real-time%2520novel%2520view%250Asynthesis%2520by%2520leveraging%2520geometric%2520priors%2520learned%2520from%2520large-scale%2520multi-view%250Adatasets%2520and%2520computing%25203D%2520Gaussian%2520centers%2520via%2520back-projection.%2520Despite%250Aoffering%2520strong%2520geometric%2520cues%252C%2520both%2520feed-forward%2520multi-view%2520depth%2520estimation%250Aand%2520flow-depth%2520joint%2520estimation%2520face%2520key%2520limitations%253A%2520the%2520former%2520suffers%2520from%250Amislocation%2520and%2520artifact%2520issues%2520in%2520low-texture%2520or%2520repetitive%2520regions%252C%2520while%2520the%250Alatter%2520is%2520prone%2520to%2520local%2520noise%2520and%2520global%2520inconsistency%2520due%2520to%2520unreliable%250Amatches%2520when%2520ground-truth%2520flow%2520supervision%2520is%2520unavailable.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520JointSplat%252C%2520a%2520unified%2520framework%2520that%2520leverages%2520the%2520complementarity%250Abetween%2520optical%2520flow%2520and%2520depth%2520via%2520a%2520novel%2520probabilistic%2520optimization%250Amechanism.%2520Specifically%252C%2520this%2520pixel-level%2520mechanism%2520scales%2520the%2520information%250Afusion%2520between%2520depth%2520and%2520flow%2520based%2520on%2520the%2520matching%2520probability%2520of%2520optical%2520flow%250Aduring%2520training.%2520Building%2520upon%2520the%2520above%2520mechanism%252C%2520we%2520further%2520propose%2520a%2520novel%250Amulti-view%2520depth-consistency%2520loss%2520to%2520leverage%2520the%2520reliability%2520of%2520supervision%250Awhile%2520suppressing%2520misleading%2520gradients%2520in%2520uncertain%2520areas.%2520Evaluated%2520on%250ARealEstate10K%2520and%2520ACID%252C%2520JointSplat%2520consistently%2520outperforms%2520state-of-the-art%250A%2528SOTA%2529%2520methods%252C%2520demonstrating%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520proposed%250Aprobabilistic%2520joint%2520flow-depth%2520optimization%2520approach%2520for%2520high-fidelity%250Asparse-view%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointSplat%3A%20Probabilistic%20Joint%20Flow-Depth%20Optimization%20for%20Sparse-View%0A%20%20Gaussian%20Splatting&entry.906535625=Yang%20Xiao%20and%20Guoan%20Xu%20and%20Qiang%20Wu%20and%20Wenjing%20Jia&entry.1292438233=%20%20Reconstructing%203D%20scenes%20from%20sparse%20viewpoints%20is%20a%20long-standing%20challenge%0Awith%20wide%20applications.%20Recent%20advances%20in%20feed-forward%203D%20Gaussian%20sparse-view%0Areconstruction%20methods%20provide%20an%20efficient%20solution%20for%20real-time%20novel%20view%0Asynthesis%20by%20leveraging%20geometric%20priors%20learned%20from%20large-scale%20multi-view%0Adatasets%20and%20computing%203D%20Gaussian%20centers%20via%20back-projection.%20Despite%0Aoffering%20strong%20geometric%20cues%2C%20both%20feed-forward%20multi-view%20depth%20estimation%0Aand%20flow-depth%20joint%20estimation%20face%20key%20limitations%3A%20the%20former%20suffers%20from%0Amislocation%20and%20artifact%20issues%20in%20low-texture%20or%20repetitive%20regions%2C%20while%20the%0Alatter%20is%20prone%20to%20local%20noise%20and%20global%20inconsistency%20due%20to%20unreliable%0Amatches%20when%20ground-truth%20flow%20supervision%20is%20unavailable.%20To%20overcome%20this%2C%20we%0Apropose%20JointSplat%2C%20a%20unified%20framework%20that%20leverages%20the%20complementarity%0Abetween%20optical%20flow%20and%20depth%20via%20a%20novel%20probabilistic%20optimization%0Amechanism.%20Specifically%2C%20this%20pixel-level%20mechanism%20scales%20the%20information%0Afusion%20between%20depth%20and%20flow%20based%20on%20the%20matching%20probability%20of%20optical%20flow%0Aduring%20training.%20Building%20upon%20the%20above%20mechanism%2C%20we%20further%20propose%20a%20novel%0Amulti-view%20depth-consistency%20loss%20to%20leverage%20the%20reliability%20of%20supervision%0Awhile%20suppressing%20misleading%20gradients%20in%20uncertain%20areas.%20Evaluated%20on%0ARealEstate10K%20and%20ACID%2C%20JointSplat%20consistently%20outperforms%20state-of-the-art%0A%28SOTA%29%20methods%2C%20demonstrating%20the%20effectiveness%20and%20robustness%20of%20our%20proposed%0Aprobabilistic%20joint%20flow-depth%20optimization%20approach%20for%20high-fidelity%0Asparse-view%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03872v1&entry.124074799=Read"},
{"title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large\n  Multimodal Models", "author": "Fangrui Zhu and Hanhui Wang and Yiming Xie and Jing Gu and Tianye Ding and Jianwei Yang and Huaizu Jiang", "abstract": "  Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts\noften rely on explicit 3D inputs or specialized model architectures, we ask:\ncan LMMs reason about 3D space using only structured 2D representations derived\nfrom perception? We introduce Struct2D, a perception-guided prompting framework\nthat combines bird's-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs\n(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning\nabilities when provided with structured 2D inputs, effectively handling tasks\nsuch as relative direction estimation and route planning. Building on these\ninsights, we construct Struct2D-Set, a large-scale instruction tuning dataset\nwith 200K fine-grained QA pairs across eight spatial reasoning categories,\ngenerated automatically from 3D indoor scenes. We fine-tune an open-source LMM\n(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple\nbenchmarks, including 3D question answering, dense captioning, and object\ngrounding. Our approach demonstrates that structured 2D inputs can effectively\nbridge perception and language reasoning in LMMs-without requiring explicit 3D\nrepresentations as input. We will release both our code and dataset to support\nfuture research.\n", "link": "http://arxiv.org/abs/2506.04220v1", "date": "2025-06-04", "relevancy": 3.1487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Struct2D%3A%20A%20Perception-Guided%20Framework%20for%20Spatial%20Reasoning%20in%20Large%0A%20%20Multimodal%20Models&body=Title%3A%20Struct2D%3A%20A%20Perception-Guided%20Framework%20for%20Spatial%20Reasoning%20in%20Large%0A%20%20Multimodal%20Models%0AAuthor%3A%20Fangrui%20Zhu%20and%20Hanhui%20Wang%20and%20Yiming%20Xie%20and%20Jing%20Gu%20and%20Tianye%20Ding%20and%20Jianwei%20Yang%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Unlocking%20spatial%20reasoning%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20is%20crucial%20for%0Aenabling%20intelligent%20interaction%20with%203D%20environments.%20While%20prior%20efforts%0Aoften%20rely%20on%20explicit%203D%20inputs%20or%20specialized%20model%20architectures%2C%20we%20ask%3A%0Acan%20LMMs%20reason%20about%203D%20space%20using%20only%20structured%202D%20representations%20derived%0Afrom%20perception%3F%20We%20introduce%20Struct2D%2C%20a%20perception-guided%20prompting%20framework%0Athat%20combines%20bird%27s-eye-view%20%28BEV%29%20images%20with%20object%20marks%20and%20object-centric%0Ametadata%2C%20optionally%20incorporating%20egocentric%20keyframes%20when%20needed.%20Using%0AStruct2D%2C%20we%20conduct%20an%20in-depth%20zero-shot%20analysis%20of%20closed-source%20LMMs%0A%28e.g.%2C%20GPT-o3%29%20and%20find%20that%20they%20exhibit%20surprisingly%20strong%20spatial%20reasoning%0Aabilities%20when%20provided%20with%20structured%202D%20inputs%2C%20effectively%20handling%20tasks%0Asuch%20as%20relative%20direction%20estimation%20and%20route%20planning.%20Building%20on%20these%0Ainsights%2C%20we%20construct%20Struct2D-Set%2C%20a%20large-scale%20instruction%20tuning%20dataset%0Awith%20200K%20fine-grained%20QA%20pairs%20across%20eight%20spatial%20reasoning%20categories%2C%0Agenerated%20automatically%20from%203D%20indoor%20scenes.%20We%20fine-tune%20an%20open-source%20LMM%0A%28Qwen2.5VL%29%20on%20Struct2D-Set%2C%20achieving%20competitive%20performance%20on%20multiple%0Abenchmarks%2C%20including%203D%20question%20answering%2C%20dense%20captioning%2C%20and%20object%0Agrounding.%20Our%20approach%20demonstrates%20that%20structured%202D%20inputs%20can%20effectively%0Abridge%20perception%20and%20language%20reasoning%20in%20LMMs-without%20requiring%20explicit%203D%0Arepresentations%20as%20input.%20We%20will%20release%20both%20our%20code%20and%20dataset%20to%20support%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStruct2D%253A%2520A%2520Perception-Guided%2520Framework%2520for%2520Spatial%2520Reasoning%2520in%2520Large%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DFangrui%2520Zhu%2520and%2520Hanhui%2520Wang%2520and%2520Yiming%2520Xie%2520and%2520Jing%2520Gu%2520and%2520Tianye%2520Ding%2520and%2520Jianwei%2520Yang%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520Unlocking%2520spatial%2520reasoning%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520is%2520crucial%2520for%250Aenabling%2520intelligent%2520interaction%2520with%25203D%2520environments.%2520While%2520prior%2520efforts%250Aoften%2520rely%2520on%2520explicit%25203D%2520inputs%2520or%2520specialized%2520model%2520architectures%252C%2520we%2520ask%253A%250Acan%2520LMMs%2520reason%2520about%25203D%2520space%2520using%2520only%2520structured%25202D%2520representations%2520derived%250Afrom%2520perception%253F%2520We%2520introduce%2520Struct2D%252C%2520a%2520perception-guided%2520prompting%2520framework%250Athat%2520combines%2520bird%2527s-eye-view%2520%2528BEV%2529%2520images%2520with%2520object%2520marks%2520and%2520object-centric%250Ametadata%252C%2520optionally%2520incorporating%2520egocentric%2520keyframes%2520when%2520needed.%2520Using%250AStruct2D%252C%2520we%2520conduct%2520an%2520in-depth%2520zero-shot%2520analysis%2520of%2520closed-source%2520LMMs%250A%2528e.g.%252C%2520GPT-o3%2529%2520and%2520find%2520that%2520they%2520exhibit%2520surprisingly%2520strong%2520spatial%2520reasoning%250Aabilities%2520when%2520provided%2520with%2520structured%25202D%2520inputs%252C%2520effectively%2520handling%2520tasks%250Asuch%2520as%2520relative%2520direction%2520estimation%2520and%2520route%2520planning.%2520Building%2520on%2520these%250Ainsights%252C%2520we%2520construct%2520Struct2D-Set%252C%2520a%2520large-scale%2520instruction%2520tuning%2520dataset%250Awith%2520200K%2520fine-grained%2520QA%2520pairs%2520across%2520eight%2520spatial%2520reasoning%2520categories%252C%250Agenerated%2520automatically%2520from%25203D%2520indoor%2520scenes.%2520We%2520fine-tune%2520an%2520open-source%2520LMM%250A%2528Qwen2.5VL%2529%2520on%2520Struct2D-Set%252C%2520achieving%2520competitive%2520performance%2520on%2520multiple%250Abenchmarks%252C%2520including%25203D%2520question%2520answering%252C%2520dense%2520captioning%252C%2520and%2520object%250Agrounding.%2520Our%2520approach%2520demonstrates%2520that%2520structured%25202D%2520inputs%2520can%2520effectively%250Abridge%2520perception%2520and%2520language%2520reasoning%2520in%2520LMMs-without%2520requiring%2520explicit%25203D%250Arepresentations%2520as%2520input.%2520We%2520will%2520release%2520both%2520our%2520code%2520and%2520dataset%2520to%2520support%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Struct2D%3A%20A%20Perception-Guided%20Framework%20for%20Spatial%20Reasoning%20in%20Large%0A%20%20Multimodal%20Models&entry.906535625=Fangrui%20Zhu%20and%20Hanhui%20Wang%20and%20Yiming%20Xie%20and%20Jing%20Gu%20and%20Tianye%20Ding%20and%20Jianwei%20Yang%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Unlocking%20spatial%20reasoning%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20is%20crucial%20for%0Aenabling%20intelligent%20interaction%20with%203D%20environments.%20While%20prior%20efforts%0Aoften%20rely%20on%20explicit%203D%20inputs%20or%20specialized%20model%20architectures%2C%20we%20ask%3A%0Acan%20LMMs%20reason%20about%203D%20space%20using%20only%20structured%202D%20representations%20derived%0Afrom%20perception%3F%20We%20introduce%20Struct2D%2C%20a%20perception-guided%20prompting%20framework%0Athat%20combines%20bird%27s-eye-view%20%28BEV%29%20images%20with%20object%20marks%20and%20object-centric%0Ametadata%2C%20optionally%20incorporating%20egocentric%20keyframes%20when%20needed.%20Using%0AStruct2D%2C%20we%20conduct%20an%20in-depth%20zero-shot%20analysis%20of%20closed-source%20LMMs%0A%28e.g.%2C%20GPT-o3%29%20and%20find%20that%20they%20exhibit%20surprisingly%20strong%20spatial%20reasoning%0Aabilities%20when%20provided%20with%20structured%202D%20inputs%2C%20effectively%20handling%20tasks%0Asuch%20as%20relative%20direction%20estimation%20and%20route%20planning.%20Building%20on%20these%0Ainsights%2C%20we%20construct%20Struct2D-Set%2C%20a%20large-scale%20instruction%20tuning%20dataset%0Awith%20200K%20fine-grained%20QA%20pairs%20across%20eight%20spatial%20reasoning%20categories%2C%0Agenerated%20automatically%20from%203D%20indoor%20scenes.%20We%20fine-tune%20an%20open-source%20LMM%0A%28Qwen2.5VL%29%20on%20Struct2D-Set%2C%20achieving%20competitive%20performance%20on%20multiple%0Abenchmarks%2C%20including%203D%20question%20answering%2C%20dense%20captioning%2C%20and%20object%0Agrounding.%20Our%20approach%20demonstrates%20that%20structured%202D%20inputs%20can%20effectively%0Abridge%20perception%20and%20language%20reasoning%20in%20LMMs-without%20requiring%20explicit%203D%0Arepresentations%20as%20input.%20We%20will%20release%20both%20our%20code%20and%20dataset%20to%20support%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04220v1&entry.124074799=Read"},
{"title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data", "author": "Ben Moran and Mauro Comi and Steven Bohez and Tom Erez and Zhibin Li and Leonard Hasenclever", "abstract": "  Creating accurate, physical simulations directly from real-world robot motion\nholds great value for safe, scalable, and affordable robot learning, yet\nremains exceptionally challenging. Real robot data suffers from occlusions,\nnoisy camera poses, dynamic scene elements, which hinder the creation of\ngeometrically accurate and photorealistic digital twins of unseen objects. We\nintroduce a novel real-to-sim framework tackling all these challenges at once.\nOur key insight is a hybrid scene representation merging the photorealistic\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\nphysics simulation within a single representation. We propose an end-to-end\noptimization pipeline that leverages differentiable rendering and\ndifferentiable physics within MuJoCo to jointly refine all scene components -\nfrom object geometry and appearance to robot poses and physical parameters -\ndirectly from raw and imprecise robot trajectories. This unified optimization\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\ngenerate photorealistic novel views, and perform annotation-free robot pose\ncalibration. We demonstrate the effectiveness of our approach both in\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\nmanipulator, enabling more practical and robust real-to-simulation pipelines.\n", "link": "http://arxiv.org/abs/2506.04120v1", "date": "2025-06-04", "relevancy": 3.1107, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.629}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6246}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data&body=Title%3A%20Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data%0AAuthor%3A%20Ben%20Moran%20and%20Mauro%20Comi%20and%20Steven%20Bohez%20and%20Tom%20Erez%20and%20Zhibin%20Li%20and%20Leonard%20Hasenclever%0AAbstract%3A%20%20%20Creating%20accurate%2C%20physical%20simulations%20directly%20from%20real-world%20robot%20motion%0Aholds%20great%20value%20for%20safe%2C%20scalable%2C%20and%20affordable%20robot%20learning%2C%20yet%0Aremains%20exceptionally%20challenging.%20Real%20robot%20data%20suffers%20from%20occlusions%2C%0Anoisy%20camera%20poses%2C%20dynamic%20scene%20elements%2C%20which%20hinder%20the%20creation%20of%0Ageometrically%20accurate%20and%20photorealistic%20digital%20twins%20of%20unseen%20objects.%20We%0Aintroduce%20a%20novel%20real-to-sim%20framework%20tackling%20all%20these%20challenges%20at%20once.%0AOur%20key%20insight%20is%20a%20hybrid%20scene%20representation%20merging%20the%20photorealistic%0Arendering%20of%203D%20Gaussian%20Splatting%20with%20explicit%20object%20meshes%20suitable%20for%0Aphysics%20simulation%20within%20a%20single%20representation.%20We%20propose%20an%20end-to-end%0Aoptimization%20pipeline%20that%20leverages%20differentiable%20rendering%20and%0Adifferentiable%20physics%20within%20MuJoCo%20to%20jointly%20refine%20all%20scene%20components%20-%0Afrom%20object%20geometry%20and%20appearance%20to%20robot%20poses%20and%20physical%20parameters%20-%0Adirectly%20from%20raw%20and%20imprecise%20robot%20trajectories.%20This%20unified%20optimization%0Aallows%20us%20to%20simultaneously%20achieve%20high-fidelity%20object%20mesh%20reconstruction%2C%0Agenerate%20photorealistic%20novel%20views%2C%20and%20perform%20annotation-free%20robot%20pose%0Acalibration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20both%20in%0Asimulation%20and%20on%20challenging%20real-world%20sequences%20using%20an%20ALOHA%202%20bi-manual%0Amanipulator%2C%20enabling%20more%20practical%20and%20robust%20real-to-simulation%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatting%2520Physical%2520Scenes%253A%2520End-to-End%2520Real-to-Sim%2520from%2520Imperfect%2520Robot%250A%2520%2520Data%26entry.906535625%3DBen%2520Moran%2520and%2520Mauro%2520Comi%2520and%2520Steven%2520Bohez%2520and%2520Tom%2520Erez%2520and%2520Zhibin%2520Li%2520and%2520Leonard%2520Hasenclever%26entry.1292438233%3D%2520%2520Creating%2520accurate%252C%2520physical%2520simulations%2520directly%2520from%2520real-world%2520robot%2520motion%250Aholds%2520great%2520value%2520for%2520safe%252C%2520scalable%252C%2520and%2520affordable%2520robot%2520learning%252C%2520yet%250Aremains%2520exceptionally%2520challenging.%2520Real%2520robot%2520data%2520suffers%2520from%2520occlusions%252C%250Anoisy%2520camera%2520poses%252C%2520dynamic%2520scene%2520elements%252C%2520which%2520hinder%2520the%2520creation%2520of%250Ageometrically%2520accurate%2520and%2520photorealistic%2520digital%2520twins%2520of%2520unseen%2520objects.%2520We%250Aintroduce%2520a%2520novel%2520real-to-sim%2520framework%2520tackling%2520all%2520these%2520challenges%2520at%2520once.%250AOur%2520key%2520insight%2520is%2520a%2520hybrid%2520scene%2520representation%2520merging%2520the%2520photorealistic%250Arendering%2520of%25203D%2520Gaussian%2520Splatting%2520with%2520explicit%2520object%2520meshes%2520suitable%2520for%250Aphysics%2520simulation%2520within%2520a%2520single%2520representation.%2520We%2520propose%2520an%2520end-to-end%250Aoptimization%2520pipeline%2520that%2520leverages%2520differentiable%2520rendering%2520and%250Adifferentiable%2520physics%2520within%2520MuJoCo%2520to%2520jointly%2520refine%2520all%2520scene%2520components%2520-%250Afrom%2520object%2520geometry%2520and%2520appearance%2520to%2520robot%2520poses%2520and%2520physical%2520parameters%2520-%250Adirectly%2520from%2520raw%2520and%2520imprecise%2520robot%2520trajectories.%2520This%2520unified%2520optimization%250Aallows%2520us%2520to%2520simultaneously%2520achieve%2520high-fidelity%2520object%2520mesh%2520reconstruction%252C%250Agenerate%2520photorealistic%2520novel%2520views%252C%2520and%2520perform%2520annotation-free%2520robot%2520pose%250Acalibration.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520both%2520in%250Asimulation%2520and%2520on%2520challenging%2520real-world%2520sequences%2520using%2520an%2520ALOHA%25202%2520bi-manual%250Amanipulator%252C%2520enabling%2520more%2520practical%2520and%2520robust%2520real-to-simulation%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data&entry.906535625=Ben%20Moran%20and%20Mauro%20Comi%20and%20Steven%20Bohez%20and%20Tom%20Erez%20and%20Zhibin%20Li%20and%20Leonard%20Hasenclever&entry.1292438233=%20%20Creating%20accurate%2C%20physical%20simulations%20directly%20from%20real-world%20robot%20motion%0Aholds%20great%20value%20for%20safe%2C%20scalable%2C%20and%20affordable%20robot%20learning%2C%20yet%0Aremains%20exceptionally%20challenging.%20Real%20robot%20data%20suffers%20from%20occlusions%2C%0Anoisy%20camera%20poses%2C%20dynamic%20scene%20elements%2C%20which%20hinder%20the%20creation%20of%0Ageometrically%20accurate%20and%20photorealistic%20digital%20twins%20of%20unseen%20objects.%20We%0Aintroduce%20a%20novel%20real-to-sim%20framework%20tackling%20all%20these%20challenges%20at%20once.%0AOur%20key%20insight%20is%20a%20hybrid%20scene%20representation%20merging%20the%20photorealistic%0Arendering%20of%203D%20Gaussian%20Splatting%20with%20explicit%20object%20meshes%20suitable%20for%0Aphysics%20simulation%20within%20a%20single%20representation.%20We%20propose%20an%20end-to-end%0Aoptimization%20pipeline%20that%20leverages%20differentiable%20rendering%20and%0Adifferentiable%20physics%20within%20MuJoCo%20to%20jointly%20refine%20all%20scene%20components%20-%0Afrom%20object%20geometry%20and%20appearance%20to%20robot%20poses%20and%20physical%20parameters%20-%0Adirectly%20from%20raw%20and%20imprecise%20robot%20trajectories.%20This%20unified%20optimization%0Aallows%20us%20to%20simultaneously%20achieve%20high-fidelity%20object%20mesh%20reconstruction%2C%0Agenerate%20photorealistic%20novel%20views%2C%20and%20perform%20annotation-free%20robot%20pose%0Acalibration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20both%20in%0Asimulation%20and%20on%20challenging%20real-world%20sequences%20using%20an%20ALOHA%202%20bi-manual%0Amanipulator%2C%20enabling%20more%20practical%20and%20robust%20real-to-simulation%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04120v1&entry.124074799=Read"},
{"title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous\n  Driving with Vision-Language Models", "author": "Yujin Wang and Quanfeng Liu and Jiaqi Fan and Jinlong Hong and Hongqing Chu and Mengjian Tian and Bingzhao Gao and Hong Chen", "abstract": "  Understanding and addressing corner cases is essential for ensuring the\nsafety and reliability of autonomous driving systems. Vision-language models\n(VLMs) play a crucial role in enhancing scenario comprehension, yet they face\nsignificant challenges, such as hallucination and insufficient real-world\ngrounding, which compromise their performance in critical driving scenarios. In\nthis work, RAC3, a novel framework designed to enhance the performance of VLMs\nin corner case comprehension, is proposed. RAC3 integrates a frequency-spatial\nfusion (FSF) image encoder, a cross-modal alignment training method for\nembedding models with hard and semi-hard negative mining, and a fast querying\nand retrieval pipeline based on K-Means clustering and hierarchical navigable\nsmall world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting\nstrategy to guide analogical reasoning and reduce hallucinations during\ninference is introduced. Moreover, an update mechanism is integrated into RAC3\nto ensure continual learning within the framework. Extensive experiments on the\nCODA and nuScenes datasets demonstrate that RAC3 significantly improves corner\ncase comprehension across multiple downstream tasks. Compared to prior\nstate-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the\nCODA-LM benchmark and shows consistent performance gains when integrated with\nend-to-end frameworks like DriveLM. These results demonstrate the effectiveness\nof retrieval-augmented strategies and cross-modal alignment for safer and more\ninterpretable autonomous driving.\n", "link": "http://arxiv.org/abs/2412.11050v3", "date": "2025-06-04", "relevancy": 3.0907, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6417}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAC3%3A%20Retrieval-Augmented%20Corner%20Case%20Comprehension%20for%20Autonomous%0A%20%20Driving%20with%20Vision-Language%20Models&body=Title%3A%20RAC3%3A%20Retrieval-Augmented%20Corner%20Case%20Comprehension%20for%20Autonomous%0A%20%20Driving%20with%20Vision-Language%20Models%0AAuthor%3A%20Yujin%20Wang%20and%20Quanfeng%20Liu%20and%20Jiaqi%20Fan%20and%20Jinlong%20Hong%20and%20Hongqing%20Chu%20and%20Mengjian%20Tian%20and%20Bingzhao%20Gao%20and%20Hong%20Chen%0AAbstract%3A%20%20%20Understanding%20and%20addressing%20corner%20cases%20is%20essential%20for%20ensuring%20the%0Asafety%20and%20reliability%20of%20autonomous%20driving%20systems.%20Vision-language%20models%0A%28VLMs%29%20play%20a%20crucial%20role%20in%20enhancing%20scenario%20comprehension%2C%20yet%20they%20face%0Asignificant%20challenges%2C%20such%20as%20hallucination%20and%20insufficient%20real-world%0Agrounding%2C%20which%20compromise%20their%20performance%20in%20critical%20driving%20scenarios.%20In%0Athis%20work%2C%20RAC3%2C%20a%20novel%20framework%20designed%20to%20enhance%20the%20performance%20of%20VLMs%0Ain%20corner%20case%20comprehension%2C%20is%20proposed.%20RAC3%20integrates%20a%20frequency-spatial%0Afusion%20%28FSF%29%20image%20encoder%2C%20a%20cross-modal%20alignment%20training%20method%20for%0Aembedding%20models%20with%20hard%20and%20semi-hard%20negative%20mining%2C%20and%20a%20fast%20querying%0Aand%20retrieval%20pipeline%20based%20on%20K-Means%20clustering%20and%20hierarchical%20navigable%0Asmall%20world%20%28HNSW%29%20indexing.%20A%20multimodal%20chain-of-thought%20%28CoT%29%20prompting%0Astrategy%20to%20guide%20analogical%20reasoning%20and%20reduce%20hallucinations%20during%0Ainference%20is%20introduced.%20Moreover%2C%20an%20update%20mechanism%20is%20integrated%20into%20RAC3%0Ato%20ensure%20continual%20learning%20within%20the%20framework.%20Extensive%20experiments%20on%20the%0ACODA%20and%20nuScenes%20datasets%20demonstrate%20that%20RAC3%20significantly%20improves%20corner%0Acase%20comprehension%20across%20multiple%20downstream%20tasks.%20Compared%20to%20prior%0Astate-of-the-art%20methods%2C%20RAC3%20achieves%20the%20highest%20final%20score%20of%2074.46%20on%20the%0ACODA-LM%20benchmark%20and%20shows%20consistent%20performance%20gains%20when%20integrated%20with%0Aend-to-end%20frameworks%20like%20DriveLM.%20These%20results%20demonstrate%20the%20effectiveness%0Aof%20retrieval-augmented%20strategies%20and%20cross-modal%20alignment%20for%20safer%20and%20more%0Ainterpretable%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11050v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAC3%253A%2520Retrieval-Augmented%2520Corner%2520Case%2520Comprehension%2520for%2520Autonomous%250A%2520%2520Driving%2520with%2520Vision-Language%2520Models%26entry.906535625%3DYujin%2520Wang%2520and%2520Quanfeng%2520Liu%2520and%2520Jiaqi%2520Fan%2520and%2520Jinlong%2520Hong%2520and%2520Hongqing%2520Chu%2520and%2520Mengjian%2520Tian%2520and%2520Bingzhao%2520Gao%2520and%2520Hong%2520Chen%26entry.1292438233%3D%2520%2520Understanding%2520and%2520addressing%2520corner%2520cases%2520is%2520essential%2520for%2520ensuring%2520the%250Asafety%2520and%2520reliability%2520of%2520autonomous%2520driving%2520systems.%2520Vision-language%2520models%250A%2528VLMs%2529%2520play%2520a%2520crucial%2520role%2520in%2520enhancing%2520scenario%2520comprehension%252C%2520yet%2520they%2520face%250Asignificant%2520challenges%252C%2520such%2520as%2520hallucination%2520and%2520insufficient%2520real-world%250Agrounding%252C%2520which%2520compromise%2520their%2520performance%2520in%2520critical%2520driving%2520scenarios.%2520In%250Athis%2520work%252C%2520RAC3%252C%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520the%2520performance%2520of%2520VLMs%250Ain%2520corner%2520case%2520comprehension%252C%2520is%2520proposed.%2520RAC3%2520integrates%2520a%2520frequency-spatial%250Afusion%2520%2528FSF%2529%2520image%2520encoder%252C%2520a%2520cross-modal%2520alignment%2520training%2520method%2520for%250Aembedding%2520models%2520with%2520hard%2520and%2520semi-hard%2520negative%2520mining%252C%2520and%2520a%2520fast%2520querying%250Aand%2520retrieval%2520pipeline%2520based%2520on%2520K-Means%2520clustering%2520and%2520hierarchical%2520navigable%250Asmall%2520world%2520%2528HNSW%2529%2520indexing.%2520A%2520multimodal%2520chain-of-thought%2520%2528CoT%2529%2520prompting%250Astrategy%2520to%2520guide%2520analogical%2520reasoning%2520and%2520reduce%2520hallucinations%2520during%250Ainference%2520is%2520introduced.%2520Moreover%252C%2520an%2520update%2520mechanism%2520is%2520integrated%2520into%2520RAC3%250Ato%2520ensure%2520continual%2520learning%2520within%2520the%2520framework.%2520Extensive%2520experiments%2520on%2520the%250ACODA%2520and%2520nuScenes%2520datasets%2520demonstrate%2520that%2520RAC3%2520significantly%2520improves%2520corner%250Acase%2520comprehension%2520across%2520multiple%2520downstream%2520tasks.%2520Compared%2520to%2520prior%250Astate-of-the-art%2520methods%252C%2520RAC3%2520achieves%2520the%2520highest%2520final%2520score%2520of%252074.46%2520on%2520the%250ACODA-LM%2520benchmark%2520and%2520shows%2520consistent%2520performance%2520gains%2520when%2520integrated%2520with%250Aend-to-end%2520frameworks%2520like%2520DriveLM.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520retrieval-augmented%2520strategies%2520and%2520cross-modal%2520alignment%2520for%2520safer%2520and%2520more%250Ainterpretable%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11050v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAC3%3A%20Retrieval-Augmented%20Corner%20Case%20Comprehension%20for%20Autonomous%0A%20%20Driving%20with%20Vision-Language%20Models&entry.906535625=Yujin%20Wang%20and%20Quanfeng%20Liu%20and%20Jiaqi%20Fan%20and%20Jinlong%20Hong%20and%20Hongqing%20Chu%20and%20Mengjian%20Tian%20and%20Bingzhao%20Gao%20and%20Hong%20Chen&entry.1292438233=%20%20Understanding%20and%20addressing%20corner%20cases%20is%20essential%20for%20ensuring%20the%0Asafety%20and%20reliability%20of%20autonomous%20driving%20systems.%20Vision-language%20models%0A%28VLMs%29%20play%20a%20crucial%20role%20in%20enhancing%20scenario%20comprehension%2C%20yet%20they%20face%0Asignificant%20challenges%2C%20such%20as%20hallucination%20and%20insufficient%20real-world%0Agrounding%2C%20which%20compromise%20their%20performance%20in%20critical%20driving%20scenarios.%20In%0Athis%20work%2C%20RAC3%2C%20a%20novel%20framework%20designed%20to%20enhance%20the%20performance%20of%20VLMs%0Ain%20corner%20case%20comprehension%2C%20is%20proposed.%20RAC3%20integrates%20a%20frequency-spatial%0Afusion%20%28FSF%29%20image%20encoder%2C%20a%20cross-modal%20alignment%20training%20method%20for%0Aembedding%20models%20with%20hard%20and%20semi-hard%20negative%20mining%2C%20and%20a%20fast%20querying%0Aand%20retrieval%20pipeline%20based%20on%20K-Means%20clustering%20and%20hierarchical%20navigable%0Asmall%20world%20%28HNSW%29%20indexing.%20A%20multimodal%20chain-of-thought%20%28CoT%29%20prompting%0Astrategy%20to%20guide%20analogical%20reasoning%20and%20reduce%20hallucinations%20during%0Ainference%20is%20introduced.%20Moreover%2C%20an%20update%20mechanism%20is%20integrated%20into%20RAC3%0Ato%20ensure%20continual%20learning%20within%20the%20framework.%20Extensive%20experiments%20on%20the%0ACODA%20and%20nuScenes%20datasets%20demonstrate%20that%20RAC3%20significantly%20improves%20corner%0Acase%20comprehension%20across%20multiple%20downstream%20tasks.%20Compared%20to%20prior%0Astate-of-the-art%20methods%2C%20RAC3%20achieves%20the%20highest%20final%20score%20of%2074.46%20on%20the%0ACODA-LM%20benchmark%20and%20shows%20consistent%20performance%20gains%20when%20integrated%20with%0Aend-to-end%20frameworks%20like%20DriveLM.%20These%20results%20demonstrate%20the%20effectiveness%0Aof%20retrieval-augmented%20strategies%20and%20cross-modal%20alignment%20for%20safer%20and%20more%0Ainterpretable%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11050v3&entry.124074799=Read"},
{"title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", "author": "Souhail Hadgi and Luca Moschella and Andrea Santilli and Diego Gomez and Qixing Huang and Emanuele Rodol\u00e0 and Simone Melzi and Maks Ovsjanikov", "abstract": "  Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations. Our code and weights are available at\nhttps://github.com/Souhail-01/3d-text-alignment\n", "link": "http://arxiv.org/abs/2503.05283v2", "date": "2025-06-04", "relevancy": 3.07, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Escaping%20Plato%27s%20Cave%3A%20Towards%20the%20Alignment%20of%203D%20and%20Text%20Latent%0A%20%20Spaces&body=Title%3A%20Escaping%20Plato%27s%20Cave%3A%20Towards%20the%20Alignment%20of%203D%20and%20Text%20Latent%0A%20%20Spaces%0AAuthor%3A%20Souhail%20Hadgi%20and%20Luca%20Moschella%20and%20Andrea%20Santilli%20and%20Diego%20Gomez%20and%20Qixing%20Huang%20and%20Emanuele%20Rodol%C3%A0%20and%20Simone%20Melzi%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20that%2C%20when%20trained%20at%20scale%2C%20uni-modal%202D%20vision%20and%0Atext%20encoders%20converge%20to%20learned%20features%20that%20share%20remarkable%20structural%0Aproperties%2C%20despite%20arising%20from%20different%20representations.%20However%2C%20the%20role%0Aof%203D%20encoders%20with%20respect%20to%20other%20modalities%20remains%20unexplored.%0AFurthermore%2C%20existing%203D%20foundation%20models%20that%20leverage%20large%20datasets%20are%0Atypically%20trained%20with%20explicit%20alignment%20objectives%20with%20respect%20to%20frozen%0Aencoders%20from%20other%20representations.%20In%20this%20work%2C%20we%20investigate%20the%0Apossibility%20of%20a%20posteriori%20alignment%20of%20representations%20obtained%20from%0Auni-modal%203D%20encoders%20compared%20to%20text-based%20feature%20spaces.%20We%20show%20that%20naive%0Apost-training%20feature%20alignment%20of%20uni-modal%20text%20and%203D%20encoders%20results%20in%0Alimited%20performance.%20We%20then%20focus%20on%20extracting%20subspaces%20of%20the%20corresponding%0Afeature%20spaces%20and%20discover%20that%20by%20projecting%20learned%20representations%20onto%0Awell-chosen%20lower-dimensional%20subspaces%20the%20quality%20of%20alignment%20becomes%0Asignificantly%20higher%2C%20leading%20to%20improved%20accuracy%20on%20matching%20and%20retrieval%0Atasks.%20Our%20analysis%20further%20sheds%20light%20on%20the%20nature%20of%20these%20shared%0Asubspaces%2C%20which%20roughly%20separate%20between%20semantic%20and%20geometric%20data%0Arepresentations.%20Overall%2C%20ours%20is%20the%20first%20work%20that%20helps%20to%20establish%20a%0Abaseline%20for%20post-training%20alignment%20of%203D%20uni-modal%20and%20text%20feature%20spaces%2C%0Aand%20helps%20to%20highlight%20both%20the%20shared%20and%20unique%20properties%20of%203D%20data%0Acompared%20to%20other%20representations.%20Our%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/Souhail-01/3d-text-alignment%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEscaping%2520Plato%2527s%2520Cave%253A%2520Towards%2520the%2520Alignment%2520of%25203D%2520and%2520Text%2520Latent%250A%2520%2520Spaces%26entry.906535625%3DSouhail%2520Hadgi%2520and%2520Luca%2520Moschella%2520and%2520Andrea%2520Santilli%2520and%2520Diego%2520Gomez%2520and%2520Qixing%2520Huang%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Simone%2520Melzi%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520shown%2520that%252C%2520when%2520trained%2520at%2520scale%252C%2520uni-modal%25202D%2520vision%2520and%250Atext%2520encoders%2520converge%2520to%2520learned%2520features%2520that%2520share%2520remarkable%2520structural%250Aproperties%252C%2520despite%2520arising%2520from%2520different%2520representations.%2520However%252C%2520the%2520role%250Aof%25203D%2520encoders%2520with%2520respect%2520to%2520other%2520modalities%2520remains%2520unexplored.%250AFurthermore%252C%2520existing%25203D%2520foundation%2520models%2520that%2520leverage%2520large%2520datasets%2520are%250Atypically%2520trained%2520with%2520explicit%2520alignment%2520objectives%2520with%2520respect%2520to%2520frozen%250Aencoders%2520from%2520other%2520representations.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Apossibility%2520of%2520a%2520posteriori%2520alignment%2520of%2520representations%2520obtained%2520from%250Auni-modal%25203D%2520encoders%2520compared%2520to%2520text-based%2520feature%2520spaces.%2520We%2520show%2520that%2520naive%250Apost-training%2520feature%2520alignment%2520of%2520uni-modal%2520text%2520and%25203D%2520encoders%2520results%2520in%250Alimited%2520performance.%2520We%2520then%2520focus%2520on%2520extracting%2520subspaces%2520of%2520the%2520corresponding%250Afeature%2520spaces%2520and%2520discover%2520that%2520by%2520projecting%2520learned%2520representations%2520onto%250Awell-chosen%2520lower-dimensional%2520subspaces%2520the%2520quality%2520of%2520alignment%2520becomes%250Asignificantly%2520higher%252C%2520leading%2520to%2520improved%2520accuracy%2520on%2520matching%2520and%2520retrieval%250Atasks.%2520Our%2520analysis%2520further%2520sheds%2520light%2520on%2520the%2520nature%2520of%2520these%2520shared%250Asubspaces%252C%2520which%2520roughly%2520separate%2520between%2520semantic%2520and%2520geometric%2520data%250Arepresentations.%2520Overall%252C%2520ours%2520is%2520the%2520first%2520work%2520that%2520helps%2520to%2520establish%2520a%250Abaseline%2520for%2520post-training%2520alignment%2520of%25203D%2520uni-modal%2520and%2520text%2520feature%2520spaces%252C%250Aand%2520helps%2520to%2520highlight%2520both%2520the%2520shared%2520and%2520unique%2520properties%2520of%25203D%2520data%250Acompared%2520to%2520other%2520representations.%2520Our%2520code%2520and%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/Souhail-01/3d-text-alignment%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Escaping%20Plato%27s%20Cave%3A%20Towards%20the%20Alignment%20of%203D%20and%20Text%20Latent%0A%20%20Spaces&entry.906535625=Souhail%20Hadgi%20and%20Luca%20Moschella%20and%20Andrea%20Santilli%20and%20Diego%20Gomez%20and%20Qixing%20Huang%20and%20Emanuele%20Rodol%C3%A0%20and%20Simone%20Melzi%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Recent%20works%20have%20shown%20that%2C%20when%20trained%20at%20scale%2C%20uni-modal%202D%20vision%20and%0Atext%20encoders%20converge%20to%20learned%20features%20that%20share%20remarkable%20structural%0Aproperties%2C%20despite%20arising%20from%20different%20representations.%20However%2C%20the%20role%0Aof%203D%20encoders%20with%20respect%20to%20other%20modalities%20remains%20unexplored.%0AFurthermore%2C%20existing%203D%20foundation%20models%20that%20leverage%20large%20datasets%20are%0Atypically%20trained%20with%20explicit%20alignment%20objectives%20with%20respect%20to%20frozen%0Aencoders%20from%20other%20representations.%20In%20this%20work%2C%20we%20investigate%20the%0Apossibility%20of%20a%20posteriori%20alignment%20of%20representations%20obtained%20from%0Auni-modal%203D%20encoders%20compared%20to%20text-based%20feature%20spaces.%20We%20show%20that%20naive%0Apost-training%20feature%20alignment%20of%20uni-modal%20text%20and%203D%20encoders%20results%20in%0Alimited%20performance.%20We%20then%20focus%20on%20extracting%20subspaces%20of%20the%20corresponding%0Afeature%20spaces%20and%20discover%20that%20by%20projecting%20learned%20representations%20onto%0Awell-chosen%20lower-dimensional%20subspaces%20the%20quality%20of%20alignment%20becomes%0Asignificantly%20higher%2C%20leading%20to%20improved%20accuracy%20on%20matching%20and%20retrieval%0Atasks.%20Our%20analysis%20further%20sheds%20light%20on%20the%20nature%20of%20these%20shared%0Asubspaces%2C%20which%20roughly%20separate%20between%20semantic%20and%20geometric%20data%0Arepresentations.%20Overall%2C%20ours%20is%20the%20first%20work%20that%20helps%20to%20establish%20a%0Abaseline%20for%20post-training%20alignment%20of%203D%20uni-modal%20and%20text%20feature%20spaces%2C%0Aand%20helps%20to%20highlight%20both%20the%20shared%20and%20unique%20properties%20of%203D%20data%0Acompared%20to%20other%20representations.%20Our%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/Souhail-01/3d-text-alignment%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05283v2&entry.124074799=Read"},
{"title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with\n  Fine-grained Multi-axis Perception Tasks", "author": "Keanu Nichols and Nazia Tasnim and Yuting Yan and Nicholas Ikechukwu and Elva Zou and Deepti Ghadiyaram and Bryan A. Plummer", "abstract": "  Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark\n", "link": "http://arxiv.org/abs/2505.21649v4", "date": "2025-06-04", "relevancy": 3.053, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks&body=Title%3A%20Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks%0AAuthor%3A%20Keanu%20Nichols%20and%20Nazia%20Tasnim%20and%20Yuting%20Yan%20and%20Nicholas%20Ikechukwu%20and%20Elva%20Zou%20and%20Deepti%20Ghadiyaram%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Object%20orientation%20understanding%20represents%20a%20fundamental%20challenge%20in%20visual%0Aperception%20critical%20for%20applications%20like%20robotic%20manipulation%20and%20augmented%0Areality.%20Current%20vision-language%20benchmarks%20fail%20to%20isolate%20this%20capability%2C%0Aoften%20conflating%20it%20with%20positional%20relationships%20and%20general%20scene%0Aunderstanding.%20We%20introduce%20DORI%20%28Discriminative%20Orientation%20Reasoning%0AIntelligence%29%2C%20a%20comprehensive%20benchmark%20establishing%20object%20orientation%0Aperception%20as%20a%20primary%20evaluation%20target.%20DORI%20assesses%20four%20dimensions%20of%0Aorientation%20comprehension%3A%20frontal%20alignment%2C%20rotational%20transformations%2C%0Arelative%20directional%20relationships%2C%20and%20canonical%20orientation%20understanding.%0AThrough%20carefully%20curated%20tasks%20from%2011%20datasets%20spanning%2067%20object%20categories%0Aacross%20synthetic%20and%20real-world%20scenarios%2C%20DORI%20provides%20insights%20on%20how%0Amulti-modal%20systems%20understand%20object%20orientations.%20Our%20evaluation%20of%2015%0Astate-of-the-art%20vision-language%20models%20reveals%20critical%20limitations%3A%20even%20the%0Abest%20models%20achieve%20only%2054.2%25%20accuracy%20on%20coarse%20tasks%20and%2033.0%25%20on%20granular%0Aorientation%20judgments%2C%20with%20performance%20deteriorating%20for%20tasks%20requiring%0Areference%20frame%20shifts%20or%20compound%20rotations.%20These%20findings%20demonstrate%20the%0Aneed%20for%20dedicated%20orientation%20representation%20mechanisms%2C%20as%20models%20show%0Asystematic%20inability%20to%20perform%20precise%20angular%20estimations%2C%20track%20orientation%0Achanges%20across%20viewpoints%2C%20and%20understand%20compound%20rotations%20-%20suggesting%0Alimitations%20in%20their%20internal%203D%20spatial%20representations.%20As%20the%20first%0Adiagnostic%20framework%20specifically%20designed%20for%20orientation%20awareness%20in%0Amultimodal%20systems%2C%20DORI%20offers%20implications%20for%20improving%20robotic%20control%2C%203D%0Ascene%20reconstruction%2C%20and%20human-AI%20interaction%20in%20physical%20environments.%20DORI%0Adata%3A%20https%3A//huggingface.co/datasets/appledora/DORI-Benchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21649v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRight%2520Side%2520Up%253F%2520Disentangling%2520Orientation%2520Understanding%2520in%2520MLLMs%2520with%250A%2520%2520Fine-grained%2520Multi-axis%2520Perception%2520Tasks%26entry.906535625%3DKeanu%2520Nichols%2520and%2520Nazia%2520Tasnim%2520and%2520Yuting%2520Yan%2520and%2520Nicholas%2520Ikechukwu%2520and%2520Elva%2520Zou%2520and%2520Deepti%2520Ghadiyaram%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Object%2520orientation%2520understanding%2520represents%2520a%2520fundamental%2520challenge%2520in%2520visual%250Aperception%2520critical%2520for%2520applications%2520like%2520robotic%2520manipulation%2520and%2520augmented%250Areality.%2520Current%2520vision-language%2520benchmarks%2520fail%2520to%2520isolate%2520this%2520capability%252C%250Aoften%2520conflating%2520it%2520with%2520positional%2520relationships%2520and%2520general%2520scene%250Aunderstanding.%2520We%2520introduce%2520DORI%2520%2528Discriminative%2520Orientation%2520Reasoning%250AIntelligence%2529%252C%2520a%2520comprehensive%2520benchmark%2520establishing%2520object%2520orientation%250Aperception%2520as%2520a%2520primary%2520evaluation%2520target.%2520DORI%2520assesses%2520four%2520dimensions%2520of%250Aorientation%2520comprehension%253A%2520frontal%2520alignment%252C%2520rotational%2520transformations%252C%250Arelative%2520directional%2520relationships%252C%2520and%2520canonical%2520orientation%2520understanding.%250AThrough%2520carefully%2520curated%2520tasks%2520from%252011%2520datasets%2520spanning%252067%2520object%2520categories%250Aacross%2520synthetic%2520and%2520real-world%2520scenarios%252C%2520DORI%2520provides%2520insights%2520on%2520how%250Amulti-modal%2520systems%2520understand%2520object%2520orientations.%2520Our%2520evaluation%2520of%252015%250Astate-of-the-art%2520vision-language%2520models%2520reveals%2520critical%2520limitations%253A%2520even%2520the%250Abest%2520models%2520achieve%2520only%252054.2%2525%2520accuracy%2520on%2520coarse%2520tasks%2520and%252033.0%2525%2520on%2520granular%250Aorientation%2520judgments%252C%2520with%2520performance%2520deteriorating%2520for%2520tasks%2520requiring%250Areference%2520frame%2520shifts%2520or%2520compound%2520rotations.%2520These%2520findings%2520demonstrate%2520the%250Aneed%2520for%2520dedicated%2520orientation%2520representation%2520mechanisms%252C%2520as%2520models%2520show%250Asystematic%2520inability%2520to%2520perform%2520precise%2520angular%2520estimations%252C%2520track%2520orientation%250Achanges%2520across%2520viewpoints%252C%2520and%2520understand%2520compound%2520rotations%2520-%2520suggesting%250Alimitations%2520in%2520their%2520internal%25203D%2520spatial%2520representations.%2520As%2520the%2520first%250Adiagnostic%2520framework%2520specifically%2520designed%2520for%2520orientation%2520awareness%2520in%250Amultimodal%2520systems%252C%2520DORI%2520offers%2520implications%2520for%2520improving%2520robotic%2520control%252C%25203D%250Ascene%2520reconstruction%252C%2520and%2520human-AI%2520interaction%2520in%2520physical%2520environments.%2520DORI%250Adata%253A%2520https%253A//huggingface.co/datasets/appledora/DORI-Benchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21649v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Right%20Side%20Up%3F%20Disentangling%20Orientation%20Understanding%20in%20MLLMs%20with%0A%20%20Fine-grained%20Multi-axis%20Perception%20Tasks&entry.906535625=Keanu%20Nichols%20and%20Nazia%20Tasnim%20and%20Yuting%20Yan%20and%20Nicholas%20Ikechukwu%20and%20Elva%20Zou%20and%20Deepti%20Ghadiyaram%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Object%20orientation%20understanding%20represents%20a%20fundamental%20challenge%20in%20visual%0Aperception%20critical%20for%20applications%20like%20robotic%20manipulation%20and%20augmented%0Areality.%20Current%20vision-language%20benchmarks%20fail%20to%20isolate%20this%20capability%2C%0Aoften%20conflating%20it%20with%20positional%20relationships%20and%20general%20scene%0Aunderstanding.%20We%20introduce%20DORI%20%28Discriminative%20Orientation%20Reasoning%0AIntelligence%29%2C%20a%20comprehensive%20benchmark%20establishing%20object%20orientation%0Aperception%20as%20a%20primary%20evaluation%20target.%20DORI%20assesses%20four%20dimensions%20of%0Aorientation%20comprehension%3A%20frontal%20alignment%2C%20rotational%20transformations%2C%0Arelative%20directional%20relationships%2C%20and%20canonical%20orientation%20understanding.%0AThrough%20carefully%20curated%20tasks%20from%2011%20datasets%20spanning%2067%20object%20categories%0Aacross%20synthetic%20and%20real-world%20scenarios%2C%20DORI%20provides%20insights%20on%20how%0Amulti-modal%20systems%20understand%20object%20orientations.%20Our%20evaluation%20of%2015%0Astate-of-the-art%20vision-language%20models%20reveals%20critical%20limitations%3A%20even%20the%0Abest%20models%20achieve%20only%2054.2%25%20accuracy%20on%20coarse%20tasks%20and%2033.0%25%20on%20granular%0Aorientation%20judgments%2C%20with%20performance%20deteriorating%20for%20tasks%20requiring%0Areference%20frame%20shifts%20or%20compound%20rotations.%20These%20findings%20demonstrate%20the%0Aneed%20for%20dedicated%20orientation%20representation%20mechanisms%2C%20as%20models%20show%0Asystematic%20inability%20to%20perform%20precise%20angular%20estimations%2C%20track%20orientation%0Achanges%20across%20viewpoints%2C%20and%20understand%20compound%20rotations%20-%20suggesting%0Alimitations%20in%20their%20internal%203D%20spatial%20representations.%20As%20the%20first%0Adiagnostic%20framework%20specifically%20designed%20for%20orientation%20awareness%20in%0Amultimodal%20systems%2C%20DORI%20offers%20implications%20for%20improving%20robotic%20control%2C%203D%0Ascene%20reconstruction%2C%20and%20human-AI%20interaction%20in%20physical%20environments.%20DORI%0Adata%3A%20https%3A//huggingface.co/datasets/appledora/DORI-Benchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21649v4&entry.124074799=Read"},
{"title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation", "author": "Bin Lin and Zongjian Li and Xinhua Cheng and Yuwei Niu and Yang Ye and Xianyi He and Shenghai Yuan and Wangbo Yu and Shaodong Wang and Yunyang Ge and Yatian Pang and Li Yuan", "abstract": "  Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld achieves\nimpressive performance across diverse tasks, including image understanding,\ngeneration, manipulation, and perception. We fully open-source the UniWorld\nframework, including model weights, training and evaluation scripts, and\ndatasets to promote reproducibility and further research.\n", "link": "http://arxiv.org/abs/2506.03147v2", "date": "2025-06-04", "relevancy": 3.0414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation&body=Title%3A%20UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation%0AAuthor%3A%20Bin%20Lin%20and%20Zongjian%20Li%20and%20Xinhua%20Cheng%20and%20Yuwei%20Niu%20and%20Yang%20Ye%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Wangbo%20Yu%20and%20Shaodong%20Wang%20and%20Yunyang%20Ge%20and%20Yatian%20Pang%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Although%20existing%20unified%20models%20achieve%20strong%20performance%20in%0Avision-language%20understanding%20and%20text-to-image%20generation%2C%20they%20remain%20limited%0Ain%20addressing%20image%20perception%20and%20manipulation%20--%20capabilities%20increasingly%0Ademanded%20in%20practical%20applications.%20Recently%2C%20OpenAI%20introduced%20the%20powerful%0AGPT-4o-Image%20model%2C%20which%20showcases%20advanced%20capabilities%20in%20comprehensive%0Aimage%20perception%20and%20manipulation%2C%20sparking%20widespread%20interest.%20Through%0Acarefully%20designed%20experiments%2C%20we%20observe%20that%20GPT-4o-Image%20likely%20relies%20on%0Asemantic%20encoders%20rather%20than%20VAEs%20for%20feature%20extraction%2C%20despite%20VAEs%20being%0Acommonly%20regarded%20as%20crucial%20for%20image%20manipulation%20tasks.%20Inspired%20by%20this%0Ainsight%2C%20we%20propose%20UniWorld%2C%20a%20unified%20generative%20framework%20built%20upon%0Asemantic%20features%20extracted%20from%20powerful%20multimodal%20large%20language%20models%20and%0Acontrastive%20semantic%20encoders.%20Using%20only%202.7M%20training%20data%2C%20UniWorld%20achieves%0Aimpressive%20performance%20across%20diverse%20tasks%2C%20including%20image%20understanding%2C%0Ageneration%2C%20manipulation%2C%20and%20perception.%20We%20fully%20open-source%20the%20UniWorld%0Aframework%2C%20including%20model%20weights%2C%20training%20and%20evaluation%20scripts%2C%20and%0Adatasets%20to%20promote%20reproducibility%20and%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniWorld%253A%2520High-Resolution%2520Semantic%2520Encoders%2520for%2520Unified%2520Visual%250A%2520%2520Understanding%2520and%2520Generation%26entry.906535625%3DBin%2520Lin%2520and%2520Zongjian%2520Li%2520and%2520Xinhua%2520Cheng%2520and%2520Yuwei%2520Niu%2520and%2520Yang%2520Ye%2520and%2520Xianyi%2520He%2520and%2520Shenghai%2520Yuan%2520and%2520Wangbo%2520Yu%2520and%2520Shaodong%2520Wang%2520and%2520Yunyang%2520Ge%2520and%2520Yatian%2520Pang%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Although%2520existing%2520unified%2520models%2520achieve%2520strong%2520performance%2520in%250Avision-language%2520understanding%2520and%2520text-to-image%2520generation%252C%2520they%2520remain%2520limited%250Ain%2520addressing%2520image%2520perception%2520and%2520manipulation%2520--%2520capabilities%2520increasingly%250Ademanded%2520in%2520practical%2520applications.%2520Recently%252C%2520OpenAI%2520introduced%2520the%2520powerful%250AGPT-4o-Image%2520model%252C%2520which%2520showcases%2520advanced%2520capabilities%2520in%2520comprehensive%250Aimage%2520perception%2520and%2520manipulation%252C%2520sparking%2520widespread%2520interest.%2520Through%250Acarefully%2520designed%2520experiments%252C%2520we%2520observe%2520that%2520GPT-4o-Image%2520likely%2520relies%2520on%250Asemantic%2520encoders%2520rather%2520than%2520VAEs%2520for%2520feature%2520extraction%252C%2520despite%2520VAEs%2520being%250Acommonly%2520regarded%2520as%2520crucial%2520for%2520image%2520manipulation%2520tasks.%2520Inspired%2520by%2520this%250Ainsight%252C%2520we%2520propose%2520UniWorld%252C%2520a%2520unified%2520generative%2520framework%2520built%2520upon%250Asemantic%2520features%2520extracted%2520from%2520powerful%2520multimodal%2520large%2520language%2520models%2520and%250Acontrastive%2520semantic%2520encoders.%2520Using%2520only%25202.7M%2520training%2520data%252C%2520UniWorld%2520achieves%250Aimpressive%2520performance%2520across%2520diverse%2520tasks%252C%2520including%2520image%2520understanding%252C%250Ageneration%252C%2520manipulation%252C%2520and%2520perception.%2520We%2520fully%2520open-source%2520the%2520UniWorld%250Aframework%252C%2520including%2520model%2520weights%252C%2520training%2520and%2520evaluation%2520scripts%252C%2520and%250Adatasets%2520to%2520promote%2520reproducibility%2520and%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation&entry.906535625=Bin%20Lin%20and%20Zongjian%20Li%20and%20Xinhua%20Cheng%20and%20Yuwei%20Niu%20and%20Yang%20Ye%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Wangbo%20Yu%20and%20Shaodong%20Wang%20and%20Yunyang%20Ge%20and%20Yatian%20Pang%20and%20Li%20Yuan&entry.1292438233=%20%20Although%20existing%20unified%20models%20achieve%20strong%20performance%20in%0Avision-language%20understanding%20and%20text-to-image%20generation%2C%20they%20remain%20limited%0Ain%20addressing%20image%20perception%20and%20manipulation%20--%20capabilities%20increasingly%0Ademanded%20in%20practical%20applications.%20Recently%2C%20OpenAI%20introduced%20the%20powerful%0AGPT-4o-Image%20model%2C%20which%20showcases%20advanced%20capabilities%20in%20comprehensive%0Aimage%20perception%20and%20manipulation%2C%20sparking%20widespread%20interest.%20Through%0Acarefully%20designed%20experiments%2C%20we%20observe%20that%20GPT-4o-Image%20likely%20relies%20on%0Asemantic%20encoders%20rather%20than%20VAEs%20for%20feature%20extraction%2C%20despite%20VAEs%20being%0Acommonly%20regarded%20as%20crucial%20for%20image%20manipulation%20tasks.%20Inspired%20by%20this%0Ainsight%2C%20we%20propose%20UniWorld%2C%20a%20unified%20generative%20framework%20built%20upon%0Asemantic%20features%20extracted%20from%20powerful%20multimodal%20large%20language%20models%20and%0Acontrastive%20semantic%20encoders.%20Using%20only%202.7M%20training%20data%2C%20UniWorld%20achieves%0Aimpressive%20performance%20across%20diverse%20tasks%2C%20including%20image%20understanding%2C%0Ageneration%2C%20manipulation%2C%20and%20perception.%20We%20fully%20open-source%20the%20UniWorld%0Aframework%2C%20including%20model%20weights%2C%20training%20and%20evaluation%20scripts%2C%20and%0Adatasets%20to%20promote%20reproducibility%20and%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03147v2&entry.124074799=Read"},
{"title": "Learning 3D Representations from Procedural 3D Programs", "author": "Xuweiyi Chen and Zezhou Cheng", "abstract": "  Self-supervised learning has emerged as a promising approach for acquiring\ntransferable 3D representations from unlabeled 3D point clouds. Unlike 2D\nimages, which are widely accessible, acquiring 3D assets requires specialized\nexpertise or professional 3D scanning equipment, making it difficult to scale\nand raising copyright concerns. To address these challenges, we propose\nlearning 3D representations from procedural 3D programs that automatically\ngenerate 3D shapes using simple primitives and augmentations. Remarkably,\ndespite lacking semantic content, the 3D representations learned from the\nprocedurally generated 3D shapes perform on par with state-of-the-art\nrepresentations learned from semantically recognizable 3D models (e.g.,\nairplanes) across various downstream 3D tasks, including shape classification,\npart segmentation, and masked point cloud completion. We provide a detailed\nanalysis on factors that make a good 3D procedural program. Extensive\nexperiments further suggest that current self-supervised learning methods on\npoint clouds do not rely on the semantics of 3D shapes, shedding light on the\nnature of 3D representations learned.\n", "link": "http://arxiv.org/abs/2411.17467v2", "date": "2025-06-04", "relevancy": 3.0195, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%203D%20Representations%20from%20Procedural%203D%20Programs&body=Title%3A%20Learning%203D%20Representations%20from%20Procedural%203D%20Programs%0AAuthor%3A%20Xuweiyi%20Chen%20and%20Zezhou%20Cheng%0AAbstract%3A%20%20%20Self-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20for%20acquiring%0Atransferable%203D%20representations%20from%20unlabeled%203D%20point%20clouds.%20Unlike%202D%0Aimages%2C%20which%20are%20widely%20accessible%2C%20acquiring%203D%20assets%20requires%20specialized%0Aexpertise%20or%20professional%203D%20scanning%20equipment%2C%20making%20it%20difficult%20to%20scale%0Aand%20raising%20copyright%20concerns.%20To%20address%20these%20challenges%2C%20we%20propose%0Alearning%203D%20representations%20from%20procedural%203D%20programs%20that%20automatically%0Agenerate%203D%20shapes%20using%20simple%20primitives%20and%20augmentations.%20Remarkably%2C%0Adespite%20lacking%20semantic%20content%2C%20the%203D%20representations%20learned%20from%20the%0Aprocedurally%20generated%203D%20shapes%20perform%20on%20par%20with%20state-of-the-art%0Arepresentations%20learned%20from%20semantically%20recognizable%203D%20models%20%28e.g.%2C%0Aairplanes%29%20across%20various%20downstream%203D%20tasks%2C%20including%20shape%20classification%2C%0Apart%20segmentation%2C%20and%20masked%20point%20cloud%20completion.%20We%20provide%20a%20detailed%0Aanalysis%20on%20factors%20that%20make%20a%20good%203D%20procedural%20program.%20Extensive%0Aexperiments%20further%20suggest%20that%20current%20self-supervised%20learning%20methods%20on%0Apoint%20clouds%20do%20not%20rely%20on%20the%20semantics%20of%203D%20shapes%2C%20shedding%20light%20on%20the%0Anature%20of%203D%20representations%20learned.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%25203D%2520Representations%2520from%2520Procedural%25203D%2520Programs%26entry.906535625%3DXuweiyi%2520Chen%2520and%2520Zezhou%2520Cheng%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520acquiring%250Atransferable%25203D%2520representations%2520from%2520unlabeled%25203D%2520point%2520clouds.%2520Unlike%25202D%250Aimages%252C%2520which%2520are%2520widely%2520accessible%252C%2520acquiring%25203D%2520assets%2520requires%2520specialized%250Aexpertise%2520or%2520professional%25203D%2520scanning%2520equipment%252C%2520making%2520it%2520difficult%2520to%2520scale%250Aand%2520raising%2520copyright%2520concerns.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Alearning%25203D%2520representations%2520from%2520procedural%25203D%2520programs%2520that%2520automatically%250Agenerate%25203D%2520shapes%2520using%2520simple%2520primitives%2520and%2520augmentations.%2520Remarkably%252C%250Adespite%2520lacking%2520semantic%2520content%252C%2520the%25203D%2520representations%2520learned%2520from%2520the%250Aprocedurally%2520generated%25203D%2520shapes%2520perform%2520on%2520par%2520with%2520state-of-the-art%250Arepresentations%2520learned%2520from%2520semantically%2520recognizable%25203D%2520models%2520%2528e.g.%252C%250Aairplanes%2529%2520across%2520various%2520downstream%25203D%2520tasks%252C%2520including%2520shape%2520classification%252C%250Apart%2520segmentation%252C%2520and%2520masked%2520point%2520cloud%2520completion.%2520We%2520provide%2520a%2520detailed%250Aanalysis%2520on%2520factors%2520that%2520make%2520a%2520good%25203D%2520procedural%2520program.%2520Extensive%250Aexperiments%2520further%2520suggest%2520that%2520current%2520self-supervised%2520learning%2520methods%2520on%250Apoint%2520clouds%2520do%2520not%2520rely%2520on%2520the%2520semantics%2520of%25203D%2520shapes%252C%2520shedding%2520light%2520on%2520the%250Anature%2520of%25203D%2520representations%2520learned.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%203D%20Representations%20from%20Procedural%203D%20Programs&entry.906535625=Xuweiyi%20Chen%20and%20Zezhou%20Cheng&entry.1292438233=%20%20Self-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20for%20acquiring%0Atransferable%203D%20representations%20from%20unlabeled%203D%20point%20clouds.%20Unlike%202D%0Aimages%2C%20which%20are%20widely%20accessible%2C%20acquiring%203D%20assets%20requires%20specialized%0Aexpertise%20or%20professional%203D%20scanning%20equipment%2C%20making%20it%20difficult%20to%20scale%0Aand%20raising%20copyright%20concerns.%20To%20address%20these%20challenges%2C%20we%20propose%0Alearning%203D%20representations%20from%20procedural%203D%20programs%20that%20automatically%0Agenerate%203D%20shapes%20using%20simple%20primitives%20and%20augmentations.%20Remarkably%2C%0Adespite%20lacking%20semantic%20content%2C%20the%203D%20representations%20learned%20from%20the%0Aprocedurally%20generated%203D%20shapes%20perform%20on%20par%20with%20state-of-the-art%0Arepresentations%20learned%20from%20semantically%20recognizable%203D%20models%20%28e.g.%2C%0Aairplanes%29%20across%20various%20downstream%203D%20tasks%2C%20including%20shape%20classification%2C%0Apart%20segmentation%2C%20and%20masked%20point%20cloud%20completion.%20We%20provide%20a%20detailed%0Aanalysis%20on%20factors%20that%20make%20a%20good%203D%20procedural%20program.%20Extensive%0Aexperiments%20further%20suggest%20that%20current%20self-supervised%20learning%20methods%20on%0Apoint%20clouds%20do%20not%20rely%20on%20the%20semantics%20of%203D%20shapes%2C%20shedding%20light%20on%20the%0Anature%20of%203D%20representations%20learned.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17467v2&entry.124074799=Read"},
{"title": "Multi-view Surface Reconstruction Using Normal and Reflectance Cues", "author": "Robin Bruneau and Baptiste Brument and Yvain Qu\u00e9au and Jean M\u00e9lou and Fran\u00e7ois Bernard Lauze and Jean-Denis Durou and Lilian Calvet", "abstract": "  Achieving high-fidelity 3D surface reconstruction while preserving fine\ndetails remains challenging, especially in the presence of materials with\ncomplex reflectance properties and without a dense-view setup. In this paper,\nwe introduce a versatile framework that incorporates multi-view normal and\noptionally reflectance maps into radiance-based surface reconstruction. Our\napproach employs a pixel-wise joint re-parametrization of reflectance and\nsurface normals, representing them as a vector of radiances under simulated,\nvarying illumination. This formulation enables seamless incorporation into\nstandard surface reconstruction pipelines, such as traditional multi-view\nstereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined\nwith the latter, our approach achieves state-of-the-art performance on\nmulti-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,\nLUCES-MV and Skoltech3D. In particular, our method excels in reconstructing\nfine-grained details and handling challenging visibility conditions. The\npresent paper is an extended version of the earlier conference paper by Brument\net al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2024), featuring an accelerated and more robust\nalgorithm as well as a broader empirical evaluation. The code and data relative\nto this article is available at https://github.com/RobinBruneau/RNb-NeuS2.\n", "link": "http://arxiv.org/abs/2506.04115v1", "date": "2025-06-04", "relevancy": 2.9621, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues&body=Title%3A%20Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues%0AAuthor%3A%20Robin%20Bruneau%20and%20Baptiste%20Brument%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet%0AAbstract%3A%20%20%20Achieving%20high-fidelity%203D%20surface%20reconstruction%20while%20preserving%20fine%0Adetails%20remains%20challenging%2C%20especially%20in%20the%20presence%20of%20materials%20with%0Acomplex%20reflectance%20properties%20and%20without%20a%20dense-view%20setup.%20In%20this%20paper%2C%0Awe%20introduce%20a%20versatile%20framework%20that%20incorporates%20multi-view%20normal%20and%0Aoptionally%20reflectance%20maps%20into%20radiance-based%20surface%20reconstruction.%20Our%0Aapproach%20employs%20a%20pixel-wise%20joint%20re-parametrization%20of%20reflectance%20and%0Asurface%20normals%2C%20representing%20them%20as%20a%20vector%20of%20radiances%20under%20simulated%2C%0Avarying%20illumination.%20This%20formulation%20enables%20seamless%20incorporation%20into%0Astandard%20surface%20reconstruction%20pipelines%2C%20such%20as%20traditional%20multi-view%0Astereo%20%28MVS%29%20frameworks%20or%20modern%20neural%20volume%20rendering%20%28NVR%29%20ones.%20Combined%0Awith%20the%20latter%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%0Amulti-view%20photometric%20stereo%20%28MVPS%29%20benchmark%20datasets%2C%20including%20DiLiGenT-MV%2C%0ALUCES-MV%20and%20Skoltech3D.%20In%20particular%2C%20our%20method%20excels%20in%20reconstructing%0Afine-grained%20details%20and%20handling%20challenging%20visibility%20conditions.%20The%0Apresent%20paper%20is%20an%20extended%20version%20of%20the%20earlier%20conference%20paper%20by%20Brument%0Aet%20al.%20%28in%20Proceedings%20of%20the%20IEEE/CVF%20Conference%20on%20Computer%20Vision%20and%0APattern%20Recognition%20%28CVPR%29%2C%202024%29%2C%20featuring%20an%20accelerated%20and%20more%20robust%0Aalgorithm%20as%20well%20as%20a%20broader%20empirical%20evaluation.%20The%20code%20and%20data%20relative%0Ato%20this%20article%20is%20available%20at%20https%3A//github.com/RobinBruneau/RNb-NeuS2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Surface%2520Reconstruction%2520Using%2520Normal%2520and%2520Reflectance%2520Cues%26entry.906535625%3DRobin%2520Bruneau%2520and%2520Baptiste%2520Brument%2520and%2520Yvain%2520Qu%25C3%25A9au%2520and%2520Jean%2520M%25C3%25A9lou%2520and%2520Fran%25C3%25A7ois%2520Bernard%2520Lauze%2520and%2520Jean-Denis%2520Durou%2520and%2520Lilian%2520Calvet%26entry.1292438233%3D%2520%2520Achieving%2520high-fidelity%25203D%2520surface%2520reconstruction%2520while%2520preserving%2520fine%250Adetails%2520remains%2520challenging%252C%2520especially%2520in%2520the%2520presence%2520of%2520materials%2520with%250Acomplex%2520reflectance%2520properties%2520and%2520without%2520a%2520dense-view%2520setup.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520a%2520versatile%2520framework%2520that%2520incorporates%2520multi-view%2520normal%2520and%250Aoptionally%2520reflectance%2520maps%2520into%2520radiance-based%2520surface%2520reconstruction.%2520Our%250Aapproach%2520employs%2520a%2520pixel-wise%2520joint%2520re-parametrization%2520of%2520reflectance%2520and%250Asurface%2520normals%252C%2520representing%2520them%2520as%2520a%2520vector%2520of%2520radiances%2520under%2520simulated%252C%250Avarying%2520illumination.%2520This%2520formulation%2520enables%2520seamless%2520incorporation%2520into%250Astandard%2520surface%2520reconstruction%2520pipelines%252C%2520such%2520as%2520traditional%2520multi-view%250Astereo%2520%2528MVS%2529%2520frameworks%2520or%2520modern%2520neural%2520volume%2520rendering%2520%2528NVR%2529%2520ones.%2520Combined%250Awith%2520the%2520latter%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%250Amulti-view%2520photometric%2520stereo%2520%2528MVPS%2529%2520benchmark%2520datasets%252C%2520including%2520DiLiGenT-MV%252C%250ALUCES-MV%2520and%2520Skoltech3D.%2520In%2520particular%252C%2520our%2520method%2520excels%2520in%2520reconstructing%250Afine-grained%2520details%2520and%2520handling%2520challenging%2520visibility%2520conditions.%2520The%250Apresent%2520paper%2520is%2520an%2520extended%2520version%2520of%2520the%2520earlier%2520conference%2520paper%2520by%2520Brument%250Aet%2520al.%2520%2528in%2520Proceedings%2520of%2520the%2520IEEE/CVF%2520Conference%2520on%2520Computer%2520Vision%2520and%250APattern%2520Recognition%2520%2528CVPR%2529%252C%25202024%2529%252C%2520featuring%2520an%2520accelerated%2520and%2520more%2520robust%250Aalgorithm%2520as%2520well%2520as%2520a%2520broader%2520empirical%2520evaluation.%2520The%2520code%2520and%2520data%2520relative%250Ato%2520this%2520article%2520is%2520available%2520at%2520https%253A//github.com/RobinBruneau/RNb-NeuS2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues&entry.906535625=Robin%20Bruneau%20and%20Baptiste%20Brument%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet&entry.1292438233=%20%20Achieving%20high-fidelity%203D%20surface%20reconstruction%20while%20preserving%20fine%0Adetails%20remains%20challenging%2C%20especially%20in%20the%20presence%20of%20materials%20with%0Acomplex%20reflectance%20properties%20and%20without%20a%20dense-view%20setup.%20In%20this%20paper%2C%0Awe%20introduce%20a%20versatile%20framework%20that%20incorporates%20multi-view%20normal%20and%0Aoptionally%20reflectance%20maps%20into%20radiance-based%20surface%20reconstruction.%20Our%0Aapproach%20employs%20a%20pixel-wise%20joint%20re-parametrization%20of%20reflectance%20and%0Asurface%20normals%2C%20representing%20them%20as%20a%20vector%20of%20radiances%20under%20simulated%2C%0Avarying%20illumination.%20This%20formulation%20enables%20seamless%20incorporation%20into%0Astandard%20surface%20reconstruction%20pipelines%2C%20such%20as%20traditional%20multi-view%0Astereo%20%28MVS%29%20frameworks%20or%20modern%20neural%20volume%20rendering%20%28NVR%29%20ones.%20Combined%0Awith%20the%20latter%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%0Amulti-view%20photometric%20stereo%20%28MVPS%29%20benchmark%20datasets%2C%20including%20DiLiGenT-MV%2C%0ALUCES-MV%20and%20Skoltech3D.%20In%20particular%2C%20our%20method%20excels%20in%20reconstructing%0Afine-grained%20details%20and%20handling%20challenging%20visibility%20conditions.%20The%0Apresent%20paper%20is%20an%20extended%20version%20of%20the%20earlier%20conference%20paper%20by%20Brument%0Aet%20al.%20%28in%20Proceedings%20of%20the%20IEEE/CVF%20Conference%20on%20Computer%20Vision%20and%0APattern%20Recognition%20%28CVPR%29%2C%202024%29%2C%20featuring%20an%20accelerated%20and%20more%20robust%0Aalgorithm%20as%20well%20as%20a%20broader%20empirical%20evaluation.%20The%20code%20and%20data%20relative%0Ato%20this%20article%20is%20available%20at%20https%3A//github.com/RobinBruneau/RNb-NeuS2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04115v1&entry.124074799=Read"},
{"title": "KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action\n  Recognition", "author": "Zhaoyu Chen and Xing Li and Qian Huang and Qiang Geng and Tianjin Yang and Shihao Han", "abstract": "  Point cloud sequence-based 3D action recognition has achieved impressive\nperformance and efficiency. However, existing point cloud sequence modeling\nmethods cannot adequately balance the precision of limb micro-movements with\nthe integrity of posture macro-structure, leading to the loss of crucial\ninformation cues in action inference. To overcome this limitation, we introduce\nD-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding\nmodule. D-Hyperpoint encapsulates both regional-momentary motion and\nglobal-static posture, effectively summarizing the unit human action at each\nmoment. In addition, we present a D-Hyperpoint KANsMixer module, which is\nrecursively applied to nested groupings of D-Hyperpoints to learn the action\ndiscrimination information and creatively integrates Kolmogorov-Arnold Networks\n(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we\npropose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for\n3D action recognition. Extensive experiments on two public datasets: MSR\nAction3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our\nmethod.\n", "link": "http://arxiv.org/abs/2409.09444v2", "date": "2025-06-04", "relevancy": 2.8976, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6068}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5842}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAN-HyperpointNet%20for%20Point%20Cloud%20Sequence-Based%203D%20Human%20Action%0A%20%20Recognition&body=Title%3A%20KAN-HyperpointNet%20for%20Point%20Cloud%20Sequence-Based%203D%20Human%20Action%0A%20%20Recognition%0AAuthor%3A%20Zhaoyu%20Chen%20and%20Xing%20Li%20and%20Qian%20Huang%20and%20Qiang%20Geng%20and%20Tianjin%20Yang%20and%20Shihao%20Han%0AAbstract%3A%20%20%20Point%20cloud%20sequence-based%203D%20action%20recognition%20has%20achieved%20impressive%0Aperformance%20and%20efficiency.%20However%2C%20existing%20point%20cloud%20sequence%20modeling%0Amethods%20cannot%20adequately%20balance%20the%20precision%20of%20limb%20micro-movements%20with%0Athe%20integrity%20of%20posture%20macro-structure%2C%20leading%20to%20the%20loss%20of%20crucial%0Ainformation%20cues%20in%20action%20inference.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0AD-Hyperpoint%2C%20a%20novel%20data%20type%20generated%20through%20a%20D-Hyperpoint%20Embedding%0Amodule.%20D-Hyperpoint%20encapsulates%20both%20regional-momentary%20motion%20and%0Aglobal-static%20posture%2C%20effectively%20summarizing%20the%20unit%20human%20action%20at%20each%0Amoment.%20In%20addition%2C%20we%20present%20a%20D-Hyperpoint%20KANsMixer%20module%2C%20which%20is%0Arecursively%20applied%20to%20nested%20groupings%20of%20D-Hyperpoints%20to%20learn%20the%20action%0Adiscrimination%20information%20and%20creatively%20integrates%20Kolmogorov-Arnold%20Networks%0A%28KAN%29%20to%20enhance%20spatio-temporal%20interaction%20within%20D-Hyperpoints.%20Finally%2C%20we%0Apropose%20KAN-HyperpointNet%2C%20a%20spatio-temporal%20decoupled%20network%20architecture%20for%0A3D%20action%20recognition.%20Extensive%20experiments%20on%20two%20public%20datasets%3A%20MSR%0AAction3D%20and%20NTU-RGB%2BD%2060%2C%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAN-HyperpointNet%2520for%2520Point%2520Cloud%2520Sequence-Based%25203D%2520Human%2520Action%250A%2520%2520Recognition%26entry.906535625%3DZhaoyu%2520Chen%2520and%2520Xing%2520Li%2520and%2520Qian%2520Huang%2520and%2520Qiang%2520Geng%2520and%2520Tianjin%2520Yang%2520and%2520Shihao%2520Han%26entry.1292438233%3D%2520%2520Point%2520cloud%2520sequence-based%25203D%2520action%2520recognition%2520has%2520achieved%2520impressive%250Aperformance%2520and%2520efficiency.%2520However%252C%2520existing%2520point%2520cloud%2520sequence%2520modeling%250Amethods%2520cannot%2520adequately%2520balance%2520the%2520precision%2520of%2520limb%2520micro-movements%2520with%250Athe%2520integrity%2520of%2520posture%2520macro-structure%252C%2520leading%2520to%2520the%2520loss%2520of%2520crucial%250Ainformation%2520cues%2520in%2520action%2520inference.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%250AD-Hyperpoint%252C%2520a%2520novel%2520data%2520type%2520generated%2520through%2520a%2520D-Hyperpoint%2520Embedding%250Amodule.%2520D-Hyperpoint%2520encapsulates%2520both%2520regional-momentary%2520motion%2520and%250Aglobal-static%2520posture%252C%2520effectively%2520summarizing%2520the%2520unit%2520human%2520action%2520at%2520each%250Amoment.%2520In%2520addition%252C%2520we%2520present%2520a%2520D-Hyperpoint%2520KANsMixer%2520module%252C%2520which%2520is%250Arecursively%2520applied%2520to%2520nested%2520groupings%2520of%2520D-Hyperpoints%2520to%2520learn%2520the%2520action%250Adiscrimination%2520information%2520and%2520creatively%2520integrates%2520Kolmogorov-Arnold%2520Networks%250A%2528KAN%2529%2520to%2520enhance%2520spatio-temporal%2520interaction%2520within%2520D-Hyperpoints.%2520Finally%252C%2520we%250Apropose%2520KAN-HyperpointNet%252C%2520a%2520spatio-temporal%2520decoupled%2520network%2520architecture%2520for%250A3D%2520action%2520recognition.%2520Extensive%2520experiments%2520on%2520two%2520public%2520datasets%253A%2520MSR%250AAction3D%2520and%2520NTU-RGB%252BD%252060%252C%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN-HyperpointNet%20for%20Point%20Cloud%20Sequence-Based%203D%20Human%20Action%0A%20%20Recognition&entry.906535625=Zhaoyu%20Chen%20and%20Xing%20Li%20and%20Qian%20Huang%20and%20Qiang%20Geng%20and%20Tianjin%20Yang%20and%20Shihao%20Han&entry.1292438233=%20%20Point%20cloud%20sequence-based%203D%20action%20recognition%20has%20achieved%20impressive%0Aperformance%20and%20efficiency.%20However%2C%20existing%20point%20cloud%20sequence%20modeling%0Amethods%20cannot%20adequately%20balance%20the%20precision%20of%20limb%20micro-movements%20with%0Athe%20integrity%20of%20posture%20macro-structure%2C%20leading%20to%20the%20loss%20of%20crucial%0Ainformation%20cues%20in%20action%20inference.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0AD-Hyperpoint%2C%20a%20novel%20data%20type%20generated%20through%20a%20D-Hyperpoint%20Embedding%0Amodule.%20D-Hyperpoint%20encapsulates%20both%20regional-momentary%20motion%20and%0Aglobal-static%20posture%2C%20effectively%20summarizing%20the%20unit%20human%20action%20at%20each%0Amoment.%20In%20addition%2C%20we%20present%20a%20D-Hyperpoint%20KANsMixer%20module%2C%20which%20is%0Arecursively%20applied%20to%20nested%20groupings%20of%20D-Hyperpoints%20to%20learn%20the%20action%0Adiscrimination%20information%20and%20creatively%20integrates%20Kolmogorov-Arnold%20Networks%0A%28KAN%29%20to%20enhance%20spatio-temporal%20interaction%20within%20D-Hyperpoints.%20Finally%2C%20we%0Apropose%20KAN-HyperpointNet%2C%20a%20spatio-temporal%20decoupled%20network%20architecture%20for%0A3D%20action%20recognition.%20Extensive%20experiments%20on%20two%20public%20datasets%3A%20MSR%0AAction3D%20and%20NTU-RGB%2BD%2060%2C%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09444v2&entry.124074799=Read"},
{"title": "Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model\n  Co-development", "author": "Daoyuan Chen and Haibin Wang and Yilun Huang and Ce Ge and Yaliang Li and Bolin Ding and Jingren Zhou", "abstract": "  The emergence of multimodal large models has advanced artificial\nintelligence, introducing unprecedented levels of performance and\nfunctionality. However, optimizing these models remains challenging due to\nhistorically isolated paths of model-centric and data-centric developments,\nleading to suboptimal outcomes and inefficient resource utilization. In\nresponse, we present a new sandbox suite tailored for integrated data-model\nco-development. This sandbox provides a feedback-driven experimental platform,\nenabling cost-effective iteration and guided refinement of both data and\nmodels. Our proposed ``Probe-Analyze-Refine'' workflow, validated through\npractical use cases on multimodal tasks such as image-text pre-training with\nCLIP, image-to-text generation with LLaVA-like models, and text-to-video\ngeneration with DiT-based models, yields transferable and notable performance\nboosts, such as topping the VBench leaderboard. A comprehensive set of over 100\nexperiments demonstrated the suite's usability and extensibility, while also\nuncovering insights into the interplay between data quality, diversity, model\nbehavior, and computational costs. All codes, datasets, and models are\nopen-sourced to foster future research and applications that would otherwise be\ninfeasible due to the lack of a dedicated co-development infrastructure.\n", "link": "http://arxiv.org/abs/2407.11784v3", "date": "2025-06-04", "relevancy": 2.8352, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Juicer%20Sandbox%3A%20A%20Feedback-Driven%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development&body=Title%3A%20Data-Juicer%20Sandbox%3A%20A%20Feedback-Driven%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development%0AAuthor%3A%20Daoyuan%20Chen%20and%20Haibin%20Wang%20and%20Yilun%20Huang%20and%20Ce%20Ge%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20The%20emergence%20of%20multimodal%20large%20models%20has%20advanced%20artificial%0Aintelligence%2C%20introducing%20unprecedented%20levels%20of%20performance%20and%0Afunctionality.%20However%2C%20optimizing%20these%20models%20remains%20challenging%20due%20to%0Ahistorically%20isolated%20paths%20of%20model-centric%20and%20data-centric%20developments%2C%0Aleading%20to%20suboptimal%20outcomes%20and%20inefficient%20resource%20utilization.%20In%0Aresponse%2C%20we%20present%20a%20new%20sandbox%20suite%20tailored%20for%20integrated%20data-model%0Aco-development.%20This%20sandbox%20provides%20a%20feedback-driven%20experimental%20platform%2C%0Aenabling%20cost-effective%20iteration%20and%20guided%20refinement%20of%20both%20data%20and%0Amodels.%20Our%20proposed%20%60%60Probe-Analyze-Refine%27%27%20workflow%2C%20validated%20through%0Apractical%20use%20cases%20on%20multimodal%20tasks%20such%20as%20image-text%20pre-training%20with%0ACLIP%2C%20image-to-text%20generation%20with%20LLaVA-like%20models%2C%20and%20text-to-video%0Ageneration%20with%20DiT-based%20models%2C%20yields%20transferable%20and%20notable%20performance%0Aboosts%2C%20such%20as%20topping%20the%20VBench%20leaderboard.%20A%20comprehensive%20set%20of%20over%20100%0Aexperiments%20demonstrated%20the%20suite%27s%20usability%20and%20extensibility%2C%20while%20also%0Auncovering%20insights%20into%20the%20interplay%20between%20data%20quality%2C%20diversity%2C%20model%0Abehavior%2C%20and%20computational%20costs.%20All%20codes%2C%20datasets%2C%20and%20models%20are%0Aopen-sourced%20to%20foster%20future%20research%20and%20applications%20that%20would%20otherwise%20be%0Ainfeasible%20due%20to%20the%20lack%20of%20a%20dedicated%20co-development%20infrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11784v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Juicer%2520Sandbox%253A%2520A%2520Feedback-Driven%2520Suite%2520for%2520Multimodal%2520Data-Model%250A%2520%2520Co-development%26entry.906535625%3DDaoyuan%2520Chen%2520and%2520Haibin%2520Wang%2520and%2520Yilun%2520Huang%2520and%2520Ce%2520Ge%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520multimodal%2520large%2520models%2520has%2520advanced%2520artificial%250Aintelligence%252C%2520introducing%2520unprecedented%2520levels%2520of%2520performance%2520and%250Afunctionality.%2520However%252C%2520optimizing%2520these%2520models%2520remains%2520challenging%2520due%2520to%250Ahistorically%2520isolated%2520paths%2520of%2520model-centric%2520and%2520data-centric%2520developments%252C%250Aleading%2520to%2520suboptimal%2520outcomes%2520and%2520inefficient%2520resource%2520utilization.%2520In%250Aresponse%252C%2520we%2520present%2520a%2520new%2520sandbox%2520suite%2520tailored%2520for%2520integrated%2520data-model%250Aco-development.%2520This%2520sandbox%2520provides%2520a%2520feedback-driven%2520experimental%2520platform%252C%250Aenabling%2520cost-effective%2520iteration%2520and%2520guided%2520refinement%2520of%2520both%2520data%2520and%250Amodels.%2520Our%2520proposed%2520%2560%2560Probe-Analyze-Refine%2527%2527%2520workflow%252C%2520validated%2520through%250Apractical%2520use%2520cases%2520on%2520multimodal%2520tasks%2520such%2520as%2520image-text%2520pre-training%2520with%250ACLIP%252C%2520image-to-text%2520generation%2520with%2520LLaVA-like%2520models%252C%2520and%2520text-to-video%250Ageneration%2520with%2520DiT-based%2520models%252C%2520yields%2520transferable%2520and%2520notable%2520performance%250Aboosts%252C%2520such%2520as%2520topping%2520the%2520VBench%2520leaderboard.%2520A%2520comprehensive%2520set%2520of%2520over%2520100%250Aexperiments%2520demonstrated%2520the%2520suite%2527s%2520usability%2520and%2520extensibility%252C%2520while%2520also%250Auncovering%2520insights%2520into%2520the%2520interplay%2520between%2520data%2520quality%252C%2520diversity%252C%2520model%250Abehavior%252C%2520and%2520computational%2520costs.%2520All%2520codes%252C%2520datasets%252C%2520and%2520models%2520are%250Aopen-sourced%2520to%2520foster%2520future%2520research%2520and%2520applications%2520that%2520would%2520otherwise%2520be%250Ainfeasible%2520due%2520to%2520the%2520lack%2520of%2520a%2520dedicated%2520co-development%2520infrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11784v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Juicer%20Sandbox%3A%20A%20Feedback-Driven%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development&entry.906535625=Daoyuan%20Chen%20and%20Haibin%20Wang%20and%20Yilun%20Huang%20and%20Ce%20Ge%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20The%20emergence%20of%20multimodal%20large%20models%20has%20advanced%20artificial%0Aintelligence%2C%20introducing%20unprecedented%20levels%20of%20performance%20and%0Afunctionality.%20However%2C%20optimizing%20these%20models%20remains%20challenging%20due%20to%0Ahistorically%20isolated%20paths%20of%20model-centric%20and%20data-centric%20developments%2C%0Aleading%20to%20suboptimal%20outcomes%20and%20inefficient%20resource%20utilization.%20In%0Aresponse%2C%20we%20present%20a%20new%20sandbox%20suite%20tailored%20for%20integrated%20data-model%0Aco-development.%20This%20sandbox%20provides%20a%20feedback-driven%20experimental%20platform%2C%0Aenabling%20cost-effective%20iteration%20and%20guided%20refinement%20of%20both%20data%20and%0Amodels.%20Our%20proposed%20%60%60Probe-Analyze-Refine%27%27%20workflow%2C%20validated%20through%0Apractical%20use%20cases%20on%20multimodal%20tasks%20such%20as%20image-text%20pre-training%20with%0ACLIP%2C%20image-to-text%20generation%20with%20LLaVA-like%20models%2C%20and%20text-to-video%0Ageneration%20with%20DiT-based%20models%2C%20yields%20transferable%20and%20notable%20performance%0Aboosts%2C%20such%20as%20topping%20the%20VBench%20leaderboard.%20A%20comprehensive%20set%20of%20over%20100%0Aexperiments%20demonstrated%20the%20suite%27s%20usability%20and%20extensibility%2C%20while%20also%0Auncovering%20insights%20into%20the%20interplay%20between%20data%20quality%2C%20diversity%2C%20model%0Abehavior%2C%20and%20computational%20costs.%20All%20codes%2C%20datasets%2C%20and%20models%20are%0Aopen-sourced%20to%20foster%20future%20research%20and%20applications%20that%20would%20otherwise%20be%0Ainfeasible%20due%20to%20the%20lack%20of%20a%20dedicated%20co-development%20infrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11784v3&entry.124074799=Read"},
{"title": "Language-Image Alignment with Fixed Text Encoders", "author": "Jingfeng Yang and Ziyang Wu and Yue Zhao and Yi Ma", "abstract": "  Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.\n", "link": "http://arxiv.org/abs/2506.04209v1", "date": "2025-06-04", "relevancy": 2.8326, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Image%20Alignment%20with%20Fixed%20Text%20Encoders&body=Title%3A%20Language-Image%20Alignment%20with%20Fixed%20Text%20Encoders%0AAuthor%3A%20Jingfeng%20Yang%20and%20Ziyang%20Wu%20and%20Yue%20Zhao%20and%20Yi%20Ma%0AAbstract%3A%20%20%20Currently%2C%20the%20most%20dominant%20approach%20to%20establishing%20language-image%0Aalignment%20is%20to%20pre-train%20text%20and%20image%20encoders%20jointly%20through%20contrastive%0Alearning%2C%20such%20as%20CLIP%20and%20its%20variants.%20In%20this%20work%2C%20we%20question%20whether%20such%0Aa%20costly%20joint%20training%20is%20necessary.%20In%20particular%2C%20we%20investigate%20if%20a%0Apre-trained%20fixed%20large%20language%20model%20%28LLM%29%20offers%20a%20good%20enough%20text%20encoder%0Ato%20guide%20visual%20representation%20learning.%20That%20is%2C%20we%20propose%20to%20learn%0ALanguage-Image%20alignment%20with%20a%20Fixed%20Text%20encoder%20%28LIFT%29%20from%20an%20LLM%20by%0Atraining%20only%20the%20image%20encoder.%20Somewhat%20surprisingly%2C%20through%20comprehensive%0Abenchmarking%20and%20ablation%20studies%2C%20we%20find%20that%20this%20much%20simplified%20framework%0ALIFT%20is%20highly%20effective%20and%20it%20outperforms%20CLIP%20in%20most%20scenarios%20that%20involve%0Acompositional%20understanding%20and%20long%20captions%2C%20while%20achieving%20considerable%0Agains%20in%20computational%20efficiency.%20Our%20work%20takes%20a%20first%20step%20towards%0Asystematically%20exploring%20how%20text%20embeddings%20from%20LLMs%20can%20guide%20visual%0Alearning%20and%20suggests%20an%20alternative%20design%20choice%20for%20learning%0Alanguage-aligned%20visual%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Image%2520Alignment%2520with%2520Fixed%2520Text%2520Encoders%26entry.906535625%3DJingfeng%2520Yang%2520and%2520Ziyang%2520Wu%2520and%2520Yue%2520Zhao%2520and%2520Yi%2520Ma%26entry.1292438233%3D%2520%2520Currently%252C%2520the%2520most%2520dominant%2520approach%2520to%2520establishing%2520language-image%250Aalignment%2520is%2520to%2520pre-train%2520text%2520and%2520image%2520encoders%2520jointly%2520through%2520contrastive%250Alearning%252C%2520such%2520as%2520CLIP%2520and%2520its%2520variants.%2520In%2520this%2520work%252C%2520we%2520question%2520whether%2520such%250Aa%2520costly%2520joint%2520training%2520is%2520necessary.%2520In%2520particular%252C%2520we%2520investigate%2520if%2520a%250Apre-trained%2520fixed%2520large%2520language%2520model%2520%2528LLM%2529%2520offers%2520a%2520good%2520enough%2520text%2520encoder%250Ato%2520guide%2520visual%2520representation%2520learning.%2520That%2520is%252C%2520we%2520propose%2520to%2520learn%250ALanguage-Image%2520alignment%2520with%2520a%2520Fixed%2520Text%2520encoder%2520%2528LIFT%2529%2520from%2520an%2520LLM%2520by%250Atraining%2520only%2520the%2520image%2520encoder.%2520Somewhat%2520surprisingly%252C%2520through%2520comprehensive%250Abenchmarking%2520and%2520ablation%2520studies%252C%2520we%2520find%2520that%2520this%2520much%2520simplified%2520framework%250ALIFT%2520is%2520highly%2520effective%2520and%2520it%2520outperforms%2520CLIP%2520in%2520most%2520scenarios%2520that%2520involve%250Acompositional%2520understanding%2520and%2520long%2520captions%252C%2520while%2520achieving%2520considerable%250Agains%2520in%2520computational%2520efficiency.%2520Our%2520work%2520takes%2520a%2520first%2520step%2520towards%250Asystematically%2520exploring%2520how%2520text%2520embeddings%2520from%2520LLMs%2520can%2520guide%2520visual%250Alearning%2520and%2520suggests%2520an%2520alternative%2520design%2520choice%2520for%2520learning%250Alanguage-aligned%2520visual%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Image%20Alignment%20with%20Fixed%20Text%20Encoders&entry.906535625=Jingfeng%20Yang%20and%20Ziyang%20Wu%20and%20Yue%20Zhao%20and%20Yi%20Ma&entry.1292438233=%20%20Currently%2C%20the%20most%20dominant%20approach%20to%20establishing%20language-image%0Aalignment%20is%20to%20pre-train%20text%20and%20image%20encoders%20jointly%20through%20contrastive%0Alearning%2C%20such%20as%20CLIP%20and%20its%20variants.%20In%20this%20work%2C%20we%20question%20whether%20such%0Aa%20costly%20joint%20training%20is%20necessary.%20In%20particular%2C%20we%20investigate%20if%20a%0Apre-trained%20fixed%20large%20language%20model%20%28LLM%29%20offers%20a%20good%20enough%20text%20encoder%0Ato%20guide%20visual%20representation%20learning.%20That%20is%2C%20we%20propose%20to%20learn%0ALanguage-Image%20alignment%20with%20a%20Fixed%20Text%20encoder%20%28LIFT%29%20from%20an%20LLM%20by%0Atraining%20only%20the%20image%20encoder.%20Somewhat%20surprisingly%2C%20through%20comprehensive%0Abenchmarking%20and%20ablation%20studies%2C%20we%20find%20that%20this%20much%20simplified%20framework%0ALIFT%20is%20highly%20effective%20and%20it%20outperforms%20CLIP%20in%20most%20scenarios%20that%20involve%0Acompositional%20understanding%20and%20long%20captions%2C%20while%20achieving%20considerable%0Agains%20in%20computational%20efficiency.%20Our%20work%20takes%20a%20first%20step%20towards%0Asystematically%20exploring%20how%20text%20embeddings%20from%20LLMs%20can%20guide%20visual%0Alearning%20and%20suggests%20an%20alternative%20design%20choice%20for%20learning%0Alanguage-aligned%20visual%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04209v1&entry.124074799=Read"},
{"title": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with\n  Vision Feature Resample", "author": "Ze Feng and Jiang-Jiang Liu and Sen Yang and Lingyu Xiao and Xiaofan Li and Wankou Yang and Jingdong Wang", "abstract": "  In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.\n", "link": "http://arxiv.org/abs/2506.03928v1", "date": "2025-06-04", "relevancy": 2.8074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Remember%3A%20Alleviating%20Visual%20Forgetting%20in%20Efficient%20MLLM%20with%0A%20%20Vision%20Feature%20Resample&body=Title%3A%20Vision%20Remember%3A%20Alleviating%20Visual%20Forgetting%20in%20Efficient%20MLLM%20with%0A%20%20Vision%20Feature%20Resample%0AAuthor%3A%20Ze%20Feng%20and%20Jiang-Jiang%20Liu%20and%20Sen%20Yang%20and%20Lingyu%20Xiao%20and%20Xiaofan%20Li%20and%20Wankou%20Yang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20Efficient%20Multimodal%20Large%20Language%20Model.%0ARedundant%20vision%20tokens%20consume%20a%20significant%20amount%20of%20computational%20memory%0Aand%20resources.%20Therefore%2C%20many%20previous%20works%20compress%20them%20in%20the%20Vision%0AProjector%20to%20reduce%20the%20number%20of%20vision%20tokens.%20However%2C%20simply%20compressing%20in%0Athe%20Vision%20Projector%20can%20lead%20to%20the%20loss%20of%20visual%20information%2C%20especially%20for%0Atasks%20that%20rely%20on%20fine-grained%20spatial%20relationships%2C%20such%20as%20OCR%20and%20Chart%20%5C%26%0ATable%20Understanding.%20To%20address%20this%20problem%2C%20we%20propose%20Vision%20Remember%2C%20which%0Ais%20inserted%20between%20the%20LLM%20decoder%20layers%20to%20allow%20vision%20tokens%20to%0Are-memorize%20vision%20features.%20Specifically%2C%20we%20retain%20multi-level%20vision%0Afeatures%20and%20resample%20them%20with%20the%20vision%20tokens%20that%20have%20interacted%20with%20the%0Atext%20token.%20During%20the%20resampling%20process%2C%20each%20vision%20token%20only%20attends%20to%20a%0Alocal%20region%20in%20vision%20features%2C%20which%20is%20referred%20to%20as%20saliency-enhancing%0Alocal%20attention.%20Saliency-enhancing%20local%20attention%20not%20only%20improves%0Acomputational%20efficiency%20but%20also%20captures%20more%20fine-grained%20contextual%0Ainformation%20and%20spatial%20relationships%20within%20the%20region.%20Comprehensive%0Aexperiments%20on%20multiple%20visual%20understanding%20benchmarks%20validate%20the%0Aeffectiveness%20of%20our%20method%20when%20combined%20with%20various%20Efficient%20Vision%0AProjectors%2C%20showing%20performance%20gains%20without%20sacrificing%20efficiency.%20Based%20on%0AVision%20Remember%2C%20LLaVA-VR%20with%20only%202B%20parameters%20is%20also%20superior%20to%20previous%0Arepresentative%20MLLMs%20such%20as%20Tokenpacker-HD-7B%20and%20DeepSeek-VL-7B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Remember%253A%2520Alleviating%2520Visual%2520Forgetting%2520in%2520Efficient%2520MLLM%2520with%250A%2520%2520Vision%2520Feature%2520Resample%26entry.906535625%3DZe%2520Feng%2520and%2520Jiang-Jiang%2520Liu%2520and%2520Sen%2520Yang%2520and%2520Lingyu%2520Xiao%2520and%2520Xiaofan%2520Li%2520and%2520Wankou%2520Yang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520Efficient%2520Multimodal%2520Large%2520Language%2520Model.%250ARedundant%2520vision%2520tokens%2520consume%2520a%2520significant%2520amount%2520of%2520computational%2520memory%250Aand%2520resources.%2520Therefore%252C%2520many%2520previous%2520works%2520compress%2520them%2520in%2520the%2520Vision%250AProjector%2520to%2520reduce%2520the%2520number%2520of%2520vision%2520tokens.%2520However%252C%2520simply%2520compressing%2520in%250Athe%2520Vision%2520Projector%2520can%2520lead%2520to%2520the%2520loss%2520of%2520visual%2520information%252C%2520especially%2520for%250Atasks%2520that%2520rely%2520on%2520fine-grained%2520spatial%2520relationships%252C%2520such%2520as%2520OCR%2520and%2520Chart%2520%255C%2526%250ATable%2520Understanding.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Vision%2520Remember%252C%2520which%250Ais%2520inserted%2520between%2520the%2520LLM%2520decoder%2520layers%2520to%2520allow%2520vision%2520tokens%2520to%250Are-memorize%2520vision%2520features.%2520Specifically%252C%2520we%2520retain%2520multi-level%2520vision%250Afeatures%2520and%2520resample%2520them%2520with%2520the%2520vision%2520tokens%2520that%2520have%2520interacted%2520with%2520the%250Atext%2520token.%2520During%2520the%2520resampling%2520process%252C%2520each%2520vision%2520token%2520only%2520attends%2520to%2520a%250Alocal%2520region%2520in%2520vision%2520features%252C%2520which%2520is%2520referred%2520to%2520as%2520saliency-enhancing%250Alocal%2520attention.%2520Saliency-enhancing%2520local%2520attention%2520not%2520only%2520improves%250Acomputational%2520efficiency%2520but%2520also%2520captures%2520more%2520fine-grained%2520contextual%250Ainformation%2520and%2520spatial%2520relationships%2520within%2520the%2520region.%2520Comprehensive%250Aexperiments%2520on%2520multiple%2520visual%2520understanding%2520benchmarks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520when%2520combined%2520with%2520various%2520Efficient%2520Vision%250AProjectors%252C%2520showing%2520performance%2520gains%2520without%2520sacrificing%2520efficiency.%2520Based%2520on%250AVision%2520Remember%252C%2520LLaVA-VR%2520with%2520only%25202B%2520parameters%2520is%2520also%2520superior%2520to%2520previous%250Arepresentative%2520MLLMs%2520such%2520as%2520Tokenpacker-HD-7B%2520and%2520DeepSeek-VL-7B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Remember%3A%20Alleviating%20Visual%20Forgetting%20in%20Efficient%20MLLM%20with%0A%20%20Vision%20Feature%20Resample&entry.906535625=Ze%20Feng%20and%20Jiang-Jiang%20Liu%20and%20Sen%20Yang%20and%20Lingyu%20Xiao%20and%20Xiaofan%20Li%20and%20Wankou%20Yang%20and%20Jingdong%20Wang&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20Efficient%20Multimodal%20Large%20Language%20Model.%0ARedundant%20vision%20tokens%20consume%20a%20significant%20amount%20of%20computational%20memory%0Aand%20resources.%20Therefore%2C%20many%20previous%20works%20compress%20them%20in%20the%20Vision%0AProjector%20to%20reduce%20the%20number%20of%20vision%20tokens.%20However%2C%20simply%20compressing%20in%0Athe%20Vision%20Projector%20can%20lead%20to%20the%20loss%20of%20visual%20information%2C%20especially%20for%0Atasks%20that%20rely%20on%20fine-grained%20spatial%20relationships%2C%20such%20as%20OCR%20and%20Chart%20%5C%26%0ATable%20Understanding.%20To%20address%20this%20problem%2C%20we%20propose%20Vision%20Remember%2C%20which%0Ais%20inserted%20between%20the%20LLM%20decoder%20layers%20to%20allow%20vision%20tokens%20to%0Are-memorize%20vision%20features.%20Specifically%2C%20we%20retain%20multi-level%20vision%0Afeatures%20and%20resample%20them%20with%20the%20vision%20tokens%20that%20have%20interacted%20with%20the%0Atext%20token.%20During%20the%20resampling%20process%2C%20each%20vision%20token%20only%20attends%20to%20a%0Alocal%20region%20in%20vision%20features%2C%20which%20is%20referred%20to%20as%20saliency-enhancing%0Alocal%20attention.%20Saliency-enhancing%20local%20attention%20not%20only%20improves%0Acomputational%20efficiency%20but%20also%20captures%20more%20fine-grained%20contextual%0Ainformation%20and%20spatial%20relationships%20within%20the%20region.%20Comprehensive%0Aexperiments%20on%20multiple%20visual%20understanding%20benchmarks%20validate%20the%0Aeffectiveness%20of%20our%20method%20when%20combined%20with%20various%20Efficient%20Vision%0AProjectors%2C%20showing%20performance%20gains%20without%20sacrificing%20efficiency.%20Based%20on%0AVision%20Remember%2C%20LLaVA-VR%20with%20only%202B%20parameters%20is%20also%20superior%20to%20previous%0Arepresentative%20MLLMs%20such%20as%20Tokenpacker-HD-7B%20and%20DeepSeek-VL-7B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03928v1&entry.124074799=Read"},
{"title": "Animal Pose Labeling Using General-Purpose Point Trackers", "author": "Zhuoyang Pan and Boxiao Pan and Guandao Yang and Adam W. Harley and Leonidas Guibas", "abstract": "  Automatically estimating animal poses from videos is important for studying\nanimal behaviors. Existing methods do not perform reliably since they are\ntrained on datasets that are not comprehensive enough to capture all necessary\nanimal behaviors. However, it is very challenging to collect such datasets due\nto the large variations in animal morphology. In this paper, we propose an\nanimal pose labeling pipeline that follows a different strategy, i.e. test time\noptimization. Given a video, we fine-tune a lightweight appearance embedding\ninside a pre-trained general-purpose point tracker on a sparse set of annotated\nframes. These annotations can be obtained from human labelers or off-the-shelf\npose detectors. The fine-tuned model is then applied to the rest of the frames\nfor automatic labeling. Our method achieves state-of-the-art performance at a\nreasonable annotation cost. We believe our pipeline offers a valuable tool for\nthe automatic quantification of animal behavior. Visit our project webpage at\nhttps://zhuoyang-pan.github.io/animal-labeling.\n", "link": "http://arxiv.org/abs/2506.03868v1", "date": "2025-06-04", "relevancy": 2.8025, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5791}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5538}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Animal%20Pose%20Labeling%20Using%20General-Purpose%20Point%20Trackers&body=Title%3A%20Animal%20Pose%20Labeling%20Using%20General-Purpose%20Point%20Trackers%0AAuthor%3A%20Zhuoyang%20Pan%20and%20Boxiao%20Pan%20and%20Guandao%20Yang%20and%20Adam%20W.%20Harley%20and%20Leonidas%20Guibas%0AAbstract%3A%20%20%20Automatically%20estimating%20animal%20poses%20from%20videos%20is%20important%20for%20studying%0Aanimal%20behaviors.%20Existing%20methods%20do%20not%20perform%20reliably%20since%20they%20are%0Atrained%20on%20datasets%20that%20are%20not%20comprehensive%20enough%20to%20capture%20all%20necessary%0Aanimal%20behaviors.%20However%2C%20it%20is%20very%20challenging%20to%20collect%20such%20datasets%20due%0Ato%20the%20large%20variations%20in%20animal%20morphology.%20In%20this%20paper%2C%20we%20propose%20an%0Aanimal%20pose%20labeling%20pipeline%20that%20follows%20a%20different%20strategy%2C%20i.e.%20test%20time%0Aoptimization.%20Given%20a%20video%2C%20we%20fine-tune%20a%20lightweight%20appearance%20embedding%0Ainside%20a%20pre-trained%20general-purpose%20point%20tracker%20on%20a%20sparse%20set%20of%20annotated%0Aframes.%20These%20annotations%20can%20be%20obtained%20from%20human%20labelers%20or%20off-the-shelf%0Apose%20detectors.%20The%20fine-tuned%20model%20is%20then%20applied%20to%20the%20rest%20of%20the%20frames%0Afor%20automatic%20labeling.%20Our%20method%20achieves%20state-of-the-art%20performance%20at%20a%0Areasonable%20annotation%20cost.%20We%20believe%20our%20pipeline%20offers%20a%20valuable%20tool%20for%0Athe%20automatic%20quantification%20of%20animal%20behavior.%20Visit%20our%20project%20webpage%20at%0Ahttps%3A//zhuoyang-pan.github.io/animal-labeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnimal%2520Pose%2520Labeling%2520Using%2520General-Purpose%2520Point%2520Trackers%26entry.906535625%3DZhuoyang%2520Pan%2520and%2520Boxiao%2520Pan%2520and%2520Guandao%2520Yang%2520and%2520Adam%2520W.%2520Harley%2520and%2520Leonidas%2520Guibas%26entry.1292438233%3D%2520%2520Automatically%2520estimating%2520animal%2520poses%2520from%2520videos%2520is%2520important%2520for%2520studying%250Aanimal%2520behaviors.%2520Existing%2520methods%2520do%2520not%2520perform%2520reliably%2520since%2520they%2520are%250Atrained%2520on%2520datasets%2520that%2520are%2520not%2520comprehensive%2520enough%2520to%2520capture%2520all%2520necessary%250Aanimal%2520behaviors.%2520However%252C%2520it%2520is%2520very%2520challenging%2520to%2520collect%2520such%2520datasets%2520due%250Ato%2520the%2520large%2520variations%2520in%2520animal%2520morphology.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aanimal%2520pose%2520labeling%2520pipeline%2520that%2520follows%2520a%2520different%2520strategy%252C%2520i.e.%2520test%2520time%250Aoptimization.%2520Given%2520a%2520video%252C%2520we%2520fine-tune%2520a%2520lightweight%2520appearance%2520embedding%250Ainside%2520a%2520pre-trained%2520general-purpose%2520point%2520tracker%2520on%2520a%2520sparse%2520set%2520of%2520annotated%250Aframes.%2520These%2520annotations%2520can%2520be%2520obtained%2520from%2520human%2520labelers%2520or%2520off-the-shelf%250Apose%2520detectors.%2520The%2520fine-tuned%2520model%2520is%2520then%2520applied%2520to%2520the%2520rest%2520of%2520the%2520frames%250Afor%2520automatic%2520labeling.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520at%2520a%250Areasonable%2520annotation%2520cost.%2520We%2520believe%2520our%2520pipeline%2520offers%2520a%2520valuable%2520tool%2520for%250Athe%2520automatic%2520quantification%2520of%2520animal%2520behavior.%2520Visit%2520our%2520project%2520webpage%2520at%250Ahttps%253A//zhuoyang-pan.github.io/animal-labeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Animal%20Pose%20Labeling%20Using%20General-Purpose%20Point%20Trackers&entry.906535625=Zhuoyang%20Pan%20and%20Boxiao%20Pan%20and%20Guandao%20Yang%20and%20Adam%20W.%20Harley%20and%20Leonidas%20Guibas&entry.1292438233=%20%20Automatically%20estimating%20animal%20poses%20from%20videos%20is%20important%20for%20studying%0Aanimal%20behaviors.%20Existing%20methods%20do%20not%20perform%20reliably%20since%20they%20are%0Atrained%20on%20datasets%20that%20are%20not%20comprehensive%20enough%20to%20capture%20all%20necessary%0Aanimal%20behaviors.%20However%2C%20it%20is%20very%20challenging%20to%20collect%20such%20datasets%20due%0Ato%20the%20large%20variations%20in%20animal%20morphology.%20In%20this%20paper%2C%20we%20propose%20an%0Aanimal%20pose%20labeling%20pipeline%20that%20follows%20a%20different%20strategy%2C%20i.e.%20test%20time%0Aoptimization.%20Given%20a%20video%2C%20we%20fine-tune%20a%20lightweight%20appearance%20embedding%0Ainside%20a%20pre-trained%20general-purpose%20point%20tracker%20on%20a%20sparse%20set%20of%20annotated%0Aframes.%20These%20annotations%20can%20be%20obtained%20from%20human%20labelers%20or%20off-the-shelf%0Apose%20detectors.%20The%20fine-tuned%20model%20is%20then%20applied%20to%20the%20rest%20of%20the%20frames%0Afor%20automatic%20labeling.%20Our%20method%20achieves%20state-of-the-art%20performance%20at%20a%0Areasonable%20annotation%20cost.%20We%20believe%20our%20pipeline%20offers%20a%20valuable%20tool%20for%0Athe%20automatic%20quantification%20of%20animal%20behavior.%20Visit%20our%20project%20webpage%20at%0Ahttps%3A//zhuoyang-pan.github.io/animal-labeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03868v1&entry.124074799=Read"},
{"title": "Analytical Reconstruction of Periodically Deformed Objects in\n  Time-resolved CT", "author": "Qianwei Qu and Christian M. Schlep\u00fctz and Marco Stampanoni", "abstract": "  Time-resolved CT is an advanced measurement technique that has been widely\nused to observe dynamic objects, including periodically varying structures such\nas hearts, lungs, or hearing structures. To reconstruct these objects from CT\nprojections, a common approach is to divide the projections into several\ncollections based on their motion phases and perform reconstruction within each\ncollection, assuming they originate from a static object. This describes the\ngating-based method, which is the standard approach for time-periodic\nreconstruction. However, the gating-based reconstruction algorithm only\nutilizes a limited subset of projections within each collection and ignores the\ncorrelation between different collections, leading to inefficient use of the\nradiation dose. To address this issue, we propose two analytical reconstruction\npipelines in this paper, and validate them with experimental data captured\nusing tomographic synchrotron microscopy. We demonstrate that our approaches\nsignificantly reduce random noise in the reconstructed images without blurring\nthe sharp features of the observed objects. Equivalently, our methods can\nachieve the same reconstruction quality as gating-based methods but with a\nlower radiation dose. Our code is available at github.com/PeriodRecon.\n", "link": "http://arxiv.org/abs/2506.03792v1", "date": "2025-06-04", "relevancy": 2.7629, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5602}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5602}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analytical%20Reconstruction%20of%20Periodically%20Deformed%20Objects%20in%0A%20%20Time-resolved%20CT&body=Title%3A%20Analytical%20Reconstruction%20of%20Periodically%20Deformed%20Objects%20in%0A%20%20Time-resolved%20CT%0AAuthor%3A%20Qianwei%20Qu%20and%20Christian%20M.%20Schlep%C3%BCtz%20and%20Marco%20Stampanoni%0AAbstract%3A%20%20%20Time-resolved%20CT%20is%20an%20advanced%20measurement%20technique%20that%20has%20been%20widely%0Aused%20to%20observe%20dynamic%20objects%2C%20including%20periodically%20varying%20structures%20such%0Aas%20hearts%2C%20lungs%2C%20or%20hearing%20structures.%20To%20reconstruct%20these%20objects%20from%20CT%0Aprojections%2C%20a%20common%20approach%20is%20to%20divide%20the%20projections%20into%20several%0Acollections%20based%20on%20their%20motion%20phases%20and%20perform%20reconstruction%20within%20each%0Acollection%2C%20assuming%20they%20originate%20from%20a%20static%20object.%20This%20describes%20the%0Agating-based%20method%2C%20which%20is%20the%20standard%20approach%20for%20time-periodic%0Areconstruction.%20However%2C%20the%20gating-based%20reconstruction%20algorithm%20only%0Autilizes%20a%20limited%20subset%20of%20projections%20within%20each%20collection%20and%20ignores%20the%0Acorrelation%20between%20different%20collections%2C%20leading%20to%20inefficient%20use%20of%20the%0Aradiation%20dose.%20To%20address%20this%20issue%2C%20we%20propose%20two%20analytical%20reconstruction%0Apipelines%20in%20this%20paper%2C%20and%20validate%20them%20with%20experimental%20data%20captured%0Ausing%20tomographic%20synchrotron%20microscopy.%20We%20demonstrate%20that%20our%20approaches%0Asignificantly%20reduce%20random%20noise%20in%20the%20reconstructed%20images%20without%20blurring%0Athe%20sharp%20features%20of%20the%20observed%20objects.%20Equivalently%2C%20our%20methods%20can%0Aachieve%20the%20same%20reconstruction%20quality%20as%20gating-based%20methods%20but%20with%20a%0Alower%20radiation%20dose.%20Our%20code%20is%20available%20at%20github.com/PeriodRecon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalytical%2520Reconstruction%2520of%2520Periodically%2520Deformed%2520Objects%2520in%250A%2520%2520Time-resolved%2520CT%26entry.906535625%3DQianwei%2520Qu%2520and%2520Christian%2520M.%2520Schlep%25C3%25BCtz%2520and%2520Marco%2520Stampanoni%26entry.1292438233%3D%2520%2520Time-resolved%2520CT%2520is%2520an%2520advanced%2520measurement%2520technique%2520that%2520has%2520been%2520widely%250Aused%2520to%2520observe%2520dynamic%2520objects%252C%2520including%2520periodically%2520varying%2520structures%2520such%250Aas%2520hearts%252C%2520lungs%252C%2520or%2520hearing%2520structures.%2520To%2520reconstruct%2520these%2520objects%2520from%2520CT%250Aprojections%252C%2520a%2520common%2520approach%2520is%2520to%2520divide%2520the%2520projections%2520into%2520several%250Acollections%2520based%2520on%2520their%2520motion%2520phases%2520and%2520perform%2520reconstruction%2520within%2520each%250Acollection%252C%2520assuming%2520they%2520originate%2520from%2520a%2520static%2520object.%2520This%2520describes%2520the%250Agating-based%2520method%252C%2520which%2520is%2520the%2520standard%2520approach%2520for%2520time-periodic%250Areconstruction.%2520However%252C%2520the%2520gating-based%2520reconstruction%2520algorithm%2520only%250Autilizes%2520a%2520limited%2520subset%2520of%2520projections%2520within%2520each%2520collection%2520and%2520ignores%2520the%250Acorrelation%2520between%2520different%2520collections%252C%2520leading%2520to%2520inefficient%2520use%2520of%2520the%250Aradiation%2520dose.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520two%2520analytical%2520reconstruction%250Apipelines%2520in%2520this%2520paper%252C%2520and%2520validate%2520them%2520with%2520experimental%2520data%2520captured%250Ausing%2520tomographic%2520synchrotron%2520microscopy.%2520We%2520demonstrate%2520that%2520our%2520approaches%250Asignificantly%2520reduce%2520random%2520noise%2520in%2520the%2520reconstructed%2520images%2520without%2520blurring%250Athe%2520sharp%2520features%2520of%2520the%2520observed%2520objects.%2520Equivalently%252C%2520our%2520methods%2520can%250Aachieve%2520the%2520same%2520reconstruction%2520quality%2520as%2520gating-based%2520methods%2520but%2520with%2520a%250Alower%2520radiation%2520dose.%2520Our%2520code%2520is%2520available%2520at%2520github.com/PeriodRecon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analytical%20Reconstruction%20of%20Periodically%20Deformed%20Objects%20in%0A%20%20Time-resolved%20CT&entry.906535625=Qianwei%20Qu%20and%20Christian%20M.%20Schlep%C3%BCtz%20and%20Marco%20Stampanoni&entry.1292438233=%20%20Time-resolved%20CT%20is%20an%20advanced%20measurement%20technique%20that%20has%20been%20widely%0Aused%20to%20observe%20dynamic%20objects%2C%20including%20periodically%20varying%20structures%20such%0Aas%20hearts%2C%20lungs%2C%20or%20hearing%20structures.%20To%20reconstruct%20these%20objects%20from%20CT%0Aprojections%2C%20a%20common%20approach%20is%20to%20divide%20the%20projections%20into%20several%0Acollections%20based%20on%20their%20motion%20phases%20and%20perform%20reconstruction%20within%20each%0Acollection%2C%20assuming%20they%20originate%20from%20a%20static%20object.%20This%20describes%20the%0Agating-based%20method%2C%20which%20is%20the%20standard%20approach%20for%20time-periodic%0Areconstruction.%20However%2C%20the%20gating-based%20reconstruction%20algorithm%20only%0Autilizes%20a%20limited%20subset%20of%20projections%20within%20each%20collection%20and%20ignores%20the%0Acorrelation%20between%20different%20collections%2C%20leading%20to%20inefficient%20use%20of%20the%0Aradiation%20dose.%20To%20address%20this%20issue%2C%20we%20propose%20two%20analytical%20reconstruction%0Apipelines%20in%20this%20paper%2C%20and%20validate%20them%20with%20experimental%20data%20captured%0Ausing%20tomographic%20synchrotron%20microscopy.%20We%20demonstrate%20that%20our%20approaches%0Asignificantly%20reduce%20random%20noise%20in%20the%20reconstructed%20images%20without%20blurring%0Athe%20sharp%20features%20of%20the%20observed%20objects.%20Equivalently%2C%20our%20methods%20can%0Aachieve%20the%20same%20reconstruction%20quality%20as%20gating-based%20methods%20but%20with%20a%0Alower%20radiation%20dose.%20Our%20code%20is%20available%20at%20github.com/PeriodRecon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03792v1&entry.124074799=Read"},
{"title": "Vocabulary-free few-shot learning for Vision-Language Models", "author": "Maxime Zanella and Cl\u00e9ment Fuchs and Ismail Ben Ayed and Christophe De Vleeschouwer", "abstract": "  Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have\ngreatly expanded their ability to generalize across tasks using only a few\nlabeled examples. However, existing approaches primarily build upon the strong\nzero-shot priors of these models by leveraging carefully designed,\ntask-specific prompts. This dependence on predefined class names can restrict\ntheir applicability, especially in scenarios where exact class names are\nunavailable or difficult to specify. To address this limitation, we introduce\nvocabulary-free few-shot learning for VLMs, a setting where target class\ninstances - that is, images - are available but their corresponding names are\nnot. We propose Similarity Mapping (SiM), a simple yet effective baseline that\nclassifies target instances solely based on similarity scores with a set of\ngeneric prompts (textual or visual), eliminating the need for carefully\nhandcrafted prompts. Although conceptually straightforward, SiM demonstrates\nstrong performance, operates with high computational efficiency (learning the\nmapping typically takes less than one second), and provides interpretability by\nlinking target classes to generic prompts. We believe that our approach could\nserve as an important baseline for future research in vocabulary-free few-shot\nlearning. Code is available at\nhttps://github.com/MaxZanella/vocabulary-free-FSL.\n", "link": "http://arxiv.org/abs/2506.04005v1", "date": "2025-06-04", "relevancy": 2.752, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vocabulary-free%20few-shot%20learning%20for%20Vision-Language%20Models&body=Title%3A%20Vocabulary-free%20few-shot%20learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Maxime%20Zanella%20and%20Cl%C3%A9ment%20Fuchs%20and%20Ismail%20Ben%20Ayed%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20Recent%20advances%20in%20few-shot%20adaptation%20for%20Vision-Language%20Models%20%28VLMs%29%20have%0Agreatly%20expanded%20their%20ability%20to%20generalize%20across%20tasks%20using%20only%20a%20few%0Alabeled%20examples.%20However%2C%20existing%20approaches%20primarily%20build%20upon%20the%20strong%0Azero-shot%20priors%20of%20these%20models%20by%20leveraging%20carefully%20designed%2C%0Atask-specific%20prompts.%20This%20dependence%20on%20predefined%20class%20names%20can%20restrict%0Atheir%20applicability%2C%20especially%20in%20scenarios%20where%20exact%20class%20names%20are%0Aunavailable%20or%20difficult%20to%20specify.%20To%20address%20this%20limitation%2C%20we%20introduce%0Avocabulary-free%20few-shot%20learning%20for%20VLMs%2C%20a%20setting%20where%20target%20class%0Ainstances%20-%20that%20is%2C%20images%20-%20are%20available%20but%20their%20corresponding%20names%20are%0Anot.%20We%20propose%20Similarity%20Mapping%20%28SiM%29%2C%20a%20simple%20yet%20effective%20baseline%20that%0Aclassifies%20target%20instances%20solely%20based%20on%20similarity%20scores%20with%20a%20set%20of%0Ageneric%20prompts%20%28textual%20or%20visual%29%2C%20eliminating%20the%20need%20for%20carefully%0Ahandcrafted%20prompts.%20Although%20conceptually%20straightforward%2C%20SiM%20demonstrates%0Astrong%20performance%2C%20operates%20with%20high%20computational%20efficiency%20%28learning%20the%0Amapping%20typically%20takes%20less%20than%20one%20second%29%2C%20and%20provides%20interpretability%20by%0Alinking%20target%20classes%20to%20generic%20prompts.%20We%20believe%20that%20our%20approach%20could%0Aserve%20as%20an%20important%20baseline%20for%20future%20research%20in%20vocabulary-free%20few-shot%0Alearning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MaxZanella/vocabulary-free-FSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVocabulary-free%2520few-shot%2520learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DMaxime%2520Zanella%2520and%2520Cl%25C3%25A9ment%2520Fuchs%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520few-shot%2520adaptation%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%250Agreatly%2520expanded%2520their%2520ability%2520to%2520generalize%2520across%2520tasks%2520using%2520only%2520a%2520few%250Alabeled%2520examples.%2520However%252C%2520existing%2520approaches%2520primarily%2520build%2520upon%2520the%2520strong%250Azero-shot%2520priors%2520of%2520these%2520models%2520by%2520leveraging%2520carefully%2520designed%252C%250Atask-specific%2520prompts.%2520This%2520dependence%2520on%2520predefined%2520class%2520names%2520can%2520restrict%250Atheir%2520applicability%252C%2520especially%2520in%2520scenarios%2520where%2520exact%2520class%2520names%2520are%250Aunavailable%2520or%2520difficult%2520to%2520specify.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250Avocabulary-free%2520few-shot%2520learning%2520for%2520VLMs%252C%2520a%2520setting%2520where%2520target%2520class%250Ainstances%2520-%2520that%2520is%252C%2520images%2520-%2520are%2520available%2520but%2520their%2520corresponding%2520names%2520are%250Anot.%2520We%2520propose%2520Similarity%2520Mapping%2520%2528SiM%2529%252C%2520a%2520simple%2520yet%2520effective%2520baseline%2520that%250Aclassifies%2520target%2520instances%2520solely%2520based%2520on%2520similarity%2520scores%2520with%2520a%2520set%2520of%250Ageneric%2520prompts%2520%2528textual%2520or%2520visual%2529%252C%2520eliminating%2520the%2520need%2520for%2520carefully%250Ahandcrafted%2520prompts.%2520Although%2520conceptually%2520straightforward%252C%2520SiM%2520demonstrates%250Astrong%2520performance%252C%2520operates%2520with%2520high%2520computational%2520efficiency%2520%2528learning%2520the%250Amapping%2520typically%2520takes%2520less%2520than%2520one%2520second%2529%252C%2520and%2520provides%2520interpretability%2520by%250Alinking%2520target%2520classes%2520to%2520generic%2520prompts.%2520We%2520believe%2520that%2520our%2520approach%2520could%250Aserve%2520as%2520an%2520important%2520baseline%2520for%2520future%2520research%2520in%2520vocabulary-free%2520few-shot%250Alearning.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/MaxZanella/vocabulary-free-FSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vocabulary-free%20few-shot%20learning%20for%20Vision-Language%20Models&entry.906535625=Maxime%20Zanella%20and%20Cl%C3%A9ment%20Fuchs%20and%20Ismail%20Ben%20Ayed%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20Recent%20advances%20in%20few-shot%20adaptation%20for%20Vision-Language%20Models%20%28VLMs%29%20have%0Agreatly%20expanded%20their%20ability%20to%20generalize%20across%20tasks%20using%20only%20a%20few%0Alabeled%20examples.%20However%2C%20existing%20approaches%20primarily%20build%20upon%20the%20strong%0Azero-shot%20priors%20of%20these%20models%20by%20leveraging%20carefully%20designed%2C%0Atask-specific%20prompts.%20This%20dependence%20on%20predefined%20class%20names%20can%20restrict%0Atheir%20applicability%2C%20especially%20in%20scenarios%20where%20exact%20class%20names%20are%0Aunavailable%20or%20difficult%20to%20specify.%20To%20address%20this%20limitation%2C%20we%20introduce%0Avocabulary-free%20few-shot%20learning%20for%20VLMs%2C%20a%20setting%20where%20target%20class%0Ainstances%20-%20that%20is%2C%20images%20-%20are%20available%20but%20their%20corresponding%20names%20are%0Anot.%20We%20propose%20Similarity%20Mapping%20%28SiM%29%2C%20a%20simple%20yet%20effective%20baseline%20that%0Aclassifies%20target%20instances%20solely%20based%20on%20similarity%20scores%20with%20a%20set%20of%0Ageneric%20prompts%20%28textual%20or%20visual%29%2C%20eliminating%20the%20need%20for%20carefully%0Ahandcrafted%20prompts.%20Although%20conceptually%20straightforward%2C%20SiM%20demonstrates%0Astrong%20performance%2C%20operates%20with%20high%20computational%20efficiency%20%28learning%20the%0Amapping%20typically%20takes%20less%20than%20one%20second%29%2C%20and%20provides%20interpretability%20by%0Alinking%20target%20classes%20to%20generic%20prompts.%20We%20believe%20that%20our%20approach%20could%0Aserve%20as%20an%20important%20baseline%20for%20future%20research%20in%20vocabulary-free%20few-shot%0Alearning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MaxZanella/vocabulary-free-FSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04005v1&entry.124074799=Read"},
{"title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation", "author": "Tianyu Huang and Wangguandong Zheng and Tengfei Wang and Yuhao Liu and Zhenwei Wang and Junta Wu and Jie Jiang and Hui Li and Rynson W. H. Lau and Wangmeng Zuo and Chunchao Guo", "abstract": "  Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.\n", "link": "http://arxiv.org/abs/2506.04225v1", "date": "2025-06-04", "relevancy": 2.7467, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6868}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6868}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voyager%3A%20Long-Range%20and%20World-Consistent%20Video%20Diffusion%20for%20Explorable%0A%20%203D%20Scene%20Generation&body=Title%3A%20Voyager%3A%20Long-Range%20and%20World-Consistent%20Video%20Diffusion%20for%20Explorable%0A%20%203D%20Scene%20Generation%0AAuthor%3A%20Tianyu%20Huang%20and%20Wangguandong%20Zheng%20and%20Tengfei%20Wang%20and%20Yuhao%20Liu%20and%20Zhenwei%20Wang%20and%20Junta%20Wu%20and%20Jie%20Jiang%20and%20Hui%20Li%20and%20Rynson%20W.%20H.%20Lau%20and%20Wangmeng%20Zuo%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Real-world%20applications%20like%20video%20gaming%20and%20virtual%20reality%20often%20demand%0Athe%20ability%20to%20model%203D%20scenes%20that%20users%20can%20explore%20along%20custom%20camera%0Atrajectories.%20While%20significant%20progress%20has%20been%20made%20in%20generating%203D%20objects%0Afrom%20text%20or%20images%2C%20creating%20long-range%2C%203D-consistent%2C%20explorable%203D%20scenes%0Aremains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20present%20Voyager%2C%20a%0Anovel%20video%20diffusion%20framework%20that%20generates%20world-consistent%203D%20point-cloud%0Asequences%20from%20a%20single%20image%20with%20user-defined%20camera%20path.%20Unlike%20existing%0Aapproaches%2C%20Voyager%20achieves%20end-to-end%20scene%20generation%20and%20reconstruction%0Awith%20inherent%20consistency%20across%20frames%2C%20eliminating%20the%20need%20for%203D%0Areconstruction%20pipelines%20%28e.g.%2C%20structure-from-motion%20or%20multi-view%20stereo%29.%0AOur%20method%20integrates%20three%20key%20components%3A%201%29%20World-Consistent%20Video%0ADiffusion%3A%20A%20unified%20architecture%20that%20jointly%20generates%20aligned%20RGB%20and%20depth%0Avideo%20sequences%2C%20conditioned%20on%20existing%20world%20observation%20to%20ensure%20global%0Acoherence%202%29%20Long-Range%20World%20Exploration%3A%20An%20efficient%20world%20cache%20with%20point%0Aculling%20and%20an%20auto-regressive%20inference%20with%20smooth%20video%20sampling%20for%0Aiterative%20scene%20extension%20with%20context-aware%20consistency%2C%20and%203%29%20Scalable%20Data%0AEngine%3A%20A%20video%20reconstruction%20pipeline%20that%20automates%20camera%20pose%20estimation%0Aand%20metric%20depth%20prediction%20for%20arbitrary%20videos%2C%20enabling%20large-scale%2C%20diverse%0Atraining%20data%20curation%20without%20manual%203D%20annotations.%20Collectively%2C%20these%0Adesigns%20result%20in%20a%20clear%20improvement%20over%20existing%20methods%20in%20visual%20quality%0Aand%20geometric%20accuracy%2C%20with%20versatile%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoyager%253A%2520Long-Range%2520and%2520World-Consistent%2520Video%2520Diffusion%2520for%2520Explorable%250A%2520%25203D%2520Scene%2520Generation%26entry.906535625%3DTianyu%2520Huang%2520and%2520Wangguandong%2520Zheng%2520and%2520Tengfei%2520Wang%2520and%2520Yuhao%2520Liu%2520and%2520Zhenwei%2520Wang%2520and%2520Junta%2520Wu%2520and%2520Jie%2520Jiang%2520and%2520Hui%2520Li%2520and%2520Rynson%2520W.%2520H.%2520Lau%2520and%2520Wangmeng%2520Zuo%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Real-world%2520applications%2520like%2520video%2520gaming%2520and%2520virtual%2520reality%2520often%2520demand%250Athe%2520ability%2520to%2520model%25203D%2520scenes%2520that%2520users%2520can%2520explore%2520along%2520custom%2520camera%250Atrajectories.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%2520generating%25203D%2520objects%250Afrom%2520text%2520or%2520images%252C%2520creating%2520long-range%252C%25203D-consistent%252C%2520explorable%25203D%2520scenes%250Aremains%2520a%2520complex%2520and%2520challenging%2520problem.%2520In%2520this%2520work%252C%2520we%2520present%2520Voyager%252C%2520a%250Anovel%2520video%2520diffusion%2520framework%2520that%2520generates%2520world-consistent%25203D%2520point-cloud%250Asequences%2520from%2520a%2520single%2520image%2520with%2520user-defined%2520camera%2520path.%2520Unlike%2520existing%250Aapproaches%252C%2520Voyager%2520achieves%2520end-to-end%2520scene%2520generation%2520and%2520reconstruction%250Awith%2520inherent%2520consistency%2520across%2520frames%252C%2520eliminating%2520the%2520need%2520for%25203D%250Areconstruction%2520pipelines%2520%2528e.g.%252C%2520structure-from-motion%2520or%2520multi-view%2520stereo%2529.%250AOur%2520method%2520integrates%2520three%2520key%2520components%253A%25201%2529%2520World-Consistent%2520Video%250ADiffusion%253A%2520A%2520unified%2520architecture%2520that%2520jointly%2520generates%2520aligned%2520RGB%2520and%2520depth%250Avideo%2520sequences%252C%2520conditioned%2520on%2520existing%2520world%2520observation%2520to%2520ensure%2520global%250Acoherence%25202%2529%2520Long-Range%2520World%2520Exploration%253A%2520An%2520efficient%2520world%2520cache%2520with%2520point%250Aculling%2520and%2520an%2520auto-regressive%2520inference%2520with%2520smooth%2520video%2520sampling%2520for%250Aiterative%2520scene%2520extension%2520with%2520context-aware%2520consistency%252C%2520and%25203%2529%2520Scalable%2520Data%250AEngine%253A%2520A%2520video%2520reconstruction%2520pipeline%2520that%2520automates%2520camera%2520pose%2520estimation%250Aand%2520metric%2520depth%2520prediction%2520for%2520arbitrary%2520videos%252C%2520enabling%2520large-scale%252C%2520diverse%250Atraining%2520data%2520curation%2520without%2520manual%25203D%2520annotations.%2520Collectively%252C%2520these%250Adesigns%2520result%2520in%2520a%2520clear%2520improvement%2520over%2520existing%2520methods%2520in%2520visual%2520quality%250Aand%2520geometric%2520accuracy%252C%2520with%2520versatile%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voyager%3A%20Long-Range%20and%20World-Consistent%20Video%20Diffusion%20for%20Explorable%0A%20%203D%20Scene%20Generation&entry.906535625=Tianyu%20Huang%20and%20Wangguandong%20Zheng%20and%20Tengfei%20Wang%20and%20Yuhao%20Liu%20and%20Zhenwei%20Wang%20and%20Junta%20Wu%20and%20Jie%20Jiang%20and%20Hui%20Li%20and%20Rynson%20W.%20H.%20Lau%20and%20Wangmeng%20Zuo%20and%20Chunchao%20Guo&entry.1292438233=%20%20Real-world%20applications%20like%20video%20gaming%20and%20virtual%20reality%20often%20demand%0Athe%20ability%20to%20model%203D%20scenes%20that%20users%20can%20explore%20along%20custom%20camera%0Atrajectories.%20While%20significant%20progress%20has%20been%20made%20in%20generating%203D%20objects%0Afrom%20text%20or%20images%2C%20creating%20long-range%2C%203D-consistent%2C%20explorable%203D%20scenes%0Aremains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20present%20Voyager%2C%20a%0Anovel%20video%20diffusion%20framework%20that%20generates%20world-consistent%203D%20point-cloud%0Asequences%20from%20a%20single%20image%20with%20user-defined%20camera%20path.%20Unlike%20existing%0Aapproaches%2C%20Voyager%20achieves%20end-to-end%20scene%20generation%20and%20reconstruction%0Awith%20inherent%20consistency%20across%20frames%2C%20eliminating%20the%20need%20for%203D%0Areconstruction%20pipelines%20%28e.g.%2C%20structure-from-motion%20or%20multi-view%20stereo%29.%0AOur%20method%20integrates%20three%20key%20components%3A%201%29%20World-Consistent%20Video%0ADiffusion%3A%20A%20unified%20architecture%20that%20jointly%20generates%20aligned%20RGB%20and%20depth%0Avideo%20sequences%2C%20conditioned%20on%20existing%20world%20observation%20to%20ensure%20global%0Acoherence%202%29%20Long-Range%20World%20Exploration%3A%20An%20efficient%20world%20cache%20with%20point%0Aculling%20and%20an%20auto-regressive%20inference%20with%20smooth%20video%20sampling%20for%0Aiterative%20scene%20extension%20with%20context-aware%20consistency%2C%20and%203%29%20Scalable%20Data%0AEngine%3A%20A%20video%20reconstruction%20pipeline%20that%20automates%20camera%20pose%20estimation%0Aand%20metric%20depth%20prediction%20for%20arbitrary%20videos%2C%20enabling%20large-scale%2C%20diverse%0Atraining%20data%20curation%20without%20manual%203D%20annotations.%20Collectively%2C%20these%0Adesigns%20result%20in%20a%20clear%20improvement%20over%20existing%20methods%20in%20visual%20quality%0Aand%20geometric%20accuracy%2C%20with%20versatile%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04225v1&entry.124074799=Read"},
{"title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal\n  Representation Learning", "author": "Negin Baghbanzadeh and Sajad Ashkezari and Elham Dolatabadi and Arash Afkanpour", "abstract": "  Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning.\n", "link": "http://arxiv.org/abs/2506.02738v2", "date": "2025-06-04", "relevancy": 2.7419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning&body=Title%3A%20Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning%0AAuthor%3A%20Negin%20Baghbanzadeh%20and%20Sajad%20Ashkezari%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour%0AAbstract%3A%20%20%20Compound%20figures%2C%20which%20are%20multi-panel%20composites%20containing%20diverse%0Asubfigures%2C%20are%20ubiquitous%20in%20biomedical%20literature%2C%20yet%20large-scale%20subfigure%0Aextraction%20remains%20largely%20unaddressed.%20Prior%20work%20on%20subfigure%20extraction%20has%0Abeen%20limited%20in%20both%20dataset%20size%20and%20generalizability%2C%20leaving%20a%20critical%20open%0Aquestion%3A%20How%20does%20high-fidelity%20image-text%20alignment%20via%20large-scale%20subfigure%0Aextraction%20impact%20representation%20learning%20in%20vision-language%20models%3F%20We%20address%0Athis%20gap%20by%20introducing%20a%20scalable%20subfigure%20extraction%20pipeline%20based%20on%0Atransformer-based%20object%20detection%2C%20trained%20on%20a%20synthetic%20corpus%20of%20500%2C000%0Acompound%20figures%2C%20and%20achieving%20state-of-the-art%20performance%20on%20both%20ImageCLEF%0A2016%20and%20synthetic%20benchmarks.%20Using%20this%20pipeline%2C%20we%20release%20OPEN-PMC-18M%2C%20a%0Alarge-scale%20high%20quality%20biomedical%20vision-language%20dataset%20comprising%2018%0Amillion%20clinically%20relevant%20subfigure-caption%20pairs%20spanning%20radiology%2C%0Amicroscopy%2C%20and%20visible%20light%20photography.%20We%20train%20and%20evaluate%0Avision-language%20models%20on%20our%20curated%20datasets%20and%20show%20improved%20performance%0Aacross%20retrieval%2C%20zero-shot%20classification%2C%20and%20robustness%20benchmarks%2C%0Aoutperforming%20existing%20baselines.%20We%20release%20our%20dataset%2C%20models%2C%20and%20code%20to%0Asupport%20reproducible%20benchmarks%20and%20further%20study%20into%20biomedical%0Avision-language%20modeling%20and%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-PMC-18M%253A%2520A%2520High-Fidelity%2520Large%2520Scale%2520Medical%2520Dataset%2520for%2520Multimodal%250A%2520%2520Representation%2520Learning%26entry.906535625%3DNegin%2520Baghbanzadeh%2520and%2520Sajad%2520Ashkezari%2520and%2520Elham%2520Dolatabadi%2520and%2520Arash%2520Afkanpour%26entry.1292438233%3D%2520%2520Compound%2520figures%252C%2520which%2520are%2520multi-panel%2520composites%2520containing%2520diverse%250Asubfigures%252C%2520are%2520ubiquitous%2520in%2520biomedical%2520literature%252C%2520yet%2520large-scale%2520subfigure%250Aextraction%2520remains%2520largely%2520unaddressed.%2520Prior%2520work%2520on%2520subfigure%2520extraction%2520has%250Abeen%2520limited%2520in%2520both%2520dataset%2520size%2520and%2520generalizability%252C%2520leaving%2520a%2520critical%2520open%250Aquestion%253A%2520How%2520does%2520high-fidelity%2520image-text%2520alignment%2520via%2520large-scale%2520subfigure%250Aextraction%2520impact%2520representation%2520learning%2520in%2520vision-language%2520models%253F%2520We%2520address%250Athis%2520gap%2520by%2520introducing%2520a%2520scalable%2520subfigure%2520extraction%2520pipeline%2520based%2520on%250Atransformer-based%2520object%2520detection%252C%2520trained%2520on%2520a%2520synthetic%2520corpus%2520of%2520500%252C000%250Acompound%2520figures%252C%2520and%2520achieving%2520state-of-the-art%2520performance%2520on%2520both%2520ImageCLEF%250A2016%2520and%2520synthetic%2520benchmarks.%2520Using%2520this%2520pipeline%252C%2520we%2520release%2520OPEN-PMC-18M%252C%2520a%250Alarge-scale%2520high%2520quality%2520biomedical%2520vision-language%2520dataset%2520comprising%252018%250Amillion%2520clinically%2520relevant%2520subfigure-caption%2520pairs%2520spanning%2520radiology%252C%250Amicroscopy%252C%2520and%2520visible%2520light%2520photography.%2520We%2520train%2520and%2520evaluate%250Avision-language%2520models%2520on%2520our%2520curated%2520datasets%2520and%2520show%2520improved%2520performance%250Aacross%2520retrieval%252C%2520zero-shot%2520classification%252C%2520and%2520robustness%2520benchmarks%252C%250Aoutperforming%2520existing%2520baselines.%2520We%2520release%2520our%2520dataset%252C%2520models%252C%2520and%2520code%2520to%250Asupport%2520reproducible%2520benchmarks%2520and%2520further%2520study%2520into%2520biomedical%250Avision-language%2520modeling%2520and%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning&entry.906535625=Negin%20Baghbanzadeh%20and%20Sajad%20Ashkezari%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour&entry.1292438233=%20%20Compound%20figures%2C%20which%20are%20multi-panel%20composites%20containing%20diverse%0Asubfigures%2C%20are%20ubiquitous%20in%20biomedical%20literature%2C%20yet%20large-scale%20subfigure%0Aextraction%20remains%20largely%20unaddressed.%20Prior%20work%20on%20subfigure%20extraction%20has%0Abeen%20limited%20in%20both%20dataset%20size%20and%20generalizability%2C%20leaving%20a%20critical%20open%0Aquestion%3A%20How%20does%20high-fidelity%20image-text%20alignment%20via%20large-scale%20subfigure%0Aextraction%20impact%20representation%20learning%20in%20vision-language%20models%3F%20We%20address%0Athis%20gap%20by%20introducing%20a%20scalable%20subfigure%20extraction%20pipeline%20based%20on%0Atransformer-based%20object%20detection%2C%20trained%20on%20a%20synthetic%20corpus%20of%20500%2C000%0Acompound%20figures%2C%20and%20achieving%20state-of-the-art%20performance%20on%20both%20ImageCLEF%0A2016%20and%20synthetic%20benchmarks.%20Using%20this%20pipeline%2C%20we%20release%20OPEN-PMC-18M%2C%20a%0Alarge-scale%20high%20quality%20biomedical%20vision-language%20dataset%20comprising%2018%0Amillion%20clinically%20relevant%20subfigure-caption%20pairs%20spanning%20radiology%2C%0Amicroscopy%2C%20and%20visible%20light%20photography.%20We%20train%20and%20evaluate%0Avision-language%20models%20on%20our%20curated%20datasets%20and%20show%20improved%20performance%0Aacross%20retrieval%2C%20zero-shot%20classification%2C%20and%20robustness%20benchmarks%2C%0Aoutperforming%20existing%20baselines.%20We%20release%20our%20dataset%2C%20models%2C%20and%20code%20to%0Asupport%20reproducible%20benchmarks%20and%20further%20study%20into%20biomedical%0Avision-language%20modeling%20and%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02738v2&entry.124074799=Read"},
{"title": "B\u00e9zier Splatting for Fast and Differentiable Vector Graphics Rendering", "author": "Xi Liu and Chaoyi Zhou and Nanxuan Zhao and Siyu Huang", "abstract": "  Differentiable vector graphics (VGs) are widely used in image vectorization\nand vector synthesis, while existing representations are costly to optimize and\nstruggle to achieve high-quality rendering results for high-resolution images.\nThis work introduces a new differentiable VG representation, dubbed B\\'ezier\nSplatting, that enables fast yet high-fidelity VG rasterization. B\\'ezier\nSplatting samples 2D Gaussians along B\\'ezier curves, which naturally provide\npositional gradients at object boundaries. Thanks to the efficient\nsplatting-based differentiable rasterizer, B\\'ezier Splatting achieves 30x and\n150x faster per forward and backward rasterization step for open curves\ncompared to DiffVG. Additionally, we introduce an adaptive pruning and\ndensification strategy that dynamically adjusts the spatial distribution of\ncurves to escape local minima, further improving VG quality. Furthermore, our\nnew VG representation supports conversion to standard XML-based SVG format,\nenhancing interoperability with existing VG tools and pipelines. Experimental\nresults show that B\\'ezier Splatting significantly outperforms existing methods\nwith better visual fidelity and significant optimization speedup.\n", "link": "http://arxiv.org/abs/2503.16424v3", "date": "2025-06-04", "relevancy": 2.7216, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5569}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B%C3%A9zier%20Splatting%20for%20Fast%20and%20Differentiable%20Vector%20Graphics%20Rendering&body=Title%3A%20B%C3%A9zier%20Splatting%20for%20Fast%20and%20Differentiable%20Vector%20Graphics%20Rendering%0AAuthor%3A%20Xi%20Liu%20and%20Chaoyi%20Zhou%20and%20Nanxuan%20Zhao%20and%20Siyu%20Huang%0AAbstract%3A%20%20%20Differentiable%20vector%20graphics%20%28VGs%29%20are%20widely%20used%20in%20image%20vectorization%0Aand%20vector%20synthesis%2C%20while%20existing%20representations%20are%20costly%20to%20optimize%20and%0Astruggle%20to%20achieve%20high-quality%20rendering%20results%20for%20high-resolution%20images.%0AThis%20work%20introduces%20a%20new%20differentiable%20VG%20representation%2C%20dubbed%20B%5C%27ezier%0ASplatting%2C%20that%20enables%20fast%20yet%20high-fidelity%20VG%20rasterization.%20B%5C%27ezier%0ASplatting%20samples%202D%20Gaussians%20along%20B%5C%27ezier%20curves%2C%20which%20naturally%20provide%0Apositional%20gradients%20at%20object%20boundaries.%20Thanks%20to%20the%20efficient%0Asplatting-based%20differentiable%20rasterizer%2C%20B%5C%27ezier%20Splatting%20achieves%2030x%20and%0A150x%20faster%20per%20forward%20and%20backward%20rasterization%20step%20for%20open%20curves%0Acompared%20to%20DiffVG.%20Additionally%2C%20we%20introduce%20an%20adaptive%20pruning%20and%0Adensification%20strategy%20that%20dynamically%20adjusts%20the%20spatial%20distribution%20of%0Acurves%20to%20escape%20local%20minima%2C%20further%20improving%20VG%20quality.%20Furthermore%2C%20our%0Anew%20VG%20representation%20supports%20conversion%20to%20standard%20XML-based%20SVG%20format%2C%0Aenhancing%20interoperability%20with%20existing%20VG%20tools%20and%20pipelines.%20Experimental%0Aresults%20show%20that%20B%5C%27ezier%20Splatting%20significantly%20outperforms%20existing%20methods%0Awith%20better%20visual%20fidelity%20and%20significant%20optimization%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16424v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB%25C3%25A9zier%2520Splatting%2520for%2520Fast%2520and%2520Differentiable%2520Vector%2520Graphics%2520Rendering%26entry.906535625%3DXi%2520Liu%2520and%2520Chaoyi%2520Zhou%2520and%2520Nanxuan%2520Zhao%2520and%2520Siyu%2520Huang%26entry.1292438233%3D%2520%2520Differentiable%2520vector%2520graphics%2520%2528VGs%2529%2520are%2520widely%2520used%2520in%2520image%2520vectorization%250Aand%2520vector%2520synthesis%252C%2520while%2520existing%2520representations%2520are%2520costly%2520to%2520optimize%2520and%250Astruggle%2520to%2520achieve%2520high-quality%2520rendering%2520results%2520for%2520high-resolution%2520images.%250AThis%2520work%2520introduces%2520a%2520new%2520differentiable%2520VG%2520representation%252C%2520dubbed%2520B%255C%2527ezier%250ASplatting%252C%2520that%2520enables%2520fast%2520yet%2520high-fidelity%2520VG%2520rasterization.%2520B%255C%2527ezier%250ASplatting%2520samples%25202D%2520Gaussians%2520along%2520B%255C%2527ezier%2520curves%252C%2520which%2520naturally%2520provide%250Apositional%2520gradients%2520at%2520object%2520boundaries.%2520Thanks%2520to%2520the%2520efficient%250Asplatting-based%2520differentiable%2520rasterizer%252C%2520B%255C%2527ezier%2520Splatting%2520achieves%252030x%2520and%250A150x%2520faster%2520per%2520forward%2520and%2520backward%2520rasterization%2520step%2520for%2520open%2520curves%250Acompared%2520to%2520DiffVG.%2520Additionally%252C%2520we%2520introduce%2520an%2520adaptive%2520pruning%2520and%250Adensification%2520strategy%2520that%2520dynamically%2520adjusts%2520the%2520spatial%2520distribution%2520of%250Acurves%2520to%2520escape%2520local%2520minima%252C%2520further%2520improving%2520VG%2520quality.%2520Furthermore%252C%2520our%250Anew%2520VG%2520representation%2520supports%2520conversion%2520to%2520standard%2520XML-based%2520SVG%2520format%252C%250Aenhancing%2520interoperability%2520with%2520existing%2520VG%2520tools%2520and%2520pipelines.%2520Experimental%250Aresults%2520show%2520that%2520B%255C%2527ezier%2520Splatting%2520significantly%2520outperforms%2520existing%2520methods%250Awith%2520better%2520visual%2520fidelity%2520and%2520significant%2520optimization%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16424v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B%C3%A9zier%20Splatting%20for%20Fast%20and%20Differentiable%20Vector%20Graphics%20Rendering&entry.906535625=Xi%20Liu%20and%20Chaoyi%20Zhou%20and%20Nanxuan%20Zhao%20and%20Siyu%20Huang&entry.1292438233=%20%20Differentiable%20vector%20graphics%20%28VGs%29%20are%20widely%20used%20in%20image%20vectorization%0Aand%20vector%20synthesis%2C%20while%20existing%20representations%20are%20costly%20to%20optimize%20and%0Astruggle%20to%20achieve%20high-quality%20rendering%20results%20for%20high-resolution%20images.%0AThis%20work%20introduces%20a%20new%20differentiable%20VG%20representation%2C%20dubbed%20B%5C%27ezier%0ASplatting%2C%20that%20enables%20fast%20yet%20high-fidelity%20VG%20rasterization.%20B%5C%27ezier%0ASplatting%20samples%202D%20Gaussians%20along%20B%5C%27ezier%20curves%2C%20which%20naturally%20provide%0Apositional%20gradients%20at%20object%20boundaries.%20Thanks%20to%20the%20efficient%0Asplatting-based%20differentiable%20rasterizer%2C%20B%5C%27ezier%20Splatting%20achieves%2030x%20and%0A150x%20faster%20per%20forward%20and%20backward%20rasterization%20step%20for%20open%20curves%0Acompared%20to%20DiffVG.%20Additionally%2C%20we%20introduce%20an%20adaptive%20pruning%20and%0Adensification%20strategy%20that%20dynamically%20adjusts%20the%20spatial%20distribution%20of%0Acurves%20to%20escape%20local%20minima%2C%20further%20improving%20VG%20quality.%20Furthermore%2C%20our%0Anew%20VG%20representation%20supports%20conversion%20to%20standard%20XML-based%20SVG%20format%2C%0Aenhancing%20interoperability%20with%20existing%20VG%20tools%20and%20pipelines.%20Experimental%0Aresults%20show%20that%20B%5C%27ezier%20Splatting%20significantly%20outperforms%20existing%20methods%0Awith%20better%20visual%20fidelity%20and%20significant%20optimization%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16424v3&entry.124074799=Read"},
{"title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language\n  Model Interpretability", "author": "Adam Karvonen and Can Rager and Johnny Lin and Curt Tigges and Joseph Bloom and David Chanin and Yeu-Tong Lau and Eoin Farrell and Callum McDougall and Kola Ayonrinde and Demian Till and Matthew Wearden and Arthur Conmy and Samuel Marks and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across eight\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at:\nwww.neuronpedia.org/sae-bench\n", "link": "http://arxiv.org/abs/2503.09532v4", "date": "2025-06-04", "relevancy": 2.6934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability&body=Title%3A%20SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability%0AAuthor%3A%20Adam%20Karvonen%20and%20Can%20Rager%20and%20Johnny%20Lin%20and%20Curt%20Tigges%20and%20Joseph%20Bloom%20and%20David%20Chanin%20and%20Yeu-Tong%20Lau%20and%20Eoin%20Farrell%20and%20Callum%20McDougall%20and%20Kola%20Ayonrinde%20and%20Demian%20Till%20and%20Matthew%20Wearden%20and%20Arthur%20Conmy%20and%20Samuel%20Marks%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20technique%20for%20interpreting%20language%0Amodel%20activations%2C%20and%20there%20is%20extensive%20recent%20work%20on%20improving%20SAE%0Aeffectiveness.%20However%2C%20most%20prior%20work%20evaluates%20progress%20using%20unsupervised%0Aproxy%20metrics%20with%20unclear%20practical%20relevance.%20We%20introduce%20SAEBench%2C%20a%0Acomprehensive%20evaluation%20suite%20that%20measures%20SAE%20performance%20across%20eight%0Adiverse%20metrics%2C%20spanning%20interpretability%2C%20feature%20disentanglement%20and%0Apractical%20applications%20like%20unlearning.%20To%20enable%20systematic%20comparison%2C%20we%0Aopen-source%20a%20suite%20of%20over%20200%20SAEs%20across%20eight%20recently%20proposed%20SAE%0Aarchitectures%20and%20training%20algorithms.%20Our%20evaluation%20reveals%20that%20gains%20on%0Aproxy%20metrics%20do%20not%20reliably%20translate%20to%20better%20practical%20performance.%20For%0Ainstance%2C%20while%20Matryoshka%20SAEs%20slightly%20underperform%20on%20existing%20proxy%0Ametrics%2C%20they%20substantially%20outperform%20other%20architectures%20on%20feature%0Adisentanglement%20metrics%3B%20moreover%2C%20this%20advantage%20grows%20with%20SAE%20scale.%20By%0Aproviding%20a%20standardized%20framework%20for%20measuring%20progress%20in%20SAE%20development%2C%0ASAEBench%20enables%20researchers%20to%20study%20scaling%20trends%20and%20make%20nuanced%0Acomparisons%20between%20different%20SAE%20architectures%20and%20training%20methodologies.%20Our%0Ainteractive%20interface%20enables%20researchers%20to%20flexibly%20visualize%20relationships%0Abetween%20metrics%20across%20hundreds%20of%20open-source%20SAEs%20at%3A%0Awww.neuronpedia.org/sae-bench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09532v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAEBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Sparse%2520Autoencoders%2520in%2520Language%250A%2520%2520Model%2520Interpretability%26entry.906535625%3DAdam%2520Karvonen%2520and%2520Can%2520Rager%2520and%2520Johnny%2520Lin%2520and%2520Curt%2520Tigges%2520and%2520Joseph%2520Bloom%2520and%2520David%2520Chanin%2520and%2520Yeu-Tong%2520Lau%2520and%2520Eoin%2520Farrell%2520and%2520Callum%2520McDougall%2520and%2520Kola%2520Ayonrinde%2520and%2520Demian%2520Till%2520and%2520Matthew%2520Wearden%2520and%2520Arthur%2520Conmy%2520and%2520Samuel%2520Marks%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520popular%2520technique%2520for%2520interpreting%2520language%250Amodel%2520activations%252C%2520and%2520there%2520is%2520extensive%2520recent%2520work%2520on%2520improving%2520SAE%250Aeffectiveness.%2520However%252C%2520most%2520prior%2520work%2520evaluates%2520progress%2520using%2520unsupervised%250Aproxy%2520metrics%2520with%2520unclear%2520practical%2520relevance.%2520We%2520introduce%2520SAEBench%252C%2520a%250Acomprehensive%2520evaluation%2520suite%2520that%2520measures%2520SAE%2520performance%2520across%2520eight%250Adiverse%2520metrics%252C%2520spanning%2520interpretability%252C%2520feature%2520disentanglement%2520and%250Apractical%2520applications%2520like%2520unlearning.%2520To%2520enable%2520systematic%2520comparison%252C%2520we%250Aopen-source%2520a%2520suite%2520of%2520over%2520200%2520SAEs%2520across%2520eight%2520recently%2520proposed%2520SAE%250Aarchitectures%2520and%2520training%2520algorithms.%2520Our%2520evaluation%2520reveals%2520that%2520gains%2520on%250Aproxy%2520metrics%2520do%2520not%2520reliably%2520translate%2520to%2520better%2520practical%2520performance.%2520For%250Ainstance%252C%2520while%2520Matryoshka%2520SAEs%2520slightly%2520underperform%2520on%2520existing%2520proxy%250Ametrics%252C%2520they%2520substantially%2520outperform%2520other%2520architectures%2520on%2520feature%250Adisentanglement%2520metrics%253B%2520moreover%252C%2520this%2520advantage%2520grows%2520with%2520SAE%2520scale.%2520By%250Aproviding%2520a%2520standardized%2520framework%2520for%2520measuring%2520progress%2520in%2520SAE%2520development%252C%250ASAEBench%2520enables%2520researchers%2520to%2520study%2520scaling%2520trends%2520and%2520make%2520nuanced%250Acomparisons%2520between%2520different%2520SAE%2520architectures%2520and%2520training%2520methodologies.%2520Our%250Ainteractive%2520interface%2520enables%2520researchers%2520to%2520flexibly%2520visualize%2520relationships%250Abetween%2520metrics%2520across%2520hundreds%2520of%2520open-source%2520SAEs%2520at%253A%250Awww.neuronpedia.org/sae-bench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09532v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability&entry.906535625=Adam%20Karvonen%20and%20Can%20Rager%20and%20Johnny%20Lin%20and%20Curt%20Tigges%20and%20Joseph%20Bloom%20and%20David%20Chanin%20and%20Yeu-Tong%20Lau%20and%20Eoin%20Farrell%20and%20Callum%20McDougall%20and%20Kola%20Ayonrinde%20and%20Demian%20Till%20and%20Matthew%20Wearden%20and%20Arthur%20Conmy%20and%20Samuel%20Marks%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20technique%20for%20interpreting%20language%0Amodel%20activations%2C%20and%20there%20is%20extensive%20recent%20work%20on%20improving%20SAE%0Aeffectiveness.%20However%2C%20most%20prior%20work%20evaluates%20progress%20using%20unsupervised%0Aproxy%20metrics%20with%20unclear%20practical%20relevance.%20We%20introduce%20SAEBench%2C%20a%0Acomprehensive%20evaluation%20suite%20that%20measures%20SAE%20performance%20across%20eight%0Adiverse%20metrics%2C%20spanning%20interpretability%2C%20feature%20disentanglement%20and%0Apractical%20applications%20like%20unlearning.%20To%20enable%20systematic%20comparison%2C%20we%0Aopen-source%20a%20suite%20of%20over%20200%20SAEs%20across%20eight%20recently%20proposed%20SAE%0Aarchitectures%20and%20training%20algorithms.%20Our%20evaluation%20reveals%20that%20gains%20on%0Aproxy%20metrics%20do%20not%20reliably%20translate%20to%20better%20practical%20performance.%20For%0Ainstance%2C%20while%20Matryoshka%20SAEs%20slightly%20underperform%20on%20existing%20proxy%0Ametrics%2C%20they%20substantially%20outperform%20other%20architectures%20on%20feature%0Adisentanglement%20metrics%3B%20moreover%2C%20this%20advantage%20grows%20with%20SAE%20scale.%20By%0Aproviding%20a%20standardized%20framework%20for%20measuring%20progress%20in%20SAE%20development%2C%0ASAEBench%20enables%20researchers%20to%20study%20scaling%20trends%20and%20make%20nuanced%0Acomparisons%20between%20different%20SAE%20architectures%20and%20training%20methodologies.%20Our%0Ainteractive%20interface%20enables%20researchers%20to%20flexibly%20visualize%20relationships%0Abetween%20metrics%20across%20hundreds%20of%20open-source%20SAEs%20at%3A%0Awww.neuronpedia.org/sae-bench%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09532v4&entry.124074799=Read"},
{"title": "Objective drives the consistency of representational similarity across\n  datasets", "author": "Laure Ciernik and Lorenz Linhardt and Marco Morik and Jonas Dippel and Simon Kornblith and Lukas Muttenthaler", "abstract": "  The Platonic Representation Hypothesis claims that recent foundation models\nare converging to a shared representation space as a function of their\ndownstream task performance, irrespective of the objectives and data modalities\nused to train these models (Huh et al., 2024). Representational similarity is\ngenerally measured for individual datasets and is not necessarily consistent\nacross datasets. Thus, one may wonder whether this convergence of model\nrepresentations is confounded by the datasets commonly used in machine\nlearning. Here, we propose a systematic way to measure how representational\nsimilarity between models varies with the set of stimuli used to construct the\nrepresentations. We find that the objective function is a crucial factor in\ndetermining the consistency of representational similarities across datasets.\nSpecifically, self-supervised vision models learn representations whose\nrelative pairwise similarities generalize better from one dataset to another\ncompared to those of image classification or image-text models. Moreover, the\ncorrespondence between representational similarities and the models' task\nbehavior is dataset-dependent, being most strongly pronounced for single-domain\ndatasets. Our work provides a framework for analyzing similarities of model\nrepresentations across datasets and linking those similarities to differences\nin task behavior.\n", "link": "http://arxiv.org/abs/2411.05561v2", "date": "2025-06-04", "relevancy": 2.6741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Objective%20drives%20the%20consistency%20of%20representational%20similarity%20across%0A%20%20datasets&body=Title%3A%20Objective%20drives%20the%20consistency%20of%20representational%20similarity%20across%0A%20%20datasets%0AAuthor%3A%20Laure%20Ciernik%20and%20Lorenz%20Linhardt%20and%20Marco%20Morik%20and%20Jonas%20Dippel%20and%20Simon%20Kornblith%20and%20Lukas%20Muttenthaler%0AAbstract%3A%20%20%20The%20Platonic%20Representation%20Hypothesis%20claims%20that%20recent%20foundation%20models%0Aare%20converging%20to%20a%20shared%20representation%20space%20as%20a%20function%20of%20their%0Adownstream%20task%20performance%2C%20irrespective%20of%20the%20objectives%20and%20data%20modalities%0Aused%20to%20train%20these%20models%20%28Huh%20et%20al.%2C%202024%29.%20Representational%20similarity%20is%0Agenerally%20measured%20for%20individual%20datasets%20and%20is%20not%20necessarily%20consistent%0Aacross%20datasets.%20Thus%2C%20one%20may%20wonder%20whether%20this%20convergence%20of%20model%0Arepresentations%20is%20confounded%20by%20the%20datasets%20commonly%20used%20in%20machine%0Alearning.%20Here%2C%20we%20propose%20a%20systematic%20way%20to%20measure%20how%20representational%0Asimilarity%20between%20models%20varies%20with%20the%20set%20of%20stimuli%20used%20to%20construct%20the%0Arepresentations.%20We%20find%20that%20the%20objective%20function%20is%20a%20crucial%20factor%20in%0Adetermining%20the%20consistency%20of%20representational%20similarities%20across%20datasets.%0ASpecifically%2C%20self-supervised%20vision%20models%20learn%20representations%20whose%0Arelative%20pairwise%20similarities%20generalize%20better%20from%20one%20dataset%20to%20another%0Acompared%20to%20those%20of%20image%20classification%20or%20image-text%20models.%20Moreover%2C%20the%0Acorrespondence%20between%20representational%20similarities%20and%20the%20models%27%20task%0Abehavior%20is%20dataset-dependent%2C%20being%20most%20strongly%20pronounced%20for%20single-domain%0Adatasets.%20Our%20work%20provides%20a%20framework%20for%20analyzing%20similarities%20of%20model%0Arepresentations%20across%20datasets%20and%20linking%20those%20similarities%20to%20differences%0Ain%20task%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjective%2520drives%2520the%2520consistency%2520of%2520representational%2520similarity%2520across%250A%2520%2520datasets%26entry.906535625%3DLaure%2520Ciernik%2520and%2520Lorenz%2520Linhardt%2520and%2520Marco%2520Morik%2520and%2520Jonas%2520Dippel%2520and%2520Simon%2520Kornblith%2520and%2520Lukas%2520Muttenthaler%26entry.1292438233%3D%2520%2520The%2520Platonic%2520Representation%2520Hypothesis%2520claims%2520that%2520recent%2520foundation%2520models%250Aare%2520converging%2520to%2520a%2520shared%2520representation%2520space%2520as%2520a%2520function%2520of%2520their%250Adownstream%2520task%2520performance%252C%2520irrespective%2520of%2520the%2520objectives%2520and%2520data%2520modalities%250Aused%2520to%2520train%2520these%2520models%2520%2528Huh%2520et%2520al.%252C%25202024%2529.%2520Representational%2520similarity%2520is%250Agenerally%2520measured%2520for%2520individual%2520datasets%2520and%2520is%2520not%2520necessarily%2520consistent%250Aacross%2520datasets.%2520Thus%252C%2520one%2520may%2520wonder%2520whether%2520this%2520convergence%2520of%2520model%250Arepresentations%2520is%2520confounded%2520by%2520the%2520datasets%2520commonly%2520used%2520in%2520machine%250Alearning.%2520Here%252C%2520we%2520propose%2520a%2520systematic%2520way%2520to%2520measure%2520how%2520representational%250Asimilarity%2520between%2520models%2520varies%2520with%2520the%2520set%2520of%2520stimuli%2520used%2520to%2520construct%2520the%250Arepresentations.%2520We%2520find%2520that%2520the%2520objective%2520function%2520is%2520a%2520crucial%2520factor%2520in%250Adetermining%2520the%2520consistency%2520of%2520representational%2520similarities%2520across%2520datasets.%250ASpecifically%252C%2520self-supervised%2520vision%2520models%2520learn%2520representations%2520whose%250Arelative%2520pairwise%2520similarities%2520generalize%2520better%2520from%2520one%2520dataset%2520to%2520another%250Acompared%2520to%2520those%2520of%2520image%2520classification%2520or%2520image-text%2520models.%2520Moreover%252C%2520the%250Acorrespondence%2520between%2520representational%2520similarities%2520and%2520the%2520models%2527%2520task%250Abehavior%2520is%2520dataset-dependent%252C%2520being%2520most%2520strongly%2520pronounced%2520for%2520single-domain%250Adatasets.%2520Our%2520work%2520provides%2520a%2520framework%2520for%2520analyzing%2520similarities%2520of%2520model%250Arepresentations%2520across%2520datasets%2520and%2520linking%2520those%2520similarities%2520to%2520differences%250Ain%2520task%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Objective%20drives%20the%20consistency%20of%20representational%20similarity%20across%0A%20%20datasets&entry.906535625=Laure%20Ciernik%20and%20Lorenz%20Linhardt%20and%20Marco%20Morik%20and%20Jonas%20Dippel%20and%20Simon%20Kornblith%20and%20Lukas%20Muttenthaler&entry.1292438233=%20%20The%20Platonic%20Representation%20Hypothesis%20claims%20that%20recent%20foundation%20models%0Aare%20converging%20to%20a%20shared%20representation%20space%20as%20a%20function%20of%20their%0Adownstream%20task%20performance%2C%20irrespective%20of%20the%20objectives%20and%20data%20modalities%0Aused%20to%20train%20these%20models%20%28Huh%20et%20al.%2C%202024%29.%20Representational%20similarity%20is%0Agenerally%20measured%20for%20individual%20datasets%20and%20is%20not%20necessarily%20consistent%0Aacross%20datasets.%20Thus%2C%20one%20may%20wonder%20whether%20this%20convergence%20of%20model%0Arepresentations%20is%20confounded%20by%20the%20datasets%20commonly%20used%20in%20machine%0Alearning.%20Here%2C%20we%20propose%20a%20systematic%20way%20to%20measure%20how%20representational%0Asimilarity%20between%20models%20varies%20with%20the%20set%20of%20stimuli%20used%20to%20construct%20the%0Arepresentations.%20We%20find%20that%20the%20objective%20function%20is%20a%20crucial%20factor%20in%0Adetermining%20the%20consistency%20of%20representational%20similarities%20across%20datasets.%0ASpecifically%2C%20self-supervised%20vision%20models%20learn%20representations%20whose%0Arelative%20pairwise%20similarities%20generalize%20better%20from%20one%20dataset%20to%20another%0Acompared%20to%20those%20of%20image%20classification%20or%20image-text%20models.%20Moreover%2C%20the%0Acorrespondence%20between%20representational%20similarities%20and%20the%20models%27%20task%0Abehavior%20is%20dataset-dependent%2C%20being%20most%20strongly%20pronounced%20for%20single-domain%0Adatasets.%20Our%20work%20provides%20a%20framework%20for%20analyzing%20similarities%20of%20model%0Arepresentations%20across%20datasets%20and%20linking%20those%20similarities%20to%20differences%0Ain%20task%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05561v2&entry.124074799=Read"},
{"title": "SViMo: Synchronized Diffusion for Video and Motion Generation in\n  Hand-object Interaction Scenarios", "author": "Lingwei Dang and Ruizhi Shao and Hongwen Zhang and Wei Min and Yebin Liu and Qingyao Wu", "abstract": "  Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at https://github.com/Droliven/SViMo\\_project.\n", "link": "http://arxiv.org/abs/2506.02444v2", "date": "2025-06-04", "relevancy": 2.641, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6932}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.639}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SViMo%3A%20Synchronized%20Diffusion%20for%20Video%20and%20Motion%20Generation%20in%0A%20%20Hand-object%20Interaction%20Scenarios&body=Title%3A%20SViMo%3A%20Synchronized%20Diffusion%20for%20Video%20and%20Motion%20Generation%20in%0A%20%20Hand-object%20Interaction%20Scenarios%0AAuthor%3A%20Lingwei%20Dang%20and%20Ruizhi%20Shao%20and%20Hongwen%20Zhang%20and%20Wei%20Min%20and%20Yebin%20Liu%20and%20Qingyao%20Wu%0AAbstract%3A%20%20%20Hand-Object%20Interaction%20%28HOI%29%20generation%20has%20significant%20application%0Apotential.%20However%2C%20current%203D%20HOI%20motion%20generation%20approaches%20heavily%20rely%20on%0Apredefined%203D%20object%20models%20and%20lab-captured%20motion%20data%2C%20limiting%0Ageneralization%20capabilities.%20Meanwhile%2C%20HOI%20video%20generation%20methods%20prioritize%0Apixel-level%20visual%20fidelity%2C%20often%20sacrificing%20physical%20plausibility.%0ARecognizing%20that%20visual%20appearance%20and%20motion%20patterns%20share%20fundamental%0Aphysical%20laws%20in%20the%20real%20world%2C%20we%20propose%20a%20novel%20framework%20that%20combines%0Avisual%20priors%20and%20dynamic%20constraints%20within%20a%20synchronized%20diffusion%20process%0Ato%20generate%20the%20HOI%20video%20and%20motion%20simultaneously.%20To%20integrate%20the%0Aheterogeneous%20semantics%2C%20appearance%2C%20and%20motion%20features%2C%20our%20method%20implements%0Atri-modal%20adaptive%20modulation%20for%20feature%20aligning%2C%20coupled%20with%203D%0Afull-attention%20for%20modeling%20inter-%20and%20intra-modal%20dependencies.%20Furthermore%2C%0Awe%20introduce%20a%20vision-aware%203D%20interaction%20diffusion%20model%20that%20generates%0Aexplicit%203D%20interaction%20sequences%20directly%20from%20the%20synchronized%20diffusion%0Aoutputs%2C%20then%20feeds%20them%20back%20to%20establish%20a%20closed-loop%20feedback%20cycle.%20This%0Aarchitecture%20eliminates%20dependencies%20on%20predefined%20object%20models%20or%20explicit%0Apose%20guidance%20while%20significantly%20enhancing%20video-motion%20consistency.%0AExperimental%20results%20demonstrate%20our%20method%27s%20superiority%20over%20state-of-the-art%0Aapproaches%20in%20generating%20high-fidelity%2C%20dynamically%20plausible%20HOI%20sequences%2C%0Awith%20notable%20generalization%20capabilities%20in%20unseen%20real-world%20scenarios.%0AProject%20page%20at%20https%3A//github.com/Droliven/SViMo%5C_project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSViMo%253A%2520Synchronized%2520Diffusion%2520for%2520Video%2520and%2520Motion%2520Generation%2520in%250A%2520%2520Hand-object%2520Interaction%2520Scenarios%26entry.906535625%3DLingwei%2520Dang%2520and%2520Ruizhi%2520Shao%2520and%2520Hongwen%2520Zhang%2520and%2520Wei%2520Min%2520and%2520Yebin%2520Liu%2520and%2520Qingyao%2520Wu%26entry.1292438233%3D%2520%2520Hand-Object%2520Interaction%2520%2528HOI%2529%2520generation%2520has%2520significant%2520application%250Apotential.%2520However%252C%2520current%25203D%2520HOI%2520motion%2520generation%2520approaches%2520heavily%2520rely%2520on%250Apredefined%25203D%2520object%2520models%2520and%2520lab-captured%2520motion%2520data%252C%2520limiting%250Ageneralization%2520capabilities.%2520Meanwhile%252C%2520HOI%2520video%2520generation%2520methods%2520prioritize%250Apixel-level%2520visual%2520fidelity%252C%2520often%2520sacrificing%2520physical%2520plausibility.%250ARecognizing%2520that%2520visual%2520appearance%2520and%2520motion%2520patterns%2520share%2520fundamental%250Aphysical%2520laws%2520in%2520the%2520real%2520world%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520combines%250Avisual%2520priors%2520and%2520dynamic%2520constraints%2520within%2520a%2520synchronized%2520diffusion%2520process%250Ato%2520generate%2520the%2520HOI%2520video%2520and%2520motion%2520simultaneously.%2520To%2520integrate%2520the%250Aheterogeneous%2520semantics%252C%2520appearance%252C%2520and%2520motion%2520features%252C%2520our%2520method%2520implements%250Atri-modal%2520adaptive%2520modulation%2520for%2520feature%2520aligning%252C%2520coupled%2520with%25203D%250Afull-attention%2520for%2520modeling%2520inter-%2520and%2520intra-modal%2520dependencies.%2520Furthermore%252C%250Awe%2520introduce%2520a%2520vision-aware%25203D%2520interaction%2520diffusion%2520model%2520that%2520generates%250Aexplicit%25203D%2520interaction%2520sequences%2520directly%2520from%2520the%2520synchronized%2520diffusion%250Aoutputs%252C%2520then%2520feeds%2520them%2520back%2520to%2520establish%2520a%2520closed-loop%2520feedback%2520cycle.%2520This%250Aarchitecture%2520eliminates%2520dependencies%2520on%2520predefined%2520object%2520models%2520or%2520explicit%250Apose%2520guidance%2520while%2520significantly%2520enhancing%2520video-motion%2520consistency.%250AExperimental%2520results%2520demonstrate%2520our%2520method%2527s%2520superiority%2520over%2520state-of-the-art%250Aapproaches%2520in%2520generating%2520high-fidelity%252C%2520dynamically%2520plausible%2520HOI%2520sequences%252C%250Awith%2520notable%2520generalization%2520capabilities%2520in%2520unseen%2520real-world%2520scenarios.%250AProject%2520page%2520at%2520https%253A//github.com/Droliven/SViMo%255C_project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SViMo%3A%20Synchronized%20Diffusion%20for%20Video%20and%20Motion%20Generation%20in%0A%20%20Hand-object%20Interaction%20Scenarios&entry.906535625=Lingwei%20Dang%20and%20Ruizhi%20Shao%20and%20Hongwen%20Zhang%20and%20Wei%20Min%20and%20Yebin%20Liu%20and%20Qingyao%20Wu&entry.1292438233=%20%20Hand-Object%20Interaction%20%28HOI%29%20generation%20has%20significant%20application%0Apotential.%20However%2C%20current%203D%20HOI%20motion%20generation%20approaches%20heavily%20rely%20on%0Apredefined%203D%20object%20models%20and%20lab-captured%20motion%20data%2C%20limiting%0Ageneralization%20capabilities.%20Meanwhile%2C%20HOI%20video%20generation%20methods%20prioritize%0Apixel-level%20visual%20fidelity%2C%20often%20sacrificing%20physical%20plausibility.%0ARecognizing%20that%20visual%20appearance%20and%20motion%20patterns%20share%20fundamental%0Aphysical%20laws%20in%20the%20real%20world%2C%20we%20propose%20a%20novel%20framework%20that%20combines%0Avisual%20priors%20and%20dynamic%20constraints%20within%20a%20synchronized%20diffusion%20process%0Ato%20generate%20the%20HOI%20video%20and%20motion%20simultaneously.%20To%20integrate%20the%0Aheterogeneous%20semantics%2C%20appearance%2C%20and%20motion%20features%2C%20our%20method%20implements%0Atri-modal%20adaptive%20modulation%20for%20feature%20aligning%2C%20coupled%20with%203D%0Afull-attention%20for%20modeling%20inter-%20and%20intra-modal%20dependencies.%20Furthermore%2C%0Awe%20introduce%20a%20vision-aware%203D%20interaction%20diffusion%20model%20that%20generates%0Aexplicit%203D%20interaction%20sequences%20directly%20from%20the%20synchronized%20diffusion%0Aoutputs%2C%20then%20feeds%20them%20back%20to%20establish%20a%20closed-loop%20feedback%20cycle.%20This%0Aarchitecture%20eliminates%20dependencies%20on%20predefined%20object%20models%20or%20explicit%0Apose%20guidance%20while%20significantly%20enhancing%20video-motion%20consistency.%0AExperimental%20results%20demonstrate%20our%20method%27s%20superiority%20over%20state-of-the-art%0Aapproaches%20in%20generating%20high-fidelity%2C%20dynamically%20plausible%20HOI%20sequences%2C%0Awith%20notable%20generalization%20capabilities%20in%20unseen%20real-world%20scenarios.%0AProject%20page%20at%20https%3A//github.com/Droliven/SViMo%5C_project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02444v2&entry.124074799=Read"},
{"title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at\n  Scale", "author": "Jarvis Guo and Tuney Zheng and Yuelin Bai and Bo Li and Yubo Wang and King Zhu and Yizhi Li and Graham Neubig and Wenhu Chen and Xiang Yue", "abstract": "  Open-source multimodal large language models (MLLMs) have shown significant\npotential in a broad range of multimodal tasks. However, their reasoning\ncapabilities remain constrained by existing instruction-tuning datasets, which\nwere predominately repurposed from academic datasets such as VQA, AI2D, and\nChartQA. These datasets target simplistic tasks, and only provide phrase-level\nanswers without any intermediate rationales. To address these challenges, we\nintroduce a scalable and cost-effective method to construct a large-scale\nmultimodal instruction-tuning dataset with rich intermediate rationales\ndesigned to elicit CoT reasoning. Using only open models, we create a dataset\ncontaining 12M instruction-response pairs to cover diverse, reasoning-intensive\ntasks with detailed and faithful rationales. Experiments demonstrate that\ntraining MLLMs on this dataset significantly improves reasoning capabilities,\nachieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%),\nMMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates\nnotable improvements of up to 4% on non-reasoning-based benchmarks. Ablation\nstudies further highlight the importance of key components, such as rewriting\nand self-filtering, in the dataset construction process.\n", "link": "http://arxiv.org/abs/2412.05237v2", "date": "2025-06-04", "relevancy": 2.6295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAmmoTH-VL%3A%20Eliciting%20Multimodal%20Reasoning%20with%20Instruction%20Tuning%20at%0A%20%20Scale&body=Title%3A%20MAmmoTH-VL%3A%20Eliciting%20Multimodal%20Reasoning%20with%20Instruction%20Tuning%20at%0A%20%20Scale%0AAuthor%3A%20Jarvis%20Guo%20and%20Tuney%20Zheng%20and%20Yuelin%20Bai%20and%20Bo%20Li%20and%20Yubo%20Wang%20and%20King%20Zhu%20and%20Yizhi%20Li%20and%20Graham%20Neubig%20and%20Wenhu%20Chen%20and%20Xiang%20Yue%0AAbstract%3A%20%20%20Open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20significant%0Apotential%20in%20a%20broad%20range%20of%20multimodal%20tasks.%20However%2C%20their%20reasoning%0Acapabilities%20remain%20constrained%20by%20existing%20instruction-tuning%20datasets%2C%20which%0Awere%20predominately%20repurposed%20from%20academic%20datasets%20such%20as%20VQA%2C%20AI2D%2C%20and%0AChartQA.%20These%20datasets%20target%20simplistic%20tasks%2C%20and%20only%20provide%20phrase-level%0Aanswers%20without%20any%20intermediate%20rationales.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20scalable%20and%20cost-effective%20method%20to%20construct%20a%20large-scale%0Amultimodal%20instruction-tuning%20dataset%20with%20rich%20intermediate%20rationales%0Adesigned%20to%20elicit%20CoT%20reasoning.%20Using%20only%20open%20models%2C%20we%20create%20a%20dataset%0Acontaining%2012M%20instruction-response%20pairs%20to%20cover%20diverse%2C%20reasoning-intensive%0Atasks%20with%20detailed%20and%20faithful%20rationales.%20Experiments%20demonstrate%20that%0Atraining%20MLLMs%20on%20this%20dataset%20significantly%20improves%20reasoning%20capabilities%2C%0Aachieving%20state-of-the-art%20performance%20on%20benchmarks%20such%20as%20MathVerse%20%28%2B8.1%25%29%2C%0AMMMU-Pro%20%28%2B7%25%29%2C%20and%20MuirBench%20%28%2B13.3%25%29.%20Additionally%2C%20the%20model%20demonstrates%0Anotable%20improvements%20of%20up%20to%204%25%20on%20non-reasoning-based%20benchmarks.%20Ablation%0Astudies%20further%20highlight%20the%20importance%20of%20key%20components%2C%20such%20as%20rewriting%0Aand%20self-filtering%2C%20in%20the%20dataset%20construction%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAmmoTH-VL%253A%2520Eliciting%2520Multimodal%2520Reasoning%2520with%2520Instruction%2520Tuning%2520at%250A%2520%2520Scale%26entry.906535625%3DJarvis%2520Guo%2520and%2520Tuney%2520Zheng%2520and%2520Yuelin%2520Bai%2520and%2520Bo%2520Li%2520and%2520Yubo%2520Wang%2520and%2520King%2520Zhu%2520and%2520Yizhi%2520Li%2520and%2520Graham%2520Neubig%2520and%2520Wenhu%2520Chen%2520and%2520Xiang%2520Yue%26entry.1292438233%3D%2520%2520Open-source%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520significant%250Apotential%2520in%2520a%2520broad%2520range%2520of%2520multimodal%2520tasks.%2520However%252C%2520their%2520reasoning%250Acapabilities%2520remain%2520constrained%2520by%2520existing%2520instruction-tuning%2520datasets%252C%2520which%250Awere%2520predominately%2520repurposed%2520from%2520academic%2520datasets%2520such%2520as%2520VQA%252C%2520AI2D%252C%2520and%250AChartQA.%2520These%2520datasets%2520target%2520simplistic%2520tasks%252C%2520and%2520only%2520provide%2520phrase-level%250Aanswers%2520without%2520any%2520intermediate%2520rationales.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520scalable%2520and%2520cost-effective%2520method%2520to%2520construct%2520a%2520large-scale%250Amultimodal%2520instruction-tuning%2520dataset%2520with%2520rich%2520intermediate%2520rationales%250Adesigned%2520to%2520elicit%2520CoT%2520reasoning.%2520Using%2520only%2520open%2520models%252C%2520we%2520create%2520a%2520dataset%250Acontaining%252012M%2520instruction-response%2520pairs%2520to%2520cover%2520diverse%252C%2520reasoning-intensive%250Atasks%2520with%2520detailed%2520and%2520faithful%2520rationales.%2520Experiments%2520demonstrate%2520that%250Atraining%2520MLLMs%2520on%2520this%2520dataset%2520significantly%2520improves%2520reasoning%2520capabilities%252C%250Aachieving%2520state-of-the-art%2520performance%2520on%2520benchmarks%2520such%2520as%2520MathVerse%2520%2528%252B8.1%2525%2529%252C%250AMMMU-Pro%2520%2528%252B7%2525%2529%252C%2520and%2520MuirBench%2520%2528%252B13.3%2525%2529.%2520Additionally%252C%2520the%2520model%2520demonstrates%250Anotable%2520improvements%2520of%2520up%2520to%25204%2525%2520on%2520non-reasoning-based%2520benchmarks.%2520Ablation%250Astudies%2520further%2520highlight%2520the%2520importance%2520of%2520key%2520components%252C%2520such%2520as%2520rewriting%250Aand%2520self-filtering%252C%2520in%2520the%2520dataset%2520construction%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAmmoTH-VL%3A%20Eliciting%20Multimodal%20Reasoning%20with%20Instruction%20Tuning%20at%0A%20%20Scale&entry.906535625=Jarvis%20Guo%20and%20Tuney%20Zheng%20and%20Yuelin%20Bai%20and%20Bo%20Li%20and%20Yubo%20Wang%20and%20King%20Zhu%20and%20Yizhi%20Li%20and%20Graham%20Neubig%20and%20Wenhu%20Chen%20and%20Xiang%20Yue&entry.1292438233=%20%20Open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20significant%0Apotential%20in%20a%20broad%20range%20of%20multimodal%20tasks.%20However%2C%20their%20reasoning%0Acapabilities%20remain%20constrained%20by%20existing%20instruction-tuning%20datasets%2C%20which%0Awere%20predominately%20repurposed%20from%20academic%20datasets%20such%20as%20VQA%2C%20AI2D%2C%20and%0AChartQA.%20These%20datasets%20target%20simplistic%20tasks%2C%20and%20only%20provide%20phrase-level%0Aanswers%20without%20any%20intermediate%20rationales.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20scalable%20and%20cost-effective%20method%20to%20construct%20a%20large-scale%0Amultimodal%20instruction-tuning%20dataset%20with%20rich%20intermediate%20rationales%0Adesigned%20to%20elicit%20CoT%20reasoning.%20Using%20only%20open%20models%2C%20we%20create%20a%20dataset%0Acontaining%2012M%20instruction-response%20pairs%20to%20cover%20diverse%2C%20reasoning-intensive%0Atasks%20with%20detailed%20and%20faithful%20rationales.%20Experiments%20demonstrate%20that%0Atraining%20MLLMs%20on%20this%20dataset%20significantly%20improves%20reasoning%20capabilities%2C%0Aachieving%20state-of-the-art%20performance%20on%20benchmarks%20such%20as%20MathVerse%20%28%2B8.1%25%29%2C%0AMMMU-Pro%20%28%2B7%25%29%2C%20and%20MuirBench%20%28%2B13.3%25%29.%20Additionally%2C%20the%20model%20demonstrates%0Anotable%20improvements%20of%20up%20to%204%25%20on%20non-reasoning-based%20benchmarks.%20Ablation%0Astudies%20further%20highlight%20the%20importance%20of%20key%20components%2C%20such%20as%20rewriting%0Aand%20self-filtering%2C%20in%20the%20dataset%20construction%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05237v2&entry.124074799=Read"},
{"title": "Prior Learning in Introspective VAEs", "author": "Ioannis Athanasiadis and Fredrik Lindsten and Michael Felsberg", "abstract": "  Variational Autoencoders (VAEs) are a popular framework for unsupervised\nlearning and data generation. A plethora of methods have been proposed focusing\non improving VAEs, with the incorporation of adversarial objectives and the\nintegration of prior learning mechanisms being prominent directions. When it\ncomes to the former, an indicative instance is the recently introduced family\nof Introspective VAEs aiming at ensuring that a low likelihood is assigned to\nunrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE),\none of only two members of the Introspective VAE family, the other being the\noriginal IntroVAE. We select S-IntroVAE for its state-of-the-art status and its\ntraining stability. In particular, we investigate the implication of\nincorporating a multimodal and trainable prior into this S-IntroVAE. Namely, we\nformulate the prior as a third player and show that when trained in cooperation\nwith the decoder constitutes an effective way for prior learning, which shares\nthe Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a\nmodified formulation of the optimal ELBO in S-IntroVAE, we develop\ntheoretically motivated regularizations, namely (i) adaptive variance clipping\nto stabilize training when learning the prior and (ii) responsibility\nregularization to discourage the formation of inactive prior modes. Finally, we\nperform a series of targeted experiments on a 2D density estimation benchmark\nand in an image generation setting comprised of the (F)-MNIST and CIFAR-10\ndatasets demonstrating the effect of prior learning in S-IntroVAE in generation\nand representation learning.\n", "link": "http://arxiv.org/abs/2408.13805v3", "date": "2025-06-04", "relevancy": 2.624, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior%20Learning%20in%20Introspective%20VAEs&body=Title%3A%20Prior%20Learning%20in%20Introspective%20VAEs%0AAuthor%3A%20Ioannis%20Athanasiadis%20and%20Fredrik%20Lindsten%20and%20Michael%20Felsberg%0AAbstract%3A%20%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20a%20popular%20framework%20for%20unsupervised%0Alearning%20and%20data%20generation.%20A%20plethora%20of%20methods%20have%20been%20proposed%20focusing%0Aon%20improving%20VAEs%2C%20with%20the%20incorporation%20of%20adversarial%20objectives%20and%20the%0Aintegration%20of%20prior%20learning%20mechanisms%20being%20prominent%20directions.%20When%20it%0Acomes%20to%20the%20former%2C%20an%20indicative%20instance%20is%20the%20recently%20introduced%20family%0Aof%20Introspective%20VAEs%20aiming%20at%20ensuring%20that%20a%20low%20likelihood%20is%20assigned%20to%0Aunrealistic%20samples.%20In%20this%20study%2C%20we%20focus%20on%20the%20Soft-IntroVAE%20%28S-IntroVAE%29%2C%0Aone%20of%20only%20two%20members%20of%20the%20Introspective%20VAE%20family%2C%20the%20other%20being%20the%0Aoriginal%20IntroVAE.%20We%20select%20S-IntroVAE%20for%20its%20state-of-the-art%20status%20and%20its%0Atraining%20stability.%20In%20particular%2C%20we%20investigate%20the%20implication%20of%0Aincorporating%20a%20multimodal%20and%20trainable%20prior%20into%20this%20S-IntroVAE.%20Namely%2C%20we%0Aformulate%20the%20prior%20as%20a%20third%20player%20and%20show%20that%20when%20trained%20in%20cooperation%0Awith%20the%20decoder%20constitutes%20an%20effective%20way%20for%20prior%20learning%2C%20which%20shares%0Athe%20Nash%20Equilibrium%20with%20the%20vanilla%20S-IntroVAE.%20Furthermore%2C%20based%20on%20a%0Amodified%20formulation%20of%20the%20optimal%20ELBO%20in%20S-IntroVAE%2C%20we%20develop%0Atheoretically%20motivated%20regularizations%2C%20namely%20%28i%29%20adaptive%20variance%20clipping%0Ato%20stabilize%20training%20when%20learning%20the%20prior%20and%20%28ii%29%20responsibility%0Aregularization%20to%20discourage%20the%20formation%20of%20inactive%20prior%20modes.%20Finally%2C%20we%0Aperform%20a%20series%20of%20targeted%20experiments%20on%20a%202D%20density%20estimation%20benchmark%0Aand%20in%20an%20image%20generation%20setting%20comprised%20of%20the%20%28F%29-MNIST%20and%20CIFAR-10%0Adatasets%20demonstrating%20the%20effect%20of%20prior%20learning%20in%20S-IntroVAE%20in%20generation%0Aand%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13805v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior%2520Learning%2520in%2520Introspective%2520VAEs%26entry.906535625%3DIoannis%2520Athanasiadis%2520and%2520Fredrik%2520Lindsten%2520and%2520Michael%2520Felsberg%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520a%2520popular%2520framework%2520for%2520unsupervised%250Alearning%2520and%2520data%2520generation.%2520A%2520plethora%2520of%2520methods%2520have%2520been%2520proposed%2520focusing%250Aon%2520improving%2520VAEs%252C%2520with%2520the%2520incorporation%2520of%2520adversarial%2520objectives%2520and%2520the%250Aintegration%2520of%2520prior%2520learning%2520mechanisms%2520being%2520prominent%2520directions.%2520When%2520it%250Acomes%2520to%2520the%2520former%252C%2520an%2520indicative%2520instance%2520is%2520the%2520recently%2520introduced%2520family%250Aof%2520Introspective%2520VAEs%2520aiming%2520at%2520ensuring%2520that%2520a%2520low%2520likelihood%2520is%2520assigned%2520to%250Aunrealistic%2520samples.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520the%2520Soft-IntroVAE%2520%2528S-IntroVAE%2529%252C%250Aone%2520of%2520only%2520two%2520members%2520of%2520the%2520Introspective%2520VAE%2520family%252C%2520the%2520other%2520being%2520the%250Aoriginal%2520IntroVAE.%2520We%2520select%2520S-IntroVAE%2520for%2520its%2520state-of-the-art%2520status%2520and%2520its%250Atraining%2520stability.%2520In%2520particular%252C%2520we%2520investigate%2520the%2520implication%2520of%250Aincorporating%2520a%2520multimodal%2520and%2520trainable%2520prior%2520into%2520this%2520S-IntroVAE.%2520Namely%252C%2520we%250Aformulate%2520the%2520prior%2520as%2520a%2520third%2520player%2520and%2520show%2520that%2520when%2520trained%2520in%2520cooperation%250Awith%2520the%2520decoder%2520constitutes%2520an%2520effective%2520way%2520for%2520prior%2520learning%252C%2520which%2520shares%250Athe%2520Nash%2520Equilibrium%2520with%2520the%2520vanilla%2520S-IntroVAE.%2520Furthermore%252C%2520based%2520on%2520a%250Amodified%2520formulation%2520of%2520the%2520optimal%2520ELBO%2520in%2520S-IntroVAE%252C%2520we%2520develop%250Atheoretically%2520motivated%2520regularizations%252C%2520namely%2520%2528i%2529%2520adaptive%2520variance%2520clipping%250Ato%2520stabilize%2520training%2520when%2520learning%2520the%2520prior%2520and%2520%2528ii%2529%2520responsibility%250Aregularization%2520to%2520discourage%2520the%2520formation%2520of%2520inactive%2520prior%2520modes.%2520Finally%252C%2520we%250Aperform%2520a%2520series%2520of%2520targeted%2520experiments%2520on%2520a%25202D%2520density%2520estimation%2520benchmark%250Aand%2520in%2520an%2520image%2520generation%2520setting%2520comprised%2520of%2520the%2520%2528F%2529-MNIST%2520and%2520CIFAR-10%250Adatasets%2520demonstrating%2520the%2520effect%2520of%2520prior%2520learning%2520in%2520S-IntroVAE%2520in%2520generation%250Aand%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13805v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior%20Learning%20in%20Introspective%20VAEs&entry.906535625=Ioannis%20Athanasiadis%20and%20Fredrik%20Lindsten%20and%20Michael%20Felsberg&entry.1292438233=%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20a%20popular%20framework%20for%20unsupervised%0Alearning%20and%20data%20generation.%20A%20plethora%20of%20methods%20have%20been%20proposed%20focusing%0Aon%20improving%20VAEs%2C%20with%20the%20incorporation%20of%20adversarial%20objectives%20and%20the%0Aintegration%20of%20prior%20learning%20mechanisms%20being%20prominent%20directions.%20When%20it%0Acomes%20to%20the%20former%2C%20an%20indicative%20instance%20is%20the%20recently%20introduced%20family%0Aof%20Introspective%20VAEs%20aiming%20at%20ensuring%20that%20a%20low%20likelihood%20is%20assigned%20to%0Aunrealistic%20samples.%20In%20this%20study%2C%20we%20focus%20on%20the%20Soft-IntroVAE%20%28S-IntroVAE%29%2C%0Aone%20of%20only%20two%20members%20of%20the%20Introspective%20VAE%20family%2C%20the%20other%20being%20the%0Aoriginal%20IntroVAE.%20We%20select%20S-IntroVAE%20for%20its%20state-of-the-art%20status%20and%20its%0Atraining%20stability.%20In%20particular%2C%20we%20investigate%20the%20implication%20of%0Aincorporating%20a%20multimodal%20and%20trainable%20prior%20into%20this%20S-IntroVAE.%20Namely%2C%20we%0Aformulate%20the%20prior%20as%20a%20third%20player%20and%20show%20that%20when%20trained%20in%20cooperation%0Awith%20the%20decoder%20constitutes%20an%20effective%20way%20for%20prior%20learning%2C%20which%20shares%0Athe%20Nash%20Equilibrium%20with%20the%20vanilla%20S-IntroVAE.%20Furthermore%2C%20based%20on%20a%0Amodified%20formulation%20of%20the%20optimal%20ELBO%20in%20S-IntroVAE%2C%20we%20develop%0Atheoretically%20motivated%20regularizations%2C%20namely%20%28i%29%20adaptive%20variance%20clipping%0Ato%20stabilize%20training%20when%20learning%20the%20prior%20and%20%28ii%29%20responsibility%0Aregularization%20to%20discourage%20the%20formation%20of%20inactive%20prior%20modes.%20Finally%2C%20we%0Aperform%20a%20series%20of%20targeted%20experiments%20on%20a%202D%20density%20estimation%20benchmark%0Aand%20in%20an%20image%20generation%20setting%20comprised%20of%20the%20%28F%29-MNIST%20and%20CIFAR-10%0Adatasets%20demonstrating%20the%20effect%20of%20prior%20learning%20in%20S-IntroVAE%20in%20generation%0Aand%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13805v3&entry.124074799=Read"},
{"title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for\n  Federated Learning", "author": "Luiz Manella Pereira and M. Hadi Amini", "abstract": "  Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds.\n", "link": "http://arxiv.org/abs/2506.04071v1", "date": "2025-06-04", "relevancy": 2.5975, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5315}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport-based%20Domain%20Alignment%20as%20a%20Preprocessing%20Step%20for%0A%20%20Federated%20Learning&body=Title%3A%20Optimal%20Transport-based%20Domain%20Alignment%20as%20a%20Preprocessing%20Step%20for%0A%20%20Federated%20Learning%0AAuthor%3A%20Luiz%20Manella%20Pereira%20and%20M.%20Hadi%20Amini%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20subfield%20of%20machine%20learning%20that%20avoids%20sharing%0Alocal%20data%20with%20a%20central%20server%2C%20which%20can%20enhance%20privacy%20and%20scalability.%0AThe%20inability%20to%20consolidate%20data%20leads%20to%20a%20unique%20problem%20called%20dataset%0Aimbalance%2C%20where%20agents%20in%20a%20network%20do%20not%20have%20equal%20representation%20of%20the%0Alabels%20one%20is%20trying%20to%20learn%20to%20predict.%20In%20FL%2C%20fusing%20locally-trained%20models%0Awith%20unbalanced%20datasets%20may%20deteriorate%20the%20performance%20of%20global%20model%0Aaggregation%2C%20and%20reduce%20the%20quality%20of%20updated%20local%20models%20and%20the%20accuracy%20of%0Athe%20distributed%20agents%27%20decisions.%20In%20this%20work%2C%20we%20introduce%20an%20Optimal%0ATransport-based%20preprocessing%20algorithm%20that%20aligns%20the%20datasets%20by%20minimizing%0Athe%20distributional%20discrepancy%20of%20data%20along%20the%20edge%20devices.%20We%20accomplish%0Athis%20by%20leveraging%20Wasserstein%20barycenters%20when%20computing%20channel-wise%0Aaverages.%20These%20barycenters%20are%20collected%20in%20a%20trusted%20central%20server%20where%0Athey%20collectively%20generate%20a%20target%20RGB%20space.%20By%20projecting%20our%20dataset%0Atowards%20this%20target%20space%2C%20we%20minimize%20the%20distributional%20discrepancy%20on%20a%0Aglobal%20level%2C%20which%20facilitates%20the%20learning%20process%20due%20to%20a%20minimization%20of%0Avariance%20across%20the%20samples.%20We%20demonstrate%20the%20capabilities%20of%20the%20proposed%0Aapproach%20over%20the%20CIFAR-10%20dataset%2C%20where%20we%20show%20its%20capability%20of%20reaching%0Ahigher%20degrees%20of%20generalization%20in%20fewer%20communication%20rounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Transport-based%2520Domain%2520Alignment%2520as%2520a%2520Preprocessing%2520Step%2520for%250A%2520%2520Federated%2520Learning%26entry.906535625%3DLuiz%2520Manella%2520Pereira%2520and%2520M.%2520Hadi%2520Amini%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520subfield%2520of%2520machine%2520learning%2520that%2520avoids%2520sharing%250Alocal%2520data%2520with%2520a%2520central%2520server%252C%2520which%2520can%2520enhance%2520privacy%2520and%2520scalability.%250AThe%2520inability%2520to%2520consolidate%2520data%2520leads%2520to%2520a%2520unique%2520problem%2520called%2520dataset%250Aimbalance%252C%2520where%2520agents%2520in%2520a%2520network%2520do%2520not%2520have%2520equal%2520representation%2520of%2520the%250Alabels%2520one%2520is%2520trying%2520to%2520learn%2520to%2520predict.%2520In%2520FL%252C%2520fusing%2520locally-trained%2520models%250Awith%2520unbalanced%2520datasets%2520may%2520deteriorate%2520the%2520performance%2520of%2520global%2520model%250Aaggregation%252C%2520and%2520reduce%2520the%2520quality%2520of%2520updated%2520local%2520models%2520and%2520the%2520accuracy%2520of%250Athe%2520distributed%2520agents%2527%2520decisions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520Optimal%250ATransport-based%2520preprocessing%2520algorithm%2520that%2520aligns%2520the%2520datasets%2520by%2520minimizing%250Athe%2520distributional%2520discrepancy%2520of%2520data%2520along%2520the%2520edge%2520devices.%2520We%2520accomplish%250Athis%2520by%2520leveraging%2520Wasserstein%2520barycenters%2520when%2520computing%2520channel-wise%250Aaverages.%2520These%2520barycenters%2520are%2520collected%2520in%2520a%2520trusted%2520central%2520server%2520where%250Athey%2520collectively%2520generate%2520a%2520target%2520RGB%2520space.%2520By%2520projecting%2520our%2520dataset%250Atowards%2520this%2520target%2520space%252C%2520we%2520minimize%2520the%2520distributional%2520discrepancy%2520on%2520a%250Aglobal%2520level%252C%2520which%2520facilitates%2520the%2520learning%2520process%2520due%2520to%2520a%2520minimization%2520of%250Avariance%2520across%2520the%2520samples.%2520We%2520demonstrate%2520the%2520capabilities%2520of%2520the%2520proposed%250Aapproach%2520over%2520the%2520CIFAR-10%2520dataset%252C%2520where%2520we%2520show%2520its%2520capability%2520of%2520reaching%250Ahigher%2520degrees%2520of%2520generalization%2520in%2520fewer%2520communication%2520rounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport-based%20Domain%20Alignment%20as%20a%20Preprocessing%20Step%20for%0A%20%20Federated%20Learning&entry.906535625=Luiz%20Manella%20Pereira%20and%20M.%20Hadi%20Amini&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20subfield%20of%20machine%20learning%20that%20avoids%20sharing%0Alocal%20data%20with%20a%20central%20server%2C%20which%20can%20enhance%20privacy%20and%20scalability.%0AThe%20inability%20to%20consolidate%20data%20leads%20to%20a%20unique%20problem%20called%20dataset%0Aimbalance%2C%20where%20agents%20in%20a%20network%20do%20not%20have%20equal%20representation%20of%20the%0Alabels%20one%20is%20trying%20to%20learn%20to%20predict.%20In%20FL%2C%20fusing%20locally-trained%20models%0Awith%20unbalanced%20datasets%20may%20deteriorate%20the%20performance%20of%20global%20model%0Aaggregation%2C%20and%20reduce%20the%20quality%20of%20updated%20local%20models%20and%20the%20accuracy%20of%0Athe%20distributed%20agents%27%20decisions.%20In%20this%20work%2C%20we%20introduce%20an%20Optimal%0ATransport-based%20preprocessing%20algorithm%20that%20aligns%20the%20datasets%20by%20minimizing%0Athe%20distributional%20discrepancy%20of%20data%20along%20the%20edge%20devices.%20We%20accomplish%0Athis%20by%20leveraging%20Wasserstein%20barycenters%20when%20computing%20channel-wise%0Aaverages.%20These%20barycenters%20are%20collected%20in%20a%20trusted%20central%20server%20where%0Athey%20collectively%20generate%20a%20target%20RGB%20space.%20By%20projecting%20our%20dataset%0Atowards%20this%20target%20space%2C%20we%20minimize%20the%20distributional%20discrepancy%20on%20a%0Aglobal%20level%2C%20which%20facilitates%20the%20learning%20process%20due%20to%20a%20minimization%20of%0Avariance%20across%20the%20samples.%20We%20demonstrate%20the%20capabilities%20of%20the%20proposed%0Aapproach%20over%20the%20CIFAR-10%20dataset%2C%20where%20we%20show%20its%20capability%20of%20reaching%0Ahigher%20degrees%20of%20generalization%20in%20fewer%20communication%20rounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04071v1&entry.124074799=Read"},
{"title": "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian\n  Sketches", "author": "Omri Lev and Vishwak Srinivasan and Moshe Shenfeld and Katrina Ligett and Ayush Sekhari and Ashia C. Wilson", "abstract": "  Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime.\n", "link": "http://arxiv.org/abs/2505.24603v2", "date": "2025-06-04", "relevancy": 2.5555, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5336}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5129}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches&body=Title%3A%20The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches%0AAuthor%3A%20Omri%20Lev%20and%20Vishwak%20Srinivasan%20and%20Moshe%20Shenfeld%20and%20Katrina%20Ligett%20and%20Ayush%20Sekhari%20and%20Ashia%20C.%20Wilson%0AAbstract%3A%20%20%20Gaussian%20sketching%2C%20which%20consists%20of%20pre-multiplying%20the%20data%20with%20a%20random%0AGaussian%20matrix%2C%20is%20a%20widely%20used%20technique%20for%20multiple%20problems%20in%20data%0Ascience%20and%20machine%20learning%2C%20with%20applications%20spanning%20computationally%0Aefficient%20optimization%2C%20coded%20computing%2C%20and%20federated%20learning.%20This%20operation%0Aalso%20provides%20differential%20privacy%20guarantees%20due%20to%20its%20inherent%20randomness.%0AIn%20this%20work%2C%20we%20revisit%20this%20operation%20through%20the%20lens%20of%20Renyi%20Differential%0APrivacy%20%28RDP%29%2C%20providing%20a%20refined%20privacy%20analysis%20that%20yields%20significantly%0Atighter%20bounds%20than%20prior%20results.%20We%20then%20demonstrate%20how%20this%20improved%0Aanalysis%20leads%20to%20performance%20improvement%20in%20different%20linear%20regression%0Asettings%2C%20establishing%20theoretical%20utility%20guarantees.%20Empirically%2C%20our%20methods%0Aimprove%20performance%20across%20multiple%20datasets%20and%2C%20in%20several%20cases%2C%20reduce%0Aruntime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Gaussian%2520Mixing%2520Mechanism%253A%2520Renyi%2520Differential%2520Privacy%2520via%2520Gaussian%250A%2520%2520Sketches%26entry.906535625%3DOmri%2520Lev%2520and%2520Vishwak%2520Srinivasan%2520and%2520Moshe%2520Shenfeld%2520and%2520Katrina%2520Ligett%2520and%2520Ayush%2520Sekhari%2520and%2520Ashia%2520C.%2520Wilson%26entry.1292438233%3D%2520%2520Gaussian%2520sketching%252C%2520which%2520consists%2520of%2520pre-multiplying%2520the%2520data%2520with%2520a%2520random%250AGaussian%2520matrix%252C%2520is%2520a%2520widely%2520used%2520technique%2520for%2520multiple%2520problems%2520in%2520data%250Ascience%2520and%2520machine%2520learning%252C%2520with%2520applications%2520spanning%2520computationally%250Aefficient%2520optimization%252C%2520coded%2520computing%252C%2520and%2520federated%2520learning.%2520This%2520operation%250Aalso%2520provides%2520differential%2520privacy%2520guarantees%2520due%2520to%2520its%2520inherent%2520randomness.%250AIn%2520this%2520work%252C%2520we%2520revisit%2520this%2520operation%2520through%2520the%2520lens%2520of%2520Renyi%2520Differential%250APrivacy%2520%2528RDP%2529%252C%2520providing%2520a%2520refined%2520privacy%2520analysis%2520that%2520yields%2520significantly%250Atighter%2520bounds%2520than%2520prior%2520results.%2520We%2520then%2520demonstrate%2520how%2520this%2520improved%250Aanalysis%2520leads%2520to%2520performance%2520improvement%2520in%2520different%2520linear%2520regression%250Asettings%252C%2520establishing%2520theoretical%2520utility%2520guarantees.%2520Empirically%252C%2520our%2520methods%250Aimprove%2520performance%2520across%2520multiple%2520datasets%2520and%252C%2520in%2520several%2520cases%252C%2520reduce%250Aruntime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Gaussian%20Mixing%20Mechanism%3A%20Renyi%20Differential%20Privacy%20via%20Gaussian%0A%20%20Sketches&entry.906535625=Omri%20Lev%20and%20Vishwak%20Srinivasan%20and%20Moshe%20Shenfeld%20and%20Katrina%20Ligett%20and%20Ayush%20Sekhari%20and%20Ashia%20C.%20Wilson&entry.1292438233=%20%20Gaussian%20sketching%2C%20which%20consists%20of%20pre-multiplying%20the%20data%20with%20a%20random%0AGaussian%20matrix%2C%20is%20a%20widely%20used%20technique%20for%20multiple%20problems%20in%20data%0Ascience%20and%20machine%20learning%2C%20with%20applications%20spanning%20computationally%0Aefficient%20optimization%2C%20coded%20computing%2C%20and%20federated%20learning.%20This%20operation%0Aalso%20provides%20differential%20privacy%20guarantees%20due%20to%20its%20inherent%20randomness.%0AIn%20this%20work%2C%20we%20revisit%20this%20operation%20through%20the%20lens%20of%20Renyi%20Differential%0APrivacy%20%28RDP%29%2C%20providing%20a%20refined%20privacy%20analysis%20that%20yields%20significantly%0Atighter%20bounds%20than%20prior%20results.%20We%20then%20demonstrate%20how%20this%20improved%0Aanalysis%20leads%20to%20performance%20improvement%20in%20different%20linear%20regression%0Asettings%2C%20establishing%20theoretical%20utility%20guarantees.%20Empirically%2C%20our%20methods%0Aimprove%20performance%20across%20multiple%20datasets%20and%2C%20in%20several%20cases%2C%20reduce%0Aruntime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24603v2&entry.124074799=Read"},
{"title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics\n  in the Billion Parameter Era", "author": "Dan Oneata and Desmond Elliott and Stella Frank", "abstract": "  Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.\n", "link": "http://arxiv.org/abs/2506.03994v1", "date": "2025-06-04", "relevancy": 2.5439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20What%20Tastes%20Good%3A%20Revisiting%20Multimodal%20Distributional%20Semantics%0A%20%20in%20the%20Billion%20Parameter%20Era&body=Title%3A%20Seeing%20What%20Tastes%20Good%3A%20Revisiting%20Multimodal%20Distributional%20Semantics%0A%20%20in%20the%20Billion%20Parameter%20Era%0AAuthor%3A%20Dan%20Oneata%20and%20Desmond%20Elliott%20and%20Stella%20Frank%0AAbstract%3A%20%20%20Human%20learning%20and%20conceptual%20representation%20is%20grounded%20in%20sensorimotor%0Aexperience%2C%20in%20contrast%20to%20state-of-the-art%20foundation%20models.%20In%20this%20paper%2C%0Awe%20investigate%20how%20well%20such%20large-scale%20models%2C%20trained%20on%20vast%20quantities%20of%0Adata%2C%20represent%20the%20semantic%20feature%20norms%20of%20concrete%20object%20concepts%2C%20e.g.%20a%0AROSE%20is%20red%2C%20smells%20sweet%2C%20and%20is%20a%20flower.%20More%20specifically%2C%20we%20use%20probing%0Atasks%20to%20test%20which%20properties%20of%20objects%20these%20models%20are%20aware%20of.%20We%0Aevaluate%20image%20encoders%20trained%20on%20image%20data%20alone%2C%20as%20well%20as%0Amultimodally-trained%20image%20encoders%20and%20language-only%20models%2C%20on%20predicting%20an%0Aextended%20denser%20version%20of%20the%20classic%20McRae%20norms%20and%20the%20newer%20Binder%20dataset%0Aof%20attribute%20ratings.%20We%20find%20that%20multimodal%20image%20encoders%20slightly%0Aoutperform%20language-only%20approaches%2C%20and%20that%20image-only%20encoders%20perform%0Acomparably%20to%20the%20language%20models%2C%20even%20on%20non-visual%20attributes%20that%20are%0Aclassified%20as%20%22encyclopedic%22%20or%20%22function%22.%20These%20results%20offer%20new%20insights%0Ainto%20what%20can%20be%20learned%20from%20pure%20unimodal%20learning%2C%20and%20the%20complementarity%0Aof%20the%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520What%2520Tastes%2520Good%253A%2520Revisiting%2520Multimodal%2520Distributional%2520Semantics%250A%2520%2520in%2520the%2520Billion%2520Parameter%2520Era%26entry.906535625%3DDan%2520Oneata%2520and%2520Desmond%2520Elliott%2520and%2520Stella%2520Frank%26entry.1292438233%3D%2520%2520Human%2520learning%2520and%2520conceptual%2520representation%2520is%2520grounded%2520in%2520sensorimotor%250Aexperience%252C%2520in%2520contrast%2520to%2520state-of-the-art%2520foundation%2520models.%2520In%2520this%2520paper%252C%250Awe%2520investigate%2520how%2520well%2520such%2520large-scale%2520models%252C%2520trained%2520on%2520vast%2520quantities%2520of%250Adata%252C%2520represent%2520the%2520semantic%2520feature%2520norms%2520of%2520concrete%2520object%2520concepts%252C%2520e.g.%2520a%250AROSE%2520is%2520red%252C%2520smells%2520sweet%252C%2520and%2520is%2520a%2520flower.%2520More%2520specifically%252C%2520we%2520use%2520probing%250Atasks%2520to%2520test%2520which%2520properties%2520of%2520objects%2520these%2520models%2520are%2520aware%2520of.%2520We%250Aevaluate%2520image%2520encoders%2520trained%2520on%2520image%2520data%2520alone%252C%2520as%2520well%2520as%250Amultimodally-trained%2520image%2520encoders%2520and%2520language-only%2520models%252C%2520on%2520predicting%2520an%250Aextended%2520denser%2520version%2520of%2520the%2520classic%2520McRae%2520norms%2520and%2520the%2520newer%2520Binder%2520dataset%250Aof%2520attribute%2520ratings.%2520We%2520find%2520that%2520multimodal%2520image%2520encoders%2520slightly%250Aoutperform%2520language-only%2520approaches%252C%2520and%2520that%2520image-only%2520encoders%2520perform%250Acomparably%2520to%2520the%2520language%2520models%252C%2520even%2520on%2520non-visual%2520attributes%2520that%2520are%250Aclassified%2520as%2520%2522encyclopedic%2522%2520or%2520%2522function%2522.%2520These%2520results%2520offer%2520new%2520insights%250Ainto%2520what%2520can%2520be%2520learned%2520from%2520pure%2520unimodal%2520learning%252C%2520and%2520the%2520complementarity%250Aof%2520the%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20What%20Tastes%20Good%3A%20Revisiting%20Multimodal%20Distributional%20Semantics%0A%20%20in%20the%20Billion%20Parameter%20Era&entry.906535625=Dan%20Oneata%20and%20Desmond%20Elliott%20and%20Stella%20Frank&entry.1292438233=%20%20Human%20learning%20and%20conceptual%20representation%20is%20grounded%20in%20sensorimotor%0Aexperience%2C%20in%20contrast%20to%20state-of-the-art%20foundation%20models.%20In%20this%20paper%2C%0Awe%20investigate%20how%20well%20such%20large-scale%20models%2C%20trained%20on%20vast%20quantities%20of%0Adata%2C%20represent%20the%20semantic%20feature%20norms%20of%20concrete%20object%20concepts%2C%20e.g.%20a%0AROSE%20is%20red%2C%20smells%20sweet%2C%20and%20is%20a%20flower.%20More%20specifically%2C%20we%20use%20probing%0Atasks%20to%20test%20which%20properties%20of%20objects%20these%20models%20are%20aware%20of.%20We%0Aevaluate%20image%20encoders%20trained%20on%20image%20data%20alone%2C%20as%20well%20as%0Amultimodally-trained%20image%20encoders%20and%20language-only%20models%2C%20on%20predicting%20an%0Aextended%20denser%20version%20of%20the%20classic%20McRae%20norms%20and%20the%20newer%20Binder%20dataset%0Aof%20attribute%20ratings.%20We%20find%20that%20multimodal%20image%20encoders%20slightly%0Aoutperform%20language-only%20approaches%2C%20and%20that%20image-only%20encoders%20perform%0Acomparably%20to%20the%20language%20models%2C%20even%20on%20non-visual%20attributes%20that%20are%0Aclassified%20as%20%22encyclopedic%22%20or%20%22function%22.%20These%20results%20offer%20new%20insights%0Ainto%20what%20can%20be%20learned%20from%20pure%20unimodal%20learning%2C%20and%20the%20complementarity%0Aof%20the%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03994v1&entry.124074799=Read"},
{"title": "FlexiReg: Flexible Urban Region Representation Learning", "author": "Fengze Sun and Yanchuan Chang and Egemen Tanin and Shanika Karunasekera and Jianzhong Qi", "abstract": "  The increasing availability of urban data offers new opportunities for\nlearning region representations, which can be used as input to machine learning\nmodels for downstream tasks such as check-in or crime prediction. While\nexisting solutions have produced promising results, an issue is their fixed\nformation of regions and fixed input region features, which may not suit the\nneeds of different downstream tasks. To address this limitation, we propose a\nmodel named FlexiReg for urban region representation learning that is flexible\nwith both the formation of urban regions and the input region features.\nFlexiReg is based on a spatial grid partitioning over the spatial area of\ninterest. It learns representations for the grid cells, leveraging publicly\naccessible data, including POI, land use, satellite imagery, and street view\nimagery. We propose adaptive aggregation to fuse the cell representations and\nprompt learning techniques to tailor the representations towards different\ntasks, addressing the needs of varying formations of urban regions and\ndownstream tasks. Extensive experiments on five real-world datasets demonstrate\nthat FlexiReg outperforms state-of-the-art models by up to 202% in term of the\naccuracy of four diverse downstream tasks using the produced urban region\nrepresentations.\n", "link": "http://arxiv.org/abs/2503.09128v3", "date": "2025-06-04", "relevancy": 2.5432, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5164}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning&body=Title%3A%20FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning%0AAuthor%3A%20Fengze%20Sun%20and%20Yanchuan%20Chang%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Jianzhong%20Qi%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20urban%20data%20offers%20new%20opportunities%20for%0Alearning%20region%20representations%2C%20which%20can%20be%20used%20as%20input%20to%20machine%20learning%0Amodels%20for%20downstream%20tasks%20such%20as%20check-in%20or%20crime%20prediction.%20While%0Aexisting%20solutions%20have%20produced%20promising%20results%2C%20an%20issue%20is%20their%20fixed%0Aformation%20of%20regions%20and%20fixed%20input%20region%20features%2C%20which%20may%20not%20suit%20the%0Aneeds%20of%20different%20downstream%20tasks.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Amodel%20named%20FlexiReg%20for%20urban%20region%20representation%20learning%20that%20is%20flexible%0Awith%20both%20the%20formation%20of%20urban%20regions%20and%20the%20input%20region%20features.%0AFlexiReg%20is%20based%20on%20a%20spatial%20grid%20partitioning%20over%20the%20spatial%20area%20of%0Ainterest.%20It%20learns%20representations%20for%20the%20grid%20cells%2C%20leveraging%20publicly%0Aaccessible%20data%2C%20including%20POI%2C%20land%20use%2C%20satellite%20imagery%2C%20and%20street%20view%0Aimagery.%20We%20propose%20adaptive%20aggregation%20to%20fuse%20the%20cell%20representations%20and%0Aprompt%20learning%20techniques%20to%20tailor%20the%20representations%20towards%20different%0Atasks%2C%20addressing%20the%20needs%20of%20varying%20formations%20of%20urban%20regions%20and%0Adownstream%20tasks.%20Extensive%20experiments%20on%20five%20real-world%20datasets%20demonstrate%0Athat%20FlexiReg%20outperforms%20state-of-the-art%20models%20by%20up%20to%20202%25%20in%20term%20of%20the%0Aaccuracy%20of%20four%20diverse%20downstream%20tasks%20using%20the%20produced%20urban%20region%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09128v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiReg%253A%2520Flexible%2520Urban%2520Region%2520Representation%2520Learning%26entry.906535625%3DFengze%2520Sun%2520and%2520Yanchuan%2520Chang%2520and%2520Egemen%2520Tanin%2520and%2520Shanika%2520Karunasekera%2520and%2520Jianzhong%2520Qi%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520urban%2520data%2520offers%2520new%2520opportunities%2520for%250Alearning%2520region%2520representations%252C%2520which%2520can%2520be%2520used%2520as%2520input%2520to%2520machine%2520learning%250Amodels%2520for%2520downstream%2520tasks%2520such%2520as%2520check-in%2520or%2520crime%2520prediction.%2520While%250Aexisting%2520solutions%2520have%2520produced%2520promising%2520results%252C%2520an%2520issue%2520is%2520their%2520fixed%250Aformation%2520of%2520regions%2520and%2520fixed%2520input%2520region%2520features%252C%2520which%2520may%2520not%2520suit%2520the%250Aneeds%2520of%2520different%2520downstream%2520tasks.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Amodel%2520named%2520FlexiReg%2520for%2520urban%2520region%2520representation%2520learning%2520that%2520is%2520flexible%250Awith%2520both%2520the%2520formation%2520of%2520urban%2520regions%2520and%2520the%2520input%2520region%2520features.%250AFlexiReg%2520is%2520based%2520on%2520a%2520spatial%2520grid%2520partitioning%2520over%2520the%2520spatial%2520area%2520of%250Ainterest.%2520It%2520learns%2520representations%2520for%2520the%2520grid%2520cells%252C%2520leveraging%2520publicly%250Aaccessible%2520data%252C%2520including%2520POI%252C%2520land%2520use%252C%2520satellite%2520imagery%252C%2520and%2520street%2520view%250Aimagery.%2520We%2520propose%2520adaptive%2520aggregation%2520to%2520fuse%2520the%2520cell%2520representations%2520and%250Aprompt%2520learning%2520techniques%2520to%2520tailor%2520the%2520representations%2520towards%2520different%250Atasks%252C%2520addressing%2520the%2520needs%2520of%2520varying%2520formations%2520of%2520urban%2520regions%2520and%250Adownstream%2520tasks.%2520Extensive%2520experiments%2520on%2520five%2520real-world%2520datasets%2520demonstrate%250Athat%2520FlexiReg%2520outperforms%2520state-of-the-art%2520models%2520by%2520up%2520to%2520202%2525%2520in%2520term%2520of%2520the%250Aaccuracy%2520of%2520four%2520diverse%2520downstream%2520tasks%2520using%2520the%2520produced%2520urban%2520region%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09128v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiReg%3A%20Flexible%20Urban%20Region%20Representation%20Learning&entry.906535625=Fengze%20Sun%20and%20Yanchuan%20Chang%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Jianzhong%20Qi&entry.1292438233=%20%20The%20increasing%20availability%20of%20urban%20data%20offers%20new%20opportunities%20for%0Alearning%20region%20representations%2C%20which%20can%20be%20used%20as%20input%20to%20machine%20learning%0Amodels%20for%20downstream%20tasks%20such%20as%20check-in%20or%20crime%20prediction.%20While%0Aexisting%20solutions%20have%20produced%20promising%20results%2C%20an%20issue%20is%20their%20fixed%0Aformation%20of%20regions%20and%20fixed%20input%20region%20features%2C%20which%20may%20not%20suit%20the%0Aneeds%20of%20different%20downstream%20tasks.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Amodel%20named%20FlexiReg%20for%20urban%20region%20representation%20learning%20that%20is%20flexible%0Awith%20both%20the%20formation%20of%20urban%20regions%20and%20the%20input%20region%20features.%0AFlexiReg%20is%20based%20on%20a%20spatial%20grid%20partitioning%20over%20the%20spatial%20area%20of%0Ainterest.%20It%20learns%20representations%20for%20the%20grid%20cells%2C%20leveraging%20publicly%0Aaccessible%20data%2C%20including%20POI%2C%20land%20use%2C%20satellite%20imagery%2C%20and%20street%20view%0Aimagery.%20We%20propose%20adaptive%20aggregation%20to%20fuse%20the%20cell%20representations%20and%0Aprompt%20learning%20techniques%20to%20tailor%20the%20representations%20towards%20different%0Atasks%2C%20addressing%20the%20needs%20of%20varying%20formations%20of%20urban%20regions%20and%0Adownstream%20tasks.%20Extensive%20experiments%20on%20five%20real-world%20datasets%20demonstrate%0Athat%20FlexiReg%20outperforms%20state-of-the-art%20models%20by%20up%20to%20202%25%20in%20term%20of%20the%0Aaccuracy%20of%20four%20diverse%20downstream%20tasks%20using%20the%20produced%20urban%20region%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09128v3&entry.124074799=Read"},
{"title": "On the Usage of Gaussian Process for Efficient Data Valuation", "author": "Cl\u00e9ment B\u00e9nesse and Patrick Mesana and Ath\u00e9na\u00efs Gautier and S\u00e9bastien Gambs", "abstract": "  In machine learning, knowing the impact of a given datum on model training is\na fundamental task referred to as Data Valuation. Building on previous works\nfrom the literature, we have designed a novel canonical decomposition allowing\npractitioners to analyze any data valuation method as the combination of two\nparts: a utility function that captures characteristics from a given model and\nan aggregation procedure that merges such information. We also propose to use\nGaussian Processes as a means to easily access the utility function on\n``sub-models'', which are models trained on a subset of the training set. The\nstrength of our approach stems from both its theoretical grounding in Bayesian\ntheory, and its practical reach, by enabling fast estimation of valuations\nthanks to efficient update formulae.\n", "link": "http://arxiv.org/abs/2506.04026v1", "date": "2025-06-04", "relevancy": 2.5424, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5239}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5069}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Usage%20of%20Gaussian%20Process%20for%20Efficient%20Data%20Valuation&body=Title%3A%20On%20the%20Usage%20of%20Gaussian%20Process%20for%20Efficient%20Data%20Valuation%0AAuthor%3A%20Cl%C3%A9ment%20B%C3%A9nesse%20and%20Patrick%20Mesana%20and%20Ath%C3%A9na%C3%AFs%20Gautier%20and%20S%C3%A9bastien%20Gambs%0AAbstract%3A%20%20%20In%20machine%20learning%2C%20knowing%20the%20impact%20of%20a%20given%20datum%20on%20model%20training%20is%0Aa%20fundamental%20task%20referred%20to%20as%20Data%20Valuation.%20Building%20on%20previous%20works%0Afrom%20the%20literature%2C%20we%20have%20designed%20a%20novel%20canonical%20decomposition%20allowing%0Apractitioners%20to%20analyze%20any%20data%20valuation%20method%20as%20the%20combination%20of%20two%0Aparts%3A%20a%20utility%20function%20that%20captures%20characteristics%20from%20a%20given%20model%20and%0Aan%20aggregation%20procedure%20that%20merges%20such%20information.%20We%20also%20propose%20to%20use%0AGaussian%20Processes%20as%20a%20means%20to%20easily%20access%20the%20utility%20function%20on%0A%60%60sub-models%27%27%2C%20which%20are%20models%20trained%20on%20a%20subset%20of%20the%20training%20set.%20The%0Astrength%20of%20our%20approach%20stems%20from%20both%20its%20theoretical%20grounding%20in%20Bayesian%0Atheory%2C%20and%20its%20practical%20reach%2C%20by%20enabling%20fast%20estimation%20of%20valuations%0Athanks%20to%20efficient%20update%20formulae.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Usage%2520of%2520Gaussian%2520Process%2520for%2520Efficient%2520Data%2520Valuation%26entry.906535625%3DCl%25C3%25A9ment%2520B%25C3%25A9nesse%2520and%2520Patrick%2520Mesana%2520and%2520Ath%25C3%25A9na%25C3%25AFs%2520Gautier%2520and%2520S%25C3%25A9bastien%2520Gambs%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%252C%2520knowing%2520the%2520impact%2520of%2520a%2520given%2520datum%2520on%2520model%2520training%2520is%250Aa%2520fundamental%2520task%2520referred%2520to%2520as%2520Data%2520Valuation.%2520Building%2520on%2520previous%2520works%250Afrom%2520the%2520literature%252C%2520we%2520have%2520designed%2520a%2520novel%2520canonical%2520decomposition%2520allowing%250Apractitioners%2520to%2520analyze%2520any%2520data%2520valuation%2520method%2520as%2520the%2520combination%2520of%2520two%250Aparts%253A%2520a%2520utility%2520function%2520that%2520captures%2520characteristics%2520from%2520a%2520given%2520model%2520and%250Aan%2520aggregation%2520procedure%2520that%2520merges%2520such%2520information.%2520We%2520also%2520propose%2520to%2520use%250AGaussian%2520Processes%2520as%2520a%2520means%2520to%2520easily%2520access%2520the%2520utility%2520function%2520on%250A%2560%2560sub-models%2527%2527%252C%2520which%2520are%2520models%2520trained%2520on%2520a%2520subset%2520of%2520the%2520training%2520set.%2520The%250Astrength%2520of%2520our%2520approach%2520stems%2520from%2520both%2520its%2520theoretical%2520grounding%2520in%2520Bayesian%250Atheory%252C%2520and%2520its%2520practical%2520reach%252C%2520by%2520enabling%2520fast%2520estimation%2520of%2520valuations%250Athanks%2520to%2520efficient%2520update%2520formulae.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Usage%20of%20Gaussian%20Process%20for%20Efficient%20Data%20Valuation&entry.906535625=Cl%C3%A9ment%20B%C3%A9nesse%20and%20Patrick%20Mesana%20and%20Ath%C3%A9na%C3%AFs%20Gautier%20and%20S%C3%A9bastien%20Gambs&entry.1292438233=%20%20In%20machine%20learning%2C%20knowing%20the%20impact%20of%20a%20given%20datum%20on%20model%20training%20is%0Aa%20fundamental%20task%20referred%20to%20as%20Data%20Valuation.%20Building%20on%20previous%20works%0Afrom%20the%20literature%2C%20we%20have%20designed%20a%20novel%20canonical%20decomposition%20allowing%0Apractitioners%20to%20analyze%20any%20data%20valuation%20method%20as%20the%20combination%20of%20two%0Aparts%3A%20a%20utility%20function%20that%20captures%20characteristics%20from%20a%20given%20model%20and%0Aan%20aggregation%20procedure%20that%20merges%20such%20information.%20We%20also%20propose%20to%20use%0AGaussian%20Processes%20as%20a%20means%20to%20easily%20access%20the%20utility%20function%20on%0A%60%60sub-models%27%27%2C%20which%20are%20models%20trained%20on%20a%20subset%20of%20the%20training%20set.%20The%0Astrength%20of%20our%20approach%20stems%20from%20both%20its%20theoretical%20grounding%20in%20Bayesian%0Atheory%2C%20and%20its%20practical%20reach%2C%20by%20enabling%20fast%20estimation%20of%20valuations%0Athanks%20to%20efficient%20update%20formulae.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04026v1&entry.124074799=Read"},
{"title": "TracLLM: A Generic Framework for Attributing Long Context LLMs", "author": "Yanting Wang and Wei Zou and Runpeng Geng and Jinyuan Jia", "abstract": "  Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.\n", "link": "http://arxiv.org/abs/2506.04202v1", "date": "2025-06-04", "relevancy": 2.5393, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TracLLM%3A%20A%20Generic%20Framework%20for%20Attributing%20Long%20Context%20LLMs&body=Title%3A%20TracLLM%3A%20A%20Generic%20Framework%20for%20Attributing%20Long%20Context%20LLMs%0AAuthor%3A%20Yanting%20Wang%20and%20Wei%20Zou%20and%20Runpeng%20Geng%20and%20Jinyuan%20Jia%0AAbstract%3A%20%20%20Long%20context%20large%20language%20models%20%28LLMs%29%20are%20deployed%20in%20many%20real-world%0Aapplications%20such%20as%20RAG%2C%20agent%2C%20and%20broad%20LLM-integrated%20applications.%20Given%0Aan%20instruction%20and%20a%20long%20context%20%28e.g.%2C%20documents%2C%20PDF%20files%2C%20webpages%29%2C%20a%0Along%20context%20LLM%20can%20generate%20an%20output%20grounded%20in%20the%20provided%20context%2C%0Aaiming%20to%20provide%20more%20accurate%2C%20up-to-date%2C%20and%20verifiable%20outputs%20while%0Areducing%20hallucinations%20and%20unsupported%20claims.%20This%20raises%20a%20research%0Aquestion%3A%20how%20to%20pinpoint%20the%20texts%20%28e.g.%2C%20sentences%2C%20passages%2C%20or%20paragraphs%29%0Ain%20the%20context%20that%20contribute%20most%20to%20or%20are%20responsible%20for%20the%20generated%0Aoutput%20by%20an%20LLM%3F%20This%20process%2C%20which%20we%20call%20context%20traceback%2C%20has%20various%0Areal-world%20applications%2C%20such%20as%201%29%20debugging%20LLM-based%20systems%2C%202%29%20conducting%0Apost-attack%20forensic%20analysis%20for%20attacks%20%28e.g.%2C%20prompt%20injection%20attack%2C%0Aknowledge%20corruption%20attacks%29%20to%20an%20LLM%2C%20and%203%29%20highlighting%20knowledge%20sources%0Ato%20enhance%20the%20trust%20of%20users%20towards%20outputs%20generated%20by%20LLMs.%20When%20applied%0Ato%20context%20traceback%20for%20long%20context%20LLMs%2C%20existing%20feature%20attribution%0Amethods%20such%20as%20Shapley%20have%20sub-optimal%20performance%20and/or%20incur%20a%20large%0Acomputational%20cost.%20In%20this%20work%2C%20we%20develop%20TracLLM%2C%20the%20first%20generic%20context%0Atraceback%20framework%20tailored%20to%20long%20context%20LLMs.%20Our%20framework%20can%20improve%0Athe%20effectiveness%20and%20efficiency%20of%20existing%20feature%20attribution%20methods.%20To%0Aimprove%20the%20efficiency%2C%20we%20develop%20an%20informed%20search%20based%20algorithm%20in%0ATracLLM.%20We%20also%20develop%20contribution%20score%20ensemble/denoising%20techniques%20to%0Aimprove%20the%20accuracy%20of%20TracLLM.%20Our%20evaluation%20results%20show%20TracLLM%20can%0Aeffectively%20identify%20texts%20in%20a%20long%20context%20that%20lead%20to%20the%20output%20of%20an%20LLM.%0AOur%20code%20and%20data%20are%20at%3A%20https%3A//github.com/Wang-Yanting/TracLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracLLM%253A%2520A%2520Generic%2520Framework%2520for%2520Attributing%2520Long%2520Context%2520LLMs%26entry.906535625%3DYanting%2520Wang%2520and%2520Wei%2520Zou%2520and%2520Runpeng%2520Geng%2520and%2520Jinyuan%2520Jia%26entry.1292438233%3D%2520%2520Long%2520context%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520deployed%2520in%2520many%2520real-world%250Aapplications%2520such%2520as%2520RAG%252C%2520agent%252C%2520and%2520broad%2520LLM-integrated%2520applications.%2520Given%250Aan%2520instruction%2520and%2520a%2520long%2520context%2520%2528e.g.%252C%2520documents%252C%2520PDF%2520files%252C%2520webpages%2529%252C%2520a%250Along%2520context%2520LLM%2520can%2520generate%2520an%2520output%2520grounded%2520in%2520the%2520provided%2520context%252C%250Aaiming%2520to%2520provide%2520more%2520accurate%252C%2520up-to-date%252C%2520and%2520verifiable%2520outputs%2520while%250Areducing%2520hallucinations%2520and%2520unsupported%2520claims.%2520This%2520raises%2520a%2520research%250Aquestion%253A%2520how%2520to%2520pinpoint%2520the%2520texts%2520%2528e.g.%252C%2520sentences%252C%2520passages%252C%2520or%2520paragraphs%2529%250Ain%2520the%2520context%2520that%2520contribute%2520most%2520to%2520or%2520are%2520responsible%2520for%2520the%2520generated%250Aoutput%2520by%2520an%2520LLM%253F%2520This%2520process%252C%2520which%2520we%2520call%2520context%2520traceback%252C%2520has%2520various%250Areal-world%2520applications%252C%2520such%2520as%25201%2529%2520debugging%2520LLM-based%2520systems%252C%25202%2529%2520conducting%250Apost-attack%2520forensic%2520analysis%2520for%2520attacks%2520%2528e.g.%252C%2520prompt%2520injection%2520attack%252C%250Aknowledge%2520corruption%2520attacks%2529%2520to%2520an%2520LLM%252C%2520and%25203%2529%2520highlighting%2520knowledge%2520sources%250Ato%2520enhance%2520the%2520trust%2520of%2520users%2520towards%2520outputs%2520generated%2520by%2520LLMs.%2520When%2520applied%250Ato%2520context%2520traceback%2520for%2520long%2520context%2520LLMs%252C%2520existing%2520feature%2520attribution%250Amethods%2520such%2520as%2520Shapley%2520have%2520sub-optimal%2520performance%2520and/or%2520incur%2520a%2520large%250Acomputational%2520cost.%2520In%2520this%2520work%252C%2520we%2520develop%2520TracLLM%252C%2520the%2520first%2520generic%2520context%250Atraceback%2520framework%2520tailored%2520to%2520long%2520context%2520LLMs.%2520Our%2520framework%2520can%2520improve%250Athe%2520effectiveness%2520and%2520efficiency%2520of%2520existing%2520feature%2520attribution%2520methods.%2520To%250Aimprove%2520the%2520efficiency%252C%2520we%2520develop%2520an%2520informed%2520search%2520based%2520algorithm%2520in%250ATracLLM.%2520We%2520also%2520develop%2520contribution%2520score%2520ensemble/denoising%2520techniques%2520to%250Aimprove%2520the%2520accuracy%2520of%2520TracLLM.%2520Our%2520evaluation%2520results%2520show%2520TracLLM%2520can%250Aeffectively%2520identify%2520texts%2520in%2520a%2520long%2520context%2520that%2520lead%2520to%2520the%2520output%2520of%2520an%2520LLM.%250AOur%2520code%2520and%2520data%2520are%2520at%253A%2520https%253A//github.com/Wang-Yanting/TracLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TracLLM%3A%20A%20Generic%20Framework%20for%20Attributing%20Long%20Context%20LLMs&entry.906535625=Yanting%20Wang%20and%20Wei%20Zou%20and%20Runpeng%20Geng%20and%20Jinyuan%20Jia&entry.1292438233=%20%20Long%20context%20large%20language%20models%20%28LLMs%29%20are%20deployed%20in%20many%20real-world%0Aapplications%20such%20as%20RAG%2C%20agent%2C%20and%20broad%20LLM-integrated%20applications.%20Given%0Aan%20instruction%20and%20a%20long%20context%20%28e.g.%2C%20documents%2C%20PDF%20files%2C%20webpages%29%2C%20a%0Along%20context%20LLM%20can%20generate%20an%20output%20grounded%20in%20the%20provided%20context%2C%0Aaiming%20to%20provide%20more%20accurate%2C%20up-to-date%2C%20and%20verifiable%20outputs%20while%0Areducing%20hallucinations%20and%20unsupported%20claims.%20This%20raises%20a%20research%0Aquestion%3A%20how%20to%20pinpoint%20the%20texts%20%28e.g.%2C%20sentences%2C%20passages%2C%20or%20paragraphs%29%0Ain%20the%20context%20that%20contribute%20most%20to%20or%20are%20responsible%20for%20the%20generated%0Aoutput%20by%20an%20LLM%3F%20This%20process%2C%20which%20we%20call%20context%20traceback%2C%20has%20various%0Areal-world%20applications%2C%20such%20as%201%29%20debugging%20LLM-based%20systems%2C%202%29%20conducting%0Apost-attack%20forensic%20analysis%20for%20attacks%20%28e.g.%2C%20prompt%20injection%20attack%2C%0Aknowledge%20corruption%20attacks%29%20to%20an%20LLM%2C%20and%203%29%20highlighting%20knowledge%20sources%0Ato%20enhance%20the%20trust%20of%20users%20towards%20outputs%20generated%20by%20LLMs.%20When%20applied%0Ato%20context%20traceback%20for%20long%20context%20LLMs%2C%20existing%20feature%20attribution%0Amethods%20such%20as%20Shapley%20have%20sub-optimal%20performance%20and/or%20incur%20a%20large%0Acomputational%20cost.%20In%20this%20work%2C%20we%20develop%20TracLLM%2C%20the%20first%20generic%20context%0Atraceback%20framework%20tailored%20to%20long%20context%20LLMs.%20Our%20framework%20can%20improve%0Athe%20effectiveness%20and%20efficiency%20of%20existing%20feature%20attribution%20methods.%20To%0Aimprove%20the%20efficiency%2C%20we%20develop%20an%20informed%20search%20based%20algorithm%20in%0ATracLLM.%20We%20also%20develop%20contribution%20score%20ensemble/denoising%20techniques%20to%0Aimprove%20the%20accuracy%20of%20TracLLM.%20Our%20evaluation%20results%20show%20TracLLM%20can%0Aeffectively%20identify%20texts%20in%20a%20long%20context%20that%20lead%20to%20the%20output%20of%20an%20LLM.%0AOur%20code%20and%20data%20are%20at%3A%20https%3A//github.com/Wang-Yanting/TracLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04202v1&entry.124074799=Read"},
{"title": "Sample Compression for Self Certified Continual Learning", "author": "Jacob Comeau and Mathieu Bazinet and Pascal Germain and Cem Subakan", "abstract": "  Continual learning algorithms aim to learn from a sequence of tasks, making\nthe training distribution non-stationary. The majority of existing continual\nlearning approaches in the literature rely on heuristics and do not provide\nlearning guarantees. In this paper, we present a new method called Continual\nPick-to-Learn (CoP2L), which is able to retain the most representative samples\nfor each task in an efficient way. CoP2L combines the Pick-to-Learn algorithm\n(rooted in the sample compression theory) and the experience replay continual\nlearning scheme. This allows us to provide non-vacuous upper bounds on the\ngeneralization loss of the learned predictors, numerically computable after\neach task. We empirically evaluate our approach on several standard continual\nlearning benchmarks across Class-Incremental, Task-Incremental, and\nDomain-Incremental settings. Our results show that CoP2L is highly competitive\nacross all setups, often outperforming existing baselines, and significantly\nmitigating catastrophic forgetting compared to vanilla experience replay in the\nClass-Incremental setting. It is possible to leverage the bounds provided by\nCoP2L in practical scenarios to certify the predictor reliability on previously\nlearned tasks, in order to improve the trustworthiness of the continual\nlearning algorithm.\n", "link": "http://arxiv.org/abs/2503.10503v3", "date": "2025-06-04", "relevancy": 2.5218, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5156}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20Compression%20for%20Self%20Certified%20Continual%20Learning&body=Title%3A%20Sample%20Compression%20for%20Self%20Certified%20Continual%20Learning%0AAuthor%3A%20Jacob%20Comeau%20and%20Mathieu%20Bazinet%20and%20Pascal%20Germain%20and%20Cem%20Subakan%0AAbstract%3A%20%20%20Continual%20learning%20algorithms%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%2C%20making%0Athe%20training%20distribution%20non-stationary.%20The%20majority%20of%20existing%20continual%0Alearning%20approaches%20in%20the%20literature%20rely%20on%20heuristics%20and%20do%20not%20provide%0Alearning%20guarantees.%20In%20this%20paper%2C%20we%20present%20a%20new%20method%20called%20Continual%0APick-to-Learn%20%28CoP2L%29%2C%20which%20is%20able%20to%20retain%20the%20most%20representative%20samples%0Afor%20each%20task%20in%20an%20efficient%20way.%20CoP2L%20combines%20the%20Pick-to-Learn%20algorithm%0A%28rooted%20in%20the%20sample%20compression%20theory%29%20and%20the%20experience%20replay%20continual%0Alearning%20scheme.%20This%20allows%20us%20to%20provide%20non-vacuous%20upper%20bounds%20on%20the%0Ageneralization%20loss%20of%20the%20learned%20predictors%2C%20numerically%20computable%20after%0Aeach%20task.%20We%20empirically%20evaluate%20our%20approach%20on%20several%20standard%20continual%0Alearning%20benchmarks%20across%20Class-Incremental%2C%20Task-Incremental%2C%20and%0ADomain-Incremental%20settings.%20Our%20results%20show%20that%20CoP2L%20is%20highly%20competitive%0Aacross%20all%20setups%2C%20often%20outperforming%20existing%20baselines%2C%20and%20significantly%0Amitigating%20catastrophic%20forgetting%20compared%20to%20vanilla%20experience%20replay%20in%20the%0AClass-Incremental%20setting.%20It%20is%20possible%20to%20leverage%20the%20bounds%20provided%20by%0ACoP2L%20in%20practical%20scenarios%20to%20certify%20the%20predictor%20reliability%20on%20previously%0Alearned%20tasks%2C%20in%20order%20to%20improve%20the%20trustworthiness%20of%20the%20continual%0Alearning%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520Compression%2520for%2520Self%2520Certified%2520Continual%2520Learning%26entry.906535625%3DJacob%2520Comeau%2520and%2520Mathieu%2520Bazinet%2520and%2520Pascal%2520Germain%2520and%2520Cem%2520Subakan%26entry.1292438233%3D%2520%2520Continual%2520learning%2520algorithms%2520aim%2520to%2520learn%2520from%2520a%2520sequence%2520of%2520tasks%252C%2520making%250Athe%2520training%2520distribution%2520non-stationary.%2520The%2520majority%2520of%2520existing%2520continual%250Alearning%2520approaches%2520in%2520the%2520literature%2520rely%2520on%2520heuristics%2520and%2520do%2520not%2520provide%250Alearning%2520guarantees.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520method%2520called%2520Continual%250APick-to-Learn%2520%2528CoP2L%2529%252C%2520which%2520is%2520able%2520to%2520retain%2520the%2520most%2520representative%2520samples%250Afor%2520each%2520task%2520in%2520an%2520efficient%2520way.%2520CoP2L%2520combines%2520the%2520Pick-to-Learn%2520algorithm%250A%2528rooted%2520in%2520the%2520sample%2520compression%2520theory%2529%2520and%2520the%2520experience%2520replay%2520continual%250Alearning%2520scheme.%2520This%2520allows%2520us%2520to%2520provide%2520non-vacuous%2520upper%2520bounds%2520on%2520the%250Ageneralization%2520loss%2520of%2520the%2520learned%2520predictors%252C%2520numerically%2520computable%2520after%250Aeach%2520task.%2520We%2520empirically%2520evaluate%2520our%2520approach%2520on%2520several%2520standard%2520continual%250Alearning%2520benchmarks%2520across%2520Class-Incremental%252C%2520Task-Incremental%252C%2520and%250ADomain-Incremental%2520settings.%2520Our%2520results%2520show%2520that%2520CoP2L%2520is%2520highly%2520competitive%250Aacross%2520all%2520setups%252C%2520often%2520outperforming%2520existing%2520baselines%252C%2520and%2520significantly%250Amitigating%2520catastrophic%2520forgetting%2520compared%2520to%2520vanilla%2520experience%2520replay%2520in%2520the%250AClass-Incremental%2520setting.%2520It%2520is%2520possible%2520to%2520leverage%2520the%2520bounds%2520provided%2520by%250ACoP2L%2520in%2520practical%2520scenarios%2520to%2520certify%2520the%2520predictor%2520reliability%2520on%2520previously%250Alearned%2520tasks%252C%2520in%2520order%2520to%2520improve%2520the%2520trustworthiness%2520of%2520the%2520continual%250Alearning%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20Compression%20for%20Self%20Certified%20Continual%20Learning&entry.906535625=Jacob%20Comeau%20and%20Mathieu%20Bazinet%20and%20Pascal%20Germain%20and%20Cem%20Subakan&entry.1292438233=%20%20Continual%20learning%20algorithms%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%2C%20making%0Athe%20training%20distribution%20non-stationary.%20The%20majority%20of%20existing%20continual%0Alearning%20approaches%20in%20the%20literature%20rely%20on%20heuristics%20and%20do%20not%20provide%0Alearning%20guarantees.%20In%20this%20paper%2C%20we%20present%20a%20new%20method%20called%20Continual%0APick-to-Learn%20%28CoP2L%29%2C%20which%20is%20able%20to%20retain%20the%20most%20representative%20samples%0Afor%20each%20task%20in%20an%20efficient%20way.%20CoP2L%20combines%20the%20Pick-to-Learn%20algorithm%0A%28rooted%20in%20the%20sample%20compression%20theory%29%20and%20the%20experience%20replay%20continual%0Alearning%20scheme.%20This%20allows%20us%20to%20provide%20non-vacuous%20upper%20bounds%20on%20the%0Ageneralization%20loss%20of%20the%20learned%20predictors%2C%20numerically%20computable%20after%0Aeach%20task.%20We%20empirically%20evaluate%20our%20approach%20on%20several%20standard%20continual%0Alearning%20benchmarks%20across%20Class-Incremental%2C%20Task-Incremental%2C%20and%0ADomain-Incremental%20settings.%20Our%20results%20show%20that%20CoP2L%20is%20highly%20competitive%0Aacross%20all%20setups%2C%20often%20outperforming%20existing%20baselines%2C%20and%20significantly%0Amitigating%20catastrophic%20forgetting%20compared%20to%20vanilla%20experience%20replay%20in%20the%0AClass-Incremental%20setting.%20It%20is%20possible%20to%20leverage%20the%20bounds%20provided%20by%0ACoP2L%20in%20practical%20scenarios%20to%20certify%20the%20predictor%20reliability%20on%20previously%0Alearned%20tasks%2C%20in%20order%20to%20improve%20the%20trustworthiness%20of%20the%20continual%0Alearning%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10503v3&entry.124074799=Read"},
{"title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation", "author": "Thomas Pickard and Aline Villavicencio and Maggie Mi and Wei He and Dylan Phelps and Marco Idiart", "abstract": "  Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.\n", "link": "http://arxiv.org/abs/2503.15358v3", "date": "2025-06-04", "relevancy": 2.5159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemEval-2025%20Task%201%3A%20AdMIRe%20--%20Advancing%20Multimodal%20Idiomaticity%0A%20%20Representation&body=Title%3A%20SemEval-2025%20Task%201%3A%20AdMIRe%20--%20Advancing%20Multimodal%20Idiomaticity%0A%20%20Representation%0AAuthor%3A%20Thomas%20Pickard%20and%20Aline%20Villavicencio%20and%20Maggie%20Mi%20and%20Wei%20He%20and%20Dylan%20Phelps%20and%20Marco%20Idiart%0AAbstract%3A%20%20%20Idiomatic%20expressions%20present%20a%20unique%20challenge%20in%20NLP%2C%20as%20their%20meanings%0Aare%20often%20not%20directly%20inferable%20from%20their%20constituent%20words.%20Despite%20recent%0Aadvancements%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20idiomaticity%20remains%20a%0Asignificant%20obstacle%20to%20robust%20semantic%20representation.%20We%20present%20datasets%20and%0Atasks%20for%20SemEval-2025%20Task%201%3A%20AdMiRe%20%28Advancing%20Multimodal%20Idiomaticity%0ARepresentation%29%2C%20which%20challenges%20the%20community%20to%20assess%20and%20improve%20models%27%0Aability%20to%20interpret%20idiomatic%20expressions%20in%20multimodal%20contexts%20and%20in%0Amultiple%20languages.%20Participants%20competed%20in%20two%20subtasks%3A%20ranking%20images%20based%0Aon%20their%20alignment%20with%20idiomatic%20or%20literal%20meanings%2C%20and%20predicting%20the%20next%0Aimage%20in%20a%20sequence.%20The%20most%20effective%20methods%20achieved%20human-level%0Aperformance%20by%20leveraging%20pretrained%20LLMs%20and%20vision-language%20models%20in%0Amixture-of-experts%20settings%2C%20with%20multiple%20queries%20used%20to%20smooth%20over%20the%0Aweaknesses%20in%20these%20models%27%20representations%20of%20idiomaticity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15358v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemEval-2025%2520Task%25201%253A%2520AdMIRe%2520--%2520Advancing%2520Multimodal%2520Idiomaticity%250A%2520%2520Representation%26entry.906535625%3DThomas%2520Pickard%2520and%2520Aline%2520Villavicencio%2520and%2520Maggie%2520Mi%2520and%2520Wei%2520He%2520and%2520Dylan%2520Phelps%2520and%2520Marco%2520Idiart%26entry.1292438233%3D%2520%2520Idiomatic%2520expressions%2520present%2520a%2520unique%2520challenge%2520in%2520NLP%252C%2520as%2520their%2520meanings%250Aare%2520often%2520not%2520directly%2520inferable%2520from%2520their%2520constituent%2520words.%2520Despite%2520recent%250Aadvancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520idiomaticity%2520remains%2520a%250Asignificant%2520obstacle%2520to%2520robust%2520semantic%2520representation.%2520We%2520present%2520datasets%2520and%250Atasks%2520for%2520SemEval-2025%2520Task%25201%253A%2520AdMiRe%2520%2528Advancing%2520Multimodal%2520Idiomaticity%250ARepresentation%2529%252C%2520which%2520challenges%2520the%2520community%2520to%2520assess%2520and%2520improve%2520models%2527%250Aability%2520to%2520interpret%2520idiomatic%2520expressions%2520in%2520multimodal%2520contexts%2520and%2520in%250Amultiple%2520languages.%2520Participants%2520competed%2520in%2520two%2520subtasks%253A%2520ranking%2520images%2520based%250Aon%2520their%2520alignment%2520with%2520idiomatic%2520or%2520literal%2520meanings%252C%2520and%2520predicting%2520the%2520next%250Aimage%2520in%2520a%2520sequence.%2520The%2520most%2520effective%2520methods%2520achieved%2520human-level%250Aperformance%2520by%2520leveraging%2520pretrained%2520LLMs%2520and%2520vision-language%2520models%2520in%250Amixture-of-experts%2520settings%252C%2520with%2520multiple%2520queries%2520used%2520to%2520smooth%2520over%2520the%250Aweaknesses%2520in%2520these%2520models%2527%2520representations%2520of%2520idiomaticity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15358v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemEval-2025%20Task%201%3A%20AdMIRe%20--%20Advancing%20Multimodal%20Idiomaticity%0A%20%20Representation&entry.906535625=Thomas%20Pickard%20and%20Aline%20Villavicencio%20and%20Maggie%20Mi%20and%20Wei%20He%20and%20Dylan%20Phelps%20and%20Marco%20Idiart&entry.1292438233=%20%20Idiomatic%20expressions%20present%20a%20unique%20challenge%20in%20NLP%2C%20as%20their%20meanings%0Aare%20often%20not%20directly%20inferable%20from%20their%20constituent%20words.%20Despite%20recent%0Aadvancements%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20idiomaticity%20remains%20a%0Asignificant%20obstacle%20to%20robust%20semantic%20representation.%20We%20present%20datasets%20and%0Atasks%20for%20SemEval-2025%20Task%201%3A%20AdMiRe%20%28Advancing%20Multimodal%20Idiomaticity%0ARepresentation%29%2C%20which%20challenges%20the%20community%20to%20assess%20and%20improve%20models%27%0Aability%20to%20interpret%20idiomatic%20expressions%20in%20multimodal%20contexts%20and%20in%0Amultiple%20languages.%20Participants%20competed%20in%20two%20subtasks%3A%20ranking%20images%20based%0Aon%20their%20alignment%20with%20idiomatic%20or%20literal%20meanings%2C%20and%20predicting%20the%20next%0Aimage%20in%20a%20sequence.%20The%20most%20effective%20methods%20achieved%20human-level%0Aperformance%20by%20leveraging%20pretrained%20LLMs%20and%20vision-language%20models%20in%0Amixture-of-experts%20settings%2C%20with%20multiple%20queries%20used%20to%20smooth%20over%20the%0Aweaknesses%20in%20these%20models%27%20representations%20of%20idiomaticity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15358v3&entry.124074799=Read"},
{"title": "CoLa: Chinese Character Decomposition with Compositional Latent\n  Components", "author": "Fan Shi and Haiyang Yu and Bin Li and Xiangyang Xue", "abstract": "  Humans can decompose Chinese characters into compositional components and\nrecombine them to recognize unseen characters. This reflects two cognitive\nprinciples: Compositionality, the idea that complex concepts are built on\nsimpler parts; and Learning-to-learn, the ability to learn strategies for\ndecomposing and recombining components to form new concepts. These principles\nprovide inductive biases that support efficient generalization. They are\ncritical to Chinese character recognition (CCR) in solving the zero-shot\nproblem, which results from the common long-tail distribution of Chinese\ncharacter datasets. Existing methods have made substantial progress in modeling\ncompositionality via predefined radical or stroke decomposition. However, they\noften ignore the learning-to-learn capability, limiting their ability to\ngeneralize beyond human-defined schemes. Inspired by these principles, we\npropose a deep latent variable model that learns Compositional Latent\ncomponents of Chinese characters (CoLa) without relying on human-defined\ndecomposition schemes. Recognition and matching can be performed by comparing\ncompositional latent components in the latent space, enabling zero-shot\ncharacter recognition. The experiments illustrate that CoLa outperforms\nprevious methods in both character the radical zero-shot CCR. Visualization\nindicates that the learned components can reflect the structure of characters\nin an interpretable way. Moreover, despite being trained on historical\ndocuments, CoLa can analyze components of oracle bone characters, highlighting\nits cross-dataset generalization ability.\n", "link": "http://arxiv.org/abs/2506.03798v1", "date": "2025-06-04", "relevancy": 2.5121, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoLa%3A%20Chinese%20Character%20Decomposition%20with%20Compositional%20Latent%0A%20%20Components&body=Title%3A%20CoLa%3A%20Chinese%20Character%20Decomposition%20with%20Compositional%20Latent%0A%20%20Components%0AAuthor%3A%20Fan%20Shi%20and%20Haiyang%20Yu%20and%20Bin%20Li%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20Humans%20can%20decompose%20Chinese%20characters%20into%20compositional%20components%20and%0Arecombine%20them%20to%20recognize%20unseen%20characters.%20This%20reflects%20two%20cognitive%0Aprinciples%3A%20Compositionality%2C%20the%20idea%20that%20complex%20concepts%20are%20built%20on%0Asimpler%20parts%3B%20and%20Learning-to-learn%2C%20the%20ability%20to%20learn%20strategies%20for%0Adecomposing%20and%20recombining%20components%20to%20form%20new%20concepts.%20These%20principles%0Aprovide%20inductive%20biases%20that%20support%20efficient%20generalization.%20They%20are%0Acritical%20to%20Chinese%20character%20recognition%20%28CCR%29%20in%20solving%20the%20zero-shot%0Aproblem%2C%20which%20results%20from%20the%20common%20long-tail%20distribution%20of%20Chinese%0Acharacter%20datasets.%20Existing%20methods%20have%20made%20substantial%20progress%20in%20modeling%0Acompositionality%20via%20predefined%20radical%20or%20stroke%20decomposition.%20However%2C%20they%0Aoften%20ignore%20the%20learning-to-learn%20capability%2C%20limiting%20their%20ability%20to%0Ageneralize%20beyond%20human-defined%20schemes.%20Inspired%20by%20these%20principles%2C%20we%0Apropose%20a%20deep%20latent%20variable%20model%20that%20learns%20Compositional%20Latent%0Acomponents%20of%20Chinese%20characters%20%28CoLa%29%20without%20relying%20on%20human-defined%0Adecomposition%20schemes.%20Recognition%20and%20matching%20can%20be%20performed%20by%20comparing%0Acompositional%20latent%20components%20in%20the%20latent%20space%2C%20enabling%20zero-shot%0Acharacter%20recognition.%20The%20experiments%20illustrate%20that%20CoLa%20outperforms%0Aprevious%20methods%20in%20both%20character%20the%20radical%20zero-shot%20CCR.%20Visualization%0Aindicates%20that%20the%20learned%20components%20can%20reflect%20the%20structure%20of%20characters%0Ain%20an%20interpretable%20way.%20Moreover%2C%20despite%20being%20trained%20on%20historical%0Adocuments%2C%20CoLa%20can%20analyze%20components%20of%20oracle%20bone%20characters%2C%20highlighting%0Aits%20cross-dataset%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoLa%253A%2520Chinese%2520Character%2520Decomposition%2520with%2520Compositional%2520Latent%250A%2520%2520Components%26entry.906535625%3DFan%2520Shi%2520and%2520Haiyang%2520Yu%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520Humans%2520can%2520decompose%2520Chinese%2520characters%2520into%2520compositional%2520components%2520and%250Arecombine%2520them%2520to%2520recognize%2520unseen%2520characters.%2520This%2520reflects%2520two%2520cognitive%250Aprinciples%253A%2520Compositionality%252C%2520the%2520idea%2520that%2520complex%2520concepts%2520are%2520built%2520on%250Asimpler%2520parts%253B%2520and%2520Learning-to-learn%252C%2520the%2520ability%2520to%2520learn%2520strategies%2520for%250Adecomposing%2520and%2520recombining%2520components%2520to%2520form%2520new%2520concepts.%2520These%2520principles%250Aprovide%2520inductive%2520biases%2520that%2520support%2520efficient%2520generalization.%2520They%2520are%250Acritical%2520to%2520Chinese%2520character%2520recognition%2520%2528CCR%2529%2520in%2520solving%2520the%2520zero-shot%250Aproblem%252C%2520which%2520results%2520from%2520the%2520common%2520long-tail%2520distribution%2520of%2520Chinese%250Acharacter%2520datasets.%2520Existing%2520methods%2520have%2520made%2520substantial%2520progress%2520in%2520modeling%250Acompositionality%2520via%2520predefined%2520radical%2520or%2520stroke%2520decomposition.%2520However%252C%2520they%250Aoften%2520ignore%2520the%2520learning-to-learn%2520capability%252C%2520limiting%2520their%2520ability%2520to%250Ageneralize%2520beyond%2520human-defined%2520schemes.%2520Inspired%2520by%2520these%2520principles%252C%2520we%250Apropose%2520a%2520deep%2520latent%2520variable%2520model%2520that%2520learns%2520Compositional%2520Latent%250Acomponents%2520of%2520Chinese%2520characters%2520%2528CoLa%2529%2520without%2520relying%2520on%2520human-defined%250Adecomposition%2520schemes.%2520Recognition%2520and%2520matching%2520can%2520be%2520performed%2520by%2520comparing%250Acompositional%2520latent%2520components%2520in%2520the%2520latent%2520space%252C%2520enabling%2520zero-shot%250Acharacter%2520recognition.%2520The%2520experiments%2520illustrate%2520that%2520CoLa%2520outperforms%250Aprevious%2520methods%2520in%2520both%2520character%2520the%2520radical%2520zero-shot%2520CCR.%2520Visualization%250Aindicates%2520that%2520the%2520learned%2520components%2520can%2520reflect%2520the%2520structure%2520of%2520characters%250Ain%2520an%2520interpretable%2520way.%2520Moreover%252C%2520despite%2520being%2520trained%2520on%2520historical%250Adocuments%252C%2520CoLa%2520can%2520analyze%2520components%2520of%2520oracle%2520bone%2520characters%252C%2520highlighting%250Aits%2520cross-dataset%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLa%3A%20Chinese%20Character%20Decomposition%20with%20Compositional%20Latent%0A%20%20Components&entry.906535625=Fan%20Shi%20and%20Haiyang%20Yu%20and%20Bin%20Li%20and%20Xiangyang%20Xue&entry.1292438233=%20%20Humans%20can%20decompose%20Chinese%20characters%20into%20compositional%20components%20and%0Arecombine%20them%20to%20recognize%20unseen%20characters.%20This%20reflects%20two%20cognitive%0Aprinciples%3A%20Compositionality%2C%20the%20idea%20that%20complex%20concepts%20are%20built%20on%0Asimpler%20parts%3B%20and%20Learning-to-learn%2C%20the%20ability%20to%20learn%20strategies%20for%0Adecomposing%20and%20recombining%20components%20to%20form%20new%20concepts.%20These%20principles%0Aprovide%20inductive%20biases%20that%20support%20efficient%20generalization.%20They%20are%0Acritical%20to%20Chinese%20character%20recognition%20%28CCR%29%20in%20solving%20the%20zero-shot%0Aproblem%2C%20which%20results%20from%20the%20common%20long-tail%20distribution%20of%20Chinese%0Acharacter%20datasets.%20Existing%20methods%20have%20made%20substantial%20progress%20in%20modeling%0Acompositionality%20via%20predefined%20radical%20or%20stroke%20decomposition.%20However%2C%20they%0Aoften%20ignore%20the%20learning-to-learn%20capability%2C%20limiting%20their%20ability%20to%0Ageneralize%20beyond%20human-defined%20schemes.%20Inspired%20by%20these%20principles%2C%20we%0Apropose%20a%20deep%20latent%20variable%20model%20that%20learns%20Compositional%20Latent%0Acomponents%20of%20Chinese%20characters%20%28CoLa%29%20without%20relying%20on%20human-defined%0Adecomposition%20schemes.%20Recognition%20and%20matching%20can%20be%20performed%20by%20comparing%0Acompositional%20latent%20components%20in%20the%20latent%20space%2C%20enabling%20zero-shot%0Acharacter%20recognition.%20The%20experiments%20illustrate%20that%20CoLa%20outperforms%0Aprevious%20methods%20in%20both%20character%20the%20radical%20zero-shot%20CCR.%20Visualization%0Aindicates%20that%20the%20learned%20components%20can%20reflect%20the%20structure%20of%20characters%0Ain%20an%20interpretable%20way.%20Moreover%2C%20despite%20being%20trained%20on%20historical%0Adocuments%2C%20CoLa%20can%20analyze%20components%20of%20oracle%20bone%20characters%2C%20highlighting%0Aits%20cross-dataset%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03798v1&entry.124074799=Read"},
{"title": "Explainability-Based Token Replacement on LLM-Generated Text", "author": "Hadi Mohammadi and Anastasia Giachanou and Daniel L. Oberski and Ayoub Bagheri", "abstract": "  Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.\n", "link": "http://arxiv.org/abs/2506.04050v1", "date": "2025-06-04", "relevancy": 2.4945, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5092}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4939}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainability-Based%20Token%20Replacement%20on%20LLM-Generated%20Text&body=Title%3A%20Explainability-Based%20Token%20Replacement%20on%20LLM-Generated%20Text%0AAuthor%3A%20Hadi%20Mohammadi%20and%20Anastasia%20Giachanou%20and%20Daniel%20L.%20Oberski%20and%20Ayoub%20Bagheri%0AAbstract%3A%20%20%20Generative%20models%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20have%20shown%0Aremarkable%20progress%20in%20producing%20text%20that%20appears%20human-like.%20However%2C%20they%0Aoften%20exhibit%20patterns%20that%20make%20their%20output%20easier%20to%20detect%20than%20text%0Awritten%20by%20humans.%20In%20this%20paper%2C%20we%20investigate%20how%20explainable%20AI%20%28XAI%29%0Amethods%20can%20be%20used%20to%20reduce%20the%20detectability%20of%20AI-generated%20text%20%28AIGT%29%0Awhile%20also%20introducing%20a%20robust%20ensemble-based%20detection%20approach.%20We%20begin%20by%0Atraining%20an%20ensemble%20classifier%20to%20distinguish%20AIGT%20from%20human-written%20text%2C%0Athen%20apply%20SHAP%20and%20LIME%20to%20identify%20tokens%20that%20most%20strongly%20influence%20its%0Apredictions.%20We%20propose%20four%20explainability-based%20token%20replacement%20strategies%0Ato%20modify%20these%20influential%20tokens.%20Our%20findings%20show%20that%20these%20token%0Areplacement%20approaches%20can%20significantly%20diminish%20a%20single%20classifier%27s%20ability%0Ato%20detect%20AIGT.%20However%2C%20our%20ensemble%20classifier%20maintains%20strong%20performance%0Aacross%20multiple%20languages%20and%20domains%2C%20showing%20that%20a%20multi-model%20approach%20can%0Amitigate%20the%20impact%20of%20token-level%20manipulations.%20These%20results%20show%20that%20XAI%0Amethods%20can%20make%20AIGT%20harder%20to%20detect%20by%20focusing%20on%20the%20most%20influential%0Atokens.%20At%20the%20same%20time%2C%20they%20highlight%20the%20need%20for%20robust%2C%20ensemble-based%0Adetection%20strategies%20that%20can%20adapt%20to%20evolving%20approaches%20for%20hiding%20AIGT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainability-Based%2520Token%2520Replacement%2520on%2520LLM-Generated%2520Text%26entry.906535625%3DHadi%2520Mohammadi%2520and%2520Anastasia%2520Giachanou%2520and%2520Daniel%2520L.%2520Oberski%2520and%2520Ayoub%2520Bagheri%26entry.1292438233%3D%2520%2520Generative%2520models%252C%2520especially%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520have%2520shown%250Aremarkable%2520progress%2520in%2520producing%2520text%2520that%2520appears%2520human-like.%2520However%252C%2520they%250Aoften%2520exhibit%2520patterns%2520that%2520make%2520their%2520output%2520easier%2520to%2520detect%2520than%2520text%250Awritten%2520by%2520humans.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520explainable%2520AI%2520%2528XAI%2529%250Amethods%2520can%2520be%2520used%2520to%2520reduce%2520the%2520detectability%2520of%2520AI-generated%2520text%2520%2528AIGT%2529%250Awhile%2520also%2520introducing%2520a%2520robust%2520ensemble-based%2520detection%2520approach.%2520We%2520begin%2520by%250Atraining%2520an%2520ensemble%2520classifier%2520to%2520distinguish%2520AIGT%2520from%2520human-written%2520text%252C%250Athen%2520apply%2520SHAP%2520and%2520LIME%2520to%2520identify%2520tokens%2520that%2520most%2520strongly%2520influence%2520its%250Apredictions.%2520We%2520propose%2520four%2520explainability-based%2520token%2520replacement%2520strategies%250Ato%2520modify%2520these%2520influential%2520tokens.%2520Our%2520findings%2520show%2520that%2520these%2520token%250Areplacement%2520approaches%2520can%2520significantly%2520diminish%2520a%2520single%2520classifier%2527s%2520ability%250Ato%2520detect%2520AIGT.%2520However%252C%2520our%2520ensemble%2520classifier%2520maintains%2520strong%2520performance%250Aacross%2520multiple%2520languages%2520and%2520domains%252C%2520showing%2520that%2520a%2520multi-model%2520approach%2520can%250Amitigate%2520the%2520impact%2520of%2520token-level%2520manipulations.%2520These%2520results%2520show%2520that%2520XAI%250Amethods%2520can%2520make%2520AIGT%2520harder%2520to%2520detect%2520by%2520focusing%2520on%2520the%2520most%2520influential%250Atokens.%2520At%2520the%2520same%2520time%252C%2520they%2520highlight%2520the%2520need%2520for%2520robust%252C%2520ensemble-based%250Adetection%2520strategies%2520that%2520can%2520adapt%2520to%2520evolving%2520approaches%2520for%2520hiding%2520AIGT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainability-Based%20Token%20Replacement%20on%20LLM-Generated%20Text&entry.906535625=Hadi%20Mohammadi%20and%20Anastasia%20Giachanou%20and%20Daniel%20L.%20Oberski%20and%20Ayoub%20Bagheri&entry.1292438233=%20%20Generative%20models%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20have%20shown%0Aremarkable%20progress%20in%20producing%20text%20that%20appears%20human-like.%20However%2C%20they%0Aoften%20exhibit%20patterns%20that%20make%20their%20output%20easier%20to%20detect%20than%20text%0Awritten%20by%20humans.%20In%20this%20paper%2C%20we%20investigate%20how%20explainable%20AI%20%28XAI%29%0Amethods%20can%20be%20used%20to%20reduce%20the%20detectability%20of%20AI-generated%20text%20%28AIGT%29%0Awhile%20also%20introducing%20a%20robust%20ensemble-based%20detection%20approach.%20We%20begin%20by%0Atraining%20an%20ensemble%20classifier%20to%20distinguish%20AIGT%20from%20human-written%20text%2C%0Athen%20apply%20SHAP%20and%20LIME%20to%20identify%20tokens%20that%20most%20strongly%20influence%20its%0Apredictions.%20We%20propose%20four%20explainability-based%20token%20replacement%20strategies%0Ato%20modify%20these%20influential%20tokens.%20Our%20findings%20show%20that%20these%20token%0Areplacement%20approaches%20can%20significantly%20diminish%20a%20single%20classifier%27s%20ability%0Ato%20detect%20AIGT.%20However%2C%20our%20ensemble%20classifier%20maintains%20strong%20performance%0Aacross%20multiple%20languages%20and%20domains%2C%20showing%20that%20a%20multi-model%20approach%20can%0Amitigate%20the%20impact%20of%20token-level%20manipulations.%20These%20results%20show%20that%20XAI%0Amethods%20can%20make%20AIGT%20harder%20to%20detect%20by%20focusing%20on%20the%20most%20influential%0Atokens.%20At%20the%20same%20time%2C%20they%20highlight%20the%20need%20for%20robust%2C%20ensemble-based%0Adetection%20strategies%20that%20can%20adapt%20to%20evolving%20approaches%20for%20hiding%20AIGT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04050v1&entry.124074799=Read"},
{"title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation", "author": "Yuansheng Ni and Ping Nie and Kai Zou and Xiang Yue and Wenhu Chen", "abstract": "  Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.\n", "link": "http://arxiv.org/abs/2506.03930v1", "date": "2025-06-04", "relevancy": 2.487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCoder%3A%20Fine-Tuning%20LLMs%20for%20Executable%20Python%20Visualization%20Code%0A%20%20Generation&body=Title%3A%20VisCoder%3A%20Fine-Tuning%20LLMs%20for%20Executable%20Python%20Visualization%20Code%0A%20%20Generation%0AAuthor%3A%20Yuansheng%20Ni%20and%20Ping%20Nie%20and%20Kai%20Zou%20and%20Xiang%20Yue%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20struggle%20with%20visualization%20tasks%20like%0Aplotting%20diagrams%2C%20charts%2C%20where%20success%20depends%20on%20both%20code%20correctness%20and%0Avisual%20semantics.%20Existing%20instruction-tuning%20datasets%20lack%20execution-grounded%0Asupervision%20and%20offer%20limited%20support%20for%20iterative%20code%20correction%2C%20resulting%0Ain%20fragile%20and%20unreliable%20plot%20generation.%20We%20present%20VisCode-200K%2C%20a%0Alarge-scale%20instruction%20tuning%20dataset%20for%20Python-based%20visualization%20and%0Aself-correction.%20It%20contains%20over%20200K%20examples%20from%20two%20sources%3A%20%281%29%20validated%0Aplotting%20code%20from%20open-source%20repositories%2C%20paired%20with%20natural%20language%0Ainstructions%20and%20rendered%20plots%3B%20and%20%282%29%2045K%20multi-turn%20correction%20dialogues%0Afrom%20Code-Feedback%2C%20enabling%20models%20to%20revise%20faulty%20code%20using%20runtime%0Afeedback.%20We%20fine-tune%20Qwen2.5-Coder-Instruct%20on%20VisCode-200K%20to%20create%0AVisCoder%2C%20and%20evaluate%20it%20on%20PandasPlotBench.%20VisCoder%20significantly%0Aoutperforms%20strong%20open-source%20baselines%20and%20approaches%20the%20performance%20of%0Aproprietary%20models%20like%20GPT-4o-mini.%20We%20further%20adopt%20a%20self-debug%20evaluation%0Aprotocol%20to%20assess%20iterative%20repair%2C%20demonstrating%20the%20benefits%20of%0Afeedback-driven%20learning%20for%20executable%2C%20visually%20accurate%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCoder%253A%2520Fine-Tuning%2520LLMs%2520for%2520Executable%2520Python%2520Visualization%2520Code%250A%2520%2520Generation%26entry.906535625%3DYuansheng%2520Ni%2520and%2520Ping%2520Nie%2520and%2520Kai%2520Zou%2520and%2520Xiang%2520Yue%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520struggle%2520with%2520visualization%2520tasks%2520like%250Aplotting%2520diagrams%252C%2520charts%252C%2520where%2520success%2520depends%2520on%2520both%2520code%2520correctness%2520and%250Avisual%2520semantics.%2520Existing%2520instruction-tuning%2520datasets%2520lack%2520execution-grounded%250Asupervision%2520and%2520offer%2520limited%2520support%2520for%2520iterative%2520code%2520correction%252C%2520resulting%250Ain%2520fragile%2520and%2520unreliable%2520plot%2520generation.%2520We%2520present%2520VisCode-200K%252C%2520a%250Alarge-scale%2520instruction%2520tuning%2520dataset%2520for%2520Python-based%2520visualization%2520and%250Aself-correction.%2520It%2520contains%2520over%2520200K%2520examples%2520from%2520two%2520sources%253A%2520%25281%2529%2520validated%250Aplotting%2520code%2520from%2520open-source%2520repositories%252C%2520paired%2520with%2520natural%2520language%250Ainstructions%2520and%2520rendered%2520plots%253B%2520and%2520%25282%2529%252045K%2520multi-turn%2520correction%2520dialogues%250Afrom%2520Code-Feedback%252C%2520enabling%2520models%2520to%2520revise%2520faulty%2520code%2520using%2520runtime%250Afeedback.%2520We%2520fine-tune%2520Qwen2.5-Coder-Instruct%2520on%2520VisCode-200K%2520to%2520create%250AVisCoder%252C%2520and%2520evaluate%2520it%2520on%2520PandasPlotBench.%2520VisCoder%2520significantly%250Aoutperforms%2520strong%2520open-source%2520baselines%2520and%2520approaches%2520the%2520performance%2520of%250Aproprietary%2520models%2520like%2520GPT-4o-mini.%2520We%2520further%2520adopt%2520a%2520self-debug%2520evaluation%250Aprotocol%2520to%2520assess%2520iterative%2520repair%252C%2520demonstrating%2520the%2520benefits%2520of%250Afeedback-driven%2520learning%2520for%2520executable%252C%2520visually%2520accurate%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCoder%3A%20Fine-Tuning%20LLMs%20for%20Executable%20Python%20Visualization%20Code%0A%20%20Generation&entry.906535625=Yuansheng%20Ni%20and%20Ping%20Nie%20and%20Kai%20Zou%20and%20Xiang%20Yue%20and%20Wenhu%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20struggle%20with%20visualization%20tasks%20like%0Aplotting%20diagrams%2C%20charts%2C%20where%20success%20depends%20on%20both%20code%20correctness%20and%0Avisual%20semantics.%20Existing%20instruction-tuning%20datasets%20lack%20execution-grounded%0Asupervision%20and%20offer%20limited%20support%20for%20iterative%20code%20correction%2C%20resulting%0Ain%20fragile%20and%20unreliable%20plot%20generation.%20We%20present%20VisCode-200K%2C%20a%0Alarge-scale%20instruction%20tuning%20dataset%20for%20Python-based%20visualization%20and%0Aself-correction.%20It%20contains%20over%20200K%20examples%20from%20two%20sources%3A%20%281%29%20validated%0Aplotting%20code%20from%20open-source%20repositories%2C%20paired%20with%20natural%20language%0Ainstructions%20and%20rendered%20plots%3B%20and%20%282%29%2045K%20multi-turn%20correction%20dialogues%0Afrom%20Code-Feedback%2C%20enabling%20models%20to%20revise%20faulty%20code%20using%20runtime%0Afeedback.%20We%20fine-tune%20Qwen2.5-Coder-Instruct%20on%20VisCode-200K%20to%20create%0AVisCoder%2C%20and%20evaluate%20it%20on%20PandasPlotBench.%20VisCoder%20significantly%0Aoutperforms%20strong%20open-source%20baselines%20and%20approaches%20the%20performance%20of%0Aproprietary%20models%20like%20GPT-4o-mini.%20We%20further%20adopt%20a%20self-debug%20evaluation%0Aprotocol%20to%20assess%20iterative%20repair%2C%20demonstrating%20the%20benefits%20of%0Afeedback-driven%20learning%20for%20executable%2C%20visually%20accurate%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03930v1&entry.124074799=Read"},
{"title": "Engagement-Driven Content Generation with Large Language Models", "author": "Erica Coppolillo and Federico Cinus and Marco Minici and Francesco Bonchi and Giuseppe Manco", "abstract": "  Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation.\n", "link": "http://arxiv.org/abs/2411.13187v4", "date": "2025-06-04", "relevancy": 2.4787, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models&body=Title%3A%20Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models%0AAuthor%3A%20Erica%20Coppolillo%20and%20Federico%20Cinus%20and%20Marco%20Minici%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20significant%20persuasive%20capabilities%0Ain%20one-on-one%20interactions%2C%20but%20their%20influence%20within%20social%20networks%2C%20where%0Ainterconnected%20users%20and%20complex%20opinion%20dynamics%20pose%20unique%20challenges%2C%0Aremains%20underexplored.%20This%20paper%20addresses%20the%20research%20question%3A%20%5Cemph%7BCan%0ALLMs%20generate%20meaningful%20content%20that%20maximizes%20user%20engagement%20on%20social%0Anetworks%3F%7D%0A%20%20To%20answer%20this%2C%20we%20propose%20a%20pipeline%20using%20reinforcement%20learning%20with%0Asimulated%20feedback%2C%20where%20the%20network%27s%20response%20to%20LLM-generated%20content%0A%28i.e.%2C%20the%20reward%29%20is%20simulated%20through%20a%20formal%20engagement%20model.%20This%0Aapproach%20bypasses%20the%20temporal%20cost%20and%20complexity%20of%20live%20experiments%2C%0Aenabling%20an%20efficient%20feedback%20loop%20between%20the%20LLM%20and%20the%20network%20under%0Astudy.%20It%20also%20allows%20to%20control%20over%20endogenous%20factors%20such%20as%20the%20LLM%27s%0Aposition%20within%20the%20social%20network%20and%20the%20distribution%20of%20opinions%20on%20a%20given%0Atopic.%20Our%20approach%20is%20adaptive%20to%20the%20opinion%20distribution%20of%20the%20underlying%0Anetwork%20and%20agnostic%20to%20the%20specifics%20of%20the%20engagement%20model%2C%20which%20is%0Aembedded%20as%20a%20plug-and-play%20component.%20Such%20flexibility%20makes%20it%20suitable%20for%0Amore%20complex%20engagement%20tasks%20and%20interventions%20in%20computational%20social%0Ascience.%0A%20%20Using%20our%20framework%2C%20we%20analyze%20the%20performance%20of%20LLMs%20in%20generating%20social%0Aengagement%20under%20different%20conditions%2C%20showcasing%20their%20full%20potential%20in%20this%0Atask.%20The%20experimental%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mminici/Engagement-Driven-Content-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13187v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngagement-Driven%2520Content%2520Generation%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DErica%2520Coppolillo%2520and%2520Federico%2520Cinus%2520and%2520Marco%2520Minici%2520and%2520Francesco%2520Bonchi%2520and%2520Giuseppe%2520Manco%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520significant%2520persuasive%2520capabilities%250Ain%2520one-on-one%2520interactions%252C%2520but%2520their%2520influence%2520within%2520social%2520networks%252C%2520where%250Ainterconnected%2520users%2520and%2520complex%2520opinion%2520dynamics%2520pose%2520unique%2520challenges%252C%250Aremains%2520underexplored.%2520This%2520paper%2520addresses%2520the%2520research%2520question%253A%2520%255Cemph%257BCan%250ALLMs%2520generate%2520meaningful%2520content%2520that%2520maximizes%2520user%2520engagement%2520on%2520social%250Anetworks%253F%257D%250A%2520%2520To%2520answer%2520this%252C%2520we%2520propose%2520a%2520pipeline%2520using%2520reinforcement%2520learning%2520with%250Asimulated%2520feedback%252C%2520where%2520the%2520network%2527s%2520response%2520to%2520LLM-generated%2520content%250A%2528i.e.%252C%2520the%2520reward%2529%2520is%2520simulated%2520through%2520a%2520formal%2520engagement%2520model.%2520This%250Aapproach%2520bypasses%2520the%2520temporal%2520cost%2520and%2520complexity%2520of%2520live%2520experiments%252C%250Aenabling%2520an%2520efficient%2520feedback%2520loop%2520between%2520the%2520LLM%2520and%2520the%2520network%2520under%250Astudy.%2520It%2520also%2520allows%2520to%2520control%2520over%2520endogenous%2520factors%2520such%2520as%2520the%2520LLM%2527s%250Aposition%2520within%2520the%2520social%2520network%2520and%2520the%2520distribution%2520of%2520opinions%2520on%2520a%2520given%250Atopic.%2520Our%2520approach%2520is%2520adaptive%2520to%2520the%2520opinion%2520distribution%2520of%2520the%2520underlying%250Anetwork%2520and%2520agnostic%2520to%2520the%2520specifics%2520of%2520the%2520engagement%2520model%252C%2520which%2520is%250Aembedded%2520as%2520a%2520plug-and-play%2520component.%2520Such%2520flexibility%2520makes%2520it%2520suitable%2520for%250Amore%2520complex%2520engagement%2520tasks%2520and%2520interventions%2520in%2520computational%2520social%250Ascience.%250A%2520%2520Using%2520our%2520framework%252C%2520we%2520analyze%2520the%2520performance%2520of%2520LLMs%2520in%2520generating%2520social%250Aengagement%2520under%2520different%2520conditions%252C%2520showcasing%2520their%2520full%2520potential%2520in%2520this%250Atask.%2520The%2520experimental%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mminici/Engagement-Driven-Content-Generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13187v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models&entry.906535625=Erica%20Coppolillo%20and%20Federico%20Cinus%20and%20Marco%20Minici%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20significant%20persuasive%20capabilities%0Ain%20one-on-one%20interactions%2C%20but%20their%20influence%20within%20social%20networks%2C%20where%0Ainterconnected%20users%20and%20complex%20opinion%20dynamics%20pose%20unique%20challenges%2C%0Aremains%20underexplored.%20This%20paper%20addresses%20the%20research%20question%3A%20%5Cemph%7BCan%0ALLMs%20generate%20meaningful%20content%20that%20maximizes%20user%20engagement%20on%20social%0Anetworks%3F%7D%0A%20%20To%20answer%20this%2C%20we%20propose%20a%20pipeline%20using%20reinforcement%20learning%20with%0Asimulated%20feedback%2C%20where%20the%20network%27s%20response%20to%20LLM-generated%20content%0A%28i.e.%2C%20the%20reward%29%20is%20simulated%20through%20a%20formal%20engagement%20model.%20This%0Aapproach%20bypasses%20the%20temporal%20cost%20and%20complexity%20of%20live%20experiments%2C%0Aenabling%20an%20efficient%20feedback%20loop%20between%20the%20LLM%20and%20the%20network%20under%0Astudy.%20It%20also%20allows%20to%20control%20over%20endogenous%20factors%20such%20as%20the%20LLM%27s%0Aposition%20within%20the%20social%20network%20and%20the%20distribution%20of%20opinions%20on%20a%20given%0Atopic.%20Our%20approach%20is%20adaptive%20to%20the%20opinion%20distribution%20of%20the%20underlying%0Anetwork%20and%20agnostic%20to%20the%20specifics%20of%20the%20engagement%20model%2C%20which%20is%0Aembedded%20as%20a%20plug-and-play%20component.%20Such%20flexibility%20makes%20it%20suitable%20for%0Amore%20complex%20engagement%20tasks%20and%20interventions%20in%20computational%20social%0Ascience.%0A%20%20Using%20our%20framework%2C%20we%20analyze%20the%20performance%20of%20LLMs%20in%20generating%20social%0Aengagement%20under%20different%20conditions%2C%20showcasing%20their%20full%20potential%20in%20this%0Atask.%20The%20experimental%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mminici/Engagement-Driven-Content-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13187v4&entry.124074799=Read"},
{"title": "Personalized MR-Informed Diffusion Models for 3D PET Image\n  Reconstruction", "author": "George Webber and Alexander Hammers and Andrew P. King and Andrew J. Reader", "abstract": "  Recent work has shown improved lesion detectability and flexibility to\nreconstruction hyperparameters (e.g. scanner geometry or dose level) when PET\nimages are reconstructed by leveraging pre-trained diffusion models. Such\nmethods train a diffusion model (without sinogram data) on high-quality, but\nstill noisy, PET images. In this work, we propose a simple method for\ngenerating subject-specific PET images from a dataset of multi-subject PET-MR\nscans, synthesizing \"pseudo-PET\" images by transforming between different\npatients' anatomy using image registration. The images we synthesize retain\ninformation from the subject's MR scan, leading to higher resolution and the\nretention of anatomical features compared to the original set of PET images.\nWith simulated and real [$^{18}$F]FDG datasets, we show that pre-training a\npersonalized diffusion model with subject-specific \"pseudo-PET\" images improves\nreconstruction accuracy with low-count data. In particular, the method shows\npromise in combining information from a guidance MR scan without overly\nimposing anatomical features, demonstrating an improved trade-off between\nreconstructing PET-unique image features versus features present in both PET\nand MR. We believe this approach for generating and utilizing synthetic data\nhas further applications to medical imaging tasks, particularly because\npatient-specific PET images can be generated without resorting to generative\ndeep learning or large training datasets.\n", "link": "http://arxiv.org/abs/2506.03804v1", "date": "2025-06-04", "relevancy": 2.4613, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction&body=Title%3A%20Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction%0AAuthor%3A%20George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20improved%20lesion%20detectability%20and%20flexibility%20to%0Areconstruction%20hyperparameters%20%28e.g.%20scanner%20geometry%20or%20dose%20level%29%20when%20PET%0Aimages%20are%20reconstructed%20by%20leveraging%20pre-trained%20diffusion%20models.%20Such%0Amethods%20train%20a%20diffusion%20model%20%28without%20sinogram%20data%29%20on%20high-quality%2C%20but%0Astill%20noisy%2C%20PET%20images.%20In%20this%20work%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20subject-specific%20PET%20images%20from%20a%20dataset%20of%20multi-subject%20PET-MR%0Ascans%2C%20synthesizing%20%22pseudo-PET%22%20images%20by%20transforming%20between%20different%0Apatients%27%20anatomy%20using%20image%20registration.%20The%20images%20we%20synthesize%20retain%0Ainformation%20from%20the%20subject%27s%20MR%20scan%2C%20leading%20to%20higher%20resolution%20and%20the%0Aretention%20of%20anatomical%20features%20compared%20to%20the%20original%20set%20of%20PET%20images.%0AWith%20simulated%20and%20real%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%2C%20we%20show%20that%20pre-training%20a%0Apersonalized%20diffusion%20model%20with%20subject-specific%20%22pseudo-PET%22%20images%20improves%0Areconstruction%20accuracy%20with%20low-count%20data.%20In%20particular%2C%20the%20method%20shows%0Apromise%20in%20combining%20information%20from%20a%20guidance%20MR%20scan%20without%20overly%0Aimposing%20anatomical%20features%2C%20demonstrating%20an%20improved%20trade-off%20between%0Areconstructing%20PET-unique%20image%20features%20versus%20features%20present%20in%20both%20PET%0Aand%20MR.%20We%20believe%20this%20approach%20for%20generating%20and%20utilizing%20synthetic%20data%0Ahas%20further%20applications%20to%20medical%20imaging%20tasks%2C%20particularly%20because%0Apatient-specific%20PET%20images%20can%20be%20generated%20without%20resorting%20to%20generative%0Adeep%20learning%20or%20large%20training%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520MR-Informed%2520Diffusion%2520Models%2520for%25203D%2520PET%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DGeorge%2520Webber%2520and%2520Alexander%2520Hammers%2520and%2520Andrew%2520P.%2520King%2520and%2520Andrew%2520J.%2520Reader%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520improved%2520lesion%2520detectability%2520and%2520flexibility%2520to%250Areconstruction%2520hyperparameters%2520%2528e.g.%2520scanner%2520geometry%2520or%2520dose%2520level%2529%2520when%2520PET%250Aimages%2520are%2520reconstructed%2520by%2520leveraging%2520pre-trained%2520diffusion%2520models.%2520Such%250Amethods%2520train%2520a%2520diffusion%2520model%2520%2528without%2520sinogram%2520data%2529%2520on%2520high-quality%252C%2520but%250Astill%2520noisy%252C%2520PET%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520method%2520for%250Agenerating%2520subject-specific%2520PET%2520images%2520from%2520a%2520dataset%2520of%2520multi-subject%2520PET-MR%250Ascans%252C%2520synthesizing%2520%2522pseudo-PET%2522%2520images%2520by%2520transforming%2520between%2520different%250Apatients%2527%2520anatomy%2520using%2520image%2520registration.%2520The%2520images%2520we%2520synthesize%2520retain%250Ainformation%2520from%2520the%2520subject%2527s%2520MR%2520scan%252C%2520leading%2520to%2520higher%2520resolution%2520and%2520the%250Aretention%2520of%2520anatomical%2520features%2520compared%2520to%2520the%2520original%2520set%2520of%2520PET%2520images.%250AWith%2520simulated%2520and%2520real%2520%255B%2524%255E%257B18%257D%2524F%255DFDG%2520datasets%252C%2520we%2520show%2520that%2520pre-training%2520a%250Apersonalized%2520diffusion%2520model%2520with%2520subject-specific%2520%2522pseudo-PET%2522%2520images%2520improves%250Areconstruction%2520accuracy%2520with%2520low-count%2520data.%2520In%2520particular%252C%2520the%2520method%2520shows%250Apromise%2520in%2520combining%2520information%2520from%2520a%2520guidance%2520MR%2520scan%2520without%2520overly%250Aimposing%2520anatomical%2520features%252C%2520demonstrating%2520an%2520improved%2520trade-off%2520between%250Areconstructing%2520PET-unique%2520image%2520features%2520versus%2520features%2520present%2520in%2520both%2520PET%250Aand%2520MR.%2520We%2520believe%2520this%2520approach%2520for%2520generating%2520and%2520utilizing%2520synthetic%2520data%250Ahas%2520further%2520applications%2520to%2520medical%2520imaging%2520tasks%252C%2520particularly%2520because%250Apatient-specific%2520PET%2520images%2520can%2520be%2520generated%2520without%2520resorting%2520to%2520generative%250Adeep%2520learning%2520or%2520large%2520training%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction&entry.906535625=George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader&entry.1292438233=%20%20Recent%20work%20has%20shown%20improved%20lesion%20detectability%20and%20flexibility%20to%0Areconstruction%20hyperparameters%20%28e.g.%20scanner%20geometry%20or%20dose%20level%29%20when%20PET%0Aimages%20are%20reconstructed%20by%20leveraging%20pre-trained%20diffusion%20models.%20Such%0Amethods%20train%20a%20diffusion%20model%20%28without%20sinogram%20data%29%20on%20high-quality%2C%20but%0Astill%20noisy%2C%20PET%20images.%20In%20this%20work%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20subject-specific%20PET%20images%20from%20a%20dataset%20of%20multi-subject%20PET-MR%0Ascans%2C%20synthesizing%20%22pseudo-PET%22%20images%20by%20transforming%20between%20different%0Apatients%27%20anatomy%20using%20image%20registration.%20The%20images%20we%20synthesize%20retain%0Ainformation%20from%20the%20subject%27s%20MR%20scan%2C%20leading%20to%20higher%20resolution%20and%20the%0Aretention%20of%20anatomical%20features%20compared%20to%20the%20original%20set%20of%20PET%20images.%0AWith%20simulated%20and%20real%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%2C%20we%20show%20that%20pre-training%20a%0Apersonalized%20diffusion%20model%20with%20subject-specific%20%22pseudo-PET%22%20images%20improves%0Areconstruction%20accuracy%20with%20low-count%20data.%20In%20particular%2C%20the%20method%20shows%0Apromise%20in%20combining%20information%20from%20a%20guidance%20MR%20scan%20without%20overly%0Aimposing%20anatomical%20features%2C%20demonstrating%20an%20improved%20trade-off%20between%0Areconstructing%20PET-unique%20image%20features%20versus%20features%20present%20in%20both%20PET%0Aand%20MR.%20We%20believe%20this%20approach%20for%20generating%20and%20utilizing%20synthetic%20data%0Ahas%20further%20applications%20to%20medical%20imaging%20tasks%2C%20particularly%20because%0Apatient-specific%20PET%20images%20can%20be%20generated%20without%20resorting%20to%20generative%0Adeep%20learning%20or%20large%20training%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03804v1&entry.124074799=Read"},
{"title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency\n  Enhancement Framework for 4D MRI imaging", "author": "Xuanru Zhou and Jiarun Liu and Shoujun Yu and Hao Yang and Cheng Li and Tao Tan and Shanshan Wang", "abstract": "  In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency.\n", "link": "http://arxiv.org/abs/2506.04116v1", "date": "2025-06-04", "relevancy": 2.4485, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6224}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6101}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Diffusion-Driven%20Temporal%20Super-Resolution%20and%20Spatial%20Consistency%0A%20%20Enhancement%20Framework%20for%204D%20MRI%20imaging&body=Title%3A%20A%20Diffusion-Driven%20Temporal%20Super-Resolution%20and%20Spatial%20Consistency%0A%20%20Enhancement%20Framework%20for%204D%20MRI%20imaging%0AAuthor%3A%20Xuanru%20Zhou%20and%20Jiarun%20Liu%20and%20Shoujun%20Yu%20and%20Hao%20Yang%20and%20Cheng%20Li%20and%20Tao%20Tan%20and%20Shanshan%20Wang%0AAbstract%3A%20%20%20In%20medical%20imaging%2C%204D%20MRI%20enables%20dynamic%203D%20visualization%2C%20yet%20the%0Atrade-off%20between%20spatial%20and%20temporal%20resolution%20requires%20prolonged%20scan%20time%0Athat%20can%20compromise%20temporal%20fidelity--especially%20during%20rapid%2C%20large-amplitude%0Amotion.%20Traditional%20approaches%20typically%20rely%20on%20registration-based%0Ainterpolation%20to%20generate%20intermediate%20frames.%20However%2C%20these%20methods%20struggle%0Awith%20large%20deformations%2C%20resulting%20in%20misregistration%2C%20artifacts%2C%20and%0Adiminished%20spatial%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%0ATSSC-Net%2C%20a%20novel%20framework%20that%20generates%20intermediate%20frames%20while%20preserving%0Aspatial%20consistency.%20To%20improve%20temporal%20fidelity%20under%20fast%20motion%2C%20our%0Adiffusion-based%20temporal%20super-resolution%20network%20generates%20intermediate%20frames%0Ausing%20the%20start%20and%20end%20frames%20as%20key%20references%2C%20achieving%206x%20temporal%0Asuper-resolution%20in%20a%20single%20inference%20step.%20Additionally%2C%20we%20introduce%20a%20novel%0Atri-directional%20Mamba-based%20module%20that%20leverages%20long-range%20contextual%0Ainformation%20to%20effectively%20resolve%20spatial%20inconsistencies%20arising%20from%0Across-slice%20misalignment%2C%20thereby%20enhancing%20volumetric%20coherence%20and%20correcting%0Across-slice%20errors.%20Extensive%20experiments%20were%20performed%20on%20the%20public%20ACDC%0Acardiac%20MRI%20dataset%20and%20a%20real-world%20dynamic%204D%20knee%20joint%20dataset.%20The%20results%0Ademonstrate%20that%20TSSC-Net%20can%20generate%20high-resolution%20dynamic%20MRI%20from%0Afast-motion%20data%20while%20preserving%20structural%20fidelity%20and%20spatial%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Diffusion-Driven%2520Temporal%2520Super-Resolution%2520and%2520Spatial%2520Consistency%250A%2520%2520Enhancement%2520Framework%2520for%25204D%2520MRI%2520imaging%26entry.906535625%3DXuanru%2520Zhou%2520and%2520Jiarun%2520Liu%2520and%2520Shoujun%2520Yu%2520and%2520Hao%2520Yang%2520and%2520Cheng%2520Li%2520and%2520Tao%2520Tan%2520and%2520Shanshan%2520Wang%26entry.1292438233%3D%2520%2520In%2520medical%2520imaging%252C%25204D%2520MRI%2520enables%2520dynamic%25203D%2520visualization%252C%2520yet%2520the%250Atrade-off%2520between%2520spatial%2520and%2520temporal%2520resolution%2520requires%2520prolonged%2520scan%2520time%250Athat%2520can%2520compromise%2520temporal%2520fidelity--especially%2520during%2520rapid%252C%2520large-amplitude%250Amotion.%2520Traditional%2520approaches%2520typically%2520rely%2520on%2520registration-based%250Ainterpolation%2520to%2520generate%2520intermediate%2520frames.%2520However%252C%2520these%2520methods%2520struggle%250Awith%2520large%2520deformations%252C%2520resulting%2520in%2520misregistration%252C%2520artifacts%252C%2520and%250Adiminished%2520spatial%2520consistency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250ATSSC-Net%252C%2520a%2520novel%2520framework%2520that%2520generates%2520intermediate%2520frames%2520while%2520preserving%250Aspatial%2520consistency.%2520To%2520improve%2520temporal%2520fidelity%2520under%2520fast%2520motion%252C%2520our%250Adiffusion-based%2520temporal%2520super-resolution%2520network%2520generates%2520intermediate%2520frames%250Ausing%2520the%2520start%2520and%2520end%2520frames%2520as%2520key%2520references%252C%2520achieving%25206x%2520temporal%250Asuper-resolution%2520in%2520a%2520single%2520inference%2520step.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%250Atri-directional%2520Mamba-based%2520module%2520that%2520leverages%2520long-range%2520contextual%250Ainformation%2520to%2520effectively%2520resolve%2520spatial%2520inconsistencies%2520arising%2520from%250Across-slice%2520misalignment%252C%2520thereby%2520enhancing%2520volumetric%2520coherence%2520and%2520correcting%250Across-slice%2520errors.%2520Extensive%2520experiments%2520were%2520performed%2520on%2520the%2520public%2520ACDC%250Acardiac%2520MRI%2520dataset%2520and%2520a%2520real-world%2520dynamic%25204D%2520knee%2520joint%2520dataset.%2520The%2520results%250Ademonstrate%2520that%2520TSSC-Net%2520can%2520generate%2520high-resolution%2520dynamic%2520MRI%2520from%250Afast-motion%2520data%2520while%2520preserving%2520structural%2520fidelity%2520and%2520spatial%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Diffusion-Driven%20Temporal%20Super-Resolution%20and%20Spatial%20Consistency%0A%20%20Enhancement%20Framework%20for%204D%20MRI%20imaging&entry.906535625=Xuanru%20Zhou%20and%20Jiarun%20Liu%20and%20Shoujun%20Yu%20and%20Hao%20Yang%20and%20Cheng%20Li%20and%20Tao%20Tan%20and%20Shanshan%20Wang&entry.1292438233=%20%20In%20medical%20imaging%2C%204D%20MRI%20enables%20dynamic%203D%20visualization%2C%20yet%20the%0Atrade-off%20between%20spatial%20and%20temporal%20resolution%20requires%20prolonged%20scan%20time%0Athat%20can%20compromise%20temporal%20fidelity--especially%20during%20rapid%2C%20large-amplitude%0Amotion.%20Traditional%20approaches%20typically%20rely%20on%20registration-based%0Ainterpolation%20to%20generate%20intermediate%20frames.%20However%2C%20these%20methods%20struggle%0Awith%20large%20deformations%2C%20resulting%20in%20misregistration%2C%20artifacts%2C%20and%0Adiminished%20spatial%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%0ATSSC-Net%2C%20a%20novel%20framework%20that%20generates%20intermediate%20frames%20while%20preserving%0Aspatial%20consistency.%20To%20improve%20temporal%20fidelity%20under%20fast%20motion%2C%20our%0Adiffusion-based%20temporal%20super-resolution%20network%20generates%20intermediate%20frames%0Ausing%20the%20start%20and%20end%20frames%20as%20key%20references%2C%20achieving%206x%20temporal%0Asuper-resolution%20in%20a%20single%20inference%20step.%20Additionally%2C%20we%20introduce%20a%20novel%0Atri-directional%20Mamba-based%20module%20that%20leverages%20long-range%20contextual%0Ainformation%20to%20effectively%20resolve%20spatial%20inconsistencies%20arising%20from%0Across-slice%20misalignment%2C%20thereby%20enhancing%20volumetric%20coherence%20and%20correcting%0Across-slice%20errors.%20Extensive%20experiments%20were%20performed%20on%20the%20public%20ACDC%0Acardiac%20MRI%20dataset%20and%20a%20real-world%20dynamic%204D%20knee%20joint%20dataset.%20The%20results%0Ademonstrate%20that%20TSSC-Net%20can%20generate%20high-resolution%20dynamic%20MRI%20from%0Afast-motion%20data%20while%20preserving%20structural%20fidelity%20and%20spatial%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04116v1&entry.124074799=Read"},
{"title": "Identifying Alzheimer's Disease Prediction Strategies of Convolutional\n  Neural Network Classifiers using R2* Maps and Spectral Clustering", "author": "Christian Tinauer and Maximilian Sackl and Stefan Ropele and Christian Langkammer", "abstract": "  Deep learning models have shown strong performance in classifying Alzheimer's\ndisease (AD) from R2* maps, but their decision-making remains opaque, raising\nconcerns about interpretability. Previous studies suggest biases in model\ndecisions, necessitating further analysis. This study uses Layer-wise Relevance\nPropagation (LRP) and spectral clustering to explore classifier decision\nstrategies across preprocessing and training configurations using R2* maps. We\ntrained a 3D convolutional neural network on R2* maps, generating relevance\nheatmaps via LRP and applied spectral clustering to identify dominant patterns.\nt-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess\nclustering structure. Spectral clustering revealed distinct decision patterns,\nwith the relevance-guided model showing the clearest separation between AD and\nnormal control (NC) cases. The t-SNE visualization confirmed that this model\naligned heatmap groupings with the underlying subject groups. Our findings\nhighlight the significant impact of preprocessing and training choices on deep\nlearning models trained on R2* maps, even with similar performance metrics.\nSpectral clustering offers a structured method to identify classification\nstrategy differences, emphasizing the importance of explainability in medical\nAI.\n", "link": "http://arxiv.org/abs/2506.03890v1", "date": "2025-06-04", "relevancy": 2.4458, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Alzheimer%27s%20Disease%20Prediction%20Strategies%20of%20Convolutional%0A%20%20Neural%20Network%20Classifiers%20using%20R2%2A%20Maps%20and%20Spectral%20Clustering&body=Title%3A%20Identifying%20Alzheimer%27s%20Disease%20Prediction%20Strategies%20of%20Convolutional%0A%20%20Neural%20Network%20Classifiers%20using%20R2%2A%20Maps%20and%20Spectral%20Clustering%0AAuthor%3A%20Christian%20Tinauer%20and%20Maximilian%20Sackl%20and%20Stefan%20Ropele%20and%20Christian%20Langkammer%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20shown%20strong%20performance%20in%20classifying%20Alzheimer%27s%0Adisease%20%28AD%29%20from%20R2%2A%20maps%2C%20but%20their%20decision-making%20remains%20opaque%2C%20raising%0Aconcerns%20about%20interpretability.%20Previous%20studies%20suggest%20biases%20in%20model%0Adecisions%2C%20necessitating%20further%20analysis.%20This%20study%20uses%20Layer-wise%20Relevance%0APropagation%20%28LRP%29%20and%20spectral%20clustering%20to%20explore%20classifier%20decision%0Astrategies%20across%20preprocessing%20and%20training%20configurations%20using%20R2%2A%20maps.%20We%0Atrained%20a%203D%20convolutional%20neural%20network%20on%20R2%2A%20maps%2C%20generating%20relevance%0Aheatmaps%20via%20LRP%20and%20applied%20spectral%20clustering%20to%20identify%20dominant%20patterns.%0At-Stochastic%20Neighbor%20Embedding%20%28t-SNE%29%20visualization%20was%20used%20to%20assess%0Aclustering%20structure.%20Spectral%20clustering%20revealed%20distinct%20decision%20patterns%2C%0Awith%20the%20relevance-guided%20model%20showing%20the%20clearest%20separation%20between%20AD%20and%0Anormal%20control%20%28NC%29%20cases.%20The%20t-SNE%20visualization%20confirmed%20that%20this%20model%0Aaligned%20heatmap%20groupings%20with%20the%20underlying%20subject%20groups.%20Our%20findings%0Ahighlight%20the%20significant%20impact%20of%20preprocessing%20and%20training%20choices%20on%20deep%0Alearning%20models%20trained%20on%20R2%2A%20maps%2C%20even%20with%20similar%20performance%20metrics.%0ASpectral%20clustering%20offers%20a%20structured%20method%20to%20identify%20classification%0Astrategy%20differences%2C%20emphasizing%20the%20importance%20of%20explainability%20in%20medical%0AAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Alzheimer%2527s%2520Disease%2520Prediction%2520Strategies%2520of%2520Convolutional%250A%2520%2520Neural%2520Network%2520Classifiers%2520using%2520R2%252A%2520Maps%2520and%2520Spectral%2520Clustering%26entry.906535625%3DChristian%2520Tinauer%2520and%2520Maximilian%2520Sackl%2520and%2520Stefan%2520Ropele%2520and%2520Christian%2520Langkammer%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520shown%2520strong%2520performance%2520in%2520classifying%2520Alzheimer%2527s%250Adisease%2520%2528AD%2529%2520from%2520R2%252A%2520maps%252C%2520but%2520their%2520decision-making%2520remains%2520opaque%252C%2520raising%250Aconcerns%2520about%2520interpretability.%2520Previous%2520studies%2520suggest%2520biases%2520in%2520model%250Adecisions%252C%2520necessitating%2520further%2520analysis.%2520This%2520study%2520uses%2520Layer-wise%2520Relevance%250APropagation%2520%2528LRP%2529%2520and%2520spectral%2520clustering%2520to%2520explore%2520classifier%2520decision%250Astrategies%2520across%2520preprocessing%2520and%2520training%2520configurations%2520using%2520R2%252A%2520maps.%2520We%250Atrained%2520a%25203D%2520convolutional%2520neural%2520network%2520on%2520R2%252A%2520maps%252C%2520generating%2520relevance%250Aheatmaps%2520via%2520LRP%2520and%2520applied%2520spectral%2520clustering%2520to%2520identify%2520dominant%2520patterns.%250At-Stochastic%2520Neighbor%2520Embedding%2520%2528t-SNE%2529%2520visualization%2520was%2520used%2520to%2520assess%250Aclustering%2520structure.%2520Spectral%2520clustering%2520revealed%2520distinct%2520decision%2520patterns%252C%250Awith%2520the%2520relevance-guided%2520model%2520showing%2520the%2520clearest%2520separation%2520between%2520AD%2520and%250Anormal%2520control%2520%2528NC%2529%2520cases.%2520The%2520t-SNE%2520visualization%2520confirmed%2520that%2520this%2520model%250Aaligned%2520heatmap%2520groupings%2520with%2520the%2520underlying%2520subject%2520groups.%2520Our%2520findings%250Ahighlight%2520the%2520significant%2520impact%2520of%2520preprocessing%2520and%2520training%2520choices%2520on%2520deep%250Alearning%2520models%2520trained%2520on%2520R2%252A%2520maps%252C%2520even%2520with%2520similar%2520performance%2520metrics.%250ASpectral%2520clustering%2520offers%2520a%2520structured%2520method%2520to%2520identify%2520classification%250Astrategy%2520differences%252C%2520emphasizing%2520the%2520importance%2520of%2520explainability%2520in%2520medical%250AAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Alzheimer%27s%20Disease%20Prediction%20Strategies%20of%20Convolutional%0A%20%20Neural%20Network%20Classifiers%20using%20R2%2A%20Maps%20and%20Spectral%20Clustering&entry.906535625=Christian%20Tinauer%20and%20Maximilian%20Sackl%20and%20Stefan%20Ropele%20and%20Christian%20Langkammer&entry.1292438233=%20%20Deep%20learning%20models%20have%20shown%20strong%20performance%20in%20classifying%20Alzheimer%27s%0Adisease%20%28AD%29%20from%20R2%2A%20maps%2C%20but%20their%20decision-making%20remains%20opaque%2C%20raising%0Aconcerns%20about%20interpretability.%20Previous%20studies%20suggest%20biases%20in%20model%0Adecisions%2C%20necessitating%20further%20analysis.%20This%20study%20uses%20Layer-wise%20Relevance%0APropagation%20%28LRP%29%20and%20spectral%20clustering%20to%20explore%20classifier%20decision%0Astrategies%20across%20preprocessing%20and%20training%20configurations%20using%20R2%2A%20maps.%20We%0Atrained%20a%203D%20convolutional%20neural%20network%20on%20R2%2A%20maps%2C%20generating%20relevance%0Aheatmaps%20via%20LRP%20and%20applied%20spectral%20clustering%20to%20identify%20dominant%20patterns.%0At-Stochastic%20Neighbor%20Embedding%20%28t-SNE%29%20visualization%20was%20used%20to%20assess%0Aclustering%20structure.%20Spectral%20clustering%20revealed%20distinct%20decision%20patterns%2C%0Awith%20the%20relevance-guided%20model%20showing%20the%20clearest%20separation%20between%20AD%20and%0Anormal%20control%20%28NC%29%20cases.%20The%20t-SNE%20visualization%20confirmed%20that%20this%20model%0Aaligned%20heatmap%20groupings%20with%20the%20underlying%20subject%20groups.%20Our%20findings%0Ahighlight%20the%20significant%20impact%20of%20preprocessing%20and%20training%20choices%20on%20deep%0Alearning%20models%20trained%20on%20R2%2A%20maps%2C%20even%20with%20similar%20performance%20metrics.%0ASpectral%20clustering%20offers%20a%20structured%20method%20to%20identify%20classification%0Astrategy%20differences%2C%20emphasizing%20the%20importance%20of%20explainability%20in%20medical%0AAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03890v1&entry.124074799=Read"},
{"title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "author": "Soyoung Oh and Xinting Huang and Mathis Pink and Michael Hahn and Vera Demberg", "abstract": "  Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer.\n", "link": "http://arxiv.org/abs/2506.01723v2", "date": "2025-06-04", "relevancy": 2.4355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs&body=Title%3A%20Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs%0AAuthor%3A%20Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg%0AAbstract%3A%20%20%20Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%0Anon-compositional%20figurative%20meanings%2C%20which%20often%20strongly%20diverge%20from%20the%0Aidiom%27s%20literal%20interpretation.%20This%20duality%20requires%20a%20model%20to%20learn%0Arepresenting%20and%20deciding%20between%20the%20two%20meanings%20to%20interpret%20an%20idiom%20in%20a%0Afigurative%20sense%2C%20or%20literally.%20In%20this%20paper%2C%20we%20employ%20tools%20from%20mechanistic%0Ainterpretability%20to%20trace%20how%20a%20large%20pretrained%20causal%20transformer%0A%28LLama3.2-1B-base%29%20deals%20with%20this%20ambiguity.%20We%20localize%20three%20steps%20of%20idiom%0Aprocessing%3A%20First%2C%20the%20idiom%27s%20figurative%20meaning%20is%20retrieved%20in%20early%0Aattention%20and%20MLP%20sublayers.%20We%20identify%20specific%20attention%20heads%20which%20boost%0Athe%20figurative%20meaning%20of%20the%20idiom%20while%20suppressing%20the%20idiom%27s%20literal%0Ainterpretation.%20The%20model%20subsequently%20represents%20the%20figurative%20representation%0Athrough%20an%20intermediate%20path.%20Meanwhile%2C%20a%20parallel%20bypass%20route%20forwards%0Aliteral%20interpretation%2C%20ensuring%20that%20a%20both%20reading%20remain%20available.%20Overall%2C%0Aour%20findings%20provide%20a%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20an%0Aautoregressive%20transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTug-of-war%2520between%2520idiom%2527s%2520figurative%2520and%2520literal%2520meanings%2520in%2520LLMs%26entry.906535625%3DSoyoung%2520Oh%2520and%2520Xinting%2520Huang%2520and%2520Mathis%2520Pink%2520and%2520Michael%2520Hahn%2520and%2520Vera%2520Demberg%26entry.1292438233%3D%2520%2520Idioms%2520present%2520a%2520unique%2520challenge%2520for%2520language%2520models%2520due%2520to%2520their%250Anon-compositional%2520figurative%2520meanings%252C%2520which%2520often%2520strongly%2520diverge%2520from%2520the%250Aidiom%2527s%2520literal%2520interpretation.%2520This%2520duality%2520requires%2520a%2520model%2520to%2520learn%250Arepresenting%2520and%2520deciding%2520between%2520the%2520two%2520meanings%2520to%2520interpret%2520an%2520idiom%2520in%2520a%250Afigurative%2520sense%252C%2520or%2520literally.%2520In%2520this%2520paper%252C%2520we%2520employ%2520tools%2520from%2520mechanistic%250Ainterpretability%2520to%2520trace%2520how%2520a%2520large%2520pretrained%2520causal%2520transformer%250A%2528LLama3.2-1B-base%2529%2520deals%2520with%2520this%2520ambiguity.%2520We%2520localize%2520three%2520steps%2520of%2520idiom%250Aprocessing%253A%2520First%252C%2520the%2520idiom%2527s%2520figurative%2520meaning%2520is%2520retrieved%2520in%2520early%250Aattention%2520and%2520MLP%2520sublayers.%2520We%2520identify%2520specific%2520attention%2520heads%2520which%2520boost%250Athe%2520figurative%2520meaning%2520of%2520the%2520idiom%2520while%2520suppressing%2520the%2520idiom%2527s%2520literal%250Ainterpretation.%2520The%2520model%2520subsequently%2520represents%2520the%2520figurative%2520representation%250Athrough%2520an%2520intermediate%2520path.%2520Meanwhile%252C%2520a%2520parallel%2520bypass%2520route%2520forwards%250Aliteral%2520interpretation%252C%2520ensuring%2520that%2520a%2520both%2520reading%2520remain%2520available.%2520Overall%252C%250Aour%2520findings%2520provide%2520a%2520mechanistic%2520evidence%2520for%2520idiom%2520comprehension%2520in%2520an%250Aautoregressive%2520transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tug-of-war%20between%20idiom%27s%20figurative%20and%20literal%20meanings%20in%20LLMs&entry.906535625=Soyoung%20Oh%20and%20Xinting%20Huang%20and%20Mathis%20Pink%20and%20Michael%20Hahn%20and%20Vera%20Demberg&entry.1292438233=%20%20Idioms%20present%20a%20unique%20challenge%20for%20language%20models%20due%20to%20their%0Anon-compositional%20figurative%20meanings%2C%20which%20often%20strongly%20diverge%20from%20the%0Aidiom%27s%20literal%20interpretation.%20This%20duality%20requires%20a%20model%20to%20learn%0Arepresenting%20and%20deciding%20between%20the%20two%20meanings%20to%20interpret%20an%20idiom%20in%20a%0Afigurative%20sense%2C%20or%20literally.%20In%20this%20paper%2C%20we%20employ%20tools%20from%20mechanistic%0Ainterpretability%20to%20trace%20how%20a%20large%20pretrained%20causal%20transformer%0A%28LLama3.2-1B-base%29%20deals%20with%20this%20ambiguity.%20We%20localize%20three%20steps%20of%20idiom%0Aprocessing%3A%20First%2C%20the%20idiom%27s%20figurative%20meaning%20is%20retrieved%20in%20early%0Aattention%20and%20MLP%20sublayers.%20We%20identify%20specific%20attention%20heads%20which%20boost%0Athe%20figurative%20meaning%20of%20the%20idiom%20while%20suppressing%20the%20idiom%27s%20literal%0Ainterpretation.%20The%20model%20subsequently%20represents%20the%20figurative%20representation%0Athrough%20an%20intermediate%20path.%20Meanwhile%2C%20a%20parallel%20bypass%20route%20forwards%0Aliteral%20interpretation%2C%20ensuring%20that%20a%20both%20reading%20remain%20available.%20Overall%2C%0Aour%20findings%20provide%20a%20mechanistic%20evidence%20for%20idiom%20comprehension%20in%20an%0Aautoregressive%20transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01723v2&entry.124074799=Read"},
{"title": "CondiMen: Conditional Multi-Person Mesh Recovery", "author": "Br\u00e9gier Romain and Baradel Fabien and Lucas Thomas and Galaaoui Salma and Armando Matthieu and Weinzaepfel Philippe and Rogez Gr\u00e9gory", "abstract": "  Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand.\n", "link": "http://arxiv.org/abs/2412.13058v2", "date": "2025-06-04", "relevancy": 2.4069, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6305}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery&body=Title%3A%20CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery%0AAuthor%3A%20Br%C3%A9gier%20Romain%20and%20Baradel%20Fabien%20and%20Lucas%20Thomas%20and%20Galaaoui%20Salma%20and%20Armando%20Matthieu%20and%20Weinzaepfel%20Philippe%20and%20Rogez%20Gr%C3%A9gory%0AAbstract%3A%20%20%20Multi-person%20human%20mesh%20recovery%20%28HMR%29%20consists%20in%20detecting%20all%20individuals%0Ain%20a%20given%20input%20image%2C%20and%20predicting%20the%20body%20shape%2C%20pose%2C%20and%203D%20location%0Afor%20each%20detected%20person.%20The%20dominant%20approaches%20to%20this%20task%20rely%20on%20neural%0Anetworks%20trained%20to%20output%20a%20single%20prediction%20for%20each%20detected%20individual.%20In%0Acontrast%2C%20we%20propose%20CondiMen%2C%20a%20method%20that%20outputs%20a%20joint%20parametric%0Adistribution%20over%20likely%20poses%2C%20body%20shapes%2C%20intrinsics%20and%20distances%20to%20the%0Acamera%2C%20using%20a%20Bayesian%20network.%20This%20approach%20offers%20several%20advantages.%0AFirst%2C%20a%20probability%20distribution%20can%20handle%20some%20inherent%20ambiguities%20of%20this%0Atask%20--%20such%20as%20the%20uncertainty%20between%20a%20person%27s%20size%20and%20their%20distance%20to%0Athe%20camera%2C%20or%20simply%20the%20loss%20of%20information%20when%20projecting%203D%20data%20onto%20the%0A2D%20image%20plane.%20Second%2C%20the%20output%20distribution%20can%20be%20combined%20with%20additional%0Ainformation%20to%20produce%20better%20predictions%2C%20by%20using%20e.g.%20known%20camera%20or%20body%0Ashape%20parameters%2C%20or%20by%20exploiting%20multi-view%20observations.%20Third%2C%20one%20can%0Aefficiently%20extract%20the%20most%20likely%20predictions%20from%20the%20output%20distribution%2C%0Amaking%20our%20proposed%20approach%20suitable%20for%20real-time%20applications.%20Empirically%0Awe%20find%20that%20our%20model%20i%29%20achieves%20performance%20on%20par%20with%20or%20better%20than%20the%0Astate-of-the-art%2C%20ii%29%20captures%20uncertainties%20and%20correlations%20inherent%20in%20pose%0Aestimation%20and%20iii%29%20can%20exploit%20additional%20information%20at%20test%20time%2C%20such%20as%0Amulti-view%20consistency%20or%20body%20shape%20priors.%20CondiMen%20spices%20up%20the%20modeling%20of%0Aambiguity%2C%20using%20just%20the%20right%20ingredients%20on%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCondiMen%253A%2520Conditional%2520Multi-Person%2520Mesh%2520Recovery%26entry.906535625%3DBr%25C3%25A9gier%2520Romain%2520and%2520Baradel%2520Fabien%2520and%2520Lucas%2520Thomas%2520and%2520Galaaoui%2520Salma%2520and%2520Armando%2520Matthieu%2520and%2520Weinzaepfel%2520Philippe%2520and%2520Rogez%2520Gr%25C3%25A9gory%26entry.1292438233%3D%2520%2520Multi-person%2520human%2520mesh%2520recovery%2520%2528HMR%2529%2520consists%2520in%2520detecting%2520all%2520individuals%250Ain%2520a%2520given%2520input%2520image%252C%2520and%2520predicting%2520the%2520body%2520shape%252C%2520pose%252C%2520and%25203D%2520location%250Afor%2520each%2520detected%2520person.%2520The%2520dominant%2520approaches%2520to%2520this%2520task%2520rely%2520on%2520neural%250Anetworks%2520trained%2520to%2520output%2520a%2520single%2520prediction%2520for%2520each%2520detected%2520individual.%2520In%250Acontrast%252C%2520we%2520propose%2520CondiMen%252C%2520a%2520method%2520that%2520outputs%2520a%2520joint%2520parametric%250Adistribution%2520over%2520likely%2520poses%252C%2520body%2520shapes%252C%2520intrinsics%2520and%2520distances%2520to%2520the%250Acamera%252C%2520using%2520a%2520Bayesian%2520network.%2520This%2520approach%2520offers%2520several%2520advantages.%250AFirst%252C%2520a%2520probability%2520distribution%2520can%2520handle%2520some%2520inherent%2520ambiguities%2520of%2520this%250Atask%2520--%2520such%2520as%2520the%2520uncertainty%2520between%2520a%2520person%2527s%2520size%2520and%2520their%2520distance%2520to%250Athe%2520camera%252C%2520or%2520simply%2520the%2520loss%2520of%2520information%2520when%2520projecting%25203D%2520data%2520onto%2520the%250A2D%2520image%2520plane.%2520Second%252C%2520the%2520output%2520distribution%2520can%2520be%2520combined%2520with%2520additional%250Ainformation%2520to%2520produce%2520better%2520predictions%252C%2520by%2520using%2520e.g.%2520known%2520camera%2520or%2520body%250Ashape%2520parameters%252C%2520or%2520by%2520exploiting%2520multi-view%2520observations.%2520Third%252C%2520one%2520can%250Aefficiently%2520extract%2520the%2520most%2520likely%2520predictions%2520from%2520the%2520output%2520distribution%252C%250Amaking%2520our%2520proposed%2520approach%2520suitable%2520for%2520real-time%2520applications.%2520Empirically%250Awe%2520find%2520that%2520our%2520model%2520i%2529%2520achieves%2520performance%2520on%2520par%2520with%2520or%2520better%2520than%2520the%250Astate-of-the-art%252C%2520ii%2529%2520captures%2520uncertainties%2520and%2520correlations%2520inherent%2520in%2520pose%250Aestimation%2520and%2520iii%2529%2520can%2520exploit%2520additional%2520information%2520at%2520test%2520time%252C%2520such%2520as%250Amulti-view%2520consistency%2520or%2520body%2520shape%2520priors.%2520CondiMen%2520spices%2520up%2520the%2520modeling%2520of%250Aambiguity%252C%2520using%2520just%2520the%2520right%2520ingredients%2520on%2520hand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery&entry.906535625=Br%C3%A9gier%20Romain%20and%20Baradel%20Fabien%20and%20Lucas%20Thomas%20and%20Galaaoui%20Salma%20and%20Armando%20Matthieu%20and%20Weinzaepfel%20Philippe%20and%20Rogez%20Gr%C3%A9gory&entry.1292438233=%20%20Multi-person%20human%20mesh%20recovery%20%28HMR%29%20consists%20in%20detecting%20all%20individuals%0Ain%20a%20given%20input%20image%2C%20and%20predicting%20the%20body%20shape%2C%20pose%2C%20and%203D%20location%0Afor%20each%20detected%20person.%20The%20dominant%20approaches%20to%20this%20task%20rely%20on%20neural%0Anetworks%20trained%20to%20output%20a%20single%20prediction%20for%20each%20detected%20individual.%20In%0Acontrast%2C%20we%20propose%20CondiMen%2C%20a%20method%20that%20outputs%20a%20joint%20parametric%0Adistribution%20over%20likely%20poses%2C%20body%20shapes%2C%20intrinsics%20and%20distances%20to%20the%0Acamera%2C%20using%20a%20Bayesian%20network.%20This%20approach%20offers%20several%20advantages.%0AFirst%2C%20a%20probability%20distribution%20can%20handle%20some%20inherent%20ambiguities%20of%20this%0Atask%20--%20such%20as%20the%20uncertainty%20between%20a%20person%27s%20size%20and%20their%20distance%20to%0Athe%20camera%2C%20or%20simply%20the%20loss%20of%20information%20when%20projecting%203D%20data%20onto%20the%0A2D%20image%20plane.%20Second%2C%20the%20output%20distribution%20can%20be%20combined%20with%20additional%0Ainformation%20to%20produce%20better%20predictions%2C%20by%20using%20e.g.%20known%20camera%20or%20body%0Ashape%20parameters%2C%20or%20by%20exploiting%20multi-view%20observations.%20Third%2C%20one%20can%0Aefficiently%20extract%20the%20most%20likely%20predictions%20from%20the%20output%20distribution%2C%0Amaking%20our%20proposed%20approach%20suitable%20for%20real-time%20applications.%20Empirically%0Awe%20find%20that%20our%20model%20i%29%20achieves%20performance%20on%20par%20with%20or%20better%20than%20the%0Astate-of-the-art%2C%20ii%29%20captures%20uncertainties%20and%20correlations%20inherent%20in%20pose%0Aestimation%20and%20iii%29%20can%20exploit%20additional%20information%20at%20test%20time%2C%20such%20as%0Amulti-view%20consistency%20or%20body%20shape%20priors.%20CondiMen%20spices%20up%20the%20modeling%20of%0Aambiguity%2C%20using%20just%20the%20right%20ingredients%20on%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13058v2&entry.124074799=Read"},
{"title": "Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford\n  Day-and-Night Dataset", "author": "Zirui Wang and Wenjing Bian and Xinghui Li and Yifu Tao and Jianeng Wang and Maurice Fallon and Victor Adrian Prisacariu", "abstract": "  We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for\nnovel view synthesis (NVS) and visual relocalisation under challenging lighting\nconditions. Existing datasets often lack crucial combinations of features such\nas ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF\nmotion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA\nglasses to capture egocentric video and applying multi-session SLAM to estimate\ncamera poses, reconstruct 3D point clouds, and align sequences captured under\nvarying lighting conditions, including both day and night. The dataset spans\nover 30 $\\mathrm{km}$ of recorded trajectories and covers an area of 40,000\n$\\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.\nIt supports two core benchmarks, NVS and relocalisation, providing a unique\nplatform for evaluating models in realistic and diverse environments.\n", "link": "http://arxiv.org/abs/2506.04224v1", "date": "2025-06-04", "relevancy": 2.4034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6059}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20in%20the%20Dark%3A%20Benchmarking%20Egocentric%203D%20Vision%20with%20the%20Oxford%0A%20%20Day-and-Night%20Dataset&body=Title%3A%20Seeing%20in%20the%20Dark%3A%20Benchmarking%20Egocentric%203D%20Vision%20with%20the%20Oxford%0A%20%20Day-and-Night%20Dataset%0AAuthor%3A%20Zirui%20Wang%20and%20Wenjing%20Bian%20and%20Xinghui%20Li%20and%20Yifu%20Tao%20and%20Jianeng%20Wang%20and%20Maurice%20Fallon%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20introduce%20Oxford%20Day-and-Night%2C%20a%20large-scale%2C%20egocentric%20dataset%20for%0Anovel%20view%20synthesis%20%28NVS%29%20and%20visual%20relocalisation%20under%20challenging%20lighting%0Aconditions.%20Existing%20datasets%20often%20lack%20crucial%20combinations%20of%20features%20such%0Aas%20ground-truth%203D%20geometry%2C%20wide-ranging%20lighting%20variation%2C%20and%20full%206DoF%0Amotion.%20Oxford%20Day-and-Night%20addresses%20these%20gaps%20by%20leveraging%20Meta%20ARIA%0Aglasses%20to%20capture%20egocentric%20video%20and%20applying%20multi-session%20SLAM%20to%20estimate%0Acamera%20poses%2C%20reconstruct%203D%20point%20clouds%2C%20and%20align%20sequences%20captured%20under%0Avarying%20lighting%20conditions%2C%20including%20both%20day%20and%20night.%20The%20dataset%20spans%0Aover%2030%20%24%5Cmathrm%7Bkm%7D%24%20of%20recorded%20trajectories%20and%20covers%20an%20area%20of%2040%2C000%0A%24%5Cmathrm%7Bm%7D%5E2%24%2C%20offering%20a%20rich%20foundation%20for%20egocentric%203D%20vision%20research.%0AIt%20supports%20two%20core%20benchmarks%2C%20NVS%20and%20relocalisation%2C%20providing%20a%20unique%0Aplatform%20for%20evaluating%20models%20in%20realistic%20and%20diverse%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520in%2520the%2520Dark%253A%2520Benchmarking%2520Egocentric%25203D%2520Vision%2520with%2520the%2520Oxford%250A%2520%2520Day-and-Night%2520Dataset%26entry.906535625%3DZirui%2520Wang%2520and%2520Wenjing%2520Bian%2520and%2520Xinghui%2520Li%2520and%2520Yifu%2520Tao%2520and%2520Jianeng%2520Wang%2520and%2520Maurice%2520Fallon%2520and%2520Victor%2520Adrian%2520Prisacariu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Oxford%2520Day-and-Night%252C%2520a%2520large-scale%252C%2520egocentric%2520dataset%2520for%250Anovel%2520view%2520synthesis%2520%2528NVS%2529%2520and%2520visual%2520relocalisation%2520under%2520challenging%2520lighting%250Aconditions.%2520Existing%2520datasets%2520often%2520lack%2520crucial%2520combinations%2520of%2520features%2520such%250Aas%2520ground-truth%25203D%2520geometry%252C%2520wide-ranging%2520lighting%2520variation%252C%2520and%2520full%25206DoF%250Amotion.%2520Oxford%2520Day-and-Night%2520addresses%2520these%2520gaps%2520by%2520leveraging%2520Meta%2520ARIA%250Aglasses%2520to%2520capture%2520egocentric%2520video%2520and%2520applying%2520multi-session%2520SLAM%2520to%2520estimate%250Acamera%2520poses%252C%2520reconstruct%25203D%2520point%2520clouds%252C%2520and%2520align%2520sequences%2520captured%2520under%250Avarying%2520lighting%2520conditions%252C%2520including%2520both%2520day%2520and%2520night.%2520The%2520dataset%2520spans%250Aover%252030%2520%2524%255Cmathrm%257Bkm%257D%2524%2520of%2520recorded%2520trajectories%2520and%2520covers%2520an%2520area%2520of%252040%252C000%250A%2524%255Cmathrm%257Bm%257D%255E2%2524%252C%2520offering%2520a%2520rich%2520foundation%2520for%2520egocentric%25203D%2520vision%2520research.%250AIt%2520supports%2520two%2520core%2520benchmarks%252C%2520NVS%2520and%2520relocalisation%252C%2520providing%2520a%2520unique%250Aplatform%2520for%2520evaluating%2520models%2520in%2520realistic%2520and%2520diverse%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20in%20the%20Dark%3A%20Benchmarking%20Egocentric%203D%20Vision%20with%20the%20Oxford%0A%20%20Day-and-Night%20Dataset&entry.906535625=Zirui%20Wang%20and%20Wenjing%20Bian%20and%20Xinghui%20Li%20and%20Yifu%20Tao%20and%20Jianeng%20Wang%20and%20Maurice%20Fallon%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20introduce%20Oxford%20Day-and-Night%2C%20a%20large-scale%2C%20egocentric%20dataset%20for%0Anovel%20view%20synthesis%20%28NVS%29%20and%20visual%20relocalisation%20under%20challenging%20lighting%0Aconditions.%20Existing%20datasets%20often%20lack%20crucial%20combinations%20of%20features%20such%0Aas%20ground-truth%203D%20geometry%2C%20wide-ranging%20lighting%20variation%2C%20and%20full%206DoF%0Amotion.%20Oxford%20Day-and-Night%20addresses%20these%20gaps%20by%20leveraging%20Meta%20ARIA%0Aglasses%20to%20capture%20egocentric%20video%20and%20applying%20multi-session%20SLAM%20to%20estimate%0Acamera%20poses%2C%20reconstruct%203D%20point%20clouds%2C%20and%20align%20sequences%20captured%20under%0Avarying%20lighting%20conditions%2C%20including%20both%20day%20and%20night.%20The%20dataset%20spans%0Aover%2030%20%24%5Cmathrm%7Bkm%7D%24%20of%20recorded%20trajectories%20and%20covers%20an%20area%20of%2040%2C000%0A%24%5Cmathrm%7Bm%7D%5E2%24%2C%20offering%20a%20rich%20foundation%20for%20egocentric%203D%20vision%20research.%0AIt%20supports%20two%20core%20benchmarks%2C%20NVS%20and%20relocalisation%2C%20providing%20a%20unique%0Aplatform%20for%20evaluating%20models%20in%20realistic%20and%20diverse%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04224v1&entry.124074799=Read"},
{"title": "FlexTok: Resampling Images into 1D Token Sequences of Flexible Length", "author": "Roman Bachmann and Jesse Allardice and David Mizrahi and Enrico Fini and O\u011fuzhan Fatih Kar and Elmira Amirloo and Alaaeldin El-Nouby and Amir Zamir and Afshin Dehghan", "abstract": "  Image tokenization has enabled major advances in autoregressive image\ngeneration by providing compressed, discrete representations that are more\nefficient to process than raw pixels. While traditional approaches use 2D grid\ntokenization, recent methods like TiTok have shown that 1D tokenization can\nachieve high generation quality by eliminating grid redundancies. However,\nthese methods typically use a fixed number of tokens and thus cannot adapt to\nan image's inherent complexity. We introduce FlexTok, a tokenizer that projects\n2D images into variable-length, ordered 1D token sequences. For example, a\n256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,\nhierarchically and semantically compressing its information. By training a\nrectified flow model as the decoder and using nested dropout, FlexTok produces\nplausible reconstructions regardless of the chosen token sequence length. We\nevaluate our approach in an autoregressive generation setting using a simple\nGPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to\n128 tokens, outperforming TiTok and matching state-of-the-art methods with far\nfewer tokens. We further extend the model to support to text-conditioned image\ngeneration and examine how FlexTok relates to traditional 2D tokenization. A\nkey finding is that FlexTok enables next-token prediction to describe images in\na coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate\ndepends on the complexity of the generation task.\n", "link": "http://arxiv.org/abs/2502.13967v2", "date": "2025-06-04", "relevancy": 2.4034, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5766}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length&body=Title%3A%20FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length%0AAuthor%3A%20Roman%20Bachmann%20and%20Jesse%20Allardice%20and%20David%20Mizrahi%20and%20Enrico%20Fini%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Elmira%20Amirloo%20and%20Alaaeldin%20El-Nouby%20and%20Amir%20Zamir%20and%20Afshin%20Dehghan%0AAbstract%3A%20%20%20Image%20tokenization%20has%20enabled%20major%20advances%20in%20autoregressive%20image%0Ageneration%20by%20providing%20compressed%2C%20discrete%20representations%20that%20are%20more%0Aefficient%20to%20process%20than%20raw%20pixels.%20While%20traditional%20approaches%20use%202D%20grid%0Atokenization%2C%20recent%20methods%20like%20TiTok%20have%20shown%20that%201D%20tokenization%20can%0Aachieve%20high%20generation%20quality%20by%20eliminating%20grid%20redundancies.%20However%2C%0Athese%20methods%20typically%20use%20a%20fixed%20number%20of%20tokens%20and%20thus%20cannot%20adapt%20to%0Aan%20image%27s%20inherent%20complexity.%20We%20introduce%20FlexTok%2C%20a%20tokenizer%20that%20projects%0A2D%20images%20into%20variable-length%2C%20ordered%201D%20token%20sequences.%20For%20example%2C%20a%0A256x256%20image%20can%20be%20resampled%20into%20anywhere%20from%201%20to%20256%20discrete%20tokens%2C%0Ahierarchically%20and%20semantically%20compressing%20its%20information.%20By%20training%20a%0Arectified%20flow%20model%20as%20the%20decoder%20and%20using%20nested%20dropout%2C%20FlexTok%20produces%0Aplausible%20reconstructions%20regardless%20of%20the%20chosen%20token%20sequence%20length.%20We%0Aevaluate%20our%20approach%20in%20an%20autoregressive%20generation%20setting%20using%20a%20simple%0AGPT-style%20Transformer.%20On%20ImageNet%2C%20this%20approach%20achieves%20an%20FID%3C2%20across%208%20to%0A128%20tokens%2C%20outperforming%20TiTok%20and%20matching%20state-of-the-art%20methods%20with%20far%0Afewer%20tokens.%20We%20further%20extend%20the%20model%20to%20support%20to%20text-conditioned%20image%0Ageneration%20and%20examine%20how%20FlexTok%20relates%20to%20traditional%202D%20tokenization.%20A%0Akey%20finding%20is%20that%20FlexTok%20enables%20next-token%20prediction%20to%20describe%20images%20in%0Aa%20coarse-to-fine%20%22visual%20vocabulary%22%2C%20and%20that%20the%20number%20of%20tokens%20to%20generate%0Adepends%20on%20the%20complexity%20of%20the%20generation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexTok%253A%2520Resampling%2520Images%2520into%25201D%2520Token%2520Sequences%2520of%2520Flexible%2520Length%26entry.906535625%3DRoman%2520Bachmann%2520and%2520Jesse%2520Allardice%2520and%2520David%2520Mizrahi%2520and%2520Enrico%2520Fini%2520and%2520O%25C4%259Fuzhan%2520Fatih%2520Kar%2520and%2520Elmira%2520Amirloo%2520and%2520Alaaeldin%2520El-Nouby%2520and%2520Amir%2520Zamir%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3D%2520%2520Image%2520tokenization%2520has%2520enabled%2520major%2520advances%2520in%2520autoregressive%2520image%250Ageneration%2520by%2520providing%2520compressed%252C%2520discrete%2520representations%2520that%2520are%2520more%250Aefficient%2520to%2520process%2520than%2520raw%2520pixels.%2520While%2520traditional%2520approaches%2520use%25202D%2520grid%250Atokenization%252C%2520recent%2520methods%2520like%2520TiTok%2520have%2520shown%2520that%25201D%2520tokenization%2520can%250Aachieve%2520high%2520generation%2520quality%2520by%2520eliminating%2520grid%2520redundancies.%2520However%252C%250Athese%2520methods%2520typically%2520use%2520a%2520fixed%2520number%2520of%2520tokens%2520and%2520thus%2520cannot%2520adapt%2520to%250Aan%2520image%2527s%2520inherent%2520complexity.%2520We%2520introduce%2520FlexTok%252C%2520a%2520tokenizer%2520that%2520projects%250A2D%2520images%2520into%2520variable-length%252C%2520ordered%25201D%2520token%2520sequences.%2520For%2520example%252C%2520a%250A256x256%2520image%2520can%2520be%2520resampled%2520into%2520anywhere%2520from%25201%2520to%2520256%2520discrete%2520tokens%252C%250Ahierarchically%2520and%2520semantically%2520compressing%2520its%2520information.%2520By%2520training%2520a%250Arectified%2520flow%2520model%2520as%2520the%2520decoder%2520and%2520using%2520nested%2520dropout%252C%2520FlexTok%2520produces%250Aplausible%2520reconstructions%2520regardless%2520of%2520the%2520chosen%2520token%2520sequence%2520length.%2520We%250Aevaluate%2520our%2520approach%2520in%2520an%2520autoregressive%2520generation%2520setting%2520using%2520a%2520simple%250AGPT-style%2520Transformer.%2520On%2520ImageNet%252C%2520this%2520approach%2520achieves%2520an%2520FID%253C2%2520across%25208%2520to%250A128%2520tokens%252C%2520outperforming%2520TiTok%2520and%2520matching%2520state-of-the-art%2520methods%2520with%2520far%250Afewer%2520tokens.%2520We%2520further%2520extend%2520the%2520model%2520to%2520support%2520to%2520text-conditioned%2520image%250Ageneration%2520and%2520examine%2520how%2520FlexTok%2520relates%2520to%2520traditional%25202D%2520tokenization.%2520A%250Akey%2520finding%2520is%2520that%2520FlexTok%2520enables%2520next-token%2520prediction%2520to%2520describe%2520images%2520in%250Aa%2520coarse-to-fine%2520%2522visual%2520vocabulary%2522%252C%2520and%2520that%2520the%2520number%2520of%2520tokens%2520to%2520generate%250Adepends%2520on%2520the%2520complexity%2520of%2520the%2520generation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length&entry.906535625=Roman%20Bachmann%20and%20Jesse%20Allardice%20and%20David%20Mizrahi%20and%20Enrico%20Fini%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Elmira%20Amirloo%20and%20Alaaeldin%20El-Nouby%20and%20Amir%20Zamir%20and%20Afshin%20Dehghan&entry.1292438233=%20%20Image%20tokenization%20has%20enabled%20major%20advances%20in%20autoregressive%20image%0Ageneration%20by%20providing%20compressed%2C%20discrete%20representations%20that%20are%20more%0Aefficient%20to%20process%20than%20raw%20pixels.%20While%20traditional%20approaches%20use%202D%20grid%0Atokenization%2C%20recent%20methods%20like%20TiTok%20have%20shown%20that%201D%20tokenization%20can%0Aachieve%20high%20generation%20quality%20by%20eliminating%20grid%20redundancies.%20However%2C%0Athese%20methods%20typically%20use%20a%20fixed%20number%20of%20tokens%20and%20thus%20cannot%20adapt%20to%0Aan%20image%27s%20inherent%20complexity.%20We%20introduce%20FlexTok%2C%20a%20tokenizer%20that%20projects%0A2D%20images%20into%20variable-length%2C%20ordered%201D%20token%20sequences.%20For%20example%2C%20a%0A256x256%20image%20can%20be%20resampled%20into%20anywhere%20from%201%20to%20256%20discrete%20tokens%2C%0Ahierarchically%20and%20semantically%20compressing%20its%20information.%20By%20training%20a%0Arectified%20flow%20model%20as%20the%20decoder%20and%20using%20nested%20dropout%2C%20FlexTok%20produces%0Aplausible%20reconstructions%20regardless%20of%20the%20chosen%20token%20sequence%20length.%20We%0Aevaluate%20our%20approach%20in%20an%20autoregressive%20generation%20setting%20using%20a%20simple%0AGPT-style%20Transformer.%20On%20ImageNet%2C%20this%20approach%20achieves%20an%20FID%3C2%20across%208%20to%0A128%20tokens%2C%20outperforming%20TiTok%20and%20matching%20state-of-the-art%20methods%20with%20far%0Afewer%20tokens.%20We%20further%20extend%20the%20model%20to%20support%20to%20text-conditioned%20image%0Ageneration%20and%20examine%20how%20FlexTok%20relates%20to%20traditional%202D%20tokenization.%20A%0Akey%20finding%20is%20that%20FlexTok%20enables%20next-token%20prediction%20to%20describe%20images%20in%0Aa%20coarse-to-fine%20%22visual%20vocabulary%22%2C%20and%20that%20the%20number%20of%20tokens%20to%20generate%0Adepends%20on%20the%20complexity%20of%20the%20generation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13967v2&entry.124074799=Read"},
{"title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "author": "Hieu Trung Nguyen and Bao Nguyen and Viet Anh Nguyen", "abstract": "  Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.\n", "link": "http://arxiv.org/abs/2506.03978v1", "date": "2025-06-04", "relevancy": 2.3974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Pruning%20for%20Diverse%20Best-of-N%20Reasoning%20Optimization&body=Title%3A%20Structured%20Pruning%20for%20Diverse%20Best-of-N%20Reasoning%20Optimization%0AAuthor%3A%20Hieu%20Trung%20Nguyen%20and%20Bao%20Nguyen%20and%20Viet%20Anh%20Nguyen%0AAbstract%3A%20%20%20Model%20pruning%20in%20transformer-based%20language%20models%2C%20traditionally%20viewed%20as%20a%0Ameans%20of%20achieving%20computational%20savings%2C%20can%20enhance%20the%20model%27s%20reasoning%0Acapabilities.%20In%20this%20work%2C%20we%20uncover%20a%20surprising%20phenomenon%3A%20the%20selective%0Apruning%20of%20certain%20attention%20heads%20leads%20to%20improvements%20in%20reasoning%0Aperformance%2C%20particularly%20on%20challenging%20tasks.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20SPRINT%2C%20a%20novel%20contrastive%20learning%20framework%20that%20dynamically%0Aselects%20the%20optimal%20head%20and%20layer%20to%20prune%20during%20inference.%20By%20aligning%0Aquestion%20embeddings%20with%20head%20embeddings%2C%20SPRINT%20identifies%20those%20pruned-head%0Aconfigurations%20that%20result%20in%20more%20accurate%20reasoning.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20traditional%20best-of-%24N%24%0Aand%20random%20head%20selection%20strategies%20on%20the%20MATH500%20and%20GSM8K%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Pruning%2520for%2520Diverse%2520Best-of-N%2520Reasoning%2520Optimization%26entry.906535625%3DHieu%2520Trung%2520Nguyen%2520and%2520Bao%2520Nguyen%2520and%2520Viet%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Model%2520pruning%2520in%2520transformer-based%2520language%2520models%252C%2520traditionally%2520viewed%2520as%2520a%250Ameans%2520of%2520achieving%2520computational%2520savings%252C%2520can%2520enhance%2520the%2520model%2527s%2520reasoning%250Acapabilities.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520surprising%2520phenomenon%253A%2520the%2520selective%250Apruning%2520of%2520certain%2520attention%2520heads%2520leads%2520to%2520improvements%2520in%2520reasoning%250Aperformance%252C%2520particularly%2520on%2520challenging%2520tasks.%2520Motivated%2520by%2520this%2520observation%252C%250Awe%2520propose%2520SPRINT%252C%2520a%2520novel%2520contrastive%2520learning%2520framework%2520that%2520dynamically%250Aselects%2520the%2520optimal%2520head%2520and%2520layer%2520to%2520prune%2520during%2520inference.%2520By%2520aligning%250Aquestion%2520embeddings%2520with%2520head%2520embeddings%252C%2520SPRINT%2520identifies%2520those%2520pruned-head%250Aconfigurations%2520that%2520result%2520in%2520more%2520accurate%2520reasoning.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520traditional%2520best-of-%2524N%2524%250Aand%2520random%2520head%2520selection%2520strategies%2520on%2520the%2520MATH500%2520and%2520GSM8K%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Pruning%20for%20Diverse%20Best-of-N%20Reasoning%20Optimization&entry.906535625=Hieu%20Trung%20Nguyen%20and%20Bao%20Nguyen%20and%20Viet%20Anh%20Nguyen&entry.1292438233=%20%20Model%20pruning%20in%20transformer-based%20language%20models%2C%20traditionally%20viewed%20as%20a%0Ameans%20of%20achieving%20computational%20savings%2C%20can%20enhance%20the%20model%27s%20reasoning%0Acapabilities.%20In%20this%20work%2C%20we%20uncover%20a%20surprising%20phenomenon%3A%20the%20selective%0Apruning%20of%20certain%20attention%20heads%20leads%20to%20improvements%20in%20reasoning%0Aperformance%2C%20particularly%20on%20challenging%20tasks.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20SPRINT%2C%20a%20novel%20contrastive%20learning%20framework%20that%20dynamically%0Aselects%20the%20optimal%20head%20and%20layer%20to%20prune%20during%20inference.%20By%20aligning%0Aquestion%20embeddings%20with%20head%20embeddings%2C%20SPRINT%20identifies%20those%20pruned-head%0Aconfigurations%20that%20result%20in%20more%20accurate%20reasoning.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20traditional%20best-of-%24N%24%0Aand%20random%20head%20selection%20strategies%20on%20the%20MATH500%20and%20GSM8K%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03978v1&entry.124074799=Read"},
{"title": "GlobalBuildingAtlas: An Open Global and Complete Dataset of Building\n  Polygons, Heights and LoD1 3D Models", "author": "Xiao Xiang Zhu and Sining Chen and Fahong Zhang and Yilei Shi and Yuanyuan Wang", "abstract": "  We introduce GlobalBuildingAtlas, a publicly available dataset providing\nglobal and complete coverage of building polygons, heights and Level of Detail\n1 (LoD1) 3D building models. This is the first open dataset to offer high\nquality, consistent, and complete building data in 2D and 3D form at the\nindividual building level on a global scale. Towards this dataset, we developed\nmachine learning-based pipelines to derive building polygons and heights\n(called GBA.Height) from global PlanetScope satellite data, respectively. Also\na quality-based fusion strategy was employed to generate higher-quality\npolygons (called GBA.Polygon) based on existing open building polygons,\nincluding our own derived one. With more than 2.75 billion buildings worldwide,\nGBA.Polygon surpasses the most comprehensive database to date by more than 1\nbillion buildings. GBA.Height offers the most detailed and accurate global 3D\nbuilding height maps to date, achieving a spatial resolution of 3x3 meters-30\ntimes finer than previous global products (90 m), enabling a high-resolution\nand reliable analysis of building volumes at both local and global scales.\nFinally, we generated a global LoD1 building model (called GBA.LoD1) from the\nresulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete\nglobal LoD1 building models, including 2.68 billion building instances with\npredicted heights, i.e., with a height completeness of more than 97%, achieving\nRMSEs ranging from 1.5 m to 8.9 m across different continents. With its height\naccuracy, comprehensive global coverage and rich spatial details,\nGlobalBuildingAltas offers novel insights on the status quo of global\nbuildings, which unlocks unprecedented geospatial analysis possibilities, as\nshowcased by a better illustration of where people live and a more\ncomprehensive monitoring of the progress on the 11th Sustainable Development\nGoal of the United Nations.\n", "link": "http://arxiv.org/abs/2506.04106v1", "date": "2025-06-04", "relevancy": 2.3955, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlobalBuildingAtlas%3A%20An%20Open%20Global%20and%20Complete%20Dataset%20of%20Building%0A%20%20Polygons%2C%20Heights%20and%20LoD1%203D%20Models&body=Title%3A%20GlobalBuildingAtlas%3A%20An%20Open%20Global%20and%20Complete%20Dataset%20of%20Building%0A%20%20Polygons%2C%20Heights%20and%20LoD1%203D%20Models%0AAuthor%3A%20Xiao%20Xiang%20Zhu%20and%20Sining%20Chen%20and%20Fahong%20Zhang%20and%20Yilei%20Shi%20and%20Yuanyuan%20Wang%0AAbstract%3A%20%20%20We%20introduce%20GlobalBuildingAtlas%2C%20a%20publicly%20available%20dataset%20providing%0Aglobal%20and%20complete%20coverage%20of%20building%20polygons%2C%20heights%20and%20Level%20of%20Detail%0A1%20%28LoD1%29%203D%20building%20models.%20This%20is%20the%20first%20open%20dataset%20to%20offer%20high%0Aquality%2C%20consistent%2C%20and%20complete%20building%20data%20in%202D%20and%203D%20form%20at%20the%0Aindividual%20building%20level%20on%20a%20global%20scale.%20Towards%20this%20dataset%2C%20we%20developed%0Amachine%20learning-based%20pipelines%20to%20derive%20building%20polygons%20and%20heights%0A%28called%20GBA.Height%29%20from%20global%20PlanetScope%20satellite%20data%2C%20respectively.%20Also%0Aa%20quality-based%20fusion%20strategy%20was%20employed%20to%20generate%20higher-quality%0Apolygons%20%28called%20GBA.Polygon%29%20based%20on%20existing%20open%20building%20polygons%2C%0Aincluding%20our%20own%20derived%20one.%20With%20more%20than%202.75%20billion%20buildings%20worldwide%2C%0AGBA.Polygon%20surpasses%20the%20most%20comprehensive%20database%20to%20date%20by%20more%20than%201%0Abillion%20buildings.%20GBA.Height%20offers%20the%20most%20detailed%20and%20accurate%20global%203D%0Abuilding%20height%20maps%20to%20date%2C%20achieving%20a%20spatial%20resolution%20of%203x3%20meters-30%0Atimes%20finer%20than%20previous%20global%20products%20%2890%20m%29%2C%20enabling%20a%20high-resolution%0Aand%20reliable%20analysis%20of%20building%20volumes%20at%20both%20local%20and%20global%20scales.%0AFinally%2C%20we%20generated%20a%20global%20LoD1%20building%20model%20%28called%20GBA.LoD1%29%20from%20the%0Aresulting%20GBA.Polygon%20and%20GBA.Height.%20GBA.LoD1%20represents%20the%20first%20complete%0Aglobal%20LoD1%20building%20models%2C%20including%202.68%20billion%20building%20instances%20with%0Apredicted%20heights%2C%20i.e.%2C%20with%20a%20height%20completeness%20of%20more%20than%2097%25%2C%20achieving%0ARMSEs%20ranging%20from%201.5%20m%20to%208.9%20m%20across%20different%20continents.%20With%20its%20height%0Aaccuracy%2C%20comprehensive%20global%20coverage%20and%20rich%20spatial%20details%2C%0AGlobalBuildingAltas%20offers%20novel%20insights%20on%20the%20status%20quo%20of%20global%0Abuildings%2C%20which%20unlocks%20unprecedented%20geospatial%20analysis%20possibilities%2C%20as%0Ashowcased%20by%20a%20better%20illustration%20of%20where%20people%20live%20and%20a%20more%0Acomprehensive%20monitoring%20of%20the%20progress%20on%20the%2011th%20Sustainable%20Development%0AGoal%20of%20the%20United%20Nations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobalBuildingAtlas%253A%2520An%2520Open%2520Global%2520and%2520Complete%2520Dataset%2520of%2520Building%250A%2520%2520Polygons%252C%2520Heights%2520and%2520LoD1%25203D%2520Models%26entry.906535625%3DXiao%2520Xiang%2520Zhu%2520and%2520Sining%2520Chen%2520and%2520Fahong%2520Zhang%2520and%2520Yilei%2520Shi%2520and%2520Yuanyuan%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520GlobalBuildingAtlas%252C%2520a%2520publicly%2520available%2520dataset%2520providing%250Aglobal%2520and%2520complete%2520coverage%2520of%2520building%2520polygons%252C%2520heights%2520and%2520Level%2520of%2520Detail%250A1%2520%2528LoD1%2529%25203D%2520building%2520models.%2520This%2520is%2520the%2520first%2520open%2520dataset%2520to%2520offer%2520high%250Aquality%252C%2520consistent%252C%2520and%2520complete%2520building%2520data%2520in%25202D%2520and%25203D%2520form%2520at%2520the%250Aindividual%2520building%2520level%2520on%2520a%2520global%2520scale.%2520Towards%2520this%2520dataset%252C%2520we%2520developed%250Amachine%2520learning-based%2520pipelines%2520to%2520derive%2520building%2520polygons%2520and%2520heights%250A%2528called%2520GBA.Height%2529%2520from%2520global%2520PlanetScope%2520satellite%2520data%252C%2520respectively.%2520Also%250Aa%2520quality-based%2520fusion%2520strategy%2520was%2520employed%2520to%2520generate%2520higher-quality%250Apolygons%2520%2528called%2520GBA.Polygon%2529%2520based%2520on%2520existing%2520open%2520building%2520polygons%252C%250Aincluding%2520our%2520own%2520derived%2520one.%2520With%2520more%2520than%25202.75%2520billion%2520buildings%2520worldwide%252C%250AGBA.Polygon%2520surpasses%2520the%2520most%2520comprehensive%2520database%2520to%2520date%2520by%2520more%2520than%25201%250Abillion%2520buildings.%2520GBA.Height%2520offers%2520the%2520most%2520detailed%2520and%2520accurate%2520global%25203D%250Abuilding%2520height%2520maps%2520to%2520date%252C%2520achieving%2520a%2520spatial%2520resolution%2520of%25203x3%2520meters-30%250Atimes%2520finer%2520than%2520previous%2520global%2520products%2520%252890%2520m%2529%252C%2520enabling%2520a%2520high-resolution%250Aand%2520reliable%2520analysis%2520of%2520building%2520volumes%2520at%2520both%2520local%2520and%2520global%2520scales.%250AFinally%252C%2520we%2520generated%2520a%2520global%2520LoD1%2520building%2520model%2520%2528called%2520GBA.LoD1%2529%2520from%2520the%250Aresulting%2520GBA.Polygon%2520and%2520GBA.Height.%2520GBA.LoD1%2520represents%2520the%2520first%2520complete%250Aglobal%2520LoD1%2520building%2520models%252C%2520including%25202.68%2520billion%2520building%2520instances%2520with%250Apredicted%2520heights%252C%2520i.e.%252C%2520with%2520a%2520height%2520completeness%2520of%2520more%2520than%252097%2525%252C%2520achieving%250ARMSEs%2520ranging%2520from%25201.5%2520m%2520to%25208.9%2520m%2520across%2520different%2520continents.%2520With%2520its%2520height%250Aaccuracy%252C%2520comprehensive%2520global%2520coverage%2520and%2520rich%2520spatial%2520details%252C%250AGlobalBuildingAltas%2520offers%2520novel%2520insights%2520on%2520the%2520status%2520quo%2520of%2520global%250Abuildings%252C%2520which%2520unlocks%2520unprecedented%2520geospatial%2520analysis%2520possibilities%252C%2520as%250Ashowcased%2520by%2520a%2520better%2520illustration%2520of%2520where%2520people%2520live%2520and%2520a%2520more%250Acomprehensive%2520monitoring%2520of%2520the%2520progress%2520on%2520the%252011th%2520Sustainable%2520Development%250AGoal%2520of%2520the%2520United%2520Nations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlobalBuildingAtlas%3A%20An%20Open%20Global%20and%20Complete%20Dataset%20of%20Building%0A%20%20Polygons%2C%20Heights%20and%20LoD1%203D%20Models&entry.906535625=Xiao%20Xiang%20Zhu%20and%20Sining%20Chen%20and%20Fahong%20Zhang%20and%20Yilei%20Shi%20and%20Yuanyuan%20Wang&entry.1292438233=%20%20We%20introduce%20GlobalBuildingAtlas%2C%20a%20publicly%20available%20dataset%20providing%0Aglobal%20and%20complete%20coverage%20of%20building%20polygons%2C%20heights%20and%20Level%20of%20Detail%0A1%20%28LoD1%29%203D%20building%20models.%20This%20is%20the%20first%20open%20dataset%20to%20offer%20high%0Aquality%2C%20consistent%2C%20and%20complete%20building%20data%20in%202D%20and%203D%20form%20at%20the%0Aindividual%20building%20level%20on%20a%20global%20scale.%20Towards%20this%20dataset%2C%20we%20developed%0Amachine%20learning-based%20pipelines%20to%20derive%20building%20polygons%20and%20heights%0A%28called%20GBA.Height%29%20from%20global%20PlanetScope%20satellite%20data%2C%20respectively.%20Also%0Aa%20quality-based%20fusion%20strategy%20was%20employed%20to%20generate%20higher-quality%0Apolygons%20%28called%20GBA.Polygon%29%20based%20on%20existing%20open%20building%20polygons%2C%0Aincluding%20our%20own%20derived%20one.%20With%20more%20than%202.75%20billion%20buildings%20worldwide%2C%0AGBA.Polygon%20surpasses%20the%20most%20comprehensive%20database%20to%20date%20by%20more%20than%201%0Abillion%20buildings.%20GBA.Height%20offers%20the%20most%20detailed%20and%20accurate%20global%203D%0Abuilding%20height%20maps%20to%20date%2C%20achieving%20a%20spatial%20resolution%20of%203x3%20meters-30%0Atimes%20finer%20than%20previous%20global%20products%20%2890%20m%29%2C%20enabling%20a%20high-resolution%0Aand%20reliable%20analysis%20of%20building%20volumes%20at%20both%20local%20and%20global%20scales.%0AFinally%2C%20we%20generated%20a%20global%20LoD1%20building%20model%20%28called%20GBA.LoD1%29%20from%20the%0Aresulting%20GBA.Polygon%20and%20GBA.Height.%20GBA.LoD1%20represents%20the%20first%20complete%0Aglobal%20LoD1%20building%20models%2C%20including%202.68%20billion%20building%20instances%20with%0Apredicted%20heights%2C%20i.e.%2C%20with%20a%20height%20completeness%20of%20more%20than%2097%25%2C%20achieving%0ARMSEs%20ranging%20from%201.5%20m%20to%208.9%20m%20across%20different%20continents.%20With%20its%20height%0Aaccuracy%2C%20comprehensive%20global%20coverage%20and%20rich%20spatial%20details%2C%0AGlobalBuildingAltas%20offers%20novel%20insights%20on%20the%20status%20quo%20of%20global%0Abuildings%2C%20which%20unlocks%20unprecedented%20geospatial%20analysis%20possibilities%2C%20as%0Ashowcased%20by%20a%20better%20illustration%20of%20where%20people%20live%20and%20a%20more%0Acomprehensive%20monitoring%20of%20the%20progress%20on%20the%2011th%20Sustainable%20Development%0AGoal%20of%20the%20United%20Nations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04106v1&entry.124074799=Read"},
{"title": "Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot\n  Energy-Based Multimodal Motion Prediction", "author": "Ze Zhang and Georg Hess and Junjie Hu and Emmanuel Dean and Lennart Svensson and Knut \u00c5kesson", "abstract": "  This paper proposes an integrated approach for the safe and efficient control\nof mobile robots in dynamic and uncertain environments. The approach consists\nof two key steps: one-shot multimodal motion prediction to anticipate motions\nof dynamic obstacles and model predictive control to incorporate these\npredictions into the motion planning process. Motion prediction is driven by an\nenergy-based neural network that generates high-resolution, multi-step\npredictions in a single operation. The prediction outcomes are further utilized\nto create geometric shapes formulated as mathematical constraints. Instead of\ntreating each dynamic obstacle individually, predicted obstacles are grouped by\nproximity in an unsupervised way to improve performance and efficiency. The\noverall collision-free navigation is handled by model predictive control with a\nspecific design for proactive dynamic obstacle avoidance. The proposed approach\nallows mobile robots to navigate effectively in dynamic environments. Its\nperformance is accessed across various scenarios that represent typical\nwarehouse settings. The results demonstrate that the proposed approach\noutperforms other existing dynamic obstacle avoidance methods.\n", "link": "http://arxiv.org/abs/2505.00237v3", "date": "2025-06-04", "relevancy": 2.3797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6118}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Future-Oriented%20Navigation%3A%20Dynamic%20Obstacle%20Avoidance%20with%20One-Shot%0A%20%20Energy-Based%20Multimodal%20Motion%20Prediction&body=Title%3A%20Future-Oriented%20Navigation%3A%20Dynamic%20Obstacle%20Avoidance%20with%20One-Shot%0A%20%20Energy-Based%20Multimodal%20Motion%20Prediction%0AAuthor%3A%20Ze%20Zhang%20and%20Georg%20Hess%20and%20Junjie%20Hu%20and%20Emmanuel%20Dean%20and%20Lennart%20Svensson%20and%20Knut%20%C3%85kesson%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20integrated%20approach%20for%20the%20safe%20and%20efficient%20control%0Aof%20mobile%20robots%20in%20dynamic%20and%20uncertain%20environments.%20The%20approach%20consists%0Aof%20two%20key%20steps%3A%20one-shot%20multimodal%20motion%20prediction%20to%20anticipate%20motions%0Aof%20dynamic%20obstacles%20and%20model%20predictive%20control%20to%20incorporate%20these%0Apredictions%20into%20the%20motion%20planning%20process.%20Motion%20prediction%20is%20driven%20by%20an%0Aenergy-based%20neural%20network%20that%20generates%20high-resolution%2C%20multi-step%0Apredictions%20in%20a%20single%20operation.%20The%20prediction%20outcomes%20are%20further%20utilized%0Ato%20create%20geometric%20shapes%20formulated%20as%20mathematical%20constraints.%20Instead%20of%0Atreating%20each%20dynamic%20obstacle%20individually%2C%20predicted%20obstacles%20are%20grouped%20by%0Aproximity%20in%20an%20unsupervised%20way%20to%20improve%20performance%20and%20efficiency.%20The%0Aoverall%20collision-free%20navigation%20is%20handled%20by%20model%20predictive%20control%20with%20a%0Aspecific%20design%20for%20proactive%20dynamic%20obstacle%20avoidance.%20The%20proposed%20approach%0Aallows%20mobile%20robots%20to%20navigate%20effectively%20in%20dynamic%20environments.%20Its%0Aperformance%20is%20accessed%20across%20various%20scenarios%20that%20represent%20typical%0Awarehouse%20settings.%20The%20results%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20other%20existing%20dynamic%20obstacle%20avoidance%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00237v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuture-Oriented%2520Navigation%253A%2520Dynamic%2520Obstacle%2520Avoidance%2520with%2520One-Shot%250A%2520%2520Energy-Based%2520Multimodal%2520Motion%2520Prediction%26entry.906535625%3DZe%2520Zhang%2520and%2520Georg%2520Hess%2520and%2520Junjie%2520Hu%2520and%2520Emmanuel%2520Dean%2520and%2520Lennart%2520Svensson%2520and%2520Knut%2520%25C3%2585kesson%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520an%2520integrated%2520approach%2520for%2520the%2520safe%2520and%2520efficient%2520control%250Aof%2520mobile%2520robots%2520in%2520dynamic%2520and%2520uncertain%2520environments.%2520The%2520approach%2520consists%250Aof%2520two%2520key%2520steps%253A%2520one-shot%2520multimodal%2520motion%2520prediction%2520to%2520anticipate%2520motions%250Aof%2520dynamic%2520obstacles%2520and%2520model%2520predictive%2520control%2520to%2520incorporate%2520these%250Apredictions%2520into%2520the%2520motion%2520planning%2520process.%2520Motion%2520prediction%2520is%2520driven%2520by%2520an%250Aenergy-based%2520neural%2520network%2520that%2520generates%2520high-resolution%252C%2520multi-step%250Apredictions%2520in%2520a%2520single%2520operation.%2520The%2520prediction%2520outcomes%2520are%2520further%2520utilized%250Ato%2520create%2520geometric%2520shapes%2520formulated%2520as%2520mathematical%2520constraints.%2520Instead%2520of%250Atreating%2520each%2520dynamic%2520obstacle%2520individually%252C%2520predicted%2520obstacles%2520are%2520grouped%2520by%250Aproximity%2520in%2520an%2520unsupervised%2520way%2520to%2520improve%2520performance%2520and%2520efficiency.%2520The%250Aoverall%2520collision-free%2520navigation%2520is%2520handled%2520by%2520model%2520predictive%2520control%2520with%2520a%250Aspecific%2520design%2520for%2520proactive%2520dynamic%2520obstacle%2520avoidance.%2520The%2520proposed%2520approach%250Aallows%2520mobile%2520robots%2520to%2520navigate%2520effectively%2520in%2520dynamic%2520environments.%2520Its%250Aperformance%2520is%2520accessed%2520across%2520various%2520scenarios%2520that%2520represent%2520typical%250Awarehouse%2520settings.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%250Aoutperforms%2520other%2520existing%2520dynamic%2520obstacle%2520avoidance%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00237v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Future-Oriented%20Navigation%3A%20Dynamic%20Obstacle%20Avoidance%20with%20One-Shot%0A%20%20Energy-Based%20Multimodal%20Motion%20Prediction&entry.906535625=Ze%20Zhang%20and%20Georg%20Hess%20and%20Junjie%20Hu%20and%20Emmanuel%20Dean%20and%20Lennart%20Svensson%20and%20Knut%20%C3%85kesson&entry.1292438233=%20%20This%20paper%20proposes%20an%20integrated%20approach%20for%20the%20safe%20and%20efficient%20control%0Aof%20mobile%20robots%20in%20dynamic%20and%20uncertain%20environments.%20The%20approach%20consists%0Aof%20two%20key%20steps%3A%20one-shot%20multimodal%20motion%20prediction%20to%20anticipate%20motions%0Aof%20dynamic%20obstacles%20and%20model%20predictive%20control%20to%20incorporate%20these%0Apredictions%20into%20the%20motion%20planning%20process.%20Motion%20prediction%20is%20driven%20by%20an%0Aenergy-based%20neural%20network%20that%20generates%20high-resolution%2C%20multi-step%0Apredictions%20in%20a%20single%20operation.%20The%20prediction%20outcomes%20are%20further%20utilized%0Ato%20create%20geometric%20shapes%20formulated%20as%20mathematical%20constraints.%20Instead%20of%0Atreating%20each%20dynamic%20obstacle%20individually%2C%20predicted%20obstacles%20are%20grouped%20by%0Aproximity%20in%20an%20unsupervised%20way%20to%20improve%20performance%20and%20efficiency.%20The%0Aoverall%20collision-free%20navigation%20is%20handled%20by%20model%20predictive%20control%20with%20a%0Aspecific%20design%20for%20proactive%20dynamic%20obstacle%20avoidance.%20The%20proposed%20approach%0Aallows%20mobile%20robots%20to%20navigate%20effectively%20in%20dynamic%20environments.%20Its%0Aperformance%20is%20accessed%20across%20various%20scenarios%20that%20represent%20typical%0Awarehouse%20settings.%20The%20results%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20other%20existing%20dynamic%20obstacle%20avoidance%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00237v3&entry.124074799=Read"},
{"title": "Optimizing Mesh to Improve the Triangular Expansion Algorithm for\n  Computing Visibility Regions", "author": "Jan Mikula and Miroslav Kulich", "abstract": "  This paper addresses the problem of improving the query performance of the\ntriangular expansion algorithm (TEA) for computing visibility regions by\nfinding the most advantageous instance of the triangular mesh, the\npreprocessing structure. The TEA recursively traverses the mesh while keeping\ntrack of the visible region, the set of all points visible from a query point\nin a polygonal world. We show that the measured query time is approximately\nproportional to the number of triangle edge expansions during the mesh\ntraversal. We propose a new type of triangular mesh that minimizes the expected\nnumber of expansions assuming the query points are drawn from a known\nprobability distribution. We design a heuristic method to approximate the mesh\nand evaluate the approach on many challenging instances that resemble\nreal-world environments. The proposed mesh improves the mean query times by\n12-16% compared to the reference constrained Delaunay triangulation. The\napproach is suitable to boost offline applications that require computing\nmillions of queries without addressing the preprocessing time. The\nimplementation is publicly available to replicate our experiments and serve the\ncommunity.\n", "link": "http://arxiv.org/abs/2506.04086v1", "date": "2025-06-04", "relevancy": 2.3784, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4902}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4721}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Mesh%20to%20Improve%20the%20Triangular%20Expansion%20Algorithm%20for%0A%20%20Computing%20Visibility%20Regions&body=Title%3A%20Optimizing%20Mesh%20to%20Improve%20the%20Triangular%20Expansion%20Algorithm%20for%0A%20%20Computing%20Visibility%20Regions%0AAuthor%3A%20Jan%20Mikula%20and%20Miroslav%20Kulich%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20improving%20the%20query%20performance%20of%20the%0Atriangular%20expansion%20algorithm%20%28TEA%29%20for%20computing%20visibility%20regions%20by%0Afinding%20the%20most%20advantageous%20instance%20of%20the%20triangular%20mesh%2C%20the%0Apreprocessing%20structure.%20The%20TEA%20recursively%20traverses%20the%20mesh%20while%20keeping%0Atrack%20of%20the%20visible%20region%2C%20the%20set%20of%20all%20points%20visible%20from%20a%20query%20point%0Ain%20a%20polygonal%20world.%20We%20show%20that%20the%20measured%20query%20time%20is%20approximately%0Aproportional%20to%20the%20number%20of%20triangle%20edge%20expansions%20during%20the%20mesh%0Atraversal.%20We%20propose%20a%20new%20type%20of%20triangular%20mesh%20that%20minimizes%20the%20expected%0Anumber%20of%20expansions%20assuming%20the%20query%20points%20are%20drawn%20from%20a%20known%0Aprobability%20distribution.%20We%20design%20a%20heuristic%20method%20to%20approximate%20the%20mesh%0Aand%20evaluate%20the%20approach%20on%20many%20challenging%20instances%20that%20resemble%0Areal-world%20environments.%20The%20proposed%20mesh%20improves%20the%20mean%20query%20times%20by%0A12-16%25%20compared%20to%20the%20reference%20constrained%20Delaunay%20triangulation.%20The%0Aapproach%20is%20suitable%20to%20boost%20offline%20applications%20that%20require%20computing%0Amillions%20of%20queries%20without%20addressing%20the%20preprocessing%20time.%20The%0Aimplementation%20is%20publicly%20available%20to%20replicate%20our%20experiments%20and%20serve%20the%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Mesh%2520to%2520Improve%2520the%2520Triangular%2520Expansion%2520Algorithm%2520for%250A%2520%2520Computing%2520Visibility%2520Regions%26entry.906535625%3DJan%2520Mikula%2520and%2520Miroslav%2520Kulich%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520improving%2520the%2520query%2520performance%2520of%2520the%250Atriangular%2520expansion%2520algorithm%2520%2528TEA%2529%2520for%2520computing%2520visibility%2520regions%2520by%250Afinding%2520the%2520most%2520advantageous%2520instance%2520of%2520the%2520triangular%2520mesh%252C%2520the%250Apreprocessing%2520structure.%2520The%2520TEA%2520recursively%2520traverses%2520the%2520mesh%2520while%2520keeping%250Atrack%2520of%2520the%2520visible%2520region%252C%2520the%2520set%2520of%2520all%2520points%2520visible%2520from%2520a%2520query%2520point%250Ain%2520a%2520polygonal%2520world.%2520We%2520show%2520that%2520the%2520measured%2520query%2520time%2520is%2520approximately%250Aproportional%2520to%2520the%2520number%2520of%2520triangle%2520edge%2520expansions%2520during%2520the%2520mesh%250Atraversal.%2520We%2520propose%2520a%2520new%2520type%2520of%2520triangular%2520mesh%2520that%2520minimizes%2520the%2520expected%250Anumber%2520of%2520expansions%2520assuming%2520the%2520query%2520points%2520are%2520drawn%2520from%2520a%2520known%250Aprobability%2520distribution.%2520We%2520design%2520a%2520heuristic%2520method%2520to%2520approximate%2520the%2520mesh%250Aand%2520evaluate%2520the%2520approach%2520on%2520many%2520challenging%2520instances%2520that%2520resemble%250Areal-world%2520environments.%2520The%2520proposed%2520mesh%2520improves%2520the%2520mean%2520query%2520times%2520by%250A12-16%2525%2520compared%2520to%2520the%2520reference%2520constrained%2520Delaunay%2520triangulation.%2520The%250Aapproach%2520is%2520suitable%2520to%2520boost%2520offline%2520applications%2520that%2520require%2520computing%250Amillions%2520of%2520queries%2520without%2520addressing%2520the%2520preprocessing%2520time.%2520The%250Aimplementation%2520is%2520publicly%2520available%2520to%2520replicate%2520our%2520experiments%2520and%2520serve%2520the%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Mesh%20to%20Improve%20the%20Triangular%20Expansion%20Algorithm%20for%0A%20%20Computing%20Visibility%20Regions&entry.906535625=Jan%20Mikula%20and%20Miroslav%20Kulich&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20improving%20the%20query%20performance%20of%20the%0Atriangular%20expansion%20algorithm%20%28TEA%29%20for%20computing%20visibility%20regions%20by%0Afinding%20the%20most%20advantageous%20instance%20of%20the%20triangular%20mesh%2C%20the%0Apreprocessing%20structure.%20The%20TEA%20recursively%20traverses%20the%20mesh%20while%20keeping%0Atrack%20of%20the%20visible%20region%2C%20the%20set%20of%20all%20points%20visible%20from%20a%20query%20point%0Ain%20a%20polygonal%20world.%20We%20show%20that%20the%20measured%20query%20time%20is%20approximately%0Aproportional%20to%20the%20number%20of%20triangle%20edge%20expansions%20during%20the%20mesh%0Atraversal.%20We%20propose%20a%20new%20type%20of%20triangular%20mesh%20that%20minimizes%20the%20expected%0Anumber%20of%20expansions%20assuming%20the%20query%20points%20are%20drawn%20from%20a%20known%0Aprobability%20distribution.%20We%20design%20a%20heuristic%20method%20to%20approximate%20the%20mesh%0Aand%20evaluate%20the%20approach%20on%20many%20challenging%20instances%20that%20resemble%0Areal-world%20environments.%20The%20proposed%20mesh%20improves%20the%20mean%20query%20times%20by%0A12-16%25%20compared%20to%20the%20reference%20constrained%20Delaunay%20triangulation.%20The%0Aapproach%20is%20suitable%20to%20boost%20offline%20applications%20that%20require%20computing%0Amillions%20of%20queries%20without%20addressing%20the%20preprocessing%20time.%20The%0Aimplementation%20is%20publicly%20available%20to%20replicate%20our%20experiments%20and%20serve%20the%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04086v1&entry.124074799=Read"},
{"title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos", "author": "Kejian Zhu and Zhuoran Jin and Hongbang Yuan and Jiachun Li and Shangqing Tu and Pengfei Cao and Yubo Chen and Kang Liu and Jun Zhao", "abstract": "  The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.\n", "link": "http://arxiv.org/abs/2506.04141v1", "date": "2025-06-04", "relevancy": 2.3776, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMR-V%3A%20What%27s%20Left%20Unsaid%3F%20A%20Benchmark%20for%20Multimodal%20Deep%20Reasoning%20in%0A%20%20Videos&body=Title%3A%20MMR-V%3A%20What%27s%20Left%20Unsaid%3F%20A%20Benchmark%20for%20Multimodal%20Deep%20Reasoning%20in%0A%20%20Videos%0AAuthor%3A%20Kejian%20Zhu%20and%20Zhuoran%20Jin%20and%20Hongbang%20Yuan%20and%20Jiachun%20Li%20and%20Shangqing%20Tu%20and%20Pengfei%20Cao%20and%20Yubo%20Chen%20and%20Kang%20Liu%20and%20Jun%20Zhao%0AAbstract%3A%20%20%20The%20sequential%20structure%20of%20videos%20poses%20a%20challenge%20to%20the%20ability%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20locate%20multi-frame%20evidence%20and%0Aconduct%20multimodal%20reasoning.%20However%2C%20existing%20video%20benchmarks%20mainly%20focus%0Aon%20understanding%20tasks%2C%20which%20only%20require%20models%20to%20match%20frames%20mentioned%20in%0Athe%20question%20%28hereafter%20referred%20to%20as%20%22question%20frame%22%29%20and%20perceive%20a%20few%0Aadjacent%20frames.%20To%20address%20this%20gap%2C%20we%20propose%20MMR-V%3A%20A%20Benchmark%20for%0AMultimodal%20Deep%20Reasoning%20in%20Videos.%20The%20benchmark%20is%20characterized%20by%20the%0Afollowing%20features.%20%281%29%20Long-range%2C%20multi-frame%20reasoning%3A%20Models%20are%20required%0Ato%20infer%20and%20analyze%20evidence%20frames%20that%20may%20be%20far%20from%20the%20question%20frame.%0A%282%29%20Beyond%20perception%3A%20Questions%20cannot%20be%20answered%20through%20direct%20perception%0Aalone%20but%20require%20reasoning%20over%20hidden%20information.%20%283%29%20Reliability%3A%20All%20tasks%0Aare%20manually%20annotated%2C%20referencing%20extensive%20real-world%20user%20understanding%20to%0Aalign%20with%20common%20perceptions.%20%284%29%20Confusability%3A%20Carefully%20designed%20distractor%0Aannotation%20strategies%20to%20reduce%20model%20shortcuts.%20MMR-V%20consists%20of%20317%20videos%0Aand%201%2C257%20tasks.%20Our%20experiments%20reveal%20that%20current%20models%20still%20struggle%20with%0Amulti-modal%20reasoning%3B%20even%20the%20best-performing%20model%2C%20o4-mini%2C%20achieves%20only%0A52.5%25%20accuracy.%20Additionally%2C%20current%20reasoning%20enhancement%20strategies%0A%28Chain-of-Thought%20and%20scaling%20test-time%20compute%29%20bring%20limited%20gains.%20Further%0Aanalysis%20indicates%20that%20the%20CoT%20demanded%20for%20multi-modal%20reasoning%20differs%20from%0Ait%20in%20textual%20reasoning%2C%20which%20partly%20explains%20the%20limited%20performance%20gains.%0AWe%20hope%20that%20MMR-V%20can%20inspire%20further%20research%20into%20enhancing%20multi-modal%0Areasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMR-V%253A%2520What%2527s%2520Left%2520Unsaid%253F%2520A%2520Benchmark%2520for%2520Multimodal%2520Deep%2520Reasoning%2520in%250A%2520%2520Videos%26entry.906535625%3DKejian%2520Zhu%2520and%2520Zhuoran%2520Jin%2520and%2520Hongbang%2520Yuan%2520and%2520Jiachun%2520Li%2520and%2520Shangqing%2520Tu%2520and%2520Pengfei%2520Cao%2520and%2520Yubo%2520Chen%2520and%2520Kang%2520Liu%2520and%2520Jun%2520Zhao%26entry.1292438233%3D%2520%2520The%2520sequential%2520structure%2520of%2520videos%2520poses%2520a%2520challenge%2520to%2520the%2520ability%2520of%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520locate%2520multi-frame%2520evidence%2520and%250Aconduct%2520multimodal%2520reasoning.%2520However%252C%2520existing%2520video%2520benchmarks%2520mainly%2520focus%250Aon%2520understanding%2520tasks%252C%2520which%2520only%2520require%2520models%2520to%2520match%2520frames%2520mentioned%2520in%250Athe%2520question%2520%2528hereafter%2520referred%2520to%2520as%2520%2522question%2520frame%2522%2529%2520and%2520perceive%2520a%2520few%250Aadjacent%2520frames.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520MMR-V%253A%2520A%2520Benchmark%2520for%250AMultimodal%2520Deep%2520Reasoning%2520in%2520Videos.%2520The%2520benchmark%2520is%2520characterized%2520by%2520the%250Afollowing%2520features.%2520%25281%2529%2520Long-range%252C%2520multi-frame%2520reasoning%253A%2520Models%2520are%2520required%250Ato%2520infer%2520and%2520analyze%2520evidence%2520frames%2520that%2520may%2520be%2520far%2520from%2520the%2520question%2520frame.%250A%25282%2529%2520Beyond%2520perception%253A%2520Questions%2520cannot%2520be%2520answered%2520through%2520direct%2520perception%250Aalone%2520but%2520require%2520reasoning%2520over%2520hidden%2520information.%2520%25283%2529%2520Reliability%253A%2520All%2520tasks%250Aare%2520manually%2520annotated%252C%2520referencing%2520extensive%2520real-world%2520user%2520understanding%2520to%250Aalign%2520with%2520common%2520perceptions.%2520%25284%2529%2520Confusability%253A%2520Carefully%2520designed%2520distractor%250Aannotation%2520strategies%2520to%2520reduce%2520model%2520shortcuts.%2520MMR-V%2520consists%2520of%2520317%2520videos%250Aand%25201%252C257%2520tasks.%2520Our%2520experiments%2520reveal%2520that%2520current%2520models%2520still%2520struggle%2520with%250Amulti-modal%2520reasoning%253B%2520even%2520the%2520best-performing%2520model%252C%2520o4-mini%252C%2520achieves%2520only%250A52.5%2525%2520accuracy.%2520Additionally%252C%2520current%2520reasoning%2520enhancement%2520strategies%250A%2528Chain-of-Thought%2520and%2520scaling%2520test-time%2520compute%2529%2520bring%2520limited%2520gains.%2520Further%250Aanalysis%2520indicates%2520that%2520the%2520CoT%2520demanded%2520for%2520multi-modal%2520reasoning%2520differs%2520from%250Ait%2520in%2520textual%2520reasoning%252C%2520which%2520partly%2520explains%2520the%2520limited%2520performance%2520gains.%250AWe%2520hope%2520that%2520MMR-V%2520can%2520inspire%2520further%2520research%2520into%2520enhancing%2520multi-modal%250Areasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMR-V%3A%20What%27s%20Left%20Unsaid%3F%20A%20Benchmark%20for%20Multimodal%20Deep%20Reasoning%20in%0A%20%20Videos&entry.906535625=Kejian%20Zhu%20and%20Zhuoran%20Jin%20and%20Hongbang%20Yuan%20and%20Jiachun%20Li%20and%20Shangqing%20Tu%20and%20Pengfei%20Cao%20and%20Yubo%20Chen%20and%20Kang%20Liu%20and%20Jun%20Zhao&entry.1292438233=%20%20The%20sequential%20structure%20of%20videos%20poses%20a%20challenge%20to%20the%20ability%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20locate%20multi-frame%20evidence%20and%0Aconduct%20multimodal%20reasoning.%20However%2C%20existing%20video%20benchmarks%20mainly%20focus%0Aon%20understanding%20tasks%2C%20which%20only%20require%20models%20to%20match%20frames%20mentioned%20in%0Athe%20question%20%28hereafter%20referred%20to%20as%20%22question%20frame%22%29%20and%20perceive%20a%20few%0Aadjacent%20frames.%20To%20address%20this%20gap%2C%20we%20propose%20MMR-V%3A%20A%20Benchmark%20for%0AMultimodal%20Deep%20Reasoning%20in%20Videos.%20The%20benchmark%20is%20characterized%20by%20the%0Afollowing%20features.%20%281%29%20Long-range%2C%20multi-frame%20reasoning%3A%20Models%20are%20required%0Ato%20infer%20and%20analyze%20evidence%20frames%20that%20may%20be%20far%20from%20the%20question%20frame.%0A%282%29%20Beyond%20perception%3A%20Questions%20cannot%20be%20answered%20through%20direct%20perception%0Aalone%20but%20require%20reasoning%20over%20hidden%20information.%20%283%29%20Reliability%3A%20All%20tasks%0Aare%20manually%20annotated%2C%20referencing%20extensive%20real-world%20user%20understanding%20to%0Aalign%20with%20common%20perceptions.%20%284%29%20Confusability%3A%20Carefully%20designed%20distractor%0Aannotation%20strategies%20to%20reduce%20model%20shortcuts.%20MMR-V%20consists%20of%20317%20videos%0Aand%201%2C257%20tasks.%20Our%20experiments%20reveal%20that%20current%20models%20still%20struggle%20with%0Amulti-modal%20reasoning%3B%20even%20the%20best-performing%20model%2C%20o4-mini%2C%20achieves%20only%0A52.5%25%20accuracy.%20Additionally%2C%20current%20reasoning%20enhancement%20strategies%0A%28Chain-of-Thought%20and%20scaling%20test-time%20compute%29%20bring%20limited%20gains.%20Further%0Aanalysis%20indicates%20that%20the%20CoT%20demanded%20for%20multi-modal%20reasoning%20differs%20from%0Ait%20in%20textual%20reasoning%2C%20which%20partly%20explains%20the%20limited%20performance%20gains.%0AWe%20hope%20that%20MMR-V%20can%20inspire%20further%20research%20into%20enhancing%20multi-modal%0Areasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04141v1&entry.124074799=Read"},
{"title": "Point Cloud Quality Assessment Using the Perceptual Clustering Weighted\n  Graph (PCW-Graph) and Attention Fusion Network", "author": "Abdelouahed Laazoufi and Mohammed El Hassouni and Hocine Cherifi", "abstract": "  No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for\nevaluating 3D content in real-world applications where reference models are\nunavailable.\n", "link": "http://arxiv.org/abs/2506.04081v1", "date": "2025-06-04", "relevancy": 2.3754, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4611}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Quality%20Assessment%20Using%20the%20Perceptual%20Clustering%20Weighted%0A%20%20Graph%20%28PCW-Graph%29%20and%20Attention%20Fusion%20Network&body=Title%3A%20Point%20Cloud%20Quality%20Assessment%20Using%20the%20Perceptual%20Clustering%20Weighted%0A%20%20Graph%20%28PCW-Graph%29%20and%20Attention%20Fusion%20Network%0AAuthor%3A%20Abdelouahed%20Laazoufi%20and%20Mohammed%20El%20Hassouni%20and%20Hocine%20Cherifi%0AAbstract%3A%20%20%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20%28NR-PCQA%29%20is%20critical%20for%0Aevaluating%203D%20content%20in%20real-world%20applications%20where%20reference%20models%20are%0Aunavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Cloud%2520Quality%2520Assessment%2520Using%2520the%2520Perceptual%2520Clustering%2520Weighted%250A%2520%2520Graph%2520%2528PCW-Graph%2529%2520and%2520Attention%2520Fusion%2520Network%26entry.906535625%3DAbdelouahed%2520Laazoufi%2520and%2520Mohammed%2520El%2520Hassouni%2520and%2520Hocine%2520Cherifi%26entry.1292438233%3D%2520%2520No-Reference%2520Point%2520Cloud%2520Quality%2520Assessment%2520%2528NR-PCQA%2529%2520is%2520critical%2520for%250Aevaluating%25203D%2520content%2520in%2520real-world%2520applications%2520where%2520reference%2520models%2520are%250Aunavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Quality%20Assessment%20Using%20the%20Perceptual%20Clustering%20Weighted%0A%20%20Graph%20%28PCW-Graph%29%20and%20Attention%20Fusion%20Network&entry.906535625=Abdelouahed%20Laazoufi%20and%20Mohammed%20El%20Hassouni%20and%20Hocine%20Cherifi&entry.1292438233=%20%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20%28NR-PCQA%29%20is%20critical%20for%0Aevaluating%203D%20content%20in%20real-world%20applications%20where%20reference%20models%20are%0Aunavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04081v1&entry.124074799=Read"},
{"title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective\n  Video Understanding", "author": "Hongzhi Zhang and Jingyuan Zhang and Xingguang Ji and Qi Wang and Fuzheng Zhang", "abstract": "  Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.\n", "link": "http://arxiv.org/abs/2506.03990v1", "date": "2025-06-04", "relevancy": 2.3749, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6029}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5875}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynTok%3A%20Dynamic%20Compression%20of%20Visual%20Tokens%20for%20Efficient%20and%20Effective%0A%20%20Video%20Understanding&body=Title%3A%20DynTok%3A%20Dynamic%20Compression%20of%20Visual%20Tokens%20for%20Efficient%20and%20Effective%0A%20%20Video%20Understanding%0AAuthor%3A%20Hongzhi%20Zhang%20and%20Jingyuan%20Zhang%20and%20Xingguang%20Ji%20and%20Qi%20Wang%20and%20Fuzheng%20Zhang%0AAbstract%3A%20%20%20Typical%20video%20modeling%20methods%2C%20such%20as%20LLava%2C%20represent%20videos%20as%20sequences%0Aof%20visual%20tokens%2C%20which%20are%20then%20processed%20by%20the%20LLM%20backbone%20for%20effective%0Avideo%20understanding.%20However%2C%20this%20approach%20leads%20to%20a%20massive%20number%20of%20visual%0Atokens%2C%20especially%20for%20long%20videos.%20A%20practical%20solution%20is%20to%20first%20extract%0Arelevant%20visual%20information%20from%20the%20large%20visual%20context%20before%20feeding%20it%0Ainto%20the%20LLM%20backbone%2C%20thereby%20reducing%20computational%20overhead.%20In%20this%20work%2C%0Awe%20introduce%20DynTok%2C%20a%20novel%20%5Ctextbf%7BDyn%7Damic%20video%20%5Ctextbf%7BTok%7Den%20compression%0Astrategy.%20DynTok%20adaptively%20splits%20visual%20tokens%20into%20groups%20and%20merges%20them%0Awithin%20each%20group%2C%20achieving%20high%20compression%20in%20regions%20with%20low%20information%0Adensity%20while%20preserving%20essential%20content.%20Our%20method%20reduces%20the%20number%20of%0Atokens%20to%2044.4%25%20of%20the%20original%20size%20while%20maintaining%20comparable%20performance.%0AIt%20further%20benefits%20from%20increasing%20the%20number%20of%20video%20frames%20and%20achieves%0A65.3%25%20on%20Video-MME%20and%2072.5%25%20on%20MLVU.%20By%20applying%20this%20simple%20yet%20effective%0Acompression%20method%2C%20we%20expose%20the%20redundancy%20in%20video%20token%20representations%20and%0Aoffer%20insights%20for%20designing%20more%20efficient%20video%20modeling%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynTok%253A%2520Dynamic%2520Compression%2520of%2520Visual%2520Tokens%2520for%2520Efficient%2520and%2520Effective%250A%2520%2520Video%2520Understanding%26entry.906535625%3DHongzhi%2520Zhang%2520and%2520Jingyuan%2520Zhang%2520and%2520Xingguang%2520Ji%2520and%2520Qi%2520Wang%2520and%2520Fuzheng%2520Zhang%26entry.1292438233%3D%2520%2520Typical%2520video%2520modeling%2520methods%252C%2520such%2520as%2520LLava%252C%2520represent%2520videos%2520as%2520sequences%250Aof%2520visual%2520tokens%252C%2520which%2520are%2520then%2520processed%2520by%2520the%2520LLM%2520backbone%2520for%2520effective%250Avideo%2520understanding.%2520However%252C%2520this%2520approach%2520leads%2520to%2520a%2520massive%2520number%2520of%2520visual%250Atokens%252C%2520especially%2520for%2520long%2520videos.%2520A%2520practical%2520solution%2520is%2520to%2520first%2520extract%250Arelevant%2520visual%2520information%2520from%2520the%2520large%2520visual%2520context%2520before%2520feeding%2520it%250Ainto%2520the%2520LLM%2520backbone%252C%2520thereby%2520reducing%2520computational%2520overhead.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520DynTok%252C%2520a%2520novel%2520%255Ctextbf%257BDyn%257Damic%2520video%2520%255Ctextbf%257BTok%257Den%2520compression%250Astrategy.%2520DynTok%2520adaptively%2520splits%2520visual%2520tokens%2520into%2520groups%2520and%2520merges%2520them%250Awithin%2520each%2520group%252C%2520achieving%2520high%2520compression%2520in%2520regions%2520with%2520low%2520information%250Adensity%2520while%2520preserving%2520essential%2520content.%2520Our%2520method%2520reduces%2520the%2520number%2520of%250Atokens%2520to%252044.4%2525%2520of%2520the%2520original%2520size%2520while%2520maintaining%2520comparable%2520performance.%250AIt%2520further%2520benefits%2520from%2520increasing%2520the%2520number%2520of%2520video%2520frames%2520and%2520achieves%250A65.3%2525%2520on%2520Video-MME%2520and%252072.5%2525%2520on%2520MLVU.%2520By%2520applying%2520this%2520simple%2520yet%2520effective%250Acompression%2520method%252C%2520we%2520expose%2520the%2520redundancy%2520in%2520video%2520token%2520representations%2520and%250Aoffer%2520insights%2520for%2520designing%2520more%2520efficient%2520video%2520modeling%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynTok%3A%20Dynamic%20Compression%20of%20Visual%20Tokens%20for%20Efficient%20and%20Effective%0A%20%20Video%20Understanding&entry.906535625=Hongzhi%20Zhang%20and%20Jingyuan%20Zhang%20and%20Xingguang%20Ji%20and%20Qi%20Wang%20and%20Fuzheng%20Zhang&entry.1292438233=%20%20Typical%20video%20modeling%20methods%2C%20such%20as%20LLava%2C%20represent%20videos%20as%20sequences%0Aof%20visual%20tokens%2C%20which%20are%20then%20processed%20by%20the%20LLM%20backbone%20for%20effective%0Avideo%20understanding.%20However%2C%20this%20approach%20leads%20to%20a%20massive%20number%20of%20visual%0Atokens%2C%20especially%20for%20long%20videos.%20A%20practical%20solution%20is%20to%20first%20extract%0Arelevant%20visual%20information%20from%20the%20large%20visual%20context%20before%20feeding%20it%0Ainto%20the%20LLM%20backbone%2C%20thereby%20reducing%20computational%20overhead.%20In%20this%20work%2C%0Awe%20introduce%20DynTok%2C%20a%20novel%20%5Ctextbf%7BDyn%7Damic%20video%20%5Ctextbf%7BTok%7Den%20compression%0Astrategy.%20DynTok%20adaptively%20splits%20visual%20tokens%20into%20groups%20and%20merges%20them%0Awithin%20each%20group%2C%20achieving%20high%20compression%20in%20regions%20with%20low%20information%0Adensity%20while%20preserving%20essential%20content.%20Our%20method%20reduces%20the%20number%20of%0Atokens%20to%2044.4%25%20of%20the%20original%20size%20while%20maintaining%20comparable%20performance.%0AIt%20further%20benefits%20from%20increasing%20the%20number%20of%20video%20frames%20and%20achieves%0A65.3%25%20on%20Video-MME%20and%2072.5%25%20on%20MLVU.%20By%20applying%20this%20simple%20yet%20effective%0Acompression%20method%2C%20we%20expose%20the%20redundancy%20in%20video%20token%20representations%20and%0Aoffer%20insights%20for%20designing%20more%20efficient%20video%20modeling%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03990v1&entry.124074799=Read"},
{"title": "EuroLLM-9B: Technical Report", "author": "Pedro Henrique Martins and Jo\u00e3o Alves and Patrick Fernandes and Nuno M. Guerreiro and Ricardo Rei and Amin Farajian and Mateusz Klimaszewski and Duarte M. Alves and Jos\u00e9 Pombal and Manuel Faysse and Pierre Colombo and Fran\u00e7ois Yvon and Barry Haddow and Jos\u00e9 G. C. de Souza and Alexandra Birch and Andr\u00e9 F. T. Martins", "abstract": "  This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.\n", "link": "http://arxiv.org/abs/2506.04079v1", "date": "2025-06-04", "relevancy": 2.3746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EuroLLM-9B%3A%20Technical%20Report&body=Title%3A%20EuroLLM-9B%3A%20Technical%20Report%0AAuthor%3A%20Pedro%20Henrique%20Martins%20and%20Jo%C3%A3o%20Alves%20and%20Patrick%20Fernandes%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Amin%20Farajian%20and%20Mateusz%20Klimaszewski%20and%20Duarte%20M.%20Alves%20and%20Jos%C3%A9%20Pombal%20and%20Manuel%20Faysse%20and%20Pierre%20Colombo%20and%20Fran%C3%A7ois%20Yvon%20and%20Barry%20Haddow%20and%20Jos%C3%A9%20G.%20C.%20de%20Souza%20and%20Alexandra%20Birch%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20This%20report%20presents%20EuroLLM-9B%2C%20a%20large%20language%20model%20trained%20from%20scratch%0Ato%20support%20the%20needs%20of%20European%20citizens%20by%20covering%20all%2024%20official%20European%0AUnion%20languages%20and%2011%20additional%20languages.%20EuroLLM%20addresses%20the%20issue%20of%0AEuropean%20languages%20being%20underrepresented%20and%20underserved%20in%20existing%20open%0Alarge%20language%20models.%20We%20provide%20a%20comprehensive%20overview%20of%20EuroLLM-9B%27s%0Adevelopment%2C%20including%20tokenizer%20design%2C%20architectural%20specifications%2C%20data%0Afiltering%2C%20and%20training%20procedures.%20We%20describe%20the%20pre-training%20data%0Acollection%20and%20filtering%20pipeline%2C%20including%20the%20creation%20of%20EuroFilter%2C%20an%0AAI-based%20multilingual%20filter%2C%20as%20well%20as%20the%20design%20of%20EuroBlocks-Synthetic%2C%20a%0Anovel%20synthetic%20dataset%20for%20post-training%20that%20enhances%20language%20coverage%20for%0AEuropean%20languages.%20Evaluation%20results%20demonstrate%20EuroLLM-9B%27s%20competitive%0Aperformance%20on%20multilingual%20benchmarks%20and%20machine%20translation%20tasks%2C%0Aestablishing%20it%20as%20the%20leading%20open%20European-made%20LLM%20of%20its%20size.%20To%20support%0Aopen%20research%20and%20adoption%2C%20we%20release%20all%20major%20components%20of%20this%20work%2C%0Aincluding%20the%20base%20and%20instruction-tuned%20models%2C%20the%20EuroFilter%20classifier%2C%20and%0Athe%20synthetic%20post-training%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuroLLM-9B%253A%2520Technical%2520Report%26entry.906535625%3DPedro%2520Henrique%2520Martins%2520and%2520Jo%25C3%25A3o%2520Alves%2520and%2520Patrick%2520Fernandes%2520and%2520Nuno%2520M.%2520Guerreiro%2520and%2520Ricardo%2520Rei%2520and%2520Amin%2520Farajian%2520and%2520Mateusz%2520Klimaszewski%2520and%2520Duarte%2520M.%2520Alves%2520and%2520Jos%25C3%25A9%2520Pombal%2520and%2520Manuel%2520Faysse%2520and%2520Pierre%2520Colombo%2520and%2520Fran%25C3%25A7ois%2520Yvon%2520and%2520Barry%2520Haddow%2520and%2520Jos%25C3%25A9%2520G.%2520C.%2520de%2520Souza%2520and%2520Alexandra%2520Birch%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520This%2520report%2520presents%2520EuroLLM-9B%252C%2520a%2520large%2520language%2520model%2520trained%2520from%2520scratch%250Ato%2520support%2520the%2520needs%2520of%2520European%2520citizens%2520by%2520covering%2520all%252024%2520official%2520European%250AUnion%2520languages%2520and%252011%2520additional%2520languages.%2520EuroLLM%2520addresses%2520the%2520issue%2520of%250AEuropean%2520languages%2520being%2520underrepresented%2520and%2520underserved%2520in%2520existing%2520open%250Alarge%2520language%2520models.%2520We%2520provide%2520a%2520comprehensive%2520overview%2520of%2520EuroLLM-9B%2527s%250Adevelopment%252C%2520including%2520tokenizer%2520design%252C%2520architectural%2520specifications%252C%2520data%250Afiltering%252C%2520and%2520training%2520procedures.%2520We%2520describe%2520the%2520pre-training%2520data%250Acollection%2520and%2520filtering%2520pipeline%252C%2520including%2520the%2520creation%2520of%2520EuroFilter%252C%2520an%250AAI-based%2520multilingual%2520filter%252C%2520as%2520well%2520as%2520the%2520design%2520of%2520EuroBlocks-Synthetic%252C%2520a%250Anovel%2520synthetic%2520dataset%2520for%2520post-training%2520that%2520enhances%2520language%2520coverage%2520for%250AEuropean%2520languages.%2520Evaluation%2520results%2520demonstrate%2520EuroLLM-9B%2527s%2520competitive%250Aperformance%2520on%2520multilingual%2520benchmarks%2520and%2520machine%2520translation%2520tasks%252C%250Aestablishing%2520it%2520as%2520the%2520leading%2520open%2520European-made%2520LLM%2520of%2520its%2520size.%2520To%2520support%250Aopen%2520research%2520and%2520adoption%252C%2520we%2520release%2520all%2520major%2520components%2520of%2520this%2520work%252C%250Aincluding%2520the%2520base%2520and%2520instruction-tuned%2520models%252C%2520the%2520EuroFilter%2520classifier%252C%2520and%250Athe%2520synthetic%2520post-training%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EuroLLM-9B%3A%20Technical%20Report&entry.906535625=Pedro%20Henrique%20Martins%20and%20Jo%C3%A3o%20Alves%20and%20Patrick%20Fernandes%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Amin%20Farajian%20and%20Mateusz%20Klimaszewski%20and%20Duarte%20M.%20Alves%20and%20Jos%C3%A9%20Pombal%20and%20Manuel%20Faysse%20and%20Pierre%20Colombo%20and%20Fran%C3%A7ois%20Yvon%20and%20Barry%20Haddow%20and%20Jos%C3%A9%20G.%20C.%20de%20Souza%20and%20Alexandra%20Birch%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20This%20report%20presents%20EuroLLM-9B%2C%20a%20large%20language%20model%20trained%20from%20scratch%0Ato%20support%20the%20needs%20of%20European%20citizens%20by%20covering%20all%2024%20official%20European%0AUnion%20languages%20and%2011%20additional%20languages.%20EuroLLM%20addresses%20the%20issue%20of%0AEuropean%20languages%20being%20underrepresented%20and%20underserved%20in%20existing%20open%0Alarge%20language%20models.%20We%20provide%20a%20comprehensive%20overview%20of%20EuroLLM-9B%27s%0Adevelopment%2C%20including%20tokenizer%20design%2C%20architectural%20specifications%2C%20data%0Afiltering%2C%20and%20training%20procedures.%20We%20describe%20the%20pre-training%20data%0Acollection%20and%20filtering%20pipeline%2C%20including%20the%20creation%20of%20EuroFilter%2C%20an%0AAI-based%20multilingual%20filter%2C%20as%20well%20as%20the%20design%20of%20EuroBlocks-Synthetic%2C%20a%0Anovel%20synthetic%20dataset%20for%20post-training%20that%20enhances%20language%20coverage%20for%0AEuropean%20languages.%20Evaluation%20results%20demonstrate%20EuroLLM-9B%27s%20competitive%0Aperformance%20on%20multilingual%20benchmarks%20and%20machine%20translation%20tasks%2C%0Aestablishing%20it%20as%20the%20leading%20open%20European-made%20LLM%20of%20its%20size.%20To%20support%0Aopen%20research%20and%20adoption%2C%20we%20release%20all%20major%20components%20of%20this%20work%2C%0Aincluding%20the%20base%20and%20instruction-tuned%20models%2C%20the%20EuroFilter%20classifier%2C%20and%0Athe%20synthetic%20post-training%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04079v1&entry.124074799=Read"},
{"title": "Video, How Do Your Tokens Merge?", "author": "Sam Pollard and Michael Wray", "abstract": "  Video transformer models require huge amounts of compute resources due to the\nspatio-temporal scaling of the input. Tackling this, recent methods have\nproposed to drop or merge tokens for image models, whether randomly or via\nlearned methods. Merging tokens has many benefits: it can be plugged into any\nvision transformer, does not require model re-training, and it propagates\ninformation that would otherwise be dropped through the model. Before now,\nvideo token merging has not been evaluated on temporally complex datasets for\nvideo understanding. In this work, we explore training-free token merging for\nvideo to provide comprehensive experiments and find best practices across four\nvideo transformers on three datasets that exhibit coarse and fine-grained\naction recognition. Our results showcase the benefits of video token merging\nwith a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\\%$ for\nViViT). Code available at\nhttps://github.com/sjpollard/video-how-do-your-tokens-merge.\n", "link": "http://arxiv.org/abs/2506.03885v1", "date": "2025-06-04", "relevancy": 2.3609, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5965}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.595}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%2C%20How%20Do%20Your%20Tokens%20Merge%3F&body=Title%3A%20Video%2C%20How%20Do%20Your%20Tokens%20Merge%3F%0AAuthor%3A%20Sam%20Pollard%20and%20Michael%20Wray%0AAbstract%3A%20%20%20Video%20transformer%20models%20require%20huge%20amounts%20of%20compute%20resources%20due%20to%20the%0Aspatio-temporal%20scaling%20of%20the%20input.%20Tackling%20this%2C%20recent%20methods%20have%0Aproposed%20to%20drop%20or%20merge%20tokens%20for%20image%20models%2C%20whether%20randomly%20or%20via%0Alearned%20methods.%20Merging%20tokens%20has%20many%20benefits%3A%20it%20can%20be%20plugged%20into%20any%0Avision%20transformer%2C%20does%20not%20require%20model%20re-training%2C%20and%20it%20propagates%0Ainformation%20that%20would%20otherwise%20be%20dropped%20through%20the%20model.%20Before%20now%2C%0Avideo%20token%20merging%20has%20not%20been%20evaluated%20on%20temporally%20complex%20datasets%20for%0Avideo%20understanding.%20In%20this%20work%2C%20we%20explore%20training-free%20token%20merging%20for%0Avideo%20to%20provide%20comprehensive%20experiments%20and%20find%20best%20practices%20across%20four%0Avideo%20transformers%20on%20three%20datasets%20that%20exhibit%20coarse%20and%20fine-grained%0Aaction%20recognition.%20Our%20results%20showcase%20the%20benefits%20of%20video%20token%20merging%0Awith%20a%20speedup%20of%20around%20%242.5%24X%20while%20maintaining%20accuracy%20%28avg.%20%24-0.55%5C%25%24%20for%0AViViT%29.%20Code%20available%20at%0Ahttps%3A//github.com/sjpollard/video-how-do-your-tokens-merge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%252C%2520How%2520Do%2520Your%2520Tokens%2520Merge%253F%26entry.906535625%3DSam%2520Pollard%2520and%2520Michael%2520Wray%26entry.1292438233%3D%2520%2520Video%2520transformer%2520models%2520require%2520huge%2520amounts%2520of%2520compute%2520resources%2520due%2520to%2520the%250Aspatio-temporal%2520scaling%2520of%2520the%2520input.%2520Tackling%2520this%252C%2520recent%2520methods%2520have%250Aproposed%2520to%2520drop%2520or%2520merge%2520tokens%2520for%2520image%2520models%252C%2520whether%2520randomly%2520or%2520via%250Alearned%2520methods.%2520Merging%2520tokens%2520has%2520many%2520benefits%253A%2520it%2520can%2520be%2520plugged%2520into%2520any%250Avision%2520transformer%252C%2520does%2520not%2520require%2520model%2520re-training%252C%2520and%2520it%2520propagates%250Ainformation%2520that%2520would%2520otherwise%2520be%2520dropped%2520through%2520the%2520model.%2520Before%2520now%252C%250Avideo%2520token%2520merging%2520has%2520not%2520been%2520evaluated%2520on%2520temporally%2520complex%2520datasets%2520for%250Avideo%2520understanding.%2520In%2520this%2520work%252C%2520we%2520explore%2520training-free%2520token%2520merging%2520for%250Avideo%2520to%2520provide%2520comprehensive%2520experiments%2520and%2520find%2520best%2520practices%2520across%2520four%250Avideo%2520transformers%2520on%2520three%2520datasets%2520that%2520exhibit%2520coarse%2520and%2520fine-grained%250Aaction%2520recognition.%2520Our%2520results%2520showcase%2520the%2520benefits%2520of%2520video%2520token%2520merging%250Awith%2520a%2520speedup%2520of%2520around%2520%25242.5%2524X%2520while%2520maintaining%2520accuracy%2520%2528avg.%2520%2524-0.55%255C%2525%2524%2520for%250AViViT%2529.%2520Code%2520available%2520at%250Ahttps%253A//github.com/sjpollard/video-how-do-your-tokens-merge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%2C%20How%20Do%20Your%20Tokens%20Merge%3F&entry.906535625=Sam%20Pollard%20and%20Michael%20Wray&entry.1292438233=%20%20Video%20transformer%20models%20require%20huge%20amounts%20of%20compute%20resources%20due%20to%20the%0Aspatio-temporal%20scaling%20of%20the%20input.%20Tackling%20this%2C%20recent%20methods%20have%0Aproposed%20to%20drop%20or%20merge%20tokens%20for%20image%20models%2C%20whether%20randomly%20or%20via%0Alearned%20methods.%20Merging%20tokens%20has%20many%20benefits%3A%20it%20can%20be%20plugged%20into%20any%0Avision%20transformer%2C%20does%20not%20require%20model%20re-training%2C%20and%20it%20propagates%0Ainformation%20that%20would%20otherwise%20be%20dropped%20through%20the%20model.%20Before%20now%2C%0Avideo%20token%20merging%20has%20not%20been%20evaluated%20on%20temporally%20complex%20datasets%20for%0Avideo%20understanding.%20In%20this%20work%2C%20we%20explore%20training-free%20token%20merging%20for%0Avideo%20to%20provide%20comprehensive%20experiments%20and%20find%20best%20practices%20across%20four%0Avideo%20transformers%20on%20three%20datasets%20that%20exhibit%20coarse%20and%20fine-grained%0Aaction%20recognition.%20Our%20results%20showcase%20the%20benefits%20of%20video%20token%20merging%0Awith%20a%20speedup%20of%20around%20%242.5%24X%20while%20maintaining%20accuracy%20%28avg.%20%24-0.55%5C%25%24%20for%0AViViT%29.%20Code%20available%20at%0Ahttps%3A//github.com/sjpollard/video-how-do-your-tokens-merge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03885v1&entry.124074799=Read"},
{"title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship\n  Construction and Feature-rich Attention Fusion", "author": "Le Cheng and Peican Zhu and Yangming Guo and Keke Tang and Chao Gao and Zhen Wang", "abstract": "  Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.12894v2", "date": "2025-06-04", "relevancy": 2.3459, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4761}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4692}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperDet%3A%20Source%20Detection%20in%20Hypergraphs%20via%20Interactive%20Relationship%0A%20%20Construction%20and%20Feature-rich%20Attention%20Fusion&body=Title%3A%20HyperDet%3A%20Source%20Detection%20in%20Hypergraphs%20via%20Interactive%20Relationship%0A%20%20Construction%20and%20Feature-rich%20Attention%20Fusion%0AAuthor%3A%20Le%20Cheng%20and%20Peican%20Zhu%20and%20Yangming%20Guo%20and%20Keke%20Tang%20and%20Chao%20Gao%20and%20Zhen%20Wang%0AAbstract%3A%20%20%20Hypergraphs%20offer%20superior%20modeling%20capabilities%20for%20social%20networks%2C%0Aparticularly%20in%20capturing%20group%20phenomena%20that%20extend%20beyond%20pairwise%0Ainteractions%20in%20rumor%20propagation.%20Existing%20approaches%20in%20rumor%20source%0Adetection%20predominantly%20focus%20on%20dyadic%20interactions%2C%20which%20inadequately%0Aaddress%20the%20complexity%20of%20more%20intricate%20relational%20structures.%20In%20this%20study%2C%0Awe%20present%20a%20novel%20approach%20for%20Source%20Detection%20in%20Hypergraphs%20%28HyperDet%29%20via%0AInteractive%20Relationship%20Construction%20and%20Feature-rich%20Attention%20Fusion.%0ASpecifically%2C%20our%20methodology%20employs%20an%20Interactive%20Relationship%20Construction%0Amodule%20to%20accurately%20model%20both%20the%20static%20topology%20and%20dynamic%20interactions%0Aamong%20users%2C%20followed%20by%20the%20Feature-rich%20Attention%20Fusion%20module%2C%20which%0Aautonomously%20learns%20node%20features%20and%20discriminates%20between%20nodes%20using%20a%0Aself-attention%20mechanism%2C%20thereby%20effectively%20learning%20node%20representations%0Aunder%20the%20framework%20of%20accurately%20modeled%20higher-order%20relationships.%20Extensive%0Aexperimental%20validation%20confirms%20the%20efficacy%20of%20our%20HyperDet%20approach%2C%0Ashowcasing%20its%20superiority%20relative%20to%20current%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperDet%253A%2520Source%2520Detection%2520in%2520Hypergraphs%2520via%2520Interactive%2520Relationship%250A%2520%2520Construction%2520and%2520Feature-rich%2520Attention%2520Fusion%26entry.906535625%3DLe%2520Cheng%2520and%2520Peican%2520Zhu%2520and%2520Yangming%2520Guo%2520and%2520Keke%2520Tang%2520and%2520Chao%2520Gao%2520and%2520Zhen%2520Wang%26entry.1292438233%3D%2520%2520Hypergraphs%2520offer%2520superior%2520modeling%2520capabilities%2520for%2520social%2520networks%252C%250Aparticularly%2520in%2520capturing%2520group%2520phenomena%2520that%2520extend%2520beyond%2520pairwise%250Ainteractions%2520in%2520rumor%2520propagation.%2520Existing%2520approaches%2520in%2520rumor%2520source%250Adetection%2520predominantly%2520focus%2520on%2520dyadic%2520interactions%252C%2520which%2520inadequately%250Aaddress%2520the%2520complexity%2520of%2520more%2520intricate%2520relational%2520structures.%2520In%2520this%2520study%252C%250Awe%2520present%2520a%2520novel%2520approach%2520for%2520Source%2520Detection%2520in%2520Hypergraphs%2520%2528HyperDet%2529%2520via%250AInteractive%2520Relationship%2520Construction%2520and%2520Feature-rich%2520Attention%2520Fusion.%250ASpecifically%252C%2520our%2520methodology%2520employs%2520an%2520Interactive%2520Relationship%2520Construction%250Amodule%2520to%2520accurately%2520model%2520both%2520the%2520static%2520topology%2520and%2520dynamic%2520interactions%250Aamong%2520users%252C%2520followed%2520by%2520the%2520Feature-rich%2520Attention%2520Fusion%2520module%252C%2520which%250Aautonomously%2520learns%2520node%2520features%2520and%2520discriminates%2520between%2520nodes%2520using%2520a%250Aself-attention%2520mechanism%252C%2520thereby%2520effectively%2520learning%2520node%2520representations%250Aunder%2520the%2520framework%2520of%2520accurately%2520modeled%2520higher-order%2520relationships.%2520Extensive%250Aexperimental%2520validation%2520confirms%2520the%2520efficacy%2520of%2520our%2520HyperDet%2520approach%252C%250Ashowcasing%2520its%2520superiority%2520relative%2520to%2520current%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperDet%3A%20Source%20Detection%20in%20Hypergraphs%20via%20Interactive%20Relationship%0A%20%20Construction%20and%20Feature-rich%20Attention%20Fusion&entry.906535625=Le%20Cheng%20and%20Peican%20Zhu%20and%20Yangming%20Guo%20and%20Keke%20Tang%20and%20Chao%20Gao%20and%20Zhen%20Wang&entry.1292438233=%20%20Hypergraphs%20offer%20superior%20modeling%20capabilities%20for%20social%20networks%2C%0Aparticularly%20in%20capturing%20group%20phenomena%20that%20extend%20beyond%20pairwise%0Ainteractions%20in%20rumor%20propagation.%20Existing%20approaches%20in%20rumor%20source%0Adetection%20predominantly%20focus%20on%20dyadic%20interactions%2C%20which%20inadequately%0Aaddress%20the%20complexity%20of%20more%20intricate%20relational%20structures.%20In%20this%20study%2C%0Awe%20present%20a%20novel%20approach%20for%20Source%20Detection%20in%20Hypergraphs%20%28HyperDet%29%20via%0AInteractive%20Relationship%20Construction%20and%20Feature-rich%20Attention%20Fusion.%0ASpecifically%2C%20our%20methodology%20employs%20an%20Interactive%20Relationship%20Construction%0Amodule%20to%20accurately%20model%20both%20the%20static%20topology%20and%20dynamic%20interactions%0Aamong%20users%2C%20followed%20by%20the%20Feature-rich%20Attention%20Fusion%20module%2C%20which%0Aautonomously%20learns%20node%20features%20and%20discriminates%20between%20nodes%20using%20a%0Aself-attention%20mechanism%2C%20thereby%20effectively%20learning%20node%20representations%0Aunder%20the%20framework%20of%20accurately%20modeled%20higher-order%20relationships.%20Extensive%0Aexperimental%20validation%20confirms%20the%20efficacy%20of%20our%20HyperDet%20approach%2C%0Ashowcasing%20its%20superiority%20relative%20to%20current%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12894v2&entry.124074799=Read"},
{"title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature\n  Concatenation on Medical Imaging", "author": "Dashti A. Ali and Richard K. G. Do and William R. Jarnagin and Aras T. Asaad and Amber L. Simpson", "abstract": "  In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments.\n", "link": "http://arxiv.org/abs/2505.23637v2", "date": "2025-06-04", "relevancy": 2.3441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging&body=Title%3A%20Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging%0AAuthor%3A%20Dashti%20A.%20Ali%20and%20Richard%20K.%20G.%20Do%20and%20William%20R.%20Jarnagin%20and%20Aras%20T.%20Asaad%20and%20Amber%20L.%20Simpson%0AAbstract%3A%20%20%20In%20medical%20image%20analysis%2C%20feature%20engineering%20plays%20an%20important%20role%20in%20the%0Adesign%20and%20performance%20of%20machine%20learning%20models.%20Persistent%20homology%20%28PH%29%2C%0Afrom%20the%20field%20of%20topological%20data%20analysis%20%28TDA%29%2C%20demonstrates%20robustness%20and%0Astability%20to%20data%20perturbations%20and%20addresses%20the%20limitation%20from%20traditional%0Afeature%20extraction%20approaches%20where%20a%20small%20change%20in%20input%20results%20in%20a%20large%0Achange%20in%20feature%20representation.%20Using%20PH%2C%20we%20store%20persistent%20topological%20and%0Ageometrical%20features%20in%20the%20form%20of%20the%20persistence%20barcode%20whereby%20large%20bars%0Arepresent%20global%20topological%20features%20and%20small%20bars%20encapsulate%20geometrical%0Ainformation%20of%20the%20data.%20When%20multiple%20barcodes%20are%20computed%20from%202D%20or%203D%0Amedical%20images%2C%20two%20approaches%20can%20be%20used%20to%20construct%20the%20final%20topological%0Afeature%20vector%20in%20each%20dimension%3A%20aggregating%20persistence%20barcodes%20followed%20by%0Afeaturization%20or%20concatenating%20topological%20feature%20vectors%20derived%20from%20each%0Abarcode.%20In%20this%20study%2C%20we%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Amedical%20imaging%20datasets%20to%20compare%20the%20effects%20of%20the%20two%20aforementioned%0Aapproaches%20on%20the%20performance%20of%20classification%20models.%20The%20results%20of%20this%0Aanalysis%20indicate%20that%20feature%20concatenation%20preserves%20detailed%20topological%0Ainformation%20from%20individual%20barcodes%2C%20yields%20better%20classification%20performance%0Aand%20is%20therefore%20a%20preferred%20approach%20when%20conducting%20similar%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520the%2520Effects%2520of%2520Persistence%2520Barcodes%2520Aggregation%2520and%2520Feature%250A%2520%2520Concatenation%2520on%2520Medical%2520Imaging%26entry.906535625%3DDashti%2520A.%2520Ali%2520and%2520Richard%2520K.%2520G.%2520Do%2520and%2520William%2520R.%2520Jarnagin%2520and%2520Aras%2520T.%2520Asaad%2520and%2520Amber%2520L.%2520Simpson%26entry.1292438233%3D%2520%2520In%2520medical%2520image%2520analysis%252C%2520feature%2520engineering%2520plays%2520an%2520important%2520role%2520in%2520the%250Adesign%2520and%2520performance%2520of%2520machine%2520learning%2520models.%2520Persistent%2520homology%2520%2528PH%2529%252C%250Afrom%2520the%2520field%2520of%2520topological%2520data%2520analysis%2520%2528TDA%2529%252C%2520demonstrates%2520robustness%2520and%250Astability%2520to%2520data%2520perturbations%2520and%2520addresses%2520the%2520limitation%2520from%2520traditional%250Afeature%2520extraction%2520approaches%2520where%2520a%2520small%2520change%2520in%2520input%2520results%2520in%2520a%2520large%250Achange%2520in%2520feature%2520representation.%2520Using%2520PH%252C%2520we%2520store%2520persistent%2520topological%2520and%250Ageometrical%2520features%2520in%2520the%2520form%2520of%2520the%2520persistence%2520barcode%2520whereby%2520large%2520bars%250Arepresent%2520global%2520topological%2520features%2520and%2520small%2520bars%2520encapsulate%2520geometrical%250Ainformation%2520of%2520the%2520data.%2520When%2520multiple%2520barcodes%2520are%2520computed%2520from%25202D%2520or%25203D%250Amedical%2520images%252C%2520two%2520approaches%2520can%2520be%2520used%2520to%2520construct%2520the%2520final%2520topological%250Afeature%2520vector%2520in%2520each%2520dimension%253A%2520aggregating%2520persistence%2520barcodes%2520followed%2520by%250Afeaturization%2520or%2520concatenating%2520topological%2520feature%2520vectors%2520derived%2520from%2520each%250Abarcode.%2520In%2520this%2520study%252C%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520across%2520diverse%250Amedical%2520imaging%2520datasets%2520to%2520compare%2520the%2520effects%2520of%2520the%2520two%2520aforementioned%250Aapproaches%2520on%2520the%2520performance%2520of%2520classification%2520models.%2520The%2520results%2520of%2520this%250Aanalysis%2520indicate%2520that%2520feature%2520concatenation%2520preserves%2520detailed%2520topological%250Ainformation%2520from%2520individual%2520barcodes%252C%2520yields%2520better%2520classification%2520performance%250Aand%2520is%2520therefore%2520a%2520preferred%2520approach%2520when%2520conducting%2520similar%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20the%20Effects%20of%20Persistence%20Barcodes%20Aggregation%20and%20Feature%0A%20%20Concatenation%20on%20Medical%20Imaging&entry.906535625=Dashti%20A.%20Ali%20and%20Richard%20K.%20G.%20Do%20and%20William%20R.%20Jarnagin%20and%20Aras%20T.%20Asaad%20and%20Amber%20L.%20Simpson&entry.1292438233=%20%20In%20medical%20image%20analysis%2C%20feature%20engineering%20plays%20an%20important%20role%20in%20the%0Adesign%20and%20performance%20of%20machine%20learning%20models.%20Persistent%20homology%20%28PH%29%2C%0Afrom%20the%20field%20of%20topological%20data%20analysis%20%28TDA%29%2C%20demonstrates%20robustness%20and%0Astability%20to%20data%20perturbations%20and%20addresses%20the%20limitation%20from%20traditional%0Afeature%20extraction%20approaches%20where%20a%20small%20change%20in%20input%20results%20in%20a%20large%0Achange%20in%20feature%20representation.%20Using%20PH%2C%20we%20store%20persistent%20topological%20and%0Ageometrical%20features%20in%20the%20form%20of%20the%20persistence%20barcode%20whereby%20large%20bars%0Arepresent%20global%20topological%20features%20and%20small%20bars%20encapsulate%20geometrical%0Ainformation%20of%20the%20data.%20When%20multiple%20barcodes%20are%20computed%20from%202D%20or%203D%0Amedical%20images%2C%20two%20approaches%20can%20be%20used%20to%20construct%20the%20final%20topological%0Afeature%20vector%20in%20each%20dimension%3A%20aggregating%20persistence%20barcodes%20followed%20by%0Afeaturization%20or%20concatenating%20topological%20feature%20vectors%20derived%20from%20each%0Abarcode.%20In%20this%20study%2C%20we%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Amedical%20imaging%20datasets%20to%20compare%20the%20effects%20of%20the%20two%20aforementioned%0Aapproaches%20on%20the%20performance%20of%20classification%20models.%20The%20results%20of%20this%0Aanalysis%20indicate%20that%20feature%20concatenation%20preserves%20detailed%20topological%0Ainformation%20from%20individual%20barcodes%2C%20yields%20better%20classification%20performance%0Aand%20is%20therefore%20a%20preferred%20approach%20when%20conducting%20similar%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23637v2&entry.124074799=Read"},
{"title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts", "author": "Ming Zhang and Yuhui Wang and Yujiong Shen and Tingyi Yang and Changhao Jiang and Yilong Wu and Shihan Dou and Qinhao Chen and Zhiheng Xi and Zhihao Zhang and Yi Dong and Zhen Wang and Zhihui Fei and Mingyang Wan and Tao Liang and Guojun Ma and Qi Zhang and Tao Gui and Xuanjing Huang", "abstract": "  Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.\n", "link": "http://arxiv.org/abs/2503.06706v2", "date": "2025-06-04", "relevancy": 2.3408, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFDial%3A%20A%20Structured%20Dialogue%20Instruction%20Fine-tuning%20Method%20Based%20on%0A%20%20UML%20Flowcharts&body=Title%3A%20PFDial%3A%20A%20Structured%20Dialogue%20Instruction%20Fine-tuning%20Method%20Based%20on%0A%20%20UML%20Flowcharts%0AAuthor%3A%20Ming%20Zhang%20and%20Yuhui%20Wang%20and%20Yujiong%20Shen%20and%20Tingyi%20Yang%20and%20Changhao%20Jiang%20and%20Yilong%20Wu%20and%20Shihan%20Dou%20and%20Qinhao%20Chen%20and%20Zhiheng%20Xi%20and%20Zhihao%20Zhang%20and%20Yi%20Dong%20and%20Zhen%20Wang%20and%20Zhihui%20Fei%20and%20Mingyang%20Wan%20and%20Tao%20Liang%20and%20Guojun%20Ma%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Process-driven%20dialogue%20systems%2C%20which%20operate%20under%20strict%20predefined%0Aprocess%20constraints%2C%20are%20essential%20in%20customer%20service%20and%20equipment%0Amaintenance%20scenarios.%20Although%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Aremarkable%20progress%20in%20dialogue%20and%20reasoning%2C%20they%20still%20struggle%20to%20solve%0Athese%20strictly%20constrained%20dialogue%20tasks.%20To%20address%20this%20challenge%2C%20we%0Aconstruct%20Process%20Flow%20Dialogue%20%28PFDial%29%20dataset%2C%20which%20contains%2012%2C705%0Ahigh-quality%20Chinese%20dialogue%20instructions%20derived%20from%20440%20flowcharts%0Acontaining%205%2C055%20process%20nodes.%20Based%20on%20PlantUML%20specification%2C%20each%20UML%0Aflowchart%20is%20converted%20into%20atomic%20dialogue%20units%20i.e.%2C%20structured%20five-tuples.%0AExperimental%20results%20demonstrate%20that%20a%207B%20model%20trained%20with%20merely%20800%0Asamples%2C%20and%20a%200.5B%20model%20trained%20on%20total%20data%20both%20can%20surpass%2090%25%20accuracy.%0AAdditionally%2C%20the%208B%20model%20can%20surpass%20GPT-4o%20up%20to%2043.88%25%20with%20an%20average%20of%0A11.00%25.%20We%20further%20evaluate%20models%27%20performance%20on%20challenging%20backward%0Atransitions%20in%20process%20flows%20and%20conduct%20an%20in-depth%20analysis%20of%20various%0Adataset%20formats%20to%20reveal%20their%20impact%20on%20model%20performance%20in%20handling%0Adecision%20and%20sequential%20branches.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/PFDial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFDial%253A%2520A%2520Structured%2520Dialogue%2520Instruction%2520Fine-tuning%2520Method%2520Based%2520on%250A%2520%2520UML%2520Flowcharts%26entry.906535625%3DMing%2520Zhang%2520and%2520Yuhui%2520Wang%2520and%2520Yujiong%2520Shen%2520and%2520Tingyi%2520Yang%2520and%2520Changhao%2520Jiang%2520and%2520Yilong%2520Wu%2520and%2520Shihan%2520Dou%2520and%2520Qinhao%2520Chen%2520and%2520Zhiheng%2520Xi%2520and%2520Zhihao%2520Zhang%2520and%2520Yi%2520Dong%2520and%2520Zhen%2520Wang%2520and%2520Zhihui%2520Fei%2520and%2520Mingyang%2520Wan%2520and%2520Tao%2520Liang%2520and%2520Guojun%2520Ma%2520and%2520Qi%2520Zhang%2520and%2520Tao%2520Gui%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520Process-driven%2520dialogue%2520systems%252C%2520which%2520operate%2520under%2520strict%2520predefined%250Aprocess%2520constraints%252C%2520are%2520essential%2520in%2520customer%2520service%2520and%2520equipment%250Amaintenance%2520scenarios.%2520Although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%250Aremarkable%2520progress%2520in%2520dialogue%2520and%2520reasoning%252C%2520they%2520still%2520struggle%2520to%2520solve%250Athese%2520strictly%2520constrained%2520dialogue%2520tasks.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aconstruct%2520Process%2520Flow%2520Dialogue%2520%2528PFDial%2529%2520dataset%252C%2520which%2520contains%252012%252C705%250Ahigh-quality%2520Chinese%2520dialogue%2520instructions%2520derived%2520from%2520440%2520flowcharts%250Acontaining%25205%252C055%2520process%2520nodes.%2520Based%2520on%2520PlantUML%2520specification%252C%2520each%2520UML%250Aflowchart%2520is%2520converted%2520into%2520atomic%2520dialogue%2520units%2520i.e.%252C%2520structured%2520five-tuples.%250AExperimental%2520results%2520demonstrate%2520that%2520a%25207B%2520model%2520trained%2520with%2520merely%2520800%250Asamples%252C%2520and%2520a%25200.5B%2520model%2520trained%2520on%2520total%2520data%2520both%2520can%2520surpass%252090%2525%2520accuracy.%250AAdditionally%252C%2520the%25208B%2520model%2520can%2520surpass%2520GPT-4o%2520up%2520to%252043.88%2525%2520with%2520an%2520average%2520of%250A11.00%2525.%2520We%2520further%2520evaluate%2520models%2527%2520performance%2520on%2520challenging%2520backward%250Atransitions%2520in%2520process%2520flows%2520and%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520various%250Adataset%2520formats%2520to%2520reveal%2520their%2520impact%2520on%2520model%2520performance%2520in%2520handling%250Adecision%2520and%2520sequential%2520branches.%2520The%2520data%2520is%2520released%2520in%250Ahttps%253A//github.com/KongLongGeFDU/PFDial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFDial%3A%20A%20Structured%20Dialogue%20Instruction%20Fine-tuning%20Method%20Based%20on%0A%20%20UML%20Flowcharts&entry.906535625=Ming%20Zhang%20and%20Yuhui%20Wang%20and%20Yujiong%20Shen%20and%20Tingyi%20Yang%20and%20Changhao%20Jiang%20and%20Yilong%20Wu%20and%20Shihan%20Dou%20and%20Qinhao%20Chen%20and%20Zhiheng%20Xi%20and%20Zhihao%20Zhang%20and%20Yi%20Dong%20and%20Zhen%20Wang%20and%20Zhihui%20Fei%20and%20Mingyang%20Wan%20and%20Tao%20Liang%20and%20Guojun%20Ma%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Process-driven%20dialogue%20systems%2C%20which%20operate%20under%20strict%20predefined%0Aprocess%20constraints%2C%20are%20essential%20in%20customer%20service%20and%20equipment%0Amaintenance%20scenarios.%20Although%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Aremarkable%20progress%20in%20dialogue%20and%20reasoning%2C%20they%20still%20struggle%20to%20solve%0Athese%20strictly%20constrained%20dialogue%20tasks.%20To%20address%20this%20challenge%2C%20we%0Aconstruct%20Process%20Flow%20Dialogue%20%28PFDial%29%20dataset%2C%20which%20contains%2012%2C705%0Ahigh-quality%20Chinese%20dialogue%20instructions%20derived%20from%20440%20flowcharts%0Acontaining%205%2C055%20process%20nodes.%20Based%20on%20PlantUML%20specification%2C%20each%20UML%0Aflowchart%20is%20converted%20into%20atomic%20dialogue%20units%20i.e.%2C%20structured%20five-tuples.%0AExperimental%20results%20demonstrate%20that%20a%207B%20model%20trained%20with%20merely%20800%0Asamples%2C%20and%20a%200.5B%20model%20trained%20on%20total%20data%20both%20can%20surpass%2090%25%20accuracy.%0AAdditionally%2C%20the%208B%20model%20can%20surpass%20GPT-4o%20up%20to%2043.88%25%20with%20an%20average%20of%0A11.00%25.%20We%20further%20evaluate%20models%27%20performance%20on%20challenging%20backward%0Atransitions%20in%20process%20flows%20and%20conduct%20an%20in-depth%20analysis%20of%20various%0Adataset%20formats%20to%20reveal%20their%20impact%20on%20model%20performance%20in%20handling%0Adecision%20and%20sequential%20branches.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/PFDial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06706v2&entry.124074799=Read"},
{"title": "Enhancing Safety of Foundation Models for Visual Navigation through\n  Collision Avoidance via Repulsive Estimation", "author": "Joonkyung Kim and Joonyeol Sim and Woojun Kim and Katia Sycara and Changjoo Nam", "abstract": "  We propose CARE (Collision Avoidance via Repulsive Estimation), a\nplug-and-play module that enhances the safety of vision-based navigation\nwithout requiring additional range sensors or fine-tuning of pretrained models.\nWhile recent foundation models using only RGB inputs have shown strong\nperformance, they often fail to generalize in out-of-distribution (OOD)\nenvironments with unseen objects or variations in camera parameters (e.g.,\nfield of view, pose, or focal length). Without fine-tuning, these models may\ngenerate unsafe trajectories that lead to collisions, requiring costly data\ncollection and retraining. CARE addresses this limitation by seamlessly\nintegrating with any RGB-based navigation system that outputs local\ntrajectories, dynamically adjusting them using repulsive force vectors derived\nfrom monocular depth maps. We evaluate CARE by combining it with\nstate-of-the-art vision-based navigation models across multiple robot\nplatforms. CARE consistently reduces collision rates (up to 100%) without\nsacrificing goal-reaching performance and improves collision-free travel\ndistance by up to 10.7x in exploration tasks.\n", "link": "http://arxiv.org/abs/2506.03834v1", "date": "2025-06-04", "relevancy": 2.3323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Safety%20of%20Foundation%20Models%20for%20Visual%20Navigation%20through%0A%20%20Collision%20Avoidance%20via%20Repulsive%20Estimation&body=Title%3A%20Enhancing%20Safety%20of%20Foundation%20Models%20for%20Visual%20Navigation%20through%0A%20%20Collision%20Avoidance%20via%20Repulsive%20Estimation%0AAuthor%3A%20Joonkyung%20Kim%20and%20Joonyeol%20Sim%20and%20Woojun%20Kim%20and%20Katia%20Sycara%20and%20Changjoo%20Nam%0AAbstract%3A%20%20%20We%20propose%20CARE%20%28Collision%20Avoidance%20via%20Repulsive%20Estimation%29%2C%20a%0Aplug-and-play%20module%20that%20enhances%20the%20safety%20of%20vision-based%20navigation%0Awithout%20requiring%20additional%20range%20sensors%20or%20fine-tuning%20of%20pretrained%20models.%0AWhile%20recent%20foundation%20models%20using%20only%20RGB%20inputs%20have%20shown%20strong%0Aperformance%2C%20they%20often%20fail%20to%20generalize%20in%20out-of-distribution%20%28OOD%29%0Aenvironments%20with%20unseen%20objects%20or%20variations%20in%20camera%20parameters%20%28e.g.%2C%0Afield%20of%20view%2C%20pose%2C%20or%20focal%20length%29.%20Without%20fine-tuning%2C%20these%20models%20may%0Agenerate%20unsafe%20trajectories%20that%20lead%20to%20collisions%2C%20requiring%20costly%20data%0Acollection%20and%20retraining.%20CARE%20addresses%20this%20limitation%20by%20seamlessly%0Aintegrating%20with%20any%20RGB-based%20navigation%20system%20that%20outputs%20local%0Atrajectories%2C%20dynamically%20adjusting%20them%20using%20repulsive%20force%20vectors%20derived%0Afrom%20monocular%20depth%20maps.%20We%20evaluate%20CARE%20by%20combining%20it%20with%0Astate-of-the-art%20vision-based%20navigation%20models%20across%20multiple%20robot%0Aplatforms.%20CARE%20consistently%20reduces%20collision%20rates%20%28up%20to%20100%25%29%20without%0Asacrificing%20goal-reaching%20performance%20and%20improves%20collision-free%20travel%0Adistance%20by%20up%20to%2010.7x%20in%20exploration%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Safety%2520of%2520Foundation%2520Models%2520for%2520Visual%2520Navigation%2520through%250A%2520%2520Collision%2520Avoidance%2520via%2520Repulsive%2520Estimation%26entry.906535625%3DJoonkyung%2520Kim%2520and%2520Joonyeol%2520Sim%2520and%2520Woojun%2520Kim%2520and%2520Katia%2520Sycara%2520and%2520Changjoo%2520Nam%26entry.1292438233%3D%2520%2520We%2520propose%2520CARE%2520%2528Collision%2520Avoidance%2520via%2520Repulsive%2520Estimation%2529%252C%2520a%250Aplug-and-play%2520module%2520that%2520enhances%2520the%2520safety%2520of%2520vision-based%2520navigation%250Awithout%2520requiring%2520additional%2520range%2520sensors%2520or%2520fine-tuning%2520of%2520pretrained%2520models.%250AWhile%2520recent%2520foundation%2520models%2520using%2520only%2520RGB%2520inputs%2520have%2520shown%2520strong%250Aperformance%252C%2520they%2520often%2520fail%2520to%2520generalize%2520in%2520out-of-distribution%2520%2528OOD%2529%250Aenvironments%2520with%2520unseen%2520objects%2520or%2520variations%2520in%2520camera%2520parameters%2520%2528e.g.%252C%250Afield%2520of%2520view%252C%2520pose%252C%2520or%2520focal%2520length%2529.%2520Without%2520fine-tuning%252C%2520these%2520models%2520may%250Agenerate%2520unsafe%2520trajectories%2520that%2520lead%2520to%2520collisions%252C%2520requiring%2520costly%2520data%250Acollection%2520and%2520retraining.%2520CARE%2520addresses%2520this%2520limitation%2520by%2520seamlessly%250Aintegrating%2520with%2520any%2520RGB-based%2520navigation%2520system%2520that%2520outputs%2520local%250Atrajectories%252C%2520dynamically%2520adjusting%2520them%2520using%2520repulsive%2520force%2520vectors%2520derived%250Afrom%2520monocular%2520depth%2520maps.%2520We%2520evaluate%2520CARE%2520by%2520combining%2520it%2520with%250Astate-of-the-art%2520vision-based%2520navigation%2520models%2520across%2520multiple%2520robot%250Aplatforms.%2520CARE%2520consistently%2520reduces%2520collision%2520rates%2520%2528up%2520to%2520100%2525%2529%2520without%250Asacrificing%2520goal-reaching%2520performance%2520and%2520improves%2520collision-free%2520travel%250Adistance%2520by%2520up%2520to%252010.7x%2520in%2520exploration%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Safety%20of%20Foundation%20Models%20for%20Visual%20Navigation%20through%0A%20%20Collision%20Avoidance%20via%20Repulsive%20Estimation&entry.906535625=Joonkyung%20Kim%20and%20Joonyeol%20Sim%20and%20Woojun%20Kim%20and%20Katia%20Sycara%20and%20Changjoo%20Nam&entry.1292438233=%20%20We%20propose%20CARE%20%28Collision%20Avoidance%20via%20Repulsive%20Estimation%29%2C%20a%0Aplug-and-play%20module%20that%20enhances%20the%20safety%20of%20vision-based%20navigation%0Awithout%20requiring%20additional%20range%20sensors%20or%20fine-tuning%20of%20pretrained%20models.%0AWhile%20recent%20foundation%20models%20using%20only%20RGB%20inputs%20have%20shown%20strong%0Aperformance%2C%20they%20often%20fail%20to%20generalize%20in%20out-of-distribution%20%28OOD%29%0Aenvironments%20with%20unseen%20objects%20or%20variations%20in%20camera%20parameters%20%28e.g.%2C%0Afield%20of%20view%2C%20pose%2C%20or%20focal%20length%29.%20Without%20fine-tuning%2C%20these%20models%20may%0Agenerate%20unsafe%20trajectories%20that%20lead%20to%20collisions%2C%20requiring%20costly%20data%0Acollection%20and%20retraining.%20CARE%20addresses%20this%20limitation%20by%20seamlessly%0Aintegrating%20with%20any%20RGB-based%20navigation%20system%20that%20outputs%20local%0Atrajectories%2C%20dynamically%20adjusting%20them%20using%20repulsive%20force%20vectors%20derived%0Afrom%20monocular%20depth%20maps.%20We%20evaluate%20CARE%20by%20combining%20it%20with%0Astate-of-the-art%20vision-based%20navigation%20models%20across%20multiple%20robot%0Aplatforms.%20CARE%20consistently%20reduces%20collision%20rates%20%28up%20to%20100%25%29%20without%0Asacrificing%20goal-reaching%20performance%20and%20improves%20collision-free%20travel%0Adistance%20by%20up%20to%2010.7x%20in%20exploration%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03834v1&entry.124074799=Read"},
{"title": "ConText: Driving In-context Learning for Text Removal and Segmentation", "author": "Fei Zhang and Pei Zhang and Baosong Yang and Fei Huang and Yanfeng Wang and Ya Zhang", "abstract": "  This paper presents the first study on adapting the visual in-context\nlearning (V-ICL) paradigm to optical character recognition tasks, specifically\nfocusing on text removal and segmentation. Most existing V-ICL generalists\nemploy a reasoning-as-reconstruction approach: they turn to using a\nstraightforward image-label compositor as the prompt and query input, and then\nmasking the query label to generate the desired output. This direct prompt\nconfines the model to a challenging single-step reasoning process. To address\nthis, we propose a task-chaining compositor in the form of\nimage-removal-segmentation, providing an enhanced prompt that elicits reasoning\nwith enriched intermediates. Additionally, we introduce context-aware\naggregation, integrating the chained prompt pattern into the latent query\nrepresentation, thereby strengthening the model's in-context reasoning. We also\nconsider the issue of visual heterogeneity, which complicates the selection of\nhomogeneous demonstrations in text recognition. Accordingly, this is\neffectively addressed through a simple self-prompting strategy, preventing the\nmodel's in-context learnability from devolving into specialist-like,\ncontext-free inference. Collectively, these insights culminate in our ConText\nmodel, which achieves new state-of-the-art across both in- and out-of-domain\nbenchmarks. The code is available at https://github.com/Ferenas/ConText.\n", "link": "http://arxiv.org/abs/2506.03799v1", "date": "2025-06-04", "relevancy": 2.3244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConText%3A%20Driving%20In-context%20Learning%20for%20Text%20Removal%20and%20Segmentation&body=Title%3A%20ConText%3A%20Driving%20In-context%20Learning%20for%20Text%20Removal%20and%20Segmentation%0AAuthor%3A%20Fei%20Zhang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Fei%20Huang%20and%20Yanfeng%20Wang%20and%20Ya%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first%20study%20on%20adapting%20the%20visual%20in-context%0Alearning%20%28V-ICL%29%20paradigm%20to%20optical%20character%20recognition%20tasks%2C%20specifically%0Afocusing%20on%20text%20removal%20and%20segmentation.%20Most%20existing%20V-ICL%20generalists%0Aemploy%20a%20reasoning-as-reconstruction%20approach%3A%20they%20turn%20to%20using%20a%0Astraightforward%20image-label%20compositor%20as%20the%20prompt%20and%20query%20input%2C%20and%20then%0Amasking%20the%20query%20label%20to%20generate%20the%20desired%20output.%20This%20direct%20prompt%0Aconfines%20the%20model%20to%20a%20challenging%20single-step%20reasoning%20process.%20To%20address%0Athis%2C%20we%20propose%20a%20task-chaining%20compositor%20in%20the%20form%20of%0Aimage-removal-segmentation%2C%20providing%20an%20enhanced%20prompt%20that%20elicits%20reasoning%0Awith%20enriched%20intermediates.%20Additionally%2C%20we%20introduce%20context-aware%0Aaggregation%2C%20integrating%20the%20chained%20prompt%20pattern%20into%20the%20latent%20query%0Arepresentation%2C%20thereby%20strengthening%20the%20model%27s%20in-context%20reasoning.%20We%20also%0Aconsider%20the%20issue%20of%20visual%20heterogeneity%2C%20which%20complicates%20the%20selection%20of%0Ahomogeneous%20demonstrations%20in%20text%20recognition.%20Accordingly%2C%20this%20is%0Aeffectively%20addressed%20through%20a%20simple%20self-prompting%20strategy%2C%20preventing%20the%0Amodel%27s%20in-context%20learnability%20from%20devolving%20into%20specialist-like%2C%0Acontext-free%20inference.%20Collectively%2C%20these%20insights%20culminate%20in%20our%20ConText%0Amodel%2C%20which%20achieves%20new%20state-of-the-art%20across%20both%20in-%20and%20out-of-domain%0Abenchmarks.%20The%20code%20is%20available%20at%20https%3A//github.com/Ferenas/ConText.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConText%253A%2520Driving%2520In-context%2520Learning%2520for%2520Text%2520Removal%2520and%2520Segmentation%26entry.906535625%3DFei%2520Zhang%2520and%2520Pei%2520Zhang%2520and%2520Baosong%2520Yang%2520and%2520Fei%2520Huang%2520and%2520Yanfeng%2520Wang%2520and%2520Ya%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first%2520study%2520on%2520adapting%2520the%2520visual%2520in-context%250Alearning%2520%2528V-ICL%2529%2520paradigm%2520to%2520optical%2520character%2520recognition%2520tasks%252C%2520specifically%250Afocusing%2520on%2520text%2520removal%2520and%2520segmentation.%2520Most%2520existing%2520V-ICL%2520generalists%250Aemploy%2520a%2520reasoning-as-reconstruction%2520approach%253A%2520they%2520turn%2520to%2520using%2520a%250Astraightforward%2520image-label%2520compositor%2520as%2520the%2520prompt%2520and%2520query%2520input%252C%2520and%2520then%250Amasking%2520the%2520query%2520label%2520to%2520generate%2520the%2520desired%2520output.%2520This%2520direct%2520prompt%250Aconfines%2520the%2520model%2520to%2520a%2520challenging%2520single-step%2520reasoning%2520process.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520task-chaining%2520compositor%2520in%2520the%2520form%2520of%250Aimage-removal-segmentation%252C%2520providing%2520an%2520enhanced%2520prompt%2520that%2520elicits%2520reasoning%250Awith%2520enriched%2520intermediates.%2520Additionally%252C%2520we%2520introduce%2520context-aware%250Aaggregation%252C%2520integrating%2520the%2520chained%2520prompt%2520pattern%2520into%2520the%2520latent%2520query%250Arepresentation%252C%2520thereby%2520strengthening%2520the%2520model%2527s%2520in-context%2520reasoning.%2520We%2520also%250Aconsider%2520the%2520issue%2520of%2520visual%2520heterogeneity%252C%2520which%2520complicates%2520the%2520selection%2520of%250Ahomogeneous%2520demonstrations%2520in%2520text%2520recognition.%2520Accordingly%252C%2520this%2520is%250Aeffectively%2520addressed%2520through%2520a%2520simple%2520self-prompting%2520strategy%252C%2520preventing%2520the%250Amodel%2527s%2520in-context%2520learnability%2520from%2520devolving%2520into%2520specialist-like%252C%250Acontext-free%2520inference.%2520Collectively%252C%2520these%2520insights%2520culminate%2520in%2520our%2520ConText%250Amodel%252C%2520which%2520achieves%2520new%2520state-of-the-art%2520across%2520both%2520in-%2520and%2520out-of-domain%250Abenchmarks.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Ferenas/ConText.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConText%3A%20Driving%20In-context%20Learning%20for%20Text%20Removal%20and%20Segmentation&entry.906535625=Fei%20Zhang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Fei%20Huang%20and%20Yanfeng%20Wang%20and%20Ya%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20the%20first%20study%20on%20adapting%20the%20visual%20in-context%0Alearning%20%28V-ICL%29%20paradigm%20to%20optical%20character%20recognition%20tasks%2C%20specifically%0Afocusing%20on%20text%20removal%20and%20segmentation.%20Most%20existing%20V-ICL%20generalists%0Aemploy%20a%20reasoning-as-reconstruction%20approach%3A%20they%20turn%20to%20using%20a%0Astraightforward%20image-label%20compositor%20as%20the%20prompt%20and%20query%20input%2C%20and%20then%0Amasking%20the%20query%20label%20to%20generate%20the%20desired%20output.%20This%20direct%20prompt%0Aconfines%20the%20model%20to%20a%20challenging%20single-step%20reasoning%20process.%20To%20address%0Athis%2C%20we%20propose%20a%20task-chaining%20compositor%20in%20the%20form%20of%0Aimage-removal-segmentation%2C%20providing%20an%20enhanced%20prompt%20that%20elicits%20reasoning%0Awith%20enriched%20intermediates.%20Additionally%2C%20we%20introduce%20context-aware%0Aaggregation%2C%20integrating%20the%20chained%20prompt%20pattern%20into%20the%20latent%20query%0Arepresentation%2C%20thereby%20strengthening%20the%20model%27s%20in-context%20reasoning.%20We%20also%0Aconsider%20the%20issue%20of%20visual%20heterogeneity%2C%20which%20complicates%20the%20selection%20of%0Ahomogeneous%20demonstrations%20in%20text%20recognition.%20Accordingly%2C%20this%20is%0Aeffectively%20addressed%20through%20a%20simple%20self-prompting%20strategy%2C%20preventing%20the%0Amodel%27s%20in-context%20learnability%20from%20devolving%20into%20specialist-like%2C%0Acontext-free%20inference.%20Collectively%2C%20these%20insights%20culminate%20in%20our%20ConText%0Amodel%2C%20which%20achieves%20new%20state-of-the-art%20across%20both%20in-%20and%20out-of-domain%0Abenchmarks.%20The%20code%20is%20available%20at%20https%3A//github.com/Ferenas/ConText.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03799v1&entry.124074799=Read"},
{"title": "Galileo: Learning Global & Local Features of Many Remote Sensing\n  Modalities", "author": "Gabriel Tseng and Anthony Fuller and Marlena Reil and Henry Herzog and Patrick Beukema and Favyen Bastani and James R. Green and Evan Shelhamer and Hannah Kerner and David Rolnick", "abstract": "  We introduce a highly multimodal transformer to represent many remote sensing\nmodalities - multispectral optical, synthetic aperture radar, elevation,\nweather, pseudo-labels, and more - across space and time. These inputs are\nuseful for diverse remote sensing tasks, such as crop mapping and flood\ndetection. However, learning shared representations of remote sensing data is\nchallenging, given the diversity of relevant data modalities, and because\nobjects of interest vary massively in scale, from small boats (1-2 pixels and\nfast) to glaciers (thousands of pixels and slow). We present a novel\nself-supervised learning algorithm that extracts multi-scale features across a\nflexible set of input modalities through masked modeling. Our dual global and\nlocal contrastive losses differ in their targets (deep representations vs.\nshallow input projections) and masking strategies (structured vs. not). Our\nGalileo is a single generalist model that outperforms SoTA specialist models\nfor satellite images and pixel time series across eleven benchmarks and\nmultiple tasks.\n", "link": "http://arxiv.org/abs/2502.09356v3", "date": "2025-06-04", "relevancy": 2.312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6145}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Galileo%3A%20Learning%20Global%20%26%20Local%20Features%20of%20Many%20Remote%20Sensing%0A%20%20Modalities&body=Title%3A%20Galileo%3A%20Learning%20Global%20%26%20Local%20Features%20of%20Many%20Remote%20Sensing%0A%20%20Modalities%0AAuthor%3A%20Gabriel%20Tseng%20and%20Anthony%20Fuller%20and%20Marlena%20Reil%20and%20Henry%20Herzog%20and%20Patrick%20Beukema%20and%20Favyen%20Bastani%20and%20James%20R.%20Green%20and%20Evan%20Shelhamer%20and%20Hannah%20Kerner%20and%20David%20Rolnick%0AAbstract%3A%20%20%20We%20introduce%20a%20highly%20multimodal%20transformer%20to%20represent%20many%20remote%20sensing%0Amodalities%20-%20multispectral%20optical%2C%20synthetic%20aperture%20radar%2C%20elevation%2C%0Aweather%2C%20pseudo-labels%2C%20and%20more%20-%20across%20space%20and%20time.%20These%20inputs%20are%0Auseful%20for%20diverse%20remote%20sensing%20tasks%2C%20such%20as%20crop%20mapping%20and%20flood%0Adetection.%20However%2C%20learning%20shared%20representations%20of%20remote%20sensing%20data%20is%0Achallenging%2C%20given%20the%20diversity%20of%20relevant%20data%20modalities%2C%20and%20because%0Aobjects%20of%20interest%20vary%20massively%20in%20scale%2C%20from%20small%20boats%20%281-2%20pixels%20and%0Afast%29%20to%20glaciers%20%28thousands%20of%20pixels%20and%20slow%29.%20We%20present%20a%20novel%0Aself-supervised%20learning%20algorithm%20that%20extracts%20multi-scale%20features%20across%20a%0Aflexible%20set%20of%20input%20modalities%20through%20masked%20modeling.%20Our%20dual%20global%20and%0Alocal%20contrastive%20losses%20differ%20in%20their%20targets%20%28deep%20representations%20vs.%0Ashallow%20input%20projections%29%20and%20masking%20strategies%20%28structured%20vs.%20not%29.%20Our%0AGalileo%20is%20a%20single%20generalist%20model%20that%20outperforms%20SoTA%20specialist%20models%0Afor%20satellite%20images%20and%20pixel%20time%20series%20across%20eleven%20benchmarks%20and%0Amultiple%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09356v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGalileo%253A%2520Learning%2520Global%2520%2526%2520Local%2520Features%2520of%2520Many%2520Remote%2520Sensing%250A%2520%2520Modalities%26entry.906535625%3DGabriel%2520Tseng%2520and%2520Anthony%2520Fuller%2520and%2520Marlena%2520Reil%2520and%2520Henry%2520Herzog%2520and%2520Patrick%2520Beukema%2520and%2520Favyen%2520Bastani%2520and%2520James%2520R.%2520Green%2520and%2520Evan%2520Shelhamer%2520and%2520Hannah%2520Kerner%2520and%2520David%2520Rolnick%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520highly%2520multimodal%2520transformer%2520to%2520represent%2520many%2520remote%2520sensing%250Amodalities%2520-%2520multispectral%2520optical%252C%2520synthetic%2520aperture%2520radar%252C%2520elevation%252C%250Aweather%252C%2520pseudo-labels%252C%2520and%2520more%2520-%2520across%2520space%2520and%2520time.%2520These%2520inputs%2520are%250Auseful%2520for%2520diverse%2520remote%2520sensing%2520tasks%252C%2520such%2520as%2520crop%2520mapping%2520and%2520flood%250Adetection.%2520However%252C%2520learning%2520shared%2520representations%2520of%2520remote%2520sensing%2520data%2520is%250Achallenging%252C%2520given%2520the%2520diversity%2520of%2520relevant%2520data%2520modalities%252C%2520and%2520because%250Aobjects%2520of%2520interest%2520vary%2520massively%2520in%2520scale%252C%2520from%2520small%2520boats%2520%25281-2%2520pixels%2520and%250Afast%2529%2520to%2520glaciers%2520%2528thousands%2520of%2520pixels%2520and%2520slow%2529.%2520We%2520present%2520a%2520novel%250Aself-supervised%2520learning%2520algorithm%2520that%2520extracts%2520multi-scale%2520features%2520across%2520a%250Aflexible%2520set%2520of%2520input%2520modalities%2520through%2520masked%2520modeling.%2520Our%2520dual%2520global%2520and%250Alocal%2520contrastive%2520losses%2520differ%2520in%2520their%2520targets%2520%2528deep%2520representations%2520vs.%250Ashallow%2520input%2520projections%2529%2520and%2520masking%2520strategies%2520%2528structured%2520vs.%2520not%2529.%2520Our%250AGalileo%2520is%2520a%2520single%2520generalist%2520model%2520that%2520outperforms%2520SoTA%2520specialist%2520models%250Afor%2520satellite%2520images%2520and%2520pixel%2520time%2520series%2520across%2520eleven%2520benchmarks%2520and%250Amultiple%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09356v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Galileo%3A%20Learning%20Global%20%26%20Local%20Features%20of%20Many%20Remote%20Sensing%0A%20%20Modalities&entry.906535625=Gabriel%20Tseng%20and%20Anthony%20Fuller%20and%20Marlena%20Reil%20and%20Henry%20Herzog%20and%20Patrick%20Beukema%20and%20Favyen%20Bastani%20and%20James%20R.%20Green%20and%20Evan%20Shelhamer%20and%20Hannah%20Kerner%20and%20David%20Rolnick&entry.1292438233=%20%20We%20introduce%20a%20highly%20multimodal%20transformer%20to%20represent%20many%20remote%20sensing%0Amodalities%20-%20multispectral%20optical%2C%20synthetic%20aperture%20radar%2C%20elevation%2C%0Aweather%2C%20pseudo-labels%2C%20and%20more%20-%20across%20space%20and%20time.%20These%20inputs%20are%0Auseful%20for%20diverse%20remote%20sensing%20tasks%2C%20such%20as%20crop%20mapping%20and%20flood%0Adetection.%20However%2C%20learning%20shared%20representations%20of%20remote%20sensing%20data%20is%0Achallenging%2C%20given%20the%20diversity%20of%20relevant%20data%20modalities%2C%20and%20because%0Aobjects%20of%20interest%20vary%20massively%20in%20scale%2C%20from%20small%20boats%20%281-2%20pixels%20and%0Afast%29%20to%20glaciers%20%28thousands%20of%20pixels%20and%20slow%29.%20We%20present%20a%20novel%0Aself-supervised%20learning%20algorithm%20that%20extracts%20multi-scale%20features%20across%20a%0Aflexible%20set%20of%20input%20modalities%20through%20masked%20modeling.%20Our%20dual%20global%20and%0Alocal%20contrastive%20losses%20differ%20in%20their%20targets%20%28deep%20representations%20vs.%0Ashallow%20input%20projections%29%20and%20masking%20strategies%20%28structured%20vs.%20not%29.%20Our%0AGalileo%20is%20a%20single%20generalist%20model%20that%20outperforms%20SoTA%20specialist%20models%0Afor%20satellite%20images%20and%20pixel%20time%20series%20across%20eleven%20benchmarks%20and%0Amultiple%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09356v3&entry.124074799=Read"},
{"title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?", "author": "Giacomo Camposampiero and Michael Hersche and Roger Wattenhofer and Abu Sebastian and Abbas Rahimi", "abstract": "  This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its extension,\nI-RAVEN-X, which tests the ability to generalize to longer reasoning rules and\nranges of the attribute values. To assess the influence of visual uncertainties\non these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset,\nwhich otherwise assumes an oracle perception. We adopt a two-fold strategy to\nsimulate this imperfect visual perception: 1) we introduce confounding\nattributes which, being sampled at random, do not contribute to the prediction\nof the correct answer of the puzzles, and 2) we smoothen the distributions of\nthe input attributes' values. We observe a sharp decline in OpenAI's o3-mini\ntask accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% --\napproaching random chance -- on the more challenging I-RAVEN-X, which increases\ninput length and range and emulates perceptual uncertainty. This drop occurred\ndespite spending 3.4x more reasoning tokens. A similar trend is also observed\nfor DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic\nprobabilistic abductive model, ARLC, that achieves state-of-the-art\nperformances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\naccuracy reduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models.\n", "link": "http://arxiv.org/abs/2503.11207v2", "date": "2025-06-04", "relevancy": 2.3028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5812}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Reasoning%20Models%20do%20Analogical%20Reasoning%20under%20Perceptual%0A%20%20Uncertainty%3F&body=Title%3A%20Can%20Large%20Reasoning%20Models%20do%20Analogical%20Reasoning%20under%20Perceptual%0A%20%20Uncertainty%3F%0AAuthor%3A%20Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi%0AAbstract%3A%20%20%20This%20work%20presents%20a%20first%20evaluation%20of%20two%20state-of-the-art%20Large%20Reasoning%0AModels%20%28LRMs%29%2C%20OpenAI%27s%20o3-mini%20and%20DeepSeek%20R1%2C%20on%20analogical%20reasoning%2C%0Afocusing%20on%20well-established%20nonverbal%20human%20IQ%20tests%20based%20on%20Raven%27s%0Aprogressive%20matrices.%20We%20benchmark%20with%20the%20I-RAVEN%20dataset%20and%20its%20extension%2C%0AI-RAVEN-X%2C%20which%20tests%20the%20ability%20to%20generalize%20to%20longer%20reasoning%20rules%20and%0Aranges%20of%20the%20attribute%20values.%20To%20assess%20the%20influence%20of%20visual%20uncertainties%0Aon%20these%20symbolic%20analogical%20reasoning%20tests%2C%20we%20extend%20the%20I-RAVEN-X%20dataset%2C%0Awhich%20otherwise%20assumes%20an%20oracle%20perception.%20We%20adopt%20a%20two-fold%20strategy%20to%0Asimulate%20this%20imperfect%20visual%20perception%3A%201%29%20we%20introduce%20confounding%0Aattributes%20which%2C%20being%20sampled%20at%20random%2C%20do%20not%20contribute%20to%20the%20prediction%0Aof%20the%20correct%20answer%20of%20the%20puzzles%2C%20and%202%29%20we%20smoothen%20the%20distributions%20of%0Athe%20input%20attributes%27%20values.%20We%20observe%20a%20sharp%20decline%20in%20OpenAI%27s%20o3-mini%0Atask%20accuracy%2C%20dropping%20from%2086.6%25%20on%20the%20original%20I-RAVEN%20to%20just%2017.0%25%20--%0Aapproaching%20random%20chance%20--%20on%20the%20more%20challenging%20I-RAVEN-X%2C%20which%20increases%0Ainput%20length%20and%20range%20and%20emulates%20perceptual%20uncertainty.%20This%20drop%20occurred%0Adespite%20spending%203.4x%20more%20reasoning%20tokens.%20A%20similar%20trend%20is%20also%20observed%0Afor%20DeepSeek%20R1%3A%20from%2080.6%25%20to%2023.2%25.%20On%20the%20other%20hand%2C%20a%20neuro-symbolic%0Aprobabilistic%20abductive%20model%2C%20ARLC%2C%20that%20achieves%20state-of-the-art%0Aperformances%20on%20I-RAVEN%2C%20can%20robustly%20reason%20under%20all%20these%0Aout-of-distribution%20tests%2C%20maintaining%20strong%20accuracy%20with%20only%20a%20modest%0Aaccuracy%20reduction%20from%2098.6%25%20to%2088.0%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/raven-large-language-models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Reasoning%2520Models%2520do%2520Analogical%2520Reasoning%2520under%2520Perceptual%250A%2520%2520Uncertainty%253F%26entry.906535625%3DGiacomo%2520Camposampiero%2520and%2520Michael%2520Hersche%2520and%2520Roger%2520Wattenhofer%2520and%2520Abu%2520Sebastian%2520and%2520Abbas%2520Rahimi%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520first%2520evaluation%2520of%2520two%2520state-of-the-art%2520Large%2520Reasoning%250AModels%2520%2528LRMs%2529%252C%2520OpenAI%2527s%2520o3-mini%2520and%2520DeepSeek%2520R1%252C%2520on%2520analogical%2520reasoning%252C%250Afocusing%2520on%2520well-established%2520nonverbal%2520human%2520IQ%2520tests%2520based%2520on%2520Raven%2527s%250Aprogressive%2520matrices.%2520We%2520benchmark%2520with%2520the%2520I-RAVEN%2520dataset%2520and%2520its%2520extension%252C%250AI-RAVEN-X%252C%2520which%2520tests%2520the%2520ability%2520to%2520generalize%2520to%2520longer%2520reasoning%2520rules%2520and%250Aranges%2520of%2520the%2520attribute%2520values.%2520To%2520assess%2520the%2520influence%2520of%2520visual%2520uncertainties%250Aon%2520these%2520symbolic%2520analogical%2520reasoning%2520tests%252C%2520we%2520extend%2520the%2520I-RAVEN-X%2520dataset%252C%250Awhich%2520otherwise%2520assumes%2520an%2520oracle%2520perception.%2520We%2520adopt%2520a%2520two-fold%2520strategy%2520to%250Asimulate%2520this%2520imperfect%2520visual%2520perception%253A%25201%2529%2520we%2520introduce%2520confounding%250Aattributes%2520which%252C%2520being%2520sampled%2520at%2520random%252C%2520do%2520not%2520contribute%2520to%2520the%2520prediction%250Aof%2520the%2520correct%2520answer%2520of%2520the%2520puzzles%252C%2520and%25202%2529%2520we%2520smoothen%2520the%2520distributions%2520of%250Athe%2520input%2520attributes%2527%2520values.%2520We%2520observe%2520a%2520sharp%2520decline%2520in%2520OpenAI%2527s%2520o3-mini%250Atask%2520accuracy%252C%2520dropping%2520from%252086.6%2525%2520on%2520the%2520original%2520I-RAVEN%2520to%2520just%252017.0%2525%2520--%250Aapproaching%2520random%2520chance%2520--%2520on%2520the%2520more%2520challenging%2520I-RAVEN-X%252C%2520which%2520increases%250Ainput%2520length%2520and%2520range%2520and%2520emulates%2520perceptual%2520uncertainty.%2520This%2520drop%2520occurred%250Adespite%2520spending%25203.4x%2520more%2520reasoning%2520tokens.%2520A%2520similar%2520trend%2520is%2520also%2520observed%250Afor%2520DeepSeek%2520R1%253A%2520from%252080.6%2525%2520to%252023.2%2525.%2520On%2520the%2520other%2520hand%252C%2520a%2520neuro-symbolic%250Aprobabilistic%2520abductive%2520model%252C%2520ARLC%252C%2520that%2520achieves%2520state-of-the-art%250Aperformances%2520on%2520I-RAVEN%252C%2520can%2520robustly%2520reason%2520under%2520all%2520these%250Aout-of-distribution%2520tests%252C%2520maintaining%2520strong%2520accuracy%2520with%2520only%2520a%2520modest%250Aaccuracy%2520reduction%2520from%252098.6%2525%2520to%252088.0%2525.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IBM/raven-large-language-models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Reasoning%20Models%20do%20Analogical%20Reasoning%20under%20Perceptual%0A%20%20Uncertainty%3F&entry.906535625=Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi&entry.1292438233=%20%20This%20work%20presents%20a%20first%20evaluation%20of%20two%20state-of-the-art%20Large%20Reasoning%0AModels%20%28LRMs%29%2C%20OpenAI%27s%20o3-mini%20and%20DeepSeek%20R1%2C%20on%20analogical%20reasoning%2C%0Afocusing%20on%20well-established%20nonverbal%20human%20IQ%20tests%20based%20on%20Raven%27s%0Aprogressive%20matrices.%20We%20benchmark%20with%20the%20I-RAVEN%20dataset%20and%20its%20extension%2C%0AI-RAVEN-X%2C%20which%20tests%20the%20ability%20to%20generalize%20to%20longer%20reasoning%20rules%20and%0Aranges%20of%20the%20attribute%20values.%20To%20assess%20the%20influence%20of%20visual%20uncertainties%0Aon%20these%20symbolic%20analogical%20reasoning%20tests%2C%20we%20extend%20the%20I-RAVEN-X%20dataset%2C%0Awhich%20otherwise%20assumes%20an%20oracle%20perception.%20We%20adopt%20a%20two-fold%20strategy%20to%0Asimulate%20this%20imperfect%20visual%20perception%3A%201%29%20we%20introduce%20confounding%0Aattributes%20which%2C%20being%20sampled%20at%20random%2C%20do%20not%20contribute%20to%20the%20prediction%0Aof%20the%20correct%20answer%20of%20the%20puzzles%2C%20and%202%29%20we%20smoothen%20the%20distributions%20of%0Athe%20input%20attributes%27%20values.%20We%20observe%20a%20sharp%20decline%20in%20OpenAI%27s%20o3-mini%0Atask%20accuracy%2C%20dropping%20from%2086.6%25%20on%20the%20original%20I-RAVEN%20to%20just%2017.0%25%20--%0Aapproaching%20random%20chance%20--%20on%20the%20more%20challenging%20I-RAVEN-X%2C%20which%20increases%0Ainput%20length%20and%20range%20and%20emulates%20perceptual%20uncertainty.%20This%20drop%20occurred%0Adespite%20spending%203.4x%20more%20reasoning%20tokens.%20A%20similar%20trend%20is%20also%20observed%0Afor%20DeepSeek%20R1%3A%20from%2080.6%25%20to%2023.2%25.%20On%20the%20other%20hand%2C%20a%20neuro-symbolic%0Aprobabilistic%20abductive%20model%2C%20ARLC%2C%20that%20achieves%20state-of-the-art%0Aperformances%20on%20I-RAVEN%2C%20can%20robustly%20reason%20under%20all%20these%0Aout-of-distribution%20tests%2C%20maintaining%20strong%20accuracy%20with%20only%20a%20modest%0Aaccuracy%20reduction%20from%2098.6%25%20to%2088.0%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/raven-large-language-models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11207v2&entry.124074799=Read"},
{"title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector", "author": "Boyong He and Yuxiang Ji and Zhuoyue Tan and Liaoni Wu", "abstract": "  Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher.\n", "link": "http://arxiv.org/abs/2506.04211v1", "date": "2025-06-04", "relevancy": 2.2971, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6159}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.567}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Domain%20Teacher%3A%20Diffusion%20Guided%20Domain%20Adaptive%20Object%0A%20%20Detector&body=Title%3A%20Diffusion%20Domain%20Teacher%3A%20Diffusion%20Guided%20Domain%20Adaptive%20Object%0A%20%20Detector%0AAuthor%3A%20Boyong%20He%20and%20Yuxiang%20Ji%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu%0AAbstract%3A%20%20%20Object%20detectors%20often%20suffer%20a%20decrease%20in%20performance%20due%20to%20the%20large%0Adomain%20gap%20between%20the%20training%20data%20%28source%20domain%29%20and%20real-world%20data%0A%28target%20domain%29.%20Diffusion-based%20generative%20models%20have%20shown%20remarkable%0Aabilities%20in%20generating%20high-quality%20and%20diverse%20images%2C%20suggesting%20their%0Apotential%20for%20extracting%20valuable%20feature%20from%20various%20domains.%20To%20effectively%0Aleverage%20the%20cross-domain%20feature%20representation%20of%20diffusion%20models%2C%20in%20this%0Apaper%2C%20we%20train%20a%20detector%20with%20frozen-weight%20diffusion%20model%20on%20the%20source%0Adomain%2C%20then%20employ%20it%20as%20a%20teacher%20model%20to%20generate%20pseudo%20labels%20on%20the%0Aunlabeled%20target%20domain%2C%20which%20are%20used%20to%20guide%20the%20supervised%20learning%20of%20the%0Astudent%20model%20on%20the%20target%20domain.%20We%20refer%20to%20this%20approach%20as%20Diffusion%0ADomain%20Teacher%20%28DDT%29.%20By%20employing%20this%20straightforward%20yet%20potent%20framework%2C%0Awe%20significantly%20improve%20cross-domain%20object%20detection%20performance%20without%0Acompromising%20the%20inference%20speed.%20Our%20method%20achieves%20an%20average%20mAP%0Aimprovement%20of%2021.2%25%20compared%20to%20the%20baseline%20on%206%20datasets%20from%20three%20common%0Across-domain%20detection%20benchmarks%20%28Cross-Camera%2C%20Syn2Real%2C%20Real2Artistic%7D%2C%0Asurpassing%20the%20current%20state-of-the-art%20%28SOTA%29%20methods%20by%20an%20average%20of%205.7%25%0AmAP.%20Furthermore%2C%20extensive%20experiments%20demonstrate%20that%20our%20method%0Aconsistently%20brings%20improvements%20even%20in%20more%20powerful%20and%20complex%20models%2C%0Ahighlighting%20broadly%20applicable%20and%20effective%20domain%20adaptation%20capability%20of%0Aour%20DDT.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/heboyong/Diffusion-Domain-Teacher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Domain%2520Teacher%253A%2520Diffusion%2520Guided%2520Domain%2520Adaptive%2520Object%250A%2520%2520Detector%26entry.906535625%3DBoyong%2520He%2520and%2520Yuxiang%2520Ji%2520and%2520Zhuoyue%2520Tan%2520and%2520Liaoni%2520Wu%26entry.1292438233%3D%2520%2520Object%2520detectors%2520often%2520suffer%2520a%2520decrease%2520in%2520performance%2520due%2520to%2520the%2520large%250Adomain%2520gap%2520between%2520the%2520training%2520data%2520%2528source%2520domain%2529%2520and%2520real-world%2520data%250A%2528target%2520domain%2529.%2520Diffusion-based%2520generative%2520models%2520have%2520shown%2520remarkable%250Aabilities%2520in%2520generating%2520high-quality%2520and%2520diverse%2520images%252C%2520suggesting%2520their%250Apotential%2520for%2520extracting%2520valuable%2520feature%2520from%2520various%2520domains.%2520To%2520effectively%250Aleverage%2520the%2520cross-domain%2520feature%2520representation%2520of%2520diffusion%2520models%252C%2520in%2520this%250Apaper%252C%2520we%2520train%2520a%2520detector%2520with%2520frozen-weight%2520diffusion%2520model%2520on%2520the%2520source%250Adomain%252C%2520then%2520employ%2520it%2520as%2520a%2520teacher%2520model%2520to%2520generate%2520pseudo%2520labels%2520on%2520the%250Aunlabeled%2520target%2520domain%252C%2520which%2520are%2520used%2520to%2520guide%2520the%2520supervised%2520learning%2520of%2520the%250Astudent%2520model%2520on%2520the%2520target%2520domain.%2520We%2520refer%2520to%2520this%2520approach%2520as%2520Diffusion%250ADomain%2520Teacher%2520%2528DDT%2529.%2520By%2520employing%2520this%2520straightforward%2520yet%2520potent%2520framework%252C%250Awe%2520significantly%2520improve%2520cross-domain%2520object%2520detection%2520performance%2520without%250Acompromising%2520the%2520inference%2520speed.%2520Our%2520method%2520achieves%2520an%2520average%2520mAP%250Aimprovement%2520of%252021.2%2525%2520compared%2520to%2520the%2520baseline%2520on%25206%2520datasets%2520from%2520three%2520common%250Across-domain%2520detection%2520benchmarks%2520%2528Cross-Camera%252C%2520Syn2Real%252C%2520Real2Artistic%257D%252C%250Asurpassing%2520the%2520current%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520by%2520an%2520average%2520of%25205.7%2525%250AmAP.%2520Furthermore%252C%2520extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aconsistently%2520brings%2520improvements%2520even%2520in%2520more%2520powerful%2520and%2520complex%2520models%252C%250Ahighlighting%2520broadly%2520applicable%2520and%2520effective%2520domain%2520adaptation%2520capability%2520of%250Aour%2520DDT.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/heboyong/Diffusion-Domain-Teacher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Domain%20Teacher%3A%20Diffusion%20Guided%20Domain%20Adaptive%20Object%0A%20%20Detector&entry.906535625=Boyong%20He%20and%20Yuxiang%20Ji%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu&entry.1292438233=%20%20Object%20detectors%20often%20suffer%20a%20decrease%20in%20performance%20due%20to%20the%20large%0Adomain%20gap%20between%20the%20training%20data%20%28source%20domain%29%20and%20real-world%20data%0A%28target%20domain%29.%20Diffusion-based%20generative%20models%20have%20shown%20remarkable%0Aabilities%20in%20generating%20high-quality%20and%20diverse%20images%2C%20suggesting%20their%0Apotential%20for%20extracting%20valuable%20feature%20from%20various%20domains.%20To%20effectively%0Aleverage%20the%20cross-domain%20feature%20representation%20of%20diffusion%20models%2C%20in%20this%0Apaper%2C%20we%20train%20a%20detector%20with%20frozen-weight%20diffusion%20model%20on%20the%20source%0Adomain%2C%20then%20employ%20it%20as%20a%20teacher%20model%20to%20generate%20pseudo%20labels%20on%20the%0Aunlabeled%20target%20domain%2C%20which%20are%20used%20to%20guide%20the%20supervised%20learning%20of%20the%0Astudent%20model%20on%20the%20target%20domain.%20We%20refer%20to%20this%20approach%20as%20Diffusion%0ADomain%20Teacher%20%28DDT%29.%20By%20employing%20this%20straightforward%20yet%20potent%20framework%2C%0Awe%20significantly%20improve%20cross-domain%20object%20detection%20performance%20without%0Acompromising%20the%20inference%20speed.%20Our%20method%20achieves%20an%20average%20mAP%0Aimprovement%20of%2021.2%25%20compared%20to%20the%20baseline%20on%206%20datasets%20from%20three%20common%0Across-domain%20detection%20benchmarks%20%28Cross-Camera%2C%20Syn2Real%2C%20Real2Artistic%7D%2C%0Asurpassing%20the%20current%20state-of-the-art%20%28SOTA%29%20methods%20by%20an%20average%20of%205.7%25%0AmAP.%20Furthermore%2C%20extensive%20experiments%20demonstrate%20that%20our%20method%0Aconsistently%20brings%20improvements%20even%20in%20more%20powerful%20and%20complex%20models%2C%0Ahighlighting%20broadly%20applicable%20and%20effective%20domain%20adaptation%20capability%20of%0Aour%20DDT.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/heboyong/Diffusion-Domain-Teacher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04211v1&entry.124074799=Read"},
{"title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "author": "Tingle Li and Baihe Huang and Xiaobin Zhuang and Dongya Jia and Jiawei Chen and Yuping Wang and Zhuo Chen and Gopala Anumanchipalli and Yuxuan Wang", "abstract": "  Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/\n", "link": "http://arxiv.org/abs/2506.04214v1", "date": "2025-06-04", "relevancy": 2.2947, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6258}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5711}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sounding%20that%20Object%3A%20Interactive%20Object-Aware%20Image%20to%20Audio%20Generation&body=Title%3A%20Sounding%20that%20Object%3A%20Interactive%20Object-Aware%20Image%20to%20Audio%20Generation%0AAuthor%3A%20Tingle%20Li%20and%20Baihe%20Huang%20and%20Xiaobin%20Zhuang%20and%20Dongya%20Jia%20and%20Jiawei%20Chen%20and%20Yuping%20Wang%20and%20Zhuo%20Chen%20and%20Gopala%20Anumanchipalli%20and%20Yuxuan%20Wang%0AAbstract%3A%20%20%20Generating%20accurate%20sounds%20for%20complex%20audio-visual%20scenes%20is%20challenging%2C%0Aespecially%20in%20the%20presence%20of%20multiple%20objects%20and%20sound%20sources.%20In%20this%0Apaper%2C%20we%20propose%20an%20%7B%5Cem%20interactive%20object-aware%20audio%20generation%7D%20model%20that%0Agrounds%20sound%20generation%20in%20user-selected%20visual%20objects%20within%20images.%20Our%0Amethod%20integrates%20object-centric%20learning%20into%20a%20conditional%20latent%20diffusion%0Amodel%2C%20which%20learns%20to%20associate%20image%20regions%20with%20their%20corresponding%20sounds%0Athrough%20multi-modal%20attention.%20At%20test%20time%2C%20our%20model%20employs%20image%0Asegmentation%20to%20allow%20users%20to%20interactively%20generate%20sounds%20at%20the%20%7B%5Cem%0Aobject%7D%20level.%20We%20theoretically%20validate%20that%20our%20attention%20mechanism%0Afunctionally%20approximates%20test-time%20segmentation%20masks%2C%20ensuring%20the%20generated%0Aaudio%20aligns%20with%20selected%20objects.%20Quantitative%20and%20qualitative%20evaluations%0Ashow%20that%20our%20model%20outperforms%20baselines%2C%20achieving%20better%20alignment%20between%0Aobjects%20and%20their%20associated%20sounds.%20Project%20page%3A%0Ahttps%3A//tinglok.netlify.app/files/avobject/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSounding%2520that%2520Object%253A%2520Interactive%2520Object-Aware%2520Image%2520to%2520Audio%2520Generation%26entry.906535625%3DTingle%2520Li%2520and%2520Baihe%2520Huang%2520and%2520Xiaobin%2520Zhuang%2520and%2520Dongya%2520Jia%2520and%2520Jiawei%2520Chen%2520and%2520Yuping%2520Wang%2520and%2520Zhuo%2520Chen%2520and%2520Gopala%2520Anumanchipalli%2520and%2520Yuxuan%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520accurate%2520sounds%2520for%2520complex%2520audio-visual%2520scenes%2520is%2520challenging%252C%250Aespecially%2520in%2520the%2520presence%2520of%2520multiple%2520objects%2520and%2520sound%2520sources.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520%257B%255Cem%2520interactive%2520object-aware%2520audio%2520generation%257D%2520model%2520that%250Agrounds%2520sound%2520generation%2520in%2520user-selected%2520visual%2520objects%2520within%2520images.%2520Our%250Amethod%2520integrates%2520object-centric%2520learning%2520into%2520a%2520conditional%2520latent%2520diffusion%250Amodel%252C%2520which%2520learns%2520to%2520associate%2520image%2520regions%2520with%2520their%2520corresponding%2520sounds%250Athrough%2520multi-modal%2520attention.%2520At%2520test%2520time%252C%2520our%2520model%2520employs%2520image%250Asegmentation%2520to%2520allow%2520users%2520to%2520interactively%2520generate%2520sounds%2520at%2520the%2520%257B%255Cem%250Aobject%257D%2520level.%2520We%2520theoretically%2520validate%2520that%2520our%2520attention%2520mechanism%250Afunctionally%2520approximates%2520test-time%2520segmentation%2520masks%252C%2520ensuring%2520the%2520generated%250Aaudio%2520aligns%2520with%2520selected%2520objects.%2520Quantitative%2520and%2520qualitative%2520evaluations%250Ashow%2520that%2520our%2520model%2520outperforms%2520baselines%252C%2520achieving%2520better%2520alignment%2520between%250Aobjects%2520and%2520their%2520associated%2520sounds.%2520Project%2520page%253A%250Ahttps%253A//tinglok.netlify.app/files/avobject/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sounding%20that%20Object%3A%20Interactive%20Object-Aware%20Image%20to%20Audio%20Generation&entry.906535625=Tingle%20Li%20and%20Baihe%20Huang%20and%20Xiaobin%20Zhuang%20and%20Dongya%20Jia%20and%20Jiawei%20Chen%20and%20Yuping%20Wang%20and%20Zhuo%20Chen%20and%20Gopala%20Anumanchipalli%20and%20Yuxuan%20Wang&entry.1292438233=%20%20Generating%20accurate%20sounds%20for%20complex%20audio-visual%20scenes%20is%20challenging%2C%0Aespecially%20in%20the%20presence%20of%20multiple%20objects%20and%20sound%20sources.%20In%20this%0Apaper%2C%20we%20propose%20an%20%7B%5Cem%20interactive%20object-aware%20audio%20generation%7D%20model%20that%0Agrounds%20sound%20generation%20in%20user-selected%20visual%20objects%20within%20images.%20Our%0Amethod%20integrates%20object-centric%20learning%20into%20a%20conditional%20latent%20diffusion%0Amodel%2C%20which%20learns%20to%20associate%20image%20regions%20with%20their%20corresponding%20sounds%0Athrough%20multi-modal%20attention.%20At%20test%20time%2C%20our%20model%20employs%20image%0Asegmentation%20to%20allow%20users%20to%20interactively%20generate%20sounds%20at%20the%20%7B%5Cem%0Aobject%7D%20level.%20We%20theoretically%20validate%20that%20our%20attention%20mechanism%0Afunctionally%20approximates%20test-time%20segmentation%20masks%2C%20ensuring%20the%20generated%0Aaudio%20aligns%20with%20selected%20objects.%20Quantitative%20and%20qualitative%20evaluations%0Ashow%20that%20our%20model%20outperforms%20baselines%2C%20achieving%20better%20alignment%20between%0Aobjects%20and%20their%20associated%20sounds.%20Project%20page%3A%0Ahttps%3A//tinglok.netlify.app/files/avobject/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04214v1&entry.124074799=Read"},
{"title": "Learning from Noise: Enhancing DNNs for Event-Based Vision through\n  Controlled Noise Injection", "author": "Marcin Kowalczyk and Kamil Jeziorek and Tomasz Kryjak", "abstract": "  Event-based sensors offer significant advantages over traditional frame-based\ncameras, especially in scenarios involving rapid motion or challenging lighting\nconditions. However, event data frequently suffers from considerable noise,\nnegatively impacting the performance and robustness of deep learning models.\nTraditionally, this problem has been addressed by applying filtering algorithms\nto the event stream, but this may also remove some of relevant data. In this\npaper, we propose a novel noise-injection training methodology designed to\nenhance the neural networks robustness against varying levels of event noise.\nOur approach introduces controlled noise directly into the training data,\nenabling models to learn noise-resilient representations. We have conducted\nextensive evaluations of the proposed method using multiple benchmark datasets\n(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,\nincluding Convolutional Neural Networks, Vision Transformers, Spiking Neural\nNetworks, and Graph Convolutional Networks. Experimental results show that our\nnoise-injection training strategy achieves stable performance over a range of\nnoise intensities, consistently outperforms event-filtering techniques, and\nachieves the highest average classification accuracy, making it a viable\nalternative to traditional event-data filtering methods in an object\nclassification system. Code: https://github.com/vision-agh/DVS_Filtering\n", "link": "http://arxiv.org/abs/2506.03918v1", "date": "2025-06-04", "relevancy": 2.2886, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5867}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5716}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Noise%3A%20Enhancing%20DNNs%20for%20Event-Based%20Vision%20through%0A%20%20Controlled%20Noise%20Injection&body=Title%3A%20Learning%20from%20Noise%3A%20Enhancing%20DNNs%20for%20Event-Based%20Vision%20through%0A%20%20Controlled%20Noise%20Injection%0AAuthor%3A%20Marcin%20Kowalczyk%20and%20Kamil%20Jeziorek%20and%20Tomasz%20Kryjak%0AAbstract%3A%20%20%20Event-based%20sensors%20offer%20significant%20advantages%20over%20traditional%20frame-based%0Acameras%2C%20especially%20in%20scenarios%20involving%20rapid%20motion%20or%20challenging%20lighting%0Aconditions.%20However%2C%20event%20data%20frequently%20suffers%20from%20considerable%20noise%2C%0Anegatively%20impacting%20the%20performance%20and%20robustness%20of%20deep%20learning%20models.%0ATraditionally%2C%20this%20problem%20has%20been%20addressed%20by%20applying%20filtering%20algorithms%0Ato%20the%20event%20stream%2C%20but%20this%20may%20also%20remove%20some%20of%20relevant%20data.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20noise-injection%20training%20methodology%20designed%20to%0Aenhance%20the%20neural%20networks%20robustness%20against%20varying%20levels%20of%20event%20noise.%0AOur%20approach%20introduces%20controlled%20noise%20directly%20into%20the%20training%20data%2C%0Aenabling%20models%20to%20learn%20noise-resilient%20representations.%20We%20have%20conducted%0Aextensive%20evaluations%20of%20the%20proposed%20method%20using%20multiple%20benchmark%20datasets%0A%28N-Caltech101%2C%20N-Cars%2C%20and%20Mini%20N-ImageNet%29%20and%20various%20network%20architectures%2C%0Aincluding%20Convolutional%20Neural%20Networks%2C%20Vision%20Transformers%2C%20Spiking%20Neural%0ANetworks%2C%20and%20Graph%20Convolutional%20Networks.%20Experimental%20results%20show%20that%20our%0Anoise-injection%20training%20strategy%20achieves%20stable%20performance%20over%20a%20range%20of%0Anoise%20intensities%2C%20consistently%20outperforms%20event-filtering%20techniques%2C%20and%0Aachieves%20the%20highest%20average%20classification%20accuracy%2C%20making%20it%20a%20viable%0Aalternative%20to%20traditional%20event-data%20filtering%20methods%20in%20an%20object%0Aclassification%20system.%20Code%3A%20https%3A//github.com/vision-agh/DVS_Filtering%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Noise%253A%2520Enhancing%2520DNNs%2520for%2520Event-Based%2520Vision%2520through%250A%2520%2520Controlled%2520Noise%2520Injection%26entry.906535625%3DMarcin%2520Kowalczyk%2520and%2520Kamil%2520Jeziorek%2520and%2520Tomasz%2520Kryjak%26entry.1292438233%3D%2520%2520Event-based%2520sensors%2520offer%2520significant%2520advantages%2520over%2520traditional%2520frame-based%250Acameras%252C%2520especially%2520in%2520scenarios%2520involving%2520rapid%2520motion%2520or%2520challenging%2520lighting%250Aconditions.%2520However%252C%2520event%2520data%2520frequently%2520suffers%2520from%2520considerable%2520noise%252C%250Anegatively%2520impacting%2520the%2520performance%2520and%2520robustness%2520of%2520deep%2520learning%2520models.%250ATraditionally%252C%2520this%2520problem%2520has%2520been%2520addressed%2520by%2520applying%2520filtering%2520algorithms%250Ato%2520the%2520event%2520stream%252C%2520but%2520this%2520may%2520also%2520remove%2520some%2520of%2520relevant%2520data.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520noise-injection%2520training%2520methodology%2520designed%2520to%250Aenhance%2520the%2520neural%2520networks%2520robustness%2520against%2520varying%2520levels%2520of%2520event%2520noise.%250AOur%2520approach%2520introduces%2520controlled%2520noise%2520directly%2520into%2520the%2520training%2520data%252C%250Aenabling%2520models%2520to%2520learn%2520noise-resilient%2520representations.%2520We%2520have%2520conducted%250Aextensive%2520evaluations%2520of%2520the%2520proposed%2520method%2520using%2520multiple%2520benchmark%2520datasets%250A%2528N-Caltech101%252C%2520N-Cars%252C%2520and%2520Mini%2520N-ImageNet%2529%2520and%2520various%2520network%2520architectures%252C%250Aincluding%2520Convolutional%2520Neural%2520Networks%252C%2520Vision%2520Transformers%252C%2520Spiking%2520Neural%250ANetworks%252C%2520and%2520Graph%2520Convolutional%2520Networks.%2520Experimental%2520results%2520show%2520that%2520our%250Anoise-injection%2520training%2520strategy%2520achieves%2520stable%2520performance%2520over%2520a%2520range%2520of%250Anoise%2520intensities%252C%2520consistently%2520outperforms%2520event-filtering%2520techniques%252C%2520and%250Aachieves%2520the%2520highest%2520average%2520classification%2520accuracy%252C%2520making%2520it%2520a%2520viable%250Aalternative%2520to%2520traditional%2520event-data%2520filtering%2520methods%2520in%2520an%2520object%250Aclassification%2520system.%2520Code%253A%2520https%253A//github.com/vision-agh/DVS_Filtering%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Noise%3A%20Enhancing%20DNNs%20for%20Event-Based%20Vision%20through%0A%20%20Controlled%20Noise%20Injection&entry.906535625=Marcin%20Kowalczyk%20and%20Kamil%20Jeziorek%20and%20Tomasz%20Kryjak&entry.1292438233=%20%20Event-based%20sensors%20offer%20significant%20advantages%20over%20traditional%20frame-based%0Acameras%2C%20especially%20in%20scenarios%20involving%20rapid%20motion%20or%20challenging%20lighting%0Aconditions.%20However%2C%20event%20data%20frequently%20suffers%20from%20considerable%20noise%2C%0Anegatively%20impacting%20the%20performance%20and%20robustness%20of%20deep%20learning%20models.%0ATraditionally%2C%20this%20problem%20has%20been%20addressed%20by%20applying%20filtering%20algorithms%0Ato%20the%20event%20stream%2C%20but%20this%20may%20also%20remove%20some%20of%20relevant%20data.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20noise-injection%20training%20methodology%20designed%20to%0Aenhance%20the%20neural%20networks%20robustness%20against%20varying%20levels%20of%20event%20noise.%0AOur%20approach%20introduces%20controlled%20noise%20directly%20into%20the%20training%20data%2C%0Aenabling%20models%20to%20learn%20noise-resilient%20representations.%20We%20have%20conducted%0Aextensive%20evaluations%20of%20the%20proposed%20method%20using%20multiple%20benchmark%20datasets%0A%28N-Caltech101%2C%20N-Cars%2C%20and%20Mini%20N-ImageNet%29%20and%20various%20network%20architectures%2C%0Aincluding%20Convolutional%20Neural%20Networks%2C%20Vision%20Transformers%2C%20Spiking%20Neural%0ANetworks%2C%20and%20Graph%20Convolutional%20Networks.%20Experimental%20results%20show%20that%20our%0Anoise-injection%20training%20strategy%20achieves%20stable%20performance%20over%20a%20range%20of%0Anoise%20intensities%2C%20consistently%20outperforms%20event-filtering%20techniques%2C%20and%0Aachieves%20the%20highest%20average%20classification%20accuracy%2C%20making%20it%20a%20viable%0Aalternative%20to%20traditional%20event-data%20filtering%20methods%20in%20an%20object%0Aclassification%20system.%20Code%3A%20https%3A//github.com/vision-agh/DVS_Filtering%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03918v1&entry.124074799=Read"},
{"title": "VCT: Training Consistency Models with Variational Noise Coupling", "author": "Gianluigi Silvestri and Luca Ambrogioni and Chieh-Hsin Lai and Yuhta Takida and Yuki Mitsufuji", "abstract": "  Consistency Training (CT) has recently emerged as a strong alternative to\ndiffusion models for image generation. However, non-distillation CT often\nsuffers from high variance and instability, motivating ongoing research into\nits training dynamics. We propose Variational Consistency Training (VCT), a\nflexible and effective framework compatible with various forward kernels,\nincluding those in flow matching. Its key innovation is a learned noise-data\ncoupling scheme inspired by Variational Autoencoders, where a data-dependent\nencoder models noise emission. This enables VCT to adaptively learn\nnoise-todata pairings, reducing training variance relative to the fixed,\nunsorted pairings in classical CT. Experiments on multiple image datasets\ndemonstrate significant improvements: our method surpasses baselines, achieves\nstate-of-the-art FID among non-distillation CT approaches on CIFAR-10, and\nmatches SoTA performance on ImageNet 64 x 64 with only two sampling steps. Code\nis available at https://github.com/sony/vct.\n", "link": "http://arxiv.org/abs/2502.18197v2", "date": "2025-06-04", "relevancy": 2.2834, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.601}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5558}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCT%3A%20Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling&body=Title%3A%20VCT%3A%20Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling%0AAuthor%3A%20Gianluigi%20Silvestri%20and%20Luca%20Ambrogioni%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Consistency%20Training%20%28CT%29%20has%20recently%20emerged%20as%20a%20strong%20alternative%20to%0Adiffusion%20models%20for%20image%20generation.%20However%2C%20non-distillation%20CT%20often%0Asuffers%20from%20high%20variance%20and%20instability%2C%20motivating%20ongoing%20research%20into%0Aits%20training%20dynamics.%20We%20propose%20Variational%20Consistency%20Training%20%28VCT%29%2C%20a%0Aflexible%20and%20effective%20framework%20compatible%20with%20various%20forward%20kernels%2C%0Aincluding%20those%20in%20flow%20matching.%20Its%20key%20innovation%20is%20a%20learned%20noise-data%0Acoupling%20scheme%20inspired%20by%20Variational%20Autoencoders%2C%20where%20a%20data-dependent%0Aencoder%20models%20noise%20emission.%20This%20enables%20VCT%20to%20adaptively%20learn%0Anoise-todata%20pairings%2C%20reducing%20training%20variance%20relative%20to%20the%20fixed%2C%0Aunsorted%20pairings%20in%20classical%20CT.%20Experiments%20on%20multiple%20image%20datasets%0Ademonstrate%20significant%20improvements%3A%20our%20method%20surpasses%20baselines%2C%20achieves%0Astate-of-the-art%20FID%20among%20non-distillation%20CT%20approaches%20on%20CIFAR-10%2C%20and%0Amatches%20SoTA%20performance%20on%20ImageNet%2064%20x%2064%20with%20only%20two%20sampling%20steps.%20Code%0Ais%20available%20at%20https%3A//github.com/sony/vct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCT%253A%2520Training%2520Consistency%2520Models%2520with%2520Variational%2520Noise%2520Coupling%26entry.906535625%3DGianluigi%2520Silvestri%2520and%2520Luca%2520Ambrogioni%2520and%2520Chieh-Hsin%2520Lai%2520and%2520Yuhta%2520Takida%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Consistency%2520Training%2520%2528CT%2529%2520has%2520recently%2520emerged%2520as%2520a%2520strong%2520alternative%2520to%250Adiffusion%2520models%2520for%2520image%2520generation.%2520However%252C%2520non-distillation%2520CT%2520often%250Asuffers%2520from%2520high%2520variance%2520and%2520instability%252C%2520motivating%2520ongoing%2520research%2520into%250Aits%2520training%2520dynamics.%2520We%2520propose%2520Variational%2520Consistency%2520Training%2520%2528VCT%2529%252C%2520a%250Aflexible%2520and%2520effective%2520framework%2520compatible%2520with%2520various%2520forward%2520kernels%252C%250Aincluding%2520those%2520in%2520flow%2520matching.%2520Its%2520key%2520innovation%2520is%2520a%2520learned%2520noise-data%250Acoupling%2520scheme%2520inspired%2520by%2520Variational%2520Autoencoders%252C%2520where%2520a%2520data-dependent%250Aencoder%2520models%2520noise%2520emission.%2520This%2520enables%2520VCT%2520to%2520adaptively%2520learn%250Anoise-todata%2520pairings%252C%2520reducing%2520training%2520variance%2520relative%2520to%2520the%2520fixed%252C%250Aunsorted%2520pairings%2520in%2520classical%2520CT.%2520Experiments%2520on%2520multiple%2520image%2520datasets%250Ademonstrate%2520significant%2520improvements%253A%2520our%2520method%2520surpasses%2520baselines%252C%2520achieves%250Astate-of-the-art%2520FID%2520among%2520non-distillation%2520CT%2520approaches%2520on%2520CIFAR-10%252C%2520and%250Amatches%2520SoTA%2520performance%2520on%2520ImageNet%252064%2520x%252064%2520with%2520only%2520two%2520sampling%2520steps.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/sony/vct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCT%3A%20Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling&entry.906535625=Gianluigi%20Silvestri%20and%20Luca%20Ambrogioni%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Consistency%20Training%20%28CT%29%20has%20recently%20emerged%20as%20a%20strong%20alternative%20to%0Adiffusion%20models%20for%20image%20generation.%20However%2C%20non-distillation%20CT%20often%0Asuffers%20from%20high%20variance%20and%20instability%2C%20motivating%20ongoing%20research%20into%0Aits%20training%20dynamics.%20We%20propose%20Variational%20Consistency%20Training%20%28VCT%29%2C%20a%0Aflexible%20and%20effective%20framework%20compatible%20with%20various%20forward%20kernels%2C%0Aincluding%20those%20in%20flow%20matching.%20Its%20key%20innovation%20is%20a%20learned%20noise-data%0Acoupling%20scheme%20inspired%20by%20Variational%20Autoencoders%2C%20where%20a%20data-dependent%0Aencoder%20models%20noise%20emission.%20This%20enables%20VCT%20to%20adaptively%20learn%0Anoise-todata%20pairings%2C%20reducing%20training%20variance%20relative%20to%20the%20fixed%2C%0Aunsorted%20pairings%20in%20classical%20CT.%20Experiments%20on%20multiple%20image%20datasets%0Ademonstrate%20significant%20improvements%3A%20our%20method%20surpasses%20baselines%2C%20achieves%0Astate-of-the-art%20FID%20among%20non-distillation%20CT%20approaches%20on%20CIFAR-10%2C%20and%0Amatches%20SoTA%20performance%20on%20ImageNet%2064%20x%2064%20with%20only%20two%20sampling%20steps.%20Code%0Ais%20available%20at%20https%3A//github.com/sony/vct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18197v2&entry.124074799=Read"},
{"title": "How to Use Graph Data in the Wild to Help Graph Anomaly Detection?", "author": "Yuxuan Cao and Jiarong Xu and Chen Zhao and Jiaan Wang and Carl Yang and Chunping Wang and Yang Yang", "abstract": "  In recent years, graph anomaly detection has found extensive applications in\nvarious domains such as social, financial, and communication networks. However,\nanomalies in graph-structured data present unique challenges, including label\nscarcity, ill-defined anomalies, and varying anomaly types, making supervised\nor semi-supervised methods unreliable. Researchers often adopt unsupervised\napproaches to address these challenges, assuming that anomalies deviate\nsignificantly from the normal data distribution. Yet, when the available data\nis insufficient, capturing the normal distribution accurately and\ncomprehensively becomes difficult. To overcome this limitation, we propose to\nutilize external graph data (i.e., graph data in the wild) to help anomaly\ndetection tasks. This naturally raises the question: How can we use external\ndata to help graph anomaly detection tasks? To answer this question, we propose\na framework called Wild-GAD. It is built upon a unified database, UniWildGraph,\nwhich comprises a large and diverse collection of graph data with broad domain\ncoverage, ample data volume, and a unified feature space. Further, we develop\nselection criteria based on representativity and diversity to identify the most\nsuitable external data for anomaly detection task. Extensive experiments on six\nreal-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the\nbaseline methods, our framework has an average 18% AUCROC and 32% AUCPR\nimprovement over the best-competing methods.\n", "link": "http://arxiv.org/abs/2506.04190v1", "date": "2025-06-04", "relevancy": 2.2804, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4616}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Use%20Graph%20Data%20in%20the%20Wild%20to%20Help%20Graph%20Anomaly%20Detection%3F&body=Title%3A%20How%20to%20Use%20Graph%20Data%20in%20the%20Wild%20to%20Help%20Graph%20Anomaly%20Detection%3F%0AAuthor%3A%20Yuxuan%20Cao%20and%20Jiarong%20Xu%20and%20Chen%20Zhao%20and%20Jiaan%20Wang%20and%20Carl%20Yang%20and%20Chunping%20Wang%20and%20Yang%20Yang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20graph%20anomaly%20detection%20has%20found%20extensive%20applications%20in%0Avarious%20domains%20such%20as%20social%2C%20financial%2C%20and%20communication%20networks.%20However%2C%0Aanomalies%20in%20graph-structured%20data%20present%20unique%20challenges%2C%20including%20label%0Ascarcity%2C%20ill-defined%20anomalies%2C%20and%20varying%20anomaly%20types%2C%20making%20supervised%0Aor%20semi-supervised%20methods%20unreliable.%20Researchers%20often%20adopt%20unsupervised%0Aapproaches%20to%20address%20these%20challenges%2C%20assuming%20that%20anomalies%20deviate%0Asignificantly%20from%20the%20normal%20data%20distribution.%20Yet%2C%20when%20the%20available%20data%0Ais%20insufficient%2C%20capturing%20the%20normal%20distribution%20accurately%20and%0Acomprehensively%20becomes%20difficult.%20To%20overcome%20this%20limitation%2C%20we%20propose%20to%0Autilize%20external%20graph%20data%20%28i.e.%2C%20graph%20data%20in%20the%20wild%29%20to%20help%20anomaly%0Adetection%20tasks.%20This%20naturally%20raises%20the%20question%3A%20How%20can%20we%20use%20external%0Adata%20to%20help%20graph%20anomaly%20detection%20tasks%3F%20To%20answer%20this%20question%2C%20we%20propose%0Aa%20framework%20called%20Wild-GAD.%20It%20is%20built%20upon%20a%20unified%20database%2C%20UniWildGraph%2C%0Awhich%20comprises%20a%20large%20and%20diverse%20collection%20of%20graph%20data%20with%20broad%20domain%0Acoverage%2C%20ample%20data%20volume%2C%20and%20a%20unified%20feature%20space.%20Further%2C%20we%20develop%0Aselection%20criteria%20based%20on%20representativity%20and%20diversity%20to%20identify%20the%20most%0Asuitable%20external%20data%20for%20anomaly%20detection%20task.%20Extensive%20experiments%20on%20six%0Areal-world%20datasets%20demonstrate%20the%20effectiveness%20of%20Wild-GAD.%20Compared%20to%20the%0Abaseline%20methods%2C%20our%20framework%20has%20an%20average%2018%25%20AUCROC%20and%2032%25%20AUCPR%0Aimprovement%20over%20the%20best-competing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Use%2520Graph%2520Data%2520in%2520the%2520Wild%2520to%2520Help%2520Graph%2520Anomaly%2520Detection%253F%26entry.906535625%3DYuxuan%2520Cao%2520and%2520Jiarong%2520Xu%2520and%2520Chen%2520Zhao%2520and%2520Jiaan%2520Wang%2520and%2520Carl%2520Yang%2520and%2520Chunping%2520Wang%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520graph%2520anomaly%2520detection%2520has%2520found%2520extensive%2520applications%2520in%250Avarious%2520domains%2520such%2520as%2520social%252C%2520financial%252C%2520and%2520communication%2520networks.%2520However%252C%250Aanomalies%2520in%2520graph-structured%2520data%2520present%2520unique%2520challenges%252C%2520including%2520label%250Ascarcity%252C%2520ill-defined%2520anomalies%252C%2520and%2520varying%2520anomaly%2520types%252C%2520making%2520supervised%250Aor%2520semi-supervised%2520methods%2520unreliable.%2520Researchers%2520often%2520adopt%2520unsupervised%250Aapproaches%2520to%2520address%2520these%2520challenges%252C%2520assuming%2520that%2520anomalies%2520deviate%250Asignificantly%2520from%2520the%2520normal%2520data%2520distribution.%2520Yet%252C%2520when%2520the%2520available%2520data%250Ais%2520insufficient%252C%2520capturing%2520the%2520normal%2520distribution%2520accurately%2520and%250Acomprehensively%2520becomes%2520difficult.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520to%250Autilize%2520external%2520graph%2520data%2520%2528i.e.%252C%2520graph%2520data%2520in%2520the%2520wild%2529%2520to%2520help%2520anomaly%250Adetection%2520tasks.%2520This%2520naturally%2520raises%2520the%2520question%253A%2520How%2520can%2520we%2520use%2520external%250Adata%2520to%2520help%2520graph%2520anomaly%2520detection%2520tasks%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520propose%250Aa%2520framework%2520called%2520Wild-GAD.%2520It%2520is%2520built%2520upon%2520a%2520unified%2520database%252C%2520UniWildGraph%252C%250Awhich%2520comprises%2520a%2520large%2520and%2520diverse%2520collection%2520of%2520graph%2520data%2520with%2520broad%2520domain%250Acoverage%252C%2520ample%2520data%2520volume%252C%2520and%2520a%2520unified%2520feature%2520space.%2520Further%252C%2520we%2520develop%250Aselection%2520criteria%2520based%2520on%2520representativity%2520and%2520diversity%2520to%2520identify%2520the%2520most%250Asuitable%2520external%2520data%2520for%2520anomaly%2520detection%2520task.%2520Extensive%2520experiments%2520on%2520six%250Areal-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520Wild-GAD.%2520Compared%2520to%2520the%250Abaseline%2520methods%252C%2520our%2520framework%2520has%2520an%2520average%252018%2525%2520AUCROC%2520and%252032%2525%2520AUCPR%250Aimprovement%2520over%2520the%2520best-competing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Use%20Graph%20Data%20in%20the%20Wild%20to%20Help%20Graph%20Anomaly%20Detection%3F&entry.906535625=Yuxuan%20Cao%20and%20Jiarong%20Xu%20and%20Chen%20Zhao%20and%20Jiaan%20Wang%20and%20Carl%20Yang%20and%20Chunping%20Wang%20and%20Yang%20Yang&entry.1292438233=%20%20In%20recent%20years%2C%20graph%20anomaly%20detection%20has%20found%20extensive%20applications%20in%0Avarious%20domains%20such%20as%20social%2C%20financial%2C%20and%20communication%20networks.%20However%2C%0Aanomalies%20in%20graph-structured%20data%20present%20unique%20challenges%2C%20including%20label%0Ascarcity%2C%20ill-defined%20anomalies%2C%20and%20varying%20anomaly%20types%2C%20making%20supervised%0Aor%20semi-supervised%20methods%20unreliable.%20Researchers%20often%20adopt%20unsupervised%0Aapproaches%20to%20address%20these%20challenges%2C%20assuming%20that%20anomalies%20deviate%0Asignificantly%20from%20the%20normal%20data%20distribution.%20Yet%2C%20when%20the%20available%20data%0Ais%20insufficient%2C%20capturing%20the%20normal%20distribution%20accurately%20and%0Acomprehensively%20becomes%20difficult.%20To%20overcome%20this%20limitation%2C%20we%20propose%20to%0Autilize%20external%20graph%20data%20%28i.e.%2C%20graph%20data%20in%20the%20wild%29%20to%20help%20anomaly%0Adetection%20tasks.%20This%20naturally%20raises%20the%20question%3A%20How%20can%20we%20use%20external%0Adata%20to%20help%20graph%20anomaly%20detection%20tasks%3F%20To%20answer%20this%20question%2C%20we%20propose%0Aa%20framework%20called%20Wild-GAD.%20It%20is%20built%20upon%20a%20unified%20database%2C%20UniWildGraph%2C%0Awhich%20comprises%20a%20large%20and%20diverse%20collection%20of%20graph%20data%20with%20broad%20domain%0Acoverage%2C%20ample%20data%20volume%2C%20and%20a%20unified%20feature%20space.%20Further%2C%20we%20develop%0Aselection%20criteria%20based%20on%20representativity%20and%20diversity%20to%20identify%20the%20most%0Asuitable%20external%20data%20for%20anomaly%20detection%20task.%20Extensive%20experiments%20on%20six%0Areal-world%20datasets%20demonstrate%20the%20effectiveness%20of%20Wild-GAD.%20Compared%20to%20the%0Abaseline%20methods%2C%20our%20framework%20has%20an%20average%2018%25%20AUCROC%20and%2032%25%20AUCPR%0Aimprovement%20over%20the%20best-competing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04190v1&entry.124074799=Read"},
{"title": "MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal\n  Models", "author": "Huanqia Cai and Yijun Yang and Winston Hu", "abstract": "  IQ testing has served as a foundational methodology for evaluating human\ncognitive capabilities, deliberately decoupling assessment from linguistic\nbackground, language proficiency, or domain-specific knowledge to isolate core\ncompetencies in abstraction and reasoning. Yet, artificial intelligence\nresearch currently lacks systematic benchmarks to quantify these critical\ncognitive capabilities in multimodal systems. To address this crucial gap, we\npropose MM-IQ, a comprehensive evaluation framework, which comprises a\nlarge-scale training set with 4,776 visual reasoning problems and 2,710\nmeticulously curated test items spanning 8 distinct reasoning paradigms.\nThrough systematic evaluation of existing open-source and proprietary\nmultimodal models, our benchmark reveals striking limitations: even\nstate-of-the-art architectures achieve only marginally superior performance to\nrandom chance (33.17% vs. 25% baseline accuracy). This substantial performance\nchasm highlights the inadequacy of current multimodal models in approximating\nfundamental human reasoning capacities, underscoring the need for\nparadigm-shifting advancements to bridge this cognitive divide. Moreover,\ninspired by the recent surge of large reasoning models, we also release a\nmultimodal reasoning model as the baseline that is trained via reinforcement\nlearning with verifiable reward functions, reaching competitive performance to\nthe state-of-the-art with a notably smaller model size.\n", "link": "http://arxiv.org/abs/2502.00698v2", "date": "2025-06-04", "relevancy": 2.2788, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-IQ%3A%20Benchmarking%20Human-Like%20Abstraction%20and%20Reasoning%20in%20Multimodal%0A%20%20Models&body=Title%3A%20MM-IQ%3A%20Benchmarking%20Human-Like%20Abstraction%20and%20Reasoning%20in%20Multimodal%0A%20%20Models%0AAuthor%3A%20Huanqia%20Cai%20and%20Yijun%20Yang%20and%20Winston%20Hu%0AAbstract%3A%20%20%20IQ%20testing%20has%20served%20as%20a%20foundational%20methodology%20for%20evaluating%20human%0Acognitive%20capabilities%2C%20deliberately%20decoupling%20assessment%20from%20linguistic%0Abackground%2C%20language%20proficiency%2C%20or%20domain-specific%20knowledge%20to%20isolate%20core%0Acompetencies%20in%20abstraction%20and%20reasoning.%20Yet%2C%20artificial%20intelligence%0Aresearch%20currently%20lacks%20systematic%20benchmarks%20to%20quantify%20these%20critical%0Acognitive%20capabilities%20in%20multimodal%20systems.%20To%20address%20this%20crucial%20gap%2C%20we%0Apropose%20MM-IQ%2C%20a%20comprehensive%20evaluation%20framework%2C%20which%20comprises%20a%0Alarge-scale%20training%20set%20with%204%2C776%20visual%20reasoning%20problems%20and%202%2C710%0Ameticulously%20curated%20test%20items%20spanning%208%20distinct%20reasoning%20paradigms.%0AThrough%20systematic%20evaluation%20of%20existing%20open-source%20and%20proprietary%0Amultimodal%20models%2C%20our%20benchmark%20reveals%20striking%20limitations%3A%20even%0Astate-of-the-art%20architectures%20achieve%20only%20marginally%20superior%20performance%20to%0Arandom%20chance%20%2833.17%25%20vs.%2025%25%20baseline%20accuracy%29.%20This%20substantial%20performance%0Achasm%20highlights%20the%20inadequacy%20of%20current%20multimodal%20models%20in%20approximating%0Afundamental%20human%20reasoning%20capacities%2C%20underscoring%20the%20need%20for%0Aparadigm-shifting%20advancements%20to%20bridge%20this%20cognitive%20divide.%20Moreover%2C%0Ainspired%20by%20the%20recent%20surge%20of%20large%20reasoning%20models%2C%20we%20also%20release%20a%0Amultimodal%20reasoning%20model%20as%20the%20baseline%20that%20is%20trained%20via%20reinforcement%0Alearning%20with%20verifiable%20reward%20functions%2C%20reaching%20competitive%20performance%20to%0Athe%20state-of-the-art%20with%20a%20notably%20smaller%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-IQ%253A%2520Benchmarking%2520Human-Like%2520Abstraction%2520and%2520Reasoning%2520in%2520Multimodal%250A%2520%2520Models%26entry.906535625%3DHuanqia%2520Cai%2520and%2520Yijun%2520Yang%2520and%2520Winston%2520Hu%26entry.1292438233%3D%2520%2520IQ%2520testing%2520has%2520served%2520as%2520a%2520foundational%2520methodology%2520for%2520evaluating%2520human%250Acognitive%2520capabilities%252C%2520deliberately%2520decoupling%2520assessment%2520from%2520linguistic%250Abackground%252C%2520language%2520proficiency%252C%2520or%2520domain-specific%2520knowledge%2520to%2520isolate%2520core%250Acompetencies%2520in%2520abstraction%2520and%2520reasoning.%2520Yet%252C%2520artificial%2520intelligence%250Aresearch%2520currently%2520lacks%2520systematic%2520benchmarks%2520to%2520quantify%2520these%2520critical%250Acognitive%2520capabilities%2520in%2520multimodal%2520systems.%2520To%2520address%2520this%2520crucial%2520gap%252C%2520we%250Apropose%2520MM-IQ%252C%2520a%2520comprehensive%2520evaluation%2520framework%252C%2520which%2520comprises%2520a%250Alarge-scale%2520training%2520set%2520with%25204%252C776%2520visual%2520reasoning%2520problems%2520and%25202%252C710%250Ameticulously%2520curated%2520test%2520items%2520spanning%25208%2520distinct%2520reasoning%2520paradigms.%250AThrough%2520systematic%2520evaluation%2520of%2520existing%2520open-source%2520and%2520proprietary%250Amultimodal%2520models%252C%2520our%2520benchmark%2520reveals%2520striking%2520limitations%253A%2520even%250Astate-of-the-art%2520architectures%2520achieve%2520only%2520marginally%2520superior%2520performance%2520to%250Arandom%2520chance%2520%252833.17%2525%2520vs.%252025%2525%2520baseline%2520accuracy%2529.%2520This%2520substantial%2520performance%250Achasm%2520highlights%2520the%2520inadequacy%2520of%2520current%2520multimodal%2520models%2520in%2520approximating%250Afundamental%2520human%2520reasoning%2520capacities%252C%2520underscoring%2520the%2520need%2520for%250Aparadigm-shifting%2520advancements%2520to%2520bridge%2520this%2520cognitive%2520divide.%2520Moreover%252C%250Ainspired%2520by%2520the%2520recent%2520surge%2520of%2520large%2520reasoning%2520models%252C%2520we%2520also%2520release%2520a%250Amultimodal%2520reasoning%2520model%2520as%2520the%2520baseline%2520that%2520is%2520trained%2520via%2520reinforcement%250Alearning%2520with%2520verifiable%2520reward%2520functions%252C%2520reaching%2520competitive%2520performance%2520to%250Athe%2520state-of-the-art%2520with%2520a%2520notably%2520smaller%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-IQ%3A%20Benchmarking%20Human-Like%20Abstraction%20and%20Reasoning%20in%20Multimodal%0A%20%20Models&entry.906535625=Huanqia%20Cai%20and%20Yijun%20Yang%20and%20Winston%20Hu&entry.1292438233=%20%20IQ%20testing%20has%20served%20as%20a%20foundational%20methodology%20for%20evaluating%20human%0Acognitive%20capabilities%2C%20deliberately%20decoupling%20assessment%20from%20linguistic%0Abackground%2C%20language%20proficiency%2C%20or%20domain-specific%20knowledge%20to%20isolate%20core%0Acompetencies%20in%20abstraction%20and%20reasoning.%20Yet%2C%20artificial%20intelligence%0Aresearch%20currently%20lacks%20systematic%20benchmarks%20to%20quantify%20these%20critical%0Acognitive%20capabilities%20in%20multimodal%20systems.%20To%20address%20this%20crucial%20gap%2C%20we%0Apropose%20MM-IQ%2C%20a%20comprehensive%20evaluation%20framework%2C%20which%20comprises%20a%0Alarge-scale%20training%20set%20with%204%2C776%20visual%20reasoning%20problems%20and%202%2C710%0Ameticulously%20curated%20test%20items%20spanning%208%20distinct%20reasoning%20paradigms.%0AThrough%20systematic%20evaluation%20of%20existing%20open-source%20and%20proprietary%0Amultimodal%20models%2C%20our%20benchmark%20reveals%20striking%20limitations%3A%20even%0Astate-of-the-art%20architectures%20achieve%20only%20marginally%20superior%20performance%20to%0Arandom%20chance%20%2833.17%25%20vs.%2025%25%20baseline%20accuracy%29.%20This%20substantial%20performance%0Achasm%20highlights%20the%20inadequacy%20of%20current%20multimodal%20models%20in%20approximating%0Afundamental%20human%20reasoning%20capacities%2C%20underscoring%20the%20need%20for%0Aparadigm-shifting%20advancements%20to%20bridge%20this%20cognitive%20divide.%20Moreover%2C%0Ainspired%20by%20the%20recent%20surge%20of%20large%20reasoning%20models%2C%20we%20also%20release%20a%0Amultimodal%20reasoning%20model%20as%20the%20baseline%20that%20is%20trained%20via%20reinforcement%0Alearning%20with%20verifiable%20reward%20functions%2C%20reaching%20competitive%20performance%20to%0Athe%20state-of-the-art%20with%20a%20notably%20smaller%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00698v2&entry.124074799=Read"},
{"title": "Learning Cascade Ranking as One Network", "author": "Yunli Wang and Zhen Zhang and Zhiqiang Wang and Zixuan Yang and Yu Li and Jian Yang and Shiyang Wen and Peng Jiang and Kun Gai", "abstract": "  Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances have introduced interaction-aware training paradigms,\nbut still struggle to 1) align training objectives with the goal of the entire\ncascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn\neffective collaboration patterns for different stages. To address these\nchallenges, we propose LCRON, which introduces a novel surrogate loss function\nderived from the lower bound probability that ground truth items are selected\nby cascade ranking, ensuring alignment with the overall objective of the\nsystem. According to the properties of the derived bound, we further design an\nauxiliary loss for each stage to drive the reduction of this bound, leading to\na more robust and effective top-k selection. LCRON enables end-to-end training\nof the entire cascade ranking system as a unified network. Experimental results\ndemonstrate that LCRON achieves significant improvement over existing methods\non public benchmarks and industrial applications, addressing key limitations in\ncascade ranking training and significantly enhancing system performance.\n", "link": "http://arxiv.org/abs/2503.09492v3", "date": "2025-06-04", "relevancy": 2.277, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4742}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Cascade%20Ranking%20as%20One%20Network&body=Title%3A%20Learning%20Cascade%20Ranking%20as%20One%20Network%0AAuthor%3A%20Yunli%20Wang%20and%20Zhen%20Zhang%20and%20Zhiqiang%20Wang%20and%20Zixuan%20Yang%20and%20Yu%20Li%20and%20Jian%20Yang%20and%20Shiyang%20Wen%20and%20Peng%20Jiang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Cascade%20Ranking%20is%20a%20prevalent%20architecture%20in%20large-scale%20top-k%20selection%0Asystems%20like%20recommendation%20and%20advertising%20platforms.%20Traditional%20training%0Amethods%20focus%20on%20single-stage%20optimization%2C%20neglecting%20interactions%20between%0Astages.%20Recent%20advances%20have%20introduced%20interaction-aware%20training%20paradigms%2C%0Abut%20still%20struggle%20to%201%29%20align%20training%20objectives%20with%20the%20goal%20of%20the%20entire%0Acascade%20ranking%20%28i.e.%2C%20end-to-end%20recall%20of%20ground-truth%20items%29%20and%202%29%20learn%0Aeffective%20collaboration%20patterns%20for%20different%20stages.%20To%20address%20these%0Achallenges%2C%20we%20propose%20LCRON%2C%20which%20introduces%20a%20novel%20surrogate%20loss%20function%0Aderived%20from%20the%20lower%20bound%20probability%20that%20ground%20truth%20items%20are%20selected%0Aby%20cascade%20ranking%2C%20ensuring%20alignment%20with%20the%20overall%20objective%20of%20the%0Asystem.%20According%20to%20the%20properties%20of%20the%20derived%20bound%2C%20we%20further%20design%20an%0Aauxiliary%20loss%20for%20each%20stage%20to%20drive%20the%20reduction%20of%20this%20bound%2C%20leading%20to%0Aa%20more%20robust%20and%20effective%20top-k%20selection.%20LCRON%20enables%20end-to-end%20training%0Aof%20the%20entire%20cascade%20ranking%20system%20as%20a%20unified%20network.%20Experimental%20results%0Ademonstrate%20that%20LCRON%20achieves%20significant%20improvement%20over%20existing%20methods%0Aon%20public%20benchmarks%20and%20industrial%20applications%2C%20addressing%20key%20limitations%20in%0Acascade%20ranking%20training%20and%20significantly%20enhancing%20system%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09492v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Cascade%2520Ranking%2520as%2520One%2520Network%26entry.906535625%3DYunli%2520Wang%2520and%2520Zhen%2520Zhang%2520and%2520Zhiqiang%2520Wang%2520and%2520Zixuan%2520Yang%2520and%2520Yu%2520Li%2520and%2520Jian%2520Yang%2520and%2520Shiyang%2520Wen%2520and%2520Peng%2520Jiang%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Cascade%2520Ranking%2520is%2520a%2520prevalent%2520architecture%2520in%2520large-scale%2520top-k%2520selection%250Asystems%2520like%2520recommendation%2520and%2520advertising%2520platforms.%2520Traditional%2520training%250Amethods%2520focus%2520on%2520single-stage%2520optimization%252C%2520neglecting%2520interactions%2520between%250Astages.%2520Recent%2520advances%2520have%2520introduced%2520interaction-aware%2520training%2520paradigms%252C%250Abut%2520still%2520struggle%2520to%25201%2529%2520align%2520training%2520objectives%2520with%2520the%2520goal%2520of%2520the%2520entire%250Acascade%2520ranking%2520%2528i.e.%252C%2520end-to-end%2520recall%2520of%2520ground-truth%2520items%2529%2520and%25202%2529%2520learn%250Aeffective%2520collaboration%2520patterns%2520for%2520different%2520stages.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520LCRON%252C%2520which%2520introduces%2520a%2520novel%2520surrogate%2520loss%2520function%250Aderived%2520from%2520the%2520lower%2520bound%2520probability%2520that%2520ground%2520truth%2520items%2520are%2520selected%250Aby%2520cascade%2520ranking%252C%2520ensuring%2520alignment%2520with%2520the%2520overall%2520objective%2520of%2520the%250Asystem.%2520According%2520to%2520the%2520properties%2520of%2520the%2520derived%2520bound%252C%2520we%2520further%2520design%2520an%250Aauxiliary%2520loss%2520for%2520each%2520stage%2520to%2520drive%2520the%2520reduction%2520of%2520this%2520bound%252C%2520leading%2520to%250Aa%2520more%2520robust%2520and%2520effective%2520top-k%2520selection.%2520LCRON%2520enables%2520end-to-end%2520training%250Aof%2520the%2520entire%2520cascade%2520ranking%2520system%2520as%2520a%2520unified%2520network.%2520Experimental%2520results%250Ademonstrate%2520that%2520LCRON%2520achieves%2520significant%2520improvement%2520over%2520existing%2520methods%250Aon%2520public%2520benchmarks%2520and%2520industrial%2520applications%252C%2520addressing%2520key%2520limitations%2520in%250Acascade%2520ranking%2520training%2520and%2520significantly%2520enhancing%2520system%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09492v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Cascade%20Ranking%20as%20One%20Network&entry.906535625=Yunli%20Wang%20and%20Zhen%20Zhang%20and%20Zhiqiang%20Wang%20and%20Zixuan%20Yang%20and%20Yu%20Li%20and%20Jian%20Yang%20and%20Shiyang%20Wen%20and%20Peng%20Jiang%20and%20Kun%20Gai&entry.1292438233=%20%20Cascade%20Ranking%20is%20a%20prevalent%20architecture%20in%20large-scale%20top-k%20selection%0Asystems%20like%20recommendation%20and%20advertising%20platforms.%20Traditional%20training%0Amethods%20focus%20on%20single-stage%20optimization%2C%20neglecting%20interactions%20between%0Astages.%20Recent%20advances%20have%20introduced%20interaction-aware%20training%20paradigms%2C%0Abut%20still%20struggle%20to%201%29%20align%20training%20objectives%20with%20the%20goal%20of%20the%20entire%0Acascade%20ranking%20%28i.e.%2C%20end-to-end%20recall%20of%20ground-truth%20items%29%20and%202%29%20learn%0Aeffective%20collaboration%20patterns%20for%20different%20stages.%20To%20address%20these%0Achallenges%2C%20we%20propose%20LCRON%2C%20which%20introduces%20a%20novel%20surrogate%20loss%20function%0Aderived%20from%20the%20lower%20bound%20probability%20that%20ground%20truth%20items%20are%20selected%0Aby%20cascade%20ranking%2C%20ensuring%20alignment%20with%20the%20overall%20objective%20of%20the%0Asystem.%20According%20to%20the%20properties%20of%20the%20derived%20bound%2C%20we%20further%20design%20an%0Aauxiliary%20loss%20for%20each%20stage%20to%20drive%20the%20reduction%20of%20this%20bound%2C%20leading%20to%0Aa%20more%20robust%20and%20effective%20top-k%20selection.%20LCRON%20enables%20end-to-end%20training%0Aof%20the%20entire%20cascade%20ranking%20system%20as%20a%20unified%20network.%20Experimental%20results%0Ademonstrate%20that%20LCRON%20achieves%20significant%20improvement%20over%20existing%20methods%0Aon%20public%20benchmarks%20and%20industrial%20applications%2C%20addressing%20key%20limitations%20in%0Acascade%20ranking%20training%20and%20significantly%20enhancing%20system%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09492v3&entry.124074799=Read"},
{"title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning", "author": "Shuang Chen and Yue Guo and Zhaochen Su and Yafu Li and Yulun Wu and Jiacheng Chen and Jiayu Chen and Weijie Wang and Xiaoye Qu and Yu Cheng", "abstract": "  Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.\n", "link": "http://arxiv.org/abs/2506.04207v1", "date": "2025-06-04", "relevancy": 2.2566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multimodal%20Reasoning%3A%20From%20Optimized%20Cold%20Start%20to%20Staged%0A%20%20Reinforcement%20Learning&body=Title%3A%20Advancing%20Multimodal%20Reasoning%3A%20From%20Optimized%20Cold%20Start%20to%20Staged%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Shuang%20Chen%20and%20Yue%20Guo%20and%20Zhaochen%20Su%20and%20Yafu%20Li%20and%20Yulun%20Wu%20and%20Jiacheng%20Chen%20and%20Jiayu%20Chen%20and%20Weijie%20Wang%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Inspired%20by%20the%20remarkable%20reasoning%20capabilities%20of%20Deepseek-R1%20in%20complex%0Atextual%20tasks%2C%20many%20works%20attempt%20to%20incentivize%20similar%20capabilities%20in%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20by%20directly%20applying%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20they%20still%20struggle%20to%20activate%20complex%20reasoning.%20In%0Athis%20paper%2C%20rather%20than%20examining%20multimodal%20RL%20in%20isolation%2C%20we%20delve%20into%0Acurrent%20training%20pipelines%20and%20identify%20three%20crucial%20phenomena%3A%201%29%20Effective%0Acold%20start%20initialization%20is%20critical%20for%20enhancing%20MLLM%20reasoning.%0AIntriguingly%2C%20we%20find%20that%20initializing%20with%20carefully%20selected%20text%20data%20alone%0Acan%20lead%20to%20performance%20surpassing%20many%20recent%20multimodal%20reasoning%20models%2C%0Aeven%20before%20multimodal%20RL.%202%29%20Standard%20GRPO%20applied%20to%20multimodal%20RL%20suffers%0Afrom%20gradient%20stagnation%2C%20which%20degrades%20training%20stability%20and%20performance.%203%29%0ASubsequent%20text-only%20RL%20training%2C%20following%20the%20multimodal%20RL%20phase%2C%20further%0Aenhances%20multimodal%20reasoning.%20This%20staged%20training%20approach%20effectively%0Abalances%20perceptual%20grounding%20and%20cognitive%20reasoning%20development.%20By%0Aincorporating%20the%20above%20insights%20and%20addressing%20multimodal%20RL%20issues%2C%20we%0Aintroduce%20ReVisual-R1%2C%20achieving%20a%20new%20state-of-the-art%20among%20open-source%207B%0AMLLMs%20on%20challenging%20benchmarks%20including%20MathVerse%2C%20MathVision%2C%20WeMath%2C%0ALogicVista%2C%20DynaMath%2C%20and%20challenging%20AIME2024%20and%20AIME2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multimodal%2520Reasoning%253A%2520From%2520Optimized%2520Cold%2520Start%2520to%2520Staged%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DShuang%2520Chen%2520and%2520Yue%2520Guo%2520and%2520Zhaochen%2520Su%2520and%2520Yafu%2520Li%2520and%2520Yulun%2520Wu%2520and%2520Jiacheng%2520Chen%2520and%2520Jiayu%2520Chen%2520and%2520Weijie%2520Wang%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520remarkable%2520reasoning%2520capabilities%2520of%2520Deepseek-R1%2520in%2520complex%250Atextual%2520tasks%252C%2520many%2520works%2520attempt%2520to%2520incentivize%2520similar%2520capabilities%2520in%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520by%2520directly%2520applying%2520reinforcement%250Alearning%2520%2528RL%2529.%2520However%252C%2520they%2520still%2520struggle%2520to%2520activate%2520complex%2520reasoning.%2520In%250Athis%2520paper%252C%2520rather%2520than%2520examining%2520multimodal%2520RL%2520in%2520isolation%252C%2520we%2520delve%2520into%250Acurrent%2520training%2520pipelines%2520and%2520identify%2520three%2520crucial%2520phenomena%253A%25201%2529%2520Effective%250Acold%2520start%2520initialization%2520is%2520critical%2520for%2520enhancing%2520MLLM%2520reasoning.%250AIntriguingly%252C%2520we%2520find%2520that%2520initializing%2520with%2520carefully%2520selected%2520text%2520data%2520alone%250Acan%2520lead%2520to%2520performance%2520surpassing%2520many%2520recent%2520multimodal%2520reasoning%2520models%252C%250Aeven%2520before%2520multimodal%2520RL.%25202%2529%2520Standard%2520GRPO%2520applied%2520to%2520multimodal%2520RL%2520suffers%250Afrom%2520gradient%2520stagnation%252C%2520which%2520degrades%2520training%2520stability%2520and%2520performance.%25203%2529%250ASubsequent%2520text-only%2520RL%2520training%252C%2520following%2520the%2520multimodal%2520RL%2520phase%252C%2520further%250Aenhances%2520multimodal%2520reasoning.%2520This%2520staged%2520training%2520approach%2520effectively%250Abalances%2520perceptual%2520grounding%2520and%2520cognitive%2520reasoning%2520development.%2520By%250Aincorporating%2520the%2520above%2520insights%2520and%2520addressing%2520multimodal%2520RL%2520issues%252C%2520we%250Aintroduce%2520ReVisual-R1%252C%2520achieving%2520a%2520new%2520state-of-the-art%2520among%2520open-source%25207B%250AMLLMs%2520on%2520challenging%2520benchmarks%2520including%2520MathVerse%252C%2520MathVision%252C%2520WeMath%252C%250ALogicVista%252C%2520DynaMath%252C%2520and%2520challenging%2520AIME2024%2520and%2520AIME2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multimodal%20Reasoning%3A%20From%20Optimized%20Cold%20Start%20to%20Staged%0A%20%20Reinforcement%20Learning&entry.906535625=Shuang%20Chen%20and%20Yue%20Guo%20and%20Zhaochen%20Su%20and%20Yafu%20Li%20and%20Yulun%20Wu%20and%20Jiacheng%20Chen%20and%20Jiayu%20Chen%20and%20Weijie%20Wang%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng&entry.1292438233=%20%20Inspired%20by%20the%20remarkable%20reasoning%20capabilities%20of%20Deepseek-R1%20in%20complex%0Atextual%20tasks%2C%20many%20works%20attempt%20to%20incentivize%20similar%20capabilities%20in%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20by%20directly%20applying%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20they%20still%20struggle%20to%20activate%20complex%20reasoning.%20In%0Athis%20paper%2C%20rather%20than%20examining%20multimodal%20RL%20in%20isolation%2C%20we%20delve%20into%0Acurrent%20training%20pipelines%20and%20identify%20three%20crucial%20phenomena%3A%201%29%20Effective%0Acold%20start%20initialization%20is%20critical%20for%20enhancing%20MLLM%20reasoning.%0AIntriguingly%2C%20we%20find%20that%20initializing%20with%20carefully%20selected%20text%20data%20alone%0Acan%20lead%20to%20performance%20surpassing%20many%20recent%20multimodal%20reasoning%20models%2C%0Aeven%20before%20multimodal%20RL.%202%29%20Standard%20GRPO%20applied%20to%20multimodal%20RL%20suffers%0Afrom%20gradient%20stagnation%2C%20which%20degrades%20training%20stability%20and%20performance.%203%29%0ASubsequent%20text-only%20RL%20training%2C%20following%20the%20multimodal%20RL%20phase%2C%20further%0Aenhances%20multimodal%20reasoning.%20This%20staged%20training%20approach%20effectively%0Abalances%20perceptual%20grounding%20and%20cognitive%20reasoning%20development.%20By%0Aincorporating%20the%20above%20insights%20and%20addressing%20multimodal%20RL%20issues%2C%20we%0Aintroduce%20ReVisual-R1%2C%20achieving%20a%20new%20state-of-the-art%20among%20open-source%207B%0AMLLMs%20on%20challenging%20benchmarks%20including%20MathVerse%2C%20MathVision%2C%20WeMath%2C%0ALogicVista%2C%20DynaMath%2C%20and%20challenging%20AIME2024%20and%20AIME2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04207v1&entry.124074799=Read"},
{"title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity\n  Environments", "author": "Di Wen and Lei Qi and Kunyu Peng and Kailun Yang and Fei Teng and Ao Luo and Jia Fu and Yufan Chen and Ruiping Liu and Yitian Shi and M. Saquib Sarfraz and Rainer Stiefelhagen", "abstract": "  Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space.\n", "link": "http://arxiv.org/abs/2506.02845v2", "date": "2025-06-04", "relevancy": 2.2331, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Go%20Beyond%20Earth%3A%20Understanding%20Human%20Actions%20and%20Scenes%20in%20Microgravity%0A%20%20Environments&body=Title%3A%20Go%20Beyond%20Earth%3A%20Understanding%20Human%20Actions%20and%20Scenes%20in%20Microgravity%0A%20%20Environments%0AAuthor%3A%20Di%20Wen%20and%20Lei%20Qi%20and%20Kunyu%20Peng%20and%20Kailun%20Yang%20and%20Fei%20Teng%20and%20Ao%20Luo%20and%20Jia%20Fu%20and%20Yufan%20Chen%20and%20Ruiping%20Liu%20and%20Yitian%20Shi%20and%20M.%20Saquib%20Sarfraz%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Despite%20substantial%20progress%20in%20video%20understanding%2C%20most%20existing%20datasets%0Aare%20limited%20to%20Earth%27s%20gravitational%20conditions.%20However%2C%20microgravity%20alters%0Ahuman%20motion%2C%20interactions%2C%20and%20visual%20semantics%2C%20revealing%20a%20critical%20gap%20for%0Areal-world%20vision%20systems.%20This%20presents%20a%20challenge%20for%20domain-robust%20video%0Aunderstanding%20in%20safety-critical%20space%20applications.%20To%20address%20this%2C%20we%0Aintroduce%20MicroG-4M%2C%20the%20first%20benchmark%20for%20spatio-temporal%20and%20semantic%0Aunderstanding%20of%20human%20activities%20in%20microgravity.%20Constructed%20from%20real-world%0Aspace%20missions%20and%20cinematic%20simulations%2C%20the%20dataset%20includes%204%2C759%20clips%0Acovering%2050%20actions%2C%201%2C238%20context-rich%20captions%2C%20and%20over%207%2C000%0Aquestion-answer%20pairs%20on%20astronaut%20activities%20and%20scene%20understanding.%0AMicroG-4M%20supports%20three%20core%20tasks%3A%20fine-grained%20multi-label%20action%0Arecognition%2C%20temporal%20video%20captioning%2C%20and%20visual%20question%20answering%2C%20enabling%0Aa%20comprehensive%20evaluation%20of%20both%20spatial%20localization%20and%20semantic%20reasoning%0Ain%20microgravity%20contexts.%20We%20establish%20baselines%20using%20state-of-the-art%20models.%0AAll%20data%2C%20annotations%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/LEI-QI-233/HAR-in-Space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGo%2520Beyond%2520Earth%253A%2520Understanding%2520Human%2520Actions%2520and%2520Scenes%2520in%2520Microgravity%250A%2520%2520Environments%26entry.906535625%3DDi%2520Wen%2520and%2520Lei%2520Qi%2520and%2520Kunyu%2520Peng%2520and%2520Kailun%2520Yang%2520and%2520Fei%2520Teng%2520and%2520Ao%2520Luo%2520and%2520Jia%2520Fu%2520and%2520Yufan%2520Chen%2520and%2520Ruiping%2520Liu%2520and%2520Yitian%2520Shi%2520and%2520M.%2520Saquib%2520Sarfraz%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Despite%2520substantial%2520progress%2520in%2520video%2520understanding%252C%2520most%2520existing%2520datasets%250Aare%2520limited%2520to%2520Earth%2527s%2520gravitational%2520conditions.%2520However%252C%2520microgravity%2520alters%250Ahuman%2520motion%252C%2520interactions%252C%2520and%2520visual%2520semantics%252C%2520revealing%2520a%2520critical%2520gap%2520for%250Areal-world%2520vision%2520systems.%2520This%2520presents%2520a%2520challenge%2520for%2520domain-robust%2520video%250Aunderstanding%2520in%2520safety-critical%2520space%2520applications.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520MicroG-4M%252C%2520the%2520first%2520benchmark%2520for%2520spatio-temporal%2520and%2520semantic%250Aunderstanding%2520of%2520human%2520activities%2520in%2520microgravity.%2520Constructed%2520from%2520real-world%250Aspace%2520missions%2520and%2520cinematic%2520simulations%252C%2520the%2520dataset%2520includes%25204%252C759%2520clips%250Acovering%252050%2520actions%252C%25201%252C238%2520context-rich%2520captions%252C%2520and%2520over%25207%252C000%250Aquestion-answer%2520pairs%2520on%2520astronaut%2520activities%2520and%2520scene%2520understanding.%250AMicroG-4M%2520supports%2520three%2520core%2520tasks%253A%2520fine-grained%2520multi-label%2520action%250Arecognition%252C%2520temporal%2520video%2520captioning%252C%2520and%2520visual%2520question%2520answering%252C%2520enabling%250Aa%2520comprehensive%2520evaluation%2520of%2520both%2520spatial%2520localization%2520and%2520semantic%2520reasoning%250Ain%2520microgravity%2520contexts.%2520We%2520establish%2520baselines%2520using%2520state-of-the-art%2520models.%250AAll%2520data%252C%2520annotations%252C%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/LEI-QI-233/HAR-in-Space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Go%20Beyond%20Earth%3A%20Understanding%20Human%20Actions%20and%20Scenes%20in%20Microgravity%0A%20%20Environments&entry.906535625=Di%20Wen%20and%20Lei%20Qi%20and%20Kunyu%20Peng%20and%20Kailun%20Yang%20and%20Fei%20Teng%20and%20Ao%20Luo%20and%20Jia%20Fu%20and%20Yufan%20Chen%20and%20Ruiping%20Liu%20and%20Yitian%20Shi%20and%20M.%20Saquib%20Sarfraz%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Despite%20substantial%20progress%20in%20video%20understanding%2C%20most%20existing%20datasets%0Aare%20limited%20to%20Earth%27s%20gravitational%20conditions.%20However%2C%20microgravity%20alters%0Ahuman%20motion%2C%20interactions%2C%20and%20visual%20semantics%2C%20revealing%20a%20critical%20gap%20for%0Areal-world%20vision%20systems.%20This%20presents%20a%20challenge%20for%20domain-robust%20video%0Aunderstanding%20in%20safety-critical%20space%20applications.%20To%20address%20this%2C%20we%0Aintroduce%20MicroG-4M%2C%20the%20first%20benchmark%20for%20spatio-temporal%20and%20semantic%0Aunderstanding%20of%20human%20activities%20in%20microgravity.%20Constructed%20from%20real-world%0Aspace%20missions%20and%20cinematic%20simulations%2C%20the%20dataset%20includes%204%2C759%20clips%0Acovering%2050%20actions%2C%201%2C238%20context-rich%20captions%2C%20and%20over%207%2C000%0Aquestion-answer%20pairs%20on%20astronaut%20activities%20and%20scene%20understanding.%0AMicroG-4M%20supports%20three%20core%20tasks%3A%20fine-grained%20multi-label%20action%0Arecognition%2C%20temporal%20video%20captioning%2C%20and%20visual%20question%20answering%2C%20enabling%0Aa%20comprehensive%20evaluation%20of%20both%20spatial%20localization%20and%20semantic%20reasoning%0Ain%20microgravity%20contexts.%20We%20establish%20baselines%20using%20state-of-the-art%20models.%0AAll%20data%2C%20annotations%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/LEI-QI-233/HAR-in-Space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02845v2&entry.124074799=Read"},
{"title": "Person Re-Identification System at Semantic Level based on Pedestrian\n  Attributes Ontology", "author": "Ngoc Q. Ly and Hieu N. M. Cao and Thi T. Nguyen", "abstract": "  Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods.\n", "link": "http://arxiv.org/abs/2506.04143v1", "date": "2025-06-04", "relevancy": 2.2275, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5759}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Person%20Re-Identification%20System%20at%20Semantic%20Level%20based%20on%20Pedestrian%0A%20%20Attributes%20Ontology&body=Title%3A%20Person%20Re-Identification%20System%20at%20Semantic%20Level%20based%20on%20Pedestrian%0A%20%20Attributes%20Ontology%0AAuthor%3A%20Ngoc%20Q.%20Ly%20and%20Hieu%20N.%20M.%20Cao%20and%20Thi%20T.%20Nguyen%0AAbstract%3A%20%20%20Person%20Re-Identification%20%28Re-ID%29%20is%20a%20very%20important%20task%20in%20video%0Asurveillance%20systems%20such%20as%20tracking%20people%2C%20finding%20people%20in%20public%20places%2C%0Aor%20analysing%20customer%20behavior%20in%20supermarkets.%20Although%20there%20have%20been%20many%0Aworks%20to%20solve%20this%20problem%2C%20there%20are%20still%20remaining%20challenges%20such%20as%0Alarge-scale%20datasets%2C%20imbalanced%20data%2C%20viewpoint%2C%20fine%20grained%20data%0A%28attributes%29%2C%20the%20Local%20Features%20are%20not%20employed%20at%20semantic%20level%20in%20online%0Astage%20of%20Re-ID%20task%2C%20furthermore%2C%20the%20imbalanced%20data%20problem%20of%20attributes%20are%0Anot%20taken%20into%20consideration.%20This%20paper%20has%20proposed%20a%20Unified%20Re-ID%20system%0Aconsisted%20of%20three%20main%20modules%20such%20as%20Pedestrian%20Attribute%20Ontology%20%28PAO%29%2C%0ALocal%20Multi-task%20DCNN%20%28Local%20MDCNN%29%2C%20Imbalance%20Data%20Solver%20%28IDS%29.%20The%20new%20main%0Apoint%20of%20our%20Re-ID%20system%20is%20the%20power%20of%20mutual%20support%20of%20PAO%2C%20Local%20MDCNN%0Aand%20IDS%20to%20exploit%20the%20inner-group%20correlations%20of%20attributes%20and%20pre-filter%0Athe%20mismatch%20candidates%20from%20Gallery%20set%20based%20on%20semantic%20information%20as%0AFashion%20Attributes%20and%20Facial%20Attributes%2C%20to%20solve%20the%20imbalanced%20data%20of%0Aattributes%20without%20adjusting%20network%20architecture%20and%20data%20augmentation.%20We%0Aexperimented%20on%20the%20well-known%20Market1501%20dataset.%20The%20experimental%20results%0Ahave%20shown%20the%20effectiveness%20of%20our%20Re-ID%20system%20and%20it%20could%20achieve%20the%0Ahigher%20performance%20on%20Market1501%20dataset%20in%20comparison%20to%20some%20state-of-the-art%0ARe-ID%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerson%2520Re-Identification%2520System%2520at%2520Semantic%2520Level%2520based%2520on%2520Pedestrian%250A%2520%2520Attributes%2520Ontology%26entry.906535625%3DNgoc%2520Q.%2520Ly%2520and%2520Hieu%2520N.%2520M.%2520Cao%2520and%2520Thi%2520T.%2520Nguyen%26entry.1292438233%3D%2520%2520Person%2520Re-Identification%2520%2528Re-ID%2529%2520is%2520a%2520very%2520important%2520task%2520in%2520video%250Asurveillance%2520systems%2520such%2520as%2520tracking%2520people%252C%2520finding%2520people%2520in%2520public%2520places%252C%250Aor%2520analysing%2520customer%2520behavior%2520in%2520supermarkets.%2520Although%2520there%2520have%2520been%2520many%250Aworks%2520to%2520solve%2520this%2520problem%252C%2520there%2520are%2520still%2520remaining%2520challenges%2520such%2520as%250Alarge-scale%2520datasets%252C%2520imbalanced%2520data%252C%2520viewpoint%252C%2520fine%2520grained%2520data%250A%2528attributes%2529%252C%2520the%2520Local%2520Features%2520are%2520not%2520employed%2520at%2520semantic%2520level%2520in%2520online%250Astage%2520of%2520Re-ID%2520task%252C%2520furthermore%252C%2520the%2520imbalanced%2520data%2520problem%2520of%2520attributes%2520are%250Anot%2520taken%2520into%2520consideration.%2520This%2520paper%2520has%2520proposed%2520a%2520Unified%2520Re-ID%2520system%250Aconsisted%2520of%2520three%2520main%2520modules%2520such%2520as%2520Pedestrian%2520Attribute%2520Ontology%2520%2528PAO%2529%252C%250ALocal%2520Multi-task%2520DCNN%2520%2528Local%2520MDCNN%2529%252C%2520Imbalance%2520Data%2520Solver%2520%2528IDS%2529.%2520The%2520new%2520main%250Apoint%2520of%2520our%2520Re-ID%2520system%2520is%2520the%2520power%2520of%2520mutual%2520support%2520of%2520PAO%252C%2520Local%2520MDCNN%250Aand%2520IDS%2520to%2520exploit%2520the%2520inner-group%2520correlations%2520of%2520attributes%2520and%2520pre-filter%250Athe%2520mismatch%2520candidates%2520from%2520Gallery%2520set%2520based%2520on%2520semantic%2520information%2520as%250AFashion%2520Attributes%2520and%2520Facial%2520Attributes%252C%2520to%2520solve%2520the%2520imbalanced%2520data%2520of%250Aattributes%2520without%2520adjusting%2520network%2520architecture%2520and%2520data%2520augmentation.%2520We%250Aexperimented%2520on%2520the%2520well-known%2520Market1501%2520dataset.%2520The%2520experimental%2520results%250Ahave%2520shown%2520the%2520effectiveness%2520of%2520our%2520Re-ID%2520system%2520and%2520it%2520could%2520achieve%2520the%250Ahigher%2520performance%2520on%2520Market1501%2520dataset%2520in%2520comparison%2520to%2520some%2520state-of-the-art%250ARe-ID%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Person%20Re-Identification%20System%20at%20Semantic%20Level%20based%20on%20Pedestrian%0A%20%20Attributes%20Ontology&entry.906535625=Ngoc%20Q.%20Ly%20and%20Hieu%20N.%20M.%20Cao%20and%20Thi%20T.%20Nguyen&entry.1292438233=%20%20Person%20Re-Identification%20%28Re-ID%29%20is%20a%20very%20important%20task%20in%20video%0Asurveillance%20systems%20such%20as%20tracking%20people%2C%20finding%20people%20in%20public%20places%2C%0Aor%20analysing%20customer%20behavior%20in%20supermarkets.%20Although%20there%20have%20been%20many%0Aworks%20to%20solve%20this%20problem%2C%20there%20are%20still%20remaining%20challenges%20such%20as%0Alarge-scale%20datasets%2C%20imbalanced%20data%2C%20viewpoint%2C%20fine%20grained%20data%0A%28attributes%29%2C%20the%20Local%20Features%20are%20not%20employed%20at%20semantic%20level%20in%20online%0Astage%20of%20Re-ID%20task%2C%20furthermore%2C%20the%20imbalanced%20data%20problem%20of%20attributes%20are%0Anot%20taken%20into%20consideration.%20This%20paper%20has%20proposed%20a%20Unified%20Re-ID%20system%0Aconsisted%20of%20three%20main%20modules%20such%20as%20Pedestrian%20Attribute%20Ontology%20%28PAO%29%2C%0ALocal%20Multi-task%20DCNN%20%28Local%20MDCNN%29%2C%20Imbalance%20Data%20Solver%20%28IDS%29.%20The%20new%20main%0Apoint%20of%20our%20Re-ID%20system%20is%20the%20power%20of%20mutual%20support%20of%20PAO%2C%20Local%20MDCNN%0Aand%20IDS%20to%20exploit%20the%20inner-group%20correlations%20of%20attributes%20and%20pre-filter%0Athe%20mismatch%20candidates%20from%20Gallery%20set%20based%20on%20semantic%20information%20as%0AFashion%20Attributes%20and%20Facial%20Attributes%2C%20to%20solve%20the%20imbalanced%20data%20of%0Aattributes%20without%20adjusting%20network%20architecture%20and%20data%20augmentation.%20We%0Aexperimented%20on%20the%20well-known%20Market1501%20dataset.%20The%20experimental%20results%0Ahave%20shown%20the%20effectiveness%20of%20our%20Re-ID%20system%20and%20it%20could%20achieve%20the%0Ahigher%20performance%20on%20Market1501%20dataset%20in%20comparison%20to%20some%20state-of-the-art%0ARe-ID%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04143v1&entry.124074799=Read"},
{"title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion\n  Models for Domain-Generalized Detection", "author": "Boyong He and Yuxiang Ji and Qianwen Ye and Zhuoyue Tan and Liaoni Wu", "abstract": "  Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\nhttps://github.com/heboyong/Generalized-Diffusion-Detector.\n", "link": "http://arxiv.org/abs/2503.02101v2", "date": "2025-06-04", "relevancy": 2.2248, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6009}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5541}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Diffusion%20Detector%3A%20Mining%20Robust%20Features%20from%20Diffusion%0A%20%20Models%20for%20Domain-Generalized%20Detection&body=Title%3A%20Generalized%20Diffusion%20Detector%3A%20Mining%20Robust%20Features%20from%20Diffusion%0A%20%20Models%20for%20Domain-Generalized%20Detection%0AAuthor%3A%20Boyong%20He%20and%20Yuxiang%20Ji%20and%20Qianwen%20Ye%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20for%20object%20detection%20aims%20to%20enhance%20detectors%27%0Aperformance%20in%20unseen%20scenarios.%20This%20task%20remains%20challenging%20due%20to%20complex%0Avariations%20in%20real-world%20applications.%20Recently%2C%20diffusion%20models%20have%0Ademonstrated%20remarkable%20capabilities%20in%20diverse%20scene%20generation%2C%20which%0Ainspires%20us%20to%20explore%20their%20potential%20for%20improving%20DG%20tasks.%20Instead%20of%0Agenerating%20images%2C%20our%20method%20extracts%20multi-step%20intermediate%20features%20during%0Athe%20diffusion%20process%20to%20obtain%20domain-invariant%20features%20for%20generalized%0Adetection.%20Furthermore%2C%20we%20propose%20an%20efficient%20knowledge%20transfer%20framework%0Athat%20enables%20detectors%20to%20inherit%20the%20generalization%20capabilities%20of%20diffusion%0Amodels%20through%20feature%20and%20object-level%20alignment%2C%20without%20increasing%20inference%0Atime.%20We%20conduct%20extensive%20experiments%20on%20six%20challenging%20DG%20benchmarks.%20The%0Aresults%20demonstrate%20that%20our%20method%20achieves%20substantial%20improvements%20of%2014.0%25%0AmAP%20over%20existing%20DG%20approaches%20across%20different%20domains%20and%20corruption%20types.%0ANotably%2C%20our%20method%20even%20outperforms%20most%20domain%20adaptation%20methods%20without%0Aaccessing%20any%20target%20domain%20data.%20Moreover%2C%20the%20diffusion-guided%20detectors%20show%0Aconsistent%20improvements%20of%2015.9%25%20mAP%20on%20average%20compared%20to%20the%20baseline.%20Our%0Awork%20aims%20to%20present%20an%20effective%20approach%20for%20domain-generalized%20detection%20and%0Aprovide%20potential%20insights%20for%20robust%20visual%20recognition%20in%20real-world%0Ascenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/heboyong/Generalized-Diffusion-Detector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Diffusion%2520Detector%253A%2520Mining%2520Robust%2520Features%2520from%2520Diffusion%250A%2520%2520Models%2520for%2520Domain-Generalized%2520Detection%26entry.906535625%3DBoyong%2520He%2520and%2520Yuxiang%2520Ji%2520and%2520Qianwen%2520Ye%2520and%2520Zhuoyue%2520Tan%2520and%2520Liaoni%2520Wu%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520%2528DG%2529%2520for%2520object%2520detection%2520aims%2520to%2520enhance%2520detectors%2527%250Aperformance%2520in%2520unseen%2520scenarios.%2520This%2520task%2520remains%2520challenging%2520due%2520to%2520complex%250Avariations%2520in%2520real-world%2520applications.%2520Recently%252C%2520diffusion%2520models%2520have%250Ademonstrated%2520remarkable%2520capabilities%2520in%2520diverse%2520scene%2520generation%252C%2520which%250Ainspires%2520us%2520to%2520explore%2520their%2520potential%2520for%2520improving%2520DG%2520tasks.%2520Instead%2520of%250Agenerating%2520images%252C%2520our%2520method%2520extracts%2520multi-step%2520intermediate%2520features%2520during%250Athe%2520diffusion%2520process%2520to%2520obtain%2520domain-invariant%2520features%2520for%2520generalized%250Adetection.%2520Furthermore%252C%2520we%2520propose%2520an%2520efficient%2520knowledge%2520transfer%2520framework%250Athat%2520enables%2520detectors%2520to%2520inherit%2520the%2520generalization%2520capabilities%2520of%2520diffusion%250Amodels%2520through%2520feature%2520and%2520object-level%2520alignment%252C%2520without%2520increasing%2520inference%250Atime.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520six%2520challenging%2520DG%2520benchmarks.%2520The%250Aresults%2520demonstrate%2520that%2520our%2520method%2520achieves%2520substantial%2520improvements%2520of%252014.0%2525%250AmAP%2520over%2520existing%2520DG%2520approaches%2520across%2520different%2520domains%2520and%2520corruption%2520types.%250ANotably%252C%2520our%2520method%2520even%2520outperforms%2520most%2520domain%2520adaptation%2520methods%2520without%250Aaccessing%2520any%2520target%2520domain%2520data.%2520Moreover%252C%2520the%2520diffusion-guided%2520detectors%2520show%250Aconsistent%2520improvements%2520of%252015.9%2525%2520mAP%2520on%2520average%2520compared%2520to%2520the%2520baseline.%2520Our%250Awork%2520aims%2520to%2520present%2520an%2520effective%2520approach%2520for%2520domain-generalized%2520detection%2520and%250Aprovide%2520potential%2520insights%2520for%2520robust%2520visual%2520recognition%2520in%2520real-world%250Ascenarios.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/heboyong/Generalized-Diffusion-Detector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Diffusion%20Detector%3A%20Mining%20Robust%20Features%20from%20Diffusion%0A%20%20Models%20for%20Domain-Generalized%20Detection&entry.906535625=Boyong%20He%20and%20Yuxiang%20Ji%20and%20Qianwen%20Ye%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20for%20object%20detection%20aims%20to%20enhance%20detectors%27%0Aperformance%20in%20unseen%20scenarios.%20This%20task%20remains%20challenging%20due%20to%20complex%0Avariations%20in%20real-world%20applications.%20Recently%2C%20diffusion%20models%20have%0Ademonstrated%20remarkable%20capabilities%20in%20diverse%20scene%20generation%2C%20which%0Ainspires%20us%20to%20explore%20their%20potential%20for%20improving%20DG%20tasks.%20Instead%20of%0Agenerating%20images%2C%20our%20method%20extracts%20multi-step%20intermediate%20features%20during%0Athe%20diffusion%20process%20to%20obtain%20domain-invariant%20features%20for%20generalized%0Adetection.%20Furthermore%2C%20we%20propose%20an%20efficient%20knowledge%20transfer%20framework%0Athat%20enables%20detectors%20to%20inherit%20the%20generalization%20capabilities%20of%20diffusion%0Amodels%20through%20feature%20and%20object-level%20alignment%2C%20without%20increasing%20inference%0Atime.%20We%20conduct%20extensive%20experiments%20on%20six%20challenging%20DG%20benchmarks.%20The%0Aresults%20demonstrate%20that%20our%20method%20achieves%20substantial%20improvements%20of%2014.0%25%0AmAP%20over%20existing%20DG%20approaches%20across%20different%20domains%20and%20corruption%20types.%0ANotably%2C%20our%20method%20even%20outperforms%20most%20domain%20adaptation%20methods%20without%0Aaccessing%20any%20target%20domain%20data.%20Moreover%2C%20the%20diffusion-guided%20detectors%20show%0Aconsistent%20improvements%20of%2015.9%25%20mAP%20on%20average%20compared%20to%20the%20baseline.%20Our%0Awork%20aims%20to%20present%20an%20effective%20approach%20for%20domain-generalized%20detection%20and%0Aprovide%20potential%20insights%20for%20robust%20visual%20recognition%20in%20real-world%0Ascenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/heboyong/Generalized-Diffusion-Detector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02101v2&entry.124074799=Read"},
{"title": "Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and\n  Multi-Agent Environments", "author": "Mikel Malag\u00f3n and Josu Ceberio and Jose A. Lozano", "abstract": "  Advances in large models, reinforcement learning, and open-endedness have\naccelerated progress toward autonomous agents that can learn and interact in\nthe real world. To achieve this, flexible tools are needed to create rich, yet\ncomputationally efficient, environments. While scalable 2D environments fail to\naddress key real-world challenges like 3D navigation and spatial reasoning,\nmore complex 3D environments are computationally expensive and lack features\nlike customizability and multi-agent support. This paper introduces Craftium, a\nhighly customizable and easy-to-use platform for building rich 3D single- and\nmulti-agent environments. We showcase environments of different complexity and\nnature: from single- and multi-agent tasks to vast worlds with many creatures\nand biomes, and customizable procedural task generators. Benchmarking shows\nthat Craftium significantly reduces the computational cost of alternatives of\nsimilar richness, achieving +2K steps per second more than Minecraft-based\nframeworks.\n", "link": "http://arxiv.org/abs/2407.03969v2", "date": "2025-06-04", "relevancy": 2.2169, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5669}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5517}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Craftium%3A%20Bridging%20Flexibility%20and%20Efficiency%20for%20Rich%203D%20Single-%20and%0A%20%20Multi-Agent%20Environments&body=Title%3A%20Craftium%3A%20Bridging%20Flexibility%20and%20Efficiency%20for%20Rich%203D%20Single-%20and%0A%20%20Multi-Agent%20Environments%0AAuthor%3A%20Mikel%20Malag%C3%B3n%20and%20Josu%20Ceberio%20and%20Jose%20A.%20Lozano%0AAbstract%3A%20%20%20Advances%20in%20large%20models%2C%20reinforcement%20learning%2C%20and%20open-endedness%20have%0Aaccelerated%20progress%20toward%20autonomous%20agents%20that%20can%20learn%20and%20interact%20in%0Athe%20real%20world.%20To%20achieve%20this%2C%20flexible%20tools%20are%20needed%20to%20create%20rich%2C%20yet%0Acomputationally%20efficient%2C%20environments.%20While%20scalable%202D%20environments%20fail%20to%0Aaddress%20key%20real-world%20challenges%20like%203D%20navigation%20and%20spatial%20reasoning%2C%0Amore%20complex%203D%20environments%20are%20computationally%20expensive%20and%20lack%20features%0Alike%20customizability%20and%20multi-agent%20support.%20This%20paper%20introduces%20Craftium%2C%20a%0Ahighly%20customizable%20and%20easy-to-use%20platform%20for%20building%20rich%203D%20single-%20and%0Amulti-agent%20environments.%20We%20showcase%20environments%20of%20different%20complexity%20and%0Anature%3A%20from%20single-%20and%20multi-agent%20tasks%20to%20vast%20worlds%20with%20many%20creatures%0Aand%20biomes%2C%20and%20customizable%20procedural%20task%20generators.%20Benchmarking%20shows%0Athat%20Craftium%20significantly%20reduces%20the%20computational%20cost%20of%20alternatives%20of%0Asimilar%20richness%2C%20achieving%20%2B2K%20steps%20per%20second%20more%20than%20Minecraft-based%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCraftium%253A%2520Bridging%2520Flexibility%2520and%2520Efficiency%2520for%2520Rich%25203D%2520Single-%2520and%250A%2520%2520Multi-Agent%2520Environments%26entry.906535625%3DMikel%2520Malag%25C3%25B3n%2520and%2520Josu%2520Ceberio%2520and%2520Jose%2520A.%2520Lozano%26entry.1292438233%3D%2520%2520Advances%2520in%2520large%2520models%252C%2520reinforcement%2520learning%252C%2520and%2520open-endedness%2520have%250Aaccelerated%2520progress%2520toward%2520autonomous%2520agents%2520that%2520can%2520learn%2520and%2520interact%2520in%250Athe%2520real%2520world.%2520To%2520achieve%2520this%252C%2520flexible%2520tools%2520are%2520needed%2520to%2520create%2520rich%252C%2520yet%250Acomputationally%2520efficient%252C%2520environments.%2520While%2520scalable%25202D%2520environments%2520fail%2520to%250Aaddress%2520key%2520real-world%2520challenges%2520like%25203D%2520navigation%2520and%2520spatial%2520reasoning%252C%250Amore%2520complex%25203D%2520environments%2520are%2520computationally%2520expensive%2520and%2520lack%2520features%250Alike%2520customizability%2520and%2520multi-agent%2520support.%2520This%2520paper%2520introduces%2520Craftium%252C%2520a%250Ahighly%2520customizable%2520and%2520easy-to-use%2520platform%2520for%2520building%2520rich%25203D%2520single-%2520and%250Amulti-agent%2520environments.%2520We%2520showcase%2520environments%2520of%2520different%2520complexity%2520and%250Anature%253A%2520from%2520single-%2520and%2520multi-agent%2520tasks%2520to%2520vast%2520worlds%2520with%2520many%2520creatures%250Aand%2520biomes%252C%2520and%2520customizable%2520procedural%2520task%2520generators.%2520Benchmarking%2520shows%250Athat%2520Craftium%2520significantly%2520reduces%2520the%2520computational%2520cost%2520of%2520alternatives%2520of%250Asimilar%2520richness%252C%2520achieving%2520%252B2K%2520steps%2520per%2520second%2520more%2520than%2520Minecraft-based%250Aframeworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Craftium%3A%20Bridging%20Flexibility%20and%20Efficiency%20for%20Rich%203D%20Single-%20and%0A%20%20Multi-Agent%20Environments&entry.906535625=Mikel%20Malag%C3%B3n%20and%20Josu%20Ceberio%20and%20Jose%20A.%20Lozano&entry.1292438233=%20%20Advances%20in%20large%20models%2C%20reinforcement%20learning%2C%20and%20open-endedness%20have%0Aaccelerated%20progress%20toward%20autonomous%20agents%20that%20can%20learn%20and%20interact%20in%0Athe%20real%20world.%20To%20achieve%20this%2C%20flexible%20tools%20are%20needed%20to%20create%20rich%2C%20yet%0Acomputationally%20efficient%2C%20environments.%20While%20scalable%202D%20environments%20fail%20to%0Aaddress%20key%20real-world%20challenges%20like%203D%20navigation%20and%20spatial%20reasoning%2C%0Amore%20complex%203D%20environments%20are%20computationally%20expensive%20and%20lack%20features%0Alike%20customizability%20and%20multi-agent%20support.%20This%20paper%20introduces%20Craftium%2C%20a%0Ahighly%20customizable%20and%20easy-to-use%20platform%20for%20building%20rich%203D%20single-%20and%0Amulti-agent%20environments.%20We%20showcase%20environments%20of%20different%20complexity%20and%0Anature%3A%20from%20single-%20and%20multi-agent%20tasks%20to%20vast%20worlds%20with%20many%20creatures%0Aand%20biomes%2C%20and%20customizable%20procedural%20task%20generators.%20Benchmarking%20shows%0Athat%20Craftium%20significantly%20reduces%20the%20computational%20cost%20of%20alternatives%20of%0Asimilar%20richness%2C%20achieving%20%2B2K%20steps%20per%20second%20more%20than%20Minecraft-based%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03969v2&entry.124074799=Read"},
{"title": "Pseudo-Simulation for Autonomous Driving", "author": "Wei Cao and Marcel Hallgarten and Tianyu Li and Daniel Dauner and Xunjiang Gu and Caojun Wang and Yakov Miron and Marco Aiello and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta", "abstract": "  Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n", "link": "http://arxiv.org/abs/2506.04218v1", "date": "2025-06-04", "relevancy": 2.2122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Simulation%20for%20Autonomous%20Driving&body=Title%3A%20Pseudo-Simulation%20for%20Autonomous%20Driving%0AAuthor%3A%20Wei%20Cao%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Daniel%20Dauner%20and%20Xunjiang%20Gu%20and%20Caojun%20Wang%20and%20Yakov%20Miron%20and%20Marco%20Aiello%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20Existing%20evaluation%20paradigms%20for%20Autonomous%20Vehicles%20%28AVs%29%20face%20critical%0Alimitations.%20Real-world%20evaluation%20is%20often%20challenging%20due%20to%20safety%20concerns%0Aand%20a%20lack%20of%20reproducibility%2C%20whereas%20closed-loop%20simulation%20can%20face%0Ainsufficient%20realism%20or%20high%20computational%20costs.%20Open-loop%20evaluation%2C%20while%0Abeing%20efficient%20and%20data-driven%2C%20relies%20on%20metrics%20that%20generally%20overlook%0Acompounding%20errors.%20In%20this%20paper%2C%20we%20propose%20pseudo-simulation%2C%20a%20novel%0Aparadigm%20that%20addresses%20these%20limitations.%20Pseudo-simulation%20operates%20on%20real%0Adatasets%2C%20similar%20to%20open-loop%20evaluation%2C%20but%20augments%20them%20with%20synthetic%0Aobservations%20generated%20prior%20to%20evaluation%20using%203D%20Gaussian%20Splatting.%20Our%20key%0Aidea%20is%20to%20approximate%20potential%20future%20states%20the%20AV%20might%20encounter%20by%0Agenerating%20a%20diverse%20set%20of%20observations%20that%20vary%20in%20position%2C%20heading%2C%20and%0Aspeed.%20Our%20method%20then%20assigns%20a%20higher%20importance%20to%20synthetic%20observations%0Athat%20best%20match%20the%20AV%27s%20likely%20behavior%20using%20a%20novel%20proximity-based%0Aweighting%20scheme.%20This%20enables%20evaluating%20error%20recovery%20and%20the%20mitigation%20of%0Acausal%20confusion%2C%20as%20in%20closed-loop%20benchmarks%2C%20without%20requiring%20sequential%0Ainteractive%20simulation.%20We%20show%20that%20pseudo-simulation%20is%20better%20correlated%0Awith%20closed-loop%20simulations%20%28R%5E2%3D0.8%29%20than%20the%20best%20existing%20open-loop%0Aapproach%20%28R%5E2%3D0.7%29.%20We%20also%20establish%20a%20public%20leaderboard%20for%20the%20community%20to%0Abenchmark%20new%20methodologies%20with%20pseudo-simulation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Simulation%2520for%2520Autonomous%2520Driving%26entry.906535625%3DWei%2520Cao%2520and%2520Marcel%2520Hallgarten%2520and%2520Tianyu%2520Li%2520and%2520Daniel%2520Dauner%2520and%2520Xunjiang%2520Gu%2520and%2520Caojun%2520Wang%2520and%2520Yakov%2520Miron%2520and%2520Marco%2520Aiello%2520and%2520Hongyang%2520Li%2520and%2520Igor%2520Gilitschenski%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520Existing%2520evaluation%2520paradigms%2520for%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520face%2520critical%250Alimitations.%2520Real-world%2520evaluation%2520is%2520often%2520challenging%2520due%2520to%2520safety%2520concerns%250Aand%2520a%2520lack%2520of%2520reproducibility%252C%2520whereas%2520closed-loop%2520simulation%2520can%2520face%250Ainsufficient%2520realism%2520or%2520high%2520computational%2520costs.%2520Open-loop%2520evaluation%252C%2520while%250Abeing%2520efficient%2520and%2520data-driven%252C%2520relies%2520on%2520metrics%2520that%2520generally%2520overlook%250Acompounding%2520errors.%2520In%2520this%2520paper%252C%2520we%2520propose%2520pseudo-simulation%252C%2520a%2520novel%250Aparadigm%2520that%2520addresses%2520these%2520limitations.%2520Pseudo-simulation%2520operates%2520on%2520real%250Adatasets%252C%2520similar%2520to%2520open-loop%2520evaluation%252C%2520but%2520augments%2520them%2520with%2520synthetic%250Aobservations%2520generated%2520prior%2520to%2520evaluation%2520using%25203D%2520Gaussian%2520Splatting.%2520Our%2520key%250Aidea%2520is%2520to%2520approximate%2520potential%2520future%2520states%2520the%2520AV%2520might%2520encounter%2520by%250Agenerating%2520a%2520diverse%2520set%2520of%2520observations%2520that%2520vary%2520in%2520position%252C%2520heading%252C%2520and%250Aspeed.%2520Our%2520method%2520then%2520assigns%2520a%2520higher%2520importance%2520to%2520synthetic%2520observations%250Athat%2520best%2520match%2520the%2520AV%2527s%2520likely%2520behavior%2520using%2520a%2520novel%2520proximity-based%250Aweighting%2520scheme.%2520This%2520enables%2520evaluating%2520error%2520recovery%2520and%2520the%2520mitigation%2520of%250Acausal%2520confusion%252C%2520as%2520in%2520closed-loop%2520benchmarks%252C%2520without%2520requiring%2520sequential%250Ainteractive%2520simulation.%2520We%2520show%2520that%2520pseudo-simulation%2520is%2520better%2520correlated%250Awith%2520closed-loop%2520simulations%2520%2528R%255E2%253D0.8%2529%2520than%2520the%2520best%2520existing%2520open-loop%250Aapproach%2520%2528R%255E2%253D0.7%2529.%2520We%2520also%2520establish%2520a%2520public%2520leaderboard%2520for%2520the%2520community%2520to%250Abenchmark%2520new%2520methodologies%2520with%2520pseudo-simulation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/autonomousvision/navsim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Simulation%20for%20Autonomous%20Driving&entry.906535625=Wei%20Cao%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Daniel%20Dauner%20and%20Xunjiang%20Gu%20and%20Caojun%20Wang%20and%20Yakov%20Miron%20and%20Marco%20Aiello%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20Existing%20evaluation%20paradigms%20for%20Autonomous%20Vehicles%20%28AVs%29%20face%20critical%0Alimitations.%20Real-world%20evaluation%20is%20often%20challenging%20due%20to%20safety%20concerns%0Aand%20a%20lack%20of%20reproducibility%2C%20whereas%20closed-loop%20simulation%20can%20face%0Ainsufficient%20realism%20or%20high%20computational%20costs.%20Open-loop%20evaluation%2C%20while%0Abeing%20efficient%20and%20data-driven%2C%20relies%20on%20metrics%20that%20generally%20overlook%0Acompounding%20errors.%20In%20this%20paper%2C%20we%20propose%20pseudo-simulation%2C%20a%20novel%0Aparadigm%20that%20addresses%20these%20limitations.%20Pseudo-simulation%20operates%20on%20real%0Adatasets%2C%20similar%20to%20open-loop%20evaluation%2C%20but%20augments%20them%20with%20synthetic%0Aobservations%20generated%20prior%20to%20evaluation%20using%203D%20Gaussian%20Splatting.%20Our%20key%0Aidea%20is%20to%20approximate%20potential%20future%20states%20the%20AV%20might%20encounter%20by%0Agenerating%20a%20diverse%20set%20of%20observations%20that%20vary%20in%20position%2C%20heading%2C%20and%0Aspeed.%20Our%20method%20then%20assigns%20a%20higher%20importance%20to%20synthetic%20observations%0Athat%20best%20match%20the%20AV%27s%20likely%20behavior%20using%20a%20novel%20proximity-based%0Aweighting%20scheme.%20This%20enables%20evaluating%20error%20recovery%20and%20the%20mitigation%20of%0Acausal%20confusion%2C%20as%20in%20closed-loop%20benchmarks%2C%20without%20requiring%20sequential%0Ainteractive%20simulation.%20We%20show%20that%20pseudo-simulation%20is%20better%20correlated%0Awith%20closed-loop%20simulations%20%28R%5E2%3D0.8%29%20than%20the%20best%20existing%20open-loop%0Aapproach%20%28R%5E2%3D0.7%29.%20We%20also%20establish%20a%20public%20leaderboard%20for%20the%20community%20to%0Abenchmark%20new%20methodologies%20with%20pseudo-simulation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04218v1&entry.124074799=Read"},
{"title": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion", "author": "Caleb Chin and Aashish Khubchandani and Harshvardhan Maskara and Kyuseong Choi and Jacob Feitelberg and Albert Gong and Manit Paul and Tathagata Sadhukhan and Anish Agarwal and Raaz Dwivedi", "abstract": "  Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.\n", "link": "http://arxiv.org/abs/2506.04166v1", "date": "2025-06-04", "relevancy": 2.2046, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.442}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%0A%20%20Neighbor-Based%20Matrix%20Completion&body=Title%3A%20N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%0A%20%20Neighbor-Based%20Matrix%20Completion%0AAuthor%3A%20Caleb%20Chin%20and%20Aashish%20Khubchandani%20and%20Harshvardhan%20Maskara%20and%20Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Albert%20Gong%20and%20Manit%20Paul%20and%20Tathagata%20Sadhukhan%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi%0AAbstract%3A%20%20%20Nearest%20neighbor%20%28NN%29%20methods%20have%20re-emerged%20as%20competitive%20tools%20for%20matrix%0Acompletion%2C%20offering%20strong%20empirical%20performance%20and%20recent%20theoretical%0Aguarantees%2C%20including%20entry-wise%20error%20bounds%2C%20confidence%20intervals%2C%20and%0Aminimax%20optimality.%20Despite%20their%20simplicity%2C%20recent%20work%20has%20shown%20that%20NN%0Aapproaches%20are%20robust%20to%20a%20range%20of%20missingness%20patterns%20and%20effective%20across%0Adiverse%20applications.%20This%20paper%20introduces%20N%24%5E2%24%2C%20a%20unified%20Python%20package%20and%0Atestbed%20that%20consolidates%20a%20broad%20class%20of%20NN-based%20methods%20through%20a%20modular%2C%0Aextensible%20interface.%20Built%20for%20both%20researchers%20and%20practitioners%2C%20N%24%5E2%24%0Asupports%20rapid%20experimentation%20and%20benchmarking.%20Using%20this%20framework%2C%20we%0Aintroduce%20a%20new%20NN%20variant%20that%20achieves%20state-of-the-art%20results%20in%20several%0Asettings.%20We%20also%20release%20a%20benchmark%20suite%20of%20real-world%20datasets%2C%20from%0Ahealthcare%20and%20recommender%20systems%20to%20causal%20inference%20and%20LLM%20evaluation%2C%0Adesigned%20to%20stress-test%20matrix%20completion%20methods%20beyond%20synthetic%20scenarios.%0AOur%20experiments%20demonstrate%20that%20while%20classical%20methods%20excel%20on%20idealized%0Adata%2C%20NN-based%20techniques%20consistently%20outperform%20them%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN%2524%255E2%2524%253A%2520A%2520Unified%2520Python%2520Package%2520and%2520Test%2520Bench%2520for%2520Nearest%250A%2520%2520Neighbor-Based%2520Matrix%2520Completion%26entry.906535625%3DCaleb%2520Chin%2520and%2520Aashish%2520Khubchandani%2520and%2520Harshvardhan%2520Maskara%2520and%2520Kyuseong%2520Choi%2520and%2520Jacob%2520Feitelberg%2520and%2520Albert%2520Gong%2520and%2520Manit%2520Paul%2520and%2520Tathagata%2520Sadhukhan%2520and%2520Anish%2520Agarwal%2520and%2520Raaz%2520Dwivedi%26entry.1292438233%3D%2520%2520Nearest%2520neighbor%2520%2528NN%2529%2520methods%2520have%2520re-emerged%2520as%2520competitive%2520tools%2520for%2520matrix%250Acompletion%252C%2520offering%2520strong%2520empirical%2520performance%2520and%2520recent%2520theoretical%250Aguarantees%252C%2520including%2520entry-wise%2520error%2520bounds%252C%2520confidence%2520intervals%252C%2520and%250Aminimax%2520optimality.%2520Despite%2520their%2520simplicity%252C%2520recent%2520work%2520has%2520shown%2520that%2520NN%250Aapproaches%2520are%2520robust%2520to%2520a%2520range%2520of%2520missingness%2520patterns%2520and%2520effective%2520across%250Adiverse%2520applications.%2520This%2520paper%2520introduces%2520N%2524%255E2%2524%252C%2520a%2520unified%2520Python%2520package%2520and%250Atestbed%2520that%2520consolidates%2520a%2520broad%2520class%2520of%2520NN-based%2520methods%2520through%2520a%2520modular%252C%250Aextensible%2520interface.%2520Built%2520for%2520both%2520researchers%2520and%2520practitioners%252C%2520N%2524%255E2%2524%250Asupports%2520rapid%2520experimentation%2520and%2520benchmarking.%2520Using%2520this%2520framework%252C%2520we%250Aintroduce%2520a%2520new%2520NN%2520variant%2520that%2520achieves%2520state-of-the-art%2520results%2520in%2520several%250Asettings.%2520We%2520also%2520release%2520a%2520benchmark%2520suite%2520of%2520real-world%2520datasets%252C%2520from%250Ahealthcare%2520and%2520recommender%2520systems%2520to%2520causal%2520inference%2520and%2520LLM%2520evaluation%252C%250Adesigned%2520to%2520stress-test%2520matrix%2520completion%2520methods%2520beyond%2520synthetic%2520scenarios.%250AOur%2520experiments%2520demonstrate%2520that%2520while%2520classical%2520methods%2520excel%2520on%2520idealized%250Adata%252C%2520NN-based%2520techniques%2520consistently%2520outperform%2520them%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N%24%5E2%24%3A%20A%20Unified%20Python%20Package%20and%20Test%20Bench%20for%20Nearest%0A%20%20Neighbor-Based%20Matrix%20Completion&entry.906535625=Caleb%20Chin%20and%20Aashish%20Khubchandani%20and%20Harshvardhan%20Maskara%20and%20Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Albert%20Gong%20and%20Manit%20Paul%20and%20Tathagata%20Sadhukhan%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi&entry.1292438233=%20%20Nearest%20neighbor%20%28NN%29%20methods%20have%20re-emerged%20as%20competitive%20tools%20for%20matrix%0Acompletion%2C%20offering%20strong%20empirical%20performance%20and%20recent%20theoretical%0Aguarantees%2C%20including%20entry-wise%20error%20bounds%2C%20confidence%20intervals%2C%20and%0Aminimax%20optimality.%20Despite%20their%20simplicity%2C%20recent%20work%20has%20shown%20that%20NN%0Aapproaches%20are%20robust%20to%20a%20range%20of%20missingness%20patterns%20and%20effective%20across%0Adiverse%20applications.%20This%20paper%20introduces%20N%24%5E2%24%2C%20a%20unified%20Python%20package%20and%0Atestbed%20that%20consolidates%20a%20broad%20class%20of%20NN-based%20methods%20through%20a%20modular%2C%0Aextensible%20interface.%20Built%20for%20both%20researchers%20and%20practitioners%2C%20N%24%5E2%24%0Asupports%20rapid%20experimentation%20and%20benchmarking.%20Using%20this%20framework%2C%20we%0Aintroduce%20a%20new%20NN%20variant%20that%20achieves%20state-of-the-art%20results%20in%20several%0Asettings.%20We%20also%20release%20a%20benchmark%20suite%20of%20real-world%20datasets%2C%20from%0Ahealthcare%20and%20recommender%20systems%20to%20causal%20inference%20and%20LLM%20evaluation%2C%0Adesigned%20to%20stress-test%20matrix%20completion%20methods%20beyond%20synthetic%20scenarios.%0AOur%20experiments%20demonstrate%20that%20while%20classical%20methods%20excel%20on%20idealized%0Adata%2C%20NN-based%20techniques%20consistently%20outperform%20them%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04166v1&entry.124074799=Read"},
{"title": "LoGU: Long-form Generation with Uncertainty Expressions", "author": "Ruihan Yang and Caiqi Zhang and Zhisong Zhang and Xinting Huang and Sen Yang and Nigel Collier and Dong Yu and Deqing Yang", "abstract": "  While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.\n", "link": "http://arxiv.org/abs/2410.14309v4", "date": "2025-06-04", "relevancy": 2.2005, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6268}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoGU%3A%20Long-form%20Generation%20with%20Uncertainty%20Expressions&body=Title%3A%20LoGU%3A%20Long-form%20Generation%20with%20Uncertainty%20Expressions%0AAuthor%3A%20Ruihan%20Yang%20and%20Caiqi%20Zhang%20and%20Zhisong%20Zhang%20and%20Xinting%20Huang%20and%20Sen%20Yang%20and%20Nigel%20Collier%20and%20Dong%20Yu%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20capabilities%2C%20they%0Astill%20struggle%20with%20generating%20factually%20incorrect%20content%20%28i.e.%2C%0Ahallucinations%29.%20A%20promising%20approach%20to%20mitigate%20this%20issue%20is%20enabling%20models%0Ato%20express%20uncertainty%20when%20unsure.%20Previous%20research%20on%20uncertainty%20modeling%0Ahas%20primarily%20focused%20on%20short-form%20QA%2C%20but%20realworld%20applications%20often%0Arequire%20much%20longer%20responses.%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20Long-form%0AGeneration%20with%20Uncertainty%28LoGU%29.%20We%20identify%20two%20key%20challenges%3A%20Uncertainty%0ASuppression%2C%20where%20models%20hesitate%20to%20express%20uncertainty%2C%20and%20Uncertainty%0AMisalignment%2C%20where%20models%20convey%20uncertainty%20inaccurately.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20refinement-based%20data%20collection%20framework%20and%20a%0Atwo-stage%20training%20pipeline.%20Our%20framework%20adopts%20a%20divide-and-conquer%0Astrategy%2C%20refining%20uncertainty%20based%20on%20atomic%20claims.%20The%20collected%20data%20are%0Athen%20used%20in%20training%20through%20supervised%20fine-tuning%20%28SFT%29%20and%20direct%0Apreference%20optimization%20%28DPO%29%20to%20enhance%20uncertainty%20expression.%20Extensive%0Aexperiments%20on%20three%20long-form%20instruction%20following%20datasets%20show%20that%20our%0Amethod%20significantly%20improves%20accuracy%2C%20reduces%20hallucinations%2C%20and%20maintains%0Athe%20comprehensiveness%20of%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14309v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoGU%253A%2520Long-form%2520Generation%2520with%2520Uncertainty%2520Expressions%26entry.906535625%3DRuihan%2520Yang%2520and%2520Caiqi%2520Zhang%2520and%2520Zhisong%2520Zhang%2520and%2520Xinting%2520Huang%2520and%2520Sen%2520Yang%2520and%2520Nigel%2520Collier%2520and%2520Dong%2520Yu%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520impressive%2520capabilities%252C%2520they%250Astill%2520struggle%2520with%2520generating%2520factually%2520incorrect%2520content%2520%2528i.e.%252C%250Ahallucinations%2529.%2520A%2520promising%2520approach%2520to%2520mitigate%2520this%2520issue%2520is%2520enabling%2520models%250Ato%2520express%2520uncertainty%2520when%2520unsure.%2520Previous%2520research%2520on%2520uncertainty%2520modeling%250Ahas%2520primarily%2520focused%2520on%2520short-form%2520QA%252C%2520but%2520realworld%2520applications%2520often%250Arequire%2520much%2520longer%2520responses.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520task%2520of%2520Long-form%250AGeneration%2520with%2520Uncertainty%2528LoGU%2529.%2520We%2520identify%2520two%2520key%2520challenges%253A%2520Uncertainty%250ASuppression%252C%2520where%2520models%2520hesitate%2520to%2520express%2520uncertainty%252C%2520and%2520Uncertainty%250AMisalignment%252C%2520where%2520models%2520convey%2520uncertainty%2520inaccurately.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520refinement-based%2520data%2520collection%2520framework%2520and%2520a%250Atwo-stage%2520training%2520pipeline.%2520Our%2520framework%2520adopts%2520a%2520divide-and-conquer%250Astrategy%252C%2520refining%2520uncertainty%2520based%2520on%2520atomic%2520claims.%2520The%2520collected%2520data%2520are%250Athen%2520used%2520in%2520training%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520direct%250Apreference%2520optimization%2520%2528DPO%2529%2520to%2520enhance%2520uncertainty%2520expression.%2520Extensive%250Aexperiments%2520on%2520three%2520long-form%2520instruction%2520following%2520datasets%2520show%2520that%2520our%250Amethod%2520significantly%2520improves%2520accuracy%252C%2520reduces%2520hallucinations%252C%2520and%2520maintains%250Athe%2520comprehensiveness%2520of%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14309v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoGU%3A%20Long-form%20Generation%20with%20Uncertainty%20Expressions&entry.906535625=Ruihan%20Yang%20and%20Caiqi%20Zhang%20and%20Zhisong%20Zhang%20and%20Xinting%20Huang%20and%20Sen%20Yang%20and%20Nigel%20Collier%20and%20Dong%20Yu%20and%20Deqing%20Yang&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20capabilities%2C%20they%0Astill%20struggle%20with%20generating%20factually%20incorrect%20content%20%28i.e.%2C%0Ahallucinations%29.%20A%20promising%20approach%20to%20mitigate%20this%20issue%20is%20enabling%20models%0Ato%20express%20uncertainty%20when%20unsure.%20Previous%20research%20on%20uncertainty%20modeling%0Ahas%20primarily%20focused%20on%20short-form%20QA%2C%20but%20realworld%20applications%20often%0Arequire%20much%20longer%20responses.%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20Long-form%0AGeneration%20with%20Uncertainty%28LoGU%29.%20We%20identify%20two%20key%20challenges%3A%20Uncertainty%0ASuppression%2C%20where%20models%20hesitate%20to%20express%20uncertainty%2C%20and%20Uncertainty%0AMisalignment%2C%20where%20models%20convey%20uncertainty%20inaccurately.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20refinement-based%20data%20collection%20framework%20and%20a%0Atwo-stage%20training%20pipeline.%20Our%20framework%20adopts%20a%20divide-and-conquer%0Astrategy%2C%20refining%20uncertainty%20based%20on%20atomic%20claims.%20The%20collected%20data%20are%0Athen%20used%20in%20training%20through%20supervised%20fine-tuning%20%28SFT%29%20and%20direct%0Apreference%20optimization%20%28DPO%29%20to%20enhance%20uncertainty%20expression.%20Extensive%0Aexperiments%20on%20three%20long-form%20instruction%20following%20datasets%20show%20that%20our%0Amethod%20significantly%20improves%20accuracy%2C%20reduces%20hallucinations%2C%20and%20maintains%0Athe%20comprehensiveness%20of%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14309v4&entry.124074799=Read"},
{"title": "X-Factor: Quality Is a Dataset-Intrinsic Property", "author": "Josiah Couch and Miao Li and Rima Arnaout and Ramy Arnaout", "abstract": "  In the universal quest to optimize machine-learning classifiers, three\nfactors -- model architecture, dataset size, and class balance -- have been\nshown to influence test-time performance but do not fully account for it.\nPreviously, evidence was presented for an additional factor that can be\nreferred to as dataset quality, but it was unclear whether this was actually a\njoint property of the dataset and the model architecture, or an intrinsic\nproperty of the dataset itself. If quality is truly dataset-intrinsic and\nindependent of model architecture, dataset size, and class balance, then the\nsame datasets should perform better (or worse) regardless of these other\nfactors. To test this hypothesis, here we create thousands of datasets, each\ncontrolled for size and class balance, and use them to train classifiers with a\nwide range of architectures, from random forests and support-vector machines to\ndeep networks. We find that classifier performance correlates strongly by\nsubset across architectures ($R^2=0.79$), supporting quality as an intrinsic\nproperty of datasets independent of dataset size and class balance and of model\narchitecture. Digging deeper, we find that dataset quality appears to be an\nemergent property of something more fundamental: the quality of datasets'\nconstituent classes. Thus, quality joins size, class balance, and model\narchitecture as an independent correlate of performance and a separate target\nfor optimizing machine-learning-based classification.\n", "link": "http://arxiv.org/abs/2505.22813v2", "date": "2025-06-04", "relevancy": 2.1891, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Factor%3A%20Quality%20Is%20a%20Dataset-Intrinsic%20Property&body=Title%3A%20X-Factor%3A%20Quality%20Is%20a%20Dataset-Intrinsic%20Property%0AAuthor%3A%20Josiah%20Couch%20and%20Miao%20Li%20and%20Rima%20Arnaout%20and%20Ramy%20Arnaout%0AAbstract%3A%20%20%20In%20the%20universal%20quest%20to%20optimize%20machine-learning%20classifiers%2C%20three%0Afactors%20--%20model%20architecture%2C%20dataset%20size%2C%20and%20class%20balance%20--%20have%20been%0Ashown%20to%20influence%20test-time%20performance%20but%20do%20not%20fully%20account%20for%20it.%0APreviously%2C%20evidence%20was%20presented%20for%20an%20additional%20factor%20that%20can%20be%0Areferred%20to%20as%20dataset%20quality%2C%20but%20it%20was%20unclear%20whether%20this%20was%20actually%20a%0Ajoint%20property%20of%20the%20dataset%20and%20the%20model%20architecture%2C%20or%20an%20intrinsic%0Aproperty%20of%20the%20dataset%20itself.%20If%20quality%20is%20truly%20dataset-intrinsic%20and%0Aindependent%20of%20model%20architecture%2C%20dataset%20size%2C%20and%20class%20balance%2C%20then%20the%0Asame%20datasets%20should%20perform%20better%20%28or%20worse%29%20regardless%20of%20these%20other%0Afactors.%20To%20test%20this%20hypothesis%2C%20here%20we%20create%20thousands%20of%20datasets%2C%20each%0Acontrolled%20for%20size%20and%20class%20balance%2C%20and%20use%20them%20to%20train%20classifiers%20with%20a%0Awide%20range%20of%20architectures%2C%20from%20random%20forests%20and%20support-vector%20machines%20to%0Adeep%20networks.%20We%20find%20that%20classifier%20performance%20correlates%20strongly%20by%0Asubset%20across%20architectures%20%28%24R%5E2%3D0.79%24%29%2C%20supporting%20quality%20as%20an%20intrinsic%0Aproperty%20of%20datasets%20independent%20of%20dataset%20size%20and%20class%20balance%20and%20of%20model%0Aarchitecture.%20Digging%20deeper%2C%20we%20find%20that%20dataset%20quality%20appears%20to%20be%20an%0Aemergent%20property%20of%20something%20more%20fundamental%3A%20the%20quality%20of%20datasets%27%0Aconstituent%20classes.%20Thus%2C%20quality%20joins%20size%2C%20class%20balance%2C%20and%20model%0Aarchitecture%20as%20an%20independent%20correlate%20of%20performance%20and%20a%20separate%20target%0Afor%20optimizing%20machine-learning-based%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Factor%253A%2520Quality%2520Is%2520a%2520Dataset-Intrinsic%2520Property%26entry.906535625%3DJosiah%2520Couch%2520and%2520Miao%2520Li%2520and%2520Rima%2520Arnaout%2520and%2520Ramy%2520Arnaout%26entry.1292438233%3D%2520%2520In%2520the%2520universal%2520quest%2520to%2520optimize%2520machine-learning%2520classifiers%252C%2520three%250Afactors%2520--%2520model%2520architecture%252C%2520dataset%2520size%252C%2520and%2520class%2520balance%2520--%2520have%2520been%250Ashown%2520to%2520influence%2520test-time%2520performance%2520but%2520do%2520not%2520fully%2520account%2520for%2520it.%250APreviously%252C%2520evidence%2520was%2520presented%2520for%2520an%2520additional%2520factor%2520that%2520can%2520be%250Areferred%2520to%2520as%2520dataset%2520quality%252C%2520but%2520it%2520was%2520unclear%2520whether%2520this%2520was%2520actually%2520a%250Ajoint%2520property%2520of%2520the%2520dataset%2520and%2520the%2520model%2520architecture%252C%2520or%2520an%2520intrinsic%250Aproperty%2520of%2520the%2520dataset%2520itself.%2520If%2520quality%2520is%2520truly%2520dataset-intrinsic%2520and%250Aindependent%2520of%2520model%2520architecture%252C%2520dataset%2520size%252C%2520and%2520class%2520balance%252C%2520then%2520the%250Asame%2520datasets%2520should%2520perform%2520better%2520%2528or%2520worse%2529%2520regardless%2520of%2520these%2520other%250Afactors.%2520To%2520test%2520this%2520hypothesis%252C%2520here%2520we%2520create%2520thousands%2520of%2520datasets%252C%2520each%250Acontrolled%2520for%2520size%2520and%2520class%2520balance%252C%2520and%2520use%2520them%2520to%2520train%2520classifiers%2520with%2520a%250Awide%2520range%2520of%2520architectures%252C%2520from%2520random%2520forests%2520and%2520support-vector%2520machines%2520to%250Adeep%2520networks.%2520We%2520find%2520that%2520classifier%2520performance%2520correlates%2520strongly%2520by%250Asubset%2520across%2520architectures%2520%2528%2524R%255E2%253D0.79%2524%2529%252C%2520supporting%2520quality%2520as%2520an%2520intrinsic%250Aproperty%2520of%2520datasets%2520independent%2520of%2520dataset%2520size%2520and%2520class%2520balance%2520and%2520of%2520model%250Aarchitecture.%2520Digging%2520deeper%252C%2520we%2520find%2520that%2520dataset%2520quality%2520appears%2520to%2520be%2520an%250Aemergent%2520property%2520of%2520something%2520more%2520fundamental%253A%2520the%2520quality%2520of%2520datasets%2527%250Aconstituent%2520classes.%2520Thus%252C%2520quality%2520joins%2520size%252C%2520class%2520balance%252C%2520and%2520model%250Aarchitecture%2520as%2520an%2520independent%2520correlate%2520of%2520performance%2520and%2520a%2520separate%2520target%250Afor%2520optimizing%2520machine-learning-based%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Factor%3A%20Quality%20Is%20a%20Dataset-Intrinsic%20Property&entry.906535625=Josiah%20Couch%20and%20Miao%20Li%20and%20Rima%20Arnaout%20and%20Ramy%20Arnaout&entry.1292438233=%20%20In%20the%20universal%20quest%20to%20optimize%20machine-learning%20classifiers%2C%20three%0Afactors%20--%20model%20architecture%2C%20dataset%20size%2C%20and%20class%20balance%20--%20have%20been%0Ashown%20to%20influence%20test-time%20performance%20but%20do%20not%20fully%20account%20for%20it.%0APreviously%2C%20evidence%20was%20presented%20for%20an%20additional%20factor%20that%20can%20be%0Areferred%20to%20as%20dataset%20quality%2C%20but%20it%20was%20unclear%20whether%20this%20was%20actually%20a%0Ajoint%20property%20of%20the%20dataset%20and%20the%20model%20architecture%2C%20or%20an%20intrinsic%0Aproperty%20of%20the%20dataset%20itself.%20If%20quality%20is%20truly%20dataset-intrinsic%20and%0Aindependent%20of%20model%20architecture%2C%20dataset%20size%2C%20and%20class%20balance%2C%20then%20the%0Asame%20datasets%20should%20perform%20better%20%28or%20worse%29%20regardless%20of%20these%20other%0Afactors.%20To%20test%20this%20hypothesis%2C%20here%20we%20create%20thousands%20of%20datasets%2C%20each%0Acontrolled%20for%20size%20and%20class%20balance%2C%20and%20use%20them%20to%20train%20classifiers%20with%20a%0Awide%20range%20of%20architectures%2C%20from%20random%20forests%20and%20support-vector%20machines%20to%0Adeep%20networks.%20We%20find%20that%20classifier%20performance%20correlates%20strongly%20by%0Asubset%20across%20architectures%20%28%24R%5E2%3D0.79%24%29%2C%20supporting%20quality%20as%20an%20intrinsic%0Aproperty%20of%20datasets%20independent%20of%20dataset%20size%20and%20class%20balance%20and%20of%20model%0Aarchitecture.%20Digging%20deeper%2C%20we%20find%20that%20dataset%20quality%20appears%20to%20be%20an%0Aemergent%20property%20of%20something%20more%20fundamental%3A%20the%20quality%20of%20datasets%27%0Aconstituent%20classes.%20Thus%2C%20quality%20joins%20size%2C%20class%20balance%2C%20and%20model%0Aarchitecture%20as%20an%20independent%20correlate%20of%20performance%20and%20a%20separate%20target%0Afor%20optimizing%20machine-learning-based%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22813v2&entry.124074799=Read"},
{"title": "Attention-Only Transformers via Unrolled Subspace Denoising", "author": "Peng Wang and Yifu Lu and Yaodong Yu and Druv Pai and Qing Qu and Yi Ma", "abstract": "  Despite the popularity of transformers in practice, their architectures are\nempirically designed and neither mathematically justified nor interpretable.\nMoreover, as indicated by many empirical studies, some components of\ntransformer architectures may be redundant. To derive a fully interpretable\ntransformer architecture with only necessary components, we contend that the\ngoal of representation learning is to compress a set of noisy initial token\nrepresentations towards a mixture of low-dimensional subspaces. To compress\nthese noisy token representations, an associated denoising operation naturally\ntakes the form of a multi-head (subspace) self-attention. By unrolling such\niterative denoising operations into a deep network, we arrive at a highly\ncompact architecture that consists of \\textit{only} self-attention operators\nwith skip connections at each layer. Moreover, we show that each layer performs\nhighly efficient denoising: it improves the signal-to-noise ratio of token\nrepresentations \\textit{at a linear rate} with respect to the number of layers.\nDespite its simplicity, extensive experiments on vision and language tasks\ndemonstrate that such a transformer achieves performance close to that of\nstandard transformer architectures such as GPT-2 and CRATE.\n", "link": "http://arxiv.org/abs/2506.03790v1", "date": "2025-06-04", "relevancy": 2.1884, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.594}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5755}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Only%20Transformers%20via%20Unrolled%20Subspace%20Denoising&body=Title%3A%20Attention-Only%20Transformers%20via%20Unrolled%20Subspace%20Denoising%0AAuthor%3A%20Peng%20Wang%20and%20Yifu%20Lu%20and%20Yaodong%20Yu%20and%20Druv%20Pai%20and%20Qing%20Qu%20and%20Yi%20Ma%0AAbstract%3A%20%20%20Despite%20the%20popularity%20of%20transformers%20in%20practice%2C%20their%20architectures%20are%0Aempirically%20designed%20and%20neither%20mathematically%20justified%20nor%20interpretable.%0AMoreover%2C%20as%20indicated%20by%20many%20empirical%20studies%2C%20some%20components%20of%0Atransformer%20architectures%20may%20be%20redundant.%20To%20derive%20a%20fully%20interpretable%0Atransformer%20architecture%20with%20only%20necessary%20components%2C%20we%20contend%20that%20the%0Agoal%20of%20representation%20learning%20is%20to%20compress%20a%20set%20of%20noisy%20initial%20token%0Arepresentations%20towards%20a%20mixture%20of%20low-dimensional%20subspaces.%20To%20compress%0Athese%20noisy%20token%20representations%2C%20an%20associated%20denoising%20operation%20naturally%0Atakes%20the%20form%20of%20a%20multi-head%20%28subspace%29%20self-attention.%20By%20unrolling%20such%0Aiterative%20denoising%20operations%20into%20a%20deep%20network%2C%20we%20arrive%20at%20a%20highly%0Acompact%20architecture%20that%20consists%20of%20%5Ctextit%7Bonly%7D%20self-attention%20operators%0Awith%20skip%20connections%20at%20each%20layer.%20Moreover%2C%20we%20show%20that%20each%20layer%20performs%0Ahighly%20efficient%20denoising%3A%20it%20improves%20the%20signal-to-noise%20ratio%20of%20token%0Arepresentations%20%5Ctextit%7Bat%20a%20linear%20rate%7D%20with%20respect%20to%20the%20number%20of%20layers.%0ADespite%20its%20simplicity%2C%20extensive%20experiments%20on%20vision%20and%20language%20tasks%0Ademonstrate%20that%20such%20a%20transformer%20achieves%20performance%20close%20to%20that%20of%0Astandard%20transformer%20architectures%20such%20as%20GPT-2%20and%20CRATE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Only%2520Transformers%2520via%2520Unrolled%2520Subspace%2520Denoising%26entry.906535625%3DPeng%2520Wang%2520and%2520Yifu%2520Lu%2520and%2520Yaodong%2520Yu%2520and%2520Druv%2520Pai%2520and%2520Qing%2520Qu%2520and%2520Yi%2520Ma%26entry.1292438233%3D%2520%2520Despite%2520the%2520popularity%2520of%2520transformers%2520in%2520practice%252C%2520their%2520architectures%2520are%250Aempirically%2520designed%2520and%2520neither%2520mathematically%2520justified%2520nor%2520interpretable.%250AMoreover%252C%2520as%2520indicated%2520by%2520many%2520empirical%2520studies%252C%2520some%2520components%2520of%250Atransformer%2520architectures%2520may%2520be%2520redundant.%2520To%2520derive%2520a%2520fully%2520interpretable%250Atransformer%2520architecture%2520with%2520only%2520necessary%2520components%252C%2520we%2520contend%2520that%2520the%250Agoal%2520of%2520representation%2520learning%2520is%2520to%2520compress%2520a%2520set%2520of%2520noisy%2520initial%2520token%250Arepresentations%2520towards%2520a%2520mixture%2520of%2520low-dimensional%2520subspaces.%2520To%2520compress%250Athese%2520noisy%2520token%2520representations%252C%2520an%2520associated%2520denoising%2520operation%2520naturally%250Atakes%2520the%2520form%2520of%2520a%2520multi-head%2520%2528subspace%2529%2520self-attention.%2520By%2520unrolling%2520such%250Aiterative%2520denoising%2520operations%2520into%2520a%2520deep%2520network%252C%2520we%2520arrive%2520at%2520a%2520highly%250Acompact%2520architecture%2520that%2520consists%2520of%2520%255Ctextit%257Bonly%257D%2520self-attention%2520operators%250Awith%2520skip%2520connections%2520at%2520each%2520layer.%2520Moreover%252C%2520we%2520show%2520that%2520each%2520layer%2520performs%250Ahighly%2520efficient%2520denoising%253A%2520it%2520improves%2520the%2520signal-to-noise%2520ratio%2520of%2520token%250Arepresentations%2520%255Ctextit%257Bat%2520a%2520linear%2520rate%257D%2520with%2520respect%2520to%2520the%2520number%2520of%2520layers.%250ADespite%2520its%2520simplicity%252C%2520extensive%2520experiments%2520on%2520vision%2520and%2520language%2520tasks%250Ademonstrate%2520that%2520such%2520a%2520transformer%2520achieves%2520performance%2520close%2520to%2520that%2520of%250Astandard%2520transformer%2520architectures%2520such%2520as%2520GPT-2%2520and%2520CRATE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Only%20Transformers%20via%20Unrolled%20Subspace%20Denoising&entry.906535625=Peng%20Wang%20and%20Yifu%20Lu%20and%20Yaodong%20Yu%20and%20Druv%20Pai%20and%20Qing%20Qu%20and%20Yi%20Ma&entry.1292438233=%20%20Despite%20the%20popularity%20of%20transformers%20in%20practice%2C%20their%20architectures%20are%0Aempirically%20designed%20and%20neither%20mathematically%20justified%20nor%20interpretable.%0AMoreover%2C%20as%20indicated%20by%20many%20empirical%20studies%2C%20some%20components%20of%0Atransformer%20architectures%20may%20be%20redundant.%20To%20derive%20a%20fully%20interpretable%0Atransformer%20architecture%20with%20only%20necessary%20components%2C%20we%20contend%20that%20the%0Agoal%20of%20representation%20learning%20is%20to%20compress%20a%20set%20of%20noisy%20initial%20token%0Arepresentations%20towards%20a%20mixture%20of%20low-dimensional%20subspaces.%20To%20compress%0Athese%20noisy%20token%20representations%2C%20an%20associated%20denoising%20operation%20naturally%0Atakes%20the%20form%20of%20a%20multi-head%20%28subspace%29%20self-attention.%20By%20unrolling%20such%0Aiterative%20denoising%20operations%20into%20a%20deep%20network%2C%20we%20arrive%20at%20a%20highly%0Acompact%20architecture%20that%20consists%20of%20%5Ctextit%7Bonly%7D%20self-attention%20operators%0Awith%20skip%20connections%20at%20each%20layer.%20Moreover%2C%20we%20show%20that%20each%20layer%20performs%0Ahighly%20efficient%20denoising%3A%20it%20improves%20the%20signal-to-noise%20ratio%20of%20token%0Arepresentations%20%5Ctextit%7Bat%20a%20linear%20rate%7D%20with%20respect%20to%20the%20number%20of%20layers.%0ADespite%20its%20simplicity%2C%20extensive%20experiments%20on%20vision%20and%20language%20tasks%0Ademonstrate%20that%20such%20a%20transformer%20achieves%20performance%20close%20to%20that%20of%0Astandard%20transformer%20architectures%20such%20as%20GPT-2%20and%20CRATE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03790v1&entry.124074799=Read"},
{"title": "Implicit Inversion turns CLIP into a Decoder", "author": "Antonio D'Orazio and Maria Rosaria Briglia and Donato Crisostomi and Dario Loi and Emanuele Rodol\u00e0 and Iacopo Masi", "abstract": "  CLIP is a discriminative model trained to align images and text in a shared\nembedding space. Due to its multimodal structure, it serves as the backbone of\nmany generative pipelines, where a decoder is trained to map from the shared\nspace back to images. In this work, we show that image synthesis is\nnevertheless possible using CLIP alone -- without any decoder, training, or\nfine-tuning. Our approach optimizes a frequency-aware implicit neural\nrepresentation that encourages coarse-to-fine generation by stratifying\nfrequencies across network layers. To stabilize this inverse mapping, we\nintroduce adversarially robust initialization, a lightweight Orthogonal\nProcrustes projection to align local text and image embeddings, and a blending\nloss that anchors outputs to natural image statistics. Without altering CLIP's\nweights, this framework unlocks capabilities such as text-to-image generation,\nstyle transfer, and image reconstruction. These findings suggest that\ndiscriminative models may hold untapped generative potential, hidden in plain\nsight.\n", "link": "http://arxiv.org/abs/2505.23161v2", "date": "2025-06-04", "relevancy": 2.1608, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Inversion%20turns%20CLIP%20into%20a%20Decoder&body=Title%3A%20Implicit%20Inversion%20turns%20CLIP%20into%20a%20Decoder%0AAuthor%3A%20Antonio%20D%27Orazio%20and%20Maria%20Rosaria%20Briglia%20and%20Donato%20Crisostomi%20and%20Dario%20Loi%20and%20Emanuele%20Rodol%C3%A0%20and%20Iacopo%20Masi%0AAbstract%3A%20%20%20CLIP%20is%20a%20discriminative%20model%20trained%20to%20align%20images%20and%20text%20in%20a%20shared%0Aembedding%20space.%20Due%20to%20its%20multimodal%20structure%2C%20it%20serves%20as%20the%20backbone%20of%0Amany%20generative%20pipelines%2C%20where%20a%20decoder%20is%20trained%20to%20map%20from%20the%20shared%0Aspace%20back%20to%20images.%20In%20this%20work%2C%20we%20show%20that%20image%20synthesis%20is%0Anevertheless%20possible%20using%20CLIP%20alone%20--%20without%20any%20decoder%2C%20training%2C%20or%0Afine-tuning.%20Our%20approach%20optimizes%20a%20frequency-aware%20implicit%20neural%0Arepresentation%20that%20encourages%20coarse-to-fine%20generation%20by%20stratifying%0Afrequencies%20across%20network%20layers.%20To%20stabilize%20this%20inverse%20mapping%2C%20we%0Aintroduce%20adversarially%20robust%20initialization%2C%20a%20lightweight%20Orthogonal%0AProcrustes%20projection%20to%20align%20local%20text%20and%20image%20embeddings%2C%20and%20a%20blending%0Aloss%20that%20anchors%20outputs%20to%20natural%20image%20statistics.%20Without%20altering%20CLIP%27s%0Aweights%2C%20this%20framework%20unlocks%20capabilities%20such%20as%20text-to-image%20generation%2C%0Astyle%20transfer%2C%20and%20image%20reconstruction.%20These%20findings%20suggest%20that%0Adiscriminative%20models%20may%20hold%20untapped%20generative%20potential%2C%20hidden%20in%20plain%0Asight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Inversion%2520turns%2520CLIP%2520into%2520a%2520Decoder%26entry.906535625%3DAntonio%2520D%2527Orazio%2520and%2520Maria%2520Rosaria%2520Briglia%2520and%2520Donato%2520Crisostomi%2520and%2520Dario%2520Loi%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Iacopo%2520Masi%26entry.1292438233%3D%2520%2520CLIP%2520is%2520a%2520discriminative%2520model%2520trained%2520to%2520align%2520images%2520and%2520text%2520in%2520a%2520shared%250Aembedding%2520space.%2520Due%2520to%2520its%2520multimodal%2520structure%252C%2520it%2520serves%2520as%2520the%2520backbone%2520of%250Amany%2520generative%2520pipelines%252C%2520where%2520a%2520decoder%2520is%2520trained%2520to%2520map%2520from%2520the%2520shared%250Aspace%2520back%2520to%2520images.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520image%2520synthesis%2520is%250Anevertheless%2520possible%2520using%2520CLIP%2520alone%2520--%2520without%2520any%2520decoder%252C%2520training%252C%2520or%250Afine-tuning.%2520Our%2520approach%2520optimizes%2520a%2520frequency-aware%2520implicit%2520neural%250Arepresentation%2520that%2520encourages%2520coarse-to-fine%2520generation%2520by%2520stratifying%250Afrequencies%2520across%2520network%2520layers.%2520To%2520stabilize%2520this%2520inverse%2520mapping%252C%2520we%250Aintroduce%2520adversarially%2520robust%2520initialization%252C%2520a%2520lightweight%2520Orthogonal%250AProcrustes%2520projection%2520to%2520align%2520local%2520text%2520and%2520image%2520embeddings%252C%2520and%2520a%2520blending%250Aloss%2520that%2520anchors%2520outputs%2520to%2520natural%2520image%2520statistics.%2520Without%2520altering%2520CLIP%2527s%250Aweights%252C%2520this%2520framework%2520unlocks%2520capabilities%2520such%2520as%2520text-to-image%2520generation%252C%250Astyle%2520transfer%252C%2520and%2520image%2520reconstruction.%2520These%2520findings%2520suggest%2520that%250Adiscriminative%2520models%2520may%2520hold%2520untapped%2520generative%2520potential%252C%2520hidden%2520in%2520plain%250Asight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Inversion%20turns%20CLIP%20into%20a%20Decoder&entry.906535625=Antonio%20D%27Orazio%20and%20Maria%20Rosaria%20Briglia%20and%20Donato%20Crisostomi%20and%20Dario%20Loi%20and%20Emanuele%20Rodol%C3%A0%20and%20Iacopo%20Masi&entry.1292438233=%20%20CLIP%20is%20a%20discriminative%20model%20trained%20to%20align%20images%20and%20text%20in%20a%20shared%0Aembedding%20space.%20Due%20to%20its%20multimodal%20structure%2C%20it%20serves%20as%20the%20backbone%20of%0Amany%20generative%20pipelines%2C%20where%20a%20decoder%20is%20trained%20to%20map%20from%20the%20shared%0Aspace%20back%20to%20images.%20In%20this%20work%2C%20we%20show%20that%20image%20synthesis%20is%0Anevertheless%20possible%20using%20CLIP%20alone%20--%20without%20any%20decoder%2C%20training%2C%20or%0Afine-tuning.%20Our%20approach%20optimizes%20a%20frequency-aware%20implicit%20neural%0Arepresentation%20that%20encourages%20coarse-to-fine%20generation%20by%20stratifying%0Afrequencies%20across%20network%20layers.%20To%20stabilize%20this%20inverse%20mapping%2C%20we%0Aintroduce%20adversarially%20robust%20initialization%2C%20a%20lightweight%20Orthogonal%0AProcrustes%20projection%20to%20align%20local%20text%20and%20image%20embeddings%2C%20and%20a%20blending%0Aloss%20that%20anchors%20outputs%20to%20natural%20image%20statistics.%20Without%20altering%20CLIP%27s%0Aweights%2C%20this%20framework%20unlocks%20capabilities%20such%20as%20text-to-image%20generation%2C%0Astyle%20transfer%2C%20and%20image%20reconstruction.%20These%20findings%20suggest%20that%0Adiscriminative%20models%20may%20hold%20untapped%20generative%20potential%2C%20hidden%20in%20plain%0Asight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23161v2&entry.124074799=Read"},
{"title": "MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking", "author": "Yaxiong Lei and Mingyue Zhao and Yuheng Wang and Shijing He and Yusuke Sugano and Mohamed Khamis and Juan Ye", "abstract": "  Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as\nusers naturally change their postures and device orientations. Traditional\ncalibration approaches, like one-off, fail to adapt to these dynamic\nconditions, leading to degraded performance over time. We present MAC-Gaze, a\nMotion-Aware continual Calibration approach that leverages smartphone Inertial\nmeasurement unit (IMU) sensors and continual learning techniques to\nautomatically detect changes in user motion states and update the gaze tracking\nmodel accordingly. Our system integrates a pre-trained visual gaze estimator\nand an IMU-based activity recognition model with a clustering-based hybrid\ndecision-making mechanism that triggers recalibration when motion patterns\ndeviate significantly from previously encountered states. To enable\naccumulative learning of new motion conditions while mitigating catastrophic\nforgetting, we employ replay-based continual learning, allowing the model to\nmaintain performance across previously encountered motion conditions. We\nevaluate our system through extensive experiments on the publicly available\nRGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+\nimages, 800K+ IMU readings), encompassing a wide range of postures under\nvarious motion conditions including sitting, standing, lying, and walking.\nResults demonstrate that our method reduces gaze estimation error by 19.9% on\nRGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to\n1.92 cm) compared to traditional calibration approaches. Our framework provides\na robust solution for maintaining gaze estimation accuracy in mobile scenarios.\n", "link": "http://arxiv.org/abs/2505.22769v2", "date": "2025-06-04", "relevancy": 2.158, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5464}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5415}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAC-Gaze%3A%20Motion-Aware%20Continual%20Calibration%20for%20Mobile%20Gaze%20Tracking&body=Title%3A%20MAC-Gaze%3A%20Motion-Aware%20Continual%20Calibration%20for%20Mobile%20Gaze%20Tracking%0AAuthor%3A%20Yaxiong%20Lei%20and%20Mingyue%20Zhao%20and%20Yuheng%20Wang%20and%20Shijing%20He%20and%20Yusuke%20Sugano%20and%20Mohamed%20Khamis%20and%20Juan%20Ye%0AAbstract%3A%20%20%20Mobile%20gaze%20tracking%20faces%20a%20fundamental%20challenge%3A%20maintaining%20accuracy%20as%0Ausers%20naturally%20change%20their%20postures%20and%20device%20orientations.%20Traditional%0Acalibration%20approaches%2C%20like%20one-off%2C%20fail%20to%20adapt%20to%20these%20dynamic%0Aconditions%2C%20leading%20to%20degraded%20performance%20over%20time.%20We%20present%20MAC-Gaze%2C%20a%0AMotion-Aware%20continual%20Calibration%20approach%20that%20leverages%20smartphone%20Inertial%0Ameasurement%20unit%20%28IMU%29%20sensors%20and%20continual%20learning%20techniques%20to%0Aautomatically%20detect%20changes%20in%20user%20motion%20states%20and%20update%20the%20gaze%20tracking%0Amodel%20accordingly.%20Our%20system%20integrates%20a%20pre-trained%20visual%20gaze%20estimator%0Aand%20an%20IMU-based%20activity%20recognition%20model%20with%20a%20clustering-based%20hybrid%0Adecision-making%20mechanism%20that%20triggers%20recalibration%20when%20motion%20patterns%0Adeviate%20significantly%20from%20previously%20encountered%20states.%20To%20enable%0Aaccumulative%20learning%20of%20new%20motion%20conditions%20while%20mitigating%20catastrophic%0Aforgetting%2C%20we%20employ%20replay-based%20continual%20learning%2C%20allowing%20the%20model%20to%0Amaintain%20performance%20across%20previously%20encountered%20motion%20conditions.%20We%0Aevaluate%20our%20system%20through%20extensive%20experiments%20on%20the%20publicly%20available%0ARGBDGaze%20dataset%20and%20our%20own%2010-hour%20multimodal%20MotionGaze%20dataset%20%28481K%2B%0Aimages%2C%20800K%2B%20IMU%20readings%29%2C%20encompassing%20a%20wide%20range%20of%20postures%20under%0Avarious%20motion%20conditions%20including%20sitting%2C%20standing%2C%20lying%2C%20and%20walking.%0AResults%20demonstrate%20that%20our%20method%20reduces%20gaze%20estimation%20error%20by%2019.9%25%20on%0ARGBDGaze%20%28from%201.73%20cm%20to%201.41%20cm%29%20and%20by%2031.7%25%20on%20MotionGaze%20%28from%202.81%20cm%20to%0A1.92%20cm%29%20compared%20to%20traditional%20calibration%20approaches.%20Our%20framework%20provides%0Aa%20robust%20solution%20for%20maintaining%20gaze%20estimation%20accuracy%20in%20mobile%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAC-Gaze%253A%2520Motion-Aware%2520Continual%2520Calibration%2520for%2520Mobile%2520Gaze%2520Tracking%26entry.906535625%3DYaxiong%2520Lei%2520and%2520Mingyue%2520Zhao%2520and%2520Yuheng%2520Wang%2520and%2520Shijing%2520He%2520and%2520Yusuke%2520Sugano%2520and%2520Mohamed%2520Khamis%2520and%2520Juan%2520Ye%26entry.1292438233%3D%2520%2520Mobile%2520gaze%2520tracking%2520faces%2520a%2520fundamental%2520challenge%253A%2520maintaining%2520accuracy%2520as%250Ausers%2520naturally%2520change%2520their%2520postures%2520and%2520device%2520orientations.%2520Traditional%250Acalibration%2520approaches%252C%2520like%2520one-off%252C%2520fail%2520to%2520adapt%2520to%2520these%2520dynamic%250Aconditions%252C%2520leading%2520to%2520degraded%2520performance%2520over%2520time.%2520We%2520present%2520MAC-Gaze%252C%2520a%250AMotion-Aware%2520continual%2520Calibration%2520approach%2520that%2520leverages%2520smartphone%2520Inertial%250Ameasurement%2520unit%2520%2528IMU%2529%2520sensors%2520and%2520continual%2520learning%2520techniques%2520to%250Aautomatically%2520detect%2520changes%2520in%2520user%2520motion%2520states%2520and%2520update%2520the%2520gaze%2520tracking%250Amodel%2520accordingly.%2520Our%2520system%2520integrates%2520a%2520pre-trained%2520visual%2520gaze%2520estimator%250Aand%2520an%2520IMU-based%2520activity%2520recognition%2520model%2520with%2520a%2520clustering-based%2520hybrid%250Adecision-making%2520mechanism%2520that%2520triggers%2520recalibration%2520when%2520motion%2520patterns%250Adeviate%2520significantly%2520from%2520previously%2520encountered%2520states.%2520To%2520enable%250Aaccumulative%2520learning%2520of%2520new%2520motion%2520conditions%2520while%2520mitigating%2520catastrophic%250Aforgetting%252C%2520we%2520employ%2520replay-based%2520continual%2520learning%252C%2520allowing%2520the%2520model%2520to%250Amaintain%2520performance%2520across%2520previously%2520encountered%2520motion%2520conditions.%2520We%250Aevaluate%2520our%2520system%2520through%2520extensive%2520experiments%2520on%2520the%2520publicly%2520available%250ARGBDGaze%2520dataset%2520and%2520our%2520own%252010-hour%2520multimodal%2520MotionGaze%2520dataset%2520%2528481K%252B%250Aimages%252C%2520800K%252B%2520IMU%2520readings%2529%252C%2520encompassing%2520a%2520wide%2520range%2520of%2520postures%2520under%250Avarious%2520motion%2520conditions%2520including%2520sitting%252C%2520standing%252C%2520lying%252C%2520and%2520walking.%250AResults%2520demonstrate%2520that%2520our%2520method%2520reduces%2520gaze%2520estimation%2520error%2520by%252019.9%2525%2520on%250ARGBDGaze%2520%2528from%25201.73%2520cm%2520to%25201.41%2520cm%2529%2520and%2520by%252031.7%2525%2520on%2520MotionGaze%2520%2528from%25202.81%2520cm%2520to%250A1.92%2520cm%2529%2520compared%2520to%2520traditional%2520calibration%2520approaches.%2520Our%2520framework%2520provides%250Aa%2520robust%2520solution%2520for%2520maintaining%2520gaze%2520estimation%2520accuracy%2520in%2520mobile%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAC-Gaze%3A%20Motion-Aware%20Continual%20Calibration%20for%20Mobile%20Gaze%20Tracking&entry.906535625=Yaxiong%20Lei%20and%20Mingyue%20Zhao%20and%20Yuheng%20Wang%20and%20Shijing%20He%20and%20Yusuke%20Sugano%20and%20Mohamed%20Khamis%20and%20Juan%20Ye&entry.1292438233=%20%20Mobile%20gaze%20tracking%20faces%20a%20fundamental%20challenge%3A%20maintaining%20accuracy%20as%0Ausers%20naturally%20change%20their%20postures%20and%20device%20orientations.%20Traditional%0Acalibration%20approaches%2C%20like%20one-off%2C%20fail%20to%20adapt%20to%20these%20dynamic%0Aconditions%2C%20leading%20to%20degraded%20performance%20over%20time.%20We%20present%20MAC-Gaze%2C%20a%0AMotion-Aware%20continual%20Calibration%20approach%20that%20leverages%20smartphone%20Inertial%0Ameasurement%20unit%20%28IMU%29%20sensors%20and%20continual%20learning%20techniques%20to%0Aautomatically%20detect%20changes%20in%20user%20motion%20states%20and%20update%20the%20gaze%20tracking%0Amodel%20accordingly.%20Our%20system%20integrates%20a%20pre-trained%20visual%20gaze%20estimator%0Aand%20an%20IMU-based%20activity%20recognition%20model%20with%20a%20clustering-based%20hybrid%0Adecision-making%20mechanism%20that%20triggers%20recalibration%20when%20motion%20patterns%0Adeviate%20significantly%20from%20previously%20encountered%20states.%20To%20enable%0Aaccumulative%20learning%20of%20new%20motion%20conditions%20while%20mitigating%20catastrophic%0Aforgetting%2C%20we%20employ%20replay-based%20continual%20learning%2C%20allowing%20the%20model%20to%0Amaintain%20performance%20across%20previously%20encountered%20motion%20conditions.%20We%0Aevaluate%20our%20system%20through%20extensive%20experiments%20on%20the%20publicly%20available%0ARGBDGaze%20dataset%20and%20our%20own%2010-hour%20multimodal%20MotionGaze%20dataset%20%28481K%2B%0Aimages%2C%20800K%2B%20IMU%20readings%29%2C%20encompassing%20a%20wide%20range%20of%20postures%20under%0Avarious%20motion%20conditions%20including%20sitting%2C%20standing%2C%20lying%2C%20and%20walking.%0AResults%20demonstrate%20that%20our%20method%20reduces%20gaze%20estimation%20error%20by%2019.9%25%20on%0ARGBDGaze%20%28from%201.73%20cm%20to%201.41%20cm%29%20and%20by%2031.7%25%20on%20MotionGaze%20%28from%202.81%20cm%20to%0A1.92%20cm%29%20compared%20to%20traditional%20calibration%20approaches.%20Our%20framework%20provides%0Aa%20robust%20solution%20for%20maintaining%20gaze%20estimation%20accuracy%20in%20mobile%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22769v2&entry.124074799=Read"},
{"title": "Average Calibration Losses for Reliable Uncertainty in Medical Image\n  Segmentation", "author": "Theodore Barfoot and Luis C. Garcia-Peraza-Herrera and Samet Akcay and Ben Glocker and Tom Vercauteren", "abstract": "  Deep neural networks for medical image segmentation are often overconfident,\ncompromising both reliability and clinical utility. In this work, we propose\ndifferentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)\nas an auxiliary loss that can be computed on a per-image basis. We compare both\nhard- and soft-binning approaches to directly improve pixel-wise calibration.\nOur experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that\nincorporating mL1-ACE significantly reduces calibration errors, particularly\nAverage Calibration Error (ACE) and Maximum Calibration Error (MCE), while\nlargely maintaining high Dice Similarity Coefficients (DSCs). We find that the\nsoft-binned variant yields the greatest improvements in calibration, over the\nDice plus cross-entropy loss baseline, but often compromises segmentation\nperformance, with hard-binned mL1-ACE maintaining segmentation performance,\nalbeit with weaker calibration improvement. To gain further insight into\ncalibration performance and its variability across an imaging dataset, we\nintroduce dataset reliability histograms, an aggregation of per-image\nreliability diagrams. The resulting analysis highlights improved alignment\nbetween predicted confidences and true accuracies. Overall, our approach not\nonly enhances the trustworthiness of segmentation predictions but also shows\npotential for safer integration of deep learning methods into clinical\nworkflows. We share our code here:\nhttps://github.com/cai4cai/Average-Calibration-Losses\n", "link": "http://arxiv.org/abs/2506.03942v1", "date": "2025-06-04", "relevancy": 2.1546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5656}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Average%20Calibration%20Losses%20for%20Reliable%20Uncertainty%20in%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Average%20Calibration%20Losses%20for%20Reliable%20Uncertainty%20in%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Theodore%20Barfoot%20and%20Luis%20C.%20Garcia-Peraza-Herrera%20and%20Samet%20Akcay%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20are%20often%20overconfident%2C%0Acompromising%20both%20reliability%20and%20clinical%20utility.%20In%20this%20work%2C%20we%20propose%0Adifferentiable%20formulations%20of%20marginal%20L1%20Average%20Calibration%20Error%20%28mL1-ACE%29%0Aas%20an%20auxiliary%20loss%20that%20can%20be%20computed%20on%20a%20per-image%20basis.%20We%20compare%20both%0Ahard-%20and%20soft-binning%20approaches%20to%20directly%20improve%20pixel-wise%20calibration.%0AOur%20experiments%20on%20four%20datasets%20%28ACDC%2C%20AMOS%2C%20KiTS%2C%20BraTS%29%20demonstrate%20that%0Aincorporating%20mL1-ACE%20significantly%20reduces%20calibration%20errors%2C%20particularly%0AAverage%20Calibration%20Error%20%28ACE%29%20and%20Maximum%20Calibration%20Error%20%28MCE%29%2C%20while%0Alargely%20maintaining%20high%20Dice%20Similarity%20Coefficients%20%28DSCs%29.%20We%20find%20that%20the%0Asoft-binned%20variant%20yields%20the%20greatest%20improvements%20in%20calibration%2C%20over%20the%0ADice%20plus%20cross-entropy%20loss%20baseline%2C%20but%20often%20compromises%20segmentation%0Aperformance%2C%20with%20hard-binned%20mL1-ACE%20maintaining%20segmentation%20performance%2C%0Aalbeit%20with%20weaker%20calibration%20improvement.%20To%20gain%20further%20insight%20into%0Acalibration%20performance%20and%20its%20variability%20across%20an%20imaging%20dataset%2C%20we%0Aintroduce%20dataset%20reliability%20histograms%2C%20an%20aggregation%20of%20per-image%0Areliability%20diagrams.%20The%20resulting%20analysis%20highlights%20improved%20alignment%0Abetween%20predicted%20confidences%20and%20true%20accuracies.%20Overall%2C%20our%20approach%20not%0Aonly%20enhances%20the%20trustworthiness%20of%20segmentation%20predictions%20but%20also%20shows%0Apotential%20for%20safer%20integration%20of%20deep%20learning%20methods%20into%20clinical%0Aworkflows.%20We%20share%20our%20code%20here%3A%0Ahttps%3A//github.com/cai4cai/Average-Calibration-Losses%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAverage%2520Calibration%2520Losses%2520for%2520Reliable%2520Uncertainty%2520in%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DTheodore%2520Barfoot%2520and%2520Luis%2520C.%2520Garcia-Peraza-Herrera%2520and%2520Samet%2520Akcay%2520and%2520Ben%2520Glocker%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520for%2520medical%2520image%2520segmentation%2520are%2520often%2520overconfident%252C%250Acompromising%2520both%2520reliability%2520and%2520clinical%2520utility.%2520In%2520this%2520work%252C%2520we%2520propose%250Adifferentiable%2520formulations%2520of%2520marginal%2520L1%2520Average%2520Calibration%2520Error%2520%2528mL1-ACE%2529%250Aas%2520an%2520auxiliary%2520loss%2520that%2520can%2520be%2520computed%2520on%2520a%2520per-image%2520basis.%2520We%2520compare%2520both%250Ahard-%2520and%2520soft-binning%2520approaches%2520to%2520directly%2520improve%2520pixel-wise%2520calibration.%250AOur%2520experiments%2520on%2520four%2520datasets%2520%2528ACDC%252C%2520AMOS%252C%2520KiTS%252C%2520BraTS%2529%2520demonstrate%2520that%250Aincorporating%2520mL1-ACE%2520significantly%2520reduces%2520calibration%2520errors%252C%2520particularly%250AAverage%2520Calibration%2520Error%2520%2528ACE%2529%2520and%2520Maximum%2520Calibration%2520Error%2520%2528MCE%2529%252C%2520while%250Alargely%2520maintaining%2520high%2520Dice%2520Similarity%2520Coefficients%2520%2528DSCs%2529.%2520We%2520find%2520that%2520the%250Asoft-binned%2520variant%2520yields%2520the%2520greatest%2520improvements%2520in%2520calibration%252C%2520over%2520the%250ADice%2520plus%2520cross-entropy%2520loss%2520baseline%252C%2520but%2520often%2520compromises%2520segmentation%250Aperformance%252C%2520with%2520hard-binned%2520mL1-ACE%2520maintaining%2520segmentation%2520performance%252C%250Aalbeit%2520with%2520weaker%2520calibration%2520improvement.%2520To%2520gain%2520further%2520insight%2520into%250Acalibration%2520performance%2520and%2520its%2520variability%2520across%2520an%2520imaging%2520dataset%252C%2520we%250Aintroduce%2520dataset%2520reliability%2520histograms%252C%2520an%2520aggregation%2520of%2520per-image%250Areliability%2520diagrams.%2520The%2520resulting%2520analysis%2520highlights%2520improved%2520alignment%250Abetween%2520predicted%2520confidences%2520and%2520true%2520accuracies.%2520Overall%252C%2520our%2520approach%2520not%250Aonly%2520enhances%2520the%2520trustworthiness%2520of%2520segmentation%2520predictions%2520but%2520also%2520shows%250Apotential%2520for%2520safer%2520integration%2520of%2520deep%2520learning%2520methods%2520into%2520clinical%250Aworkflows.%2520We%2520share%2520our%2520code%2520here%253A%250Ahttps%253A//github.com/cai4cai/Average-Calibration-Losses%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Average%20Calibration%20Losses%20for%20Reliable%20Uncertainty%20in%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Theodore%20Barfoot%20and%20Luis%20C.%20Garcia-Peraza-Herrera%20and%20Samet%20Akcay%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20are%20often%20overconfident%2C%0Acompromising%20both%20reliability%20and%20clinical%20utility.%20In%20this%20work%2C%20we%20propose%0Adifferentiable%20formulations%20of%20marginal%20L1%20Average%20Calibration%20Error%20%28mL1-ACE%29%0Aas%20an%20auxiliary%20loss%20that%20can%20be%20computed%20on%20a%20per-image%20basis.%20We%20compare%20both%0Ahard-%20and%20soft-binning%20approaches%20to%20directly%20improve%20pixel-wise%20calibration.%0AOur%20experiments%20on%20four%20datasets%20%28ACDC%2C%20AMOS%2C%20KiTS%2C%20BraTS%29%20demonstrate%20that%0Aincorporating%20mL1-ACE%20significantly%20reduces%20calibration%20errors%2C%20particularly%0AAverage%20Calibration%20Error%20%28ACE%29%20and%20Maximum%20Calibration%20Error%20%28MCE%29%2C%20while%0Alargely%20maintaining%20high%20Dice%20Similarity%20Coefficients%20%28DSCs%29.%20We%20find%20that%20the%0Asoft-binned%20variant%20yields%20the%20greatest%20improvements%20in%20calibration%2C%20over%20the%0ADice%20plus%20cross-entropy%20loss%20baseline%2C%20but%20often%20compromises%20segmentation%0Aperformance%2C%20with%20hard-binned%20mL1-ACE%20maintaining%20segmentation%20performance%2C%0Aalbeit%20with%20weaker%20calibration%20improvement.%20To%20gain%20further%20insight%20into%0Acalibration%20performance%20and%20its%20variability%20across%20an%20imaging%20dataset%2C%20we%0Aintroduce%20dataset%20reliability%20histograms%2C%20an%20aggregation%20of%20per-image%0Areliability%20diagrams.%20The%20resulting%20analysis%20highlights%20improved%20alignment%0Abetween%20predicted%20confidences%20and%20true%20accuracies.%20Overall%2C%20our%20approach%20not%0Aonly%20enhances%20the%20trustworthiness%20of%20segmentation%20predictions%20but%20also%20shows%0Apotential%20for%20safer%20integration%20of%20deep%20learning%20methods%20into%20clinical%0Aworkflows.%20We%20share%20our%20code%20here%3A%0Ahttps%3A//github.com/cai4cai/Average-Calibration-Losses%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03942v1&entry.124074799=Read"},
{"title": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization", "author": "Jiulong Wu and Zhengliang Shi and Shuaiqiang Wang and Jizhou Huang and Dawei Yin and Lingyong Yan and Min Cao and Min Zhang", "abstract": "  Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.\n", "link": "http://arxiv.org/abs/2506.04039v1", "date": "2025-06-04", "relevancy": 2.1488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%0A%20%20Entity-Centric%20Multimodal%20Preference%20Optimization&body=Title%3A%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%0A%20%20Entity-Centric%20Multimodal%20Preference%20Optimization%0AAuthor%3A%20Jiulong%20Wu%20and%20Zhengliang%20Shi%20and%20Shuaiqiang%20Wang%20and%20Jizhou%20Huang%20and%20Dawei%20Yin%20and%20Lingyong%20Yan%20and%20Min%20Cao%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20across%20multiple%20tasks.%20However%2C%20their%20trustworthiness%20is%20often%0Achallenged%20by%20hallucinations%2C%20which%20can%20be%20attributed%20to%20the%20modality%0Amisalignment%20and%20the%20inherent%20hallucinations%20of%20their%20underlying%20Large%20Language%0AModels%20%28LLMs%29%20backbone.%20Existing%20preference%20alignment%20methods%20focus%20on%20aligning%0Amodel%20responses%20with%20human%20preferences%20while%20neglecting%20image-text%20modality%0Aalignment%2C%20resulting%20in%20over-reliance%20on%20LLMs%20and%20hallucinations.%20In%20this%0Apaper%2C%20we%20propose%20Entity-centric%20Multimodal%20Preference%20Optimization%20%28EMPO%29%2C%0Awhich%20achieves%20enhanced%20modality%20alignment%20than%20existing%20human%20preference%0Aalignment%20methods.%20Besides%2C%20to%20overcome%20the%20scarcity%20of%20high-quality%20multimodal%0Apreference%20data%2C%20we%20utilize%20open-source%20instruction%20datasets%20to%20automatically%0Aconstruct%20high-quality%20preference%20data%20across%20three%20aspects%3A%20image%2C%0Ainstruction%2C%20and%20response.%20Experiments%20on%20two%20human%20preference%20datasets%20and%0Afive%20multimodal%20hallucination%20benchmarks%20demonstrate%20the%20effectiveness%20of%20EMPO%2C%0Ae.g.%2C%20reducing%20hallucination%20rates%20by%2085.9%25%20on%20Object-HalBench%20and%2049.8%25%20on%0AMM-HalBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%2520via%250A%2520%2520Entity-Centric%2520Multimodal%2520Preference%2520Optimization%26entry.906535625%3DJiulong%2520Wu%2520and%2520Zhengliang%2520Shi%2520and%2520Shuaiqiang%2520Wang%2520and%2520Jizhou%2520Huang%2520and%2520Dawei%2520Yin%2520and%2520Lingyong%2520Yan%2520and%2520Min%2520Cao%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520across%2520multiple%2520tasks.%2520However%252C%2520their%2520trustworthiness%2520is%2520often%250Achallenged%2520by%2520hallucinations%252C%2520which%2520can%2520be%2520attributed%2520to%2520the%2520modality%250Amisalignment%2520and%2520the%2520inherent%2520hallucinations%2520of%2520their%2520underlying%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520backbone.%2520Existing%2520preference%2520alignment%2520methods%2520focus%2520on%2520aligning%250Amodel%2520responses%2520with%2520human%2520preferences%2520while%2520neglecting%2520image-text%2520modality%250Aalignment%252C%2520resulting%2520in%2520over-reliance%2520on%2520LLMs%2520and%2520hallucinations.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520Entity-centric%2520Multimodal%2520Preference%2520Optimization%2520%2528EMPO%2529%252C%250Awhich%2520achieves%2520enhanced%2520modality%2520alignment%2520than%2520existing%2520human%2520preference%250Aalignment%2520methods.%2520Besides%252C%2520to%2520overcome%2520the%2520scarcity%2520of%2520high-quality%2520multimodal%250Apreference%2520data%252C%2520we%2520utilize%2520open-source%2520instruction%2520datasets%2520to%2520automatically%250Aconstruct%2520high-quality%2520preference%2520data%2520across%2520three%2520aspects%253A%2520image%252C%250Ainstruction%252C%2520and%2520response.%2520Experiments%2520on%2520two%2520human%2520preference%2520datasets%2520and%250Afive%2520multimodal%2520hallucination%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520EMPO%252C%250Ae.g.%252C%2520reducing%2520hallucination%2520rates%2520by%252085.9%2525%2520on%2520Object-HalBench%2520and%252049.8%2525%2520on%250AMM-HalBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%0A%20%20Entity-Centric%20Multimodal%20Preference%20Optimization&entry.906535625=Jiulong%20Wu%20and%20Zhengliang%20Shi%20and%20Shuaiqiang%20Wang%20and%20Jizhou%20Huang%20and%20Dawei%20Yin%20and%20Lingyong%20Yan%20and%20Min%20Cao%20and%20Min%20Zhang&entry.1292438233=%20%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20across%20multiple%20tasks.%20However%2C%20their%20trustworthiness%20is%20often%0Achallenged%20by%20hallucinations%2C%20which%20can%20be%20attributed%20to%20the%20modality%0Amisalignment%20and%20the%20inherent%20hallucinations%20of%20their%20underlying%20Large%20Language%0AModels%20%28LLMs%29%20backbone.%20Existing%20preference%20alignment%20methods%20focus%20on%20aligning%0Amodel%20responses%20with%20human%20preferences%20while%20neglecting%20image-text%20modality%0Aalignment%2C%20resulting%20in%20over-reliance%20on%20LLMs%20and%20hallucinations.%20In%20this%0Apaper%2C%20we%20propose%20Entity-centric%20Multimodal%20Preference%20Optimization%20%28EMPO%29%2C%0Awhich%20achieves%20enhanced%20modality%20alignment%20than%20existing%20human%20preference%0Aalignment%20methods.%20Besides%2C%20to%20overcome%20the%20scarcity%20of%20high-quality%20multimodal%0Apreference%20data%2C%20we%20utilize%20open-source%20instruction%20datasets%20to%20automatically%0Aconstruct%20high-quality%20preference%20data%20across%20three%20aspects%3A%20image%2C%0Ainstruction%2C%20and%20response.%20Experiments%20on%20two%20human%20preference%20datasets%20and%0Afive%20multimodal%20hallucination%20benchmarks%20demonstrate%20the%20effectiveness%20of%20EMPO%2C%0Ae.g.%2C%20reducing%20hallucination%20rates%20by%2085.9%25%20on%20Object-HalBench%20and%2049.8%25%20on%0AMM-HalBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04039v1&entry.124074799=Read"},
{"title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for\n  Accelerated Convergence in Permutation-Invariant Neural Networks for\n  Reinforcement Learning", "author": "Junaid Muzaffar and Khubaib Ahmed and Ingo Frommholz and Zeeshan Pervez and Ahsan ul Haq", "abstract": "  Training reinforcement learning (RL) agents often requires significant\ncomputational resources and extended training times. To address this, we build\nupon the foundation laid by Google Brain's Sensory Neuron, which introduced a\nnovel neural architecture for reinforcement learning tasks that maintained\npermutation in-variance in the sensory neuron system. While the baseline model\ndemonstrated significant performance improvements over traditional approaches,\nwe identified opportunities to enhance the efficiency of the learning process\nfurther. We propose a modified attention mechanism incorporating a non-linear\ntransformation of the key vectors (K) using a mapping function, resulting in a\nnew set of key vectors (K'). This non-linear mapping enhances the\nrepresentational capacity of the attention mechanism, allowing the model to\nencode more complex feature interactions and accelerating convergence without\ncompromising performance. Our enhanced model demonstrates significant\nimprovements in learning efficiency, showcasing the potential for non-linear\nattention mechanisms in advancing reinforcement learning algorithms.\n", "link": "http://arxiv.org/abs/2506.00691v2", "date": "2025-06-04", "relevancy": 2.1479, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Sensory%20Neurons%3A%20Nonlinear%20Attention%20Mechanisms%20for%0A%20%20Accelerated%20Convergence%20in%20Permutation-Invariant%20Neural%20Networks%20for%0A%20%20Reinforcement%20Learning&body=Title%3A%20Optimizing%20Sensory%20Neurons%3A%20Nonlinear%20Attention%20Mechanisms%20for%0A%20%20Accelerated%20Convergence%20in%20Permutation-Invariant%20Neural%20Networks%20for%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Junaid%20Muzaffar%20and%20Khubaib%20Ahmed%20and%20Ingo%20Frommholz%20and%20Zeeshan%20Pervez%20and%20Ahsan%20ul%20Haq%0AAbstract%3A%20%20%20Training%20reinforcement%20learning%20%28RL%29%20agents%20often%20requires%20significant%0Acomputational%20resources%20and%20extended%20training%20times.%20To%20address%20this%2C%20we%20build%0Aupon%20the%20foundation%20laid%20by%20Google%20Brain%27s%20Sensory%20Neuron%2C%20which%20introduced%20a%0Anovel%20neural%20architecture%20for%20reinforcement%20learning%20tasks%20that%20maintained%0Apermutation%20in-variance%20in%20the%20sensory%20neuron%20system.%20While%20the%20baseline%20model%0Ademonstrated%20significant%20performance%20improvements%20over%20traditional%20approaches%2C%0Awe%20identified%20opportunities%20to%20enhance%20the%20efficiency%20of%20the%20learning%20process%0Afurther.%20We%20propose%20a%20modified%20attention%20mechanism%20incorporating%20a%20non-linear%0Atransformation%20of%20the%20key%20vectors%20%28K%29%20using%20a%20mapping%20function%2C%20resulting%20in%20a%0Anew%20set%20of%20key%20vectors%20%28K%27%29.%20This%20non-linear%20mapping%20enhances%20the%0Arepresentational%20capacity%20of%20the%20attention%20mechanism%2C%20allowing%20the%20model%20to%0Aencode%20more%20complex%20feature%20interactions%20and%20accelerating%20convergence%20without%0Acompromising%20performance.%20Our%20enhanced%20model%20demonstrates%20significant%0Aimprovements%20in%20learning%20efficiency%2C%20showcasing%20the%20potential%20for%20non-linear%0Aattention%20mechanisms%20in%20advancing%20reinforcement%20learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Sensory%2520Neurons%253A%2520Nonlinear%2520Attention%2520Mechanisms%2520for%250A%2520%2520Accelerated%2520Convergence%2520in%2520Permutation-Invariant%2520Neural%2520Networks%2520for%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DJunaid%2520Muzaffar%2520and%2520Khubaib%2520Ahmed%2520and%2520Ingo%2520Frommholz%2520and%2520Zeeshan%2520Pervez%2520and%2520Ahsan%2520ul%2520Haq%26entry.1292438233%3D%2520%2520Training%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520often%2520requires%2520significant%250Acomputational%2520resources%2520and%2520extended%2520training%2520times.%2520To%2520address%2520this%252C%2520we%2520build%250Aupon%2520the%2520foundation%2520laid%2520by%2520Google%2520Brain%2527s%2520Sensory%2520Neuron%252C%2520which%2520introduced%2520a%250Anovel%2520neural%2520architecture%2520for%2520reinforcement%2520learning%2520tasks%2520that%2520maintained%250Apermutation%2520in-variance%2520in%2520the%2520sensory%2520neuron%2520system.%2520While%2520the%2520baseline%2520model%250Ademonstrated%2520significant%2520performance%2520improvements%2520over%2520traditional%2520approaches%252C%250Awe%2520identified%2520opportunities%2520to%2520enhance%2520the%2520efficiency%2520of%2520the%2520learning%2520process%250Afurther.%2520We%2520propose%2520a%2520modified%2520attention%2520mechanism%2520incorporating%2520a%2520non-linear%250Atransformation%2520of%2520the%2520key%2520vectors%2520%2528K%2529%2520using%2520a%2520mapping%2520function%252C%2520resulting%2520in%2520a%250Anew%2520set%2520of%2520key%2520vectors%2520%2528K%2527%2529.%2520This%2520non-linear%2520mapping%2520enhances%2520the%250Arepresentational%2520capacity%2520of%2520the%2520attention%2520mechanism%252C%2520allowing%2520the%2520model%2520to%250Aencode%2520more%2520complex%2520feature%2520interactions%2520and%2520accelerating%2520convergence%2520without%250Acompromising%2520performance.%2520Our%2520enhanced%2520model%2520demonstrates%2520significant%250Aimprovements%2520in%2520learning%2520efficiency%252C%2520showcasing%2520the%2520potential%2520for%2520non-linear%250Aattention%2520mechanisms%2520in%2520advancing%2520reinforcement%2520learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Sensory%20Neurons%3A%20Nonlinear%20Attention%20Mechanisms%20for%0A%20%20Accelerated%20Convergence%20in%20Permutation-Invariant%20Neural%20Networks%20for%0A%20%20Reinforcement%20Learning&entry.906535625=Junaid%20Muzaffar%20and%20Khubaib%20Ahmed%20and%20Ingo%20Frommholz%20and%20Zeeshan%20Pervez%20and%20Ahsan%20ul%20Haq&entry.1292438233=%20%20Training%20reinforcement%20learning%20%28RL%29%20agents%20often%20requires%20significant%0Acomputational%20resources%20and%20extended%20training%20times.%20To%20address%20this%2C%20we%20build%0Aupon%20the%20foundation%20laid%20by%20Google%20Brain%27s%20Sensory%20Neuron%2C%20which%20introduced%20a%0Anovel%20neural%20architecture%20for%20reinforcement%20learning%20tasks%20that%20maintained%0Apermutation%20in-variance%20in%20the%20sensory%20neuron%20system.%20While%20the%20baseline%20model%0Ademonstrated%20significant%20performance%20improvements%20over%20traditional%20approaches%2C%0Awe%20identified%20opportunities%20to%20enhance%20the%20efficiency%20of%20the%20learning%20process%0Afurther.%20We%20propose%20a%20modified%20attention%20mechanism%20incorporating%20a%20non-linear%0Atransformation%20of%20the%20key%20vectors%20%28K%29%20using%20a%20mapping%20function%2C%20resulting%20in%20a%0Anew%20set%20of%20key%20vectors%20%28K%27%29.%20This%20non-linear%20mapping%20enhances%20the%0Arepresentational%20capacity%20of%20the%20attention%20mechanism%2C%20allowing%20the%20model%20to%0Aencode%20more%20complex%20feature%20interactions%20and%20accelerating%20convergence%20without%0Acompromising%20performance.%20Our%20enhanced%20model%20demonstrates%20significant%0Aimprovements%20in%20learning%20efficiency%2C%20showcasing%20the%20potential%20for%20non-linear%0Aattention%20mechanisms%20in%20advancing%20reinforcement%20learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00691v2&entry.124074799=Read"},
{"title": "Phase-based Nonlinear Model Predictive Control for Humanoid Walking\n  Stabilization with Single and Double Support Time Adjustments", "author": "Kwanwoo Lee and Gyeongjae Park and Jaeheung Park", "abstract": "  Balance control for humanoid robots has been extensively studied to enable\nrobots to navigate in real-world environments. However, balance controllers\nthat explicitly optimize the durations of both the single support phase, also\nknown as step timing, and the Double Support Phase (DSP) have not been widely\nexplored due to the inherent nonlinearity of the associated optimization\nproblem. Consequently, many recent approaches either ignore the DSP or adjust\nits duration based on heuristics or on linearization techniques that rely on\nsequential coordination of balance strategies. This study proposes a novel\nphase-based nonlinear Model Predictive Control (MPC) framework that\nsimultaneously optimizes Zero Moment Point~(ZMP) modulation, step location,\nstep timing, and DSP duration to maintain balance under external disturbances.\nIn simulation, the proposed controller was compared with two state-of-the-art\nframeworks that rely on heuristics or sequential coordination of balance\nstrategies under two scenarios: forward walking on terrain emulating compliant\nground and external push recovery while walking in place. Overall, the findings\nsuggest that the proposed method offers more flexible coordination of balance\nstrategies than the sequential approach, and consistently outperforms the\nheuristic approach. The robustness and effectiveness of the proposed controller\nwere also validated through experiments with a real humanoid robot.\n", "link": "http://arxiv.org/abs/2506.03856v1", "date": "2025-06-04", "relevancy": 2.1459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5895}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5295}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phase-based%20Nonlinear%20Model%20Predictive%20Control%20for%20Humanoid%20Walking%0A%20%20Stabilization%20with%20Single%20and%20Double%20Support%20Time%20Adjustments&body=Title%3A%20Phase-based%20Nonlinear%20Model%20Predictive%20Control%20for%20Humanoid%20Walking%0A%20%20Stabilization%20with%20Single%20and%20Double%20Support%20Time%20Adjustments%0AAuthor%3A%20Kwanwoo%20Lee%20and%20Gyeongjae%20Park%20and%20Jaeheung%20Park%0AAbstract%3A%20%20%20Balance%20control%20for%20humanoid%20robots%20has%20been%20extensively%20studied%20to%20enable%0Arobots%20to%20navigate%20in%20real-world%20environments.%20However%2C%20balance%20controllers%0Athat%20explicitly%20optimize%20the%20durations%20of%20both%20the%20single%20support%20phase%2C%20also%0Aknown%20as%20step%20timing%2C%20and%20the%20Double%20Support%20Phase%20%28DSP%29%20have%20not%20been%20widely%0Aexplored%20due%20to%20the%20inherent%20nonlinearity%20of%20the%20associated%20optimization%0Aproblem.%20Consequently%2C%20many%20recent%20approaches%20either%20ignore%20the%20DSP%20or%20adjust%0Aits%20duration%20based%20on%20heuristics%20or%20on%20linearization%20techniques%20that%20rely%20on%0Asequential%20coordination%20of%20balance%20strategies.%20This%20study%20proposes%20a%20novel%0Aphase-based%20nonlinear%20Model%20Predictive%20Control%20%28MPC%29%20framework%20that%0Asimultaneously%20optimizes%20Zero%20Moment%20Point~%28ZMP%29%20modulation%2C%20step%20location%2C%0Astep%20timing%2C%20and%20DSP%20duration%20to%20maintain%20balance%20under%20external%20disturbances.%0AIn%20simulation%2C%20the%20proposed%20controller%20was%20compared%20with%20two%20state-of-the-art%0Aframeworks%20that%20rely%20on%20heuristics%20or%20sequential%20coordination%20of%20balance%0Astrategies%20under%20two%20scenarios%3A%20forward%20walking%20on%20terrain%20emulating%20compliant%0Aground%20and%20external%20push%20recovery%20while%20walking%20in%20place.%20Overall%2C%20the%20findings%0Asuggest%20that%20the%20proposed%20method%20offers%20more%20flexible%20coordination%20of%20balance%0Astrategies%20than%20the%20sequential%20approach%2C%20and%20consistently%20outperforms%20the%0Aheuristic%20approach.%20The%20robustness%20and%20effectiveness%20of%20the%20proposed%20controller%0Awere%20also%20validated%20through%20experiments%20with%20a%20real%20humanoid%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhase-based%2520Nonlinear%2520Model%2520Predictive%2520Control%2520for%2520Humanoid%2520Walking%250A%2520%2520Stabilization%2520with%2520Single%2520and%2520Double%2520Support%2520Time%2520Adjustments%26entry.906535625%3DKwanwoo%2520Lee%2520and%2520Gyeongjae%2520Park%2520and%2520Jaeheung%2520Park%26entry.1292438233%3D%2520%2520Balance%2520control%2520for%2520humanoid%2520robots%2520has%2520been%2520extensively%2520studied%2520to%2520enable%250Arobots%2520to%2520navigate%2520in%2520real-world%2520environments.%2520However%252C%2520balance%2520controllers%250Athat%2520explicitly%2520optimize%2520the%2520durations%2520of%2520both%2520the%2520single%2520support%2520phase%252C%2520also%250Aknown%2520as%2520step%2520timing%252C%2520and%2520the%2520Double%2520Support%2520Phase%2520%2528DSP%2529%2520have%2520not%2520been%2520widely%250Aexplored%2520due%2520to%2520the%2520inherent%2520nonlinearity%2520of%2520the%2520associated%2520optimization%250Aproblem.%2520Consequently%252C%2520many%2520recent%2520approaches%2520either%2520ignore%2520the%2520DSP%2520or%2520adjust%250Aits%2520duration%2520based%2520on%2520heuristics%2520or%2520on%2520linearization%2520techniques%2520that%2520rely%2520on%250Asequential%2520coordination%2520of%2520balance%2520strategies.%2520This%2520study%2520proposes%2520a%2520novel%250Aphase-based%2520nonlinear%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520framework%2520that%250Asimultaneously%2520optimizes%2520Zero%2520Moment%2520Point~%2528ZMP%2529%2520modulation%252C%2520step%2520location%252C%250Astep%2520timing%252C%2520and%2520DSP%2520duration%2520to%2520maintain%2520balance%2520under%2520external%2520disturbances.%250AIn%2520simulation%252C%2520the%2520proposed%2520controller%2520was%2520compared%2520with%2520two%2520state-of-the-art%250Aframeworks%2520that%2520rely%2520on%2520heuristics%2520or%2520sequential%2520coordination%2520of%2520balance%250Astrategies%2520under%2520two%2520scenarios%253A%2520forward%2520walking%2520on%2520terrain%2520emulating%2520compliant%250Aground%2520and%2520external%2520push%2520recovery%2520while%2520walking%2520in%2520place.%2520Overall%252C%2520the%2520findings%250Asuggest%2520that%2520the%2520proposed%2520method%2520offers%2520more%2520flexible%2520coordination%2520of%2520balance%250Astrategies%2520than%2520the%2520sequential%2520approach%252C%2520and%2520consistently%2520outperforms%2520the%250Aheuristic%2520approach.%2520The%2520robustness%2520and%2520effectiveness%2520of%2520the%2520proposed%2520controller%250Awere%2520also%2520validated%2520through%2520experiments%2520with%2520a%2520real%2520humanoid%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phase-based%20Nonlinear%20Model%20Predictive%20Control%20for%20Humanoid%20Walking%0A%20%20Stabilization%20with%20Single%20and%20Double%20Support%20Time%20Adjustments&entry.906535625=Kwanwoo%20Lee%20and%20Gyeongjae%20Park%20and%20Jaeheung%20Park&entry.1292438233=%20%20Balance%20control%20for%20humanoid%20robots%20has%20been%20extensively%20studied%20to%20enable%0Arobots%20to%20navigate%20in%20real-world%20environments.%20However%2C%20balance%20controllers%0Athat%20explicitly%20optimize%20the%20durations%20of%20both%20the%20single%20support%20phase%2C%20also%0Aknown%20as%20step%20timing%2C%20and%20the%20Double%20Support%20Phase%20%28DSP%29%20have%20not%20been%20widely%0Aexplored%20due%20to%20the%20inherent%20nonlinearity%20of%20the%20associated%20optimization%0Aproblem.%20Consequently%2C%20many%20recent%20approaches%20either%20ignore%20the%20DSP%20or%20adjust%0Aits%20duration%20based%20on%20heuristics%20or%20on%20linearization%20techniques%20that%20rely%20on%0Asequential%20coordination%20of%20balance%20strategies.%20This%20study%20proposes%20a%20novel%0Aphase-based%20nonlinear%20Model%20Predictive%20Control%20%28MPC%29%20framework%20that%0Asimultaneously%20optimizes%20Zero%20Moment%20Point~%28ZMP%29%20modulation%2C%20step%20location%2C%0Astep%20timing%2C%20and%20DSP%20duration%20to%20maintain%20balance%20under%20external%20disturbances.%0AIn%20simulation%2C%20the%20proposed%20controller%20was%20compared%20with%20two%20state-of-the-art%0Aframeworks%20that%20rely%20on%20heuristics%20or%20sequential%20coordination%20of%20balance%0Astrategies%20under%20two%20scenarios%3A%20forward%20walking%20on%20terrain%20emulating%20compliant%0Aground%20and%20external%20push%20recovery%20while%20walking%20in%20place.%20Overall%2C%20the%20findings%0Asuggest%20that%20the%20proposed%20method%20offers%20more%20flexible%20coordination%20of%20balance%0Astrategies%20than%20the%20sequential%20approach%2C%20and%20consistently%20outperforms%20the%0Aheuristic%20approach.%20The%20robustness%20and%20effectiveness%20of%20the%20proposed%20controller%0Awere%20also%20validated%20through%20experiments%20with%20a%20real%20humanoid%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03856v1&entry.124074799=Read"},
{"title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models", "author": "Lingyu Li and Yixu Wang and Haiquan Zhao and Shuqi Kong and Yan Teng and Chunbo Li and Yingchun Wang", "abstract": "  With large language models (LLMs) increasingly deployed as cognitive engines\nfor AI agents, the reliability and effectiveness critically hinge on their\nintrinsic epistemic agency, which remains understudied. Epistemic agency, the\nability to flexibly construct, adapt, and monitor beliefs about dynamic\nenvironments, represents a base-model-level capacity independent of specific\ntools, modules, or applications. We characterize the holistic process\nunderlying epistemic agency, which unfolds in seven interrelated dimensions:\nprediction, decision-making, perception, memory, counterfactual thinking,\nbelief updating, and meta-reflection. Correspondingly, we propose\nReflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven\ntasks with long-term relevance and minimization of data leakage. Through a\ncomprehensive evaluation of 16 models using three prompting strategies, we\nidentify a clear three-tier performance hierarchy and significant limitations\nof current LLMs, particularly in meta-reflection capabilities. While\nstate-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our\nfindings suggest several promising research directions, including enhancing\ncore cognitive functions, improving cross-functional coordination, and\ndeveloping adaptive processing mechanisms. Our code and data are available at\nhttps://github.com/AI45Lab/ReflectionBench.\n", "link": "http://arxiv.org/abs/2410.16270v3", "date": "2025-06-04", "relevancy": 2.14, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflection-Bench%3A%20Evaluating%20Epistemic%20Agency%20in%20Large%20Language%20Models&body=Title%3A%20Reflection-Bench%3A%20Evaluating%20Epistemic%20Agency%20in%20Large%20Language%20Models%0AAuthor%3A%20Lingyu%20Li%20and%20Yixu%20Wang%20and%20Haiquan%20Zhao%20and%20Shuqi%20Kong%20and%20Yan%20Teng%20and%20Chunbo%20Li%20and%20Yingchun%20Wang%0AAbstract%3A%20%20%20With%20large%20language%20models%20%28LLMs%29%20increasingly%20deployed%20as%20cognitive%20engines%0Afor%20AI%20agents%2C%20the%20reliability%20and%20effectiveness%20critically%20hinge%20on%20their%0Aintrinsic%20epistemic%20agency%2C%20which%20remains%20understudied.%20Epistemic%20agency%2C%20the%0Aability%20to%20flexibly%20construct%2C%20adapt%2C%20and%20monitor%20beliefs%20about%20dynamic%0Aenvironments%2C%20represents%20a%20base-model-level%20capacity%20independent%20of%20specific%0Atools%2C%20modules%2C%20or%20applications.%20We%20characterize%20the%20holistic%20process%0Aunderlying%20epistemic%20agency%2C%20which%20unfolds%20in%20seven%20interrelated%20dimensions%3A%0Aprediction%2C%20decision-making%2C%20perception%2C%20memory%2C%20counterfactual%20thinking%2C%0Abelief%20updating%2C%20and%20meta-reflection.%20Correspondingly%2C%20we%20propose%0AReflection-Bench%2C%20a%20cognitive-psychology-inspired%20benchmark%20consisting%20of%20seven%0Atasks%20with%20long-term%20relevance%20and%20minimization%20of%20data%20leakage.%20Through%20a%0Acomprehensive%20evaluation%20of%2016%20models%20using%20three%20prompting%20strategies%2C%20we%0Aidentify%20a%20clear%20three-tier%20performance%20hierarchy%20and%20significant%20limitations%0Aof%20current%20LLMs%2C%20particularly%20in%20meta-reflection%20capabilities.%20While%0Astate-of-the-art%20LLMs%20demonstrate%20rudimentary%20signs%20of%20epistemic%20agency%2C%20our%0Afindings%20suggest%20several%20promising%20research%20directions%2C%20including%20enhancing%0Acore%20cognitive%20functions%2C%20improving%20cross-functional%20coordination%2C%20and%0Adeveloping%20adaptive%20processing%20mechanisms.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/AI45Lab/ReflectionBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16270v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflection-Bench%253A%2520Evaluating%2520Epistemic%2520Agency%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DLingyu%2520Li%2520and%2520Yixu%2520Wang%2520and%2520Haiquan%2520Zhao%2520and%2520Shuqi%2520Kong%2520and%2520Yan%2520Teng%2520and%2520Chunbo%2520Li%2520and%2520Yingchun%2520Wang%26entry.1292438233%3D%2520%2520With%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520deployed%2520as%2520cognitive%2520engines%250Afor%2520AI%2520agents%252C%2520the%2520reliability%2520and%2520effectiveness%2520critically%2520hinge%2520on%2520their%250Aintrinsic%2520epistemic%2520agency%252C%2520which%2520remains%2520understudied.%2520Epistemic%2520agency%252C%2520the%250Aability%2520to%2520flexibly%2520construct%252C%2520adapt%252C%2520and%2520monitor%2520beliefs%2520about%2520dynamic%250Aenvironments%252C%2520represents%2520a%2520base-model-level%2520capacity%2520independent%2520of%2520specific%250Atools%252C%2520modules%252C%2520or%2520applications.%2520We%2520characterize%2520the%2520holistic%2520process%250Aunderlying%2520epistemic%2520agency%252C%2520which%2520unfolds%2520in%2520seven%2520interrelated%2520dimensions%253A%250Aprediction%252C%2520decision-making%252C%2520perception%252C%2520memory%252C%2520counterfactual%2520thinking%252C%250Abelief%2520updating%252C%2520and%2520meta-reflection.%2520Correspondingly%252C%2520we%2520propose%250AReflection-Bench%252C%2520a%2520cognitive-psychology-inspired%2520benchmark%2520consisting%2520of%2520seven%250Atasks%2520with%2520long-term%2520relevance%2520and%2520minimization%2520of%2520data%2520leakage.%2520Through%2520a%250Acomprehensive%2520evaluation%2520of%252016%2520models%2520using%2520three%2520prompting%2520strategies%252C%2520we%250Aidentify%2520a%2520clear%2520three-tier%2520performance%2520hierarchy%2520and%2520significant%2520limitations%250Aof%2520current%2520LLMs%252C%2520particularly%2520in%2520meta-reflection%2520capabilities.%2520While%250Astate-of-the-art%2520LLMs%2520demonstrate%2520rudimentary%2520signs%2520of%2520epistemic%2520agency%252C%2520our%250Afindings%2520suggest%2520several%2520promising%2520research%2520directions%252C%2520including%2520enhancing%250Acore%2520cognitive%2520functions%252C%2520improving%2520cross-functional%2520coordination%252C%2520and%250Adeveloping%2520adaptive%2520processing%2520mechanisms.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/AI45Lab/ReflectionBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16270v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflection-Bench%3A%20Evaluating%20Epistemic%20Agency%20in%20Large%20Language%20Models&entry.906535625=Lingyu%20Li%20and%20Yixu%20Wang%20and%20Haiquan%20Zhao%20and%20Shuqi%20Kong%20and%20Yan%20Teng%20and%20Chunbo%20Li%20and%20Yingchun%20Wang&entry.1292438233=%20%20With%20large%20language%20models%20%28LLMs%29%20increasingly%20deployed%20as%20cognitive%20engines%0Afor%20AI%20agents%2C%20the%20reliability%20and%20effectiveness%20critically%20hinge%20on%20their%0Aintrinsic%20epistemic%20agency%2C%20which%20remains%20understudied.%20Epistemic%20agency%2C%20the%0Aability%20to%20flexibly%20construct%2C%20adapt%2C%20and%20monitor%20beliefs%20about%20dynamic%0Aenvironments%2C%20represents%20a%20base-model-level%20capacity%20independent%20of%20specific%0Atools%2C%20modules%2C%20or%20applications.%20We%20characterize%20the%20holistic%20process%0Aunderlying%20epistemic%20agency%2C%20which%20unfolds%20in%20seven%20interrelated%20dimensions%3A%0Aprediction%2C%20decision-making%2C%20perception%2C%20memory%2C%20counterfactual%20thinking%2C%0Abelief%20updating%2C%20and%20meta-reflection.%20Correspondingly%2C%20we%20propose%0AReflection-Bench%2C%20a%20cognitive-psychology-inspired%20benchmark%20consisting%20of%20seven%0Atasks%20with%20long-term%20relevance%20and%20minimization%20of%20data%20leakage.%20Through%20a%0Acomprehensive%20evaluation%20of%2016%20models%20using%20three%20prompting%20strategies%2C%20we%0Aidentify%20a%20clear%20three-tier%20performance%20hierarchy%20and%20significant%20limitations%0Aof%20current%20LLMs%2C%20particularly%20in%20meta-reflection%20capabilities.%20While%0Astate-of-the-art%20LLMs%20demonstrate%20rudimentary%20signs%20of%20epistemic%20agency%2C%20our%0Afindings%20suggest%20several%20promising%20research%20directions%2C%20including%20enhancing%0Acore%20cognitive%20functions%2C%20improving%20cross-functional%20coordination%2C%20and%0Adeveloping%20adaptive%20processing%20mechanisms.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/AI45Lab/ReflectionBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16270v3&entry.124074799=Read"},
{"title": "CARL: Camera-Agnostic Representation Learning for Spectral Image\n  Analysis", "author": "Alexander Baumann and Leonardo Ayala and Silvia Seidlitz and Jan Sellner and Alexander Studier-Fischer and Berkin \u00d6zdemir and Lena Maier-Hein and Slobodan Ilic", "abstract": "  Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.\n", "link": "http://arxiv.org/abs/2504.19223v2", "date": "2025-06-04", "relevancy": 2.1321, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5384}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5324}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARL%3A%20Camera-Agnostic%20Representation%20Learning%20for%20Spectral%20Image%0A%20%20Analysis&body=Title%3A%20CARL%3A%20Camera-Agnostic%20Representation%20Learning%20for%20Spectral%20Image%0A%20%20Analysis%0AAuthor%3A%20Alexander%20Baumann%20and%20Leonardo%20Ayala%20and%20Silvia%20Seidlitz%20and%20Jan%20Sellner%20and%20Alexander%20Studier-Fischer%20and%20Berkin%20%C3%96zdemir%20and%20Lena%20Maier-Hein%20and%20Slobodan%20Ilic%0AAbstract%3A%20%20%20Spectral%20imaging%20offers%20promising%20applications%20across%20diverse%20domains%2C%0Aincluding%20medicine%20and%20urban%20scene%20understanding%2C%20and%20is%20already%20established%20as%0Aa%20critical%20modality%20in%20remote%20sensing.%20However%2C%20variability%20in%20channel%0Adimensionality%20and%20captured%20wavelengths%20among%20spectral%20cameras%20impede%20the%0Adevelopment%20of%20AI-driven%20methodologies%2C%20leading%20to%20camera-specific%20models%20with%0Alimited%20generalizability%20and%20inadequate%20cross-camera%20applicability.%20To%20address%0Athis%20bottleneck%2C%20we%20introduce%20%24%5Ctextbf%7BCARL%7D%24%2C%20a%20model%20for%0A%24%5Ctextbf%7BC%7D%24amera-%24%5Ctextbf%7BA%7D%24gnostic%20%24%5Ctextbf%7BR%7D%24epresentation%0A%24%5Ctextbf%7BL%7D%24earning%20across%20RGB%2C%20multispectral%2C%20and%20hyperspectral%20imaging%0Amodalities.%20To%20enable%20the%20conversion%20of%20a%20spectral%20image%20with%20any%20channel%0Adimensionality%20to%20a%20camera-agnostic%20embedding%2C%20we%20introduce%20wavelength%0Apositional%20encoding%20and%20a%20self-attention-cross-attention%20mechanism%20to%20compress%0Aspectral%20information%20into%20learned%20query%20representations.%20Spectral-spatial%0Apre-training%20is%20achieved%20with%20a%20novel%20spectral%20self-supervised%20JEPA-inspired%0Astrategy%20tailored%20to%20CARL.%20Large-scale%20experiments%20across%20the%20domains%20of%0Amedical%20imaging%2C%20autonomous%20driving%2C%20and%20satellite%20imaging%20demonstrate%20our%0Amodel%27s%20unique%20robustness%20to%20spectral%20heterogeneity%2C%20outperforming%20on%20datasets%0Awith%20simulated%20and%20real-world%20cross-camera%20spectral%20variations.%20The%20scalability%0Aand%20versatility%20of%20the%20proposed%20approach%20position%20our%20model%20as%20a%20backbone%20for%0Afuture%20spectral%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARL%253A%2520Camera-Agnostic%2520Representation%2520Learning%2520for%2520Spectral%2520Image%250A%2520%2520Analysis%26entry.906535625%3DAlexander%2520Baumann%2520and%2520Leonardo%2520Ayala%2520and%2520Silvia%2520Seidlitz%2520and%2520Jan%2520Sellner%2520and%2520Alexander%2520Studier-Fischer%2520and%2520Berkin%2520%25C3%2596zdemir%2520and%2520Lena%2520Maier-Hein%2520and%2520Slobodan%2520Ilic%26entry.1292438233%3D%2520%2520Spectral%2520imaging%2520offers%2520promising%2520applications%2520across%2520diverse%2520domains%252C%250Aincluding%2520medicine%2520and%2520urban%2520scene%2520understanding%252C%2520and%2520is%2520already%2520established%2520as%250Aa%2520critical%2520modality%2520in%2520remote%2520sensing.%2520However%252C%2520variability%2520in%2520channel%250Adimensionality%2520and%2520captured%2520wavelengths%2520among%2520spectral%2520cameras%2520impede%2520the%250Adevelopment%2520of%2520AI-driven%2520methodologies%252C%2520leading%2520to%2520camera-specific%2520models%2520with%250Alimited%2520generalizability%2520and%2520inadequate%2520cross-camera%2520applicability.%2520To%2520address%250Athis%2520bottleneck%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BCARL%257D%2524%252C%2520a%2520model%2520for%250A%2524%255Ctextbf%257BC%257D%2524amera-%2524%255Ctextbf%257BA%257D%2524gnostic%2520%2524%255Ctextbf%257BR%257D%2524epresentation%250A%2524%255Ctextbf%257BL%257D%2524earning%2520across%2520RGB%252C%2520multispectral%252C%2520and%2520hyperspectral%2520imaging%250Amodalities.%2520To%2520enable%2520the%2520conversion%2520of%2520a%2520spectral%2520image%2520with%2520any%2520channel%250Adimensionality%2520to%2520a%2520camera-agnostic%2520embedding%252C%2520we%2520introduce%2520wavelength%250Apositional%2520encoding%2520and%2520a%2520self-attention-cross-attention%2520mechanism%2520to%2520compress%250Aspectral%2520information%2520into%2520learned%2520query%2520representations.%2520Spectral-spatial%250Apre-training%2520is%2520achieved%2520with%2520a%2520novel%2520spectral%2520self-supervised%2520JEPA-inspired%250Astrategy%2520tailored%2520to%2520CARL.%2520Large-scale%2520experiments%2520across%2520the%2520domains%2520of%250Amedical%2520imaging%252C%2520autonomous%2520driving%252C%2520and%2520satellite%2520imaging%2520demonstrate%2520our%250Amodel%2527s%2520unique%2520robustness%2520to%2520spectral%2520heterogeneity%252C%2520outperforming%2520on%2520datasets%250Awith%2520simulated%2520and%2520real-world%2520cross-camera%2520spectral%2520variations.%2520The%2520scalability%250Aand%2520versatility%2520of%2520the%2520proposed%2520approach%2520position%2520our%2520model%2520as%2520a%2520backbone%2520for%250Afuture%2520spectral%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARL%3A%20Camera-Agnostic%20Representation%20Learning%20for%20Spectral%20Image%0A%20%20Analysis&entry.906535625=Alexander%20Baumann%20and%20Leonardo%20Ayala%20and%20Silvia%20Seidlitz%20and%20Jan%20Sellner%20and%20Alexander%20Studier-Fischer%20and%20Berkin%20%C3%96zdemir%20and%20Lena%20Maier-Hein%20and%20Slobodan%20Ilic&entry.1292438233=%20%20Spectral%20imaging%20offers%20promising%20applications%20across%20diverse%20domains%2C%0Aincluding%20medicine%20and%20urban%20scene%20understanding%2C%20and%20is%20already%20established%20as%0Aa%20critical%20modality%20in%20remote%20sensing.%20However%2C%20variability%20in%20channel%0Adimensionality%20and%20captured%20wavelengths%20among%20spectral%20cameras%20impede%20the%0Adevelopment%20of%20AI-driven%20methodologies%2C%20leading%20to%20camera-specific%20models%20with%0Alimited%20generalizability%20and%20inadequate%20cross-camera%20applicability.%20To%20address%0Athis%20bottleneck%2C%20we%20introduce%20%24%5Ctextbf%7BCARL%7D%24%2C%20a%20model%20for%0A%24%5Ctextbf%7BC%7D%24amera-%24%5Ctextbf%7BA%7D%24gnostic%20%24%5Ctextbf%7BR%7D%24epresentation%0A%24%5Ctextbf%7BL%7D%24earning%20across%20RGB%2C%20multispectral%2C%20and%20hyperspectral%20imaging%0Amodalities.%20To%20enable%20the%20conversion%20of%20a%20spectral%20image%20with%20any%20channel%0Adimensionality%20to%20a%20camera-agnostic%20embedding%2C%20we%20introduce%20wavelength%0Apositional%20encoding%20and%20a%20self-attention-cross-attention%20mechanism%20to%20compress%0Aspectral%20information%20into%20learned%20query%20representations.%20Spectral-spatial%0Apre-training%20is%20achieved%20with%20a%20novel%20spectral%20self-supervised%20JEPA-inspired%0Astrategy%20tailored%20to%20CARL.%20Large-scale%20experiments%20across%20the%20domains%20of%0Amedical%20imaging%2C%20autonomous%20driving%2C%20and%20satellite%20imaging%20demonstrate%20our%0Amodel%27s%20unique%20robustness%20to%20spectral%20heterogeneity%2C%20outperforming%20on%20datasets%0Awith%20simulated%20and%20real-world%20cross-camera%20spectral%20variations.%20The%20scalability%0Aand%20versatility%20of%20the%20proposed%20approach%20position%20our%20model%20as%20a%20backbone%20for%0Afuture%20spectral%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19223v2&entry.124074799=Read"},
{"title": "Children's Voice Privacy: First Steps And Emerging Challenges", "author": "Ajinkya Kulkarni and Francisco Teixeira and Enno Hermann and Thomas Rolland and Isabel Trancoso and Mathew Magimai Doss", "abstract": "  Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research.\n", "link": "http://arxiv.org/abs/2506.00100v2", "date": "2025-06-04", "relevancy": 2.1278, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Children%27s%20Voice%20Privacy%3A%20First%20Steps%20And%20Emerging%20Challenges&body=Title%3A%20Children%27s%20Voice%20Privacy%3A%20First%20Steps%20And%20Emerging%20Challenges%0AAuthor%3A%20Ajinkya%20Kulkarni%20and%20Francisco%20Teixeira%20and%20Enno%20Hermann%20and%20Thomas%20Rolland%20and%20Isabel%20Trancoso%20and%20Mathew%20Magimai%20Doss%0AAbstract%3A%20%20%20Children%20are%20one%20of%20the%20most%20under-represented%20groups%20in%20speech%20technologies%2C%0Aas%20well%20as%20one%20of%20the%20most%20vulnerable%20in%20terms%20of%20privacy.%20Despite%20this%2C%0Aanonymization%20techniques%20targeting%20this%20population%20have%20received%20little%0Aattention.%20In%20this%20study%2C%20we%20seek%20to%20bridge%20this%20gap%2C%20and%20establish%20a%20baseline%0Afor%20the%20use%20of%20voice%20anonymization%20techniques%20designed%20for%20adult%20speech%20when%0Aapplied%20to%20children%27s%20voices.%20Such%20an%20evaluation%20is%20essential%2C%20as%20children%27s%0Aspeech%20presents%20a%20distinct%20set%20of%20challenges%20when%20compared%20to%20that%20of%20adults.%0AThis%20study%20comprises%20three%20children%27s%20datasets%2C%20six%20anonymization%20methods%2C%20and%0Aobjective%20and%20subjective%20utility%20metrics%20for%20evaluation.%20Our%20results%20show%20that%0Aexisting%20systems%20for%20adults%20are%20still%20able%20to%20protect%20children%27s%20voice%20privacy%2C%0Abut%20suffer%20from%20much%20higher%20utility%20degradation.%20In%20addition%2C%20our%20subjective%0Astudy%20displays%20the%20challenges%20of%20automatic%20evaluation%20methods%20for%20speech%0Aquality%20in%20children%27s%20speech%2C%20highlighting%20the%20need%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChildren%2527s%2520Voice%2520Privacy%253A%2520First%2520Steps%2520And%2520Emerging%2520Challenges%26entry.906535625%3DAjinkya%2520Kulkarni%2520and%2520Francisco%2520Teixeira%2520and%2520Enno%2520Hermann%2520and%2520Thomas%2520Rolland%2520and%2520Isabel%2520Trancoso%2520and%2520Mathew%2520Magimai%2520Doss%26entry.1292438233%3D%2520%2520Children%2520are%2520one%2520of%2520the%2520most%2520under-represented%2520groups%2520in%2520speech%2520technologies%252C%250Aas%2520well%2520as%2520one%2520of%2520the%2520most%2520vulnerable%2520in%2520terms%2520of%2520privacy.%2520Despite%2520this%252C%250Aanonymization%2520techniques%2520targeting%2520this%2520population%2520have%2520received%2520little%250Aattention.%2520In%2520this%2520study%252C%2520we%2520seek%2520to%2520bridge%2520this%2520gap%252C%2520and%2520establish%2520a%2520baseline%250Afor%2520the%2520use%2520of%2520voice%2520anonymization%2520techniques%2520designed%2520for%2520adult%2520speech%2520when%250Aapplied%2520to%2520children%2527s%2520voices.%2520Such%2520an%2520evaluation%2520is%2520essential%252C%2520as%2520children%2527s%250Aspeech%2520presents%2520a%2520distinct%2520set%2520of%2520challenges%2520when%2520compared%2520to%2520that%2520of%2520adults.%250AThis%2520study%2520comprises%2520three%2520children%2527s%2520datasets%252C%2520six%2520anonymization%2520methods%252C%2520and%250Aobjective%2520and%2520subjective%2520utility%2520metrics%2520for%2520evaluation.%2520Our%2520results%2520show%2520that%250Aexisting%2520systems%2520for%2520adults%2520are%2520still%2520able%2520to%2520protect%2520children%2527s%2520voice%2520privacy%252C%250Abut%2520suffer%2520from%2520much%2520higher%2520utility%2520degradation.%2520In%2520addition%252C%2520our%2520subjective%250Astudy%2520displays%2520the%2520challenges%2520of%2520automatic%2520evaluation%2520methods%2520for%2520speech%250Aquality%2520in%2520children%2527s%2520speech%252C%2520highlighting%2520the%2520need%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Children%27s%20Voice%20Privacy%3A%20First%20Steps%20And%20Emerging%20Challenges&entry.906535625=Ajinkya%20Kulkarni%20and%20Francisco%20Teixeira%20and%20Enno%20Hermann%20and%20Thomas%20Rolland%20and%20Isabel%20Trancoso%20and%20Mathew%20Magimai%20Doss&entry.1292438233=%20%20Children%20are%20one%20of%20the%20most%20under-represented%20groups%20in%20speech%20technologies%2C%0Aas%20well%20as%20one%20of%20the%20most%20vulnerable%20in%20terms%20of%20privacy.%20Despite%20this%2C%0Aanonymization%20techniques%20targeting%20this%20population%20have%20received%20little%0Aattention.%20In%20this%20study%2C%20we%20seek%20to%20bridge%20this%20gap%2C%20and%20establish%20a%20baseline%0Afor%20the%20use%20of%20voice%20anonymization%20techniques%20designed%20for%20adult%20speech%20when%0Aapplied%20to%20children%27s%20voices.%20Such%20an%20evaluation%20is%20essential%2C%20as%20children%27s%0Aspeech%20presents%20a%20distinct%20set%20of%20challenges%20when%20compared%20to%20that%20of%20adults.%0AThis%20study%20comprises%20three%20children%27s%20datasets%2C%20six%20anonymization%20methods%2C%20and%0Aobjective%20and%20subjective%20utility%20metrics%20for%20evaluation.%20Our%20results%20show%20that%0Aexisting%20systems%20for%20adults%20are%20still%20able%20to%20protect%20children%27s%20voice%20privacy%2C%0Abut%20suffer%20from%20much%20higher%20utility%20degradation.%20In%20addition%2C%20our%20subjective%0Astudy%20displays%20the%20challenges%20of%20automatic%20evaluation%20methods%20for%20speech%0Aquality%20in%20children%27s%20speech%2C%20highlighting%20the%20need%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00100v2&entry.124074799=Read"},
{"title": "Rapid Bone Scintigraphy Enhancement via Semantic Prior Distillation from\n  Segment Anything Model", "author": "Pengchen Liang and Leijun Shi and Huiping Yao and Bin Pu and Jianguo Chen and Lei Zhao and Haishan Huang and Zhuangzhuang Chen and Zhaozhao Xu and Lite Xu and Qing Chang and Yiwei Li", "abstract": "  Rapid bone scintigraphy is crucial for diagnosing skeletal disorders and\ndetecting tumor metastases in children, as it shortens scan duration and\nreduces discomfort. However, accelerated acquisition often degrades image\nquality, impairing the visibility of fine anatomical details and potentially\ncompromising diagnosis. To overcome this limitation, we introduce the first\napplication of SAM-based semantic priors for medical image restoration,\nutilizing the Segment Anything Model (SAM) to enhance pediatric rapid bone\nscintigraphy. Our approach employs two cascaded networks, $f^{IR1}$ and\n$f^{IR2}$, supported by three specialized modules: a Semantic Prior Integration\n(SPI) module, a Semantic Knowledge Distillation (SKD) module, and a Semantic\nConsistency Module (SCM). The SPI and SKD modules inject domain-specific\nsemantic cues from a fine-tuned SAM, while the SCM preserves coherent semantic\nfeature representations across both cascaded stages. Moreover, we present RBS,\na novel Rapid Bone Scintigraphy dataset comprising paired standard (20 cm/min)\nand rapid (40 cm/min) scans from 137 pediatric patients aged 0.5 - 16 years,\nmaking it the first dataset tailored for pediatric rapid bone scintigraphy\nrestoration. Extensive experiments on both a public endoscopic dataset and our\nRBS dataset demonstrate that our method consistently surpasses existing\ntechniques in PSNR, SSIM, FID, and LPIPS metrics.\n", "link": "http://arxiv.org/abs/2503.02321v3", "date": "2025-06-04", "relevancy": 2.1218, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5428}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5385}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Bone%20Scintigraphy%20Enhancement%20via%20Semantic%20Prior%20Distillation%20from%0A%20%20Segment%20Anything%20Model&body=Title%3A%20Rapid%20Bone%20Scintigraphy%20Enhancement%20via%20Semantic%20Prior%20Distillation%20from%0A%20%20Segment%20Anything%20Model%0AAuthor%3A%20Pengchen%20Liang%20and%20Leijun%20Shi%20and%20Huiping%20Yao%20and%20Bin%20Pu%20and%20Jianguo%20Chen%20and%20Lei%20Zhao%20and%20Haishan%20Huang%20and%20Zhuangzhuang%20Chen%20and%20Zhaozhao%20Xu%20and%20Lite%20Xu%20and%20Qing%20Chang%20and%20Yiwei%20Li%0AAbstract%3A%20%20%20Rapid%20bone%20scintigraphy%20is%20crucial%20for%20diagnosing%20skeletal%20disorders%20and%0Adetecting%20tumor%20metastases%20in%20children%2C%20as%20it%20shortens%20scan%20duration%20and%0Areduces%20discomfort.%20However%2C%20accelerated%20acquisition%20often%20degrades%20image%0Aquality%2C%20impairing%20the%20visibility%20of%20fine%20anatomical%20details%20and%20potentially%0Acompromising%20diagnosis.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20the%20first%0Aapplication%20of%20SAM-based%20semantic%20priors%20for%20medical%20image%20restoration%2C%0Autilizing%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20pediatric%20rapid%20bone%0Ascintigraphy.%20Our%20approach%20employs%20two%20cascaded%20networks%2C%20%24f%5E%7BIR1%7D%24%20and%0A%24f%5E%7BIR2%7D%24%2C%20supported%20by%20three%20specialized%20modules%3A%20a%20Semantic%20Prior%20Integration%0A%28SPI%29%20module%2C%20a%20Semantic%20Knowledge%20Distillation%20%28SKD%29%20module%2C%20and%20a%20Semantic%0AConsistency%20Module%20%28SCM%29.%20The%20SPI%20and%20SKD%20modules%20inject%20domain-specific%0Asemantic%20cues%20from%20a%20fine-tuned%20SAM%2C%20while%20the%20SCM%20preserves%20coherent%20semantic%0Afeature%20representations%20across%20both%20cascaded%20stages.%20Moreover%2C%20we%20present%20RBS%2C%0Aa%20novel%20Rapid%20Bone%20Scintigraphy%20dataset%20comprising%20paired%20standard%20%2820%20cm/min%29%0Aand%20rapid%20%2840%20cm/min%29%20scans%20from%20137%20pediatric%20patients%20aged%200.5%20-%2016%20years%2C%0Amaking%20it%20the%20first%20dataset%20tailored%20for%20pediatric%20rapid%20bone%20scintigraphy%0Arestoration.%20Extensive%20experiments%20on%20both%20a%20public%20endoscopic%20dataset%20and%20our%0ARBS%20dataset%20demonstrate%20that%20our%20method%20consistently%20surpasses%20existing%0Atechniques%20in%20PSNR%2C%20SSIM%2C%20FID%2C%20and%20LPIPS%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02321v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Bone%2520Scintigraphy%2520Enhancement%2520via%2520Semantic%2520Prior%2520Distillation%2520from%250A%2520%2520Segment%2520Anything%2520Model%26entry.906535625%3DPengchen%2520Liang%2520and%2520Leijun%2520Shi%2520and%2520Huiping%2520Yao%2520and%2520Bin%2520Pu%2520and%2520Jianguo%2520Chen%2520and%2520Lei%2520Zhao%2520and%2520Haishan%2520Huang%2520and%2520Zhuangzhuang%2520Chen%2520and%2520Zhaozhao%2520Xu%2520and%2520Lite%2520Xu%2520and%2520Qing%2520Chang%2520and%2520Yiwei%2520Li%26entry.1292438233%3D%2520%2520Rapid%2520bone%2520scintigraphy%2520is%2520crucial%2520for%2520diagnosing%2520skeletal%2520disorders%2520and%250Adetecting%2520tumor%2520metastases%2520in%2520children%252C%2520as%2520it%2520shortens%2520scan%2520duration%2520and%250Areduces%2520discomfort.%2520However%252C%2520accelerated%2520acquisition%2520often%2520degrades%2520image%250Aquality%252C%2520impairing%2520the%2520visibility%2520of%2520fine%2520anatomical%2520details%2520and%2520potentially%250Acompromising%2520diagnosis.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520first%250Aapplication%2520of%2520SAM-based%2520semantic%2520priors%2520for%2520medical%2520image%2520restoration%252C%250Autilizing%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520enhance%2520pediatric%2520rapid%2520bone%250Ascintigraphy.%2520Our%2520approach%2520employs%2520two%2520cascaded%2520networks%252C%2520%2524f%255E%257BIR1%257D%2524%2520and%250A%2524f%255E%257BIR2%257D%2524%252C%2520supported%2520by%2520three%2520specialized%2520modules%253A%2520a%2520Semantic%2520Prior%2520Integration%250A%2528SPI%2529%2520module%252C%2520a%2520Semantic%2520Knowledge%2520Distillation%2520%2528SKD%2529%2520module%252C%2520and%2520a%2520Semantic%250AConsistency%2520Module%2520%2528SCM%2529.%2520The%2520SPI%2520and%2520SKD%2520modules%2520inject%2520domain-specific%250Asemantic%2520cues%2520from%2520a%2520fine-tuned%2520SAM%252C%2520while%2520the%2520SCM%2520preserves%2520coherent%2520semantic%250Afeature%2520representations%2520across%2520both%2520cascaded%2520stages.%2520Moreover%252C%2520we%2520present%2520RBS%252C%250Aa%2520novel%2520Rapid%2520Bone%2520Scintigraphy%2520dataset%2520comprising%2520paired%2520standard%2520%252820%2520cm/min%2529%250Aand%2520rapid%2520%252840%2520cm/min%2529%2520scans%2520from%2520137%2520pediatric%2520patients%2520aged%25200.5%2520-%252016%2520years%252C%250Amaking%2520it%2520the%2520first%2520dataset%2520tailored%2520for%2520pediatric%2520rapid%2520bone%2520scintigraphy%250Arestoration.%2520Extensive%2520experiments%2520on%2520both%2520a%2520public%2520endoscopic%2520dataset%2520and%2520our%250ARBS%2520dataset%2520demonstrate%2520that%2520our%2520method%2520consistently%2520surpasses%2520existing%250Atechniques%2520in%2520PSNR%252C%2520SSIM%252C%2520FID%252C%2520and%2520LPIPS%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02321v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Bone%20Scintigraphy%20Enhancement%20via%20Semantic%20Prior%20Distillation%20from%0A%20%20Segment%20Anything%20Model&entry.906535625=Pengchen%20Liang%20and%20Leijun%20Shi%20and%20Huiping%20Yao%20and%20Bin%20Pu%20and%20Jianguo%20Chen%20and%20Lei%20Zhao%20and%20Haishan%20Huang%20and%20Zhuangzhuang%20Chen%20and%20Zhaozhao%20Xu%20and%20Lite%20Xu%20and%20Qing%20Chang%20and%20Yiwei%20Li&entry.1292438233=%20%20Rapid%20bone%20scintigraphy%20is%20crucial%20for%20diagnosing%20skeletal%20disorders%20and%0Adetecting%20tumor%20metastases%20in%20children%2C%20as%20it%20shortens%20scan%20duration%20and%0Areduces%20discomfort.%20However%2C%20accelerated%20acquisition%20often%20degrades%20image%0Aquality%2C%20impairing%20the%20visibility%20of%20fine%20anatomical%20details%20and%20potentially%0Acompromising%20diagnosis.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20the%20first%0Aapplication%20of%20SAM-based%20semantic%20priors%20for%20medical%20image%20restoration%2C%0Autilizing%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20pediatric%20rapid%20bone%0Ascintigraphy.%20Our%20approach%20employs%20two%20cascaded%20networks%2C%20%24f%5E%7BIR1%7D%24%20and%0A%24f%5E%7BIR2%7D%24%2C%20supported%20by%20three%20specialized%20modules%3A%20a%20Semantic%20Prior%20Integration%0A%28SPI%29%20module%2C%20a%20Semantic%20Knowledge%20Distillation%20%28SKD%29%20module%2C%20and%20a%20Semantic%0AConsistency%20Module%20%28SCM%29.%20The%20SPI%20and%20SKD%20modules%20inject%20domain-specific%0Asemantic%20cues%20from%20a%20fine-tuned%20SAM%2C%20while%20the%20SCM%20preserves%20coherent%20semantic%0Afeature%20representations%20across%20both%20cascaded%20stages.%20Moreover%2C%20we%20present%20RBS%2C%0Aa%20novel%20Rapid%20Bone%20Scintigraphy%20dataset%20comprising%20paired%20standard%20%2820%20cm/min%29%0Aand%20rapid%20%2840%20cm/min%29%20scans%20from%20137%20pediatric%20patients%20aged%200.5%20-%2016%20years%2C%0Amaking%20it%20the%20first%20dataset%20tailored%20for%20pediatric%20rapid%20bone%20scintigraphy%0Arestoration.%20Extensive%20experiments%20on%20both%20a%20public%20endoscopic%20dataset%20and%20our%0ARBS%20dataset%20demonstrate%20that%20our%20method%20consistently%20surpasses%20existing%0Atechniques%20in%20PSNR%2C%20SSIM%2C%20FID%2C%20and%20LPIPS%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02321v3&entry.124074799=Read"},
{"title": "Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning\n  with MPC-PID Demonstration", "author": "Chengdong Wu and Sven Kirchner and Nils Purschke and Alois C. Knoll", "abstract": "  The controller is one of the most important modules in the autonomous driving\npipeline, ensuring the vehicle reaches its desired position. In this work, a\nreinforcement learning based lateral control approach, despite the\nimperfections in the vehicle models due to measurement errors and\nsimplifications, is presented. Our approach ensures comfortable, efficient, and\nrobust control performance considering the interface between controlling and\nother modules. The controller consists of the conventional Model Predictive\nControl (MPC)-PID part as the basis and the demonstrator, and the Deep\nReinforcement Learning (DRL) part which leverages the online information from\nthe MPC-PID part. The controller's performance is evaluated in CARLA using the\nground truth of the waypoints as inputs. Experimental results demonstrate the\neffectiveness of the controller when vehicle information is incomplete, and the\ntraining of DRL can be stabilized with the demonstration part. These findings\nhighlight the potential to reduce development and integration efforts for\nautonomous driving pipelines in the future.\n", "link": "http://arxiv.org/abs/2506.04040v1", "date": "2025-06-04", "relevancy": 2.1207, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Vehicle%20Lateral%20Control%20Using%20Deep%20Reinforcement%20Learning%0A%20%20with%20MPC-PID%20Demonstration&body=Title%3A%20Autonomous%20Vehicle%20Lateral%20Control%20Using%20Deep%20Reinforcement%20Learning%0A%20%20with%20MPC-PID%20Demonstration%0AAuthor%3A%20Chengdong%20Wu%20and%20Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Alois%20C.%20Knoll%0AAbstract%3A%20%20%20The%20controller%20is%20one%20of%20the%20most%20important%20modules%20in%20the%20autonomous%20driving%0Apipeline%2C%20ensuring%20the%20vehicle%20reaches%20its%20desired%20position.%20In%20this%20work%2C%20a%0Areinforcement%20learning%20based%20lateral%20control%20approach%2C%20despite%20the%0Aimperfections%20in%20the%20vehicle%20models%20due%20to%20measurement%20errors%20and%0Asimplifications%2C%20is%20presented.%20Our%20approach%20ensures%20comfortable%2C%20efficient%2C%20and%0Arobust%20control%20performance%20considering%20the%20interface%20between%20controlling%20and%0Aother%20modules.%20The%20controller%20consists%20of%20the%20conventional%20Model%20Predictive%0AControl%20%28MPC%29-PID%20part%20as%20the%20basis%20and%20the%20demonstrator%2C%20and%20the%20Deep%0AReinforcement%20Learning%20%28DRL%29%20part%20which%20leverages%20the%20online%20information%20from%0Athe%20MPC-PID%20part.%20The%20controller%27s%20performance%20is%20evaluated%20in%20CARLA%20using%20the%0Aground%20truth%20of%20the%20waypoints%20as%20inputs.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20controller%20when%20vehicle%20information%20is%20incomplete%2C%20and%20the%0Atraining%20of%20DRL%20can%20be%20stabilized%20with%20the%20demonstration%20part.%20These%20findings%0Ahighlight%20the%20potential%20to%20reduce%20development%20and%20integration%20efforts%20for%0Aautonomous%20driving%20pipelines%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Vehicle%2520Lateral%2520Control%2520Using%2520Deep%2520Reinforcement%2520Learning%250A%2520%2520with%2520MPC-PID%2520Demonstration%26entry.906535625%3DChengdong%2520Wu%2520and%2520Sven%2520Kirchner%2520and%2520Nils%2520Purschke%2520and%2520Alois%2520C.%2520Knoll%26entry.1292438233%3D%2520%2520The%2520controller%2520is%2520one%2520of%2520the%2520most%2520important%2520modules%2520in%2520the%2520autonomous%2520driving%250Apipeline%252C%2520ensuring%2520the%2520vehicle%2520reaches%2520its%2520desired%2520position.%2520In%2520this%2520work%252C%2520a%250Areinforcement%2520learning%2520based%2520lateral%2520control%2520approach%252C%2520despite%2520the%250Aimperfections%2520in%2520the%2520vehicle%2520models%2520due%2520to%2520measurement%2520errors%2520and%250Asimplifications%252C%2520is%2520presented.%2520Our%2520approach%2520ensures%2520comfortable%252C%2520efficient%252C%2520and%250Arobust%2520control%2520performance%2520considering%2520the%2520interface%2520between%2520controlling%2520and%250Aother%2520modules.%2520The%2520controller%2520consists%2520of%2520the%2520conventional%2520Model%2520Predictive%250AControl%2520%2528MPC%2529-PID%2520part%2520as%2520the%2520basis%2520and%2520the%2520demonstrator%252C%2520and%2520the%2520Deep%250AReinforcement%2520Learning%2520%2528DRL%2529%2520part%2520which%2520leverages%2520the%2520online%2520information%2520from%250Athe%2520MPC-PID%2520part.%2520The%2520controller%2527s%2520performance%2520is%2520evaluated%2520in%2520CARLA%2520using%2520the%250Aground%2520truth%2520of%2520the%2520waypoints%2520as%2520inputs.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520controller%2520when%2520vehicle%2520information%2520is%2520incomplete%252C%2520and%2520the%250Atraining%2520of%2520DRL%2520can%2520be%2520stabilized%2520with%2520the%2520demonstration%2520part.%2520These%2520findings%250Ahighlight%2520the%2520potential%2520to%2520reduce%2520development%2520and%2520integration%2520efforts%2520for%250Aautonomous%2520driving%2520pipelines%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Vehicle%20Lateral%20Control%20Using%20Deep%20Reinforcement%20Learning%0A%20%20with%20MPC-PID%20Demonstration&entry.906535625=Chengdong%20Wu%20and%20Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Alois%20C.%20Knoll&entry.1292438233=%20%20The%20controller%20is%20one%20of%20the%20most%20important%20modules%20in%20the%20autonomous%20driving%0Apipeline%2C%20ensuring%20the%20vehicle%20reaches%20its%20desired%20position.%20In%20this%20work%2C%20a%0Areinforcement%20learning%20based%20lateral%20control%20approach%2C%20despite%20the%0Aimperfections%20in%20the%20vehicle%20models%20due%20to%20measurement%20errors%20and%0Asimplifications%2C%20is%20presented.%20Our%20approach%20ensures%20comfortable%2C%20efficient%2C%20and%0Arobust%20control%20performance%20considering%20the%20interface%20between%20controlling%20and%0Aother%20modules.%20The%20controller%20consists%20of%20the%20conventional%20Model%20Predictive%0AControl%20%28MPC%29-PID%20part%20as%20the%20basis%20and%20the%20demonstrator%2C%20and%20the%20Deep%0AReinforcement%20Learning%20%28DRL%29%20part%20which%20leverages%20the%20online%20information%20from%0Athe%20MPC-PID%20part.%20The%20controller%27s%20performance%20is%20evaluated%20in%20CARLA%20using%20the%0Aground%20truth%20of%20the%20waypoints%20as%20inputs.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20controller%20when%20vehicle%20information%20is%20incomplete%2C%20and%20the%0Atraining%20of%20DRL%20can%20be%20stabilized%20with%20the%20demonstration%20part.%20These%20findings%0Ahighlight%20the%20potential%20to%20reduce%20development%20and%20integration%20efforts%20for%0Aautonomous%20driving%20pipelines%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04040v1&entry.124074799=Read"},
{"title": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation", "author": "Mingxuan Xia and Haobo Wang and Yixuan Li and Zewei Yu and Jindong Wang and Junbo Zhao and Runze Wu", "abstract": "  Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.\n", "link": "http://arxiv.org/abs/2506.03857v1", "date": "2025-06-04", "relevancy": 2.1199, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5789}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5228}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Candidates%2C%20then%20Distill%3A%20A%20Teacher-Student%20Framework%20for%0A%20%20LLM-driven%20Data%20Annotation&body=Title%3A%20Prompt%20Candidates%2C%20then%20Distill%3A%20A%20Teacher-Student%20Framework%20for%0A%20%20LLM-driven%20Data%20Annotation%0AAuthor%3A%20Mingxuan%20Xia%20and%20Haobo%20Wang%20and%20Yixuan%20Li%20and%20Zewei%20Yu%20and%20Jindong%20Wang%20and%20Junbo%20Zhao%20and%20Runze%20Wu%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%0Apotential%20for%20data%20annotation%2C%20markedly%20reducing%20the%20labor%20costs%20associated%0Awith%20downstream%20applications.%20However%2C%20existing%20methods%20mostly%20adopt%20an%0Aaggressive%20strategy%20by%20prompting%20LLM%20to%20determine%20a%20single%20gold%20label%20for%20each%0Aunlabeled%20sample.%20Due%20to%20the%20inherent%20uncertainty%20within%20LLMs%2C%20they%20often%0Aproduce%20incorrect%20labels%20for%20difficult%20samples%2C%20severely%20compromising%20the%20data%0Aquality%20for%20downstream%20applications.%20Motivated%20by%20ambiguity%20aversion%20in%20human%0Abehaviors%2C%20we%20propose%20a%20novel%20candidate%20annotation%20paradigm%20wherein%20large%0Alanguage%20models%20are%20encouraged%20to%20output%20all%20possible%20labels%20when%20incurring%0Auncertainty.%20To%20ensure%20unique%20labels%20are%20provided%20for%20downstream%20tasks%2C%20we%0Adevelop%20a%20teacher-student%20framework%20CanDist%20that%20distills%20candidate%20annotations%0Awith%20a%20Small%20Language%20Model%20%28SLM%29.%20We%20further%20provide%20a%20rigorous%20justification%0Ademonstrating%20that%20distilling%20candidate%20annotations%20from%20the%20teacher%20LLM%20offers%0Asuperior%20theoretical%20guarantees%20compared%20to%20directly%20using%20single%20annotations.%0AExtensive%20experiments%20across%20six%20text%20classification%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/MingxuanXia/CanDist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Candidates%252C%2520then%2520Distill%253A%2520A%2520Teacher-Student%2520Framework%2520for%250A%2520%2520LLM-driven%2520Data%2520Annotation%26entry.906535625%3DMingxuan%2520Xia%2520and%2520Haobo%2520Wang%2520and%2520Yixuan%2520Li%2520and%2520Zewei%2520Yu%2520and%2520Jindong%2520Wang%2520and%2520Junbo%2520Zhao%2520and%2520Runze%2520Wu%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%250Apotential%2520for%2520data%2520annotation%252C%2520markedly%2520reducing%2520the%2520labor%2520costs%2520associated%250Awith%2520downstream%2520applications.%2520However%252C%2520existing%2520methods%2520mostly%2520adopt%2520an%250Aaggressive%2520strategy%2520by%2520prompting%2520LLM%2520to%2520determine%2520a%2520single%2520gold%2520label%2520for%2520each%250Aunlabeled%2520sample.%2520Due%2520to%2520the%2520inherent%2520uncertainty%2520within%2520LLMs%252C%2520they%2520often%250Aproduce%2520incorrect%2520labels%2520for%2520difficult%2520samples%252C%2520severely%2520compromising%2520the%2520data%250Aquality%2520for%2520downstream%2520applications.%2520Motivated%2520by%2520ambiguity%2520aversion%2520in%2520human%250Abehaviors%252C%2520we%2520propose%2520a%2520novel%2520candidate%2520annotation%2520paradigm%2520wherein%2520large%250Alanguage%2520models%2520are%2520encouraged%2520to%2520output%2520all%2520possible%2520labels%2520when%2520incurring%250Auncertainty.%2520To%2520ensure%2520unique%2520labels%2520are%2520provided%2520for%2520downstream%2520tasks%252C%2520we%250Adevelop%2520a%2520teacher-student%2520framework%2520CanDist%2520that%2520distills%2520candidate%2520annotations%250Awith%2520a%2520Small%2520Language%2520Model%2520%2528SLM%2529.%2520We%2520further%2520provide%2520a%2520rigorous%2520justification%250Ademonstrating%2520that%2520distilling%2520candidate%2520annotations%2520from%2520the%2520teacher%2520LLM%2520offers%250Asuperior%2520theoretical%2520guarantees%2520compared%2520to%2520directly%2520using%2520single%2520annotations.%250AExtensive%2520experiments%2520across%2520six%2520text%2520classification%2520tasks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/MingxuanXia/CanDist.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Candidates%2C%20then%20Distill%3A%20A%20Teacher-Student%20Framework%20for%0A%20%20LLM-driven%20Data%20Annotation&entry.906535625=Mingxuan%20Xia%20and%20Haobo%20Wang%20and%20Yixuan%20Li%20and%20Zewei%20Yu%20and%20Jindong%20Wang%20and%20Junbo%20Zhao%20and%20Runze%20Wu&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%0Apotential%20for%20data%20annotation%2C%20markedly%20reducing%20the%20labor%20costs%20associated%0Awith%20downstream%20applications.%20However%2C%20existing%20methods%20mostly%20adopt%20an%0Aaggressive%20strategy%20by%20prompting%20LLM%20to%20determine%20a%20single%20gold%20label%20for%20each%0Aunlabeled%20sample.%20Due%20to%20the%20inherent%20uncertainty%20within%20LLMs%2C%20they%20often%0Aproduce%20incorrect%20labels%20for%20difficult%20samples%2C%20severely%20compromising%20the%20data%0Aquality%20for%20downstream%20applications.%20Motivated%20by%20ambiguity%20aversion%20in%20human%0Abehaviors%2C%20we%20propose%20a%20novel%20candidate%20annotation%20paradigm%20wherein%20large%0Alanguage%20models%20are%20encouraged%20to%20output%20all%20possible%20labels%20when%20incurring%0Auncertainty.%20To%20ensure%20unique%20labels%20are%20provided%20for%20downstream%20tasks%2C%20we%0Adevelop%20a%20teacher-student%20framework%20CanDist%20that%20distills%20candidate%20annotations%0Awith%20a%20Small%20Language%20Model%20%28SLM%29.%20We%20further%20provide%20a%20rigorous%20justification%0Ademonstrating%20that%20distilling%20candidate%20annotations%20from%20the%20teacher%20LLM%20offers%0Asuperior%20theoretical%20guarantees%20compared%20to%20directly%20using%20single%20annotations.%0AExtensive%20experiments%20across%20six%20text%20classification%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/MingxuanXia/CanDist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03857v1&entry.124074799=Read"},
{"title": "Faster Approx. Top-K: Harnessing the Full Power of Two Stages", "author": "Yashas Samaga and Varun Yerram and Spandana Raj Babbula and Prateek Jain and Praneeth Netrapalli", "abstract": "  We consider the Top-$K$ selection problem, which aims to identify the\nlargest-$K$ elements from an array. Top-$K$ selection arises in many machine\nlearning algorithms and often becomes a bottleneck on accelerators, which are\noptimized for dense matrix multiplications. To address this problem,\n\\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage\n\\textit{approximate} Top-$K$ algorithm: (i) partition the input array and\nselect the top-$1$ element from each partition, (ii) sort this \\textit{smaller\nsubset} and return the top $K$ elements. In this paper, we consider a\ngeneralized version of this algorithm, where the first stage selects top-$K'$\nelements, for some $1 \\leq K' \\leq K$, from each partition. Our contributions\nare as follows: (i) we derive an expression for the expected recall of this\ngeneralized algorithm and show that choosing $K' > 1$ with fewer partitions in\nthe first stage reduces the input size to the second stage more effectively\nwhile maintaining the same expected recall as the original algorithm, (ii) we\nderive a bound on the expected recall for the original algorithm in\n\\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of\n$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud\nTPUv5e and achieve around an order of magnitude speedups over the original\nalgorithm without sacrificing recall on real-world tasks.\n", "link": "http://arxiv.org/abs/2506.04165v1", "date": "2025-06-04", "relevancy": 2.1198, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4405}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4211}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Approx.%20Top-K%3A%20Harnessing%20the%20Full%20Power%20of%20Two%20Stages&body=Title%3A%20Faster%20Approx.%20Top-K%3A%20Harnessing%20the%20Full%20Power%20of%20Two%20Stages%0AAuthor%3A%20Yashas%20Samaga%20and%20Varun%20Yerram%20and%20Spandana%20Raj%20Babbula%20and%20Prateek%20Jain%20and%20Praneeth%20Netrapalli%0AAbstract%3A%20%20%20We%20consider%20the%20Top-%24K%24%20selection%20problem%2C%20which%20aims%20to%20identify%20the%0Alargest-%24K%24%20elements%20from%20an%20array.%20Top-%24K%24%20selection%20arises%20in%20many%20machine%0Alearning%20algorithms%20and%20often%20becomes%20a%20bottleneck%20on%20accelerators%2C%20which%20are%0Aoptimized%20for%20dense%20matrix%20multiplications.%20To%20address%20this%20problem%2C%0A%5Ccitet%7Bchern2022tpuknnknearestneighbor%7D%20proposed%20a%20fast%20two-stage%0A%5Ctextit%7Bapproximate%7D%20Top-%24K%24%20algorithm%3A%20%28i%29%20partition%20the%20input%20array%20and%0Aselect%20the%20top-%241%24%20element%20from%20each%20partition%2C%20%28ii%29%20sort%20this%20%5Ctextit%7Bsmaller%0Asubset%7D%20and%20return%20the%20top%20%24K%24%20elements.%20In%20this%20paper%2C%20we%20consider%20a%0Ageneralized%20version%20of%20this%20algorithm%2C%20where%20the%20first%20stage%20selects%20top-%24K%27%24%0Aelements%2C%20for%20some%20%241%20%5Cleq%20K%27%20%5Cleq%20K%24%2C%20from%20each%20partition.%20Our%20contributions%0Aare%20as%20follows%3A%20%28i%29%20we%20derive%20an%20expression%20for%20the%20expected%20recall%20of%20this%0Ageneralized%20algorithm%20and%20show%20that%20choosing%20%24K%27%20%3E%201%24%20with%20fewer%20partitions%20in%0Athe%20first%20stage%20reduces%20the%20input%20size%20to%20the%20second%20stage%20more%20effectively%0Awhile%20maintaining%20the%20same%20expected%20recall%20as%20the%20original%20algorithm%2C%20%28ii%29%20we%0Aderive%20a%20bound%20on%20the%20expected%20recall%20for%20the%20original%20algorithm%20in%0A%5Ccitet%7Bchern2022tpuknnknearestneighbor%7D%20that%20is%20provably%20tighter%20by%20a%20factor%20of%0A%242%24%20than%20the%20one%20in%20that%20paper%2C%20and%20%28iii%29%20we%20implement%20our%20algorithm%20on%20Cloud%0ATPUv5e%20and%20achieve%20around%20an%20order%20of%20magnitude%20speedups%20over%20the%20original%0Aalgorithm%20without%20sacrificing%20recall%20on%20real-world%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Approx.%2520Top-K%253A%2520Harnessing%2520the%2520Full%2520Power%2520of%2520Two%2520Stages%26entry.906535625%3DYashas%2520Samaga%2520and%2520Varun%2520Yerram%2520and%2520Spandana%2520Raj%2520Babbula%2520and%2520Prateek%2520Jain%2520and%2520Praneeth%2520Netrapalli%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520Top-%2524K%2524%2520selection%2520problem%252C%2520which%2520aims%2520to%2520identify%2520the%250Alargest-%2524K%2524%2520elements%2520from%2520an%2520array.%2520Top-%2524K%2524%2520selection%2520arises%2520in%2520many%2520machine%250Alearning%2520algorithms%2520and%2520often%2520becomes%2520a%2520bottleneck%2520on%2520accelerators%252C%2520which%2520are%250Aoptimized%2520for%2520dense%2520matrix%2520multiplications.%2520To%2520address%2520this%2520problem%252C%250A%255Ccitet%257Bchern2022tpuknnknearestneighbor%257D%2520proposed%2520a%2520fast%2520two-stage%250A%255Ctextit%257Bapproximate%257D%2520Top-%2524K%2524%2520algorithm%253A%2520%2528i%2529%2520partition%2520the%2520input%2520array%2520and%250Aselect%2520the%2520top-%25241%2524%2520element%2520from%2520each%2520partition%252C%2520%2528ii%2529%2520sort%2520this%2520%255Ctextit%257Bsmaller%250Asubset%257D%2520and%2520return%2520the%2520top%2520%2524K%2524%2520elements.%2520In%2520this%2520paper%252C%2520we%2520consider%2520a%250Ageneralized%2520version%2520of%2520this%2520algorithm%252C%2520where%2520the%2520first%2520stage%2520selects%2520top-%2524K%2527%2524%250Aelements%252C%2520for%2520some%2520%25241%2520%255Cleq%2520K%2527%2520%255Cleq%2520K%2524%252C%2520from%2520each%2520partition.%2520Our%2520contributions%250Aare%2520as%2520follows%253A%2520%2528i%2529%2520we%2520derive%2520an%2520expression%2520for%2520the%2520expected%2520recall%2520of%2520this%250Ageneralized%2520algorithm%2520and%2520show%2520that%2520choosing%2520%2524K%2527%2520%253E%25201%2524%2520with%2520fewer%2520partitions%2520in%250Athe%2520first%2520stage%2520reduces%2520the%2520input%2520size%2520to%2520the%2520second%2520stage%2520more%2520effectively%250Awhile%2520maintaining%2520the%2520same%2520expected%2520recall%2520as%2520the%2520original%2520algorithm%252C%2520%2528ii%2529%2520we%250Aderive%2520a%2520bound%2520on%2520the%2520expected%2520recall%2520for%2520the%2520original%2520algorithm%2520in%250A%255Ccitet%257Bchern2022tpuknnknearestneighbor%257D%2520that%2520is%2520provably%2520tighter%2520by%2520a%2520factor%2520of%250A%25242%2524%2520than%2520the%2520one%2520in%2520that%2520paper%252C%2520and%2520%2528iii%2529%2520we%2520implement%2520our%2520algorithm%2520on%2520Cloud%250ATPUv5e%2520and%2520achieve%2520around%2520an%2520order%2520of%2520magnitude%2520speedups%2520over%2520the%2520original%250Aalgorithm%2520without%2520sacrificing%2520recall%2520on%2520real-world%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Approx.%20Top-K%3A%20Harnessing%20the%20Full%20Power%20of%20Two%20Stages&entry.906535625=Yashas%20Samaga%20and%20Varun%20Yerram%20and%20Spandana%20Raj%20Babbula%20and%20Prateek%20Jain%20and%20Praneeth%20Netrapalli&entry.1292438233=%20%20We%20consider%20the%20Top-%24K%24%20selection%20problem%2C%20which%20aims%20to%20identify%20the%0Alargest-%24K%24%20elements%20from%20an%20array.%20Top-%24K%24%20selection%20arises%20in%20many%20machine%0Alearning%20algorithms%20and%20often%20becomes%20a%20bottleneck%20on%20accelerators%2C%20which%20are%0Aoptimized%20for%20dense%20matrix%20multiplications.%20To%20address%20this%20problem%2C%0A%5Ccitet%7Bchern2022tpuknnknearestneighbor%7D%20proposed%20a%20fast%20two-stage%0A%5Ctextit%7Bapproximate%7D%20Top-%24K%24%20algorithm%3A%20%28i%29%20partition%20the%20input%20array%20and%0Aselect%20the%20top-%241%24%20element%20from%20each%20partition%2C%20%28ii%29%20sort%20this%20%5Ctextit%7Bsmaller%0Asubset%7D%20and%20return%20the%20top%20%24K%24%20elements.%20In%20this%20paper%2C%20we%20consider%20a%0Ageneralized%20version%20of%20this%20algorithm%2C%20where%20the%20first%20stage%20selects%20top-%24K%27%24%0Aelements%2C%20for%20some%20%241%20%5Cleq%20K%27%20%5Cleq%20K%24%2C%20from%20each%20partition.%20Our%20contributions%0Aare%20as%20follows%3A%20%28i%29%20we%20derive%20an%20expression%20for%20the%20expected%20recall%20of%20this%0Ageneralized%20algorithm%20and%20show%20that%20choosing%20%24K%27%20%3E%201%24%20with%20fewer%20partitions%20in%0Athe%20first%20stage%20reduces%20the%20input%20size%20to%20the%20second%20stage%20more%20effectively%0Awhile%20maintaining%20the%20same%20expected%20recall%20as%20the%20original%20algorithm%2C%20%28ii%29%20we%0Aderive%20a%20bound%20on%20the%20expected%20recall%20for%20the%20original%20algorithm%20in%0A%5Ccitet%7Bchern2022tpuknnknearestneighbor%7D%20that%20is%20provably%20tighter%20by%20a%20factor%20of%0A%242%24%20than%20the%20one%20in%20that%20paper%2C%20and%20%28iii%29%20we%20implement%20our%20algorithm%20on%20Cloud%0ATPUv5e%20and%20achieve%20around%20an%20order%20of%20magnitude%20speedups%20over%20the%20original%0Aalgorithm%20without%20sacrificing%20recall%20on%20real-world%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04165v1&entry.124074799=Read"},
{"title": "What Makes Treatment Effects Identifiable? Characterizations and\n  Estimators Beyond Unconfoundedness", "author": "Yang Cai and Alkis Kalavasis and Katerina Mamali and Anay Mehrotra and Manolis Zampetakis", "abstract": "  Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.\n", "link": "http://arxiv.org/abs/2506.04194v1", "date": "2025-06-04", "relevancy": 1.952, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4085}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3841}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20Treatment%20Effects%20Identifiable%3F%20Characterizations%20and%0A%20%20Estimators%20Beyond%20Unconfoundedness&body=Title%3A%20What%20Makes%20Treatment%20Effects%20Identifiable%3F%20Characterizations%20and%0A%20%20Estimators%20Beyond%20Unconfoundedness%0AAuthor%3A%20Yang%20Cai%20and%20Alkis%20Kalavasis%20and%20Katerina%20Mamali%20and%20Anay%20Mehrotra%20and%20Manolis%20Zampetakis%0AAbstract%3A%20%20%20Most%20of%20the%20widely%20used%20estimators%20of%20the%20average%20treatment%20effect%20%28ATE%29%20in%0Acausal%20inference%20rely%20on%20the%20assumptions%20of%20unconfoundedness%20and%20overlap.%0AUnconfoundedness%20requires%20that%20the%20observed%20covariates%20account%20for%20all%0Acorrelations%20between%20the%20outcome%20and%20treatment.%20Overlap%20requires%20the%20existence%0Aof%20randomness%20in%20treatment%20decisions%20for%20all%20individuals.%20Nevertheless%2C%20many%0Atypes%20of%20studies%20frequently%20violate%20unconfoundedness%20or%20overlap%2C%20for%20instance%2C%0Aobservational%20studies%20with%20deterministic%20treatment%20decisions%20--%20popularly%20known%0Aas%20Regression%20Discontinuity%20designs%20--%20violate%20overlap.%0A%20%20In%20this%20paper%2C%20we%20initiate%20the%20study%20of%20general%20conditions%20that%20enable%20the%0Aidentification%20of%20the%20average%20treatment%20effect%2C%20extending%20beyond%0Aunconfoundedness%20and%20overlap.%20In%20particular%2C%20following%20the%20paradigm%20of%0Astatistical%20learning%20theory%2C%20we%20provide%20an%20interpretable%20condition%20that%20is%0Asufficient%20and%20nearly%20necessary%20for%20the%20identification%20of%20ATE.%20Moreover%2C%20this%0Acondition%20characterizes%20the%20identification%20of%20the%20average%20treatment%20effect%20on%0Athe%20treated%20%28ATT%29%20and%20can%20be%20used%20to%20characterize%20other%20treatment%20effects%20as%0Awell.%20To%20illustrate%20the%20utility%20of%20our%20condition%2C%20we%20present%20several%0Awell-studied%20scenarios%20where%20our%20condition%20is%20satisfied%20and%2C%20hence%2C%20we%20prove%0Athat%20ATE%20can%20be%20identified%20in%20regimes%20that%20prior%20works%20could%20not%20capture.%20For%0Aexample%2C%20under%20mild%20assumptions%20on%20the%20data%20distributions%2C%20this%20holds%20for%20the%0Amodels%20proposed%20by%20Tan%20%282006%29%20and%20Rosenbaum%20%282002%29%2C%20and%20the%20Regression%0ADiscontinuity%20design%20model%20introduced%20by%20Thistlethwaite%20and%20Campbell%20%281960%29.%0AFor%20each%20of%20these%20scenarios%2C%20we%20also%20show%20that%2C%20under%20natural%20additional%0Aassumptions%2C%20ATE%20can%20be%20estimated%20from%20finite%20samples.%0A%20%20We%20believe%20these%20findings%20open%20new%20avenues%20for%20bridging%20learning-theoretic%0Ainsights%20and%20causal%20inference%20methodologies%2C%20particularly%20in%20observational%0Astudies%20with%20complex%20treatment%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520Treatment%2520Effects%2520Identifiable%253F%2520Characterizations%2520and%250A%2520%2520Estimators%2520Beyond%2520Unconfoundedness%26entry.906535625%3DYang%2520Cai%2520and%2520Alkis%2520Kalavasis%2520and%2520Katerina%2520Mamali%2520and%2520Anay%2520Mehrotra%2520and%2520Manolis%2520Zampetakis%26entry.1292438233%3D%2520%2520Most%2520of%2520the%2520widely%2520used%2520estimators%2520of%2520the%2520average%2520treatment%2520effect%2520%2528ATE%2529%2520in%250Acausal%2520inference%2520rely%2520on%2520the%2520assumptions%2520of%2520unconfoundedness%2520and%2520overlap.%250AUnconfoundedness%2520requires%2520that%2520the%2520observed%2520covariates%2520account%2520for%2520all%250Acorrelations%2520between%2520the%2520outcome%2520and%2520treatment.%2520Overlap%2520requires%2520the%2520existence%250Aof%2520randomness%2520in%2520treatment%2520decisions%2520for%2520all%2520individuals.%2520Nevertheless%252C%2520many%250Atypes%2520of%2520studies%2520frequently%2520violate%2520unconfoundedness%2520or%2520overlap%252C%2520for%2520instance%252C%250Aobservational%2520studies%2520with%2520deterministic%2520treatment%2520decisions%2520--%2520popularly%2520known%250Aas%2520Regression%2520Discontinuity%2520designs%2520--%2520violate%2520overlap.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520initiate%2520the%2520study%2520of%2520general%2520conditions%2520that%2520enable%2520the%250Aidentification%2520of%2520the%2520average%2520treatment%2520effect%252C%2520extending%2520beyond%250Aunconfoundedness%2520and%2520overlap.%2520In%2520particular%252C%2520following%2520the%2520paradigm%2520of%250Astatistical%2520learning%2520theory%252C%2520we%2520provide%2520an%2520interpretable%2520condition%2520that%2520is%250Asufficient%2520and%2520nearly%2520necessary%2520for%2520the%2520identification%2520of%2520ATE.%2520Moreover%252C%2520this%250Acondition%2520characterizes%2520the%2520identification%2520of%2520the%2520average%2520treatment%2520effect%2520on%250Athe%2520treated%2520%2528ATT%2529%2520and%2520can%2520be%2520used%2520to%2520characterize%2520other%2520treatment%2520effects%2520as%250Awell.%2520To%2520illustrate%2520the%2520utility%2520of%2520our%2520condition%252C%2520we%2520present%2520several%250Awell-studied%2520scenarios%2520where%2520our%2520condition%2520is%2520satisfied%2520and%252C%2520hence%252C%2520we%2520prove%250Athat%2520ATE%2520can%2520be%2520identified%2520in%2520regimes%2520that%2520prior%2520works%2520could%2520not%2520capture.%2520For%250Aexample%252C%2520under%2520mild%2520assumptions%2520on%2520the%2520data%2520distributions%252C%2520this%2520holds%2520for%2520the%250Amodels%2520proposed%2520by%2520Tan%2520%25282006%2529%2520and%2520Rosenbaum%2520%25282002%2529%252C%2520and%2520the%2520Regression%250ADiscontinuity%2520design%2520model%2520introduced%2520by%2520Thistlethwaite%2520and%2520Campbell%2520%25281960%2529.%250AFor%2520each%2520of%2520these%2520scenarios%252C%2520we%2520also%2520show%2520that%252C%2520under%2520natural%2520additional%250Aassumptions%252C%2520ATE%2520can%2520be%2520estimated%2520from%2520finite%2520samples.%250A%2520%2520We%2520believe%2520these%2520findings%2520open%2520new%2520avenues%2520for%2520bridging%2520learning-theoretic%250Ainsights%2520and%2520causal%2520inference%2520methodologies%252C%2520particularly%2520in%2520observational%250Astudies%2520with%2520complex%2520treatment%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20Treatment%20Effects%20Identifiable%3F%20Characterizations%20and%0A%20%20Estimators%20Beyond%20Unconfoundedness&entry.906535625=Yang%20Cai%20and%20Alkis%20Kalavasis%20and%20Katerina%20Mamali%20and%20Anay%20Mehrotra%20and%20Manolis%20Zampetakis&entry.1292438233=%20%20Most%20of%20the%20widely%20used%20estimators%20of%20the%20average%20treatment%20effect%20%28ATE%29%20in%0Acausal%20inference%20rely%20on%20the%20assumptions%20of%20unconfoundedness%20and%20overlap.%0AUnconfoundedness%20requires%20that%20the%20observed%20covariates%20account%20for%20all%0Acorrelations%20between%20the%20outcome%20and%20treatment.%20Overlap%20requires%20the%20existence%0Aof%20randomness%20in%20treatment%20decisions%20for%20all%20individuals.%20Nevertheless%2C%20many%0Atypes%20of%20studies%20frequently%20violate%20unconfoundedness%20or%20overlap%2C%20for%20instance%2C%0Aobservational%20studies%20with%20deterministic%20treatment%20decisions%20--%20popularly%20known%0Aas%20Regression%20Discontinuity%20designs%20--%20violate%20overlap.%0A%20%20In%20this%20paper%2C%20we%20initiate%20the%20study%20of%20general%20conditions%20that%20enable%20the%0Aidentification%20of%20the%20average%20treatment%20effect%2C%20extending%20beyond%0Aunconfoundedness%20and%20overlap.%20In%20particular%2C%20following%20the%20paradigm%20of%0Astatistical%20learning%20theory%2C%20we%20provide%20an%20interpretable%20condition%20that%20is%0Asufficient%20and%20nearly%20necessary%20for%20the%20identification%20of%20ATE.%20Moreover%2C%20this%0Acondition%20characterizes%20the%20identification%20of%20the%20average%20treatment%20effect%20on%0Athe%20treated%20%28ATT%29%20and%20can%20be%20used%20to%20characterize%20other%20treatment%20effects%20as%0Awell.%20To%20illustrate%20the%20utility%20of%20our%20condition%2C%20we%20present%20several%0Awell-studied%20scenarios%20where%20our%20condition%20is%20satisfied%20and%2C%20hence%2C%20we%20prove%0Athat%20ATE%20can%20be%20identified%20in%20regimes%20that%20prior%20works%20could%20not%20capture.%20For%0Aexample%2C%20under%20mild%20assumptions%20on%20the%20data%20distributions%2C%20this%20holds%20for%20the%0Amodels%20proposed%20by%20Tan%20%282006%29%20and%20Rosenbaum%20%282002%29%2C%20and%20the%20Regression%0ADiscontinuity%20design%20model%20introduced%20by%20Thistlethwaite%20and%20Campbell%20%281960%29.%0AFor%20each%20of%20these%20scenarios%2C%20we%20also%20show%20that%2C%20under%20natural%20additional%0Aassumptions%2C%20ATE%20can%20be%20estimated%20from%20finite%20samples.%0A%20%20We%20believe%20these%20findings%20open%20new%20avenues%20for%20bridging%20learning-theoretic%0Ainsights%20and%20causal%20inference%20methodologies%2C%20particularly%20in%20observational%0Astudies%20with%20complex%20treatment%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04194v1&entry.124074799=Read"},
{"title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search\n  Advertising", "author": "Zhenhui Liu and Chunyuan Yuan and Ming Pang and Zheng Fang and Li Yuan and Xue Jiang and Changping Peng and Zhangang Lin and Zheng Luo and Jingping Shao", "abstract": "  Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.\n", "link": "http://arxiv.org/abs/2506.03827v1", "date": "2025-06-04", "relevancy": 1.9386, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5299}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4601}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-objective%20Aligned%20Bidword%20Generation%20Model%20for%20E-commerce%20Search%0A%20%20Advertising&body=Title%3A%20Multi-objective%20Aligned%20Bidword%20Generation%20Model%20for%20E-commerce%20Search%0A%20%20Advertising%0AAuthor%3A%20Zhenhui%20Liu%20and%20Chunyuan%20Yuan%20and%20Ming%20Pang%20and%20Zheng%20Fang%20and%20Li%20Yuan%20and%20Xue%20Jiang%20and%20Changping%20Peng%20and%20Zhangang%20Lin%20and%20Zheng%20Luo%20and%20Jingping%20Shao%0AAbstract%3A%20%20%20Retrieval%20systems%20primarily%20address%20the%20challenge%20of%20matching%20user%20queries%0Awith%20the%20most%20relevant%20advertisements%2C%20playing%20a%20crucial%20role%20in%20e-commerce%0Asearch%20advertising.%20The%20diversity%20of%20user%20needs%20and%20expressions%20often%20produces%0Amassive%20long-tail%20queries%20that%20cannot%20be%20matched%20with%20merchant%20bidwords%20or%0Aproduct%20titles%2C%20which%20results%20in%20some%20advertisements%20not%20being%20recalled%2C%0Aultimately%20harming%20user%20experience%20and%20search%20efficiency.%20Existing%20query%0Arewriting%20research%20focuses%20on%20various%20methods%20such%20as%20query%20log%20mining%2C%0Aquery-bidword%20vector%20matching%2C%20or%20generation-based%20rewriting.%20However%2C%20these%0Amethods%20often%20fail%20to%20simultaneously%20optimize%20the%20relevance%20and%20authenticity%20of%0Athe%20user%27s%20original%20query%20and%20rewrite%20and%20maximize%20the%20revenue%20potential%20of%0Arecalled%20ads.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20Multi-objective%20aligned%20Bidword%20Generation%20Model%0A%28MoBGM%29%2C%20which%20is%20composed%20of%20a%20discriminator%2C%20generator%2C%20and%20preference%0Aalignment%20module%2C%20to%20address%20these%20challenges.%20To%20simultaneously%20improve%20the%0Arelevance%20and%20authenticity%20of%20the%20query%20and%20rewrite%20and%20maximize%20the%20platform%0Arevenue%2C%20we%20design%20a%20discriminator%20to%20optimize%20these%20key%20objectives.%20Using%20the%0Afeedback%20signal%20of%20the%20discriminator%2C%20we%20train%20a%20multi-objective%20aligned%0Abidword%20generator%20that%20aims%20to%20maximize%20the%20combined%20effect%20of%20the%20three%0Aobjectives.%20Extensive%20offline%20and%20online%20experiments%20show%20that%20our%20proposed%0Aalgorithm%20significantly%20outperforms%20the%20state%20of%20the%20art.%20After%20deployment%2C%20the%0Aalgorithm%20has%20created%20huge%20commercial%20value%20for%20the%20platform%2C%20further%20verifying%0Aits%20feasibility%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-objective%2520Aligned%2520Bidword%2520Generation%2520Model%2520for%2520E-commerce%2520Search%250A%2520%2520Advertising%26entry.906535625%3DZhenhui%2520Liu%2520and%2520Chunyuan%2520Yuan%2520and%2520Ming%2520Pang%2520and%2520Zheng%2520Fang%2520and%2520Li%2520Yuan%2520and%2520Xue%2520Jiang%2520and%2520Changping%2520Peng%2520and%2520Zhangang%2520Lin%2520and%2520Zheng%2520Luo%2520and%2520Jingping%2520Shao%26entry.1292438233%3D%2520%2520Retrieval%2520systems%2520primarily%2520address%2520the%2520challenge%2520of%2520matching%2520user%2520queries%250Awith%2520the%2520most%2520relevant%2520advertisements%252C%2520playing%2520a%2520crucial%2520role%2520in%2520e-commerce%250Asearch%2520advertising.%2520The%2520diversity%2520of%2520user%2520needs%2520and%2520expressions%2520often%2520produces%250Amassive%2520long-tail%2520queries%2520that%2520cannot%2520be%2520matched%2520with%2520merchant%2520bidwords%2520or%250Aproduct%2520titles%252C%2520which%2520results%2520in%2520some%2520advertisements%2520not%2520being%2520recalled%252C%250Aultimately%2520harming%2520user%2520experience%2520and%2520search%2520efficiency.%2520Existing%2520query%250Arewriting%2520research%2520focuses%2520on%2520various%2520methods%2520such%2520as%2520query%2520log%2520mining%252C%250Aquery-bidword%2520vector%2520matching%252C%2520or%2520generation-based%2520rewriting.%2520However%252C%2520these%250Amethods%2520often%2520fail%2520to%2520simultaneously%2520optimize%2520the%2520relevance%2520and%2520authenticity%2520of%250Athe%2520user%2527s%2520original%2520query%2520and%2520rewrite%2520and%2520maximize%2520the%2520revenue%2520potential%2520of%250Arecalled%2520ads.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Multi-objective%2520aligned%2520Bidword%2520Generation%2520Model%250A%2528MoBGM%2529%252C%2520which%2520is%2520composed%2520of%2520a%2520discriminator%252C%2520generator%252C%2520and%2520preference%250Aalignment%2520module%252C%2520to%2520address%2520these%2520challenges.%2520To%2520simultaneously%2520improve%2520the%250Arelevance%2520and%2520authenticity%2520of%2520the%2520query%2520and%2520rewrite%2520and%2520maximize%2520the%2520platform%250Arevenue%252C%2520we%2520design%2520a%2520discriminator%2520to%2520optimize%2520these%2520key%2520objectives.%2520Using%2520the%250Afeedback%2520signal%2520of%2520the%2520discriminator%252C%2520we%2520train%2520a%2520multi-objective%2520aligned%250Abidword%2520generator%2520that%2520aims%2520to%2520maximize%2520the%2520combined%2520effect%2520of%2520the%2520three%250Aobjectives.%2520Extensive%2520offline%2520and%2520online%2520experiments%2520show%2520that%2520our%2520proposed%250Aalgorithm%2520significantly%2520outperforms%2520the%2520state%2520of%2520the%2520art.%2520After%2520deployment%252C%2520the%250Aalgorithm%2520has%2520created%2520huge%2520commercial%2520value%2520for%2520the%2520platform%252C%2520further%2520verifying%250Aits%2520feasibility%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-objective%20Aligned%20Bidword%20Generation%20Model%20for%20E-commerce%20Search%0A%20%20Advertising&entry.906535625=Zhenhui%20Liu%20and%20Chunyuan%20Yuan%20and%20Ming%20Pang%20and%20Zheng%20Fang%20and%20Li%20Yuan%20and%20Xue%20Jiang%20and%20Changping%20Peng%20and%20Zhangang%20Lin%20and%20Zheng%20Luo%20and%20Jingping%20Shao&entry.1292438233=%20%20Retrieval%20systems%20primarily%20address%20the%20challenge%20of%20matching%20user%20queries%0Awith%20the%20most%20relevant%20advertisements%2C%20playing%20a%20crucial%20role%20in%20e-commerce%0Asearch%20advertising.%20The%20diversity%20of%20user%20needs%20and%20expressions%20often%20produces%0Amassive%20long-tail%20queries%20that%20cannot%20be%20matched%20with%20merchant%20bidwords%20or%0Aproduct%20titles%2C%20which%20results%20in%20some%20advertisements%20not%20being%20recalled%2C%0Aultimately%20harming%20user%20experience%20and%20search%20efficiency.%20Existing%20query%0Arewriting%20research%20focuses%20on%20various%20methods%20such%20as%20query%20log%20mining%2C%0Aquery-bidword%20vector%20matching%2C%20or%20generation-based%20rewriting.%20However%2C%20these%0Amethods%20often%20fail%20to%20simultaneously%20optimize%20the%20relevance%20and%20authenticity%20of%0Athe%20user%27s%20original%20query%20and%20rewrite%20and%20maximize%20the%20revenue%20potential%20of%0Arecalled%20ads.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20Multi-objective%20aligned%20Bidword%20Generation%20Model%0A%28MoBGM%29%2C%20which%20is%20composed%20of%20a%20discriminator%2C%20generator%2C%20and%20preference%0Aalignment%20module%2C%20to%20address%20these%20challenges.%20To%20simultaneously%20improve%20the%0Arelevance%20and%20authenticity%20of%20the%20query%20and%20rewrite%20and%20maximize%20the%20platform%0Arevenue%2C%20we%20design%20a%20discriminator%20to%20optimize%20these%20key%20objectives.%20Using%20the%0Afeedback%20signal%20of%20the%20discriminator%2C%20we%20train%20a%20multi-objective%20aligned%0Abidword%20generator%20that%20aims%20to%20maximize%20the%20combined%20effect%20of%20the%20three%0Aobjectives.%20Extensive%20offline%20and%20online%20experiments%20show%20that%20our%20proposed%0Aalgorithm%20significantly%20outperforms%20the%20state%20of%20the%20art.%20After%20deployment%2C%20the%0Aalgorithm%20has%20created%20huge%20commercial%20value%20for%20the%20platform%2C%20further%20verifying%0Aits%20feasibility%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03827v1&entry.124074799=Read"},
{"title": "Enhancing Experimental Efficiency in Materials Design: A Comparative\n  Study of Taguchi and Machine Learning Methods", "author": "Shyam Prabhu and P Akshay Kumar and Antov Selwinston and Pavan Taduvai and Shreya Bairi and Rohit Batra", "abstract": "  Materials design problems often require optimizing multiple variables,\nrendering full factorial exploration impractical. Design of experiment (DOE)\nmethods, such as Taguchi technique, are commonly used to efficiently sample the\ndesign space but they inherently lack the ability to capture non-linear\ndependency of process variables. In this work, we demonstrate how machine\nlearning (ML) methods can be used to overcome these limitations. We compare the\nperformance of Taguchi method against an active learning based Gaussian process\nregression (GPR) model in a wire arc additive manufacturing (WAAM) process to\naccurately predict aspects of bead geometry, including penetration depth, bead\nwidth, and height. While Taguchi method utilized a three-factor, five-level L25\northogonal array to suggest weld parameters, the GPR model used an\nuncertainty-based exploration acquisition function coupled with latin hypercube\nsampling for initial training data. Accuracy and efficiency of both models was\nevaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.\nThis work applies to broader materials processing domain requiring efficient\nexploration of complex parameters.\n", "link": "http://arxiv.org/abs/2506.03910v1", "date": "2025-06-04", "relevancy": 1.3443, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4577}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Experimental%20Efficiency%20in%20Materials%20Design%3A%20A%20Comparative%0A%20%20Study%20of%20Taguchi%20and%20Machine%20Learning%20Methods&body=Title%3A%20Enhancing%20Experimental%20Efficiency%20in%20Materials%20Design%3A%20A%20Comparative%0A%20%20Study%20of%20Taguchi%20and%20Machine%20Learning%20Methods%0AAuthor%3A%20Shyam%20Prabhu%20and%20P%20Akshay%20Kumar%20and%20Antov%20Selwinston%20and%20Pavan%20Taduvai%20and%20Shreya%20Bairi%20and%20Rohit%20Batra%0AAbstract%3A%20%20%20Materials%20design%20problems%20often%20require%20optimizing%20multiple%20variables%2C%0Arendering%20full%20factorial%20exploration%20impractical.%20Design%20of%20experiment%20%28DOE%29%0Amethods%2C%20such%20as%20Taguchi%20technique%2C%20are%20commonly%20used%20to%20efficiently%20sample%20the%0Adesign%20space%20but%20they%20inherently%20lack%20the%20ability%20to%20capture%20non-linear%0Adependency%20of%20process%20variables.%20In%20this%20work%2C%20we%20demonstrate%20how%20machine%0Alearning%20%28ML%29%20methods%20can%20be%20used%20to%20overcome%20these%20limitations.%20We%20compare%20the%0Aperformance%20of%20Taguchi%20method%20against%20an%20active%20learning%20based%20Gaussian%20process%0Aregression%20%28GPR%29%20model%20in%20a%20wire%20arc%20additive%20manufacturing%20%28WAAM%29%20process%20to%0Aaccurately%20predict%20aspects%20of%20bead%20geometry%2C%20including%20penetration%20depth%2C%20bead%0Awidth%2C%20and%20height.%20While%20Taguchi%20method%20utilized%20a%20three-factor%2C%20five-level%20L25%0Aorthogonal%20array%20to%20suggest%20weld%20parameters%2C%20the%20GPR%20model%20used%20an%0Auncertainty-based%20exploration%20acquisition%20function%20coupled%20with%20latin%20hypercube%0Asampling%20for%20initial%20training%20data.%20Accuracy%20and%20efficiency%20of%20both%20models%20was%0Aevaluated%20on%2015%20test%20cases%2C%20with%20GPR%20outperforming%20Taguchi%20in%20both%20metrics.%0AThis%20work%20applies%20to%20broader%20materials%20processing%20domain%20requiring%20efficient%0Aexploration%20of%20complex%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Experimental%2520Efficiency%2520in%2520Materials%2520Design%253A%2520A%2520Comparative%250A%2520%2520Study%2520of%2520Taguchi%2520and%2520Machine%2520Learning%2520Methods%26entry.906535625%3DShyam%2520Prabhu%2520and%2520P%2520Akshay%2520Kumar%2520and%2520Antov%2520Selwinston%2520and%2520Pavan%2520Taduvai%2520and%2520Shreya%2520Bairi%2520and%2520Rohit%2520Batra%26entry.1292438233%3D%2520%2520Materials%2520design%2520problems%2520often%2520require%2520optimizing%2520multiple%2520variables%252C%250Arendering%2520full%2520factorial%2520exploration%2520impractical.%2520Design%2520of%2520experiment%2520%2528DOE%2529%250Amethods%252C%2520such%2520as%2520Taguchi%2520technique%252C%2520are%2520commonly%2520used%2520to%2520efficiently%2520sample%2520the%250Adesign%2520space%2520but%2520they%2520inherently%2520lack%2520the%2520ability%2520to%2520capture%2520non-linear%250Adependency%2520of%2520process%2520variables.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520how%2520machine%250Alearning%2520%2528ML%2529%2520methods%2520can%2520be%2520used%2520to%2520overcome%2520these%2520limitations.%2520We%2520compare%2520the%250Aperformance%2520of%2520Taguchi%2520method%2520against%2520an%2520active%2520learning%2520based%2520Gaussian%2520process%250Aregression%2520%2528GPR%2529%2520model%2520in%2520a%2520wire%2520arc%2520additive%2520manufacturing%2520%2528WAAM%2529%2520process%2520to%250Aaccurately%2520predict%2520aspects%2520of%2520bead%2520geometry%252C%2520including%2520penetration%2520depth%252C%2520bead%250Awidth%252C%2520and%2520height.%2520While%2520Taguchi%2520method%2520utilized%2520a%2520three-factor%252C%2520five-level%2520L25%250Aorthogonal%2520array%2520to%2520suggest%2520weld%2520parameters%252C%2520the%2520GPR%2520model%2520used%2520an%250Auncertainty-based%2520exploration%2520acquisition%2520function%2520coupled%2520with%2520latin%2520hypercube%250Asampling%2520for%2520initial%2520training%2520data.%2520Accuracy%2520and%2520efficiency%2520of%2520both%2520models%2520was%250Aevaluated%2520on%252015%2520test%2520cases%252C%2520with%2520GPR%2520outperforming%2520Taguchi%2520in%2520both%2520metrics.%250AThis%2520work%2520applies%2520to%2520broader%2520materials%2520processing%2520domain%2520requiring%2520efficient%250Aexploration%2520of%2520complex%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Experimental%20Efficiency%20in%20Materials%20Design%3A%20A%20Comparative%0A%20%20Study%20of%20Taguchi%20and%20Machine%20Learning%20Methods&entry.906535625=Shyam%20Prabhu%20and%20P%20Akshay%20Kumar%20and%20Antov%20Selwinston%20and%20Pavan%20Taduvai%20and%20Shreya%20Bairi%20and%20Rohit%20Batra&entry.1292438233=%20%20Materials%20design%20problems%20often%20require%20optimizing%20multiple%20variables%2C%0Arendering%20full%20factorial%20exploration%20impractical.%20Design%20of%20experiment%20%28DOE%29%0Amethods%2C%20such%20as%20Taguchi%20technique%2C%20are%20commonly%20used%20to%20efficiently%20sample%20the%0Adesign%20space%20but%20they%20inherently%20lack%20the%20ability%20to%20capture%20non-linear%0Adependency%20of%20process%20variables.%20In%20this%20work%2C%20we%20demonstrate%20how%20machine%0Alearning%20%28ML%29%20methods%20can%20be%20used%20to%20overcome%20these%20limitations.%20We%20compare%20the%0Aperformance%20of%20Taguchi%20method%20against%20an%20active%20learning%20based%20Gaussian%20process%0Aregression%20%28GPR%29%20model%20in%20a%20wire%20arc%20additive%20manufacturing%20%28WAAM%29%20process%20to%0Aaccurately%20predict%20aspects%20of%20bead%20geometry%2C%20including%20penetration%20depth%2C%20bead%0Awidth%2C%20and%20height.%20While%20Taguchi%20method%20utilized%20a%20three-factor%2C%20five-level%20L25%0Aorthogonal%20array%20to%20suggest%20weld%20parameters%2C%20the%20GPR%20model%20used%20an%0Auncertainty-based%20exploration%20acquisition%20function%20coupled%20with%20latin%20hypercube%0Asampling%20for%20initial%20training%20data.%20Accuracy%20and%20efficiency%20of%20both%20models%20was%0Aevaluated%20on%2015%20test%20cases%2C%20with%20GPR%20outperforming%20Taguchi%20in%20both%20metrics.%0AThis%20work%20applies%20to%20broader%20materials%20processing%20domain%20requiring%20efficient%0Aexploration%20of%20complex%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03910v1&entry.124074799=Read"},
{"title": "FLIP: Flowability-Informed Powder Weighing", "author": "Nikola Radulov and Alex Wright and Thomas little and Andrew I. Cooper and Gabriella Pizzuto", "abstract": "  Autonomous manipulation of powders remains a significant challenge for\nrobotic automation in scientific laboratories. The inherent variability and\ncomplex physical interactions of powders in flow, coupled with variability in\nlaboratory conditions necessitates adaptive automation. This work introduces\nFLIP, a flowability-informed powder weighing framework designed to enhance\nrobotic policy learning for granular material handling. Our key contribution\nlies in using material flowability, quantified by the angle of repose, to\noptimise physics-based simulations through Bayesian inference. This yields\nmaterial-specific simulation environments capable of generating accurate\ntraining data, which reflects diverse powder behaviours, for training `robot\nchemists'. Building on this, FLIP integrates quantified flowability into a\ncurriculum learning strategy, fostering efficient acquisition of robust robotic\npolicies by gradually introducing more challenging, less flowable powders. We\nvalidate the efficacy of our method on a robotic powder weighing task under\nreal-world laboratory conditions. Experimental results show that FLIP with a\ncurriculum strategy achieves a low dispensing error of 2.12 +- 1.53 mg,\noutperforming methods that do not leverage flowability data, such as domain\nrandomisation (6.11 +- 3.92 mg). These results demonstrate FLIP's improved\nability to generalise to previously unseen, more cohesive powders and to new\ntarget masses.\n", "link": "http://arxiv.org/abs/2506.03896v1", "date": "2025-06-04", "relevancy": 1.4569, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5429}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4856}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLIP%3A%20Flowability-Informed%20Powder%20Weighing&body=Title%3A%20FLIP%3A%20Flowability-Informed%20Powder%20Weighing%0AAuthor%3A%20Nikola%20Radulov%20and%20Alex%20Wright%20and%20Thomas%20little%20and%20Andrew%20I.%20Cooper%20and%20Gabriella%20Pizzuto%0AAbstract%3A%20%20%20Autonomous%20manipulation%20of%20powders%20remains%20a%20significant%20challenge%20for%0Arobotic%20automation%20in%20scientific%20laboratories.%20The%20inherent%20variability%20and%0Acomplex%20physical%20interactions%20of%20powders%20in%20flow%2C%20coupled%20with%20variability%20in%0Alaboratory%20conditions%20necessitates%20adaptive%20automation.%20This%20work%20introduces%0AFLIP%2C%20a%20flowability-informed%20powder%20weighing%20framework%20designed%20to%20enhance%0Arobotic%20policy%20learning%20for%20granular%20material%20handling.%20Our%20key%20contribution%0Alies%20in%20using%20material%20flowability%2C%20quantified%20by%20the%20angle%20of%20repose%2C%20to%0Aoptimise%20physics-based%20simulations%20through%20Bayesian%20inference.%20This%20yields%0Amaterial-specific%20simulation%20environments%20capable%20of%20generating%20accurate%0Atraining%20data%2C%20which%20reflects%20diverse%20powder%20behaviours%2C%20for%20training%20%60robot%0Achemists%27.%20Building%20on%20this%2C%20FLIP%20integrates%20quantified%20flowability%20into%20a%0Acurriculum%20learning%20strategy%2C%20fostering%20efficient%20acquisition%20of%20robust%20robotic%0Apolicies%20by%20gradually%20introducing%20more%20challenging%2C%20less%20flowable%20powders.%20We%0Avalidate%20the%20efficacy%20of%20our%20method%20on%20a%20robotic%20powder%20weighing%20task%20under%0Areal-world%20laboratory%20conditions.%20Experimental%20results%20show%20that%20FLIP%20with%20a%0Acurriculum%20strategy%20achieves%20a%20low%20dispensing%20error%20of%202.12%20%2B-%201.53%20mg%2C%0Aoutperforming%20methods%20that%20do%20not%20leverage%20flowability%20data%2C%20such%20as%20domain%0Arandomisation%20%286.11%20%2B-%203.92%20mg%29.%20These%20results%20demonstrate%20FLIP%27s%20improved%0Aability%20to%20generalise%20to%20previously%20unseen%2C%20more%20cohesive%20powders%20and%20to%20new%0Atarget%20masses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLIP%253A%2520Flowability-Informed%2520Powder%2520Weighing%26entry.906535625%3DNikola%2520Radulov%2520and%2520Alex%2520Wright%2520and%2520Thomas%2520little%2520and%2520Andrew%2520I.%2520Cooper%2520and%2520Gabriella%2520Pizzuto%26entry.1292438233%3D%2520%2520Autonomous%2520manipulation%2520of%2520powders%2520remains%2520a%2520significant%2520challenge%2520for%250Arobotic%2520automation%2520in%2520scientific%2520laboratories.%2520The%2520inherent%2520variability%2520and%250Acomplex%2520physical%2520interactions%2520of%2520powders%2520in%2520flow%252C%2520coupled%2520with%2520variability%2520in%250Alaboratory%2520conditions%2520necessitates%2520adaptive%2520automation.%2520This%2520work%2520introduces%250AFLIP%252C%2520a%2520flowability-informed%2520powder%2520weighing%2520framework%2520designed%2520to%2520enhance%250Arobotic%2520policy%2520learning%2520for%2520granular%2520material%2520handling.%2520Our%2520key%2520contribution%250Alies%2520in%2520using%2520material%2520flowability%252C%2520quantified%2520by%2520the%2520angle%2520of%2520repose%252C%2520to%250Aoptimise%2520physics-based%2520simulations%2520through%2520Bayesian%2520inference.%2520This%2520yields%250Amaterial-specific%2520simulation%2520environments%2520capable%2520of%2520generating%2520accurate%250Atraining%2520data%252C%2520which%2520reflects%2520diverse%2520powder%2520behaviours%252C%2520for%2520training%2520%2560robot%250Achemists%2527.%2520Building%2520on%2520this%252C%2520FLIP%2520integrates%2520quantified%2520flowability%2520into%2520a%250Acurriculum%2520learning%2520strategy%252C%2520fostering%2520efficient%2520acquisition%2520of%2520robust%2520robotic%250Apolicies%2520by%2520gradually%2520introducing%2520more%2520challenging%252C%2520less%2520flowable%2520powders.%2520We%250Avalidate%2520the%2520efficacy%2520of%2520our%2520method%2520on%2520a%2520robotic%2520powder%2520weighing%2520task%2520under%250Areal-world%2520laboratory%2520conditions.%2520Experimental%2520results%2520show%2520that%2520FLIP%2520with%2520a%250Acurriculum%2520strategy%2520achieves%2520a%2520low%2520dispensing%2520error%2520of%25202.12%2520%252B-%25201.53%2520mg%252C%250Aoutperforming%2520methods%2520that%2520do%2520not%2520leverage%2520flowability%2520data%252C%2520such%2520as%2520domain%250Arandomisation%2520%25286.11%2520%252B-%25203.92%2520mg%2529.%2520These%2520results%2520demonstrate%2520FLIP%2527s%2520improved%250Aability%2520to%2520generalise%2520to%2520previously%2520unseen%252C%2520more%2520cohesive%2520powders%2520and%2520to%2520new%250Atarget%2520masses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLIP%3A%20Flowability-Informed%20Powder%20Weighing&entry.906535625=Nikola%20Radulov%20and%20Alex%20Wright%20and%20Thomas%20little%20and%20Andrew%20I.%20Cooper%20and%20Gabriella%20Pizzuto&entry.1292438233=%20%20Autonomous%20manipulation%20of%20powders%20remains%20a%20significant%20challenge%20for%0Arobotic%20automation%20in%20scientific%20laboratories.%20The%20inherent%20variability%20and%0Acomplex%20physical%20interactions%20of%20powders%20in%20flow%2C%20coupled%20with%20variability%20in%0Alaboratory%20conditions%20necessitates%20adaptive%20automation.%20This%20work%20introduces%0AFLIP%2C%20a%20flowability-informed%20powder%20weighing%20framework%20designed%20to%20enhance%0Arobotic%20policy%20learning%20for%20granular%20material%20handling.%20Our%20key%20contribution%0Alies%20in%20using%20material%20flowability%2C%20quantified%20by%20the%20angle%20of%20repose%2C%20to%0Aoptimise%20physics-based%20simulations%20through%20Bayesian%20inference.%20This%20yields%0Amaterial-specific%20simulation%20environments%20capable%20of%20generating%20accurate%0Atraining%20data%2C%20which%20reflects%20diverse%20powder%20behaviours%2C%20for%20training%20%60robot%0Achemists%27.%20Building%20on%20this%2C%20FLIP%20integrates%20quantified%20flowability%20into%20a%0Acurriculum%20learning%20strategy%2C%20fostering%20efficient%20acquisition%20of%20robust%20robotic%0Apolicies%20by%20gradually%20introducing%20more%20challenging%2C%20less%20flowable%20powders.%20We%0Avalidate%20the%20efficacy%20of%20our%20method%20on%20a%20robotic%20powder%20weighing%20task%20under%0Areal-world%20laboratory%20conditions.%20Experimental%20results%20show%20that%20FLIP%20with%20a%0Acurriculum%20strategy%20achieves%20a%20low%20dispensing%20error%20of%202.12%20%2B-%201.53%20mg%2C%0Aoutperforming%20methods%20that%20do%20not%20leverage%20flowability%20data%2C%20such%20as%20domain%0Arandomisation%20%286.11%20%2B-%203.92%20mg%29.%20These%20results%20demonstrate%20FLIP%27s%20improved%0Aability%20to%20generalise%20to%20previously%20unseen%2C%20more%20cohesive%20powders%20and%20to%20new%0Atarget%20masses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03896v1&entry.124074799=Read"},
{"title": "Weight for Robustness: A Comprehensive Approach towards Optimal\n  Fault-Tolerant Asynchronous ML", "author": "Tehila Dahan and Kfir Y. Levy", "abstract": "  We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems.\n", "link": "http://arxiv.org/abs/2501.09621v2", "date": "2025-06-04", "relevancy": 1.8507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weight%20for%20Robustness%3A%20A%20Comprehensive%20Approach%20towards%20Optimal%0A%20%20Fault-Tolerant%20Asynchronous%20ML&body=Title%3A%20Weight%20for%20Robustness%3A%20A%20Comprehensive%20Approach%20towards%20Optimal%0A%20%20Fault-Tolerant%20Asynchronous%20ML%0AAuthor%3A%20Tehila%20Dahan%20and%20Kfir%20Y.%20Levy%0AAbstract%3A%20%20%20We%20address%20the%20challenges%20of%20Byzantine-robust%20training%20in%20asynchronous%0Adistributed%20machine%20learning%20systems%2C%20aiming%20to%20enhance%20efficiency%20amid%20massive%0Aparallelization%20and%20heterogeneous%20computing%20resources.%20Asynchronous%20systems%2C%0Amarked%20by%20independently%20operating%20workers%20and%20intermittent%20updates%2C%20uniquely%0Astruggle%20with%20maintaining%20integrity%20against%20Byzantine%20failures%2C%20which%20encompass%0Amalicious%20or%20erroneous%20actions%20that%20disrupt%20learning.%20The%20inherent%20delays%20in%0Asuch%20settings%20not%20only%20introduce%20additional%20bias%20to%20the%20system%20but%20also%20obscure%0Athe%20disruptions%20caused%20by%20Byzantine%20faults.%20To%20tackle%20these%20issues%2C%20we%20adapt%0Athe%20Byzantine%20framework%20to%20asynchronous%20dynamics%20by%20introducing%20a%20novel%0Aweighted%20robust%20aggregation%20framework.%20This%20allows%20for%20the%20extension%20of%20robust%0Aaggregators%20and%20a%20recent%20meta-aggregator%20to%20their%20weighted%20versions%2C%20mitigating%0Athe%20effects%20of%20delayed%20updates.%20By%20further%20incorporating%20a%20recent%0Avariance-reduction%20technique%2C%20we%20achieve%20an%20optimal%20convergence%20rate%20for%20the%0Afirst%20time%20in%20an%20asynchronous%20Byzantine%20environment.%20Our%20methodology%20is%0Arigorously%20validated%20through%20empirical%20and%20theoretical%20analysis%2C%20demonstrating%0Aits%20effectiveness%20in%20enhancing%20fault%20tolerance%20and%20optimizing%20performance%20in%0Aasynchronous%20ML%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeight%2520for%2520Robustness%253A%2520A%2520Comprehensive%2520Approach%2520towards%2520Optimal%250A%2520%2520Fault-Tolerant%2520Asynchronous%2520ML%26entry.906535625%3DTehila%2520Dahan%2520and%2520Kfir%2520Y.%2520Levy%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenges%2520of%2520Byzantine-robust%2520training%2520in%2520asynchronous%250Adistributed%2520machine%2520learning%2520systems%252C%2520aiming%2520to%2520enhance%2520efficiency%2520amid%2520massive%250Aparallelization%2520and%2520heterogeneous%2520computing%2520resources.%2520Asynchronous%2520systems%252C%250Amarked%2520by%2520independently%2520operating%2520workers%2520and%2520intermittent%2520updates%252C%2520uniquely%250Astruggle%2520with%2520maintaining%2520integrity%2520against%2520Byzantine%2520failures%252C%2520which%2520encompass%250Amalicious%2520or%2520erroneous%2520actions%2520that%2520disrupt%2520learning.%2520The%2520inherent%2520delays%2520in%250Asuch%2520settings%2520not%2520only%2520introduce%2520additional%2520bias%2520to%2520the%2520system%2520but%2520also%2520obscure%250Athe%2520disruptions%2520caused%2520by%2520Byzantine%2520faults.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520adapt%250Athe%2520Byzantine%2520framework%2520to%2520asynchronous%2520dynamics%2520by%2520introducing%2520a%2520novel%250Aweighted%2520robust%2520aggregation%2520framework.%2520This%2520allows%2520for%2520the%2520extension%2520of%2520robust%250Aaggregators%2520and%2520a%2520recent%2520meta-aggregator%2520to%2520their%2520weighted%2520versions%252C%2520mitigating%250Athe%2520effects%2520of%2520delayed%2520updates.%2520By%2520further%2520incorporating%2520a%2520recent%250Avariance-reduction%2520technique%252C%2520we%2520achieve%2520an%2520optimal%2520convergence%2520rate%2520for%2520the%250Afirst%2520time%2520in%2520an%2520asynchronous%2520Byzantine%2520environment.%2520Our%2520methodology%2520is%250Arigorously%2520validated%2520through%2520empirical%2520and%2520theoretical%2520analysis%252C%2520demonstrating%250Aits%2520effectiveness%2520in%2520enhancing%2520fault%2520tolerance%2520and%2520optimizing%2520performance%2520in%250Aasynchronous%2520ML%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weight%20for%20Robustness%3A%20A%20Comprehensive%20Approach%20towards%20Optimal%0A%20%20Fault-Tolerant%20Asynchronous%20ML&entry.906535625=Tehila%20Dahan%20and%20Kfir%20Y.%20Levy&entry.1292438233=%20%20We%20address%20the%20challenges%20of%20Byzantine-robust%20training%20in%20asynchronous%0Adistributed%20machine%20learning%20systems%2C%20aiming%20to%20enhance%20efficiency%20amid%20massive%0Aparallelization%20and%20heterogeneous%20computing%20resources.%20Asynchronous%20systems%2C%0Amarked%20by%20independently%20operating%20workers%20and%20intermittent%20updates%2C%20uniquely%0Astruggle%20with%20maintaining%20integrity%20against%20Byzantine%20failures%2C%20which%20encompass%0Amalicious%20or%20erroneous%20actions%20that%20disrupt%20learning.%20The%20inherent%20delays%20in%0Asuch%20settings%20not%20only%20introduce%20additional%20bias%20to%20the%20system%20but%20also%20obscure%0Athe%20disruptions%20caused%20by%20Byzantine%20faults.%20To%20tackle%20these%20issues%2C%20we%20adapt%0Athe%20Byzantine%20framework%20to%20asynchronous%20dynamics%20by%20introducing%20a%20novel%0Aweighted%20robust%20aggregation%20framework.%20This%20allows%20for%20the%20extension%20of%20robust%0Aaggregators%20and%20a%20recent%20meta-aggregator%20to%20their%20weighted%20versions%2C%20mitigating%0Athe%20effects%20of%20delayed%20updates.%20By%20further%20incorporating%20a%20recent%0Avariance-reduction%20technique%2C%20we%20achieve%20an%20optimal%20convergence%20rate%20for%20the%0Afirst%20time%20in%20an%20asynchronous%20Byzantine%20environment.%20Our%20methodology%20is%0Arigorously%20validated%20through%20empirical%20and%20theoretical%20analysis%2C%20demonstrating%0Aits%20effectiveness%20in%20enhancing%20fault%20tolerance%20and%20optimizing%20performance%20in%0Aasynchronous%20ML%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09621v2&entry.124074799=Read"},
{"title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results", "author": "Andrea Santilli and Adam Golinski and Michael Kirchhof and Federico Danieli and Arno Blaas and Miao Xiong and Luca Zappella and Sinead Williamson", "abstract": "  Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving\ntheir safety and reliability. Evaluations often use metrics like AUROC to\nassess how well UQ methods (e.g., negative sequence probabilities) correlate\nwith task correctness functions (e.g., ROUGE-L). We show that mutual\nbiases--when both UQ methods and correctness functions are biased by the same\nfactors--systematically distort evaluation. First, we formally prove that any\nmutual bias non-randomly skews AUROC rankings, compromising benchmark\nintegrity. Second, we confirm this happens empirically by testing 7 widely used\ncorrectness functions, from lexical-based and embedding-based metrics to\nLM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our\nanalysis shows that length biases in correctness functions distort UQ\nassessments by interacting with length biases in UQ methods. We identify\nLM-as-a-judge methods as the least length-biased, offering a promising path for\na fairer UQ evaluation.\n", "link": "http://arxiv.org/abs/2504.13677v2", "date": "2025-06-04", "relevancy": 1.9633, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4981}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results&body=Title%3A%20Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results%0AAuthor%3A%20Andrea%20Santilli%20and%20Adam%20Golinski%20and%20Michael%20Kirchhof%20and%20Federico%20Danieli%20and%20Arno%20Blaas%20and%20Miao%20Xiong%20and%20Luca%20Zappella%20and%20Sinead%20Williamson%0AAbstract%3A%20%20%20Uncertainty%20Quantification%20%28UQ%29%20in%20Language%20Models%20%28LMs%29%20is%20key%20to%20improving%0Atheir%20safety%20and%20reliability.%20Evaluations%20often%20use%20metrics%20like%20AUROC%20to%0Aassess%20how%20well%20UQ%20methods%20%28e.g.%2C%20negative%20sequence%20probabilities%29%20correlate%0Awith%20task%20correctness%20functions%20%28e.g.%2C%20ROUGE-L%29.%20We%20show%20that%20mutual%0Abiases--when%20both%20UQ%20methods%20and%20correctness%20functions%20are%20biased%20by%20the%20same%0Afactors--systematically%20distort%20evaluation.%20First%2C%20we%20formally%20prove%20that%20any%0Amutual%20bias%20non-randomly%20skews%20AUROC%20rankings%2C%20compromising%20benchmark%0Aintegrity.%20Second%2C%20we%20confirm%20this%20happens%20empirically%20by%20testing%207%20widely%20used%0Acorrectness%20functions%2C%20from%20lexical-based%20and%20embedding-based%20metrics%20to%0ALM-as-a-judge%20approaches%2C%20across%204%20datasets%20x%204%20models%20x%208%20UQ%20methods.%20Our%0Aanalysis%20shows%20that%20length%20biases%20in%20correctness%20functions%20distort%20UQ%0Aassessments%20by%20interacting%20with%20length%20biases%20in%20UQ%20methods.%20We%20identify%0ALM-as-a-judge%20methods%20as%20the%20least%20length-biased%2C%20offering%20a%20promising%20path%20for%0Aa%20fairer%20UQ%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Uncertainty%2520Quantification%2520Evaluation%2520in%2520Language%2520Models%253A%250A%2520%2520Spurious%2520Interactions%2520with%2520Response%2520Length%2520Bias%2520Results%26entry.906535625%3DAndrea%2520Santilli%2520and%2520Adam%2520Golinski%2520and%2520Michael%2520Kirchhof%2520and%2520Federico%2520Danieli%2520and%2520Arno%2520Blaas%2520and%2520Miao%2520Xiong%2520and%2520Luca%2520Zappella%2520and%2520Sinead%2520Williamson%26entry.1292438233%3D%2520%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520in%2520Language%2520Models%2520%2528LMs%2529%2520is%2520key%2520to%2520improving%250Atheir%2520safety%2520and%2520reliability.%2520Evaluations%2520often%2520use%2520metrics%2520like%2520AUROC%2520to%250Aassess%2520how%2520well%2520UQ%2520methods%2520%2528e.g.%252C%2520negative%2520sequence%2520probabilities%2529%2520correlate%250Awith%2520task%2520correctness%2520functions%2520%2528e.g.%252C%2520ROUGE-L%2529.%2520We%2520show%2520that%2520mutual%250Abiases--when%2520both%2520UQ%2520methods%2520and%2520correctness%2520functions%2520are%2520biased%2520by%2520the%2520same%250Afactors--systematically%2520distort%2520evaluation.%2520First%252C%2520we%2520formally%2520prove%2520that%2520any%250Amutual%2520bias%2520non-randomly%2520skews%2520AUROC%2520rankings%252C%2520compromising%2520benchmark%250Aintegrity.%2520Second%252C%2520we%2520confirm%2520this%2520happens%2520empirically%2520by%2520testing%25207%2520widely%2520used%250Acorrectness%2520functions%252C%2520from%2520lexical-based%2520and%2520embedding-based%2520metrics%2520to%250ALM-as-a-judge%2520approaches%252C%2520across%25204%2520datasets%2520x%25204%2520models%2520x%25208%2520UQ%2520methods.%2520Our%250Aanalysis%2520shows%2520that%2520length%2520biases%2520in%2520correctness%2520functions%2520distort%2520UQ%250Aassessments%2520by%2520interacting%2520with%2520length%2520biases%2520in%2520UQ%2520methods.%2520We%2520identify%250ALM-as-a-judge%2520methods%2520as%2520the%2520least%2520length-biased%252C%2520offering%2520a%2520promising%2520path%2520for%250Aa%2520fairer%2520UQ%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results&entry.906535625=Andrea%20Santilli%20and%20Adam%20Golinski%20and%20Michael%20Kirchhof%20and%20Federico%20Danieli%20and%20Arno%20Blaas%20and%20Miao%20Xiong%20and%20Luca%20Zappella%20and%20Sinead%20Williamson&entry.1292438233=%20%20Uncertainty%20Quantification%20%28UQ%29%20in%20Language%20Models%20%28LMs%29%20is%20key%20to%20improving%0Atheir%20safety%20and%20reliability.%20Evaluations%20often%20use%20metrics%20like%20AUROC%20to%0Aassess%20how%20well%20UQ%20methods%20%28e.g.%2C%20negative%20sequence%20probabilities%29%20correlate%0Awith%20task%20correctness%20functions%20%28e.g.%2C%20ROUGE-L%29.%20We%20show%20that%20mutual%0Abiases--when%20both%20UQ%20methods%20and%20correctness%20functions%20are%20biased%20by%20the%20same%0Afactors--systematically%20distort%20evaluation.%20First%2C%20we%20formally%20prove%20that%20any%0Amutual%20bias%20non-randomly%20skews%20AUROC%20rankings%2C%20compromising%20benchmark%0Aintegrity.%20Second%2C%20we%20confirm%20this%20happens%20empirically%20by%20testing%207%20widely%20used%0Acorrectness%20functions%2C%20from%20lexical-based%20and%20embedding-based%20metrics%20to%0ALM-as-a-judge%20approaches%2C%20across%204%20datasets%20x%204%20models%20x%208%20UQ%20methods.%20Our%0Aanalysis%20shows%20that%20length%20biases%20in%20correctness%20functions%20distort%20UQ%0Aassessments%20by%20interacting%20with%20length%20biases%20in%20UQ%20methods.%20We%20identify%0ALM-as-a-judge%20methods%20as%20the%20least%20length-biased%2C%20offering%20a%20promising%20path%20for%0Aa%20fairer%20UQ%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13677v2&entry.124074799=Read"},
{"title": "Multimodal Tabular Reasoning with Privileged Structured Information", "author": "Jun-Peng Jiang and Yu Xia and Hai-Long Sun and Shiyin Lu and Qing-Guo Chen and Weihua Luo and Kaifu Zhang and De-Chuan Zhan and Han-Jia Ye", "abstract": "  Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.\n", "link": "http://arxiv.org/abs/2506.04088v1", "date": "2025-06-04", "relevancy": 1.6396, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5101}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Tabular%20Reasoning%20with%20Privileged%20Structured%20Information&body=Title%3A%20Multimodal%20Tabular%20Reasoning%20with%20Privileged%20Structured%20Information%0AAuthor%3A%20Jun-Peng%20Jiang%20and%20Yu%20Xia%20and%20Hai-Long%20Sun%20and%20Shiyin%20Lu%20and%20Qing-Guo%20Chen%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Tabular%20reasoning%20involves%20multi-step%20information%20extraction%20and%20logical%0Ainference%20over%20tabular%20data.%20While%20recent%20advances%20have%20leveraged%20large%0Alanguage%20models%20%28LLMs%29%20for%20reasoning%20over%20structured%20tables%2C%20such%20high-quality%0Atextual%20representations%20are%20often%20unavailable%20in%20real-world%20settings%2C%20where%0Atables%20typically%20appear%20as%20images.%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20tabular%0Areasoning%20from%20table%20images%2C%20leveraging%20privileged%20structured%20information%0Aavailable%20during%20training%20to%20enhance%20multimodal%20large%20language%20models%20%28MLLMs%29.%0AThe%20key%20challenges%20lie%20in%20the%20complexity%20of%20accurately%20aligning%20structured%0Ainformation%20with%20visual%20representations%2C%20and%20in%20effectively%20transferring%0Astructured%20reasoning%20skills%20to%20MLLMs%20despite%20the%20input%20modality%20gap.%20To%20address%0Athese%2C%20we%20introduce%20TabUlar%20Reasoning%20with%20Bridged%20infOrmation%20%28%7B%5Csc%20Turbo%7D%29%2C%20a%0Anew%20framework%20for%20multimodal%20tabular%20reasoning%20with%20privileged%20structured%0Atables.%20%7B%5Csc%20Turbo%7D%20benefits%20from%20a%20structure-aware%20reasoning%20trace%20generator%0Abased%20on%20DeepSeek-R1%2C%20contributing%20to%20high-quality%20modality-bridged%20data.%20On%0Athis%20basis%2C%20%7B%5Csc%20Turbo%7D%20repeatedly%20generates%20and%20selects%20the%20advantageous%0Areasoning%20paths%2C%20further%20enhancing%20the%20model%27s%20tabular%20reasoning%20ability.%0AExperimental%20results%20demonstrate%20that%2C%20with%20limited%20%28%249%24k%29%20data%2C%20%7B%5Csc%20Turbo%7D%0Aachieves%20state-of-the-art%20performance%20%28%24%2B7.2%5C%25%24%20vs.%20previous%20SOTA%29%20across%0Amultiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Tabular%2520Reasoning%2520with%2520Privileged%2520Structured%2520Information%26entry.906535625%3DJun-Peng%2520Jiang%2520and%2520Yu%2520Xia%2520and%2520Hai-Long%2520Sun%2520and%2520Shiyin%2520Lu%2520and%2520Qing-Guo%2520Chen%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520De-Chuan%2520Zhan%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Tabular%2520reasoning%2520involves%2520multi-step%2520information%2520extraction%2520and%2520logical%250Ainference%2520over%2520tabular%2520data.%2520While%2520recent%2520advances%2520have%2520leveraged%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520for%2520reasoning%2520over%2520structured%2520tables%252C%2520such%2520high-quality%250Atextual%2520representations%2520are%2520often%2520unavailable%2520in%2520real-world%2520settings%252C%2520where%250Atables%2520typically%2520appear%2520as%2520images.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520task%2520of%2520tabular%250Areasoning%2520from%2520table%2520images%252C%2520leveraging%2520privileged%2520structured%2520information%250Aavailable%2520during%2520training%2520to%2520enhance%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%250AThe%2520key%2520challenges%2520lie%2520in%2520the%2520complexity%2520of%2520accurately%2520aligning%2520structured%250Ainformation%2520with%2520visual%2520representations%252C%2520and%2520in%2520effectively%2520transferring%250Astructured%2520reasoning%2520skills%2520to%2520MLLMs%2520despite%2520the%2520input%2520modality%2520gap.%2520To%2520address%250Athese%252C%2520we%2520introduce%2520TabUlar%2520Reasoning%2520with%2520Bridged%2520infOrmation%2520%2528%257B%255Csc%2520Turbo%257D%2529%252C%2520a%250Anew%2520framework%2520for%2520multimodal%2520tabular%2520reasoning%2520with%2520privileged%2520structured%250Atables.%2520%257B%255Csc%2520Turbo%257D%2520benefits%2520from%2520a%2520structure-aware%2520reasoning%2520trace%2520generator%250Abased%2520on%2520DeepSeek-R1%252C%2520contributing%2520to%2520high-quality%2520modality-bridged%2520data.%2520On%250Athis%2520basis%252C%2520%257B%255Csc%2520Turbo%257D%2520repeatedly%2520generates%2520and%2520selects%2520the%2520advantageous%250Areasoning%2520paths%252C%2520further%2520enhancing%2520the%2520model%2527s%2520tabular%2520reasoning%2520ability.%250AExperimental%2520results%2520demonstrate%2520that%252C%2520with%2520limited%2520%2528%25249%2524k%2529%2520data%252C%2520%257B%255Csc%2520Turbo%257D%250Aachieves%2520state-of-the-art%2520performance%2520%2528%2524%252B7.2%255C%2525%2524%2520vs.%2520previous%2520SOTA%2529%2520across%250Amultiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Tabular%20Reasoning%20with%20Privileged%20Structured%20Information&entry.906535625=Jun-Peng%20Jiang%20and%20Yu%20Xia%20and%20Hai-Long%20Sun%20and%20Shiyin%20Lu%20and%20Qing-Guo%20Chen%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Tabular%20reasoning%20involves%20multi-step%20information%20extraction%20and%20logical%0Ainference%20over%20tabular%20data.%20While%20recent%20advances%20have%20leveraged%20large%0Alanguage%20models%20%28LLMs%29%20for%20reasoning%20over%20structured%20tables%2C%20such%20high-quality%0Atextual%20representations%20are%20often%20unavailable%20in%20real-world%20settings%2C%20where%0Atables%20typically%20appear%20as%20images.%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20tabular%0Areasoning%20from%20table%20images%2C%20leveraging%20privileged%20structured%20information%0Aavailable%20during%20training%20to%20enhance%20multimodal%20large%20language%20models%20%28MLLMs%29.%0AThe%20key%20challenges%20lie%20in%20the%20complexity%20of%20accurately%20aligning%20structured%0Ainformation%20with%20visual%20representations%2C%20and%20in%20effectively%20transferring%0Astructured%20reasoning%20skills%20to%20MLLMs%20despite%20the%20input%20modality%20gap.%20To%20address%0Athese%2C%20we%20introduce%20TabUlar%20Reasoning%20with%20Bridged%20infOrmation%20%28%7B%5Csc%20Turbo%7D%29%2C%20a%0Anew%20framework%20for%20multimodal%20tabular%20reasoning%20with%20privileged%20structured%0Atables.%20%7B%5Csc%20Turbo%7D%20benefits%20from%20a%20structure-aware%20reasoning%20trace%20generator%0Abased%20on%20DeepSeek-R1%2C%20contributing%20to%20high-quality%20modality-bridged%20data.%20On%0Athis%20basis%2C%20%7B%5Csc%20Turbo%7D%20repeatedly%20generates%20and%20selects%20the%20advantageous%0Areasoning%20paths%2C%20further%20enhancing%20the%20model%27s%20tabular%20reasoning%20ability.%0AExperimental%20results%20demonstrate%20that%2C%20with%20limited%20%28%249%24k%29%20data%2C%20%7B%5Csc%20Turbo%7D%0Aachieves%20state-of-the-art%20performance%20%28%24%2B7.2%5C%25%24%20vs.%20previous%20SOTA%29%20across%0Amultiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04088v1&entry.124074799=Read"},
{"title": "Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win", "author": "Lorenz Kummer and Samir Moustafa and Anatol Ehrlich and Franka Bause and Nikolaus Suess and Wilfried N. Gansterer and Nils M. Kriege", "abstract": "  The lottery ticket hypothesis (LTH) is well-studied for convolutional neural\nnetworks but has been validated only empirically for graph neural networks\n(GNNs), for which theoretical findings are largely lacking. In this paper, we\nidentify the expressivity of sparse subnetworks, i.e. their ability to\ndistinguish non-isomorphic graphs, as crucial for finding winning tickets that\npreserve the predictive performance. We establish conditions under which the\nexpressivity of a sparsely initialized GNN matches that of the full network,\nparticularly when compared to the Weisfeiler-Leman test, and in that context\nput forward and prove a Strong Expressive Lottery Ticket Hypothesis. We\nsubsequently show that an increased expressivity in the initialization\npotentially accelerates model convergence and improves generalization. Our\nfindings establish novel theoretical foundations for both LTH and GNN research,\nhighlighting the importance of maintaining expressivity in sparsely initialized\nGNNs. We illustrate our results using examples from drug discovery.\n", "link": "http://arxiv.org/abs/2506.03919v1", "date": "2025-06-04", "relevancy": 1.9367, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4714}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weisfeiler%20and%20Leman%20Go%20Gambling%3A%20Why%20Expressive%20Lottery%20Tickets%20Win&body=Title%3A%20Weisfeiler%20and%20Leman%20Go%20Gambling%3A%20Why%20Expressive%20Lottery%20Tickets%20Win%0AAuthor%3A%20Lorenz%20Kummer%20and%20Samir%20Moustafa%20and%20Anatol%20Ehrlich%20and%20Franka%20Bause%20and%20Nikolaus%20Suess%20and%20Wilfried%20N.%20Gansterer%20and%20Nils%20M.%20Kriege%0AAbstract%3A%20%20%20The%20lottery%20ticket%20hypothesis%20%28LTH%29%20is%20well-studied%20for%20convolutional%20neural%0Anetworks%20but%20has%20been%20validated%20only%20empirically%20for%20graph%20neural%20networks%0A%28GNNs%29%2C%20for%20which%20theoretical%20findings%20are%20largely%20lacking.%20In%20this%20paper%2C%20we%0Aidentify%20the%20expressivity%20of%20sparse%20subnetworks%2C%20i.e.%20their%20ability%20to%0Adistinguish%20non-isomorphic%20graphs%2C%20as%20crucial%20for%20finding%20winning%20tickets%20that%0Apreserve%20the%20predictive%20performance.%20We%20establish%20conditions%20under%20which%20the%0Aexpressivity%20of%20a%20sparsely%20initialized%20GNN%20matches%20that%20of%20the%20full%20network%2C%0Aparticularly%20when%20compared%20to%20the%20Weisfeiler-Leman%20test%2C%20and%20in%20that%20context%0Aput%20forward%20and%20prove%20a%20Strong%20Expressive%20Lottery%20Ticket%20Hypothesis.%20We%0Asubsequently%20show%20that%20an%20increased%20expressivity%20in%20the%20initialization%0Apotentially%20accelerates%20model%20convergence%20and%20improves%20generalization.%20Our%0Afindings%20establish%20novel%20theoretical%20foundations%20for%20both%20LTH%20and%20GNN%20research%2C%0Ahighlighting%20the%20importance%20of%20maintaining%20expressivity%20in%20sparsely%20initialized%0AGNNs.%20We%20illustrate%20our%20results%20using%20examples%20from%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeisfeiler%2520and%2520Leman%2520Go%2520Gambling%253A%2520Why%2520Expressive%2520Lottery%2520Tickets%2520Win%26entry.906535625%3DLorenz%2520Kummer%2520and%2520Samir%2520Moustafa%2520and%2520Anatol%2520Ehrlich%2520and%2520Franka%2520Bause%2520and%2520Nikolaus%2520Suess%2520and%2520Wilfried%2520N.%2520Gansterer%2520and%2520Nils%2520M.%2520Kriege%26entry.1292438233%3D%2520%2520The%2520lottery%2520ticket%2520hypothesis%2520%2528LTH%2529%2520is%2520well-studied%2520for%2520convolutional%2520neural%250Anetworks%2520but%2520has%2520been%2520validated%2520only%2520empirically%2520for%2520graph%2520neural%2520networks%250A%2528GNNs%2529%252C%2520for%2520which%2520theoretical%2520findings%2520are%2520largely%2520lacking.%2520In%2520this%2520paper%252C%2520we%250Aidentify%2520the%2520expressivity%2520of%2520sparse%2520subnetworks%252C%2520i.e.%2520their%2520ability%2520to%250Adistinguish%2520non-isomorphic%2520graphs%252C%2520as%2520crucial%2520for%2520finding%2520winning%2520tickets%2520that%250Apreserve%2520the%2520predictive%2520performance.%2520We%2520establish%2520conditions%2520under%2520which%2520the%250Aexpressivity%2520of%2520a%2520sparsely%2520initialized%2520GNN%2520matches%2520that%2520of%2520the%2520full%2520network%252C%250Aparticularly%2520when%2520compared%2520to%2520the%2520Weisfeiler-Leman%2520test%252C%2520and%2520in%2520that%2520context%250Aput%2520forward%2520and%2520prove%2520a%2520Strong%2520Expressive%2520Lottery%2520Ticket%2520Hypothesis.%2520We%250Asubsequently%2520show%2520that%2520an%2520increased%2520expressivity%2520in%2520the%2520initialization%250Apotentially%2520accelerates%2520model%2520convergence%2520and%2520improves%2520generalization.%2520Our%250Afindings%2520establish%2520novel%2520theoretical%2520foundations%2520for%2520both%2520LTH%2520and%2520GNN%2520research%252C%250Ahighlighting%2520the%2520importance%2520of%2520maintaining%2520expressivity%2520in%2520sparsely%2520initialized%250AGNNs.%2520We%2520illustrate%2520our%2520results%2520using%2520examples%2520from%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weisfeiler%20and%20Leman%20Go%20Gambling%3A%20Why%20Expressive%20Lottery%20Tickets%20Win&entry.906535625=Lorenz%20Kummer%20and%20Samir%20Moustafa%20and%20Anatol%20Ehrlich%20and%20Franka%20Bause%20and%20Nikolaus%20Suess%20and%20Wilfried%20N.%20Gansterer%20and%20Nils%20M.%20Kriege&entry.1292438233=%20%20The%20lottery%20ticket%20hypothesis%20%28LTH%29%20is%20well-studied%20for%20convolutional%20neural%0Anetworks%20but%20has%20been%20validated%20only%20empirically%20for%20graph%20neural%20networks%0A%28GNNs%29%2C%20for%20which%20theoretical%20findings%20are%20largely%20lacking.%20In%20this%20paper%2C%20we%0Aidentify%20the%20expressivity%20of%20sparse%20subnetworks%2C%20i.e.%20their%20ability%20to%0Adistinguish%20non-isomorphic%20graphs%2C%20as%20crucial%20for%20finding%20winning%20tickets%20that%0Apreserve%20the%20predictive%20performance.%20We%20establish%20conditions%20under%20which%20the%0Aexpressivity%20of%20a%20sparsely%20initialized%20GNN%20matches%20that%20of%20the%20full%20network%2C%0Aparticularly%20when%20compared%20to%20the%20Weisfeiler-Leman%20test%2C%20and%20in%20that%20context%0Aput%20forward%20and%20prove%20a%20Strong%20Expressive%20Lottery%20Ticket%20Hypothesis.%20We%0Asubsequently%20show%20that%20an%20increased%20expressivity%20in%20the%20initialization%0Apotentially%20accelerates%20model%20convergence%20and%20improves%20generalization.%20Our%0Afindings%20establish%20novel%20theoretical%20foundations%20for%20both%20LTH%20and%20GNN%20research%2C%0Ahighlighting%20the%20importance%20of%20maintaining%20expressivity%20in%20sparsely%20initialized%0AGNNs.%20We%20illustrate%20our%20results%20using%20examples%20from%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03919v1&entry.124074799=Read"},
{"title": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images", "author": "Yeqi Fang and Rong Zhou", "abstract": "  Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging.\n", "link": "http://arxiv.org/abs/2504.00816v3", "date": "2025-06-04", "relevancy": 1.5705, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5283}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images&body=Title%3A%20Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images%0AAuthor%3A%20Yeqi%20Fang%20and%20Rong%20Zhou%0AAbstract%3A%20%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20molecular%20imaging%20tool%0Awidely%20used%20in%20medicine.%20Traditional%20PET%20systems%20rely%20on%20complete%20detector%0Arings%20for%20full%20angular%20coverage%20and%20reliable%20data%20collection.%20However%2C%0Aincomplete-ring%20PET%20scanners%20have%20emerged%20due%20to%20hardware%20failures%2C%20cost%0Aconstraints%2C%20or%20specific%20clinical%20needs.%20Standard%20reconstruction%20algorithms%0Aoften%20suffer%20from%20performance%20degradation%20with%20these%20systems%20because%20of%20reduced%0Adata%20completeness%20and%20geometric%20inconsistencies.%20We%20present%20a%20two-stage%0Adeep-learning%20framework%20that%2C%20without%20incorporating%20any%20time-of-flight%20%28TOF%29%0Ainformation%2C%20restores%20high-quality%20images%20from%20data%20with%20about%2050%25%20missing%0Acoincidences%20-%20double%20the%20loss%20levels%20previously%20addressed%20by%20CNN-based%0Amethods.%20The%20pipeline%20operates%20in%20two%20stages%3A%20a%20projection-domain%20Attention%0AU-Net%20first%20predicts%20the%20missing%20sections%20of%20the%20sinogram%20by%20leveraging%20spatial%0Acontext%20from%20neighbouring%20slices%2C%20after%20which%20the%20completed%20data%20are%0Areconstructed%20with%20OSEM%20algorithm%20and%20passed%20to%20a%20U-Net-diffusion%20module%20that%0Aremoves%20residual%20artefacts%20while%20reinstating%20high-frequency%20detail.%20Using%20206%0Abrain%20volumes%20from%20a%20public%20dataset%2C%20the%20result%20shows%20that%20our%20model%0Asuccessfully%20preserves%20most%20anatomical%20structures%20and%20tracer%20distribution%0Afeatures%20with%20PSNR%20of%2030.92%20dB%20and%20SSIM%20of%200.9708.%20We%20also%20achieve%20higher%0Ainference%20speed%2C%20thus%20providing%20an%20effective%20solution%20for%20incomplete-ring%20PET%0Aimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00816v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-stage%2520deep%2520learning%2520framework%2520for%2520the%2520restoration%2520of%2520incomplete-ring%250A%2520%2520PET%2520images%26entry.906535625%3DYeqi%2520Fang%2520and%2520Rong%2520Zhou%26entry.1292438233%3D%2520%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520is%2520an%2520important%2520molecular%2520imaging%2520tool%250Awidely%2520used%2520in%2520medicine.%2520Traditional%2520PET%2520systems%2520rely%2520on%2520complete%2520detector%250Arings%2520for%2520full%2520angular%2520coverage%2520and%2520reliable%2520data%2520collection.%2520However%252C%250Aincomplete-ring%2520PET%2520scanners%2520have%2520emerged%2520due%2520to%2520hardware%2520failures%252C%2520cost%250Aconstraints%252C%2520or%2520specific%2520clinical%2520needs.%2520Standard%2520reconstruction%2520algorithms%250Aoften%2520suffer%2520from%2520performance%2520degradation%2520with%2520these%2520systems%2520because%2520of%2520reduced%250Adata%2520completeness%2520and%2520geometric%2520inconsistencies.%2520We%2520present%2520a%2520two-stage%250Adeep-learning%2520framework%2520that%252C%2520without%2520incorporating%2520any%2520time-of-flight%2520%2528TOF%2529%250Ainformation%252C%2520restores%2520high-quality%2520images%2520from%2520data%2520with%2520about%252050%2525%2520missing%250Acoincidences%2520-%2520double%2520the%2520loss%2520levels%2520previously%2520addressed%2520by%2520CNN-based%250Amethods.%2520The%2520pipeline%2520operates%2520in%2520two%2520stages%253A%2520a%2520projection-domain%2520Attention%250AU-Net%2520first%2520predicts%2520the%2520missing%2520sections%2520of%2520the%2520sinogram%2520by%2520leveraging%2520spatial%250Acontext%2520from%2520neighbouring%2520slices%252C%2520after%2520which%2520the%2520completed%2520data%2520are%250Areconstructed%2520with%2520OSEM%2520algorithm%2520and%2520passed%2520to%2520a%2520U-Net-diffusion%2520module%2520that%250Aremoves%2520residual%2520artefacts%2520while%2520reinstating%2520high-frequency%2520detail.%2520Using%2520206%250Abrain%2520volumes%2520from%2520a%2520public%2520dataset%252C%2520the%2520result%2520shows%2520that%2520our%2520model%250Asuccessfully%2520preserves%2520most%2520anatomical%2520structures%2520and%2520tracer%2520distribution%250Afeatures%2520with%2520PSNR%2520of%252030.92%2520dB%2520and%2520SSIM%2520of%25200.9708.%2520We%2520also%2520achieve%2520higher%250Ainference%2520speed%252C%2520thus%2520providing%2520an%2520effective%2520solution%2520for%2520incomplete-ring%2520PET%250Aimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00816v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-stage%20deep%20learning%20framework%20for%20the%20restoration%20of%20incomplete-ring%0A%20%20PET%20images&entry.906535625=Yeqi%20Fang%20and%20Rong%20Zhou&entry.1292438233=%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20molecular%20imaging%20tool%0Awidely%20used%20in%20medicine.%20Traditional%20PET%20systems%20rely%20on%20complete%20detector%0Arings%20for%20full%20angular%20coverage%20and%20reliable%20data%20collection.%20However%2C%0Aincomplete-ring%20PET%20scanners%20have%20emerged%20due%20to%20hardware%20failures%2C%20cost%0Aconstraints%2C%20or%20specific%20clinical%20needs.%20Standard%20reconstruction%20algorithms%0Aoften%20suffer%20from%20performance%20degradation%20with%20these%20systems%20because%20of%20reduced%0Adata%20completeness%20and%20geometric%20inconsistencies.%20We%20present%20a%20two-stage%0Adeep-learning%20framework%20that%2C%20without%20incorporating%20any%20time-of-flight%20%28TOF%29%0Ainformation%2C%20restores%20high-quality%20images%20from%20data%20with%20about%2050%25%20missing%0Acoincidences%20-%20double%20the%20loss%20levels%20previously%20addressed%20by%20CNN-based%0Amethods.%20The%20pipeline%20operates%20in%20two%20stages%3A%20a%20projection-domain%20Attention%0AU-Net%20first%20predicts%20the%20missing%20sections%20of%20the%20sinogram%20by%20leveraging%20spatial%0Acontext%20from%20neighbouring%20slices%2C%20after%20which%20the%20completed%20data%20are%0Areconstructed%20with%20OSEM%20algorithm%20and%20passed%20to%20a%20U-Net-diffusion%20module%20that%0Aremoves%20residual%20artefacts%20while%20reinstating%20high-frequency%20detail.%20Using%20206%0Abrain%20volumes%20from%20a%20public%20dataset%2C%20the%20result%20shows%20that%20our%20model%0Asuccessfully%20preserves%20most%20anatomical%20structures%20and%20tracer%20distribution%0Afeatures%20with%20PSNR%20of%2030.92%20dB%20and%20SSIM%20of%200.9708.%20We%20also%20achieve%20higher%0Ainference%20speed%2C%20thus%20providing%20an%20effective%20solution%20for%20incomplete-ring%20PET%0Aimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00816v3&entry.124074799=Read"},
{"title": "Optimistic critics can empower small actors", "author": "Olya Mastikhina and Dhruv Sreenivas and Pablo Samuel Castro", "abstract": "  Actor-critic methods have been central to many of the recent advances in deep\nreinforcement learning. The most common approach is to use symmetric\narchitectures, whereby both actor and critic have the same network topology and\nnumber of parameters. However, recent works have argued for the advantages of\nasymmetric setups, specifically with the use of smaller actors. We perform\nbroad empirical investigations and analyses to better understand the\nimplications of this and find that, in general, smaller actors result in\nperformance degradation and overfit critics. Our analyses suggest poor data\ncollection, due to value underestimation, as one of the main causes for this\nbehavior, and further highlight the crucial role the critic can play in\nalleviating this pathology. We explore techniques to mitigate the observed\nvalue underestimation, which enables further research in asymmetric\nactor-critic methods.\n", "link": "http://arxiv.org/abs/2506.01016v2", "date": "2025-06-04", "relevancy": 1.6803, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20critics%20can%20empower%20small%20actors&body=Title%3A%20Optimistic%20critics%20can%20empower%20small%20actors%0AAuthor%3A%20Olya%20Mastikhina%20and%20Dhruv%20Sreenivas%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Actor-critic%20methods%20have%20been%20central%20to%20many%20of%20the%20recent%20advances%20in%20deep%0Areinforcement%20learning.%20The%20most%20common%20approach%20is%20to%20use%20symmetric%0Aarchitectures%2C%20whereby%20both%20actor%20and%20critic%20have%20the%20same%20network%20topology%20and%0Anumber%20of%20parameters.%20However%2C%20recent%20works%20have%20argued%20for%20the%20advantages%20of%0Aasymmetric%20setups%2C%20specifically%20with%20the%20use%20of%20smaller%20actors.%20We%20perform%0Abroad%20empirical%20investigations%20and%20analyses%20to%20better%20understand%20the%0Aimplications%20of%20this%20and%20find%20that%2C%20in%20general%2C%20smaller%20actors%20result%20in%0Aperformance%20degradation%20and%20overfit%20critics.%20Our%20analyses%20suggest%20poor%20data%0Acollection%2C%20due%20to%20value%20underestimation%2C%20as%20one%20of%20the%20main%20causes%20for%20this%0Abehavior%2C%20and%20further%20highlight%20the%20crucial%20role%20the%20critic%20can%20play%20in%0Aalleviating%20this%20pathology.%20We%20explore%20techniques%20to%20mitigate%20the%20observed%0Avalue%20underestimation%2C%20which%20enables%20further%20research%20in%20asymmetric%0Aactor-critic%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520critics%2520can%2520empower%2520small%2520actors%26entry.906535625%3DOlya%2520Mastikhina%2520and%2520Dhruv%2520Sreenivas%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Actor-critic%2520methods%2520have%2520been%2520central%2520to%2520many%2520of%2520the%2520recent%2520advances%2520in%2520deep%250Areinforcement%2520learning.%2520The%2520most%2520common%2520approach%2520is%2520to%2520use%2520symmetric%250Aarchitectures%252C%2520whereby%2520both%2520actor%2520and%2520critic%2520have%2520the%2520same%2520network%2520topology%2520and%250Anumber%2520of%2520parameters.%2520However%252C%2520recent%2520works%2520have%2520argued%2520for%2520the%2520advantages%2520of%250Aasymmetric%2520setups%252C%2520specifically%2520with%2520the%2520use%2520of%2520smaller%2520actors.%2520We%2520perform%250Abroad%2520empirical%2520investigations%2520and%2520analyses%2520to%2520better%2520understand%2520the%250Aimplications%2520of%2520this%2520and%2520find%2520that%252C%2520in%2520general%252C%2520smaller%2520actors%2520result%2520in%250Aperformance%2520degradation%2520and%2520overfit%2520critics.%2520Our%2520analyses%2520suggest%2520poor%2520data%250Acollection%252C%2520due%2520to%2520value%2520underestimation%252C%2520as%2520one%2520of%2520the%2520main%2520causes%2520for%2520this%250Abehavior%252C%2520and%2520further%2520highlight%2520the%2520crucial%2520role%2520the%2520critic%2520can%2520play%2520in%250Aalleviating%2520this%2520pathology.%2520We%2520explore%2520techniques%2520to%2520mitigate%2520the%2520observed%250Avalue%2520underestimation%252C%2520which%2520enables%2520further%2520research%2520in%2520asymmetric%250Aactor-critic%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20critics%20can%20empower%20small%20actors&entry.906535625=Olya%20Mastikhina%20and%20Dhruv%20Sreenivas%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Actor-critic%20methods%20have%20been%20central%20to%20many%20of%20the%20recent%20advances%20in%20deep%0Areinforcement%20learning.%20The%20most%20common%20approach%20is%20to%20use%20symmetric%0Aarchitectures%2C%20whereby%20both%20actor%20and%20critic%20have%20the%20same%20network%20topology%20and%0Anumber%20of%20parameters.%20However%2C%20recent%20works%20have%20argued%20for%20the%20advantages%20of%0Aasymmetric%20setups%2C%20specifically%20with%20the%20use%20of%20smaller%20actors.%20We%20perform%0Abroad%20empirical%20investigations%20and%20analyses%20to%20better%20understand%20the%0Aimplications%20of%20this%20and%20find%20that%2C%20in%20general%2C%20smaller%20actors%20result%20in%0Aperformance%20degradation%20and%20overfit%20critics.%20Our%20analyses%20suggest%20poor%20data%0Acollection%2C%20due%20to%20value%20underestimation%2C%20as%20one%20of%20the%20main%20causes%20for%20this%0Abehavior%2C%20and%20further%20highlight%20the%20crucial%20role%20the%20critic%20can%20play%20in%0Aalleviating%20this%20pathology.%20We%20explore%20techniques%20to%20mitigate%20the%20observed%0Avalue%20underestimation%2C%20which%20enables%20further%20research%20in%20asymmetric%0Aactor-critic%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01016v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


