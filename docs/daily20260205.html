<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260203.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "S-MUSt3R: Sliding Multi-view 3D Reconstruction", "author": "Leonid Antsfeld and Boris Chidlovskii and Yohann Cabon and Vincent Leroy and Jerome Revaud", "abstract": "The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.", "link": "http://arxiv.org/abs/2602.04517v1", "date": "2026-02-04", "relevancy": 3.2626, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6671}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6452}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-MUSt3R%3A%20Sliding%20Multi-view%203D%20Reconstruction&body=Title%3A%20S-MUSt3R%3A%20Sliding%20Multi-view%203D%20Reconstruction%0AAuthor%3A%20Leonid%20Antsfeld%20and%20Boris%20Chidlovskii%20and%20Yohann%20Cabon%20and%20Vincent%20Leroy%20and%20Jerome%20Revaud%0AAbstract%3A%20The%20recent%20paradigm%20shift%20in%203D%20vision%20led%20to%20the%20rise%20of%20foundation%20models%20with%20remarkable%20capabilities%20in%203D%20perception%20from%20uncalibrated%20images.%20However%2C%20extending%20these%20models%20to%20large-scale%20RGB%20stream%203D%20reconstruction%20remains%20challenging%20due%20to%20memory%20limitations.%20This%20work%20proposes%20S-MUSt3R%2C%20a%20simple%20and%20efficient%20pipeline%20that%20extends%20the%20limits%20of%20foundation%20models%20for%20monocular%203D%20reconstruction.%20Our%20approach%20addresses%20the%20scalability%20bottleneck%20of%20foundation%20models%20through%20a%20simple%20strategy%20of%20sequence%20segmentation%20followed%20by%20segment%20alignment%20and%20lightweight%20loop%20closure%20optimization.%20Without%20model%20retraining%2C%20we%20benefit%20from%20remarkable%203D%20reconstruction%20capacities%20of%20MUSt3R%20model%20and%20achieve%20trajectory%20and%20reconstruction%20performance%20comparable%20to%20traditional%20methods%20with%20more%20complex%20architecture.%20We%20evaluate%20S-MUSt3R%20on%20TUM%2C%207-Scenes%20and%20proprietary%20robot%20navigation%20datasets%20and%20show%20that%20S-MUSt3R%20runs%20successfully%20on%20long%20RGB%20sequences%20and%20produces%20accurate%20and%20consistent%203D%20reconstruction.%20Our%20results%20highlight%20the%20potential%20of%20leveraging%20the%20MUSt3R%20model%20for%20scalable%20monocular%203D%20scene%20in%20real-world%20settings%2C%20with%20an%20important%20advantage%20of%20making%20predictions%20directly%20in%20the%20metric%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-MUSt3R%253A%2520Sliding%2520Multi-view%25203D%2520Reconstruction%26entry.906535625%3DLeonid%2520Antsfeld%2520and%2520Boris%2520Chidlovskii%2520and%2520Yohann%2520Cabon%2520and%2520Vincent%2520Leroy%2520and%2520Jerome%2520Revaud%26entry.1292438233%3DThe%2520recent%2520paradigm%2520shift%2520in%25203D%2520vision%2520led%2520to%2520the%2520rise%2520of%2520foundation%2520models%2520with%2520remarkable%2520capabilities%2520in%25203D%2520perception%2520from%2520uncalibrated%2520images.%2520However%252C%2520extending%2520these%2520models%2520to%2520large-scale%2520RGB%2520stream%25203D%2520reconstruction%2520remains%2520challenging%2520due%2520to%2520memory%2520limitations.%2520This%2520work%2520proposes%2520S-MUSt3R%252C%2520a%2520simple%2520and%2520efficient%2520pipeline%2520that%2520extends%2520the%2520limits%2520of%2520foundation%2520models%2520for%2520monocular%25203D%2520reconstruction.%2520Our%2520approach%2520addresses%2520the%2520scalability%2520bottleneck%2520of%2520foundation%2520models%2520through%2520a%2520simple%2520strategy%2520of%2520sequence%2520segmentation%2520followed%2520by%2520segment%2520alignment%2520and%2520lightweight%2520loop%2520closure%2520optimization.%2520Without%2520model%2520retraining%252C%2520we%2520benefit%2520from%2520remarkable%25203D%2520reconstruction%2520capacities%2520of%2520MUSt3R%2520model%2520and%2520achieve%2520trajectory%2520and%2520reconstruction%2520performance%2520comparable%2520to%2520traditional%2520methods%2520with%2520more%2520complex%2520architecture.%2520We%2520evaluate%2520S-MUSt3R%2520on%2520TUM%252C%25207-Scenes%2520and%2520proprietary%2520robot%2520navigation%2520datasets%2520and%2520show%2520that%2520S-MUSt3R%2520runs%2520successfully%2520on%2520long%2520RGB%2520sequences%2520and%2520produces%2520accurate%2520and%2520consistent%25203D%2520reconstruction.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520leveraging%2520the%2520MUSt3R%2520model%2520for%2520scalable%2520monocular%25203D%2520scene%2520in%2520real-world%2520settings%252C%2520with%2520an%2520important%2520advantage%2520of%2520making%2520predictions%2520directly%2520in%2520the%2520metric%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-MUSt3R%3A%20Sliding%20Multi-view%203D%20Reconstruction&entry.906535625=Leonid%20Antsfeld%20and%20Boris%20Chidlovskii%20and%20Yohann%20Cabon%20and%20Vincent%20Leroy%20and%20Jerome%20Revaud&entry.1292438233=The%20recent%20paradigm%20shift%20in%203D%20vision%20led%20to%20the%20rise%20of%20foundation%20models%20with%20remarkable%20capabilities%20in%203D%20perception%20from%20uncalibrated%20images.%20However%2C%20extending%20these%20models%20to%20large-scale%20RGB%20stream%203D%20reconstruction%20remains%20challenging%20due%20to%20memory%20limitations.%20This%20work%20proposes%20S-MUSt3R%2C%20a%20simple%20and%20efficient%20pipeline%20that%20extends%20the%20limits%20of%20foundation%20models%20for%20monocular%203D%20reconstruction.%20Our%20approach%20addresses%20the%20scalability%20bottleneck%20of%20foundation%20models%20through%20a%20simple%20strategy%20of%20sequence%20segmentation%20followed%20by%20segment%20alignment%20and%20lightweight%20loop%20closure%20optimization.%20Without%20model%20retraining%2C%20we%20benefit%20from%20remarkable%203D%20reconstruction%20capacities%20of%20MUSt3R%20model%20and%20achieve%20trajectory%20and%20reconstruction%20performance%20comparable%20to%20traditional%20methods%20with%20more%20complex%20architecture.%20We%20evaluate%20S-MUSt3R%20on%20TUM%2C%207-Scenes%20and%20proprietary%20robot%20navigation%20datasets%20and%20show%20that%20S-MUSt3R%20runs%20successfully%20on%20long%20RGB%20sequences%20and%20produces%20accurate%20and%20consistent%203D%20reconstruction.%20Our%20results%20highlight%20the%20potential%20of%20leveraging%20the%20MUSt3R%20model%20for%20scalable%20monocular%203D%20scene%20in%20real-world%20settings%2C%20with%20an%20important%20advantage%20of%20making%20predictions%20directly%20in%20the%20metric%20space.&entry.1838667208=http%3A//arxiv.org/abs/2602.04517v1&entry.124074799=Read"},
{"title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy", "author": "Haijier Chen and Bo Xu and Shoujian Zhang and Haoze Liu and Jiaxuan Lin and Jingrong Wang", "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks, demonstrating the superior multi-task capabilities.", "link": "http://arxiv.org/abs/2509.24385v2", "date": "2026-02-04", "relevancy": 3.2426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vid-LLM%3A%20A%20Compact%20Video-based%203D%20Multimodal%20LLM%20with%20Reconstruction-Reasoning%20Synergy&body=Title%3A%20Vid-LLM%3A%20A%20Compact%20Video-based%203D%20Multimodal%20LLM%20with%20Reconstruction-Reasoning%20Synergy%0AAuthor%3A%20Haijier%20Chen%20and%20Bo%20Xu%20and%20Shoujian%20Zhang%20and%20Haoze%20Liu%20and%20Jiaxuan%20Lin%20and%20Jingrong%20Wang%0AAbstract%3A%20Recent%20developments%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20improved%20Vision-Language%20%28VL%29%20reasoning%20in%202D%20domains.%20However%2C%20extending%20these%20capabilities%20to%203D%20scene%20understanding%20remains%20a%20major%20challenge.%20Existing%203D%20Multimodal%20Large%20Language%20Models%20%283D-MLLMs%29%20often%20depend%20on%203D%20data%20inputs%2C%20which%20limits%20scalability%20and%20generalization.%20To%20address%20this%20limitation%2C%20we%20propose%20Vid-LLM%2C%20a%20video-based%203D-MLLM%20that%20directly%20processes%20video%20inputs%20without%20requiring%20external%203D%20data%2C%20making%20it%20practical%20for%20real-world%20deployment.%20In%20our%20method%2C%20the%20geometric%20prior%20are%20directly%20used%20to%20improve%20the%20performance%20of%20the%20sceen%20perception.%20To%20integrate%20the%20geometric%20cues%20into%20the%20MLLM%20compactly%2C%20we%20design%20a%20Cross-Task%20Adapter%20%28CTA%29%20module%20to%20align%20the%203D%20geometric%20priors%20with%20the%20vision-language%20representations.%20To%20ensure%20geometric%20consistency%20and%20integrity%2C%20we%20introduce%20a%20Metric%20Depth%20Model%20that%20recovers%20real-scale%20geometry%20from%20the%20reconstruction%20outputs.%20Finally%2C%20the%20model%20is%20fine-tuned%20with%20a%20two-stage%20distillation%20optimization%20strategy%2C%20realizing%20fast%20convergence%20and%20stabilizes%20training.%20Extensive%20experiments%20across%20diverse%20benchmarks%20verified%20the%20effectiveness%20of%20our%20method%20on%203D%20Question%20Answering%2C%203D%20Dense%20Captioning%20and%203D%20Visual%20Grounding%20tasks%2C%20demonstrating%20the%20superior%20multi-task%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVid-LLM%253A%2520A%2520Compact%2520Video-based%25203D%2520Multimodal%2520LLM%2520with%2520Reconstruction-Reasoning%2520Synergy%26entry.906535625%3DHaijier%2520Chen%2520and%2520Bo%2520Xu%2520and%2520Shoujian%2520Zhang%2520and%2520Haoze%2520Liu%2520and%2520Jiaxuan%2520Lin%2520and%2520Jingrong%2520Wang%26entry.1292438233%3DRecent%2520developments%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520improved%2520Vision-Language%2520%2528VL%2529%2520reasoning%2520in%25202D%2520domains.%2520However%252C%2520extending%2520these%2520capabilities%2520to%25203D%2520scene%2520understanding%2520remains%2520a%2520major%2520challenge.%2520Existing%25203D%2520Multimodal%2520Large%2520Language%2520Models%2520%25283D-MLLMs%2529%2520often%2520depend%2520on%25203D%2520data%2520inputs%252C%2520which%2520limits%2520scalability%2520and%2520generalization.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Vid-LLM%252C%2520a%2520video-based%25203D-MLLM%2520that%2520directly%2520processes%2520video%2520inputs%2520without%2520requiring%2520external%25203D%2520data%252C%2520making%2520it%2520practical%2520for%2520real-world%2520deployment.%2520In%2520our%2520method%252C%2520the%2520geometric%2520prior%2520are%2520directly%2520used%2520to%2520improve%2520the%2520performance%2520of%2520the%2520sceen%2520perception.%2520To%2520integrate%2520the%2520geometric%2520cues%2520into%2520the%2520MLLM%2520compactly%252C%2520we%2520design%2520a%2520Cross-Task%2520Adapter%2520%2528CTA%2529%2520module%2520to%2520align%2520the%25203D%2520geometric%2520priors%2520with%2520the%2520vision-language%2520representations.%2520To%2520ensure%2520geometric%2520consistency%2520and%2520integrity%252C%2520we%2520introduce%2520a%2520Metric%2520Depth%2520Model%2520that%2520recovers%2520real-scale%2520geometry%2520from%2520the%2520reconstruction%2520outputs.%2520Finally%252C%2520the%2520model%2520is%2520fine-tuned%2520with%2520a%2520two-stage%2520distillation%2520optimization%2520strategy%252C%2520realizing%2520fast%2520convergence%2520and%2520stabilizes%2520training.%2520Extensive%2520experiments%2520across%2520diverse%2520benchmarks%2520verified%2520the%2520effectiveness%2520of%2520our%2520method%2520on%25203D%2520Question%2520Answering%252C%25203D%2520Dense%2520Captioning%2520and%25203D%2520Visual%2520Grounding%2520tasks%252C%2520demonstrating%2520the%2520superior%2520multi-task%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vid-LLM%3A%20A%20Compact%20Video-based%203D%20Multimodal%20LLM%20with%20Reconstruction-Reasoning%20Synergy&entry.906535625=Haijier%20Chen%20and%20Bo%20Xu%20and%20Shoujian%20Zhang%20and%20Haoze%20Liu%20and%20Jiaxuan%20Lin%20and%20Jingrong%20Wang&entry.1292438233=Recent%20developments%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20improved%20Vision-Language%20%28VL%29%20reasoning%20in%202D%20domains.%20However%2C%20extending%20these%20capabilities%20to%203D%20scene%20understanding%20remains%20a%20major%20challenge.%20Existing%203D%20Multimodal%20Large%20Language%20Models%20%283D-MLLMs%29%20often%20depend%20on%203D%20data%20inputs%2C%20which%20limits%20scalability%20and%20generalization.%20To%20address%20this%20limitation%2C%20we%20propose%20Vid-LLM%2C%20a%20video-based%203D-MLLM%20that%20directly%20processes%20video%20inputs%20without%20requiring%20external%203D%20data%2C%20making%20it%20practical%20for%20real-world%20deployment.%20In%20our%20method%2C%20the%20geometric%20prior%20are%20directly%20used%20to%20improve%20the%20performance%20of%20the%20sceen%20perception.%20To%20integrate%20the%20geometric%20cues%20into%20the%20MLLM%20compactly%2C%20we%20design%20a%20Cross-Task%20Adapter%20%28CTA%29%20module%20to%20align%20the%203D%20geometric%20priors%20with%20the%20vision-language%20representations.%20To%20ensure%20geometric%20consistency%20and%20integrity%2C%20we%20introduce%20a%20Metric%20Depth%20Model%20that%20recovers%20real-scale%20geometry%20from%20the%20reconstruction%20outputs.%20Finally%2C%20the%20model%20is%20fine-tuned%20with%20a%20two-stage%20distillation%20optimization%20strategy%2C%20realizing%20fast%20convergence%20and%20stabilizes%20training.%20Extensive%20experiments%20across%20diverse%20benchmarks%20verified%20the%20effectiveness%20of%20our%20method%20on%203D%20Question%20Answering%2C%203D%20Dense%20Captioning%20and%203D%20Visual%20Grounding%20tasks%2C%20demonstrating%20the%20superior%20multi-task%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2509.24385v2&entry.124074799=Read"},
{"title": "AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation", "author": "Jin-Chuan Shi and Binhong Ye and Tao Liu and Junzhe He and Yangjinhui Xu and Xiaoyang Liu and Zeju Li and Hao Chen and Chunhua Shen", "abstract": "Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.", "link": "http://arxiv.org/abs/2602.04672v1", "date": "2026-02-04", "relevancy": 3.112, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6367}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6157}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGILE%3A%20Hand-Object%20Interaction%20Reconstruction%20from%20Video%20via%20Agentic%20Generation&body=Title%3A%20AGILE%3A%20Hand-Object%20Interaction%20Reconstruction%20from%20Video%20via%20Agentic%20Generation%0AAuthor%3A%20Jin-Chuan%20Shi%20and%20Binhong%20Ye%20and%20Tao%20Liu%20and%20Junzhe%20He%20and%20Yangjinhui%20Xu%20and%20Xiaoyang%20Liu%20and%20Zeju%20Li%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20Reconstructing%20dynamic%20hand-object%20interactions%20from%20monocular%20videos%20is%20critical%20for%20dexterous%20manipulation%20data%20collection%20and%20creating%20realistic%20digital%20twins%20for%20robotics%20and%20VR.%20However%2C%20current%20methods%20face%20two%20prohibitive%20barriers%3A%20%281%29%20reliance%20on%20neural%20rendering%20often%20yields%20fragmented%2C%20non-simulation-ready%20geometries%20under%20heavy%20occlusion%2C%20and%20%282%29%20dependence%20on%20brittle%20Structure-from-Motion%20%28SfM%29%20initialization%20leads%20to%20frequent%20failures%20on%20in-the-wild%20footage.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20AGILE%2C%20a%20robust%20framework%20that%20shifts%20the%20paradigm%20from%20reconstruction%20to%20agentic%20generation%20for%20interaction%20learning.%20First%2C%20we%20employ%20an%20agentic%20pipeline%20where%20a%20Vision-Language%20Model%20%28VLM%29%20guides%20a%20generative%20model%20to%20synthesize%20a%20complete%2C%20watertight%20object%20mesh%20with%20high-fidelity%20texture%2C%20independent%20of%20video%20occlusions.%20Second%2C%20bypassing%20fragile%20SfM%20entirely%2C%20we%20propose%20a%20robust%20anchor-and-track%20strategy.%20We%20initialize%20the%20object%20pose%20at%20a%20single%20interaction%20onset%20frame%20using%20a%20foundation%20model%20and%20propagate%20it%20temporally%20by%20leveraging%20the%20strong%20visual%20similarity%20between%20our%20generated%20asset%20and%20video%20observations.%20Finally%2C%20a%20contact-aware%20optimization%20integrates%20semantic%2C%20geometric%2C%20and%20interaction%20stability%20constraints%20to%20enforce%20physical%20plausibility.%20Extensive%20experiments%20on%20HO3D%2C%20DexYCB%2C%20and%20in-the-wild%20videos%20reveal%20that%20AGILE%20outperforms%20baselines%20in%20global%20geometric%20accuracy%20while%20demonstrating%20exceptional%20robustness%20on%20challenging%20sequences%20where%20prior%20art%20frequently%20collapses.%20By%20prioritizing%20physical%20validity%2C%20our%20method%20produces%20simulation-ready%20assets%20validated%20via%20real-to-sim%20retargeting%20for%20robotic%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGILE%253A%2520Hand-Object%2520Interaction%2520Reconstruction%2520from%2520Video%2520via%2520Agentic%2520Generation%26entry.906535625%3DJin-Chuan%2520Shi%2520and%2520Binhong%2520Ye%2520and%2520Tao%2520Liu%2520and%2520Junzhe%2520He%2520and%2520Yangjinhui%2520Xu%2520and%2520Xiaoyang%2520Liu%2520and%2520Zeju%2520Li%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3DReconstructing%2520dynamic%2520hand-object%2520interactions%2520from%2520monocular%2520videos%2520is%2520critical%2520for%2520dexterous%2520manipulation%2520data%2520collection%2520and%2520creating%2520realistic%2520digital%2520twins%2520for%2520robotics%2520and%2520VR.%2520However%252C%2520current%2520methods%2520face%2520two%2520prohibitive%2520barriers%253A%2520%25281%2529%2520reliance%2520on%2520neural%2520rendering%2520often%2520yields%2520fragmented%252C%2520non-simulation-ready%2520geometries%2520under%2520heavy%2520occlusion%252C%2520and%2520%25282%2529%2520dependence%2520on%2520brittle%2520Structure-from-Motion%2520%2528SfM%2529%2520initialization%2520leads%2520to%2520frequent%2520failures%2520on%2520in-the-wild%2520footage.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520AGILE%252C%2520a%2520robust%2520framework%2520that%2520shifts%2520the%2520paradigm%2520from%2520reconstruction%2520to%2520agentic%2520generation%2520for%2520interaction%2520learning.%2520First%252C%2520we%2520employ%2520an%2520agentic%2520pipeline%2520where%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%2520guides%2520a%2520generative%2520model%2520to%2520synthesize%2520a%2520complete%252C%2520watertight%2520object%2520mesh%2520with%2520high-fidelity%2520texture%252C%2520independent%2520of%2520video%2520occlusions.%2520Second%252C%2520bypassing%2520fragile%2520SfM%2520entirely%252C%2520we%2520propose%2520a%2520robust%2520anchor-and-track%2520strategy.%2520We%2520initialize%2520the%2520object%2520pose%2520at%2520a%2520single%2520interaction%2520onset%2520frame%2520using%2520a%2520foundation%2520model%2520and%2520propagate%2520it%2520temporally%2520by%2520leveraging%2520the%2520strong%2520visual%2520similarity%2520between%2520our%2520generated%2520asset%2520and%2520video%2520observations.%2520Finally%252C%2520a%2520contact-aware%2520optimization%2520integrates%2520semantic%252C%2520geometric%252C%2520and%2520interaction%2520stability%2520constraints%2520to%2520enforce%2520physical%2520plausibility.%2520Extensive%2520experiments%2520on%2520HO3D%252C%2520DexYCB%252C%2520and%2520in-the-wild%2520videos%2520reveal%2520that%2520AGILE%2520outperforms%2520baselines%2520in%2520global%2520geometric%2520accuracy%2520while%2520demonstrating%2520exceptional%2520robustness%2520on%2520challenging%2520sequences%2520where%2520prior%2520art%2520frequently%2520collapses.%2520By%2520prioritizing%2520physical%2520validity%252C%2520our%2520method%2520produces%2520simulation-ready%2520assets%2520validated%2520via%2520real-to-sim%2520retargeting%2520for%2520robotic%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGILE%3A%20Hand-Object%20Interaction%20Reconstruction%20from%20Video%20via%20Agentic%20Generation&entry.906535625=Jin-Chuan%20Shi%20and%20Binhong%20Ye%20and%20Tao%20Liu%20and%20Junzhe%20He%20and%20Yangjinhui%20Xu%20and%20Xiaoyang%20Liu%20and%20Zeju%20Li%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=Reconstructing%20dynamic%20hand-object%20interactions%20from%20monocular%20videos%20is%20critical%20for%20dexterous%20manipulation%20data%20collection%20and%20creating%20realistic%20digital%20twins%20for%20robotics%20and%20VR.%20However%2C%20current%20methods%20face%20two%20prohibitive%20barriers%3A%20%281%29%20reliance%20on%20neural%20rendering%20often%20yields%20fragmented%2C%20non-simulation-ready%20geometries%20under%20heavy%20occlusion%2C%20and%20%282%29%20dependence%20on%20brittle%20Structure-from-Motion%20%28SfM%29%20initialization%20leads%20to%20frequent%20failures%20on%20in-the-wild%20footage.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20AGILE%2C%20a%20robust%20framework%20that%20shifts%20the%20paradigm%20from%20reconstruction%20to%20agentic%20generation%20for%20interaction%20learning.%20First%2C%20we%20employ%20an%20agentic%20pipeline%20where%20a%20Vision-Language%20Model%20%28VLM%29%20guides%20a%20generative%20model%20to%20synthesize%20a%20complete%2C%20watertight%20object%20mesh%20with%20high-fidelity%20texture%2C%20independent%20of%20video%20occlusions.%20Second%2C%20bypassing%20fragile%20SfM%20entirely%2C%20we%20propose%20a%20robust%20anchor-and-track%20strategy.%20We%20initialize%20the%20object%20pose%20at%20a%20single%20interaction%20onset%20frame%20using%20a%20foundation%20model%20and%20propagate%20it%20temporally%20by%20leveraging%20the%20strong%20visual%20similarity%20between%20our%20generated%20asset%20and%20video%20observations.%20Finally%2C%20a%20contact-aware%20optimization%20integrates%20semantic%2C%20geometric%2C%20and%20interaction%20stability%20constraints%20to%20enforce%20physical%20plausibility.%20Extensive%20experiments%20on%20HO3D%2C%20DexYCB%2C%20and%20in-the-wild%20videos%20reveal%20that%20AGILE%20outperforms%20baselines%20in%20global%20geometric%20accuracy%20while%20demonstrating%20exceptional%20robustness%20on%20challenging%20sequences%20where%20prior%20art%20frequently%20collapses.%20By%20prioritizing%20physical%20validity%2C%20our%20method%20produces%20simulation-ready%20assets%20validated%20via%20real-to-sim%20retargeting%20for%20robotic%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.04672v1&entry.124074799=Read"},
{"title": "Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models", "author": "Cem Eteke and Enzo Tartaglione", "abstract": "3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.", "link": "http://arxiv.org/abs/2602.04549v1", "date": "2026-02-04", "relevancy": 3.1093, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6319}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6312}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nix%20and%20Fix%3A%20Targeting%201000x%20Compression%20of%203D%20Gaussian%20Splatting%20with%20Diffusion%20Models&body=Title%3A%20Nix%20and%20Fix%3A%20Targeting%201000x%20Compression%20of%203D%20Gaussian%20Splatting%20with%20Diffusion%20Models%0AAuthor%3A%20Cem%20Eteke%20and%20Enzo%20Tartaglione%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20revolutionized%20novel%20view%20rendering.%20Instead%20of%20inferring%20from%20dense%20spatial%20points%2C%20as%20implicit%20representations%20do%2C%203DGS%20uses%20sparse%20Gaussians.%20This%20enables%20real-time%20performance%20but%20increases%20space%20requirements%2C%20hindering%20applications%20such%20as%20immersive%20communication.%203DGS%20compression%20emerged%20as%20a%20field%20aimed%20at%20alleviating%20this%20issue.%20While%20impressive%20progress%20has%20been%20made%2C%20at%20low%20rates%2C%20compression%20introduces%20artifacts%20that%20degrade%20visual%20quality%20significantly.%20We%20introduce%20NiFi%2C%20a%20method%20for%20extreme%203DGS%20compression%20through%20restoration%20via%20artifact-aware%2C%20diffusion-based%20one-step%20distillation.%20We%20show%20that%20our%20method%20achieves%20state-of-the-art%20perceptual%20quality%20at%20extremely%20low%20rates%2C%20down%20to%200.1%20MB%2C%20and%20towards%201000x%20rate%20improvement%20over%203DGS%20at%20comparable%20perceptual%20performance.%20The%20code%20will%20be%20open-sourced%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNix%2520and%2520Fix%253A%2520Targeting%25201000x%2520Compression%2520of%25203D%2520Gaussian%2520Splatting%2520with%2520Diffusion%2520Models%26entry.906535625%3DCem%2520Eteke%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520revolutionized%2520novel%2520view%2520rendering.%2520Instead%2520of%2520inferring%2520from%2520dense%2520spatial%2520points%252C%2520as%2520implicit%2520representations%2520do%252C%25203DGS%2520uses%2520sparse%2520Gaussians.%2520This%2520enables%2520real-time%2520performance%2520but%2520increases%2520space%2520requirements%252C%2520hindering%2520applications%2520such%2520as%2520immersive%2520communication.%25203DGS%2520compression%2520emerged%2520as%2520a%2520field%2520aimed%2520at%2520alleviating%2520this%2520issue.%2520While%2520impressive%2520progress%2520has%2520been%2520made%252C%2520at%2520low%2520rates%252C%2520compression%2520introduces%2520artifacts%2520that%2520degrade%2520visual%2520quality%2520significantly.%2520We%2520introduce%2520NiFi%252C%2520a%2520method%2520for%2520extreme%25203DGS%2520compression%2520through%2520restoration%2520via%2520artifact-aware%252C%2520diffusion-based%2520one-step%2520distillation.%2520We%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520perceptual%2520quality%2520at%2520extremely%2520low%2520rates%252C%2520down%2520to%25200.1%2520MB%252C%2520and%2520towards%25201000x%2520rate%2520improvement%2520over%25203DGS%2520at%2520comparable%2520perceptual%2520performance.%2520The%2520code%2520will%2520be%2520open-sourced%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nix%20and%20Fix%3A%20Targeting%201000x%20Compression%20of%203D%20Gaussian%20Splatting%20with%20Diffusion%20Models&entry.906535625=Cem%20Eteke%20and%20Enzo%20Tartaglione&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20revolutionized%20novel%20view%20rendering.%20Instead%20of%20inferring%20from%20dense%20spatial%20points%2C%20as%20implicit%20representations%20do%2C%203DGS%20uses%20sparse%20Gaussians.%20This%20enables%20real-time%20performance%20but%20increases%20space%20requirements%2C%20hindering%20applications%20such%20as%20immersive%20communication.%203DGS%20compression%20emerged%20as%20a%20field%20aimed%20at%20alleviating%20this%20issue.%20While%20impressive%20progress%20has%20been%20made%2C%20at%20low%20rates%2C%20compression%20introduces%20artifacts%20that%20degrade%20visual%20quality%20significantly.%20We%20introduce%20NiFi%2C%20a%20method%20for%20extreme%203DGS%20compression%20through%20restoration%20via%20artifact-aware%2C%20diffusion-based%20one-step%20distillation.%20We%20show%20that%20our%20method%20achieves%20state-of-the-art%20perceptual%20quality%20at%20extremely%20low%20rates%2C%20down%20to%200.1%20MB%2C%20and%20towards%201000x%20rate%20improvement%20over%203DGS%20at%20comparable%20perceptual%20performance.%20The%20code%20will%20be%20open-sourced%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2602.04549v1&entry.124074799=Read"},
{"title": "Think3D: Thinking with Space for Spatial Reasoning", "author": "Zaibin Zhang and Yuhan Wu and Lianjie Jia and Yifan Wang and Zhongbo Zhang and Yijiang Li and Binghao Ran and Fuxi Zhang and Zhuohan Sun and Zhenfei Yin and Lijun Wang and Huchuan Lu", "abstract": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "link": "http://arxiv.org/abs/2601.13029v2", "date": "2026-02-04", "relevancy": 3.0031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think3D%3A%20Thinking%20with%20Space%20for%20Spatial%20Reasoning&body=Title%3A%20Think3D%3A%20Thinking%20with%20Space%20for%20Spatial%20Reasoning%0AAuthor%3A%20Zaibin%20Zhang%20and%20Yuhan%20Wu%20and%20Lianjie%20Jia%20and%20Yifan%20Wang%20and%20Zhongbo%20Zhang%20and%20Yijiang%20Li%20and%20Binghao%20Ran%20and%20Fuxi%20Zhang%20and%20Zhuohan%20Sun%20and%20Zhenfei%20Yin%20and%20Lijun%20Wang%20and%20Huchuan%20Lu%0AAbstract%3A%20Understanding%20and%20reasoning%20about%20the%20physical%20world%20requires%20spatial%20intelligence%3A%20the%20ability%20to%20interpret%20geometry%2C%20perspective%2C%20and%20spatial%20relations%20beyond%202D%20perception.%20While%20recent%20vision%20large%20models%20%28VLMs%29%20excel%20at%20visual%20understanding%2C%20they%20remain%20fundamentally%202D%20perceivers%20and%20struggle%20with%20genuine%203D%20reasoning.%20We%20introduce%20Think3D%2C%20a%20framework%20that%20enables%20VLM%20agents%20to%20think%20with%203D%20space.%20By%20leveraging%203D%20reconstruction%20models%20that%20recover%20point%20clouds%20and%20camera%20poses%20from%20images%20or%20videos%2C%20Think3D%20allows%20the%20agent%20to%20actively%20manipulate%20space%20through%20camera-based%20operations%20and%20ego/global-view%20switching%2C%20transforming%20spatial%20reasoning%20into%20an%20interactive%203D%20chain-of-thought%20process.%20Without%20additional%20training%2C%20Think3D%20significantly%20improves%20the%20spatial%20reasoning%20performance%20of%20advanced%20models%20such%20as%20GPT-4.1%20and%20Gemini%202.5%20Pro%2C%20yielding%20average%20gains%20of%20%2B7.8%25%20on%20BLINK%20Multi-view%20and%20MindCube%2C%20and%20%2B4.7%25%20on%20VSI-Bench.%20We%20further%20show%20that%20smaller%20models%2C%20which%20struggle%20with%20spatial%20exploration%2C%20benefit%20significantly%20from%20a%20reinforcement%20learning%20policy%20that%20enables%20the%20model%20to%20select%20informative%20viewpoints%20and%20operations.%20With%20RL%2C%20the%20benefit%20from%20tool%20usage%20increases%20from%20%2B0.7%25%20to%20%2B6.8%25.%20Our%20findings%20demonstrate%20that%20training-free%2C%20tool-augmented%20spatial%20exploration%20is%20a%20viable%20path%20toward%20more%20flexible%20and%20human-like%203D%20reasoning%20in%20multimodal%20agents%2C%20establishing%20a%20new%20dimension%20of%20multimodal%20intelligence.%20Code%20and%20weights%20are%20released%20at%20https%3A//github.com/zhangzaibin/spagent.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink3D%253A%2520Thinking%2520with%2520Space%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DZaibin%2520Zhang%2520and%2520Yuhan%2520Wu%2520and%2520Lianjie%2520Jia%2520and%2520Yifan%2520Wang%2520and%2520Zhongbo%2520Zhang%2520and%2520Yijiang%2520Li%2520and%2520Binghao%2520Ran%2520and%2520Fuxi%2520Zhang%2520and%2520Zhuohan%2520Sun%2520and%2520Zhenfei%2520Yin%2520and%2520Lijun%2520Wang%2520and%2520Huchuan%2520Lu%26entry.1292438233%3DUnderstanding%2520and%2520reasoning%2520about%2520the%2520physical%2520world%2520requires%2520spatial%2520intelligence%253A%2520the%2520ability%2520to%2520interpret%2520geometry%252C%2520perspective%252C%2520and%2520spatial%2520relations%2520beyond%25202D%2520perception.%2520While%2520recent%2520vision%2520large%2520models%2520%2528VLMs%2529%2520excel%2520at%2520visual%2520understanding%252C%2520they%2520remain%2520fundamentally%25202D%2520perceivers%2520and%2520struggle%2520with%2520genuine%25203D%2520reasoning.%2520We%2520introduce%2520Think3D%252C%2520a%2520framework%2520that%2520enables%2520VLM%2520agents%2520to%2520think%2520with%25203D%2520space.%2520By%2520leveraging%25203D%2520reconstruction%2520models%2520that%2520recover%2520point%2520clouds%2520and%2520camera%2520poses%2520from%2520images%2520or%2520videos%252C%2520Think3D%2520allows%2520the%2520agent%2520to%2520actively%2520manipulate%2520space%2520through%2520camera-based%2520operations%2520and%2520ego/global-view%2520switching%252C%2520transforming%2520spatial%2520reasoning%2520into%2520an%2520interactive%25203D%2520chain-of-thought%2520process.%2520Without%2520additional%2520training%252C%2520Think3D%2520significantly%2520improves%2520the%2520spatial%2520reasoning%2520performance%2520of%2520advanced%2520models%2520such%2520as%2520GPT-4.1%2520and%2520Gemini%25202.5%2520Pro%252C%2520yielding%2520average%2520gains%2520of%2520%252B7.8%2525%2520on%2520BLINK%2520Multi-view%2520and%2520MindCube%252C%2520and%2520%252B4.7%2525%2520on%2520VSI-Bench.%2520We%2520further%2520show%2520that%2520smaller%2520models%252C%2520which%2520struggle%2520with%2520spatial%2520exploration%252C%2520benefit%2520significantly%2520from%2520a%2520reinforcement%2520learning%2520policy%2520that%2520enables%2520the%2520model%2520to%2520select%2520informative%2520viewpoints%2520and%2520operations.%2520With%2520RL%252C%2520the%2520benefit%2520from%2520tool%2520usage%2520increases%2520from%2520%252B0.7%2525%2520to%2520%252B6.8%2525.%2520Our%2520findings%2520demonstrate%2520that%2520training-free%252C%2520tool-augmented%2520spatial%2520exploration%2520is%2520a%2520viable%2520path%2520toward%2520more%2520flexible%2520and%2520human-like%25203D%2520reasoning%2520in%2520multimodal%2520agents%252C%2520establishing%2520a%2520new%2520dimension%2520of%2520multimodal%2520intelligence.%2520Code%2520and%2520weights%2520are%2520released%2520at%2520https%253A//github.com/zhangzaibin/spagent.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think3D%3A%20Thinking%20with%20Space%20for%20Spatial%20Reasoning&entry.906535625=Zaibin%20Zhang%20and%20Yuhan%20Wu%20and%20Lianjie%20Jia%20and%20Yifan%20Wang%20and%20Zhongbo%20Zhang%20and%20Yijiang%20Li%20and%20Binghao%20Ran%20and%20Fuxi%20Zhang%20and%20Zhuohan%20Sun%20and%20Zhenfei%20Yin%20and%20Lijun%20Wang%20and%20Huchuan%20Lu&entry.1292438233=Understanding%20and%20reasoning%20about%20the%20physical%20world%20requires%20spatial%20intelligence%3A%20the%20ability%20to%20interpret%20geometry%2C%20perspective%2C%20and%20spatial%20relations%20beyond%202D%20perception.%20While%20recent%20vision%20large%20models%20%28VLMs%29%20excel%20at%20visual%20understanding%2C%20they%20remain%20fundamentally%202D%20perceivers%20and%20struggle%20with%20genuine%203D%20reasoning.%20We%20introduce%20Think3D%2C%20a%20framework%20that%20enables%20VLM%20agents%20to%20think%20with%203D%20space.%20By%20leveraging%203D%20reconstruction%20models%20that%20recover%20point%20clouds%20and%20camera%20poses%20from%20images%20or%20videos%2C%20Think3D%20allows%20the%20agent%20to%20actively%20manipulate%20space%20through%20camera-based%20operations%20and%20ego/global-view%20switching%2C%20transforming%20spatial%20reasoning%20into%20an%20interactive%203D%20chain-of-thought%20process.%20Without%20additional%20training%2C%20Think3D%20significantly%20improves%20the%20spatial%20reasoning%20performance%20of%20advanced%20models%20such%20as%20GPT-4.1%20and%20Gemini%202.5%20Pro%2C%20yielding%20average%20gains%20of%20%2B7.8%25%20on%20BLINK%20Multi-view%20and%20MindCube%2C%20and%20%2B4.7%25%20on%20VSI-Bench.%20We%20further%20show%20that%20smaller%20models%2C%20which%20struggle%20with%20spatial%20exploration%2C%20benefit%20significantly%20from%20a%20reinforcement%20learning%20policy%20that%20enables%20the%20model%20to%20select%20informative%20viewpoints%20and%20operations.%20With%20RL%2C%20the%20benefit%20from%20tool%20usage%20increases%20from%20%2B0.7%25%20to%20%2B6.8%25.%20Our%20findings%20demonstrate%20that%20training-free%2C%20tool-augmented%20spatial%20exploration%20is%20a%20viable%20path%20toward%20more%20flexible%20and%20human-like%203D%20reasoning%20in%20multimodal%20agents%2C%20establishing%20a%20new%20dimension%20of%20multimodal%20intelligence.%20Code%20and%20weights%20are%20released%20at%20https%3A//github.com/zhangzaibin/spagent.&entry.1838667208=http%3A//arxiv.org/abs/2601.13029v2&entry.124074799=Read"},
{"title": "LiDAR, GNSS and IMU Sensor Fine Alignment through Dynamic Time Warping to Construct 3D City Maps", "author": "Haitian Wang and Hezam Albaqami and Xinyu Wang and Muhammad Ibrahim and Zainy M. Malakan and Abdullah M. Algamdi and Mohammed H. Alghamdi and Ajmal Mian", "abstract": "LiDAR-based 3D mapping suffers from cumulative drift causing global misalignment, particularly in GNSS-constrained environments. To address this, we propose a unified framework that fuses LiDAR, GNSS, and IMU data for high-resolution city-scale mapping. The method performs velocity-based temporal alignment using Dynamic Time Warping and refines GNSS and IMU signals via extended Kalman filtering. Local maps are built using Normal Distributions Transform-based registration and pose graph optimization with loop closure detection, while global consistency is enforced using GNSS-constrained anchors followed by fine registration of overlapping segments. We also introduce a large-scale multimodal dataset captured in Perth, Western Australia to facilitate future research in this direction. Our dataset comprises 144,000 frames acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS trajectories, and MEMS-IMU measurements across 21 urban loops. To assess geometric consistency, we evaluated our method using alignment metrics based on road centerlines and intersections to capture both global and local accuracy. The proposed framework reduces the average global alignment error from 3.32m to 1.24m, achieving a 61.4% improvement, and significantly decreases the intersection centroid offset from 13.22m to 2.01m, corresponding to an 84.8% enhancement. The constructed high-fidelity map and raw dataset are publicly available through https://ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration, and its visualization can be viewed at https://www.youtube.com/watch?v=-ZUgs1KyMks. The source code is available at https://github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps. This dataset and method together establish a new benchmark for evaluating 3D city mapping in GNSS-constrained environments.", "link": "http://arxiv.org/abs/2507.08420v3", "date": "2026-02-04", "relevancy": 2.9942, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.615}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6145}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR%2C%20GNSS%20and%20IMU%20Sensor%20Fine%20Alignment%20through%20Dynamic%20Time%20Warping%20to%20Construct%203D%20City%20Maps&body=Title%3A%20LiDAR%2C%20GNSS%20and%20IMU%20Sensor%20Fine%20Alignment%20through%20Dynamic%20Time%20Warping%20to%20Construct%203D%20City%20Maps%0AAuthor%3A%20Haitian%20Wang%20and%20Hezam%20Albaqami%20and%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Zainy%20M.%20Malakan%20and%20Abdullah%20M.%20Algamdi%20and%20Mohammed%20H.%20Alghamdi%20and%20Ajmal%20Mian%0AAbstract%3A%20LiDAR-based%203D%20mapping%20suffers%20from%20cumulative%20drift%20causing%20global%20misalignment%2C%20particularly%20in%20GNSS-constrained%20environments.%20To%20address%20this%2C%20we%20propose%20a%20unified%20framework%20that%20fuses%20LiDAR%2C%20GNSS%2C%20and%20IMU%20data%20for%20high-resolution%20city-scale%20mapping.%20The%20method%20performs%20velocity-based%20temporal%20alignment%20using%20Dynamic%20Time%20Warping%20and%20refines%20GNSS%20and%20IMU%20signals%20via%20extended%20Kalman%20filtering.%20Local%20maps%20are%20built%20using%20Normal%20Distributions%20Transform-based%20registration%20and%20pose%20graph%20optimization%20with%20loop%20closure%20detection%2C%20while%20global%20consistency%20is%20enforced%20using%20GNSS-constrained%20anchors%20followed%20by%20fine%20registration%20of%20overlapping%20segments.%20We%20also%20introduce%20a%20large-scale%20multimodal%20dataset%20captured%20in%20Perth%2C%20Western%20Australia%20to%20facilitate%20future%20research%20in%20this%20direction.%20Our%20dataset%20comprises%20144%2C000%20frames%20acquired%20with%20a%20128-channel%20Ouster%20LiDAR%2C%20synchronized%20RTK-GNSS%20trajectories%2C%20and%20MEMS-IMU%20measurements%20across%2021%20urban%20loops.%20To%20assess%20geometric%20consistency%2C%20we%20evaluated%20our%20method%20using%20alignment%20metrics%20based%20on%20road%20centerlines%20and%20intersections%20to%20capture%20both%20global%20and%20local%20accuracy.%20The%20proposed%20framework%20reduces%20the%20average%20global%20alignment%20error%20from%203.32m%20to%201.24m%2C%20achieving%20a%2061.4%25%20improvement%2C%20and%20significantly%20decreases%20the%20intersection%20centroid%20offset%20from%2013.22m%20to%202.01m%2C%20corresponding%20to%20an%2084.8%25%20enhancement.%20The%20constructed%20high-fidelity%20map%20and%20raw%20dataset%20are%20publicly%20available%20through%20https%3A//ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration%2C%20and%20its%20visualization%20can%20be%20viewed%20at%20https%3A//www.youtube.com/watch%3Fv%3D-ZUgs1KyMks.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps.%20This%20dataset%20and%20method%20together%20establish%20a%20new%20benchmark%20for%20evaluating%203D%20city%20mapping%20in%20GNSS-constrained%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08420v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR%252C%2520GNSS%2520and%2520IMU%2520Sensor%2520Fine%2520Alignment%2520through%2520Dynamic%2520Time%2520Warping%2520to%2520Construct%25203D%2520City%2520Maps%26entry.906535625%3DHaitian%2520Wang%2520and%2520Hezam%2520Albaqami%2520and%2520Xinyu%2520Wang%2520and%2520Muhammad%2520Ibrahim%2520and%2520Zainy%2520M.%2520Malakan%2520and%2520Abdullah%2520M.%2520Algamdi%2520and%2520Mohammed%2520H.%2520Alghamdi%2520and%2520Ajmal%2520Mian%26entry.1292438233%3DLiDAR-based%25203D%2520mapping%2520suffers%2520from%2520cumulative%2520drift%2520causing%2520global%2520misalignment%252C%2520particularly%2520in%2520GNSS-constrained%2520environments.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520unified%2520framework%2520that%2520fuses%2520LiDAR%252C%2520GNSS%252C%2520and%2520IMU%2520data%2520for%2520high-resolution%2520city-scale%2520mapping.%2520The%2520method%2520performs%2520velocity-based%2520temporal%2520alignment%2520using%2520Dynamic%2520Time%2520Warping%2520and%2520refines%2520GNSS%2520and%2520IMU%2520signals%2520via%2520extended%2520Kalman%2520filtering.%2520Local%2520maps%2520are%2520built%2520using%2520Normal%2520Distributions%2520Transform-based%2520registration%2520and%2520pose%2520graph%2520optimization%2520with%2520loop%2520closure%2520detection%252C%2520while%2520global%2520consistency%2520is%2520enforced%2520using%2520GNSS-constrained%2520anchors%2520followed%2520by%2520fine%2520registration%2520of%2520overlapping%2520segments.%2520We%2520also%2520introduce%2520a%2520large-scale%2520multimodal%2520dataset%2520captured%2520in%2520Perth%252C%2520Western%2520Australia%2520to%2520facilitate%2520future%2520research%2520in%2520this%2520direction.%2520Our%2520dataset%2520comprises%2520144%252C000%2520frames%2520acquired%2520with%2520a%2520128-channel%2520Ouster%2520LiDAR%252C%2520synchronized%2520RTK-GNSS%2520trajectories%252C%2520and%2520MEMS-IMU%2520measurements%2520across%252021%2520urban%2520loops.%2520To%2520assess%2520geometric%2520consistency%252C%2520we%2520evaluated%2520our%2520method%2520using%2520alignment%2520metrics%2520based%2520on%2520road%2520centerlines%2520and%2520intersections%2520to%2520capture%2520both%2520global%2520and%2520local%2520accuracy.%2520The%2520proposed%2520framework%2520reduces%2520the%2520average%2520global%2520alignment%2520error%2520from%25203.32m%2520to%25201.24m%252C%2520achieving%2520a%252061.4%2525%2520improvement%252C%2520and%2520significantly%2520decreases%2520the%2520intersection%2520centroid%2520offset%2520from%252013.22m%2520to%25202.01m%252C%2520corresponding%2520to%2520an%252084.8%2525%2520enhancement.%2520The%2520constructed%2520high-fidelity%2520map%2520and%2520raw%2520dataset%2520are%2520publicly%2520available%2520through%2520https%253A//ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration%252C%2520and%2520its%2520visualization%2520can%2520be%2520viewed%2520at%2520https%253A//www.youtube.com/watch%253Fv%253D-ZUgs1KyMks.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps.%2520This%2520dataset%2520and%2520method%2520together%2520establish%2520a%2520new%2520benchmark%2520for%2520evaluating%25203D%2520city%2520mapping%2520in%2520GNSS-constrained%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08420v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR%2C%20GNSS%20and%20IMU%20Sensor%20Fine%20Alignment%20through%20Dynamic%20Time%20Warping%20to%20Construct%203D%20City%20Maps&entry.906535625=Haitian%20Wang%20and%20Hezam%20Albaqami%20and%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Zainy%20M.%20Malakan%20and%20Abdullah%20M.%20Algamdi%20and%20Mohammed%20H.%20Alghamdi%20and%20Ajmal%20Mian&entry.1292438233=LiDAR-based%203D%20mapping%20suffers%20from%20cumulative%20drift%20causing%20global%20misalignment%2C%20particularly%20in%20GNSS-constrained%20environments.%20To%20address%20this%2C%20we%20propose%20a%20unified%20framework%20that%20fuses%20LiDAR%2C%20GNSS%2C%20and%20IMU%20data%20for%20high-resolution%20city-scale%20mapping.%20The%20method%20performs%20velocity-based%20temporal%20alignment%20using%20Dynamic%20Time%20Warping%20and%20refines%20GNSS%20and%20IMU%20signals%20via%20extended%20Kalman%20filtering.%20Local%20maps%20are%20built%20using%20Normal%20Distributions%20Transform-based%20registration%20and%20pose%20graph%20optimization%20with%20loop%20closure%20detection%2C%20while%20global%20consistency%20is%20enforced%20using%20GNSS-constrained%20anchors%20followed%20by%20fine%20registration%20of%20overlapping%20segments.%20We%20also%20introduce%20a%20large-scale%20multimodal%20dataset%20captured%20in%20Perth%2C%20Western%20Australia%20to%20facilitate%20future%20research%20in%20this%20direction.%20Our%20dataset%20comprises%20144%2C000%20frames%20acquired%20with%20a%20128-channel%20Ouster%20LiDAR%2C%20synchronized%20RTK-GNSS%20trajectories%2C%20and%20MEMS-IMU%20measurements%20across%2021%20urban%20loops.%20To%20assess%20geometric%20consistency%2C%20we%20evaluated%20our%20method%20using%20alignment%20metrics%20based%20on%20road%20centerlines%20and%20intersections%20to%20capture%20both%20global%20and%20local%20accuracy.%20The%20proposed%20framework%20reduces%20the%20average%20global%20alignment%20error%20from%203.32m%20to%201.24m%2C%20achieving%20a%2061.4%25%20improvement%2C%20and%20significantly%20decreases%20the%20intersection%20centroid%20offset%20from%2013.22m%20to%202.01m%2C%20corresponding%20to%20an%2084.8%25%20enhancement.%20The%20constructed%20high-fidelity%20map%20and%20raw%20dataset%20are%20publicly%20available%20through%20https%3A//ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration%2C%20and%20its%20visualization%20can%20be%20viewed%20at%20https%3A//www.youtube.com/watch%3Fv%3D-ZUgs1KyMks.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps.%20This%20dataset%20and%20method%20together%20establish%20a%20new%20benchmark%20for%20evaluating%203D%20city%20mapping%20in%20GNSS-constrained%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2507.08420v3&entry.124074799=Read"},
{"title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "author": "Aaron Wilhelm and Nils Napp", "abstract": "Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method's effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.", "link": "http://arxiv.org/abs/2505.11620v2", "date": "2026-02-04", "relevancy": 2.9314, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5982}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization&body=Title%3A%20Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization%0AAuthor%3A%20Aaron%20Wilhelm%20and%20Nils%20Napp%0AAbstract%3A%20Ground%20texture%20localization%20using%20a%20downward-facing%20camera%20offers%20a%20low-cost%2C%20high-precision%20localization%20solution%20that%20is%20robust%20to%20dynamic%20environments%20and%20requires%20no%20environmental%20modification.%20We%20present%20a%20significantly%20improved%20bag-of-words%20%28BoW%29%20image%20retrieval%20system%20for%20ground%20texture%20localization%2C%20achieving%20substantially%20higher%20accuracy%20for%20global%20localization%20and%20higher%20precision%20and%20recall%20for%20loop%20closure%20detection%20in%20SLAM.%20Our%20approach%20leverages%20an%20approximate%20%24k%24-means%20%28AKM%29%20vocabulary%20with%20soft%20assignment%2C%20and%20exploits%20the%20consistent%20orientation%20and%20constant%20scale%20constraints%20inherent%20to%20ground%20texture%20localization.%20Identifying%20the%20different%20needs%20of%20global%20localization%20vs.%20loop%20closure%20detection%20for%20SLAM%2C%20we%20present%20both%20high-accuracy%20and%20high-speed%20versions%20of%20our%20algorithm.%20We%20test%20the%20effect%20of%20each%20of%20our%20proposed%20improvements%20through%20an%20ablation%20study%20and%20demonstrate%20our%20method%27s%20effectiveness%20for%20both%20global%20localization%20and%20loop%20closure%20detection.%20With%20numerous%20ground%20texture%20localization%20systems%20already%20using%20BoW%2C%20our%20method%20can%20readily%20replace%20other%20generic%20BoW%20systems%20in%20their%20pipeline%20and%20immediately%20improve%20their%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Bag-of-Words%2520Image%2520Retrieval%2520with%2520Geometric%2520Constraints%2520for%2520Ground%2520Texture%2520Localization%26entry.906535625%3DAaron%2520Wilhelm%2520and%2520Nils%2520Napp%26entry.1292438233%3DGround%2520texture%2520localization%2520using%2520a%2520downward-facing%2520camera%2520offers%2520a%2520low-cost%252C%2520high-precision%2520localization%2520solution%2520that%2520is%2520robust%2520to%2520dynamic%2520environments%2520and%2520requires%2520no%2520environmental%2520modification.%2520We%2520present%2520a%2520significantly%2520improved%2520bag-of-words%2520%2528BoW%2529%2520image%2520retrieval%2520system%2520for%2520ground%2520texture%2520localization%252C%2520achieving%2520substantially%2520higher%2520accuracy%2520for%2520global%2520localization%2520and%2520higher%2520precision%2520and%2520recall%2520for%2520loop%2520closure%2520detection%2520in%2520SLAM.%2520Our%2520approach%2520leverages%2520an%2520approximate%2520%2524k%2524-means%2520%2528AKM%2529%2520vocabulary%2520with%2520soft%2520assignment%252C%2520and%2520exploits%2520the%2520consistent%2520orientation%2520and%2520constant%2520scale%2520constraints%2520inherent%2520to%2520ground%2520texture%2520localization.%2520Identifying%2520the%2520different%2520needs%2520of%2520global%2520localization%2520vs.%2520loop%2520closure%2520detection%2520for%2520SLAM%252C%2520we%2520present%2520both%2520high-accuracy%2520and%2520high-speed%2520versions%2520of%2520our%2520algorithm.%2520We%2520test%2520the%2520effect%2520of%2520each%2520of%2520our%2520proposed%2520improvements%2520through%2520an%2520ablation%2520study%2520and%2520demonstrate%2520our%2520method%2527s%2520effectiveness%2520for%2520both%2520global%2520localization%2520and%2520loop%2520closure%2520detection.%2520With%2520numerous%2520ground%2520texture%2520localization%2520systems%2520already%2520using%2520BoW%252C%2520our%2520method%2520can%2520readily%2520replace%2520other%2520generic%2520BoW%2520systems%2520in%2520their%2520pipeline%2520and%2520immediately%2520improve%2520their%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization&entry.906535625=Aaron%20Wilhelm%20and%20Nils%20Napp&entry.1292438233=Ground%20texture%20localization%20using%20a%20downward-facing%20camera%20offers%20a%20low-cost%2C%20high-precision%20localization%20solution%20that%20is%20robust%20to%20dynamic%20environments%20and%20requires%20no%20environmental%20modification.%20We%20present%20a%20significantly%20improved%20bag-of-words%20%28BoW%29%20image%20retrieval%20system%20for%20ground%20texture%20localization%2C%20achieving%20substantially%20higher%20accuracy%20for%20global%20localization%20and%20higher%20precision%20and%20recall%20for%20loop%20closure%20detection%20in%20SLAM.%20Our%20approach%20leverages%20an%20approximate%20%24k%24-means%20%28AKM%29%20vocabulary%20with%20soft%20assignment%2C%20and%20exploits%20the%20consistent%20orientation%20and%20constant%20scale%20constraints%20inherent%20to%20ground%20texture%20localization.%20Identifying%20the%20different%20needs%20of%20global%20localization%20vs.%20loop%20closure%20detection%20for%20SLAM%2C%20we%20present%20both%20high-accuracy%20and%20high-speed%20versions%20of%20our%20algorithm.%20We%20test%20the%20effect%20of%20each%20of%20our%20proposed%20improvements%20through%20an%20ablation%20study%20and%20demonstrate%20our%20method%27s%20effectiveness%20for%20both%20global%20localization%20and%20loop%20closure%20detection.%20With%20numerous%20ground%20texture%20localization%20systems%20already%20using%20BoW%2C%20our%20method%20can%20readily%20replace%20other%20generic%20BoW%20systems%20in%20their%20pipeline%20and%20immediately%20improve%20their%20results.&entry.1838667208=http%3A//arxiv.org/abs/2505.11620v2&entry.124074799=Read"},
{"title": "Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning", "author": "Hanmo Chen and Guangtao Lyu and Chenghao Xu and Jiexi Yan and Xu Yang and Cheng Deng", "abstract": "As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.", "link": "http://arxiv.org/abs/2601.21904v3", "date": "2026-02-04", "relevancy": 2.9123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5853}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&body=Title%3A%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning%0AAuthor%3A%20Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng%0AAbstract%3A%20As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21904v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Global%2520Alignment%253A%2520Fine-Grained%2520Motion-Language%2520Retrieval%2520via%2520Pyramidal%2520Shapley-Taylor%2520Learning%26entry.906535625%3DHanmo%2520Chen%2520and%2520Guangtao%2520Lyu%2520and%2520Chenghao%2520Xu%2520and%2520Jiexi%2520Yan%2520and%2520Xu%2520Yang%2520and%2520Cheng%2520Deng%26entry.1292438233%3DAs%2520a%2520foundational%2520task%2520in%2520human-centric%2520cross-modal%2520intelligence%252C%2520motion-language%2520retrieval%2520aims%2520to%2520bridge%2520the%2520semantic%2520gap%2520between%2520natural%2520language%2520and%2520human%2520motion%252C%2520enabling%2520intuitive%2520motion%2520analysis%252C%2520yet%2520existing%2520approaches%2520predominantly%2520focus%2520on%2520aligning%2520entire%2520motion%2520sequences%2520with%2520global%2520textual%2520representations.%2520This%2520global-centric%2520paradigm%2520overlooks%2520fine-grained%2520interactions%2520between%2520local%2520motion%2520segments%2520and%2520individual%2520body%2520joints%2520and%2520text%2520tokens%252C%2520inevitably%2520leading%2520to%2520suboptimal%2520retrieval%2520performance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520draw%2520inspiration%2520from%2520the%2520pyramidal%2520process%2520of%2520human%2520motion%2520perception%2520%2528from%2520joint%2520dynamics%2520to%2520segment%2520coherence%252C%2520and%2520finally%2520to%2520holistic%2520comprehension%2529%2520and%2520propose%2520a%2520novel%2520Pyramidal%2520Shapley-Taylor%2520%2528PST%2529%2520learning%2520framework%2520for%2520fine-grained%2520motion-language%2520retrieval.%2520Specifically%252C%2520the%2520framework%2520decomposes%2520human%2520motion%2520into%2520temporal%2520segments%2520and%2520spatial%2520body%2520joints%252C%2520and%2520learns%2520cross-modal%2520correspondences%2520through%2520progressive%2520joint-wise%2520and%2520segment-wise%2520alignment%2520in%2520a%2520pyramidal%2520fashion%252C%2520effectively%2520capturing%2520both%2520local%2520semantic%2520details%2520and%2520hierarchical%2520structural%2520relationships.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520precise%2520alignment%2520between%2520motion%2520segments%2520and%2520body%2520joints%2520and%2520their%2520corresponding%2520text%2520tokens.%2520The%2520code%2520of%2520this%2520work%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21904v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&entry.906535625=Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng&entry.1292438233=As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.21904v3&entry.124074799=Read"},
{"title": "DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking", "author": "Sijia Chen and Lijuan Ma and Yanqiu Yu and En Yu and Liman Liu and Wenbing Tao", "abstract": "Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.", "link": "http://arxiv.org/abs/2602.04692v1", "date": "2026-02-04", "relevancy": 2.9114, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.59}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5794}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRMOT%3A%20A%20Dataset%20and%20Framework%20for%20RGBD%20Referring%20Multi-Object%20Tracking&body=Title%3A%20DRMOT%3A%20A%20Dataset%20and%20Framework%20for%20RGBD%20Referring%20Multi-Object%20Tracking%0AAuthor%3A%20Sijia%20Chen%20and%20Lijuan%20Ma%20and%20Yanqiu%20Yu%20and%20En%20Yu%20and%20Liman%20Liu%20and%20Wenbing%20Tao%0AAbstract%3A%20Referring%20Multi-Object%20Tracking%20%28RMOT%29%20aims%20to%20track%20specific%20targets%20based%20on%20language%20descriptions%20and%20is%20vital%20for%20interactive%20AI%20systems%20such%20as%20robotics%20and%20autonomous%20driving.%20However%2C%20existing%20RMOT%20models%20rely%20solely%20on%202D%20RGB%20data%2C%20making%20it%20challenging%20to%20accurately%20detect%20and%20associate%20targets%20characterized%20by%20complex%20spatial%20semantics%20%28e.g.%2C%20%60%60the%20person%20closest%20to%20the%20camera%27%27%29%20and%20to%20maintain%20reliable%20identities%20under%20severe%20occlusion%2C%20due%20to%20the%20absence%20of%20explicit%203D%20spatial%20information.%20In%20this%20work%2C%20we%20propose%20a%20novel%20task%2C%20RGBD%20Referring%20Multi-Object%20Tracking%20%28DRMOT%29%2C%20which%20explicitly%20requires%20models%20to%20fuse%20RGB%2C%20Depth%20%28D%29%2C%20and%20Language%20%28L%29%20modalities%20to%20achieve%203D-aware%20tracking.%20To%20advance%20research%20on%20the%20DRMOT%20task%2C%20we%20construct%20a%20tailored%20RGBD%20referring%20multi-object%20tracking%20dataset%2C%20named%20DRSet%2C%20designed%20to%20evaluate%20models%27%20spatial-semantic%20grounding%20and%20tracking%20capabilities.%20Specifically%2C%20DRSet%20contains%20RGB%20images%20and%20depth%20maps%20from%20187%20scenes%2C%20along%20with%20240%20language%20descriptions%2C%20among%20which%2056%20descriptions%20incorporate%20depth-related%20information.%20Furthermore%2C%20we%20propose%20DRTrack%2C%20a%20MLLM-guided%20depth-referring%20tracking%20framework.%20DRTrack%20performs%20depth-aware%20target%20grounding%20from%20joint%20RGB-D-L%20inputs%20and%20enforces%20robust%20trajectory%20association%20by%20incorporating%20depth%20cues.%20Extensive%20experiments%20on%20the%20DRSet%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRMOT%253A%2520A%2520Dataset%2520and%2520Framework%2520for%2520RGBD%2520Referring%2520Multi-Object%2520Tracking%26entry.906535625%3DSijia%2520Chen%2520and%2520Lijuan%2520Ma%2520and%2520Yanqiu%2520Yu%2520and%2520En%2520Yu%2520and%2520Liman%2520Liu%2520and%2520Wenbing%2520Tao%26entry.1292438233%3DReferring%2520Multi-Object%2520Tracking%2520%2528RMOT%2529%2520aims%2520to%2520track%2520specific%2520targets%2520based%2520on%2520language%2520descriptions%2520and%2520is%2520vital%2520for%2520interactive%2520AI%2520systems%2520such%2520as%2520robotics%2520and%2520autonomous%2520driving.%2520However%252C%2520existing%2520RMOT%2520models%2520rely%2520solely%2520on%25202D%2520RGB%2520data%252C%2520making%2520it%2520challenging%2520to%2520accurately%2520detect%2520and%2520associate%2520targets%2520characterized%2520by%2520complex%2520spatial%2520semantics%2520%2528e.g.%252C%2520%2560%2560the%2520person%2520closest%2520to%2520the%2520camera%2527%2527%2529%2520and%2520to%2520maintain%2520reliable%2520identities%2520under%2520severe%2520occlusion%252C%2520due%2520to%2520the%2520absence%2520of%2520explicit%25203D%2520spatial%2520information.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520task%252C%2520RGBD%2520Referring%2520Multi-Object%2520Tracking%2520%2528DRMOT%2529%252C%2520which%2520explicitly%2520requires%2520models%2520to%2520fuse%2520RGB%252C%2520Depth%2520%2528D%2529%252C%2520and%2520Language%2520%2528L%2529%2520modalities%2520to%2520achieve%25203D-aware%2520tracking.%2520To%2520advance%2520research%2520on%2520the%2520DRMOT%2520task%252C%2520we%2520construct%2520a%2520tailored%2520RGBD%2520referring%2520multi-object%2520tracking%2520dataset%252C%2520named%2520DRSet%252C%2520designed%2520to%2520evaluate%2520models%2527%2520spatial-semantic%2520grounding%2520and%2520tracking%2520capabilities.%2520Specifically%252C%2520DRSet%2520contains%2520RGB%2520images%2520and%2520depth%2520maps%2520from%2520187%2520scenes%252C%2520along%2520with%2520240%2520language%2520descriptions%252C%2520among%2520which%252056%2520descriptions%2520incorporate%2520depth-related%2520information.%2520Furthermore%252C%2520we%2520propose%2520DRTrack%252C%2520a%2520MLLM-guided%2520depth-referring%2520tracking%2520framework.%2520DRTrack%2520performs%2520depth-aware%2520target%2520grounding%2520from%2520joint%2520RGB-D-L%2520inputs%2520and%2520enforces%2520robust%2520trajectory%2520association%2520by%2520incorporating%2520depth%2520cues.%2520Extensive%2520experiments%2520on%2520the%2520DRSet%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRMOT%3A%20A%20Dataset%20and%20Framework%20for%20RGBD%20Referring%20Multi-Object%20Tracking&entry.906535625=Sijia%20Chen%20and%20Lijuan%20Ma%20and%20Yanqiu%20Yu%20and%20En%20Yu%20and%20Liman%20Liu%20and%20Wenbing%20Tao&entry.1292438233=Referring%20Multi-Object%20Tracking%20%28RMOT%29%20aims%20to%20track%20specific%20targets%20based%20on%20language%20descriptions%20and%20is%20vital%20for%20interactive%20AI%20systems%20such%20as%20robotics%20and%20autonomous%20driving.%20However%2C%20existing%20RMOT%20models%20rely%20solely%20on%202D%20RGB%20data%2C%20making%20it%20challenging%20to%20accurately%20detect%20and%20associate%20targets%20characterized%20by%20complex%20spatial%20semantics%20%28e.g.%2C%20%60%60the%20person%20closest%20to%20the%20camera%27%27%29%20and%20to%20maintain%20reliable%20identities%20under%20severe%20occlusion%2C%20due%20to%20the%20absence%20of%20explicit%203D%20spatial%20information.%20In%20this%20work%2C%20we%20propose%20a%20novel%20task%2C%20RGBD%20Referring%20Multi-Object%20Tracking%20%28DRMOT%29%2C%20which%20explicitly%20requires%20models%20to%20fuse%20RGB%2C%20Depth%20%28D%29%2C%20and%20Language%20%28L%29%20modalities%20to%20achieve%203D-aware%20tracking.%20To%20advance%20research%20on%20the%20DRMOT%20task%2C%20we%20construct%20a%20tailored%20RGBD%20referring%20multi-object%20tracking%20dataset%2C%20named%20DRSet%2C%20designed%20to%20evaluate%20models%27%20spatial-semantic%20grounding%20and%20tracking%20capabilities.%20Specifically%2C%20DRSet%20contains%20RGB%20images%20and%20depth%20maps%20from%20187%20scenes%2C%20along%20with%20240%20language%20descriptions%2C%20among%20which%2056%20descriptions%20incorporate%20depth-related%20information.%20Furthermore%2C%20we%20propose%20DRTrack%2C%20a%20MLLM-guided%20depth-referring%20tracking%20framework.%20DRTrack%20performs%20depth-aware%20target%20grounding%20from%20joint%20RGB-D-L%20inputs%20and%20enforces%20robust%20trajectory%20association%20by%20incorporating%20depth%20cues.%20Extensive%20experiments%20on%20the%20DRSet%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2602.04692v1&entry.124074799=Read"},
{"title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis", "author": "Luca Zedda and Andrea Loddo and Cecilia Di Ruberto", "abstract": "Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.", "link": "http://arxiv.org/abs/2602.04547v1", "date": "2026-02-04", "relevancy": 2.7877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniRad%3A%20A%20Radiological%20Foundation%20Model%20for%20Multi-Task%20Medical%20Image%20Analysis&body=Title%3A%20OmniRad%3A%20A%20Radiological%20Foundation%20Model%20for%20Multi-Task%20Medical%20Image%20Analysis%0AAuthor%3A%20Luca%20Zedda%20and%20Andrea%20Loddo%20and%20Cecilia%20Di%20Ruberto%0AAbstract%3A%20Radiological%20analysis%20increasingly%20benefits%20from%20pretrained%20visual%20representations%20that%20can%20support%20heterogeneous%20downstream%20tasks%20across%20imaging%20modalities.%20In%20this%20work%2C%20we%20introduce%20OmniRad%2C%20a%20self-supervised%20radiological%20foundation%20model%20pretrained%20on%201.2%20million%20medical%20images%2C%20designed%20with%20radiology-inspired%20principles%20emphasizing%20representation%20reuse%20and%20cross-task%20transferability.%20We%20evaluate%20the%20pretrained%20encoder%20under%20multiple%20downstream%20adaptation%20regimes%2C%20including%20lightweight%20task-specific%20adapters%20with%20a%20frozen%20backbone%20as%20well%20as%20full%20end-to-end%20fine-tuning%20for%20classification%2C%20allowing%20us%20to%20assess%20both%20representation%20quality%20and%20task-specific%20performance.%20OmniRad%20is%20evaluated%20on%20a%20broad%20suite%20of%20public%20benchmarks%20spanning%20classification%20and%20segmentation%20across%20multiple%20modalities.%20On%20the%20MedMNISTv2%20collection%2C%20OmniRad%20improves%20classification%20F1%20by%20up%20to%202.05%25%20over%20competing%20foundation%20models.%20For%20dense%20prediction%2C%20OmniRad%20attains%20mean%20Dice%20score%20improvements%20across%20six%20MedSegBench%20datasets%20when%20using%20frozen%20representations.%20Qualitative%20analyses%20and%20latent-space%20visualizations%20suggest%20improved%20feature%20clustering%20and%20modality-related%20separation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniRad%253A%2520A%2520Radiological%2520Foundation%2520Model%2520for%2520Multi-Task%2520Medical%2520Image%2520Analysis%26entry.906535625%3DLuca%2520Zedda%2520and%2520Andrea%2520Loddo%2520and%2520Cecilia%2520Di%2520Ruberto%26entry.1292438233%3DRadiological%2520analysis%2520increasingly%2520benefits%2520from%2520pretrained%2520visual%2520representations%2520that%2520can%2520support%2520heterogeneous%2520downstream%2520tasks%2520across%2520imaging%2520modalities.%2520In%2520this%2520work%252C%2520we%2520introduce%2520OmniRad%252C%2520a%2520self-supervised%2520radiological%2520foundation%2520model%2520pretrained%2520on%25201.2%2520million%2520medical%2520images%252C%2520designed%2520with%2520radiology-inspired%2520principles%2520emphasizing%2520representation%2520reuse%2520and%2520cross-task%2520transferability.%2520We%2520evaluate%2520the%2520pretrained%2520encoder%2520under%2520multiple%2520downstream%2520adaptation%2520regimes%252C%2520including%2520lightweight%2520task-specific%2520adapters%2520with%2520a%2520frozen%2520backbone%2520as%2520well%2520as%2520full%2520end-to-end%2520fine-tuning%2520for%2520classification%252C%2520allowing%2520us%2520to%2520assess%2520both%2520representation%2520quality%2520and%2520task-specific%2520performance.%2520OmniRad%2520is%2520evaluated%2520on%2520a%2520broad%2520suite%2520of%2520public%2520benchmarks%2520spanning%2520classification%2520and%2520segmentation%2520across%2520multiple%2520modalities.%2520On%2520the%2520MedMNISTv2%2520collection%252C%2520OmniRad%2520improves%2520classification%2520F1%2520by%2520up%2520to%25202.05%2525%2520over%2520competing%2520foundation%2520models.%2520For%2520dense%2520prediction%252C%2520OmniRad%2520attains%2520mean%2520Dice%2520score%2520improvements%2520across%2520six%2520MedSegBench%2520datasets%2520when%2520using%2520frozen%2520representations.%2520Qualitative%2520analyses%2520and%2520latent-space%2520visualizations%2520suggest%2520improved%2520feature%2520clustering%2520and%2520modality-related%2520separation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniRad%3A%20A%20Radiological%20Foundation%20Model%20for%20Multi-Task%20Medical%20Image%20Analysis&entry.906535625=Luca%20Zedda%20and%20Andrea%20Loddo%20and%20Cecilia%20Di%20Ruberto&entry.1292438233=Radiological%20analysis%20increasingly%20benefits%20from%20pretrained%20visual%20representations%20that%20can%20support%20heterogeneous%20downstream%20tasks%20across%20imaging%20modalities.%20In%20this%20work%2C%20we%20introduce%20OmniRad%2C%20a%20self-supervised%20radiological%20foundation%20model%20pretrained%20on%201.2%20million%20medical%20images%2C%20designed%20with%20radiology-inspired%20principles%20emphasizing%20representation%20reuse%20and%20cross-task%20transferability.%20We%20evaluate%20the%20pretrained%20encoder%20under%20multiple%20downstream%20adaptation%20regimes%2C%20including%20lightweight%20task-specific%20adapters%20with%20a%20frozen%20backbone%20as%20well%20as%20full%20end-to-end%20fine-tuning%20for%20classification%2C%20allowing%20us%20to%20assess%20both%20representation%20quality%20and%20task-specific%20performance.%20OmniRad%20is%20evaluated%20on%20a%20broad%20suite%20of%20public%20benchmarks%20spanning%20classification%20and%20segmentation%20across%20multiple%20modalities.%20On%20the%20MedMNISTv2%20collection%2C%20OmniRad%20improves%20classification%20F1%20by%20up%20to%202.05%25%20over%20competing%20foundation%20models.%20For%20dense%20prediction%2C%20OmniRad%20attains%20mean%20Dice%20score%20improvements%20across%20six%20MedSegBench%20datasets%20when%20using%20frozen%20representations.%20Qualitative%20analyses%20and%20latent-space%20visualizations%20suggest%20improved%20feature%20clustering%20and%20modality-related%20separation.&entry.1838667208=http%3A//arxiv.org/abs/2602.04547v1&entry.124074799=Read"},
{"title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration", "author": "Jiaheng Liu and Yuanxing Zhang and Shihao Li and Xinping Lei", "abstract": "For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.", "link": "http://arxiv.org/abs/2602.04575v1", "date": "2026-02-04", "relevancy": 2.735, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5775}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5429}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vibe%20AIGC%3A%20A%20New%20Paradigm%20for%20Content%20Generation%20via%20Agentic%20Orchestration&body=Title%3A%20Vibe%20AIGC%3A%20A%20New%20Paradigm%20for%20Content%20Generation%20via%20Agentic%20Orchestration%0AAuthor%3A%20Jiaheng%20Liu%20and%20Yuanxing%20Zhang%20and%20Shihao%20Li%20and%20Xinping%20Lei%0AAbstract%3A%20For%20the%20past%20decade%2C%20the%20trajectory%20of%20generative%20artificial%20intelligence%20%28AI%29%20has%20been%20dominated%20by%20a%20model-centric%20paradigm%20driven%20by%20scaling%20laws.%20Despite%20significant%20leaps%20in%20visual%20fidelity%2C%20this%20approach%20has%20encountered%20a%20%60%60usability%20ceiling%27%27%20manifested%20as%20the%20Intent-Execution%20Gap%20%28i.e.%2C%20the%20fundamental%20disparity%20between%20a%20creator%27s%20high-level%20intent%20and%20the%20stochastic%2C%20black-box%20nature%20of%20current%20single-shot%20models%29.%20In%20this%20paper%2C%20inspired%20by%20the%20Vibe%20Coding%2C%20we%20introduce%20the%20%5Ctextbf%7BVibe%20AIGC%7D%2C%20a%20new%20paradigm%20for%20content%20generation%20via%20agentic%20orchestration%2C%20which%20represents%20the%20autonomous%20synthesis%20of%20hierarchical%20multi-agent%20workflows.%0A%20%20Under%20this%20paradigm%2C%20the%20user%27s%20role%20transcends%20traditional%20prompt%20engineering%2C%20evolving%20into%20a%20Commander%20who%20provides%20a%20Vibe%2C%20a%20high-level%20representation%20encompassing%20aesthetic%20preferences%2C%20functional%20logic%2C%20and%20etc.%20A%20centralized%20Meta-Planner%20then%20functions%20as%20a%20system%20architect%2C%20deconstructing%20this%20%60%60Vibe%27%27%20into%20executable%2C%20verifiable%2C%20and%20adaptive%20agentic%20pipelines.%20By%20transitioning%20from%20stochastic%20inference%20to%20logical%20orchestration%2C%20Vibe%20AIGC%20bridges%20the%20gap%20between%20human%20imagination%20and%20machine%20execution.%20We%20contend%20that%20this%20shift%20will%20redefine%20the%20human-AI%20collaborative%20economy%2C%20transforming%20AI%20from%20a%20fragile%20inference%20engine%20into%20a%20robust%20system-level%20engineering%20partner%20that%20democratizes%20the%20creation%20of%20complex%2C%20long-horizon%20digital%20assets.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibe%2520AIGC%253A%2520A%2520New%2520Paradigm%2520for%2520Content%2520Generation%2520via%2520Agentic%2520Orchestration%26entry.906535625%3DJiaheng%2520Liu%2520and%2520Yuanxing%2520Zhang%2520and%2520Shihao%2520Li%2520and%2520Xinping%2520Lei%26entry.1292438233%3DFor%2520the%2520past%2520decade%252C%2520the%2520trajectory%2520of%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520been%2520dominated%2520by%2520a%2520model-centric%2520paradigm%2520driven%2520by%2520scaling%2520laws.%2520Despite%2520significant%2520leaps%2520in%2520visual%2520fidelity%252C%2520this%2520approach%2520has%2520encountered%2520a%2520%2560%2560usability%2520ceiling%2527%2527%2520manifested%2520as%2520the%2520Intent-Execution%2520Gap%2520%2528i.e.%252C%2520the%2520fundamental%2520disparity%2520between%2520a%2520creator%2527s%2520high-level%2520intent%2520and%2520the%2520stochastic%252C%2520black-box%2520nature%2520of%2520current%2520single-shot%2520models%2529.%2520In%2520this%2520paper%252C%2520inspired%2520by%2520the%2520Vibe%2520Coding%252C%2520we%2520introduce%2520the%2520%255Ctextbf%257BVibe%2520AIGC%257D%252C%2520a%2520new%2520paradigm%2520for%2520content%2520generation%2520via%2520agentic%2520orchestration%252C%2520which%2520represents%2520the%2520autonomous%2520synthesis%2520of%2520hierarchical%2520multi-agent%2520workflows.%250A%2520%2520Under%2520this%2520paradigm%252C%2520the%2520user%2527s%2520role%2520transcends%2520traditional%2520prompt%2520engineering%252C%2520evolving%2520into%2520a%2520Commander%2520who%2520provides%2520a%2520Vibe%252C%2520a%2520high-level%2520representation%2520encompassing%2520aesthetic%2520preferences%252C%2520functional%2520logic%252C%2520and%2520etc.%2520A%2520centralized%2520Meta-Planner%2520then%2520functions%2520as%2520a%2520system%2520architect%252C%2520deconstructing%2520this%2520%2560%2560Vibe%2527%2527%2520into%2520executable%252C%2520verifiable%252C%2520and%2520adaptive%2520agentic%2520pipelines.%2520By%2520transitioning%2520from%2520stochastic%2520inference%2520to%2520logical%2520orchestration%252C%2520Vibe%2520AIGC%2520bridges%2520the%2520gap%2520between%2520human%2520imagination%2520and%2520machine%2520execution.%2520We%2520contend%2520that%2520this%2520shift%2520will%2520redefine%2520the%2520human-AI%2520collaborative%2520economy%252C%2520transforming%2520AI%2520from%2520a%2520fragile%2520inference%2520engine%2520into%2520a%2520robust%2520system-level%2520engineering%2520partner%2520that%2520democratizes%2520the%2520creation%2520of%2520complex%252C%2520long-horizon%2520digital%2520assets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vibe%20AIGC%3A%20A%20New%20Paradigm%20for%20Content%20Generation%20via%20Agentic%20Orchestration&entry.906535625=Jiaheng%20Liu%20and%20Yuanxing%20Zhang%20and%20Shihao%20Li%20and%20Xinping%20Lei&entry.1292438233=For%20the%20past%20decade%2C%20the%20trajectory%20of%20generative%20artificial%20intelligence%20%28AI%29%20has%20been%20dominated%20by%20a%20model-centric%20paradigm%20driven%20by%20scaling%20laws.%20Despite%20significant%20leaps%20in%20visual%20fidelity%2C%20this%20approach%20has%20encountered%20a%20%60%60usability%20ceiling%27%27%20manifested%20as%20the%20Intent-Execution%20Gap%20%28i.e.%2C%20the%20fundamental%20disparity%20between%20a%20creator%27s%20high-level%20intent%20and%20the%20stochastic%2C%20black-box%20nature%20of%20current%20single-shot%20models%29.%20In%20this%20paper%2C%20inspired%20by%20the%20Vibe%20Coding%2C%20we%20introduce%20the%20%5Ctextbf%7BVibe%20AIGC%7D%2C%20a%20new%20paradigm%20for%20content%20generation%20via%20agentic%20orchestration%2C%20which%20represents%20the%20autonomous%20synthesis%20of%20hierarchical%20multi-agent%20workflows.%0A%20%20Under%20this%20paradigm%2C%20the%20user%27s%20role%20transcends%20traditional%20prompt%20engineering%2C%20evolving%20into%20a%20Commander%20who%20provides%20a%20Vibe%2C%20a%20high-level%20representation%20encompassing%20aesthetic%20preferences%2C%20functional%20logic%2C%20and%20etc.%20A%20centralized%20Meta-Planner%20then%20functions%20as%20a%20system%20architect%2C%20deconstructing%20this%20%60%60Vibe%27%27%20into%20executable%2C%20verifiable%2C%20and%20adaptive%20agentic%20pipelines.%20By%20transitioning%20from%20stochastic%20inference%20to%20logical%20orchestration%2C%20Vibe%20AIGC%20bridges%20the%20gap%20between%20human%20imagination%20and%20machine%20execution.%20We%20contend%20that%20this%20shift%20will%20redefine%20the%20human-AI%20collaborative%20economy%2C%20transforming%20AI%20from%20a%20fragile%20inference%20engine%20into%20a%20robust%20system-level%20engineering%20partner%20that%20democratizes%20the%20creation%20of%20complex%2C%20long-horizon%20digital%20assets.&entry.1838667208=http%3A//arxiv.org/abs/2602.04575v1&entry.124074799=Read"},
{"title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking", "author": "Weiguang Zhao and Haoran Xu and Xingyu Miao and Qin Zhao and Rui Zhang and Kaizhu Huang and Ning Gao and Peizhou Cao and Mingze Sun and Mulin Yu and Tao Lu and Linning Xu and Junting Dong and Jiangmiao Pang", "abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.", "link": "http://arxiv.org/abs/2602.04441v1", "date": "2026-02-04", "relevancy": 2.7289, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5815}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthVerse%3A%20A%20Large-Scale%20Diverse%20Synthetic%20Dataset%20for%20Point%20Tracking&body=Title%3A%20SynthVerse%3A%20A%20Large-Scale%20Diverse%20Synthetic%20Dataset%20for%20Point%20Tracking%0AAuthor%3A%20Weiguang%20Zhao%20and%20Haoran%20Xu%20and%20Xingyu%20Miao%20and%20Qin%20Zhao%20and%20Rui%20Zhang%20and%20Kaizhu%20Huang%20and%20Ning%20Gao%20and%20Peizhou%20Cao%20and%20Mingze%20Sun%20and%20Mulin%20Yu%20and%20Tao%20Lu%20and%20Linning%20Xu%20and%20Junting%20Dong%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Point%20tracking%20aims%20to%20follow%20visual%20points%20through%20complex%20motion%2C%20occlusion%2C%20and%20viewpoint%20changes%2C%20and%20has%20advanced%20rapidly%20with%20modern%20foundation%20models.%20Yet%20progress%20toward%20general%20point%20tracking%20remains%20constrained%20by%20limited%20high-quality%20data%2C%20as%20existing%20datasets%20often%20provide%20insufficient%20diversity%20and%20imperfect%20trajectory%20annotations.%20To%20this%20end%2C%20we%20introduce%20SynthVerse%2C%20a%20large-scale%2C%20diverse%20synthetic%20dataset%20specifically%20designed%20for%20point%20tracking.%20SynthVerse%20includes%20several%20new%20domains%20and%20object%20types%20missing%20from%20existing%20synthetic%20datasets%2C%20such%20as%20animated-film-style%20content%2C%20embodied%20manipulation%2C%20scene%20navigation%2C%20and%20articulated%20objects.%20SynthVerse%20substantially%20expands%20dataset%20diversity%20by%20covering%20a%20broader%20range%20of%20object%20categories%20and%20providing%20high-quality%20dynamic%20motions%20and%20interactions%2C%20enabling%20more%20robust%20training%20and%20evaluation%20for%20general%20point%20tracking.%20In%20addition%2C%20we%20establish%20a%20highly%20diverse%20point%20tracking%20benchmark%20to%20systematically%20evaluate%20state-of-the-art%20methods%20under%20broader%20domain%20shifts.%20Extensive%20experiments%20and%20analyses%20demonstrate%20that%20training%20with%20SynthVerse%20yields%20consistent%20improvements%20in%20generalization%20and%20reveal%20limitations%20of%20existing%20trackers%20under%20diverse%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthVerse%253A%2520A%2520Large-Scale%2520Diverse%2520Synthetic%2520Dataset%2520for%2520Point%2520Tracking%26entry.906535625%3DWeiguang%2520Zhao%2520and%2520Haoran%2520Xu%2520and%2520Xingyu%2520Miao%2520and%2520Qin%2520Zhao%2520and%2520Rui%2520Zhang%2520and%2520Kaizhu%2520Huang%2520and%2520Ning%2520Gao%2520and%2520Peizhou%2520Cao%2520and%2520Mingze%2520Sun%2520and%2520Mulin%2520Yu%2520and%2520Tao%2520Lu%2520and%2520Linning%2520Xu%2520and%2520Junting%2520Dong%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DPoint%2520tracking%2520aims%2520to%2520follow%2520visual%2520points%2520through%2520complex%2520motion%252C%2520occlusion%252C%2520and%2520viewpoint%2520changes%252C%2520and%2520has%2520advanced%2520rapidly%2520with%2520modern%2520foundation%2520models.%2520Yet%2520progress%2520toward%2520general%2520point%2520tracking%2520remains%2520constrained%2520by%2520limited%2520high-quality%2520data%252C%2520as%2520existing%2520datasets%2520often%2520provide%2520insufficient%2520diversity%2520and%2520imperfect%2520trajectory%2520annotations.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SynthVerse%252C%2520a%2520large-scale%252C%2520diverse%2520synthetic%2520dataset%2520specifically%2520designed%2520for%2520point%2520tracking.%2520SynthVerse%2520includes%2520several%2520new%2520domains%2520and%2520object%2520types%2520missing%2520from%2520existing%2520synthetic%2520datasets%252C%2520such%2520as%2520animated-film-style%2520content%252C%2520embodied%2520manipulation%252C%2520scene%2520navigation%252C%2520and%2520articulated%2520objects.%2520SynthVerse%2520substantially%2520expands%2520dataset%2520diversity%2520by%2520covering%2520a%2520broader%2520range%2520of%2520object%2520categories%2520and%2520providing%2520high-quality%2520dynamic%2520motions%2520and%2520interactions%252C%2520enabling%2520more%2520robust%2520training%2520and%2520evaluation%2520for%2520general%2520point%2520tracking.%2520In%2520addition%252C%2520we%2520establish%2520a%2520highly%2520diverse%2520point%2520tracking%2520benchmark%2520to%2520systematically%2520evaluate%2520state-of-the-art%2520methods%2520under%2520broader%2520domain%2520shifts.%2520Extensive%2520experiments%2520and%2520analyses%2520demonstrate%2520that%2520training%2520with%2520SynthVerse%2520yields%2520consistent%2520improvements%2520in%2520generalization%2520and%2520reveal%2520limitations%2520of%2520existing%2520trackers%2520under%2520diverse%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthVerse%3A%20A%20Large-Scale%20Diverse%20Synthetic%20Dataset%20for%20Point%20Tracking&entry.906535625=Weiguang%20Zhao%20and%20Haoran%20Xu%20and%20Xingyu%20Miao%20and%20Qin%20Zhao%20and%20Rui%20Zhang%20and%20Kaizhu%20Huang%20and%20Ning%20Gao%20and%20Peizhou%20Cao%20and%20Mingze%20Sun%20and%20Mulin%20Yu%20and%20Tao%20Lu%20and%20Linning%20Xu%20and%20Junting%20Dong%20and%20Jiangmiao%20Pang&entry.1292438233=Point%20tracking%20aims%20to%20follow%20visual%20points%20through%20complex%20motion%2C%20occlusion%2C%20and%20viewpoint%20changes%2C%20and%20has%20advanced%20rapidly%20with%20modern%20foundation%20models.%20Yet%20progress%20toward%20general%20point%20tracking%20remains%20constrained%20by%20limited%20high-quality%20data%2C%20as%20existing%20datasets%20often%20provide%20insufficient%20diversity%20and%20imperfect%20trajectory%20annotations.%20To%20this%20end%2C%20we%20introduce%20SynthVerse%2C%20a%20large-scale%2C%20diverse%20synthetic%20dataset%20specifically%20designed%20for%20point%20tracking.%20SynthVerse%20includes%20several%20new%20domains%20and%20object%20types%20missing%20from%20existing%20synthetic%20datasets%2C%20such%20as%20animated-film-style%20content%2C%20embodied%20manipulation%2C%20scene%20navigation%2C%20and%20articulated%20objects.%20SynthVerse%20substantially%20expands%20dataset%20diversity%20by%20covering%20a%20broader%20range%20of%20object%20categories%20and%20providing%20high-quality%20dynamic%20motions%20and%20interactions%2C%20enabling%20more%20robust%20training%20and%20evaluation%20for%20general%20point%20tracking.%20In%20addition%2C%20we%20establish%20a%20highly%20diverse%20point%20tracking%20benchmark%20to%20systematically%20evaluate%20state-of-the-art%20methods%20under%20broader%20domain%20shifts.%20Extensive%20experiments%20and%20analyses%20demonstrate%20that%20training%20with%20SynthVerse%20yields%20consistent%20improvements%20in%20generalization%20and%20reveal%20limitations%20of%20existing%20trackers%20under%20diverse%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.04441v1&entry.124074799=Read"},
{"title": "When LLaVA Meets Objects: Token Composition for Vision-Language-Models", "author": "Soumya Jahagirdar and Walid Bousselham and Anna Kukleva and Hilde Kuehne", "abstract": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.", "link": "http://arxiv.org/abs/2602.04864v1", "date": "2026-02-04", "relevancy": 2.7197, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models&body=Title%3A%20When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models%0AAuthor%3A%20Soumya%20Jahagirdar%20and%20Walid%20Bousselham%20and%20Anna%20Kukleva%20and%20Hilde%20Kuehne%0AAbstract%3A%20Current%20autoregressive%20Vision%20Language%20Models%20%28VLMs%29%20usually%20rely%20on%20a%20large%20number%20of%20visual%20tokens%20to%20represent%20images%2C%20resulting%20in%20a%20need%20for%20more%20compute%20especially%20at%20inference%20time.%20To%20address%20this%20problem%2C%20we%20propose%20Mask-LLaVA%2C%20a%20framework%20that%20leverages%20different%20levels%20of%20visual%20features%20to%20create%20a%20compact%20yet%20information-rich%20visual%20representation%20for%20autoregressive%20VLMs.%20Namely%2C%20we%20combine%20mask-based%20object%20representations%20together%20with%20global%20tokens%20and%20local%20patch%20tokens.%20While%20all%20tokens%20are%20used%20during%20training%2C%20it%20shows%20that%20the%20resulting%20model%20can%20flexibly%20drop%20especially%20the%20number%20of%20mask-based%20object-tokens%20at%20test%20time%2C%20allowing%20to%20adapt%20the%20number%20of%20tokens%20during%20inference%20without%20the%20need%20to%20retrain%20the%20model%20and%20without%20a%20significant%20drop%20in%20performance.%20We%20evaluate%20the%20proposed%20approach%20on%20a%20suite%20of%20standard%20benchmarks%20showing%20results%20competitive%20to%20current%20token%20efficient%20methods%20and%20comparable%20to%20the%20original%20LLaVA%20baseline%20using%20only%20a%20fraction%20of%20visual%20tokens.%20Our%20analysis%20demonstrates%20that%20combining%20multi-level%20features%20enables%20efficient%20learning%20with%20fewer%20tokens%20while%20allowing%20dynamic%20token%20selection%20at%20test%20time%20for%20good%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LLaVA%2520Meets%2520Objects%253A%2520Token%2520Composition%2520for%2520Vision-Language-Models%26entry.906535625%3DSoumya%2520Jahagirdar%2520and%2520Walid%2520Bousselham%2520and%2520Anna%2520Kukleva%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3DCurrent%2520autoregressive%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520usually%2520rely%2520on%2520a%2520large%2520number%2520of%2520visual%2520tokens%2520to%2520represent%2520images%252C%2520resulting%2520in%2520a%2520need%2520for%2520more%2520compute%2520especially%2520at%2520inference%2520time.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Mask-LLaVA%252C%2520a%2520framework%2520that%2520leverages%2520different%2520levels%2520of%2520visual%2520features%2520to%2520create%2520a%2520compact%2520yet%2520information-rich%2520visual%2520representation%2520for%2520autoregressive%2520VLMs.%2520Namely%252C%2520we%2520combine%2520mask-based%2520object%2520representations%2520together%2520with%2520global%2520tokens%2520and%2520local%2520patch%2520tokens.%2520While%2520all%2520tokens%2520are%2520used%2520during%2520training%252C%2520it%2520shows%2520that%2520the%2520resulting%2520model%2520can%2520flexibly%2520drop%2520especially%2520the%2520number%2520of%2520mask-based%2520object-tokens%2520at%2520test%2520time%252C%2520allowing%2520to%2520adapt%2520the%2520number%2520of%2520tokens%2520during%2520inference%2520without%2520the%2520need%2520to%2520retrain%2520the%2520model%2520and%2520without%2520a%2520significant%2520drop%2520in%2520performance.%2520We%2520evaluate%2520the%2520proposed%2520approach%2520on%2520a%2520suite%2520of%2520standard%2520benchmarks%2520showing%2520results%2520competitive%2520to%2520current%2520token%2520efficient%2520methods%2520and%2520comparable%2520to%2520the%2520original%2520LLaVA%2520baseline%2520using%2520only%2520a%2520fraction%2520of%2520visual%2520tokens.%2520Our%2520analysis%2520demonstrates%2520that%2520combining%2520multi-level%2520features%2520enables%2520efficient%2520learning%2520with%2520fewer%2520tokens%2520while%2520allowing%2520dynamic%2520token%2520selection%2520at%2520test%2520time%2520for%2520good%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LLaVA%20Meets%20Objects%3A%20Token%20Composition%20for%20Vision-Language-Models&entry.906535625=Soumya%20Jahagirdar%20and%20Walid%20Bousselham%20and%20Anna%20Kukleva%20and%20Hilde%20Kuehne&entry.1292438233=Current%20autoregressive%20Vision%20Language%20Models%20%28VLMs%29%20usually%20rely%20on%20a%20large%20number%20of%20visual%20tokens%20to%20represent%20images%2C%20resulting%20in%20a%20need%20for%20more%20compute%20especially%20at%20inference%20time.%20To%20address%20this%20problem%2C%20we%20propose%20Mask-LLaVA%2C%20a%20framework%20that%20leverages%20different%20levels%20of%20visual%20features%20to%20create%20a%20compact%20yet%20information-rich%20visual%20representation%20for%20autoregressive%20VLMs.%20Namely%2C%20we%20combine%20mask-based%20object%20representations%20together%20with%20global%20tokens%20and%20local%20patch%20tokens.%20While%20all%20tokens%20are%20used%20during%20training%2C%20it%20shows%20that%20the%20resulting%20model%20can%20flexibly%20drop%20especially%20the%20number%20of%20mask-based%20object-tokens%20at%20test%20time%2C%20allowing%20to%20adapt%20the%20number%20of%20tokens%20during%20inference%20without%20the%20need%20to%20retrain%20the%20model%20and%20without%20a%20significant%20drop%20in%20performance.%20We%20evaluate%20the%20proposed%20approach%20on%20a%20suite%20of%20standard%20benchmarks%20showing%20results%20competitive%20to%20current%20token%20efficient%20methods%20and%20comparable%20to%20the%20original%20LLaVA%20baseline%20using%20only%20a%20fraction%20of%20visual%20tokens.%20Our%20analysis%20demonstrates%20that%20combining%20multi-level%20features%20enables%20efficient%20learning%20with%20fewer%20tokens%20while%20allowing%20dynamic%20token%20selection%20at%20test%20time%20for%20good%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2602.04864v1&entry.124074799=Read"},
{"title": "Focus-LIME: Surgical Interpretation of Long-Context Large Language Models via Proxy-Based Neighborhood Selection", "author": "Junhao Liu and Haonan Yu and Zhenyu Yan and Xin Zhang", "abstract": "As Large Language Models (LLMs) scale to handle massive context windows, achieving surgical feature-level interpretation is essential for high-stakes tasks like legal auditing and code debugging. However, existing local model-agnostic explanation methods face a critical dilemma in these scenarios: feature-based methods suffer from attribution dilution due to high feature dimensionality, thus failing to provide faithful explanations. In this paper, we propose Focus-LIME, a coarse-to-fine framework designed to restore the tractability of surgical interpretation. Focus-LIME utilizes a proxy model to curate the perturbation neighborhood, allowing the target model to perform fine-grained attribution exclusively within the optimized context. Empirical evaluations on long-context benchmarks demonstrate that our method makes surgical explanations practicable and provides faithful explanations to users.", "link": "http://arxiv.org/abs/2602.04607v1", "date": "2026-02-04", "relevancy": 2.7054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus-LIME%3A%20Surgical%20Interpretation%20of%20Long-Context%20Large%20Language%20Models%20via%20Proxy-Based%20Neighborhood%20Selection&body=Title%3A%20Focus-LIME%3A%20Surgical%20Interpretation%20of%20Long-Context%20Large%20Language%20Models%20via%20Proxy-Based%20Neighborhood%20Selection%0AAuthor%3A%20Junhao%20Liu%20and%20Haonan%20Yu%20and%20Zhenyu%20Yan%20and%20Xin%20Zhang%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20scale%20to%20handle%20massive%20context%20windows%2C%20achieving%20surgical%20feature-level%20interpretation%20is%20essential%20for%20high-stakes%20tasks%20like%20legal%20auditing%20and%20code%20debugging.%20However%2C%20existing%20local%20model-agnostic%20explanation%20methods%20face%20a%20critical%20dilemma%20in%20these%20scenarios%3A%20feature-based%20methods%20suffer%20from%20attribution%20dilution%20due%20to%20high%20feature%20dimensionality%2C%20thus%20failing%20to%20provide%20faithful%20explanations.%20In%20this%20paper%2C%20we%20propose%20Focus-LIME%2C%20a%20coarse-to-fine%20framework%20designed%20to%20restore%20the%20tractability%20of%20surgical%20interpretation.%20Focus-LIME%20utilizes%20a%20proxy%20model%20to%20curate%20the%20perturbation%20neighborhood%2C%20allowing%20the%20target%20model%20to%20perform%20fine-grained%20attribution%20exclusively%20within%20the%20optimized%20context.%20Empirical%20evaluations%20on%20long-context%20benchmarks%20demonstrate%20that%20our%20method%20makes%20surgical%20explanations%20practicable%20and%20provides%20faithful%20explanations%20to%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus-LIME%253A%2520Surgical%2520Interpretation%2520of%2520Long-Context%2520Large%2520Language%2520Models%2520via%2520Proxy-Based%2520Neighborhood%2520Selection%26entry.906535625%3DJunhao%2520Liu%2520and%2520Haonan%2520Yu%2520and%2520Zhenyu%2520Yan%2520and%2520Xin%2520Zhang%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520scale%2520to%2520handle%2520massive%2520context%2520windows%252C%2520achieving%2520surgical%2520feature-level%2520interpretation%2520is%2520essential%2520for%2520high-stakes%2520tasks%2520like%2520legal%2520auditing%2520and%2520code%2520debugging.%2520However%252C%2520existing%2520local%2520model-agnostic%2520explanation%2520methods%2520face%2520a%2520critical%2520dilemma%2520in%2520these%2520scenarios%253A%2520feature-based%2520methods%2520suffer%2520from%2520attribution%2520dilution%2520due%2520to%2520high%2520feature%2520dimensionality%252C%2520thus%2520failing%2520to%2520provide%2520faithful%2520explanations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Focus-LIME%252C%2520a%2520coarse-to-fine%2520framework%2520designed%2520to%2520restore%2520the%2520tractability%2520of%2520surgical%2520interpretation.%2520Focus-LIME%2520utilizes%2520a%2520proxy%2520model%2520to%2520curate%2520the%2520perturbation%2520neighborhood%252C%2520allowing%2520the%2520target%2520model%2520to%2520perform%2520fine-grained%2520attribution%2520exclusively%2520within%2520the%2520optimized%2520context.%2520Empirical%2520evaluations%2520on%2520long-context%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520makes%2520surgical%2520explanations%2520practicable%2520and%2520provides%2520faithful%2520explanations%2520to%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus-LIME%3A%20Surgical%20Interpretation%20of%20Long-Context%20Large%20Language%20Models%20via%20Proxy-Based%20Neighborhood%20Selection&entry.906535625=Junhao%20Liu%20and%20Haonan%20Yu%20and%20Zhenyu%20Yan%20and%20Xin%20Zhang&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20scale%20to%20handle%20massive%20context%20windows%2C%20achieving%20surgical%20feature-level%20interpretation%20is%20essential%20for%20high-stakes%20tasks%20like%20legal%20auditing%20and%20code%20debugging.%20However%2C%20existing%20local%20model-agnostic%20explanation%20methods%20face%20a%20critical%20dilemma%20in%20these%20scenarios%3A%20feature-based%20methods%20suffer%20from%20attribution%20dilution%20due%20to%20high%20feature%20dimensionality%2C%20thus%20failing%20to%20provide%20faithful%20explanations.%20In%20this%20paper%2C%20we%20propose%20Focus-LIME%2C%20a%20coarse-to-fine%20framework%20designed%20to%20restore%20the%20tractability%20of%20surgical%20interpretation.%20Focus-LIME%20utilizes%20a%20proxy%20model%20to%20curate%20the%20perturbation%20neighborhood%2C%20allowing%20the%20target%20model%20to%20perform%20fine-grained%20attribution%20exclusively%20within%20the%20optimized%20context.%20Empirical%20evaluations%20on%20long-context%20benchmarks%20demonstrate%20that%20our%20method%20makes%20surgical%20explanations%20practicable%20and%20provides%20faithful%20explanations%20to%20users.&entry.1838667208=http%3A//arxiv.org/abs/2602.04607v1&entry.124074799=Read"},
{"title": "Annotation Free Spacecraft Detection and Segmentation using Vision Language Models", "author": "Samet Hicsonmez and Jose Sosa and Dan Pineau and Inder Pal Singh and Arunkumar Rathinam and Abd El Rahman Shabayek and Djamila Aouada", "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.", "link": "http://arxiv.org/abs/2602.04699v1", "date": "2026-02-04", "relevancy": 2.6964, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation%20Free%20Spacecraft%20Detection%20and%20Segmentation%20using%20Vision%20Language%20Models&body=Title%3A%20Annotation%20Free%20Spacecraft%20Detection%20and%20Segmentation%20using%20Vision%20Language%20Models%0AAuthor%3A%20Samet%20Hicsonmez%20and%20Jose%20Sosa%20and%20Dan%20Pineau%20and%20Inder%20Pal%20Singh%20and%20Arunkumar%20Rathinam%20and%20Abd%20El%20Rahman%20Shabayek%20and%20Djamila%20Aouada%0AAbstract%3A%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20open-world%20zero-shot%20visual%20recognition.%20However%2C%20their%20potential%20in%20space-related%20applications%20remains%20largely%20unexplored.%20In%20the%20space%20domain%2C%20accurate%20manual%20annotation%20is%20particularly%20challenging%20due%20to%20factors%20such%20as%20low%20visibility%2C%20illumination%20variations%2C%20and%20object%20blending%20with%20planetary%20backgrounds.%20Developing%20methods%20that%20can%20detect%20and%20segment%20spacecraft%20and%20orbital%20targets%20without%20requiring%20extensive%20manual%20labeling%20is%20therefore%20of%20critical%20importance.%20In%20this%20work%2C%20we%20propose%20an%20annotation-free%20detection%20and%20segmentation%20pipeline%20for%20space%20targets%20using%20VLMs.%20Our%20approach%20begins%20by%20automatically%20generating%20pseudo-labels%20for%20a%20small%20subset%20of%20unlabeled%20real%20data%20with%20a%20pre-trained%20VLM.%20These%20pseudo-labels%20are%20then%20leveraged%20in%20a%20teacher-student%20label%20distillation%20framework%20to%20train%20lightweight%20models.%20Despite%20the%20inherent%20noise%20in%20the%20pseudo-labels%2C%20the%20distillation%20process%20leads%20to%20substantial%20performance%20gains%20over%20direct%20zero-shot%20VLM%20inference.%20Experimental%20evaluations%20on%20the%20SPARK-2024%2C%20SPEED%2B%2C%20and%20TANGO%20datasets%20on%20segmentation%20tasks%20demonstrate%20consistent%20improvements%20in%20average%20precision%20%28AP%29%20by%20up%20to%2010%20points.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/giddyyupp/annotation-free-spacecraft-segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation%2520Free%2520Spacecraft%2520Detection%2520and%2520Segmentation%2520using%2520Vision%2520Language%2520Models%26entry.906535625%3DSamet%2520Hicsonmez%2520and%2520Jose%2520Sosa%2520and%2520Dan%2520Pineau%2520and%2520Inder%2520Pal%2520Singh%2520and%2520Arunkumar%2520Rathinam%2520and%2520Abd%2520El%2520Rahman%2520Shabayek%2520and%2520Djamila%2520Aouada%26entry.1292438233%3DVision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520open-world%2520zero-shot%2520visual%2520recognition.%2520However%252C%2520their%2520potential%2520in%2520space-related%2520applications%2520remains%2520largely%2520unexplored.%2520In%2520the%2520space%2520domain%252C%2520accurate%2520manual%2520annotation%2520is%2520particularly%2520challenging%2520due%2520to%2520factors%2520such%2520as%2520low%2520visibility%252C%2520illumination%2520variations%252C%2520and%2520object%2520blending%2520with%2520planetary%2520backgrounds.%2520Developing%2520methods%2520that%2520can%2520detect%2520and%2520segment%2520spacecraft%2520and%2520orbital%2520targets%2520without%2520requiring%2520extensive%2520manual%2520labeling%2520is%2520therefore%2520of%2520critical%2520importance.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520annotation-free%2520detection%2520and%2520segmentation%2520pipeline%2520for%2520space%2520targets%2520using%2520VLMs.%2520Our%2520approach%2520begins%2520by%2520automatically%2520generating%2520pseudo-labels%2520for%2520a%2520small%2520subset%2520of%2520unlabeled%2520real%2520data%2520with%2520a%2520pre-trained%2520VLM.%2520These%2520pseudo-labels%2520are%2520then%2520leveraged%2520in%2520a%2520teacher-student%2520label%2520distillation%2520framework%2520to%2520train%2520lightweight%2520models.%2520Despite%2520the%2520inherent%2520noise%2520in%2520the%2520pseudo-labels%252C%2520the%2520distillation%2520process%2520leads%2520to%2520substantial%2520performance%2520gains%2520over%2520direct%2520zero-shot%2520VLM%2520inference.%2520Experimental%2520evaluations%2520on%2520the%2520SPARK-2024%252C%2520SPEED%252B%252C%2520and%2520TANGO%2520datasets%2520on%2520segmentation%2520tasks%2520demonstrate%2520consistent%2520improvements%2520in%2520average%2520precision%2520%2528AP%2529%2520by%2520up%2520to%252010%2520points.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/giddyyupp/annotation-free-spacecraft-segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation%20Free%20Spacecraft%20Detection%20and%20Segmentation%20using%20Vision%20Language%20Models&entry.906535625=Samet%20Hicsonmez%20and%20Jose%20Sosa%20and%20Dan%20Pineau%20and%20Inder%20Pal%20Singh%20and%20Arunkumar%20Rathinam%20and%20Abd%20El%20Rahman%20Shabayek%20and%20Djamila%20Aouada&entry.1292438233=Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20open-world%20zero-shot%20visual%20recognition.%20However%2C%20their%20potential%20in%20space-related%20applications%20remains%20largely%20unexplored.%20In%20the%20space%20domain%2C%20accurate%20manual%20annotation%20is%20particularly%20challenging%20due%20to%20factors%20such%20as%20low%20visibility%2C%20illumination%20variations%2C%20and%20object%20blending%20with%20planetary%20backgrounds.%20Developing%20methods%20that%20can%20detect%20and%20segment%20spacecraft%20and%20orbital%20targets%20without%20requiring%20extensive%20manual%20labeling%20is%20therefore%20of%20critical%20importance.%20In%20this%20work%2C%20we%20propose%20an%20annotation-free%20detection%20and%20segmentation%20pipeline%20for%20space%20targets%20using%20VLMs.%20Our%20approach%20begins%20by%20automatically%20generating%20pseudo-labels%20for%20a%20small%20subset%20of%20unlabeled%20real%20data%20with%20a%20pre-trained%20VLM.%20These%20pseudo-labels%20are%20then%20leveraged%20in%20a%20teacher-student%20label%20distillation%20framework%20to%20train%20lightweight%20models.%20Despite%20the%20inherent%20noise%20in%20the%20pseudo-labels%2C%20the%20distillation%20process%20leads%20to%20substantial%20performance%20gains%20over%20direct%20zero-shot%20VLM%20inference.%20Experimental%20evaluations%20on%20the%20SPARK-2024%2C%20SPEED%2B%2C%20and%20TANGO%20datasets%20on%20segmentation%20tasks%20demonstrate%20consistent%20improvements%20in%20average%20precision%20%28AP%29%20by%20up%20to%2010%20points.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/giddyyupp/annotation-free-spacecraft-segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2602.04699v1&entry.124074799=Read"},
{"title": "Temporal Slowness in Central Vision Drives Semantic Object Learning", "author": "Timothy Schauml\u00f6ffel and Arthur Aubret and Gemma Roig and Jochen Triesch", "abstract": "Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.", "link": "http://arxiv.org/abs/2602.04462v1", "date": "2026-02-04", "relevancy": 2.6829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Slowness%20in%20Central%20Vision%20Drives%20Semantic%20Object%20Learning&body=Title%3A%20Temporal%20Slowness%20in%20Central%20Vision%20Drives%20Semantic%20Object%20Learning%0AAuthor%3A%20Timothy%20Schauml%C3%B6ffel%20and%20Arthur%20Aubret%20and%20Gemma%20Roig%20and%20Jochen%20Triesch%0AAbstract%3A%20Humans%20acquire%20semantic%20object%20representations%20from%20egocentric%20visual%20streams%20with%20minimal%20supervision.%20Importantly%2C%20the%20visual%20system%20processes%20with%20high%20resolution%20only%20the%20center%20of%20its%20field%20of%20view%20and%20learns%20similar%20representations%20for%20visual%20inputs%20occurring%20close%20in%20time.%20This%20emphasizes%20slowly%20changing%20information%20around%20gaze%20locations.%20This%20study%20investigates%20the%20role%20of%20central%20vision%20and%20slowness%20learning%20in%20the%20formation%20of%20semantic%20object%20representations%20from%20human-like%20visual%20experience.%20We%20simulate%20five%20months%20of%20human-like%20visual%20experience%20using%20the%20Ego4D%20dataset%20and%20generate%20gaze%20coordinates%20with%20a%20state-of-the-art%20gaze%20prediction%20model.%20Using%20these%20predictions%2C%20we%20extract%20crops%20that%20mimic%20central%20vision%20and%20train%20a%20time-contrastive%20Self-Supervised%20Learning%20model%20on%20them.%20Our%20results%20show%20that%20combining%20temporal%20slowness%20and%20central%20vision%20improves%20the%20encoding%20of%20different%20semantic%20facets%20of%20object%20representations.%20Specifically%2C%20focusing%20on%20central%20vision%20strengthens%20the%20extraction%20of%20foreground%20object%20features%2C%20while%20considering%20temporal%20slowness%2C%20especially%20during%20fixational%20eye%20movements%2C%20allows%20the%20model%20to%20encode%20broader%20semantic%20information%20about%20objects.%20These%20findings%20provide%20new%20insights%20into%20the%20mechanisms%20by%20which%20humans%20may%20develop%20semantic%20object%20representations%20from%20natural%20visual%20experience.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Slowness%2520in%2520Central%2520Vision%2520Drives%2520Semantic%2520Object%2520Learning%26entry.906535625%3DTimothy%2520Schauml%25C3%25B6ffel%2520and%2520Arthur%2520Aubret%2520and%2520Gemma%2520Roig%2520and%2520Jochen%2520Triesch%26entry.1292438233%3DHumans%2520acquire%2520semantic%2520object%2520representations%2520from%2520egocentric%2520visual%2520streams%2520with%2520minimal%2520supervision.%2520Importantly%252C%2520the%2520visual%2520system%2520processes%2520with%2520high%2520resolution%2520only%2520the%2520center%2520of%2520its%2520field%2520of%2520view%2520and%2520learns%2520similar%2520representations%2520for%2520visual%2520inputs%2520occurring%2520close%2520in%2520time.%2520This%2520emphasizes%2520slowly%2520changing%2520information%2520around%2520gaze%2520locations.%2520This%2520study%2520investigates%2520the%2520role%2520of%2520central%2520vision%2520and%2520slowness%2520learning%2520in%2520the%2520formation%2520of%2520semantic%2520object%2520representations%2520from%2520human-like%2520visual%2520experience.%2520We%2520simulate%2520five%2520months%2520of%2520human-like%2520visual%2520experience%2520using%2520the%2520Ego4D%2520dataset%2520and%2520generate%2520gaze%2520coordinates%2520with%2520a%2520state-of-the-art%2520gaze%2520prediction%2520model.%2520Using%2520these%2520predictions%252C%2520we%2520extract%2520crops%2520that%2520mimic%2520central%2520vision%2520and%2520train%2520a%2520time-contrastive%2520Self-Supervised%2520Learning%2520model%2520on%2520them.%2520Our%2520results%2520show%2520that%2520combining%2520temporal%2520slowness%2520and%2520central%2520vision%2520improves%2520the%2520encoding%2520of%2520different%2520semantic%2520facets%2520of%2520object%2520representations.%2520Specifically%252C%2520focusing%2520on%2520central%2520vision%2520strengthens%2520the%2520extraction%2520of%2520foreground%2520object%2520features%252C%2520while%2520considering%2520temporal%2520slowness%252C%2520especially%2520during%2520fixational%2520eye%2520movements%252C%2520allows%2520the%2520model%2520to%2520encode%2520broader%2520semantic%2520information%2520about%2520objects.%2520These%2520findings%2520provide%2520new%2520insights%2520into%2520the%2520mechanisms%2520by%2520which%2520humans%2520may%2520develop%2520semantic%2520object%2520representations%2520from%2520natural%2520visual%2520experience.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Slowness%20in%20Central%20Vision%20Drives%20Semantic%20Object%20Learning&entry.906535625=Timothy%20Schauml%C3%B6ffel%20and%20Arthur%20Aubret%20and%20Gemma%20Roig%20and%20Jochen%20Triesch&entry.1292438233=Humans%20acquire%20semantic%20object%20representations%20from%20egocentric%20visual%20streams%20with%20minimal%20supervision.%20Importantly%2C%20the%20visual%20system%20processes%20with%20high%20resolution%20only%20the%20center%20of%20its%20field%20of%20view%20and%20learns%20similar%20representations%20for%20visual%20inputs%20occurring%20close%20in%20time.%20This%20emphasizes%20slowly%20changing%20information%20around%20gaze%20locations.%20This%20study%20investigates%20the%20role%20of%20central%20vision%20and%20slowness%20learning%20in%20the%20formation%20of%20semantic%20object%20representations%20from%20human-like%20visual%20experience.%20We%20simulate%20five%20months%20of%20human-like%20visual%20experience%20using%20the%20Ego4D%20dataset%20and%20generate%20gaze%20coordinates%20with%20a%20state-of-the-art%20gaze%20prediction%20model.%20Using%20these%20predictions%2C%20we%20extract%20crops%20that%20mimic%20central%20vision%20and%20train%20a%20time-contrastive%20Self-Supervised%20Learning%20model%20on%20them.%20Our%20results%20show%20that%20combining%20temporal%20slowness%20and%20central%20vision%20improves%20the%20encoding%20of%20different%20semantic%20facets%20of%20object%20representations.%20Specifically%2C%20focusing%20on%20central%20vision%20strengthens%20the%20extraction%20of%20foreground%20object%20features%2C%20while%20considering%20temporal%20slowness%2C%20especially%20during%20fixational%20eye%20movements%2C%20allows%20the%20model%20to%20encode%20broader%20semantic%20information%20about%20objects.%20These%20findings%20provide%20new%20insights%20into%20the%20mechanisms%20by%20which%20humans%20may%20develop%20semantic%20object%20representations%20from%20natural%20visual%20experience.&entry.1838667208=http%3A//arxiv.org/abs/2602.04462v1&entry.124074799=Read"},
{"title": "PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective", "author": "Haokui Zhang and Congyang Ou and Dawei Yan and Peng Wang and Qingsen Yan and Ying Li and Rong Xiao and Chunhua Shen", "abstract": "Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.", "link": "http://arxiv.org/abs/2602.04657v1", "date": "2026-02-04", "relevancy": 2.6504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective&body=Title%3A%20PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective%0AAuthor%3A%20Haokui%20Zhang%20and%20Congyang%20Ou%20and%20Dawei%20Yan%20and%20Peng%20Wang%20and%20Qingsen%20Yan%20and%20Ying%20Li%20and%20Rong%20Xiao%20and%20Chunhua%20Shen%0AAbstract%3A%20Recently%2C%20reducing%20redundant%20visual%20tokens%20in%20vision-language%20models%20%28VLMs%29%20to%20accelerate%20VLM%20inference%20has%20emerged%20as%20a%20hot%20topic.%20However%2C%20most%20existing%20methods%20rely%20on%20heuristics%20constructed%20based%20on%20inter-visual-token%20similarity%20or%20cross-modal%20visual-text%20similarity%2C%20which%20gives%20rise%20to%20certain%20limitations%20in%20compression%20performance%20and%20practical%20deployment.%20In%20contrast%2C%20we%20propose%20PIO-FVLM%20from%20the%20perspective%20of%20inference%20objectives%2C%20which%20transforms%20visual%20token%20compression%20into%20preserving%20output%20result%20invariance%20and%20selects%20tokens%20primarily%20by%20their%20importance%20to%20this%20goal.%20Specially%2C%20vision%20tokens%20are%20reordered%20with%20the%20guidance%20of%20token-level%20gradient%20saliency%20generated%20by%20our%20designed%20layer-local%20proxy%20loss%2C%20a%20coarse%20constraint%20from%20the%20current%20layer%20to%20the%20final%20result.%20Then%20the%20most%20valuable%20vision%20tokens%20are%20selected%20following%20the%20non-maximum%20suppression%20%28NMS%29%20principle.%20The%20proposed%20PIO-FVLM%20is%20training-free%20and%20compatible%20with%20FlashAttention%2C%20friendly%20to%20practical%20application%20and%20deployment.%20It%20can%20be%20deployed%20independently%20as%20an%20encoder-free%20method%2C%20or%20combined%20with%20encoder%20compression%20approaches%20like%20VisionZip%20for%20use%20as%20an%20encoder-involved%20method.%20On%20LLaVA-Next-7B%2C%20PIO-FVLM%20retains%20just%2011.1%25%20of%20visual%20tokens%20but%20maintains%2097.2%25%20of%20the%20original%20performance%2C%20with%20a%202.67%24%5Ctimes%24%20prefill%20speedup%2C%202.11%24%5Ctimes%24%20inference%20speedup%2C%206.22%24%5Ctimes%24%20lower%20FLOPs%2C%20and%206.05%24%5Ctimes%24%20reduced%20KV%20Cache%20overhead.%20Our%20code%20is%20available%20at%20https%3A//github.com/ocy1/PIO-FVLM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIO-FVLM%253A%2520Rethinking%2520Training-Free%2520Visual%2520Token%2520Reduction%2520for%2520VLM%2520Acceleration%2520from%2520an%2520Inference-Objective%2520Perspective%26entry.906535625%3DHaokui%2520Zhang%2520and%2520Congyang%2520Ou%2520and%2520Dawei%2520Yan%2520and%2520Peng%2520Wang%2520and%2520Qingsen%2520Yan%2520and%2520Ying%2520Li%2520and%2520Rong%2520Xiao%2520and%2520Chunhua%2520Shen%26entry.1292438233%3DRecently%252C%2520reducing%2520redundant%2520visual%2520tokens%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520accelerate%2520VLM%2520inference%2520has%2520emerged%2520as%2520a%2520hot%2520topic.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%2520heuristics%2520constructed%2520based%2520on%2520inter-visual-token%2520similarity%2520or%2520cross-modal%2520visual-text%2520similarity%252C%2520which%2520gives%2520rise%2520to%2520certain%2520limitations%2520in%2520compression%2520performance%2520and%2520practical%2520deployment.%2520In%2520contrast%252C%2520we%2520propose%2520PIO-FVLM%2520from%2520the%2520perspective%2520of%2520inference%2520objectives%252C%2520which%2520transforms%2520visual%2520token%2520compression%2520into%2520preserving%2520output%2520result%2520invariance%2520and%2520selects%2520tokens%2520primarily%2520by%2520their%2520importance%2520to%2520this%2520goal.%2520Specially%252C%2520vision%2520tokens%2520are%2520reordered%2520with%2520the%2520guidance%2520of%2520token-level%2520gradient%2520saliency%2520generated%2520by%2520our%2520designed%2520layer-local%2520proxy%2520loss%252C%2520a%2520coarse%2520constraint%2520from%2520the%2520current%2520layer%2520to%2520the%2520final%2520result.%2520Then%2520the%2520most%2520valuable%2520vision%2520tokens%2520are%2520selected%2520following%2520the%2520non-maximum%2520suppression%2520%2528NMS%2529%2520principle.%2520The%2520proposed%2520PIO-FVLM%2520is%2520training-free%2520and%2520compatible%2520with%2520FlashAttention%252C%2520friendly%2520to%2520practical%2520application%2520and%2520deployment.%2520It%2520can%2520be%2520deployed%2520independently%2520as%2520an%2520encoder-free%2520method%252C%2520or%2520combined%2520with%2520encoder%2520compression%2520approaches%2520like%2520VisionZip%2520for%2520use%2520as%2520an%2520encoder-involved%2520method.%2520On%2520LLaVA-Next-7B%252C%2520PIO-FVLM%2520retains%2520just%252011.1%2525%2520of%2520visual%2520tokens%2520but%2520maintains%252097.2%2525%2520of%2520the%2520original%2520performance%252C%2520with%2520a%25202.67%2524%255Ctimes%2524%2520prefill%2520speedup%252C%25202.11%2524%255Ctimes%2524%2520inference%2520speedup%252C%25206.22%2524%255Ctimes%2524%2520lower%2520FLOPs%252C%2520and%25206.05%2524%255Ctimes%2524%2520reduced%2520KV%2520Cache%2520overhead.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ocy1/PIO-FVLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective&entry.906535625=Haokui%20Zhang%20and%20Congyang%20Ou%20and%20Dawei%20Yan%20and%20Peng%20Wang%20and%20Qingsen%20Yan%20and%20Ying%20Li%20and%20Rong%20Xiao%20and%20Chunhua%20Shen&entry.1292438233=Recently%2C%20reducing%20redundant%20visual%20tokens%20in%20vision-language%20models%20%28VLMs%29%20to%20accelerate%20VLM%20inference%20has%20emerged%20as%20a%20hot%20topic.%20However%2C%20most%20existing%20methods%20rely%20on%20heuristics%20constructed%20based%20on%20inter-visual-token%20similarity%20or%20cross-modal%20visual-text%20similarity%2C%20which%20gives%20rise%20to%20certain%20limitations%20in%20compression%20performance%20and%20practical%20deployment.%20In%20contrast%2C%20we%20propose%20PIO-FVLM%20from%20the%20perspective%20of%20inference%20objectives%2C%20which%20transforms%20visual%20token%20compression%20into%20preserving%20output%20result%20invariance%20and%20selects%20tokens%20primarily%20by%20their%20importance%20to%20this%20goal.%20Specially%2C%20vision%20tokens%20are%20reordered%20with%20the%20guidance%20of%20token-level%20gradient%20saliency%20generated%20by%20our%20designed%20layer-local%20proxy%20loss%2C%20a%20coarse%20constraint%20from%20the%20current%20layer%20to%20the%20final%20result.%20Then%20the%20most%20valuable%20vision%20tokens%20are%20selected%20following%20the%20non-maximum%20suppression%20%28NMS%29%20principle.%20The%20proposed%20PIO-FVLM%20is%20training-free%20and%20compatible%20with%20FlashAttention%2C%20friendly%20to%20practical%20application%20and%20deployment.%20It%20can%20be%20deployed%20independently%20as%20an%20encoder-free%20method%2C%20or%20combined%20with%20encoder%20compression%20approaches%20like%20VisionZip%20for%20use%20as%20an%20encoder-involved%20method.%20On%20LLaVA-Next-7B%2C%20PIO-FVLM%20retains%20just%2011.1%25%20of%20visual%20tokens%20but%20maintains%2097.2%25%20of%20the%20original%20performance%2C%20with%20a%202.67%24%5Ctimes%24%20prefill%20speedup%2C%202.11%24%5Ctimes%24%20inference%20speedup%2C%206.22%24%5Ctimes%24%20lower%20FLOPs%2C%20and%206.05%24%5Ctimes%24%20reduced%20KV%20Cache%20overhead.%20Our%20code%20is%20available%20at%20https%3A//github.com/ocy1/PIO-FVLM.&entry.1838667208=http%3A//arxiv.org/abs/2602.04657v1&entry.124074799=Read"},
{"title": "Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition", "author": "Dhyey Manish Rajani and Michael Milford and Tobias Fischer", "abstract": "Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.", "link": "http://arxiv.org/abs/2602.04401v1", "date": "2026-02-04", "relevancy": 2.6484, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5576}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantile%20Transfer%20for%20Reliable%20Operating%20Point%20Selection%20in%20Visual%20Place%20Recognition&body=Title%3A%20Quantile%20Transfer%20for%20Reliable%20Operating%20Point%20Selection%20in%20Visual%20Place%20Recognition%0AAuthor%3A%20Dhyey%20Manish%20Rajani%20and%20Michael%20Milford%20and%20Tobias%20Fischer%0AAbstract%3A%20Visual%20Place%20Recognition%20%28VPR%29%20is%20a%20key%20component%20for%20localisation%20in%20GNSS-denied%20environments%2C%20but%20its%20performance%20critically%20depends%20on%20selecting%20an%20image%20matching%20threshold%20%28operating%20point%29%20that%20balances%20precision%20and%20recall.%20Thresholds%20are%20typically%20hand-tuned%20offline%20for%20a%20specific%20environment%20and%20fixed%20during%20deployment%2C%20leading%20to%20degraded%20performance%20under%20environmental%20change.%20We%20propose%20a%20method%20that%2C%20given%20a%20user-defined%20precision%20requirement%2C%20automatically%20selects%20the%20operating%20point%20of%20a%20VPR%20system%20to%20maximise%20recall.%20The%20method%20uses%20a%20small%20calibration%20traversal%20with%20known%20correspondences%20and%20transfers%20thresholds%20to%20deployment%20via%20quantile%20normalisation%20of%20similarity%20score%20distributions.%20This%20quantile%20transfer%20ensures%20that%20thresholds%20remain%20stable%20across%20calibration%20sizes%20and%20query%20subsets%2C%20making%20the%20method%20robust%20to%20sampling%20variability.%20Experiments%20with%20multiple%20state-of-the-art%20VPR%20techniques%20and%20datasets%20show%20that%20the%20proposed%20approach%20consistently%20outperforms%20the%20state-of-the-art%2C%20delivering%20up%20to%2025%25%20higher%20recall%20in%20high-precision%20operating%20regimes.%20The%20method%20eliminates%20manual%20tuning%20by%20adapting%20to%20new%20environments%20and%20generalising%20across%20operating%20conditions.%20Our%20code%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantile%2520Transfer%2520for%2520Reliable%2520Operating%2520Point%2520Selection%2520in%2520Visual%2520Place%2520Recognition%26entry.906535625%3DDhyey%2520Manish%2520Rajani%2520and%2520Michael%2520Milford%2520and%2520Tobias%2520Fischer%26entry.1292438233%3DVisual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520a%2520key%2520component%2520for%2520localisation%2520in%2520GNSS-denied%2520environments%252C%2520but%2520its%2520performance%2520critically%2520depends%2520on%2520selecting%2520an%2520image%2520matching%2520threshold%2520%2528operating%2520point%2529%2520that%2520balances%2520precision%2520and%2520recall.%2520Thresholds%2520are%2520typically%2520hand-tuned%2520offline%2520for%2520a%2520specific%2520environment%2520and%2520fixed%2520during%2520deployment%252C%2520leading%2520to%2520degraded%2520performance%2520under%2520environmental%2520change.%2520We%2520propose%2520a%2520method%2520that%252C%2520given%2520a%2520user-defined%2520precision%2520requirement%252C%2520automatically%2520selects%2520the%2520operating%2520point%2520of%2520a%2520VPR%2520system%2520to%2520maximise%2520recall.%2520The%2520method%2520uses%2520a%2520small%2520calibration%2520traversal%2520with%2520known%2520correspondences%2520and%2520transfers%2520thresholds%2520to%2520deployment%2520via%2520quantile%2520normalisation%2520of%2520similarity%2520score%2520distributions.%2520This%2520quantile%2520transfer%2520ensures%2520that%2520thresholds%2520remain%2520stable%2520across%2520calibration%2520sizes%2520and%2520query%2520subsets%252C%2520making%2520the%2520method%2520robust%2520to%2520sampling%2520variability.%2520Experiments%2520with%2520multiple%2520state-of-the-art%2520VPR%2520techniques%2520and%2520datasets%2520show%2520that%2520the%2520proposed%2520approach%2520consistently%2520outperforms%2520the%2520state-of-the-art%252C%2520delivering%2520up%2520to%252025%2525%2520higher%2520recall%2520in%2520high-precision%2520operating%2520regimes.%2520The%2520method%2520eliminates%2520manual%2520tuning%2520by%2520adapting%2520to%2520new%2520environments%2520and%2520generalising%2520across%2520operating%2520conditions.%2520Our%2520code%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantile%20Transfer%20for%20Reliable%20Operating%20Point%20Selection%20in%20Visual%20Place%20Recognition&entry.906535625=Dhyey%20Manish%20Rajani%20and%20Michael%20Milford%20and%20Tobias%20Fischer&entry.1292438233=Visual%20Place%20Recognition%20%28VPR%29%20is%20a%20key%20component%20for%20localisation%20in%20GNSS-denied%20environments%2C%20but%20its%20performance%20critically%20depends%20on%20selecting%20an%20image%20matching%20threshold%20%28operating%20point%29%20that%20balances%20precision%20and%20recall.%20Thresholds%20are%20typically%20hand-tuned%20offline%20for%20a%20specific%20environment%20and%20fixed%20during%20deployment%2C%20leading%20to%20degraded%20performance%20under%20environmental%20change.%20We%20propose%20a%20method%20that%2C%20given%20a%20user-defined%20precision%20requirement%2C%20automatically%20selects%20the%20operating%20point%20of%20a%20VPR%20system%20to%20maximise%20recall.%20The%20method%20uses%20a%20small%20calibration%20traversal%20with%20known%20correspondences%20and%20transfers%20thresholds%20to%20deployment%20via%20quantile%20normalisation%20of%20similarity%20score%20distributions.%20This%20quantile%20transfer%20ensures%20that%20thresholds%20remain%20stable%20across%20calibration%20sizes%20and%20query%20subsets%2C%20making%20the%20method%20robust%20to%20sampling%20variability.%20Experiments%20with%20multiple%20state-of-the-art%20VPR%20techniques%20and%20datasets%20show%20that%20the%20proposed%20approach%20consistently%20outperforms%20the%20state-of-the-art%2C%20delivering%20up%20to%2025%25%20higher%20recall%20in%20high-precision%20operating%20regimes.%20The%20method%20eliminates%20manual%20tuning%20by%20adapting%20to%20new%20environments%20and%20generalising%20across%20operating%20conditions.%20Our%20code%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2602.04401v1&entry.124074799=Read"},
{"title": "ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry", "author": "Marcin Mo\u017cejko and Dawid Uchal and Krzysztof Gogolewski and Piotr Kupidura and Szymon \u0141ukasik and Jakub Giezga\u0142a and Tomasz Noco\u0144 and Kacper Pietrzyk and Robert Pieniuta and Mateusz Sulimowicz and Michal Orzy\u0142owski and Tomasz Si\u0142kowski and Karol Zagr\u00f3dka and Eike Staub and Ewa Szczurek", "abstract": "We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.", "link": "http://arxiv.org/abs/2602.04585v1", "date": "2026-02-04", "relevancy": 2.6446, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.538}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImmuVis%3A%20Hyperconvolutional%20Foundation%20Model%20for%20Imaging%20Mass%20Cytometry&body=Title%3A%20ImmuVis%3A%20Hyperconvolutional%20Foundation%20Model%20for%20Imaging%20Mass%20Cytometry%0AAuthor%3A%20Marcin%20Mo%C5%BCejko%20and%20Dawid%20Uchal%20and%20Krzysztof%20Gogolewski%20and%20Piotr%20Kupidura%20and%20Szymon%20%C5%81ukasik%20and%20Jakub%20Giezga%C5%82a%20and%20Tomasz%20Noco%C5%84%20and%20Kacper%20Pietrzyk%20and%20Robert%20Pieniuta%20and%20Mateusz%20Sulimowicz%20and%20Michal%20Orzy%C5%82owski%20and%20Tomasz%20Si%C5%82kowski%20and%20Karol%20Zagr%C3%B3dka%20and%20Eike%20Staub%20and%20Ewa%20Szczurek%0AAbstract%3A%20We%20present%20ImmuVis%2C%20an%20efficient%20convolutional%20foundation%20model%20for%20imaging%20mass%20cytometry%20%28IMC%29%2C%20a%20high-throughput%20multiplex%20imaging%20technology%20that%20handles%20molecular%20marker%20measurements%20as%20image%20channels%20and%20enables%20large-scale%20spatial%20tissue%20profiling.%20Unlike%20natural%20images%2C%20multiplex%20imaging%20lacks%20a%20fixed%20channel%20space%2C%20as%20real-world%20marker%20sets%20vary%20across%20studies%2C%20violating%20a%20core%20assumption%20of%20standard%20vision%20backbones.%20To%20address%20this%2C%20ImmuVis%20introduces%20marker-adaptive%20hyperconvolutions%20that%20generate%20convolutional%20kernels%20from%20learned%20marker%20embeddings%2C%20enabling%20a%20single%20model%20to%20operate%20on%20arbitrary%20measured%20marker%20subsets%20without%20retraining.%20We%20pretrain%20ImmuVis%20on%20the%20largest%20to-date%20dataset%2C%20IMC17M%20%2828%20cohorts%2C%2024%2C405%20images%2C%20265%20markers%2C%20over%2017M%20patches%29%2C%20using%20self-supervised%20masked%20reconstruction.%20ImmuVis%20outperforms%20SOTA%20baselines%20and%20ablations%20in%20virtual%20staining%20and%20downstream%20classification%20tasks%20at%20substantially%20lower%20compute%20cost%20than%20transformer-based%20alternatives%2C%20and%20is%20the%20sole%20model%20that%20provides%20calibrated%20uncertainty%20via%20a%20heteroscedastic%20likelihood%20objective.%20These%20results%20position%20ImmuVis%20as%20a%20practical%2C%20efficient%20foundation%20model%20for%20real-world%20IMC%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImmuVis%253A%2520Hyperconvolutional%2520Foundation%2520Model%2520for%2520Imaging%2520Mass%2520Cytometry%26entry.906535625%3DMarcin%2520Mo%25C5%25BCejko%2520and%2520Dawid%2520Uchal%2520and%2520Krzysztof%2520Gogolewski%2520and%2520Piotr%2520Kupidura%2520and%2520Szymon%2520%25C5%2581ukasik%2520and%2520Jakub%2520Giezga%25C5%2582a%2520and%2520Tomasz%2520Noco%25C5%2584%2520and%2520Kacper%2520Pietrzyk%2520and%2520Robert%2520Pieniuta%2520and%2520Mateusz%2520Sulimowicz%2520and%2520Michal%2520Orzy%25C5%2582owski%2520and%2520Tomasz%2520Si%25C5%2582kowski%2520and%2520Karol%2520Zagr%25C3%25B3dka%2520and%2520Eike%2520Staub%2520and%2520Ewa%2520Szczurek%26entry.1292438233%3DWe%2520present%2520ImmuVis%252C%2520an%2520efficient%2520convolutional%2520foundation%2520model%2520for%2520imaging%2520mass%2520cytometry%2520%2528IMC%2529%252C%2520a%2520high-throughput%2520multiplex%2520imaging%2520technology%2520that%2520handles%2520molecular%2520marker%2520measurements%2520as%2520image%2520channels%2520and%2520enables%2520large-scale%2520spatial%2520tissue%2520profiling.%2520Unlike%2520natural%2520images%252C%2520multiplex%2520imaging%2520lacks%2520a%2520fixed%2520channel%2520space%252C%2520as%2520real-world%2520marker%2520sets%2520vary%2520across%2520studies%252C%2520violating%2520a%2520core%2520assumption%2520of%2520standard%2520vision%2520backbones.%2520To%2520address%2520this%252C%2520ImmuVis%2520introduces%2520marker-adaptive%2520hyperconvolutions%2520that%2520generate%2520convolutional%2520kernels%2520from%2520learned%2520marker%2520embeddings%252C%2520enabling%2520a%2520single%2520model%2520to%2520operate%2520on%2520arbitrary%2520measured%2520marker%2520subsets%2520without%2520retraining.%2520We%2520pretrain%2520ImmuVis%2520on%2520the%2520largest%2520to-date%2520dataset%252C%2520IMC17M%2520%252828%2520cohorts%252C%252024%252C405%2520images%252C%2520265%2520markers%252C%2520over%252017M%2520patches%2529%252C%2520using%2520self-supervised%2520masked%2520reconstruction.%2520ImmuVis%2520outperforms%2520SOTA%2520baselines%2520and%2520ablations%2520in%2520virtual%2520staining%2520and%2520downstream%2520classification%2520tasks%2520at%2520substantially%2520lower%2520compute%2520cost%2520than%2520transformer-based%2520alternatives%252C%2520and%2520is%2520the%2520sole%2520model%2520that%2520provides%2520calibrated%2520uncertainty%2520via%2520a%2520heteroscedastic%2520likelihood%2520objective.%2520These%2520results%2520position%2520ImmuVis%2520as%2520a%2520practical%252C%2520efficient%2520foundation%2520model%2520for%2520real-world%2520IMC%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImmuVis%3A%20Hyperconvolutional%20Foundation%20Model%20for%20Imaging%20Mass%20Cytometry&entry.906535625=Marcin%20Mo%C5%BCejko%20and%20Dawid%20Uchal%20and%20Krzysztof%20Gogolewski%20and%20Piotr%20Kupidura%20and%20Szymon%20%C5%81ukasik%20and%20Jakub%20Giezga%C5%82a%20and%20Tomasz%20Noco%C5%84%20and%20Kacper%20Pietrzyk%20and%20Robert%20Pieniuta%20and%20Mateusz%20Sulimowicz%20and%20Michal%20Orzy%C5%82owski%20and%20Tomasz%20Si%C5%82kowski%20and%20Karol%20Zagr%C3%B3dka%20and%20Eike%20Staub%20and%20Ewa%20Szczurek&entry.1292438233=We%20present%20ImmuVis%2C%20an%20efficient%20convolutional%20foundation%20model%20for%20imaging%20mass%20cytometry%20%28IMC%29%2C%20a%20high-throughput%20multiplex%20imaging%20technology%20that%20handles%20molecular%20marker%20measurements%20as%20image%20channels%20and%20enables%20large-scale%20spatial%20tissue%20profiling.%20Unlike%20natural%20images%2C%20multiplex%20imaging%20lacks%20a%20fixed%20channel%20space%2C%20as%20real-world%20marker%20sets%20vary%20across%20studies%2C%20violating%20a%20core%20assumption%20of%20standard%20vision%20backbones.%20To%20address%20this%2C%20ImmuVis%20introduces%20marker-adaptive%20hyperconvolutions%20that%20generate%20convolutional%20kernels%20from%20learned%20marker%20embeddings%2C%20enabling%20a%20single%20model%20to%20operate%20on%20arbitrary%20measured%20marker%20subsets%20without%20retraining.%20We%20pretrain%20ImmuVis%20on%20the%20largest%20to-date%20dataset%2C%20IMC17M%20%2828%20cohorts%2C%2024%2C405%20images%2C%20265%20markers%2C%20over%2017M%20patches%29%2C%20using%20self-supervised%20masked%20reconstruction.%20ImmuVis%20outperforms%20SOTA%20baselines%20and%20ablations%20in%20virtual%20staining%20and%20downstream%20classification%20tasks%20at%20substantially%20lower%20compute%20cost%20than%20transformer-based%20alternatives%2C%20and%20is%20the%20sole%20model%20that%20provides%20calibrated%20uncertainty%20via%20a%20heteroscedastic%20likelihood%20objective.%20These%20results%20position%20ImmuVis%20as%20a%20practical%2C%20efficient%20foundation%20model%20for%20real-world%20IMC%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2602.04585v1&entry.124074799=Read"},
{"title": "GRAM: Spatial general-purpose audio representation models for real-world applications", "author": "Goksenin Yuksel and Marcel van Gerven and Kiki van der Heijden", "abstract": "Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverberation and noise is limited. Furthermore, most audio foundation models ignore the spatial dimension of real-world acoustic environments, ruling out tasks involving sound localization. To address these limitations, we propose GRAM: a general-purpose real-world audio model that employs a multi-channel masked autoencoder to efficiently learn spatial audio representations. We evaluated GRAM and other audio foundation models in a standardized manner on high-quality simulations of naturalistic, spatial acoustic environments as well as recordings of real-world environments and release these two complementary benchmark task suites: NatHEAR and RealSELD. Our results demonstrate that GRAM outperforms all state-of-the-art self-supervised audio foundation models on NatHEAR and the clean, single-channel version HEAR, while using only a fraction of the training data. GRAM also shows state-of-the-art localization performance in simulated environments and generalizes efficiently to real-world recordings in RealSELD. Taken together, GRAM presents a significant advance toward robust spatial audio foundation models for real-world environments.", "link": "http://arxiv.org/abs/2506.00934v5", "date": "2026-02-04", "relevancy": 2.6348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAM%3A%20Spatial%20general-purpose%20audio%20representation%20models%20for%20real-world%20applications&body=Title%3A%20GRAM%3A%20Spatial%20general-purpose%20audio%20representation%20models%20for%20real-world%20applications%0AAuthor%3A%20Goksenin%20Yuksel%20and%20Marcel%20van%20Gerven%20and%20Kiki%20van%20der%20Heijden%0AAbstract%3A%20Audio%20foundation%20models%20learn%20general-purpose%20audio%20representations%20that%20facilitate%20a%20wide%20range%20of%20downstream%20tasks.%20While%20the%20performance%20of%20these%20models%20has%20greatly%20increased%20for%20conventional%20single-channel%2C%20dry%20audio%20clips%2C%20their%20success%20in%20real-world%20acoustic%20environments%20with%20reverberation%20and%20noise%20is%20limited.%20Furthermore%2C%20most%20audio%20foundation%20models%20ignore%20the%20spatial%20dimension%20of%20real-world%20acoustic%20environments%2C%20ruling%20out%20tasks%20involving%20sound%20localization.%20To%20address%20these%20limitations%2C%20we%20propose%20GRAM%3A%20a%20general-purpose%20real-world%20audio%20model%20that%20employs%20a%20multi-channel%20masked%20autoencoder%20to%20efficiently%20learn%20spatial%20audio%20representations.%20We%20evaluated%20GRAM%20and%20other%20audio%20foundation%20models%20in%20a%20standardized%20manner%20on%20high-quality%20simulations%20of%20naturalistic%2C%20spatial%20acoustic%20environments%20as%20well%20as%20recordings%20of%20real-world%20environments%20and%20release%20these%20two%20complementary%20benchmark%20task%20suites%3A%20NatHEAR%20and%20RealSELD.%20Our%20results%20demonstrate%20that%20GRAM%20outperforms%20all%20state-of-the-art%20self-supervised%20audio%20foundation%20models%20on%20NatHEAR%20and%20the%20clean%2C%20single-channel%20version%20HEAR%2C%20while%20using%20only%20a%20fraction%20of%20the%20training%20data.%20GRAM%20also%20shows%20state-of-the-art%20localization%20performance%20in%20simulated%20environments%20and%20generalizes%20efficiently%20to%20real-world%20recordings%20in%20RealSELD.%20Taken%20together%2C%20GRAM%20presents%20a%20significant%20advance%20toward%20robust%20spatial%20audio%20foundation%20models%20for%20real-world%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2506.00934v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAM%253A%2520Spatial%2520general-purpose%2520audio%2520representation%2520models%2520for%2520real-world%2520applications%26entry.906535625%3DGoksenin%2520Yuksel%2520and%2520Marcel%2520van%2520Gerven%2520and%2520Kiki%2520van%2520der%2520Heijden%26entry.1292438233%3DAudio%2520foundation%2520models%2520learn%2520general-purpose%2520audio%2520representations%2520that%2520facilitate%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%2520While%2520the%2520performance%2520of%2520these%2520models%2520has%2520greatly%2520increased%2520for%2520conventional%2520single-channel%252C%2520dry%2520audio%2520clips%252C%2520their%2520success%2520in%2520real-world%2520acoustic%2520environments%2520with%2520reverberation%2520and%2520noise%2520is%2520limited.%2520Furthermore%252C%2520most%2520audio%2520foundation%2520models%2520ignore%2520the%2520spatial%2520dimension%2520of%2520real-world%2520acoustic%2520environments%252C%2520ruling%2520out%2520tasks%2520involving%2520sound%2520localization.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520GRAM%253A%2520a%2520general-purpose%2520real-world%2520audio%2520model%2520that%2520employs%2520a%2520multi-channel%2520masked%2520autoencoder%2520to%2520efficiently%2520learn%2520spatial%2520audio%2520representations.%2520We%2520evaluated%2520GRAM%2520and%2520other%2520audio%2520foundation%2520models%2520in%2520a%2520standardized%2520manner%2520on%2520high-quality%2520simulations%2520of%2520naturalistic%252C%2520spatial%2520acoustic%2520environments%2520as%2520well%2520as%2520recordings%2520of%2520real-world%2520environments%2520and%2520release%2520these%2520two%2520complementary%2520benchmark%2520task%2520suites%253A%2520NatHEAR%2520and%2520RealSELD.%2520Our%2520results%2520demonstrate%2520that%2520GRAM%2520outperforms%2520all%2520state-of-the-art%2520self-supervised%2520audio%2520foundation%2520models%2520on%2520NatHEAR%2520and%2520the%2520clean%252C%2520single-channel%2520version%2520HEAR%252C%2520while%2520using%2520only%2520a%2520fraction%2520of%2520the%2520training%2520data.%2520GRAM%2520also%2520shows%2520state-of-the-art%2520localization%2520performance%2520in%2520simulated%2520environments%2520and%2520generalizes%2520efficiently%2520to%2520real-world%2520recordings%2520in%2520RealSELD.%2520Taken%2520together%252C%2520GRAM%2520presents%2520a%2520significant%2520advance%2520toward%2520robust%2520spatial%2520audio%2520foundation%2520models%2520for%2520real-world%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00934v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAM%3A%20Spatial%20general-purpose%20audio%20representation%20models%20for%20real-world%20applications&entry.906535625=Goksenin%20Yuksel%20and%20Marcel%20van%20Gerven%20and%20Kiki%20van%20der%20Heijden&entry.1292438233=Audio%20foundation%20models%20learn%20general-purpose%20audio%20representations%20that%20facilitate%20a%20wide%20range%20of%20downstream%20tasks.%20While%20the%20performance%20of%20these%20models%20has%20greatly%20increased%20for%20conventional%20single-channel%2C%20dry%20audio%20clips%2C%20their%20success%20in%20real-world%20acoustic%20environments%20with%20reverberation%20and%20noise%20is%20limited.%20Furthermore%2C%20most%20audio%20foundation%20models%20ignore%20the%20spatial%20dimension%20of%20real-world%20acoustic%20environments%2C%20ruling%20out%20tasks%20involving%20sound%20localization.%20To%20address%20these%20limitations%2C%20we%20propose%20GRAM%3A%20a%20general-purpose%20real-world%20audio%20model%20that%20employs%20a%20multi-channel%20masked%20autoencoder%20to%20efficiently%20learn%20spatial%20audio%20representations.%20We%20evaluated%20GRAM%20and%20other%20audio%20foundation%20models%20in%20a%20standardized%20manner%20on%20high-quality%20simulations%20of%20naturalistic%2C%20spatial%20acoustic%20environments%20as%20well%20as%20recordings%20of%20real-world%20environments%20and%20release%20these%20two%20complementary%20benchmark%20task%20suites%3A%20NatHEAR%20and%20RealSELD.%20Our%20results%20demonstrate%20that%20GRAM%20outperforms%20all%20state-of-the-art%20self-supervised%20audio%20foundation%20models%20on%20NatHEAR%20and%20the%20clean%2C%20single-channel%20version%20HEAR%2C%20while%20using%20only%20a%20fraction%20of%20the%20training%20data.%20GRAM%20also%20shows%20state-of-the-art%20localization%20performance%20in%20simulated%20environments%20and%20generalizes%20efficiently%20to%20real-world%20recordings%20in%20RealSELD.%20Taken%20together%2C%20GRAM%20presents%20a%20significant%20advance%20toward%20robust%20spatial%20audio%20foundation%20models%20for%20real-world%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2506.00934v5&entry.124074799=Read"},
{"title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression", "author": "Peijun Zhu and Ning Yang and Baoliang Tian and Jiayu Wei and Weihao Zhang and Haijun Zhang and Pin Lv", "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs. Code is available at https://github.com/szdtzpj/Breaking_the_moe_trilemma", "link": "http://arxiv.org/abs/2510.02345v2", "date": "2026-02-04", "relevancy": 2.6316, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20MoE%20LLM%20Trilemma%3A%20Dynamic%20Expert%20Clustering%20with%20Structured%20Compression&body=Title%3A%20Breaking%20the%20MoE%20LLM%20Trilemma%3A%20Dynamic%20Expert%20Clustering%20with%20Structured%20Compression%0AAuthor%3A%20Peijun%20Zhu%20and%20Ning%20Yang%20and%20Baoliang%20Tian%20and%20Jiayu%20Wei%20and%20Weihao%20Zhang%20and%20Haijun%20Zhang%20and%20Pin%20Lv%0AAbstract%3A%20Mixture-of-Experts%20%28MoE%29%20Large%20Language%20Models%20%28LLMs%29%20face%20a%20trilemma%20of%20load%20imbalance%2C%20parameter%20redundancy%2C%20and%20communication%20overhead.%20We%20introduce%20a%20unified%20framework%20based%20on%20dynamic%20expert%20clustering%20and%20structured%20compression%20to%20address%20these%20issues%20cohesively.%20Our%20method%20employs%20an%20online%20clustering%20procedure%20that%20periodically%20regroups%20experts%20using%20a%20fused%20metric%20of%20parameter%20and%20activation%20similarity%2C%20which%20stabilizes%20expert%20utilization.%20To%20our%20knowledge%2C%20this%20is%20one%20of%20the%20first%20frameworks%20to%20leverage%20the%20semantic%20embedding%20capability%20of%20the%20router%20to%20dynamically%20reconfigure%20the%20model%27s%20architecture%20during%20training%20for%20substantial%20efficiency%20gains.%20Within%20each%20cluster%2C%20we%20decompose%20expert%20weights%20into%20a%20shared%20base%20matrix%20and%20extremely%20low-rank%20residual%20adapters%2C%20achieving%20up%20to%20fivefold%20parameter%20reduction%20per%20group%20while%20preserving%20specialization.%20This%20structure%20enables%20a%20two-stage%20hierarchical%20routing%20strategy%3A%20tokens%20are%20first%20assigned%20to%20a%20cluster%2C%20then%20to%20specific%20experts%20within%20it%2C%20drastically%20reducing%20the%20routing%20search%20space%20and%20the%20volume%20of%20all-to-all%20communication.%20Furthermore%2C%20a%20heterogeneous%20precision%20scheme%2C%20which%20stores%20shared%20bases%20in%20FP16%20and%20residual%20factors%20in%20INT4%2C%20coupled%20with%20dynamic%20offloading%20of%20inactive%20clusters%2C%20reduces%20peak%20memory%20consumption%20to%20levels%20comparable%20to%20dense%20models.%20Evaluated%20on%20GLUE%20and%20WikiText-103%2C%20our%20framework%20matches%20the%20quality%20of%20standard%20MoE%20models%20while%20reducing%20total%20parameters%20by%20approximately%2080%25%2C%20improving%20throughput%20by%2010%25%20to%2020%25%2C%20and%20lowering%20expert%20load%20variance%20by%20a%20factor%20of%20over%20three.%20Our%20work%20demonstrates%20that%20structural%20reorganization%20is%20a%20principled%20path%20toward%20scalable%2C%20efficient%2C%20and%20memory-effective%20MoE%20LLMs.%20Code%20is%20available%20at%20https%3A//github.com/szdtzpj/Breaking_the_moe_trilemma%0ALink%3A%20http%3A//arxiv.org/abs/2510.02345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520MoE%2520LLM%2520Trilemma%253A%2520Dynamic%2520Expert%2520Clustering%2520with%2520Structured%2520Compression%26entry.906535625%3DPeijun%2520Zhu%2520and%2520Ning%2520Yang%2520and%2520Baoliang%2520Tian%2520and%2520Jiayu%2520Wei%2520and%2520Weihao%2520Zhang%2520and%2520Haijun%2520Zhang%2520and%2520Pin%2520Lv%26entry.1292438233%3DMixture-of-Experts%2520%2528MoE%2529%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520a%2520trilemma%2520of%2520load%2520imbalance%252C%2520parameter%2520redundancy%252C%2520and%2520communication%2520overhead.%2520We%2520introduce%2520a%2520unified%2520framework%2520based%2520on%2520dynamic%2520expert%2520clustering%2520and%2520structured%2520compression%2520to%2520address%2520these%2520issues%2520cohesively.%2520Our%2520method%2520employs%2520an%2520online%2520clustering%2520procedure%2520that%2520periodically%2520regroups%2520experts%2520using%2520a%2520fused%2520metric%2520of%2520parameter%2520and%2520activation%2520similarity%252C%2520which%2520stabilizes%2520expert%2520utilization.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520one%2520of%2520the%2520first%2520frameworks%2520to%2520leverage%2520the%2520semantic%2520embedding%2520capability%2520of%2520the%2520router%2520to%2520dynamically%2520reconfigure%2520the%2520model%2527s%2520architecture%2520during%2520training%2520for%2520substantial%2520efficiency%2520gains.%2520Within%2520each%2520cluster%252C%2520we%2520decompose%2520expert%2520weights%2520into%2520a%2520shared%2520base%2520matrix%2520and%2520extremely%2520low-rank%2520residual%2520adapters%252C%2520achieving%2520up%2520to%2520fivefold%2520parameter%2520reduction%2520per%2520group%2520while%2520preserving%2520specialization.%2520This%2520structure%2520enables%2520a%2520two-stage%2520hierarchical%2520routing%2520strategy%253A%2520tokens%2520are%2520first%2520assigned%2520to%2520a%2520cluster%252C%2520then%2520to%2520specific%2520experts%2520within%2520it%252C%2520drastically%2520reducing%2520the%2520routing%2520search%2520space%2520and%2520the%2520volume%2520of%2520all-to-all%2520communication.%2520Furthermore%252C%2520a%2520heterogeneous%2520precision%2520scheme%252C%2520which%2520stores%2520shared%2520bases%2520in%2520FP16%2520and%2520residual%2520factors%2520in%2520INT4%252C%2520coupled%2520with%2520dynamic%2520offloading%2520of%2520inactive%2520clusters%252C%2520reduces%2520peak%2520memory%2520consumption%2520to%2520levels%2520comparable%2520to%2520dense%2520models.%2520Evaluated%2520on%2520GLUE%2520and%2520WikiText-103%252C%2520our%2520framework%2520matches%2520the%2520quality%2520of%2520standard%2520MoE%2520models%2520while%2520reducing%2520total%2520parameters%2520by%2520approximately%252080%2525%252C%2520improving%2520throughput%2520by%252010%2525%2520to%252020%2525%252C%2520and%2520lowering%2520expert%2520load%2520variance%2520by%2520a%2520factor%2520of%2520over%2520three.%2520Our%2520work%2520demonstrates%2520that%2520structural%2520reorganization%2520is%2520a%2520principled%2520path%2520toward%2520scalable%252C%2520efficient%252C%2520and%2520memory-effective%2520MoE%2520LLMs.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/szdtzpj/Breaking_the_moe_trilemma%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20MoE%20LLM%20Trilemma%3A%20Dynamic%20Expert%20Clustering%20with%20Structured%20Compression&entry.906535625=Peijun%20Zhu%20and%20Ning%20Yang%20and%20Baoliang%20Tian%20and%20Jiayu%20Wei%20and%20Weihao%20Zhang%20and%20Haijun%20Zhang%20and%20Pin%20Lv&entry.1292438233=Mixture-of-Experts%20%28MoE%29%20Large%20Language%20Models%20%28LLMs%29%20face%20a%20trilemma%20of%20load%20imbalance%2C%20parameter%20redundancy%2C%20and%20communication%20overhead.%20We%20introduce%20a%20unified%20framework%20based%20on%20dynamic%20expert%20clustering%20and%20structured%20compression%20to%20address%20these%20issues%20cohesively.%20Our%20method%20employs%20an%20online%20clustering%20procedure%20that%20periodically%20regroups%20experts%20using%20a%20fused%20metric%20of%20parameter%20and%20activation%20similarity%2C%20which%20stabilizes%20expert%20utilization.%20To%20our%20knowledge%2C%20this%20is%20one%20of%20the%20first%20frameworks%20to%20leverage%20the%20semantic%20embedding%20capability%20of%20the%20router%20to%20dynamically%20reconfigure%20the%20model%27s%20architecture%20during%20training%20for%20substantial%20efficiency%20gains.%20Within%20each%20cluster%2C%20we%20decompose%20expert%20weights%20into%20a%20shared%20base%20matrix%20and%20extremely%20low-rank%20residual%20adapters%2C%20achieving%20up%20to%20fivefold%20parameter%20reduction%20per%20group%20while%20preserving%20specialization.%20This%20structure%20enables%20a%20two-stage%20hierarchical%20routing%20strategy%3A%20tokens%20are%20first%20assigned%20to%20a%20cluster%2C%20then%20to%20specific%20experts%20within%20it%2C%20drastically%20reducing%20the%20routing%20search%20space%20and%20the%20volume%20of%20all-to-all%20communication.%20Furthermore%2C%20a%20heterogeneous%20precision%20scheme%2C%20which%20stores%20shared%20bases%20in%20FP16%20and%20residual%20factors%20in%20INT4%2C%20coupled%20with%20dynamic%20offloading%20of%20inactive%20clusters%2C%20reduces%20peak%20memory%20consumption%20to%20levels%20comparable%20to%20dense%20models.%20Evaluated%20on%20GLUE%20and%20WikiText-103%2C%20our%20framework%20matches%20the%20quality%20of%20standard%20MoE%20models%20while%20reducing%20total%20parameters%20by%20approximately%2080%25%2C%20improving%20throughput%20by%2010%25%20to%2020%25%2C%20and%20lowering%20expert%20load%20variance%20by%20a%20factor%20of%20over%20three.%20Our%20work%20demonstrates%20that%20structural%20reorganization%20is%20a%20principled%20path%20toward%20scalable%2C%20efficient%2C%20and%20memory-effective%20MoE%20LLMs.%20Code%20is%20available%20at%20https%3A//github.com/szdtzpj/Breaking_the_moe_trilemma&entry.1838667208=http%3A//arxiv.org/abs/2510.02345v2&entry.124074799=Read"},
{"title": "Reinforced Attention Learning", "author": "Bangzheng Li and Jianmo Ni and Chen Qu and Ian Miao and Liu Yang and Xingyu Fu and Muhao Chen and Derek Zhiyuan Cheng", "abstract": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\n  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.", "link": "http://arxiv.org/abs/2602.04884v1", "date": "2026-02-04", "relevancy": 2.6132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.522}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20Attention%20Learning&body=Title%3A%20Reinforced%20Attention%20Learning%0AAuthor%3A%20Bangzheng%20Li%20and%20Jianmo%20Ni%20and%20Chen%20Qu%20and%20Ian%20Miao%20and%20Liu%20Yang%20and%20Xingyu%20Fu%20and%20Muhao%20Chen%20and%20Derek%20Zhiyuan%20Cheng%0AAbstract%3A%20Post-training%20with%20Reinforcement%20Learning%20%28RL%29%20has%20substantially%20improved%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20via%20test-time%20scaling.%20However%2C%20extending%20this%20paradigm%20to%20Multimodal%20LLMs%20%28MLLMs%29%20through%20verbose%20rationales%20yields%20limited%20gains%20for%20perception%20and%20can%20even%20degrade%20performance.%0A%20%20We%20propose%20Reinforced%20Attention%20Learning%20%28RAL%29%2C%20a%20policy-gradient%20framework%20that%20directly%20optimizes%20internal%20attention%20distributions%20rather%20than%20output%20token%20sequences.%20By%20shifting%20optimization%20from%20what%20to%20generate%20to%20where%20to%20attend%2C%20RAL%20promotes%20effective%20information%20allocation%20and%20improved%20grounding%20in%20complex%20multimodal%20inputs.%20Experiments%20across%20diverse%20image%20and%20video%20benchmarks%20show%20consistent%20gains%20over%20GRPO%20and%20other%20baselines.%20We%20further%20introduce%20On-Policy%20Attention%20Distillation%2C%20demonstrating%20that%20transferring%20latent%20attention%20behaviors%20yields%20stronger%20cross-modal%20alignment%20than%20standard%20knowledge%20distillation.%20Our%20results%20position%20attention%20policies%20as%20a%20principled%20and%20general%20alternative%20for%20multimodal%20post-training.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520Attention%2520Learning%26entry.906535625%3DBangzheng%2520Li%2520and%2520Jianmo%2520Ni%2520and%2520Chen%2520Qu%2520and%2520Ian%2520Miao%2520and%2520Liu%2520Yang%2520and%2520Xingyu%2520Fu%2520and%2520Muhao%2520Chen%2520and%2520Derek%2520Zhiyuan%2520Cheng%26entry.1292438233%3DPost-training%2520with%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520substantially%2520improved%2520reasoning%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520via%2520test-time%2520scaling.%2520However%252C%2520extending%2520this%2520paradigm%2520to%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520through%2520verbose%2520rationales%2520yields%2520limited%2520gains%2520for%2520perception%2520and%2520can%2520even%2520degrade%2520performance.%250A%2520%2520We%2520propose%2520Reinforced%2520Attention%2520Learning%2520%2528RAL%2529%252C%2520a%2520policy-gradient%2520framework%2520that%2520directly%2520optimizes%2520internal%2520attention%2520distributions%2520rather%2520than%2520output%2520token%2520sequences.%2520By%2520shifting%2520optimization%2520from%2520what%2520to%2520generate%2520to%2520where%2520to%2520attend%252C%2520RAL%2520promotes%2520effective%2520information%2520allocation%2520and%2520improved%2520grounding%2520in%2520complex%2520multimodal%2520inputs.%2520Experiments%2520across%2520diverse%2520image%2520and%2520video%2520benchmarks%2520show%2520consistent%2520gains%2520over%2520GRPO%2520and%2520other%2520baselines.%2520We%2520further%2520introduce%2520On-Policy%2520Attention%2520Distillation%252C%2520demonstrating%2520that%2520transferring%2520latent%2520attention%2520behaviors%2520yields%2520stronger%2520cross-modal%2520alignment%2520than%2520standard%2520knowledge%2520distillation.%2520Our%2520results%2520position%2520attention%2520policies%2520as%2520a%2520principled%2520and%2520general%2520alternative%2520for%2520multimodal%2520post-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20Attention%20Learning&entry.906535625=Bangzheng%20Li%20and%20Jianmo%20Ni%20and%20Chen%20Qu%20and%20Ian%20Miao%20and%20Liu%20Yang%20and%20Xingyu%20Fu%20and%20Muhao%20Chen%20and%20Derek%20Zhiyuan%20Cheng&entry.1292438233=Post-training%20with%20Reinforcement%20Learning%20%28RL%29%20has%20substantially%20improved%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20via%20test-time%20scaling.%20However%2C%20extending%20this%20paradigm%20to%20Multimodal%20LLMs%20%28MLLMs%29%20through%20verbose%20rationales%20yields%20limited%20gains%20for%20perception%20and%20can%20even%20degrade%20performance.%0A%20%20We%20propose%20Reinforced%20Attention%20Learning%20%28RAL%29%2C%20a%20policy-gradient%20framework%20that%20directly%20optimizes%20internal%20attention%20distributions%20rather%20than%20output%20token%20sequences.%20By%20shifting%20optimization%20from%20what%20to%20generate%20to%20where%20to%20attend%2C%20RAL%20promotes%20effective%20information%20allocation%20and%20improved%20grounding%20in%20complex%20multimodal%20inputs.%20Experiments%20across%20diverse%20image%20and%20video%20benchmarks%20show%20consistent%20gains%20over%20GRPO%20and%20other%20baselines.%20We%20further%20introduce%20On-Policy%20Attention%20Distillation%2C%20demonstrating%20that%20transferring%20latent%20attention%20behaviors%20yields%20stronger%20cross-modal%20alignment%20than%20standard%20knowledge%20distillation.%20Our%20results%20position%20attention%20policies%20as%20a%20principled%20and%20general%20alternative%20for%20multimodal%20post-training.&entry.1838667208=http%3A//arxiv.org/abs/2602.04884v1&entry.124074799=Read"},
{"title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "author": "Yi-Chun Liao and Chieh-Lin Tsai and Yuan-Hao Chang and Cam\u00e9lia Slimani and Jalil Boukhobza and Tei-Wei Kuo", "abstract": "Although deep learning has demonstrated remarkable capability in learning from unstructured data, modern tree-based ensemble models remain superior in extracting relevant information and learning from structured datasets. While several efforts have been made to accelerate tree-based models, the inherent characteristics of the models pose significant challenges for conventional accelerators. Recent research leveraging content-addressable memory (CAM) offers a promising solution for accelerating tree-based models, yet existing designs suffer from excessive memory consumption and low utilization. This work addresses these challenges by introducing RETENTION, an end-to-end framework that significantly reduces CAM capacity requirement for tree-based model inference. We propose an iterative pruning algorithm with a novel pruning criterion tailored for bagging-based models (e.g., Random Forest), which minimizes model complexity while ensuring controlled accuracy degradation. Additionally, we present a tree mapping scheme that incorporates two innovative data placement strategies to alleviate the memory redundancy caused by the widespread use of don't care states in CAM. Experimental results show that implementing the tree mapping scheme alone reduces CAM capacity requirement by $1.46\\times$ to $21.30 \\times$, while the full RETENTION framework achieves $4.35\\times$ to $207.12\\times$ reduction with less than 3\\% accuracy loss. These results demonstrate that RETENTION is highly effective in minimizing CAM resource demand, providing a resource-efficient direction for tree-based model acceleration.", "link": "http://arxiv.org/abs/2506.05994v2", "date": "2026-02-04", "relevancy": 2.5943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%20with%20Content-Addressable%20Memory&body=Title%3A%20RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%20with%20Content-Addressable%20Memory%0AAuthor%3A%20Yi-Chun%20Liao%20and%20Chieh-Lin%20Tsai%20and%20Yuan-Hao%20Chang%20and%20Cam%C3%A9lia%20Slimani%20and%20Jalil%20Boukhobza%20and%20Tei-Wei%20Kuo%0AAbstract%3A%20Although%20deep%20learning%20has%20demonstrated%20remarkable%20capability%20in%20learning%20from%20unstructured%20data%2C%20modern%20tree-based%20ensemble%20models%20remain%20superior%20in%20extracting%20relevant%20information%20and%20learning%20from%20structured%20datasets.%20While%20several%20efforts%20have%20been%20made%20to%20accelerate%20tree-based%20models%2C%20the%20inherent%20characteristics%20of%20the%20models%20pose%20significant%20challenges%20for%20conventional%20accelerators.%20Recent%20research%20leveraging%20content-addressable%20memory%20%28CAM%29%20offers%20a%20promising%20solution%20for%20accelerating%20tree-based%20models%2C%20yet%20existing%20designs%20suffer%20from%20excessive%20memory%20consumption%20and%20low%20utilization.%20This%20work%20addresses%20these%20challenges%20by%20introducing%20RETENTION%2C%20an%20end-to-end%20framework%20that%20significantly%20reduces%20CAM%20capacity%20requirement%20for%20tree-based%20model%20inference.%20We%20propose%20an%20iterative%20pruning%20algorithm%20with%20a%20novel%20pruning%20criterion%20tailored%20for%20bagging-based%20models%20%28e.g.%2C%20Random%20Forest%29%2C%20which%20minimizes%20model%20complexity%20while%20ensuring%20controlled%20accuracy%20degradation.%20Additionally%2C%20we%20present%20a%20tree%20mapping%20scheme%20that%20incorporates%20two%20innovative%20data%20placement%20strategies%20to%20alleviate%20the%20memory%20redundancy%20caused%20by%20the%20widespread%20use%20of%20don%27t%20care%20states%20in%20CAM.%20Experimental%20results%20show%20that%20implementing%20the%20tree%20mapping%20scheme%20alone%20reduces%20CAM%20capacity%20requirement%20by%20%241.46%5Ctimes%24%20to%20%2421.30%20%5Ctimes%24%2C%20while%20the%20full%20RETENTION%20framework%20achieves%20%244.35%5Ctimes%24%20to%20%24207.12%5Ctimes%24%20reduction%20with%20less%20than%203%5C%25%20accuracy%20loss.%20These%20results%20demonstrate%20that%20RETENTION%20is%20highly%20effective%20in%20minimizing%20CAM%20resource%20demand%2C%20providing%20a%20resource-efficient%20direction%20for%20tree-based%20model%20acceleration.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRETENTION%253A%2520Resource-Efficient%2520Tree-Based%2520Ensemble%2520Model%2520Acceleration%2520with%2520Content-Addressable%2520Memory%26entry.906535625%3DYi-Chun%2520Liao%2520and%2520Chieh-Lin%2520Tsai%2520and%2520Yuan-Hao%2520Chang%2520and%2520Cam%25C3%25A9lia%2520Slimani%2520and%2520Jalil%2520Boukhobza%2520and%2520Tei-Wei%2520Kuo%26entry.1292438233%3DAlthough%2520deep%2520learning%2520has%2520demonstrated%2520remarkable%2520capability%2520in%2520learning%2520from%2520unstructured%2520data%252C%2520modern%2520tree-based%2520ensemble%2520models%2520remain%2520superior%2520in%2520extracting%2520relevant%2520information%2520and%2520learning%2520from%2520structured%2520datasets.%2520While%2520several%2520efforts%2520have%2520been%2520made%2520to%2520accelerate%2520tree-based%2520models%252C%2520the%2520inherent%2520characteristics%2520of%2520the%2520models%2520pose%2520significant%2520challenges%2520for%2520conventional%2520accelerators.%2520Recent%2520research%2520leveraging%2520content-addressable%2520memory%2520%2528CAM%2529%2520offers%2520a%2520promising%2520solution%2520for%2520accelerating%2520tree-based%2520models%252C%2520yet%2520existing%2520designs%2520suffer%2520from%2520excessive%2520memory%2520consumption%2520and%2520low%2520utilization.%2520This%2520work%2520addresses%2520these%2520challenges%2520by%2520introducing%2520RETENTION%252C%2520an%2520end-to-end%2520framework%2520that%2520significantly%2520reduces%2520CAM%2520capacity%2520requirement%2520for%2520tree-based%2520model%2520inference.%2520We%2520propose%2520an%2520iterative%2520pruning%2520algorithm%2520with%2520a%2520novel%2520pruning%2520criterion%2520tailored%2520for%2520bagging-based%2520models%2520%2528e.g.%252C%2520Random%2520Forest%2529%252C%2520which%2520minimizes%2520model%2520complexity%2520while%2520ensuring%2520controlled%2520accuracy%2520degradation.%2520Additionally%252C%2520we%2520present%2520a%2520tree%2520mapping%2520scheme%2520that%2520incorporates%2520two%2520innovative%2520data%2520placement%2520strategies%2520to%2520alleviate%2520the%2520memory%2520redundancy%2520caused%2520by%2520the%2520widespread%2520use%2520of%2520don%2527t%2520care%2520states%2520in%2520CAM.%2520Experimental%2520results%2520show%2520that%2520implementing%2520the%2520tree%2520mapping%2520scheme%2520alone%2520reduces%2520CAM%2520capacity%2520requirement%2520by%2520%25241.46%255Ctimes%2524%2520to%2520%252421.30%2520%255Ctimes%2524%252C%2520while%2520the%2520full%2520RETENTION%2520framework%2520achieves%2520%25244.35%255Ctimes%2524%2520to%2520%2524207.12%255Ctimes%2524%2520reduction%2520with%2520less%2520than%25203%255C%2525%2520accuracy%2520loss.%2520These%2520results%2520demonstrate%2520that%2520RETENTION%2520is%2520highly%2520effective%2520in%2520minimizing%2520CAM%2520resource%2520demand%252C%2520providing%2520a%2520resource-efficient%2520direction%2520for%2520tree-based%2520model%2520acceleration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RETENTION%3A%20Resource-Efficient%20Tree-Based%20Ensemble%20Model%20Acceleration%20with%20Content-Addressable%20Memory&entry.906535625=Yi-Chun%20Liao%20and%20Chieh-Lin%20Tsai%20and%20Yuan-Hao%20Chang%20and%20Cam%C3%A9lia%20Slimani%20and%20Jalil%20Boukhobza%20and%20Tei-Wei%20Kuo&entry.1292438233=Although%20deep%20learning%20has%20demonstrated%20remarkable%20capability%20in%20learning%20from%20unstructured%20data%2C%20modern%20tree-based%20ensemble%20models%20remain%20superior%20in%20extracting%20relevant%20information%20and%20learning%20from%20structured%20datasets.%20While%20several%20efforts%20have%20been%20made%20to%20accelerate%20tree-based%20models%2C%20the%20inherent%20characteristics%20of%20the%20models%20pose%20significant%20challenges%20for%20conventional%20accelerators.%20Recent%20research%20leveraging%20content-addressable%20memory%20%28CAM%29%20offers%20a%20promising%20solution%20for%20accelerating%20tree-based%20models%2C%20yet%20existing%20designs%20suffer%20from%20excessive%20memory%20consumption%20and%20low%20utilization.%20This%20work%20addresses%20these%20challenges%20by%20introducing%20RETENTION%2C%20an%20end-to-end%20framework%20that%20significantly%20reduces%20CAM%20capacity%20requirement%20for%20tree-based%20model%20inference.%20We%20propose%20an%20iterative%20pruning%20algorithm%20with%20a%20novel%20pruning%20criterion%20tailored%20for%20bagging-based%20models%20%28e.g.%2C%20Random%20Forest%29%2C%20which%20minimizes%20model%20complexity%20while%20ensuring%20controlled%20accuracy%20degradation.%20Additionally%2C%20we%20present%20a%20tree%20mapping%20scheme%20that%20incorporates%20two%20innovative%20data%20placement%20strategies%20to%20alleviate%20the%20memory%20redundancy%20caused%20by%20the%20widespread%20use%20of%20don%27t%20care%20states%20in%20CAM.%20Experimental%20results%20show%20that%20implementing%20the%20tree%20mapping%20scheme%20alone%20reduces%20CAM%20capacity%20requirement%20by%20%241.46%5Ctimes%24%20to%20%2421.30%20%5Ctimes%24%2C%20while%20the%20full%20RETENTION%20framework%20achieves%20%244.35%5Ctimes%24%20to%20%24207.12%5Ctimes%24%20reduction%20with%20less%20than%203%5C%25%20accuracy%20loss.%20These%20results%20demonstrate%20that%20RETENTION%20is%20highly%20effective%20in%20minimizing%20CAM%20resource%20demand%2C%20providing%20a%20resource-efficient%20direction%20for%20tree-based%20model%20acceleration.&entry.1838667208=http%3A//arxiv.org/abs/2506.05994v2&entry.124074799=Read"},
{"title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA", "author": "Pu Zhao and Arash Akbari and Xuan Shen and Zhenglun Kong and Yixin Shen and Sung-En Chang and Timothy Rupprecht and Lei Lu and Enfu Nan and Changdi Yang and Yumei He and Weiyan Shi and Xingchen Xu and Yu Huang and Wei Jiang and Wei Wang and Yue Chen and Yong He and Yanzhi Wang", "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.", "link": "http://arxiv.org/abs/2512.22208v2", "date": "2026-02-04", "relevancy": 2.5769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Multimodal%20Moxin%20Models%20with%20Moxin-VLM%20and%20Moxin-VLA&body=Title%3A%20Open-Source%20Multimodal%20Moxin%20Models%20with%20Moxin-VLM%20and%20Moxin-VLA%0AAuthor%3A%20Pu%20Zhao%20and%20Arash%20Akbari%20and%20Xuan%20Shen%20and%20Zhenglun%20Kong%20and%20Yixin%20Shen%20and%20Sung-En%20Chang%20and%20Timothy%20Rupprecht%20and%20Lei%20Lu%20and%20Enfu%20Nan%20and%20Changdi%20Yang%20and%20Yumei%20He%20and%20Weiyan%20Shi%20and%20Xingchen%20Xu%20and%20Yu%20Huang%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Yue%20Chen%20and%20Yong%20He%20and%20Yanzhi%20Wang%0AAbstract%3A%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20undergone%20a%20significant%20transformation%2C%20marked%20by%20a%20rapid%20rise%20in%20both%20their%20popularity%20and%20capabilities.%20Leading%20this%20evolution%20are%20proprietary%20LLMs%20like%20GPT-4%20and%20GPT-o1%2C%20which%20have%20captured%20widespread%20attention%20in%20the%20AI%20community%20due%20to%20their%20remarkable%20performance%20and%20versatility.%20Simultaneously%2C%20open-source%20LLMs%2C%20such%20as%20LLaMA%20and%20Mistral%2C%20have%20made%20great%20contributions%20to%20the%20ever-increasing%20popularity%20of%20LLMs%20due%20to%20the%20ease%20to%20customize%20and%20deploy%20the%20models%20across%20diverse%20applications.%20Moxin%207B%20is%20introduced%20as%20a%20fully%20open-source%20LLM%20developed%20in%20accordance%20with%20the%20Model%20Openness%20Framework%2C%20which%20moves%20beyond%20the%20simple%20sharing%20of%20model%20weights%20to%20embrace%20complete%20transparency%20in%20training%2C%20datasets%2C%20and%20implementation%20detail%2C%20thus%20fostering%20a%20more%20inclusive%20and%20collaborative%20research%20environment%20that%20can%20sustain%20a%20healthy%20open-source%20ecosystem.%20To%20further%20equip%20Moxin%20with%20various%20capabilities%20in%20different%20tasks%2C%20we%20develop%20three%20variants%20based%20on%20Moxin%2C%20including%20Moxin-VLM%2C%20Moxin-VLA%2C%20and%20Moxin-Chinese%2C%20which%20target%20the%20vision-language%2C%20vision-language-action%2C%20and%20Chinese%20capabilities%2C%20respectively.%20Experiments%20show%20that%20our%20models%20achieve%20superior%20performance%20in%20various%20evaluations.%20We%20adopt%20open-source%20framework%20and%20open%20data%20for%20the%20training.%20We%20release%20our%20models%2C%20along%20with%20the%20available%20data%20and%20code%20to%20derive%20these%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Source%2520Multimodal%2520Moxin%2520Models%2520with%2520Moxin-VLM%2520and%2520Moxin-VLA%26entry.906535625%3DPu%2520Zhao%2520and%2520Arash%2520Akbari%2520and%2520Xuan%2520Shen%2520and%2520Zhenglun%2520Kong%2520and%2520Yixin%2520Shen%2520and%2520Sung-En%2520Chang%2520and%2520Timothy%2520Rupprecht%2520and%2520Lei%2520Lu%2520and%2520Enfu%2520Nan%2520and%2520Changdi%2520Yang%2520and%2520Yumei%2520He%2520and%2520Weiyan%2520Shi%2520and%2520Xingchen%2520Xu%2520and%2520Yu%2520Huang%2520and%2520Wei%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Yue%2520Chen%2520and%2520Yong%2520He%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3DRecently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520undergone%2520a%2520significant%2520transformation%252C%2520marked%2520by%2520a%2520rapid%2520rise%2520in%2520both%2520their%2520popularity%2520and%2520capabilities.%2520Leading%2520this%2520evolution%2520are%2520proprietary%2520LLMs%2520like%2520GPT-4%2520and%2520GPT-o1%252C%2520which%2520have%2520captured%2520widespread%2520attention%2520in%2520the%2520AI%2520community%2520due%2520to%2520their%2520remarkable%2520performance%2520and%2520versatility.%2520Simultaneously%252C%2520open-source%2520LLMs%252C%2520such%2520as%2520LLaMA%2520and%2520Mistral%252C%2520have%2520made%2520great%2520contributions%2520to%2520the%2520ever-increasing%2520popularity%2520of%2520LLMs%2520due%2520to%2520the%2520ease%2520to%2520customize%2520and%2520deploy%2520the%2520models%2520across%2520diverse%2520applications.%2520Moxin%25207B%2520is%2520introduced%2520as%2520a%2520fully%2520open-source%2520LLM%2520developed%2520in%2520accordance%2520with%2520the%2520Model%2520Openness%2520Framework%252C%2520which%2520moves%2520beyond%2520the%2520simple%2520sharing%2520of%2520model%2520weights%2520to%2520embrace%2520complete%2520transparency%2520in%2520training%252C%2520datasets%252C%2520and%2520implementation%2520detail%252C%2520thus%2520fostering%2520a%2520more%2520inclusive%2520and%2520collaborative%2520research%2520environment%2520that%2520can%2520sustain%2520a%2520healthy%2520open-source%2520ecosystem.%2520To%2520further%2520equip%2520Moxin%2520with%2520various%2520capabilities%2520in%2520different%2520tasks%252C%2520we%2520develop%2520three%2520variants%2520based%2520on%2520Moxin%252C%2520including%2520Moxin-VLM%252C%2520Moxin-VLA%252C%2520and%2520Moxin-Chinese%252C%2520which%2520target%2520the%2520vision-language%252C%2520vision-language-action%252C%2520and%2520Chinese%2520capabilities%252C%2520respectively.%2520Experiments%2520show%2520that%2520our%2520models%2520achieve%2520superior%2520performance%2520in%2520various%2520evaluations.%2520We%2520adopt%2520open-source%2520framework%2520and%2520open%2520data%2520for%2520the%2520training.%2520We%2520release%2520our%2520models%252C%2520along%2520with%2520the%2520available%2520data%2520and%2520code%2520to%2520derive%2520these%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Multimodal%20Moxin%20Models%20with%20Moxin-VLM%20and%20Moxin-VLA&entry.906535625=Pu%20Zhao%20and%20Arash%20Akbari%20and%20Xuan%20Shen%20and%20Zhenglun%20Kong%20and%20Yixin%20Shen%20and%20Sung-En%20Chang%20and%20Timothy%20Rupprecht%20and%20Lei%20Lu%20and%20Enfu%20Nan%20and%20Changdi%20Yang%20and%20Yumei%20He%20and%20Weiyan%20Shi%20and%20Xingchen%20Xu%20and%20Yu%20Huang%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Yue%20Chen%20and%20Yong%20He%20and%20Yanzhi%20Wang&entry.1292438233=Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20undergone%20a%20significant%20transformation%2C%20marked%20by%20a%20rapid%20rise%20in%20both%20their%20popularity%20and%20capabilities.%20Leading%20this%20evolution%20are%20proprietary%20LLMs%20like%20GPT-4%20and%20GPT-o1%2C%20which%20have%20captured%20widespread%20attention%20in%20the%20AI%20community%20due%20to%20their%20remarkable%20performance%20and%20versatility.%20Simultaneously%2C%20open-source%20LLMs%2C%20such%20as%20LLaMA%20and%20Mistral%2C%20have%20made%20great%20contributions%20to%20the%20ever-increasing%20popularity%20of%20LLMs%20due%20to%20the%20ease%20to%20customize%20and%20deploy%20the%20models%20across%20diverse%20applications.%20Moxin%207B%20is%20introduced%20as%20a%20fully%20open-source%20LLM%20developed%20in%20accordance%20with%20the%20Model%20Openness%20Framework%2C%20which%20moves%20beyond%20the%20simple%20sharing%20of%20model%20weights%20to%20embrace%20complete%20transparency%20in%20training%2C%20datasets%2C%20and%20implementation%20detail%2C%20thus%20fostering%20a%20more%20inclusive%20and%20collaborative%20research%20environment%20that%20can%20sustain%20a%20healthy%20open-source%20ecosystem.%20To%20further%20equip%20Moxin%20with%20various%20capabilities%20in%20different%20tasks%2C%20we%20develop%20three%20variants%20based%20on%20Moxin%2C%20including%20Moxin-VLM%2C%20Moxin-VLA%2C%20and%20Moxin-Chinese%2C%20which%20target%20the%20vision-language%2C%20vision-language-action%2C%20and%20Chinese%20capabilities%2C%20respectively.%20Experiments%20show%20that%20our%20models%20achieve%20superior%20performance%20in%20various%20evaluations.%20We%20adopt%20open-source%20framework%20and%20open%20data%20for%20the%20training.%20We%20release%20our%20models%2C%20along%20with%20the%20available%20data%20and%20code%20to%20derive%20these%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.22208v2&entry.124074799=Read"},
{"title": "Fluid Representations in Reasoning Models", "author": "Dmitrii Kharlapenko and Alessandro Stolfo and Arthur Conmy and Mrinmaya Sachan and Zhijing Jin", "abstract": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.", "link": "http://arxiv.org/abs/2602.04843v1", "date": "2026-02-04", "relevancy": 2.5722, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fluid%20Representations%20in%20Reasoning%20Models&body=Title%3A%20Fluid%20Representations%20in%20Reasoning%20Models%0AAuthor%3A%20Dmitrii%20Kharlapenko%20and%20Alessandro%20Stolfo%20and%20Arthur%20Conmy%20and%20Mrinmaya%20Sachan%20and%20Zhijing%20Jin%0AAbstract%3A%20Reasoning%20language%20models%2C%20which%20generate%20long%20chains%20of%20thought%2C%20dramatically%20outperform%20non-reasoning%20language%20models%20on%20abstract%20problems.%20However%2C%20the%20internal%20model%20mechanisms%20that%20allow%20this%20superior%20performance%20remain%20poorly%20understood.%20We%20present%20a%20mechanistic%20analysis%20of%20how%20QwQ-32B%20-%20a%20model%20specifically%20trained%20to%20produce%20extensive%20reasoning%20traces%20-%20process%20abstract%20structural%20information.%20On%20Mystery%20Blocksworld%20-%20a%20semantically%20obfuscated%20planning%20domain%20-%20we%20find%20that%20QwQ-32B%20gradually%20improves%20its%20internal%20representation%20of%20actions%20and%20concepts%20during%20reasoning.%20The%20model%20develops%20abstract%20encodings%20that%20focus%20on%20structure%20rather%20than%20specific%20action%20names.%20Through%20steering%20experiments%2C%20we%20establish%20causal%20evidence%20that%20these%20adaptations%20improve%20problem%20solving%3A%20injecting%20refined%20representations%20from%20successful%20traces%20boosts%20accuracy%2C%20while%20symbolic%20representations%20can%20replace%20many%20obfuscated%20encodings%20with%20minimal%20performance%20loss.%20We%20find%20that%20one%20of%20the%20factors%20driving%20reasoning%20model%20performance%20is%20in-context%20refinement%20of%20token%20representations%2C%20which%20we%20dub%20Fluid%20Reasoning%20Representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFluid%2520Representations%2520in%2520Reasoning%2520Models%26entry.906535625%3DDmitrii%2520Kharlapenko%2520and%2520Alessandro%2520Stolfo%2520and%2520Arthur%2520Conmy%2520and%2520Mrinmaya%2520Sachan%2520and%2520Zhijing%2520Jin%26entry.1292438233%3DReasoning%2520language%2520models%252C%2520which%2520generate%2520long%2520chains%2520of%2520thought%252C%2520dramatically%2520outperform%2520non-reasoning%2520language%2520models%2520on%2520abstract%2520problems.%2520However%252C%2520the%2520internal%2520model%2520mechanisms%2520that%2520allow%2520this%2520superior%2520performance%2520remain%2520poorly%2520understood.%2520We%2520present%2520a%2520mechanistic%2520analysis%2520of%2520how%2520QwQ-32B%2520-%2520a%2520model%2520specifically%2520trained%2520to%2520produce%2520extensive%2520reasoning%2520traces%2520-%2520process%2520abstract%2520structural%2520information.%2520On%2520Mystery%2520Blocksworld%2520-%2520a%2520semantically%2520obfuscated%2520planning%2520domain%2520-%2520we%2520find%2520that%2520QwQ-32B%2520gradually%2520improves%2520its%2520internal%2520representation%2520of%2520actions%2520and%2520concepts%2520during%2520reasoning.%2520The%2520model%2520develops%2520abstract%2520encodings%2520that%2520focus%2520on%2520structure%2520rather%2520than%2520specific%2520action%2520names.%2520Through%2520steering%2520experiments%252C%2520we%2520establish%2520causal%2520evidence%2520that%2520these%2520adaptations%2520improve%2520problem%2520solving%253A%2520injecting%2520refined%2520representations%2520from%2520successful%2520traces%2520boosts%2520accuracy%252C%2520while%2520symbolic%2520representations%2520can%2520replace%2520many%2520obfuscated%2520encodings%2520with%2520minimal%2520performance%2520loss.%2520We%2520find%2520that%2520one%2520of%2520the%2520factors%2520driving%2520reasoning%2520model%2520performance%2520is%2520in-context%2520refinement%2520of%2520token%2520representations%252C%2520which%2520we%2520dub%2520Fluid%2520Reasoning%2520Representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fluid%20Representations%20in%20Reasoning%20Models&entry.906535625=Dmitrii%20Kharlapenko%20and%20Alessandro%20Stolfo%20and%20Arthur%20Conmy%20and%20Mrinmaya%20Sachan%20and%20Zhijing%20Jin&entry.1292438233=Reasoning%20language%20models%2C%20which%20generate%20long%20chains%20of%20thought%2C%20dramatically%20outperform%20non-reasoning%20language%20models%20on%20abstract%20problems.%20However%2C%20the%20internal%20model%20mechanisms%20that%20allow%20this%20superior%20performance%20remain%20poorly%20understood.%20We%20present%20a%20mechanistic%20analysis%20of%20how%20QwQ-32B%20-%20a%20model%20specifically%20trained%20to%20produce%20extensive%20reasoning%20traces%20-%20process%20abstract%20structural%20information.%20On%20Mystery%20Blocksworld%20-%20a%20semantically%20obfuscated%20planning%20domain%20-%20we%20find%20that%20QwQ-32B%20gradually%20improves%20its%20internal%20representation%20of%20actions%20and%20concepts%20during%20reasoning.%20The%20model%20develops%20abstract%20encodings%20that%20focus%20on%20structure%20rather%20than%20specific%20action%20names.%20Through%20steering%20experiments%2C%20we%20establish%20causal%20evidence%20that%20these%20adaptations%20improve%20problem%20solving%3A%20injecting%20refined%20representations%20from%20successful%20traces%20boosts%20accuracy%2C%20while%20symbolic%20representations%20can%20replace%20many%20obfuscated%20encodings%20with%20minimal%20performance%20loss.%20We%20find%20that%20one%20of%20the%20factors%20driving%20reasoning%20model%20performance%20is%20in-context%20refinement%20of%20token%20representations%2C%20which%20we%20dub%20Fluid%20Reasoning%20Representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.04843v1&entry.124074799=Read"},
{"title": "Beyond Learning on Molecules by Weakly Supervising on Molecules", "author": "Gordan Prastalo and Kevin Maik Jablonka", "abstract": "Molecular representations are inherently task-dependent, yet most pre-trained molecular encoders are not. Task conditioning promises representations that reorganize based on task descriptions, but existing approaches rely on expensive labeled data. We show that weak supervision on programmatically derived molecular motifs is sufficient. Our Adaptive Chemical Embedding Model (ACE-Mol) learns from hundreds of motifs paired with natural language descriptors that are cheap to compute, trivial to scale. Conventional encoders slowly search the embedding space for task-relevant structure, whereas ACE-Mol immediately aligns its representations with the task. ACE-Mol achieves state-of-the-art performance across molecular property prediction benchmarks with interpretable, chemically meaningful representations.", "link": "http://arxiv.org/abs/2602.04696v1", "date": "2026-02-04", "relevancy": 2.561, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Learning%20on%20Molecules%20by%20Weakly%20Supervising%20on%20Molecules&body=Title%3A%20Beyond%20Learning%20on%20Molecules%20by%20Weakly%20Supervising%20on%20Molecules%0AAuthor%3A%20Gordan%20Prastalo%20and%20Kevin%20Maik%20Jablonka%0AAbstract%3A%20Molecular%20representations%20are%20inherently%20task-dependent%2C%20yet%20most%20pre-trained%20molecular%20encoders%20are%20not.%20Task%20conditioning%20promises%20representations%20that%20reorganize%20based%20on%20task%20descriptions%2C%20but%20existing%20approaches%20rely%20on%20expensive%20labeled%20data.%20We%20show%20that%20weak%20supervision%20on%20programmatically%20derived%20molecular%20motifs%20is%20sufficient.%20Our%20Adaptive%20Chemical%20Embedding%20Model%20%28ACE-Mol%29%20learns%20from%20hundreds%20of%20motifs%20paired%20with%20natural%20language%20descriptors%20that%20are%20cheap%20to%20compute%2C%20trivial%20to%20scale.%20Conventional%20encoders%20slowly%20search%20the%20embedding%20space%20for%20task-relevant%20structure%2C%20whereas%20ACE-Mol%20immediately%20aligns%20its%20representations%20with%20the%20task.%20ACE-Mol%20achieves%20state-of-the-art%20performance%20across%20molecular%20property%20prediction%20benchmarks%20with%20interpretable%2C%20chemically%20meaningful%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Learning%2520on%2520Molecules%2520by%2520Weakly%2520Supervising%2520on%2520Molecules%26entry.906535625%3DGordan%2520Prastalo%2520and%2520Kevin%2520Maik%2520Jablonka%26entry.1292438233%3DMolecular%2520representations%2520are%2520inherently%2520task-dependent%252C%2520yet%2520most%2520pre-trained%2520molecular%2520encoders%2520are%2520not.%2520Task%2520conditioning%2520promises%2520representations%2520that%2520reorganize%2520based%2520on%2520task%2520descriptions%252C%2520but%2520existing%2520approaches%2520rely%2520on%2520expensive%2520labeled%2520data.%2520We%2520show%2520that%2520weak%2520supervision%2520on%2520programmatically%2520derived%2520molecular%2520motifs%2520is%2520sufficient.%2520Our%2520Adaptive%2520Chemical%2520Embedding%2520Model%2520%2528ACE-Mol%2529%2520learns%2520from%2520hundreds%2520of%2520motifs%2520paired%2520with%2520natural%2520language%2520descriptors%2520that%2520are%2520cheap%2520to%2520compute%252C%2520trivial%2520to%2520scale.%2520Conventional%2520encoders%2520slowly%2520search%2520the%2520embedding%2520space%2520for%2520task-relevant%2520structure%252C%2520whereas%2520ACE-Mol%2520immediately%2520aligns%2520its%2520representations%2520with%2520the%2520task.%2520ACE-Mol%2520achieves%2520state-of-the-art%2520performance%2520across%2520molecular%2520property%2520prediction%2520benchmarks%2520with%2520interpretable%252C%2520chemically%2520meaningful%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Learning%20on%20Molecules%20by%20Weakly%20Supervising%20on%20Molecules&entry.906535625=Gordan%20Prastalo%20and%20Kevin%20Maik%20Jablonka&entry.1292438233=Molecular%20representations%20are%20inherently%20task-dependent%2C%20yet%20most%20pre-trained%20molecular%20encoders%20are%20not.%20Task%20conditioning%20promises%20representations%20that%20reorganize%20based%20on%20task%20descriptions%2C%20but%20existing%20approaches%20rely%20on%20expensive%20labeled%20data.%20We%20show%20that%20weak%20supervision%20on%20programmatically%20derived%20molecular%20motifs%20is%20sufficient.%20Our%20Adaptive%20Chemical%20Embedding%20Model%20%28ACE-Mol%29%20learns%20from%20hundreds%20of%20motifs%20paired%20with%20natural%20language%20descriptors%20that%20are%20cheap%20to%20compute%2C%20trivial%20to%20scale.%20Conventional%20encoders%20slowly%20search%20the%20embedding%20space%20for%20task-relevant%20structure%2C%20whereas%20ACE-Mol%20immediately%20aligns%20its%20representations%20with%20the%20task.%20ACE-Mol%20achieves%20state-of-the-art%20performance%20across%20molecular%20property%20prediction%20benchmarks%20with%20interpretable%2C%20chemically%20meaningful%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.04696v1&entry.124074799=Read"},
{"title": "History-Guided Iterative Visual Reasoning with Self-Correction", "author": "Xinglong Yang and Zhilin Peng and Zhanzhan Liu and Haochen Shi and Sheng-Jun Huang", "abstract": "Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \\texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\\%, representing a 107\\% improvement over the baseline.", "link": "http://arxiv.org/abs/2602.04413v1", "date": "2026-02-04", "relevancy": 2.5574, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20History-Guided%20Iterative%20Visual%20Reasoning%20with%20Self-Correction&body=Title%3A%20History-Guided%20Iterative%20Visual%20Reasoning%20with%20Self-Correction%0AAuthor%3A%20Xinglong%20Yang%20and%20Zhilin%20Peng%20and%20Zhanzhan%20Liu%20and%20Haochen%20Shi%20and%20Sheng-Jun%20Huang%0AAbstract%3A%20Self-consistency%20methods%20are%20the%20core%20technique%20for%20improving%20the%20reasoning%20reliability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20By%20generating%20multiple%20reasoning%20results%20through%20repeated%20sampling%20and%20selecting%20the%20best%20answer%20via%20voting%2C%20they%20play%20an%20important%20role%20in%20cross-modal%20tasks.%20However%2C%20most%20existing%20self-consistency%20methods%20are%20limited%20to%20a%20fixed%20%60%60repeated%20sampling%20and%20voting%27%27%20paradigm%20and%20do%20not%20reuse%20historical%20reasoning%20information.%20As%20a%20result%2C%20models%20struggle%20to%20actively%20correct%20visual%20understanding%20errors%20and%20dynamically%20adjust%20their%20reasoning%20during%20iteration.%20Inspired%20by%20the%20human%20reasoning%20behavior%20of%20repeated%20verification%20and%20dynamic%20error%20correction%2C%20we%20propose%20the%20H-GIVR%20framework.%20During%20iterative%20reasoning%2C%20the%20MLLM%20observes%20the%20image%20multiple%20times%20and%20uses%20previously%20generated%20answers%20as%20references%20for%20subsequent%20steps%2C%20enabling%20dynamic%20correction%20of%20errors%20and%20improving%20answer%20accuracy.%20We%20conduct%20comprehensive%20experiments%20on%20five%20datasets%20and%20three%20models.%20The%20results%20show%20that%20the%20H-GIVR%20framework%20can%20significantly%20improve%20cross-modal%20reasoning%20accuracy%20while%20maintaining%20low%20computational%20cost.%20For%20instance%2C%20using%20%5Ctexttt%7BLlama3.2-vision%3A11b%7D%20on%20the%20ScienceQA%20dataset%2C%20the%20model%20requires%20an%20average%20of%202.57%20responses%20per%20question%20to%20achieve%20an%20accuracy%20of%2078.90%5C%25%2C%20representing%20a%20107%5C%25%20improvement%20over%20the%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistory-Guided%2520Iterative%2520Visual%2520Reasoning%2520with%2520Self-Correction%26entry.906535625%3DXinglong%2520Yang%2520and%2520Zhilin%2520Peng%2520and%2520Zhanzhan%2520Liu%2520and%2520Haochen%2520Shi%2520and%2520Sheng-Jun%2520Huang%26entry.1292438233%3DSelf-consistency%2520methods%2520are%2520the%2520core%2520technique%2520for%2520improving%2520the%2520reasoning%2520reliability%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520By%2520generating%2520multiple%2520reasoning%2520results%2520through%2520repeated%2520sampling%2520and%2520selecting%2520the%2520best%2520answer%2520via%2520voting%252C%2520they%2520play%2520an%2520important%2520role%2520in%2520cross-modal%2520tasks.%2520However%252C%2520most%2520existing%2520self-consistency%2520methods%2520are%2520limited%2520to%2520a%2520fixed%2520%2560%2560repeated%2520sampling%2520and%2520voting%2527%2527%2520paradigm%2520and%2520do%2520not%2520reuse%2520historical%2520reasoning%2520information.%2520As%2520a%2520result%252C%2520models%2520struggle%2520to%2520actively%2520correct%2520visual%2520understanding%2520errors%2520and%2520dynamically%2520adjust%2520their%2520reasoning%2520during%2520iteration.%2520Inspired%2520by%2520the%2520human%2520reasoning%2520behavior%2520of%2520repeated%2520verification%2520and%2520dynamic%2520error%2520correction%252C%2520we%2520propose%2520the%2520H-GIVR%2520framework.%2520During%2520iterative%2520reasoning%252C%2520the%2520MLLM%2520observes%2520the%2520image%2520multiple%2520times%2520and%2520uses%2520previously%2520generated%2520answers%2520as%2520references%2520for%2520subsequent%2520steps%252C%2520enabling%2520dynamic%2520correction%2520of%2520errors%2520and%2520improving%2520answer%2520accuracy.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520five%2520datasets%2520and%2520three%2520models.%2520The%2520results%2520show%2520that%2520the%2520H-GIVR%2520framework%2520can%2520significantly%2520improve%2520cross-modal%2520reasoning%2520accuracy%2520while%2520maintaining%2520low%2520computational%2520cost.%2520For%2520instance%252C%2520using%2520%255Ctexttt%257BLlama3.2-vision%253A11b%257D%2520on%2520the%2520ScienceQA%2520dataset%252C%2520the%2520model%2520requires%2520an%2520average%2520of%25202.57%2520responses%2520per%2520question%2520to%2520achieve%2520an%2520accuracy%2520of%252078.90%255C%2525%252C%2520representing%2520a%2520107%255C%2525%2520improvement%2520over%2520the%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=History-Guided%20Iterative%20Visual%20Reasoning%20with%20Self-Correction&entry.906535625=Xinglong%20Yang%20and%20Zhilin%20Peng%20and%20Zhanzhan%20Liu%20and%20Haochen%20Shi%20and%20Sheng-Jun%20Huang&entry.1292438233=Self-consistency%20methods%20are%20the%20core%20technique%20for%20improving%20the%20reasoning%20reliability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20By%20generating%20multiple%20reasoning%20results%20through%20repeated%20sampling%20and%20selecting%20the%20best%20answer%20via%20voting%2C%20they%20play%20an%20important%20role%20in%20cross-modal%20tasks.%20However%2C%20most%20existing%20self-consistency%20methods%20are%20limited%20to%20a%20fixed%20%60%60repeated%20sampling%20and%20voting%27%27%20paradigm%20and%20do%20not%20reuse%20historical%20reasoning%20information.%20As%20a%20result%2C%20models%20struggle%20to%20actively%20correct%20visual%20understanding%20errors%20and%20dynamically%20adjust%20their%20reasoning%20during%20iteration.%20Inspired%20by%20the%20human%20reasoning%20behavior%20of%20repeated%20verification%20and%20dynamic%20error%20correction%2C%20we%20propose%20the%20H-GIVR%20framework.%20During%20iterative%20reasoning%2C%20the%20MLLM%20observes%20the%20image%20multiple%20times%20and%20uses%20previously%20generated%20answers%20as%20references%20for%20subsequent%20steps%2C%20enabling%20dynamic%20correction%20of%20errors%20and%20improving%20answer%20accuracy.%20We%20conduct%20comprehensive%20experiments%20on%20five%20datasets%20and%20three%20models.%20The%20results%20show%20that%20the%20H-GIVR%20framework%20can%20significantly%20improve%20cross-modal%20reasoning%20accuracy%20while%20maintaining%20low%20computational%20cost.%20For%20instance%2C%20using%20%5Ctexttt%7BLlama3.2-vision%3A11b%7D%20on%20the%20ScienceQA%20dataset%2C%20the%20model%20requires%20an%20average%20of%202.57%20responses%20per%20question%20to%20achieve%20an%20accuracy%20of%2078.90%5C%25%2C%20representing%20a%20107%5C%25%20improvement%20over%20the%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2602.04413v1&entry.124074799=Read"},
{"title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task", "author": "Hannah Pinson", "abstract": "Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.", "link": "http://arxiv.org/abs/2602.04832v1", "date": "2026-02-04", "relevancy": 2.5551, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5587}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4945}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20not%20a%20Lottery%2C%20it%27s%20a%20Race%3A%20Understanding%20How%20Gradient%20Descent%20Adapts%20the%20Network%27s%20Capacity%20to%20the%20Task&body=Title%3A%20It%27s%20not%20a%20Lottery%2C%20it%27s%20a%20Race%3A%20Understanding%20How%20Gradient%20Descent%20Adapts%20the%20Network%27s%20Capacity%20to%20the%20Task%0AAuthor%3A%20Hannah%20Pinson%0AAbstract%3A%20Our%20theoretical%20understanding%20of%20neural%20networks%20is%20lagging%20behind%20their%20empirical%20success.%20One%20of%20the%20important%20unexplained%20phenomena%20is%20why%20and%20how%2C%20during%20the%20process%20of%20training%20with%20gradient%20descent%2C%20the%20theoretical%20capacity%20of%20neural%20networks%20is%20reduced%20to%20an%20effective%20capacity%20that%20fits%20the%20task.%20We%20here%20investigate%20the%20mechanism%20by%20which%20gradient%20descent%20achieves%20this%20through%20analyzing%20the%20learning%20dynamics%20at%20the%20level%20of%20individual%20neurons%20in%20single%20hidden%20layer%20ReLU%20networks.%20We%20identify%20three%20dynamical%20principles%20--%20mutual%20alignment%2C%20unlocking%20and%20racing%20--%20that%20together%20explain%20why%20we%20can%20often%20successfully%20reduce%20capacity%20after%20training%20through%20the%20merging%20of%20equivalent%20neurons%20or%20the%20pruning%20of%20low%20norm%20weights.%20We%20specifically%20explain%20the%20mechanism%20behind%20the%20lottery%20ticket%20conjecture%2C%20or%20why%20the%20specific%2C%20beneficial%20initial%20conditions%20of%20some%20neurons%20lead%20them%20to%20obtain%20higher%20weight%20norms.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520not%2520a%2520Lottery%252C%2520it%2527s%2520a%2520Race%253A%2520Understanding%2520How%2520Gradient%2520Descent%2520Adapts%2520the%2520Network%2527s%2520Capacity%2520to%2520the%2520Task%26entry.906535625%3DHannah%2520Pinson%26entry.1292438233%3DOur%2520theoretical%2520understanding%2520of%2520neural%2520networks%2520is%2520lagging%2520behind%2520their%2520empirical%2520success.%2520One%2520of%2520the%2520important%2520unexplained%2520phenomena%2520is%2520why%2520and%2520how%252C%2520during%2520the%2520process%2520of%2520training%2520with%2520gradient%2520descent%252C%2520the%2520theoretical%2520capacity%2520of%2520neural%2520networks%2520is%2520reduced%2520to%2520an%2520effective%2520capacity%2520that%2520fits%2520the%2520task.%2520We%2520here%2520investigate%2520the%2520mechanism%2520by%2520which%2520gradient%2520descent%2520achieves%2520this%2520through%2520analyzing%2520the%2520learning%2520dynamics%2520at%2520the%2520level%2520of%2520individual%2520neurons%2520in%2520single%2520hidden%2520layer%2520ReLU%2520networks.%2520We%2520identify%2520three%2520dynamical%2520principles%2520--%2520mutual%2520alignment%252C%2520unlocking%2520and%2520racing%2520--%2520that%2520together%2520explain%2520why%2520we%2520can%2520often%2520successfully%2520reduce%2520capacity%2520after%2520training%2520through%2520the%2520merging%2520of%2520equivalent%2520neurons%2520or%2520the%2520pruning%2520of%2520low%2520norm%2520weights.%2520We%2520specifically%2520explain%2520the%2520mechanism%2520behind%2520the%2520lottery%2520ticket%2520conjecture%252C%2520or%2520why%2520the%2520specific%252C%2520beneficial%2520initial%2520conditions%2520of%2520some%2520neurons%2520lead%2520them%2520to%2520obtain%2520higher%2520weight%2520norms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20not%20a%20Lottery%2C%20it%27s%20a%20Race%3A%20Understanding%20How%20Gradient%20Descent%20Adapts%20the%20Network%27s%20Capacity%20to%20the%20Task&entry.906535625=Hannah%20Pinson&entry.1292438233=Our%20theoretical%20understanding%20of%20neural%20networks%20is%20lagging%20behind%20their%20empirical%20success.%20One%20of%20the%20important%20unexplained%20phenomena%20is%20why%20and%20how%2C%20during%20the%20process%20of%20training%20with%20gradient%20descent%2C%20the%20theoretical%20capacity%20of%20neural%20networks%20is%20reduced%20to%20an%20effective%20capacity%20that%20fits%20the%20task.%20We%20here%20investigate%20the%20mechanism%20by%20which%20gradient%20descent%20achieves%20this%20through%20analyzing%20the%20learning%20dynamics%20at%20the%20level%20of%20individual%20neurons%20in%20single%20hidden%20layer%20ReLU%20networks.%20We%20identify%20three%20dynamical%20principles%20--%20mutual%20alignment%2C%20unlocking%20and%20racing%20--%20that%20together%20explain%20why%20we%20can%20often%20successfully%20reduce%20capacity%20after%20training%20through%20the%20merging%20of%20equivalent%20neurons%20or%20the%20pruning%20of%20low%20norm%20weights.%20We%20specifically%20explain%20the%20mechanism%20behind%20the%20lottery%20ticket%20conjecture%2C%20or%20why%20the%20specific%2C%20beneficial%20initial%20conditions%20of%20some%20neurons%20lead%20them%20to%20obtain%20higher%20weight%20norms.&entry.1838667208=http%3A//arxiv.org/abs/2602.04832v1&entry.124074799=Read"},
{"title": "TrajVG: 3D Trajectory-Coupled Visual Geometry Learning", "author": "Xingyu Miao and Weiguang Zhao and Tao Lu and Linning Yu and Mulin Yu and Yang Long and Jiangmiao Pang and Junting Dong", "abstract": "Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.", "link": "http://arxiv.org/abs/2602.04439v1", "date": "2026-02-04", "relevancy": 2.5307, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6368}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6355}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrajVG%3A%203D%20Trajectory-Coupled%20Visual%20Geometry%20Learning&body=Title%3A%20TrajVG%3A%203D%20Trajectory-Coupled%20Visual%20Geometry%20Learning%0AAuthor%3A%20Xingyu%20Miao%20and%20Weiguang%20Zhao%20and%20Tao%20Lu%20and%20Linning%20Yu%20and%20Mulin%20Yu%20and%20Yang%20Long%20and%20Jiangmiao%20Pang%20and%20Junting%20Dong%0AAbstract%3A%20Feed-forward%20multi-frame%203D%20reconstruction%20models%20often%20degrade%20on%20videos%20with%20object%20motion.%20Global-reference%20becomes%20ambiguous%20under%20multiple%20motions%2C%20while%20the%20local%20pointmap%20relies%20heavily%20on%20estimated%20relative%20poses%20and%20can%20drift%2C%20causing%20cross-frame%20misalignment%20and%20duplicated%20structures.%20We%20propose%20TrajVG%2C%20a%20reconstruction%20framework%20that%20makes%20cross-frame%203D%20correspondence%20an%20explicit%20prediction%20by%20estimating%20camera-coordinate%203D%20trajectories.%20We%20couple%20sparse%20trajectories%2C%20per-frame%20local%20point%20maps%2C%20and%20relative%20camera%20poses%20with%20geometric%20consistency%20objectives%3A%20%28i%29%20bidirectional%20trajectory-pointmap%20consistency%20with%20controlled%20gradient%20flow%2C%20and%20%28ii%29%20a%20pose%20consistency%20objective%20driven%20by%20static%20track%20anchors%20that%20suppresses%20gradients%20from%20dynamic%20regions.%20To%20scale%20training%20to%20in-the-wild%20videos%20where%203D%20trajectory%20labels%20are%20scarce%2C%20we%20reformulate%20the%20same%20coupling%20constraints%20into%20self-supervised%20objectives%20using%20only%20pseudo%202D%20tracks%2C%20enabling%20unified%20training%20with%20mixed%20supervision.%20Extensive%20experiments%20across%203D%20tracking%2C%20pose%20estimation%2C%20pointmap%20reconstruction%2C%20and%20video%20depth%20show%20that%20TrajVG%20surpasses%20the%20current%20feedforward%20performance%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajVG%253A%25203D%2520Trajectory-Coupled%2520Visual%2520Geometry%2520Learning%26entry.906535625%3DXingyu%2520Miao%2520and%2520Weiguang%2520Zhao%2520and%2520Tao%2520Lu%2520and%2520Linning%2520Yu%2520and%2520Mulin%2520Yu%2520and%2520Yang%2520Long%2520and%2520Jiangmiao%2520Pang%2520and%2520Junting%2520Dong%26entry.1292438233%3DFeed-forward%2520multi-frame%25203D%2520reconstruction%2520models%2520often%2520degrade%2520on%2520videos%2520with%2520object%2520motion.%2520Global-reference%2520becomes%2520ambiguous%2520under%2520multiple%2520motions%252C%2520while%2520the%2520local%2520pointmap%2520relies%2520heavily%2520on%2520estimated%2520relative%2520poses%2520and%2520can%2520drift%252C%2520causing%2520cross-frame%2520misalignment%2520and%2520duplicated%2520structures.%2520We%2520propose%2520TrajVG%252C%2520a%2520reconstruction%2520framework%2520that%2520makes%2520cross-frame%25203D%2520correspondence%2520an%2520explicit%2520prediction%2520by%2520estimating%2520camera-coordinate%25203D%2520trajectories.%2520We%2520couple%2520sparse%2520trajectories%252C%2520per-frame%2520local%2520point%2520maps%252C%2520and%2520relative%2520camera%2520poses%2520with%2520geometric%2520consistency%2520objectives%253A%2520%2528i%2529%2520bidirectional%2520trajectory-pointmap%2520consistency%2520with%2520controlled%2520gradient%2520flow%252C%2520and%2520%2528ii%2529%2520a%2520pose%2520consistency%2520objective%2520driven%2520by%2520static%2520track%2520anchors%2520that%2520suppresses%2520gradients%2520from%2520dynamic%2520regions.%2520To%2520scale%2520training%2520to%2520in-the-wild%2520videos%2520where%25203D%2520trajectory%2520labels%2520are%2520scarce%252C%2520we%2520reformulate%2520the%2520same%2520coupling%2520constraints%2520into%2520self-supervised%2520objectives%2520using%2520only%2520pseudo%25202D%2520tracks%252C%2520enabling%2520unified%2520training%2520with%2520mixed%2520supervision.%2520Extensive%2520experiments%2520across%25203D%2520tracking%252C%2520pose%2520estimation%252C%2520pointmap%2520reconstruction%252C%2520and%2520video%2520depth%2520show%2520that%2520TrajVG%2520surpasses%2520the%2520current%2520feedforward%2520performance%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrajVG%3A%203D%20Trajectory-Coupled%20Visual%20Geometry%20Learning&entry.906535625=Xingyu%20Miao%20and%20Weiguang%20Zhao%20and%20Tao%20Lu%20and%20Linning%20Yu%20and%20Mulin%20Yu%20and%20Yang%20Long%20and%20Jiangmiao%20Pang%20and%20Junting%20Dong&entry.1292438233=Feed-forward%20multi-frame%203D%20reconstruction%20models%20often%20degrade%20on%20videos%20with%20object%20motion.%20Global-reference%20becomes%20ambiguous%20under%20multiple%20motions%2C%20while%20the%20local%20pointmap%20relies%20heavily%20on%20estimated%20relative%20poses%20and%20can%20drift%2C%20causing%20cross-frame%20misalignment%20and%20duplicated%20structures.%20We%20propose%20TrajVG%2C%20a%20reconstruction%20framework%20that%20makes%20cross-frame%203D%20correspondence%20an%20explicit%20prediction%20by%20estimating%20camera-coordinate%203D%20trajectories.%20We%20couple%20sparse%20trajectories%2C%20per-frame%20local%20point%20maps%2C%20and%20relative%20camera%20poses%20with%20geometric%20consistency%20objectives%3A%20%28i%29%20bidirectional%20trajectory-pointmap%20consistency%20with%20controlled%20gradient%20flow%2C%20and%20%28ii%29%20a%20pose%20consistency%20objective%20driven%20by%20static%20track%20anchors%20that%20suppresses%20gradients%20from%20dynamic%20regions.%20To%20scale%20training%20to%20in-the-wild%20videos%20where%203D%20trajectory%20labels%20are%20scarce%2C%20we%20reformulate%20the%20same%20coupling%20constraints%20into%20self-supervised%20objectives%20using%20only%20pseudo%202D%20tracks%2C%20enabling%20unified%20training%20with%20mixed%20supervision.%20Extensive%20experiments%20across%203D%20tracking%2C%20pose%20estimation%2C%20pointmap%20reconstruction%2C%20and%20video%20depth%20show%20that%20TrajVG%20surpasses%20the%20current%20feedforward%20performance%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2602.04439v1&entry.124074799=Read"},
{"title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation", "author": "Jiahao Zhan and Zizhang Li and Hong-Xing Yu and Jiajun Wu", "abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.", "link": "http://arxiv.org/abs/2602.04876v1", "date": "2026-02-04", "relevancy": 2.5263, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6589}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6366}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerpetualWonder%3A%20Long-Horizon%20Action-Conditioned%204D%20Scene%20Generation&body=Title%3A%20PerpetualWonder%3A%20Long-Horizon%20Action-Conditioned%204D%20Scene%20Generation%0AAuthor%3A%20Jiahao%20Zhan%20and%20Zizhang%20Li%20and%20Hong-Xing%20Yu%20and%20Jiajun%20Wu%0AAbstract%3A%20We%20introduce%20PerpetualWonder%2C%20a%20hybrid%20generative%20simulator%20that%20enables%20long-horizon%2C%20action-conditioned%204D%20scene%20generation%20from%20a%20single%20image.%20Current%20works%20fail%20at%20this%20task%20because%20their%20physical%20state%20is%20decoupled%20from%20their%20visual%20representation%2C%20which%20prevents%20generative%20refinements%20to%20update%20the%20underlying%20physics%20for%20subsequent%20interactions.%20PerpetualWonder%20solves%20this%20by%20introducing%20the%20first%20true%20closed-loop%20system.%20It%20features%20a%20novel%20unified%20representation%20that%20creates%20a%20bidirectional%20link%20between%20the%20physical%20state%20and%20visual%20primitives%2C%20allowing%20generative%20refinements%20to%20correct%20both%20the%20dynamics%20and%20appearance.%20It%20also%20introduces%20a%20robust%20update%20mechanism%20that%20gathers%20supervision%20from%20multiple%20viewpoints%20to%20resolve%20optimization%20ambiguity.%20Experiments%20demonstrate%20that%20from%20a%20single%20image%2C%20PerpetualWonder%20can%20successfully%20simulate%20complex%2C%20multi-step%20interactions%20from%20long-horizon%20actions%2C%20maintaining%20physical%20plausibility%20and%20visual%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerpetualWonder%253A%2520Long-Horizon%2520Action-Conditioned%25204D%2520Scene%2520Generation%26entry.906535625%3DJiahao%2520Zhan%2520and%2520Zizhang%2520Li%2520and%2520Hong-Xing%2520Yu%2520and%2520Jiajun%2520Wu%26entry.1292438233%3DWe%2520introduce%2520PerpetualWonder%252C%2520a%2520hybrid%2520generative%2520simulator%2520that%2520enables%2520long-horizon%252C%2520action-conditioned%25204D%2520scene%2520generation%2520from%2520a%2520single%2520image.%2520Current%2520works%2520fail%2520at%2520this%2520task%2520because%2520their%2520physical%2520state%2520is%2520decoupled%2520from%2520their%2520visual%2520representation%252C%2520which%2520prevents%2520generative%2520refinements%2520to%2520update%2520the%2520underlying%2520physics%2520for%2520subsequent%2520interactions.%2520PerpetualWonder%2520solves%2520this%2520by%2520introducing%2520the%2520first%2520true%2520closed-loop%2520system.%2520It%2520features%2520a%2520novel%2520unified%2520representation%2520that%2520creates%2520a%2520bidirectional%2520link%2520between%2520the%2520physical%2520state%2520and%2520visual%2520primitives%252C%2520allowing%2520generative%2520refinements%2520to%2520correct%2520both%2520the%2520dynamics%2520and%2520appearance.%2520It%2520also%2520introduces%2520a%2520robust%2520update%2520mechanism%2520that%2520gathers%2520supervision%2520from%2520multiple%2520viewpoints%2520to%2520resolve%2520optimization%2520ambiguity.%2520Experiments%2520demonstrate%2520that%2520from%2520a%2520single%2520image%252C%2520PerpetualWonder%2520can%2520successfully%2520simulate%2520complex%252C%2520multi-step%2520interactions%2520from%2520long-horizon%2520actions%252C%2520maintaining%2520physical%2520plausibility%2520and%2520visual%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerpetualWonder%3A%20Long-Horizon%20Action-Conditioned%204D%20Scene%20Generation&entry.906535625=Jiahao%20Zhan%20and%20Zizhang%20Li%20and%20Hong-Xing%20Yu%20and%20Jiajun%20Wu&entry.1292438233=We%20introduce%20PerpetualWonder%2C%20a%20hybrid%20generative%20simulator%20that%20enables%20long-horizon%2C%20action-conditioned%204D%20scene%20generation%20from%20a%20single%20image.%20Current%20works%20fail%20at%20this%20task%20because%20their%20physical%20state%20is%20decoupled%20from%20their%20visual%20representation%2C%20which%20prevents%20generative%20refinements%20to%20update%20the%20underlying%20physics%20for%20subsequent%20interactions.%20PerpetualWonder%20solves%20this%20by%20introducing%20the%20first%20true%20closed-loop%20system.%20It%20features%20a%20novel%20unified%20representation%20that%20creates%20a%20bidirectional%20link%20between%20the%20physical%20state%20and%20visual%20primitives%2C%20allowing%20generative%20refinements%20to%20correct%20both%20the%20dynamics%20and%20appearance.%20It%20also%20introduces%20a%20robust%20update%20mechanism%20that%20gathers%20supervision%20from%20multiple%20viewpoints%20to%20resolve%20optimization%20ambiguity.%20Experiments%20demonstrate%20that%20from%20a%20single%20image%2C%20PerpetualWonder%20can%20successfully%20simulate%20complex%2C%20multi-step%20interactions%20from%20long-horizon%20actions%2C%20maintaining%20physical%20plausibility%20and%20visual%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2602.04876v1&entry.124074799=Read"},
{"title": "Decomposing Query-Key Feature Interactions Using Contrastive Covariances", "author": "Andrew Lee and Yonatan Belinkov and Fernanda Vi\u00e9gas and Martin Wattenberg", "abstract": "Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.", "link": "http://arxiv.org/abs/2602.04752v1", "date": "2026-02-04", "relevancy": 2.5241, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20Query-Key%20Feature%20Interactions%20Using%20Contrastive%20Covariances&body=Title%3A%20Decomposing%20Query-Key%20Feature%20Interactions%20Using%20Contrastive%20Covariances%0AAuthor%3A%20Andrew%20Lee%20and%20Yonatan%20Belinkov%20and%20Fernanda%20Vi%C3%A9gas%20and%20Martin%20Wattenberg%0AAbstract%3A%20Despite%20the%20central%20role%20of%20attention%20heads%20in%20Transformers%2C%20we%20lack%20tools%20to%20understand%20why%20a%20model%20attends%20to%20a%20particular%20token.%20To%20address%20this%2C%20we%20study%20the%20query-key%20%28QK%29%20space%20--%20the%20bilinear%20joint%20embedding%20space%20between%20queries%20and%20keys.%20We%20present%20a%20contrastive%20covariance%20method%20to%20decompose%20the%20QK%20space%20into%20low-rank%2C%20human-interpretable%20components.%20It%20is%20when%20features%20in%20keys%20and%20queries%20align%20in%20these%20low-rank%20subspaces%20that%20high%20attention%20scores%20are%20produced.%20We%20first%20study%20our%20method%20both%20analytically%20and%20empirically%20in%20a%20simplified%20setting.%20We%20then%20apply%20our%20method%20to%20large%20language%20models%20to%20identify%20human-interpretable%20QK%20subspaces%20for%20categorical%20semantic%20features%20and%20binding%20features.%20Finally%2C%20we%20demonstrate%20how%20attention%20scores%20can%20be%20attributed%20to%20our%20identified%20features.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520Query-Key%2520Feature%2520Interactions%2520Using%2520Contrastive%2520Covariances%26entry.906535625%3DAndrew%2520Lee%2520and%2520Yonatan%2520Belinkov%2520and%2520Fernanda%2520Vi%25C3%25A9gas%2520and%2520Martin%2520Wattenberg%26entry.1292438233%3DDespite%2520the%2520central%2520role%2520of%2520attention%2520heads%2520in%2520Transformers%252C%2520we%2520lack%2520tools%2520to%2520understand%2520why%2520a%2520model%2520attends%2520to%2520a%2520particular%2520token.%2520To%2520address%2520this%252C%2520we%2520study%2520the%2520query-key%2520%2528QK%2529%2520space%2520--%2520the%2520bilinear%2520joint%2520embedding%2520space%2520between%2520queries%2520and%2520keys.%2520We%2520present%2520a%2520contrastive%2520covariance%2520method%2520to%2520decompose%2520the%2520QK%2520space%2520into%2520low-rank%252C%2520human-interpretable%2520components.%2520It%2520is%2520when%2520features%2520in%2520keys%2520and%2520queries%2520align%2520in%2520these%2520low-rank%2520subspaces%2520that%2520high%2520attention%2520scores%2520are%2520produced.%2520We%2520first%2520study%2520our%2520method%2520both%2520analytically%2520and%2520empirically%2520in%2520a%2520simplified%2520setting.%2520We%2520then%2520apply%2520our%2520method%2520to%2520large%2520language%2520models%2520to%2520identify%2520human-interpretable%2520QK%2520subspaces%2520for%2520categorical%2520semantic%2520features%2520and%2520binding%2520features.%2520Finally%252C%2520we%2520demonstrate%2520how%2520attention%2520scores%2520can%2520be%2520attributed%2520to%2520our%2520identified%2520features.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20Query-Key%20Feature%20Interactions%20Using%20Contrastive%20Covariances&entry.906535625=Andrew%20Lee%20and%20Yonatan%20Belinkov%20and%20Fernanda%20Vi%C3%A9gas%20and%20Martin%20Wattenberg&entry.1292438233=Despite%20the%20central%20role%20of%20attention%20heads%20in%20Transformers%2C%20we%20lack%20tools%20to%20understand%20why%20a%20model%20attends%20to%20a%20particular%20token.%20To%20address%20this%2C%20we%20study%20the%20query-key%20%28QK%29%20space%20--%20the%20bilinear%20joint%20embedding%20space%20between%20queries%20and%20keys.%20We%20present%20a%20contrastive%20covariance%20method%20to%20decompose%20the%20QK%20space%20into%20low-rank%2C%20human-interpretable%20components.%20It%20is%20when%20features%20in%20keys%20and%20queries%20align%20in%20these%20low-rank%20subspaces%20that%20high%20attention%20scores%20are%20produced.%20We%20first%20study%20our%20method%20both%20analytically%20and%20empirically%20in%20a%20simplified%20setting.%20We%20then%20apply%20our%20method%20to%20large%20language%20models%20to%20identify%20human-interpretable%20QK%20subspaces%20for%20categorical%20semantic%20features%20and%20binding%20features.%20Finally%2C%20we%20demonstrate%20how%20attention%20scores%20can%20be%20attributed%20to%20our%20identified%20features.&entry.1838667208=http%3A//arxiv.org/abs/2602.04752v1&entry.124074799=Read"},
{"title": "RIGA-Fold: A General Framework for Protein Inverse Folding via Recurrent Interaction and Geometric Awareness", "author": "Sisi Yuan and Jiehuang Chen and Junchuang Cai and Dong Xu and Xueliang Li and Zexuan Zhu and Junkai Ji", "abstract": "Protein inverse folding, the task of predicting amino acid sequences for desired structures, is pivotal for de novo protein design. However, existing GNN-based methods typically suffer from restricted receptive fields that miss long-range dependencies and a \"single-pass\" inference paradigm that leads to error accumulation. To address these bottlenecks, we propose RIGA-Fold, a framework that synergizes Recurrent Interaction with Geometric Awareness. At the micro-level, we introduce a Geometric Attention Update (GAU) module where edge features explicitly serve as attention keys, ensuring strictly SE(3)-invariant local encoding. At the macro-level, we design an attention-based Global Context Bridge that acts as a soft gating mechanism to dynamically inject global topological information. Furthermore, to bridge the gap between structural and sequence modalities, we introduce an enhanced variant, RIGA-Fold*, which integrates trainable geometric features with frozen evolutionary priors from ESM-2 and ESM-IF via a dual-stream architecture. Finally, a biologically inspired ``predict-recycle-refine'' strategy is implemented to iteratively denoise sequence distributions. Extensive experiments on CATH 4.2, TS50, and TS500 benchmarks demonstrate that our geometric framework is highly competitive, while RIGA-Fold* significantly outperforms state-of-the-art baselines in both sequence recovery and structural consistency.", "link": "http://arxiv.org/abs/2602.04637v1", "date": "2026-02-04", "relevancy": 2.524, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5173}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5092}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIGA-Fold%3A%20A%20General%20Framework%20for%20Protein%20Inverse%20Folding%20via%20Recurrent%20Interaction%20and%20Geometric%20Awareness&body=Title%3A%20RIGA-Fold%3A%20A%20General%20Framework%20for%20Protein%20Inverse%20Folding%20via%20Recurrent%20Interaction%20and%20Geometric%20Awareness%0AAuthor%3A%20Sisi%20Yuan%20and%20Jiehuang%20Chen%20and%20Junchuang%20Cai%20and%20Dong%20Xu%20and%20Xueliang%20Li%20and%20Zexuan%20Zhu%20and%20Junkai%20Ji%0AAbstract%3A%20Protein%20inverse%20folding%2C%20the%20task%20of%20predicting%20amino%20acid%20sequences%20for%20desired%20structures%2C%20is%20pivotal%20for%20de%20novo%20protein%20design.%20However%2C%20existing%20GNN-based%20methods%20typically%20suffer%20from%20restricted%20receptive%20fields%20that%20miss%20long-range%20dependencies%20and%20a%20%22single-pass%22%20inference%20paradigm%20that%20leads%20to%20error%20accumulation.%20To%20address%20these%20bottlenecks%2C%20we%20propose%20RIGA-Fold%2C%20a%20framework%20that%20synergizes%20Recurrent%20Interaction%20with%20Geometric%20Awareness.%20At%20the%20micro-level%2C%20we%20introduce%20a%20Geometric%20Attention%20Update%20%28GAU%29%20module%20where%20edge%20features%20explicitly%20serve%20as%20attention%20keys%2C%20ensuring%20strictly%20SE%283%29-invariant%20local%20encoding.%20At%20the%20macro-level%2C%20we%20design%20an%20attention-based%20Global%20Context%20Bridge%20that%20acts%20as%20a%20soft%20gating%20mechanism%20to%20dynamically%20inject%20global%20topological%20information.%20Furthermore%2C%20to%20bridge%20the%20gap%20between%20structural%20and%20sequence%20modalities%2C%20we%20introduce%20an%20enhanced%20variant%2C%20RIGA-Fold%2A%2C%20which%20integrates%20trainable%20geometric%20features%20with%20frozen%20evolutionary%20priors%20from%20ESM-2%20and%20ESM-IF%20via%20a%20dual-stream%20architecture.%20Finally%2C%20a%20biologically%20inspired%20%60%60predict-recycle-refine%27%27%20strategy%20is%20implemented%20to%20iteratively%20denoise%20sequence%20distributions.%20Extensive%20experiments%20on%20CATH%204.2%2C%20TS50%2C%20and%20TS500%20benchmarks%20demonstrate%20that%20our%20geometric%20framework%20is%20highly%20competitive%2C%20while%20RIGA-Fold%2A%20significantly%20outperforms%20state-of-the-art%20baselines%20in%20both%20sequence%20recovery%20and%20structural%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIGA-Fold%253A%2520A%2520General%2520Framework%2520for%2520Protein%2520Inverse%2520Folding%2520via%2520Recurrent%2520Interaction%2520and%2520Geometric%2520Awareness%26entry.906535625%3DSisi%2520Yuan%2520and%2520Jiehuang%2520Chen%2520and%2520Junchuang%2520Cai%2520and%2520Dong%2520Xu%2520and%2520Xueliang%2520Li%2520and%2520Zexuan%2520Zhu%2520and%2520Junkai%2520Ji%26entry.1292438233%3DProtein%2520inverse%2520folding%252C%2520the%2520task%2520of%2520predicting%2520amino%2520acid%2520sequences%2520for%2520desired%2520structures%252C%2520is%2520pivotal%2520for%2520de%2520novo%2520protein%2520design.%2520However%252C%2520existing%2520GNN-based%2520methods%2520typically%2520suffer%2520from%2520restricted%2520receptive%2520fields%2520that%2520miss%2520long-range%2520dependencies%2520and%2520a%2520%2522single-pass%2522%2520inference%2520paradigm%2520that%2520leads%2520to%2520error%2520accumulation.%2520To%2520address%2520these%2520bottlenecks%252C%2520we%2520propose%2520RIGA-Fold%252C%2520a%2520framework%2520that%2520synergizes%2520Recurrent%2520Interaction%2520with%2520Geometric%2520Awareness.%2520At%2520the%2520micro-level%252C%2520we%2520introduce%2520a%2520Geometric%2520Attention%2520Update%2520%2528GAU%2529%2520module%2520where%2520edge%2520features%2520explicitly%2520serve%2520as%2520attention%2520keys%252C%2520ensuring%2520strictly%2520SE%25283%2529-invariant%2520local%2520encoding.%2520At%2520the%2520macro-level%252C%2520we%2520design%2520an%2520attention-based%2520Global%2520Context%2520Bridge%2520that%2520acts%2520as%2520a%2520soft%2520gating%2520mechanism%2520to%2520dynamically%2520inject%2520global%2520topological%2520information.%2520Furthermore%252C%2520to%2520bridge%2520the%2520gap%2520between%2520structural%2520and%2520sequence%2520modalities%252C%2520we%2520introduce%2520an%2520enhanced%2520variant%252C%2520RIGA-Fold%252A%252C%2520which%2520integrates%2520trainable%2520geometric%2520features%2520with%2520frozen%2520evolutionary%2520priors%2520from%2520ESM-2%2520and%2520ESM-IF%2520via%2520a%2520dual-stream%2520architecture.%2520Finally%252C%2520a%2520biologically%2520inspired%2520%2560%2560predict-recycle-refine%2527%2527%2520strategy%2520is%2520implemented%2520to%2520iteratively%2520denoise%2520sequence%2520distributions.%2520Extensive%2520experiments%2520on%2520CATH%25204.2%252C%2520TS50%252C%2520and%2520TS500%2520benchmarks%2520demonstrate%2520that%2520our%2520geometric%2520framework%2520is%2520highly%2520competitive%252C%2520while%2520RIGA-Fold%252A%2520significantly%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520sequence%2520recovery%2520and%2520structural%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIGA-Fold%3A%20A%20General%20Framework%20for%20Protein%20Inverse%20Folding%20via%20Recurrent%20Interaction%20and%20Geometric%20Awareness&entry.906535625=Sisi%20Yuan%20and%20Jiehuang%20Chen%20and%20Junchuang%20Cai%20and%20Dong%20Xu%20and%20Xueliang%20Li%20and%20Zexuan%20Zhu%20and%20Junkai%20Ji&entry.1292438233=Protein%20inverse%20folding%2C%20the%20task%20of%20predicting%20amino%20acid%20sequences%20for%20desired%20structures%2C%20is%20pivotal%20for%20de%20novo%20protein%20design.%20However%2C%20existing%20GNN-based%20methods%20typically%20suffer%20from%20restricted%20receptive%20fields%20that%20miss%20long-range%20dependencies%20and%20a%20%22single-pass%22%20inference%20paradigm%20that%20leads%20to%20error%20accumulation.%20To%20address%20these%20bottlenecks%2C%20we%20propose%20RIGA-Fold%2C%20a%20framework%20that%20synergizes%20Recurrent%20Interaction%20with%20Geometric%20Awareness.%20At%20the%20micro-level%2C%20we%20introduce%20a%20Geometric%20Attention%20Update%20%28GAU%29%20module%20where%20edge%20features%20explicitly%20serve%20as%20attention%20keys%2C%20ensuring%20strictly%20SE%283%29-invariant%20local%20encoding.%20At%20the%20macro-level%2C%20we%20design%20an%20attention-based%20Global%20Context%20Bridge%20that%20acts%20as%20a%20soft%20gating%20mechanism%20to%20dynamically%20inject%20global%20topological%20information.%20Furthermore%2C%20to%20bridge%20the%20gap%20between%20structural%20and%20sequence%20modalities%2C%20we%20introduce%20an%20enhanced%20variant%2C%20RIGA-Fold%2A%2C%20which%20integrates%20trainable%20geometric%20features%20with%20frozen%20evolutionary%20priors%20from%20ESM-2%20and%20ESM-IF%20via%20a%20dual-stream%20architecture.%20Finally%2C%20a%20biologically%20inspired%20%60%60predict-recycle-refine%27%27%20strategy%20is%20implemented%20to%20iteratively%20denoise%20sequence%20distributions.%20Extensive%20experiments%20on%20CATH%204.2%2C%20TS50%2C%20and%20TS500%20benchmarks%20demonstrate%20that%20our%20geometric%20framework%20is%20highly%20competitive%2C%20while%20RIGA-Fold%2A%20significantly%20outperforms%20state-of-the-art%20baselines%20in%20both%20sequence%20recovery%20and%20structural%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2602.04637v1&entry.124074799=Read"},
{"title": "Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery", "author": "Jizhou Han and Shaokun Wang and Yuhang He and Chenhao Ding and Qiang Wang and Xinyuan Gao and SongLin Dong and Yihong Gong", "abstract": "Generalized Category Discovery (GCD) focuses on classifying known categories while simultaneously discovering novel categories from unlabeled data. However, previous GCD methods face challenges due to inconsistent optimization objectives and category confusion. This leads to feature overlap and ultimately hinders performance on novel categories. To address these issues, we propose the Neural Collapse-inspired Generalized Category Discovery (NC-GCD) framework. By pre-assigning and fixing Equiangular Tight Frame (ETF) prototypes, our method ensures an optimal geometric structure and a consistent optimization objective for both known and novel categories. We introduce a Consistent ETF Alignment Loss that unifies supervised and unsupervised ETF alignment and enhances category separability. Additionally, a Semantic Consistency Matcher (SCM) is designed to maintain stable and consistent label assignments across clustering iterations. Our method achieves strong performance on multiple GCD benchmarks, significantly enhancing novel category accuracy and demonstrating its effectiveness.", "link": "http://arxiv.org/abs/2507.04725v3", "date": "2026-02-04", "relevancy": 2.5193, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5038}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Supervised-Unsupervised%20Alignment%20for%20Generalized%20Category%20Discovery&body=Title%3A%20Consistent%20Supervised-Unsupervised%20Alignment%20for%20Generalized%20Category%20Discovery%0AAuthor%3A%20Jizhou%20Han%20and%20Shaokun%20Wang%20and%20Yuhang%20He%20and%20Chenhao%20Ding%20and%20Qiang%20Wang%20and%20Xinyuan%20Gao%20and%20SongLin%20Dong%20and%20Yihong%20Gong%0AAbstract%3A%20Generalized%20Category%20Discovery%20%28GCD%29%20focuses%20on%20classifying%20known%20categories%20while%20simultaneously%20discovering%20novel%20categories%20from%20unlabeled%20data.%20However%2C%20previous%20GCD%20methods%20face%20challenges%20due%20to%20inconsistent%20optimization%20objectives%20and%20category%20confusion.%20This%20leads%20to%20feature%20overlap%20and%20ultimately%20hinders%20performance%20on%20novel%20categories.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Neural%20Collapse-inspired%20Generalized%20Category%20Discovery%20%28NC-GCD%29%20framework.%20By%20pre-assigning%20and%20fixing%20Equiangular%20Tight%20Frame%20%28ETF%29%20prototypes%2C%20our%20method%20ensures%20an%20optimal%20geometric%20structure%20and%20a%20consistent%20optimization%20objective%20for%20both%20known%20and%20novel%20categories.%20We%20introduce%20a%20Consistent%20ETF%20Alignment%20Loss%20that%20unifies%20supervised%20and%20unsupervised%20ETF%20alignment%20and%20enhances%20category%20separability.%20Additionally%2C%20a%20Semantic%20Consistency%20Matcher%20%28SCM%29%20is%20designed%20to%20maintain%20stable%20and%20consistent%20label%20assignments%20across%20clustering%20iterations.%20Our%20method%20achieves%20strong%20performance%20on%20multiple%20GCD%20benchmarks%2C%20significantly%20enhancing%20novel%20category%20accuracy%20and%20demonstrating%20its%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04725v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Supervised-Unsupervised%2520Alignment%2520for%2520Generalized%2520Category%2520Discovery%26entry.906535625%3DJizhou%2520Han%2520and%2520Shaokun%2520Wang%2520and%2520Yuhang%2520He%2520and%2520Chenhao%2520Ding%2520and%2520Qiang%2520Wang%2520and%2520Xinyuan%2520Gao%2520and%2520SongLin%2520Dong%2520and%2520Yihong%2520Gong%26entry.1292438233%3DGeneralized%2520Category%2520Discovery%2520%2528GCD%2529%2520focuses%2520on%2520classifying%2520known%2520categories%2520while%2520simultaneously%2520discovering%2520novel%2520categories%2520from%2520unlabeled%2520data.%2520However%252C%2520previous%2520GCD%2520methods%2520face%2520challenges%2520due%2520to%2520inconsistent%2520optimization%2520objectives%2520and%2520category%2520confusion.%2520This%2520leads%2520to%2520feature%2520overlap%2520and%2520ultimately%2520hinders%2520performance%2520on%2520novel%2520categories.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520Neural%2520Collapse-inspired%2520Generalized%2520Category%2520Discovery%2520%2528NC-GCD%2529%2520framework.%2520By%2520pre-assigning%2520and%2520fixing%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520prototypes%252C%2520our%2520method%2520ensures%2520an%2520optimal%2520geometric%2520structure%2520and%2520a%2520consistent%2520optimization%2520objective%2520for%2520both%2520known%2520and%2520novel%2520categories.%2520We%2520introduce%2520a%2520Consistent%2520ETF%2520Alignment%2520Loss%2520that%2520unifies%2520supervised%2520and%2520unsupervised%2520ETF%2520alignment%2520and%2520enhances%2520category%2520separability.%2520Additionally%252C%2520a%2520Semantic%2520Consistency%2520Matcher%2520%2528SCM%2529%2520is%2520designed%2520to%2520maintain%2520stable%2520and%2520consistent%2520label%2520assignments%2520across%2520clustering%2520iterations.%2520Our%2520method%2520achieves%2520strong%2520performance%2520on%2520multiple%2520GCD%2520benchmarks%252C%2520significantly%2520enhancing%2520novel%2520category%2520accuracy%2520and%2520demonstrating%2520its%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04725v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Supervised-Unsupervised%20Alignment%20for%20Generalized%20Category%20Discovery&entry.906535625=Jizhou%20Han%20and%20Shaokun%20Wang%20and%20Yuhang%20He%20and%20Chenhao%20Ding%20and%20Qiang%20Wang%20and%20Xinyuan%20Gao%20and%20SongLin%20Dong%20and%20Yihong%20Gong&entry.1292438233=Generalized%20Category%20Discovery%20%28GCD%29%20focuses%20on%20classifying%20known%20categories%20while%20simultaneously%20discovering%20novel%20categories%20from%20unlabeled%20data.%20However%2C%20previous%20GCD%20methods%20face%20challenges%20due%20to%20inconsistent%20optimization%20objectives%20and%20category%20confusion.%20This%20leads%20to%20feature%20overlap%20and%20ultimately%20hinders%20performance%20on%20novel%20categories.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Neural%20Collapse-inspired%20Generalized%20Category%20Discovery%20%28NC-GCD%29%20framework.%20By%20pre-assigning%20and%20fixing%20Equiangular%20Tight%20Frame%20%28ETF%29%20prototypes%2C%20our%20method%20ensures%20an%20optimal%20geometric%20structure%20and%20a%20consistent%20optimization%20objective%20for%20both%20known%20and%20novel%20categories.%20We%20introduce%20a%20Consistent%20ETF%20Alignment%20Loss%20that%20unifies%20supervised%20and%20unsupervised%20ETF%20alignment%20and%20enhances%20category%20separability.%20Additionally%2C%20a%20Semantic%20Consistency%20Matcher%20%28SCM%29%20is%20designed%20to%20maintain%20stable%20and%20consistent%20label%20assignments%20across%20clustering%20iterations.%20Our%20method%20achieves%20strong%20performance%20on%20multiple%20GCD%20benchmarks%2C%20significantly%20enhancing%20novel%20category%20accuracy%20and%20demonstrating%20its%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2507.04725v3&entry.124074799=Read"},
{"title": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image", "author": "Yan Chen and Jie Peng and Moajjem Hossain Chowdhury and Tianlong Chen and Yunmei Liu", "abstract": "Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\\%$ in F1 score and reductions of $88\\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.", "link": "http://arxiv.org/abs/2602.04769v1", "date": "2026-02-04", "relevancy": 2.4884, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroCanvas%3A%20VLLM-Powered%20Robust%20Seizure%20Detection%20by%20Reformulating%20Multichannel%20EEG%20as%20Image&body=Title%3A%20NeuroCanvas%3A%20VLLM-Powered%20Robust%20Seizure%20Detection%20by%20Reformulating%20Multichannel%20EEG%20as%20Image%0AAuthor%3A%20Yan%20Chen%20and%20Jie%20Peng%20and%20Moajjem%20Hossain%20Chowdhury%20and%20Tianlong%20Chen%20and%20Yunmei%20Liu%0AAbstract%3A%20Accurate%20and%20timely%20seizure%20detection%20from%20Electroencephalography%20%28EEG%29%20is%20critical%20for%20clinical%20intervention%2C%20yet%20manual%20review%20of%20long-term%20recordings%20is%20labor-intensive.%20Recent%20efforts%20to%20encode%20EEG%20signals%20into%20large%20language%20models%20%28LLMs%29%20show%20promise%20in%20handling%20neural%20signals%20across%20diverse%20patients%2C%20but%20two%20significant%20challenges%20remain%3A%20%281%29%20multi-channel%20heterogeneity%2C%20as%20seizure-relevant%20information%20varies%20substantially%20across%20EEG%20channels%2C%20and%20%282%29%20computing%20inefficiency%2C%20as%20the%20EEG%20signals%20need%20to%20be%20encoded%20into%20a%20massive%20number%20of%20tokens%20for%20the%20prediction.%20To%20address%20these%20issues%2C%20we%20draw%20the%20EEG%20signal%20and%20propose%20the%20novel%20NeuroCanvas%20framework.%20Specifically%2C%20NeuroCanvas%20consists%20of%20two%20modules%3A%20%28i%29%20The%20Entropy-guided%20Channel%20Selector%20%28ECS%29%20selects%20the%20seizure-relevant%20channels%20input%20to%20LLM%20and%20%28ii%29%20the%20following%20Canvas%20of%20Neuron%20Signal%20%28CNS%29%20converts%20selected%20multi-channel%20heterogeneous%20EEG%20signals%20into%20structured%20visual%20representations.%20The%20ECS%20module%20alleviates%20the%20multi-channel%20heterogeneity%20issue%2C%20and%20the%20CNS%20uses%20compact%20visual%20tokens%20to%20represent%20the%20EEG%20signals%20that%20improve%20the%20computing%20efficiency.%20We%20evaluate%20NeuroCanvas%20across%20multiple%20seizure%20detection%20datasets%2C%20demonstrating%20a%20significant%20improvement%20of%20%2420%5C%25%24%20in%20F1%20score%20and%20reductions%20of%20%2488%5C%25%24%20in%20inference%20latency.%20These%20results%20highlight%20NeuroCanvas%20as%20a%20scalable%20and%20effective%20solution%20for%20real-time%20and%20resource-efficient%20seizure%20detection%20in%20clinical%20practice.The%20code%20will%20be%20released%20at%20https%3A//github.com/Yanchen30247/seizure_detect.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroCanvas%253A%2520VLLM-Powered%2520Robust%2520Seizure%2520Detection%2520by%2520Reformulating%2520Multichannel%2520EEG%2520as%2520Image%26entry.906535625%3DYan%2520Chen%2520and%2520Jie%2520Peng%2520and%2520Moajjem%2520Hossain%2520Chowdhury%2520and%2520Tianlong%2520Chen%2520and%2520Yunmei%2520Liu%26entry.1292438233%3DAccurate%2520and%2520timely%2520seizure%2520detection%2520from%2520Electroencephalography%2520%2528EEG%2529%2520is%2520critical%2520for%2520clinical%2520intervention%252C%2520yet%2520manual%2520review%2520of%2520long-term%2520recordings%2520is%2520labor-intensive.%2520Recent%2520efforts%2520to%2520encode%2520EEG%2520signals%2520into%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520promise%2520in%2520handling%2520neural%2520signals%2520across%2520diverse%2520patients%252C%2520but%2520two%2520significant%2520challenges%2520remain%253A%2520%25281%2529%2520multi-channel%2520heterogeneity%252C%2520as%2520seizure-relevant%2520information%2520varies%2520substantially%2520across%2520EEG%2520channels%252C%2520and%2520%25282%2529%2520computing%2520inefficiency%252C%2520as%2520the%2520EEG%2520signals%2520need%2520to%2520be%2520encoded%2520into%2520a%2520massive%2520number%2520of%2520tokens%2520for%2520the%2520prediction.%2520To%2520address%2520these%2520issues%252C%2520we%2520draw%2520the%2520EEG%2520signal%2520and%2520propose%2520the%2520novel%2520NeuroCanvas%2520framework.%2520Specifically%252C%2520NeuroCanvas%2520consists%2520of%2520two%2520modules%253A%2520%2528i%2529%2520The%2520Entropy-guided%2520Channel%2520Selector%2520%2528ECS%2529%2520selects%2520the%2520seizure-relevant%2520channels%2520input%2520to%2520LLM%2520and%2520%2528ii%2529%2520the%2520following%2520Canvas%2520of%2520Neuron%2520Signal%2520%2528CNS%2529%2520converts%2520selected%2520multi-channel%2520heterogeneous%2520EEG%2520signals%2520into%2520structured%2520visual%2520representations.%2520The%2520ECS%2520module%2520alleviates%2520the%2520multi-channel%2520heterogeneity%2520issue%252C%2520and%2520the%2520CNS%2520uses%2520compact%2520visual%2520tokens%2520to%2520represent%2520the%2520EEG%2520signals%2520that%2520improve%2520the%2520computing%2520efficiency.%2520We%2520evaluate%2520NeuroCanvas%2520across%2520multiple%2520seizure%2520detection%2520datasets%252C%2520demonstrating%2520a%2520significant%2520improvement%2520of%2520%252420%255C%2525%2524%2520in%2520F1%2520score%2520and%2520reductions%2520of%2520%252488%255C%2525%2524%2520in%2520inference%2520latency.%2520These%2520results%2520highlight%2520NeuroCanvas%2520as%2520a%2520scalable%2520and%2520effective%2520solution%2520for%2520real-time%2520and%2520resource-efficient%2520seizure%2520detection%2520in%2520clinical%2520practice.The%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/Yanchen30247/seizure_detect.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroCanvas%3A%20VLLM-Powered%20Robust%20Seizure%20Detection%20by%20Reformulating%20Multichannel%20EEG%20as%20Image&entry.906535625=Yan%20Chen%20and%20Jie%20Peng%20and%20Moajjem%20Hossain%20Chowdhury%20and%20Tianlong%20Chen%20and%20Yunmei%20Liu&entry.1292438233=Accurate%20and%20timely%20seizure%20detection%20from%20Electroencephalography%20%28EEG%29%20is%20critical%20for%20clinical%20intervention%2C%20yet%20manual%20review%20of%20long-term%20recordings%20is%20labor-intensive.%20Recent%20efforts%20to%20encode%20EEG%20signals%20into%20large%20language%20models%20%28LLMs%29%20show%20promise%20in%20handling%20neural%20signals%20across%20diverse%20patients%2C%20but%20two%20significant%20challenges%20remain%3A%20%281%29%20multi-channel%20heterogeneity%2C%20as%20seizure-relevant%20information%20varies%20substantially%20across%20EEG%20channels%2C%20and%20%282%29%20computing%20inefficiency%2C%20as%20the%20EEG%20signals%20need%20to%20be%20encoded%20into%20a%20massive%20number%20of%20tokens%20for%20the%20prediction.%20To%20address%20these%20issues%2C%20we%20draw%20the%20EEG%20signal%20and%20propose%20the%20novel%20NeuroCanvas%20framework.%20Specifically%2C%20NeuroCanvas%20consists%20of%20two%20modules%3A%20%28i%29%20The%20Entropy-guided%20Channel%20Selector%20%28ECS%29%20selects%20the%20seizure-relevant%20channels%20input%20to%20LLM%20and%20%28ii%29%20the%20following%20Canvas%20of%20Neuron%20Signal%20%28CNS%29%20converts%20selected%20multi-channel%20heterogeneous%20EEG%20signals%20into%20structured%20visual%20representations.%20The%20ECS%20module%20alleviates%20the%20multi-channel%20heterogeneity%20issue%2C%20and%20the%20CNS%20uses%20compact%20visual%20tokens%20to%20represent%20the%20EEG%20signals%20that%20improve%20the%20computing%20efficiency.%20We%20evaluate%20NeuroCanvas%20across%20multiple%20seizure%20detection%20datasets%2C%20demonstrating%20a%20significant%20improvement%20of%20%2420%5C%25%24%20in%20F1%20score%20and%20reductions%20of%20%2488%5C%25%24%20in%20inference%20latency.%20These%20results%20highlight%20NeuroCanvas%20as%20a%20scalable%20and%20effective%20solution%20for%20real-time%20and%20resource-efficient%20seizure%20detection%20in%20clinical%20practice.The%20code%20will%20be%20released%20at%20https%3A//github.com/Yanchen30247/seizure_detect.&entry.1838667208=http%3A//arxiv.org/abs/2602.04769v1&entry.124074799=Read"},
{"title": "Relational Scene Graphs for Object Grounding of Natural Language Commands", "author": "Julia Kuhn and Francesco Verdoja and Tsvetomila Mihaylova and Ville Kyrki", "abstract": "Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.", "link": "http://arxiv.org/abs/2602.04635v1", "date": "2026-02-04", "relevancy": 2.4831, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relational%20Scene%20Graphs%20for%20Object%20Grounding%20of%20Natural%20Language%20Commands&body=Title%3A%20Relational%20Scene%20Graphs%20for%20Object%20Grounding%20of%20Natural%20Language%20Commands%0AAuthor%3A%20Julia%20Kuhn%20and%20Francesco%20Verdoja%20and%20Tsvetomila%20Mihaylova%20and%20Ville%20Kyrki%0AAbstract%3A%20Robots%20are%20finding%20wider%20adoption%20in%20human%20environments%2C%20increasing%20the%20need%20for%20natural%20human-robot%20interaction.%20However%2C%20understanding%20a%20natural%20language%20command%20requires%20the%20robot%20to%20infer%20the%20intended%20task%20and%20how%20to%20decompose%20it%20into%20executable%20actions%2C%20and%20to%20ground%20those%20actions%20in%20the%20robot%27s%20knowledge%20of%20the%20environment%2C%20including%20relevant%20objects%2C%20agents%2C%20and%20locations.%20This%20challenge%20can%20be%20addressed%20by%20combining%20the%20capabilities%20of%20Large%20language%20models%20%28LLMs%29%20to%20understand%20natural%20language%20with%203D%20scene%20graphs%20%283DSGs%29%20for%20grounding%20inferred%20actions%20in%20a%20semantic%20representation%20of%20the%20environment.%20However%2C%20many%203DSGs%20lack%20explicit%20spatial%20relations%20between%20objects%2C%20even%20though%20humans%20often%20rely%20on%20these%20relations%20to%20describe%20an%20environment.%20This%20paper%20investigates%20whether%20incorporating%20open-%20or%20closed-vocabulary%20spatial%20relations%20into%203DSGs%20can%20improve%20the%20ability%20of%20LLMs%20to%20interpret%20natural%20language%20commands.%20To%20address%20this%2C%20we%20propose%20an%20LLM-based%20pipeline%20for%20target%20object%20grounding%20from%20open-vocabulary%20language%20commands%20and%20a%20vision%20language%20model%20%28VLM%29-based%20pipeline%20to%20add%20open-vocabulary%20spatial%20edges%20to%203DSGs%20from%20images%20captured%20while%20mapping.%20Finally%2C%20two%20LLMs%20are%20evaluated%20in%20a%20study%20assessing%20their%20performance%20on%20the%20downstream%20task%20of%20target%20object%20grounding.%20Our%20study%20demonstrates%20that%20explicit%20spatial%20relations%20improve%20the%20ability%20of%20LLMs%20to%20ground%20objects.%20Moreover%2C%20open-vocabulary%20relation%20generation%20with%20VLMs%20proves%20feasible%20from%20robot-captured%20images%2C%20but%20their%20advantage%20over%20closed-vocabulary%20relations%20is%20found%20to%20be%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelational%2520Scene%2520Graphs%2520for%2520Object%2520Grounding%2520of%2520Natural%2520Language%2520Commands%26entry.906535625%3DJulia%2520Kuhn%2520and%2520Francesco%2520Verdoja%2520and%2520Tsvetomila%2520Mihaylova%2520and%2520Ville%2520Kyrki%26entry.1292438233%3DRobots%2520are%2520finding%2520wider%2520adoption%2520in%2520human%2520environments%252C%2520increasing%2520the%2520need%2520for%2520natural%2520human-robot%2520interaction.%2520However%252C%2520understanding%2520a%2520natural%2520language%2520command%2520requires%2520the%2520robot%2520to%2520infer%2520the%2520intended%2520task%2520and%2520how%2520to%2520decompose%2520it%2520into%2520executable%2520actions%252C%2520and%2520to%2520ground%2520those%2520actions%2520in%2520the%2520robot%2527s%2520knowledge%2520of%2520the%2520environment%252C%2520including%2520relevant%2520objects%252C%2520agents%252C%2520and%2520locations.%2520This%2520challenge%2520can%2520be%2520addressed%2520by%2520combining%2520the%2520capabilities%2520of%2520Large%2520language%2520models%2520%2528LLMs%2529%2520to%2520understand%2520natural%2520language%2520with%25203D%2520scene%2520graphs%2520%25283DSGs%2529%2520for%2520grounding%2520inferred%2520actions%2520in%2520a%2520semantic%2520representation%2520of%2520the%2520environment.%2520However%252C%2520many%25203DSGs%2520lack%2520explicit%2520spatial%2520relations%2520between%2520objects%252C%2520even%2520though%2520humans%2520often%2520rely%2520on%2520these%2520relations%2520to%2520describe%2520an%2520environment.%2520This%2520paper%2520investigates%2520whether%2520incorporating%2520open-%2520or%2520closed-vocabulary%2520spatial%2520relations%2520into%25203DSGs%2520can%2520improve%2520the%2520ability%2520of%2520LLMs%2520to%2520interpret%2520natural%2520language%2520commands.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520LLM-based%2520pipeline%2520for%2520target%2520object%2520grounding%2520from%2520open-vocabulary%2520language%2520commands%2520and%2520a%2520vision%2520language%2520model%2520%2528VLM%2529-based%2520pipeline%2520to%2520add%2520open-vocabulary%2520spatial%2520edges%2520to%25203DSGs%2520from%2520images%2520captured%2520while%2520mapping.%2520Finally%252C%2520two%2520LLMs%2520are%2520evaluated%2520in%2520a%2520study%2520assessing%2520their%2520performance%2520on%2520the%2520downstream%2520task%2520of%2520target%2520object%2520grounding.%2520Our%2520study%2520demonstrates%2520that%2520explicit%2520spatial%2520relations%2520improve%2520the%2520ability%2520of%2520LLMs%2520to%2520ground%2520objects.%2520Moreover%252C%2520open-vocabulary%2520relation%2520generation%2520with%2520VLMs%2520proves%2520feasible%2520from%2520robot-captured%2520images%252C%2520but%2520their%2520advantage%2520over%2520closed-vocabulary%2520relations%2520is%2520found%2520to%2520be%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relational%20Scene%20Graphs%20for%20Object%20Grounding%20of%20Natural%20Language%20Commands&entry.906535625=Julia%20Kuhn%20and%20Francesco%20Verdoja%20and%20Tsvetomila%20Mihaylova%20and%20Ville%20Kyrki&entry.1292438233=Robots%20are%20finding%20wider%20adoption%20in%20human%20environments%2C%20increasing%20the%20need%20for%20natural%20human-robot%20interaction.%20However%2C%20understanding%20a%20natural%20language%20command%20requires%20the%20robot%20to%20infer%20the%20intended%20task%20and%20how%20to%20decompose%20it%20into%20executable%20actions%2C%20and%20to%20ground%20those%20actions%20in%20the%20robot%27s%20knowledge%20of%20the%20environment%2C%20including%20relevant%20objects%2C%20agents%2C%20and%20locations.%20This%20challenge%20can%20be%20addressed%20by%20combining%20the%20capabilities%20of%20Large%20language%20models%20%28LLMs%29%20to%20understand%20natural%20language%20with%203D%20scene%20graphs%20%283DSGs%29%20for%20grounding%20inferred%20actions%20in%20a%20semantic%20representation%20of%20the%20environment.%20However%2C%20many%203DSGs%20lack%20explicit%20spatial%20relations%20between%20objects%2C%20even%20though%20humans%20often%20rely%20on%20these%20relations%20to%20describe%20an%20environment.%20This%20paper%20investigates%20whether%20incorporating%20open-%20or%20closed-vocabulary%20spatial%20relations%20into%203DSGs%20can%20improve%20the%20ability%20of%20LLMs%20to%20interpret%20natural%20language%20commands.%20To%20address%20this%2C%20we%20propose%20an%20LLM-based%20pipeline%20for%20target%20object%20grounding%20from%20open-vocabulary%20language%20commands%20and%20a%20vision%20language%20model%20%28VLM%29-based%20pipeline%20to%20add%20open-vocabulary%20spatial%20edges%20to%203DSGs%20from%20images%20captured%20while%20mapping.%20Finally%2C%20two%20LLMs%20are%20evaluated%20in%20a%20study%20assessing%20their%20performance%20on%20the%20downstream%20task%20of%20target%20object%20grounding.%20Our%20study%20demonstrates%20that%20explicit%20spatial%20relations%20improve%20the%20ability%20of%20LLMs%20to%20ground%20objects.%20Moreover%2C%20open-vocabulary%20relation%20generation%20with%20VLMs%20proves%20feasible%20from%20robot-captured%20images%2C%20but%20their%20advantage%20over%20closed-vocabulary%20relations%20is%20found%20to%20be%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2602.04635v1&entry.124074799=Read"},
{"title": "Protein Autoregressive Modeling via Multiscale Structure Generation", "author": "Yanru Qu and Cheng-Yen Hsieh and Zaixiang Zheng and Ge Liu and Quanquan Gu", "abstract": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.", "link": "http://arxiv.org/abs/2602.04883v1", "date": "2026-02-04", "relevancy": 2.4666, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4934}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protein%20Autoregressive%20Modeling%20via%20Multiscale%20Structure%20Generation&body=Title%3A%20Protein%20Autoregressive%20Modeling%20via%20Multiscale%20Structure%20Generation%0AAuthor%3A%20Yanru%20Qu%20and%20Cheng-Yen%20Hsieh%20and%20Zaixiang%20Zheng%20and%20Ge%20Liu%20and%20Quanquan%20Gu%0AAbstract%3A%20We%20present%20protein%20autoregressive%20modeling%20%28PAR%29%2C%20the%20first%20multi-scale%20autoregressive%20framework%20for%20protein%20backbone%20generation%20via%20coarse-to-fine%20next-scale%20prediction.%20Using%20the%20hierarchical%20nature%20of%20proteins%2C%20PAR%20generates%20structures%20that%20mimic%20sculpting%20a%20statue%2C%20forming%20a%20coarse%20topology%20and%20refining%20structural%20details%20over%20scales.%20To%20achieve%20this%2C%20PAR%20consists%20of%20three%20key%20components%3A%20%28i%29%20multi-scale%20downsampling%20operations%20that%20represent%20protein%20structures%20across%20multiple%20scales%20during%20training%3B%20%28ii%29%20an%20autoregressive%20transformer%20that%20encodes%20multi-scale%20information%20and%20produces%20conditional%20embeddings%20to%20guide%20structure%20generation%3B%20%28iii%29%20a%20flow-based%20backbone%20decoder%20that%20generates%20backbone%20atoms%20conditioned%20on%20these%20embeddings.%20Moreover%2C%20autoregressive%20models%20suffer%20from%20exposure%20bias%2C%20caused%20by%20the%20training%20and%20the%20generation%20procedure%20mismatch%2C%20and%20substantially%20degrades%20structure%20generation%20quality.%20We%20effectively%20alleviate%20this%20issue%20by%20adopting%20noisy%20context%20learning%20and%20scheduled%20sampling%2C%20enabling%20robust%20backbone%20generation.%20Notably%2C%20PAR%20exhibits%20strong%20zero-shot%20generalization%2C%20supporting%20flexible%20human-prompted%20conditional%20generation%20and%20motif%20scaffolding%20without%20requiring%20fine-tuning.%20On%20the%20unconditional%20generation%20benchmark%2C%20PAR%20effectively%20learns%20protein%20distributions%20and%20produces%20backbones%20of%20high%20design%20quality%2C%20and%20exhibits%20favorable%20scaling%20behavior.%20Together%2C%20these%20properties%20establish%20PAR%20as%20a%20promising%20framework%20for%20protein%20structure%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtein%2520Autoregressive%2520Modeling%2520via%2520Multiscale%2520Structure%2520Generation%26entry.906535625%3DYanru%2520Qu%2520and%2520Cheng-Yen%2520Hsieh%2520and%2520Zaixiang%2520Zheng%2520and%2520Ge%2520Liu%2520and%2520Quanquan%2520Gu%26entry.1292438233%3DWe%2520present%2520protein%2520autoregressive%2520modeling%2520%2528PAR%2529%252C%2520the%2520first%2520multi-scale%2520autoregressive%2520framework%2520for%2520protein%2520backbone%2520generation%2520via%2520coarse-to-fine%2520next-scale%2520prediction.%2520Using%2520the%2520hierarchical%2520nature%2520of%2520proteins%252C%2520PAR%2520generates%2520structures%2520that%2520mimic%2520sculpting%2520a%2520statue%252C%2520forming%2520a%2520coarse%2520topology%2520and%2520refining%2520structural%2520details%2520over%2520scales.%2520To%2520achieve%2520this%252C%2520PAR%2520consists%2520of%2520three%2520key%2520components%253A%2520%2528i%2529%2520multi-scale%2520downsampling%2520operations%2520that%2520represent%2520protein%2520structures%2520across%2520multiple%2520scales%2520during%2520training%253B%2520%2528ii%2529%2520an%2520autoregressive%2520transformer%2520that%2520encodes%2520multi-scale%2520information%2520and%2520produces%2520conditional%2520embeddings%2520to%2520guide%2520structure%2520generation%253B%2520%2528iii%2529%2520a%2520flow-based%2520backbone%2520decoder%2520that%2520generates%2520backbone%2520atoms%2520conditioned%2520on%2520these%2520embeddings.%2520Moreover%252C%2520autoregressive%2520models%2520suffer%2520from%2520exposure%2520bias%252C%2520caused%2520by%2520the%2520training%2520and%2520the%2520generation%2520procedure%2520mismatch%252C%2520and%2520substantially%2520degrades%2520structure%2520generation%2520quality.%2520We%2520effectively%2520alleviate%2520this%2520issue%2520by%2520adopting%2520noisy%2520context%2520learning%2520and%2520scheduled%2520sampling%252C%2520enabling%2520robust%2520backbone%2520generation.%2520Notably%252C%2520PAR%2520exhibits%2520strong%2520zero-shot%2520generalization%252C%2520supporting%2520flexible%2520human-prompted%2520conditional%2520generation%2520and%2520motif%2520scaffolding%2520without%2520requiring%2520fine-tuning.%2520On%2520the%2520unconditional%2520generation%2520benchmark%252C%2520PAR%2520effectively%2520learns%2520protein%2520distributions%2520and%2520produces%2520backbones%2520of%2520high%2520design%2520quality%252C%2520and%2520exhibits%2520favorable%2520scaling%2520behavior.%2520Together%252C%2520these%2520properties%2520establish%2520PAR%2520as%2520a%2520promising%2520framework%2520for%2520protein%2520structure%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protein%20Autoregressive%20Modeling%20via%20Multiscale%20Structure%20Generation&entry.906535625=Yanru%20Qu%20and%20Cheng-Yen%20Hsieh%20and%20Zaixiang%20Zheng%20and%20Ge%20Liu%20and%20Quanquan%20Gu&entry.1292438233=We%20present%20protein%20autoregressive%20modeling%20%28PAR%29%2C%20the%20first%20multi-scale%20autoregressive%20framework%20for%20protein%20backbone%20generation%20via%20coarse-to-fine%20next-scale%20prediction.%20Using%20the%20hierarchical%20nature%20of%20proteins%2C%20PAR%20generates%20structures%20that%20mimic%20sculpting%20a%20statue%2C%20forming%20a%20coarse%20topology%20and%20refining%20structural%20details%20over%20scales.%20To%20achieve%20this%2C%20PAR%20consists%20of%20three%20key%20components%3A%20%28i%29%20multi-scale%20downsampling%20operations%20that%20represent%20protein%20structures%20across%20multiple%20scales%20during%20training%3B%20%28ii%29%20an%20autoregressive%20transformer%20that%20encodes%20multi-scale%20information%20and%20produces%20conditional%20embeddings%20to%20guide%20structure%20generation%3B%20%28iii%29%20a%20flow-based%20backbone%20decoder%20that%20generates%20backbone%20atoms%20conditioned%20on%20these%20embeddings.%20Moreover%2C%20autoregressive%20models%20suffer%20from%20exposure%20bias%2C%20caused%20by%20the%20training%20and%20the%20generation%20procedure%20mismatch%2C%20and%20substantially%20degrades%20structure%20generation%20quality.%20We%20effectively%20alleviate%20this%20issue%20by%20adopting%20noisy%20context%20learning%20and%20scheduled%20sampling%2C%20enabling%20robust%20backbone%20generation.%20Notably%2C%20PAR%20exhibits%20strong%20zero-shot%20generalization%2C%20supporting%20flexible%20human-prompted%20conditional%20generation%20and%20motif%20scaffolding%20without%20requiring%20fine-tuning.%20On%20the%20unconditional%20generation%20benchmark%2C%20PAR%20effectively%20learns%20protein%20distributions%20and%20produces%20backbones%20of%20high%20design%20quality%2C%20and%20exhibits%20favorable%20scaling%20behavior.%20Together%2C%20these%20properties%20establish%20PAR%20as%20a%20promising%20framework%20for%20protein%20structure%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2602.04883v1&entry.124074799=Read"},
{"title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents", "author": "Yaorui Shi and Yuxin Chen and Siyuan Wang and Sihang Li and Hengxing Cai and Qi Gu and Xiang Wang and An Zhang", "abstract": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.", "link": "http://arxiv.org/abs/2509.23040v3", "date": "2026-02-04", "relevancy": 2.464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Back%20to%20Reason%20Forward%3A%20Revisitable%20Memory%20for%20Long-Context%20LLM%20Agents&body=Title%3A%20Look%20Back%20to%20Reason%20Forward%3A%20Revisitable%20Memory%20for%20Long-Context%20LLM%20Agents%0AAuthor%3A%20Yaorui%20Shi%20and%20Yuxin%20Chen%20and%20Siyuan%20Wang%20and%20Sihang%20Li%20and%20Hengxing%20Cai%20and%20Qi%20Gu%20and%20Xiang%20Wang%20and%20An%20Zhang%0AAbstract%3A%20Large%20language%20models%20face%20challenges%20in%20long-context%20question%20answering%2C%20where%20key%20evidence%20of%20a%20query%20may%20be%20dispersed%20across%20millions%20of%20tokens.%20Existing%20works%20equip%20large%20language%20models%20with%20a%20memory%20buffer%20that%20is%20dynamically%20updated%20via%20a%20linear%20document%20scan%2C%20also%20known%20as%20the%20%22memorize%20while%20reading%22%20methods.%20While%20this%20approach%20scales%20efficiently%2C%20it%20suffers%20from%20pruning%20of%20latent%20evidence%2C%20information%20loss%20through%20overwriting%2C%20and%20sparse%20reinforcement%20learning%20signals.%20To%20tackle%20these%20challenges%2C%20we%20present%20ReMemR1%2C%20which%20integrates%20the%20mechanism%20of%20memory%20retrieval%20into%20the%20memory%20update%20process%2C%20enabling%20the%20agent%20to%20selectively%20callback%20historical%20memories%20for%20non-linear%20reasoning.%20To%20further%20strengthen%20training%2C%20we%20propose%20a%20multi-level%20reward%20design%2C%20which%20combines%20final-answer%20rewards%20with%20dense%2C%20step-level%20signals%20that%20guide%20effective%20memory%20use.%20Together%2C%20these%20contributions%20mitigate%20information%20degradation%2C%20improve%20supervision%2C%20and%20support%20complex%20multi-hop%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20ReMemR1%20significantly%20outperforms%20state-of-the-art%20baselines%20on%20long-context%20question%20answering%20while%20incurring%20negligible%20computational%20overhead%2C%20validating%20its%20ability%20to%20trade%20marginal%20cost%20for%20robust%20long-context%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Back%2520to%2520Reason%2520Forward%253A%2520Revisitable%2520Memory%2520for%2520Long-Context%2520LLM%2520Agents%26entry.906535625%3DYaorui%2520Shi%2520and%2520Yuxin%2520Chen%2520and%2520Siyuan%2520Wang%2520and%2520Sihang%2520Li%2520and%2520Hengxing%2520Cai%2520and%2520Qi%2520Gu%2520and%2520Xiang%2520Wang%2520and%2520An%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520face%2520challenges%2520in%2520long-context%2520question%2520answering%252C%2520where%2520key%2520evidence%2520of%2520a%2520query%2520may%2520be%2520dispersed%2520across%2520millions%2520of%2520tokens.%2520Existing%2520works%2520equip%2520large%2520language%2520models%2520with%2520a%2520memory%2520buffer%2520that%2520is%2520dynamically%2520updated%2520via%2520a%2520linear%2520document%2520scan%252C%2520also%2520known%2520as%2520the%2520%2522memorize%2520while%2520reading%2522%2520methods.%2520While%2520this%2520approach%2520scales%2520efficiently%252C%2520it%2520suffers%2520from%2520pruning%2520of%2520latent%2520evidence%252C%2520information%2520loss%2520through%2520overwriting%252C%2520and%2520sparse%2520reinforcement%2520learning%2520signals.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520present%2520ReMemR1%252C%2520which%2520integrates%2520the%2520mechanism%2520of%2520memory%2520retrieval%2520into%2520the%2520memory%2520update%2520process%252C%2520enabling%2520the%2520agent%2520to%2520selectively%2520callback%2520historical%2520memories%2520for%2520non-linear%2520reasoning.%2520To%2520further%2520strengthen%2520training%252C%2520we%2520propose%2520a%2520multi-level%2520reward%2520design%252C%2520which%2520combines%2520final-answer%2520rewards%2520with%2520dense%252C%2520step-level%2520signals%2520that%2520guide%2520effective%2520memory%2520use.%2520Together%252C%2520these%2520contributions%2520mitigate%2520information%2520degradation%252C%2520improve%2520supervision%252C%2520and%2520support%2520complex%2520multi-hop%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ReMemR1%2520significantly%2520outperforms%2520state-of-the-art%2520baselines%2520on%2520long-context%2520question%2520answering%2520while%2520incurring%2520negligible%2520computational%2520overhead%252C%2520validating%2520its%2520ability%2520to%2520trade%2520marginal%2520cost%2520for%2520robust%2520long-context%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Back%20to%20Reason%20Forward%3A%20Revisitable%20Memory%20for%20Long-Context%20LLM%20Agents&entry.906535625=Yaorui%20Shi%20and%20Yuxin%20Chen%20and%20Siyuan%20Wang%20and%20Sihang%20Li%20and%20Hengxing%20Cai%20and%20Qi%20Gu%20and%20Xiang%20Wang%20and%20An%20Zhang&entry.1292438233=Large%20language%20models%20face%20challenges%20in%20long-context%20question%20answering%2C%20where%20key%20evidence%20of%20a%20query%20may%20be%20dispersed%20across%20millions%20of%20tokens.%20Existing%20works%20equip%20large%20language%20models%20with%20a%20memory%20buffer%20that%20is%20dynamically%20updated%20via%20a%20linear%20document%20scan%2C%20also%20known%20as%20the%20%22memorize%20while%20reading%22%20methods.%20While%20this%20approach%20scales%20efficiently%2C%20it%20suffers%20from%20pruning%20of%20latent%20evidence%2C%20information%20loss%20through%20overwriting%2C%20and%20sparse%20reinforcement%20learning%20signals.%20To%20tackle%20these%20challenges%2C%20we%20present%20ReMemR1%2C%20which%20integrates%20the%20mechanism%20of%20memory%20retrieval%20into%20the%20memory%20update%20process%2C%20enabling%20the%20agent%20to%20selectively%20callback%20historical%20memories%20for%20non-linear%20reasoning.%20To%20further%20strengthen%20training%2C%20we%20propose%20a%20multi-level%20reward%20design%2C%20which%20combines%20final-answer%20rewards%20with%20dense%2C%20step-level%20signals%20that%20guide%20effective%20memory%20use.%20Together%2C%20these%20contributions%20mitigate%20information%20degradation%2C%20improve%20supervision%2C%20and%20support%20complex%20multi-hop%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20ReMemR1%20significantly%20outperforms%20state-of-the-art%20baselines%20on%20long-context%20question%20answering%20while%20incurring%20negligible%20computational%20overhead%2C%20validating%20its%20ability%20to%20trade%20marginal%20cost%20for%20robust%20long-context%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2509.23040v3&entry.124074799=Read"},
{"title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce", "author": "Rahul Bajaj and Anuj Garg", "abstract": "Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.", "link": "http://arxiv.org/abs/2602.04605v1", "date": "2026-02-04", "relevancy": 2.4594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RexBERT%3A%20Context%20Specialized%20Bidirectional%20Encoders%20for%20E-commerce&body=Title%3A%20RexBERT%3A%20Context%20Specialized%20Bidirectional%20Encoders%20for%20E-commerce%0AAuthor%3A%20Rahul%20Bajaj%20and%20Anuj%20Garg%0AAbstract%3A%20Encoder-only%20transformers%20remain%20indispensable%20in%20retrieval%2C%20classification%2C%20and%20ranking%20systems%20where%20latency%2C%20stability%2C%20and%20cost%20are%20paramount.%20Most%20general%20purpose%20encoders%2C%20however%2C%20are%20trained%20on%20generic%20corpora%20with%20limited%20coverage%20of%20specialized%20domains.%20We%20introduce%20RexBERT%2C%20a%20family%20of%20BERT-style%20encoders%20designed%20specifically%20for%20e-commerce%20semantics.%20We%20make%20three%20contributions.%20First%2C%20we%20release%20Ecom-niverse%2C%20a%20350%20billion%20token%20corpus%20curated%20from%20diverse%20retail%20and%20shopping%20sources.%20We%20describe%20a%20modular%20pipeline%20that%20isolates%20and%20extracts%20e-commerce%20content%20from%20FineFineWeb%20and%20other%20open%20web%20resources%2C%20and%20characterize%20the%20resulting%20domain%20distribution.%20Second%2C%20we%20present%20a%20reproducible%20pretraining%20recipe%20building%20on%20ModernBERT%27s%20architectural%20advances.%20The%20recipe%20consists%20of%20three%20phases%3A%20general%20pre-training%2C%20context%20extension%2C%20and%20annealed%20domain%20specialization.%20Third%2C%20we%20train%20RexBERT%20models%20ranging%20from%2017M%20to%20400M%20parameters%20and%20evaluate%20them%20on%20token%20classification%2C%20semantic%20similarity%2C%20and%20general%20natural%20language%20understanding%20tasks%20using%20e-commerce%20datasets.%20Despite%20having%202-3x%20fewer%20parameters%2C%20RexBERT%20outperforms%20larger%20general-purpose%20encoders%20and%20matches%20or%20surpasses%20modern%20long-context%20models%20on%20domain-specific%20benchmarks.%20Our%20results%20demonstrate%20that%20high%20quality%20in-domain%20data%20combined%20with%20a%20principled%20training%20approach%20provides%20a%20stronger%20foundation%20for%20e-commerce%20applications%20than%20indiscriminate%20scaling%20alone.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRexBERT%253A%2520Context%2520Specialized%2520Bidirectional%2520Encoders%2520for%2520E-commerce%26entry.906535625%3DRahul%2520Bajaj%2520and%2520Anuj%2520Garg%26entry.1292438233%3DEncoder-only%2520transformers%2520remain%2520indispensable%2520in%2520retrieval%252C%2520classification%252C%2520and%2520ranking%2520systems%2520where%2520latency%252C%2520stability%252C%2520and%2520cost%2520are%2520paramount.%2520Most%2520general%2520purpose%2520encoders%252C%2520however%252C%2520are%2520trained%2520on%2520generic%2520corpora%2520with%2520limited%2520coverage%2520of%2520specialized%2520domains.%2520We%2520introduce%2520RexBERT%252C%2520a%2520family%2520of%2520BERT-style%2520encoders%2520designed%2520specifically%2520for%2520e-commerce%2520semantics.%2520We%2520make%2520three%2520contributions.%2520First%252C%2520we%2520release%2520Ecom-niverse%252C%2520a%2520350%2520billion%2520token%2520corpus%2520curated%2520from%2520diverse%2520retail%2520and%2520shopping%2520sources.%2520We%2520describe%2520a%2520modular%2520pipeline%2520that%2520isolates%2520and%2520extracts%2520e-commerce%2520content%2520from%2520FineFineWeb%2520and%2520other%2520open%2520web%2520resources%252C%2520and%2520characterize%2520the%2520resulting%2520domain%2520distribution.%2520Second%252C%2520we%2520present%2520a%2520reproducible%2520pretraining%2520recipe%2520building%2520on%2520ModernBERT%2527s%2520architectural%2520advances.%2520The%2520recipe%2520consists%2520of%2520three%2520phases%253A%2520general%2520pre-training%252C%2520context%2520extension%252C%2520and%2520annealed%2520domain%2520specialization.%2520Third%252C%2520we%2520train%2520RexBERT%2520models%2520ranging%2520from%252017M%2520to%2520400M%2520parameters%2520and%2520evaluate%2520them%2520on%2520token%2520classification%252C%2520semantic%2520similarity%252C%2520and%2520general%2520natural%2520language%2520understanding%2520tasks%2520using%2520e-commerce%2520datasets.%2520Despite%2520having%25202-3x%2520fewer%2520parameters%252C%2520RexBERT%2520outperforms%2520larger%2520general-purpose%2520encoders%2520and%2520matches%2520or%2520surpasses%2520modern%2520long-context%2520models%2520on%2520domain-specific%2520benchmarks.%2520Our%2520results%2520demonstrate%2520that%2520high%2520quality%2520in-domain%2520data%2520combined%2520with%2520a%2520principled%2520training%2520approach%2520provides%2520a%2520stronger%2520foundation%2520for%2520e-commerce%2520applications%2520than%2520indiscriminate%2520scaling%2520alone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RexBERT%3A%20Context%20Specialized%20Bidirectional%20Encoders%20for%20E-commerce&entry.906535625=Rahul%20Bajaj%20and%20Anuj%20Garg&entry.1292438233=Encoder-only%20transformers%20remain%20indispensable%20in%20retrieval%2C%20classification%2C%20and%20ranking%20systems%20where%20latency%2C%20stability%2C%20and%20cost%20are%20paramount.%20Most%20general%20purpose%20encoders%2C%20however%2C%20are%20trained%20on%20generic%20corpora%20with%20limited%20coverage%20of%20specialized%20domains.%20We%20introduce%20RexBERT%2C%20a%20family%20of%20BERT-style%20encoders%20designed%20specifically%20for%20e-commerce%20semantics.%20We%20make%20three%20contributions.%20First%2C%20we%20release%20Ecom-niverse%2C%20a%20350%20billion%20token%20corpus%20curated%20from%20diverse%20retail%20and%20shopping%20sources.%20We%20describe%20a%20modular%20pipeline%20that%20isolates%20and%20extracts%20e-commerce%20content%20from%20FineFineWeb%20and%20other%20open%20web%20resources%2C%20and%20characterize%20the%20resulting%20domain%20distribution.%20Second%2C%20we%20present%20a%20reproducible%20pretraining%20recipe%20building%20on%20ModernBERT%27s%20architectural%20advances.%20The%20recipe%20consists%20of%20three%20phases%3A%20general%20pre-training%2C%20context%20extension%2C%20and%20annealed%20domain%20specialization.%20Third%2C%20we%20train%20RexBERT%20models%20ranging%20from%2017M%20to%20400M%20parameters%20and%20evaluate%20them%20on%20token%20classification%2C%20semantic%20similarity%2C%20and%20general%20natural%20language%20understanding%20tasks%20using%20e-commerce%20datasets.%20Despite%20having%202-3x%20fewer%20parameters%2C%20RexBERT%20outperforms%20larger%20general-purpose%20encoders%20and%20matches%20or%20surpasses%20modern%20long-context%20models%20on%20domain-specific%20benchmarks.%20Our%20results%20demonstrate%20that%20high%20quality%20in-domain%20data%20combined%20with%20a%20principled%20training%20approach%20provides%20a%20stronger%20foundation%20for%20e-commerce%20applications%20than%20indiscriminate%20scaling%20alone.&entry.1838667208=http%3A//arxiv.org/abs/2602.04605v1&entry.124074799=Read"},
{"title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization", "author": "Luca Della Libera and Cem Subakan and Mirco Ravanelli", "abstract": "Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.", "link": "http://arxiv.org/abs/2601.23174v2", "date": "2026-02-04", "relevancy": 2.447, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.481}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Fixed%20Frames%3A%20Dynamic%20Character-Aligned%20Speech%20Tokenization&body=Title%3A%20Beyond%20Fixed%20Frames%3A%20Dynamic%20Character-Aligned%20Speech%20Tokenization%0AAuthor%3A%20Luca%20Della%20Libera%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli%0AAbstract%3A%20Neural%20audio%20codecs%20are%20at%20the%20core%20of%20modern%20conversational%20speech%20technologies%2C%20converting%20continuous%20speech%20into%20sequences%20of%20discrete%20tokens%20that%20can%20be%20processed%20by%20LLMs.%20However%2C%20existing%20codecs%20typically%20operate%20at%20fixed%20frame%20rates%2C%20allocating%20tokens%20uniformly%20in%20time%20and%20producing%20unnecessarily%20long%20sequences.%20In%20this%20work%2C%20we%20introduce%20DyCAST%2C%20a%20Dynamic%20Character-Aligned%20Speech%20Tokenizer%20that%20enables%20variable-frame-rate%20tokenization%20through%20soft%20character-level%20alignment%20and%20explicit%20duration%20modeling.%20DyCAST%20learns%20to%20associate%20tokens%20with%20character-level%20linguistic%20units%20during%20training%20and%20supports%20alignment-free%20inference%20with%20direct%20control%20over%20token%20durations%20at%20decoding%20time.%20To%20improve%20speech%20resynthesis%20quality%20at%20low%20frame%20rates%2C%20we%20further%20introduce%20a%20retrieval-augmented%20decoding%20mechanism%20that%20enhances%20reconstruction%20fidelity%20without%20increasing%20bitrate.%20Experiments%20show%20that%20DyCAST%20achieves%20competitive%20speech%20resynthesis%20quality%20and%20downstream%20performance%20while%20using%20significantly%20fewer%20tokens%20than%20fixed-frame-rate%20codecs.%20Code%20and%20checkpoints%20will%20be%20released%20publicly%20at%20https%3A//github.com/lucadellalib/dycast.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Fixed%2520Frames%253A%2520Dynamic%2520Character-Aligned%2520Speech%2520Tokenization%26entry.906535625%3DLuca%2520Della%2520Libera%2520and%2520Cem%2520Subakan%2520and%2520Mirco%2520Ravanelli%26entry.1292438233%3DNeural%2520audio%2520codecs%2520are%2520at%2520the%2520core%2520of%2520modern%2520conversational%2520speech%2520technologies%252C%2520converting%2520continuous%2520speech%2520into%2520sequences%2520of%2520discrete%2520tokens%2520that%2520can%2520be%2520processed%2520by%2520LLMs.%2520However%252C%2520existing%2520codecs%2520typically%2520operate%2520at%2520fixed%2520frame%2520rates%252C%2520allocating%2520tokens%2520uniformly%2520in%2520time%2520and%2520producing%2520unnecessarily%2520long%2520sequences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DyCAST%252C%2520a%2520Dynamic%2520Character-Aligned%2520Speech%2520Tokenizer%2520that%2520enables%2520variable-frame-rate%2520tokenization%2520through%2520soft%2520character-level%2520alignment%2520and%2520explicit%2520duration%2520modeling.%2520DyCAST%2520learns%2520to%2520associate%2520tokens%2520with%2520character-level%2520linguistic%2520units%2520during%2520training%2520and%2520supports%2520alignment-free%2520inference%2520with%2520direct%2520control%2520over%2520token%2520durations%2520at%2520decoding%2520time.%2520To%2520improve%2520speech%2520resynthesis%2520quality%2520at%2520low%2520frame%2520rates%252C%2520we%2520further%2520introduce%2520a%2520retrieval-augmented%2520decoding%2520mechanism%2520that%2520enhances%2520reconstruction%2520fidelity%2520without%2520increasing%2520bitrate.%2520Experiments%2520show%2520that%2520DyCAST%2520achieves%2520competitive%2520speech%2520resynthesis%2520quality%2520and%2520downstream%2520performance%2520while%2520using%2520significantly%2520fewer%2520tokens%2520than%2520fixed-frame-rate%2520codecs.%2520Code%2520and%2520checkpoints%2520will%2520be%2520released%2520publicly%2520at%2520https%253A//github.com/lucadellalib/dycast.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Fixed%20Frames%3A%20Dynamic%20Character-Aligned%20Speech%20Tokenization&entry.906535625=Luca%20Della%20Libera%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli&entry.1292438233=Neural%20audio%20codecs%20are%20at%20the%20core%20of%20modern%20conversational%20speech%20technologies%2C%20converting%20continuous%20speech%20into%20sequences%20of%20discrete%20tokens%20that%20can%20be%20processed%20by%20LLMs.%20However%2C%20existing%20codecs%20typically%20operate%20at%20fixed%20frame%20rates%2C%20allocating%20tokens%20uniformly%20in%20time%20and%20producing%20unnecessarily%20long%20sequences.%20In%20this%20work%2C%20we%20introduce%20DyCAST%2C%20a%20Dynamic%20Character-Aligned%20Speech%20Tokenizer%20that%20enables%20variable-frame-rate%20tokenization%20through%20soft%20character-level%20alignment%20and%20explicit%20duration%20modeling.%20DyCAST%20learns%20to%20associate%20tokens%20with%20character-level%20linguistic%20units%20during%20training%20and%20supports%20alignment-free%20inference%20with%20direct%20control%20over%20token%20durations%20at%20decoding%20time.%20To%20improve%20speech%20resynthesis%20quality%20at%20low%20frame%20rates%2C%20we%20further%20introduce%20a%20retrieval-augmented%20decoding%20mechanism%20that%20enhances%20reconstruction%20fidelity%20without%20increasing%20bitrate.%20Experiments%20show%20that%20DyCAST%20achieves%20competitive%20speech%20resynthesis%20quality%20and%20downstream%20performance%20while%20using%20significantly%20fewer%20tokens%20than%20fixed-frame-rate%20codecs.%20Code%20and%20checkpoints%20will%20be%20released%20publicly%20at%20https%3A//github.com/lucadellalib/dycast.&entry.1838667208=http%3A//arxiv.org/abs/2601.23174v2&entry.124074799=Read"},
{"title": "Identifying Intervenable and Interpretable Features via Orthogonality Regularization", "author": "Moritz Miller and Florent Draye and Bernhard Sch\u00f6lkopf", "abstract": "With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\\texttt{https://github.com/mrtzmllr/sae-icm}$.", "link": "http://arxiv.org/abs/2602.04718v1", "date": "2026-02-04", "relevancy": 2.4407, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Intervenable%20and%20Interpretable%20Features%20via%20Orthogonality%20Regularization&body=Title%3A%20Identifying%20Intervenable%20and%20Interpretable%20Features%20via%20Orthogonality%20Regularization%0AAuthor%3A%20Moritz%20Miller%20and%20Florent%20Draye%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20With%20recent%20progress%20on%20fine-tuning%20language%20models%20around%20a%20fixed%20sparse%20autoencoder%2C%20we%20disentangle%20the%20decoder%20matrix%20into%20almost%20orthogonal%20features.%20This%20reduces%20interference%20and%20superposition%20between%20the%20features%2C%20while%20keeping%20performance%20on%20the%20target%20dataset%20essentially%20unchanged.%20Our%20orthogonality%20penalty%20leads%20to%20identifiable%20features%2C%20ensuring%20the%20uniqueness%20of%20the%20decomposition.%20Further%2C%20we%20find%20that%20the%20distance%20between%20embedded%20feature%20explanations%20increases%20with%20stricter%20orthogonality%20penalty%2C%20a%20desirable%20property%20for%20interpretability.%20Invoking%20the%20%24%5Ctextit%7BIndependent%20Causal%20Mechanisms%7D%24%20principle%2C%20we%20argue%20that%20orthogonality%20promotes%20modular%20representations%20amenable%20to%20causal%20intervention.%20We%20empirically%20show%20that%20these%20increasingly%20orthogonalized%20features%20allow%20for%20isolated%20interventions.%20Our%20code%20is%20available%20under%20%24%5Ctexttt%7Bhttps%3A//github.com/mrtzmllr/sae-icm%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Intervenable%2520and%2520Interpretable%2520Features%2520via%2520Orthogonality%2520Regularization%26entry.906535625%3DMoritz%2520Miller%2520and%2520Florent%2520Draye%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3DWith%2520recent%2520progress%2520on%2520fine-tuning%2520language%2520models%2520around%2520a%2520fixed%2520sparse%2520autoencoder%252C%2520we%2520disentangle%2520the%2520decoder%2520matrix%2520into%2520almost%2520orthogonal%2520features.%2520This%2520reduces%2520interference%2520and%2520superposition%2520between%2520the%2520features%252C%2520while%2520keeping%2520performance%2520on%2520the%2520target%2520dataset%2520essentially%2520unchanged.%2520Our%2520orthogonality%2520penalty%2520leads%2520to%2520identifiable%2520features%252C%2520ensuring%2520the%2520uniqueness%2520of%2520the%2520decomposition.%2520Further%252C%2520we%2520find%2520that%2520the%2520distance%2520between%2520embedded%2520feature%2520explanations%2520increases%2520with%2520stricter%2520orthogonality%2520penalty%252C%2520a%2520desirable%2520property%2520for%2520interpretability.%2520Invoking%2520the%2520%2524%255Ctextit%257BIndependent%2520Causal%2520Mechanisms%257D%2524%2520principle%252C%2520we%2520argue%2520that%2520orthogonality%2520promotes%2520modular%2520representations%2520amenable%2520to%2520causal%2520intervention.%2520We%2520empirically%2520show%2520that%2520these%2520increasingly%2520orthogonalized%2520features%2520allow%2520for%2520isolated%2520interventions.%2520Our%2520code%2520is%2520available%2520under%2520%2524%255Ctexttt%257Bhttps%253A//github.com/mrtzmllr/sae-icm%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Intervenable%20and%20Interpretable%20Features%20via%20Orthogonality%20Regularization&entry.906535625=Moritz%20Miller%20and%20Florent%20Draye%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=With%20recent%20progress%20on%20fine-tuning%20language%20models%20around%20a%20fixed%20sparse%20autoencoder%2C%20we%20disentangle%20the%20decoder%20matrix%20into%20almost%20orthogonal%20features.%20This%20reduces%20interference%20and%20superposition%20between%20the%20features%2C%20while%20keeping%20performance%20on%20the%20target%20dataset%20essentially%20unchanged.%20Our%20orthogonality%20penalty%20leads%20to%20identifiable%20features%2C%20ensuring%20the%20uniqueness%20of%20the%20decomposition.%20Further%2C%20we%20find%20that%20the%20distance%20between%20embedded%20feature%20explanations%20increases%20with%20stricter%20orthogonality%20penalty%2C%20a%20desirable%20property%20for%20interpretability.%20Invoking%20the%20%24%5Ctextit%7BIndependent%20Causal%20Mechanisms%7D%24%20principle%2C%20we%20argue%20that%20orthogonality%20promotes%20modular%20representations%20amenable%20to%20causal%20intervention.%20We%20empirically%20show%20that%20these%20increasingly%20orthogonalized%20features%20allow%20for%20isolated%20interventions.%20Our%20code%20is%20available%20under%20%24%5Ctexttt%7Bhttps%3A//github.com/mrtzmllr/sae-icm%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2602.04718v1&entry.124074799=Read"},
{"title": "Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning", "author": "Abdulrahman Alotaibi and Irene Tenison and Miriam Kim and Isaac Lee and Lalana Kagal", "abstract": "The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.", "link": "http://arxiv.org/abs/2602.04536v1", "date": "2026-02-04", "relevancy": 2.4293, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4873}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4859}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20to%20Generalize%3A%20Iterative%20Adaptation%20for%20Generalization%20in%20Federated%20Learning&body=Title%3A%20Forget%20to%20Generalize%3A%20Iterative%20Adaptation%20for%20Generalization%20in%20Federated%20Learning%0AAuthor%3A%20Abdulrahman%20Alotaibi%20and%20Irene%20Tenison%20and%20Miriam%20Kim%20and%20Isaac%20Lee%20and%20Lalana%20Kagal%0AAbstract%3A%20The%20Web%20is%20naturally%20heterogeneous%20with%20user%20devices%2C%20geographic%20regions%2C%20browsing%20patterns%2C%20and%20contexts%20all%20leading%20to%20highly%20diverse%2C%20unique%20datasets.%20Federated%20Learning%20%28FL%29%20is%20an%20important%20paradigm%20for%20the%20Web%20because%20it%20enables%20privacy-preserving%2C%20collaborative%20machine%20learning%20across%20diverse%20user%20devices%2C%20web%20services%20and%20clients%20without%20needing%20to%20centralize%20sensitive%20data.%20However%2C%20its%20performance%20degrades%20severely%20under%20non-IID%20client%20distributions%20that%20is%20prevalent%20in%20real-world%20web%20systems.%20In%20this%20work%2C%20we%20propose%20a%20new%20training%20paradigm%20-%20Iterative%20Federated%20Adaptation%20%28IFA%29%20-%20that%20enhances%20generalization%20in%20heterogeneous%20federated%20settings%20through%20generation-wise%20forget%20and%20evolve%20strategy.%20Specifically%2C%20we%20divide%20training%20into%20multiple%20generations%20and%2C%20at%20the%20end%20of%20each%2C%20select%20a%20fraction%20of%20model%20parameters%20%28a%29%20randomly%20or%20%28b%29%20from%20the%20later%20layers%20of%20the%20model%20and%20reinitialize%20them.%20This%20iterative%20forget%20and%20evolve%20schedule%20allows%20the%20model%20to%20escape%20local%20minima%20and%20preserve%20globally%20relevant%20representations.%20Extensive%20experiments%20on%20CIFAR-10%2C%20MIT-Indoors%2C%20and%20Stanford%20Dogs%20datasets%20show%20that%20the%20proposed%20approach%20improves%20global%20accuracy%2C%20especially%20when%20the%20data%20cross%20clients%20are%20Non-IID.%20This%20method%20can%20be%20implemented%20on%20top%20any%20federated%20algorithm%20to%20improve%20its%20generalization%20performance.%20We%20observe%20an%20average%20of%2021.5%25improvement%20across%20datasets.%20This%20work%20advances%20the%20vision%20of%20scalable%2C%20privacy-preserving%20intelligence%20for%20real-world%20heterogeneous%20and%20distributed%20web%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520to%2520Generalize%253A%2520Iterative%2520Adaptation%2520for%2520Generalization%2520in%2520Federated%2520Learning%26entry.906535625%3DAbdulrahman%2520Alotaibi%2520and%2520Irene%2520Tenison%2520and%2520Miriam%2520Kim%2520and%2520Isaac%2520Lee%2520and%2520Lalana%2520Kagal%26entry.1292438233%3DThe%2520Web%2520is%2520naturally%2520heterogeneous%2520with%2520user%2520devices%252C%2520geographic%2520regions%252C%2520browsing%2520patterns%252C%2520and%2520contexts%2520all%2520leading%2520to%2520highly%2520diverse%252C%2520unique%2520datasets.%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520an%2520important%2520paradigm%2520for%2520the%2520Web%2520because%2520it%2520enables%2520privacy-preserving%252C%2520collaborative%2520machine%2520learning%2520across%2520diverse%2520user%2520devices%252C%2520web%2520services%2520and%2520clients%2520without%2520needing%2520to%2520centralize%2520sensitive%2520data.%2520However%252C%2520its%2520performance%2520degrades%2520severely%2520under%2520non-IID%2520client%2520distributions%2520that%2520is%2520prevalent%2520in%2520real-world%2520web%2520systems.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520training%2520paradigm%2520-%2520Iterative%2520Federated%2520Adaptation%2520%2528IFA%2529%2520-%2520that%2520enhances%2520generalization%2520in%2520heterogeneous%2520federated%2520settings%2520through%2520generation-wise%2520forget%2520and%2520evolve%2520strategy.%2520Specifically%252C%2520we%2520divide%2520training%2520into%2520multiple%2520generations%2520and%252C%2520at%2520the%2520end%2520of%2520each%252C%2520select%2520a%2520fraction%2520of%2520model%2520parameters%2520%2528a%2529%2520randomly%2520or%2520%2528b%2529%2520from%2520the%2520later%2520layers%2520of%2520the%2520model%2520and%2520reinitialize%2520them.%2520This%2520iterative%2520forget%2520and%2520evolve%2520schedule%2520allows%2520the%2520model%2520to%2520escape%2520local%2520minima%2520and%2520preserve%2520globally%2520relevant%2520representations.%2520Extensive%2520experiments%2520on%2520CIFAR-10%252C%2520MIT-Indoors%252C%2520and%2520Stanford%2520Dogs%2520datasets%2520show%2520that%2520the%2520proposed%2520approach%2520improves%2520global%2520accuracy%252C%2520especially%2520when%2520the%2520data%2520cross%2520clients%2520are%2520Non-IID.%2520This%2520method%2520can%2520be%2520implemented%2520on%2520top%2520any%2520federated%2520algorithm%2520to%2520improve%2520its%2520generalization%2520performance.%2520We%2520observe%2520an%2520average%2520of%252021.5%2525improvement%2520across%2520datasets.%2520This%2520work%2520advances%2520the%2520vision%2520of%2520scalable%252C%2520privacy-preserving%2520intelligence%2520for%2520real-world%2520heterogeneous%2520and%2520distributed%2520web%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20to%20Generalize%3A%20Iterative%20Adaptation%20for%20Generalization%20in%20Federated%20Learning&entry.906535625=Abdulrahman%20Alotaibi%20and%20Irene%20Tenison%20and%20Miriam%20Kim%20and%20Isaac%20Lee%20and%20Lalana%20Kagal&entry.1292438233=The%20Web%20is%20naturally%20heterogeneous%20with%20user%20devices%2C%20geographic%20regions%2C%20browsing%20patterns%2C%20and%20contexts%20all%20leading%20to%20highly%20diverse%2C%20unique%20datasets.%20Federated%20Learning%20%28FL%29%20is%20an%20important%20paradigm%20for%20the%20Web%20because%20it%20enables%20privacy-preserving%2C%20collaborative%20machine%20learning%20across%20diverse%20user%20devices%2C%20web%20services%20and%20clients%20without%20needing%20to%20centralize%20sensitive%20data.%20However%2C%20its%20performance%20degrades%20severely%20under%20non-IID%20client%20distributions%20that%20is%20prevalent%20in%20real-world%20web%20systems.%20In%20this%20work%2C%20we%20propose%20a%20new%20training%20paradigm%20-%20Iterative%20Federated%20Adaptation%20%28IFA%29%20-%20that%20enhances%20generalization%20in%20heterogeneous%20federated%20settings%20through%20generation-wise%20forget%20and%20evolve%20strategy.%20Specifically%2C%20we%20divide%20training%20into%20multiple%20generations%20and%2C%20at%20the%20end%20of%20each%2C%20select%20a%20fraction%20of%20model%20parameters%20%28a%29%20randomly%20or%20%28b%29%20from%20the%20later%20layers%20of%20the%20model%20and%20reinitialize%20them.%20This%20iterative%20forget%20and%20evolve%20schedule%20allows%20the%20model%20to%20escape%20local%20minima%20and%20preserve%20globally%20relevant%20representations.%20Extensive%20experiments%20on%20CIFAR-10%2C%20MIT-Indoors%2C%20and%20Stanford%20Dogs%20datasets%20show%20that%20the%20proposed%20approach%20improves%20global%20accuracy%2C%20especially%20when%20the%20data%20cross%20clients%20are%20Non-IID.%20This%20method%20can%20be%20implemented%20on%20top%20any%20federated%20algorithm%20to%20improve%20its%20generalization%20performance.%20We%20observe%20an%20average%20of%2021.5%25improvement%20across%20datasets.%20This%20work%20advances%20the%20vision%20of%20scalable%2C%20privacy-preserving%20intelligence%20for%20real-world%20heterogeneous%20and%20distributed%20web%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.04536v1&entry.124074799=Read"},
{"title": "LitS: A novel Neighborhood Descriptor for Point Clouds", "author": "Jonatan B. Bastos and Francisco F. Rivera and Oscar G. Lorenzo and David L. Vilari\u00f1o and Jos\u00e9 C. Cabaleiro and Alberto M. Esmor\u00eds and Tom\u00e1s F. Pena", "abstract": "With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.", "link": "http://arxiv.org/abs/2602.04838v1", "date": "2026-02-04", "relevancy": 2.4244, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LitS%3A%20A%20novel%20Neighborhood%20Descriptor%20for%20Point%20Clouds&body=Title%3A%20LitS%3A%20A%20novel%20Neighborhood%20Descriptor%20for%20Point%20Clouds%0AAuthor%3A%20Jonatan%20B.%20Bastos%20and%20Francisco%20F.%20Rivera%20and%20Oscar%20G.%20Lorenzo%20and%20David%20L.%20Vilari%C3%B1o%20and%20Jos%C3%A9%20C.%20Cabaleiro%20and%20Alberto%20M.%20Esmor%C3%ADs%20and%20Tom%C3%A1s%20F.%20Pena%0AAbstract%3A%20With%20the%20advancement%20of%203D%20scanning%20technologies%2C%20point%20clouds%20have%20become%20fundamental%20for%20representing%203D%20spatial%20data%2C%20with%20applications%20that%20span%20across%20various%20scientific%20and%20technological%20fields.%20Practical%20analysis%20of%20this%20data%20depends%20crucially%20on%20available%20neighborhood%20descriptors%20to%20accurately%20characterize%20the%20local%20geometries%20of%20the%20point%20cloud.%20This%20paper%20introduces%20LitS%2C%20a%20novel%20neighborhood%20descriptor%20for%202D%20and%203D%20point%20clouds.%20LitS%20are%20piecewise%20constant%20functions%20on%20the%20unit%20circle%20that%20allow%20points%20to%20keep%20track%20of%20their%20surroundings.%20Each%20element%20in%20LitS%27%20domain%20represents%20a%20direction%20with%20respect%20to%20a%20local%20reference%20system.%20Once%20constructed%2C%20evaluating%20LitS%20at%20any%20given%20direction%20gives%20us%20information%20about%20the%20number%20of%20neighbors%20in%20a%20cone-like%20region%20centered%20around%20that%20same%20direction.%20Thus%2C%20LitS%20conveys%20a%20lot%20of%20information%20about%20the%20local%20neighborhood%20of%20a%20point%2C%20which%20can%20be%20leveraged%20to%20gain%20global%20structural%20understanding%20by%20analyzing%20how%20LitS%20changes%20between%20close%20points.%20In%20addition%2C%20LitS%20comes%20in%20two%20versions%20%28%27regular%27%20and%20%27cumulative%27%29%20and%20has%20two%20parameters%2C%20allowing%20them%20to%20adapt%20to%20various%20contexts%20and%20types%20of%20point%20clouds.%20Overall%2C%20they%20are%20a%20versatile%20neighborhood%20descriptor%2C%20capable%20of%20capturing%20the%20nuances%20of%20local%20point%20arrangements%20and%20resilient%20to%20common%20point%20cloud%20data%20issues%20such%20as%20variable%20density%20and%20noise.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLitS%253A%2520A%2520novel%2520Neighborhood%2520Descriptor%2520for%2520Point%2520Clouds%26entry.906535625%3DJonatan%2520B.%2520Bastos%2520and%2520Francisco%2520F.%2520Rivera%2520and%2520Oscar%2520G.%2520Lorenzo%2520and%2520David%2520L.%2520Vilari%25C3%25B1o%2520and%2520Jos%25C3%25A9%2520C.%2520Cabaleiro%2520and%2520Alberto%2520M.%2520Esmor%25C3%25ADs%2520and%2520Tom%25C3%25A1s%2520F.%2520Pena%26entry.1292438233%3DWith%2520the%2520advancement%2520of%25203D%2520scanning%2520technologies%252C%2520point%2520clouds%2520have%2520become%2520fundamental%2520for%2520representing%25203D%2520spatial%2520data%252C%2520with%2520applications%2520that%2520span%2520across%2520various%2520scientific%2520and%2520technological%2520fields.%2520Practical%2520analysis%2520of%2520this%2520data%2520depends%2520crucially%2520on%2520available%2520neighborhood%2520descriptors%2520to%2520accurately%2520characterize%2520the%2520local%2520geometries%2520of%2520the%2520point%2520cloud.%2520This%2520paper%2520introduces%2520LitS%252C%2520a%2520novel%2520neighborhood%2520descriptor%2520for%25202D%2520and%25203D%2520point%2520clouds.%2520LitS%2520are%2520piecewise%2520constant%2520functions%2520on%2520the%2520unit%2520circle%2520that%2520allow%2520points%2520to%2520keep%2520track%2520of%2520their%2520surroundings.%2520Each%2520element%2520in%2520LitS%2527%2520domain%2520represents%2520a%2520direction%2520with%2520respect%2520to%2520a%2520local%2520reference%2520system.%2520Once%2520constructed%252C%2520evaluating%2520LitS%2520at%2520any%2520given%2520direction%2520gives%2520us%2520information%2520about%2520the%2520number%2520of%2520neighbors%2520in%2520a%2520cone-like%2520region%2520centered%2520around%2520that%2520same%2520direction.%2520Thus%252C%2520LitS%2520conveys%2520a%2520lot%2520of%2520information%2520about%2520the%2520local%2520neighborhood%2520of%2520a%2520point%252C%2520which%2520can%2520be%2520leveraged%2520to%2520gain%2520global%2520structural%2520understanding%2520by%2520analyzing%2520how%2520LitS%2520changes%2520between%2520close%2520points.%2520In%2520addition%252C%2520LitS%2520comes%2520in%2520two%2520versions%2520%2528%2527regular%2527%2520and%2520%2527cumulative%2527%2529%2520and%2520has%2520two%2520parameters%252C%2520allowing%2520them%2520to%2520adapt%2520to%2520various%2520contexts%2520and%2520types%2520of%2520point%2520clouds.%2520Overall%252C%2520they%2520are%2520a%2520versatile%2520neighborhood%2520descriptor%252C%2520capable%2520of%2520capturing%2520the%2520nuances%2520of%2520local%2520point%2520arrangements%2520and%2520resilient%2520to%2520common%2520point%2520cloud%2520data%2520issues%2520such%2520as%2520variable%2520density%2520and%2520noise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LitS%3A%20A%20novel%20Neighborhood%20Descriptor%20for%20Point%20Clouds&entry.906535625=Jonatan%20B.%20Bastos%20and%20Francisco%20F.%20Rivera%20and%20Oscar%20G.%20Lorenzo%20and%20David%20L.%20Vilari%C3%B1o%20and%20Jos%C3%A9%20C.%20Cabaleiro%20and%20Alberto%20M.%20Esmor%C3%ADs%20and%20Tom%C3%A1s%20F.%20Pena&entry.1292438233=With%20the%20advancement%20of%203D%20scanning%20technologies%2C%20point%20clouds%20have%20become%20fundamental%20for%20representing%203D%20spatial%20data%2C%20with%20applications%20that%20span%20across%20various%20scientific%20and%20technological%20fields.%20Practical%20analysis%20of%20this%20data%20depends%20crucially%20on%20available%20neighborhood%20descriptors%20to%20accurately%20characterize%20the%20local%20geometries%20of%20the%20point%20cloud.%20This%20paper%20introduces%20LitS%2C%20a%20novel%20neighborhood%20descriptor%20for%202D%20and%203D%20point%20clouds.%20LitS%20are%20piecewise%20constant%20functions%20on%20the%20unit%20circle%20that%20allow%20points%20to%20keep%20track%20of%20their%20surroundings.%20Each%20element%20in%20LitS%27%20domain%20represents%20a%20direction%20with%20respect%20to%20a%20local%20reference%20system.%20Once%20constructed%2C%20evaluating%20LitS%20at%20any%20given%20direction%20gives%20us%20information%20about%20the%20number%20of%20neighbors%20in%20a%20cone-like%20region%20centered%20around%20that%20same%20direction.%20Thus%2C%20LitS%20conveys%20a%20lot%20of%20information%20about%20the%20local%20neighborhood%20of%20a%20point%2C%20which%20can%20be%20leveraged%20to%20gain%20global%20structural%20understanding%20by%20analyzing%20how%20LitS%20changes%20between%20close%20points.%20In%20addition%2C%20LitS%20comes%20in%20two%20versions%20%28%27regular%27%20and%20%27cumulative%27%29%20and%20has%20two%20parameters%2C%20allowing%20them%20to%20adapt%20to%20various%20contexts%20and%20types%20of%20point%20clouds.%20Overall%2C%20they%20are%20a%20versatile%20neighborhood%20descriptor%2C%20capable%20of%20capturing%20the%20nuances%20of%20local%20point%20arrangements%20and%20resilient%20to%20common%20point%20cloud%20data%20issues%20such%20as%20variable%20density%20and%20noise.&entry.1838667208=http%3A//arxiv.org/abs/2602.04838v1&entry.124074799=Read"},
{"title": "Comparing Task-Agnostic Embedding Models for Tabular Data", "author": "Frederik Hoppe and Lars Kleinemeier and Astrid Franz and Udo G\u00f6bel", "abstract": "Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations extracted from tabular foundation models (TabPFN, TabICL and TabSTAR) alongside classical feature engineering (TableVectorizer and a sphere model) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple feature engineering methods achieve comparable or superior performance while requiring significantly less computational resources than tabular foundation models.", "link": "http://arxiv.org/abs/2511.14276v2", "date": "2026-02-04", "relevancy": 2.4244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Task-Agnostic%20Embedding%20Models%20for%20Tabular%20Data&body=Title%3A%20Comparing%20Task-Agnostic%20Embedding%20Models%20for%20Tabular%20Data%0AAuthor%3A%20Frederik%20Hoppe%20and%20Lars%20Kleinemeier%20and%20Astrid%20Franz%20and%20Udo%20G%C3%B6bel%0AAbstract%3A%20Recent%20foundation%20models%20for%20tabular%20data%20achieve%20strong%20task-specific%20performance%20via%20in-context%20learning.%20Nevertheless%2C%20they%20focus%20on%20direct%20prediction%20by%20encapsulating%20both%20representation%20learning%20and%20task-specific%20inference%20inside%20a%20single%2C%20resource-intensive%20network.%20This%20work%20specifically%20focuses%20on%20representation%20learning%2C%20i.e.%2C%20on%20transferable%2C%20task-agnostic%20embeddings.%20We%20systematically%20evaluate%20task-agnostic%20representations%20extracted%20from%20tabular%20foundation%20models%20%28TabPFN%2C%20TabICL%20and%20TabSTAR%29%20alongside%20classical%20feature%20engineering%20%28TableVectorizer%20and%20a%20sphere%20model%29%20across%20a%20variety%20of%20application%20tasks%20as%20outlier%20detection%20%28ADBench%29%20and%20supervised%20learning%20%28TabArena%20Lite%29.%20We%20find%20that%20simple%20feature%20engineering%20methods%20achieve%20comparable%20or%20superior%20performance%20while%20requiring%20significantly%20less%20computational%20resources%20than%20tabular%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Task-Agnostic%2520Embedding%2520Models%2520for%2520Tabular%2520Data%26entry.906535625%3DFrederik%2520Hoppe%2520and%2520Lars%2520Kleinemeier%2520and%2520Astrid%2520Franz%2520and%2520Udo%2520G%25C3%25B6bel%26entry.1292438233%3DRecent%2520foundation%2520models%2520for%2520tabular%2520data%2520achieve%2520strong%2520task-specific%2520performance%2520via%2520in-context%2520learning.%2520Nevertheless%252C%2520they%2520focus%2520on%2520direct%2520prediction%2520by%2520encapsulating%2520both%2520representation%2520learning%2520and%2520task-specific%2520inference%2520inside%2520a%2520single%252C%2520resource-intensive%2520network.%2520This%2520work%2520specifically%2520focuses%2520on%2520representation%2520learning%252C%2520i.e.%252C%2520on%2520transferable%252C%2520task-agnostic%2520embeddings.%2520We%2520systematically%2520evaluate%2520task-agnostic%2520representations%2520extracted%2520from%2520tabular%2520foundation%2520models%2520%2528TabPFN%252C%2520TabICL%2520and%2520TabSTAR%2529%2520alongside%2520classical%2520feature%2520engineering%2520%2528TableVectorizer%2520and%2520a%2520sphere%2520model%2529%2520across%2520a%2520variety%2520of%2520application%2520tasks%2520as%2520outlier%2520detection%2520%2528ADBench%2529%2520and%2520supervised%2520learning%2520%2528TabArena%2520Lite%2529.%2520We%2520find%2520that%2520simple%2520feature%2520engineering%2520methods%2520achieve%2520comparable%2520or%2520superior%2520performance%2520while%2520requiring%2520significantly%2520less%2520computational%2520resources%2520than%2520tabular%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Task-Agnostic%20Embedding%20Models%20for%20Tabular%20Data&entry.906535625=Frederik%20Hoppe%20and%20Lars%20Kleinemeier%20and%20Astrid%20Franz%20and%20Udo%20G%C3%B6bel&entry.1292438233=Recent%20foundation%20models%20for%20tabular%20data%20achieve%20strong%20task-specific%20performance%20via%20in-context%20learning.%20Nevertheless%2C%20they%20focus%20on%20direct%20prediction%20by%20encapsulating%20both%20representation%20learning%20and%20task-specific%20inference%20inside%20a%20single%2C%20resource-intensive%20network.%20This%20work%20specifically%20focuses%20on%20representation%20learning%2C%20i.e.%2C%20on%20transferable%2C%20task-agnostic%20embeddings.%20We%20systematically%20evaluate%20task-agnostic%20representations%20extracted%20from%20tabular%20foundation%20models%20%28TabPFN%2C%20TabICL%20and%20TabSTAR%29%20alongside%20classical%20feature%20engineering%20%28TableVectorizer%20and%20a%20sphere%20model%29%20across%20a%20variety%20of%20application%20tasks%20as%20outlier%20detection%20%28ADBench%29%20and%20supervised%20learning%20%28TabArena%20Lite%29.%20We%20find%20that%20simple%20feature%20engineering%20methods%20achieve%20comparable%20or%20superior%20performance%20while%20requiring%20significantly%20less%20computational%20resources%20than%20tabular%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.14276v2&entry.124074799=Read"},
{"title": "Exploiting contextual information to improve stance detection in informal political discourse with LLMs", "author": "Arman Engin Sucu and Yixiang Zhou and Mario A. Nascimento and Tony Mullen", "abstract": "This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\\% to +38.5\\%, achieving up to 74\\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.", "link": "http://arxiv.org/abs/2602.04750v1", "date": "2026-02-04", "relevancy": 2.4231, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20contextual%20information%20to%20improve%20stance%20detection%20in%20informal%20political%20discourse%20with%20LLMs&body=Title%3A%20Exploiting%20contextual%20information%20to%20improve%20stance%20detection%20in%20informal%20political%20discourse%20with%20LLMs%0AAuthor%3A%20Arman%20Engin%20Sucu%20and%20Yixiang%20Zhou%20and%20Mario%20A.%20Nascimento%20and%20Tony%20Mullen%0AAbstract%3A%20This%20study%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20political%20stance%20detection%20in%20informal%20online%20discourse%2C%20where%20language%20is%20often%20sarcastic%2C%20ambiguous%2C%20and%20context-dependent.%20We%20explore%20whether%20providing%20contextual%20information%2C%20specifically%20user%20profile%20summaries%20derived%20from%20historical%20posts%2C%20can%20improve%20classification%20accuracy.%20Using%20a%20real-world%20political%20forum%20dataset%2C%20we%20generate%20structured%20profiles%20that%20summarize%20users%27%20ideological%20leaning%2C%20recurring%20topics%2C%20and%20linguistic%20patterns.%20We%20evaluate%20seven%20state-of-the-art%20LLMs%20across%20baseline%20and%20context-enriched%20setups%20through%20a%20comprehensive%20cross-model%20evaluation.%20Our%20findings%20show%20that%20contextual%20prompts%20significantly%20boost%20accuracy%2C%20with%20improvements%20ranging%20from%20%2B17.5%5C%25%20to%20%2B38.5%5C%25%2C%20achieving%20up%20to%2074%5C%25%20accuracy%20that%20surpasses%20previous%20approaches.%20We%20also%20analyze%20how%20profile%20size%20and%20post%20selection%20strategies%20affect%20performance%2C%20showing%20that%20strategically%20chosen%20political%20content%20yields%20better%20results%20than%20larger%2C%20randomly%20selected%20contexts.%20These%20findings%20underscore%20the%20value%20of%20incorporating%20user-level%20context%20to%20enhance%20LLM%20performance%20in%20nuanced%20political%20classification%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520contextual%2520information%2520to%2520improve%2520stance%2520detection%2520in%2520informal%2520political%2520discourse%2520with%2520LLMs%26entry.906535625%3DArman%2520Engin%2520Sucu%2520and%2520Yixiang%2520Zhou%2520and%2520Mario%2520A.%2520Nascimento%2520and%2520Tony%2520Mullen%26entry.1292438233%3DThis%2520study%2520investigates%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520political%2520stance%2520detection%2520in%2520informal%2520online%2520discourse%252C%2520where%2520language%2520is%2520often%2520sarcastic%252C%2520ambiguous%252C%2520and%2520context-dependent.%2520We%2520explore%2520whether%2520providing%2520contextual%2520information%252C%2520specifically%2520user%2520profile%2520summaries%2520derived%2520from%2520historical%2520posts%252C%2520can%2520improve%2520classification%2520accuracy.%2520Using%2520a%2520real-world%2520political%2520forum%2520dataset%252C%2520we%2520generate%2520structured%2520profiles%2520that%2520summarize%2520users%2527%2520ideological%2520leaning%252C%2520recurring%2520topics%252C%2520and%2520linguistic%2520patterns.%2520We%2520evaluate%2520seven%2520state-of-the-art%2520LLMs%2520across%2520baseline%2520and%2520context-enriched%2520setups%2520through%2520a%2520comprehensive%2520cross-model%2520evaluation.%2520Our%2520findings%2520show%2520that%2520contextual%2520prompts%2520significantly%2520boost%2520accuracy%252C%2520with%2520improvements%2520ranging%2520from%2520%252B17.5%255C%2525%2520to%2520%252B38.5%255C%2525%252C%2520achieving%2520up%2520to%252074%255C%2525%2520accuracy%2520that%2520surpasses%2520previous%2520approaches.%2520We%2520also%2520analyze%2520how%2520profile%2520size%2520and%2520post%2520selection%2520strategies%2520affect%2520performance%252C%2520showing%2520that%2520strategically%2520chosen%2520political%2520content%2520yields%2520better%2520results%2520than%2520larger%252C%2520randomly%2520selected%2520contexts.%2520These%2520findings%2520underscore%2520the%2520value%2520of%2520incorporating%2520user-level%2520context%2520to%2520enhance%2520LLM%2520performance%2520in%2520nuanced%2520political%2520classification%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20contextual%20information%20to%20improve%20stance%20detection%20in%20informal%20political%20discourse%20with%20LLMs&entry.906535625=Arman%20Engin%20Sucu%20and%20Yixiang%20Zhou%20and%20Mario%20A.%20Nascimento%20and%20Tony%20Mullen&entry.1292438233=This%20study%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20political%20stance%20detection%20in%20informal%20online%20discourse%2C%20where%20language%20is%20often%20sarcastic%2C%20ambiguous%2C%20and%20context-dependent.%20We%20explore%20whether%20providing%20contextual%20information%2C%20specifically%20user%20profile%20summaries%20derived%20from%20historical%20posts%2C%20can%20improve%20classification%20accuracy.%20Using%20a%20real-world%20political%20forum%20dataset%2C%20we%20generate%20structured%20profiles%20that%20summarize%20users%27%20ideological%20leaning%2C%20recurring%20topics%2C%20and%20linguistic%20patterns.%20We%20evaluate%20seven%20state-of-the-art%20LLMs%20across%20baseline%20and%20context-enriched%20setups%20through%20a%20comprehensive%20cross-model%20evaluation.%20Our%20findings%20show%20that%20contextual%20prompts%20significantly%20boost%20accuracy%2C%20with%20improvements%20ranging%20from%20%2B17.5%5C%25%20to%20%2B38.5%5C%25%2C%20achieving%20up%20to%2074%5C%25%20accuracy%20that%20surpasses%20previous%20approaches.%20We%20also%20analyze%20how%20profile%20size%20and%20post%20selection%20strategies%20affect%20performance%2C%20showing%20that%20strategically%20chosen%20political%20content%20yields%20better%20results%20than%20larger%2C%20randomly%20selected%20contexts.%20These%20findings%20underscore%20the%20value%20of%20incorporating%20user-level%20context%20to%20enhance%20LLM%20performance%20in%20nuanced%20political%20classification%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.04750v1&entry.124074799=Read"},
{"title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts", "author": "Ziyi Huang and Xia Cui", "abstract": "This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection (Track A), which focuses on multi-label emotion detection in short texts. We propose a feature-centric framework that dynamically adapts document representations and learning algorithms to optimize language-specific performance. Our study evaluates three key components: document representation, dimensionality reduction, and model training in 28 languages, highlighting five for detailed analysis. The results show that TF-IDF remains highly effective for low-resource languages, while contextual embeddings like FastText and transformer-based document representations, such as those produced by Sentence-BERT, exhibit language-specific strengths. Principal Component Analysis (PCA) reduces training time without compromising performance, particularly benefiting FastText and neural models such as Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores the trade-off between model complexity and processing cost. Our framework provides a scalable solution for multilingual emotion detection, addressing the challenges of linguistic diversity and resource constraints.", "link": "http://arxiv.org/abs/2507.08499v2", "date": "2026-02-04", "relevancy": 2.4221, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromotionGo%20at%20SemEval-2025%20Task%2011%3A%20A%20Feature-Centric%20Framework%20for%20Cross-Lingual%20Multi-Emotion%20Detection%20in%20Short%20Texts&body=Title%3A%20PromotionGo%20at%20SemEval-2025%20Task%2011%3A%20A%20Feature-Centric%20Framework%20for%20Cross-Lingual%20Multi-Emotion%20Detection%20in%20Short%20Texts%0AAuthor%3A%20Ziyi%20Huang%20and%20Xia%20Cui%0AAbstract%3A%20This%20paper%20presents%20our%20system%20for%20SemEval%202025%20Task%2011%3A%20Bridging%20the%20Gap%20in%20Text-Based%20Emotion%20Detection%20%28Track%20A%29%2C%20which%20focuses%20on%20multi-label%20emotion%20detection%20in%20short%20texts.%20We%20propose%20a%20feature-centric%20framework%20that%20dynamically%20adapts%20document%20representations%20and%20learning%20algorithms%20to%20optimize%20language-specific%20performance.%20Our%20study%20evaluates%20three%20key%20components%3A%20document%20representation%2C%20dimensionality%20reduction%2C%20and%20model%20training%20in%2028%20languages%2C%20highlighting%20five%20for%20detailed%20analysis.%20The%20results%20show%20that%20TF-IDF%20remains%20highly%20effective%20for%20low-resource%20languages%2C%20while%20contextual%20embeddings%20like%20FastText%20and%20transformer-based%20document%20representations%2C%20such%20as%20those%20produced%20by%20Sentence-BERT%2C%20exhibit%20language-specific%20strengths.%20Principal%20Component%20Analysis%20%28PCA%29%20reduces%20training%20time%20without%20compromising%20performance%2C%20particularly%20benefiting%20FastText%20and%20neural%20models%20such%20as%20Multi-Layer%20Perceptrons%20%28MLP%29.%20Computational%20efficiency%20analysis%20underscores%20the%20trade-off%20between%20model%20complexity%20and%20processing%20cost.%20Our%20framework%20provides%20a%20scalable%20solution%20for%20multilingual%20emotion%20detection%2C%20addressing%20the%20challenges%20of%20linguistic%20diversity%20and%20resource%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromotionGo%2520at%2520SemEval-2025%2520Task%252011%253A%2520A%2520Feature-Centric%2520Framework%2520for%2520Cross-Lingual%2520Multi-Emotion%2520Detection%2520in%2520Short%2520Texts%26entry.906535625%3DZiyi%2520Huang%2520and%2520Xia%2520Cui%26entry.1292438233%3DThis%2520paper%2520presents%2520our%2520system%2520for%2520SemEval%25202025%2520Task%252011%253A%2520Bridging%2520the%2520Gap%2520in%2520Text-Based%2520Emotion%2520Detection%2520%2528Track%2520A%2529%252C%2520which%2520focuses%2520on%2520multi-label%2520emotion%2520detection%2520in%2520short%2520texts.%2520We%2520propose%2520a%2520feature-centric%2520framework%2520that%2520dynamically%2520adapts%2520document%2520representations%2520and%2520learning%2520algorithms%2520to%2520optimize%2520language-specific%2520performance.%2520Our%2520study%2520evaluates%2520three%2520key%2520components%253A%2520document%2520representation%252C%2520dimensionality%2520reduction%252C%2520and%2520model%2520training%2520in%252028%2520languages%252C%2520highlighting%2520five%2520for%2520detailed%2520analysis.%2520The%2520results%2520show%2520that%2520TF-IDF%2520remains%2520highly%2520effective%2520for%2520low-resource%2520languages%252C%2520while%2520contextual%2520embeddings%2520like%2520FastText%2520and%2520transformer-based%2520document%2520representations%252C%2520such%2520as%2520those%2520produced%2520by%2520Sentence-BERT%252C%2520exhibit%2520language-specific%2520strengths.%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520reduces%2520training%2520time%2520without%2520compromising%2520performance%252C%2520particularly%2520benefiting%2520FastText%2520and%2520neural%2520models%2520such%2520as%2520Multi-Layer%2520Perceptrons%2520%2528MLP%2529.%2520Computational%2520efficiency%2520analysis%2520underscores%2520the%2520trade-off%2520between%2520model%2520complexity%2520and%2520processing%2520cost.%2520Our%2520framework%2520provides%2520a%2520scalable%2520solution%2520for%2520multilingual%2520emotion%2520detection%252C%2520addressing%2520the%2520challenges%2520of%2520linguistic%2520diversity%2520and%2520resource%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromotionGo%20at%20SemEval-2025%20Task%2011%3A%20A%20Feature-Centric%20Framework%20for%20Cross-Lingual%20Multi-Emotion%20Detection%20in%20Short%20Texts&entry.906535625=Ziyi%20Huang%20and%20Xia%20Cui&entry.1292438233=This%20paper%20presents%20our%20system%20for%20SemEval%202025%20Task%2011%3A%20Bridging%20the%20Gap%20in%20Text-Based%20Emotion%20Detection%20%28Track%20A%29%2C%20which%20focuses%20on%20multi-label%20emotion%20detection%20in%20short%20texts.%20We%20propose%20a%20feature-centric%20framework%20that%20dynamically%20adapts%20document%20representations%20and%20learning%20algorithms%20to%20optimize%20language-specific%20performance.%20Our%20study%20evaluates%20three%20key%20components%3A%20document%20representation%2C%20dimensionality%20reduction%2C%20and%20model%20training%20in%2028%20languages%2C%20highlighting%20five%20for%20detailed%20analysis.%20The%20results%20show%20that%20TF-IDF%20remains%20highly%20effective%20for%20low-resource%20languages%2C%20while%20contextual%20embeddings%20like%20FastText%20and%20transformer-based%20document%20representations%2C%20such%20as%20those%20produced%20by%20Sentence-BERT%2C%20exhibit%20language-specific%20strengths.%20Principal%20Component%20Analysis%20%28PCA%29%20reduces%20training%20time%20without%20compromising%20performance%2C%20particularly%20benefiting%20FastText%20and%20neural%20models%20such%20as%20Multi-Layer%20Perceptrons%20%28MLP%29.%20Computational%20efficiency%20analysis%20underscores%20the%20trade-off%20between%20model%20complexity%20and%20processing%20cost.%20Our%20framework%20provides%20a%20scalable%20solution%20for%20multilingual%20emotion%20detection%2C%20addressing%20the%20challenges%20of%20linguistic%20diversity%20and%20resource%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2507.08499v2&entry.124074799=Read"},
{"title": "VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration", "author": "Jaeyoon Jung and Yejun Yoon and Seunghyun Yoon and Kunwoo Park", "abstract": "This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.", "link": "http://arxiv.org/abs/2602.04587v1", "date": "2026-02-04", "relevancy": 2.4159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VILLAIN%20at%20AVerImaTeC%3A%20Verifying%20Image-Text%20Claims%20via%20Multi-Agent%20Collaboration&body=Title%3A%20VILLAIN%20at%20AVerImaTeC%3A%20Verifying%20Image-Text%20Claims%20via%20Multi-Agent%20Collaboration%0AAuthor%3A%20Jaeyoon%20Jung%20and%20Yejun%20Yoon%20and%20Seunghyun%20Yoon%20and%20Kunwoo%20Park%0AAbstract%3A%20This%20paper%20describes%20VILLAIN%2C%20a%20multimodal%20fact-checking%20system%20that%20verifies%20image-text%20claims%20through%20prompt-based%20multi-agent%20collaboration.%20For%20the%20AVerImaTeC%20shared%20task%2C%20VILLAIN%20employs%20vision-language%20model%20agents%20across%20multiple%20stages%20of%20fact-checking.%20Textual%20and%20visual%20evidence%20is%20retrieved%20from%20the%20knowledge%20store%20enriched%20through%20additional%20web%20collection.%20To%20identify%20key%20information%20and%20address%20inconsistencies%20among%20evidence%20items%2C%20modality-specific%20and%20cross-modal%20agents%20generate%20analysis%20reports.%20In%20the%20subsequent%20stage%2C%20question-answer%20pairs%20are%20produced%20based%20on%20these%20reports.%20Finally%2C%20the%20Verdict%20Prediction%20agent%20produces%20the%20verification%20outcome%20based%20on%20the%20image-text%20claim%20and%20the%20generated%20question-answer%20pairs.%20Our%20system%20ranked%20first%20on%20the%20leaderboard%20across%20all%20evaluation%20metrics.%20The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/ssu-humane/VILLAIN.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVILLAIN%2520at%2520AVerImaTeC%253A%2520Verifying%2520Image-Text%2520Claims%2520via%2520Multi-Agent%2520Collaboration%26entry.906535625%3DJaeyoon%2520Jung%2520and%2520Yejun%2520Yoon%2520and%2520Seunghyun%2520Yoon%2520and%2520Kunwoo%2520Park%26entry.1292438233%3DThis%2520paper%2520describes%2520VILLAIN%252C%2520a%2520multimodal%2520fact-checking%2520system%2520that%2520verifies%2520image-text%2520claims%2520through%2520prompt-based%2520multi-agent%2520collaboration.%2520For%2520the%2520AVerImaTeC%2520shared%2520task%252C%2520VILLAIN%2520employs%2520vision-language%2520model%2520agents%2520across%2520multiple%2520stages%2520of%2520fact-checking.%2520Textual%2520and%2520visual%2520evidence%2520is%2520retrieved%2520from%2520the%2520knowledge%2520store%2520enriched%2520through%2520additional%2520web%2520collection.%2520To%2520identify%2520key%2520information%2520and%2520address%2520inconsistencies%2520among%2520evidence%2520items%252C%2520modality-specific%2520and%2520cross-modal%2520agents%2520generate%2520analysis%2520reports.%2520In%2520the%2520subsequent%2520stage%252C%2520question-answer%2520pairs%2520are%2520produced%2520based%2520on%2520these%2520reports.%2520Finally%252C%2520the%2520Verdict%2520Prediction%2520agent%2520produces%2520the%2520verification%2520outcome%2520based%2520on%2520the%2520image-text%2520claim%2520and%2520the%2520generated%2520question-answer%2520pairs.%2520Our%2520system%2520ranked%2520first%2520on%2520the%2520leaderboard%2520across%2520all%2520evaluation%2520metrics.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/ssu-humane/VILLAIN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VILLAIN%20at%20AVerImaTeC%3A%20Verifying%20Image-Text%20Claims%20via%20Multi-Agent%20Collaboration&entry.906535625=Jaeyoon%20Jung%20and%20Yejun%20Yoon%20and%20Seunghyun%20Yoon%20and%20Kunwoo%20Park&entry.1292438233=This%20paper%20describes%20VILLAIN%2C%20a%20multimodal%20fact-checking%20system%20that%20verifies%20image-text%20claims%20through%20prompt-based%20multi-agent%20collaboration.%20For%20the%20AVerImaTeC%20shared%20task%2C%20VILLAIN%20employs%20vision-language%20model%20agents%20across%20multiple%20stages%20of%20fact-checking.%20Textual%20and%20visual%20evidence%20is%20retrieved%20from%20the%20knowledge%20store%20enriched%20through%20additional%20web%20collection.%20To%20identify%20key%20information%20and%20address%20inconsistencies%20among%20evidence%20items%2C%20modality-specific%20and%20cross-modal%20agents%20generate%20analysis%20reports.%20In%20the%20subsequent%20stage%2C%20question-answer%20pairs%20are%20produced%20based%20on%20these%20reports.%20Finally%2C%20the%20Verdict%20Prediction%20agent%20produces%20the%20verification%20outcome%20based%20on%20the%20image-text%20claim%20and%20the%20generated%20question-answer%20pairs.%20Our%20system%20ranked%20first%20on%20the%20leaderboard%20across%20all%20evaluation%20metrics.%20The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/ssu-humane/VILLAIN.&entry.1838667208=http%3A//arxiv.org/abs/2602.04587v1&entry.124074799=Read"},
{"title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "author": "Yufeng Zhong and Lei Chen and Xuanle Zhao and Wenkang Han and Liming Zheng and Jing Huang and Deyang Jiang and Yilin Cao and Lin Ma and Zhixiong Zeng", "abstract": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "link": "http://arxiv.org/abs/2601.21639v2", "date": "2026-02-04", "relevancy": 2.4154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models&body=Title%3A%20OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models%0AAuthor%3A%20Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Xuanle%20Zhao%20and%20Wenkang%20Han%20and%20Liming%20Zheng%20and%20Jing%20Huang%20and%20Deyang%20Jiang%20and%20Yilin%20Cao%20and%20Lin%20Ma%20and%20Zhixiong%20Zeng%0AAbstract%3A%20The%20development%20of%20large%20vision%20language%20models%20drives%20the%20demand%20for%20managing%2C%20and%20applying%20massive%20amounts%20of%20multimodal%20data%2C%20making%20OCR%20technology%2C%20which%20extracts%20information%20from%20visual%20images%2C%20increasingly%20popular.%20However%2C%20existing%20OCR%20methods%20primarily%20focus%20on%20recognizing%20text%20elements%20from%20images%20or%20scanned%20documents%20%28Text-centric%20OCR%29%2C%20neglecting%20the%20identification%20of%20visual%20elements%20from%20visually%20information-dense%20image%20sources%20%28Vision-centric%20OCR%29%2C%20such%20as%20charts%2C%20web%20pages%20and%20science%20plots.%20In%20reality%2C%20these%20visually%20information-dense%20images%20are%20widespread%20on%20the%20internet%20and%20have%20significant%20real-world%20application%20value%2C%20such%20as%20data%20visualization%20and%20web%20page%20analysis.%20In%20this%20technical%20report%2C%20we%20propose%20OCRVerse%2C%20the%20first%20holistic%20OCR%20method%20in%20end-to-end%20manner%20that%20enables%20unified%20text-centric%20OCR%20and%20vision-centric%20OCR.%20To%20this%20end%2C%20we%20constructe%20comprehensive%20data%20engineering%20to%20cover%20a%20wide%20range%20of%20text-centric%20documents%2C%20such%20as%20newspapers%2C%20magazines%20and%20books%2C%20as%20well%20as%20vision-centric%20rendered%20composites%2C%20including%20charts%2C%20web%20pages%20and%20scientific%20plots.%20Moreover%2C%20we%20propose%20a%20two-stage%20SFT-RL%20multi-domain%20training%20method%20for%20OCRVerse.%20SFT%20directly%20mixes%20cross-domain%20data%20to%20train%20and%20establish%20initial%20domain%20knowledge%2C%20while%20RL%20focuses%20on%20designing%20personalized%20reward%20strategies%20for%20the%20characteristics%20of%20each%20domain.%20Specifically%2C%20since%20different%20domains%20require%20various%20output%20formats%20and%20expected%20outputs%2C%20we%20provide%20sufficient%20flexibility%20in%20the%20RL%20stage%20to%20customize%20flexible%20reward%20signals%20for%20each%20domain%2C%20thereby%20improving%20cross-domain%20fusion%20and%20avoiding%20data%20conflicts.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20OCRVerse%2C%20achieving%20competitive%20results%20across%20text-centric%20and%20vision-centric%20data%20types%2C%20even%20comparable%20to%20large-scale%20open-source%20and%20closed-source%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCRVerse%253A%2520Towards%2520Holistic%2520OCR%2520in%2520End-to-End%2520Vision-Language%2520Models%26entry.906535625%3DYufeng%2520Zhong%2520and%2520Lei%2520Chen%2520and%2520Xuanle%2520Zhao%2520and%2520Wenkang%2520Han%2520and%2520Liming%2520Zheng%2520and%2520Jing%2520Huang%2520and%2520Deyang%2520Jiang%2520and%2520Yilin%2520Cao%2520and%2520Lin%2520Ma%2520and%2520Zhixiong%2520Zeng%26entry.1292438233%3DThe%2520development%2520of%2520large%2520vision%2520language%2520models%2520drives%2520the%2520demand%2520for%2520managing%252C%2520and%2520applying%2520massive%2520amounts%2520of%2520multimodal%2520data%252C%2520making%2520OCR%2520technology%252C%2520which%2520extracts%2520information%2520from%2520visual%2520images%252C%2520increasingly%2520popular.%2520However%252C%2520existing%2520OCR%2520methods%2520primarily%2520focus%2520on%2520recognizing%2520text%2520elements%2520from%2520images%2520or%2520scanned%2520documents%2520%2528Text-centric%2520OCR%2529%252C%2520neglecting%2520the%2520identification%2520of%2520visual%2520elements%2520from%2520visually%2520information-dense%2520image%2520sources%2520%2528Vision-centric%2520OCR%2529%252C%2520such%2520as%2520charts%252C%2520web%2520pages%2520and%2520science%2520plots.%2520In%2520reality%252C%2520these%2520visually%2520information-dense%2520images%2520are%2520widespread%2520on%2520the%2520internet%2520and%2520have%2520significant%2520real-world%2520application%2520value%252C%2520such%2520as%2520data%2520visualization%2520and%2520web%2520page%2520analysis.%2520In%2520this%2520technical%2520report%252C%2520we%2520propose%2520OCRVerse%252C%2520the%2520first%2520holistic%2520OCR%2520method%2520in%2520end-to-end%2520manner%2520that%2520enables%2520unified%2520text-centric%2520OCR%2520and%2520vision-centric%2520OCR.%2520To%2520this%2520end%252C%2520we%2520constructe%2520comprehensive%2520data%2520engineering%2520to%2520cover%2520a%2520wide%2520range%2520of%2520text-centric%2520documents%252C%2520such%2520as%2520newspapers%252C%2520magazines%2520and%2520books%252C%2520as%2520well%2520as%2520vision-centric%2520rendered%2520composites%252C%2520including%2520charts%252C%2520web%2520pages%2520and%2520scientific%2520plots.%2520Moreover%252C%2520we%2520propose%2520a%2520two-stage%2520SFT-RL%2520multi-domain%2520training%2520method%2520for%2520OCRVerse.%2520SFT%2520directly%2520mixes%2520cross-domain%2520data%2520to%2520train%2520and%2520establish%2520initial%2520domain%2520knowledge%252C%2520while%2520RL%2520focuses%2520on%2520designing%2520personalized%2520reward%2520strategies%2520for%2520the%2520characteristics%2520of%2520each%2520domain.%2520Specifically%252C%2520since%2520different%2520domains%2520require%2520various%2520output%2520formats%2520and%2520expected%2520outputs%252C%2520we%2520provide%2520sufficient%2520flexibility%2520in%2520the%2520RL%2520stage%2520to%2520customize%2520flexible%2520reward%2520signals%2520for%2520each%2520domain%252C%2520thereby%2520improving%2520cross-domain%2520fusion%2520and%2520avoiding%2520data%2520conflicts.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520OCRVerse%252C%2520achieving%2520competitive%2520results%2520across%2520text-centric%2520and%2520vision-centric%2520data%2520types%252C%2520even%2520comparable%2520to%2520large-scale%2520open-source%2520and%2520closed-source%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models&entry.906535625=Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Xuanle%20Zhao%20and%20Wenkang%20Han%20and%20Liming%20Zheng%20and%20Jing%20Huang%20and%20Deyang%20Jiang%20and%20Yilin%20Cao%20and%20Lin%20Ma%20and%20Zhixiong%20Zeng&entry.1292438233=The%20development%20of%20large%20vision%20language%20models%20drives%20the%20demand%20for%20managing%2C%20and%20applying%20massive%20amounts%20of%20multimodal%20data%2C%20making%20OCR%20technology%2C%20which%20extracts%20information%20from%20visual%20images%2C%20increasingly%20popular.%20However%2C%20existing%20OCR%20methods%20primarily%20focus%20on%20recognizing%20text%20elements%20from%20images%20or%20scanned%20documents%20%28Text-centric%20OCR%29%2C%20neglecting%20the%20identification%20of%20visual%20elements%20from%20visually%20information-dense%20image%20sources%20%28Vision-centric%20OCR%29%2C%20such%20as%20charts%2C%20web%20pages%20and%20science%20plots.%20In%20reality%2C%20these%20visually%20information-dense%20images%20are%20widespread%20on%20the%20internet%20and%20have%20significant%20real-world%20application%20value%2C%20such%20as%20data%20visualization%20and%20web%20page%20analysis.%20In%20this%20technical%20report%2C%20we%20propose%20OCRVerse%2C%20the%20first%20holistic%20OCR%20method%20in%20end-to-end%20manner%20that%20enables%20unified%20text-centric%20OCR%20and%20vision-centric%20OCR.%20To%20this%20end%2C%20we%20constructe%20comprehensive%20data%20engineering%20to%20cover%20a%20wide%20range%20of%20text-centric%20documents%2C%20such%20as%20newspapers%2C%20magazines%20and%20books%2C%20as%20well%20as%20vision-centric%20rendered%20composites%2C%20including%20charts%2C%20web%20pages%20and%20scientific%20plots.%20Moreover%2C%20we%20propose%20a%20two-stage%20SFT-RL%20multi-domain%20training%20method%20for%20OCRVerse.%20SFT%20directly%20mixes%20cross-domain%20data%20to%20train%20and%20establish%20initial%20domain%20knowledge%2C%20while%20RL%20focuses%20on%20designing%20personalized%20reward%20strategies%20for%20the%20characteristics%20of%20each%20domain.%20Specifically%2C%20since%20different%20domains%20require%20various%20output%20formats%20and%20expected%20outputs%2C%20we%20provide%20sufficient%20flexibility%20in%20the%20RL%20stage%20to%20customize%20flexible%20reward%20signals%20for%20each%20domain%2C%20thereby%20improving%20cross-domain%20fusion%20and%20avoiding%20data%20conflicts.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20OCRVerse%2C%20achieving%20competitive%20results%20across%20text-centric%20and%20vision-centric%20data%20types%2C%20even%20comparable%20to%20large-scale%20open-source%20and%20closed-source%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.21639v2&entry.124074799=Read"},
{"title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?", "author": "Qing'an Liu and Juntong Feng and Yuhao Wang and Xinzhe Han and Yujie Cheng and Yue Zhu and Haiwen Diao and Yunzhi Zhuge and Huchuan Lu", "abstract": "Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.", "link": "http://arxiv.org/abs/2602.04802v1", "date": "2026-02-04", "relevancy": 2.3847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISTA-Bench%3A%20Do%20Vision-Language%20Models%20Really%20Understand%20Visualized%20Text%20as%20Well%20as%20Pure%20Text%3F&body=Title%3A%20VISTA-Bench%3A%20Do%20Vision-Language%20Models%20Really%20Understand%20Visualized%20Text%20as%20Well%20as%20Pure%20Text%3F%0AAuthor%3A%20Qing%27an%20Liu%20and%20Juntong%20Feng%20and%20Yuhao%20Wang%20and%20Xinzhe%20Han%20and%20Yujie%20Cheng%20and%20Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%20cross-modal%20understanding%20across%20textual%20and%20visual%20inputs%2C%20yet%20existing%20benchmarks%20predominantly%20focus%20on%20pure-text%20queries.%20In%20real-world%20scenarios%2C%20language%20also%20frequently%20appears%20as%20visualized%20text%20embedded%20in%20images%2C%20raising%20the%20question%20of%20whether%20current%20VLMs%20handle%20such%20input%20requests%20comparably.%20We%20introduce%20VISTA-Bench%2C%20a%20systematic%20benchmark%20from%20multimodal%20perception%2C%20reasoning%2C%20to%20unimodal%20understanding%20domains.%20It%20evaluates%20visualized%20text%20understanding%20by%20contrasting%20pure-text%20and%20visualized-text%20questions%20under%20controlled%20rendering%20conditions.%20Extensive%20evaluation%20of%20over%2020%20representative%20VLMs%20reveals%20a%20pronounced%20modality%20gap%3A%20models%20that%20perform%20well%20on%20pure-text%20queries%20often%20degrade%20substantially%20when%20equivalent%20semantic%20content%20is%20presented%20as%20visualized%20text.%20This%20gap%20is%20further%20amplified%20by%20increased%20perceptual%20difficulty%2C%20highlighting%20sensitivity%20to%20rendering%20variations%20despite%20unchanged%20semantics.%20Overall%2C%20VISTA-Bench%20provides%20a%20principled%20evaluation%20framework%20to%20diagnose%20this%20limitation%20and%20to%20guide%20progress%20toward%20more%20unified%20language%20representations%20across%20tokenized%20text%20and%20pixels.%20The%20source%20dataset%20is%20available%20at%20https%3A//github.com/QingAnLiu/VISTA-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISTA-Bench%253A%2520Do%2520Vision-Language%2520Models%2520Really%2520Understand%2520Visualized%2520Text%2520as%2520Well%2520as%2520Pure%2520Text%253F%26entry.906535625%3DQing%2527an%2520Liu%2520and%2520Juntong%2520Feng%2520and%2520Yuhao%2520Wang%2520and%2520Xinzhe%2520Han%2520and%2520Yujie%2520Cheng%2520and%2520Yue%2520Zhu%2520and%2520Haiwen%2520Diao%2520and%2520Yunzhi%2520Zhuge%2520and%2520Huchuan%2520Lu%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520in%2520cross-modal%2520understanding%2520across%2520textual%2520and%2520visual%2520inputs%252C%2520yet%2520existing%2520benchmarks%2520predominantly%2520focus%2520on%2520pure-text%2520queries.%2520In%2520real-world%2520scenarios%252C%2520language%2520also%2520frequently%2520appears%2520as%2520visualized%2520text%2520embedded%2520in%2520images%252C%2520raising%2520the%2520question%2520of%2520whether%2520current%2520VLMs%2520handle%2520such%2520input%2520requests%2520comparably.%2520We%2520introduce%2520VISTA-Bench%252C%2520a%2520systematic%2520benchmark%2520from%2520multimodal%2520perception%252C%2520reasoning%252C%2520to%2520unimodal%2520understanding%2520domains.%2520It%2520evaluates%2520visualized%2520text%2520understanding%2520by%2520contrasting%2520pure-text%2520and%2520visualized-text%2520questions%2520under%2520controlled%2520rendering%2520conditions.%2520Extensive%2520evaluation%2520of%2520over%252020%2520representative%2520VLMs%2520reveals%2520a%2520pronounced%2520modality%2520gap%253A%2520models%2520that%2520perform%2520well%2520on%2520pure-text%2520queries%2520often%2520degrade%2520substantially%2520when%2520equivalent%2520semantic%2520content%2520is%2520presented%2520as%2520visualized%2520text.%2520This%2520gap%2520is%2520further%2520amplified%2520by%2520increased%2520perceptual%2520difficulty%252C%2520highlighting%2520sensitivity%2520to%2520rendering%2520variations%2520despite%2520unchanged%2520semantics.%2520Overall%252C%2520VISTA-Bench%2520provides%2520a%2520principled%2520evaluation%2520framework%2520to%2520diagnose%2520this%2520limitation%2520and%2520to%2520guide%2520progress%2520toward%2520more%2520unified%2520language%2520representations%2520across%2520tokenized%2520text%2520and%2520pixels.%2520The%2520source%2520dataset%2520is%2520available%2520at%2520https%253A//github.com/QingAnLiu/VISTA-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISTA-Bench%3A%20Do%20Vision-Language%20Models%20Really%20Understand%20Visualized%20Text%20as%20Well%20as%20Pure%20Text%3F&entry.906535625=Qing%27an%20Liu%20and%20Juntong%20Feng%20and%20Yuhao%20Wang%20and%20Xinzhe%20Han%20and%20Yujie%20Cheng%20and%20Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%20cross-modal%20understanding%20across%20textual%20and%20visual%20inputs%2C%20yet%20existing%20benchmarks%20predominantly%20focus%20on%20pure-text%20queries.%20In%20real-world%20scenarios%2C%20language%20also%20frequently%20appears%20as%20visualized%20text%20embedded%20in%20images%2C%20raising%20the%20question%20of%20whether%20current%20VLMs%20handle%20such%20input%20requests%20comparably.%20We%20introduce%20VISTA-Bench%2C%20a%20systematic%20benchmark%20from%20multimodal%20perception%2C%20reasoning%2C%20to%20unimodal%20understanding%20domains.%20It%20evaluates%20visualized%20text%20understanding%20by%20contrasting%20pure-text%20and%20visualized-text%20questions%20under%20controlled%20rendering%20conditions.%20Extensive%20evaluation%20of%20over%2020%20representative%20VLMs%20reveals%20a%20pronounced%20modality%20gap%3A%20models%20that%20perform%20well%20on%20pure-text%20queries%20often%20degrade%20substantially%20when%20equivalent%20semantic%20content%20is%20presented%20as%20visualized%20text.%20This%20gap%20is%20further%20amplified%20by%20increased%20perceptual%20difficulty%2C%20highlighting%20sensitivity%20to%20rendering%20variations%20despite%20unchanged%20semantics.%20Overall%2C%20VISTA-Bench%20provides%20a%20principled%20evaluation%20framework%20to%20diagnose%20this%20limitation%20and%20to%20guide%20progress%20toward%20more%20unified%20language%20representations%20across%20tokenized%20text%20and%20pixels.%20The%20source%20dataset%20is%20available%20at%20https%3A//github.com/QingAnLiu/VISTA-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2602.04802v1&entry.124074799=Read"},
{"title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness", "author": "Thomas Pethick and Wanyun Xie and Mete Erdogan and Kimon Antonakopoulos and Antonio Silveti-Falls and Volkan Cevher", "abstract": "This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion.", "link": "http://arxiv.org/abs/2506.01913v3", "date": "2026-02-04", "relevancy": 2.3747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4736}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness&body=Title%3A%20Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness%0AAuthor%3A%20Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Mete%20Erdogan%20and%20Kimon%20Antonakopoulos%20and%20Antonio%20Silveti-Falls%20and%20Volkan%20Cevher%0AAbstract%3A%20This%20work%20introduces%20a%20hybrid%20non-Euclidean%20optimization%20method%20which%20generalizes%20gradient%20norm%20clipping%20by%20combining%20steepest%20descent%20and%20conditional%20gradient%20approaches.%20The%20method%20achieves%20the%20best%20of%20both%20worlds%20by%20establishing%20a%20descent%20property%20under%20a%20generalized%20notion%20of%20%28%24L_0%24%2C%24L_1%24%29-smoothness.%20Weight%20decay%20is%20incorporated%20in%20a%20principled%20manner%20by%20identifying%20a%20connection%20to%20the%20Frank-Wolfe%20short%20step.%20In%20the%20stochastic%20case%2C%20we%20show%20an%20order%20optimal%20%24O%28n%5E%7B-1/4%7D%29%24%20convergence%20rate%20by%20leveraging%20a%20momentum%20based%20gradient%20estimator.%20We%20discuss%20how%20to%20instantiate%20the%20algorithms%20for%20deep%20learning%2C%20which%20we%20dub%20Clipped%20Scion%2C%20and%20demonstrate%20their%20properties%20on%20image%20classification%20and%20language%20modeling.%20The%20code%20is%20available%20at%20https%3A//github.com/LIONS-EPFL/ClippedScion.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01913v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Gradient%2520Norm%2520Clipping%2520%2526%2520Non-Euclidean%2520%2524%2528L_0%252CL_1%2529%2524-Smoothness%26entry.906535625%3DThomas%2520Pethick%2520and%2520Wanyun%2520Xie%2520and%2520Mete%2520Erdogan%2520and%2520Kimon%2520Antonakopoulos%2520and%2520Antonio%2520Silveti-Falls%2520and%2520Volkan%2520Cevher%26entry.1292438233%3DThis%2520work%2520introduces%2520a%2520hybrid%2520non-Euclidean%2520optimization%2520method%2520which%2520generalizes%2520gradient%2520norm%2520clipping%2520by%2520combining%2520steepest%2520descent%2520and%2520conditional%2520gradient%2520approaches.%2520The%2520method%2520achieves%2520the%2520best%2520of%2520both%2520worlds%2520by%2520establishing%2520a%2520descent%2520property%2520under%2520a%2520generalized%2520notion%2520of%2520%2528%2524L_0%2524%252C%2524L_1%2524%2529-smoothness.%2520Weight%2520decay%2520is%2520incorporated%2520in%2520a%2520principled%2520manner%2520by%2520identifying%2520a%2520connection%2520to%2520the%2520Frank-Wolfe%2520short%2520step.%2520In%2520the%2520stochastic%2520case%252C%2520we%2520show%2520an%2520order%2520optimal%2520%2524O%2528n%255E%257B-1/4%257D%2529%2524%2520convergence%2520rate%2520by%2520leveraging%2520a%2520momentum%2520based%2520gradient%2520estimator.%2520We%2520discuss%2520how%2520to%2520instantiate%2520the%2520algorithms%2520for%2520deep%2520learning%252C%2520which%2520we%2520dub%2520Clipped%2520Scion%252C%2520and%2520demonstrate%2520their%2520properties%2520on%2520image%2520classification%2520and%2520language%2520modeling.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/LIONS-EPFL/ClippedScion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01913v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness&entry.906535625=Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Mete%20Erdogan%20and%20Kimon%20Antonakopoulos%20and%20Antonio%20Silveti-Falls%20and%20Volkan%20Cevher&entry.1292438233=This%20work%20introduces%20a%20hybrid%20non-Euclidean%20optimization%20method%20which%20generalizes%20gradient%20norm%20clipping%20by%20combining%20steepest%20descent%20and%20conditional%20gradient%20approaches.%20The%20method%20achieves%20the%20best%20of%20both%20worlds%20by%20establishing%20a%20descent%20property%20under%20a%20generalized%20notion%20of%20%28%24L_0%24%2C%24L_1%24%29-smoothness.%20Weight%20decay%20is%20incorporated%20in%20a%20principled%20manner%20by%20identifying%20a%20connection%20to%20the%20Frank-Wolfe%20short%20step.%20In%20the%20stochastic%20case%2C%20we%20show%20an%20order%20optimal%20%24O%28n%5E%7B-1/4%7D%29%24%20convergence%20rate%20by%20leveraging%20a%20momentum%20based%20gradient%20estimator.%20We%20discuss%20how%20to%20instantiate%20the%20algorithms%20for%20deep%20learning%2C%20which%20we%20dub%20Clipped%20Scion%2C%20and%20demonstrate%20their%20properties%20on%20image%20classification%20and%20language%20modeling.%20The%20code%20is%20available%20at%20https%3A//github.com/LIONS-EPFL/ClippedScion.&entry.1838667208=http%3A//arxiv.org/abs/2506.01913v3&entry.124074799=Read"},
{"title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention", "author": "Chengtao Lv and Yumeng Shi and Yushi Huang and Ruihao Gong and Shen Ren and Wenya Wang", "abstract": "Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.", "link": "http://arxiv.org/abs/2602.04789v1", "date": "2026-02-04", "relevancy": 2.3742, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6117}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5902}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light%20Forcing%3A%20Accelerating%20Autoregressive%20Video%20Diffusion%20via%20Sparse%20Attention&body=Title%3A%20Light%20Forcing%3A%20Accelerating%20Autoregressive%20Video%20Diffusion%20via%20Sparse%20Attention%0AAuthor%3A%20Chengtao%20Lv%20and%20Yumeng%20Shi%20and%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Shen%20Ren%20and%20Wenya%20Wang%0AAbstract%3A%20Advanced%20autoregressive%20%28AR%29%20video%20generation%20models%20have%20improved%20visual%20fidelity%20and%20interactivity%2C%20but%20the%20quadratic%20complexity%20of%20attention%20remains%20a%20primary%20bottleneck%20for%20efficient%20deployment.%20While%20existing%20sparse%20attention%20solutions%20have%20shown%20promise%20on%20bidirectional%20models%2C%20we%20identify%20that%20applying%20these%20solutions%20to%20AR%20models%20leads%20to%20considerable%20performance%20degradation%20for%20two%20reasons%3A%20isolated%20consideration%20of%20chunk%20generation%20and%20insufficient%20utilization%20of%20past%20informative%20context.%20Motivated%20by%20these%20observations%2C%20we%20propose%20%5Ctextsc%7BLight%20Forcing%7D%2C%20the%20%5Ctextit%7Bfirst%7D%20sparse%20attention%20solution%20tailored%20for%20AR%20video%20generation%20models.%20It%20incorporates%20a%20%5Ctextit%7BChunk-Aware%20Growth%7D%20mechanism%20to%20quantitatively%20estimate%20the%20contribution%20of%20each%20chunk%2C%20which%20determines%20their%20sparsity%20allocation.%20This%20progressive%20sparsity%20increase%20strategy%20enables%20the%20current%20chunk%20to%20inherit%20prior%20knowledge%20in%20earlier%20chunks%20during%20generation.%20Additionally%2C%20we%20introduce%20a%20%5Ctextit%7BHierarchical%20Sparse%20Attention%7D%20to%20capture%20informative%20historical%20and%20local%20context%20in%20a%20coarse-to-fine%20manner.%20Such%20two-level%20mask%20selection%20strategy%20%28%5Cie%2C%20frame%20and%20block%20level%29%20can%20adaptively%20handle%20diverse%20attention%20patterns.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20sparse%20attention%20in%20quality%20%28%5Ceg%2C%2084.5%20on%20VBench%29%20and%20efficiency%20%28%5Ceg%2C%20%241.2%7B%5Csim%7D1.3%5Ctimes%24%20end-to-end%20speedup%29.%20Combined%20with%20FP8%20quantization%20and%20LightVAE%2C%20%5Ctextsc%7BLight%20Forcing%7D%20further%20achieves%20a%20%242.3%5Ctimes%24%20speedup%20and%2019.7%5C%2CFPS%20on%20an%20RTX~5090%20GPU.%20Code%20will%20be%20released%20at%20%5Chref%7Bhttps%3A//github.com/chengtao-lv/LightForcing%7D%7Bhttps%3A//github.com/chengtao-lv/LightForcing%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight%2520Forcing%253A%2520Accelerating%2520Autoregressive%2520Video%2520Diffusion%2520via%2520Sparse%2520Attention%26entry.906535625%3DChengtao%2520Lv%2520and%2520Yumeng%2520Shi%2520and%2520Yushi%2520Huang%2520and%2520Ruihao%2520Gong%2520and%2520Shen%2520Ren%2520and%2520Wenya%2520Wang%26entry.1292438233%3DAdvanced%2520autoregressive%2520%2528AR%2529%2520video%2520generation%2520models%2520have%2520improved%2520visual%2520fidelity%2520and%2520interactivity%252C%2520but%2520the%2520quadratic%2520complexity%2520of%2520attention%2520remains%2520a%2520primary%2520bottleneck%2520for%2520efficient%2520deployment.%2520While%2520existing%2520sparse%2520attention%2520solutions%2520have%2520shown%2520promise%2520on%2520bidirectional%2520models%252C%2520we%2520identify%2520that%2520applying%2520these%2520solutions%2520to%2520AR%2520models%2520leads%2520to%2520considerable%2520performance%2520degradation%2520for%2520two%2520reasons%253A%2520isolated%2520consideration%2520of%2520chunk%2520generation%2520and%2520insufficient%2520utilization%2520of%2520past%2520informative%2520context.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520propose%2520%255Ctextsc%257BLight%2520Forcing%257D%252C%2520the%2520%255Ctextit%257Bfirst%257D%2520sparse%2520attention%2520solution%2520tailored%2520for%2520AR%2520video%2520generation%2520models.%2520It%2520incorporates%2520a%2520%255Ctextit%257BChunk-Aware%2520Growth%257D%2520mechanism%2520to%2520quantitatively%2520estimate%2520the%2520contribution%2520of%2520each%2520chunk%252C%2520which%2520determines%2520their%2520sparsity%2520allocation.%2520This%2520progressive%2520sparsity%2520increase%2520strategy%2520enables%2520the%2520current%2520chunk%2520to%2520inherit%2520prior%2520knowledge%2520in%2520earlier%2520chunks%2520during%2520generation.%2520Additionally%252C%2520we%2520introduce%2520a%2520%255Ctextit%257BHierarchical%2520Sparse%2520Attention%257D%2520to%2520capture%2520informative%2520historical%2520and%2520local%2520context%2520in%2520a%2520coarse-to-fine%2520manner.%2520Such%2520two-level%2520mask%2520selection%2520strategy%2520%2528%255Cie%252C%2520frame%2520and%2520block%2520level%2529%2520can%2520adaptively%2520handle%2520diverse%2520attention%2520patterns.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520sparse%2520attention%2520in%2520quality%2520%2528%255Ceg%252C%252084.5%2520on%2520VBench%2529%2520and%2520efficiency%2520%2528%255Ceg%252C%2520%25241.2%257B%255Csim%257D1.3%255Ctimes%2524%2520end-to-end%2520speedup%2529.%2520Combined%2520with%2520FP8%2520quantization%2520and%2520LightVAE%252C%2520%255Ctextsc%257BLight%2520Forcing%257D%2520further%2520achieves%2520a%2520%25242.3%255Ctimes%2524%2520speedup%2520and%252019.7%255C%252CFPS%2520on%2520an%2520RTX~5090%2520GPU.%2520Code%2520will%2520be%2520released%2520at%2520%255Chref%257Bhttps%253A//github.com/chengtao-lv/LightForcing%257D%257Bhttps%253A//github.com/chengtao-lv/LightForcing%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light%20Forcing%3A%20Accelerating%20Autoregressive%20Video%20Diffusion%20via%20Sparse%20Attention&entry.906535625=Chengtao%20Lv%20and%20Yumeng%20Shi%20and%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Shen%20Ren%20and%20Wenya%20Wang&entry.1292438233=Advanced%20autoregressive%20%28AR%29%20video%20generation%20models%20have%20improved%20visual%20fidelity%20and%20interactivity%2C%20but%20the%20quadratic%20complexity%20of%20attention%20remains%20a%20primary%20bottleneck%20for%20efficient%20deployment.%20While%20existing%20sparse%20attention%20solutions%20have%20shown%20promise%20on%20bidirectional%20models%2C%20we%20identify%20that%20applying%20these%20solutions%20to%20AR%20models%20leads%20to%20considerable%20performance%20degradation%20for%20two%20reasons%3A%20isolated%20consideration%20of%20chunk%20generation%20and%20insufficient%20utilization%20of%20past%20informative%20context.%20Motivated%20by%20these%20observations%2C%20we%20propose%20%5Ctextsc%7BLight%20Forcing%7D%2C%20the%20%5Ctextit%7Bfirst%7D%20sparse%20attention%20solution%20tailored%20for%20AR%20video%20generation%20models.%20It%20incorporates%20a%20%5Ctextit%7BChunk-Aware%20Growth%7D%20mechanism%20to%20quantitatively%20estimate%20the%20contribution%20of%20each%20chunk%2C%20which%20determines%20their%20sparsity%20allocation.%20This%20progressive%20sparsity%20increase%20strategy%20enables%20the%20current%20chunk%20to%20inherit%20prior%20knowledge%20in%20earlier%20chunks%20during%20generation.%20Additionally%2C%20we%20introduce%20a%20%5Ctextit%7BHierarchical%20Sparse%20Attention%7D%20to%20capture%20informative%20historical%20and%20local%20context%20in%20a%20coarse-to-fine%20manner.%20Such%20two-level%20mask%20selection%20strategy%20%28%5Cie%2C%20frame%20and%20block%20level%29%20can%20adaptively%20handle%20diverse%20attention%20patterns.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20sparse%20attention%20in%20quality%20%28%5Ceg%2C%2084.5%20on%20VBench%29%20and%20efficiency%20%28%5Ceg%2C%20%241.2%7B%5Csim%7D1.3%5Ctimes%24%20end-to-end%20speedup%29.%20Combined%20with%20FP8%20quantization%20and%20LightVAE%2C%20%5Ctextsc%7BLight%20Forcing%7D%20further%20achieves%20a%20%242.3%5Ctimes%24%20speedup%20and%2019.7%5C%2CFPS%20on%20an%20RTX~5090%20GPU.%20Code%20will%20be%20released%20at%20%5Chref%7Bhttps%3A//github.com/chengtao-lv/LightForcing%7D%7Bhttps%3A//github.com/chengtao-lv/LightForcing%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.04789v1&entry.124074799=Read"},
{"title": "GPU-Accelerated ANNS: Quantized for Speed, Built for Change", "author": "Hunter McCoy and Zikun Wang and Prashant Pandey", "abstract": "Approximate nearest neighbor search (ANNS) is a core problem in machine learning and information retrieval applications. GPUs offer a promising path to high-performance ANNS: they provide massive parallelism for distance computations, are readily available, and can co-locate with downstream applications.\n  Despite these advantages, current GPU-accelerated ANNS systems face three key limitations. First, real-world applications operate on evolving datasets that require fast batch updates, yet most GPU indices must be rebuilt from scratch when new data arrives. Second, high-dimensional vectors strain memory bandwidth, but current GPU systems lack efficient quantization techniques that reduce data movement without introducing costly random memory accesses. Third, the data-dependent memory accesses inherent to greedy search make overlapping compute and memory difficult, leading to reduced performance.\n  We present Jasper, a GPU-native ANNS system with both high query throughput and updatability. Jasper builds on the Vamana graph index and overcomes existing bottlenecks via three contributions: (1) a CUDA batch-parallel construction algorithm that enables lock-free streaming insertions, (2) a GPU-efficient implementation of RaBitQ quantization that reduces memory footprint up to 8x without the random access penalties, and (3) an optimized greedy search kernel that increases compute utilization, resulting in better latency hiding and higher throughput.\n  Our evaluation across five datasets shows that Jasper achieves up to 1.93x higher query throughput than CAGRA and achieves up to 80% peak utilization as measured by the roofline model. Jasper's construction scales efficiently and constructs indices an average of 2.4x faster than CAGRA while providing updatability that CAGRA lacks. Compared to BANG, the previous fastest GPU Vamana implementation, Jasper delivers 19-131x faster queries.", "link": "http://arxiv.org/abs/2601.07048v3", "date": "2026-02-04", "relevancy": 2.3734, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4921}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPU-Accelerated%20ANNS%3A%20Quantized%20for%20Speed%2C%20Built%20for%20Change&body=Title%3A%20GPU-Accelerated%20ANNS%3A%20Quantized%20for%20Speed%2C%20Built%20for%20Change%0AAuthor%3A%20Hunter%20McCoy%20and%20Zikun%20Wang%20and%20Prashant%20Pandey%0AAbstract%3A%20Approximate%20nearest%20neighbor%20search%20%28ANNS%29%20is%20a%20core%20problem%20in%20machine%20learning%20and%20information%20retrieval%20applications.%20GPUs%20offer%20a%20promising%20path%20to%20high-performance%20ANNS%3A%20they%20provide%20massive%20parallelism%20for%20distance%20computations%2C%20are%20readily%20available%2C%20and%20can%20co-locate%20with%20downstream%20applications.%0A%20%20Despite%20these%20advantages%2C%20current%20GPU-accelerated%20ANNS%20systems%20face%20three%20key%20limitations.%20First%2C%20real-world%20applications%20operate%20on%20evolving%20datasets%20that%20require%20fast%20batch%20updates%2C%20yet%20most%20GPU%20indices%20must%20be%20rebuilt%20from%20scratch%20when%20new%20data%20arrives.%20Second%2C%20high-dimensional%20vectors%20strain%20memory%20bandwidth%2C%20but%20current%20GPU%20systems%20lack%20efficient%20quantization%20techniques%20that%20reduce%20data%20movement%20without%20introducing%20costly%20random%20memory%20accesses.%20Third%2C%20the%20data-dependent%20memory%20accesses%20inherent%20to%20greedy%20search%20make%20overlapping%20compute%20and%20memory%20difficult%2C%20leading%20to%20reduced%20performance.%0A%20%20We%20present%20Jasper%2C%20a%20GPU-native%20ANNS%20system%20with%20both%20high%20query%20throughput%20and%20updatability.%20Jasper%20builds%20on%20the%20Vamana%20graph%20index%20and%20overcomes%20existing%20bottlenecks%20via%20three%20contributions%3A%20%281%29%20a%20CUDA%20batch-parallel%20construction%20algorithm%20that%20enables%20lock-free%20streaming%20insertions%2C%20%282%29%20a%20GPU-efficient%20implementation%20of%20RaBitQ%20quantization%20that%20reduces%20memory%20footprint%20up%20to%208x%20without%20the%20random%20access%20penalties%2C%20and%20%283%29%20an%20optimized%20greedy%20search%20kernel%20that%20increases%20compute%20utilization%2C%20resulting%20in%20better%20latency%20hiding%20and%20higher%20throughput.%0A%20%20Our%20evaluation%20across%20five%20datasets%20shows%20that%20Jasper%20achieves%20up%20to%201.93x%20higher%20query%20throughput%20than%20CAGRA%20and%20achieves%20up%20to%2080%25%20peak%20utilization%20as%20measured%20by%20the%20roofline%20model.%20Jasper%27s%20construction%20scales%20efficiently%20and%20constructs%20indices%20an%20average%20of%202.4x%20faster%20than%20CAGRA%20while%20providing%20updatability%20that%20CAGRA%20lacks.%20Compared%20to%20BANG%2C%20the%20previous%20fastest%20GPU%20Vamana%20implementation%2C%20Jasper%20delivers%2019-131x%20faster%20queries.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07048v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPU-Accelerated%2520ANNS%253A%2520Quantized%2520for%2520Speed%252C%2520Built%2520for%2520Change%26entry.906535625%3DHunter%2520McCoy%2520and%2520Zikun%2520Wang%2520and%2520Prashant%2520Pandey%26entry.1292438233%3DApproximate%2520nearest%2520neighbor%2520search%2520%2528ANNS%2529%2520is%2520a%2520core%2520problem%2520in%2520machine%2520learning%2520and%2520information%2520retrieval%2520applications.%2520GPUs%2520offer%2520a%2520promising%2520path%2520to%2520high-performance%2520ANNS%253A%2520they%2520provide%2520massive%2520parallelism%2520for%2520distance%2520computations%252C%2520are%2520readily%2520available%252C%2520and%2520can%2520co-locate%2520with%2520downstream%2520applications.%250A%2520%2520Despite%2520these%2520advantages%252C%2520current%2520GPU-accelerated%2520ANNS%2520systems%2520face%2520three%2520key%2520limitations.%2520First%252C%2520real-world%2520applications%2520operate%2520on%2520evolving%2520datasets%2520that%2520require%2520fast%2520batch%2520updates%252C%2520yet%2520most%2520GPU%2520indices%2520must%2520be%2520rebuilt%2520from%2520scratch%2520when%2520new%2520data%2520arrives.%2520Second%252C%2520high-dimensional%2520vectors%2520strain%2520memory%2520bandwidth%252C%2520but%2520current%2520GPU%2520systems%2520lack%2520efficient%2520quantization%2520techniques%2520that%2520reduce%2520data%2520movement%2520without%2520introducing%2520costly%2520random%2520memory%2520accesses.%2520Third%252C%2520the%2520data-dependent%2520memory%2520accesses%2520inherent%2520to%2520greedy%2520search%2520make%2520overlapping%2520compute%2520and%2520memory%2520difficult%252C%2520leading%2520to%2520reduced%2520performance.%250A%2520%2520We%2520present%2520Jasper%252C%2520a%2520GPU-native%2520ANNS%2520system%2520with%2520both%2520high%2520query%2520throughput%2520and%2520updatability.%2520Jasper%2520builds%2520on%2520the%2520Vamana%2520graph%2520index%2520and%2520overcomes%2520existing%2520bottlenecks%2520via%2520three%2520contributions%253A%2520%25281%2529%2520a%2520CUDA%2520batch-parallel%2520construction%2520algorithm%2520that%2520enables%2520lock-free%2520streaming%2520insertions%252C%2520%25282%2529%2520a%2520GPU-efficient%2520implementation%2520of%2520RaBitQ%2520quantization%2520that%2520reduces%2520memory%2520footprint%2520up%2520to%25208x%2520without%2520the%2520random%2520access%2520penalties%252C%2520and%2520%25283%2529%2520an%2520optimized%2520greedy%2520search%2520kernel%2520that%2520increases%2520compute%2520utilization%252C%2520resulting%2520in%2520better%2520latency%2520hiding%2520and%2520higher%2520throughput.%250A%2520%2520Our%2520evaluation%2520across%2520five%2520datasets%2520shows%2520that%2520Jasper%2520achieves%2520up%2520to%25201.93x%2520higher%2520query%2520throughput%2520than%2520CAGRA%2520and%2520achieves%2520up%2520to%252080%2525%2520peak%2520utilization%2520as%2520measured%2520by%2520the%2520roofline%2520model.%2520Jasper%2527s%2520construction%2520scales%2520efficiently%2520and%2520constructs%2520indices%2520an%2520average%2520of%25202.4x%2520faster%2520than%2520CAGRA%2520while%2520providing%2520updatability%2520that%2520CAGRA%2520lacks.%2520Compared%2520to%2520BANG%252C%2520the%2520previous%2520fastest%2520GPU%2520Vamana%2520implementation%252C%2520Jasper%2520delivers%252019-131x%2520faster%2520queries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07048v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPU-Accelerated%20ANNS%3A%20Quantized%20for%20Speed%2C%20Built%20for%20Change&entry.906535625=Hunter%20McCoy%20and%20Zikun%20Wang%20and%20Prashant%20Pandey&entry.1292438233=Approximate%20nearest%20neighbor%20search%20%28ANNS%29%20is%20a%20core%20problem%20in%20machine%20learning%20and%20information%20retrieval%20applications.%20GPUs%20offer%20a%20promising%20path%20to%20high-performance%20ANNS%3A%20they%20provide%20massive%20parallelism%20for%20distance%20computations%2C%20are%20readily%20available%2C%20and%20can%20co-locate%20with%20downstream%20applications.%0A%20%20Despite%20these%20advantages%2C%20current%20GPU-accelerated%20ANNS%20systems%20face%20three%20key%20limitations.%20First%2C%20real-world%20applications%20operate%20on%20evolving%20datasets%20that%20require%20fast%20batch%20updates%2C%20yet%20most%20GPU%20indices%20must%20be%20rebuilt%20from%20scratch%20when%20new%20data%20arrives.%20Second%2C%20high-dimensional%20vectors%20strain%20memory%20bandwidth%2C%20but%20current%20GPU%20systems%20lack%20efficient%20quantization%20techniques%20that%20reduce%20data%20movement%20without%20introducing%20costly%20random%20memory%20accesses.%20Third%2C%20the%20data-dependent%20memory%20accesses%20inherent%20to%20greedy%20search%20make%20overlapping%20compute%20and%20memory%20difficult%2C%20leading%20to%20reduced%20performance.%0A%20%20We%20present%20Jasper%2C%20a%20GPU-native%20ANNS%20system%20with%20both%20high%20query%20throughput%20and%20updatability.%20Jasper%20builds%20on%20the%20Vamana%20graph%20index%20and%20overcomes%20existing%20bottlenecks%20via%20three%20contributions%3A%20%281%29%20a%20CUDA%20batch-parallel%20construction%20algorithm%20that%20enables%20lock-free%20streaming%20insertions%2C%20%282%29%20a%20GPU-efficient%20implementation%20of%20RaBitQ%20quantization%20that%20reduces%20memory%20footprint%20up%20to%208x%20without%20the%20random%20access%20penalties%2C%20and%20%283%29%20an%20optimized%20greedy%20search%20kernel%20that%20increases%20compute%20utilization%2C%20resulting%20in%20better%20latency%20hiding%20and%20higher%20throughput.%0A%20%20Our%20evaluation%20across%20five%20datasets%20shows%20that%20Jasper%20achieves%20up%20to%201.93x%20higher%20query%20throughput%20than%20CAGRA%20and%20achieves%20up%20to%2080%25%20peak%20utilization%20as%20measured%20by%20the%20roofline%20model.%20Jasper%27s%20construction%20scales%20efficiently%20and%20constructs%20indices%20an%20average%20of%202.4x%20faster%20than%20CAGRA%20while%20providing%20updatability%20that%20CAGRA%20lacks.%20Compared%20to%20BANG%2C%20the%20previous%20fastest%20GPU%20Vamana%20implementation%2C%20Jasper%20delivers%2019-131x%20faster%20queries.&entry.1838667208=http%3A//arxiv.org/abs/2601.07048v3&entry.124074799=Read"},
{"title": "Finding Structure in Continual Learning", "author": "Pourya Shamsolmoali and Masoumeh Zareapoor", "abstract": "Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.", "link": "http://arxiv.org/abs/2602.04555v1", "date": "2026-02-04", "relevancy": 2.3728, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20Structure%20in%20Continual%20Learning&body=Title%3A%20Finding%20Structure%20in%20Continual%20Learning%0AAuthor%3A%20Pourya%20Shamsolmoali%20and%20Masoumeh%20Zareapoor%0AAbstract%3A%20Learning%20from%20a%20stream%20of%20tasks%20usually%20pits%20plasticity%20against%20stability%3A%20acquiring%20new%20knowledge%20often%20causes%20catastrophic%20forgetting%20of%20past%20information.%20Most%20methods%20address%20this%20by%20summing%20competing%20loss%20terms%2C%20creating%20gradient%20conflicts%20that%20are%20managed%20with%20complex%20and%20often%20inefficient%20strategies%20such%20as%20external%20memory%20replay%20or%20parameter%20regularization.%20We%20propose%20a%20reformulation%20of%20the%20continual%20learning%20objective%20using%20Douglas-Rachford%20Splitting%20%28DRS%29.%20This%20reframes%20the%20learning%20process%20not%20as%20a%20direct%20trade-off%2C%20but%20as%20a%20negotiation%20between%20two%20decoupled%20objectives%3A%20one%20promoting%20plasticity%20for%20new%20tasks%20and%20the%20other%20enforcing%20stability%20of%20old%20knowledge.%20By%20iteratively%20finding%20a%20consensus%20through%20their%20proximal%20operators%2C%20DRS%20provides%20a%20more%20principled%20and%20stable%20learning%20dynamic.%20Our%20approach%20achieves%20an%20efficient%20balance%20between%20stability%20and%20plasticity%20without%20the%20need%20for%20auxiliary%20modules%20or%20complex%20add-ons%2C%20providing%20a%20simpler%20yet%20more%20powerful%20paradigm%20for%20continual%20learning%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520Structure%2520in%2520Continual%2520Learning%26entry.906535625%3DPourya%2520Shamsolmoali%2520and%2520Masoumeh%2520Zareapoor%26entry.1292438233%3DLearning%2520from%2520a%2520stream%2520of%2520tasks%2520usually%2520pits%2520plasticity%2520against%2520stability%253A%2520acquiring%2520new%2520knowledge%2520often%2520causes%2520catastrophic%2520forgetting%2520of%2520past%2520information.%2520Most%2520methods%2520address%2520this%2520by%2520summing%2520competing%2520loss%2520terms%252C%2520creating%2520gradient%2520conflicts%2520that%2520are%2520managed%2520with%2520complex%2520and%2520often%2520inefficient%2520strategies%2520such%2520as%2520external%2520memory%2520replay%2520or%2520parameter%2520regularization.%2520We%2520propose%2520a%2520reformulation%2520of%2520the%2520continual%2520learning%2520objective%2520using%2520Douglas-Rachford%2520Splitting%2520%2528DRS%2529.%2520This%2520reframes%2520the%2520learning%2520process%2520not%2520as%2520a%2520direct%2520trade-off%252C%2520but%2520as%2520a%2520negotiation%2520between%2520two%2520decoupled%2520objectives%253A%2520one%2520promoting%2520plasticity%2520for%2520new%2520tasks%2520and%2520the%2520other%2520enforcing%2520stability%2520of%2520old%2520knowledge.%2520By%2520iteratively%2520finding%2520a%2520consensus%2520through%2520their%2520proximal%2520operators%252C%2520DRS%2520provides%2520a%2520more%2520principled%2520and%2520stable%2520learning%2520dynamic.%2520Our%2520approach%2520achieves%2520an%2520efficient%2520balance%2520between%2520stability%2520and%2520plasticity%2520without%2520the%2520need%2520for%2520auxiliary%2520modules%2520or%2520complex%2520add-ons%252C%2520providing%2520a%2520simpler%2520yet%2520more%2520powerful%2520paradigm%2520for%2520continual%2520learning%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Structure%20in%20Continual%20Learning&entry.906535625=Pourya%20Shamsolmoali%20and%20Masoumeh%20Zareapoor&entry.1292438233=Learning%20from%20a%20stream%20of%20tasks%20usually%20pits%20plasticity%20against%20stability%3A%20acquiring%20new%20knowledge%20often%20causes%20catastrophic%20forgetting%20of%20past%20information.%20Most%20methods%20address%20this%20by%20summing%20competing%20loss%20terms%2C%20creating%20gradient%20conflicts%20that%20are%20managed%20with%20complex%20and%20often%20inefficient%20strategies%20such%20as%20external%20memory%20replay%20or%20parameter%20regularization.%20We%20propose%20a%20reformulation%20of%20the%20continual%20learning%20objective%20using%20Douglas-Rachford%20Splitting%20%28DRS%29.%20This%20reframes%20the%20learning%20process%20not%20as%20a%20direct%20trade-off%2C%20but%20as%20a%20negotiation%20between%20two%20decoupled%20objectives%3A%20one%20promoting%20plasticity%20for%20new%20tasks%20and%20the%20other%20enforcing%20stability%20of%20old%20knowledge.%20By%20iteratively%20finding%20a%20consensus%20through%20their%20proximal%20operators%2C%20DRS%20provides%20a%20more%20principled%20and%20stable%20learning%20dynamic.%20Our%20approach%20achieves%20an%20efficient%20balance%20between%20stability%20and%20plasticity%20without%20the%20need%20for%20auxiliary%20modules%20or%20complex%20add-ons%2C%20providing%20a%20simpler%20yet%20more%20powerful%20paradigm%20for%20continual%20learning%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.04555v1&entry.124074799=Read"},
{"title": "Sparse Attention as Compact Kernel Regression", "author": "Saul Santos and Nuno Gon\u00e7alves and Daniel C. McNamee and Marcos Treviso and Andr\u00e9 F. T Martins", "abstract": "Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $\u03b1$-entmax attention with $\u03b1= 1 + \\frac{1}{n}$ for $n \\in \\mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \\to \\infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.", "link": "http://arxiv.org/abs/2601.22766v2", "date": "2026-02-04", "relevancy": 2.349, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5176}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Attention%20as%20Compact%20Kernel%20Regression&body=Title%3A%20Sparse%20Attention%20as%20Compact%20Kernel%20Regression%0AAuthor%3A%20Saul%20Santos%20and%20Nuno%20Gon%C3%A7alves%20and%20Daniel%20C.%20McNamee%20and%20Marcos%20Treviso%20and%20Andr%C3%A9%20F.%20T%20Martins%0AAbstract%3A%20Recent%20work%20has%20revealed%20a%20link%20between%20self-attention%20mechanisms%20in%20transformers%20and%20test-time%20kernel%20regression%20via%20the%20Nadaraya-Watson%20estimator%2C%20with%20standard%20softmax%20attention%20corresponding%20to%20a%20Gaussian%20kernel.%20However%2C%20a%20kernel-theoretic%20understanding%20of%20sparse%20attention%20mechanisms%20is%20currently%20missing.%20In%20this%20paper%2C%20we%20establish%20a%20formal%20correspondence%20between%20sparse%20attention%20and%20compact%20%28bounded%20support%29%20kernels.%20We%20show%20that%20normalized%20ReLU%20and%20sparsemax%20attention%20arise%20from%20Epanechnikov%20kernel%20regression%20under%20fixed%20and%20adaptive%20normalizations%2C%20respectively.%20More%20generally%2C%20we%20demonstrate%20that%20widely%20used%20kernels%20in%20nonparametric%20density%20estimation%20--%20including%20Epanechnikov%2C%20biweight%2C%20and%20triweight%20--%20correspond%20to%20%24%CE%B1%24-entmax%20attention%20with%20%24%CE%B1%3D%201%20%2B%20%5Cfrac%7B1%7D%7Bn%7D%24%20for%20%24n%20%5Cin%20%5Cmathbb%7BN%7D%24%2C%20while%20the%20softmax/Gaussian%20relationship%20emerges%20in%20the%20limit%20%24n%20%5Cto%20%5Cinfty%24.%20This%20unified%20perspective%20explains%20how%20sparsity%20naturally%20emerges%20from%20kernel%20design%20and%20provides%20principled%20alternatives%20to%20heuristic%20top-%24k%24%20attention%20and%20other%20associative%20memory%20mechanisms.%20Experiments%20with%20a%20kernel-regression-based%20variant%20of%20transformers%20--%20Memory%20Mosaics%20--%20show%20that%20kernel-based%20sparse%20attention%20achieves%20competitive%20performance%20on%20language%20modeling%2C%20in-context%20learning%2C%20and%20length%20generalization%20tasks%2C%20offering%20a%20principled%20framework%20for%20designing%20attention%20mechanisms.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Attention%2520as%2520Compact%2520Kernel%2520Regression%26entry.906535625%3DSaul%2520Santos%2520and%2520Nuno%2520Gon%25C3%25A7alves%2520and%2520Daniel%2520C.%2520McNamee%2520and%2520Marcos%2520Treviso%2520and%2520Andr%25C3%25A9%2520F.%2520T%2520Martins%26entry.1292438233%3DRecent%2520work%2520has%2520revealed%2520a%2520link%2520between%2520self-attention%2520mechanisms%2520in%2520transformers%2520and%2520test-time%2520kernel%2520regression%2520via%2520the%2520Nadaraya-Watson%2520estimator%252C%2520with%2520standard%2520softmax%2520attention%2520corresponding%2520to%2520a%2520Gaussian%2520kernel.%2520However%252C%2520a%2520kernel-theoretic%2520understanding%2520of%2520sparse%2520attention%2520mechanisms%2520is%2520currently%2520missing.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520formal%2520correspondence%2520between%2520sparse%2520attention%2520and%2520compact%2520%2528bounded%2520support%2529%2520kernels.%2520We%2520show%2520that%2520normalized%2520ReLU%2520and%2520sparsemax%2520attention%2520arise%2520from%2520Epanechnikov%2520kernel%2520regression%2520under%2520fixed%2520and%2520adaptive%2520normalizations%252C%2520respectively.%2520More%2520generally%252C%2520we%2520demonstrate%2520that%2520widely%2520used%2520kernels%2520in%2520nonparametric%2520density%2520estimation%2520--%2520including%2520Epanechnikov%252C%2520biweight%252C%2520and%2520triweight%2520--%2520correspond%2520to%2520%2524%25CE%25B1%2524-entmax%2520attention%2520with%2520%2524%25CE%25B1%253D%25201%2520%252B%2520%255Cfrac%257B1%257D%257Bn%257D%2524%2520for%2520%2524n%2520%255Cin%2520%255Cmathbb%257BN%257D%2524%252C%2520while%2520the%2520softmax/Gaussian%2520relationship%2520emerges%2520in%2520the%2520limit%2520%2524n%2520%255Cto%2520%255Cinfty%2524.%2520This%2520unified%2520perspective%2520explains%2520how%2520sparsity%2520naturally%2520emerges%2520from%2520kernel%2520design%2520and%2520provides%2520principled%2520alternatives%2520to%2520heuristic%2520top-%2524k%2524%2520attention%2520and%2520other%2520associative%2520memory%2520mechanisms.%2520Experiments%2520with%2520a%2520kernel-regression-based%2520variant%2520of%2520transformers%2520--%2520Memory%2520Mosaics%2520--%2520show%2520that%2520kernel-based%2520sparse%2520attention%2520achieves%2520competitive%2520performance%2520on%2520language%2520modeling%252C%2520in-context%2520learning%252C%2520and%2520length%2520generalization%2520tasks%252C%2520offering%2520a%2520principled%2520framework%2520for%2520designing%2520attention%2520mechanisms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Attention%20as%20Compact%20Kernel%20Regression&entry.906535625=Saul%20Santos%20and%20Nuno%20Gon%C3%A7alves%20and%20Daniel%20C.%20McNamee%20and%20Marcos%20Treviso%20and%20Andr%C3%A9%20F.%20T%20Martins&entry.1292438233=Recent%20work%20has%20revealed%20a%20link%20between%20self-attention%20mechanisms%20in%20transformers%20and%20test-time%20kernel%20regression%20via%20the%20Nadaraya-Watson%20estimator%2C%20with%20standard%20softmax%20attention%20corresponding%20to%20a%20Gaussian%20kernel.%20However%2C%20a%20kernel-theoretic%20understanding%20of%20sparse%20attention%20mechanisms%20is%20currently%20missing.%20In%20this%20paper%2C%20we%20establish%20a%20formal%20correspondence%20between%20sparse%20attention%20and%20compact%20%28bounded%20support%29%20kernels.%20We%20show%20that%20normalized%20ReLU%20and%20sparsemax%20attention%20arise%20from%20Epanechnikov%20kernel%20regression%20under%20fixed%20and%20adaptive%20normalizations%2C%20respectively.%20More%20generally%2C%20we%20demonstrate%20that%20widely%20used%20kernels%20in%20nonparametric%20density%20estimation%20--%20including%20Epanechnikov%2C%20biweight%2C%20and%20triweight%20--%20correspond%20to%20%24%CE%B1%24-entmax%20attention%20with%20%24%CE%B1%3D%201%20%2B%20%5Cfrac%7B1%7D%7Bn%7D%24%20for%20%24n%20%5Cin%20%5Cmathbb%7BN%7D%24%2C%20while%20the%20softmax/Gaussian%20relationship%20emerges%20in%20the%20limit%20%24n%20%5Cto%20%5Cinfty%24.%20This%20unified%20perspective%20explains%20how%20sparsity%20naturally%20emerges%20from%20kernel%20design%20and%20provides%20principled%20alternatives%20to%20heuristic%20top-%24k%24%20attention%20and%20other%20associative%20memory%20mechanisms.%20Experiments%20with%20a%20kernel-regression-based%20variant%20of%20transformers%20--%20Memory%20Mosaics%20--%20show%20that%20kernel-based%20sparse%20attention%20achieves%20competitive%20performance%20on%20language%20modeling%2C%20in-context%20learning%2C%20and%20length%20generalization%20tasks%2C%20offering%20a%20principled%20framework%20for%20designing%20attention%20mechanisms.&entry.1838667208=http%3A//arxiv.org/abs/2601.22766v2&entry.124074799=Read"},
{"title": "OverThink: Slowdown Attacks on Reasoning LLMs", "author": "Abhinav Kumar and Jaechul Roh and Ali Naseh and Marzena Karpinska and Mohit Iyyer and Amir Houmansadr and Eugene Bagdasarian", "abstract": "Most flagship language models generate explicit reasoning chains, enabling inference-time scaling. However, producing these reasoning chains increases token usage (i.e., reasoning tokens), which in turn increases latency and costs. Our OverThink attack increases overhead for applications that rely on reasoning language models (RLMs) and external context by forcing them to spend substantially more reasoning tokens while still producing contextually correct answers. An adversary mounts an attack by injecting decoy reasoning problems into public content that is consumed by RLM at inference time. Because our decoys (e.g., Markov decision processes, Sudokus, etc.) are benign, they evade safety filters. We evaluate OverThink on both closed-source and open-source reasoning models across the FreshQA, SQuAD, and MuSR datasets. We also explore the attack in multi-modal settings by creating images that cause excessive reasoning. We show that the resulting slowdown transfers across models. Finally, we explore both LLM-based and systems-level defenses, and discuss the societal, financial, and energy implications of the OverThink attacks.", "link": "http://arxiv.org/abs/2502.02542v4", "date": "2026-02-04", "relevancy": 2.3473, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&body=Title%3A%20OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs%0AAuthor%3A%20Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian%0AAbstract%3A%20Most%20flagship%20language%20models%20generate%20explicit%20reasoning%20chains%2C%20enabling%20inference-time%20scaling.%20However%2C%20producing%20these%20reasoning%20chains%20increases%20token%20usage%20%28i.e.%2C%20reasoning%20tokens%29%2C%20which%20in%20turn%20increases%20latency%20and%20costs.%20Our%20OverThink%20attack%20increases%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20language%20models%20%28RLMs%29%20and%20external%20context%20by%20forcing%20them%20to%20spend%20substantially%20more%20reasoning%20tokens%20while%20still%20producing%20contextually%20correct%20answers.%20An%20adversary%20mounts%20an%20attack%20by%20injecting%20decoy%20reasoning%20problems%20into%20public%20content%20that%20is%20consumed%20by%20RLM%20at%20inference%20time.%20Because%20our%20decoys%20%28e.g.%2C%20Markov%20decision%20processes%2C%20Sudokus%2C%20etc.%29%20are%20benign%2C%20they%20evade%20safety%20filters.%20We%20evaluate%20OverThink%20on%20both%20closed-source%20and%20open-source%20reasoning%20models%20across%20the%20FreshQA%2C%20SQuAD%2C%20and%20MuSR%20datasets.%20We%20also%20explore%20the%20attack%20in%20multi-modal%20settings%20by%20creating%20images%20that%20cause%20excessive%20reasoning.%20We%20show%20that%20the%20resulting%20slowdown%20transfers%20across%20models.%20Finally%2C%20we%20explore%20both%20LLM-based%20and%20systems-level%20defenses%2C%20and%20discuss%20the%20societal%2C%20financial%2C%20and%20energy%20implications%20of%20the%20OverThink%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2502.02542v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverThink%253A%2520Slowdown%2520Attacks%2520on%2520Reasoning%2520LLMs%26entry.906535625%3DAbhinav%2520Kumar%2520and%2520Jaechul%2520Roh%2520and%2520Ali%2520Naseh%2520and%2520Marzena%2520Karpinska%2520and%2520Mohit%2520Iyyer%2520and%2520Amir%2520Houmansadr%2520and%2520Eugene%2520Bagdasarian%26entry.1292438233%3DMost%2520flagship%2520language%2520models%2520generate%2520explicit%2520reasoning%2520chains%252C%2520enabling%2520inference-time%2520scaling.%2520However%252C%2520producing%2520these%2520reasoning%2520chains%2520increases%2520token%2520usage%2520%2528i.e.%252C%2520reasoning%2520tokens%2529%252C%2520which%2520in%2520turn%2520increases%2520latency%2520and%2520costs.%2520Our%2520OverThink%2520attack%2520increases%2520overhead%2520for%2520applications%2520that%2520rely%2520on%2520reasoning%2520language%2520models%2520%2528RLMs%2529%2520and%2520external%2520context%2520by%2520forcing%2520them%2520to%2520spend%2520substantially%2520more%2520reasoning%2520tokens%2520while%2520still%2520producing%2520contextually%2520correct%2520answers.%2520An%2520adversary%2520mounts%2520an%2520attack%2520by%2520injecting%2520decoy%2520reasoning%2520problems%2520into%2520public%2520content%2520that%2520is%2520consumed%2520by%2520RLM%2520at%2520inference%2520time.%2520Because%2520our%2520decoys%2520%2528e.g.%252C%2520Markov%2520decision%2520processes%252C%2520Sudokus%252C%2520etc.%2529%2520are%2520benign%252C%2520they%2520evade%2520safety%2520filters.%2520We%2520evaluate%2520OverThink%2520on%2520both%2520closed-source%2520and%2520open-source%2520reasoning%2520models%2520across%2520the%2520FreshQA%252C%2520SQuAD%252C%2520and%2520MuSR%2520datasets.%2520We%2520also%2520explore%2520the%2520attack%2520in%2520multi-modal%2520settings%2520by%2520creating%2520images%2520that%2520cause%2520excessive%2520reasoning.%2520We%2520show%2520that%2520the%2520resulting%2520slowdown%2520transfers%2520across%2520models.%2520Finally%252C%2520we%2520explore%2520both%2520LLM-based%2520and%2520systems-level%2520defenses%252C%2520and%2520discuss%2520the%2520societal%252C%2520financial%252C%2520and%2520energy%2520implications%2520of%2520the%2520OverThink%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02542v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&entry.906535625=Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian&entry.1292438233=Most%20flagship%20language%20models%20generate%20explicit%20reasoning%20chains%2C%20enabling%20inference-time%20scaling.%20However%2C%20producing%20these%20reasoning%20chains%20increases%20token%20usage%20%28i.e.%2C%20reasoning%20tokens%29%2C%20which%20in%20turn%20increases%20latency%20and%20costs.%20Our%20OverThink%20attack%20increases%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20language%20models%20%28RLMs%29%20and%20external%20context%20by%20forcing%20them%20to%20spend%20substantially%20more%20reasoning%20tokens%20while%20still%20producing%20contextually%20correct%20answers.%20An%20adversary%20mounts%20an%20attack%20by%20injecting%20decoy%20reasoning%20problems%20into%20public%20content%20that%20is%20consumed%20by%20RLM%20at%20inference%20time.%20Because%20our%20decoys%20%28e.g.%2C%20Markov%20decision%20processes%2C%20Sudokus%2C%20etc.%29%20are%20benign%2C%20they%20evade%20safety%20filters.%20We%20evaluate%20OverThink%20on%20both%20closed-source%20and%20open-source%20reasoning%20models%20across%20the%20FreshQA%2C%20SQuAD%2C%20and%20MuSR%20datasets.%20We%20also%20explore%20the%20attack%20in%20multi-modal%20settings%20by%20creating%20images%20that%20cause%20excessive%20reasoning.%20We%20show%20that%20the%20resulting%20slowdown%20transfers%20across%20models.%20Finally%2C%20we%20explore%20both%20LLM-based%20and%20systems-level%20defenses%2C%20and%20discuss%20the%20societal%2C%20financial%2C%20and%20energy%20implications%20of%20the%20OverThink%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2502.02542v4&entry.124074799=Read"},
{"title": "PDF-HR: Pose Distance Fields for Humanoid Robots", "author": "Yi Gu and Yukang Gao and Yangchen Zhou and Xingyu Chen and Yixiao Feng and Mingle Zhao and Yunyang Mo and Zhaorui Wang and Lixin Xu and Renjing Xu", "abstract": "Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.", "link": "http://arxiv.org/abs/2602.04851v1", "date": "2026-02-04", "relevancy": 2.3455, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDF-HR%3A%20Pose%20Distance%20Fields%20for%20Humanoid%20Robots&body=Title%3A%20PDF-HR%3A%20Pose%20Distance%20Fields%20for%20Humanoid%20Robots%0AAuthor%3A%20Yi%20Gu%20and%20Yukang%20Gao%20and%20Yangchen%20Zhou%20and%20Xingyu%20Chen%20and%20Yixiao%20Feng%20and%20Mingle%20Zhao%20and%20Yunyang%20Mo%20and%20Zhaorui%20Wang%20and%20Lixin%20Xu%20and%20Renjing%20Xu%0AAbstract%3A%20Pose%20and%20motion%20priors%20play%20a%20crucial%20role%20in%20humanoid%20robotics.%20Although%20such%20priors%20have%20been%20widely%20studied%20in%20human%20motion%20recovery%20%28HMR%29%20domain%20with%20a%20range%20of%20models%2C%20their%20adoption%20for%20humanoid%20robots%20remains%20limited%2C%20largely%20due%20to%20the%20scarcity%20of%20high-quality%20humanoid%20motion%20data.%20In%20this%20work%2C%20we%20introduce%20Pose%20Distance%20Fields%20for%20Humanoid%20Robots%20%28PDF-HR%29%2C%20a%20lightweight%20prior%20that%20represents%20the%20robot%20pose%20distribution%20as%20a%20continuous%20and%20differentiable%20manifold.%20Given%20an%20arbitrary%20pose%2C%20PDF-HR%20predicts%20its%20distance%20to%20a%20large%20corpus%20of%20retargeted%20robot%20poses%2C%20yielding%20a%20smooth%20measure%20of%20pose%20plausibility%20that%20is%20well%20suited%20for%20optimization%20and%20control.%20PDF-HR%20can%20be%20integrated%20as%20a%20reward%20shaping%20term%2C%20a%20regularizer%2C%20or%20a%20standalone%20plausibility%20scorer%20across%20diverse%20pipelines.%20We%20evaluate%20PDF-HR%20on%20various%20humanoid%20tasks%2C%20including%20single-trajectory%20motion%20tracking%2C%20general%20motion%20tracking%2C%20style-based%20motion%20mimicry%2C%20and%20general%20motion%20retargeting.%20Experiments%20show%20that%20this%20plug-and-play%20prior%20consistently%20and%20substantially%20strengthens%20strong%20baselines.%20Code%20and%20models%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDF-HR%253A%2520Pose%2520Distance%2520Fields%2520for%2520Humanoid%2520Robots%26entry.906535625%3DYi%2520Gu%2520and%2520Yukang%2520Gao%2520and%2520Yangchen%2520Zhou%2520and%2520Xingyu%2520Chen%2520and%2520Yixiao%2520Feng%2520and%2520Mingle%2520Zhao%2520and%2520Yunyang%2520Mo%2520and%2520Zhaorui%2520Wang%2520and%2520Lixin%2520Xu%2520and%2520Renjing%2520Xu%26entry.1292438233%3DPose%2520and%2520motion%2520priors%2520play%2520a%2520crucial%2520role%2520in%2520humanoid%2520robotics.%2520Although%2520such%2520priors%2520have%2520been%2520widely%2520studied%2520in%2520human%2520motion%2520recovery%2520%2528HMR%2529%2520domain%2520with%2520a%2520range%2520of%2520models%252C%2520their%2520adoption%2520for%2520humanoid%2520robots%2520remains%2520limited%252C%2520largely%2520due%2520to%2520the%2520scarcity%2520of%2520high-quality%2520humanoid%2520motion%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Pose%2520Distance%2520Fields%2520for%2520Humanoid%2520Robots%2520%2528PDF-HR%2529%252C%2520a%2520lightweight%2520prior%2520that%2520represents%2520the%2520robot%2520pose%2520distribution%2520as%2520a%2520continuous%2520and%2520differentiable%2520manifold.%2520Given%2520an%2520arbitrary%2520pose%252C%2520PDF-HR%2520predicts%2520its%2520distance%2520to%2520a%2520large%2520corpus%2520of%2520retargeted%2520robot%2520poses%252C%2520yielding%2520a%2520smooth%2520measure%2520of%2520pose%2520plausibility%2520that%2520is%2520well%2520suited%2520for%2520optimization%2520and%2520control.%2520PDF-HR%2520can%2520be%2520integrated%2520as%2520a%2520reward%2520shaping%2520term%252C%2520a%2520regularizer%252C%2520or%2520a%2520standalone%2520plausibility%2520scorer%2520across%2520diverse%2520pipelines.%2520We%2520evaluate%2520PDF-HR%2520on%2520various%2520humanoid%2520tasks%252C%2520including%2520single-trajectory%2520motion%2520tracking%252C%2520general%2520motion%2520tracking%252C%2520style-based%2520motion%2520mimicry%252C%2520and%2520general%2520motion%2520retargeting.%2520Experiments%2520show%2520that%2520this%2520plug-and-play%2520prior%2520consistently%2520and%2520substantially%2520strengthens%2520strong%2520baselines.%2520Code%2520and%2520models%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDF-HR%3A%20Pose%20Distance%20Fields%20for%20Humanoid%20Robots&entry.906535625=Yi%20Gu%20and%20Yukang%20Gao%20and%20Yangchen%20Zhou%20and%20Xingyu%20Chen%20and%20Yixiao%20Feng%20and%20Mingle%20Zhao%20and%20Yunyang%20Mo%20and%20Zhaorui%20Wang%20and%20Lixin%20Xu%20and%20Renjing%20Xu&entry.1292438233=Pose%20and%20motion%20priors%20play%20a%20crucial%20role%20in%20humanoid%20robotics.%20Although%20such%20priors%20have%20been%20widely%20studied%20in%20human%20motion%20recovery%20%28HMR%29%20domain%20with%20a%20range%20of%20models%2C%20their%20adoption%20for%20humanoid%20robots%20remains%20limited%2C%20largely%20due%20to%20the%20scarcity%20of%20high-quality%20humanoid%20motion%20data.%20In%20this%20work%2C%20we%20introduce%20Pose%20Distance%20Fields%20for%20Humanoid%20Robots%20%28PDF-HR%29%2C%20a%20lightweight%20prior%20that%20represents%20the%20robot%20pose%20distribution%20as%20a%20continuous%20and%20differentiable%20manifold.%20Given%20an%20arbitrary%20pose%2C%20PDF-HR%20predicts%20its%20distance%20to%20a%20large%20corpus%20of%20retargeted%20robot%20poses%2C%20yielding%20a%20smooth%20measure%20of%20pose%20plausibility%20that%20is%20well%20suited%20for%20optimization%20and%20control.%20PDF-HR%20can%20be%20integrated%20as%20a%20reward%20shaping%20term%2C%20a%20regularizer%2C%20or%20a%20standalone%20plausibility%20scorer%20across%20diverse%20pipelines.%20We%20evaluate%20PDF-HR%20on%20various%20humanoid%20tasks%2C%20including%20single-trajectory%20motion%20tracking%2C%20general%20motion%20tracking%2C%20style-based%20motion%20mimicry%2C%20and%20general%20motion%20retargeting.%20Experiments%20show%20that%20this%20plug-and-play%20prior%20consistently%20and%20substantially%20strengthens%20strong%20baselines.%20Code%20and%20models%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2602.04851v1&entry.124074799=Read"},
{"title": "Capturing Visual Environment Structure Correlates with Control Performance", "author": "Jiahua Dong and Yunze Man and Pavel Tokmakov and Yu-Xiong Wang", "abstract": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.", "link": "http://arxiv.org/abs/2602.04880v1", "date": "2026-02-04", "relevancy": 2.3439, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Visual%20Environment%20Structure%20Correlates%20with%20Control%20Performance&body=Title%3A%20Capturing%20Visual%20Environment%20Structure%20Correlates%20with%20Control%20Performance%0AAuthor%3A%20Jiahua%20Dong%20and%20Yunze%20Man%20and%20Pavel%20Tokmakov%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20The%20choice%20of%20visual%20representation%20is%20key%20to%20scaling%20generalist%20robot%20policies.%20However%2C%20direct%20evaluation%20via%20policy%20rollouts%20is%20expensive%2C%20even%20in%20simulation.%20Existing%20proxy%20metrics%20focus%20on%20the%20representation%27s%20capacity%20to%20capture%20narrow%20aspects%20of%20the%20visual%20world%2C%20like%20object%20shape%2C%20limiting%20generalization%20across%20environments.%20In%20this%20paper%2C%20we%20take%20an%20analytical%20perspective%3A%20we%20probe%20pretrained%20visual%20encoders%20by%20measuring%20how%20well%20they%20support%20decoding%20of%20environment%20state%20--%20including%20geometry%2C%20object%20structure%2C%20and%20physical%20attributes%20--%20from%20images.%20Leveraging%20simulation%20environments%20with%20access%20to%20ground-truth%20state%2C%20we%20show%20that%20this%20probing%20accuracy%20strongly%20correlates%20with%20downstream%20policy%20performance%20across%20diverse%20environments%20and%20learning%20settings%2C%20significantly%20outperforming%20prior%20metrics%20and%20enabling%20efficient%20representation%20selection.%20More%20broadly%2C%20our%20study%20provides%20insight%20into%20the%20representational%20properties%20that%20support%20generalizable%20manipulation%2C%20suggesting%20that%20learning%20to%20encode%20the%20latent%20physical%20state%20of%20the%20environment%20is%20a%20promising%20objective%20for%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Visual%2520Environment%2520Structure%2520Correlates%2520with%2520Control%2520Performance%26entry.906535625%3DJiahua%2520Dong%2520and%2520Yunze%2520Man%2520and%2520Pavel%2520Tokmakov%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3DThe%2520choice%2520of%2520visual%2520representation%2520is%2520key%2520to%2520scaling%2520generalist%2520robot%2520policies.%2520However%252C%2520direct%2520evaluation%2520via%2520policy%2520rollouts%2520is%2520expensive%252C%2520even%2520in%2520simulation.%2520Existing%2520proxy%2520metrics%2520focus%2520on%2520the%2520representation%2527s%2520capacity%2520to%2520capture%2520narrow%2520aspects%2520of%2520the%2520visual%2520world%252C%2520like%2520object%2520shape%252C%2520limiting%2520generalization%2520across%2520environments.%2520In%2520this%2520paper%252C%2520we%2520take%2520an%2520analytical%2520perspective%253A%2520we%2520probe%2520pretrained%2520visual%2520encoders%2520by%2520measuring%2520how%2520well%2520they%2520support%2520decoding%2520of%2520environment%2520state%2520--%2520including%2520geometry%252C%2520object%2520structure%252C%2520and%2520physical%2520attributes%2520--%2520from%2520images.%2520Leveraging%2520simulation%2520environments%2520with%2520access%2520to%2520ground-truth%2520state%252C%2520we%2520show%2520that%2520this%2520probing%2520accuracy%2520strongly%2520correlates%2520with%2520downstream%2520policy%2520performance%2520across%2520diverse%2520environments%2520and%2520learning%2520settings%252C%2520significantly%2520outperforming%2520prior%2520metrics%2520and%2520enabling%2520efficient%2520representation%2520selection.%2520More%2520broadly%252C%2520our%2520study%2520provides%2520insight%2520into%2520the%2520representational%2520properties%2520that%2520support%2520generalizable%2520manipulation%252C%2520suggesting%2520that%2520learning%2520to%2520encode%2520the%2520latent%2520physical%2520state%2520of%2520the%2520environment%2520is%2520a%2520promising%2520objective%2520for%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Visual%20Environment%20Structure%20Correlates%20with%20Control%20Performance&entry.906535625=Jiahua%20Dong%20and%20Yunze%20Man%20and%20Pavel%20Tokmakov%20and%20Yu-Xiong%20Wang&entry.1292438233=The%20choice%20of%20visual%20representation%20is%20key%20to%20scaling%20generalist%20robot%20policies.%20However%2C%20direct%20evaluation%20via%20policy%20rollouts%20is%20expensive%2C%20even%20in%20simulation.%20Existing%20proxy%20metrics%20focus%20on%20the%20representation%27s%20capacity%20to%20capture%20narrow%20aspects%20of%20the%20visual%20world%2C%20like%20object%20shape%2C%20limiting%20generalization%20across%20environments.%20In%20this%20paper%2C%20we%20take%20an%20analytical%20perspective%3A%20we%20probe%20pretrained%20visual%20encoders%20by%20measuring%20how%20well%20they%20support%20decoding%20of%20environment%20state%20--%20including%20geometry%2C%20object%20structure%2C%20and%20physical%20attributes%20--%20from%20images.%20Leveraging%20simulation%20environments%20with%20access%20to%20ground-truth%20state%2C%20we%20show%20that%20this%20probing%20accuracy%20strongly%20correlates%20with%20downstream%20policy%20performance%20across%20diverse%20environments%20and%20learning%20settings%2C%20significantly%20outperforming%20prior%20metrics%20and%20enabling%20efficient%20representation%20selection.%20More%20broadly%2C%20our%20study%20provides%20insight%20into%20the%20representational%20properties%20that%20support%20generalizable%20manipulation%2C%20suggesting%20that%20learning%20to%20encode%20the%20latent%20physical%20state%20of%20the%20environment%20is%20a%20promising%20objective%20for%20control.&entry.1838667208=http%3A//arxiv.org/abs/2602.04880v1&entry.124074799=Read"},
{"title": "Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks", "author": "Towa Sano and Gouhei Tanaka", "abstract": "Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.", "link": "http://arxiv.org/abs/2602.04436v1", "date": "2026-02-04", "relevancy": 2.3407, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hand%20Gesture%20Recognition%20from%20Doppler%20Radar%20Signals%20Using%20Echo%20State%20Networks&body=Title%3A%20Hand%20Gesture%20Recognition%20from%20Doppler%20Radar%20Signals%20Using%20Echo%20State%20Networks%0AAuthor%3A%20Towa%20Sano%20and%20Gouhei%20Tanaka%0AAbstract%3A%20Hand%20gesture%20recognition%20%28HGR%29%20is%20a%20fundamental%20technology%20in%20human%20computer%20interaction%20%28HCI%29.In%20particular%2C%20HGR%20based%20on%20Doppler%20radar%20signals%20is%20suited%20for%20in-vehicle%20interfaces%20and%20robotic%20systems%2C%20necessitating%20lightweight%20and%20computationally%20efficient%20recognition%20techniques.%20However%2C%20conventional%20deep%20learning-based%20methods%20still%20suffer%20from%20high%20computational%20costs.%20To%20address%20this%20issue%2C%20we%20propose%20an%20Echo%20State%20Network%20%28ESN%29%20approach%20for%20radar-based%20HGR%2C%20using%20frequency-modulated-continuous-wave%20%28FMCW%29%20radar%20signals.%20Raw%20radar%20data%20is%20first%20converted%20into%20feature%20maps%2C%20such%20as%20range-time%20and%20Doppler-time%20maps%2C%20which%20are%20then%20fed%20into%20one%20or%20more%20recurrent%20neural%20network-based%20reservoirs.%20The%20obtained%20reservoir%20states%20are%20processed%20by%20readout%20classifiers%2C%20including%20ridge%20regression%2C%20support%20vector%20machines%2C%20and%20random%20forests.%20Comparative%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20on%20an%2011-class%20HGR%20task%20using%20the%20Soli%20dataset%20and%20surpasses%20existing%20deep%20learning%20models%20on%20a%204-class%20HGR%20task%20using%20the%20Dop-NET%20dataset.%20The%20results%20indicate%20that%20parallel%20processing%20using%20multi-reservoir%20ESNs%20are%20effective%20for%20recognizing%20temporal%20patterns%20from%20the%20multiple%20different%20feature%20maps%20in%20the%20time-space%20and%20time-frequency%20domains.%20Our%20ESN%20approaches%20achieve%20high%20recognition%20performance%20with%20low%20computational%20cost%20in%20HGR%2C%20showing%20great%20potential%20for%20more%20advanced%20HCI%20technologies%2C%20especially%20in%20resource-constrained%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHand%2520Gesture%2520Recognition%2520from%2520Doppler%2520Radar%2520Signals%2520Using%2520Echo%2520State%2520Networks%26entry.906535625%3DTowa%2520Sano%2520and%2520Gouhei%2520Tanaka%26entry.1292438233%3DHand%2520gesture%2520recognition%2520%2528HGR%2529%2520is%2520a%2520fundamental%2520technology%2520in%2520human%2520computer%2520interaction%2520%2528HCI%2529.In%2520particular%252C%2520HGR%2520based%2520on%2520Doppler%2520radar%2520signals%2520is%2520suited%2520for%2520in-vehicle%2520interfaces%2520and%2520robotic%2520systems%252C%2520necessitating%2520lightweight%2520and%2520computationally%2520efficient%2520recognition%2520techniques.%2520However%252C%2520conventional%2520deep%2520learning-based%2520methods%2520still%2520suffer%2520from%2520high%2520computational%2520costs.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520Echo%2520State%2520Network%2520%2528ESN%2529%2520approach%2520for%2520radar-based%2520HGR%252C%2520using%2520frequency-modulated-continuous-wave%2520%2528FMCW%2529%2520radar%2520signals.%2520Raw%2520radar%2520data%2520is%2520first%2520converted%2520into%2520feature%2520maps%252C%2520such%2520as%2520range-time%2520and%2520Doppler-time%2520maps%252C%2520which%2520are%2520then%2520fed%2520into%2520one%2520or%2520more%2520recurrent%2520neural%2520network-based%2520reservoirs.%2520The%2520obtained%2520reservoir%2520states%2520are%2520processed%2520by%2520readout%2520classifiers%252C%2520including%2520ridge%2520regression%252C%2520support%2520vector%2520machines%252C%2520and%2520random%2520forests.%2520Comparative%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520on%2520an%252011-class%2520HGR%2520task%2520using%2520the%2520Soli%2520dataset%2520and%2520surpasses%2520existing%2520deep%2520learning%2520models%2520on%2520a%25204-class%2520HGR%2520task%2520using%2520the%2520Dop-NET%2520dataset.%2520The%2520results%2520indicate%2520that%2520parallel%2520processing%2520using%2520multi-reservoir%2520ESNs%2520are%2520effective%2520for%2520recognizing%2520temporal%2520patterns%2520from%2520the%2520multiple%2520different%2520feature%2520maps%2520in%2520the%2520time-space%2520and%2520time-frequency%2520domains.%2520Our%2520ESN%2520approaches%2520achieve%2520high%2520recognition%2520performance%2520with%2520low%2520computational%2520cost%2520in%2520HGR%252C%2520showing%2520great%2520potential%2520for%2520more%2520advanced%2520HCI%2520technologies%252C%2520especially%2520in%2520resource-constrained%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hand%20Gesture%20Recognition%20from%20Doppler%20Radar%20Signals%20Using%20Echo%20State%20Networks&entry.906535625=Towa%20Sano%20and%20Gouhei%20Tanaka&entry.1292438233=Hand%20gesture%20recognition%20%28HGR%29%20is%20a%20fundamental%20technology%20in%20human%20computer%20interaction%20%28HCI%29.In%20particular%2C%20HGR%20based%20on%20Doppler%20radar%20signals%20is%20suited%20for%20in-vehicle%20interfaces%20and%20robotic%20systems%2C%20necessitating%20lightweight%20and%20computationally%20efficient%20recognition%20techniques.%20However%2C%20conventional%20deep%20learning-based%20methods%20still%20suffer%20from%20high%20computational%20costs.%20To%20address%20this%20issue%2C%20we%20propose%20an%20Echo%20State%20Network%20%28ESN%29%20approach%20for%20radar-based%20HGR%2C%20using%20frequency-modulated-continuous-wave%20%28FMCW%29%20radar%20signals.%20Raw%20radar%20data%20is%20first%20converted%20into%20feature%20maps%2C%20such%20as%20range-time%20and%20Doppler-time%20maps%2C%20which%20are%20then%20fed%20into%20one%20or%20more%20recurrent%20neural%20network-based%20reservoirs.%20The%20obtained%20reservoir%20states%20are%20processed%20by%20readout%20classifiers%2C%20including%20ridge%20regression%2C%20support%20vector%20machines%2C%20and%20random%20forests.%20Comparative%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20on%20an%2011-class%20HGR%20task%20using%20the%20Soli%20dataset%20and%20surpasses%20existing%20deep%20learning%20models%20on%20a%204-class%20HGR%20task%20using%20the%20Dop-NET%20dataset.%20The%20results%20indicate%20that%20parallel%20processing%20using%20multi-reservoir%20ESNs%20are%20effective%20for%20recognizing%20temporal%20patterns%20from%20the%20multiple%20different%20feature%20maps%20in%20the%20time-space%20and%20time-frequency%20domains.%20Our%20ESN%20approaches%20achieve%20high%20recognition%20performance%20with%20low%20computational%20cost%20in%20HGR%2C%20showing%20great%20potential%20for%20more%20advanced%20HCI%20technologies%2C%20especially%20in%20resource-constrained%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.04436v1&entry.124074799=Read"},
{"title": "Dictionary Learning under Symmetries via Group Representations", "author": "Subhroshekhar Ghosh and Aaron Y. R. Low and Yong Sheng Soh and Zhuohang Feng and Brendan K. Y. Tan", "abstract": "The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix orbitopes having a particular block-diagonal structure governed by the irreducible representations of the group of symmetries. This perspective enables us to introduce a band-limiting procedure which obtains dimensionality reduction in applications. We provide guarantees for our computational ansatz to provide a desirable dictionary learning outcome. We apply our paradigm to investigate the dictionary learning problem for the groups SO(2) and SO(3). While the SO(2)-orbitope admits an exact spectrahedral description, substantially less is understood about the SO(3)-orbitope. We describe a tractable spectrahedral outer approximation of the SO(3)-orbitope, and contribute an alternating minimization paradigm to perform optimization in this setting. We provide numerical experiments to highlight the efficacy of our approach in learning SO(3)-invariant dictionaries, both on synthetic and on real world data.", "link": "http://arxiv.org/abs/2305.19557v3", "date": "2026-02-04", "relevancy": 2.324, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4805}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dictionary%20Learning%20under%20Symmetries%20via%20Group%20Representations&body=Title%3A%20Dictionary%20Learning%20under%20Symmetries%20via%20Group%20Representations%0AAuthor%3A%20Subhroshekhar%20Ghosh%20and%20Aaron%20Y.%20R.%20Low%20and%20Yong%20Sheng%20Soh%20and%20Zhuohang%20Feng%20and%20Brendan%20K.%20Y.%20Tan%0AAbstract%3A%20The%20dictionary%20learning%20problem%20can%20be%20viewed%20as%20a%20data-driven%20process%20to%20learn%20a%20suitable%20transformation%20so%20that%20data%20is%20sparsely%20represented%20directly%20from%20example%20data.%20In%20this%20paper%2C%20we%20examine%20the%20problem%20of%20learning%20a%20dictionary%20that%20is%20invariant%20under%20a%20pre-specified%20group%20of%20transformations.%20Natural%20settings%20include%20Cryo-EM%2C%20multi-object%20tracking%2C%20synchronization%2C%20pose%20estimation%2C%20etc.%20We%20specifically%20study%20this%20problem%20under%20the%20lens%20of%20mathematical%20representation%20theory.%20Leveraging%20the%20power%20of%20non-abelian%20Fourier%20analysis%20for%20functions%20over%20compact%20groups%2C%20we%20prescribe%20an%20algorithmic%20recipe%20for%20learning%20dictionaries%20that%20obey%20such%20invariances.%20We%20relate%20the%20dictionary%20learning%20problem%20in%20the%20physical%20domain%2C%20which%20is%20naturally%20modelled%20as%20being%20infinite%20dimensional%2C%20with%20the%20associated%20computational%20problem%2C%20which%20is%20necessarily%20finite%20dimensional.%20We%20establish%20that%20the%20dictionary%20learning%20problem%20can%20be%20effectively%20understood%20as%20an%20optimization%20instance%20over%20certain%20matrix%20orbitopes%20having%20a%20particular%20block-diagonal%20structure%20governed%20by%20the%20irreducible%20representations%20of%20the%20group%20of%20symmetries.%20This%20perspective%20enables%20us%20to%20introduce%20a%20band-limiting%20procedure%20which%20obtains%20dimensionality%20reduction%20in%20applications.%20We%20provide%20guarantees%20for%20our%20computational%20ansatz%20to%20provide%20a%20desirable%20dictionary%20learning%20outcome.%20We%20apply%20our%20paradigm%20to%20investigate%20the%20dictionary%20learning%20problem%20for%20the%20groups%20SO%282%29%20and%20SO%283%29.%20While%20the%20SO%282%29-orbitope%20admits%20an%20exact%20spectrahedral%20description%2C%20substantially%20less%20is%20understood%20about%20the%20SO%283%29-orbitope.%20We%20describe%20a%20tractable%20spectrahedral%20outer%20approximation%20of%20the%20SO%283%29-orbitope%2C%20and%20contribute%20an%20alternating%20minimization%20paradigm%20to%20perform%20optimization%20in%20this%20setting.%20We%20provide%20numerical%20experiments%20to%20highlight%20the%20efficacy%20of%20our%20approach%20in%20learning%20SO%283%29-invariant%20dictionaries%2C%20both%20on%20synthetic%20and%20on%20real%20world%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2305.19557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDictionary%2520Learning%2520under%2520Symmetries%2520via%2520Group%2520Representations%26entry.906535625%3DSubhroshekhar%2520Ghosh%2520and%2520Aaron%2520Y.%2520R.%2520Low%2520and%2520Yong%2520Sheng%2520Soh%2520and%2520Zhuohang%2520Feng%2520and%2520Brendan%2520K.%2520Y.%2520Tan%26entry.1292438233%3DThe%2520dictionary%2520learning%2520problem%2520can%2520be%2520viewed%2520as%2520a%2520data-driven%2520process%2520to%2520learn%2520a%2520suitable%2520transformation%2520so%2520that%2520data%2520is%2520sparsely%2520represented%2520directly%2520from%2520example%2520data.%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520problem%2520of%2520learning%2520a%2520dictionary%2520that%2520is%2520invariant%2520under%2520a%2520pre-specified%2520group%2520of%2520transformations.%2520Natural%2520settings%2520include%2520Cryo-EM%252C%2520multi-object%2520tracking%252C%2520synchronization%252C%2520pose%2520estimation%252C%2520etc.%2520We%2520specifically%2520study%2520this%2520problem%2520under%2520the%2520lens%2520of%2520mathematical%2520representation%2520theory.%2520Leveraging%2520the%2520power%2520of%2520non-abelian%2520Fourier%2520analysis%2520for%2520functions%2520over%2520compact%2520groups%252C%2520we%2520prescribe%2520an%2520algorithmic%2520recipe%2520for%2520learning%2520dictionaries%2520that%2520obey%2520such%2520invariances.%2520We%2520relate%2520the%2520dictionary%2520learning%2520problem%2520in%2520the%2520physical%2520domain%252C%2520which%2520is%2520naturally%2520modelled%2520as%2520being%2520infinite%2520dimensional%252C%2520with%2520the%2520associated%2520computational%2520problem%252C%2520which%2520is%2520necessarily%2520finite%2520dimensional.%2520We%2520establish%2520that%2520the%2520dictionary%2520learning%2520problem%2520can%2520be%2520effectively%2520understood%2520as%2520an%2520optimization%2520instance%2520over%2520certain%2520matrix%2520orbitopes%2520having%2520a%2520particular%2520block-diagonal%2520structure%2520governed%2520by%2520the%2520irreducible%2520representations%2520of%2520the%2520group%2520of%2520symmetries.%2520This%2520perspective%2520enables%2520us%2520to%2520introduce%2520a%2520band-limiting%2520procedure%2520which%2520obtains%2520dimensionality%2520reduction%2520in%2520applications.%2520We%2520provide%2520guarantees%2520for%2520our%2520computational%2520ansatz%2520to%2520provide%2520a%2520desirable%2520dictionary%2520learning%2520outcome.%2520We%2520apply%2520our%2520paradigm%2520to%2520investigate%2520the%2520dictionary%2520learning%2520problem%2520for%2520the%2520groups%2520SO%25282%2529%2520and%2520SO%25283%2529.%2520While%2520the%2520SO%25282%2529-orbitope%2520admits%2520an%2520exact%2520spectrahedral%2520description%252C%2520substantially%2520less%2520is%2520understood%2520about%2520the%2520SO%25283%2529-orbitope.%2520We%2520describe%2520a%2520tractable%2520spectrahedral%2520outer%2520approximation%2520of%2520the%2520SO%25283%2529-orbitope%252C%2520and%2520contribute%2520an%2520alternating%2520minimization%2520paradigm%2520to%2520perform%2520optimization%2520in%2520this%2520setting.%2520We%2520provide%2520numerical%2520experiments%2520to%2520highlight%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520learning%2520SO%25283%2529-invariant%2520dictionaries%252C%2520both%2520on%2520synthetic%2520and%2520on%2520real%2520world%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.19557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dictionary%20Learning%20under%20Symmetries%20via%20Group%20Representations&entry.906535625=Subhroshekhar%20Ghosh%20and%20Aaron%20Y.%20R.%20Low%20and%20Yong%20Sheng%20Soh%20and%20Zhuohang%20Feng%20and%20Brendan%20K.%20Y.%20Tan&entry.1292438233=The%20dictionary%20learning%20problem%20can%20be%20viewed%20as%20a%20data-driven%20process%20to%20learn%20a%20suitable%20transformation%20so%20that%20data%20is%20sparsely%20represented%20directly%20from%20example%20data.%20In%20this%20paper%2C%20we%20examine%20the%20problem%20of%20learning%20a%20dictionary%20that%20is%20invariant%20under%20a%20pre-specified%20group%20of%20transformations.%20Natural%20settings%20include%20Cryo-EM%2C%20multi-object%20tracking%2C%20synchronization%2C%20pose%20estimation%2C%20etc.%20We%20specifically%20study%20this%20problem%20under%20the%20lens%20of%20mathematical%20representation%20theory.%20Leveraging%20the%20power%20of%20non-abelian%20Fourier%20analysis%20for%20functions%20over%20compact%20groups%2C%20we%20prescribe%20an%20algorithmic%20recipe%20for%20learning%20dictionaries%20that%20obey%20such%20invariances.%20We%20relate%20the%20dictionary%20learning%20problem%20in%20the%20physical%20domain%2C%20which%20is%20naturally%20modelled%20as%20being%20infinite%20dimensional%2C%20with%20the%20associated%20computational%20problem%2C%20which%20is%20necessarily%20finite%20dimensional.%20We%20establish%20that%20the%20dictionary%20learning%20problem%20can%20be%20effectively%20understood%20as%20an%20optimization%20instance%20over%20certain%20matrix%20orbitopes%20having%20a%20particular%20block-diagonal%20structure%20governed%20by%20the%20irreducible%20representations%20of%20the%20group%20of%20symmetries.%20This%20perspective%20enables%20us%20to%20introduce%20a%20band-limiting%20procedure%20which%20obtains%20dimensionality%20reduction%20in%20applications.%20We%20provide%20guarantees%20for%20our%20computational%20ansatz%20to%20provide%20a%20desirable%20dictionary%20learning%20outcome.%20We%20apply%20our%20paradigm%20to%20investigate%20the%20dictionary%20learning%20problem%20for%20the%20groups%20SO%282%29%20and%20SO%283%29.%20While%20the%20SO%282%29-orbitope%20admits%20an%20exact%20spectrahedral%20description%2C%20substantially%20less%20is%20understood%20about%20the%20SO%283%29-orbitope.%20We%20describe%20a%20tractable%20spectrahedral%20outer%20approximation%20of%20the%20SO%283%29-orbitope%2C%20and%20contribute%20an%20alternating%20minimization%20paradigm%20to%20perform%20optimization%20in%20this%20setting.%20We%20provide%20numerical%20experiments%20to%20highlight%20the%20efficacy%20of%20our%20approach%20in%20learning%20SO%283%29-invariant%20dictionaries%2C%20both%20on%20synthetic%20and%20on%20real%20world%20data.&entry.1838667208=http%3A//arxiv.org/abs/2305.19557v3&entry.124074799=Read"},
{"title": "Mixture of Masters: Sparse Chess Language Models with Player Routing", "author": "Giacomo Frisoni and Lorenzo Molfetta and Davide Freddi and Gianluca Moro", "abstract": "Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.", "link": "http://arxiv.org/abs/2602.04447v1", "date": "2026-02-04", "relevancy": 2.3227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4619}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Masters%3A%20Sparse%20Chess%20Language%20Models%20with%20Player%20Routing&body=Title%3A%20Mixture%20of%20Masters%3A%20Sparse%20Chess%20Language%20Models%20with%20Player%20Routing%0AAuthor%3A%20Giacomo%20Frisoni%20and%20Lorenzo%20Molfetta%20and%20Davide%20Freddi%20and%20Gianluca%20Moro%0AAbstract%3A%20Modern%20chess%20language%20models%20are%20dense%20transformers%20trained%20on%20millions%20of%20games%20played%20by%20thousands%20of%20high-rated%20individuals.%20However%2C%20these%20monolithic%20networks%20tend%20to%20collapse%20into%20mode-averaged%20behavior%2C%20where%20stylistic%20boundaries%20are%20blurred%2C%20and%20rare%20but%20effective%20strategies%20are%20suppressed.%20To%20counteract%20homogenization%2C%20we%20introduce%20Mixture-of-Masters%20%28MoM%29%2C%20the%20first%20chess%20mixture-of-experts%20model%20with%20small-sized%20GPT%20experts%20emulating%20world-class%20grandmasters.%20Each%20expert%20is%20trained%20with%20a%20combination%20of%20self-supervised%20learning%20and%20reinforcement%20learning%20guided%20by%20chess-specific%20rewards.%20For%20each%20move%2C%20a%20post-hoc%20learnable%20gating%20network%20selects%20the%20most%20appropriate%20persona%20to%20channel%20depending%20on%20the%20game%20state%2C%20allowing%20MoM%20to%20switch%20its%20style%20dynamically%24--%24e.g.%2C%20Tal%27s%20offensive%20vocation%20or%20Petrosian%27s%20defensive%20solidity.%20When%20evaluated%20against%20Stockfish%20on%20unseen%20standard%20games%2C%20MoM%20outperforms%20both%20dense%20individual%20expert%20networks%20and%20popular%20GPT%20baselines%20trained%20on%20aggregated%20data%2C%20while%20ensuring%20generation%20variety%2C%20control%2C%20and%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Masters%253A%2520Sparse%2520Chess%2520Language%2520Models%2520with%2520Player%2520Routing%26entry.906535625%3DGiacomo%2520Frisoni%2520and%2520Lorenzo%2520Molfetta%2520and%2520Davide%2520Freddi%2520and%2520Gianluca%2520Moro%26entry.1292438233%3DModern%2520chess%2520language%2520models%2520are%2520dense%2520transformers%2520trained%2520on%2520millions%2520of%2520games%2520played%2520by%2520thousands%2520of%2520high-rated%2520individuals.%2520However%252C%2520these%2520monolithic%2520networks%2520tend%2520to%2520collapse%2520into%2520mode-averaged%2520behavior%252C%2520where%2520stylistic%2520boundaries%2520are%2520blurred%252C%2520and%2520rare%2520but%2520effective%2520strategies%2520are%2520suppressed.%2520To%2520counteract%2520homogenization%252C%2520we%2520introduce%2520Mixture-of-Masters%2520%2528MoM%2529%252C%2520the%2520first%2520chess%2520mixture-of-experts%2520model%2520with%2520small-sized%2520GPT%2520experts%2520emulating%2520world-class%2520grandmasters.%2520Each%2520expert%2520is%2520trained%2520with%2520a%2520combination%2520of%2520self-supervised%2520learning%2520and%2520reinforcement%2520learning%2520guided%2520by%2520chess-specific%2520rewards.%2520For%2520each%2520move%252C%2520a%2520post-hoc%2520learnable%2520gating%2520network%2520selects%2520the%2520most%2520appropriate%2520persona%2520to%2520channel%2520depending%2520on%2520the%2520game%2520state%252C%2520allowing%2520MoM%2520to%2520switch%2520its%2520style%2520dynamically%2524--%2524e.g.%252C%2520Tal%2527s%2520offensive%2520vocation%2520or%2520Petrosian%2527s%2520defensive%2520solidity.%2520When%2520evaluated%2520against%2520Stockfish%2520on%2520unseen%2520standard%2520games%252C%2520MoM%2520outperforms%2520both%2520dense%2520individual%2520expert%2520networks%2520and%2520popular%2520GPT%2520baselines%2520trained%2520on%2520aggregated%2520data%252C%2520while%2520ensuring%2520generation%2520variety%252C%2520control%252C%2520and%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Masters%3A%20Sparse%20Chess%20Language%20Models%20with%20Player%20Routing&entry.906535625=Giacomo%20Frisoni%20and%20Lorenzo%20Molfetta%20and%20Davide%20Freddi%20and%20Gianluca%20Moro&entry.1292438233=Modern%20chess%20language%20models%20are%20dense%20transformers%20trained%20on%20millions%20of%20games%20played%20by%20thousands%20of%20high-rated%20individuals.%20However%2C%20these%20monolithic%20networks%20tend%20to%20collapse%20into%20mode-averaged%20behavior%2C%20where%20stylistic%20boundaries%20are%20blurred%2C%20and%20rare%20but%20effective%20strategies%20are%20suppressed.%20To%20counteract%20homogenization%2C%20we%20introduce%20Mixture-of-Masters%20%28MoM%29%2C%20the%20first%20chess%20mixture-of-experts%20model%20with%20small-sized%20GPT%20experts%20emulating%20world-class%20grandmasters.%20Each%20expert%20is%20trained%20with%20a%20combination%20of%20self-supervised%20learning%20and%20reinforcement%20learning%20guided%20by%20chess-specific%20rewards.%20For%20each%20move%2C%20a%20post-hoc%20learnable%20gating%20network%20selects%20the%20most%20appropriate%20persona%20to%20channel%20depending%20on%20the%20game%20state%2C%20allowing%20MoM%20to%20switch%20its%20style%20dynamically%24--%24e.g.%2C%20Tal%27s%20offensive%20vocation%20or%20Petrosian%27s%20defensive%20solidity.%20When%20evaluated%20against%20Stockfish%20on%20unseen%20standard%20games%2C%20MoM%20outperforms%20both%20dense%20individual%20expert%20networks%20and%20popular%20GPT%20baselines%20trained%20on%20aggregated%20data%2C%20while%20ensuring%20generation%20variety%2C%20control%2C%20and%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2602.04447v1&entry.124074799=Read"},
{"title": "Vision-aligned Latent Reasoning for Multi-modal Large Language Model", "author": "Byungwoo Jeon and Yoonwoo Jeong and Hyunseok Lee and Minsu Cho and Jinwoo Shin", "abstract": "Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.", "link": "http://arxiv.org/abs/2602.04476v1", "date": "2026-02-04", "relevancy": 2.32, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-aligned%20Latent%20Reasoning%20for%20Multi-modal%20Large%20Language%20Model&body=Title%3A%20Vision-aligned%20Latent%20Reasoning%20for%20Multi-modal%20Large%20Language%20Model%0AAuthor%3A%20Byungwoo%20Jeon%20and%20Yoonwoo%20Jeong%20and%20Hyunseok%20Lee%20and%20Minsu%20Cho%20and%20Jinwoo%20Shin%0AAbstract%3A%20Despite%20recent%20advancements%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20diverse%20understanding%20tasks%2C%20these%20models%20struggle%20to%20solve%20problems%20which%20require%20extensive%20multi-step%20reasoning.%20This%20is%20primarily%20due%20to%20the%20progressive%20dilution%20of%20visual%20information%20during%20long-context%20generation%2C%20which%20hinders%20their%20ability%20to%20fully%20exploit%20test-time%20scaling.%20To%20address%20this%20issue%2C%20we%20introduce%20Vision-aligned%20Latent%20Reasoning%20%28VaLR%29%2C%20a%20simple%2C%20yet%20effective%20reasoning%20framework%20that%20dynamically%20generates%20vision-aligned%20latent%20tokens%20before%20each%20Chain%20of%20Thought%20reasoning%20step%2C%20guiding%20the%20model%20to%20reason%20based%20on%20perceptual%20cues%20in%20the%20latent%20space.%20Specifically%2C%20VaLR%20is%20trained%20to%20preserve%20visual%20knowledge%20during%20reasoning%20by%20aligning%20intermediate%20embeddings%20of%20MLLM%20with%20those%20from%20vision%20encoders.%20Empirical%20results%20demonstrate%20that%20VaLR%20consistently%20outperforms%20existing%20approaches%20across%20a%20wide%20range%20of%20benchmarks%20requiring%20long-context%20understanding%20or%20precise%20visual%20perception%2C%20while%20exhibiting%20test-time%20scaling%20behavior%20not%20observed%20in%20prior%20MLLMs.%20In%20particular%2C%20VaLR%20improves%20the%20performance%20significantly%20from%2033.0%25%20to%2052.9%25%20on%20VSI-Bench%2C%20achieving%20a%2019.9%25p%20gain%20over%20Qwen2.5-VL.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-aligned%2520Latent%2520Reasoning%2520for%2520Multi-modal%2520Large%2520Language%2520Model%26entry.906535625%3DByungwoo%2520Jeon%2520and%2520Yoonwoo%2520Jeong%2520and%2520Hyunseok%2520Lee%2520and%2520Minsu%2520Cho%2520and%2520Jinwoo%2520Shin%26entry.1292438233%3DDespite%2520recent%2520advancements%2520in%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520on%2520diverse%2520understanding%2520tasks%252C%2520these%2520models%2520struggle%2520to%2520solve%2520problems%2520which%2520require%2520extensive%2520multi-step%2520reasoning.%2520This%2520is%2520primarily%2520due%2520to%2520the%2520progressive%2520dilution%2520of%2520visual%2520information%2520during%2520long-context%2520generation%252C%2520which%2520hinders%2520their%2520ability%2520to%2520fully%2520exploit%2520test-time%2520scaling.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Vision-aligned%2520Latent%2520Reasoning%2520%2528VaLR%2529%252C%2520a%2520simple%252C%2520yet%2520effective%2520reasoning%2520framework%2520that%2520dynamically%2520generates%2520vision-aligned%2520latent%2520tokens%2520before%2520each%2520Chain%2520of%2520Thought%2520reasoning%2520step%252C%2520guiding%2520the%2520model%2520to%2520reason%2520based%2520on%2520perceptual%2520cues%2520in%2520the%2520latent%2520space.%2520Specifically%252C%2520VaLR%2520is%2520trained%2520to%2520preserve%2520visual%2520knowledge%2520during%2520reasoning%2520by%2520aligning%2520intermediate%2520embeddings%2520of%2520MLLM%2520with%2520those%2520from%2520vision%2520encoders.%2520Empirical%2520results%2520demonstrate%2520that%2520VaLR%2520consistently%2520outperforms%2520existing%2520approaches%2520across%2520a%2520wide%2520range%2520of%2520benchmarks%2520requiring%2520long-context%2520understanding%2520or%2520precise%2520visual%2520perception%252C%2520while%2520exhibiting%2520test-time%2520scaling%2520behavior%2520not%2520observed%2520in%2520prior%2520MLLMs.%2520In%2520particular%252C%2520VaLR%2520improves%2520the%2520performance%2520significantly%2520from%252033.0%2525%2520to%252052.9%2525%2520on%2520VSI-Bench%252C%2520achieving%2520a%252019.9%2525p%2520gain%2520over%2520Qwen2.5-VL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-aligned%20Latent%20Reasoning%20for%20Multi-modal%20Large%20Language%20Model&entry.906535625=Byungwoo%20Jeon%20and%20Yoonwoo%20Jeong%20and%20Hyunseok%20Lee%20and%20Minsu%20Cho%20and%20Jinwoo%20Shin&entry.1292438233=Despite%20recent%20advancements%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20diverse%20understanding%20tasks%2C%20these%20models%20struggle%20to%20solve%20problems%20which%20require%20extensive%20multi-step%20reasoning.%20This%20is%20primarily%20due%20to%20the%20progressive%20dilution%20of%20visual%20information%20during%20long-context%20generation%2C%20which%20hinders%20their%20ability%20to%20fully%20exploit%20test-time%20scaling.%20To%20address%20this%20issue%2C%20we%20introduce%20Vision-aligned%20Latent%20Reasoning%20%28VaLR%29%2C%20a%20simple%2C%20yet%20effective%20reasoning%20framework%20that%20dynamically%20generates%20vision-aligned%20latent%20tokens%20before%20each%20Chain%20of%20Thought%20reasoning%20step%2C%20guiding%20the%20model%20to%20reason%20based%20on%20perceptual%20cues%20in%20the%20latent%20space.%20Specifically%2C%20VaLR%20is%20trained%20to%20preserve%20visual%20knowledge%20during%20reasoning%20by%20aligning%20intermediate%20embeddings%20of%20MLLM%20with%20those%20from%20vision%20encoders.%20Empirical%20results%20demonstrate%20that%20VaLR%20consistently%20outperforms%20existing%20approaches%20across%20a%20wide%20range%20of%20benchmarks%20requiring%20long-context%20understanding%20or%20precise%20visual%20perception%2C%20while%20exhibiting%20test-time%20scaling%20behavior%20not%20observed%20in%20prior%20MLLMs.%20In%20particular%2C%20VaLR%20improves%20the%20performance%20significantly%20from%2033.0%25%20to%2052.9%25%20on%20VSI-Bench%2C%20achieving%20a%2019.9%25p%20gain%20over%20Qwen2.5-VL.&entry.1838667208=http%3A//arxiv.org/abs/2602.04476v1&entry.124074799=Read"},
{"title": "Domain Generalization Under Posterior Drift", "author": "Yilun Zhu and Naihao Deng and Naichen Shi and Aditya Gangrade and Clayton Scott", "abstract": "Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. For the prevailing benchmark datasets in DG, there exists a single classifier that performs well across all domains.\n  In this work, we study a fundamentally different regime where the domains satisfy a \\emph{posterior drift} assumption, in which the optimal classifier might vary substantially with domain. We establish a decision-theoretic framework for DG under posterior drift, and investigate the practical implications of this framework through experiments on language and vision tasks.", "link": "http://arxiv.org/abs/2510.04441v2", "date": "2026-02-04", "relevancy": 2.3144, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.469}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4686}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20Under%20Posterior%20Drift&body=Title%3A%20Domain%20Generalization%20Under%20Posterior%20Drift%0AAuthor%3A%20Yilun%20Zhu%20and%20Naihao%20Deng%20and%20Naichen%20Shi%20and%20Aditya%20Gangrade%20and%20Clayton%20Scott%0AAbstract%3A%20Domain%20generalization%20%28DG%29%20is%20the%20problem%20of%20generalizing%20from%20several%20distributions%20%28or%20domains%29%2C%20for%20which%20labeled%20training%20data%20are%20available%2C%20to%20a%20new%20test%20domain%20for%20which%20no%20labeled%20data%20is%20available.%20For%20the%20prevailing%20benchmark%20datasets%20in%20DG%2C%20there%20exists%20a%20single%20classifier%20that%20performs%20well%20across%20all%20domains.%0A%20%20In%20this%20work%2C%20we%20study%20a%20fundamentally%20different%20regime%20where%20the%20domains%20satisfy%20a%20%5Cemph%7Bposterior%20drift%7D%20assumption%2C%20in%20which%20the%20optimal%20classifier%20might%20vary%20substantially%20with%20domain.%20We%20establish%20a%20decision-theoretic%20framework%20for%20DG%20under%20posterior%20drift%2C%20and%20investigate%20the%20practical%20implications%20of%20this%20framework%20through%20experiments%20on%20language%20and%20vision%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520Under%2520Posterior%2520Drift%26entry.906535625%3DYilun%2520Zhu%2520and%2520Naihao%2520Deng%2520and%2520Naichen%2520Shi%2520and%2520Aditya%2520Gangrade%2520and%2520Clayton%2520Scott%26entry.1292438233%3DDomain%2520generalization%2520%2528DG%2529%2520is%2520the%2520problem%2520of%2520generalizing%2520from%2520several%2520distributions%2520%2528or%2520domains%2529%252C%2520for%2520which%2520labeled%2520training%2520data%2520are%2520available%252C%2520to%2520a%2520new%2520test%2520domain%2520for%2520which%2520no%2520labeled%2520data%2520is%2520available.%2520For%2520the%2520prevailing%2520benchmark%2520datasets%2520in%2520DG%252C%2520there%2520exists%2520a%2520single%2520classifier%2520that%2520performs%2520well%2520across%2520all%2520domains.%250A%2520%2520In%2520this%2520work%252C%2520we%2520study%2520a%2520fundamentally%2520different%2520regime%2520where%2520the%2520domains%2520satisfy%2520a%2520%255Cemph%257Bposterior%2520drift%257D%2520assumption%252C%2520in%2520which%2520the%2520optimal%2520classifier%2520might%2520vary%2520substantially%2520with%2520domain.%2520We%2520establish%2520a%2520decision-theoretic%2520framework%2520for%2520DG%2520under%2520posterior%2520drift%252C%2520and%2520investigate%2520the%2520practical%2520implications%2520of%2520this%2520framework%2520through%2520experiments%2520on%2520language%2520and%2520vision%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20Under%20Posterior%20Drift&entry.906535625=Yilun%20Zhu%20and%20Naihao%20Deng%20and%20Naichen%20Shi%20and%20Aditya%20Gangrade%20and%20Clayton%20Scott&entry.1292438233=Domain%20generalization%20%28DG%29%20is%20the%20problem%20of%20generalizing%20from%20several%20distributions%20%28or%20domains%29%2C%20for%20which%20labeled%20training%20data%20are%20available%2C%20to%20a%20new%20test%20domain%20for%20which%20no%20labeled%20data%20is%20available.%20For%20the%20prevailing%20benchmark%20datasets%20in%20DG%2C%20there%20exists%20a%20single%20classifier%20that%20performs%20well%20across%20all%20domains.%0A%20%20In%20this%20work%2C%20we%20study%20a%20fundamentally%20different%20regime%20where%20the%20domains%20satisfy%20a%20%5Cemph%7Bposterior%20drift%7D%20assumption%2C%20in%20which%20the%20optimal%20classifier%20might%20vary%20substantially%20with%20domain.%20We%20establish%20a%20decision-theoretic%20framework%20for%20DG%20under%20posterior%20drift%2C%20and%20investigate%20the%20practical%20implications%20of%20this%20framework%20through%20experiments%20on%20language%20and%20vision%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2510.04441v2&entry.124074799=Read"},
{"title": "Understanding Degradation with Vision Language Model", "author": "Guanzhou Lan and Chenyi Liao and Yuqi Yang and Qianli Ma and Zhigang Wang and Dong Wang and Bin Zhao and Xuelong Li", "abstract": "Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \\textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.", "link": "http://arxiv.org/abs/2602.04565v1", "date": "2026-02-04", "relevancy": 2.2994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Degradation%20with%20Vision%20Language%20Model&body=Title%3A%20Understanding%20Degradation%20with%20Vision%20Language%20Model%0AAuthor%3A%20Guanzhou%20Lan%20and%20Chenyi%20Liao%20and%20Yuqi%20Yang%20and%20Qianli%20Ma%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20Understanding%20visual%20degradations%20is%20a%20critical%20yet%20challenging%20problem%20in%20computer%20vision.%20While%20recent%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20qualitative%20description%2C%20they%20often%20fall%20short%20in%20understanding%20the%20parametric%20physics%20underlying%20image%20degradations.%20In%20this%20work%2C%20we%20redefine%20degradation%20understanding%20as%20a%20hierarchical%20structured%20prediction%20task%2C%20necessitating%20the%20concurrent%20estimation%20of%20degradation%20types%2C%20parameter%20keys%2C%20and%20their%20continuous%20physical%20values.%20Although%20these%20sub-tasks%20operate%20in%20disparate%20spaces%2C%20we%20prove%20that%20they%20can%20be%20unified%20under%20one%20autoregressive%20next-token%20prediction%20paradigm%2C%20whose%20error%20is%20bounded%20by%20the%20value-space%20quantization%20grid.%20Building%20on%20this%20insight%2C%20we%20introduce%20DU-VLM%2C%20a%20multimodal%20chain-of-thought%20model%20trained%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20using%20structured%20rewards.%20Furthermore%2C%20we%20show%20that%20DU-VLM%20can%20serve%20as%20a%20zero-shot%20controller%20for%20pre-trained%20diffusion%20models%2C%20enabling%20high-fidelity%20image%20restoration%20without%20fine-tuning%20the%20generative%20backbone.%20We%20also%20introduce%20%5Ctextbf%7BDU-110k%7D%2C%20a%20large-scale%20dataset%20comprising%20110%2C000%20clean-degraded%20pairs%20with%20grounded%20physical%20annotations.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20generalist%20baselines%20in%20both%20accuracy%20and%20robustness%2C%20exhibiting%20generalization%20to%20unseen%20distributions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Degradation%2520with%2520Vision%2520Language%2520Model%26entry.906535625%3DGuanzhou%2520Lan%2520and%2520Chenyi%2520Liao%2520and%2520Yuqi%2520Yang%2520and%2520Qianli%2520Ma%2520and%2520Zhigang%2520Wang%2520and%2520Dong%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3DUnderstanding%2520visual%2520degradations%2520is%2520a%2520critical%2520yet%2520challenging%2520problem%2520in%2520computer%2520vision.%2520While%2520recent%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520qualitative%2520description%252C%2520they%2520often%2520fall%2520short%2520in%2520understanding%2520the%2520parametric%2520physics%2520underlying%2520image%2520degradations.%2520In%2520this%2520work%252C%2520we%2520redefine%2520degradation%2520understanding%2520as%2520a%2520hierarchical%2520structured%2520prediction%2520task%252C%2520necessitating%2520the%2520concurrent%2520estimation%2520of%2520degradation%2520types%252C%2520parameter%2520keys%252C%2520and%2520their%2520continuous%2520physical%2520values.%2520Although%2520these%2520sub-tasks%2520operate%2520in%2520disparate%2520spaces%252C%2520we%2520prove%2520that%2520they%2520can%2520be%2520unified%2520under%2520one%2520autoregressive%2520next-token%2520prediction%2520paradigm%252C%2520whose%2520error%2520is%2520bounded%2520by%2520the%2520value-space%2520quantization%2520grid.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520DU-VLM%252C%2520a%2520multimodal%2520chain-of-thought%2520model%2520trained%2520with%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520using%2520structured%2520rewards.%2520Furthermore%252C%2520we%2520show%2520that%2520DU-VLM%2520can%2520serve%2520as%2520a%2520zero-shot%2520controller%2520for%2520pre-trained%2520diffusion%2520models%252C%2520enabling%2520high-fidelity%2520image%2520restoration%2520without%2520fine-tuning%2520the%2520generative%2520backbone.%2520We%2520also%2520introduce%2520%255Ctextbf%257BDU-110k%257D%252C%2520a%2520large-scale%2520dataset%2520comprising%2520110%252C000%2520clean-degraded%2520pairs%2520with%2520grounded%2520physical%2520annotations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520generalist%2520baselines%2520in%2520both%2520accuracy%2520and%2520robustness%252C%2520exhibiting%2520generalization%2520to%2520unseen%2520distributions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Degradation%20with%20Vision%20Language%20Model&entry.906535625=Guanzhou%20Lan%20and%20Chenyi%20Liao%20and%20Yuqi%20Yang%20and%20Qianli%20Ma%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=Understanding%20visual%20degradations%20is%20a%20critical%20yet%20challenging%20problem%20in%20computer%20vision.%20While%20recent%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20qualitative%20description%2C%20they%20often%20fall%20short%20in%20understanding%20the%20parametric%20physics%20underlying%20image%20degradations.%20In%20this%20work%2C%20we%20redefine%20degradation%20understanding%20as%20a%20hierarchical%20structured%20prediction%20task%2C%20necessitating%20the%20concurrent%20estimation%20of%20degradation%20types%2C%20parameter%20keys%2C%20and%20their%20continuous%20physical%20values.%20Although%20these%20sub-tasks%20operate%20in%20disparate%20spaces%2C%20we%20prove%20that%20they%20can%20be%20unified%20under%20one%20autoregressive%20next-token%20prediction%20paradigm%2C%20whose%20error%20is%20bounded%20by%20the%20value-space%20quantization%20grid.%20Building%20on%20this%20insight%2C%20we%20introduce%20DU-VLM%2C%20a%20multimodal%20chain-of-thought%20model%20trained%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20using%20structured%20rewards.%20Furthermore%2C%20we%20show%20that%20DU-VLM%20can%20serve%20as%20a%20zero-shot%20controller%20for%20pre-trained%20diffusion%20models%2C%20enabling%20high-fidelity%20image%20restoration%20without%20fine-tuning%20the%20generative%20backbone.%20We%20also%20introduce%20%5Ctextbf%7BDU-110k%7D%2C%20a%20large-scale%20dataset%20comprising%20110%2C000%20clean-degraded%20pairs%20with%20grounded%20physical%20annotations.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20generalist%20baselines%20in%20both%20accuracy%20and%20robustness%2C%20exhibiting%20generalization%20to%20unseen%20distributions.&entry.1838667208=http%3A//arxiv.org/abs/2602.04565v1&entry.124074799=Read"},
{"title": "Radar-Inertial Odometry For Computationally Constrained Aerial Navigation", "author": "Jan Michalczyk", "abstract": "Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.", "link": "http://arxiv.org/abs/2602.04631v1", "date": "2026-02-04", "relevancy": 2.2993, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5723}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radar-Inertial%20Odometry%20For%20Computationally%20Constrained%20Aerial%20Navigation&body=Title%3A%20Radar-Inertial%20Odometry%20For%20Computationally%20Constrained%20Aerial%20Navigation%0AAuthor%3A%20Jan%20Michalczyk%0AAbstract%3A%20Recently%2C%20the%20progress%20in%20the%20radar%20sensing%20technology%20consisting%20in%20the%20miniaturization%20of%20the%20packages%20and%20increase%20in%20measuring%20precision%20has%20drawn%20the%20interest%20of%20the%20robotics%20research%20community.%20Indeed%2C%20a%20crucial%20task%20enabling%20autonomy%20in%20robotics%20is%20to%20precisely%20determine%20the%20pose%20of%20the%20robot%20in%20space.%20To%20fulfill%20this%20task%20sensor%20fusion%20algorithms%20are%20often%20used%2C%20in%20which%20data%20from%20one%20or%20several%20exteroceptive%20sensors%20like%2C%20for%20example%2C%20LiDAR%2C%20camera%2C%20laser%20ranging%20sensor%20or%20GNSS%20are%20fused%20together%20with%20the%20Inertial%20Measurement%20Unit%20%28IMU%29%20measurements%20to%20obtain%20an%20estimate%20of%20the%20navigation%20states%20of%20the%20robot.%20Nonetheless%2C%20owing%20to%20their%20particular%20sensing%20principles%2C%20some%20exteroceptive%20sensors%20are%20often%20incapacitated%20in%20extreme%20environmental%20conditions%2C%20like%20extreme%20illumination%20or%20presence%20of%20fine%20particles%20in%20the%20environment%20like%20smoke%20or%20fog.%20Radars%20are%20largely%20immune%20to%20aforementioned%20factors%20thanks%20to%20the%20characteristics%20of%20electromagnetic%20waves%20they%20use.%20In%20this%20thesis%2C%20we%20present%20Radar-Inertial%20Odometry%20%28RIO%29%20algorithms%20to%20fuse%20the%20information%20from%20IMU%20and%20radar%20in%20order%20to%20estimate%20the%20navigation%20states%20of%20a%20%28Uncrewed%20Aerial%20Vehicle%29%20UAV%20capable%20of%20running%20on%20a%20portable%20resource-constrained%20embedded%20computer%20in%20real-time%20and%20making%20use%20of%20inexpensive%2C%20consumer-grade%20sensors.%20We%20present%20novel%20RIO%20approaches%20relying%20on%20the%20multi-state%20tightly-coupled%20Extended%20Kalman%20Filter%20%28EKF%29%20and%20Factor%20Graphs%20%28FG%29%20fusing%20instantaneous%20velocities%20of%20and%20distances%20to%203D%20points%20delivered%20by%20a%20lightweight%2C%20low-cost%2C%20off-the-shelf%20Frequency%20Modulated%20Continuous%20Wave%20%28FMCW%29%20radar%20with%20IMU%20readings.%20We%20also%20show%20a%20novel%20way%20to%20exploit%20advances%20in%20deep%20learning%20to%20retrieve%203D%20point%20correspondences%20in%20sparse%20and%20noisy%20radar%20point%20clouds.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadar-Inertial%2520Odometry%2520For%2520Computationally%2520Constrained%2520Aerial%2520Navigation%26entry.906535625%3DJan%2520Michalczyk%26entry.1292438233%3DRecently%252C%2520the%2520progress%2520in%2520the%2520radar%2520sensing%2520technology%2520consisting%2520in%2520the%2520miniaturization%2520of%2520the%2520packages%2520and%2520increase%2520in%2520measuring%2520precision%2520has%2520drawn%2520the%2520interest%2520of%2520the%2520robotics%2520research%2520community.%2520Indeed%252C%2520a%2520crucial%2520task%2520enabling%2520autonomy%2520in%2520robotics%2520is%2520to%2520precisely%2520determine%2520the%2520pose%2520of%2520the%2520robot%2520in%2520space.%2520To%2520fulfill%2520this%2520task%2520sensor%2520fusion%2520algorithms%2520are%2520often%2520used%252C%2520in%2520which%2520data%2520from%2520one%2520or%2520several%2520exteroceptive%2520sensors%2520like%252C%2520for%2520example%252C%2520LiDAR%252C%2520camera%252C%2520laser%2520ranging%2520sensor%2520or%2520GNSS%2520are%2520fused%2520together%2520with%2520the%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520measurements%2520to%2520obtain%2520an%2520estimate%2520of%2520the%2520navigation%2520states%2520of%2520the%2520robot.%2520Nonetheless%252C%2520owing%2520to%2520their%2520particular%2520sensing%2520principles%252C%2520some%2520exteroceptive%2520sensors%2520are%2520often%2520incapacitated%2520in%2520extreme%2520environmental%2520conditions%252C%2520like%2520extreme%2520illumination%2520or%2520presence%2520of%2520fine%2520particles%2520in%2520the%2520environment%2520like%2520smoke%2520or%2520fog.%2520Radars%2520are%2520largely%2520immune%2520to%2520aforementioned%2520factors%2520thanks%2520to%2520the%2520characteristics%2520of%2520electromagnetic%2520waves%2520they%2520use.%2520In%2520this%2520thesis%252C%2520we%2520present%2520Radar-Inertial%2520Odometry%2520%2528RIO%2529%2520algorithms%2520to%2520fuse%2520the%2520information%2520from%2520IMU%2520and%2520radar%2520in%2520order%2520to%2520estimate%2520the%2520navigation%2520states%2520of%2520a%2520%2528Uncrewed%2520Aerial%2520Vehicle%2529%2520UAV%2520capable%2520of%2520running%2520on%2520a%2520portable%2520resource-constrained%2520embedded%2520computer%2520in%2520real-time%2520and%2520making%2520use%2520of%2520inexpensive%252C%2520consumer-grade%2520sensors.%2520We%2520present%2520novel%2520RIO%2520approaches%2520relying%2520on%2520the%2520multi-state%2520tightly-coupled%2520Extended%2520Kalman%2520Filter%2520%2528EKF%2529%2520and%2520Factor%2520Graphs%2520%2528FG%2529%2520fusing%2520instantaneous%2520velocities%2520of%2520and%2520distances%2520to%25203D%2520points%2520delivered%2520by%2520a%2520lightweight%252C%2520low-cost%252C%2520off-the-shelf%2520Frequency%2520Modulated%2520Continuous%2520Wave%2520%2528FMCW%2529%2520radar%2520with%2520IMU%2520readings.%2520We%2520also%2520show%2520a%2520novel%2520way%2520to%2520exploit%2520advances%2520in%2520deep%2520learning%2520to%2520retrieve%25203D%2520point%2520correspondences%2520in%2520sparse%2520and%2520noisy%2520radar%2520point%2520clouds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radar-Inertial%20Odometry%20For%20Computationally%20Constrained%20Aerial%20Navigation&entry.906535625=Jan%20Michalczyk&entry.1292438233=Recently%2C%20the%20progress%20in%20the%20radar%20sensing%20technology%20consisting%20in%20the%20miniaturization%20of%20the%20packages%20and%20increase%20in%20measuring%20precision%20has%20drawn%20the%20interest%20of%20the%20robotics%20research%20community.%20Indeed%2C%20a%20crucial%20task%20enabling%20autonomy%20in%20robotics%20is%20to%20precisely%20determine%20the%20pose%20of%20the%20robot%20in%20space.%20To%20fulfill%20this%20task%20sensor%20fusion%20algorithms%20are%20often%20used%2C%20in%20which%20data%20from%20one%20or%20several%20exteroceptive%20sensors%20like%2C%20for%20example%2C%20LiDAR%2C%20camera%2C%20laser%20ranging%20sensor%20or%20GNSS%20are%20fused%20together%20with%20the%20Inertial%20Measurement%20Unit%20%28IMU%29%20measurements%20to%20obtain%20an%20estimate%20of%20the%20navigation%20states%20of%20the%20robot.%20Nonetheless%2C%20owing%20to%20their%20particular%20sensing%20principles%2C%20some%20exteroceptive%20sensors%20are%20often%20incapacitated%20in%20extreme%20environmental%20conditions%2C%20like%20extreme%20illumination%20or%20presence%20of%20fine%20particles%20in%20the%20environment%20like%20smoke%20or%20fog.%20Radars%20are%20largely%20immune%20to%20aforementioned%20factors%20thanks%20to%20the%20characteristics%20of%20electromagnetic%20waves%20they%20use.%20In%20this%20thesis%2C%20we%20present%20Radar-Inertial%20Odometry%20%28RIO%29%20algorithms%20to%20fuse%20the%20information%20from%20IMU%20and%20radar%20in%20order%20to%20estimate%20the%20navigation%20states%20of%20a%20%28Uncrewed%20Aerial%20Vehicle%29%20UAV%20capable%20of%20running%20on%20a%20portable%20resource-constrained%20embedded%20computer%20in%20real-time%20and%20making%20use%20of%20inexpensive%2C%20consumer-grade%20sensors.%20We%20present%20novel%20RIO%20approaches%20relying%20on%20the%20multi-state%20tightly-coupled%20Extended%20Kalman%20Filter%20%28EKF%29%20and%20Factor%20Graphs%20%28FG%29%20fusing%20instantaneous%20velocities%20of%20and%20distances%20to%203D%20points%20delivered%20by%20a%20lightweight%2C%20low-cost%2C%20off-the-shelf%20Frequency%20Modulated%20Continuous%20Wave%20%28FMCW%29%20radar%20with%20IMU%20readings.%20We%20also%20show%20a%20novel%20way%20to%20exploit%20advances%20in%20deep%20learning%20to%20retrieve%203D%20point%20correspondences%20in%20sparse%20and%20noisy%20radar%20point%20clouds.&entry.1838667208=http%3A//arxiv.org/abs/2602.04631v1&entry.124074799=Read"},
{"title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance", "author": "Zhemeng Zhang and Jiahua Ma and Xincheng Yang and Xin Wen and Yuzhi Zhang and Boyan Li and Yiran Qin and Jin Liu and Can Zhao and Li Kang and Haoqin Hong and Zhenfei Yin and Philip Torr and Hao Su and Ruimao Zhang and Daolin Ma", "abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.", "link": "http://arxiv.org/abs/2601.20239v2", "date": "2026-02-04", "relevancy": 2.2941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5911}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5736}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TouchGuide%3A%20Inference-Time%20Steering%20of%20Visuomotor%20Policies%20via%20Touch%20Guidance&body=Title%3A%20TouchGuide%3A%20Inference-Time%20Steering%20of%20Visuomotor%20Policies%20via%20Touch%20Guidance%0AAuthor%3A%20Zhemeng%20Zhang%20and%20Jiahua%20Ma%20and%20Xincheng%20Yang%20and%20Xin%20Wen%20and%20Yuzhi%20Zhang%20and%20Boyan%20Li%20and%20Yiran%20Qin%20and%20Jin%20Liu%20and%20Can%20Zhao%20and%20Li%20Kang%20and%20Haoqin%20Hong%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Hao%20Su%20and%20Ruimao%20Zhang%20and%20Daolin%20Ma%0AAbstract%3A%20Fine-grained%20and%20contact-rich%20manipulation%20remain%20challenging%20for%20robots%2C%20largely%20due%20to%20the%20underutilization%20of%20tactile%20feedback.%20To%20address%20this%2C%20we%20introduce%20TouchGuide%2C%20a%20novel%20cross-policy%20visuo-tactile%20fusion%20paradigm%20that%20fuses%20modalities%20within%20a%20low-dimensional%20action%20space.%20Specifically%2C%20TouchGuide%20operates%20in%20two%20stages%20to%20guide%20a%20pre-trained%20diffusion%20or%20flow-matching%20visuomotor%20policy%20at%20inference%20time.%20First%2C%20the%20policy%20produces%20a%20coarse%2C%20visually-plausible%20action%20using%20only%20visual%20inputs%20during%20early%20sampling.%20Second%2C%20a%20task-specific%20Contact%20Physical%20Model%20%28CPM%29%20provides%20tactile%20guidance%20to%20steer%20and%20refine%20the%20action%2C%20ensuring%20it%20aligns%20with%20realistic%20physical%20contact%20conditions.%20Trained%20through%20contrastive%20learning%20on%20limited%20expert%20demonstrations%2C%20the%20CPM%20provides%20a%20tactile-informed%20feasibility%20score%20to%20steer%20the%20sampling%20process%20toward%20refined%20actions%20that%20satisfy%20physical%20contact%20constraints.%20Furthermore%2C%20to%20facilitate%20TouchGuide%20training%20with%20high-quality%20and%20cost-effective%20data%2C%20we%20introduce%20TacUMI%2C%20a%20data%20collection%20system.%20TacUMI%20achieves%20a%20favorable%20trade-off%20between%20precision%20and%20affordability%3B%20by%20leveraging%20rigid%20fingertips%2C%20it%20obtains%20direct%20tactile%20feedback%2C%20thereby%20enabling%20the%20collection%20of%20reliable%20tactile%20data.%20Extensive%20experiments%20on%20five%20challenging%20contact-rich%20tasks%2C%20such%20as%20shoe%20lacing%20and%20chip%20handover%2C%20show%20that%20TouchGuide%20consistently%20and%20significantly%20outperforms%20state-of-the-art%20visuo-tactile%20policies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTouchGuide%253A%2520Inference-Time%2520Steering%2520of%2520Visuomotor%2520Policies%2520via%2520Touch%2520Guidance%26entry.906535625%3DZhemeng%2520Zhang%2520and%2520Jiahua%2520Ma%2520and%2520Xincheng%2520Yang%2520and%2520Xin%2520Wen%2520and%2520Yuzhi%2520Zhang%2520and%2520Boyan%2520Li%2520and%2520Yiran%2520Qin%2520and%2520Jin%2520Liu%2520and%2520Can%2520Zhao%2520and%2520Li%2520Kang%2520and%2520Haoqin%2520Hong%2520and%2520Zhenfei%2520Yin%2520and%2520Philip%2520Torr%2520and%2520Hao%2520Su%2520and%2520Ruimao%2520Zhang%2520and%2520Daolin%2520Ma%26entry.1292438233%3DFine-grained%2520and%2520contact-rich%2520manipulation%2520remain%2520challenging%2520for%2520robots%252C%2520largely%2520due%2520to%2520the%2520underutilization%2520of%2520tactile%2520feedback.%2520To%2520address%2520this%252C%2520we%2520introduce%2520TouchGuide%252C%2520a%2520novel%2520cross-policy%2520visuo-tactile%2520fusion%2520paradigm%2520that%2520fuses%2520modalities%2520within%2520a%2520low-dimensional%2520action%2520space.%2520Specifically%252C%2520TouchGuide%2520operates%2520in%2520two%2520stages%2520to%2520guide%2520a%2520pre-trained%2520diffusion%2520or%2520flow-matching%2520visuomotor%2520policy%2520at%2520inference%2520time.%2520First%252C%2520the%2520policy%2520produces%2520a%2520coarse%252C%2520visually-plausible%2520action%2520using%2520only%2520visual%2520inputs%2520during%2520early%2520sampling.%2520Second%252C%2520a%2520task-specific%2520Contact%2520Physical%2520Model%2520%2528CPM%2529%2520provides%2520tactile%2520guidance%2520to%2520steer%2520and%2520refine%2520the%2520action%252C%2520ensuring%2520it%2520aligns%2520with%2520realistic%2520physical%2520contact%2520conditions.%2520Trained%2520through%2520contrastive%2520learning%2520on%2520limited%2520expert%2520demonstrations%252C%2520the%2520CPM%2520provides%2520a%2520tactile-informed%2520feasibility%2520score%2520to%2520steer%2520the%2520sampling%2520process%2520toward%2520refined%2520actions%2520that%2520satisfy%2520physical%2520contact%2520constraints.%2520Furthermore%252C%2520to%2520facilitate%2520TouchGuide%2520training%2520with%2520high-quality%2520and%2520cost-effective%2520data%252C%2520we%2520introduce%2520TacUMI%252C%2520a%2520data%2520collection%2520system.%2520TacUMI%2520achieves%2520a%2520favorable%2520trade-off%2520between%2520precision%2520and%2520affordability%253B%2520by%2520leveraging%2520rigid%2520fingertips%252C%2520it%2520obtains%2520direct%2520tactile%2520feedback%252C%2520thereby%2520enabling%2520the%2520collection%2520of%2520reliable%2520tactile%2520data.%2520Extensive%2520experiments%2520on%2520five%2520challenging%2520contact-rich%2520tasks%252C%2520such%2520as%2520shoe%2520lacing%2520and%2520chip%2520handover%252C%2520show%2520that%2520TouchGuide%2520consistently%2520and%2520significantly%2520outperforms%2520state-of-the-art%2520visuo-tactile%2520policies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TouchGuide%3A%20Inference-Time%20Steering%20of%20Visuomotor%20Policies%20via%20Touch%20Guidance&entry.906535625=Zhemeng%20Zhang%20and%20Jiahua%20Ma%20and%20Xincheng%20Yang%20and%20Xin%20Wen%20and%20Yuzhi%20Zhang%20and%20Boyan%20Li%20and%20Yiran%20Qin%20and%20Jin%20Liu%20and%20Can%20Zhao%20and%20Li%20Kang%20and%20Haoqin%20Hong%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Hao%20Su%20and%20Ruimao%20Zhang%20and%20Daolin%20Ma&entry.1292438233=Fine-grained%20and%20contact-rich%20manipulation%20remain%20challenging%20for%20robots%2C%20largely%20due%20to%20the%20underutilization%20of%20tactile%20feedback.%20To%20address%20this%2C%20we%20introduce%20TouchGuide%2C%20a%20novel%20cross-policy%20visuo-tactile%20fusion%20paradigm%20that%20fuses%20modalities%20within%20a%20low-dimensional%20action%20space.%20Specifically%2C%20TouchGuide%20operates%20in%20two%20stages%20to%20guide%20a%20pre-trained%20diffusion%20or%20flow-matching%20visuomotor%20policy%20at%20inference%20time.%20First%2C%20the%20policy%20produces%20a%20coarse%2C%20visually-plausible%20action%20using%20only%20visual%20inputs%20during%20early%20sampling.%20Second%2C%20a%20task-specific%20Contact%20Physical%20Model%20%28CPM%29%20provides%20tactile%20guidance%20to%20steer%20and%20refine%20the%20action%2C%20ensuring%20it%20aligns%20with%20realistic%20physical%20contact%20conditions.%20Trained%20through%20contrastive%20learning%20on%20limited%20expert%20demonstrations%2C%20the%20CPM%20provides%20a%20tactile-informed%20feasibility%20score%20to%20steer%20the%20sampling%20process%20toward%20refined%20actions%20that%20satisfy%20physical%20contact%20constraints.%20Furthermore%2C%20to%20facilitate%20TouchGuide%20training%20with%20high-quality%20and%20cost-effective%20data%2C%20we%20introduce%20TacUMI%2C%20a%20data%20collection%20system.%20TacUMI%20achieves%20a%20favorable%20trade-off%20between%20precision%20and%20affordability%3B%20by%20leveraging%20rigid%20fingertips%2C%20it%20obtains%20direct%20tactile%20feedback%2C%20thereby%20enabling%20the%20collection%20of%20reliable%20tactile%20data.%20Extensive%20experiments%20on%20five%20challenging%20contact-rich%20tasks%2C%20such%20as%20shoe%20lacing%20and%20chip%20handover%2C%20show%20that%20TouchGuide%20consistently%20and%20significantly%20outperforms%20state-of-the-art%20visuo-tactile%20policies.&entry.1838667208=http%3A//arxiv.org/abs/2601.20239v2&entry.124074799=Read"},
{"title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning", "author": "Huy Le and Nhat Chung and Tung Kieu and Jingkang Yang and Ngan Le", "abstract": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design. Code is available at: https://github.com/Fsoft-AIC/UNO", "link": "http://arxiv.org/abs/2509.06165v5", "date": "2026-02-04", "relevancy": 2.2931, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5835}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNO%3A%20Unifying%20One-stage%20Video%20Scene%20Graph%20Generation%20via%20Object-Centric%20Visual%20Representation%20Learning&body=Title%3A%20UNO%3A%20Unifying%20One-stage%20Video%20Scene%20Graph%20Generation%20via%20Object-Centric%20Visual%20Representation%20Learning%0AAuthor%3A%20Huy%20Le%20and%20Nhat%20Chung%20and%20Tung%20Kieu%20and%20Jingkang%20Yang%20and%20Ngan%20Le%0AAbstract%3A%20Video%20Scene%20Graph%20Generation%20%28VidSGG%29%20aims%20to%20represent%20dynamic%20visual%20content%20by%20detecting%20objects%20and%20modeling%20their%20temporal%20interactions%20as%20structured%20graphs.%20Prior%20studies%20typically%20target%20either%20coarse-grained%20box-level%20or%20fine-grained%20panoptic%20pixel-level%20VidSGG%2C%20often%20requiring%20task-specific%20architectures%20and%20multi-stage%20training%20pipelines.%20In%20this%20paper%2C%20we%20present%20UNO%20%28UNified%20Object-centric%20VidSGG%29%2C%20a%20single-stage%2C%20unified%20framework%20that%20jointly%20addresses%20both%20tasks%20within%20an%20end-to-end%20architecture.%20UNO%20is%20designed%20to%20minimize%20task-specific%20modifications%20and%20maximize%20parameter%20sharing%2C%20enabling%20generalization%20across%20different%20levels%20of%20visual%20granularity.%20The%20core%20of%20UNO%20is%20an%20extended%20slot%20attention%20mechanism%20that%20decomposes%20visual%20features%20into%20object%20and%20relation%20slots.%20To%20ensure%20robust%20temporal%20modeling%2C%20we%20introduce%20object%20temporal%20consistency%20learning%2C%20which%20enforces%20consistent%20object%20representations%20across%20frames%20without%20relying%20on%20explicit%20tracking%20modules.%20Additionally%2C%20a%20dynamic%20triplet%20prediction%20module%20links%20relation%20slots%20to%20corresponding%20object%20pairs%2C%20capturing%20evolving%20interactions%20over%20time.%20We%20evaluate%20UNO%20on%20standard%20box-level%20and%20pixel-level%20VidSGG%20benchmarks.%20Results%20demonstrate%20that%20UNO%20not%20only%20achieves%20competitive%20performance%20across%20both%20tasks%20but%20also%20offers%20improved%20efficiency%20through%20a%20unified%2C%20object-centric%20design.%20Code%20is%20available%20at%3A%20https%3A//github.com/Fsoft-AIC/UNO%0ALink%3A%20http%3A//arxiv.org/abs/2509.06165v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNO%253A%2520Unifying%2520One-stage%2520Video%2520Scene%2520Graph%2520Generation%2520via%2520Object-Centric%2520Visual%2520Representation%2520Learning%26entry.906535625%3DHuy%2520Le%2520and%2520Nhat%2520Chung%2520and%2520Tung%2520Kieu%2520and%2520Jingkang%2520Yang%2520and%2520Ngan%2520Le%26entry.1292438233%3DVideo%2520Scene%2520Graph%2520Generation%2520%2528VidSGG%2529%2520aims%2520to%2520represent%2520dynamic%2520visual%2520content%2520by%2520detecting%2520objects%2520and%2520modeling%2520their%2520temporal%2520interactions%2520as%2520structured%2520graphs.%2520Prior%2520studies%2520typically%2520target%2520either%2520coarse-grained%2520box-level%2520or%2520fine-grained%2520panoptic%2520pixel-level%2520VidSGG%252C%2520often%2520requiring%2520task-specific%2520architectures%2520and%2520multi-stage%2520training%2520pipelines.%2520In%2520this%2520paper%252C%2520we%2520present%2520UNO%2520%2528UNified%2520Object-centric%2520VidSGG%2529%252C%2520a%2520single-stage%252C%2520unified%2520framework%2520that%2520jointly%2520addresses%2520both%2520tasks%2520within%2520an%2520end-to-end%2520architecture.%2520UNO%2520is%2520designed%2520to%2520minimize%2520task-specific%2520modifications%2520and%2520maximize%2520parameter%2520sharing%252C%2520enabling%2520generalization%2520across%2520different%2520levels%2520of%2520visual%2520granularity.%2520The%2520core%2520of%2520UNO%2520is%2520an%2520extended%2520slot%2520attention%2520mechanism%2520that%2520decomposes%2520visual%2520features%2520into%2520object%2520and%2520relation%2520slots.%2520To%2520ensure%2520robust%2520temporal%2520modeling%252C%2520we%2520introduce%2520object%2520temporal%2520consistency%2520learning%252C%2520which%2520enforces%2520consistent%2520object%2520representations%2520across%2520frames%2520without%2520relying%2520on%2520explicit%2520tracking%2520modules.%2520Additionally%252C%2520a%2520dynamic%2520triplet%2520prediction%2520module%2520links%2520relation%2520slots%2520to%2520corresponding%2520object%2520pairs%252C%2520capturing%2520evolving%2520interactions%2520over%2520time.%2520We%2520evaluate%2520UNO%2520on%2520standard%2520box-level%2520and%2520pixel-level%2520VidSGG%2520benchmarks.%2520Results%2520demonstrate%2520that%2520UNO%2520not%2520only%2520achieves%2520competitive%2520performance%2520across%2520both%2520tasks%2520but%2520also%2520offers%2520improved%2520efficiency%2520through%2520a%2520unified%252C%2520object-centric%2520design.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/Fsoft-AIC/UNO%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06165v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNO%3A%20Unifying%20One-stage%20Video%20Scene%20Graph%20Generation%20via%20Object-Centric%20Visual%20Representation%20Learning&entry.906535625=Huy%20Le%20and%20Nhat%20Chung%20and%20Tung%20Kieu%20and%20Jingkang%20Yang%20and%20Ngan%20Le&entry.1292438233=Video%20Scene%20Graph%20Generation%20%28VidSGG%29%20aims%20to%20represent%20dynamic%20visual%20content%20by%20detecting%20objects%20and%20modeling%20their%20temporal%20interactions%20as%20structured%20graphs.%20Prior%20studies%20typically%20target%20either%20coarse-grained%20box-level%20or%20fine-grained%20panoptic%20pixel-level%20VidSGG%2C%20often%20requiring%20task-specific%20architectures%20and%20multi-stage%20training%20pipelines.%20In%20this%20paper%2C%20we%20present%20UNO%20%28UNified%20Object-centric%20VidSGG%29%2C%20a%20single-stage%2C%20unified%20framework%20that%20jointly%20addresses%20both%20tasks%20within%20an%20end-to-end%20architecture.%20UNO%20is%20designed%20to%20minimize%20task-specific%20modifications%20and%20maximize%20parameter%20sharing%2C%20enabling%20generalization%20across%20different%20levels%20of%20visual%20granularity.%20The%20core%20of%20UNO%20is%20an%20extended%20slot%20attention%20mechanism%20that%20decomposes%20visual%20features%20into%20object%20and%20relation%20slots.%20To%20ensure%20robust%20temporal%20modeling%2C%20we%20introduce%20object%20temporal%20consistency%20learning%2C%20which%20enforces%20consistent%20object%20representations%20across%20frames%20without%20relying%20on%20explicit%20tracking%20modules.%20Additionally%2C%20a%20dynamic%20triplet%20prediction%20module%20links%20relation%20slots%20to%20corresponding%20object%20pairs%2C%20capturing%20evolving%20interactions%20over%20time.%20We%20evaluate%20UNO%20on%20standard%20box-level%20and%20pixel-level%20VidSGG%20benchmarks.%20Results%20demonstrate%20that%20UNO%20not%20only%20achieves%20competitive%20performance%20across%20both%20tasks%20but%20also%20offers%20improved%20efficiency%20through%20a%20unified%2C%20object-centric%20design.%20Code%20is%20available%20at%3A%20https%3A//github.com/Fsoft-AIC/UNO&entry.1838667208=http%3A//arxiv.org/abs/2509.06165v5&entry.124074799=Read"},
{"title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search", "author": "Tianming Liang and Qirui Du and Jian-Fang Hu and Haichao Jiang and Zicheng Lin and Wei-Shi Zheng", "abstract": "Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \\textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.", "link": "http://arxiv.org/abs/2602.04454v1", "date": "2026-02-04", "relevancy": 2.289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seg-ReSearch%3A%20Segmentation%20with%20Interleaved%20Reasoning%20and%20External%20Search&body=Title%3A%20Seg-ReSearch%3A%20Segmentation%20with%20Interleaved%20Reasoning%20and%20External%20Search%0AAuthor%3A%20Tianming%20Liang%20and%20Qirui%20Du%20and%20Jian-Fang%20Hu%20and%20Haichao%20Jiang%20and%20Zicheng%20Lin%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20Segmentation%20based%20on%20language%20has%20been%20a%20popular%20topic%20in%20computer%20vision.%20While%20recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20endowed%20segmentation%20systems%20with%20reasoning%20capabilities%2C%20these%20efforts%20remain%20confined%20by%20the%20frozen%20internal%20knowledge%20of%20MLLMs%2C%20which%20limits%20their%20potential%20for%20real-world%20scenarios%20that%20involve%20up-to-date%20information%20or%20domain-specific%20concepts.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BSeg-ReSearch%7D%2C%20a%20novel%20segmentation%20paradigm%20that%20overcomes%20the%20knowledge%20bottleneck%20of%20existing%20approaches.%20By%20enabling%20interleaved%20reasoning%20and%20external%20search%2C%20Seg-ReSearch%20empowers%20segmentation%20systems%20to%20handle%20dynamic%2C%20open-world%20queries%20that%20extend%20beyond%20the%20frozen%20knowledge%20of%20MLLMs.%20To%20effectively%20train%20this%20capability%2C%20we%20introduce%20a%20hierarchical%20reward%20design%20that%20harmonizes%20initial%20guidance%20with%20progressive%20incentives%2C%20mitigating%20the%20dilemma%20between%20sparse%20outcome%20signals%20and%20rigid%20step-wise%20supervision.%20For%20evaluation%2C%20we%20construct%20OK-VOS%2C%20a%20challenging%20benchmark%20that%20explicitly%20requires%20outside%20knowledge%20for%20video%20object%20segmentation.%20Experiments%20on%20OK-VOS%20and%20two%20existing%20reasoning%20segmentation%20benchmarks%20demonstrate%20that%20our%20Seg-ReSearch%20improves%20state-of-the-art%20approaches%20by%20a%20substantial%20margin.%20Code%20and%20data%20will%20be%20released%20at%20https%3A//github.com/iSEE-Laboratory/Seg-ReSearch.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeg-ReSearch%253A%2520Segmentation%2520with%2520Interleaved%2520Reasoning%2520and%2520External%2520Search%26entry.906535625%3DTianming%2520Liang%2520and%2520Qirui%2520Du%2520and%2520Jian-Fang%2520Hu%2520and%2520Haichao%2520Jiang%2520and%2520Zicheng%2520Lin%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3DSegmentation%2520based%2520on%2520language%2520has%2520been%2520a%2520popular%2520topic%2520in%2520computer%2520vision.%2520While%2520recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520endowed%2520segmentation%2520systems%2520with%2520reasoning%2520capabilities%252C%2520these%2520efforts%2520remain%2520confined%2520by%2520the%2520frozen%2520internal%2520knowledge%2520of%2520MLLMs%252C%2520which%2520limits%2520their%2520potential%2520for%2520real-world%2520scenarios%2520that%2520involve%2520up-to-date%2520information%2520or%2520domain-specific%2520concepts.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Ctextbf%257BSeg-ReSearch%257D%252C%2520a%2520novel%2520segmentation%2520paradigm%2520that%2520overcomes%2520the%2520knowledge%2520bottleneck%2520of%2520existing%2520approaches.%2520By%2520enabling%2520interleaved%2520reasoning%2520and%2520external%2520search%252C%2520Seg-ReSearch%2520empowers%2520segmentation%2520systems%2520to%2520handle%2520dynamic%252C%2520open-world%2520queries%2520that%2520extend%2520beyond%2520the%2520frozen%2520knowledge%2520of%2520MLLMs.%2520To%2520effectively%2520train%2520this%2520capability%252C%2520we%2520introduce%2520a%2520hierarchical%2520reward%2520design%2520that%2520harmonizes%2520initial%2520guidance%2520with%2520progressive%2520incentives%252C%2520mitigating%2520the%2520dilemma%2520between%2520sparse%2520outcome%2520signals%2520and%2520rigid%2520step-wise%2520supervision.%2520For%2520evaluation%252C%2520we%2520construct%2520OK-VOS%252C%2520a%2520challenging%2520benchmark%2520that%2520explicitly%2520requires%2520outside%2520knowledge%2520for%2520video%2520object%2520segmentation.%2520Experiments%2520on%2520OK-VOS%2520and%2520two%2520existing%2520reasoning%2520segmentation%2520benchmarks%2520demonstrate%2520that%2520our%2520Seg-ReSearch%2520improves%2520state-of-the-art%2520approaches%2520by%2520a%2520substantial%2520margin.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%2520https%253A//github.com/iSEE-Laboratory/Seg-ReSearch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seg-ReSearch%3A%20Segmentation%20with%20Interleaved%20Reasoning%20and%20External%20Search&entry.906535625=Tianming%20Liang%20and%20Qirui%20Du%20and%20Jian-Fang%20Hu%20and%20Haichao%20Jiang%20and%20Zicheng%20Lin%20and%20Wei-Shi%20Zheng&entry.1292438233=Segmentation%20based%20on%20language%20has%20been%20a%20popular%20topic%20in%20computer%20vision.%20While%20recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20endowed%20segmentation%20systems%20with%20reasoning%20capabilities%2C%20these%20efforts%20remain%20confined%20by%20the%20frozen%20internal%20knowledge%20of%20MLLMs%2C%20which%20limits%20their%20potential%20for%20real-world%20scenarios%20that%20involve%20up-to-date%20information%20or%20domain-specific%20concepts.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BSeg-ReSearch%7D%2C%20a%20novel%20segmentation%20paradigm%20that%20overcomes%20the%20knowledge%20bottleneck%20of%20existing%20approaches.%20By%20enabling%20interleaved%20reasoning%20and%20external%20search%2C%20Seg-ReSearch%20empowers%20segmentation%20systems%20to%20handle%20dynamic%2C%20open-world%20queries%20that%20extend%20beyond%20the%20frozen%20knowledge%20of%20MLLMs.%20To%20effectively%20train%20this%20capability%2C%20we%20introduce%20a%20hierarchical%20reward%20design%20that%20harmonizes%20initial%20guidance%20with%20progressive%20incentives%2C%20mitigating%20the%20dilemma%20between%20sparse%20outcome%20signals%20and%20rigid%20step-wise%20supervision.%20For%20evaluation%2C%20we%20construct%20OK-VOS%2C%20a%20challenging%20benchmark%20that%20explicitly%20requires%20outside%20knowledge%20for%20video%20object%20segmentation.%20Experiments%20on%20OK-VOS%20and%20two%20existing%20reasoning%20segmentation%20benchmarks%20demonstrate%20that%20our%20Seg-ReSearch%20improves%20state-of-the-art%20approaches%20by%20a%20substantial%20margin.%20Code%20and%20data%20will%20be%20released%20at%20https%3A//github.com/iSEE-Laboratory/Seg-ReSearch.&entry.1838667208=http%3A//arxiv.org/abs/2602.04454v1&entry.124074799=Read"},
{"title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization", "author": "Rajalaxmi Rajagopalan and Debottam Dutta and Yu-Lin Wei and Romit Roy Choudhury", "abstract": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.", "link": "http://arxiv.org/abs/2602.02388v2", "date": "2026-02-04", "relevancy": 2.2841, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5912}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5695}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Image%20Generation%20via%20Human-in-the-loop%20Bayesian%20Optimization&body=Title%3A%20Personalized%20Image%20Generation%20via%20Human-in-the-loop%20Bayesian%20Optimization%0AAuthor%3A%20Rajalaxmi%20Rajagopalan%20and%20Debottam%20Dutta%20and%20Yu-Lin%20Wei%20and%20Romit%20Roy%20Choudhury%0AAbstract%3A%20Imagine%20Alice%20has%20a%20specific%20image%20%24x%5E%5Cast%24%20in%20her%20mind%2C%20say%2C%20the%20view%20of%20the%20street%20in%20which%20she%20grew%20up%20during%20her%20childhood.%20To%20generate%20that%20exact%20image%2C%20she%20guides%20a%20generative%20model%20with%20multiple%20rounds%20of%20prompting%20and%20arrives%20at%20an%20image%20%24x%5E%7Bp%2A%7D%24.%20Although%20%24x%5E%7Bp%2A%7D%24%20is%20reasonably%20close%20to%20%24x%5E%5Cast%24%2C%20Alice%20finds%20it%20difficult%20to%20close%20that%20gap%20using%20language%20prompts.%20This%20paper%20aims%20to%20narrow%20this%20gap%20by%20observing%20that%20even%20after%20language%20has%20reached%20its%20limits%2C%20humans%20can%20still%20tell%20when%20a%20new%20image%20%24x%5E%2B%24%20is%20closer%20to%20%24x%5E%5Cast%24%20than%20%24x%5E%7Bp%2A%7D%24.%20Leveraging%20this%20observation%2C%20we%20develop%20MultiBO%20%28Multi-Choice%20Preferential%20Bayesian%20Optimization%29%20that%20carefully%20generates%20%24K%24%20new%20images%20as%20a%20function%20of%20%24x%5E%7Bp%2A%7D%24%2C%20gets%20preferential%20feedback%20from%20the%20user%2C%20uses%20the%20feedback%20to%20guide%20the%20diffusion%20model%2C%20and%20ultimately%20generates%20a%20new%20set%20of%20%24K%24%20images.%20We%20show%20that%20within%20%24B%24%20rounds%20of%20user%20feedback%2C%20it%20is%20possible%20to%20arrive%20much%20closer%20to%20%24x%5E%5Cast%24%2C%20even%20though%20the%20generative%20model%20has%20no%20information%20about%20%24x%5E%5Cast%24.%20Qualitative%20scores%20from%20%2430%24%20users%2C%20combined%20with%20quantitative%20metrics%20compared%20across%20%245%24%20baselines%2C%20show%20promising%20results%2C%20suggesting%20that%20multi-choice%20feedback%20from%20humans%20can%20be%20effectively%20harnessed%20for%20personalized%20image%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Image%2520Generation%2520via%2520Human-in-the-loop%2520Bayesian%2520Optimization%26entry.906535625%3DRajalaxmi%2520Rajagopalan%2520and%2520Debottam%2520Dutta%2520and%2520Yu-Lin%2520Wei%2520and%2520Romit%2520Roy%2520Choudhury%26entry.1292438233%3DImagine%2520Alice%2520has%2520a%2520specific%2520image%2520%2524x%255E%255Cast%2524%2520in%2520her%2520mind%252C%2520say%252C%2520the%2520view%2520of%2520the%2520street%2520in%2520which%2520she%2520grew%2520up%2520during%2520her%2520childhood.%2520To%2520generate%2520that%2520exact%2520image%252C%2520she%2520guides%2520a%2520generative%2520model%2520with%2520multiple%2520rounds%2520of%2520prompting%2520and%2520arrives%2520at%2520an%2520image%2520%2524x%255E%257Bp%252A%257D%2524.%2520Although%2520%2524x%255E%257Bp%252A%257D%2524%2520is%2520reasonably%2520close%2520to%2520%2524x%255E%255Cast%2524%252C%2520Alice%2520finds%2520it%2520difficult%2520to%2520close%2520that%2520gap%2520using%2520language%2520prompts.%2520This%2520paper%2520aims%2520to%2520narrow%2520this%2520gap%2520by%2520observing%2520that%2520even%2520after%2520language%2520has%2520reached%2520its%2520limits%252C%2520humans%2520can%2520still%2520tell%2520when%2520a%2520new%2520image%2520%2524x%255E%252B%2524%2520is%2520closer%2520to%2520%2524x%255E%255Cast%2524%2520than%2520%2524x%255E%257Bp%252A%257D%2524.%2520Leveraging%2520this%2520observation%252C%2520we%2520develop%2520MultiBO%2520%2528Multi-Choice%2520Preferential%2520Bayesian%2520Optimization%2529%2520that%2520carefully%2520generates%2520%2524K%2524%2520new%2520images%2520as%2520a%2520function%2520of%2520%2524x%255E%257Bp%252A%257D%2524%252C%2520gets%2520preferential%2520feedback%2520from%2520the%2520user%252C%2520uses%2520the%2520feedback%2520to%2520guide%2520the%2520diffusion%2520model%252C%2520and%2520ultimately%2520generates%2520a%2520new%2520set%2520of%2520%2524K%2524%2520images.%2520We%2520show%2520that%2520within%2520%2524B%2524%2520rounds%2520of%2520user%2520feedback%252C%2520it%2520is%2520possible%2520to%2520arrive%2520much%2520closer%2520to%2520%2524x%255E%255Cast%2524%252C%2520even%2520though%2520the%2520generative%2520model%2520has%2520no%2520information%2520about%2520%2524x%255E%255Cast%2524.%2520Qualitative%2520scores%2520from%2520%252430%2524%2520users%252C%2520combined%2520with%2520quantitative%2520metrics%2520compared%2520across%2520%25245%2524%2520baselines%252C%2520show%2520promising%2520results%252C%2520suggesting%2520that%2520multi-choice%2520feedback%2520from%2520humans%2520can%2520be%2520effectively%2520harnessed%2520for%2520personalized%2520image%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Image%20Generation%20via%20Human-in-the-loop%20Bayesian%20Optimization&entry.906535625=Rajalaxmi%20Rajagopalan%20and%20Debottam%20Dutta%20and%20Yu-Lin%20Wei%20and%20Romit%20Roy%20Choudhury&entry.1292438233=Imagine%20Alice%20has%20a%20specific%20image%20%24x%5E%5Cast%24%20in%20her%20mind%2C%20say%2C%20the%20view%20of%20the%20street%20in%20which%20she%20grew%20up%20during%20her%20childhood.%20To%20generate%20that%20exact%20image%2C%20she%20guides%20a%20generative%20model%20with%20multiple%20rounds%20of%20prompting%20and%20arrives%20at%20an%20image%20%24x%5E%7Bp%2A%7D%24.%20Although%20%24x%5E%7Bp%2A%7D%24%20is%20reasonably%20close%20to%20%24x%5E%5Cast%24%2C%20Alice%20finds%20it%20difficult%20to%20close%20that%20gap%20using%20language%20prompts.%20This%20paper%20aims%20to%20narrow%20this%20gap%20by%20observing%20that%20even%20after%20language%20has%20reached%20its%20limits%2C%20humans%20can%20still%20tell%20when%20a%20new%20image%20%24x%5E%2B%24%20is%20closer%20to%20%24x%5E%5Cast%24%20than%20%24x%5E%7Bp%2A%7D%24.%20Leveraging%20this%20observation%2C%20we%20develop%20MultiBO%20%28Multi-Choice%20Preferential%20Bayesian%20Optimization%29%20that%20carefully%20generates%20%24K%24%20new%20images%20as%20a%20function%20of%20%24x%5E%7Bp%2A%7D%24%2C%20gets%20preferential%20feedback%20from%20the%20user%2C%20uses%20the%20feedback%20to%20guide%20the%20diffusion%20model%2C%20and%20ultimately%20generates%20a%20new%20set%20of%20%24K%24%20images.%20We%20show%20that%20within%20%24B%24%20rounds%20of%20user%20feedback%2C%20it%20is%20possible%20to%20arrive%20much%20closer%20to%20%24x%5E%5Cast%24%2C%20even%20though%20the%20generative%20model%20has%20no%20information%20about%20%24x%5E%5Cast%24.%20Qualitative%20scores%20from%20%2430%24%20users%2C%20combined%20with%20quantitative%20metrics%20compared%20across%20%245%24%20baselines%2C%20show%20promising%20results%2C%20suggesting%20that%20multi-choice%20feedback%20from%20humans%20can%20be%20effectively%20harnessed%20for%20personalized%20image%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2602.02388v2&entry.124074799=Read"},
{"title": "LiDAR-based 3D Change Detection at City Scale", "author": "Hezam Albagami and Haitian Wang and Xinyu Wang and Muhammad Ibrahim and Zainy M. Malakan and Abdullah M. Alqamdi and Mohammed H. Alghamdi and Ajmal Mian", "abstract": "High-definition 3D city maps enable city planning and change detection, which is essential for municipal compliance, map maintenance, and asset monitoring, including both built structures and urban greenery. Conventional Digital Surface Model (DSM) and image differencing are sensitive to vertical bias and viewpoint mismatch, while original point cloud or voxel models require large memory, assume perfect alignment, and degrade thin structures. We propose an uncertainty-aware, object-centric method for city-scale LiDAR-based change detection. Our method aligns data from different time periods using multi-resolution Normal Distributions Transform (NDT) and a point-to-plane Iterative Closest Point (ICP) method, normalizes elevation, and computes a per-point level of detection from registration covariance and surface roughness to calibrate change decisions. Geometry-based associations are refined by semantic and instance segmentation and optimized using class-constrained bipartite assignment with augmented dummies to handle split-merge cases. Tiled processing bounds memory and preserves narrow ground changes, while instance-level decisions integrate overlap, displacement, and volumetric differences under local detection gating. We perform experiments on a Subiaco (Western Australia) dataset captured in 2023 and again in 2025. Our method achieves 95.3% accuracy, 90.8% mF1, and 82.9% mIoU, improving over the strongest baseline, Triplet KPConv, by 0.3, 0.6, and 1.1 points, respectively. The datasets are available on IEEE DataPort (2023: https://ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset and 2025: https://ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset). The source code is available at https://github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.", "link": "http://arxiv.org/abs/2510.21112v2", "date": "2026-02-04", "relevancy": 2.2822, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5826}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5688}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-based%203D%20Change%20Detection%20at%20City%20Scale&body=Title%3A%20LiDAR-based%203D%20Change%20Detection%20at%20City%20Scale%0AAuthor%3A%20Hezam%20Albagami%20and%20Haitian%20Wang%20and%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Zainy%20M.%20Malakan%20and%20Abdullah%20M.%20Alqamdi%20and%20Mohammed%20H.%20Alghamdi%20and%20Ajmal%20Mian%0AAbstract%3A%20High-definition%203D%20city%20maps%20enable%20city%20planning%20and%20change%20detection%2C%20which%20is%20essential%20for%20municipal%20compliance%2C%20map%20maintenance%2C%20and%20asset%20monitoring%2C%20including%20both%20built%20structures%20and%20urban%20greenery.%20Conventional%20Digital%20Surface%20Model%20%28DSM%29%20and%20image%20differencing%20are%20sensitive%20to%20vertical%20bias%20and%20viewpoint%20mismatch%2C%20while%20original%20point%20cloud%20or%20voxel%20models%20require%20large%20memory%2C%20assume%20perfect%20alignment%2C%20and%20degrade%20thin%20structures.%20We%20propose%20an%20uncertainty-aware%2C%20object-centric%20method%20for%20city-scale%20LiDAR-based%20change%20detection.%20Our%20method%20aligns%20data%20from%20different%20time%20periods%20using%20multi-resolution%20Normal%20Distributions%20Transform%20%28NDT%29%20and%20a%20point-to-plane%20Iterative%20Closest%20Point%20%28ICP%29%20method%2C%20normalizes%20elevation%2C%20and%20computes%20a%20per-point%20level%20of%20detection%20from%20registration%20covariance%20and%20surface%20roughness%20to%20calibrate%20change%20decisions.%20Geometry-based%20associations%20are%20refined%20by%20semantic%20and%20instance%20segmentation%20and%20optimized%20using%20class-constrained%20bipartite%20assignment%20with%20augmented%20dummies%20to%20handle%20split-merge%20cases.%20Tiled%20processing%20bounds%20memory%20and%20preserves%20narrow%20ground%20changes%2C%20while%20instance-level%20decisions%20integrate%20overlap%2C%20displacement%2C%20and%20volumetric%20differences%20under%20local%20detection%20gating.%20We%20perform%20experiments%20on%20a%20Subiaco%20%28Western%20Australia%29%20dataset%20captured%20in%202023%20and%20again%20in%202025.%20Our%20method%20achieves%2095.3%25%20accuracy%2C%2090.8%25%20mF1%2C%20and%2082.9%25%20mIoU%2C%20improving%20over%20the%20strongest%20baseline%2C%20Triplet%20KPConv%2C%20by%200.3%2C%200.6%2C%20and%201.1%20points%2C%20respectively.%20The%20datasets%20are%20available%20on%20IEEE%20DataPort%20%282023%3A%20https%3A//ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset%20and%202025%3A%20https%3A//ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset%29.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-based%25203D%2520Change%2520Detection%2520at%2520City%2520Scale%26entry.906535625%3DHezam%2520Albagami%2520and%2520Haitian%2520Wang%2520and%2520Xinyu%2520Wang%2520and%2520Muhammad%2520Ibrahim%2520and%2520Zainy%2520M.%2520Malakan%2520and%2520Abdullah%2520M.%2520Alqamdi%2520and%2520Mohammed%2520H.%2520Alghamdi%2520and%2520Ajmal%2520Mian%26entry.1292438233%3DHigh-definition%25203D%2520city%2520maps%2520enable%2520city%2520planning%2520and%2520change%2520detection%252C%2520which%2520is%2520essential%2520for%2520municipal%2520compliance%252C%2520map%2520maintenance%252C%2520and%2520asset%2520monitoring%252C%2520including%2520both%2520built%2520structures%2520and%2520urban%2520greenery.%2520Conventional%2520Digital%2520Surface%2520Model%2520%2528DSM%2529%2520and%2520image%2520differencing%2520are%2520sensitive%2520to%2520vertical%2520bias%2520and%2520viewpoint%2520mismatch%252C%2520while%2520original%2520point%2520cloud%2520or%2520voxel%2520models%2520require%2520large%2520memory%252C%2520assume%2520perfect%2520alignment%252C%2520and%2520degrade%2520thin%2520structures.%2520We%2520propose%2520an%2520uncertainty-aware%252C%2520object-centric%2520method%2520for%2520city-scale%2520LiDAR-based%2520change%2520detection.%2520Our%2520method%2520aligns%2520data%2520from%2520different%2520time%2520periods%2520using%2520multi-resolution%2520Normal%2520Distributions%2520Transform%2520%2528NDT%2529%2520and%2520a%2520point-to-plane%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520method%252C%2520normalizes%2520elevation%252C%2520and%2520computes%2520a%2520per-point%2520level%2520of%2520detection%2520from%2520registration%2520covariance%2520and%2520surface%2520roughness%2520to%2520calibrate%2520change%2520decisions.%2520Geometry-based%2520associations%2520are%2520refined%2520by%2520semantic%2520and%2520instance%2520segmentation%2520and%2520optimized%2520using%2520class-constrained%2520bipartite%2520assignment%2520with%2520augmented%2520dummies%2520to%2520handle%2520split-merge%2520cases.%2520Tiled%2520processing%2520bounds%2520memory%2520and%2520preserves%2520narrow%2520ground%2520changes%252C%2520while%2520instance-level%2520decisions%2520integrate%2520overlap%252C%2520displacement%252C%2520and%2520volumetric%2520differences%2520under%2520local%2520detection%2520gating.%2520We%2520perform%2520experiments%2520on%2520a%2520Subiaco%2520%2528Western%2520Australia%2529%2520dataset%2520captured%2520in%25202023%2520and%2520again%2520in%25202025.%2520Our%2520method%2520achieves%252095.3%2525%2520accuracy%252C%252090.8%2525%2520mF1%252C%2520and%252082.9%2525%2520mIoU%252C%2520improving%2520over%2520the%2520strongest%2520baseline%252C%2520Triplet%2520KPConv%252C%2520by%25200.3%252C%25200.6%252C%2520and%25201.1%2520points%252C%2520respectively.%2520The%2520datasets%2520are%2520available%2520on%2520IEEE%2520DataPort%2520%25282023%253A%2520https%253A//ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset%2520and%25202025%253A%2520https%253A//ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset%2529.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-based%203D%20Change%20Detection%20at%20City%20Scale&entry.906535625=Hezam%20Albagami%20and%20Haitian%20Wang%20and%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Zainy%20M.%20Malakan%20and%20Abdullah%20M.%20Alqamdi%20and%20Mohammed%20H.%20Alghamdi%20and%20Ajmal%20Mian&entry.1292438233=High-definition%203D%20city%20maps%20enable%20city%20planning%20and%20change%20detection%2C%20which%20is%20essential%20for%20municipal%20compliance%2C%20map%20maintenance%2C%20and%20asset%20monitoring%2C%20including%20both%20built%20structures%20and%20urban%20greenery.%20Conventional%20Digital%20Surface%20Model%20%28DSM%29%20and%20image%20differencing%20are%20sensitive%20to%20vertical%20bias%20and%20viewpoint%20mismatch%2C%20while%20original%20point%20cloud%20or%20voxel%20models%20require%20large%20memory%2C%20assume%20perfect%20alignment%2C%20and%20degrade%20thin%20structures.%20We%20propose%20an%20uncertainty-aware%2C%20object-centric%20method%20for%20city-scale%20LiDAR-based%20change%20detection.%20Our%20method%20aligns%20data%20from%20different%20time%20periods%20using%20multi-resolution%20Normal%20Distributions%20Transform%20%28NDT%29%20and%20a%20point-to-plane%20Iterative%20Closest%20Point%20%28ICP%29%20method%2C%20normalizes%20elevation%2C%20and%20computes%20a%20per-point%20level%20of%20detection%20from%20registration%20covariance%20and%20surface%20roughness%20to%20calibrate%20change%20decisions.%20Geometry-based%20associations%20are%20refined%20by%20semantic%20and%20instance%20segmentation%20and%20optimized%20using%20class-constrained%20bipartite%20assignment%20with%20augmented%20dummies%20to%20handle%20split-merge%20cases.%20Tiled%20processing%20bounds%20memory%20and%20preserves%20narrow%20ground%20changes%2C%20while%20instance-level%20decisions%20integrate%20overlap%2C%20displacement%2C%20and%20volumetric%20differences%20under%20local%20detection%20gating.%20We%20perform%20experiments%20on%20a%20Subiaco%20%28Western%20Australia%29%20dataset%20captured%20in%202023%20and%20again%20in%202025.%20Our%20method%20achieves%2095.3%25%20accuracy%2C%2090.8%25%20mF1%2C%20and%2082.9%25%20mIoU%2C%20improving%20over%20the%20strongest%20baseline%2C%20Triplet%20KPConv%2C%20by%200.3%2C%200.6%2C%20and%201.1%20points%2C%20respectively.%20The%20datasets%20are%20available%20on%20IEEE%20DataPort%20%282023%3A%20https%3A//ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset%20and%202025%3A%20https%3A//ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset%29.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.&entry.1838667208=http%3A//arxiv.org/abs/2510.21112v2&entry.124074799=Read"},
{"title": "Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints", "author": "Mahmud S. Zango and Jianglin Lan", "abstract": "Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging \"Edge AI\" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the \"Sim-to-Real\" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.", "link": "http://arxiv.org/abs/2601.13252v2", "date": "2026-02-04", "relevancy": 2.2747, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5929}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Navigation%20at%20the%20Nano-Scale%3A%20Algorithms%2C%20Architectures%2C%20and%20Constraints&body=Title%3A%20Autonomous%20Navigation%20at%20the%20Nano-Scale%3A%20Algorithms%2C%20Architectures%2C%20and%20Constraints%0AAuthor%3A%20Mahmud%20S.%20Zango%20and%20Jianglin%20Lan%0AAbstract%3A%20Autonomous%20navigation%20for%20nano-scale%20unmanned%20aerial%20vehicles%20%28nano-UAVs%29%20is%20governed%20by%20extreme%20Size%2C%20Weight%2C%20and%20Power%20%28SWaP%29%20constraints%20%28with%20the%20weight%20%3C%2050%20g%20and%20sub-100%20mW%20onboard%20processor%29%2C%20distinguishing%20it%20fundamentally%20from%20standard%20robotic%20paradigms.%20This%20review%20synthesizes%20the%20state-of-the-art%20in%20sensing%2C%20computing%2C%20and%20control%20architectures%20designed%20specifically%20for%20these%20sub-%20100mW%20computational%20envelopes.%20We%20critically%20analyse%20the%20transition%20from%20classical%20geometry-based%20methods%20to%20emerging%20%22Edge%20AI%22%20paradigms%2C%20including%20quantized%20deep%20neural%20networks%20deployed%20on%20ultra-low-power%20System-on-Chips%20%28SoCs%29%20and%20neuromorphic%20event-based%20control.%20Beyond%20algorithms%2C%20we%20evaluate%20the%20hardware-software%20co-design%20requisite%20for%20autonomy%2C%20covering%20advancements%20in%20dense%20optical%20flow%2C%20optimized%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20and%20learning-based%20flight%20control.%20While%20significant%20progress%20has%20been%20observed%20in%20visual%20navigation%20and%20relative%20pose%20estimation%2C%20our%20analysis%20reveals%20persistent%20gaps%20in%20long-term%20endurance%2C%20robust%20obstacle%20avoidance%20in%20dynamic%20environments%2C%20and%20the%20%22Sim-to-Real%22%20transfer%20of%20reinforcement%20learning%20policies.%20This%20survey%20provides%20a%20roadmap%20for%20bridging%20these%20gaps%2C%20advocating%20for%20hybrid%20architectures%20that%20fuse%20lightweight%20classical%20control%20with%20data-driven%20perception%20to%20enable%20fully%20autonomous%2C%20agile%20nano-UAVs%20in%20GPS-denied%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Navigation%2520at%2520the%2520Nano-Scale%253A%2520Algorithms%252C%2520Architectures%252C%2520and%2520Constraints%26entry.906535625%3DMahmud%2520S.%2520Zango%2520and%2520Jianglin%2520Lan%26entry.1292438233%3DAutonomous%2520navigation%2520for%2520nano-scale%2520unmanned%2520aerial%2520vehicles%2520%2528nano-UAVs%2529%2520is%2520governed%2520by%2520extreme%2520Size%252C%2520Weight%252C%2520and%2520Power%2520%2528SWaP%2529%2520constraints%2520%2528with%2520the%2520weight%2520%253C%252050%2520g%2520and%2520sub-100%2520mW%2520onboard%2520processor%2529%252C%2520distinguishing%2520it%2520fundamentally%2520from%2520standard%2520robotic%2520paradigms.%2520This%2520review%2520synthesizes%2520the%2520state-of-the-art%2520in%2520sensing%252C%2520computing%252C%2520and%2520control%2520architectures%2520designed%2520specifically%2520for%2520these%2520sub-%2520100mW%2520computational%2520envelopes.%2520We%2520critically%2520analyse%2520the%2520transition%2520from%2520classical%2520geometry-based%2520methods%2520to%2520emerging%2520%2522Edge%2520AI%2522%2520paradigms%252C%2520including%2520quantized%2520deep%2520neural%2520networks%2520deployed%2520on%2520ultra-low-power%2520System-on-Chips%2520%2528SoCs%2529%2520and%2520neuromorphic%2520event-based%2520control.%2520Beyond%2520algorithms%252C%2520we%2520evaluate%2520the%2520hardware-software%2520co-design%2520requisite%2520for%2520autonomy%252C%2520covering%2520advancements%2520in%2520dense%2520optical%2520flow%252C%2520optimized%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520and%2520learning-based%2520flight%2520control.%2520While%2520significant%2520progress%2520has%2520been%2520observed%2520in%2520visual%2520navigation%2520and%2520relative%2520pose%2520estimation%252C%2520our%2520analysis%2520reveals%2520persistent%2520gaps%2520in%2520long-term%2520endurance%252C%2520robust%2520obstacle%2520avoidance%2520in%2520dynamic%2520environments%252C%2520and%2520the%2520%2522Sim-to-Real%2522%2520transfer%2520of%2520reinforcement%2520learning%2520policies.%2520This%2520survey%2520provides%2520a%2520roadmap%2520for%2520bridging%2520these%2520gaps%252C%2520advocating%2520for%2520hybrid%2520architectures%2520that%2520fuse%2520lightweight%2520classical%2520control%2520with%2520data-driven%2520perception%2520to%2520enable%2520fully%2520autonomous%252C%2520agile%2520nano-UAVs%2520in%2520GPS-denied%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Navigation%20at%20the%20Nano-Scale%3A%20Algorithms%2C%20Architectures%2C%20and%20Constraints&entry.906535625=Mahmud%20S.%20Zango%20and%20Jianglin%20Lan&entry.1292438233=Autonomous%20navigation%20for%20nano-scale%20unmanned%20aerial%20vehicles%20%28nano-UAVs%29%20is%20governed%20by%20extreme%20Size%2C%20Weight%2C%20and%20Power%20%28SWaP%29%20constraints%20%28with%20the%20weight%20%3C%2050%20g%20and%20sub-100%20mW%20onboard%20processor%29%2C%20distinguishing%20it%20fundamentally%20from%20standard%20robotic%20paradigms.%20This%20review%20synthesizes%20the%20state-of-the-art%20in%20sensing%2C%20computing%2C%20and%20control%20architectures%20designed%20specifically%20for%20these%20sub-%20100mW%20computational%20envelopes.%20We%20critically%20analyse%20the%20transition%20from%20classical%20geometry-based%20methods%20to%20emerging%20%22Edge%20AI%22%20paradigms%2C%20including%20quantized%20deep%20neural%20networks%20deployed%20on%20ultra-low-power%20System-on-Chips%20%28SoCs%29%20and%20neuromorphic%20event-based%20control.%20Beyond%20algorithms%2C%20we%20evaluate%20the%20hardware-software%20co-design%20requisite%20for%20autonomy%2C%20covering%20advancements%20in%20dense%20optical%20flow%2C%20optimized%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20and%20learning-based%20flight%20control.%20While%20significant%20progress%20has%20been%20observed%20in%20visual%20navigation%20and%20relative%20pose%20estimation%2C%20our%20analysis%20reveals%20persistent%20gaps%20in%20long-term%20endurance%2C%20robust%20obstacle%20avoidance%20in%20dynamic%20environments%2C%20and%20the%20%22Sim-to-Real%22%20transfer%20of%20reinforcement%20learning%20policies.%20This%20survey%20provides%20a%20roadmap%20for%20bridging%20these%20gaps%2C%20advocating%20for%20hybrid%20architectures%20that%20fuse%20lightweight%20classical%20control%20with%20data-driven%20perception%20to%20enable%20fully%20autonomous%2C%20agile%20nano-UAVs%20in%20GPS-denied%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.13252v2&entry.124074799=Read"},
{"title": "PEPR: Privileged Event-based Predictive Regularization for Domain Generalization", "author": "Gabriele Magrini and Federico Becattini and Niccol\u00f2 Biondi and Pietro Pala", "abstract": "Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.", "link": "http://arxiv.org/abs/2602.04583v1", "date": "2026-02-04", "relevancy": 2.2371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEPR%3A%20Privileged%20Event-based%20Predictive%20Regularization%20for%20Domain%20Generalization&body=Title%3A%20PEPR%3A%20Privileged%20Event-based%20Predictive%20Regularization%20for%20Domain%20Generalization%0AAuthor%3A%20Gabriele%20Magrini%20and%20Federico%20Becattini%20and%20Niccol%C3%B2%20Biondi%20and%20Pietro%20Pala%0AAbstract%3A%20Deep%20neural%20networks%20for%20visual%20perception%20are%20highly%20susceptible%20to%20domain%20shift%2C%20which%20poses%20a%20critical%20challenge%20for%20real-world%20deployment%20under%20conditions%20that%20differ%20from%20the%20training%20data.%20To%20address%20this%20domain%20generalization%20challenge%2C%20we%20propose%20a%20cross-modal%20framework%20under%20the%20learning%20using%20privileged%20information%20%28LUPI%29%20paradigm%20for%20training%20a%20robust%2C%20single-modality%20RGB%20model.%20We%20leverage%20event%20cameras%20as%20a%20source%20of%20privileged%20information%2C%20available%20only%20during%20training.%20The%20two%20modalities%20exhibit%20complementary%20characteristics%3A%20the%20RGB%20stream%20is%20semantically%20dense%20but%20domain-dependent%2C%20whereas%20the%20event%20stream%20is%20sparse%20yet%20more%20domain-invariant.%20Direct%20feature%20alignment%20between%20them%20is%20therefore%20suboptimal%2C%20as%20it%20forces%20the%20RGB%20encoder%20to%20mimic%20the%20sparse%20event%20representation%2C%20thereby%20losing%20semantic%20detail.%20To%20overcome%20this%2C%20we%20introduce%20Privileged%20Event-based%20Predictive%20Regularization%20%28PEPR%29%2C%20which%20reframes%20LUPI%20as%20a%20predictive%20problem%20in%20a%20shared%20latent%20space.%20Instead%20of%20enforcing%20direct%20cross-modal%20alignment%2C%20we%20train%20the%20RGB%20encoder%20with%20PEPR%20to%20predict%20event-based%20latent%20features%2C%20distilling%20robustness%20without%20sacrificing%20semantic%20richness.%20The%20resulting%20standalone%20RGB%20model%20consistently%20improves%20robustness%20to%20day-to-night%20and%20other%20domain%20shifts%2C%20outperforming%20alignment-based%20baselines%20across%20object%20detection%20and%20semantic%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEPR%253A%2520Privileged%2520Event-based%2520Predictive%2520Regularization%2520for%2520Domain%2520Generalization%26entry.906535625%3DGabriele%2520Magrini%2520and%2520Federico%2520Becattini%2520and%2520Niccol%25C3%25B2%2520Biondi%2520and%2520Pietro%2520Pala%26entry.1292438233%3DDeep%2520neural%2520networks%2520for%2520visual%2520perception%2520are%2520highly%2520susceptible%2520to%2520domain%2520shift%252C%2520which%2520poses%2520a%2520critical%2520challenge%2520for%2520real-world%2520deployment%2520under%2520conditions%2520that%2520differ%2520from%2520the%2520training%2520data.%2520To%2520address%2520this%2520domain%2520generalization%2520challenge%252C%2520we%2520propose%2520a%2520cross-modal%2520framework%2520under%2520the%2520learning%2520using%2520privileged%2520information%2520%2528LUPI%2529%2520paradigm%2520for%2520training%2520a%2520robust%252C%2520single-modality%2520RGB%2520model.%2520We%2520leverage%2520event%2520cameras%2520as%2520a%2520source%2520of%2520privileged%2520information%252C%2520available%2520only%2520during%2520training.%2520The%2520two%2520modalities%2520exhibit%2520complementary%2520characteristics%253A%2520the%2520RGB%2520stream%2520is%2520semantically%2520dense%2520but%2520domain-dependent%252C%2520whereas%2520the%2520event%2520stream%2520is%2520sparse%2520yet%2520more%2520domain-invariant.%2520Direct%2520feature%2520alignment%2520between%2520them%2520is%2520therefore%2520suboptimal%252C%2520as%2520it%2520forces%2520the%2520RGB%2520encoder%2520to%2520mimic%2520the%2520sparse%2520event%2520representation%252C%2520thereby%2520losing%2520semantic%2520detail.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520Privileged%2520Event-based%2520Predictive%2520Regularization%2520%2528PEPR%2529%252C%2520which%2520reframes%2520LUPI%2520as%2520a%2520predictive%2520problem%2520in%2520a%2520shared%2520latent%2520space.%2520Instead%2520of%2520enforcing%2520direct%2520cross-modal%2520alignment%252C%2520we%2520train%2520the%2520RGB%2520encoder%2520with%2520PEPR%2520to%2520predict%2520event-based%2520latent%2520features%252C%2520distilling%2520robustness%2520without%2520sacrificing%2520semantic%2520richness.%2520The%2520resulting%2520standalone%2520RGB%2520model%2520consistently%2520improves%2520robustness%2520to%2520day-to-night%2520and%2520other%2520domain%2520shifts%252C%2520outperforming%2520alignment-based%2520baselines%2520across%2520object%2520detection%2520and%2520semantic%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEPR%3A%20Privileged%20Event-based%20Predictive%20Regularization%20for%20Domain%20Generalization&entry.906535625=Gabriele%20Magrini%20and%20Federico%20Becattini%20and%20Niccol%C3%B2%20Biondi%20and%20Pietro%20Pala&entry.1292438233=Deep%20neural%20networks%20for%20visual%20perception%20are%20highly%20susceptible%20to%20domain%20shift%2C%20which%20poses%20a%20critical%20challenge%20for%20real-world%20deployment%20under%20conditions%20that%20differ%20from%20the%20training%20data.%20To%20address%20this%20domain%20generalization%20challenge%2C%20we%20propose%20a%20cross-modal%20framework%20under%20the%20learning%20using%20privileged%20information%20%28LUPI%29%20paradigm%20for%20training%20a%20robust%2C%20single-modality%20RGB%20model.%20We%20leverage%20event%20cameras%20as%20a%20source%20of%20privileged%20information%2C%20available%20only%20during%20training.%20The%20two%20modalities%20exhibit%20complementary%20characteristics%3A%20the%20RGB%20stream%20is%20semantically%20dense%20but%20domain-dependent%2C%20whereas%20the%20event%20stream%20is%20sparse%20yet%20more%20domain-invariant.%20Direct%20feature%20alignment%20between%20them%20is%20therefore%20suboptimal%2C%20as%20it%20forces%20the%20RGB%20encoder%20to%20mimic%20the%20sparse%20event%20representation%2C%20thereby%20losing%20semantic%20detail.%20To%20overcome%20this%2C%20we%20introduce%20Privileged%20Event-based%20Predictive%20Regularization%20%28PEPR%29%2C%20which%20reframes%20LUPI%20as%20a%20predictive%20problem%20in%20a%20shared%20latent%20space.%20Instead%20of%20enforcing%20direct%20cross-modal%20alignment%2C%20we%20train%20the%20RGB%20encoder%20with%20PEPR%20to%20predict%20event-based%20latent%20features%2C%20distilling%20robustness%20without%20sacrificing%20semantic%20richness.%20The%20resulting%20standalone%20RGB%20model%20consistently%20improves%20robustness%20to%20day-to-night%20and%20other%20domain%20shifts%2C%20outperforming%20alignment-based%20baselines%20across%20object%20detection%20and%20semantic%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2602.04583v1&entry.124074799=Read"},
{"title": "Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation", "author": "Victorita Dolean and Daria Hrebenshchykova and St\u00e9phane Lanteri and Victor Michel-Dansac", "abstract": "Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.", "link": "http://arxiv.org/abs/2511.15445v2", "date": "2026-02-04", "relevancy": 2.2283, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4497}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4458}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation&body=Title%3A%20Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation%0AAuthor%3A%20Victorita%20Dolean%20and%20Daria%20Hrebenshchykova%20and%20St%C3%A9phane%20Lanteri%20and%20Victor%20Michel-Dansac%0AAbstract%3A%20Accurately%20simulating%20wave%20propagation%20is%20crucial%20in%20fields%20such%20as%20acoustics%2C%20electromagnetism%2C%20and%20seismic%20analysis.%20Traditional%20numerical%20methods%2C%20like%20finite%20difference%20and%20finite%20element%20approaches%2C%20are%20widely%20used%20to%20solve%20governing%20partial%20differential%20equations%20%28PDEs%29%20such%20as%20the%20Helmholtz%20equation.%20However%2C%20these%20methods%20face%20significant%20computational%20challenges%20when%20applied%20to%20high-frequency%20wave%20problems%20in%20complex%20two-dimensional%20domains.%20This%20work%20investigates%20Finite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%20and%20their%20multilevel%20extensions%20as%20a%20promising%20alternative.%20These%20methods%20leverage%20domain%20decomposition%2C%20partitioning%20the%20computational%20domain%20into%20overlapping%20sub-domains%2C%20each%20governed%20by%20a%20local%20neural%20network.%20We%20assess%20their%20accuracy%20and%20computational%20efficiency%20in%20solving%20the%20Helmholtz%20equation%20for%20the%20homogeneous%20case%2C%20demonstrating%20their%20potential%20to%20mitigate%20the%20limitations%20of%20traditional%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520network-driven%2520domain%2520decomposition%2520for%2520efficient%2520solutions%2520to%2520the%2520Helmholtz%2520equation%26entry.906535625%3DVictorita%2520Dolean%2520and%2520Daria%2520Hrebenshchykova%2520and%2520St%25C3%25A9phane%2520Lanteri%2520and%2520Victor%2520Michel-Dansac%26entry.1292438233%3DAccurately%2520simulating%2520wave%2520propagation%2520is%2520crucial%2520in%2520fields%2520such%2520as%2520acoustics%252C%2520electromagnetism%252C%2520and%2520seismic%2520analysis.%2520Traditional%2520numerical%2520methods%252C%2520like%2520finite%2520difference%2520and%2520finite%2520element%2520approaches%252C%2520are%2520widely%2520used%2520to%2520solve%2520governing%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520such%2520as%2520the%2520Helmholtz%2520equation.%2520However%252C%2520these%2520methods%2520face%2520significant%2520computational%2520challenges%2520when%2520applied%2520to%2520high-frequency%2520wave%2520problems%2520in%2520complex%2520two-dimensional%2520domains.%2520This%2520work%2520investigates%2520Finite%2520Basis%2520Physics-Informed%2520Neural%2520Networks%2520%2528FBPINNs%2529%2520and%2520their%2520multilevel%2520extensions%2520as%2520a%2520promising%2520alternative.%2520These%2520methods%2520leverage%2520domain%2520decomposition%252C%2520partitioning%2520the%2520computational%2520domain%2520into%2520overlapping%2520sub-domains%252C%2520each%2520governed%2520by%2520a%2520local%2520neural%2520network.%2520We%2520assess%2520their%2520accuracy%2520and%2520computational%2520efficiency%2520in%2520solving%2520the%2520Helmholtz%2520equation%2520for%2520the%2520homogeneous%2520case%252C%2520demonstrating%2520their%2520potential%2520to%2520mitigate%2520the%2520limitations%2520of%2520traditional%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation&entry.906535625=Victorita%20Dolean%20and%20Daria%20Hrebenshchykova%20and%20St%C3%A9phane%20Lanteri%20and%20Victor%20Michel-Dansac&entry.1292438233=Accurately%20simulating%20wave%20propagation%20is%20crucial%20in%20fields%20such%20as%20acoustics%2C%20electromagnetism%2C%20and%20seismic%20analysis.%20Traditional%20numerical%20methods%2C%20like%20finite%20difference%20and%20finite%20element%20approaches%2C%20are%20widely%20used%20to%20solve%20governing%20partial%20differential%20equations%20%28PDEs%29%20such%20as%20the%20Helmholtz%20equation.%20However%2C%20these%20methods%20face%20significant%20computational%20challenges%20when%20applied%20to%20high-frequency%20wave%20problems%20in%20complex%20two-dimensional%20domains.%20This%20work%20investigates%20Finite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%20and%20their%20multilevel%20extensions%20as%20a%20promising%20alternative.%20These%20methods%20leverage%20domain%20decomposition%2C%20partitioning%20the%20computational%20domain%20into%20overlapping%20sub-domains%2C%20each%20governed%20by%20a%20local%20neural%20network.%20We%20assess%20their%20accuracy%20and%20computational%20efficiency%20in%20solving%20the%20Helmholtz%20equation%20for%20the%20homogeneous%20case%2C%20demonstrating%20their%20potential%20to%20mitigate%20the%20limitations%20of%20traditional%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2511.15445v2&entry.124074799=Read"},
{"title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation", "author": "Yueyang Wang and Mehmet Dogar and Russell Darling and Gustav Markkula", "abstract": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.", "link": "http://arxiv.org/abs/2601.02082v2", "date": "2026-02-04", "relevancy": 2.2246, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.576}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5677}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation&body=Title%3A%20Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation%0AAuthor%3A%20Yueyang%20Wang%20and%20Mehmet%20Dogar%20and%20Russell%20Darling%20and%20Gustav%20Markkula%0AAbstract%3A%20Autonomous%20vehicles%20%28AVs%29%20are%20rapidly%20advancing%20and%20are%20expected%20to%20play%20a%20central%20role%20in%20future%20mobility.%20Ensuring%20their%20safe%20deployment%20requires%20reliable%20interaction%20with%20other%20road%20users%2C%20not%20least%20pedestrians.%20Direct%20testing%20on%20public%20roads%20is%20costly%20and%20unsafe%20for%20rare%20but%20critical%20interactions%2C%20making%20simulation%20a%20practical%20alternative.%20Within%20simulation-based%20testing%2C%20adversarial%20scenarios%20are%20widely%20used%20to%20probe%20safety%20limits%2C%20but%20many%20prioritise%20difficulty%20over%20realism%2C%20producing%20exaggerated%20behaviours%20which%20may%20result%20in%20AV%20controllers%20that%20are%20overly%20conservative.%20We%20propose%20an%20alternative%20method%2C%20instead%20using%20a%20cognitively%20inspired%20pedestrian%20model%20featuring%20both%20inter-individual%20and%20intra-individual%20variability%20to%20generate%20behaviourally%20plausible%20adversarial%20scenarios.%20We%20provide%20a%20proof%20of%20concept%20demonstration%20of%20this%20method%27s%20potential%20for%20AV%20control%20optimisation%2C%20in%20closed-loop%20testing%20and%20tuning%20of%20an%20AV%20controller.%20Our%20results%20show%20that%20replacing%20the%20rule-based%20CARLA%20pedestrian%20with%20the%20human-like%20model%20yields%20more%20realistic%20gap%20acceptance%20patterns%20and%20smoother%20vehicle%20decelerations.%20Unsafe%20interactions%20occur%20only%20for%20certain%20pedestrian%20individuals%20and%20conditions%2C%20underscoring%20the%20importance%20of%20human%20variability%20in%20AV%20testing.%20Adversarial%20scenarios%20generated%20by%20this%20model%20can%20be%20used%20to%20optimise%20AV%20control%20towards%20safer%20and%20more%20efficient%20behaviour.%20Overall%2C%20this%20work%20illustrates%20how%20incorporating%20human-like%20road%20user%20models%20into%20simulation-based%20adversarial%20testing%20can%20enhance%20the%20credibility%20of%20AV%20evaluation%20and%20provide%20a%20practical%20basis%20to%20behaviourally%20informed%20controller%20optimisation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520adversarial%2520scenario%2520generation%2520via%2520human-like%2520pedestrian%2520model%2520for%2520autonomous%2520vehicle%2520control%2520parameter%2520optimisation%26entry.906535625%3DYueyang%2520Wang%2520and%2520Mehmet%2520Dogar%2520and%2520Russell%2520Darling%2520and%2520Gustav%2520Markkula%26entry.1292438233%3DAutonomous%2520vehicles%2520%2528AVs%2529%2520are%2520rapidly%2520advancing%2520and%2520are%2520expected%2520to%2520play%2520a%2520central%2520role%2520in%2520future%2520mobility.%2520Ensuring%2520their%2520safe%2520deployment%2520requires%2520reliable%2520interaction%2520with%2520other%2520road%2520users%252C%2520not%2520least%2520pedestrians.%2520Direct%2520testing%2520on%2520public%2520roads%2520is%2520costly%2520and%2520unsafe%2520for%2520rare%2520but%2520critical%2520interactions%252C%2520making%2520simulation%2520a%2520practical%2520alternative.%2520Within%2520simulation-based%2520testing%252C%2520adversarial%2520scenarios%2520are%2520widely%2520used%2520to%2520probe%2520safety%2520limits%252C%2520but%2520many%2520prioritise%2520difficulty%2520over%2520realism%252C%2520producing%2520exaggerated%2520behaviours%2520which%2520may%2520result%2520in%2520AV%2520controllers%2520that%2520are%2520overly%2520conservative.%2520We%2520propose%2520an%2520alternative%2520method%252C%2520instead%2520using%2520a%2520cognitively%2520inspired%2520pedestrian%2520model%2520featuring%2520both%2520inter-individual%2520and%2520intra-individual%2520variability%2520to%2520generate%2520behaviourally%2520plausible%2520adversarial%2520scenarios.%2520We%2520provide%2520a%2520proof%2520of%2520concept%2520demonstration%2520of%2520this%2520method%2527s%2520potential%2520for%2520AV%2520control%2520optimisation%252C%2520in%2520closed-loop%2520testing%2520and%2520tuning%2520of%2520an%2520AV%2520controller.%2520Our%2520results%2520show%2520that%2520replacing%2520the%2520rule-based%2520CARLA%2520pedestrian%2520with%2520the%2520human-like%2520model%2520yields%2520more%2520realistic%2520gap%2520acceptance%2520patterns%2520and%2520smoother%2520vehicle%2520decelerations.%2520Unsafe%2520interactions%2520occur%2520only%2520for%2520certain%2520pedestrian%2520individuals%2520and%2520conditions%252C%2520underscoring%2520the%2520importance%2520of%2520human%2520variability%2520in%2520AV%2520testing.%2520Adversarial%2520scenarios%2520generated%2520by%2520this%2520model%2520can%2520be%2520used%2520to%2520optimise%2520AV%2520control%2520towards%2520safer%2520and%2520more%2520efficient%2520behaviour.%2520Overall%252C%2520this%2520work%2520illustrates%2520how%2520incorporating%2520human-like%2520road%2520user%2520models%2520into%2520simulation-based%2520adversarial%2520testing%2520can%2520enhance%2520the%2520credibility%2520of%2520AV%2520evaluation%2520and%2520provide%2520a%2520practical%2520basis%2520to%2520behaviourally%2520informed%2520controller%2520optimisation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20adversarial%20scenario%20generation%20via%20human-like%20pedestrian%20model%20for%20autonomous%20vehicle%20control%20parameter%20optimisation&entry.906535625=Yueyang%20Wang%20and%20Mehmet%20Dogar%20and%20Russell%20Darling%20and%20Gustav%20Markkula&entry.1292438233=Autonomous%20vehicles%20%28AVs%29%20are%20rapidly%20advancing%20and%20are%20expected%20to%20play%20a%20central%20role%20in%20future%20mobility.%20Ensuring%20their%20safe%20deployment%20requires%20reliable%20interaction%20with%20other%20road%20users%2C%20not%20least%20pedestrians.%20Direct%20testing%20on%20public%20roads%20is%20costly%20and%20unsafe%20for%20rare%20but%20critical%20interactions%2C%20making%20simulation%20a%20practical%20alternative.%20Within%20simulation-based%20testing%2C%20adversarial%20scenarios%20are%20widely%20used%20to%20probe%20safety%20limits%2C%20but%20many%20prioritise%20difficulty%20over%20realism%2C%20producing%20exaggerated%20behaviours%20which%20may%20result%20in%20AV%20controllers%20that%20are%20overly%20conservative.%20We%20propose%20an%20alternative%20method%2C%20instead%20using%20a%20cognitively%20inspired%20pedestrian%20model%20featuring%20both%20inter-individual%20and%20intra-individual%20variability%20to%20generate%20behaviourally%20plausible%20adversarial%20scenarios.%20We%20provide%20a%20proof%20of%20concept%20demonstration%20of%20this%20method%27s%20potential%20for%20AV%20control%20optimisation%2C%20in%20closed-loop%20testing%20and%20tuning%20of%20an%20AV%20controller.%20Our%20results%20show%20that%20replacing%20the%20rule-based%20CARLA%20pedestrian%20with%20the%20human-like%20model%20yields%20more%20realistic%20gap%20acceptance%20patterns%20and%20smoother%20vehicle%20decelerations.%20Unsafe%20interactions%20occur%20only%20for%20certain%20pedestrian%20individuals%20and%20conditions%2C%20underscoring%20the%20importance%20of%20human%20variability%20in%20AV%20testing.%20Adversarial%20scenarios%20generated%20by%20this%20model%20can%20be%20used%20to%20optimise%20AV%20control%20towards%20safer%20and%20more%20efficient%20behaviour.%20Overall%2C%20this%20work%20illustrates%20how%20incorporating%20human-like%20road%20user%20models%20into%20simulation-based%20adversarial%20testing%20can%20enhance%20the%20credibility%20of%20AV%20evaluation%20and%20provide%20a%20practical%20basis%20to%20behaviourally%20informed%20controller%20optimisation.&entry.1838667208=http%3A//arxiv.org/abs/2601.02082v2&entry.124074799=Read"},
{"title": "SalFormer360: a transformer-based saliency estimation model for 360-degree videos", "author": "Mahmoud Z. A. Wahba and Francesco Barbato and Sara Baldoni and Federica Battisti", "abstract": "Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.", "link": "http://arxiv.org/abs/2602.04584v1", "date": "2026-02-04", "relevancy": 2.2212, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SalFormer360%3A%20a%20transformer-based%20saliency%20estimation%20model%20for%20360-degree%20videos&body=Title%3A%20SalFormer360%3A%20a%20transformer-based%20saliency%20estimation%20model%20for%20360-degree%20videos%0AAuthor%3A%20Mahmoud%20Z.%20A.%20Wahba%20and%20Francesco%20Barbato%20and%20Sara%20Baldoni%20and%20Federica%20Battisti%0AAbstract%3A%20Saliency%20estimation%20has%20received%20growing%20attention%20in%20recent%20years%20due%20to%20its%20importance%20in%20a%20wide%20range%20of%20applications.%20In%20the%20context%20of%20360-degree%20video%2C%20it%20has%20been%20particularly%20valuable%20for%20tasks%20such%20as%20viewport%20prediction%20and%20immersive%20content%20optimization.%20In%20this%20paper%2C%20we%20propose%20SalFormer360%2C%20a%20novel%20saliency%20estimation%20model%20for%20360-degree%20videos%20built%20on%20a%20transformer-based%20architecture.%20Our%20approach%20is%20based%20on%20the%20combination%20of%20an%20existing%20encoder%20architecture%2C%20SegFormer%2C%20and%20a%20custom%20decoder.%20The%20SegFormer%20model%20was%20originally%20developed%20for%202D%20segmentation%20tasks%2C%20and%20it%20has%20been%20fine-tuned%20to%20adapt%20it%20to%20360-degree%20content.%20To%20further%20enhance%20prediction%20accuracy%20in%20our%20model%2C%20we%20incorporated%20Viewing%20Center%20Bias%20to%20reflect%20user%20attention%20in%20360-degree%20environments.%20Extensive%20experiments%20on%20the%20three%20largest%20benchmark%20datasets%20for%20saliency%20estimation%20demonstrate%20that%20SalFormer360%20outperforms%20existing%20state-of-the-art%20methods.%20In%20terms%20of%20Pearson%20Correlation%20Coefficient%2C%20our%20model%20achieves%208.4%25%20higher%20performance%20on%20Sport360%2C%202.5%25%20on%20PVS-HM%2C%20and%2018.6%25%20on%20VR-EyeTracking%20compared%20to%20previous%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSalFormer360%253A%2520a%2520transformer-based%2520saliency%2520estimation%2520model%2520for%2520360-degree%2520videos%26entry.906535625%3DMahmoud%2520Z.%2520A.%2520Wahba%2520and%2520Francesco%2520Barbato%2520and%2520Sara%2520Baldoni%2520and%2520Federica%2520Battisti%26entry.1292438233%3DSaliency%2520estimation%2520has%2520received%2520growing%2520attention%2520in%2520recent%2520years%2520due%2520to%2520its%2520importance%2520in%2520a%2520wide%2520range%2520of%2520applications.%2520In%2520the%2520context%2520of%2520360-degree%2520video%252C%2520it%2520has%2520been%2520particularly%2520valuable%2520for%2520tasks%2520such%2520as%2520viewport%2520prediction%2520and%2520immersive%2520content%2520optimization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SalFormer360%252C%2520a%2520novel%2520saliency%2520estimation%2520model%2520for%2520360-degree%2520videos%2520built%2520on%2520a%2520transformer-based%2520architecture.%2520Our%2520approach%2520is%2520based%2520on%2520the%2520combination%2520of%2520an%2520existing%2520encoder%2520architecture%252C%2520SegFormer%252C%2520and%2520a%2520custom%2520decoder.%2520The%2520SegFormer%2520model%2520was%2520originally%2520developed%2520for%25202D%2520segmentation%2520tasks%252C%2520and%2520it%2520has%2520been%2520fine-tuned%2520to%2520adapt%2520it%2520to%2520360-degree%2520content.%2520To%2520further%2520enhance%2520prediction%2520accuracy%2520in%2520our%2520model%252C%2520we%2520incorporated%2520Viewing%2520Center%2520Bias%2520to%2520reflect%2520user%2520attention%2520in%2520360-degree%2520environments.%2520Extensive%2520experiments%2520on%2520the%2520three%2520largest%2520benchmark%2520datasets%2520for%2520saliency%2520estimation%2520demonstrate%2520that%2520SalFormer360%2520outperforms%2520existing%2520state-of-the-art%2520methods.%2520In%2520terms%2520of%2520Pearson%2520Correlation%2520Coefficient%252C%2520our%2520model%2520achieves%25208.4%2525%2520higher%2520performance%2520on%2520Sport360%252C%25202.5%2525%2520on%2520PVS-HM%252C%2520and%252018.6%2525%2520on%2520VR-EyeTracking%2520compared%2520to%2520previous%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SalFormer360%3A%20a%20transformer-based%20saliency%20estimation%20model%20for%20360-degree%20videos&entry.906535625=Mahmoud%20Z.%20A.%20Wahba%20and%20Francesco%20Barbato%20and%20Sara%20Baldoni%20and%20Federica%20Battisti&entry.1292438233=Saliency%20estimation%20has%20received%20growing%20attention%20in%20recent%20years%20due%20to%20its%20importance%20in%20a%20wide%20range%20of%20applications.%20In%20the%20context%20of%20360-degree%20video%2C%20it%20has%20been%20particularly%20valuable%20for%20tasks%20such%20as%20viewport%20prediction%20and%20immersive%20content%20optimization.%20In%20this%20paper%2C%20we%20propose%20SalFormer360%2C%20a%20novel%20saliency%20estimation%20model%20for%20360-degree%20videos%20built%20on%20a%20transformer-based%20architecture.%20Our%20approach%20is%20based%20on%20the%20combination%20of%20an%20existing%20encoder%20architecture%2C%20SegFormer%2C%20and%20a%20custom%20decoder.%20The%20SegFormer%20model%20was%20originally%20developed%20for%202D%20segmentation%20tasks%2C%20and%20it%20has%20been%20fine-tuned%20to%20adapt%20it%20to%20360-degree%20content.%20To%20further%20enhance%20prediction%20accuracy%20in%20our%20model%2C%20we%20incorporated%20Viewing%20Center%20Bias%20to%20reflect%20user%20attention%20in%20360-degree%20environments.%20Extensive%20experiments%20on%20the%20three%20largest%20benchmark%20datasets%20for%20saliency%20estimation%20demonstrate%20that%20SalFormer360%20outperforms%20existing%20state-of-the-art%20methods.%20In%20terms%20of%20Pearson%20Correlation%20Coefficient%2C%20our%20model%20achieves%208.4%25%20higher%20performance%20on%20Sport360%2C%202.5%25%20on%20PVS-HM%2C%20and%2018.6%25%20on%20VR-EyeTracking%20compared%20to%20previous%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2602.04584v1&entry.124074799=Read"},
{"title": "Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion", "author": "Hanmo Chen and Chenghao Xu and Xu Yang and Xuan Chen and Cheng Deng", "abstract": "Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.", "link": "http://arxiv.org/abs/2601.21896v3", "date": "2026-02-04", "relevancy": 2.2075, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5647}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Past-%20and%20Future-Informed%20KV%20Cache%20Policy%20with%20Salience%20Estimation%20in%20Autoregressive%20Video%20Diffusion&body=Title%3A%20Past-%20and%20Future-Informed%20KV%20Cache%20Policy%20with%20Salience%20Estimation%20in%20Autoregressive%20Video%20Diffusion%0AAuthor%3A%20Hanmo%20Chen%20and%20Chenghao%20Xu%20and%20Xu%20Yang%20and%20Xuan%20Chen%20and%20Cheng%20Deng%0AAbstract%3A%20Video%20generation%20is%20pivotal%20to%20digital%20media%20creation%2C%20and%20recent%20advances%20in%20autoregressive%20video%20generation%20have%20markedly%20enhanced%20the%20efficiency%20of%20real-time%20video%20synthesis.%20However%2C%20existing%20approaches%20generally%20rely%20on%20heuristic%20KV%20Cache%20policies%2C%20which%20ignore%20differences%20in%20token%20importance%20in%20long-term%20video%20generation.%20This%20leads%20to%20the%20loss%20of%20critical%20spatiotemporal%20information%20and%20the%20accumulation%20of%20redundant%2C%20invalid%20cache%2C%20thereby%20degrading%20video%20generation%20quality%20and%20efficiency.%20To%20address%20this%20limitation%2C%20we%20first%20observe%20that%20token%20contributions%20to%20video%20generation%20are%20highly%20time-heterogeneous%20and%20accordingly%20propose%20a%20novel%20Past-%20and%20Future-Informed%20KV%20Cache%20Policy%20%28PaFu-KV%29.%20Specifically%2C%20PaFu-KV%20introduces%20a%20lightweight%20Salience%20Estimation%20Head%20distilled%20from%20a%20bidirectional%20teacher%20to%20estimate%20salience%20scores%2C%20allowing%20the%20KV%20cache%20to%20retain%20informative%20tokens%20while%20discarding%20less%20relevant%20ones.%20This%20policy%20yields%20a%20better%20quality-efficiency%20trade-off%20by%20shrinking%20KV%20cache%20capacity%20and%20reducing%20memory%20footprint%20at%20inference%20time.%20Extensive%20experiments%20on%20benchmarks%20demonstrate%20that%20our%20method%20preserves%20high-fidelity%20video%20generation%20quality%20while%20enables%20accelerated%20inference%2C%20thereby%20enabling%20more%20efficient%20long-horizon%20video%20generation.%20Our%20code%20will%20be%20released%20upon%20paper%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21896v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPast-%2520and%2520Future-Informed%2520KV%2520Cache%2520Policy%2520with%2520Salience%2520Estimation%2520in%2520Autoregressive%2520Video%2520Diffusion%26entry.906535625%3DHanmo%2520Chen%2520and%2520Chenghao%2520Xu%2520and%2520Xu%2520Yang%2520and%2520Xuan%2520Chen%2520and%2520Cheng%2520Deng%26entry.1292438233%3DVideo%2520generation%2520is%2520pivotal%2520to%2520digital%2520media%2520creation%252C%2520and%2520recent%2520advances%2520in%2520autoregressive%2520video%2520generation%2520have%2520markedly%2520enhanced%2520the%2520efficiency%2520of%2520real-time%2520video%2520synthesis.%2520However%252C%2520existing%2520approaches%2520generally%2520rely%2520on%2520heuristic%2520KV%2520Cache%2520policies%252C%2520which%2520ignore%2520differences%2520in%2520token%2520importance%2520in%2520long-term%2520video%2520generation.%2520This%2520leads%2520to%2520the%2520loss%2520of%2520critical%2520spatiotemporal%2520information%2520and%2520the%2520accumulation%2520of%2520redundant%252C%2520invalid%2520cache%252C%2520thereby%2520degrading%2520video%2520generation%2520quality%2520and%2520efficiency.%2520To%2520address%2520this%2520limitation%252C%2520we%2520first%2520observe%2520that%2520token%2520contributions%2520to%2520video%2520generation%2520are%2520highly%2520time-heterogeneous%2520and%2520accordingly%2520propose%2520a%2520novel%2520Past-%2520and%2520Future-Informed%2520KV%2520Cache%2520Policy%2520%2528PaFu-KV%2529.%2520Specifically%252C%2520PaFu-KV%2520introduces%2520a%2520lightweight%2520Salience%2520Estimation%2520Head%2520distilled%2520from%2520a%2520bidirectional%2520teacher%2520to%2520estimate%2520salience%2520scores%252C%2520allowing%2520the%2520KV%2520cache%2520to%2520retain%2520informative%2520tokens%2520while%2520discarding%2520less%2520relevant%2520ones.%2520This%2520policy%2520yields%2520a%2520better%2520quality-efficiency%2520trade-off%2520by%2520shrinking%2520KV%2520cache%2520capacity%2520and%2520reducing%2520memory%2520footprint%2520at%2520inference%2520time.%2520Extensive%2520experiments%2520on%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520preserves%2520high-fidelity%2520video%2520generation%2520quality%2520while%2520enables%2520accelerated%2520inference%252C%2520thereby%2520enabling%2520more%2520efficient%2520long-horizon%2520video%2520generation.%2520Our%2520code%2520will%2520be%2520released%2520upon%2520paper%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21896v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Past-%20and%20Future-Informed%20KV%20Cache%20Policy%20with%20Salience%20Estimation%20in%20Autoregressive%20Video%20Diffusion&entry.906535625=Hanmo%20Chen%20and%20Chenghao%20Xu%20and%20Xu%20Yang%20and%20Xuan%20Chen%20and%20Cheng%20Deng&entry.1292438233=Video%20generation%20is%20pivotal%20to%20digital%20media%20creation%2C%20and%20recent%20advances%20in%20autoregressive%20video%20generation%20have%20markedly%20enhanced%20the%20efficiency%20of%20real-time%20video%20synthesis.%20However%2C%20existing%20approaches%20generally%20rely%20on%20heuristic%20KV%20Cache%20policies%2C%20which%20ignore%20differences%20in%20token%20importance%20in%20long-term%20video%20generation.%20This%20leads%20to%20the%20loss%20of%20critical%20spatiotemporal%20information%20and%20the%20accumulation%20of%20redundant%2C%20invalid%20cache%2C%20thereby%20degrading%20video%20generation%20quality%20and%20efficiency.%20To%20address%20this%20limitation%2C%20we%20first%20observe%20that%20token%20contributions%20to%20video%20generation%20are%20highly%20time-heterogeneous%20and%20accordingly%20propose%20a%20novel%20Past-%20and%20Future-Informed%20KV%20Cache%20Policy%20%28PaFu-KV%29.%20Specifically%2C%20PaFu-KV%20introduces%20a%20lightweight%20Salience%20Estimation%20Head%20distilled%20from%20a%20bidirectional%20teacher%20to%20estimate%20salience%20scores%2C%20allowing%20the%20KV%20cache%20to%20retain%20informative%20tokens%20while%20discarding%20less%20relevant%20ones.%20This%20policy%20yields%20a%20better%20quality-efficiency%20trade-off%20by%20shrinking%20KV%20cache%20capacity%20and%20reducing%20memory%20footprint%20at%20inference%20time.%20Extensive%20experiments%20on%20benchmarks%20demonstrate%20that%20our%20method%20preserves%20high-fidelity%20video%20generation%20quality%20while%20enables%20accelerated%20inference%2C%20thereby%20enabling%20more%20efficient%20long-horizon%20video%20generation.%20Our%20code%20will%20be%20released%20upon%20paper%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.21896v3&entry.124074799=Read"},
{"title": "Dynamic Pyramid Network for Efficient Multimodal Large Language Model", "author": "Hao Ai and Kunyi Wang and Zezhou Wang and Hao Lu and Jin Tian and Yaxin Luo and Peng Xing and Jen-Yuan Huang and Huaxia Li and Gen luo", "abstract": "Multimodal large language models (MLLMs) have demonstrated impressive performance in various vision-language (VL) tasks, but their expensive computations still limit the real-world application. To address this issue, recent efforts aim to compress the visual features to save the computational costs of MLLMs. However, direct visual compression methods, e.g. efficient projectors, inevitably destroy the visual semantics in MLLM, especially in difficult samples. To overcome this shortcoming, we propose a novel dynamic pyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as a hierarchical structure where visual features are gradually compressed with increasing depth. In this case, even with a high compression ratio, fine-grained visual information can still be perceived in shallow layers. To maximize the benefit of DPN, we further propose an innovative Dynamic Pooling Experts (DPE) that can dynamically choose the optimal visual compression rate according to input features. With this design, harder samples will be assigned larger computations, thus preserving the model performance. To validate our approach, we conduct extensive experiments on two popular MLLMs and ten benchmarks. Experimental results show that DPN can save up to 56% average FLOPs on LLaVA while further achieving +0.74% performance gains. Besides, the generalization ability of DPN is also validated on the existing high-resolution MLLM called LLaVA-HR. The source code will be released at https://github.com/aihao2000/DPN-LLaVA.", "link": "http://arxiv.org/abs/2503.20322v3", "date": "2026-02-04", "relevancy": 2.2062, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5606}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Pyramid%20Network%20for%20Efficient%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Dynamic%20Pyramid%20Network%20for%20Efficient%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Hao%20Ai%20and%20Kunyi%20Wang%20and%20Zezhou%20Wang%20and%20Hao%20Lu%20and%20Jin%20Tian%20and%20Yaxin%20Luo%20and%20Peng%20Xing%20and%20Jen-Yuan%20Huang%20and%20Huaxia%20Li%20and%20Gen%20luo%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20in%20various%20vision-language%20%28VL%29%20tasks%2C%20but%20their%20expensive%20computations%20still%20limit%20the%20real-world%20application.%20To%20address%20this%20issue%2C%20recent%20efforts%20aim%20to%20compress%20the%20visual%20features%20to%20save%20the%20computational%20costs%20of%20MLLMs.%20However%2C%20direct%20visual%20compression%20methods%2C%20e.g.%20efficient%20projectors%2C%20inevitably%20destroy%20the%20visual%20semantics%20in%20MLLM%2C%20especially%20in%20difficult%20samples.%20To%20overcome%20this%20shortcoming%2C%20we%20propose%20a%20novel%20dynamic%20pyramid%20network%20%28DPN%29%20for%20efficient%20MLLMs.%20Specifically%2C%20DPN%20formulates%20MLLM%20as%20a%20hierarchical%20structure%20where%20visual%20features%20are%20gradually%20compressed%20with%20increasing%20depth.%20In%20this%20case%2C%20even%20with%20a%20high%20compression%20ratio%2C%20fine-grained%20visual%20information%20can%20still%20be%20perceived%20in%20shallow%20layers.%20To%20maximize%20the%20benefit%20of%20DPN%2C%20we%20further%20propose%20an%20innovative%20Dynamic%20Pooling%20Experts%20%28DPE%29%20that%20can%20dynamically%20choose%20the%20optimal%20visual%20compression%20rate%20according%20to%20input%20features.%20With%20this%20design%2C%20harder%20samples%20will%20be%20assigned%20larger%20computations%2C%20thus%20preserving%20the%20model%20performance.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%20experiments%20on%20two%20popular%20MLLMs%20and%20ten%20benchmarks.%20Experimental%20results%20show%20that%20DPN%20can%20save%20up%20to%2056%25%20average%20FLOPs%20on%20LLaVA%20while%20further%20achieving%20%2B0.74%25%20performance%20gains.%20Besides%2C%20the%20generalization%20ability%20of%20DPN%20is%20also%20validated%20on%20the%20existing%20high-resolution%20MLLM%20called%20LLaVA-HR.%20The%20source%20code%20will%20be%20released%20at%20https%3A//github.com/aihao2000/DPN-LLaVA.%0ALink%3A%20http%3A//arxiv.org/abs/2503.20322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Pyramid%2520Network%2520for%2520Efficient%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DHao%2520Ai%2520and%2520Kunyi%2520Wang%2520and%2520Zezhou%2520Wang%2520and%2520Hao%2520Lu%2520and%2520Jin%2520Tian%2520and%2520Yaxin%2520Luo%2520and%2520Peng%2520Xing%2520and%2520Jen-Yuan%2520Huang%2520and%2520Huaxia%2520Li%2520and%2520Gen%2520luo%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520various%2520vision-language%2520%2528VL%2529%2520tasks%252C%2520but%2520their%2520expensive%2520computations%2520still%2520limit%2520the%2520real-world%2520application.%2520To%2520address%2520this%2520issue%252C%2520recent%2520efforts%2520aim%2520to%2520compress%2520the%2520visual%2520features%2520to%2520save%2520the%2520computational%2520costs%2520of%2520MLLMs.%2520However%252C%2520direct%2520visual%2520compression%2520methods%252C%2520e.g.%2520efficient%2520projectors%252C%2520inevitably%2520destroy%2520the%2520visual%2520semantics%2520in%2520MLLM%252C%2520especially%2520in%2520difficult%2520samples.%2520To%2520overcome%2520this%2520shortcoming%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520pyramid%2520network%2520%2528DPN%2529%2520for%2520efficient%2520MLLMs.%2520Specifically%252C%2520DPN%2520formulates%2520MLLM%2520as%2520a%2520hierarchical%2520structure%2520where%2520visual%2520features%2520are%2520gradually%2520compressed%2520with%2520increasing%2520depth.%2520In%2520this%2520case%252C%2520even%2520with%2520a%2520high%2520compression%2520ratio%252C%2520fine-grained%2520visual%2520information%2520can%2520still%2520be%2520perceived%2520in%2520shallow%2520layers.%2520To%2520maximize%2520the%2520benefit%2520of%2520DPN%252C%2520we%2520further%2520propose%2520an%2520innovative%2520Dynamic%2520Pooling%2520Experts%2520%2528DPE%2529%2520that%2520can%2520dynamically%2520choose%2520the%2520optimal%2520visual%2520compression%2520rate%2520according%2520to%2520input%2520features.%2520With%2520this%2520design%252C%2520harder%2520samples%2520will%2520be%2520assigned%2520larger%2520computations%252C%2520thus%2520preserving%2520the%2520model%2520performance.%2520To%2520validate%2520our%2520approach%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520two%2520popular%2520MLLMs%2520and%2520ten%2520benchmarks.%2520Experimental%2520results%2520show%2520that%2520DPN%2520can%2520save%2520up%2520to%252056%2525%2520average%2520FLOPs%2520on%2520LLaVA%2520while%2520further%2520achieving%2520%252B0.74%2525%2520performance%2520gains.%2520Besides%252C%2520the%2520generalization%2520ability%2520of%2520DPN%2520is%2520also%2520validated%2520on%2520the%2520existing%2520high-resolution%2520MLLM%2520called%2520LLaVA-HR.%2520The%2520source%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/aihao2000/DPN-LLaVA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Pyramid%20Network%20for%20Efficient%20Multimodal%20Large%20Language%20Model&entry.906535625=Hao%20Ai%20and%20Kunyi%20Wang%20and%20Zezhou%20Wang%20and%20Hao%20Lu%20and%20Jin%20Tian%20and%20Yaxin%20Luo%20and%20Peng%20Xing%20and%20Jen-Yuan%20Huang%20and%20Huaxia%20Li%20and%20Gen%20luo&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20in%20various%20vision-language%20%28VL%29%20tasks%2C%20but%20their%20expensive%20computations%20still%20limit%20the%20real-world%20application.%20To%20address%20this%20issue%2C%20recent%20efforts%20aim%20to%20compress%20the%20visual%20features%20to%20save%20the%20computational%20costs%20of%20MLLMs.%20However%2C%20direct%20visual%20compression%20methods%2C%20e.g.%20efficient%20projectors%2C%20inevitably%20destroy%20the%20visual%20semantics%20in%20MLLM%2C%20especially%20in%20difficult%20samples.%20To%20overcome%20this%20shortcoming%2C%20we%20propose%20a%20novel%20dynamic%20pyramid%20network%20%28DPN%29%20for%20efficient%20MLLMs.%20Specifically%2C%20DPN%20formulates%20MLLM%20as%20a%20hierarchical%20structure%20where%20visual%20features%20are%20gradually%20compressed%20with%20increasing%20depth.%20In%20this%20case%2C%20even%20with%20a%20high%20compression%20ratio%2C%20fine-grained%20visual%20information%20can%20still%20be%20perceived%20in%20shallow%20layers.%20To%20maximize%20the%20benefit%20of%20DPN%2C%20we%20further%20propose%20an%20innovative%20Dynamic%20Pooling%20Experts%20%28DPE%29%20that%20can%20dynamically%20choose%20the%20optimal%20visual%20compression%20rate%20according%20to%20input%20features.%20With%20this%20design%2C%20harder%20samples%20will%20be%20assigned%20larger%20computations%2C%20thus%20preserving%20the%20model%20performance.%20To%20validate%20our%20approach%2C%20we%20conduct%20extensive%20experiments%20on%20two%20popular%20MLLMs%20and%20ten%20benchmarks.%20Experimental%20results%20show%20that%20DPN%20can%20save%20up%20to%2056%25%20average%20FLOPs%20on%20LLaVA%20while%20further%20achieving%20%2B0.74%25%20performance%20gains.%20Besides%2C%20the%20generalization%20ability%20of%20DPN%20is%20also%20validated%20on%20the%20existing%20high-resolution%20MLLM%20called%20LLaVA-HR.%20The%20source%20code%20will%20be%20released%20at%20https%3A//github.com/aihao2000/DPN-LLaVA.&entry.1838667208=http%3A//arxiv.org/abs/2503.20322v3&entry.124074799=Read"},
{"title": "User-Feedback-Driven Adaptation for Vision-and-Language Navigation", "author": "Yongqiang Yu and Xuhui Li and Hazza Mahmood and Jinxing Zhou and Haodong Hong and Longtao Jiang and Zhiqiang Xu and Qi Wu and Xiaojun Chang", "abstract": "Real-world deployment of Vision-and-Language Navigation (VLN) agents is constrained by the scarcity of reliable supervision after offline training. While recent adaptation methods attempt to mitigate distribution shifts via environment-driven self-supervision (e.g., entropy minimization), these signals are often noisy and can cause the agent to amplify its own mistakes during long-horizon sequential decision-making. In this paper, we propose a paradigm shift that positions user feedback, specifically episode-level success confirmations and goal-level corrections, as a primary and general-purpose supervision signal for VLN. Unlike internal confidence scores, user feedback is intent-aligned and in-situ consistent, directly correcting the agent's decoupling from user instructions. To effectively leverage this supervision, we introduce a user-feedback-driven learning framework featuring a topology-aware trajectory construction pipeline. This mechanism lifts sparse, goal-level corrections into dense path-level supervision by generating feasible paths on the agent's incrementally built topological graph, enabling sample-efficient imitation learning without requiring step-by-step human demonstrations. Furthermore, we develop a persistent memory bank mechanism for warm-start initialization, supporting the reuse of previously acquired topology and cached representations across navigation sessions. Extensive experiments on the GSA-R2R benchmark demonstrate that our approach transforms sparse interaction into robust supervision, consistently outperforming environment-driven baselines while exhibiting strong adaptability across diverse instruction styles.", "link": "http://arxiv.org/abs/2512.10322v2", "date": "2026-02-04", "relevancy": 2.2053, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5823}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5373}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User-Feedback-Driven%20Adaptation%20for%20Vision-and-Language%20Navigation&body=Title%3A%20User-Feedback-Driven%20Adaptation%20for%20Vision-and-Language%20Navigation%0AAuthor%3A%20Yongqiang%20Yu%20and%20Xuhui%20Li%20and%20Hazza%20Mahmood%20and%20Jinxing%20Zhou%20and%20Haodong%20Hong%20and%20Longtao%20Jiang%20and%20Zhiqiang%20Xu%20and%20Qi%20Wu%20and%20Xiaojun%20Chang%0AAbstract%3A%20Real-world%20deployment%20of%20Vision-and-Language%20Navigation%20%28VLN%29%20agents%20is%20constrained%20by%20the%20scarcity%20of%20reliable%20supervision%20after%20offline%20training.%20While%20recent%20adaptation%20methods%20attempt%20to%20mitigate%20distribution%20shifts%20via%20environment-driven%20self-supervision%20%28e.g.%2C%20entropy%20minimization%29%2C%20these%20signals%20are%20often%20noisy%20and%20can%20cause%20the%20agent%20to%20amplify%20its%20own%20mistakes%20during%20long-horizon%20sequential%20decision-making.%20In%20this%20paper%2C%20we%20propose%20a%20paradigm%20shift%20that%20positions%20user%20feedback%2C%20specifically%20episode-level%20success%20confirmations%20and%20goal-level%20corrections%2C%20as%20a%20primary%20and%20general-purpose%20supervision%20signal%20for%20VLN.%20Unlike%20internal%20confidence%20scores%2C%20user%20feedback%20is%20intent-aligned%20and%20in-situ%20consistent%2C%20directly%20correcting%20the%20agent%27s%20decoupling%20from%20user%20instructions.%20To%20effectively%20leverage%20this%20supervision%2C%20we%20introduce%20a%20user-feedback-driven%20learning%20framework%20featuring%20a%20topology-aware%20trajectory%20construction%20pipeline.%20This%20mechanism%20lifts%20sparse%2C%20goal-level%20corrections%20into%20dense%20path-level%20supervision%20by%20generating%20feasible%20paths%20on%20the%20agent%27s%20incrementally%20built%20topological%20graph%2C%20enabling%20sample-efficient%20imitation%20learning%20without%20requiring%20step-by-step%20human%20demonstrations.%20Furthermore%2C%20we%20develop%20a%20persistent%20memory%20bank%20mechanism%20for%20warm-start%20initialization%2C%20supporting%20the%20reuse%20of%20previously%20acquired%20topology%20and%20cached%20representations%20across%20navigation%20sessions.%20Extensive%20experiments%20on%20the%20GSA-R2R%20benchmark%20demonstrate%20that%20our%20approach%20transforms%20sparse%20interaction%20into%20robust%20supervision%2C%20consistently%20outperforming%20environment-driven%20baselines%20while%20exhibiting%20strong%20adaptability%20across%20diverse%20instruction%20styles.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser-Feedback-Driven%2520Adaptation%2520for%2520Vision-and-Language%2520Navigation%26entry.906535625%3DYongqiang%2520Yu%2520and%2520Xuhui%2520Li%2520and%2520Hazza%2520Mahmood%2520and%2520Jinxing%2520Zhou%2520and%2520Haodong%2520Hong%2520and%2520Longtao%2520Jiang%2520and%2520Zhiqiang%2520Xu%2520and%2520Qi%2520Wu%2520and%2520Xiaojun%2520Chang%26entry.1292438233%3DReal-world%2520deployment%2520of%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520agents%2520is%2520constrained%2520by%2520the%2520scarcity%2520of%2520reliable%2520supervision%2520after%2520offline%2520training.%2520While%2520recent%2520adaptation%2520methods%2520attempt%2520to%2520mitigate%2520distribution%2520shifts%2520via%2520environment-driven%2520self-supervision%2520%2528e.g.%252C%2520entropy%2520minimization%2529%252C%2520these%2520signals%2520are%2520often%2520noisy%2520and%2520can%2520cause%2520the%2520agent%2520to%2520amplify%2520its%2520own%2520mistakes%2520during%2520long-horizon%2520sequential%2520decision-making.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520paradigm%2520shift%2520that%2520positions%2520user%2520feedback%252C%2520specifically%2520episode-level%2520success%2520confirmations%2520and%2520goal-level%2520corrections%252C%2520as%2520a%2520primary%2520and%2520general-purpose%2520supervision%2520signal%2520for%2520VLN.%2520Unlike%2520internal%2520confidence%2520scores%252C%2520user%2520feedback%2520is%2520intent-aligned%2520and%2520in-situ%2520consistent%252C%2520directly%2520correcting%2520the%2520agent%2527s%2520decoupling%2520from%2520user%2520instructions.%2520To%2520effectively%2520leverage%2520this%2520supervision%252C%2520we%2520introduce%2520a%2520user-feedback-driven%2520learning%2520framework%2520featuring%2520a%2520topology-aware%2520trajectory%2520construction%2520pipeline.%2520This%2520mechanism%2520lifts%2520sparse%252C%2520goal-level%2520corrections%2520into%2520dense%2520path-level%2520supervision%2520by%2520generating%2520feasible%2520paths%2520on%2520the%2520agent%2527s%2520incrementally%2520built%2520topological%2520graph%252C%2520enabling%2520sample-efficient%2520imitation%2520learning%2520without%2520requiring%2520step-by-step%2520human%2520demonstrations.%2520Furthermore%252C%2520we%2520develop%2520a%2520persistent%2520memory%2520bank%2520mechanism%2520for%2520warm-start%2520initialization%252C%2520supporting%2520the%2520reuse%2520of%2520previously%2520acquired%2520topology%2520and%2520cached%2520representations%2520across%2520navigation%2520sessions.%2520Extensive%2520experiments%2520on%2520the%2520GSA-R2R%2520benchmark%2520demonstrate%2520that%2520our%2520approach%2520transforms%2520sparse%2520interaction%2520into%2520robust%2520supervision%252C%2520consistently%2520outperforming%2520environment-driven%2520baselines%2520while%2520exhibiting%2520strong%2520adaptability%2520across%2520diverse%2520instruction%2520styles.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User-Feedback-Driven%20Adaptation%20for%20Vision-and-Language%20Navigation&entry.906535625=Yongqiang%20Yu%20and%20Xuhui%20Li%20and%20Hazza%20Mahmood%20and%20Jinxing%20Zhou%20and%20Haodong%20Hong%20and%20Longtao%20Jiang%20and%20Zhiqiang%20Xu%20and%20Qi%20Wu%20and%20Xiaojun%20Chang&entry.1292438233=Real-world%20deployment%20of%20Vision-and-Language%20Navigation%20%28VLN%29%20agents%20is%20constrained%20by%20the%20scarcity%20of%20reliable%20supervision%20after%20offline%20training.%20While%20recent%20adaptation%20methods%20attempt%20to%20mitigate%20distribution%20shifts%20via%20environment-driven%20self-supervision%20%28e.g.%2C%20entropy%20minimization%29%2C%20these%20signals%20are%20often%20noisy%20and%20can%20cause%20the%20agent%20to%20amplify%20its%20own%20mistakes%20during%20long-horizon%20sequential%20decision-making.%20In%20this%20paper%2C%20we%20propose%20a%20paradigm%20shift%20that%20positions%20user%20feedback%2C%20specifically%20episode-level%20success%20confirmations%20and%20goal-level%20corrections%2C%20as%20a%20primary%20and%20general-purpose%20supervision%20signal%20for%20VLN.%20Unlike%20internal%20confidence%20scores%2C%20user%20feedback%20is%20intent-aligned%20and%20in-situ%20consistent%2C%20directly%20correcting%20the%20agent%27s%20decoupling%20from%20user%20instructions.%20To%20effectively%20leverage%20this%20supervision%2C%20we%20introduce%20a%20user-feedback-driven%20learning%20framework%20featuring%20a%20topology-aware%20trajectory%20construction%20pipeline.%20This%20mechanism%20lifts%20sparse%2C%20goal-level%20corrections%20into%20dense%20path-level%20supervision%20by%20generating%20feasible%20paths%20on%20the%20agent%27s%20incrementally%20built%20topological%20graph%2C%20enabling%20sample-efficient%20imitation%20learning%20without%20requiring%20step-by-step%20human%20demonstrations.%20Furthermore%2C%20we%20develop%20a%20persistent%20memory%20bank%20mechanism%20for%20warm-start%20initialization%2C%20supporting%20the%20reuse%20of%20previously%20acquired%20topology%20and%20cached%20representations%20across%20navigation%20sessions.%20Extensive%20experiments%20on%20the%20GSA-R2R%20benchmark%20demonstrate%20that%20our%20approach%20transforms%20sparse%20interaction%20into%20robust%20supervision%2C%20consistently%20outperforming%20environment-driven%20baselines%20while%20exhibiting%20strong%20adaptability%20across%20diverse%20instruction%20styles.&entry.1838667208=http%3A//arxiv.org/abs/2512.10322v2&entry.124074799=Read"},
{"title": "HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation", "author": "Puyue Wang and Jiawei Hu and Yan Gao and Junyan Wang and Yu Zhang and Gillian Dobbie and Tao Gu and Wafa Johal and Ting Dang and Hong Jia", "abstract": "Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \\href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.", "link": "http://arxiv.org/abs/2602.04412v1", "date": "2026-02-04", "relevancy": 2.1962, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5629}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoRD%3A%20Robust%20Humanoid%20Control%20via%20History-Conditioned%20Reinforcement%20Learning%20and%20Online%20Distillation&body=Title%3A%20HoRD%3A%20Robust%20Humanoid%20Control%20via%20History-Conditioned%20Reinforcement%20Learning%20and%20Online%20Distillation%0AAuthor%3A%20Puyue%20Wang%20and%20Jiawei%20Hu%20and%20Yan%20Gao%20and%20Junyan%20Wang%20and%20Yu%20Zhang%20and%20Gillian%20Dobbie%20and%20Tao%20Gu%20and%20Wafa%20Johal%20and%20Ting%20Dang%20and%20Hong%20Jia%0AAbstract%3A%20Humanoid%20robots%20can%20suffer%20significant%20performance%20drops%20under%20small%20changes%20in%20dynamics%2C%20task%20specifications%2C%20or%20environment%20setup.%20We%20propose%20HoRD%2C%20a%20two-stage%20learning%20framework%20for%20robust%20humanoid%20control%20under%20domain%20shift.%20First%2C%20we%20train%20a%20high-performance%20teacher%20policy%20via%20history-conditioned%20reinforcement%20learning%2C%20where%20the%20policy%20infers%20latent%20dynamics%20context%20from%20recent%20state--action%20trajectories%20to%20adapt%20online%20to%20diverse%20randomized%20dynamics.%20Second%2C%20we%20perform%20online%20distillation%20to%20transfer%20the%20teacher%27s%20robust%20control%20capabilities%20into%20a%20transformer-based%20student%20policy%20that%20operates%20on%20sparse%20root-relative%203D%20joint%20keypoint%20trajectories.%20By%20combining%20history-conditioned%20adaptation%20with%20online%20distillation%2C%20HoRD%20enables%20a%20single%20policy%20to%20adapt%20zero-shot%20to%20unseen%20domains%20without%20per-domain%20retraining.%20Extensive%20experiments%20show%20HoRD%20outperforms%20strong%20baselines%20in%20robustness%20and%20transfer%2C%20especially%20under%20unseen%20domains%20and%20external%20perturbations.%20Code%20and%20project%20page%20are%20available%20at%20%5Chref%7Bhttps%3A//tonywang-0517.github.io/hord/%7D%7Bhttps%3A//tonywang-0517.github.io/hord/%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoRD%253A%2520Robust%2520Humanoid%2520Control%2520via%2520History-Conditioned%2520Reinforcement%2520Learning%2520and%2520Online%2520Distillation%26entry.906535625%3DPuyue%2520Wang%2520and%2520Jiawei%2520Hu%2520and%2520Yan%2520Gao%2520and%2520Junyan%2520Wang%2520and%2520Yu%2520Zhang%2520and%2520Gillian%2520Dobbie%2520and%2520Tao%2520Gu%2520and%2520Wafa%2520Johal%2520and%2520Ting%2520Dang%2520and%2520Hong%2520Jia%26entry.1292438233%3DHumanoid%2520robots%2520can%2520suffer%2520significant%2520performance%2520drops%2520under%2520small%2520changes%2520in%2520dynamics%252C%2520task%2520specifications%252C%2520or%2520environment%2520setup.%2520We%2520propose%2520HoRD%252C%2520a%2520two-stage%2520learning%2520framework%2520for%2520robust%2520humanoid%2520control%2520under%2520domain%2520shift.%2520First%252C%2520we%2520train%2520a%2520high-performance%2520teacher%2520policy%2520via%2520history-conditioned%2520reinforcement%2520learning%252C%2520where%2520the%2520policy%2520infers%2520latent%2520dynamics%2520context%2520from%2520recent%2520state--action%2520trajectories%2520to%2520adapt%2520online%2520to%2520diverse%2520randomized%2520dynamics.%2520Second%252C%2520we%2520perform%2520online%2520distillation%2520to%2520transfer%2520the%2520teacher%2527s%2520robust%2520control%2520capabilities%2520into%2520a%2520transformer-based%2520student%2520policy%2520that%2520operates%2520on%2520sparse%2520root-relative%25203D%2520joint%2520keypoint%2520trajectories.%2520By%2520combining%2520history-conditioned%2520adaptation%2520with%2520online%2520distillation%252C%2520HoRD%2520enables%2520a%2520single%2520policy%2520to%2520adapt%2520zero-shot%2520to%2520unseen%2520domains%2520without%2520per-domain%2520retraining.%2520Extensive%2520experiments%2520show%2520HoRD%2520outperforms%2520strong%2520baselines%2520in%2520robustness%2520and%2520transfer%252C%2520especially%2520under%2520unseen%2520domains%2520and%2520external%2520perturbations.%2520Code%2520and%2520project%2520page%2520are%2520available%2520at%2520%255Chref%257Bhttps%253A//tonywang-0517.github.io/hord/%257D%257Bhttps%253A//tonywang-0517.github.io/hord/%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoRD%3A%20Robust%20Humanoid%20Control%20via%20History-Conditioned%20Reinforcement%20Learning%20and%20Online%20Distillation&entry.906535625=Puyue%20Wang%20and%20Jiawei%20Hu%20and%20Yan%20Gao%20and%20Junyan%20Wang%20and%20Yu%20Zhang%20and%20Gillian%20Dobbie%20and%20Tao%20Gu%20and%20Wafa%20Johal%20and%20Ting%20Dang%20and%20Hong%20Jia&entry.1292438233=Humanoid%20robots%20can%20suffer%20significant%20performance%20drops%20under%20small%20changes%20in%20dynamics%2C%20task%20specifications%2C%20or%20environment%20setup.%20We%20propose%20HoRD%2C%20a%20two-stage%20learning%20framework%20for%20robust%20humanoid%20control%20under%20domain%20shift.%20First%2C%20we%20train%20a%20high-performance%20teacher%20policy%20via%20history-conditioned%20reinforcement%20learning%2C%20where%20the%20policy%20infers%20latent%20dynamics%20context%20from%20recent%20state--action%20trajectories%20to%20adapt%20online%20to%20diverse%20randomized%20dynamics.%20Second%2C%20we%20perform%20online%20distillation%20to%20transfer%20the%20teacher%27s%20robust%20control%20capabilities%20into%20a%20transformer-based%20student%20policy%20that%20operates%20on%20sparse%20root-relative%203D%20joint%20keypoint%20trajectories.%20By%20combining%20history-conditioned%20adaptation%20with%20online%20distillation%2C%20HoRD%20enables%20a%20single%20policy%20to%20adapt%20zero-shot%20to%20unseen%20domains%20without%20per-domain%20retraining.%20Extensive%20experiments%20show%20HoRD%20outperforms%20strong%20baselines%20in%20robustness%20and%20transfer%2C%20especially%20under%20unseen%20domains%20and%20external%20perturbations.%20Code%20and%20project%20page%20are%20available%20at%20%5Chref%7Bhttps%3A//tonywang-0517.github.io/hord/%7D%7Bhttps%3A//tonywang-0517.github.io/hord/%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.04412v1&entry.124074799=Read"},
{"title": "Multi-Cue Anomaly Detection and Localization under Data Contamination", "author": "Anindya Sundar Das and Monowar Bhuyan", "abstract": "Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.", "link": "http://arxiv.org/abs/2601.22913v2", "date": "2026-02-04", "relevancy": 2.1915, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5656}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination&body=Title%3A%20Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination%0AAuthor%3A%20Anindya%20Sundar%20Das%20and%20Monowar%20Bhuyan%0AAbstract%3A%20Visual%20anomaly%20detection%20in%20real-world%20industrial%20settings%20faces%20two%20major%20limitations.%20First%2C%20most%20existing%20methods%20are%20trained%20on%20purely%20normal%20data%20or%20on%20unlabeled%20datasets%20assumed%20to%20be%20predominantly%20normal%2C%20presuming%20the%20absence%20of%20contamination%2C%20an%20assumption%20that%20is%20rarely%20satisfied%20in%20practice.%20Second%2C%20they%20assume%20no%20access%20to%20labeled%20anomaly%20samples%2C%20limiting%20the%20model%20from%20learning%20discriminative%20characteristics%20of%20true%20anomalies.%20Therefore%2C%20these%20approaches%20often%20struggle%20to%20distinguish%20anomalies%20from%20normal%20instances%2C%20resulting%20in%20reduced%20detection%20and%20weak%20localization%20performance.%20In%20real-world%20applications%2C%20where%20training%20data%20are%20frequently%20contaminated%20with%20anomalies%2C%20such%20methods%20fail%20to%20deliver%20reliable%20performance.%20In%20this%20work%2C%20we%20propose%20a%20robust%20anomaly%20detection%20framework%20that%20integrates%20limited%20anomaly%20supervision%20into%20the%20adaptive%20deviation%20learning%20paradigm.%20We%20introduce%20a%20composite%20anomaly%20score%20that%20combines%20three%20complementary%20components%3A%20a%20deviation%20score%20capturing%20statistical%20irregularity%2C%20an%20entropy-based%20uncertainty%20score%20reflecting%20predictive%20inconsistency%2C%20and%20a%20segmentation-based%20score%20highlighting%20spatial%20abnormality.%20This%20unified%20scoring%20mechanism%20enables%20accurate%20detection%20and%20supports%20gradient-based%20localization%2C%20providing%20intuitive%20and%20explainable%20visual%20evidence%20of%20anomalous%20regions.%20Following%20the%20few-anomaly%20paradigm%2C%20we%20incorporate%20a%20small%20set%20of%20labeled%20anomalies%20during%20training%20while%20simultaneously%20mitigating%20the%20influence%20of%20contaminated%20samples%20through%20adaptive%20instance%20weighting.%20Extensive%20experiments%20on%20the%20MVTec%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20and%20achieves%20strong%20detection%20and%20localization%20performance%2C%20interpretability%2C%20and%20robustness%20under%20various%20levels%20of%20data%20contamination.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Cue%2520Anomaly%2520Detection%2520and%2520Localization%2520under%2520Data%2520Contamination%26entry.906535625%3DAnindya%2520Sundar%2520Das%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3DVisual%2520anomaly%2520detection%2520in%2520real-world%2520industrial%2520settings%2520faces%2520two%2520major%2520limitations.%2520First%252C%2520most%2520existing%2520methods%2520are%2520trained%2520on%2520purely%2520normal%2520data%2520or%2520on%2520unlabeled%2520datasets%2520assumed%2520to%2520be%2520predominantly%2520normal%252C%2520presuming%2520the%2520absence%2520of%2520contamination%252C%2520an%2520assumption%2520that%2520is%2520rarely%2520satisfied%2520in%2520practice.%2520Second%252C%2520they%2520assume%2520no%2520access%2520to%2520labeled%2520anomaly%2520samples%252C%2520limiting%2520the%2520model%2520from%2520learning%2520discriminative%2520characteristics%2520of%2520true%2520anomalies.%2520Therefore%252C%2520these%2520approaches%2520often%2520struggle%2520to%2520distinguish%2520anomalies%2520from%2520normal%2520instances%252C%2520resulting%2520in%2520reduced%2520detection%2520and%2520weak%2520localization%2520performance.%2520In%2520real-world%2520applications%252C%2520where%2520training%2520data%2520are%2520frequently%2520contaminated%2520with%2520anomalies%252C%2520such%2520methods%2520fail%2520to%2520deliver%2520reliable%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520robust%2520anomaly%2520detection%2520framework%2520that%2520integrates%2520limited%2520anomaly%2520supervision%2520into%2520the%2520adaptive%2520deviation%2520learning%2520paradigm.%2520We%2520introduce%2520a%2520composite%2520anomaly%2520score%2520that%2520combines%2520three%2520complementary%2520components%253A%2520a%2520deviation%2520score%2520capturing%2520statistical%2520irregularity%252C%2520an%2520entropy-based%2520uncertainty%2520score%2520reflecting%2520predictive%2520inconsistency%252C%2520and%2520a%2520segmentation-based%2520score%2520highlighting%2520spatial%2520abnormality.%2520This%2520unified%2520scoring%2520mechanism%2520enables%2520accurate%2520detection%2520and%2520supports%2520gradient-based%2520localization%252C%2520providing%2520intuitive%2520and%2520explainable%2520visual%2520evidence%2520of%2520anomalous%2520regions.%2520Following%2520the%2520few-anomaly%2520paradigm%252C%2520we%2520incorporate%2520a%2520small%2520set%2520of%2520labeled%2520anomalies%2520during%2520training%2520while%2520simultaneously%2520mitigating%2520the%2520influence%2520of%2520contaminated%2520samples%2520through%2520adaptive%2520instance%2520weighting.%2520Extensive%2520experiments%2520on%2520the%2520MVTec%2520and%2520VisA%2520benchmarks%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520baselines%2520and%2520achieves%2520strong%2520detection%2520and%2520localization%2520performance%252C%2520interpretability%252C%2520and%2520robustness%2520under%2520various%2520levels%2520of%2520data%2520contamination.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination&entry.906535625=Anindya%20Sundar%20Das%20and%20Monowar%20Bhuyan&entry.1292438233=Visual%20anomaly%20detection%20in%20real-world%20industrial%20settings%20faces%20two%20major%20limitations.%20First%2C%20most%20existing%20methods%20are%20trained%20on%20purely%20normal%20data%20or%20on%20unlabeled%20datasets%20assumed%20to%20be%20predominantly%20normal%2C%20presuming%20the%20absence%20of%20contamination%2C%20an%20assumption%20that%20is%20rarely%20satisfied%20in%20practice.%20Second%2C%20they%20assume%20no%20access%20to%20labeled%20anomaly%20samples%2C%20limiting%20the%20model%20from%20learning%20discriminative%20characteristics%20of%20true%20anomalies.%20Therefore%2C%20these%20approaches%20often%20struggle%20to%20distinguish%20anomalies%20from%20normal%20instances%2C%20resulting%20in%20reduced%20detection%20and%20weak%20localization%20performance.%20In%20real-world%20applications%2C%20where%20training%20data%20are%20frequently%20contaminated%20with%20anomalies%2C%20such%20methods%20fail%20to%20deliver%20reliable%20performance.%20In%20this%20work%2C%20we%20propose%20a%20robust%20anomaly%20detection%20framework%20that%20integrates%20limited%20anomaly%20supervision%20into%20the%20adaptive%20deviation%20learning%20paradigm.%20We%20introduce%20a%20composite%20anomaly%20score%20that%20combines%20three%20complementary%20components%3A%20a%20deviation%20score%20capturing%20statistical%20irregularity%2C%20an%20entropy-based%20uncertainty%20score%20reflecting%20predictive%20inconsistency%2C%20and%20a%20segmentation-based%20score%20highlighting%20spatial%20abnormality.%20This%20unified%20scoring%20mechanism%20enables%20accurate%20detection%20and%20supports%20gradient-based%20localization%2C%20providing%20intuitive%20and%20explainable%20visual%20evidence%20of%20anomalous%20regions.%20Following%20the%20few-anomaly%20paradigm%2C%20we%20incorporate%20a%20small%20set%20of%20labeled%20anomalies%20during%20training%20while%20simultaneously%20mitigating%20the%20influence%20of%20contaminated%20samples%20through%20adaptive%20instance%20weighting.%20Extensive%20experiments%20on%20the%20MVTec%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20and%20achieves%20strong%20detection%20and%20localization%20performance%2C%20interpretability%2C%20and%20robustness%20under%20various%20levels%20of%20data%20contamination.&entry.1838667208=http%3A//arxiv.org/abs/2601.22913v2&entry.124074799=Read"},
{"title": "LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding", "author": "Gang Lin and Dongfang Li and Zhuoen Chen and Yukun Shi and Xuhui Chen and Baotian Hu and Min Zhang", "abstract": "The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.", "link": "http://arxiv.org/abs/2602.04541v1", "date": "2026-02-04", "relevancy": 2.1905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LycheeDecode%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Hybrid-Head%20Sparse%20Decoding&body=Title%3A%20LycheeDecode%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Hybrid-Head%20Sparse%20Decoding%0AAuthor%3A%20Gang%20Lin%20and%20Dongfang%20Li%20and%20Zhuoen%20Chen%20and%20Yukun%20Shi%20and%20Xuhui%20Chen%20and%20Baotian%20Hu%20and%20Min%20Zhang%0AAbstract%3A%20The%20proliferation%20of%20long-context%20large%20language%20models%20%28LLMs%29%20exposes%20a%20key%20bottleneck%3A%20the%20rapidly%20expanding%20key-value%20cache%20during%20decoding%2C%20which%20imposes%20heavy%20memory%20and%20latency%20costs.%20While%20recent%20approaches%20attempt%20to%20alleviate%20this%20by%20sharing%20a%20single%20set%20of%20crucial%20tokens%20across%20layers%2C%20such%20coarse-grained%20sharing%20undermines%20model%20performance%20by%20neglecting%20the%20functional%20diversity%20of%20attention%20heads.%20To%20address%20this%2C%20we%20propose%20LycheeDecode%2C%20an%20efficient%20decoding%20method%20centered%20on%20a%20fine-grained%20hybrid-head%20attention%20mechanism%20that%20employs%20a%20hardware-efficient%20top-k%20selection%20strategy.%20Specifically%2C%20the%20novel%20HardKuma-based%20mechanism%20partitions%20attention%20heads%20into%20a%20small%20subset%20of%20retrieval%20heads%20that%20dynamically%20identify%20crucial%20tokens%20and%20a%20majority%20of%20sparse%20heads%20that%20reuse%20them%20for%20efficient%20computation.%20Through%20extensive%20experiments%20on%20leading%20models%20like%20Llama3%20and%20Qwen3%20across%20diverse%20benchmarks%20for%20long-context%20understanding%20%28e.g.%2C%20LongBench%2C%20RULER%29%20and%20complex%20reasoning%20%28e.g.%2C%20AIME24%2C%20OlympiadBench%29%2C%20we%20demonstrate%20that%20LycheeDecode%20achieves%20generative%20quality%20comparable%20to%2C%20and%20at%20times%20surpassing%20even%20the%20full-attention%20baseline.%20Crucially%2C%20this%20is%20accomplished%20with%20up%20to%20a%202.7x%20speedup%20at%20a%20128K%20context%20length.%20By%20preserving%20the%20functional%20diversity%20of%20attention%20heads%2C%20our%20fine-grained%20strategy%20overcomes%20the%20performance%20bottlenecks%20of%20existing%20methods%2C%20providing%20a%20powerful%20and%20validated%20pathway%20to%20both%20efficient%20and%20high-quality%20long-context%20LLM%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLycheeDecode%253A%2520Accelerating%2520Long-Context%2520LLM%2520Inference%2520via%2520Hybrid-Head%2520Sparse%2520Decoding%26entry.906535625%3DGang%2520Lin%2520and%2520Dongfang%2520Li%2520and%2520Zhuoen%2520Chen%2520and%2520Yukun%2520Shi%2520and%2520Xuhui%2520Chen%2520and%2520Baotian%2520Hu%2520and%2520Min%2520Zhang%26entry.1292438233%3DThe%2520proliferation%2520of%2520long-context%2520large%2520language%2520models%2520%2528LLMs%2529%2520exposes%2520a%2520key%2520bottleneck%253A%2520the%2520rapidly%2520expanding%2520key-value%2520cache%2520during%2520decoding%252C%2520which%2520imposes%2520heavy%2520memory%2520and%2520latency%2520costs.%2520While%2520recent%2520approaches%2520attempt%2520to%2520alleviate%2520this%2520by%2520sharing%2520a%2520single%2520set%2520of%2520crucial%2520tokens%2520across%2520layers%252C%2520such%2520coarse-grained%2520sharing%2520undermines%2520model%2520performance%2520by%2520neglecting%2520the%2520functional%2520diversity%2520of%2520attention%2520heads.%2520To%2520address%2520this%252C%2520we%2520propose%2520LycheeDecode%252C%2520an%2520efficient%2520decoding%2520method%2520centered%2520on%2520a%2520fine-grained%2520hybrid-head%2520attention%2520mechanism%2520that%2520employs%2520a%2520hardware-efficient%2520top-k%2520selection%2520strategy.%2520Specifically%252C%2520the%2520novel%2520HardKuma-based%2520mechanism%2520partitions%2520attention%2520heads%2520into%2520a%2520small%2520subset%2520of%2520retrieval%2520heads%2520that%2520dynamically%2520identify%2520crucial%2520tokens%2520and%2520a%2520majority%2520of%2520sparse%2520heads%2520that%2520reuse%2520them%2520for%2520efficient%2520computation.%2520Through%2520extensive%2520experiments%2520on%2520leading%2520models%2520like%2520Llama3%2520and%2520Qwen3%2520across%2520diverse%2520benchmarks%2520for%2520long-context%2520understanding%2520%2528e.g.%252C%2520LongBench%252C%2520RULER%2529%2520and%2520complex%2520reasoning%2520%2528e.g.%252C%2520AIME24%252C%2520OlympiadBench%2529%252C%2520we%2520demonstrate%2520that%2520LycheeDecode%2520achieves%2520generative%2520quality%2520comparable%2520to%252C%2520and%2520at%2520times%2520surpassing%2520even%2520the%2520full-attention%2520baseline.%2520Crucially%252C%2520this%2520is%2520accomplished%2520with%2520up%2520to%2520a%25202.7x%2520speedup%2520at%2520a%2520128K%2520context%2520length.%2520By%2520preserving%2520the%2520functional%2520diversity%2520of%2520attention%2520heads%252C%2520our%2520fine-grained%2520strategy%2520overcomes%2520the%2520performance%2520bottlenecks%2520of%2520existing%2520methods%252C%2520providing%2520a%2520powerful%2520and%2520validated%2520pathway%2520to%2520both%2520efficient%2520and%2520high-quality%2520long-context%2520LLM%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LycheeDecode%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Hybrid-Head%20Sparse%20Decoding&entry.906535625=Gang%20Lin%20and%20Dongfang%20Li%20and%20Zhuoen%20Chen%20and%20Yukun%20Shi%20and%20Xuhui%20Chen%20and%20Baotian%20Hu%20and%20Min%20Zhang&entry.1292438233=The%20proliferation%20of%20long-context%20large%20language%20models%20%28LLMs%29%20exposes%20a%20key%20bottleneck%3A%20the%20rapidly%20expanding%20key-value%20cache%20during%20decoding%2C%20which%20imposes%20heavy%20memory%20and%20latency%20costs.%20While%20recent%20approaches%20attempt%20to%20alleviate%20this%20by%20sharing%20a%20single%20set%20of%20crucial%20tokens%20across%20layers%2C%20such%20coarse-grained%20sharing%20undermines%20model%20performance%20by%20neglecting%20the%20functional%20diversity%20of%20attention%20heads.%20To%20address%20this%2C%20we%20propose%20LycheeDecode%2C%20an%20efficient%20decoding%20method%20centered%20on%20a%20fine-grained%20hybrid-head%20attention%20mechanism%20that%20employs%20a%20hardware-efficient%20top-k%20selection%20strategy.%20Specifically%2C%20the%20novel%20HardKuma-based%20mechanism%20partitions%20attention%20heads%20into%20a%20small%20subset%20of%20retrieval%20heads%20that%20dynamically%20identify%20crucial%20tokens%20and%20a%20majority%20of%20sparse%20heads%20that%20reuse%20them%20for%20efficient%20computation.%20Through%20extensive%20experiments%20on%20leading%20models%20like%20Llama3%20and%20Qwen3%20across%20diverse%20benchmarks%20for%20long-context%20understanding%20%28e.g.%2C%20LongBench%2C%20RULER%29%20and%20complex%20reasoning%20%28e.g.%2C%20AIME24%2C%20OlympiadBench%29%2C%20we%20demonstrate%20that%20LycheeDecode%20achieves%20generative%20quality%20comparable%20to%2C%20and%20at%20times%20surpassing%20even%20the%20full-attention%20baseline.%20Crucially%2C%20this%20is%20accomplished%20with%20up%20to%20a%202.7x%20speedup%20at%20a%20128K%20context%20length.%20By%20preserving%20the%20functional%20diversity%20of%20attention%20heads%2C%20our%20fine-grained%20strategy%20overcomes%20the%20performance%20bottlenecks%20of%20existing%20methods%2C%20providing%20a%20powerful%20and%20validated%20pathway%20to%20both%20efficient%20and%20high-quality%20long-context%20LLM%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2602.04541v1&entry.124074799=Read"},
{"title": "Quasi-Medial Distance Field (Q-MDF): A Robust Method for Approximating and Discretizing Neural Medial Axes", "author": "Jiayi Kong and Chen Zong and Jun Luo and Shiqing Xin and Fei Hou and Hanqing Jiang and Chen Qian and Ying He", "abstract": "The medial axis, a lower-dimensional descriptor that captures the extrinsic structure of a shape, plays an important role in digital geometry processing. Despite its importance, computing the medial axis transform robustly from diverse inputs, especially point clouds with defects, remains a challenging problem. In this paper, we propose a new implicit method that deviates from traditional explicit medial axis computation. Our key technical insight is that the difference between the signed distance field (SDF) and the medial field (MF) of a solid shape relates to the unsigned distance field (UDF) of the shape's medial axis. This observation allows us to formulate medial axis extraction as an implicit reconstruction problem. By employing a modified double covering strategy, we recover the medial axis as the zero level-set of the UDF. Extensive experiments demonstrate that our method achieves higher accuracy and robustness in learning compact medial axis transforms from challenging meshes and point clouds, outperforming existing approaches.", "link": "http://arxiv.org/abs/2410.17774v2", "date": "2026-02-04", "relevancy": 2.174, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%20and%20Discretizing%20Neural%20Medial%20Axes&body=Title%3A%20Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%20and%20Discretizing%20Neural%20Medial%20Axes%0AAuthor%3A%20Jiayi%20Kong%20and%20Chen%20Zong%20and%20Jun%20Luo%20and%20Shiqing%20Xin%20and%20Fei%20Hou%20and%20Hanqing%20Jiang%20and%20Chen%20Qian%20and%20Ying%20He%0AAbstract%3A%20The%20medial%20axis%2C%20a%20lower-dimensional%20descriptor%20that%20captures%20the%20extrinsic%20structure%20of%20a%20shape%2C%20plays%20an%20important%20role%20in%20digital%20geometry%20processing.%20Despite%20its%20importance%2C%20computing%20the%20medial%20axis%20transform%20robustly%20from%20diverse%20inputs%2C%20especially%20point%20clouds%20with%20defects%2C%20remains%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20new%20implicit%20method%20that%20deviates%20from%20traditional%20explicit%20medial%20axis%20computation.%20Our%20key%20technical%20insight%20is%20that%20the%20difference%20between%20the%20signed%20distance%20field%20%28SDF%29%20and%20the%20medial%20field%20%28MF%29%20of%20a%20solid%20shape%20relates%20to%20the%20unsigned%20distance%20field%20%28UDF%29%20of%20the%20shape%27s%20medial%20axis.%20This%20observation%20allows%20us%20to%20formulate%20medial%20axis%20extraction%20as%20an%20implicit%20reconstruction%20problem.%20By%20employing%20a%20modified%20double%20covering%20strategy%2C%20we%20recover%20the%20medial%20axis%20as%20the%20zero%20level-set%20of%20the%20UDF.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20higher%20accuracy%20and%20robustness%20in%20learning%20compact%20medial%20axis%20transforms%20from%20challenging%20meshes%20and%20point%20clouds%2C%20outperforming%20existing%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2410.17774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuasi-Medial%2520Distance%2520Field%2520%2528Q-MDF%2529%253A%2520A%2520Robust%2520Method%2520for%2520Approximating%2520and%2520Discretizing%2520Neural%2520Medial%2520Axes%26entry.906535625%3DJiayi%2520Kong%2520and%2520Chen%2520Zong%2520and%2520Jun%2520Luo%2520and%2520Shiqing%2520Xin%2520and%2520Fei%2520Hou%2520and%2520Hanqing%2520Jiang%2520and%2520Chen%2520Qian%2520and%2520Ying%2520He%26entry.1292438233%3DThe%2520medial%2520axis%252C%2520a%2520lower-dimensional%2520descriptor%2520that%2520captures%2520the%2520extrinsic%2520structure%2520of%2520a%2520shape%252C%2520plays%2520an%2520important%2520role%2520in%2520digital%2520geometry%2520processing.%2520Despite%2520its%2520importance%252C%2520computing%2520the%2520medial%2520axis%2520transform%2520robustly%2520from%2520diverse%2520inputs%252C%2520especially%2520point%2520clouds%2520with%2520defects%252C%2520remains%2520a%2520challenging%2520problem.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520implicit%2520method%2520that%2520deviates%2520from%2520traditional%2520explicit%2520medial%2520axis%2520computation.%2520Our%2520key%2520technical%2520insight%2520is%2520that%2520the%2520difference%2520between%2520the%2520signed%2520distance%2520field%2520%2528SDF%2529%2520and%2520the%2520medial%2520field%2520%2528MF%2529%2520of%2520a%2520solid%2520shape%2520relates%2520to%2520the%2520unsigned%2520distance%2520field%2520%2528UDF%2529%2520of%2520the%2520shape%2527s%2520medial%2520axis.%2520This%2520observation%2520allows%2520us%2520to%2520formulate%2520medial%2520axis%2520extraction%2520as%2520an%2520implicit%2520reconstruction%2520problem.%2520By%2520employing%2520a%2520modified%2520double%2520covering%2520strategy%252C%2520we%2520recover%2520the%2520medial%2520axis%2520as%2520the%2520zero%2520level-set%2520of%2520the%2520UDF.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520higher%2520accuracy%2520and%2520robustness%2520in%2520learning%2520compact%2520medial%2520axis%2520transforms%2520from%2520challenging%2520meshes%2520and%2520point%2520clouds%252C%2520outperforming%2520existing%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%20and%20Discretizing%20Neural%20Medial%20Axes&entry.906535625=Jiayi%20Kong%20and%20Chen%20Zong%20and%20Jun%20Luo%20and%20Shiqing%20Xin%20and%20Fei%20Hou%20and%20Hanqing%20Jiang%20and%20Chen%20Qian%20and%20Ying%20He&entry.1292438233=The%20medial%20axis%2C%20a%20lower-dimensional%20descriptor%20that%20captures%20the%20extrinsic%20structure%20of%20a%20shape%2C%20plays%20an%20important%20role%20in%20digital%20geometry%20processing.%20Despite%20its%20importance%2C%20computing%20the%20medial%20axis%20transform%20robustly%20from%20diverse%20inputs%2C%20especially%20point%20clouds%20with%20defects%2C%20remains%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20new%20implicit%20method%20that%20deviates%20from%20traditional%20explicit%20medial%20axis%20computation.%20Our%20key%20technical%20insight%20is%20that%20the%20difference%20between%20the%20signed%20distance%20field%20%28SDF%29%20and%20the%20medial%20field%20%28MF%29%20of%20a%20solid%20shape%20relates%20to%20the%20unsigned%20distance%20field%20%28UDF%29%20of%20the%20shape%27s%20medial%20axis.%20This%20observation%20allows%20us%20to%20formulate%20medial%20axis%20extraction%20as%20an%20implicit%20reconstruction%20problem.%20By%20employing%20a%20modified%20double%20covering%20strategy%2C%20we%20recover%20the%20medial%20axis%20as%20the%20zero%20level-set%20of%20the%20UDF.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20higher%20accuracy%20and%20robustness%20in%20learning%20compact%20medial%20axis%20transforms%20from%20challenging%20meshes%20and%20point%20clouds%2C%20outperforming%20existing%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2410.17774v2&entry.124074799=Read"},
{"title": "LLM-Empowered Cooperative Content Caching in Vehicular Fog Caching-Assisted Platoon Networks", "author": "Bowen Tan and Qiong Wu and Pingyi Fan and Kezhi Wang and Nan Cheng and Wen Chen", "abstract": "This letter proposes a novel three-tier content caching architecture for Vehicular Fog Caching (VFC)-assisted platoon, where the VFC is formed by the vehicles driving near the platoon. The system strategically coordinates storage across local platoon vehicles, dynamic VFC clusters, and cloud server (CS) to minimize content retrieval latency. To efficiently manage distributed storage, we integrate large language models (LLMs) for real-time and intelligent caching decisions. The proposed approach leverages LLMs' ability to process heterogeneous information, including user profiles, historical data, content characteristics, and dynamic system states. Through a designed prompting framework encoding task objectives and caching constraints, the LLMs formulate caching as a decision-making task, and our hierarchical deterministic caching mapping strategy enables adaptive requests prediction and precise content placement across three tiers without frequent retraining. Simulation results demonstrate the advantages of our proposed caching scheme.", "link": "http://arxiv.org/abs/2602.04471v1", "date": "2026-02-04", "relevancy": 2.1594, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Empowered%20Cooperative%20Content%20Caching%20in%20Vehicular%20Fog%20Caching-Assisted%20Platoon%20Networks&body=Title%3A%20LLM-Empowered%20Cooperative%20Content%20Caching%20in%20Vehicular%20Fog%20Caching-Assisted%20Platoon%20Networks%0AAuthor%3A%20Bowen%20Tan%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang%20and%20Nan%20Cheng%20and%20Wen%20Chen%0AAbstract%3A%20This%20letter%20proposes%20a%20novel%20three-tier%20content%20caching%20architecture%20for%20Vehicular%20Fog%20Caching%20%28VFC%29-assisted%20platoon%2C%20where%20the%20VFC%20is%20formed%20by%20the%20vehicles%20driving%20near%20the%20platoon.%20The%20system%20strategically%20coordinates%20storage%20across%20local%20platoon%20vehicles%2C%20dynamic%20VFC%20clusters%2C%20and%20cloud%20server%20%28CS%29%20to%20minimize%20content%20retrieval%20latency.%20To%20efficiently%20manage%20distributed%20storage%2C%20we%20integrate%20large%20language%20models%20%28LLMs%29%20for%20real-time%20and%20intelligent%20caching%20decisions.%20The%20proposed%20approach%20leverages%20LLMs%27%20ability%20to%20process%20heterogeneous%20information%2C%20including%20user%20profiles%2C%20historical%20data%2C%20content%20characteristics%2C%20and%20dynamic%20system%20states.%20Through%20a%20designed%20prompting%20framework%20encoding%20task%20objectives%20and%20caching%20constraints%2C%20the%20LLMs%20formulate%20caching%20as%20a%20decision-making%20task%2C%20and%20our%20hierarchical%20deterministic%20caching%20mapping%20strategy%20enables%20adaptive%20requests%20prediction%20and%20precise%20content%20placement%20across%20three%20tiers%20without%20frequent%20retraining.%20Simulation%20results%20demonstrate%20the%20advantages%20of%20our%20proposed%20caching%20scheme.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Empowered%2520Cooperative%2520Content%2520Caching%2520in%2520Vehicular%2520Fog%2520Caching-Assisted%2520Platoon%2520Networks%26entry.906535625%3DBowen%2520Tan%2520and%2520Qiong%2520Wu%2520and%2520Pingyi%2520Fan%2520and%2520Kezhi%2520Wang%2520and%2520Nan%2520Cheng%2520and%2520Wen%2520Chen%26entry.1292438233%3DThis%2520letter%2520proposes%2520a%2520novel%2520three-tier%2520content%2520caching%2520architecture%2520for%2520Vehicular%2520Fog%2520Caching%2520%2528VFC%2529-assisted%2520platoon%252C%2520where%2520the%2520VFC%2520is%2520formed%2520by%2520the%2520vehicles%2520driving%2520near%2520the%2520platoon.%2520The%2520system%2520strategically%2520coordinates%2520storage%2520across%2520local%2520platoon%2520vehicles%252C%2520dynamic%2520VFC%2520clusters%252C%2520and%2520cloud%2520server%2520%2528CS%2529%2520to%2520minimize%2520content%2520retrieval%2520latency.%2520To%2520efficiently%2520manage%2520distributed%2520storage%252C%2520we%2520integrate%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520real-time%2520and%2520intelligent%2520caching%2520decisions.%2520The%2520proposed%2520approach%2520leverages%2520LLMs%2527%2520ability%2520to%2520process%2520heterogeneous%2520information%252C%2520including%2520user%2520profiles%252C%2520historical%2520data%252C%2520content%2520characteristics%252C%2520and%2520dynamic%2520system%2520states.%2520Through%2520a%2520designed%2520prompting%2520framework%2520encoding%2520task%2520objectives%2520and%2520caching%2520constraints%252C%2520the%2520LLMs%2520formulate%2520caching%2520as%2520a%2520decision-making%2520task%252C%2520and%2520our%2520hierarchical%2520deterministic%2520caching%2520mapping%2520strategy%2520enables%2520adaptive%2520requests%2520prediction%2520and%2520precise%2520content%2520placement%2520across%2520three%2520tiers%2520without%2520frequent%2520retraining.%2520Simulation%2520results%2520demonstrate%2520the%2520advantages%2520of%2520our%2520proposed%2520caching%2520scheme.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Empowered%20Cooperative%20Content%20Caching%20in%20Vehicular%20Fog%20Caching-Assisted%20Platoon%20Networks&entry.906535625=Bowen%20Tan%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang%20and%20Nan%20Cheng%20and%20Wen%20Chen&entry.1292438233=This%20letter%20proposes%20a%20novel%20three-tier%20content%20caching%20architecture%20for%20Vehicular%20Fog%20Caching%20%28VFC%29-assisted%20platoon%2C%20where%20the%20VFC%20is%20formed%20by%20the%20vehicles%20driving%20near%20the%20platoon.%20The%20system%20strategically%20coordinates%20storage%20across%20local%20platoon%20vehicles%2C%20dynamic%20VFC%20clusters%2C%20and%20cloud%20server%20%28CS%29%20to%20minimize%20content%20retrieval%20latency.%20To%20efficiently%20manage%20distributed%20storage%2C%20we%20integrate%20large%20language%20models%20%28LLMs%29%20for%20real-time%20and%20intelligent%20caching%20decisions.%20The%20proposed%20approach%20leverages%20LLMs%27%20ability%20to%20process%20heterogeneous%20information%2C%20including%20user%20profiles%2C%20historical%20data%2C%20content%20characteristics%2C%20and%20dynamic%20system%20states.%20Through%20a%20designed%20prompting%20framework%20encoding%20task%20objectives%20and%20caching%20constraints%2C%20the%20LLMs%20formulate%20caching%20as%20a%20decision-making%20task%2C%20and%20our%20hierarchical%20deterministic%20caching%20mapping%20strategy%20enables%20adaptive%20requests%20prediction%20and%20precise%20content%20placement%20across%20three%20tiers%20without%20frequent%20retraining.%20Simulation%20results%20demonstrate%20the%20advantages%20of%20our%20proposed%20caching%20scheme.&entry.1838667208=http%3A//arxiv.org/abs/2602.04471v1&entry.124074799=Read"},
{"title": "Robust inverse material design with physical guarantees using the Voigt-Reuss Net", "author": "Sanath Keshav and Felix Fritzen", "abstract": "We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the L\u00f6wner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\\!7.5\\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \\ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\\approx 1.7\\%$ and mean $\\approx 3.4\\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.", "link": "http://arxiv.org/abs/2511.11388v2", "date": "2026-02-04", "relevancy": 2.1519, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5648}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20inverse%20material%20design%20with%20physical%20guarantees%20using%20the%20Voigt-Reuss%20Net&body=Title%3A%20Robust%20inverse%20material%20design%20with%20physical%20guarantees%20using%20the%20Voigt-Reuss%20Net%0AAuthor%3A%20Sanath%20Keshav%20and%20Felix%20Fritzen%0AAbstract%3A%20We%20propose%20a%20spectrally%20normalized%20surrogate%20for%20forward%20and%20inverse%20mechanical%20homogenization%20with%20hard%20physical%20guarantees.%20Leveraging%20the%20Voigt-Reuss%20bounds%2C%20we%20factor%20their%20difference%20via%20a%20Cholesky-like%20operator%20and%20learn%20a%20dimensionless%2C%20symmetric%20positive%20semi-definite%20representation%20with%20eigenvalues%20in%20%24%5B0%2C1%5D%24%3B%20the%20inverse%20map%20returns%20symmetric%20positive-definite%20predictions%20that%20lie%20between%20the%20bounds%20in%20the%20L%C3%B6wner%20sense.%20In%203D%20linear%20elasticity%20on%20an%20open%20dataset%20of%20stochastic%20biphasic%20microstructures%2C%20a%20fully%20connected%20Voigt-Reuss%20net%20trained%20on%20%24%3E%5C%217.5%5Ctimes%2010%5E%7B5%7D%24%20FFT-based%20labels%20with%20236%20isotropy-invariant%20descriptors%20and%20three%20contrast%20parameters%20recovers%20the%20isotropic%20projection%20with%20near-perfect%20fidelity%20%28isotropy-related%20entries%3A%20%24R%5E2%20%5Cge%200.998%24%29%2C%20while%20anisotropy-revealing%20couplings%20are%20unidentifiable%20from%20%24SO%283%29%24-invariant%20inputs.%20Tensor-level%20relative%20Frobenius%20errors%20have%20median%20%24%5Capprox%201.7%5C%25%24%20and%20mean%20%24%5Capprox%203.4%5C%25%24%20across%20splits.%20For%202D%20plane%20strain%20on%20thresholded%20trigonometric%20microstructures%2C%20coupling%20spectral%20normalization%20with%20a%20differentiable%20renderer%20and%20a%20CNN%20yields%20%24R%5E2%3E0.99%24%20on%20all%20components%2C%20subpercent%20normalized%20losses%2C%20accurate%20tracking%20of%20percolation-induced%20eigenvalue%20jumps%2C%20and%20robust%20generalization%20to%20out-of-distribution%20images.%20Treating%20the%20parametric%20microstructure%20as%20design%20variables%2C%20batched%20first-order%20optimization%20with%20a%20single%20surrogate%20matches%20target%20tensors%20within%20a%20few%20percent%20and%20returns%20diverse%20near-optimal%20designs.%20Overall%2C%20the%20Voigt-Reuss%20net%20unifies%20accurate%2C%20physically%20admissible%20forward%20prediction%20with%20large-batch%2C%20constraint-consistent%20inverse%20design%2C%20and%20is%20generic%20to%20elliptic%20operators%20and%20coupled-physics%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520inverse%2520material%2520design%2520with%2520physical%2520guarantees%2520using%2520the%2520Voigt-Reuss%2520Net%26entry.906535625%3DSanath%2520Keshav%2520and%2520Felix%2520Fritzen%26entry.1292438233%3DWe%2520propose%2520a%2520spectrally%2520normalized%2520surrogate%2520for%2520forward%2520and%2520inverse%2520mechanical%2520homogenization%2520with%2520hard%2520physical%2520guarantees.%2520Leveraging%2520the%2520Voigt-Reuss%2520bounds%252C%2520we%2520factor%2520their%2520difference%2520via%2520a%2520Cholesky-like%2520operator%2520and%2520learn%2520a%2520dimensionless%252C%2520symmetric%2520positive%2520semi-definite%2520representation%2520with%2520eigenvalues%2520in%2520%2524%255B0%252C1%255D%2524%253B%2520the%2520inverse%2520map%2520returns%2520symmetric%2520positive-definite%2520predictions%2520that%2520lie%2520between%2520the%2520bounds%2520in%2520the%2520L%25C3%25B6wner%2520sense.%2520In%25203D%2520linear%2520elasticity%2520on%2520an%2520open%2520dataset%2520of%2520stochastic%2520biphasic%2520microstructures%252C%2520a%2520fully%2520connected%2520Voigt-Reuss%2520net%2520trained%2520on%2520%2524%253E%255C%25217.5%255Ctimes%252010%255E%257B5%257D%2524%2520FFT-based%2520labels%2520with%2520236%2520isotropy-invariant%2520descriptors%2520and%2520three%2520contrast%2520parameters%2520recovers%2520the%2520isotropic%2520projection%2520with%2520near-perfect%2520fidelity%2520%2528isotropy-related%2520entries%253A%2520%2524R%255E2%2520%255Cge%25200.998%2524%2529%252C%2520while%2520anisotropy-revealing%2520couplings%2520are%2520unidentifiable%2520from%2520%2524SO%25283%2529%2524-invariant%2520inputs.%2520Tensor-level%2520relative%2520Frobenius%2520errors%2520have%2520median%2520%2524%255Capprox%25201.7%255C%2525%2524%2520and%2520mean%2520%2524%255Capprox%25203.4%255C%2525%2524%2520across%2520splits.%2520For%25202D%2520plane%2520strain%2520on%2520thresholded%2520trigonometric%2520microstructures%252C%2520coupling%2520spectral%2520normalization%2520with%2520a%2520differentiable%2520renderer%2520and%2520a%2520CNN%2520yields%2520%2524R%255E2%253E0.99%2524%2520on%2520all%2520components%252C%2520subpercent%2520normalized%2520losses%252C%2520accurate%2520tracking%2520of%2520percolation-induced%2520eigenvalue%2520jumps%252C%2520and%2520robust%2520generalization%2520to%2520out-of-distribution%2520images.%2520Treating%2520the%2520parametric%2520microstructure%2520as%2520design%2520variables%252C%2520batched%2520first-order%2520optimization%2520with%2520a%2520single%2520surrogate%2520matches%2520target%2520tensors%2520within%2520a%2520few%2520percent%2520and%2520returns%2520diverse%2520near-optimal%2520designs.%2520Overall%252C%2520the%2520Voigt-Reuss%2520net%2520unifies%2520accurate%252C%2520physically%2520admissible%2520forward%2520prediction%2520with%2520large-batch%252C%2520constraint-consistent%2520inverse%2520design%252C%2520and%2520is%2520generic%2520to%2520elliptic%2520operators%2520and%2520coupled-physics%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20inverse%20material%20design%20with%20physical%20guarantees%20using%20the%20Voigt-Reuss%20Net&entry.906535625=Sanath%20Keshav%20and%20Felix%20Fritzen&entry.1292438233=We%20propose%20a%20spectrally%20normalized%20surrogate%20for%20forward%20and%20inverse%20mechanical%20homogenization%20with%20hard%20physical%20guarantees.%20Leveraging%20the%20Voigt-Reuss%20bounds%2C%20we%20factor%20their%20difference%20via%20a%20Cholesky-like%20operator%20and%20learn%20a%20dimensionless%2C%20symmetric%20positive%20semi-definite%20representation%20with%20eigenvalues%20in%20%24%5B0%2C1%5D%24%3B%20the%20inverse%20map%20returns%20symmetric%20positive-definite%20predictions%20that%20lie%20between%20the%20bounds%20in%20the%20L%C3%B6wner%20sense.%20In%203D%20linear%20elasticity%20on%20an%20open%20dataset%20of%20stochastic%20biphasic%20microstructures%2C%20a%20fully%20connected%20Voigt-Reuss%20net%20trained%20on%20%24%3E%5C%217.5%5Ctimes%2010%5E%7B5%7D%24%20FFT-based%20labels%20with%20236%20isotropy-invariant%20descriptors%20and%20three%20contrast%20parameters%20recovers%20the%20isotropic%20projection%20with%20near-perfect%20fidelity%20%28isotropy-related%20entries%3A%20%24R%5E2%20%5Cge%200.998%24%29%2C%20while%20anisotropy-revealing%20couplings%20are%20unidentifiable%20from%20%24SO%283%29%24-invariant%20inputs.%20Tensor-level%20relative%20Frobenius%20errors%20have%20median%20%24%5Capprox%201.7%5C%25%24%20and%20mean%20%24%5Capprox%203.4%5C%25%24%20across%20splits.%20For%202D%20plane%20strain%20on%20thresholded%20trigonometric%20microstructures%2C%20coupling%20spectral%20normalization%20with%20a%20differentiable%20renderer%20and%20a%20CNN%20yields%20%24R%5E2%3E0.99%24%20on%20all%20components%2C%20subpercent%20normalized%20losses%2C%20accurate%20tracking%20of%20percolation-induced%20eigenvalue%20jumps%2C%20and%20robust%20generalization%20to%20out-of-distribution%20images.%20Treating%20the%20parametric%20microstructure%20as%20design%20variables%2C%20batched%20first-order%20optimization%20with%20a%20single%20surrogate%20matches%20target%20tensors%20within%20a%20few%20percent%20and%20returns%20diverse%20near-optimal%20designs.%20Overall%2C%20the%20Voigt-Reuss%20net%20unifies%20accurate%2C%20physically%20admissible%20forward%20prediction%20with%20large-batch%2C%20constraint-consistent%20inverse%20design%2C%20and%20is%20generic%20to%20elliptic%20operators%20and%20coupled-physics%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.11388v2&entry.124074799=Read"},
{"title": "Mixed-Density Diffuser: Efficient Planning with Non-Uniform Temporal Resolution", "author": "Crimson Stambaugh and Rajesh P. N. Rao", "abstract": "Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional memory or computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a planning horizon and that certain parts of a predicted trajectory should be more densely generated. We propose Mixed-Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. We show that MDD surpasses the SOTA Diffusion Veteran (DV) framework across the Maze2D, Franka Kitchen, and Antmaze Datasets for Deep Data-Driven Reinforcement Learning (D4RL) task domains, achieving a new SOTA on the D4RL benchmark.", "link": "http://arxiv.org/abs/2510.23026v4", "date": "2026-02-04", "relevancy": 2.1467, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5824}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5339}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed-Density%20Diffuser%3A%20Efficient%20Planning%20with%20Non-Uniform%20Temporal%20Resolution&body=Title%3A%20Mixed-Density%20Diffuser%3A%20Efficient%20Planning%20with%20Non-Uniform%20Temporal%20Resolution%0AAuthor%3A%20Crimson%20Stambaugh%20and%20Rajesh%20P.%20N.%20Rao%0AAbstract%3A%20Recent%20studies%20demonstrate%20that%20diffusion%20planners%20benefit%20from%20sparse-step%20planning%20over%20single-step%20planning.%20Training%20models%20to%20skip%20steps%20in%20their%20trajectories%20helps%20capture%20long-term%20dependencies%20without%20additional%20memory%20or%20computational%20cost.%20However%2C%20predicting%20excessively%20sparse%20plans%20degrades%20performance.%20We%20hypothesize%20this%20temporal%20density%20threshold%20is%20non-uniform%20across%20a%20planning%20horizon%20and%20that%20certain%20parts%20of%20a%20predicted%20trajectory%20should%20be%20more%20densely%20generated.%20We%20propose%20Mixed-Density%20Diffuser%20%28MDD%29%2C%20a%20diffusion%20planner%20where%20the%20densities%20throughout%20the%20horizon%20are%20tunable%20hyperparameters.%20We%20show%20that%20MDD%20surpasses%20the%20SOTA%20Diffusion%20Veteran%20%28DV%29%20framework%20across%20the%20Maze2D%2C%20Franka%20Kitchen%2C%20and%20Antmaze%20Datasets%20for%20Deep%20Data-Driven%20Reinforcement%20Learning%20%28D4RL%29%20task%20domains%2C%20achieving%20a%20new%20SOTA%20on%20the%20D4RL%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23026v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed-Density%2520Diffuser%253A%2520Efficient%2520Planning%2520with%2520Non-Uniform%2520Temporal%2520Resolution%26entry.906535625%3DCrimson%2520Stambaugh%2520and%2520Rajesh%2520P.%2520N.%2520Rao%26entry.1292438233%3DRecent%2520studies%2520demonstrate%2520that%2520diffusion%2520planners%2520benefit%2520from%2520sparse-step%2520planning%2520over%2520single-step%2520planning.%2520Training%2520models%2520to%2520skip%2520steps%2520in%2520their%2520trajectories%2520helps%2520capture%2520long-term%2520dependencies%2520without%2520additional%2520memory%2520or%2520computational%2520cost.%2520However%252C%2520predicting%2520excessively%2520sparse%2520plans%2520degrades%2520performance.%2520We%2520hypothesize%2520this%2520temporal%2520density%2520threshold%2520is%2520non-uniform%2520across%2520a%2520planning%2520horizon%2520and%2520that%2520certain%2520parts%2520of%2520a%2520predicted%2520trajectory%2520should%2520be%2520more%2520densely%2520generated.%2520We%2520propose%2520Mixed-Density%2520Diffuser%2520%2528MDD%2529%252C%2520a%2520diffusion%2520planner%2520where%2520the%2520densities%2520throughout%2520the%2520horizon%2520are%2520tunable%2520hyperparameters.%2520We%2520show%2520that%2520MDD%2520surpasses%2520the%2520SOTA%2520Diffusion%2520Veteran%2520%2528DV%2529%2520framework%2520across%2520the%2520Maze2D%252C%2520Franka%2520Kitchen%252C%2520and%2520Antmaze%2520Datasets%2520for%2520Deep%2520Data-Driven%2520Reinforcement%2520Learning%2520%2528D4RL%2529%2520task%2520domains%252C%2520achieving%2520a%2520new%2520SOTA%2520on%2520the%2520D4RL%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23026v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed-Density%20Diffuser%3A%20Efficient%20Planning%20with%20Non-Uniform%20Temporal%20Resolution&entry.906535625=Crimson%20Stambaugh%20and%20Rajesh%20P.%20N.%20Rao&entry.1292438233=Recent%20studies%20demonstrate%20that%20diffusion%20planners%20benefit%20from%20sparse-step%20planning%20over%20single-step%20planning.%20Training%20models%20to%20skip%20steps%20in%20their%20trajectories%20helps%20capture%20long-term%20dependencies%20without%20additional%20memory%20or%20computational%20cost.%20However%2C%20predicting%20excessively%20sparse%20plans%20degrades%20performance.%20We%20hypothesize%20this%20temporal%20density%20threshold%20is%20non-uniform%20across%20a%20planning%20horizon%20and%20that%20certain%20parts%20of%20a%20predicted%20trajectory%20should%20be%20more%20densely%20generated.%20We%20propose%20Mixed-Density%20Diffuser%20%28MDD%29%2C%20a%20diffusion%20planner%20where%20the%20densities%20throughout%20the%20horizon%20are%20tunable%20hyperparameters.%20We%20show%20that%20MDD%20surpasses%20the%20SOTA%20Diffusion%20Veteran%20%28DV%29%20framework%20across%20the%20Maze2D%2C%20Franka%20Kitchen%2C%20and%20Antmaze%20Datasets%20for%20Deep%20Data-Driven%20Reinforcement%20Learning%20%28D4RL%29%20task%20domains%2C%20achieving%20a%20new%20SOTA%20on%20the%20D4RL%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2510.23026v4&entry.124074799=Read"},
{"title": "Combining Residual U-Net and Data Augmentation for Dense Temporal Segmentation of Spike Wave Discharges in Single-Channel EEG", "author": "Saurav Sengupta and Scott Kilianski and Suchetha Sharma and Sakina Lashkeri and Ashley McHugh and Mark Beenhakker and Donald E. Brown", "abstract": "Manual annotation of spike-wave discharges (SWDs), the electrographic hallmark of absence seizures, is labor-intensive for long-term electroencephalography (EEG) monitoring studies. While machine learning approaches show promise for automated detection, they often struggle with cross-subject generalization due to high inter-individual variability in seizure morphology and signal characteristics. In this study we compare the performance of 15 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs and find that a 1D U-Net performs the best. We then improve its performance by employing residual connections and data augmentation strategies combining amplitude scaling, Gaussian noise injection, and signal inversion during training to enhance cross-subject generalization. We also compare our method, named AugUNet1D, to a recently published time- and frequency-based algorithmic approach called \"Twin Peaks\" and show that AugUNet1D performs better on our dataset. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for other users.", "link": "http://arxiv.org/abs/2601.00459v2", "date": "2026-02-04", "relevancy": 2.1248, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.549}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5217}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Residual%20U-Net%20and%20Data%20Augmentation%20for%20Dense%20Temporal%20Segmentation%20of%20Spike%20Wave%20Discharges%20in%20Single-Channel%20EEG&body=Title%3A%20Combining%20Residual%20U-Net%20and%20Data%20Augmentation%20for%20Dense%20Temporal%20Segmentation%20of%20Spike%20Wave%20Discharges%20in%20Single-Channel%20EEG%0AAuthor%3A%20Saurav%20Sengupta%20and%20Scott%20Kilianski%20and%20Suchetha%20Sharma%20and%20Sakina%20Lashkeri%20and%20Ashley%20McHugh%20and%20Mark%20Beenhakker%20and%20Donald%20E.%20Brown%0AAbstract%3A%20Manual%20annotation%20of%20spike-wave%20discharges%20%28SWDs%29%2C%20the%20electrographic%20hallmark%20of%20absence%20seizures%2C%20is%20labor-intensive%20for%20long-term%20electroencephalography%20%28EEG%29%20monitoring%20studies.%20While%20machine%20learning%20approaches%20show%20promise%20for%20automated%20detection%2C%20they%20often%20struggle%20with%20cross-subject%20generalization%20due%20to%20high%20inter-individual%20variability%20in%20seizure%20morphology%20and%20signal%20characteristics.%20In%20this%20study%20we%20compare%20the%20performance%20of%2015%20machine%20learning%20classifiers%20on%20our%20own%20manually%20annotated%20dataset%20of%20961%20hours%20of%20EEG%20recordings%20from%20C3H/HeJ%20mice%2C%20including%2022%2C637%20labeled%20SWDs%20and%20find%20that%20a%201D%20U-Net%20performs%20the%20best.%20We%20then%20improve%20its%20performance%20by%20employing%20residual%20connections%20and%20data%20augmentation%20strategies%20combining%20amplitude%20scaling%2C%20Gaussian%20noise%20injection%2C%20and%20signal%20inversion%20during%20training%20to%20enhance%20cross-subject%20generalization.%20We%20also%20compare%20our%20method%2C%20named%20AugUNet1D%2C%20to%20a%20recently%20published%20time-%20and%20frequency-based%20algorithmic%20approach%20called%20%22Twin%20Peaks%22%20and%20show%20that%20AugUNet1D%20performs%20better%20on%20our%20dataset.%20AugUNet1D%2C%20pretrained%20on%20our%20manually%20annotated%20data%20or%20untrained%2C%20is%20made%20public%20for%20other%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Residual%2520U-Net%2520and%2520Data%2520Augmentation%2520for%2520Dense%2520Temporal%2520Segmentation%2520of%2520Spike%2520Wave%2520Discharges%2520in%2520Single-Channel%2520EEG%26entry.906535625%3DSaurav%2520Sengupta%2520and%2520Scott%2520Kilianski%2520and%2520Suchetha%2520Sharma%2520and%2520Sakina%2520Lashkeri%2520and%2520Ashley%2520McHugh%2520and%2520Mark%2520Beenhakker%2520and%2520Donald%2520E.%2520Brown%26entry.1292438233%3DManual%2520annotation%2520of%2520spike-wave%2520discharges%2520%2528SWDs%2529%252C%2520the%2520electrographic%2520hallmark%2520of%2520absence%2520seizures%252C%2520is%2520labor-intensive%2520for%2520long-term%2520electroencephalography%2520%2528EEG%2529%2520monitoring%2520studies.%2520While%2520machine%2520learning%2520approaches%2520show%2520promise%2520for%2520automated%2520detection%252C%2520they%2520often%2520struggle%2520with%2520cross-subject%2520generalization%2520due%2520to%2520high%2520inter-individual%2520variability%2520in%2520seizure%2520morphology%2520and%2520signal%2520characteristics.%2520In%2520this%2520study%2520we%2520compare%2520the%2520performance%2520of%252015%2520machine%2520learning%2520classifiers%2520on%2520our%2520own%2520manually%2520annotated%2520dataset%2520of%2520961%2520hours%2520of%2520EEG%2520recordings%2520from%2520C3H/HeJ%2520mice%252C%2520including%252022%252C637%2520labeled%2520SWDs%2520and%2520find%2520that%2520a%25201D%2520U-Net%2520performs%2520the%2520best.%2520We%2520then%2520improve%2520its%2520performance%2520by%2520employing%2520residual%2520connections%2520and%2520data%2520augmentation%2520strategies%2520combining%2520amplitude%2520scaling%252C%2520Gaussian%2520noise%2520injection%252C%2520and%2520signal%2520inversion%2520during%2520training%2520to%2520enhance%2520cross-subject%2520generalization.%2520We%2520also%2520compare%2520our%2520method%252C%2520named%2520AugUNet1D%252C%2520to%2520a%2520recently%2520published%2520time-%2520and%2520frequency-based%2520algorithmic%2520approach%2520called%2520%2522Twin%2520Peaks%2522%2520and%2520show%2520that%2520AugUNet1D%2520performs%2520better%2520on%2520our%2520dataset.%2520AugUNet1D%252C%2520pretrained%2520on%2520our%2520manually%2520annotated%2520data%2520or%2520untrained%252C%2520is%2520made%2520public%2520for%2520other%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Residual%20U-Net%20and%20Data%20Augmentation%20for%20Dense%20Temporal%20Segmentation%20of%20Spike%20Wave%20Discharges%20in%20Single-Channel%20EEG&entry.906535625=Saurav%20Sengupta%20and%20Scott%20Kilianski%20and%20Suchetha%20Sharma%20and%20Sakina%20Lashkeri%20and%20Ashley%20McHugh%20and%20Mark%20Beenhakker%20and%20Donald%20E.%20Brown&entry.1292438233=Manual%20annotation%20of%20spike-wave%20discharges%20%28SWDs%29%2C%20the%20electrographic%20hallmark%20of%20absence%20seizures%2C%20is%20labor-intensive%20for%20long-term%20electroencephalography%20%28EEG%29%20monitoring%20studies.%20While%20machine%20learning%20approaches%20show%20promise%20for%20automated%20detection%2C%20they%20often%20struggle%20with%20cross-subject%20generalization%20due%20to%20high%20inter-individual%20variability%20in%20seizure%20morphology%20and%20signal%20characteristics.%20In%20this%20study%20we%20compare%20the%20performance%20of%2015%20machine%20learning%20classifiers%20on%20our%20own%20manually%20annotated%20dataset%20of%20961%20hours%20of%20EEG%20recordings%20from%20C3H/HeJ%20mice%2C%20including%2022%2C637%20labeled%20SWDs%20and%20find%20that%20a%201D%20U-Net%20performs%20the%20best.%20We%20then%20improve%20its%20performance%20by%20employing%20residual%20connections%20and%20data%20augmentation%20strategies%20combining%20amplitude%20scaling%2C%20Gaussian%20noise%20injection%2C%20and%20signal%20inversion%20during%20training%20to%20enhance%20cross-subject%20generalization.%20We%20also%20compare%20our%20method%2C%20named%20AugUNet1D%2C%20to%20a%20recently%20published%20time-%20and%20frequency-based%20algorithmic%20approach%20called%20%22Twin%20Peaks%22%20and%20show%20that%20AugUNet1D%20performs%20better%20on%20our%20dataset.%20AugUNet1D%2C%20pretrained%20on%20our%20manually%20annotated%20data%20or%20untrained%2C%20is%20made%20public%20for%20other%20users.&entry.1838667208=http%3A//arxiv.org/abs/2601.00459v2&entry.124074799=Read"},
{"title": "SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking", "author": "Muhammad Taha Mukhtar and Syed Musa Ali Kazmi and Khola Naseem and Muhammad Ali Chattha and Andreas Dengel and Sheraz Ahmed and Muhammad Naseer Bajwa and Muhammad Imran Malik", "abstract": "Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.", "link": "http://arxiv.org/abs/2602.04525v1", "date": "2026-02-04", "relevancy": 2.1129, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5737}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLUM-i%3A%20Semi-supervised%20Learning%20for%20Urban%20Mapping%20of%20Informal%20Settlements%20and%20Data%20Quality%20Benchmarking&body=Title%3A%20SLUM-i%3A%20Semi-supervised%20Learning%20for%20Urban%20Mapping%20of%20Informal%20Settlements%20and%20Data%20Quality%20Benchmarking%0AAuthor%3A%20Muhammad%20Taha%20Mukhtar%20and%20Syed%20Musa%20Ali%20Kazmi%20and%20Khola%20Naseem%20and%20Muhammad%20Ali%20Chattha%20and%20Andreas%20Dengel%20and%20Sheraz%20Ahmed%20and%20Muhammad%20Naseer%20Bajwa%20and%20Muhammad%20Imran%20Malik%0AAbstract%3A%20Rapid%20urban%20expansion%20has%20fueled%20the%20growth%20of%20informal%20settlements%20in%20major%20cities%20of%20low-%20and%20middle-income%20countries%2C%20with%20Lahore%20and%20Karachi%20in%20Pakistan%20and%20Mumbai%20in%20India%20serving%20as%20prominent%20examples.%20However%2C%20large-scale%20mapping%20of%20these%20settlements%20is%20severely%20constrained%20not%20only%20by%20the%20scarcity%20of%20annotations%20but%20by%20inherent%20data%20quality%20challenges%2C%20specifically%20high%20spectral%20ambiguity%20between%20formal%20and%20informal%20structures%20and%20significant%20annotation%20noise.%20We%20address%20this%20by%20introducing%20a%20benchmark%20dataset%20for%20Lahore%2C%20constructed%20from%20scratch%2C%20along%20with%20companion%20datasets%20for%20Karachi%20and%20Mumbai%2C%20which%20were%20derived%20from%20verified%20administrative%20boundaries%2C%20totaling%201%2C869%20%24%5Ctext%7Bkm%7D%5E2%24%20of%20area.%20To%20evaluate%20the%20global%20robustness%20of%20our%20framework%2C%20we%20extend%20our%20experiments%20to%20five%20additional%20established%20benchmarks%2C%20encompassing%20eight%20cities%20across%20three%20continents%2C%20and%20provide%20comprehensive%20data%20quality%20assessments%20of%20all%20datasets.%20We%20also%20propose%20a%20new%20semi-supervised%20segmentation%20framework%20designed%20to%20mitigate%20the%20class%20imbalance%20and%20feature%20degradation%20inherent%20in%20standard%20semi-supervised%20learning%20pipelines.%20Our%20method%20integrates%20a%20Class-Aware%20Adaptive%20Thresholding%20mechanism%20that%20dynamically%20adjusts%20confidence%20thresholds%20to%20prevent%20minority%20class%20suppression%20and%20a%20Prototype%20Bank%20System%20that%20enforces%20semantic%20consistency%20by%20anchoring%20predictions%20to%20historically%20learned%20high-fidelity%20feature%20representations.%20Extensive%20experiments%20across%20a%20total%20of%20eight%20cities%20spanning%20three%20continents%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20semi-supervised%20baselines.%20Most%20notably%2C%20our%20method%20demonstrates%20superior%20domain%20transfer%20capability%20whereby%20a%20model%20trained%20on%20only%2010%25%20of%20source%20labels%20reaches%20a%200.461%20mIoU%20on%20unseen%20geographies%20and%20outperforms%20the%20zero-shot%20generalization%20of%20fully%20supervised%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLUM-i%253A%2520Semi-supervised%2520Learning%2520for%2520Urban%2520Mapping%2520of%2520Informal%2520Settlements%2520and%2520Data%2520Quality%2520Benchmarking%26entry.906535625%3DMuhammad%2520Taha%2520Mukhtar%2520and%2520Syed%2520Musa%2520Ali%2520Kazmi%2520and%2520Khola%2520Naseem%2520and%2520Muhammad%2520Ali%2520Chattha%2520and%2520Andreas%2520Dengel%2520and%2520Sheraz%2520Ahmed%2520and%2520Muhammad%2520Naseer%2520Bajwa%2520and%2520Muhammad%2520Imran%2520Malik%26entry.1292438233%3DRapid%2520urban%2520expansion%2520has%2520fueled%2520the%2520growth%2520of%2520informal%2520settlements%2520in%2520major%2520cities%2520of%2520low-%2520and%2520middle-income%2520countries%252C%2520with%2520Lahore%2520and%2520Karachi%2520in%2520Pakistan%2520and%2520Mumbai%2520in%2520India%2520serving%2520as%2520prominent%2520examples.%2520However%252C%2520large-scale%2520mapping%2520of%2520these%2520settlements%2520is%2520severely%2520constrained%2520not%2520only%2520by%2520the%2520scarcity%2520of%2520annotations%2520but%2520by%2520inherent%2520data%2520quality%2520challenges%252C%2520specifically%2520high%2520spectral%2520ambiguity%2520between%2520formal%2520and%2520informal%2520structures%2520and%2520significant%2520annotation%2520noise.%2520We%2520address%2520this%2520by%2520introducing%2520a%2520benchmark%2520dataset%2520for%2520Lahore%252C%2520constructed%2520from%2520scratch%252C%2520along%2520with%2520companion%2520datasets%2520for%2520Karachi%2520and%2520Mumbai%252C%2520which%2520were%2520derived%2520from%2520verified%2520administrative%2520boundaries%252C%2520totaling%25201%252C869%2520%2524%255Ctext%257Bkm%257D%255E2%2524%2520of%2520area.%2520To%2520evaluate%2520the%2520global%2520robustness%2520of%2520our%2520framework%252C%2520we%2520extend%2520our%2520experiments%2520to%2520five%2520additional%2520established%2520benchmarks%252C%2520encompassing%2520eight%2520cities%2520across%2520three%2520continents%252C%2520and%2520provide%2520comprehensive%2520data%2520quality%2520assessments%2520of%2520all%2520datasets.%2520We%2520also%2520propose%2520a%2520new%2520semi-supervised%2520segmentation%2520framework%2520designed%2520to%2520mitigate%2520the%2520class%2520imbalance%2520and%2520feature%2520degradation%2520inherent%2520in%2520standard%2520semi-supervised%2520learning%2520pipelines.%2520Our%2520method%2520integrates%2520a%2520Class-Aware%2520Adaptive%2520Thresholding%2520mechanism%2520that%2520dynamically%2520adjusts%2520confidence%2520thresholds%2520to%2520prevent%2520minority%2520class%2520suppression%2520and%2520a%2520Prototype%2520Bank%2520System%2520that%2520enforces%2520semantic%2520consistency%2520by%2520anchoring%2520predictions%2520to%2520historically%2520learned%2520high-fidelity%2520feature%2520representations.%2520Extensive%2520experiments%2520across%2520a%2520total%2520of%2520eight%2520cities%2520spanning%2520three%2520continents%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520semi-supervised%2520baselines.%2520Most%2520notably%252C%2520our%2520method%2520demonstrates%2520superior%2520domain%2520transfer%2520capability%2520whereby%2520a%2520model%2520trained%2520on%2520only%252010%2525%2520of%2520source%2520labels%2520reaches%2520a%25200.461%2520mIoU%2520on%2520unseen%2520geographies%2520and%2520outperforms%2520the%2520zero-shot%2520generalization%2520of%2520fully%2520supervised%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLUM-i%3A%20Semi-supervised%20Learning%20for%20Urban%20Mapping%20of%20Informal%20Settlements%20and%20Data%20Quality%20Benchmarking&entry.906535625=Muhammad%20Taha%20Mukhtar%20and%20Syed%20Musa%20Ali%20Kazmi%20and%20Khola%20Naseem%20and%20Muhammad%20Ali%20Chattha%20and%20Andreas%20Dengel%20and%20Sheraz%20Ahmed%20and%20Muhammad%20Naseer%20Bajwa%20and%20Muhammad%20Imran%20Malik&entry.1292438233=Rapid%20urban%20expansion%20has%20fueled%20the%20growth%20of%20informal%20settlements%20in%20major%20cities%20of%20low-%20and%20middle-income%20countries%2C%20with%20Lahore%20and%20Karachi%20in%20Pakistan%20and%20Mumbai%20in%20India%20serving%20as%20prominent%20examples.%20However%2C%20large-scale%20mapping%20of%20these%20settlements%20is%20severely%20constrained%20not%20only%20by%20the%20scarcity%20of%20annotations%20but%20by%20inherent%20data%20quality%20challenges%2C%20specifically%20high%20spectral%20ambiguity%20between%20formal%20and%20informal%20structures%20and%20significant%20annotation%20noise.%20We%20address%20this%20by%20introducing%20a%20benchmark%20dataset%20for%20Lahore%2C%20constructed%20from%20scratch%2C%20along%20with%20companion%20datasets%20for%20Karachi%20and%20Mumbai%2C%20which%20were%20derived%20from%20verified%20administrative%20boundaries%2C%20totaling%201%2C869%20%24%5Ctext%7Bkm%7D%5E2%24%20of%20area.%20To%20evaluate%20the%20global%20robustness%20of%20our%20framework%2C%20we%20extend%20our%20experiments%20to%20five%20additional%20established%20benchmarks%2C%20encompassing%20eight%20cities%20across%20three%20continents%2C%20and%20provide%20comprehensive%20data%20quality%20assessments%20of%20all%20datasets.%20We%20also%20propose%20a%20new%20semi-supervised%20segmentation%20framework%20designed%20to%20mitigate%20the%20class%20imbalance%20and%20feature%20degradation%20inherent%20in%20standard%20semi-supervised%20learning%20pipelines.%20Our%20method%20integrates%20a%20Class-Aware%20Adaptive%20Thresholding%20mechanism%20that%20dynamically%20adjusts%20confidence%20thresholds%20to%20prevent%20minority%20class%20suppression%20and%20a%20Prototype%20Bank%20System%20that%20enforces%20semantic%20consistency%20by%20anchoring%20predictions%20to%20historically%20learned%20high-fidelity%20feature%20representations.%20Extensive%20experiments%20across%20a%20total%20of%20eight%20cities%20spanning%20three%20continents%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20semi-supervised%20baselines.%20Most%20notably%2C%20our%20method%20demonstrates%20superior%20domain%20transfer%20capability%20whereby%20a%20model%20trained%20on%20only%2010%25%20of%20source%20labels%20reaches%20a%200.461%20mIoU%20on%20unseen%20geographies%20and%20outperforms%20the%20zero-shot%20generalization%20of%20fully%20supervised%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.04525v1&entry.124074799=Read"},
{"title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference", "author": "Zihan Wang and Hao Wang and Shi Feng and Xiaocui Yang and Daling Wang and Yiqun Zhang and Jinghao Lin and Haihua Yang and Xiaozhong Ji", "abstract": "Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus \"find it but fail to use it,\" leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\\% on average and outperforms larger medical reasoning and DR models.", "link": "http://arxiv.org/abs/2601.18496v2", "date": "2026-02-04", "relevancy": 2.1114, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEEPMED%3A%20Building%20a%20Medical%20DeepResearch%20Agent%20via%20Multi-hop%20Med-Search%20Data%20and%20Turn-Controlled%20Agentic%20Training%20%26%20Inference&body=Title%3A%20DEEPMED%3A%20Building%20a%20Medical%20DeepResearch%20Agent%20via%20Multi-hop%20Med-Search%20Data%20and%20Turn-Controlled%20Agentic%20Training%20%26%20Inference%0AAuthor%3A%20Zihan%20Wang%20and%20Hao%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yiqun%20Zhang%20and%20Jinghao%20Lin%20and%20Haihua%20Yang%20and%20Xiaozhong%20Ji%0AAbstract%3A%20Medical%20reasoning%20models%20remain%20constrained%20by%20parametric%20knowledge%20and%20are%20thus%20susceptible%20to%20forgetting%20and%20hallucinations.%20DeepResearch%20%28DR%29%20models%20ground%20outputs%20in%20verifiable%20evidence%20from%20tools%20and%20perform%20strongly%20in%20general%20domains%2C%20but%20their%20direct%20transfer%20to%20medical%20field%20yields%20relatively%20limited%20gains.%20We%20attribute%20this%20to%20two%20gaps%3A%20task%20characteristic%20and%20tool-use%20scaling.%20Medical%20questions%20require%20evidence%20interpretation%20in%20a%20knowledge-intensive%20clinical%20context%3B%20while%20general%20DR%20models%20can%20retrieve%20information%2C%20they%20often%20lack%20clinical-context%20reasoning%20and%20thus%20%22find%20it%20but%20fail%20to%20use%20it%2C%22%20leaving%20performance%20limited%20by%20medical%20abilities.%20Moreover%2C%20in%20medical%20scenarios%2C%20blindly%20scaling%20tool-call%20can%20inject%20noisy%20context%2C%20derailing%20sensitive%20medical%20reasoning%20and%20prompting%20repetitive%20evidence-seeking%20along%20incorrect%20paths.%20Therefore%2C%20we%20propose%20DeepMed.%20For%20data%2C%20we%20deploy%20a%20multi-hop%20med-search%20QA%20synthesis%20method%20supporting%20the%20model%20to%20apply%20the%20DR%20paradigm%20in%20medical%20contexts.%20For%20training%2C%20we%20introduce%20a%20difficulty-aware%20turn-penalty%20to%20suppress%20excessive%20tool-call%20growth.%20For%20inference%2C%20we%20bring%20a%20monitor%20to%20help%20validate%20hypotheses%20within%20a%20controlled%20number%20of%20steps%20and%20avoid%20context%20rot.%20Overall%2C%20on%20seven%20medical%20benchmarks%2C%20DeepMed%20improves%20its%20base%20model%20by%209.79%5C%25%20on%20average%20and%20outperforms%20larger%20medical%20reasoning%20and%20DR%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEEPMED%253A%2520Building%2520a%2520Medical%2520DeepResearch%2520Agent%2520via%2520Multi-hop%2520Med-Search%2520Data%2520and%2520Turn-Controlled%2520Agentic%2520Training%2520%2526%2520Inference%26entry.906535625%3DZihan%2520Wang%2520and%2520Hao%2520Wang%2520and%2520Shi%2520Feng%2520and%2520Xiaocui%2520Yang%2520and%2520Daling%2520Wang%2520and%2520Yiqun%2520Zhang%2520and%2520Jinghao%2520Lin%2520and%2520Haihua%2520Yang%2520and%2520Xiaozhong%2520Ji%26entry.1292438233%3DMedical%2520reasoning%2520models%2520remain%2520constrained%2520by%2520parametric%2520knowledge%2520and%2520are%2520thus%2520susceptible%2520to%2520forgetting%2520and%2520hallucinations.%2520DeepResearch%2520%2528DR%2529%2520models%2520ground%2520outputs%2520in%2520verifiable%2520evidence%2520from%2520tools%2520and%2520perform%2520strongly%2520in%2520general%2520domains%252C%2520but%2520their%2520direct%2520transfer%2520to%2520medical%2520field%2520yields%2520relatively%2520limited%2520gains.%2520We%2520attribute%2520this%2520to%2520two%2520gaps%253A%2520task%2520characteristic%2520and%2520tool-use%2520scaling.%2520Medical%2520questions%2520require%2520evidence%2520interpretation%2520in%2520a%2520knowledge-intensive%2520clinical%2520context%253B%2520while%2520general%2520DR%2520models%2520can%2520retrieve%2520information%252C%2520they%2520often%2520lack%2520clinical-context%2520reasoning%2520and%2520thus%2520%2522find%2520it%2520but%2520fail%2520to%2520use%2520it%252C%2522%2520leaving%2520performance%2520limited%2520by%2520medical%2520abilities.%2520Moreover%252C%2520in%2520medical%2520scenarios%252C%2520blindly%2520scaling%2520tool-call%2520can%2520inject%2520noisy%2520context%252C%2520derailing%2520sensitive%2520medical%2520reasoning%2520and%2520prompting%2520repetitive%2520evidence-seeking%2520along%2520incorrect%2520paths.%2520Therefore%252C%2520we%2520propose%2520DeepMed.%2520For%2520data%252C%2520we%2520deploy%2520a%2520multi-hop%2520med-search%2520QA%2520synthesis%2520method%2520supporting%2520the%2520model%2520to%2520apply%2520the%2520DR%2520paradigm%2520in%2520medical%2520contexts.%2520For%2520training%252C%2520we%2520introduce%2520a%2520difficulty-aware%2520turn-penalty%2520to%2520suppress%2520excessive%2520tool-call%2520growth.%2520For%2520inference%252C%2520we%2520bring%2520a%2520monitor%2520to%2520help%2520validate%2520hypotheses%2520within%2520a%2520controlled%2520number%2520of%2520steps%2520and%2520avoid%2520context%2520rot.%2520Overall%252C%2520on%2520seven%2520medical%2520benchmarks%252C%2520DeepMed%2520improves%2520its%2520base%2520model%2520by%25209.79%255C%2525%2520on%2520average%2520and%2520outperforms%2520larger%2520medical%2520reasoning%2520and%2520DR%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEEPMED%3A%20Building%20a%20Medical%20DeepResearch%20Agent%20via%20Multi-hop%20Med-Search%20Data%20and%20Turn-Controlled%20Agentic%20Training%20%26%20Inference&entry.906535625=Zihan%20Wang%20and%20Hao%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yiqun%20Zhang%20and%20Jinghao%20Lin%20and%20Haihua%20Yang%20and%20Xiaozhong%20Ji&entry.1292438233=Medical%20reasoning%20models%20remain%20constrained%20by%20parametric%20knowledge%20and%20are%20thus%20susceptible%20to%20forgetting%20and%20hallucinations.%20DeepResearch%20%28DR%29%20models%20ground%20outputs%20in%20verifiable%20evidence%20from%20tools%20and%20perform%20strongly%20in%20general%20domains%2C%20but%20their%20direct%20transfer%20to%20medical%20field%20yields%20relatively%20limited%20gains.%20We%20attribute%20this%20to%20two%20gaps%3A%20task%20characteristic%20and%20tool-use%20scaling.%20Medical%20questions%20require%20evidence%20interpretation%20in%20a%20knowledge-intensive%20clinical%20context%3B%20while%20general%20DR%20models%20can%20retrieve%20information%2C%20they%20often%20lack%20clinical-context%20reasoning%20and%20thus%20%22find%20it%20but%20fail%20to%20use%20it%2C%22%20leaving%20performance%20limited%20by%20medical%20abilities.%20Moreover%2C%20in%20medical%20scenarios%2C%20blindly%20scaling%20tool-call%20can%20inject%20noisy%20context%2C%20derailing%20sensitive%20medical%20reasoning%20and%20prompting%20repetitive%20evidence-seeking%20along%20incorrect%20paths.%20Therefore%2C%20we%20propose%20DeepMed.%20For%20data%2C%20we%20deploy%20a%20multi-hop%20med-search%20QA%20synthesis%20method%20supporting%20the%20model%20to%20apply%20the%20DR%20paradigm%20in%20medical%20contexts.%20For%20training%2C%20we%20introduce%20a%20difficulty-aware%20turn-penalty%20to%20suppress%20excessive%20tool-call%20growth.%20For%20inference%2C%20we%20bring%20a%20monitor%20to%20help%20validate%20hypotheses%20within%20a%20controlled%20number%20of%20steps%20and%20avoid%20context%20rot.%20Overall%2C%20on%20seven%20medical%20benchmarks%2C%20DeepMed%20improves%20its%20base%20model%20by%209.79%5C%25%20on%20average%20and%20outperforms%20larger%20medical%20reasoning%20and%20DR%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.18496v2&entry.124074799=Read"},
{"title": "A Human-Centered Privacy Approach (HCP) to AI", "author": "Luyi Sun and Wei Xu and Zaifeng Gao", "abstract": "As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.", "link": "http://arxiv.org/abs/2602.04616v1", "date": "2026-02-04", "relevancy": 2.1088, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4358}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4194}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Human-Centered%20Privacy%20Approach%20%28HCP%29%20to%20AI&body=Title%3A%20A%20Human-Centered%20Privacy%20Approach%20%28HCP%29%20to%20AI%0AAuthor%3A%20Luyi%20Sun%20and%20Wei%20Xu%20and%20Zaifeng%20Gao%0AAbstract%3A%20As%20the%20paradigm%20of%20Human-Centered%20AI%20%28HCAI%29%20gains%20prominence%2C%20its%20benefits%20to%20society%20are%20accompanied%20by%20significant%20ethical%20concerns%2C%20one%20of%20which%20is%20the%20protection%20of%20individual%20privacy.%20This%20chapter%20provides%20a%20comprehensive%20overview%20of%20privacy%20within%20HCAI%2C%20proposing%20a%20human-centered%20privacy%20%28HCP%29%20framework%2C%20providing%20integrated%20solution%20from%20technology%2C%20ethics%2C%20and%20human%20factors%20perspectives.%20The%20chapter%20begins%20by%20mapping%20privacy%20risks%20across%20each%20stage%20of%20AI%20development%20lifecycle%2C%20from%20data%20collection%20to%20deployment%20and%20reuse%2C%20highlighting%20the%20impact%20of%20privacy%20risks%20on%20the%20entire%20system.%20The%20chapter%20then%20introduces%20privacy-preserving%20techniques%20such%20as%20federated%20learning%20and%20dif%20erential%20privacy.%20Subsequent%20chapters%20integrate%20the%20crucial%20user%20perspective%20by%20examining%20mental%20models%2C%20alongside%20the%20evolving%20regulatory%20and%20ethical%20landscapes%20as%20well%20as%20privacy%20governance.%20Next%2C%20advice%20on%20design%20guidelines%20is%20provided%20based%20on%20the%20human-centered%20privacy%20framework.%20After%20that%2C%20we%20introduce%20practical%20case%20studies%20across%20diverse%20fields.%20Finally%2C%20the%20chapter%20discusses%20persistent%20open%20challenges%20and%20future%20research%20directions%2C%20concluding%20that%20a%20multidisciplinary%20approach%2C%20merging%20technical%2C%20design%2C%20policy%2C%20and%20ethical%20expertise%2C%20is%20essential%20to%20successfully%20embed%20privacy%20into%20the%20core%20of%20HCAI%2C%20thereby%20ensuring%20these%20technologies%20advance%20in%20a%20manner%20that%20respects%20and%20ensures%20human%20autonomy%2C%20trust%20and%20dignity.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Human-Centered%2520Privacy%2520Approach%2520%2528HCP%2529%2520to%2520AI%26entry.906535625%3DLuyi%2520Sun%2520and%2520Wei%2520Xu%2520and%2520Zaifeng%2520Gao%26entry.1292438233%3DAs%2520the%2520paradigm%2520of%2520Human-Centered%2520AI%2520%2528HCAI%2529%2520gains%2520prominence%252C%2520its%2520benefits%2520to%2520society%2520are%2520accompanied%2520by%2520significant%2520ethical%2520concerns%252C%2520one%2520of%2520which%2520is%2520the%2520protection%2520of%2520individual%2520privacy.%2520This%2520chapter%2520provides%2520a%2520comprehensive%2520overview%2520of%2520privacy%2520within%2520HCAI%252C%2520proposing%2520a%2520human-centered%2520privacy%2520%2528HCP%2529%2520framework%252C%2520providing%2520integrated%2520solution%2520from%2520technology%252C%2520ethics%252C%2520and%2520human%2520factors%2520perspectives.%2520The%2520chapter%2520begins%2520by%2520mapping%2520privacy%2520risks%2520across%2520each%2520stage%2520of%2520AI%2520development%2520lifecycle%252C%2520from%2520data%2520collection%2520to%2520deployment%2520and%2520reuse%252C%2520highlighting%2520the%2520impact%2520of%2520privacy%2520risks%2520on%2520the%2520entire%2520system.%2520The%2520chapter%2520then%2520introduces%2520privacy-preserving%2520techniques%2520such%2520as%2520federated%2520learning%2520and%2520dif%2520erential%2520privacy.%2520Subsequent%2520chapters%2520integrate%2520the%2520crucial%2520user%2520perspective%2520by%2520examining%2520mental%2520models%252C%2520alongside%2520the%2520evolving%2520regulatory%2520and%2520ethical%2520landscapes%2520as%2520well%2520as%2520privacy%2520governance.%2520Next%252C%2520advice%2520on%2520design%2520guidelines%2520is%2520provided%2520based%2520on%2520the%2520human-centered%2520privacy%2520framework.%2520After%2520that%252C%2520we%2520introduce%2520practical%2520case%2520studies%2520across%2520diverse%2520fields.%2520Finally%252C%2520the%2520chapter%2520discusses%2520persistent%2520open%2520challenges%2520and%2520future%2520research%2520directions%252C%2520concluding%2520that%2520a%2520multidisciplinary%2520approach%252C%2520merging%2520technical%252C%2520design%252C%2520policy%252C%2520and%2520ethical%2520expertise%252C%2520is%2520essential%2520to%2520successfully%2520embed%2520privacy%2520into%2520the%2520core%2520of%2520HCAI%252C%2520thereby%2520ensuring%2520these%2520technologies%2520advance%2520in%2520a%2520manner%2520that%2520respects%2520and%2520ensures%2520human%2520autonomy%252C%2520trust%2520and%2520dignity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Human-Centered%20Privacy%20Approach%20%28HCP%29%20to%20AI&entry.906535625=Luyi%20Sun%20and%20Wei%20Xu%20and%20Zaifeng%20Gao&entry.1292438233=As%20the%20paradigm%20of%20Human-Centered%20AI%20%28HCAI%29%20gains%20prominence%2C%20its%20benefits%20to%20society%20are%20accompanied%20by%20significant%20ethical%20concerns%2C%20one%20of%20which%20is%20the%20protection%20of%20individual%20privacy.%20This%20chapter%20provides%20a%20comprehensive%20overview%20of%20privacy%20within%20HCAI%2C%20proposing%20a%20human-centered%20privacy%20%28HCP%29%20framework%2C%20providing%20integrated%20solution%20from%20technology%2C%20ethics%2C%20and%20human%20factors%20perspectives.%20The%20chapter%20begins%20by%20mapping%20privacy%20risks%20across%20each%20stage%20of%20AI%20development%20lifecycle%2C%20from%20data%20collection%20to%20deployment%20and%20reuse%2C%20highlighting%20the%20impact%20of%20privacy%20risks%20on%20the%20entire%20system.%20The%20chapter%20then%20introduces%20privacy-preserving%20techniques%20such%20as%20federated%20learning%20and%20dif%20erential%20privacy.%20Subsequent%20chapters%20integrate%20the%20crucial%20user%20perspective%20by%20examining%20mental%20models%2C%20alongside%20the%20evolving%20regulatory%20and%20ethical%20landscapes%20as%20well%20as%20privacy%20governance.%20Next%2C%20advice%20on%20design%20guidelines%20is%20provided%20based%20on%20the%20human-centered%20privacy%20framework.%20After%20that%2C%20we%20introduce%20practical%20case%20studies%20across%20diverse%20fields.%20Finally%2C%20the%20chapter%20discusses%20persistent%20open%20challenges%20and%20future%20research%20directions%2C%20concluding%20that%20a%20multidisciplinary%20approach%2C%20merging%20technical%2C%20design%2C%20policy%2C%20and%20ethical%20expertise%2C%20is%20essential%20to%20successfully%20embed%20privacy%20into%20the%20core%20of%20HCAI%2C%20thereby%20ensuring%20these%20technologies%20advance%20in%20a%20manner%20that%20respects%20and%20ensures%20human%20autonomy%2C%20trust%20and%20dignity.&entry.1838667208=http%3A//arxiv.org/abs/2602.04616v1&entry.124074799=Read"},
{"title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging", "author": "Jia-peng Zhang and Cheng-Feng Pu and Meng-Hao Guo and Yan-Pei Cao and Shi-Min Hu", "abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.", "link": "http://arxiv.org/abs/2602.04805v1", "date": "2026-02-04", "relevancy": 2.1083, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.54}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skin%20Tokens%3A%20A%20Learned%20Compact%20Representation%20for%20Unified%20Autoregressive%20Rigging&body=Title%3A%20Skin%20Tokens%3A%20A%20Learned%20Compact%20Representation%20for%20Unified%20Autoregressive%20Rigging%0AAuthor%3A%20Jia-peng%20Zhang%20and%20Cheng-Feng%20Pu%20and%20Meng-Hao%20Guo%20and%20Yan-Pei%20Cao%20and%20Shi-Min%20Hu%0AAbstract%3A%20The%20rapid%20proliferation%20of%20generative%203D%20models%20has%20created%20a%20critical%20bottleneck%20in%20animation%20pipelines%3A%20rigging.%20Existing%20automated%20methods%20are%20fundamentally%20limited%20by%20their%20approach%20to%20skinning%2C%20treating%20it%20as%20an%20ill-posed%2C%20high-dimensional%20regression%20task%20that%20is%20inefficient%20to%20optimize%20and%20is%20typically%20decoupled%20from%20skeleton%20generation.%20We%20posit%20this%20is%20a%20representation%20problem%20and%20introduce%20SkinTokens%3A%20a%20learned%2C%20compact%2C%20and%20discrete%20representation%20for%20skinning%20weights.%20By%20leveraging%20an%20FSQ-CVAE%20to%20capture%20the%20intrinsic%20sparsity%20of%20skinning%2C%20we%20reframe%20the%20task%20from%20continuous%20regression%20to%20a%20more%20tractable%20token%20sequence%20prediction%20problem.%20This%20representation%20enables%20TokenRig%2C%20a%20unified%20autoregressive%20framework%20that%20models%20the%20entire%20rig%20as%20a%20single%20sequence%20of%20skeletal%20parameters%20and%20SkinTokens%2C%20learning%20the%20complicated%20dependencies%20between%20skeletons%20and%20skin%20deformations.%20The%20unified%20model%20is%20then%20amenable%20to%20a%20reinforcement%20learning%20stage%2C%20where%20tailored%20geometric%20and%20semantic%20rewards%20improve%20generalization%20to%20complex%2C%20out-of-distribution%20assets.%20Quantitatively%2C%20the%20SkinTokens%20representation%20leads%20to%20a%2098%25-133%25%20percents%20improvement%20in%20skinning%20accuracy%20over%20state-of-the-art%20methods%2C%20while%20the%20full%20TokenRig%20framework%2C%20refined%20with%20RL%2C%20enhances%20bone%20prediction%20by%2017%25-22%25.%20Our%20work%20presents%20a%20unified%2C%20generative%20approach%20to%20rigging%20that%20yields%20higher%20fidelity%20and%20robustness%2C%20offering%20a%20scalable%20solution%20to%20a%20long-standing%20challenge%20in%203D%20content%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkin%2520Tokens%253A%2520A%2520Learned%2520Compact%2520Representation%2520for%2520Unified%2520Autoregressive%2520Rigging%26entry.906535625%3DJia-peng%2520Zhang%2520and%2520Cheng-Feng%2520Pu%2520and%2520Meng-Hao%2520Guo%2520and%2520Yan-Pei%2520Cao%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3DThe%2520rapid%2520proliferation%2520of%2520generative%25203D%2520models%2520has%2520created%2520a%2520critical%2520bottleneck%2520in%2520animation%2520pipelines%253A%2520rigging.%2520Existing%2520automated%2520methods%2520are%2520fundamentally%2520limited%2520by%2520their%2520approach%2520to%2520skinning%252C%2520treating%2520it%2520as%2520an%2520ill-posed%252C%2520high-dimensional%2520regression%2520task%2520that%2520is%2520inefficient%2520to%2520optimize%2520and%2520is%2520typically%2520decoupled%2520from%2520skeleton%2520generation.%2520We%2520posit%2520this%2520is%2520a%2520representation%2520problem%2520and%2520introduce%2520SkinTokens%253A%2520a%2520learned%252C%2520compact%252C%2520and%2520discrete%2520representation%2520for%2520skinning%2520weights.%2520By%2520leveraging%2520an%2520FSQ-CVAE%2520to%2520capture%2520the%2520intrinsic%2520sparsity%2520of%2520skinning%252C%2520we%2520reframe%2520the%2520task%2520from%2520continuous%2520regression%2520to%2520a%2520more%2520tractable%2520token%2520sequence%2520prediction%2520problem.%2520This%2520representation%2520enables%2520TokenRig%252C%2520a%2520unified%2520autoregressive%2520framework%2520that%2520models%2520the%2520entire%2520rig%2520as%2520a%2520single%2520sequence%2520of%2520skeletal%2520parameters%2520and%2520SkinTokens%252C%2520learning%2520the%2520complicated%2520dependencies%2520between%2520skeletons%2520and%2520skin%2520deformations.%2520The%2520unified%2520model%2520is%2520then%2520amenable%2520to%2520a%2520reinforcement%2520learning%2520stage%252C%2520where%2520tailored%2520geometric%2520and%2520semantic%2520rewards%2520improve%2520generalization%2520to%2520complex%252C%2520out-of-distribution%2520assets.%2520Quantitatively%252C%2520the%2520SkinTokens%2520representation%2520leads%2520to%2520a%252098%2525-133%2525%2520percents%2520improvement%2520in%2520skinning%2520accuracy%2520over%2520state-of-the-art%2520methods%252C%2520while%2520the%2520full%2520TokenRig%2520framework%252C%2520refined%2520with%2520RL%252C%2520enhances%2520bone%2520prediction%2520by%252017%2525-22%2525.%2520Our%2520work%2520presents%2520a%2520unified%252C%2520generative%2520approach%2520to%2520rigging%2520that%2520yields%2520higher%2520fidelity%2520and%2520robustness%252C%2520offering%2520a%2520scalable%2520solution%2520to%2520a%2520long-standing%2520challenge%2520in%25203D%2520content%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skin%20Tokens%3A%20A%20Learned%20Compact%20Representation%20for%20Unified%20Autoregressive%20Rigging&entry.906535625=Jia-peng%20Zhang%20and%20Cheng-Feng%20Pu%20and%20Meng-Hao%20Guo%20and%20Yan-Pei%20Cao%20and%20Shi-Min%20Hu&entry.1292438233=The%20rapid%20proliferation%20of%20generative%203D%20models%20has%20created%20a%20critical%20bottleneck%20in%20animation%20pipelines%3A%20rigging.%20Existing%20automated%20methods%20are%20fundamentally%20limited%20by%20their%20approach%20to%20skinning%2C%20treating%20it%20as%20an%20ill-posed%2C%20high-dimensional%20regression%20task%20that%20is%20inefficient%20to%20optimize%20and%20is%20typically%20decoupled%20from%20skeleton%20generation.%20We%20posit%20this%20is%20a%20representation%20problem%20and%20introduce%20SkinTokens%3A%20a%20learned%2C%20compact%2C%20and%20discrete%20representation%20for%20skinning%20weights.%20By%20leveraging%20an%20FSQ-CVAE%20to%20capture%20the%20intrinsic%20sparsity%20of%20skinning%2C%20we%20reframe%20the%20task%20from%20continuous%20regression%20to%20a%20more%20tractable%20token%20sequence%20prediction%20problem.%20This%20representation%20enables%20TokenRig%2C%20a%20unified%20autoregressive%20framework%20that%20models%20the%20entire%20rig%20as%20a%20single%20sequence%20of%20skeletal%20parameters%20and%20SkinTokens%2C%20learning%20the%20complicated%20dependencies%20between%20skeletons%20and%20skin%20deformations.%20The%20unified%20model%20is%20then%20amenable%20to%20a%20reinforcement%20learning%20stage%2C%20where%20tailored%20geometric%20and%20semantic%20rewards%20improve%20generalization%20to%20complex%2C%20out-of-distribution%20assets.%20Quantitatively%2C%20the%20SkinTokens%20representation%20leads%20to%20a%2098%25-133%25%20percents%20improvement%20in%20skinning%20accuracy%20over%20state-of-the-art%20methods%2C%20while%20the%20full%20TokenRig%20framework%2C%20refined%20with%20RL%2C%20enhances%20bone%20prediction%20by%2017%25-22%25.%20Our%20work%20presents%20a%20unified%2C%20generative%20approach%20to%20rigging%20that%20yields%20higher%20fidelity%20and%20robustness%2C%20offering%20a%20scalable%20solution%20to%20a%20long-standing%20challenge%20in%203D%20content%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2602.04805v1&entry.124074799=Read"},
{"title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing", "author": "Zhaotian Weng and Antonis Antoniades and Deepak Nathani and Zhen Zhang and Xiao Pu and Xin Eric Wang", "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.", "link": "http://arxiv.org/abs/2602.04837v1", "date": "2026-02-04", "relevancy": 2.1073, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5451}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5388}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group-Evolving%20Agents%3A%20Open-Ended%20Self-Improvement%20via%20Experience%20Sharing&body=Title%3A%20Group-Evolving%20Agents%3A%20Open-Ended%20Self-Improvement%20via%20Experience%20Sharing%0AAuthor%3A%20Zhaotian%20Weng%20and%20Antonis%20Antoniades%20and%20Deepak%20Nathani%20and%20Zhen%20Zhang%20and%20Xiao%20Pu%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20Open-ended%20self-improving%20agents%20can%20autonomously%20modify%20their%20own%20structural%20designs%20to%20advance%20their%20capabilities%20and%20overcome%20the%20limits%20of%20pre-defined%20architectures%2C%20thus%20reducing%20reliance%20on%20human%20intervention.%20We%20introduce%20Group-Evolving%20Agents%20%28GEA%29%2C%20a%20new%20paradigm%20for%20open-ended%20self-improvements%2C%20which%20treats%20a%20group%20of%20agents%20as%20the%20fundamental%20evolutionary%20unit%2C%20enabling%20explicit%20experience%20sharing%20and%20reuse%20within%20the%20group%20throughout%20evolution.%20Unlike%20existing%20open-ended%20self-evolving%20paradigms%20that%20adopt%20tree-structured%20evolution%2C%20GEA%20overcomes%20the%20limitation%20of%20inefficient%20utilization%20of%20exploratory%20diversity%20caused%20by%20isolated%20evolutionary%20branches.%20We%20evaluate%20GEA%20on%20challenging%20coding%20benchmarks%2C%20where%20it%20significantly%20outperforms%20state-of-the-art%20self-evolving%20methods%20%2871.0%25%20vs.%2056.7%25%20on%20SWE-bench%20Verified%2C%2088.3%25%20vs.%2068.3%25%20on%20Polyglot%29%20and%20matches%20or%20exceeds%20top%20human-designed%20agent%20frameworks%20%2871.8%25%20and%2052.0%25%20on%20two%20benchmarks%2C%20respectively%29.%20Analysis%20reveals%20that%20GEA%20more%20effectively%20converts%20early-stage%20exploratory%20diversity%20into%20sustained%2C%20long-term%20progress%2C%20achieving%20stronger%20performance%20under%20the%20same%20number%20of%20evolved%20agents.%20Furthermore%2C%20GEA%20exhibits%20consistent%20transferability%20across%20different%20coding%20models%20and%20greater%20robustness%2C%20fixing%20framework-level%20bugs%20in%201.4%20iterations%20on%20average%2C%20versus%205%20for%20self-evolving%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup-Evolving%2520Agents%253A%2520Open-Ended%2520Self-Improvement%2520via%2520Experience%2520Sharing%26entry.906535625%3DZhaotian%2520Weng%2520and%2520Antonis%2520Antoniades%2520and%2520Deepak%2520Nathani%2520and%2520Zhen%2520Zhang%2520and%2520Xiao%2520Pu%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DOpen-ended%2520self-improving%2520agents%2520can%2520autonomously%2520modify%2520their%2520own%2520structural%2520designs%2520to%2520advance%2520their%2520capabilities%2520and%2520overcome%2520the%2520limits%2520of%2520pre-defined%2520architectures%252C%2520thus%2520reducing%2520reliance%2520on%2520human%2520intervention.%2520We%2520introduce%2520Group-Evolving%2520Agents%2520%2528GEA%2529%252C%2520a%2520new%2520paradigm%2520for%2520open-ended%2520self-improvements%252C%2520which%2520treats%2520a%2520group%2520of%2520agents%2520as%2520the%2520fundamental%2520evolutionary%2520unit%252C%2520enabling%2520explicit%2520experience%2520sharing%2520and%2520reuse%2520within%2520the%2520group%2520throughout%2520evolution.%2520Unlike%2520existing%2520open-ended%2520self-evolving%2520paradigms%2520that%2520adopt%2520tree-structured%2520evolution%252C%2520GEA%2520overcomes%2520the%2520limitation%2520of%2520inefficient%2520utilization%2520of%2520exploratory%2520diversity%2520caused%2520by%2520isolated%2520evolutionary%2520branches.%2520We%2520evaluate%2520GEA%2520on%2520challenging%2520coding%2520benchmarks%252C%2520where%2520it%2520significantly%2520outperforms%2520state-of-the-art%2520self-evolving%2520methods%2520%252871.0%2525%2520vs.%252056.7%2525%2520on%2520SWE-bench%2520Verified%252C%252088.3%2525%2520vs.%252068.3%2525%2520on%2520Polyglot%2529%2520and%2520matches%2520or%2520exceeds%2520top%2520human-designed%2520agent%2520frameworks%2520%252871.8%2525%2520and%252052.0%2525%2520on%2520two%2520benchmarks%252C%2520respectively%2529.%2520Analysis%2520reveals%2520that%2520GEA%2520more%2520effectively%2520converts%2520early-stage%2520exploratory%2520diversity%2520into%2520sustained%252C%2520long-term%2520progress%252C%2520achieving%2520stronger%2520performance%2520under%2520the%2520same%2520number%2520of%2520evolved%2520agents.%2520Furthermore%252C%2520GEA%2520exhibits%2520consistent%2520transferability%2520across%2520different%2520coding%2520models%2520and%2520greater%2520robustness%252C%2520fixing%2520framework-level%2520bugs%2520in%25201.4%2520iterations%2520on%2520average%252C%2520versus%25205%2520for%2520self-evolving%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group-Evolving%20Agents%3A%20Open-Ended%20Self-Improvement%20via%20Experience%20Sharing&entry.906535625=Zhaotian%20Weng%20and%20Antonis%20Antoniades%20and%20Deepak%20Nathani%20and%20Zhen%20Zhang%20and%20Xiao%20Pu%20and%20Xin%20Eric%20Wang&entry.1292438233=Open-ended%20self-improving%20agents%20can%20autonomously%20modify%20their%20own%20structural%20designs%20to%20advance%20their%20capabilities%20and%20overcome%20the%20limits%20of%20pre-defined%20architectures%2C%20thus%20reducing%20reliance%20on%20human%20intervention.%20We%20introduce%20Group-Evolving%20Agents%20%28GEA%29%2C%20a%20new%20paradigm%20for%20open-ended%20self-improvements%2C%20which%20treats%20a%20group%20of%20agents%20as%20the%20fundamental%20evolutionary%20unit%2C%20enabling%20explicit%20experience%20sharing%20and%20reuse%20within%20the%20group%20throughout%20evolution.%20Unlike%20existing%20open-ended%20self-evolving%20paradigms%20that%20adopt%20tree-structured%20evolution%2C%20GEA%20overcomes%20the%20limitation%20of%20inefficient%20utilization%20of%20exploratory%20diversity%20caused%20by%20isolated%20evolutionary%20branches.%20We%20evaluate%20GEA%20on%20challenging%20coding%20benchmarks%2C%20where%20it%20significantly%20outperforms%20state-of-the-art%20self-evolving%20methods%20%2871.0%25%20vs.%2056.7%25%20on%20SWE-bench%20Verified%2C%2088.3%25%20vs.%2068.3%25%20on%20Polyglot%29%20and%20matches%20or%20exceeds%20top%20human-designed%20agent%20frameworks%20%2871.8%25%20and%2052.0%25%20on%20two%20benchmarks%2C%20respectively%29.%20Analysis%20reveals%20that%20GEA%20more%20effectively%20converts%20early-stage%20exploratory%20diversity%20into%20sustained%2C%20long-term%20progress%2C%20achieving%20stronger%20performance%20under%20the%20same%20number%20of%20evolved%20agents.%20Furthermore%2C%20GEA%20exhibits%20consistent%20transferability%20across%20different%20coding%20models%20and%20greater%20robustness%2C%20fixing%20framework-level%20bugs%20in%201.4%20iterations%20on%20average%2C%20versus%205%20for%20self-evolving%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.04837v1&entry.124074799=Read"},
{"title": "Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings", "author": "Berkant Turan and Suhrab Asadulla and David Steinmann and Kristian Kersting and Wolfgang Stammer and Sebastian Pokutta", "abstract": "While Prover-Verifier Games (PVGs) offer a promising path toward verifiability in nonlinear classification models, they have not yet been applied to complex inputs such as high-dimensional images. Conversely, expressive concept encodings effectively allow to translate such data into interpretable concepts but are often utilised in the context of low-capacity linear predictors. In this work, we push towards real-world verifiability by combining the strengths of both approaches. We introduce Neural Concept Verifier (NCV), a unified framework combining PVGs for formal verifiability with concept encodings to handle complex, high-dimensional inputs in an interpretable way. NCV achieves this by utilizing recent minimally supervised concept discovery models to extract structured concept encodings from raw inputs. A prover then selects a subset of these encodings, which a verifier, implemented as a nonlinear predictor, uses exclusively for decision-making. Our evaluations show that NCV outperforms classic concept-based models and pixel-based PVG classifier baselines on high-dimensional, logically complex datasets and helps mitigate shortcut behavior. Overall, we demonstrate NCV as a promising step toward concept-level, verifiable AI.", "link": "http://arxiv.org/abs/2507.07532v3", "date": "2026-02-04", "relevancy": 2.1053, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Concept%20Verifier%3A%20Scaling%20Prover-Verifier%20Games%20via%20Concept%20Encodings&body=Title%3A%20Neural%20Concept%20Verifier%3A%20Scaling%20Prover-Verifier%20Games%20via%20Concept%20Encodings%0AAuthor%3A%20Berkant%20Turan%20and%20Suhrab%20Asadulla%20and%20David%20Steinmann%20and%20Kristian%20Kersting%20and%20Wolfgang%20Stammer%20and%20Sebastian%20Pokutta%0AAbstract%3A%20While%20Prover-Verifier%20Games%20%28PVGs%29%20offer%20a%20promising%20path%20toward%20verifiability%20in%20nonlinear%20classification%20models%2C%20they%20have%20not%20yet%20been%20applied%20to%20complex%20inputs%20such%20as%20high-dimensional%20images.%20Conversely%2C%20expressive%20concept%20encodings%20effectively%20allow%20to%20translate%20such%20data%20into%20interpretable%20concepts%20but%20are%20often%20utilised%20in%20the%20context%20of%20low-capacity%20linear%20predictors.%20In%20this%20work%2C%20we%20push%20towards%20real-world%20verifiability%20by%20combining%20the%20strengths%20of%20both%20approaches.%20We%20introduce%20Neural%20Concept%20Verifier%20%28NCV%29%2C%20a%20unified%20framework%20combining%20PVGs%20for%20formal%20verifiability%20with%20concept%20encodings%20to%20handle%20complex%2C%20high-dimensional%20inputs%20in%20an%20interpretable%20way.%20NCV%20achieves%20this%20by%20utilizing%20recent%20minimally%20supervised%20concept%20discovery%20models%20to%20extract%20structured%20concept%20encodings%20from%20raw%20inputs.%20A%20prover%20then%20selects%20a%20subset%20of%20these%20encodings%2C%20which%20a%20verifier%2C%20implemented%20as%20a%20nonlinear%20predictor%2C%20uses%20exclusively%20for%20decision-making.%20Our%20evaluations%20show%20that%20NCV%20outperforms%20classic%20concept-based%20models%20and%20pixel-based%20PVG%20classifier%20baselines%20on%20high-dimensional%2C%20logically%20complex%20datasets%20and%20helps%20mitigate%20shortcut%20behavior.%20Overall%2C%20we%20demonstrate%20NCV%20as%20a%20promising%20step%20toward%20concept-level%2C%20verifiable%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2507.07532v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Concept%2520Verifier%253A%2520Scaling%2520Prover-Verifier%2520Games%2520via%2520Concept%2520Encodings%26entry.906535625%3DBerkant%2520Turan%2520and%2520Suhrab%2520Asadulla%2520and%2520David%2520Steinmann%2520and%2520Kristian%2520Kersting%2520and%2520Wolfgang%2520Stammer%2520and%2520Sebastian%2520Pokutta%26entry.1292438233%3DWhile%2520Prover-Verifier%2520Games%2520%2528PVGs%2529%2520offer%2520a%2520promising%2520path%2520toward%2520verifiability%2520in%2520nonlinear%2520classification%2520models%252C%2520they%2520have%2520not%2520yet%2520been%2520applied%2520to%2520complex%2520inputs%2520such%2520as%2520high-dimensional%2520images.%2520Conversely%252C%2520expressive%2520concept%2520encodings%2520effectively%2520allow%2520to%2520translate%2520such%2520data%2520into%2520interpretable%2520concepts%2520but%2520are%2520often%2520utilised%2520in%2520the%2520context%2520of%2520low-capacity%2520linear%2520predictors.%2520In%2520this%2520work%252C%2520we%2520push%2520towards%2520real-world%2520verifiability%2520by%2520combining%2520the%2520strengths%2520of%2520both%2520approaches.%2520We%2520introduce%2520Neural%2520Concept%2520Verifier%2520%2528NCV%2529%252C%2520a%2520unified%2520framework%2520combining%2520PVGs%2520for%2520formal%2520verifiability%2520with%2520concept%2520encodings%2520to%2520handle%2520complex%252C%2520high-dimensional%2520inputs%2520in%2520an%2520interpretable%2520way.%2520NCV%2520achieves%2520this%2520by%2520utilizing%2520recent%2520minimally%2520supervised%2520concept%2520discovery%2520models%2520to%2520extract%2520structured%2520concept%2520encodings%2520from%2520raw%2520inputs.%2520A%2520prover%2520then%2520selects%2520a%2520subset%2520of%2520these%2520encodings%252C%2520which%2520a%2520verifier%252C%2520implemented%2520as%2520a%2520nonlinear%2520predictor%252C%2520uses%2520exclusively%2520for%2520decision-making.%2520Our%2520evaluations%2520show%2520that%2520NCV%2520outperforms%2520classic%2520concept-based%2520models%2520and%2520pixel-based%2520PVG%2520classifier%2520baselines%2520on%2520high-dimensional%252C%2520logically%2520complex%2520datasets%2520and%2520helps%2520mitigate%2520shortcut%2520behavior.%2520Overall%252C%2520we%2520demonstrate%2520NCV%2520as%2520a%2520promising%2520step%2520toward%2520concept-level%252C%2520verifiable%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07532v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Concept%20Verifier%3A%20Scaling%20Prover-Verifier%20Games%20via%20Concept%20Encodings&entry.906535625=Berkant%20Turan%20and%20Suhrab%20Asadulla%20and%20David%20Steinmann%20and%20Kristian%20Kersting%20and%20Wolfgang%20Stammer%20and%20Sebastian%20Pokutta&entry.1292438233=While%20Prover-Verifier%20Games%20%28PVGs%29%20offer%20a%20promising%20path%20toward%20verifiability%20in%20nonlinear%20classification%20models%2C%20they%20have%20not%20yet%20been%20applied%20to%20complex%20inputs%20such%20as%20high-dimensional%20images.%20Conversely%2C%20expressive%20concept%20encodings%20effectively%20allow%20to%20translate%20such%20data%20into%20interpretable%20concepts%20but%20are%20often%20utilised%20in%20the%20context%20of%20low-capacity%20linear%20predictors.%20In%20this%20work%2C%20we%20push%20towards%20real-world%20verifiability%20by%20combining%20the%20strengths%20of%20both%20approaches.%20We%20introduce%20Neural%20Concept%20Verifier%20%28NCV%29%2C%20a%20unified%20framework%20combining%20PVGs%20for%20formal%20verifiability%20with%20concept%20encodings%20to%20handle%20complex%2C%20high-dimensional%20inputs%20in%20an%20interpretable%20way.%20NCV%20achieves%20this%20by%20utilizing%20recent%20minimally%20supervised%20concept%20discovery%20models%20to%20extract%20structured%20concept%20encodings%20from%20raw%20inputs.%20A%20prover%20then%20selects%20a%20subset%20of%20these%20encodings%2C%20which%20a%20verifier%2C%20implemented%20as%20a%20nonlinear%20predictor%2C%20uses%20exclusively%20for%20decision-making.%20Our%20evaluations%20show%20that%20NCV%20outperforms%20classic%20concept-based%20models%20and%20pixel-based%20PVG%20classifier%20baselines%20on%20high-dimensional%2C%20logically%20complex%20datasets%20and%20helps%20mitigate%20shortcut%20behavior.%20Overall%2C%20we%20demonstrate%20NCV%20as%20a%20promising%20step%20toward%20concept-level%2C%20verifiable%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2507.07532v3&entry.124074799=Read"},
{"title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation", "author": "Lei Tong and Zhihua Liu and Chaochao Lu and Dino Oglic and Tom Diethe and Philip Teare and Sotirios A. Tsaftaris and Chen Jin", "abstract": "We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method supports causal interventions on target attributes and consistently propagates their effects to causal dependents while preserving the core identity of the image. Unlike prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling with two attribute-regularization strategies: (i) prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and (ii) a conditioned token contrastive loss that disentangles attribute factors and reduces spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, including up to a 91% reduction in MAE on Pendulum for accurate attribute control and up to an 87% reduction in FID on ADNI for high-fidelity MRI generation. These results demonstrate robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation. Code and models will be released at: https://leitong02.github.io/causaladapter/.", "link": "http://arxiv.org/abs/2509.24798v5", "date": "2026-02-04", "relevancy": 1.713, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5757}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5661}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-Adapter%3A%20Taming%20Text-to-Image%20Diffusion%20for%20Faithful%20Counterfactual%20Generation&body=Title%3A%20Causal-Adapter%3A%20Taming%20Text-to-Image%20Diffusion%20for%20Faithful%20Counterfactual%20Generation%0AAuthor%3A%20Lei%20Tong%20and%20Zhihua%20Liu%20and%20Chaochao%20Lu%20and%20Dino%20Oglic%20and%20Tom%20Diethe%20and%20Philip%20Teare%20and%20Sotirios%20A.%20Tsaftaris%20and%20Chen%20Jin%0AAbstract%3A%20We%20present%20Causal-Adapter%2C%20a%20modular%20framework%20that%20adapts%20frozen%20text-to-image%20diffusion%20backbones%20for%20counterfactual%20image%20generation.%20Our%20method%20supports%20causal%20interventions%20on%20target%20attributes%20and%20consistently%20propagates%20their%20effects%20to%20causal%20dependents%20while%20preserving%20the%20core%20identity%20of%20the%20image.%20Unlike%20prior%20approaches%20that%20rely%20on%20prompt%20engineering%20without%20explicit%20causal%20structure%2C%20Causal-Adapter%20leverages%20structural%20causal%20modeling%20with%20two%20attribute-regularization%20strategies%3A%20%28i%29%20prompt-aligned%20injection%2C%20which%20aligns%20causal%20attributes%20with%20textual%20embeddings%20for%20precise%20semantic%20control%2C%20and%20%28ii%29%20a%20conditioned%20token%20contrastive%20loss%20that%20disentangles%20attribute%20factors%20and%20reduces%20spurious%20correlations.%20Causal-Adapter%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20including%20up%20to%20a%2091%25%20reduction%20in%20MAE%20on%20Pendulum%20for%20accurate%20attribute%20control%20and%20up%20to%20an%2087%25%20reduction%20in%20FID%20on%20ADNI%20for%20high-fidelity%20MRI%20generation.%20These%20results%20demonstrate%20robust%2C%20generalizable%20counterfactual%20editing%20with%20faithful%20attribute%20modification%20and%20strong%20identity%20preservation.%20Code%20and%20models%20will%20be%20released%20at%3A%20https%3A//leitong02.github.io/causaladapter/.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24798v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-Adapter%253A%2520Taming%2520Text-to-Image%2520Diffusion%2520for%2520Faithful%2520Counterfactual%2520Generation%26entry.906535625%3DLei%2520Tong%2520and%2520Zhihua%2520Liu%2520and%2520Chaochao%2520Lu%2520and%2520Dino%2520Oglic%2520and%2520Tom%2520Diethe%2520and%2520Philip%2520Teare%2520and%2520Sotirios%2520A.%2520Tsaftaris%2520and%2520Chen%2520Jin%26entry.1292438233%3DWe%2520present%2520Causal-Adapter%252C%2520a%2520modular%2520framework%2520that%2520adapts%2520frozen%2520text-to-image%2520diffusion%2520backbones%2520for%2520counterfactual%2520image%2520generation.%2520Our%2520method%2520supports%2520causal%2520interventions%2520on%2520target%2520attributes%2520and%2520consistently%2520propagates%2520their%2520effects%2520to%2520causal%2520dependents%2520while%2520preserving%2520the%2520core%2520identity%2520of%2520the%2520image.%2520Unlike%2520prior%2520approaches%2520that%2520rely%2520on%2520prompt%2520engineering%2520without%2520explicit%2520causal%2520structure%252C%2520Causal-Adapter%2520leverages%2520structural%2520causal%2520modeling%2520with%2520two%2520attribute-regularization%2520strategies%253A%2520%2528i%2529%2520prompt-aligned%2520injection%252C%2520which%2520aligns%2520causal%2520attributes%2520with%2520textual%2520embeddings%2520for%2520precise%2520semantic%2520control%252C%2520and%2520%2528ii%2529%2520a%2520conditioned%2520token%2520contrastive%2520loss%2520that%2520disentangles%2520attribute%2520factors%2520and%2520reduces%2520spurious%2520correlations.%2520Causal-Adapter%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520including%2520up%2520to%2520a%252091%2525%2520reduction%2520in%2520MAE%2520on%2520Pendulum%2520for%2520accurate%2520attribute%2520control%2520and%2520up%2520to%2520an%252087%2525%2520reduction%2520in%2520FID%2520on%2520ADNI%2520for%2520high-fidelity%2520MRI%2520generation.%2520These%2520results%2520demonstrate%2520robust%252C%2520generalizable%2520counterfactual%2520editing%2520with%2520faithful%2520attribute%2520modification%2520and%2520strong%2520identity%2520preservation.%2520Code%2520and%2520models%2520will%2520be%2520released%2520at%253A%2520https%253A//leitong02.github.io/causaladapter/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24798v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-Adapter%3A%20Taming%20Text-to-Image%20Diffusion%20for%20Faithful%20Counterfactual%20Generation&entry.906535625=Lei%20Tong%20and%20Zhihua%20Liu%20and%20Chaochao%20Lu%20and%20Dino%20Oglic%20and%20Tom%20Diethe%20and%20Philip%20Teare%20and%20Sotirios%20A.%20Tsaftaris%20and%20Chen%20Jin&entry.1292438233=We%20present%20Causal-Adapter%2C%20a%20modular%20framework%20that%20adapts%20frozen%20text-to-image%20diffusion%20backbones%20for%20counterfactual%20image%20generation.%20Our%20method%20supports%20causal%20interventions%20on%20target%20attributes%20and%20consistently%20propagates%20their%20effects%20to%20causal%20dependents%20while%20preserving%20the%20core%20identity%20of%20the%20image.%20Unlike%20prior%20approaches%20that%20rely%20on%20prompt%20engineering%20without%20explicit%20causal%20structure%2C%20Causal-Adapter%20leverages%20structural%20causal%20modeling%20with%20two%20attribute-regularization%20strategies%3A%20%28i%29%20prompt-aligned%20injection%2C%20which%20aligns%20causal%20attributes%20with%20textual%20embeddings%20for%20precise%20semantic%20control%2C%20and%20%28ii%29%20a%20conditioned%20token%20contrastive%20loss%20that%20disentangles%20attribute%20factors%20and%20reduces%20spurious%20correlations.%20Causal-Adapter%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20including%20up%20to%20a%2091%25%20reduction%20in%20MAE%20on%20Pendulum%20for%20accurate%20attribute%20control%20and%20up%20to%20an%2087%25%20reduction%20in%20FID%20on%20ADNI%20for%20high-fidelity%20MRI%20generation.%20These%20results%20demonstrate%20robust%2C%20generalizable%20counterfactual%20editing%20with%20faithful%20attribute%20modification%20and%20strong%20identity%20preservation.%20Code%20and%20models%20will%20be%20released%20at%3A%20https%3A//leitong02.github.io/causaladapter/.&entry.1838667208=http%3A//arxiv.org/abs/2509.24798v5&entry.124074799=Read"},
{"title": "Billion-Scale Graph Foundation Models", "author": "Maya Bechler-Speicher and Yoel Gottlieb and Andrey Isakov and David Abensur and Ami Tavory and Daniel Haimovich and Ido Guy and Udi Weinsberg", "abstract": "Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.", "link": "http://arxiv.org/abs/2602.04768v1", "date": "2026-02-04", "relevancy": 2.0183, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5625}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5013}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Billion-Scale%20Graph%20Foundation%20Models&body=Title%3A%20Billion-Scale%20Graph%20Foundation%20Models%0AAuthor%3A%20Maya%20Bechler-Speicher%20and%20Yoel%20Gottlieb%20and%20Andrey%20Isakov%20and%20David%20Abensur%20and%20Ami%20Tavory%20and%20Daniel%20Haimovich%20and%20Ido%20Guy%20and%20Udi%20Weinsberg%0AAbstract%3A%20Graph-structured%20data%20underpins%20many%20critical%20applications.%20While%20foundation%20models%20have%20transformed%20language%20and%20vision%20via%20large-scale%20pretraining%20and%20lightweight%20adaptation%2C%20extending%20this%20paradigm%20to%20general%2C%20real-world%20graphs%20is%20challenging.%20In%20this%20work%2C%20we%20present%20Graph%20Billion-%20Foundation-Fusion%20%28GraphBFF%29%3A%20the%20first%20end-to-end%20recipe%20for%20building%20billion-parameter%20Graph%20Foundation%20Models%20%28GFMs%29%20for%20arbitrary%20heterogeneous%2C%20billion-scale%20graphs.%20Central%20to%20the%20recipe%20is%20the%20GraphBFF%20Transformer%2C%20a%20flexible%20and%20scalable%20architecture%20designed%20for%20practical%20billion-scale%20GFMs.%20Using%20the%20GraphBFF%2C%20we%20present%20the%20first%20neural%20scaling%20laws%20for%20general%20graphs%20and%20show%20that%20loss%20decreases%20predictably%20as%20either%20model%20capacity%20or%20training%20data%20scales%2C%20depending%20on%20which%20factor%20is%20the%20bottleneck.%20The%20GraphBFF%20framework%20provides%20concrete%20methodologies%20for%20data%20batching%2C%20pretraining%2C%20and%20fine-tuning%20for%20building%20GFMs%20at%20scale.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20with%20an%20evaluation%20of%20a%201.4%20billion-parameter%20GraphBFF%20Transformer%20pretrained%20on%20one%20billion%20samples.%20Across%20ten%20diverse%2C%20real-world%20downstream%20tasks%20on%20graphs%20unseen%20during%20training%2C%20spanning%20node-%20and%20link-level%20classification%20and%20regression%2C%20GraphBFF%20achieves%20remarkable%20zero-shot%20and%20probing%20performance%2C%20including%20in%20few-shot%20settings%2C%20with%20large%20margins%20of%20up%20to%2031%20PRAUC%20points.%20Finally%2C%20we%20discuss%20key%20challenges%20and%20open%20opportunities%20for%20making%20GFMs%20a%20practical%20and%20principled%20foundation%20for%20graph%20learning%20at%20industrial%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBillion-Scale%2520Graph%2520Foundation%2520Models%26entry.906535625%3DMaya%2520Bechler-Speicher%2520and%2520Yoel%2520Gottlieb%2520and%2520Andrey%2520Isakov%2520and%2520David%2520Abensur%2520and%2520Ami%2520Tavory%2520and%2520Daniel%2520Haimovich%2520and%2520Ido%2520Guy%2520and%2520Udi%2520Weinsberg%26entry.1292438233%3DGraph-structured%2520data%2520underpins%2520many%2520critical%2520applications.%2520While%2520foundation%2520models%2520have%2520transformed%2520language%2520and%2520vision%2520via%2520large-scale%2520pretraining%2520and%2520lightweight%2520adaptation%252C%2520extending%2520this%2520paradigm%2520to%2520general%252C%2520real-world%2520graphs%2520is%2520challenging.%2520In%2520this%2520work%252C%2520we%2520present%2520Graph%2520Billion-%2520Foundation-Fusion%2520%2528GraphBFF%2529%253A%2520the%2520first%2520end-to-end%2520recipe%2520for%2520building%2520billion-parameter%2520Graph%2520Foundation%2520Models%2520%2528GFMs%2529%2520for%2520arbitrary%2520heterogeneous%252C%2520billion-scale%2520graphs.%2520Central%2520to%2520the%2520recipe%2520is%2520the%2520GraphBFF%2520Transformer%252C%2520a%2520flexible%2520and%2520scalable%2520architecture%2520designed%2520for%2520practical%2520billion-scale%2520GFMs.%2520Using%2520the%2520GraphBFF%252C%2520we%2520present%2520the%2520first%2520neural%2520scaling%2520laws%2520for%2520general%2520graphs%2520and%2520show%2520that%2520loss%2520decreases%2520predictably%2520as%2520either%2520model%2520capacity%2520or%2520training%2520data%2520scales%252C%2520depending%2520on%2520which%2520factor%2520is%2520the%2520bottleneck.%2520The%2520GraphBFF%2520framework%2520provides%2520concrete%2520methodologies%2520for%2520data%2520batching%252C%2520pretraining%252C%2520and%2520fine-tuning%2520for%2520building%2520GFMs%2520at%2520scale.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520framework%2520with%2520an%2520evaluation%2520of%2520a%25201.4%2520billion-parameter%2520GraphBFF%2520Transformer%2520pretrained%2520on%2520one%2520billion%2520samples.%2520Across%2520ten%2520diverse%252C%2520real-world%2520downstream%2520tasks%2520on%2520graphs%2520unseen%2520during%2520training%252C%2520spanning%2520node-%2520and%2520link-level%2520classification%2520and%2520regression%252C%2520GraphBFF%2520achieves%2520remarkable%2520zero-shot%2520and%2520probing%2520performance%252C%2520including%2520in%2520few-shot%2520settings%252C%2520with%2520large%2520margins%2520of%2520up%2520to%252031%2520PRAUC%2520points.%2520Finally%252C%2520we%2520discuss%2520key%2520challenges%2520and%2520open%2520opportunities%2520for%2520making%2520GFMs%2520a%2520practical%2520and%2520principled%2520foundation%2520for%2520graph%2520learning%2520at%2520industrial%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Billion-Scale%20Graph%20Foundation%20Models&entry.906535625=Maya%20Bechler-Speicher%20and%20Yoel%20Gottlieb%20and%20Andrey%20Isakov%20and%20David%20Abensur%20and%20Ami%20Tavory%20and%20Daniel%20Haimovich%20and%20Ido%20Guy%20and%20Udi%20Weinsberg&entry.1292438233=Graph-structured%20data%20underpins%20many%20critical%20applications.%20While%20foundation%20models%20have%20transformed%20language%20and%20vision%20via%20large-scale%20pretraining%20and%20lightweight%20adaptation%2C%20extending%20this%20paradigm%20to%20general%2C%20real-world%20graphs%20is%20challenging.%20In%20this%20work%2C%20we%20present%20Graph%20Billion-%20Foundation-Fusion%20%28GraphBFF%29%3A%20the%20first%20end-to-end%20recipe%20for%20building%20billion-parameter%20Graph%20Foundation%20Models%20%28GFMs%29%20for%20arbitrary%20heterogeneous%2C%20billion-scale%20graphs.%20Central%20to%20the%20recipe%20is%20the%20GraphBFF%20Transformer%2C%20a%20flexible%20and%20scalable%20architecture%20designed%20for%20practical%20billion-scale%20GFMs.%20Using%20the%20GraphBFF%2C%20we%20present%20the%20first%20neural%20scaling%20laws%20for%20general%20graphs%20and%20show%20that%20loss%20decreases%20predictably%20as%20either%20model%20capacity%20or%20training%20data%20scales%2C%20depending%20on%20which%20factor%20is%20the%20bottleneck.%20The%20GraphBFF%20framework%20provides%20concrete%20methodologies%20for%20data%20batching%2C%20pretraining%2C%20and%20fine-tuning%20for%20building%20GFMs%20at%20scale.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20with%20an%20evaluation%20of%20a%201.4%20billion-parameter%20GraphBFF%20Transformer%20pretrained%20on%20one%20billion%20samples.%20Across%20ten%20diverse%2C%20real-world%20downstream%20tasks%20on%20graphs%20unseen%20during%20training%2C%20spanning%20node-%20and%20link-level%20classification%20and%20regression%2C%20GraphBFF%20achieves%20remarkable%20zero-shot%20and%20probing%20performance%2C%20including%20in%20few-shot%20settings%2C%20with%20large%20margins%20of%20up%20to%2031%20PRAUC%20points.%20Finally%2C%20we%20discuss%20key%20challenges%20and%20open%20opportunities%20for%20making%20GFMs%20a%20practical%20and%20principled%20foundation%20for%20graph%20learning%20at%20industrial%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2602.04768v1&entry.124074799=Read"},
{"title": "Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices", "author": "Haitian Wang and Xinyu Wang and Yiren Wang and Bo Miao and Atif Mansoor", "abstract": "On-device skin lesion analysis is constrained by the compute and energy cost of conventional CNN inference and by the need to update models as new patient data become available. Neuromorphic processors provide event-driven sparse computation and support on-chip incremental learning, yet deployment is often hindered by CNN-to-SNN conversion failures, including non-spike-compatible operators and accuracy degradation under class imbalance. We propose QANA, a quantization-aware CNN backbone embedded in an end-to-end pipeline engineered for conversion-stable neuromorphic execution. QANA replaces conversion-fragile components with spike-compatible transformations by bounding intermediate activations and aligning normalization with low-bit quantization, reducing conversion-induced distortion that disproportionately impacts rare classes. Efficiency is achieved through Ghost-based feature generation under tight FLOP budgets, while spatially-aware efficient channel attention and squeeze-and-excitation recalibrate channels without heavy global operators that are difficult to map to spiking cores. The resulting quantized projection head produces SNN-ready logits and enables incremental updates on edge hardware without full retraining or data offloading. On HAM10000, QANA achieves 91.6% Top-1 accuracy and 91.0% macro F1, improving the strongest converted SNN baseline by 3.5 percentage points in Top-1 accuracy (a 4.0% relative gain) and by 12.0 points in macro F1 (a 15.2% relative gain). On a clinical dataset, QANA achieves 90.8% Top-1 accuracy and 81.7% macro F1, improving the strongest converted SNN baseline by 3.2 points in Top-1 accuracy (a 3.7% relative gain) and by 3.6 points in macro F1 (a 4.6% relative gain). When deployed on BrainChip Akida, QANA runs in 1.5 ms per image with 1.7 mJ per image, corresponding to 94.6% lower latency and 99.0% lower energy than its GPU-based CNN implementation.", "link": "http://arxiv.org/abs/2507.15958v4", "date": "2026-02-04", "relevancy": 2.0056, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5545}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4918}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization-Aware%20Neuromorphic%20Architecture%20for%20Skin%20Disease%20Classification%20on%20Resource-Constrained%20Devices&body=Title%3A%20Quantization-Aware%20Neuromorphic%20Architecture%20for%20Skin%20Disease%20Classification%20on%20Resource-Constrained%20Devices%0AAuthor%3A%20Haitian%20Wang%20and%20Xinyu%20Wang%20and%20Yiren%20Wang%20and%20Bo%20Miao%20and%20Atif%20Mansoor%0AAbstract%3A%20On-device%20skin%20lesion%20analysis%20is%20constrained%20by%20the%20compute%20and%20energy%20cost%20of%20conventional%20CNN%20inference%20and%20by%20the%20need%20to%20update%20models%20as%20new%20patient%20data%20become%20available.%20Neuromorphic%20processors%20provide%20event-driven%20sparse%20computation%20and%20support%20on-chip%20incremental%20learning%2C%20yet%20deployment%20is%20often%20hindered%20by%20CNN-to-SNN%20conversion%20failures%2C%20including%20non-spike-compatible%20operators%20and%20accuracy%20degradation%20under%20class%20imbalance.%20We%20propose%20QANA%2C%20a%20quantization-aware%20CNN%20backbone%20embedded%20in%20an%20end-to-end%20pipeline%20engineered%20for%20conversion-stable%20neuromorphic%20execution.%20QANA%20replaces%20conversion-fragile%20components%20with%20spike-compatible%20transformations%20by%20bounding%20intermediate%20activations%20and%20aligning%20normalization%20with%20low-bit%20quantization%2C%20reducing%20conversion-induced%20distortion%20that%20disproportionately%20impacts%20rare%20classes.%20Efficiency%20is%20achieved%20through%20Ghost-based%20feature%20generation%20under%20tight%20FLOP%20budgets%2C%20while%20spatially-aware%20efficient%20channel%20attention%20and%20squeeze-and-excitation%20recalibrate%20channels%20without%20heavy%20global%20operators%20that%20are%20difficult%20to%20map%20to%20spiking%20cores.%20The%20resulting%20quantized%20projection%20head%20produces%20SNN-ready%20logits%20and%20enables%20incremental%20updates%20on%20edge%20hardware%20without%20full%20retraining%20or%20data%20offloading.%20On%20HAM10000%2C%20QANA%20achieves%2091.6%25%20Top-1%20accuracy%20and%2091.0%25%20macro%20F1%2C%20improving%20the%20strongest%20converted%20SNN%20baseline%20by%203.5%20percentage%20points%20in%20Top-1%20accuracy%20%28a%204.0%25%20relative%20gain%29%20and%20by%2012.0%20points%20in%20macro%20F1%20%28a%2015.2%25%20relative%20gain%29.%20On%20a%20clinical%20dataset%2C%20QANA%20achieves%2090.8%25%20Top-1%20accuracy%20and%2081.7%25%20macro%20F1%2C%20improving%20the%20strongest%20converted%20SNN%20baseline%20by%203.2%20points%20in%20Top-1%20accuracy%20%28a%203.7%25%20relative%20gain%29%20and%20by%203.6%20points%20in%20macro%20F1%20%28a%204.6%25%20relative%20gain%29.%20When%20deployed%20on%20BrainChip%20Akida%2C%20QANA%20runs%20in%201.5%20ms%20per%20image%20with%201.7%20mJ%20per%20image%2C%20corresponding%20to%2094.6%25%20lower%20latency%20and%2099.0%25%20lower%20energy%20than%20its%20GPU-based%20CNN%20implementation.%0ALink%3A%20http%3A//arxiv.org/abs/2507.15958v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization-Aware%2520Neuromorphic%2520Architecture%2520for%2520Skin%2520Disease%2520Classification%2520on%2520Resource-Constrained%2520Devices%26entry.906535625%3DHaitian%2520Wang%2520and%2520Xinyu%2520Wang%2520and%2520Yiren%2520Wang%2520and%2520Bo%2520Miao%2520and%2520Atif%2520Mansoor%26entry.1292438233%3DOn-device%2520skin%2520lesion%2520analysis%2520is%2520constrained%2520by%2520the%2520compute%2520and%2520energy%2520cost%2520of%2520conventional%2520CNN%2520inference%2520and%2520by%2520the%2520need%2520to%2520update%2520models%2520as%2520new%2520patient%2520data%2520become%2520available.%2520Neuromorphic%2520processors%2520provide%2520event-driven%2520sparse%2520computation%2520and%2520support%2520on-chip%2520incremental%2520learning%252C%2520yet%2520deployment%2520is%2520often%2520hindered%2520by%2520CNN-to-SNN%2520conversion%2520failures%252C%2520including%2520non-spike-compatible%2520operators%2520and%2520accuracy%2520degradation%2520under%2520class%2520imbalance.%2520We%2520propose%2520QANA%252C%2520a%2520quantization-aware%2520CNN%2520backbone%2520embedded%2520in%2520an%2520end-to-end%2520pipeline%2520engineered%2520for%2520conversion-stable%2520neuromorphic%2520execution.%2520QANA%2520replaces%2520conversion-fragile%2520components%2520with%2520spike-compatible%2520transformations%2520by%2520bounding%2520intermediate%2520activations%2520and%2520aligning%2520normalization%2520with%2520low-bit%2520quantization%252C%2520reducing%2520conversion-induced%2520distortion%2520that%2520disproportionately%2520impacts%2520rare%2520classes.%2520Efficiency%2520is%2520achieved%2520through%2520Ghost-based%2520feature%2520generation%2520under%2520tight%2520FLOP%2520budgets%252C%2520while%2520spatially-aware%2520efficient%2520channel%2520attention%2520and%2520squeeze-and-excitation%2520recalibrate%2520channels%2520without%2520heavy%2520global%2520operators%2520that%2520are%2520difficult%2520to%2520map%2520to%2520spiking%2520cores.%2520The%2520resulting%2520quantized%2520projection%2520head%2520produces%2520SNN-ready%2520logits%2520and%2520enables%2520incremental%2520updates%2520on%2520edge%2520hardware%2520without%2520full%2520retraining%2520or%2520data%2520offloading.%2520On%2520HAM10000%252C%2520QANA%2520achieves%252091.6%2525%2520Top-1%2520accuracy%2520and%252091.0%2525%2520macro%2520F1%252C%2520improving%2520the%2520strongest%2520converted%2520SNN%2520baseline%2520by%25203.5%2520percentage%2520points%2520in%2520Top-1%2520accuracy%2520%2528a%25204.0%2525%2520relative%2520gain%2529%2520and%2520by%252012.0%2520points%2520in%2520macro%2520F1%2520%2528a%252015.2%2525%2520relative%2520gain%2529.%2520On%2520a%2520clinical%2520dataset%252C%2520QANA%2520achieves%252090.8%2525%2520Top-1%2520accuracy%2520and%252081.7%2525%2520macro%2520F1%252C%2520improving%2520the%2520strongest%2520converted%2520SNN%2520baseline%2520by%25203.2%2520points%2520in%2520Top-1%2520accuracy%2520%2528a%25203.7%2525%2520relative%2520gain%2529%2520and%2520by%25203.6%2520points%2520in%2520macro%2520F1%2520%2528a%25204.6%2525%2520relative%2520gain%2529.%2520When%2520deployed%2520on%2520BrainChip%2520Akida%252C%2520QANA%2520runs%2520in%25201.5%2520ms%2520per%2520image%2520with%25201.7%2520mJ%2520per%2520image%252C%2520corresponding%2520to%252094.6%2525%2520lower%2520latency%2520and%252099.0%2525%2520lower%2520energy%2520than%2520its%2520GPU-based%2520CNN%2520implementation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15958v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization-Aware%20Neuromorphic%20Architecture%20for%20Skin%20Disease%20Classification%20on%20Resource-Constrained%20Devices&entry.906535625=Haitian%20Wang%20and%20Xinyu%20Wang%20and%20Yiren%20Wang%20and%20Bo%20Miao%20and%20Atif%20Mansoor&entry.1292438233=On-device%20skin%20lesion%20analysis%20is%20constrained%20by%20the%20compute%20and%20energy%20cost%20of%20conventional%20CNN%20inference%20and%20by%20the%20need%20to%20update%20models%20as%20new%20patient%20data%20become%20available.%20Neuromorphic%20processors%20provide%20event-driven%20sparse%20computation%20and%20support%20on-chip%20incremental%20learning%2C%20yet%20deployment%20is%20often%20hindered%20by%20CNN-to-SNN%20conversion%20failures%2C%20including%20non-spike-compatible%20operators%20and%20accuracy%20degradation%20under%20class%20imbalance.%20We%20propose%20QANA%2C%20a%20quantization-aware%20CNN%20backbone%20embedded%20in%20an%20end-to-end%20pipeline%20engineered%20for%20conversion-stable%20neuromorphic%20execution.%20QANA%20replaces%20conversion-fragile%20components%20with%20spike-compatible%20transformations%20by%20bounding%20intermediate%20activations%20and%20aligning%20normalization%20with%20low-bit%20quantization%2C%20reducing%20conversion-induced%20distortion%20that%20disproportionately%20impacts%20rare%20classes.%20Efficiency%20is%20achieved%20through%20Ghost-based%20feature%20generation%20under%20tight%20FLOP%20budgets%2C%20while%20spatially-aware%20efficient%20channel%20attention%20and%20squeeze-and-excitation%20recalibrate%20channels%20without%20heavy%20global%20operators%20that%20are%20difficult%20to%20map%20to%20spiking%20cores.%20The%20resulting%20quantized%20projection%20head%20produces%20SNN-ready%20logits%20and%20enables%20incremental%20updates%20on%20edge%20hardware%20without%20full%20retraining%20or%20data%20offloading.%20On%20HAM10000%2C%20QANA%20achieves%2091.6%25%20Top-1%20accuracy%20and%2091.0%25%20macro%20F1%2C%20improving%20the%20strongest%20converted%20SNN%20baseline%20by%203.5%20percentage%20points%20in%20Top-1%20accuracy%20%28a%204.0%25%20relative%20gain%29%20and%20by%2012.0%20points%20in%20macro%20F1%20%28a%2015.2%25%20relative%20gain%29.%20On%20a%20clinical%20dataset%2C%20QANA%20achieves%2090.8%25%20Top-1%20accuracy%20and%2081.7%25%20macro%20F1%2C%20improving%20the%20strongest%20converted%20SNN%20baseline%20by%203.2%20points%20in%20Top-1%20accuracy%20%28a%203.7%25%20relative%20gain%29%20and%20by%203.6%20points%20in%20macro%20F1%20%28a%204.6%25%20relative%20gain%29.%20When%20deployed%20on%20BrainChip%20Akida%2C%20QANA%20runs%20in%201.5%20ms%20per%20image%20with%201.7%20mJ%20per%20image%2C%20corresponding%20to%2094.6%25%20lower%20latency%20and%2099.0%25%20lower%20energy%20than%20its%20GPU-based%20CNN%20implementation.&entry.1838667208=http%3A//arxiv.org/abs/2507.15958v4&entry.124074799=Read"},
{"title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents", "author": " Tse-Hsun and  Chen", "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.\n  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.", "link": "http://arxiv.org/abs/2602.04640v1", "date": "2026-02-04", "relevancy": 1.4814, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5232}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Structured%2C%20State-Aware%2C%20and%20Execution-Grounded%20Reasoning%20for%20Software%20Engineering%20Agents&body=Title%3A%20Towards%20Structured%2C%20State-Aware%2C%20and%20Execution-Grounded%20Reasoning%20for%20Software%20Engineering%20Agents%0AAuthor%3A%20%20Tse-Hsun%20and%20%20Chen%0AAbstract%3A%20Software%20Engineering%20%28SE%29%20agents%20have%20shown%20promising%20abilities%20in%20supporting%20various%20SE%20tasks.%20Current%20SE%20agents%20remain%20fundamentally%20reactive%2C%20making%20decisions%20mainly%20based%20on%20conversation%20history%20and%20the%20most%20recent%20response.%20However%2C%20this%20reactive%20design%20provides%20no%20explicit%20structure%20or%20persistent%20state%20within%20the%20agent%27s%20memory%2C%20making%20long-horizon%20reasoning%20challenging.%20As%20a%20result%2C%20SE%20agents%20struggle%20to%20maintain%20a%20coherent%20understanding%20across%20reasoning%20steps%2C%20adapt%20their%20hypotheses%20as%20new%20evidence%20emerges%2C%20or%20incorporate%20execution%20feedback%20into%20the%20mental%20reasoning%20model%20of%20the%20system%20state.%0A%20%20In%20this%20position%20paper%2C%20we%20argue%20that%2C%20to%20further%20advance%20SE%20agents%2C%20we%20need%20to%20move%20beyond%20reactive%20behavior%20toward%20a%20structured%2C%20state-aware%2C%20and%20execution-grounded%20reasoning.%20We%20outline%20how%20explicit%20structure%2C%20persistent%20and%20evolving%20state%2C%20and%20the%20integration%20of%20execution-grounded%20feedback%20can%20help%20SE%20agents%20perform%20more%20coherent%20and%20reliable%20reasoning%20in%20long-horizon%20tasks.%20We%20also%20provide%20an%20initial%20roadmap%20for%20developing%20next-generation%20SE%20agents%20that%20can%20more%20effectively%20perform%20real-world%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Structured%252C%2520State-Aware%252C%2520and%2520Execution-Grounded%2520Reasoning%2520for%2520Software%2520Engineering%2520Agents%26entry.906535625%3D%2520Tse-Hsun%2520and%2520%2520Chen%26entry.1292438233%3DSoftware%2520Engineering%2520%2528SE%2529%2520agents%2520have%2520shown%2520promising%2520abilities%2520in%2520supporting%2520various%2520SE%2520tasks.%2520Current%2520SE%2520agents%2520remain%2520fundamentally%2520reactive%252C%2520making%2520decisions%2520mainly%2520based%2520on%2520conversation%2520history%2520and%2520the%2520most%2520recent%2520response.%2520However%252C%2520this%2520reactive%2520design%2520provides%2520no%2520explicit%2520structure%2520or%2520persistent%2520state%2520within%2520the%2520agent%2527s%2520memory%252C%2520making%2520long-horizon%2520reasoning%2520challenging.%2520As%2520a%2520result%252C%2520SE%2520agents%2520struggle%2520to%2520maintain%2520a%2520coherent%2520understanding%2520across%2520reasoning%2520steps%252C%2520adapt%2520their%2520hypotheses%2520as%2520new%2520evidence%2520emerges%252C%2520or%2520incorporate%2520execution%2520feedback%2520into%2520the%2520mental%2520reasoning%2520model%2520of%2520the%2520system%2520state.%250A%2520%2520In%2520this%2520position%2520paper%252C%2520we%2520argue%2520that%252C%2520to%2520further%2520advance%2520SE%2520agents%252C%2520we%2520need%2520to%2520move%2520beyond%2520reactive%2520behavior%2520toward%2520a%2520structured%252C%2520state-aware%252C%2520and%2520execution-grounded%2520reasoning.%2520We%2520outline%2520how%2520explicit%2520structure%252C%2520persistent%2520and%2520evolving%2520state%252C%2520and%2520the%2520integration%2520of%2520execution-grounded%2520feedback%2520can%2520help%2520SE%2520agents%2520perform%2520more%2520coherent%2520and%2520reliable%2520reasoning%2520in%2520long-horizon%2520tasks.%2520We%2520also%2520provide%2520an%2520initial%2520roadmap%2520for%2520developing%2520next-generation%2520SE%2520agents%2520that%2520can%2520more%2520effectively%2520perform%2520real-world%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Structured%2C%20State-Aware%2C%20and%20Execution-Grounded%20Reasoning%20for%20Software%20Engineering%20Agents&entry.906535625=%20Tse-Hsun%20and%20%20Chen&entry.1292438233=Software%20Engineering%20%28SE%29%20agents%20have%20shown%20promising%20abilities%20in%20supporting%20various%20SE%20tasks.%20Current%20SE%20agents%20remain%20fundamentally%20reactive%2C%20making%20decisions%20mainly%20based%20on%20conversation%20history%20and%20the%20most%20recent%20response.%20However%2C%20this%20reactive%20design%20provides%20no%20explicit%20structure%20or%20persistent%20state%20within%20the%20agent%27s%20memory%2C%20making%20long-horizon%20reasoning%20challenging.%20As%20a%20result%2C%20SE%20agents%20struggle%20to%20maintain%20a%20coherent%20understanding%20across%20reasoning%20steps%2C%20adapt%20their%20hypotheses%20as%20new%20evidence%20emerges%2C%20or%20incorporate%20execution%20feedback%20into%20the%20mental%20reasoning%20model%20of%20the%20system%20state.%0A%20%20In%20this%20position%20paper%2C%20we%20argue%20that%2C%20to%20further%20advance%20SE%20agents%2C%20we%20need%20to%20move%20beyond%20reactive%20behavior%20toward%20a%20structured%2C%20state-aware%2C%20and%20execution-grounded%20reasoning.%20We%20outline%20how%20explicit%20structure%2C%20persistent%20and%20evolving%20state%2C%20and%20the%20integration%20of%20execution-grounded%20feedback%20can%20help%20SE%20agents%20perform%20more%20coherent%20and%20reliable%20reasoning%20in%20long-horizon%20tasks.%20We%20also%20provide%20an%20initial%20roadmap%20for%20developing%20next-generation%20SE%20agents%20that%20can%20more%20effectively%20perform%20real-world%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.04640v1&entry.124074799=Read"},
{"title": "MIGHTY: Hermite Spline-based Efficient Trajectory Planning", "author": "Kota Kondo and Yuwei Wu and Vijay Kumar and Jonathan P. How", "abstract": "Hard-constraint trajectory planners often rely on commercial solvers and demand substantial computational resources. Existing soft-constraint methods achieve faster computation, but either (1) decouple spatial and temporal optimization or (2) restrict the search space. To overcome these limitations, we introduce MIGHTY, a Hermite spline-based planner that performs spatiotemporal optimization while fully leveraging the continuous search space of a spline. In simulation, MIGHTY achieves a 9.3% reduction in computation time and a 13.1% reduction in travel time over state-of-the-art baselines, with a 100% success rate. In hardware, MIGHTY completes multiple high-speed flights up to 6.7 m/s in a cluttered static environment and long-duration flights with dynamically added obstacles.", "link": "http://arxiv.org/abs/2511.10822v3", "date": "2026-02-04", "relevancy": 1.8534, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4882}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4745}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIGHTY%3A%20Hermite%20Spline-based%20Efficient%20Trajectory%20Planning&body=Title%3A%20MIGHTY%3A%20Hermite%20Spline-based%20Efficient%20Trajectory%20Planning%0AAuthor%3A%20Kota%20Kondo%20and%20Yuwei%20Wu%20and%20Vijay%20Kumar%20and%20Jonathan%20P.%20How%0AAbstract%3A%20Hard-constraint%20trajectory%20planners%20often%20rely%20on%20commercial%20solvers%20and%20demand%20substantial%20computational%20resources.%20Existing%20soft-constraint%20methods%20achieve%20faster%20computation%2C%20but%20either%20%281%29%20decouple%20spatial%20and%20temporal%20optimization%20or%20%282%29%20restrict%20the%20search%20space.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20MIGHTY%2C%20a%20Hermite%20spline-based%20planner%20that%20performs%20spatiotemporal%20optimization%20while%20fully%20leveraging%20the%20continuous%20search%20space%20of%20a%20spline.%20In%20simulation%2C%20MIGHTY%20achieves%20a%209.3%25%20reduction%20in%20computation%20time%20and%20a%2013.1%25%20reduction%20in%20travel%20time%20over%20state-of-the-art%20baselines%2C%20with%20a%20100%25%20success%20rate.%20In%20hardware%2C%20MIGHTY%20completes%20multiple%20high-speed%20flights%20up%20to%206.7%20m/s%20in%20a%20cluttered%20static%20environment%20and%20long-duration%20flights%20with%20dynamically%20added%20obstacles.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10822v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIGHTY%253A%2520Hermite%2520Spline-based%2520Efficient%2520Trajectory%2520Planning%26entry.906535625%3DKota%2520Kondo%2520and%2520Yuwei%2520Wu%2520and%2520Vijay%2520Kumar%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3DHard-constraint%2520trajectory%2520planners%2520often%2520rely%2520on%2520commercial%2520solvers%2520and%2520demand%2520substantial%2520computational%2520resources.%2520Existing%2520soft-constraint%2520methods%2520achieve%2520faster%2520computation%252C%2520but%2520either%2520%25281%2529%2520decouple%2520spatial%2520and%2520temporal%2520optimization%2520or%2520%25282%2529%2520restrict%2520the%2520search%2520space.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520MIGHTY%252C%2520a%2520Hermite%2520spline-based%2520planner%2520that%2520performs%2520spatiotemporal%2520optimization%2520while%2520fully%2520leveraging%2520the%2520continuous%2520search%2520space%2520of%2520a%2520spline.%2520In%2520simulation%252C%2520MIGHTY%2520achieves%2520a%25209.3%2525%2520reduction%2520in%2520computation%2520time%2520and%2520a%252013.1%2525%2520reduction%2520in%2520travel%2520time%2520over%2520state-of-the-art%2520baselines%252C%2520with%2520a%2520100%2525%2520success%2520rate.%2520In%2520hardware%252C%2520MIGHTY%2520completes%2520multiple%2520high-speed%2520flights%2520up%2520to%25206.7%2520m/s%2520in%2520a%2520cluttered%2520static%2520environment%2520and%2520long-duration%2520flights%2520with%2520dynamically%2520added%2520obstacles.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10822v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIGHTY%3A%20Hermite%20Spline-based%20Efficient%20Trajectory%20Planning&entry.906535625=Kota%20Kondo%20and%20Yuwei%20Wu%20and%20Vijay%20Kumar%20and%20Jonathan%20P.%20How&entry.1292438233=Hard-constraint%20trajectory%20planners%20often%20rely%20on%20commercial%20solvers%20and%20demand%20substantial%20computational%20resources.%20Existing%20soft-constraint%20methods%20achieve%20faster%20computation%2C%20but%20either%20%281%29%20decouple%20spatial%20and%20temporal%20optimization%20or%20%282%29%20restrict%20the%20search%20space.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20MIGHTY%2C%20a%20Hermite%20spline-based%20planner%20that%20performs%20spatiotemporal%20optimization%20while%20fully%20leveraging%20the%20continuous%20search%20space%20of%20a%20spline.%20In%20simulation%2C%20MIGHTY%20achieves%20a%209.3%25%20reduction%20in%20computation%20time%20and%20a%2013.1%25%20reduction%20in%20travel%20time%20over%20state-of-the-art%20baselines%2C%20with%20a%20100%25%20success%20rate.%20In%20hardware%2C%20MIGHTY%20completes%20multiple%20high-speed%20flights%20up%20to%206.7%20m/s%20in%20a%20cluttered%20static%20environment%20and%20long-duration%20flights%20with%20dynamically%20added%20obstacles.&entry.1838667208=http%3A//arxiv.org/abs/2511.10822v3&entry.124074799=Read"},
{"title": "Contrastive Continual Learning for Model Adaptability in Internet of Things", "author": "Ajesh Koyatan Chathoth", "abstract": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \\emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.", "link": "http://arxiv.org/abs/2602.04881v1", "date": "2026-02-04", "relevancy": 1.4699, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4814}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Continual%20Learning%20for%20Model%20Adaptability%20in%20Internet%20of%20Things&body=Title%3A%20Contrastive%20Continual%20Learning%20for%20Model%20Adaptability%20in%20Internet%20of%20Things%0AAuthor%3A%20Ajesh%20Koyatan%20Chathoth%0AAbstract%3A%20Internet%20of%20Things%20%28IoT%29%20deployments%20operate%20in%20nonstationary%2C%20dynamic%20environments%20where%20factors%20such%20as%20sensor%20drift%2C%20evolving%20user%20behavior%2C%20and%20heterogeneous%20user%20privacy%20requirements%20can%20affect%20application%20utility.%20Continual%20learning%20%28CL%29%20addresses%20this%20by%20adapting%20models%20over%20time%20without%20catastrophic%20forgetting.%20Meanwhile%2C%20contrastive%20learning%20has%20emerged%20as%20a%20powerful%20representation-learning%20paradigm%20that%20improves%20robustness%20and%20sample%20efficiency%20in%20a%20self-supervised%20manner.%20This%20paper%20reviews%20the%20usage%20of%20%5Cemph%7Bcontrastive%20continual%20learning%7D%20%28CCL%29%20for%20IoT%2C%20connecting%20algorithmic%20design%20%28replay%2C%20regularization%2C%20distillation%2C%20prompts%29%20with%20IoT%20system%20realities%20%28TinyML%20constraints%2C%20intermittent%20connectivity%2C%20privacy%29.%20We%20present%20a%20unifying%20problem%20formulation%2C%20derive%20common%20objectives%20that%20blend%20contrastive%20and%20distillation%20losses%2C%20propose%20an%20IoT-oriented%20reference%20architecture%20for%20on-device%2C%20edge%2C%20and%20cloud-based%20CCL%2C%20and%20provide%20guidance%20on%20evaluation%20protocols%20and%20metrics.%20Finally%2C%20we%20highlight%20open%20unique%20challenges%20with%20respect%20to%20the%20IoT%20domain%2C%20such%20as%20spanning%20tabular%20and%20streaming%20IoT%20data%2C%20concept%20drift%2C%20federated%20settings%2C%20and%20energy-aware%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Continual%2520Learning%2520for%2520Model%2520Adaptability%2520in%2520Internet%2520of%2520Things%26entry.906535625%3DAjesh%2520Koyatan%2520Chathoth%26entry.1292438233%3DInternet%2520of%2520Things%2520%2528IoT%2529%2520deployments%2520operate%2520in%2520nonstationary%252C%2520dynamic%2520environments%2520where%2520factors%2520such%2520as%2520sensor%2520drift%252C%2520evolving%2520user%2520behavior%252C%2520and%2520heterogeneous%2520user%2520privacy%2520requirements%2520can%2520affect%2520application%2520utility.%2520Continual%2520learning%2520%2528CL%2529%2520addresses%2520this%2520by%2520adapting%2520models%2520over%2520time%2520without%2520catastrophic%2520forgetting.%2520Meanwhile%252C%2520contrastive%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520representation-learning%2520paradigm%2520that%2520improves%2520robustness%2520and%2520sample%2520efficiency%2520in%2520a%2520self-supervised%2520manner.%2520This%2520paper%2520reviews%2520the%2520usage%2520of%2520%255Cemph%257Bcontrastive%2520continual%2520learning%257D%2520%2528CCL%2529%2520for%2520IoT%252C%2520connecting%2520algorithmic%2520design%2520%2528replay%252C%2520regularization%252C%2520distillation%252C%2520prompts%2529%2520with%2520IoT%2520system%2520realities%2520%2528TinyML%2520constraints%252C%2520intermittent%2520connectivity%252C%2520privacy%2529.%2520We%2520present%2520a%2520unifying%2520problem%2520formulation%252C%2520derive%2520common%2520objectives%2520that%2520blend%2520contrastive%2520and%2520distillation%2520losses%252C%2520propose%2520an%2520IoT-oriented%2520reference%2520architecture%2520for%2520on-device%252C%2520edge%252C%2520and%2520cloud-based%2520CCL%252C%2520and%2520provide%2520guidance%2520on%2520evaluation%2520protocols%2520and%2520metrics.%2520Finally%252C%2520we%2520highlight%2520open%2520unique%2520challenges%2520with%2520respect%2520to%2520the%2520IoT%2520domain%252C%2520such%2520as%2520spanning%2520tabular%2520and%2520streaming%2520IoT%2520data%252C%2520concept%2520drift%252C%2520federated%2520settings%252C%2520and%2520energy-aware%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Continual%20Learning%20for%20Model%20Adaptability%20in%20Internet%20of%20Things&entry.906535625=Ajesh%20Koyatan%20Chathoth&entry.1292438233=Internet%20of%20Things%20%28IoT%29%20deployments%20operate%20in%20nonstationary%2C%20dynamic%20environments%20where%20factors%20such%20as%20sensor%20drift%2C%20evolving%20user%20behavior%2C%20and%20heterogeneous%20user%20privacy%20requirements%20can%20affect%20application%20utility.%20Continual%20learning%20%28CL%29%20addresses%20this%20by%20adapting%20models%20over%20time%20without%20catastrophic%20forgetting.%20Meanwhile%2C%20contrastive%20learning%20has%20emerged%20as%20a%20powerful%20representation-learning%20paradigm%20that%20improves%20robustness%20and%20sample%20efficiency%20in%20a%20self-supervised%20manner.%20This%20paper%20reviews%20the%20usage%20of%20%5Cemph%7Bcontrastive%20continual%20learning%7D%20%28CCL%29%20for%20IoT%2C%20connecting%20algorithmic%20design%20%28replay%2C%20regularization%2C%20distillation%2C%20prompts%29%20with%20IoT%20system%20realities%20%28TinyML%20constraints%2C%20intermittent%20connectivity%2C%20privacy%29.%20We%20present%20a%20unifying%20problem%20formulation%2C%20derive%20common%20objectives%20that%20blend%20contrastive%20and%20distillation%20losses%2C%20propose%20an%20IoT-oriented%20reference%20architecture%20for%20on-device%2C%20edge%2C%20and%20cloud-based%20CCL%2C%20and%20provide%20guidance%20on%20evaluation%20protocols%20and%20metrics.%20Finally%2C%20we%20highlight%20open%20unique%20challenges%20with%20respect%20to%20the%20IoT%20domain%2C%20such%20as%20spanning%20tabular%20and%20streaming%20IoT%20data%2C%20concept%20drift%2C%20federated%20settings%2C%20and%20energy-aware%20training.&entry.1838667208=http%3A//arxiv.org/abs/2602.04881v1&entry.124074799=Read"},
{"title": "Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging", "author": "Sameh Khattab and Jean-Philippe Corbeil and Osman Alperen Kora\u015f and Amin Dada and Julian Friedrich and Fran\u00e7ois Beaulieu and Paul Vozila and Jens Kleesiek", "abstract": "Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\\% (average 7.5\\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.", "link": "http://arxiv.org/abs/2602.04731v1", "date": "2026-02-04", "relevancy": 2.0769, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20Finetuning%2C%20Better%20Retrieval%3A%20Rethinking%20LLM%20Adaptation%20for%20Biomedical%20Retrievers%20via%20Synthetic%20Data%20and%20Model%20Merging&body=Title%3A%20Less%20Finetuning%2C%20Better%20Retrieval%3A%20Rethinking%20LLM%20Adaptation%20for%20Biomedical%20Retrievers%20via%20Synthetic%20Data%20and%20Model%20Merging%0AAuthor%3A%20Sameh%20Khattab%20and%20Jean-Philippe%20Corbeil%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Amin%20Dada%20and%20Julian%20Friedrich%20and%20Fran%C3%A7ois%20Beaulieu%20and%20Paul%20Vozila%20and%20Jens%20Kleesiek%0AAbstract%3A%20Retrieval-augmented%20generation%20%28RAG%29%20has%20become%20the%20backbone%20of%20grounding%20Large%20Language%20Models%20%28LLMs%29%2C%20improving%20knowledge%20updates%20and%20reducing%20hallucinations.%20Recently%2C%20LLM-based%20retriever%20models%20have%20shown%20state-of-the-art%20performance%20for%20RAG%20applications.%20However%2C%20several%20technical%20aspects%20remain%20underexplored%20on%20how%20to%20adapt%20general-purpose%20LLMs%20into%20effective%20domain-specific%20retrievers%2C%20especially%20in%20specialized%20domains%20such%20as%20biomedicine.%20We%20present%20Synthesize-Train-Merge%20%28STM%29%2C%20a%20modular%20framework%20that%20enhances%20decoder-only%20LLMs%20with%20synthetic%20hard%20negatives%2C%20retrieval%20prompt%20optimization%2C%20and%20model%20merging.%20Experiments%20on%20a%20subset%20of%2012%20medical%20and%20general%20tasks%20from%20the%20MTEB%20benchmark%20show%20STM%20boosts%20task-specific%20experts%20by%20up%20to%2023.5%5C%25%20%28average%207.5%5C%25%29%20and%20produces%20merged%20models%20that%20outperform%20both%20single%20experts%20and%20strong%20baselines%20without%20extensive%20pretraining.%20Our%20results%20demonstrate%20a%20scalable%2C%20efficient%20path%20for%20turning%20general%20LLMs%20into%20high-performing%2C%20domain-specialized%20retrievers%2C%20preserving%20general-domain%20capabilities%20while%20excelling%20on%20specialized%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520Finetuning%252C%2520Better%2520Retrieval%253A%2520Rethinking%2520LLM%2520Adaptation%2520for%2520Biomedical%2520Retrievers%2520via%2520Synthetic%2520Data%2520and%2520Model%2520Merging%26entry.906535625%3DSameh%2520Khattab%2520and%2520Jean-Philippe%2520Corbeil%2520and%2520Osman%2520Alperen%2520Kora%25C5%259F%2520and%2520Amin%2520Dada%2520and%2520Julian%2520Friedrich%2520and%2520Fran%25C3%25A7ois%2520Beaulieu%2520and%2520Paul%2520Vozila%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3DRetrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520become%2520the%2520backbone%2520of%2520grounding%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520improving%2520knowledge%2520updates%2520and%2520reducing%2520hallucinations.%2520Recently%252C%2520LLM-based%2520retriever%2520models%2520have%2520shown%2520state-of-the-art%2520performance%2520for%2520RAG%2520applications.%2520However%252C%2520several%2520technical%2520aspects%2520remain%2520underexplored%2520on%2520how%2520to%2520adapt%2520general-purpose%2520LLMs%2520into%2520effective%2520domain-specific%2520retrievers%252C%2520especially%2520in%2520specialized%2520domains%2520such%2520as%2520biomedicine.%2520We%2520present%2520Synthesize-Train-Merge%2520%2528STM%2529%252C%2520a%2520modular%2520framework%2520that%2520enhances%2520decoder-only%2520LLMs%2520with%2520synthetic%2520hard%2520negatives%252C%2520retrieval%2520prompt%2520optimization%252C%2520and%2520model%2520merging.%2520Experiments%2520on%2520a%2520subset%2520of%252012%2520medical%2520and%2520general%2520tasks%2520from%2520the%2520MTEB%2520benchmark%2520show%2520STM%2520boosts%2520task-specific%2520experts%2520by%2520up%2520to%252023.5%255C%2525%2520%2528average%25207.5%255C%2525%2529%2520and%2520produces%2520merged%2520models%2520that%2520outperform%2520both%2520single%2520experts%2520and%2520strong%2520baselines%2520without%2520extensive%2520pretraining.%2520Our%2520results%2520demonstrate%2520a%2520scalable%252C%2520efficient%2520path%2520for%2520turning%2520general%2520LLMs%2520into%2520high-performing%252C%2520domain-specialized%2520retrievers%252C%2520preserving%2520general-domain%2520capabilities%2520while%2520excelling%2520on%2520specialized%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20Finetuning%2C%20Better%20Retrieval%3A%20Rethinking%20LLM%20Adaptation%20for%20Biomedical%20Retrievers%20via%20Synthetic%20Data%20and%20Model%20Merging&entry.906535625=Sameh%20Khattab%20and%20Jean-Philippe%20Corbeil%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Amin%20Dada%20and%20Julian%20Friedrich%20and%20Fran%C3%A7ois%20Beaulieu%20and%20Paul%20Vozila%20and%20Jens%20Kleesiek&entry.1292438233=Retrieval-augmented%20generation%20%28RAG%29%20has%20become%20the%20backbone%20of%20grounding%20Large%20Language%20Models%20%28LLMs%29%2C%20improving%20knowledge%20updates%20and%20reducing%20hallucinations.%20Recently%2C%20LLM-based%20retriever%20models%20have%20shown%20state-of-the-art%20performance%20for%20RAG%20applications.%20However%2C%20several%20technical%20aspects%20remain%20underexplored%20on%20how%20to%20adapt%20general-purpose%20LLMs%20into%20effective%20domain-specific%20retrievers%2C%20especially%20in%20specialized%20domains%20such%20as%20biomedicine.%20We%20present%20Synthesize-Train-Merge%20%28STM%29%2C%20a%20modular%20framework%20that%20enhances%20decoder-only%20LLMs%20with%20synthetic%20hard%20negatives%2C%20retrieval%20prompt%20optimization%2C%20and%20model%20merging.%20Experiments%20on%20a%20subset%20of%2012%20medical%20and%20general%20tasks%20from%20the%20MTEB%20benchmark%20show%20STM%20boosts%20task-specific%20experts%20by%20up%20to%2023.5%5C%25%20%28average%207.5%5C%25%29%20and%20produces%20merged%20models%20that%20outperform%20both%20single%20experts%20and%20strong%20baselines%20without%20extensive%20pretraining.%20Our%20results%20demonstrate%20a%20scalable%2C%20efficient%20path%20for%20turning%20general%20LLMs%20into%20high-performing%2C%20domain-specialized%20retrievers%2C%20preserving%20general-domain%20capabilities%20while%20excelling%20on%20specialized%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.04731v1&entry.124074799=Read"},
{"title": "metabeta -- A fast neural model for Bayesian mixed-effects regression", "author": "Alex Kipnis and Marcel Binz and Eric Schulz", "abstract": "Hierarchical data with multiple observations per group is ubiquitous in empirical sciences and is often analyzed using mixed-effects regression. In such models, Bayesian inference gives an estimate of uncertainty but is analytically intractable and requires costly approximation using Markov Chain Monte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of computation from inference time to pre-training time, amortizing over simulated datasets with known ground truth targets. We propose metabeta, a neural network model for Bayesian mixed-effects regression. Using simulated and real data, we show that it reaches stable and comparable performance to MCMC-based parameter estimation at a fraction of the usually required time, enabling new use cases for Bayesian mixed-effects modeling.", "link": "http://arxiv.org/abs/2510.07473v2", "date": "2026-02-04", "relevancy": 1.9994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.577}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20metabeta%20--%20A%20fast%20neural%20model%20for%20Bayesian%20mixed-effects%20regression&body=Title%3A%20metabeta%20--%20A%20fast%20neural%20model%20for%20Bayesian%20mixed-effects%20regression%0AAuthor%3A%20Alex%20Kipnis%20and%20Marcel%20Binz%20and%20Eric%20Schulz%0AAbstract%3A%20Hierarchical%20data%20with%20multiple%20observations%20per%20group%20is%20ubiquitous%20in%20empirical%20sciences%20and%20is%20often%20analyzed%20using%20mixed-effects%20regression.%20In%20such%20models%2C%20Bayesian%20inference%20gives%20an%20estimate%20of%20uncertainty%20but%20is%20analytically%20intractable%20and%20requires%20costly%20approximation%20using%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20methods.%20Neural%20posterior%20estimation%20shifts%20the%20bulk%20of%20computation%20from%20inference%20time%20to%20pre-training%20time%2C%20amortizing%20over%20simulated%20datasets%20with%20known%20ground%20truth%20targets.%20We%20propose%20metabeta%2C%20a%20neural%20network%20model%20for%20Bayesian%20mixed-effects%20regression.%20Using%20simulated%20and%20real%20data%2C%20we%20show%20that%20it%20reaches%20stable%20and%20comparable%20performance%20to%20MCMC-based%20parameter%20estimation%20at%20a%20fraction%20of%20the%20usually%20required%20time%2C%20enabling%20new%20use%20cases%20for%20Bayesian%20mixed-effects%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmetabeta%2520--%2520A%2520fast%2520neural%2520model%2520for%2520Bayesian%2520mixed-effects%2520regression%26entry.906535625%3DAlex%2520Kipnis%2520and%2520Marcel%2520Binz%2520and%2520Eric%2520Schulz%26entry.1292438233%3DHierarchical%2520data%2520with%2520multiple%2520observations%2520per%2520group%2520is%2520ubiquitous%2520in%2520empirical%2520sciences%2520and%2520is%2520often%2520analyzed%2520using%2520mixed-effects%2520regression.%2520In%2520such%2520models%252C%2520Bayesian%2520inference%2520gives%2520an%2520estimate%2520of%2520uncertainty%2520but%2520is%2520analytically%2520intractable%2520and%2520requires%2520costly%2520approximation%2520using%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520methods.%2520Neural%2520posterior%2520estimation%2520shifts%2520the%2520bulk%2520of%2520computation%2520from%2520inference%2520time%2520to%2520pre-training%2520time%252C%2520amortizing%2520over%2520simulated%2520datasets%2520with%2520known%2520ground%2520truth%2520targets.%2520We%2520propose%2520metabeta%252C%2520a%2520neural%2520network%2520model%2520for%2520Bayesian%2520mixed-effects%2520regression.%2520Using%2520simulated%2520and%2520real%2520data%252C%2520we%2520show%2520that%2520it%2520reaches%2520stable%2520and%2520comparable%2520performance%2520to%2520MCMC-based%2520parameter%2520estimation%2520at%2520a%2520fraction%2520of%2520the%2520usually%2520required%2520time%252C%2520enabling%2520new%2520use%2520cases%2520for%2520Bayesian%2520mixed-effects%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=metabeta%20--%20A%20fast%20neural%20model%20for%20Bayesian%20mixed-effects%20regression&entry.906535625=Alex%20Kipnis%20and%20Marcel%20Binz%20and%20Eric%20Schulz&entry.1292438233=Hierarchical%20data%20with%20multiple%20observations%20per%20group%20is%20ubiquitous%20in%20empirical%20sciences%20and%20is%20often%20analyzed%20using%20mixed-effects%20regression.%20In%20such%20models%2C%20Bayesian%20inference%20gives%20an%20estimate%20of%20uncertainty%20but%20is%20analytically%20intractable%20and%20requires%20costly%20approximation%20using%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20methods.%20Neural%20posterior%20estimation%20shifts%20the%20bulk%20of%20computation%20from%20inference%20time%20to%20pre-training%20time%2C%20amortizing%20over%20simulated%20datasets%20with%20known%20ground%20truth%20targets.%20We%20propose%20metabeta%2C%20a%20neural%20network%20model%20for%20Bayesian%20mixed-effects%20regression.%20Using%20simulated%20and%20real%20data%2C%20we%20show%20that%20it%20reaches%20stable%20and%20comparable%20performance%20to%20MCMC-based%20parameter%20estimation%20at%20a%20fraction%20of%20the%20usually%20required%20time%2C%20enabling%20new%20use%20cases%20for%20Bayesian%20mixed-effects%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2510.07473v2&entry.124074799=Read"},
{"title": "TxRay: Agentic Postmortem of Live Blockchain Attacks", "author": "Ziyue Wang and Jiangshan Yu and Kaihua Qin and Dawn Song and Arthur Gervais and Liyi Zhou", "abstract": "Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.\n  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.\n  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +22.9pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.", "link": "http://arxiv.org/abs/2602.01317v2", "date": "2026-02-04", "relevancy": 1.5731, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3995}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TxRay%3A%20Agentic%20Postmortem%20of%20Live%20Blockchain%20Attacks&body=Title%3A%20TxRay%3A%20Agentic%20Postmortem%20of%20Live%20Blockchain%20Attacks%0AAuthor%3A%20Ziyue%20Wang%20and%20Jiangshan%20Yu%20and%20Kaihua%20Qin%20and%20Dawn%20Song%20and%20Arthur%20Gervais%20and%20Liyi%20Zhou%0AAbstract%3A%20Decentralized%20Finance%20%28DeFi%29%20has%20turned%20blockchains%20into%20financial%20infrastructure%2C%20allowing%20anyone%20to%20trade%2C%20lend%2C%20and%20build%20protocols%20without%20intermediaries%2C%20but%20this%20openness%20exposes%20pools%20of%20value%20controlled%20by%20code.%20Within%20five%20years%2C%20the%20DeFi%20ecosystem%20has%20lost%20over%2015.75B%20USD%20to%20reported%20exploits.%20Many%20exploits%20arise%20from%20permissionless%20opportunities%20that%20any%20participant%20can%20trigger%20using%20only%20public%20state%20and%20standard%20interfaces%2C%20which%20we%20call%20Anyone-Can-Take%20%28ACT%29%20opportunities.%20Despite%20on-chain%20transparency%2C%20postmortem%20analysis%20remains%20slow%20and%20manual%3A%20investigations%20start%20from%20limited%20evidence%2C%20sometimes%20only%20a%20single%20transaction%20hash%2C%20and%20must%20reconstruct%20the%20exploit%20lifecycle%20by%20recovering%20related%20transactions%2C%20contract%20code%2C%20and%20state%20dependencies.%0A%20%20We%20present%20TxRay%2C%20a%20Large%20Language%20Model%20%28LLM%29%20agentic%20postmortem%20system%20that%20uses%20tool%20calls%20to%20reconstruct%20live%20ACT%20attacks%20from%20limited%20evidence.%20Starting%20from%20one%20or%20more%20seed%20transactions%2C%20TxRay%20recovers%20the%20exploit%20lifecycle%2C%20derives%20an%20evidence-backed%20root%20cause%2C%20and%20generates%20a%20runnable%2C%20self-contained%20Proof%20of%20Concept%20%28PoC%29%20that%20deterministically%20reproduces%20the%20incident.%20TxRay%20self-checks%20postmortems%20by%20encoding%20incident-specific%20semantic%20oracles%20as%20executable%20assertions.%0A%20%20To%20evaluate%20PoC%20correctness%20and%20quality%2C%20we%20develop%20PoCEvaluator%2C%20an%20independent%20agentic%20execution-and-review%20evaluator.%20On%20114%20incidents%20from%20DeFiHackLabs%2C%20TxRay%20produces%20an%20expert-aligned%20root%20cause%20and%20an%20executable%20PoC%20for%20105%20incidents%2C%20achieving%2092.11%25%20end-to-end%20reproduction.%20Under%20PoCEvaluator%2C%2098.1%25%20of%20TxRay%20PoCs%20avoid%20hard-coding%20attacker%20addresses%2C%20a%20%2B22.9pp%20lift%20over%20DeFiHackLabs.%20In%20a%20live%20deployment%2C%20TxRay%20delivers%20validated%20root%20causes%20in%2040%20minutes%20and%20PoCs%20in%2059%20minutes%20at%20median%20latency.%20TxRay%27s%20oracle-validated%20PoCs%20enable%20attack%20imitation%2C%20improving%20coverage%20by%2015.6%25%20and%2065.5%25%20over%20STING%20and%20APE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTxRay%253A%2520Agentic%2520Postmortem%2520of%2520Live%2520Blockchain%2520Attacks%26entry.906535625%3DZiyue%2520Wang%2520and%2520Jiangshan%2520Yu%2520and%2520Kaihua%2520Qin%2520and%2520Dawn%2520Song%2520and%2520Arthur%2520Gervais%2520and%2520Liyi%2520Zhou%26entry.1292438233%3DDecentralized%2520Finance%2520%2528DeFi%2529%2520has%2520turned%2520blockchains%2520into%2520financial%2520infrastructure%252C%2520allowing%2520anyone%2520to%2520trade%252C%2520lend%252C%2520and%2520build%2520protocols%2520without%2520intermediaries%252C%2520but%2520this%2520openness%2520exposes%2520pools%2520of%2520value%2520controlled%2520by%2520code.%2520Within%2520five%2520years%252C%2520the%2520DeFi%2520ecosystem%2520has%2520lost%2520over%252015.75B%2520USD%2520to%2520reported%2520exploits.%2520Many%2520exploits%2520arise%2520from%2520permissionless%2520opportunities%2520that%2520any%2520participant%2520can%2520trigger%2520using%2520only%2520public%2520state%2520and%2520standard%2520interfaces%252C%2520which%2520we%2520call%2520Anyone-Can-Take%2520%2528ACT%2529%2520opportunities.%2520Despite%2520on-chain%2520transparency%252C%2520postmortem%2520analysis%2520remains%2520slow%2520and%2520manual%253A%2520investigations%2520start%2520from%2520limited%2520evidence%252C%2520sometimes%2520only%2520a%2520single%2520transaction%2520hash%252C%2520and%2520must%2520reconstruct%2520the%2520exploit%2520lifecycle%2520by%2520recovering%2520related%2520transactions%252C%2520contract%2520code%252C%2520and%2520state%2520dependencies.%250A%2520%2520We%2520present%2520TxRay%252C%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agentic%2520postmortem%2520system%2520that%2520uses%2520tool%2520calls%2520to%2520reconstruct%2520live%2520ACT%2520attacks%2520from%2520limited%2520evidence.%2520Starting%2520from%2520one%2520or%2520more%2520seed%2520transactions%252C%2520TxRay%2520recovers%2520the%2520exploit%2520lifecycle%252C%2520derives%2520an%2520evidence-backed%2520root%2520cause%252C%2520and%2520generates%2520a%2520runnable%252C%2520self-contained%2520Proof%2520of%2520Concept%2520%2528PoC%2529%2520that%2520deterministically%2520reproduces%2520the%2520incident.%2520TxRay%2520self-checks%2520postmortems%2520by%2520encoding%2520incident-specific%2520semantic%2520oracles%2520as%2520executable%2520assertions.%250A%2520%2520To%2520evaluate%2520PoC%2520correctness%2520and%2520quality%252C%2520we%2520develop%2520PoCEvaluator%252C%2520an%2520independent%2520agentic%2520execution-and-review%2520evaluator.%2520On%2520114%2520incidents%2520from%2520DeFiHackLabs%252C%2520TxRay%2520produces%2520an%2520expert-aligned%2520root%2520cause%2520and%2520an%2520executable%2520PoC%2520for%2520105%2520incidents%252C%2520achieving%252092.11%2525%2520end-to-end%2520reproduction.%2520Under%2520PoCEvaluator%252C%252098.1%2525%2520of%2520TxRay%2520PoCs%2520avoid%2520hard-coding%2520attacker%2520addresses%252C%2520a%2520%252B22.9pp%2520lift%2520over%2520DeFiHackLabs.%2520In%2520a%2520live%2520deployment%252C%2520TxRay%2520delivers%2520validated%2520root%2520causes%2520in%252040%2520minutes%2520and%2520PoCs%2520in%252059%2520minutes%2520at%2520median%2520latency.%2520TxRay%2527s%2520oracle-validated%2520PoCs%2520enable%2520attack%2520imitation%252C%2520improving%2520coverage%2520by%252015.6%2525%2520and%252065.5%2525%2520over%2520STING%2520and%2520APE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TxRay%3A%20Agentic%20Postmortem%20of%20Live%20Blockchain%20Attacks&entry.906535625=Ziyue%20Wang%20and%20Jiangshan%20Yu%20and%20Kaihua%20Qin%20and%20Dawn%20Song%20and%20Arthur%20Gervais%20and%20Liyi%20Zhou&entry.1292438233=Decentralized%20Finance%20%28DeFi%29%20has%20turned%20blockchains%20into%20financial%20infrastructure%2C%20allowing%20anyone%20to%20trade%2C%20lend%2C%20and%20build%20protocols%20without%20intermediaries%2C%20but%20this%20openness%20exposes%20pools%20of%20value%20controlled%20by%20code.%20Within%20five%20years%2C%20the%20DeFi%20ecosystem%20has%20lost%20over%2015.75B%20USD%20to%20reported%20exploits.%20Many%20exploits%20arise%20from%20permissionless%20opportunities%20that%20any%20participant%20can%20trigger%20using%20only%20public%20state%20and%20standard%20interfaces%2C%20which%20we%20call%20Anyone-Can-Take%20%28ACT%29%20opportunities.%20Despite%20on-chain%20transparency%2C%20postmortem%20analysis%20remains%20slow%20and%20manual%3A%20investigations%20start%20from%20limited%20evidence%2C%20sometimes%20only%20a%20single%20transaction%20hash%2C%20and%20must%20reconstruct%20the%20exploit%20lifecycle%20by%20recovering%20related%20transactions%2C%20contract%20code%2C%20and%20state%20dependencies.%0A%20%20We%20present%20TxRay%2C%20a%20Large%20Language%20Model%20%28LLM%29%20agentic%20postmortem%20system%20that%20uses%20tool%20calls%20to%20reconstruct%20live%20ACT%20attacks%20from%20limited%20evidence.%20Starting%20from%20one%20or%20more%20seed%20transactions%2C%20TxRay%20recovers%20the%20exploit%20lifecycle%2C%20derives%20an%20evidence-backed%20root%20cause%2C%20and%20generates%20a%20runnable%2C%20self-contained%20Proof%20of%20Concept%20%28PoC%29%20that%20deterministically%20reproduces%20the%20incident.%20TxRay%20self-checks%20postmortems%20by%20encoding%20incident-specific%20semantic%20oracles%20as%20executable%20assertions.%0A%20%20To%20evaluate%20PoC%20correctness%20and%20quality%2C%20we%20develop%20PoCEvaluator%2C%20an%20independent%20agentic%20execution-and-review%20evaluator.%20On%20114%20incidents%20from%20DeFiHackLabs%2C%20TxRay%20produces%20an%20expert-aligned%20root%20cause%20and%20an%20executable%20PoC%20for%20105%20incidents%2C%20achieving%2092.11%25%20end-to-end%20reproduction.%20Under%20PoCEvaluator%2C%2098.1%25%20of%20TxRay%20PoCs%20avoid%20hard-coding%20attacker%20addresses%2C%20a%20%2B22.9pp%20lift%20over%20DeFiHackLabs.%20In%20a%20live%20deployment%2C%20TxRay%20delivers%20validated%20root%20causes%20in%2040%20minutes%20and%20PoCs%20in%2059%20minutes%20at%20median%20latency.%20TxRay%27s%20oracle-validated%20PoCs%20enable%20attack%20imitation%2C%20improving%20coverage%20by%2015.6%25%20and%2065.5%25%20over%20STING%20and%20APE.&entry.1838667208=http%3A//arxiv.org/abs/2602.01317v2&entry.124074799=Read"},
{"title": "Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization", "author": "Farzia Hossain and Samanta Ghosh and Shahida Begum and B. M. Shahria Alam and Mohammad Tahmid Noor and Md Parvez Mia and Nishat Tasnim Niloy", "abstract": "Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.", "link": "http://arxiv.org/abs/2602.04820v1", "date": "2026-02-04", "relevancy": 1.9769, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4858}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Reliable%20and%20Explainable%20Nail%20Disease%20Classification%3A%20Leveraging%20Adversarial%20Training%20and%20Grad-CAM%20Visualization&body=Title%3A%20Toward%20Reliable%20and%20Explainable%20Nail%20Disease%20Classification%3A%20Leveraging%20Adversarial%20Training%20and%20Grad-CAM%20Visualization%0AAuthor%3A%20Farzia%20Hossain%20and%20Samanta%20Ghosh%20and%20Shahida%20Begum%20and%20B.%20M.%20Shahria%20Alam%20and%20Mohammad%20Tahmid%20Noor%20and%20Md%20Parvez%20Mia%20and%20Nishat%20Tasnim%20Niloy%0AAbstract%3A%20Human%20nail%20diseases%20are%20gradually%20observed%20over%20all%20age%20groups%2C%20especially%20among%20older%20individuals%2C%20often%20going%20ignored%20until%20they%20become%20severe.%20Early%20detection%20and%20accurate%20diagnosis%20of%20such%20conditions%20are%20important%20because%20they%20sometimes%20reveal%20our%20body%27s%20health%20problems.%20But%20it%20is%20challenging%20due%20to%20the%20inferred%20visual%20differences%20between%20disease%20types.%20This%20paper%20presents%20a%20machine%20learning-based%20model%20for%20automated%20classification%20of%20nail%20diseases%20based%20on%20a%20publicly%20available%20dataset%2C%20which%20contains%203%2C835%20images%20scaling%20six%20categories.%20In%20224x224%20pixels%2C%20all%20images%20were%20resized%20to%20ensure%20consistency.%20To%20evaluate%20performance%2C%20four%20well-known%20CNN%20models-InceptionV3%2C%20DenseNet201%2C%20EfficientNetV2%2C%20and%20ResNet50%20were%20trained%20and%20analyzed.%20Among%20these%2C%20InceptionV3%20outperformed%20the%20others%20with%20an%20accuracy%20of%2095.57%25%2C%20while%20DenseNet201%20came%20next%20with%2094.79%25.%20To%20make%20the%20model%20stronger%20and%20less%20likely%20to%20make%20mistakes%20on%20tricky%20or%20noisy%20images%2C%20we%20used%20adversarial%20training.%20To%20help%20understand%20how%20the%20model%20makes%20decisions%2C%20we%20used%20SHAP%20to%20highlight%20important%20features%20in%20the%20predictions.%20This%20system%20could%20be%20a%20helpful%20support%20for%20doctors%2C%20making%20nail%20disease%20diagnosis%20more%20accurate%20and%20faster.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Reliable%2520and%2520Explainable%2520Nail%2520Disease%2520Classification%253A%2520Leveraging%2520Adversarial%2520Training%2520and%2520Grad-CAM%2520Visualization%26entry.906535625%3DFarzia%2520Hossain%2520and%2520Samanta%2520Ghosh%2520and%2520Shahida%2520Begum%2520and%2520B.%2520M.%2520Shahria%2520Alam%2520and%2520Mohammad%2520Tahmid%2520Noor%2520and%2520Md%2520Parvez%2520Mia%2520and%2520Nishat%2520Tasnim%2520Niloy%26entry.1292438233%3DHuman%2520nail%2520diseases%2520are%2520gradually%2520observed%2520over%2520all%2520age%2520groups%252C%2520especially%2520among%2520older%2520individuals%252C%2520often%2520going%2520ignored%2520until%2520they%2520become%2520severe.%2520Early%2520detection%2520and%2520accurate%2520diagnosis%2520of%2520such%2520conditions%2520are%2520important%2520because%2520they%2520sometimes%2520reveal%2520our%2520body%2527s%2520health%2520problems.%2520But%2520it%2520is%2520challenging%2520due%2520to%2520the%2520inferred%2520visual%2520differences%2520between%2520disease%2520types.%2520This%2520paper%2520presents%2520a%2520machine%2520learning-based%2520model%2520for%2520automated%2520classification%2520of%2520nail%2520diseases%2520based%2520on%2520a%2520publicly%2520available%2520dataset%252C%2520which%2520contains%25203%252C835%2520images%2520scaling%2520six%2520categories.%2520In%2520224x224%2520pixels%252C%2520all%2520images%2520were%2520resized%2520to%2520ensure%2520consistency.%2520To%2520evaluate%2520performance%252C%2520four%2520well-known%2520CNN%2520models-InceptionV3%252C%2520DenseNet201%252C%2520EfficientNetV2%252C%2520and%2520ResNet50%2520were%2520trained%2520and%2520analyzed.%2520Among%2520these%252C%2520InceptionV3%2520outperformed%2520the%2520others%2520with%2520an%2520accuracy%2520of%252095.57%2525%252C%2520while%2520DenseNet201%2520came%2520next%2520with%252094.79%2525.%2520To%2520make%2520the%2520model%2520stronger%2520and%2520less%2520likely%2520to%2520make%2520mistakes%2520on%2520tricky%2520or%2520noisy%2520images%252C%2520we%2520used%2520adversarial%2520training.%2520To%2520help%2520understand%2520how%2520the%2520model%2520makes%2520decisions%252C%2520we%2520used%2520SHAP%2520to%2520highlight%2520important%2520features%2520in%2520the%2520predictions.%2520This%2520system%2520could%2520be%2520a%2520helpful%2520support%2520for%2520doctors%252C%2520making%2520nail%2520disease%2520diagnosis%2520more%2520accurate%2520and%2520faster.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Reliable%20and%20Explainable%20Nail%20Disease%20Classification%3A%20Leveraging%20Adversarial%20Training%20and%20Grad-CAM%20Visualization&entry.906535625=Farzia%20Hossain%20and%20Samanta%20Ghosh%20and%20Shahida%20Begum%20and%20B.%20M.%20Shahria%20Alam%20and%20Mohammad%20Tahmid%20Noor%20and%20Md%20Parvez%20Mia%20and%20Nishat%20Tasnim%20Niloy&entry.1292438233=Human%20nail%20diseases%20are%20gradually%20observed%20over%20all%20age%20groups%2C%20especially%20among%20older%20individuals%2C%20often%20going%20ignored%20until%20they%20become%20severe.%20Early%20detection%20and%20accurate%20diagnosis%20of%20such%20conditions%20are%20important%20because%20they%20sometimes%20reveal%20our%20body%27s%20health%20problems.%20But%20it%20is%20challenging%20due%20to%20the%20inferred%20visual%20differences%20between%20disease%20types.%20This%20paper%20presents%20a%20machine%20learning-based%20model%20for%20automated%20classification%20of%20nail%20diseases%20based%20on%20a%20publicly%20available%20dataset%2C%20which%20contains%203%2C835%20images%20scaling%20six%20categories.%20In%20224x224%20pixels%2C%20all%20images%20were%20resized%20to%20ensure%20consistency.%20To%20evaluate%20performance%2C%20four%20well-known%20CNN%20models-InceptionV3%2C%20DenseNet201%2C%20EfficientNetV2%2C%20and%20ResNet50%20were%20trained%20and%20analyzed.%20Among%20these%2C%20InceptionV3%20outperformed%20the%20others%20with%20an%20accuracy%20of%2095.57%25%2C%20while%20DenseNet201%20came%20next%20with%2094.79%25.%20To%20make%20the%20model%20stronger%20and%20less%20likely%20to%20make%20mistakes%20on%20tricky%20or%20noisy%20images%2C%20we%20used%20adversarial%20training.%20To%20help%20understand%20how%20the%20model%20makes%20decisions%2C%20we%20used%20SHAP%20to%20highlight%20important%20features%20in%20the%20predictions.%20This%20system%20could%20be%20a%20helpful%20support%20for%20doctors%2C%20making%20nail%20disease%20diagnosis%20more%20accurate%20and%20faster.&entry.1838667208=http%3A//arxiv.org/abs/2602.04820v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


