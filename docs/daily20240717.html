<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240716.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive\n  Quantile-Based Geometric Consistency Densification", "author": "Zhuoxiao Li and Shanliang Yao and Yijie Chu and Angel F. Garcia-Fernandez and Yong Yue and Eng Gee Lim and Xiaohui Zhu", "abstract": "  In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting\n(3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements.\nAlthough 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to\neffectively enhance mesh extraction quality, this compression can potentially\nlead to a decrease in rendering quality. Additionally, unreliable densification\nprocesses and the calculation of depth through the accumulation of opacity can\ncompromise the detail of mesh extraction. To address this issue, we introduce\nMVG-Splatting, a solution guided by Multi-View considerations. Specifically, we\nintegrate an optimized method for calculating normals, which, combined with\nimage gradients, helps rectify inconsistencies in the original depth\ncomputations. Additionally, utilizing projection strategies akin to those in\nMulti-View Stereo (MVS), we propose an adaptive quantile-based method that\ndynamically determines the level of additional densification guided by depth\nmaps, from coarse to fine detail. Experimental evidence demonstrates that our\nmethod not only resolves the issues of rendering quality degradation caused by\ndepth discrepancies but also facilitates direct mesh extraction from dense\nGaussian point clouds using the Marching Cubes algorithm. This approach\nsignificantly enhances the overall fidelity and accuracy of the 3D\nreconstruction process, ensuring that both the geometric details and visual\nquality.\n", "link": "http://arxiv.org/abs/2407.11840v1", "date": "2024-07-16", "relevancy": 3.3909, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7337}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7138}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVG-Splatting%3A%20Multi-View%20Guided%20Gaussian%20Splatting%20with%20Adaptive%0A%20%20Quantile-Based%20Geometric%20Consistency%20Densification&body=Title%3A%20MVG-Splatting%3A%20Multi-View%20Guided%20Gaussian%20Splatting%20with%20Adaptive%0A%20%20Quantile-Based%20Geometric%20Consistency%20Densification%0AAuthor%3A%20Zhuoxiao%20Li%20and%20Shanliang%20Yao%20and%20Yijie%20Chu%20and%20Angel%20F.%20Garcia-Fernandez%20and%20Yong%20Yue%20and%20Eng%20Gee%20Lim%20and%20Xiaohui%20Zhu%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%203D%20reconstruction%2C%203D%20Gaussian%20Splatting%0A%283DGS%29%20and%202D%20Gaussian%20Splatting%20%282DGS%29%20represent%20significant%20advancements.%0AAlthough%202DGS%20compresses%203D%20Gaussian%20primitives%20into%202D%20Gaussian%20surfels%20to%0Aeffectively%20enhance%20mesh%20extraction%20quality%2C%20this%20compression%20can%20potentially%0Alead%20to%20a%20decrease%20in%20rendering%20quality.%20Additionally%2C%20unreliable%20densification%0Aprocesses%20and%20the%20calculation%20of%20depth%20through%20the%20accumulation%20of%20opacity%20can%0Acompromise%20the%20detail%20of%20mesh%20extraction.%20To%20address%20this%20issue%2C%20we%20introduce%0AMVG-Splatting%2C%20a%20solution%20guided%20by%20Multi-View%20considerations.%20Specifically%2C%20we%0Aintegrate%20an%20optimized%20method%20for%20calculating%20normals%2C%20which%2C%20combined%20with%0Aimage%20gradients%2C%20helps%20rectify%20inconsistencies%20in%20the%20original%20depth%0Acomputations.%20Additionally%2C%20utilizing%20projection%20strategies%20akin%20to%20those%20in%0AMulti-View%20Stereo%20%28MVS%29%2C%20we%20propose%20an%20adaptive%20quantile-based%20method%20that%0Adynamically%20determines%20the%20level%20of%20additional%20densification%20guided%20by%20depth%0Amaps%2C%20from%20coarse%20to%20fine%20detail.%20Experimental%20evidence%20demonstrates%20that%20our%0Amethod%20not%20only%20resolves%20the%20issues%20of%20rendering%20quality%20degradation%20caused%20by%0Adepth%20discrepancies%20but%20also%20facilitates%20direct%20mesh%20extraction%20from%20dense%0AGaussian%20point%20clouds%20using%20the%20Marching%20Cubes%20algorithm.%20This%20approach%0Asignificantly%20enhances%20the%20overall%20fidelity%20and%20accuracy%20of%20the%203D%0Areconstruction%20process%2C%20ensuring%20that%20both%20the%20geometric%20details%20and%20visual%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVG-Splatting%253A%2520Multi-View%2520Guided%2520Gaussian%2520Splatting%2520with%2520Adaptive%250A%2520%2520Quantile-Based%2520Geometric%2520Consistency%2520Densification%26entry.906535625%3DZhuoxiao%2520Li%2520and%2520Shanliang%2520Yao%2520and%2520Yijie%2520Chu%2520and%2520Angel%2520F.%2520Garcia-Fernandez%2520and%2520Yong%2520Yue%2520and%2520Eng%2520Gee%2520Lim%2520and%2520Xiaohui%2520Zhu%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%25203D%2520reconstruction%252C%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520and%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520represent%2520significant%2520advancements.%250AAlthough%25202DGS%2520compresses%25203D%2520Gaussian%2520primitives%2520into%25202D%2520Gaussian%2520surfels%2520to%250Aeffectively%2520enhance%2520mesh%2520extraction%2520quality%252C%2520this%2520compression%2520can%2520potentially%250Alead%2520to%2520a%2520decrease%2520in%2520rendering%2520quality.%2520Additionally%252C%2520unreliable%2520densification%250Aprocesses%2520and%2520the%2520calculation%2520of%2520depth%2520through%2520the%2520accumulation%2520of%2520opacity%2520can%250Acompromise%2520the%2520detail%2520of%2520mesh%2520extraction.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%250AMVG-Splatting%252C%2520a%2520solution%2520guided%2520by%2520Multi-View%2520considerations.%2520Specifically%252C%2520we%250Aintegrate%2520an%2520optimized%2520method%2520for%2520calculating%2520normals%252C%2520which%252C%2520combined%2520with%250Aimage%2520gradients%252C%2520helps%2520rectify%2520inconsistencies%2520in%2520the%2520original%2520depth%250Acomputations.%2520Additionally%252C%2520utilizing%2520projection%2520strategies%2520akin%2520to%2520those%2520in%250AMulti-View%2520Stereo%2520%2528MVS%2529%252C%2520we%2520propose%2520an%2520adaptive%2520quantile-based%2520method%2520that%250Adynamically%2520determines%2520the%2520level%2520of%2520additional%2520densification%2520guided%2520by%2520depth%250Amaps%252C%2520from%2520coarse%2520to%2520fine%2520detail.%2520Experimental%2520evidence%2520demonstrates%2520that%2520our%250Amethod%2520not%2520only%2520resolves%2520the%2520issues%2520of%2520rendering%2520quality%2520degradation%2520caused%2520by%250Adepth%2520discrepancies%2520but%2520also%2520facilitates%2520direct%2520mesh%2520extraction%2520from%2520dense%250AGaussian%2520point%2520clouds%2520using%2520the%2520Marching%2520Cubes%2520algorithm.%2520This%2520approach%250Asignificantly%2520enhances%2520the%2520overall%2520fidelity%2520and%2520accuracy%2520of%2520the%25203D%250Areconstruction%2520process%252C%2520ensuring%2520that%2520both%2520the%2520geometric%2520details%2520and%2520visual%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVG-Splatting%3A%20Multi-View%20Guided%20Gaussian%20Splatting%20with%20Adaptive%0A%20%20Quantile-Based%20Geometric%20Consistency%20Densification&entry.906535625=Zhuoxiao%20Li%20and%20Shanliang%20Yao%20and%20Yijie%20Chu%20and%20Angel%20F.%20Garcia-Fernandez%20and%20Yong%20Yue%20and%20Eng%20Gee%20Lim%20and%20Xiaohui%20Zhu&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%203D%20reconstruction%2C%203D%20Gaussian%20Splatting%0A%283DGS%29%20and%202D%20Gaussian%20Splatting%20%282DGS%29%20represent%20significant%20advancements.%0AAlthough%202DGS%20compresses%203D%20Gaussian%20primitives%20into%202D%20Gaussian%20surfels%20to%0Aeffectively%20enhance%20mesh%20extraction%20quality%2C%20this%20compression%20can%20potentially%0Alead%20to%20a%20decrease%20in%20rendering%20quality.%20Additionally%2C%20unreliable%20densification%0Aprocesses%20and%20the%20calculation%20of%20depth%20through%20the%20accumulation%20of%20opacity%20can%0Acompromise%20the%20detail%20of%20mesh%20extraction.%20To%20address%20this%20issue%2C%20we%20introduce%0AMVG-Splatting%2C%20a%20solution%20guided%20by%20Multi-View%20considerations.%20Specifically%2C%20we%0Aintegrate%20an%20optimized%20method%20for%20calculating%20normals%2C%20which%2C%20combined%20with%0Aimage%20gradients%2C%20helps%20rectify%20inconsistencies%20in%20the%20original%20depth%0Acomputations.%20Additionally%2C%20utilizing%20projection%20strategies%20akin%20to%20those%20in%0AMulti-View%20Stereo%20%28MVS%29%2C%20we%20propose%20an%20adaptive%20quantile-based%20method%20that%0Adynamically%20determines%20the%20level%20of%20additional%20densification%20guided%20by%20depth%0Amaps%2C%20from%20coarse%20to%20fine%20detail.%20Experimental%20evidence%20demonstrates%20that%20our%0Amethod%20not%20only%20resolves%20the%20issues%20of%20rendering%20quality%20degradation%20caused%20by%0Adepth%20discrepancies%20but%20also%20facilitates%20direct%20mesh%20extraction%20from%20dense%0AGaussian%20point%20clouds%20using%20the%20Marching%20Cubes%20algorithm.%20This%20approach%0Asignificantly%20enhances%20the%20overall%20fidelity%20and%20accuracy%20of%20the%203D%0Areconstruction%20process%2C%20ensuring%20that%20both%20the%20geometric%20details%20and%20visual%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11840v1&entry.124074799=Read"},
{"title": "GVGEN: Text-to-3D Generation with Volumetric Representation", "author": "Xianglong He and Junyi Chen and Sida Peng and Di Huang and Yangguang Li and Xiaoshui Huang and Chun Yuan and Wanli Ouyang and Tong He", "abstract": "  In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed ($\\sim$7\nseconds), effectively striking a balance between quality and efficiency. Our\nproject page is: https://gvgen.github.io/\n", "link": "http://arxiv.org/abs/2403.12957v2", "date": "2024-07-16", "relevancy": 3.1976, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6521}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6469}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation&body=Title%3A%20GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation%0AAuthor%3A%20Xianglong%20He%20and%20Junyi%20Chen%20and%20Sida%20Peng%20and%20Di%20Huang%20and%20Yangguang%20Li%20and%20Xiaoshui%20Huang%20and%20Chun%20Yuan%20and%20Wanli%20Ouyang%20and%20Tong%20He%0AAbstract%3A%20%20%20In%20recent%20years%2C%203D%20Gaussian%20splatting%20has%20emerged%20as%20a%20powerful%20technique%0Afor%203D%20reconstruction%20and%20generation%2C%20known%20for%20its%20fast%20and%20high-quality%0Arendering%20capabilities.%20To%20address%20these%20shortcomings%2C%20this%20paper%20introduces%20a%0Anovel%20diffusion-based%20framework%2C%20GVGEN%2C%20designed%20to%20efficiently%20generate%203D%0AGaussian%20representations%20from%20text%20input.%20We%20propose%20two%20innovative%0Atechniques%3A%281%29%20Structured%20Volumetric%20Representation.%20We%20first%20arrange%0Adisorganized%203D%20Gaussian%20points%20as%20a%20structured%20form%20GaussianVolume.%20This%0Atransformation%20allows%20the%20capture%20of%20intricate%20texture%20details%20within%20a%20volume%0Acomposed%20of%20a%20fixed%20number%20of%20Gaussians.%20To%20better%20optimize%20the%20representation%0Aof%20these%20details%2C%20we%20propose%20a%20unique%20pruning%20and%20densifying%20method%20named%20the%0ACandidate%20Pool%20Strategy%2C%20enhancing%20detail%20fidelity%20through%20selective%0Aoptimization.%20%282%29%20Coarse-to-fine%20Generation%20Pipeline.%20To%20simplify%20the%0Ageneration%20of%20GaussianVolume%20and%20empower%20the%20model%20to%20generate%20instances%20with%0Adetailed%203D%20geometry%2C%20we%20propose%20a%20coarse-to-fine%20pipeline.%20It%20initially%0Aconstructs%20a%20basic%20geometric%20structure%2C%20followed%20by%20the%20prediction%20of%20complete%0AGaussian%20attributes.%20Our%20framework%2C%20GVGEN%2C%20demonstrates%20superior%20performance%20in%0Aqualitative%20and%20quantitative%20assessments%20compared%20to%20existing%203D%20generation%0Amethods.%20Simultaneously%2C%20it%20maintains%20a%20fast%20generation%20speed%20%28%24%5Csim%247%0Aseconds%29%2C%20effectively%20striking%20a%20balance%20between%20quality%20and%20efficiency.%20Our%0Aproject%20page%20is%3A%20https%3A//gvgen.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGVGEN%253A%2520Text-to-3D%2520Generation%2520with%2520Volumetric%2520Representation%26entry.906535625%3DXianglong%2520He%2520and%2520Junyi%2520Chen%2520and%2520Sida%2520Peng%2520and%2520Di%2520Huang%2520and%2520Yangguang%2520Li%2520and%2520Xiaoshui%2520Huang%2520and%2520Chun%2520Yuan%2520and%2520Wanli%2520Ouyang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25203D%2520Gaussian%2520splatting%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%250Afor%25203D%2520reconstruction%2520and%2520generation%252C%2520known%2520for%2520its%2520fast%2520and%2520high-quality%250Arendering%2520capabilities.%2520To%2520address%2520these%2520shortcomings%252C%2520this%2520paper%2520introduces%2520a%250Anovel%2520diffusion-based%2520framework%252C%2520GVGEN%252C%2520designed%2520to%2520efficiently%2520generate%25203D%250AGaussian%2520representations%2520from%2520text%2520input.%2520We%2520propose%2520two%2520innovative%250Atechniques%253A%25281%2529%2520Structured%2520Volumetric%2520Representation.%2520We%2520first%2520arrange%250Adisorganized%25203D%2520Gaussian%2520points%2520as%2520a%2520structured%2520form%2520GaussianVolume.%2520This%250Atransformation%2520allows%2520the%2520capture%2520of%2520intricate%2520texture%2520details%2520within%2520a%2520volume%250Acomposed%2520of%2520a%2520fixed%2520number%2520of%2520Gaussians.%2520To%2520better%2520optimize%2520the%2520representation%250Aof%2520these%2520details%252C%2520we%2520propose%2520a%2520unique%2520pruning%2520and%2520densifying%2520method%2520named%2520the%250ACandidate%2520Pool%2520Strategy%252C%2520enhancing%2520detail%2520fidelity%2520through%2520selective%250Aoptimization.%2520%25282%2529%2520Coarse-to-fine%2520Generation%2520Pipeline.%2520To%2520simplify%2520the%250Ageneration%2520of%2520GaussianVolume%2520and%2520empower%2520the%2520model%2520to%2520generate%2520instances%2520with%250Adetailed%25203D%2520geometry%252C%2520we%2520propose%2520a%2520coarse-to-fine%2520pipeline.%2520It%2520initially%250Aconstructs%2520a%2520basic%2520geometric%2520structure%252C%2520followed%2520by%2520the%2520prediction%2520of%2520complete%250AGaussian%2520attributes.%2520Our%2520framework%252C%2520GVGEN%252C%2520demonstrates%2520superior%2520performance%2520in%250Aqualitative%2520and%2520quantitative%2520assessments%2520compared%2520to%2520existing%25203D%2520generation%250Amethods.%2520Simultaneously%252C%2520it%2520maintains%2520a%2520fast%2520generation%2520speed%2520%2528%2524%255Csim%25247%250Aseconds%2529%252C%2520effectively%2520striking%2520a%2520balance%2520between%2520quality%2520and%2520efficiency.%2520Our%250Aproject%2520page%2520is%253A%2520https%253A//gvgen.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation&entry.906535625=Xianglong%20He%20and%20Junyi%20Chen%20and%20Sida%20Peng%20and%20Di%20Huang%20and%20Yangguang%20Li%20and%20Xiaoshui%20Huang%20and%20Chun%20Yuan%20and%20Wanli%20Ouyang%20and%20Tong%20He&entry.1292438233=%20%20In%20recent%20years%2C%203D%20Gaussian%20splatting%20has%20emerged%20as%20a%20powerful%20technique%0Afor%203D%20reconstruction%20and%20generation%2C%20known%20for%20its%20fast%20and%20high-quality%0Arendering%20capabilities.%20To%20address%20these%20shortcomings%2C%20this%20paper%20introduces%20a%0Anovel%20diffusion-based%20framework%2C%20GVGEN%2C%20designed%20to%20efficiently%20generate%203D%0AGaussian%20representations%20from%20text%20input.%20We%20propose%20two%20innovative%0Atechniques%3A%281%29%20Structured%20Volumetric%20Representation.%20We%20first%20arrange%0Adisorganized%203D%20Gaussian%20points%20as%20a%20structured%20form%20GaussianVolume.%20This%0Atransformation%20allows%20the%20capture%20of%20intricate%20texture%20details%20within%20a%20volume%0Acomposed%20of%20a%20fixed%20number%20of%20Gaussians.%20To%20better%20optimize%20the%20representation%0Aof%20these%20details%2C%20we%20propose%20a%20unique%20pruning%20and%20densifying%20method%20named%20the%0ACandidate%20Pool%20Strategy%2C%20enhancing%20detail%20fidelity%20through%20selective%0Aoptimization.%20%282%29%20Coarse-to-fine%20Generation%20Pipeline.%20To%20simplify%20the%0Ageneration%20of%20GaussianVolume%20and%20empower%20the%20model%20to%20generate%20instances%20with%0Adetailed%203D%20geometry%2C%20we%20propose%20a%20coarse-to-fine%20pipeline.%20It%20initially%0Aconstructs%20a%20basic%20geometric%20structure%2C%20followed%20by%20the%20prediction%20of%20complete%0AGaussian%20attributes.%20Our%20framework%2C%20GVGEN%2C%20demonstrates%20superior%20performance%20in%0Aqualitative%20and%20quantitative%20assessments%20compared%20to%20existing%203D%20generation%0Amethods.%20Simultaneously%2C%20it%20maintains%20a%20fast%20generation%20speed%20%28%24%5Csim%247%0Aseconds%29%2C%20effectively%20striking%20a%20balance%20between%20quality%20and%20efficiency.%20Our%0Aproject%20page%20is%3A%20https%3A//gvgen.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12957v2&entry.124074799=Read"},
{"title": "Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting", "author": "Yunzhi Yan and Haotong Lin and Chenxu Zhou and Weijie Wang and Haiyang Sun and Kun Zhan and Xianpeng Lang and Xiaowei Zhou and Sida Peng", "abstract": "  This paper aims to tackle the problem of modeling dynamic urban streets for\nautonomous driving scenes. Recent methods extend NeRF by incorporating tracked\nvehicle poses to animate vehicles, enabling photo-realistic view synthesis of\ndynamic urban street scenes. However, significant limitations are their slow\ntraining and rendering speed. We introduce Street Gaussians, a new explicit\nscene representation that tackles these limitations. Specifically, the dynamic\nurban scene is represented as a set of point clouds equipped with semantic\nlogits and 3D Gaussians, each associated with either a foreground vehicle or\nthe background. To model the dynamics of foreground object vehicles, each\nobject point cloud is optimized with optimizable tracked poses, along with a 4D\nspherical harmonics model for the dynamic appearance. The explicit\nrepresentation allows easy composition of object vehicles and background, which\nin turn allows for scene editing operations and rendering at 135 FPS (1066\n$\\times$ 1600 resolution) within half an hour of training. The proposed method\nis evaluated on multiple challenging benchmarks, including KITTI and Waymo Open\ndatasets. Experiments show that the proposed method consistently outperforms\nstate-of-the-art methods across all datasets. The code will be released to\nensure reproducibility.\n", "link": "http://arxiv.org/abs/2401.01339v2", "date": "2024-07-16", "relevancy": 3.1971, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6961}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.664}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Street%20Gaussians%3A%20Modeling%20Dynamic%20Urban%20Scenes%20with%20Gaussian%20Splatting&body=Title%3A%20Street%20Gaussians%3A%20Modeling%20Dynamic%20Urban%20Scenes%20with%20Gaussian%20Splatting%0AAuthor%3A%20Yunzhi%20Yan%20and%20Haotong%20Lin%20and%20Chenxu%20Zhou%20and%20Weijie%20Wang%20and%20Haiyang%20Sun%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20tackle%20the%20problem%20of%20modeling%20dynamic%20urban%20streets%20for%0Aautonomous%20driving%20scenes.%20Recent%20methods%20extend%20NeRF%20by%20incorporating%20tracked%0Avehicle%20poses%20to%20animate%20vehicles%2C%20enabling%20photo-realistic%20view%20synthesis%20of%0Adynamic%20urban%20street%20scenes.%20However%2C%20significant%20limitations%20are%20their%20slow%0Atraining%20and%20rendering%20speed.%20We%20introduce%20Street%20Gaussians%2C%20a%20new%20explicit%0Ascene%20representation%20that%20tackles%20these%20limitations.%20Specifically%2C%20the%20dynamic%0Aurban%20scene%20is%20represented%20as%20a%20set%20of%20point%20clouds%20equipped%20with%20semantic%0Alogits%20and%203D%20Gaussians%2C%20each%20associated%20with%20either%20a%20foreground%20vehicle%20or%0Athe%20background.%20To%20model%20the%20dynamics%20of%20foreground%20object%20vehicles%2C%20each%0Aobject%20point%20cloud%20is%20optimized%20with%20optimizable%20tracked%20poses%2C%20along%20with%20a%204D%0Aspherical%20harmonics%20model%20for%20the%20dynamic%20appearance.%20The%20explicit%0Arepresentation%20allows%20easy%20composition%20of%20object%20vehicles%20and%20background%2C%20which%0Ain%20turn%20allows%20for%20scene%20editing%20operations%20and%20rendering%20at%20135%20FPS%20%281066%0A%24%5Ctimes%24%201600%20resolution%29%20within%20half%20an%20hour%20of%20training.%20The%20proposed%20method%0Ais%20evaluated%20on%20multiple%20challenging%20benchmarks%2C%20including%20KITTI%20and%20Waymo%20Open%0Adatasets.%20Experiments%20show%20that%20the%20proposed%20method%20consistently%20outperforms%0Astate-of-the-art%20methods%20across%20all%20datasets.%20The%20code%20will%20be%20released%20to%0Aensure%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreet%2520Gaussians%253A%2520Modeling%2520Dynamic%2520Urban%2520Scenes%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DYunzhi%2520Yan%2520and%2520Haotong%2520Lin%2520and%2520Chenxu%2520Zhou%2520and%2520Weijie%2520Wang%2520and%2520Haiyang%2520Sun%2520and%2520Kun%2520Zhan%2520and%2520Xianpeng%2520Lang%2520and%2520Xiaowei%2520Zhou%2520and%2520Sida%2520Peng%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520tackle%2520the%2520problem%2520of%2520modeling%2520dynamic%2520urban%2520streets%2520for%250Aautonomous%2520driving%2520scenes.%2520Recent%2520methods%2520extend%2520NeRF%2520by%2520incorporating%2520tracked%250Avehicle%2520poses%2520to%2520animate%2520vehicles%252C%2520enabling%2520photo-realistic%2520view%2520synthesis%2520of%250Adynamic%2520urban%2520street%2520scenes.%2520However%252C%2520significant%2520limitations%2520are%2520their%2520slow%250Atraining%2520and%2520rendering%2520speed.%2520We%2520introduce%2520Street%2520Gaussians%252C%2520a%2520new%2520explicit%250Ascene%2520representation%2520that%2520tackles%2520these%2520limitations.%2520Specifically%252C%2520the%2520dynamic%250Aurban%2520scene%2520is%2520represented%2520as%2520a%2520set%2520of%2520point%2520clouds%2520equipped%2520with%2520semantic%250Alogits%2520and%25203D%2520Gaussians%252C%2520each%2520associated%2520with%2520either%2520a%2520foreground%2520vehicle%2520or%250Athe%2520background.%2520To%2520model%2520the%2520dynamics%2520of%2520foreground%2520object%2520vehicles%252C%2520each%250Aobject%2520point%2520cloud%2520is%2520optimized%2520with%2520optimizable%2520tracked%2520poses%252C%2520along%2520with%2520a%25204D%250Aspherical%2520harmonics%2520model%2520for%2520the%2520dynamic%2520appearance.%2520The%2520explicit%250Arepresentation%2520allows%2520easy%2520composition%2520of%2520object%2520vehicles%2520and%2520background%252C%2520which%250Ain%2520turn%2520allows%2520for%2520scene%2520editing%2520operations%2520and%2520rendering%2520at%2520135%2520FPS%2520%25281066%250A%2524%255Ctimes%2524%25201600%2520resolution%2529%2520within%2520half%2520an%2520hour%2520of%2520training.%2520The%2520proposed%2520method%250Ais%2520evaluated%2520on%2520multiple%2520challenging%2520benchmarks%252C%2520including%2520KITTI%2520and%2520Waymo%2520Open%250Adatasets.%2520Experiments%2520show%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%250Astate-of-the-art%2520methods%2520across%2520all%2520datasets.%2520The%2520code%2520will%2520be%2520released%2520to%250Aensure%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Street%20Gaussians%3A%20Modeling%20Dynamic%20Urban%20Scenes%20with%20Gaussian%20Splatting&entry.906535625=Yunzhi%20Yan%20and%20Haotong%20Lin%20and%20Chenxu%20Zhou%20and%20Weijie%20Wang%20and%20Haiyang%20Sun%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng&entry.1292438233=%20%20This%20paper%20aims%20to%20tackle%20the%20problem%20of%20modeling%20dynamic%20urban%20streets%20for%0Aautonomous%20driving%20scenes.%20Recent%20methods%20extend%20NeRF%20by%20incorporating%20tracked%0Avehicle%20poses%20to%20animate%20vehicles%2C%20enabling%20photo-realistic%20view%20synthesis%20of%0Adynamic%20urban%20street%20scenes.%20However%2C%20significant%20limitations%20are%20their%20slow%0Atraining%20and%20rendering%20speed.%20We%20introduce%20Street%20Gaussians%2C%20a%20new%20explicit%0Ascene%20representation%20that%20tackles%20these%20limitations.%20Specifically%2C%20the%20dynamic%0Aurban%20scene%20is%20represented%20as%20a%20set%20of%20point%20clouds%20equipped%20with%20semantic%0Alogits%20and%203D%20Gaussians%2C%20each%20associated%20with%20either%20a%20foreground%20vehicle%20or%0Athe%20background.%20To%20model%20the%20dynamics%20of%20foreground%20object%20vehicles%2C%20each%0Aobject%20point%20cloud%20is%20optimized%20with%20optimizable%20tracked%20poses%2C%20along%20with%20a%204D%0Aspherical%20harmonics%20model%20for%20the%20dynamic%20appearance.%20The%20explicit%0Arepresentation%20allows%20easy%20composition%20of%20object%20vehicles%20and%20background%2C%20which%0Ain%20turn%20allows%20for%20scene%20editing%20operations%20and%20rendering%20at%20135%20FPS%20%281066%0A%24%5Ctimes%24%201600%20resolution%29%20within%20half%20an%20hour%20of%20training.%20The%20proposed%20method%0Ais%20evaluated%20on%20multiple%20challenging%20benchmarks%2C%20including%20KITTI%20and%20Waymo%20Open%0Adatasets.%20Experiments%20show%20that%20the%20proposed%20method%20consistently%20outperforms%0Astate-of-the-art%20methods%20across%20all%20datasets.%20The%20code%20will%20be%20released%20to%0Aensure%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01339v2&entry.124074799=Read"},
{"title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians", "author": "Seokhun Choi and Hyeonseop Song and Jaechul Kim and Taehyeong Kim and Hoseok Do", "abstract": "  Interactive segmentation of 3D Gaussians opens a great opportunity for\nreal-time manipulation of 3D scenes thanks to the real-time rendering\ncapability of 3D Gaussian Splatting. However, the current methods suffer from\ntime-consuming post-processing to deal with noisy segmentation output. Also,\nthey struggle to provide detailed segmentation, which is important for\nfine-grained manipulation of 3D scenes. In this study, we propose\nClick-Gaussian, which learns distinguishable feature fields of two-level\ngranularity, facilitating segmentation without time-consuming post-processing.\nWe delve into challenges stemming from inconsistently learned feature fields\nresulting from 2D segmentation obtained independently from a 3D scene. 3D\nsegmentation accuracy deteriorates when 2D segmentation results across the\nviews, primary cues for 3D segmentation, are in conflict. To overcome these\nissues, we propose Global Feature-guided Learning (GFL). GFL constructs the\nclusters of global feature candidates from noisy 2D segments across the views,\nwhich smooths out noises when training the features of 3D Gaussians. Our method\nruns in 10 ms per click, 15 to 130 times as fast as the previous methods, while\nalso significantly improving segmentation accuracy. Our project page is\navailable at https://seokhunchoi.github.io/Click-Gaussian\n", "link": "http://arxiv.org/abs/2407.11793v1", "date": "2024-07-16", "relevancy": 3.1082, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6783}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6358}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Click-Gaussian%3A%20Interactive%20Segmentation%20to%20Any%203D%20Gaussians&body=Title%3A%20Click-Gaussian%3A%20Interactive%20Segmentation%20to%20Any%203D%20Gaussians%0AAuthor%3A%20Seokhun%20Choi%20and%20Hyeonseop%20Song%20and%20Jaechul%20Kim%20and%20Taehyeong%20Kim%20and%20Hoseok%20Do%0AAbstract%3A%20%20%20Interactive%20segmentation%20of%203D%20Gaussians%20opens%20a%20great%20opportunity%20for%0Areal-time%20manipulation%20of%203D%20scenes%20thanks%20to%20the%20real-time%20rendering%0Acapability%20of%203D%20Gaussian%20Splatting.%20However%2C%20the%20current%20methods%20suffer%20from%0Atime-consuming%20post-processing%20to%20deal%20with%20noisy%20segmentation%20output.%20Also%2C%0Athey%20struggle%20to%20provide%20detailed%20segmentation%2C%20which%20is%20important%20for%0Afine-grained%20manipulation%20of%203D%20scenes.%20In%20this%20study%2C%20we%20propose%0AClick-Gaussian%2C%20which%20learns%20distinguishable%20feature%20fields%20of%20two-level%0Agranularity%2C%20facilitating%20segmentation%20without%20time-consuming%20post-processing.%0AWe%20delve%20into%20challenges%20stemming%20from%20inconsistently%20learned%20feature%20fields%0Aresulting%20from%202D%20segmentation%20obtained%20independently%20from%20a%203D%20scene.%203D%0Asegmentation%20accuracy%20deteriorates%20when%202D%20segmentation%20results%20across%20the%0Aviews%2C%20primary%20cues%20for%203D%20segmentation%2C%20are%20in%20conflict.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Global%20Feature-guided%20Learning%20%28GFL%29.%20GFL%20constructs%20the%0Aclusters%20of%20global%20feature%20candidates%20from%20noisy%202D%20segments%20across%20the%20views%2C%0Awhich%20smooths%20out%20noises%20when%20training%20the%20features%20of%203D%20Gaussians.%20Our%20method%0Aruns%20in%2010%20ms%20per%20click%2C%2015%20to%20130%20times%20as%20fast%20as%20the%20previous%20methods%2C%20while%0Aalso%20significantly%20improving%20segmentation%20accuracy.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//seokhunchoi.github.io/Click-Gaussian%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClick-Gaussian%253A%2520Interactive%2520Segmentation%2520to%2520Any%25203D%2520Gaussians%26entry.906535625%3DSeokhun%2520Choi%2520and%2520Hyeonseop%2520Song%2520and%2520Jaechul%2520Kim%2520and%2520Taehyeong%2520Kim%2520and%2520Hoseok%2520Do%26entry.1292438233%3D%2520%2520Interactive%2520segmentation%2520of%25203D%2520Gaussians%2520opens%2520a%2520great%2520opportunity%2520for%250Areal-time%2520manipulation%2520of%25203D%2520scenes%2520thanks%2520to%2520the%2520real-time%2520rendering%250Acapability%2520of%25203D%2520Gaussian%2520Splatting.%2520However%252C%2520the%2520current%2520methods%2520suffer%2520from%250Atime-consuming%2520post-processing%2520to%2520deal%2520with%2520noisy%2520segmentation%2520output.%2520Also%252C%250Athey%2520struggle%2520to%2520provide%2520detailed%2520segmentation%252C%2520which%2520is%2520important%2520for%250Afine-grained%2520manipulation%2520of%25203D%2520scenes.%2520In%2520this%2520study%252C%2520we%2520propose%250AClick-Gaussian%252C%2520which%2520learns%2520distinguishable%2520feature%2520fields%2520of%2520two-level%250Agranularity%252C%2520facilitating%2520segmentation%2520without%2520time-consuming%2520post-processing.%250AWe%2520delve%2520into%2520challenges%2520stemming%2520from%2520inconsistently%2520learned%2520feature%2520fields%250Aresulting%2520from%25202D%2520segmentation%2520obtained%2520independently%2520from%2520a%25203D%2520scene.%25203D%250Asegmentation%2520accuracy%2520deteriorates%2520when%25202D%2520segmentation%2520results%2520across%2520the%250Aviews%252C%2520primary%2520cues%2520for%25203D%2520segmentation%252C%2520are%2520in%2520conflict.%2520To%2520overcome%2520these%250Aissues%252C%2520we%2520propose%2520Global%2520Feature-guided%2520Learning%2520%2528GFL%2529.%2520GFL%2520constructs%2520the%250Aclusters%2520of%2520global%2520feature%2520candidates%2520from%2520noisy%25202D%2520segments%2520across%2520the%2520views%252C%250Awhich%2520smooths%2520out%2520noises%2520when%2520training%2520the%2520features%2520of%25203D%2520Gaussians.%2520Our%2520method%250Aruns%2520in%252010%2520ms%2520per%2520click%252C%252015%2520to%2520130%2520times%2520as%2520fast%2520as%2520the%2520previous%2520methods%252C%2520while%250Aalso%2520significantly%2520improving%2520segmentation%2520accuracy.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//seokhunchoi.github.io/Click-Gaussian%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Click-Gaussian%3A%20Interactive%20Segmentation%20to%20Any%203D%20Gaussians&entry.906535625=Seokhun%20Choi%20and%20Hyeonseop%20Song%20and%20Jaechul%20Kim%20and%20Taehyeong%20Kim%20and%20Hoseok%20Do&entry.1292438233=%20%20Interactive%20segmentation%20of%203D%20Gaussians%20opens%20a%20great%20opportunity%20for%0Areal-time%20manipulation%20of%203D%20scenes%20thanks%20to%20the%20real-time%20rendering%0Acapability%20of%203D%20Gaussian%20Splatting.%20However%2C%20the%20current%20methods%20suffer%20from%0Atime-consuming%20post-processing%20to%20deal%20with%20noisy%20segmentation%20output.%20Also%2C%0Athey%20struggle%20to%20provide%20detailed%20segmentation%2C%20which%20is%20important%20for%0Afine-grained%20manipulation%20of%203D%20scenes.%20In%20this%20study%2C%20we%20propose%0AClick-Gaussian%2C%20which%20learns%20distinguishable%20feature%20fields%20of%20two-level%0Agranularity%2C%20facilitating%20segmentation%20without%20time-consuming%20post-processing.%0AWe%20delve%20into%20challenges%20stemming%20from%20inconsistently%20learned%20feature%20fields%0Aresulting%20from%202D%20segmentation%20obtained%20independently%20from%20a%203D%20scene.%203D%0Asegmentation%20accuracy%20deteriorates%20when%202D%20segmentation%20results%20across%20the%0Aviews%2C%20primary%20cues%20for%203D%20segmentation%2C%20are%20in%20conflict.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Global%20Feature-guided%20Learning%20%28GFL%29.%20GFL%20constructs%20the%0Aclusters%20of%20global%20feature%20candidates%20from%20noisy%202D%20segments%20across%20the%20views%2C%0Awhich%20smooths%20out%20noises%20when%20training%20the%20features%20of%203D%20Gaussians.%20Our%20method%0Aruns%20in%2010%20ms%20per%20click%2C%2015%20to%20130%20times%20as%20fast%20as%20the%20previous%20methods%2C%20while%0Aalso%20significantly%20improving%20segmentation%20accuracy.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//seokhunchoi.github.io/Click-Gaussian%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11793v1&entry.124074799=Read"},
{"title": "SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane\n  Representation", "author": "Heyuan Li and Ce Chen and Tianhao Shi and Yuda Qiu and Sizhe An and Guanying Chen and Xiaoguang Han", "abstract": "  While recent advances in 3D-aware Generative Adversarial Networks (GANs) have\naided the development of near-frontal view human face synthesis, the challenge\nof comprehensively synthesizing a full 3D head viewable from all angles still\npersists. Although PanoHead proves the possibilities of using a large-scale\ndataset with images of both frontal and back views for full-head synthesis, it\noften causes artifacts for back views. Based on our in-depth analysis, we found\nthe reasons are mainly twofold. First, from network architecture perspective,\nwe found each plane in the utilized tri-plane/tri-grid representation space\ntends to confuse the features from both sides, causing \"mirroring\" artifacts\n(e.g., the glasses appear in the back). Second, from data supervision aspect,\nwe found that existing discriminator training in 3D GANs mainly focuses on the\nquality of the rendered image itself, and does not care much about its\nplausibility with the perspective from which it was rendered. This makes it\npossible to generate \"face\" in non-frontal views, due to its easiness to fool\nthe discriminator. In response, we propose SphereHead, a novel tri-plane\nrepresentation in the spherical coordinate system that fits the human head's\ngeometric characteristics and efficiently mitigates many of the generated\nartifacts. We further introduce a view-image consistency loss for the\ndiscriminator to emphasize the correspondence of the camera parameters and the\nimages. The combination of these efforts results in visually superior outcomes\nwith significantly fewer artifacts. Our code and dataset are publicly available\nat https://lhyfst.github.io/spherehead.\n", "link": "http://arxiv.org/abs/2404.05680v2", "date": "2024-07-16", "relevancy": 3.0549, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6338}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6338}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation&body=Title%3A%20SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation%0AAuthor%3A%20Heyuan%20Li%20and%20Ce%20Chen%20and%20Tianhao%20Shi%20and%20Yuda%20Qiu%20and%20Sizhe%20An%20and%20Guanying%20Chen%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20While%20recent%20advances%20in%203D-aware%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%0Aaided%20the%20development%20of%20near-frontal%20view%20human%20face%20synthesis%2C%20the%20challenge%0Aof%20comprehensively%20synthesizing%20a%20full%203D%20head%20viewable%20from%20all%20angles%20still%0Apersists.%20Although%20PanoHead%20proves%20the%20possibilities%20of%20using%20a%20large-scale%0Adataset%20with%20images%20of%20both%20frontal%20and%20back%20views%20for%20full-head%20synthesis%2C%20it%0Aoften%20causes%20artifacts%20for%20back%20views.%20Based%20on%20our%20in-depth%20analysis%2C%20we%20found%0Athe%20reasons%20are%20mainly%20twofold.%20First%2C%20from%20network%20architecture%20perspective%2C%0Awe%20found%20each%20plane%20in%20the%20utilized%20tri-plane/tri-grid%20representation%20space%0Atends%20to%20confuse%20the%20features%20from%20both%20sides%2C%20causing%20%22mirroring%22%20artifacts%0A%28e.g.%2C%20the%20glasses%20appear%20in%20the%20back%29.%20Second%2C%20from%20data%20supervision%20aspect%2C%0Awe%20found%20that%20existing%20discriminator%20training%20in%203D%20GANs%20mainly%20focuses%20on%20the%0Aquality%20of%20the%20rendered%20image%20itself%2C%20and%20does%20not%20care%20much%20about%20its%0Aplausibility%20with%20the%20perspective%20from%20which%20it%20was%20rendered.%20This%20makes%20it%0Apossible%20to%20generate%20%22face%22%20in%20non-frontal%20views%2C%20due%20to%20its%20easiness%20to%20fool%0Athe%20discriminator.%20In%20response%2C%20we%20propose%20SphereHead%2C%20a%20novel%20tri-plane%0Arepresentation%20in%20the%20spherical%20coordinate%20system%20that%20fits%20the%20human%20head%27s%0Ageometric%20characteristics%20and%20efficiently%20mitigates%20many%20of%20the%20generated%0Aartifacts.%20We%20further%20introduce%20a%20view-image%20consistency%20loss%20for%20the%0Adiscriminator%20to%20emphasize%20the%20correspondence%20of%20the%20camera%20parameters%20and%20the%0Aimages.%20The%20combination%20of%20these%20efforts%20results%20in%20visually%20superior%20outcomes%0Awith%20significantly%20fewer%20artifacts.%20Our%20code%20and%20dataset%20are%20publicly%20available%0Aat%20https%3A//lhyfst.github.io/spherehead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSphereHead%253A%2520Stable%25203D%2520Full-head%2520Synthesis%2520with%2520Spherical%2520Tri-plane%250A%2520%2520Representation%26entry.906535625%3DHeyuan%2520Li%2520and%2520Ce%2520Chen%2520and%2520Tianhao%2520Shi%2520and%2520Yuda%2520Qiu%2520and%2520Sizhe%2520An%2520and%2520Guanying%2520Chen%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520While%2520recent%2520advances%2520in%25203D-aware%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%250Aaided%2520the%2520development%2520of%2520near-frontal%2520view%2520human%2520face%2520synthesis%252C%2520the%2520challenge%250Aof%2520comprehensively%2520synthesizing%2520a%2520full%25203D%2520head%2520viewable%2520from%2520all%2520angles%2520still%250Apersists.%2520Although%2520PanoHead%2520proves%2520the%2520possibilities%2520of%2520using%2520a%2520large-scale%250Adataset%2520with%2520images%2520of%2520both%2520frontal%2520and%2520back%2520views%2520for%2520full-head%2520synthesis%252C%2520it%250Aoften%2520causes%2520artifacts%2520for%2520back%2520views.%2520Based%2520on%2520our%2520in-depth%2520analysis%252C%2520we%2520found%250Athe%2520reasons%2520are%2520mainly%2520twofold.%2520First%252C%2520from%2520network%2520architecture%2520perspective%252C%250Awe%2520found%2520each%2520plane%2520in%2520the%2520utilized%2520tri-plane/tri-grid%2520representation%2520space%250Atends%2520to%2520confuse%2520the%2520features%2520from%2520both%2520sides%252C%2520causing%2520%2522mirroring%2522%2520artifacts%250A%2528e.g.%252C%2520the%2520glasses%2520appear%2520in%2520the%2520back%2529.%2520Second%252C%2520from%2520data%2520supervision%2520aspect%252C%250Awe%2520found%2520that%2520existing%2520discriminator%2520training%2520in%25203D%2520GANs%2520mainly%2520focuses%2520on%2520the%250Aquality%2520of%2520the%2520rendered%2520image%2520itself%252C%2520and%2520does%2520not%2520care%2520much%2520about%2520its%250Aplausibility%2520with%2520the%2520perspective%2520from%2520which%2520it%2520was%2520rendered.%2520This%2520makes%2520it%250Apossible%2520to%2520generate%2520%2522face%2522%2520in%2520non-frontal%2520views%252C%2520due%2520to%2520its%2520easiness%2520to%2520fool%250Athe%2520discriminator.%2520In%2520response%252C%2520we%2520propose%2520SphereHead%252C%2520a%2520novel%2520tri-plane%250Arepresentation%2520in%2520the%2520spherical%2520coordinate%2520system%2520that%2520fits%2520the%2520human%2520head%2527s%250Ageometric%2520characteristics%2520and%2520efficiently%2520mitigates%2520many%2520of%2520the%2520generated%250Aartifacts.%2520We%2520further%2520introduce%2520a%2520view-image%2520consistency%2520loss%2520for%2520the%250Adiscriminator%2520to%2520emphasize%2520the%2520correspondence%2520of%2520the%2520camera%2520parameters%2520and%2520the%250Aimages.%2520The%2520combination%2520of%2520these%2520efforts%2520results%2520in%2520visually%2520superior%2520outcomes%250Awith%2520significantly%2520fewer%2520artifacts.%2520Our%2520code%2520and%2520dataset%2520are%2520publicly%2520available%250Aat%2520https%253A//lhyfst.github.io/spherehead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SphereHead%3A%20Stable%203D%20Full-head%20Synthesis%20with%20Spherical%20Tri-plane%0A%20%20Representation&entry.906535625=Heyuan%20Li%20and%20Ce%20Chen%20and%20Tianhao%20Shi%20and%20Yuda%20Qiu%20and%20Sizhe%20An%20and%20Guanying%20Chen%20and%20Xiaoguang%20Han&entry.1292438233=%20%20While%20recent%20advances%20in%203D-aware%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%0Aaided%20the%20development%20of%20near-frontal%20view%20human%20face%20synthesis%2C%20the%20challenge%0Aof%20comprehensively%20synthesizing%20a%20full%203D%20head%20viewable%20from%20all%20angles%20still%0Apersists.%20Although%20PanoHead%20proves%20the%20possibilities%20of%20using%20a%20large-scale%0Adataset%20with%20images%20of%20both%20frontal%20and%20back%20views%20for%20full-head%20synthesis%2C%20it%0Aoften%20causes%20artifacts%20for%20back%20views.%20Based%20on%20our%20in-depth%20analysis%2C%20we%20found%0Athe%20reasons%20are%20mainly%20twofold.%20First%2C%20from%20network%20architecture%20perspective%2C%0Awe%20found%20each%20plane%20in%20the%20utilized%20tri-plane/tri-grid%20representation%20space%0Atends%20to%20confuse%20the%20features%20from%20both%20sides%2C%20causing%20%22mirroring%22%20artifacts%0A%28e.g.%2C%20the%20glasses%20appear%20in%20the%20back%29.%20Second%2C%20from%20data%20supervision%20aspect%2C%0Awe%20found%20that%20existing%20discriminator%20training%20in%203D%20GANs%20mainly%20focuses%20on%20the%0Aquality%20of%20the%20rendered%20image%20itself%2C%20and%20does%20not%20care%20much%20about%20its%0Aplausibility%20with%20the%20perspective%20from%20which%20it%20was%20rendered.%20This%20makes%20it%0Apossible%20to%20generate%20%22face%22%20in%20non-frontal%20views%2C%20due%20to%20its%20easiness%20to%20fool%0Athe%20discriminator.%20In%20response%2C%20we%20propose%20SphereHead%2C%20a%20novel%20tri-plane%0Arepresentation%20in%20the%20spherical%20coordinate%20system%20that%20fits%20the%20human%20head%27s%0Ageometric%20characteristics%20and%20efficiently%20mitigates%20many%20of%20the%20generated%0Aartifacts.%20We%20further%20introduce%20a%20view-image%20consistency%20loss%20for%20the%0Adiscriminator%20to%20emphasize%20the%20correspondence%20of%20the%20camera%20parameters%20and%20the%0Aimages.%20The%20combination%20of%20these%20efforts%20results%20in%20visually%20superior%20outcomes%0Awith%20significantly%20fewer%20artifacts.%20Our%20code%20and%20dataset%20are%20publicly%20available%0Aat%20https%3A//lhyfst.github.io/spherehead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05680v2&entry.124074799=Read"},
{"title": "InterFusion: Text-Driven Generation of 3D Human-Object Interaction", "author": "Sisi Dai and Wenhao Li and Haowen Sun and Haibin Huang and Chongyang Ma and Hui Huang and Kai Xu and Ruizhen Hu", "abstract": "  In this study, we tackle the complex task of generating 3D human-object\ninteractions (HOI) from textual descriptions in a zero-shot text-to-3D manner.\nWe identify and address two key challenges: the unsatisfactory outcomes of\ndirect text-to-3D methods in HOI, largely due to the lack of paired\ntext-interaction data, and the inherent difficulties in simultaneously\ngenerating multiple concepts with complex spatial relationships. To effectively\naddress these issues, we present InterFusion, a two-stage framework\nspecifically designed for HOI generation. InterFusion involves human pose\nestimations derived from text as geometric priors, which simplifies the\ntext-to-3D conversion process and introduces additional constraints for\naccurate object generation. At the first stage, InterFusion extracts 3D human\nposes from a synthesized image dataset depicting a wide range of interactions,\nsubsequently mapping these poses to interaction descriptions. The second stage\nof InterFusion capitalizes on the latest developments in text-to-3D generation,\nenabling the production of realistic and high-quality 3D HOI scenes. This is\nachieved through a local-global optimization process, where the generation of\nhuman body and object is optimized separately, and jointly refined with a\nglobal optimization of the entire scene, ensuring a seamless and contextually\ncoherent integration. Our experimental results affirm that InterFusion\nsignificantly outperforms existing state-of-the-art methods in 3D HOI\ngeneration.\n", "link": "http://arxiv.org/abs/2403.15612v2", "date": "2024-07-16", "relevancy": 2.9833, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6274}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5813}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterFusion%3A%20Text-Driven%20Generation%20of%203D%20Human-Object%20Interaction&body=Title%3A%20InterFusion%3A%20Text-Driven%20Generation%20of%203D%20Human-Object%20Interaction%0AAuthor%3A%20Sisi%20Dai%20and%20Wenhao%20Li%20and%20Haowen%20Sun%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Hui%20Huang%20and%20Kai%20Xu%20and%20Ruizhen%20Hu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20tackle%20the%20complex%20task%20of%20generating%203D%20human-object%0Ainteractions%20%28HOI%29%20from%20textual%20descriptions%20in%20a%20zero-shot%20text-to-3D%20manner.%0AWe%20identify%20and%20address%20two%20key%20challenges%3A%20the%20unsatisfactory%20outcomes%20of%0Adirect%20text-to-3D%20methods%20in%20HOI%2C%20largely%20due%20to%20the%20lack%20of%20paired%0Atext-interaction%20data%2C%20and%20the%20inherent%20difficulties%20in%20simultaneously%0Agenerating%20multiple%20concepts%20with%20complex%20spatial%20relationships.%20To%20effectively%0Aaddress%20these%20issues%2C%20we%20present%20InterFusion%2C%20a%20two-stage%20framework%0Aspecifically%20designed%20for%20HOI%20generation.%20InterFusion%20involves%20human%20pose%0Aestimations%20derived%20from%20text%20as%20geometric%20priors%2C%20which%20simplifies%20the%0Atext-to-3D%20conversion%20process%20and%20introduces%20additional%20constraints%20for%0Aaccurate%20object%20generation.%20At%20the%20first%20stage%2C%20InterFusion%20extracts%203D%20human%0Aposes%20from%20a%20synthesized%20image%20dataset%20depicting%20a%20wide%20range%20of%20interactions%2C%0Asubsequently%20mapping%20these%20poses%20to%20interaction%20descriptions.%20The%20second%20stage%0Aof%20InterFusion%20capitalizes%20on%20the%20latest%20developments%20in%20text-to-3D%20generation%2C%0Aenabling%20the%20production%20of%20realistic%20and%20high-quality%203D%20HOI%20scenes.%20This%20is%0Aachieved%20through%20a%20local-global%20optimization%20process%2C%20where%20the%20generation%20of%0Ahuman%20body%20and%20object%20is%20optimized%20separately%2C%20and%20jointly%20refined%20with%20a%0Aglobal%20optimization%20of%20the%20entire%20scene%2C%20ensuring%20a%20seamless%20and%20contextually%0Acoherent%20integration.%20Our%20experimental%20results%20affirm%20that%20InterFusion%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%20in%203D%20HOI%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterFusion%253A%2520Text-Driven%2520Generation%2520of%25203D%2520Human-Object%2520Interaction%26entry.906535625%3DSisi%2520Dai%2520and%2520Wenhao%2520Li%2520and%2520Haowen%2520Sun%2520and%2520Haibin%2520Huang%2520and%2520Chongyang%2520Ma%2520and%2520Hui%2520Huang%2520and%2520Kai%2520Xu%2520and%2520Ruizhen%2520Hu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520tackle%2520the%2520complex%2520task%2520of%2520generating%25203D%2520human-object%250Ainteractions%2520%2528HOI%2529%2520from%2520textual%2520descriptions%2520in%2520a%2520zero-shot%2520text-to-3D%2520manner.%250AWe%2520identify%2520and%2520address%2520two%2520key%2520challenges%253A%2520the%2520unsatisfactory%2520outcomes%2520of%250Adirect%2520text-to-3D%2520methods%2520in%2520HOI%252C%2520largely%2520due%2520to%2520the%2520lack%2520of%2520paired%250Atext-interaction%2520data%252C%2520and%2520the%2520inherent%2520difficulties%2520in%2520simultaneously%250Agenerating%2520multiple%2520concepts%2520with%2520complex%2520spatial%2520relationships.%2520To%2520effectively%250Aaddress%2520these%2520issues%252C%2520we%2520present%2520InterFusion%252C%2520a%2520two-stage%2520framework%250Aspecifically%2520designed%2520for%2520HOI%2520generation.%2520InterFusion%2520involves%2520human%2520pose%250Aestimations%2520derived%2520from%2520text%2520as%2520geometric%2520priors%252C%2520which%2520simplifies%2520the%250Atext-to-3D%2520conversion%2520process%2520and%2520introduces%2520additional%2520constraints%2520for%250Aaccurate%2520object%2520generation.%2520At%2520the%2520first%2520stage%252C%2520InterFusion%2520extracts%25203D%2520human%250Aposes%2520from%2520a%2520synthesized%2520image%2520dataset%2520depicting%2520a%2520wide%2520range%2520of%2520interactions%252C%250Asubsequently%2520mapping%2520these%2520poses%2520to%2520interaction%2520descriptions.%2520The%2520second%2520stage%250Aof%2520InterFusion%2520capitalizes%2520on%2520the%2520latest%2520developments%2520in%2520text-to-3D%2520generation%252C%250Aenabling%2520the%2520production%2520of%2520realistic%2520and%2520high-quality%25203D%2520HOI%2520scenes.%2520This%2520is%250Aachieved%2520through%2520a%2520local-global%2520optimization%2520process%252C%2520where%2520the%2520generation%2520of%250Ahuman%2520body%2520and%2520object%2520is%2520optimized%2520separately%252C%2520and%2520jointly%2520refined%2520with%2520a%250Aglobal%2520optimization%2520of%2520the%2520entire%2520scene%252C%2520ensuring%2520a%2520seamless%2520and%2520contextually%250Acoherent%2520integration.%2520Our%2520experimental%2520results%2520affirm%2520that%2520InterFusion%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%25203D%2520HOI%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterFusion%3A%20Text-Driven%20Generation%20of%203D%20Human-Object%20Interaction&entry.906535625=Sisi%20Dai%20and%20Wenhao%20Li%20and%20Haowen%20Sun%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Hui%20Huang%20and%20Kai%20Xu%20and%20Ruizhen%20Hu&entry.1292438233=%20%20In%20this%20study%2C%20we%20tackle%20the%20complex%20task%20of%20generating%203D%20human-object%0Ainteractions%20%28HOI%29%20from%20textual%20descriptions%20in%20a%20zero-shot%20text-to-3D%20manner.%0AWe%20identify%20and%20address%20two%20key%20challenges%3A%20the%20unsatisfactory%20outcomes%20of%0Adirect%20text-to-3D%20methods%20in%20HOI%2C%20largely%20due%20to%20the%20lack%20of%20paired%0Atext-interaction%20data%2C%20and%20the%20inherent%20difficulties%20in%20simultaneously%0Agenerating%20multiple%20concepts%20with%20complex%20spatial%20relationships.%20To%20effectively%0Aaddress%20these%20issues%2C%20we%20present%20InterFusion%2C%20a%20two-stage%20framework%0Aspecifically%20designed%20for%20HOI%20generation.%20InterFusion%20involves%20human%20pose%0Aestimations%20derived%20from%20text%20as%20geometric%20priors%2C%20which%20simplifies%20the%0Atext-to-3D%20conversion%20process%20and%20introduces%20additional%20constraints%20for%0Aaccurate%20object%20generation.%20At%20the%20first%20stage%2C%20InterFusion%20extracts%203D%20human%0Aposes%20from%20a%20synthesized%20image%20dataset%20depicting%20a%20wide%20range%20of%20interactions%2C%0Asubsequently%20mapping%20these%20poses%20to%20interaction%20descriptions.%20The%20second%20stage%0Aof%20InterFusion%20capitalizes%20on%20the%20latest%20developments%20in%20text-to-3D%20generation%2C%0Aenabling%20the%20production%20of%20realistic%20and%20high-quality%203D%20HOI%20scenes.%20This%20is%0Aachieved%20through%20a%20local-global%20optimization%20process%2C%20where%20the%20generation%20of%0Ahuman%20body%20and%20object%20is%20optimized%20separately%2C%20and%20jointly%20refined%20with%20a%0Aglobal%20optimization%20of%20the%20entire%20scene%2C%20ensuring%20a%20seamless%20and%20contextually%0Acoherent%20integration.%20Our%20experimental%20results%20affirm%20that%20InterFusion%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%20in%203D%20HOI%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15612v2&entry.124074799=Read"},
{"title": "SlingBAG: Sliding ball adaptive growth algorithm with differentiable\n  radiation enables super-efficient iterative 3D photoacoustic image\n  reconstruction", "author": "Shuang Li and Yibing Wang and Jian Gao and Chulhong Kim and Seongwook Choi and Yu Zhang and Qian Chen and Yao Yao and Changhui Li", "abstract": "  High-quality 3D photoacoustic imaging (PAI) reconstruction under sparse view\nor limited view has long been challenging. Traditional 3D iterative-based\nreconstruction methods suffer from both slow speed and high memory consumption.\nRecently, in computer graphics, the differentiable rendering has made\nsignificant progress, particularly with the rise of 3D Gaussian Splatting.\nInspired by these, we introduce differentiable radiation into PAI, developing a\nnovel reconstruction algorithm: the Sliding Ball Adaptive Growth algorithm\n(SlingBAG) for 3D PAI, which shows ability in high-quality 3D PAI\nreconstruction both under extremely sparse view and limited view.\n  We established the point cloud dataset in PAI, and used unique differentiable\nrapid radiator based on the spherical decomposition strategy and the randomly\ninitialized point cloud adaptively optimized according to sparse sensor data.\nEach point undergoes updates in 3D coordinates, initial pressure, and\nresolution (denoted by the radius of ball). Points undergo adaptive growth\nduring iterative process, including point destroying, splitting and duplicating\nalong the gradient of their positions, manifesting the sliding ball effect.\n  Finally, our point cloud to voxel grid shader renders the final\nreconstruction results. Simulation and in vivo experiments demonstrate that our\nSlingBAG reconstruction result's SNR can be more than 40 dB under extremely\nsparse view, while the SNR of traditional back-projection algorithm's result is\nless than 20 dB. Moreover, the result of SlingBAG's structural similarity to\nthe ground truth is significantly higher, with an SSIM value of 95.6%.\n  Notably, our differentiable rapid radiator can conduct forward PA simulation\nin homogeneous, non-viscous media substantially faster than current methods\nthat numerically simulate the wave propagation, such as k-Wave. The dataset and\nall code will be open source.\n", "link": "http://arxiv.org/abs/2407.11781v1", "date": "2024-07-16", "relevancy": 2.9737, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6442}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.57}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlingBAG%3A%20Sliding%20ball%20adaptive%20growth%20algorithm%20with%20differentiable%0A%20%20radiation%20enables%20super-efficient%20iterative%203D%20photoacoustic%20image%0A%20%20reconstruction&body=Title%3A%20SlingBAG%3A%20Sliding%20ball%20adaptive%20growth%20algorithm%20with%20differentiable%0A%20%20radiation%20enables%20super-efficient%20iterative%203D%20photoacoustic%20image%0A%20%20reconstruction%0AAuthor%3A%20Shuang%20Li%20and%20Yibing%20Wang%20and%20Jian%20Gao%20and%20Chulhong%20Kim%20and%20Seongwook%20Choi%20and%20Yu%20Zhang%20and%20Qian%20Chen%20and%20Yao%20Yao%20and%20Changhui%20Li%0AAbstract%3A%20%20%20High-quality%203D%20photoacoustic%20imaging%20%28PAI%29%20reconstruction%20under%20sparse%20view%0Aor%20limited%20view%20has%20long%20been%20challenging.%20Traditional%203D%20iterative-based%0Areconstruction%20methods%20suffer%20from%20both%20slow%20speed%20and%20high%20memory%20consumption.%0ARecently%2C%20in%20computer%20graphics%2C%20the%20differentiable%20rendering%20has%20made%0Asignificant%20progress%2C%20particularly%20with%20the%20rise%20of%203D%20Gaussian%20Splatting.%0AInspired%20by%20these%2C%20we%20introduce%20differentiable%20radiation%20into%20PAI%2C%20developing%20a%0Anovel%20reconstruction%20algorithm%3A%20the%20Sliding%20Ball%20Adaptive%20Growth%20algorithm%0A%28SlingBAG%29%20for%203D%20PAI%2C%20which%20shows%20ability%20in%20high-quality%203D%20PAI%0Areconstruction%20both%20under%20extremely%20sparse%20view%20and%20limited%20view.%0A%20%20We%20established%20the%20point%20cloud%20dataset%20in%20PAI%2C%20and%20used%20unique%20differentiable%0Arapid%20radiator%20based%20on%20the%20spherical%20decomposition%20strategy%20and%20the%20randomly%0Ainitialized%20point%20cloud%20adaptively%20optimized%20according%20to%20sparse%20sensor%20data.%0AEach%20point%20undergoes%20updates%20in%203D%20coordinates%2C%20initial%20pressure%2C%20and%0Aresolution%20%28denoted%20by%20the%20radius%20of%20ball%29.%20Points%20undergo%20adaptive%20growth%0Aduring%20iterative%20process%2C%20including%20point%20destroying%2C%20splitting%20and%20duplicating%0Aalong%20the%20gradient%20of%20their%20positions%2C%20manifesting%20the%20sliding%20ball%20effect.%0A%20%20Finally%2C%20our%20point%20cloud%20to%20voxel%20grid%20shader%20renders%20the%20final%0Areconstruction%20results.%20Simulation%20and%20in%20vivo%20experiments%20demonstrate%20that%20our%0ASlingBAG%20reconstruction%20result%27s%20SNR%20can%20be%20more%20than%2040%20dB%20under%20extremely%0Asparse%20view%2C%20while%20the%20SNR%20of%20traditional%20back-projection%20algorithm%27s%20result%20is%0Aless%20than%2020%20dB.%20Moreover%2C%20the%20result%20of%20SlingBAG%27s%20structural%20similarity%20to%0Athe%20ground%20truth%20is%20significantly%20higher%2C%20with%20an%20SSIM%20value%20of%2095.6%25.%0A%20%20Notably%2C%20our%20differentiable%20rapid%20radiator%20can%20conduct%20forward%20PA%20simulation%0Ain%20homogeneous%2C%20non-viscous%20media%20substantially%20faster%20than%20current%20methods%0Athat%20numerically%20simulate%20the%20wave%20propagation%2C%20such%20as%20k-Wave.%20The%20dataset%20and%0Aall%20code%20will%20be%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlingBAG%253A%2520Sliding%2520ball%2520adaptive%2520growth%2520algorithm%2520with%2520differentiable%250A%2520%2520radiation%2520enables%2520super-efficient%2520iterative%25203D%2520photoacoustic%2520image%250A%2520%2520reconstruction%26entry.906535625%3DShuang%2520Li%2520and%2520Yibing%2520Wang%2520and%2520Jian%2520Gao%2520and%2520Chulhong%2520Kim%2520and%2520Seongwook%2520Choi%2520and%2520Yu%2520Zhang%2520and%2520Qian%2520Chen%2520and%2520Yao%2520Yao%2520and%2520Changhui%2520Li%26entry.1292438233%3D%2520%2520High-quality%25203D%2520photoacoustic%2520imaging%2520%2528PAI%2529%2520reconstruction%2520under%2520sparse%2520view%250Aor%2520limited%2520view%2520has%2520long%2520been%2520challenging.%2520Traditional%25203D%2520iterative-based%250Areconstruction%2520methods%2520suffer%2520from%2520both%2520slow%2520speed%2520and%2520high%2520memory%2520consumption.%250ARecently%252C%2520in%2520computer%2520graphics%252C%2520the%2520differentiable%2520rendering%2520has%2520made%250Asignificant%2520progress%252C%2520particularly%2520with%2520the%2520rise%2520of%25203D%2520Gaussian%2520Splatting.%250AInspired%2520by%2520these%252C%2520we%2520introduce%2520differentiable%2520radiation%2520into%2520PAI%252C%2520developing%2520a%250Anovel%2520reconstruction%2520algorithm%253A%2520the%2520Sliding%2520Ball%2520Adaptive%2520Growth%2520algorithm%250A%2528SlingBAG%2529%2520for%25203D%2520PAI%252C%2520which%2520shows%2520ability%2520in%2520high-quality%25203D%2520PAI%250Areconstruction%2520both%2520under%2520extremely%2520sparse%2520view%2520and%2520limited%2520view.%250A%2520%2520We%2520established%2520the%2520point%2520cloud%2520dataset%2520in%2520PAI%252C%2520and%2520used%2520unique%2520differentiable%250Arapid%2520radiator%2520based%2520on%2520the%2520spherical%2520decomposition%2520strategy%2520and%2520the%2520randomly%250Ainitialized%2520point%2520cloud%2520adaptively%2520optimized%2520according%2520to%2520sparse%2520sensor%2520data.%250AEach%2520point%2520undergoes%2520updates%2520in%25203D%2520coordinates%252C%2520initial%2520pressure%252C%2520and%250Aresolution%2520%2528denoted%2520by%2520the%2520radius%2520of%2520ball%2529.%2520Points%2520undergo%2520adaptive%2520growth%250Aduring%2520iterative%2520process%252C%2520including%2520point%2520destroying%252C%2520splitting%2520and%2520duplicating%250Aalong%2520the%2520gradient%2520of%2520their%2520positions%252C%2520manifesting%2520the%2520sliding%2520ball%2520effect.%250A%2520%2520Finally%252C%2520our%2520point%2520cloud%2520to%2520voxel%2520grid%2520shader%2520renders%2520the%2520final%250Areconstruction%2520results.%2520Simulation%2520and%2520in%2520vivo%2520experiments%2520demonstrate%2520that%2520our%250ASlingBAG%2520reconstruction%2520result%2527s%2520SNR%2520can%2520be%2520more%2520than%252040%2520dB%2520under%2520extremely%250Asparse%2520view%252C%2520while%2520the%2520SNR%2520of%2520traditional%2520back-projection%2520algorithm%2527s%2520result%2520is%250Aless%2520than%252020%2520dB.%2520Moreover%252C%2520the%2520result%2520of%2520SlingBAG%2527s%2520structural%2520similarity%2520to%250Athe%2520ground%2520truth%2520is%2520significantly%2520higher%252C%2520with%2520an%2520SSIM%2520value%2520of%252095.6%2525.%250A%2520%2520Notably%252C%2520our%2520differentiable%2520rapid%2520radiator%2520can%2520conduct%2520forward%2520PA%2520simulation%250Ain%2520homogeneous%252C%2520non-viscous%2520media%2520substantially%2520faster%2520than%2520current%2520methods%250Athat%2520numerically%2520simulate%2520the%2520wave%2520propagation%252C%2520such%2520as%2520k-Wave.%2520The%2520dataset%2520and%250Aall%2520code%2520will%2520be%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlingBAG%3A%20Sliding%20ball%20adaptive%20growth%20algorithm%20with%20differentiable%0A%20%20radiation%20enables%20super-efficient%20iterative%203D%20photoacoustic%20image%0A%20%20reconstruction&entry.906535625=Shuang%20Li%20and%20Yibing%20Wang%20and%20Jian%20Gao%20and%20Chulhong%20Kim%20and%20Seongwook%20Choi%20and%20Yu%20Zhang%20and%20Qian%20Chen%20and%20Yao%20Yao%20and%20Changhui%20Li&entry.1292438233=%20%20High-quality%203D%20photoacoustic%20imaging%20%28PAI%29%20reconstruction%20under%20sparse%20view%0Aor%20limited%20view%20has%20long%20been%20challenging.%20Traditional%203D%20iterative-based%0Areconstruction%20methods%20suffer%20from%20both%20slow%20speed%20and%20high%20memory%20consumption.%0ARecently%2C%20in%20computer%20graphics%2C%20the%20differentiable%20rendering%20has%20made%0Asignificant%20progress%2C%20particularly%20with%20the%20rise%20of%203D%20Gaussian%20Splatting.%0AInspired%20by%20these%2C%20we%20introduce%20differentiable%20radiation%20into%20PAI%2C%20developing%20a%0Anovel%20reconstruction%20algorithm%3A%20the%20Sliding%20Ball%20Adaptive%20Growth%20algorithm%0A%28SlingBAG%29%20for%203D%20PAI%2C%20which%20shows%20ability%20in%20high-quality%203D%20PAI%0Areconstruction%20both%20under%20extremely%20sparse%20view%20and%20limited%20view.%0A%20%20We%20established%20the%20point%20cloud%20dataset%20in%20PAI%2C%20and%20used%20unique%20differentiable%0Arapid%20radiator%20based%20on%20the%20spherical%20decomposition%20strategy%20and%20the%20randomly%0Ainitialized%20point%20cloud%20adaptively%20optimized%20according%20to%20sparse%20sensor%20data.%0AEach%20point%20undergoes%20updates%20in%203D%20coordinates%2C%20initial%20pressure%2C%20and%0Aresolution%20%28denoted%20by%20the%20radius%20of%20ball%29.%20Points%20undergo%20adaptive%20growth%0Aduring%20iterative%20process%2C%20including%20point%20destroying%2C%20splitting%20and%20duplicating%0Aalong%20the%20gradient%20of%20their%20positions%2C%20manifesting%20the%20sliding%20ball%20effect.%0A%20%20Finally%2C%20our%20point%20cloud%20to%20voxel%20grid%20shader%20renders%20the%20final%0Areconstruction%20results.%20Simulation%20and%20in%20vivo%20experiments%20demonstrate%20that%20our%0ASlingBAG%20reconstruction%20result%27s%20SNR%20can%20be%20more%20than%2040%20dB%20under%20extremely%0Asparse%20view%2C%20while%20the%20SNR%20of%20traditional%20back-projection%20algorithm%27s%20result%20is%0Aless%20than%2020%20dB.%20Moreover%2C%20the%20result%20of%20SlingBAG%27s%20structural%20similarity%20to%0Athe%20ground%20truth%20is%20significantly%20higher%2C%20with%20an%20SSIM%20value%20of%2095.6%25.%0A%20%20Notably%2C%20our%20differentiable%20rapid%20radiator%20can%20conduct%20forward%20PA%20simulation%0Ain%20homogeneous%2C%20non-viscous%20media%20substantially%20faster%20than%20current%20methods%0Athat%20numerically%20simulate%20the%20wave%20propagation%2C%20such%20as%20k-Wave.%20The%20dataset%20and%0Aall%20code%20will%20be%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11781v1&entry.124074799=Read"},
{"title": "3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods", "author": "Milena T. Bagdasarian and Paul Knoll and Florian Barthel and Anna Hilsmann and Peter Eisert and Wieland Morgenstern", "abstract": "  We present a work-in-progress survey on 3D Gaussian Splatting compression\nmethods, focusing on their statistical performance across various benchmarks.\nThis survey aims to facilitate comparability by summarizing key statistics of\ndifferent compression approaches in a tabulated format. The datasets evaluated\ninclude TanksAndTemples, MipNeRF360, DeepBlending, and SyntheticNeRF. For each\nmethod, we report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity\nIndex (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the\nresultant size in megabytes (MB), as provided by the respective authors. This\nis an ongoing, open project, and we invite contributions from the research\ncommunity as GitHub issues or pull requests. Please visit\nhttp://w-m.github.io/3dgs-compression-survey/ for more information and a\nsortable version of the table.\n", "link": "http://arxiv.org/abs/2407.09510v2", "date": "2024-07-16", "relevancy": 2.9735, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6604}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods&body=Title%3A%203DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods%0AAuthor%3A%20Milena%20T.%20Bagdasarian%20and%20Paul%20Knoll%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%20and%20Wieland%20Morgenstern%0AAbstract%3A%20%20%20We%20present%20a%20work-in-progress%20survey%20on%203D%20Gaussian%20Splatting%20compression%0Amethods%2C%20focusing%20on%20their%20statistical%20performance%20across%20various%20benchmarks.%0AThis%20survey%20aims%20to%20facilitate%20comparability%20by%20summarizing%20key%20statistics%20of%0Adifferent%20compression%20approaches%20in%20a%20tabulated%20format.%20The%20datasets%20evaluated%0Ainclude%20TanksAndTemples%2C%20MipNeRF360%2C%20DeepBlending%2C%20and%20SyntheticNeRF.%20For%20each%0Amethod%2C%20we%20report%20the%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%2C%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%2C%20and%20the%0Aresultant%20size%20in%20megabytes%20%28MB%29%2C%20as%20provided%20by%20the%20respective%20authors.%20This%0Ais%20an%20ongoing%2C%20open%20project%2C%20and%20we%20invite%20contributions%20from%20the%20research%0Acommunity%20as%20GitHub%20issues%20or%20pull%20requests.%20Please%20visit%0Ahttp%3A//w-m.github.io/3dgs-compression-survey/%20for%20more%20information%20and%20a%0Asortable%20version%20of%20the%20table.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS.zip%253A%2520A%2520survey%2520on%25203D%2520Gaussian%2520Splatting%2520Compression%2520Methods%26entry.906535625%3DMilena%2520T.%2520Bagdasarian%2520and%2520Paul%2520Knoll%2520and%2520Florian%2520Barthel%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%2520and%2520Wieland%2520Morgenstern%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520work-in-progress%2520survey%2520on%25203D%2520Gaussian%2520Splatting%2520compression%250Amethods%252C%2520focusing%2520on%2520their%2520statistical%2520performance%2520across%2520various%2520benchmarks.%250AThis%2520survey%2520aims%2520to%2520facilitate%2520comparability%2520by%2520summarizing%2520key%2520statistics%2520of%250Adifferent%2520compression%2520approaches%2520in%2520a%2520tabulated%2520format.%2520The%2520datasets%2520evaluated%250Ainclude%2520TanksAndTemples%252C%2520MipNeRF360%252C%2520DeepBlending%252C%2520and%2520SyntheticNeRF.%2520For%2520each%250Amethod%252C%2520we%2520report%2520the%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529%252C%2520Structural%2520Similarity%250AIndex%2520%2528SSIM%2529%252C%2520Learned%2520Perceptual%2520Image%2520Patch%2520Similarity%2520%2528LPIPS%2529%252C%2520and%2520the%250Aresultant%2520size%2520in%2520megabytes%2520%2528MB%2529%252C%2520as%2520provided%2520by%2520the%2520respective%2520authors.%2520This%250Ais%2520an%2520ongoing%252C%2520open%2520project%252C%2520and%2520we%2520invite%2520contributions%2520from%2520the%2520research%250Acommunity%2520as%2520GitHub%2520issues%2520or%2520pull%2520requests.%2520Please%2520visit%250Ahttp%253A//w-m.github.io/3dgs-compression-survey/%2520for%2520more%2520information%2520and%2520a%250Asortable%2520version%2520of%2520the%2520table.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods&entry.906535625=Milena%20T.%20Bagdasarian%20and%20Paul%20Knoll%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%20and%20Wieland%20Morgenstern&entry.1292438233=%20%20We%20present%20a%20work-in-progress%20survey%20on%203D%20Gaussian%20Splatting%20compression%0Amethods%2C%20focusing%20on%20their%20statistical%20performance%20across%20various%20benchmarks.%0AThis%20survey%20aims%20to%20facilitate%20comparability%20by%20summarizing%20key%20statistics%20of%0Adifferent%20compression%20approaches%20in%20a%20tabulated%20format.%20The%20datasets%20evaluated%0Ainclude%20TanksAndTemples%2C%20MipNeRF360%2C%20DeepBlending%2C%20and%20SyntheticNeRF.%20For%20each%0Amethod%2C%20we%20report%20the%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%2C%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%2C%20and%20the%0Aresultant%20size%20in%20megabytes%20%28MB%29%2C%20as%20provided%20by%20the%20respective%20authors.%20This%0Ais%20an%20ongoing%2C%20open%20project%2C%20and%20we%20invite%20contributions%20from%20the%20research%0Acommunity%20as%20GitHub%20issues%20or%20pull%20requests.%20Please%20visit%0Ahttp%3A//w-m.github.io/3dgs-compression-survey/%20for%20more%20information%20and%20a%0Asortable%20version%20of%20the%20table.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09510v2&entry.124074799=Read"},
{"title": "UrbanWorld: An Urban World Model for 3D City Generation", "author": "Yu Shang and Jiansheng Chen and Hangyu Fan and Jingtao Ding and Jie Feng and Yong Li", "abstract": "  Cities, as the most fundamental environment of human life, encompass diverse\nphysical elements such as buildings, roads and vegetation with complex\ninterconnection. Crafting realistic, interactive 3D urban environments plays a\ncrucial role in constructing AI agents capable of perceiving, decision-making,\nand acting like humans in real-world environments. However, creating\nhigh-fidelity 3D urban environments usually entails extensive manual labor from\ndesigners, involving intricate detailing and accurate representation of complex\nurban features. Therefore, how to accomplish this in an automatical way remains\na longstanding challenge. Toward this problem, we propose UrbanWorld, the first\ngenerative urban world model that can automatically create a customized,\nrealistic and interactive 3D urban world with flexible control conditions.\nUrbanWorld incorporates four key stages in the automatical crafting pipeline:\n3D layout generation from openly accessible OSM data, urban scene planning and\ndesigning with a powerful urban multimodal large language model (Urban MLLM),\ncontrollable urban asset rendering with advanced 3D diffusion techniques, and\nfinally the MLLM-assisted scene refinement. The crafted high-fidelity 3D urban\nenvironments enable realistic feedback and interactions for general AI and\nmachine perceptual systems in simulations. We are working on contributing\nUrbanWorld as an open-source and versatile platform for evaluating and\nimproving AI abilities in perception, decision-making, and interaction in\nrealistic urban environments.\n", "link": "http://arxiv.org/abs/2407.11965v1", "date": "2024-07-16", "relevancy": 2.9517, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5874}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanWorld%3A%20An%20Urban%20World%20Model%20for%203D%20City%20Generation&body=Title%3A%20UrbanWorld%3A%20An%20Urban%20World%20Model%20for%203D%20City%20Generation%0AAuthor%3A%20Yu%20Shang%20and%20Jiansheng%20Chen%20and%20Hangyu%20Fan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Yong%20Li%0AAbstract%3A%20%20%20Cities%2C%20as%20the%20most%20fundamental%20environment%20of%20human%20life%2C%20encompass%20diverse%0Aphysical%20elements%20such%20as%20buildings%2C%20roads%20and%20vegetation%20with%20complex%0Ainterconnection.%20Crafting%20realistic%2C%20interactive%203D%20urban%20environments%20plays%20a%0Acrucial%20role%20in%20constructing%20AI%20agents%20capable%20of%20perceiving%2C%20decision-making%2C%0Aand%20acting%20like%20humans%20in%20real-world%20environments.%20However%2C%20creating%0Ahigh-fidelity%203D%20urban%20environments%20usually%20entails%20extensive%20manual%20labor%20from%0Adesigners%2C%20involving%20intricate%20detailing%20and%20accurate%20representation%20of%20complex%0Aurban%20features.%20Therefore%2C%20how%20to%20accomplish%20this%20in%20an%20automatical%20way%20remains%0Aa%20longstanding%20challenge.%20Toward%20this%20problem%2C%20we%20propose%20UrbanWorld%2C%20the%20first%0Agenerative%20urban%20world%20model%20that%20can%20automatically%20create%20a%20customized%2C%0Arealistic%20and%20interactive%203D%20urban%20world%20with%20flexible%20control%20conditions.%0AUrbanWorld%20incorporates%20four%20key%20stages%20in%20the%20automatical%20crafting%20pipeline%3A%0A3D%20layout%20generation%20from%20openly%20accessible%20OSM%20data%2C%20urban%20scene%20planning%20and%0Adesigning%20with%20a%20powerful%20urban%20multimodal%20large%20language%20model%20%28Urban%20MLLM%29%2C%0Acontrollable%20urban%20asset%20rendering%20with%20advanced%203D%20diffusion%20techniques%2C%20and%0Afinally%20the%20MLLM-assisted%20scene%20refinement.%20The%20crafted%20high-fidelity%203D%20urban%0Aenvironments%20enable%20realistic%20feedback%20and%20interactions%20for%20general%20AI%20and%0Amachine%20perceptual%20systems%20in%20simulations.%20We%20are%20working%20on%20contributing%0AUrbanWorld%20as%20an%20open-source%20and%20versatile%20platform%20for%20evaluating%20and%0Aimproving%20AI%20abilities%20in%20perception%2C%20decision-making%2C%20and%20interaction%20in%0Arealistic%20urban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanWorld%253A%2520An%2520Urban%2520World%2520Model%2520for%25203D%2520City%2520Generation%26entry.906535625%3DYu%2520Shang%2520and%2520Jiansheng%2520Chen%2520and%2520Hangyu%2520Fan%2520and%2520Jingtao%2520Ding%2520and%2520Jie%2520Feng%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Cities%252C%2520as%2520the%2520most%2520fundamental%2520environment%2520of%2520human%2520life%252C%2520encompass%2520diverse%250Aphysical%2520elements%2520such%2520as%2520buildings%252C%2520roads%2520and%2520vegetation%2520with%2520complex%250Ainterconnection.%2520Crafting%2520realistic%252C%2520interactive%25203D%2520urban%2520environments%2520plays%2520a%250Acrucial%2520role%2520in%2520constructing%2520AI%2520agents%2520capable%2520of%2520perceiving%252C%2520decision-making%252C%250Aand%2520acting%2520like%2520humans%2520in%2520real-world%2520environments.%2520However%252C%2520creating%250Ahigh-fidelity%25203D%2520urban%2520environments%2520usually%2520entails%2520extensive%2520manual%2520labor%2520from%250Adesigners%252C%2520involving%2520intricate%2520detailing%2520and%2520accurate%2520representation%2520of%2520complex%250Aurban%2520features.%2520Therefore%252C%2520how%2520to%2520accomplish%2520this%2520in%2520an%2520automatical%2520way%2520remains%250Aa%2520longstanding%2520challenge.%2520Toward%2520this%2520problem%252C%2520we%2520propose%2520UrbanWorld%252C%2520the%2520first%250Agenerative%2520urban%2520world%2520model%2520that%2520can%2520automatically%2520create%2520a%2520customized%252C%250Arealistic%2520and%2520interactive%25203D%2520urban%2520world%2520with%2520flexible%2520control%2520conditions.%250AUrbanWorld%2520incorporates%2520four%2520key%2520stages%2520in%2520the%2520automatical%2520crafting%2520pipeline%253A%250A3D%2520layout%2520generation%2520from%2520openly%2520accessible%2520OSM%2520data%252C%2520urban%2520scene%2520planning%2520and%250Adesigning%2520with%2520a%2520powerful%2520urban%2520multimodal%2520large%2520language%2520model%2520%2528Urban%2520MLLM%2529%252C%250Acontrollable%2520urban%2520asset%2520rendering%2520with%2520advanced%25203D%2520diffusion%2520techniques%252C%2520and%250Afinally%2520the%2520MLLM-assisted%2520scene%2520refinement.%2520The%2520crafted%2520high-fidelity%25203D%2520urban%250Aenvironments%2520enable%2520realistic%2520feedback%2520and%2520interactions%2520for%2520general%2520AI%2520and%250Amachine%2520perceptual%2520systems%2520in%2520simulations.%2520We%2520are%2520working%2520on%2520contributing%250AUrbanWorld%2520as%2520an%2520open-source%2520and%2520versatile%2520platform%2520for%2520evaluating%2520and%250Aimproving%2520AI%2520abilities%2520in%2520perception%252C%2520decision-making%252C%2520and%2520interaction%2520in%250Arealistic%2520urban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanWorld%3A%20An%20Urban%20World%20Model%20for%203D%20City%20Generation&entry.906535625=Yu%20Shang%20and%20Jiansheng%20Chen%20and%20Hangyu%20Fan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Yong%20Li&entry.1292438233=%20%20Cities%2C%20as%20the%20most%20fundamental%20environment%20of%20human%20life%2C%20encompass%20diverse%0Aphysical%20elements%20such%20as%20buildings%2C%20roads%20and%20vegetation%20with%20complex%0Ainterconnection.%20Crafting%20realistic%2C%20interactive%203D%20urban%20environments%20plays%20a%0Acrucial%20role%20in%20constructing%20AI%20agents%20capable%20of%20perceiving%2C%20decision-making%2C%0Aand%20acting%20like%20humans%20in%20real-world%20environments.%20However%2C%20creating%0Ahigh-fidelity%203D%20urban%20environments%20usually%20entails%20extensive%20manual%20labor%20from%0Adesigners%2C%20involving%20intricate%20detailing%20and%20accurate%20representation%20of%20complex%0Aurban%20features.%20Therefore%2C%20how%20to%20accomplish%20this%20in%20an%20automatical%20way%20remains%0Aa%20longstanding%20challenge.%20Toward%20this%20problem%2C%20we%20propose%20UrbanWorld%2C%20the%20first%0Agenerative%20urban%20world%20model%20that%20can%20automatically%20create%20a%20customized%2C%0Arealistic%20and%20interactive%203D%20urban%20world%20with%20flexible%20control%20conditions.%0AUrbanWorld%20incorporates%20four%20key%20stages%20in%20the%20automatical%20crafting%20pipeline%3A%0A3D%20layout%20generation%20from%20openly%20accessible%20OSM%20data%2C%20urban%20scene%20planning%20and%0Adesigning%20with%20a%20powerful%20urban%20multimodal%20large%20language%20model%20%28Urban%20MLLM%29%2C%0Acontrollable%20urban%20asset%20rendering%20with%20advanced%203D%20diffusion%20techniques%2C%20and%0Afinally%20the%20MLLM-assisted%20scene%20refinement.%20The%20crafted%20high-fidelity%203D%20urban%0Aenvironments%20enable%20realistic%20feedback%20and%20interactions%20for%20general%20AI%20and%0Amachine%20perceptual%20systems%20in%20simulations.%20We%20are%20working%20on%20contributing%0AUrbanWorld%20as%20an%20open-source%20and%20versatile%20platform%20for%20evaluating%20and%0Aimproving%20AI%20abilities%20in%20perception%2C%20decision-making%2C%20and%20interaction%20in%0Arealistic%20urban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11965v1&entry.124074799=Read"},
{"title": "H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense\n  Mapping Using Hierarchical Hybrid Representation", "author": "Chenxing Jiang and Yiming Luo and Boyu Zhou and Shaojie Shen", "abstract": "  In recent years, implicit online dense mapping methods have achieved\nhigh-quality reconstruction results, showcasing great potential in robotics,\nAR/VR, and digital twins applications. However, existing methods struggle with\nslow texture modeling which limits their real-time performance. To address\nthese limitations, we propose a NeRF-based dense mapping method that enables\nfaster and higher-quality reconstruction. To improve texture modeling, we\nintroduce quasi-heterogeneous feature grids, which inherit the fast querying\nability of uniform feature grids while adapting to varying levels of texture\ncomplexity. Besides, we present a gradient-aided coverage-maximizing strategy\nfor keyframe selection that enables the selected keyframes to exhibit a closer\nfocus on rich-textured regions and a broader scope for weak-textured areas.\nExperimental results demonstrate that our method surpasses existing NeRF-based\napproaches in texture fidelity, geometry accuracy, and time consumption. The\ncode for our method will be available at:\nhttps://github.com/SYSU-STAR/H3-Mapping.\n", "link": "http://arxiv.org/abs/2403.10821v3", "date": "2024-07-16", "relevancy": 2.9267, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6162}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5846}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H3-Mapping%3A%20Quasi-Heterogeneous%20Feature%20Grids%20for%20Real-time%20Dense%0A%20%20Mapping%20Using%20Hierarchical%20Hybrid%20Representation&body=Title%3A%20H3-Mapping%3A%20Quasi-Heterogeneous%20Feature%20Grids%20for%20Real-time%20Dense%0A%20%20Mapping%20Using%20Hierarchical%20Hybrid%20Representation%0AAuthor%3A%20Chenxing%20Jiang%20and%20Yiming%20Luo%20and%20Boyu%20Zhou%20and%20Shaojie%20Shen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20implicit%20online%20dense%20mapping%20methods%20have%20achieved%0Ahigh-quality%20reconstruction%20results%2C%20showcasing%20great%20potential%20in%20robotics%2C%0AAR/VR%2C%20and%20digital%20twins%20applications.%20However%2C%20existing%20methods%20struggle%20with%0Aslow%20texture%20modeling%20which%20limits%20their%20real-time%20performance.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20NeRF-based%20dense%20mapping%20method%20that%20enables%0Afaster%20and%20higher-quality%20reconstruction.%20To%20improve%20texture%20modeling%2C%20we%0Aintroduce%20quasi-heterogeneous%20feature%20grids%2C%20which%20inherit%20the%20fast%20querying%0Aability%20of%20uniform%20feature%20grids%20while%20adapting%20to%20varying%20levels%20of%20texture%0Acomplexity.%20Besides%2C%20we%20present%20a%20gradient-aided%20coverage-maximizing%20strategy%0Afor%20keyframe%20selection%20that%20enables%20the%20selected%20keyframes%20to%20exhibit%20a%20closer%0Afocus%20on%20rich-textured%20regions%20and%20a%20broader%20scope%20for%20weak-textured%20areas.%0AExperimental%20results%20demonstrate%20that%20our%20method%20surpasses%20existing%20NeRF-based%0Aapproaches%20in%20texture%20fidelity%2C%20geometry%20accuracy%2C%20and%20time%20consumption.%20The%0Acode%20for%20our%20method%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SYSU-STAR/H3-Mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH3-Mapping%253A%2520Quasi-Heterogeneous%2520Feature%2520Grids%2520for%2520Real-time%2520Dense%250A%2520%2520Mapping%2520Using%2520Hierarchical%2520Hybrid%2520Representation%26entry.906535625%3DChenxing%2520Jiang%2520and%2520Yiming%2520Luo%2520and%2520Boyu%2520Zhou%2520and%2520Shaojie%2520Shen%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520implicit%2520online%2520dense%2520mapping%2520methods%2520have%2520achieved%250Ahigh-quality%2520reconstruction%2520results%252C%2520showcasing%2520great%2520potential%2520in%2520robotics%252C%250AAR/VR%252C%2520and%2520digital%2520twins%2520applications.%2520However%252C%2520existing%2520methods%2520struggle%2520with%250Aslow%2520texture%2520modeling%2520which%2520limits%2520their%2520real-time%2520performance.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520a%2520NeRF-based%2520dense%2520mapping%2520method%2520that%2520enables%250Afaster%2520and%2520higher-quality%2520reconstruction.%2520To%2520improve%2520texture%2520modeling%252C%2520we%250Aintroduce%2520quasi-heterogeneous%2520feature%2520grids%252C%2520which%2520inherit%2520the%2520fast%2520querying%250Aability%2520of%2520uniform%2520feature%2520grids%2520while%2520adapting%2520to%2520varying%2520levels%2520of%2520texture%250Acomplexity.%2520Besides%252C%2520we%2520present%2520a%2520gradient-aided%2520coverage-maximizing%2520strategy%250Afor%2520keyframe%2520selection%2520that%2520enables%2520the%2520selected%2520keyframes%2520to%2520exhibit%2520a%2520closer%250Afocus%2520on%2520rich-textured%2520regions%2520and%2520a%2520broader%2520scope%2520for%2520weak-textured%2520areas.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520surpasses%2520existing%2520NeRF-based%250Aapproaches%2520in%2520texture%2520fidelity%252C%2520geometry%2520accuracy%252C%2520and%2520time%2520consumption.%2520The%250Acode%2520for%2520our%2520method%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/SYSU-STAR/H3-Mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H3-Mapping%3A%20Quasi-Heterogeneous%20Feature%20Grids%20for%20Real-time%20Dense%0A%20%20Mapping%20Using%20Hierarchical%20Hybrid%20Representation&entry.906535625=Chenxing%20Jiang%20and%20Yiming%20Luo%20and%20Boyu%20Zhou%20and%20Shaojie%20Shen&entry.1292438233=%20%20In%20recent%20years%2C%20implicit%20online%20dense%20mapping%20methods%20have%20achieved%0Ahigh-quality%20reconstruction%20results%2C%20showcasing%20great%20potential%20in%20robotics%2C%0AAR/VR%2C%20and%20digital%20twins%20applications.%20However%2C%20existing%20methods%20struggle%20with%0Aslow%20texture%20modeling%20which%20limits%20their%20real-time%20performance.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20NeRF-based%20dense%20mapping%20method%20that%20enables%0Afaster%20and%20higher-quality%20reconstruction.%20To%20improve%20texture%20modeling%2C%20we%0Aintroduce%20quasi-heterogeneous%20feature%20grids%2C%20which%20inherit%20the%20fast%20querying%0Aability%20of%20uniform%20feature%20grids%20while%20adapting%20to%20varying%20levels%20of%20texture%0Acomplexity.%20Besides%2C%20we%20present%20a%20gradient-aided%20coverage-maximizing%20strategy%0Afor%20keyframe%20selection%20that%20enables%20the%20selected%20keyframes%20to%20exhibit%20a%20closer%0Afocus%20on%20rich-textured%20regions%20and%20a%20broader%20scope%20for%20weak-textured%20areas.%0AExperimental%20results%20demonstrate%20that%20our%20method%20surpasses%20existing%20NeRF-based%0Aapproaches%20in%20texture%20fidelity%2C%20geometry%20accuracy%2C%20and%20time%20consumption.%20The%0Acode%20for%20our%20method%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SYSU-STAR/H3-Mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10821v3&entry.124074799=Read"},
{"title": "REMM:Rotation-Equivariant Framework for End-to-End Multimodal Image\n  Matching", "author": "Han Nie and Bin Luo and Jun Liu and Zhitao Fu and Weixing Liu and Xin Su", "abstract": "  We present REMM, a rotation-equivariant framework for end-to-end multimodal\nimage matching, which fully encodes rotational differences of descriptors in\nthe whole matching pipeline. Previous learning-based methods mainly focus on\nextracting modal-invariant descriptors, while consistently ignoring the\nrotational invariance. In this paper, we demonstrate that our REMM is very\nuseful for multimodal image matching, including multimodal feature learning\nmodule and cyclic shift module. We first learn modal-invariant features through\nthe multimodal feature learning module. Then, we design the cyclic shift module\nto rotationally encode the descriptors, greatly improving the performance of\nrotation-equivariant matching, which makes them robust to any angle. To\nvalidate our method, we establish a comprehensive rotation and scale-matching\nbenchmark for evaluating the anti-rotation performance of multimodal images,\nwhich contains a combination of multi-angle and multi-scale transformations\nfrom four publicly available datasets. Extensive experiments show that our\nmethod outperforms existing methods in benchmarking and generalizes well to\nindependent datasets. Additionally, we conducted an in-depth analysis of the\nkey components of the REMM to validate the improvements brought about by the\ncyclic shift module. Code and dataset at https://github.com/HanNieWHU/REMM.\n", "link": "http://arxiv.org/abs/2407.11637v1", "date": "2024-07-16", "relevancy": 2.9095, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6133}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REMM%3ARotation-Equivariant%20Framework%20for%20End-to-End%20Multimodal%20Image%0A%20%20Matching&body=Title%3A%20REMM%3ARotation-Equivariant%20Framework%20for%20End-to-End%20Multimodal%20Image%0A%20%20Matching%0AAuthor%3A%20Han%20Nie%20and%20Bin%20Luo%20and%20Jun%20Liu%20and%20Zhitao%20Fu%20and%20Weixing%20Liu%20and%20Xin%20Su%0AAbstract%3A%20%20%20We%20present%20REMM%2C%20a%20rotation-equivariant%20framework%20for%20end-to-end%20multimodal%0Aimage%20matching%2C%20which%20fully%20encodes%20rotational%20differences%20of%20descriptors%20in%0Athe%20whole%20matching%20pipeline.%20Previous%20learning-based%20methods%20mainly%20focus%20on%0Aextracting%20modal-invariant%20descriptors%2C%20while%20consistently%20ignoring%20the%0Arotational%20invariance.%20In%20this%20paper%2C%20we%20demonstrate%20that%20our%20REMM%20is%20very%0Auseful%20for%20multimodal%20image%20matching%2C%20including%20multimodal%20feature%20learning%0Amodule%20and%20cyclic%20shift%20module.%20We%20first%20learn%20modal-invariant%20features%20through%0Athe%20multimodal%20feature%20learning%20module.%20Then%2C%20we%20design%20the%20cyclic%20shift%20module%0Ato%20rotationally%20encode%20the%20descriptors%2C%20greatly%20improving%20the%20performance%20of%0Arotation-equivariant%20matching%2C%20which%20makes%20them%20robust%20to%20any%20angle.%20To%0Avalidate%20our%20method%2C%20we%20establish%20a%20comprehensive%20rotation%20and%20scale-matching%0Abenchmark%20for%20evaluating%20the%20anti-rotation%20performance%20of%20multimodal%20images%2C%0Awhich%20contains%20a%20combination%20of%20multi-angle%20and%20multi-scale%20transformations%0Afrom%20four%20publicly%20available%20datasets.%20Extensive%20experiments%20show%20that%20our%0Amethod%20outperforms%20existing%20methods%20in%20benchmarking%20and%20generalizes%20well%20to%0Aindependent%20datasets.%20Additionally%2C%20we%20conducted%20an%20in-depth%20analysis%20of%20the%0Akey%20components%20of%20the%20REMM%20to%20validate%20the%20improvements%20brought%20about%20by%20the%0Acyclic%20shift%20module.%20Code%20and%20dataset%20at%20https%3A//github.com/HanNieWHU/REMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREMM%253ARotation-Equivariant%2520Framework%2520for%2520End-to-End%2520Multimodal%2520Image%250A%2520%2520Matching%26entry.906535625%3DHan%2520Nie%2520and%2520Bin%2520Luo%2520and%2520Jun%2520Liu%2520and%2520Zhitao%2520Fu%2520and%2520Weixing%2520Liu%2520and%2520Xin%2520Su%26entry.1292438233%3D%2520%2520We%2520present%2520REMM%252C%2520a%2520rotation-equivariant%2520framework%2520for%2520end-to-end%2520multimodal%250Aimage%2520matching%252C%2520which%2520fully%2520encodes%2520rotational%2520differences%2520of%2520descriptors%2520in%250Athe%2520whole%2520matching%2520pipeline.%2520Previous%2520learning-based%2520methods%2520mainly%2520focus%2520on%250Aextracting%2520modal-invariant%2520descriptors%252C%2520while%2520consistently%2520ignoring%2520the%250Arotational%2520invariance.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520our%2520REMM%2520is%2520very%250Auseful%2520for%2520multimodal%2520image%2520matching%252C%2520including%2520multimodal%2520feature%2520learning%250Amodule%2520and%2520cyclic%2520shift%2520module.%2520We%2520first%2520learn%2520modal-invariant%2520features%2520through%250Athe%2520multimodal%2520feature%2520learning%2520module.%2520Then%252C%2520we%2520design%2520the%2520cyclic%2520shift%2520module%250Ato%2520rotationally%2520encode%2520the%2520descriptors%252C%2520greatly%2520improving%2520the%2520performance%2520of%250Arotation-equivariant%2520matching%252C%2520which%2520makes%2520them%2520robust%2520to%2520any%2520angle.%2520To%250Avalidate%2520our%2520method%252C%2520we%2520establish%2520a%2520comprehensive%2520rotation%2520and%2520scale-matching%250Abenchmark%2520for%2520evaluating%2520the%2520anti-rotation%2520performance%2520of%2520multimodal%2520images%252C%250Awhich%2520contains%2520a%2520combination%2520of%2520multi-angle%2520and%2520multi-scale%2520transformations%250Afrom%2520four%2520publicly%2520available%2520datasets.%2520Extensive%2520experiments%2520show%2520that%2520our%250Amethod%2520outperforms%2520existing%2520methods%2520in%2520benchmarking%2520and%2520generalizes%2520well%2520to%250Aindependent%2520datasets.%2520Additionally%252C%2520we%2520conducted%2520an%2520in-depth%2520analysis%2520of%2520the%250Akey%2520components%2520of%2520the%2520REMM%2520to%2520validate%2520the%2520improvements%2520brought%2520about%2520by%2520the%250Acyclic%2520shift%2520module.%2520Code%2520and%2520dataset%2520at%2520https%253A//github.com/HanNieWHU/REMM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REMM%3ARotation-Equivariant%20Framework%20for%20End-to-End%20Multimodal%20Image%0A%20%20Matching&entry.906535625=Han%20Nie%20and%20Bin%20Luo%20and%20Jun%20Liu%20and%20Zhitao%20Fu%20and%20Weixing%20Liu%20and%20Xin%20Su&entry.1292438233=%20%20We%20present%20REMM%2C%20a%20rotation-equivariant%20framework%20for%20end-to-end%20multimodal%0Aimage%20matching%2C%20which%20fully%20encodes%20rotational%20differences%20of%20descriptors%20in%0Athe%20whole%20matching%20pipeline.%20Previous%20learning-based%20methods%20mainly%20focus%20on%0Aextracting%20modal-invariant%20descriptors%2C%20while%20consistently%20ignoring%20the%0Arotational%20invariance.%20In%20this%20paper%2C%20we%20demonstrate%20that%20our%20REMM%20is%20very%0Auseful%20for%20multimodal%20image%20matching%2C%20including%20multimodal%20feature%20learning%0Amodule%20and%20cyclic%20shift%20module.%20We%20first%20learn%20modal-invariant%20features%20through%0Athe%20multimodal%20feature%20learning%20module.%20Then%2C%20we%20design%20the%20cyclic%20shift%20module%0Ato%20rotationally%20encode%20the%20descriptors%2C%20greatly%20improving%20the%20performance%20of%0Arotation-equivariant%20matching%2C%20which%20makes%20them%20robust%20to%20any%20angle.%20To%0Avalidate%20our%20method%2C%20we%20establish%20a%20comprehensive%20rotation%20and%20scale-matching%0Abenchmark%20for%20evaluating%20the%20anti-rotation%20performance%20of%20multimodal%20images%2C%0Awhich%20contains%20a%20combination%20of%20multi-angle%20and%20multi-scale%20transformations%0Afrom%20four%20publicly%20available%20datasets.%20Extensive%20experiments%20show%20that%20our%0Amethod%20outperforms%20existing%20methods%20in%20benchmarking%20and%20generalizes%20well%20to%0Aindependent%20datasets.%20Additionally%2C%20we%20conducted%20an%20in-depth%20analysis%20of%20the%0Akey%20components%20of%20the%20REMM%20to%20validate%20the%20improvements%20brought%20about%20by%20the%0Acyclic%20shift%20module.%20Code%20and%20dataset%20at%20https%3A//github.com/HanNieWHU/REMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11637v1&entry.124074799=Read"},
{"title": "SGIFormer: Semantic-guided and Geometric-enhanced Interleaving\n  Transformer for 3D Instance Segmentation", "author": "Lei Yao and Yi Wang and Moyun Liu and Lap-Pui Chau", "abstract": "  In recent years, transformer-based models have exhibited considerable\npotential in point cloud instance segmentation. Despite the promising\nperformance achieved by existing methods, they encounter challenges such as\ninstance query initialization problems and excessive reliance on stacked\nlayers, rendering them incompatible with large-scale 3D scenes. This paper\nintroduces a novel method, named SGIFormer, for 3D instance segmentation, which\nis composed of the Semantic-guided Mix Query (SMQ) initialization and the\nGeometric-enhanced Interleaving Transformer (GIT) decoder. Specifically, the\nprinciple of our SMQ initialization scheme is to leverage the predicted\nvoxel-wise semantic information to implicitly generate the scene-aware query,\nyielding adequate scene prior and compensating for the learnable query set.\nSubsequently, we feed the formed overall query into our GIT decoder to\nalternately refine instance query and global scene features for further\ncapturing fine-grained information and reducing complex design intricacies\nsimultaneously. To emphasize geometric property, we consider bias estimation as\nan auxiliary task and progressively integrate shifted point coordinates\nembedding to reinforce instance localization. SGIFormer attains\nstate-of-the-art performance on ScanNet V2, ScanNet200 datasets, and the\nchallenging high-fidelity ScanNet++ benchmark, striking a balance between\naccuracy and efficiency. The code, weights, and demo videos are publicly\navailable at https://rayyoh.github.io/sgiformer.\n", "link": "http://arxiv.org/abs/2407.11564v1", "date": "2024-07-16", "relevancy": 2.8634, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5966}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5627}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGIFormer%3A%20Semantic-guided%20and%20Geometric-enhanced%20Interleaving%0A%20%20Transformer%20for%203D%20Instance%20Segmentation&body=Title%3A%20SGIFormer%3A%20Semantic-guided%20and%20Geometric-enhanced%20Interleaving%0A%20%20Transformer%20for%203D%20Instance%20Segmentation%0AAuthor%3A%20Lei%20Yao%20and%20Yi%20Wang%20and%20Moyun%20Liu%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%20In%20recent%20years%2C%20transformer-based%20models%20have%20exhibited%20considerable%0Apotential%20in%20point%20cloud%20instance%20segmentation.%20Despite%20the%20promising%0Aperformance%20achieved%20by%20existing%20methods%2C%20they%20encounter%20challenges%20such%20as%0Ainstance%20query%20initialization%20problems%20and%20excessive%20reliance%20on%20stacked%0Alayers%2C%20rendering%20them%20incompatible%20with%20large-scale%203D%20scenes.%20This%20paper%0Aintroduces%20a%20novel%20method%2C%20named%20SGIFormer%2C%20for%203D%20instance%20segmentation%2C%20which%0Ais%20composed%20of%20the%20Semantic-guided%20Mix%20Query%20%28SMQ%29%20initialization%20and%20the%0AGeometric-enhanced%20Interleaving%20Transformer%20%28GIT%29%20decoder.%20Specifically%2C%20the%0Aprinciple%20of%20our%20SMQ%20initialization%20scheme%20is%20to%20leverage%20the%20predicted%0Avoxel-wise%20semantic%20information%20to%20implicitly%20generate%20the%20scene-aware%20query%2C%0Ayielding%20adequate%20scene%20prior%20and%20compensating%20for%20the%20learnable%20query%20set.%0ASubsequently%2C%20we%20feed%20the%20formed%20overall%20query%20into%20our%20GIT%20decoder%20to%0Aalternately%20refine%20instance%20query%20and%20global%20scene%20features%20for%20further%0Acapturing%20fine-grained%20information%20and%20reducing%20complex%20design%20intricacies%0Asimultaneously.%20To%20emphasize%20geometric%20property%2C%20we%20consider%20bias%20estimation%20as%0Aan%20auxiliary%20task%20and%20progressively%20integrate%20shifted%20point%20coordinates%0Aembedding%20to%20reinforce%20instance%20localization.%20SGIFormer%20attains%0Astate-of-the-art%20performance%20on%20ScanNet%20V2%2C%20ScanNet200%20datasets%2C%20and%20the%0Achallenging%20high-fidelity%20ScanNet%2B%2B%20benchmark%2C%20striking%20a%20balance%20between%0Aaccuracy%20and%20efficiency.%20The%20code%2C%20weights%2C%20and%20demo%20videos%20are%20publicly%0Aavailable%20at%20https%3A//rayyoh.github.io/sgiformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGIFormer%253A%2520Semantic-guided%2520and%2520Geometric-enhanced%2520Interleaving%250A%2520%2520Transformer%2520for%25203D%2520Instance%2520Segmentation%26entry.906535625%3DLei%2520Yao%2520and%2520Yi%2520Wang%2520and%2520Moyun%2520Liu%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520transformer-based%2520models%2520have%2520exhibited%2520considerable%250Apotential%2520in%2520point%2520cloud%2520instance%2520segmentation.%2520Despite%2520the%2520promising%250Aperformance%2520achieved%2520by%2520existing%2520methods%252C%2520they%2520encounter%2520challenges%2520such%2520as%250Ainstance%2520query%2520initialization%2520problems%2520and%2520excessive%2520reliance%2520on%2520stacked%250Alayers%252C%2520rendering%2520them%2520incompatible%2520with%2520large-scale%25203D%2520scenes.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520method%252C%2520named%2520SGIFormer%252C%2520for%25203D%2520instance%2520segmentation%252C%2520which%250Ais%2520composed%2520of%2520the%2520Semantic-guided%2520Mix%2520Query%2520%2528SMQ%2529%2520initialization%2520and%2520the%250AGeometric-enhanced%2520Interleaving%2520Transformer%2520%2528GIT%2529%2520decoder.%2520Specifically%252C%2520the%250Aprinciple%2520of%2520our%2520SMQ%2520initialization%2520scheme%2520is%2520to%2520leverage%2520the%2520predicted%250Avoxel-wise%2520semantic%2520information%2520to%2520implicitly%2520generate%2520the%2520scene-aware%2520query%252C%250Ayielding%2520adequate%2520scene%2520prior%2520and%2520compensating%2520for%2520the%2520learnable%2520query%2520set.%250ASubsequently%252C%2520we%2520feed%2520the%2520formed%2520overall%2520query%2520into%2520our%2520GIT%2520decoder%2520to%250Aalternately%2520refine%2520instance%2520query%2520and%2520global%2520scene%2520features%2520for%2520further%250Acapturing%2520fine-grained%2520information%2520and%2520reducing%2520complex%2520design%2520intricacies%250Asimultaneously.%2520To%2520emphasize%2520geometric%2520property%252C%2520we%2520consider%2520bias%2520estimation%2520as%250Aan%2520auxiliary%2520task%2520and%2520progressively%2520integrate%2520shifted%2520point%2520coordinates%250Aembedding%2520to%2520reinforce%2520instance%2520localization.%2520SGIFormer%2520attains%250Astate-of-the-art%2520performance%2520on%2520ScanNet%2520V2%252C%2520ScanNet200%2520datasets%252C%2520and%2520the%250Achallenging%2520high-fidelity%2520ScanNet%252B%252B%2520benchmark%252C%2520striking%2520a%2520balance%2520between%250Aaccuracy%2520and%2520efficiency.%2520The%2520code%252C%2520weights%252C%2520and%2520demo%2520videos%2520are%2520publicly%250Aavailable%2520at%2520https%253A//rayyoh.github.io/sgiformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGIFormer%3A%20Semantic-guided%20and%20Geometric-enhanced%20Interleaving%0A%20%20Transformer%20for%203D%20Instance%20Segmentation&entry.906535625=Lei%20Yao%20and%20Yi%20Wang%20and%20Moyun%20Liu%20and%20Lap-Pui%20Chau&entry.1292438233=%20%20In%20recent%20years%2C%20transformer-based%20models%20have%20exhibited%20considerable%0Apotential%20in%20point%20cloud%20instance%20segmentation.%20Despite%20the%20promising%0Aperformance%20achieved%20by%20existing%20methods%2C%20they%20encounter%20challenges%20such%20as%0Ainstance%20query%20initialization%20problems%20and%20excessive%20reliance%20on%20stacked%0Alayers%2C%20rendering%20them%20incompatible%20with%20large-scale%203D%20scenes.%20This%20paper%0Aintroduces%20a%20novel%20method%2C%20named%20SGIFormer%2C%20for%203D%20instance%20segmentation%2C%20which%0Ais%20composed%20of%20the%20Semantic-guided%20Mix%20Query%20%28SMQ%29%20initialization%20and%20the%0AGeometric-enhanced%20Interleaving%20Transformer%20%28GIT%29%20decoder.%20Specifically%2C%20the%0Aprinciple%20of%20our%20SMQ%20initialization%20scheme%20is%20to%20leverage%20the%20predicted%0Avoxel-wise%20semantic%20information%20to%20implicitly%20generate%20the%20scene-aware%20query%2C%0Ayielding%20adequate%20scene%20prior%20and%20compensating%20for%20the%20learnable%20query%20set.%0ASubsequently%2C%20we%20feed%20the%20formed%20overall%20query%20into%20our%20GIT%20decoder%20to%0Aalternately%20refine%20instance%20query%20and%20global%20scene%20features%20for%20further%0Acapturing%20fine-grained%20information%20and%20reducing%20complex%20design%20intricacies%0Asimultaneously.%20To%20emphasize%20geometric%20property%2C%20we%20consider%20bias%20estimation%20as%0Aan%20auxiliary%20task%20and%20progressively%20integrate%20shifted%20point%20coordinates%0Aembedding%20to%20reinforce%20instance%20localization.%20SGIFormer%20attains%0Astate-of-the-art%20performance%20on%20ScanNet%20V2%2C%20ScanNet200%20datasets%2C%20and%20the%0Achallenging%20high-fidelity%20ScanNet%2B%2B%20benchmark%2C%20striking%20a%20balance%20between%0Aaccuracy%20and%20efficiency.%20The%20code%2C%20weights%2C%20and%20demo%20videos%20are%20publicly%0Aavailable%20at%20https%3A//rayyoh.github.io/sgiformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11564v1&entry.124074799=Read"},
{"title": "GV-Bench: Benchmarking Local Feature Matching for Geometric Verification\n  of Long-term Loop Closure Detection", "author": "Jingwen Yu and Hanjing Ye and Jianhao Jiao and Ping Tan and Hong Zhang", "abstract": "  Visual loop closure detection is an important module in visual simultaneous\nlocalization and mapping (SLAM), which associates current camera observation\nwith previously visited places. Loop closures correct drifts in trajectory\nestimation to build a globally consistent map. However, a false loop closure\ncan be fatal, so verification is required as an additional step to ensure\nrobustness by rejecting the false positive loops. Geometric verification has\nbeen a well-acknowledged solution that leverages spatial clues provided by\nlocal feature matching to find true positives. Existing feature matching\nmethods focus on homography and pose estimation in long-term visual\nlocalization, lacking references for geometric verification. To fill the gap,\nthis paper proposes a unified benchmark targeting geometric verification of\nloop closure detection under long-term conditional variations. Furthermore, we\nevaluate six representative local feature matching methods (handcrafted and\nlearning-based) under the benchmark, with in-depth analysis for limitations and\nfuture directions.\n", "link": "http://arxiv.org/abs/2407.11736v1", "date": "2024-07-16", "relevancy": 2.8494, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6049}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5701}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GV-Bench%3A%20Benchmarking%20Local%20Feature%20Matching%20for%20Geometric%20Verification%0A%20%20of%20Long-term%20Loop%20Closure%20Detection&body=Title%3A%20GV-Bench%3A%20Benchmarking%20Local%20Feature%20Matching%20for%20Geometric%20Verification%0A%20%20of%20Long-term%20Loop%20Closure%20Detection%0AAuthor%3A%20Jingwen%20Yu%20and%20Hanjing%20Ye%20and%20Jianhao%20Jiao%20and%20Ping%20Tan%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Visual%20loop%20closure%20detection%20is%20an%20important%20module%20in%20visual%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%20which%20associates%20current%20camera%20observation%0Awith%20previously%20visited%20places.%20Loop%20closures%20correct%20drifts%20in%20trajectory%0Aestimation%20to%20build%20a%20globally%20consistent%20map.%20However%2C%20a%20false%20loop%20closure%0Acan%20be%20fatal%2C%20so%20verification%20is%20required%20as%20an%20additional%20step%20to%20ensure%0Arobustness%20by%20rejecting%20the%20false%20positive%20loops.%20Geometric%20verification%20has%0Abeen%20a%20well-acknowledged%20solution%20that%20leverages%20spatial%20clues%20provided%20by%0Alocal%20feature%20matching%20to%20find%20true%20positives.%20Existing%20feature%20matching%0Amethods%20focus%20on%20homography%20and%20pose%20estimation%20in%20long-term%20visual%0Alocalization%2C%20lacking%20references%20for%20geometric%20verification.%20To%20fill%20the%20gap%2C%0Athis%20paper%20proposes%20a%20unified%20benchmark%20targeting%20geometric%20verification%20of%0Aloop%20closure%20detection%20under%20long-term%20conditional%20variations.%20Furthermore%2C%20we%0Aevaluate%20six%20representative%20local%20feature%20matching%20methods%20%28handcrafted%20and%0Alearning-based%29%20under%20the%20benchmark%2C%20with%20in-depth%20analysis%20for%20limitations%20and%0Afuture%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGV-Bench%253A%2520Benchmarking%2520Local%2520Feature%2520Matching%2520for%2520Geometric%2520Verification%250A%2520%2520of%2520Long-term%2520Loop%2520Closure%2520Detection%26entry.906535625%3DJingwen%2520Yu%2520and%2520Hanjing%2520Ye%2520and%2520Jianhao%2520Jiao%2520and%2520Ping%2520Tan%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Visual%2520loop%2520closure%2520detection%2520is%2520an%2520important%2520module%2520in%2520visual%2520simultaneous%250Alocalization%2520and%2520mapping%2520%2528SLAM%2529%252C%2520which%2520associates%2520current%2520camera%2520observation%250Awith%2520previously%2520visited%2520places.%2520Loop%2520closures%2520correct%2520drifts%2520in%2520trajectory%250Aestimation%2520to%2520build%2520a%2520globally%2520consistent%2520map.%2520However%252C%2520a%2520false%2520loop%2520closure%250Acan%2520be%2520fatal%252C%2520so%2520verification%2520is%2520required%2520as%2520an%2520additional%2520step%2520to%2520ensure%250Arobustness%2520by%2520rejecting%2520the%2520false%2520positive%2520loops.%2520Geometric%2520verification%2520has%250Abeen%2520a%2520well-acknowledged%2520solution%2520that%2520leverages%2520spatial%2520clues%2520provided%2520by%250Alocal%2520feature%2520matching%2520to%2520find%2520true%2520positives.%2520Existing%2520feature%2520matching%250Amethods%2520focus%2520on%2520homography%2520and%2520pose%2520estimation%2520in%2520long-term%2520visual%250Alocalization%252C%2520lacking%2520references%2520for%2520geometric%2520verification.%2520To%2520fill%2520the%2520gap%252C%250Athis%2520paper%2520proposes%2520a%2520unified%2520benchmark%2520targeting%2520geometric%2520verification%2520of%250Aloop%2520closure%2520detection%2520under%2520long-term%2520conditional%2520variations.%2520Furthermore%252C%2520we%250Aevaluate%2520six%2520representative%2520local%2520feature%2520matching%2520methods%2520%2528handcrafted%2520and%250Alearning-based%2529%2520under%2520the%2520benchmark%252C%2520with%2520in-depth%2520analysis%2520for%2520limitations%2520and%250Afuture%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GV-Bench%3A%20Benchmarking%20Local%20Feature%20Matching%20for%20Geometric%20Verification%0A%20%20of%20Long-term%20Loop%20Closure%20Detection&entry.906535625=Jingwen%20Yu%20and%20Hanjing%20Ye%20and%20Jianhao%20Jiao%20and%20Ping%20Tan%20and%20Hong%20Zhang&entry.1292438233=%20%20Visual%20loop%20closure%20detection%20is%20an%20important%20module%20in%20visual%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%20which%20associates%20current%20camera%20observation%0Awith%20previously%20visited%20places.%20Loop%20closures%20correct%20drifts%20in%20trajectory%0Aestimation%20to%20build%20a%20globally%20consistent%20map.%20However%2C%20a%20false%20loop%20closure%0Acan%20be%20fatal%2C%20so%20verification%20is%20required%20as%20an%20additional%20step%20to%20ensure%0Arobustness%20by%20rejecting%20the%20false%20positive%20loops.%20Geometric%20verification%20has%0Abeen%20a%20well-acknowledged%20solution%20that%20leverages%20spatial%20clues%20provided%20by%0Alocal%20feature%20matching%20to%20find%20true%20positives.%20Existing%20feature%20matching%0Amethods%20focus%20on%20homography%20and%20pose%20estimation%20in%20long-term%20visual%0Alocalization%2C%20lacking%20references%20for%20geometric%20verification.%20To%20fill%20the%20gap%2C%0Athis%20paper%20proposes%20a%20unified%20benchmark%20targeting%20geometric%20verification%20of%0Aloop%20closure%20detection%20under%20long-term%20conditional%20variations.%20Furthermore%2C%20we%0Aevaluate%20six%20representative%20local%20feature%20matching%20methods%20%28handcrafted%20and%0Alearning-based%29%20under%20the%20benchmark%2C%20with%20in-depth%20analysis%20for%20limitations%20and%0Afuture%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11736v1&entry.124074799=Read"},
{"title": "Learning to Make Keypoints Sub-Pixel Accurate", "author": "Shinjeong Kim and Marc Pollefeys and Daniel Barath", "abstract": "  This work addresses the challenge of sub-pixel accuracy in detecting 2D local\nfeatures, a cornerstone problem in computer vision. Despite the advancements\nbrought by neural network-based methods like SuperPoint and ALIKED, these\nmodern approaches lag behind classical ones such as SIFT in keypoint\nlocalization accuracy due to their lack of sub-pixel precision. We propose a\nnovel network that enhances any detector with sub-pixel precision by learning\nan offset vector for detected features, thereby eliminating the need for\ndesigning specialized sub-pixel accurate detectors. This optimization directly\nminimizes test-time evaluation metrics like relative pose error. Through\nextensive testing with both nearest neighbors matching and the recent LightGlue\nmatcher across various real-world datasets, our method consistently outperforms\nexisting methods in accuracy. Moreover, it adds only around 7 ms to the time of\na particular detector. The code is available at\nhttps://github.com/KimSinjeong/keypt2subpx .\n", "link": "http://arxiv.org/abs/2407.11668v1", "date": "2024-07-16", "relevancy": 2.8482, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Make%20Keypoints%20Sub-Pixel%20Accurate&body=Title%3A%20Learning%20to%20Make%20Keypoints%20Sub-Pixel%20Accurate%0AAuthor%3A%20Shinjeong%20Kim%20and%20Marc%20Pollefeys%20and%20Daniel%20Barath%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenge%20of%20sub-pixel%20accuracy%20in%20detecting%202D%20local%0Afeatures%2C%20a%20cornerstone%20problem%20in%20computer%20vision.%20Despite%20the%20advancements%0Abrought%20by%20neural%20network-based%20methods%20like%20SuperPoint%20and%20ALIKED%2C%20these%0Amodern%20approaches%20lag%20behind%20classical%20ones%20such%20as%20SIFT%20in%20keypoint%0Alocalization%20accuracy%20due%20to%20their%20lack%20of%20sub-pixel%20precision.%20We%20propose%20a%0Anovel%20network%20that%20enhances%20any%20detector%20with%20sub-pixel%20precision%20by%20learning%0Aan%20offset%20vector%20for%20detected%20features%2C%20thereby%20eliminating%20the%20need%20for%0Adesigning%20specialized%20sub-pixel%20accurate%20detectors.%20This%20optimization%20directly%0Aminimizes%20test-time%20evaluation%20metrics%20like%20relative%20pose%20error.%20Through%0Aextensive%20testing%20with%20both%20nearest%20neighbors%20matching%20and%20the%20recent%20LightGlue%0Amatcher%20across%20various%20real-world%20datasets%2C%20our%20method%20consistently%20outperforms%0Aexisting%20methods%20in%20accuracy.%20Moreover%2C%20it%20adds%20only%20around%207%20ms%20to%20the%20time%20of%0Aa%20particular%20detector.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/KimSinjeong/keypt2subpx%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Make%2520Keypoints%2520Sub-Pixel%2520Accurate%26entry.906535625%3DShinjeong%2520Kim%2520and%2520Marc%2520Pollefeys%2520and%2520Daniel%2520Barath%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenge%2520of%2520sub-pixel%2520accuracy%2520in%2520detecting%25202D%2520local%250Afeatures%252C%2520a%2520cornerstone%2520problem%2520in%2520computer%2520vision.%2520Despite%2520the%2520advancements%250Abrought%2520by%2520neural%2520network-based%2520methods%2520like%2520SuperPoint%2520and%2520ALIKED%252C%2520these%250Amodern%2520approaches%2520lag%2520behind%2520classical%2520ones%2520such%2520as%2520SIFT%2520in%2520keypoint%250Alocalization%2520accuracy%2520due%2520to%2520their%2520lack%2520of%2520sub-pixel%2520precision.%2520We%2520propose%2520a%250Anovel%2520network%2520that%2520enhances%2520any%2520detector%2520with%2520sub-pixel%2520precision%2520by%2520learning%250Aan%2520offset%2520vector%2520for%2520detected%2520features%252C%2520thereby%2520eliminating%2520the%2520need%2520for%250Adesigning%2520specialized%2520sub-pixel%2520accurate%2520detectors.%2520This%2520optimization%2520directly%250Aminimizes%2520test-time%2520evaluation%2520metrics%2520like%2520relative%2520pose%2520error.%2520Through%250Aextensive%2520testing%2520with%2520both%2520nearest%2520neighbors%2520matching%2520and%2520the%2520recent%2520LightGlue%250Amatcher%2520across%2520various%2520real-world%2520datasets%252C%2520our%2520method%2520consistently%2520outperforms%250Aexisting%2520methods%2520in%2520accuracy.%2520Moreover%252C%2520it%2520adds%2520only%2520around%25207%2520ms%2520to%2520the%2520time%2520of%250Aa%2520particular%2520detector.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/KimSinjeong/keypt2subpx%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Make%20Keypoints%20Sub-Pixel%20Accurate&entry.906535625=Shinjeong%20Kim%20and%20Marc%20Pollefeys%20and%20Daniel%20Barath&entry.1292438233=%20%20This%20work%20addresses%20the%20challenge%20of%20sub-pixel%20accuracy%20in%20detecting%202D%20local%0Afeatures%2C%20a%20cornerstone%20problem%20in%20computer%20vision.%20Despite%20the%20advancements%0Abrought%20by%20neural%20network-based%20methods%20like%20SuperPoint%20and%20ALIKED%2C%20these%0Amodern%20approaches%20lag%20behind%20classical%20ones%20such%20as%20SIFT%20in%20keypoint%0Alocalization%20accuracy%20due%20to%20their%20lack%20of%20sub-pixel%20precision.%20We%20propose%20a%0Anovel%20network%20that%20enhances%20any%20detector%20with%20sub-pixel%20precision%20by%20learning%0Aan%20offset%20vector%20for%20detected%20features%2C%20thereby%20eliminating%20the%20need%20for%0Adesigning%20specialized%20sub-pixel%20accurate%20detectors.%20This%20optimization%20directly%0Aminimizes%20test-time%20evaluation%20metrics%20like%20relative%20pose%20error.%20Through%0Aextensive%20testing%20with%20both%20nearest%20neighbors%20matching%20and%20the%20recent%20LightGlue%0Amatcher%20across%20various%20real-world%20datasets%2C%20our%20method%20consistently%20outperforms%0Aexisting%20methods%20in%20accuracy.%20Moreover%2C%20it%20adds%20only%20around%207%20ms%20to%20the%20time%20of%0Aa%20particular%20detector.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/KimSinjeong/keypt2subpx%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11668v1&entry.124074799=Read"},
{"title": "Snail-Radar: A large-scale diverse dataset for the evaluation of\n  4D-radar-based SLAM systems", "author": "Jianzhu Huai and Binliang Wang and Yuan Zhuang and Yiwen Chen and Qipeng Li and Yulong Han and Charles Toth", "abstract": "  4D radars are increasingly favored for odometry and mapping of autonomous\nsystems due to their robustness in harsh weather and dynamic environments.\nExisting datasets, however, often cover limited areas and are typically\ncaptured using a single platform. To address this gap, we present a diverse\nlarge-scale dataset specifically designed for 4D radar-based localization and\nmapping. This dataset was gathered using three different platforms: a handheld\ndevice, an e-bike, and an SUV, under a variety of environmental conditions,\nincluding clear days, nighttime, and heavy rain. The data collection occurred\nfrom September 2023 to February 2024, encompassing diverse settings such as\nroads in a vegetated campus and tunnels on highways. Each route was traversed\nmultiple times to facilitate place recognition evaluations. The sensor suite\nincluded a 3D lidar, 4D radars, stereo cameras, consumer-grade IMUs, and a\nGNSS/INS system. Sensor data packets were synchronized to GNSS time using a\ntwo-step process: a convex hull algorithm was applied to smooth host time\njitter, and then odometry and correlation algorithms were used to correct\nconstant time offsets. Extrinsic calibration between sensors was achieved\nthrough manual measurements and subsequent nonlinear optimization. The\nreference motion for the platforms was generated by registering lidar scans to\na terrestrial laser scanner (TLS) point cloud map using a lidar inertial\nodometry (LIO) method in localization mode. Additionally, a data reversion\ntechnique was introduced to enable backward LIO processing. We believe this\ndataset will boost research in radar-based point cloud registration, odometry,\nmapping, and place recognition.\n", "link": "http://arxiv.org/abs/2407.11705v1", "date": "2024-07-16", "relevancy": 2.8291, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5846}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems&body=Title%3A%20Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems%0AAuthor%3A%20Jianzhu%20Huai%20and%20Binliang%20Wang%20and%20Yuan%20Zhuang%20and%20Yiwen%20Chen%20and%20Qipeng%20Li%20and%20Yulong%20Han%20and%20Charles%20Toth%0AAbstract%3A%20%20%204D%20radars%20are%20increasingly%20favored%20for%20odometry%20and%20mapping%20of%20autonomous%0Asystems%20due%20to%20their%20robustness%20in%20harsh%20weather%20and%20dynamic%20environments.%0AExisting%20datasets%2C%20however%2C%20often%20cover%20limited%20areas%20and%20are%20typically%0Acaptured%20using%20a%20single%20platform.%20To%20address%20this%20gap%2C%20we%20present%20a%20diverse%0Alarge-scale%20dataset%20specifically%20designed%20for%204D%20radar-based%20localization%20and%0Amapping.%20This%20dataset%20was%20gathered%20using%20three%20different%20platforms%3A%20a%20handheld%0Adevice%2C%20an%20e-bike%2C%20and%20an%20SUV%2C%20under%20a%20variety%20of%20environmental%20conditions%2C%0Aincluding%20clear%20days%2C%20nighttime%2C%20and%20heavy%20rain.%20The%20data%20collection%20occurred%0Afrom%20September%202023%20to%20February%202024%2C%20encompassing%20diverse%20settings%20such%20as%0Aroads%20in%20a%20vegetated%20campus%20and%20tunnels%20on%20highways.%20Each%20route%20was%20traversed%0Amultiple%20times%20to%20facilitate%20place%20recognition%20evaluations.%20The%20sensor%20suite%0Aincluded%20a%203D%20lidar%2C%204D%20radars%2C%20stereo%20cameras%2C%20consumer-grade%20IMUs%2C%20and%20a%0AGNSS/INS%20system.%20Sensor%20data%20packets%20were%20synchronized%20to%20GNSS%20time%20using%20a%0Atwo-step%20process%3A%20a%20convex%20hull%20algorithm%20was%20applied%20to%20smooth%20host%20time%0Ajitter%2C%20and%20then%20odometry%20and%20correlation%20algorithms%20were%20used%20to%20correct%0Aconstant%20time%20offsets.%20Extrinsic%20calibration%20between%20sensors%20was%20achieved%0Athrough%20manual%20measurements%20and%20subsequent%20nonlinear%20optimization.%20The%0Areference%20motion%20for%20the%20platforms%20was%20generated%20by%20registering%20lidar%20scans%20to%0Aa%20terrestrial%20laser%20scanner%20%28TLS%29%20point%20cloud%20map%20using%20a%20lidar%20inertial%0Aodometry%20%28LIO%29%20method%20in%20localization%20mode.%20Additionally%2C%20a%20data%20reversion%0Atechnique%20was%20introduced%20to%20enable%20backward%20LIO%20processing.%20We%20believe%20this%0Adataset%20will%20boost%20research%20in%20radar-based%20point%20cloud%20registration%2C%20odometry%2C%0Amapping%2C%20and%20place%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnail-Radar%253A%2520A%2520large-scale%2520diverse%2520dataset%2520for%2520the%2520evaluation%2520of%250A%2520%25204D-radar-based%2520SLAM%2520systems%26entry.906535625%3DJianzhu%2520Huai%2520and%2520Binliang%2520Wang%2520and%2520Yuan%2520Zhuang%2520and%2520Yiwen%2520Chen%2520and%2520Qipeng%2520Li%2520and%2520Yulong%2520Han%2520and%2520Charles%2520Toth%26entry.1292438233%3D%2520%25204D%2520radars%2520are%2520increasingly%2520favored%2520for%2520odometry%2520and%2520mapping%2520of%2520autonomous%250Asystems%2520due%2520to%2520their%2520robustness%2520in%2520harsh%2520weather%2520and%2520dynamic%2520environments.%250AExisting%2520datasets%252C%2520however%252C%2520often%2520cover%2520limited%2520areas%2520and%2520are%2520typically%250Acaptured%2520using%2520a%2520single%2520platform.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520diverse%250Alarge-scale%2520dataset%2520specifically%2520designed%2520for%25204D%2520radar-based%2520localization%2520and%250Amapping.%2520This%2520dataset%2520was%2520gathered%2520using%2520three%2520different%2520platforms%253A%2520a%2520handheld%250Adevice%252C%2520an%2520e-bike%252C%2520and%2520an%2520SUV%252C%2520under%2520a%2520variety%2520of%2520environmental%2520conditions%252C%250Aincluding%2520clear%2520days%252C%2520nighttime%252C%2520and%2520heavy%2520rain.%2520The%2520data%2520collection%2520occurred%250Afrom%2520September%25202023%2520to%2520February%25202024%252C%2520encompassing%2520diverse%2520settings%2520such%2520as%250Aroads%2520in%2520a%2520vegetated%2520campus%2520and%2520tunnels%2520on%2520highways.%2520Each%2520route%2520was%2520traversed%250Amultiple%2520times%2520to%2520facilitate%2520place%2520recognition%2520evaluations.%2520The%2520sensor%2520suite%250Aincluded%2520a%25203D%2520lidar%252C%25204D%2520radars%252C%2520stereo%2520cameras%252C%2520consumer-grade%2520IMUs%252C%2520and%2520a%250AGNSS/INS%2520system.%2520Sensor%2520data%2520packets%2520were%2520synchronized%2520to%2520GNSS%2520time%2520using%2520a%250Atwo-step%2520process%253A%2520a%2520convex%2520hull%2520algorithm%2520was%2520applied%2520to%2520smooth%2520host%2520time%250Ajitter%252C%2520and%2520then%2520odometry%2520and%2520correlation%2520algorithms%2520were%2520used%2520to%2520correct%250Aconstant%2520time%2520offsets.%2520Extrinsic%2520calibration%2520between%2520sensors%2520was%2520achieved%250Athrough%2520manual%2520measurements%2520and%2520subsequent%2520nonlinear%2520optimization.%2520The%250Areference%2520motion%2520for%2520the%2520platforms%2520was%2520generated%2520by%2520registering%2520lidar%2520scans%2520to%250Aa%2520terrestrial%2520laser%2520scanner%2520%2528TLS%2529%2520point%2520cloud%2520map%2520using%2520a%2520lidar%2520inertial%250Aodometry%2520%2528LIO%2529%2520method%2520in%2520localization%2520mode.%2520Additionally%252C%2520a%2520data%2520reversion%250Atechnique%2520was%2520introduced%2520to%2520enable%2520backward%2520LIO%2520processing.%2520We%2520believe%2520this%250Adataset%2520will%2520boost%2520research%2520in%2520radar-based%2520point%2520cloud%2520registration%252C%2520odometry%252C%250Amapping%252C%2520and%2520place%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems&entry.906535625=Jianzhu%20Huai%20and%20Binliang%20Wang%20and%20Yuan%20Zhuang%20and%20Yiwen%20Chen%20and%20Qipeng%20Li%20and%20Yulong%20Han%20and%20Charles%20Toth&entry.1292438233=%20%204D%20radars%20are%20increasingly%20favored%20for%20odometry%20and%20mapping%20of%20autonomous%0Asystems%20due%20to%20their%20robustness%20in%20harsh%20weather%20and%20dynamic%20environments.%0AExisting%20datasets%2C%20however%2C%20often%20cover%20limited%20areas%20and%20are%20typically%0Acaptured%20using%20a%20single%20platform.%20To%20address%20this%20gap%2C%20we%20present%20a%20diverse%0Alarge-scale%20dataset%20specifically%20designed%20for%204D%20radar-based%20localization%20and%0Amapping.%20This%20dataset%20was%20gathered%20using%20three%20different%20platforms%3A%20a%20handheld%0Adevice%2C%20an%20e-bike%2C%20and%20an%20SUV%2C%20under%20a%20variety%20of%20environmental%20conditions%2C%0Aincluding%20clear%20days%2C%20nighttime%2C%20and%20heavy%20rain.%20The%20data%20collection%20occurred%0Afrom%20September%202023%20to%20February%202024%2C%20encompassing%20diverse%20settings%20such%20as%0Aroads%20in%20a%20vegetated%20campus%20and%20tunnels%20on%20highways.%20Each%20route%20was%20traversed%0Amultiple%20times%20to%20facilitate%20place%20recognition%20evaluations.%20The%20sensor%20suite%0Aincluded%20a%203D%20lidar%2C%204D%20radars%2C%20stereo%20cameras%2C%20consumer-grade%20IMUs%2C%20and%20a%0AGNSS/INS%20system.%20Sensor%20data%20packets%20were%20synchronized%20to%20GNSS%20time%20using%20a%0Atwo-step%20process%3A%20a%20convex%20hull%20algorithm%20was%20applied%20to%20smooth%20host%20time%0Ajitter%2C%20and%20then%20odometry%20and%20correlation%20algorithms%20were%20used%20to%20correct%0Aconstant%20time%20offsets.%20Extrinsic%20calibration%20between%20sensors%20was%20achieved%0Athrough%20manual%20measurements%20and%20subsequent%20nonlinear%20optimization.%20The%0Areference%20motion%20for%20the%20platforms%20was%20generated%20by%20registering%20lidar%20scans%20to%0Aa%20terrestrial%20laser%20scanner%20%28TLS%29%20point%20cloud%20map%20using%20a%20lidar%20inertial%0Aodometry%20%28LIO%29%20method%20in%20localization%20mode.%20Additionally%2C%20a%20data%20reversion%0Atechnique%20was%20introduced%20to%20enable%20backward%20LIO%20processing.%20We%20believe%20this%0Adataset%20will%20boost%20research%20in%20radar-based%20point%20cloud%20registration%2C%20odometry%2C%0Amapping%2C%20and%20place%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11705v1&entry.124074799=Read"},
{"title": "3D-COCO: extension of MS-COCO dataset for image detection and 3D\n  reconstruction modules", "author": "Maxence Bideaux and Alice Phe and Mohamed Chaouch and Bertrand Luvison and Quoc-Cuong Pham", "abstract": "  We introduce 3D-COCO, an extension of the original MS-COCO dataset providing\n3D models and 2D-3D alignment annotations. 3D-COCO was designed to achieve\ncomputer vision tasks such as 3D reconstruction or image detection configurable\nwith textual, 2D image, and 3D CAD model queries. We complete the existing\nMS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. By\nusing an IoU-based method, we match each MS-COCO annotation with the best 3D\nmodels to provide a 2D-3D alignment. The open-source nature of 3D-COCO is a\npremiere that should pave the way for new research on 3D-related topics. The\ndataset and its source codes is available at\nhttps://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/\n", "link": "http://arxiv.org/abs/2404.05641v3", "date": "2024-07-16", "relevancy": 2.8187, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules&body=Title%3A%203D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules%0AAuthor%3A%20Maxence%20Bideaux%20and%20Alice%20Phe%20and%20Mohamed%20Chaouch%20and%20Bertrand%20Luvison%20and%20Quoc-Cuong%20Pham%0AAbstract%3A%20%20%20We%20introduce%203D-COCO%2C%20an%20extension%20of%20the%20original%20MS-COCO%20dataset%20providing%0A3D%20models%20and%202D-3D%20alignment%20annotations.%203D-COCO%20was%20designed%20to%20achieve%0Acomputer%20vision%20tasks%20such%20as%203D%20reconstruction%20or%20image%20detection%20configurable%0Awith%20textual%2C%202D%20image%2C%20and%203D%20CAD%20model%20queries.%20We%20complete%20the%20existing%0AMS-COCO%20dataset%20with%2028K%203D%20models%20collected%20on%20ShapeNet%20and%20Objaverse.%20By%0Ausing%20an%20IoU-based%20method%2C%20we%20match%20each%20MS-COCO%20annotation%20with%20the%20best%203D%0Amodels%20to%20provide%20a%202D-3D%20alignment.%20The%20open-source%20nature%20of%203D-COCO%20is%20a%0Apremiere%20that%20should%20pave%20the%20way%20for%20new%20research%20on%203D-related%20topics.%20The%0Adataset%20and%20its%20source%20codes%20is%20available%20at%0Ahttps%3A//kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05641v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-COCO%253A%2520extension%2520of%2520MS-COCO%2520dataset%2520for%2520image%2520detection%2520and%25203D%250A%2520%2520reconstruction%2520modules%26entry.906535625%3DMaxence%2520Bideaux%2520and%2520Alice%2520Phe%2520and%2520Mohamed%2520Chaouch%2520and%2520Bertrand%2520Luvison%2520and%2520Quoc-Cuong%2520Pham%26entry.1292438233%3D%2520%2520We%2520introduce%25203D-COCO%252C%2520an%2520extension%2520of%2520the%2520original%2520MS-COCO%2520dataset%2520providing%250A3D%2520models%2520and%25202D-3D%2520alignment%2520annotations.%25203D-COCO%2520was%2520designed%2520to%2520achieve%250Acomputer%2520vision%2520tasks%2520such%2520as%25203D%2520reconstruction%2520or%2520image%2520detection%2520configurable%250Awith%2520textual%252C%25202D%2520image%252C%2520and%25203D%2520CAD%2520model%2520queries.%2520We%2520complete%2520the%2520existing%250AMS-COCO%2520dataset%2520with%252028K%25203D%2520models%2520collected%2520on%2520ShapeNet%2520and%2520Objaverse.%2520By%250Ausing%2520an%2520IoU-based%2520method%252C%2520we%2520match%2520each%2520MS-COCO%2520annotation%2520with%2520the%2520best%25203D%250Amodels%2520to%2520provide%2520a%25202D-3D%2520alignment.%2520The%2520open-source%2520nature%2520of%25203D-COCO%2520is%2520a%250Apremiere%2520that%2520should%2520pave%2520the%2520way%2520for%2520new%2520research%2520on%25203D-related%2520topics.%2520The%250Adataset%2520and%2520its%2520source%2520codes%2520is%2520available%2520at%250Ahttps%253A//kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05641v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-COCO%3A%20extension%20of%20MS-COCO%20dataset%20for%20image%20detection%20and%203D%0A%20%20reconstruction%20modules&entry.906535625=Maxence%20Bideaux%20and%20Alice%20Phe%20and%20Mohamed%20Chaouch%20and%20Bertrand%20Luvison%20and%20Quoc-Cuong%20Pham&entry.1292438233=%20%20We%20introduce%203D-COCO%2C%20an%20extension%20of%20the%20original%20MS-COCO%20dataset%20providing%0A3D%20models%20and%202D-3D%20alignment%20annotations.%203D-COCO%20was%20designed%20to%20achieve%0Acomputer%20vision%20tasks%20such%20as%203D%20reconstruction%20or%20image%20detection%20configurable%0Awith%20textual%2C%202D%20image%2C%20and%203D%20CAD%20model%20queries.%20We%20complete%20the%20existing%0AMS-COCO%20dataset%20with%2028K%203D%20models%20collected%20on%20ShapeNet%20and%20Objaverse.%20By%0Ausing%20an%20IoU-based%20method%2C%20we%20match%20each%20MS-COCO%20annotation%20with%20the%20best%203D%0Amodels%20to%20provide%20a%202D-3D%20alignment.%20The%20open-source%20nature%20of%203D-COCO%20is%20a%0Apremiere%20that%20should%20pave%20the%20way%20for%20new%20research%20on%203D-related%20topics.%20The%0Adataset%20and%20its%20source%20codes%20is%20available%20at%0Ahttps%3A//kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05641v3&entry.124074799=Read"},
{"title": "MOCA: Self-supervised Representation Learning by Predicting Masked\n  Online Codebook Assignments", "author": "Spyros Gidaris and Andrei Bursuc and Oriane Simeoni and Antonin Vobecky and Nikos Komodakis and Matthieu Cord and Patrick P\u00e9rez", "abstract": "  Self-supervised learning can be used for mitigating the greedy needs of\nVision Transformer networks for very large fully-annotated datasets. Different\nclasses of self-supervised learning offer representations with either good\ncontextual reasoning properties, e.g., using masked image modeling strategies,\nor invariance to image perturbations, e.g., with contrastive methods. In this\nwork, we propose a single-stage and standalone method, MOCA, which unifies both\ndesired properties using novel mask-and-predict objectives defined with\nhigh-level features (instead of pixel-level details). Moreover, we show how to\neffectively employ both learning paradigms in a synergistic and\ncomputation-efficient way. Doing so, we achieve new state-of-the-art results on\nlow-shot settings and strong experimental results in various evaluation\nprotocols with a training that is at least 3 times faster than prior methods.\nWe provide the implementation code at https://github.com/valeoai/MOCA.\n", "link": "http://arxiv.org/abs/2307.09361v2", "date": "2024-07-16", "relevancy": 2.8074, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOCA%3A%20Self-supervised%20Representation%20Learning%20by%20Predicting%20Masked%0A%20%20Online%20Codebook%20Assignments&body=Title%3A%20MOCA%3A%20Self-supervised%20Representation%20Learning%20by%20Predicting%20Masked%0A%20%20Online%20Codebook%20Assignments%0AAuthor%3A%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Oriane%20Simeoni%20and%20Antonin%20Vobecky%20and%20Nikos%20Komodakis%20and%20Matthieu%20Cord%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20%20%20Self-supervised%20learning%20can%20be%20used%20for%20mitigating%20the%20greedy%20needs%20of%0AVision%20Transformer%20networks%20for%20very%20large%20fully-annotated%20datasets.%20Different%0Aclasses%20of%20self-supervised%20learning%20offer%20representations%20with%20either%20good%0Acontextual%20reasoning%20properties%2C%20e.g.%2C%20using%20masked%20image%20modeling%20strategies%2C%0Aor%20invariance%20to%20image%20perturbations%2C%20e.g.%2C%20with%20contrastive%20methods.%20In%20this%0Awork%2C%20we%20propose%20a%20single-stage%20and%20standalone%20method%2C%20MOCA%2C%20which%20unifies%20both%0Adesired%20properties%20using%20novel%20mask-and-predict%20objectives%20defined%20with%0Ahigh-level%20features%20%28instead%20of%20pixel-level%20details%29.%20Moreover%2C%20we%20show%20how%20to%0Aeffectively%20employ%20both%20learning%20paradigms%20in%20a%20synergistic%20and%0Acomputation-efficient%20way.%20Doing%20so%2C%20we%20achieve%20new%20state-of-the-art%20results%20on%0Alow-shot%20settings%20and%20strong%20experimental%20results%20in%20various%20evaluation%0Aprotocols%20with%20a%20training%20that%20is%20at%20least%203%20times%20faster%20than%20prior%20methods.%0AWe%20provide%20the%20implementation%20code%20at%20https%3A//github.com/valeoai/MOCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOCA%253A%2520Self-supervised%2520Representation%2520Learning%2520by%2520Predicting%2520Masked%250A%2520%2520Online%2520Codebook%2520Assignments%26entry.906535625%3DSpyros%2520Gidaris%2520and%2520Andrei%2520Bursuc%2520and%2520Oriane%2520Simeoni%2520and%2520Antonin%2520Vobecky%2520and%2520Nikos%2520Komodakis%2520and%2520Matthieu%2520Cord%2520and%2520Patrick%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520can%2520be%2520used%2520for%2520mitigating%2520the%2520greedy%2520needs%2520of%250AVision%2520Transformer%2520networks%2520for%2520very%2520large%2520fully-annotated%2520datasets.%2520Different%250Aclasses%2520of%2520self-supervised%2520learning%2520offer%2520representations%2520with%2520either%2520good%250Acontextual%2520reasoning%2520properties%252C%2520e.g.%252C%2520using%2520masked%2520image%2520modeling%2520strategies%252C%250Aor%2520invariance%2520to%2520image%2520perturbations%252C%2520e.g.%252C%2520with%2520contrastive%2520methods.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520single-stage%2520and%2520standalone%2520method%252C%2520MOCA%252C%2520which%2520unifies%2520both%250Adesired%2520properties%2520using%2520novel%2520mask-and-predict%2520objectives%2520defined%2520with%250Ahigh-level%2520features%2520%2528instead%2520of%2520pixel-level%2520details%2529.%2520Moreover%252C%2520we%2520show%2520how%2520to%250Aeffectively%2520employ%2520both%2520learning%2520paradigms%2520in%2520a%2520synergistic%2520and%250Acomputation-efficient%2520way.%2520Doing%2520so%252C%2520we%2520achieve%2520new%2520state-of-the-art%2520results%2520on%250Alow-shot%2520settings%2520and%2520strong%2520experimental%2520results%2520in%2520various%2520evaluation%250Aprotocols%2520with%2520a%2520training%2520that%2520is%2520at%2520least%25203%2520times%2520faster%2520than%2520prior%2520methods.%250AWe%2520provide%2520the%2520implementation%2520code%2520at%2520https%253A//github.com/valeoai/MOCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOCA%3A%20Self-supervised%20Representation%20Learning%20by%20Predicting%20Masked%0A%20%20Online%20Codebook%20Assignments&entry.906535625=Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Oriane%20Simeoni%20and%20Antonin%20Vobecky%20and%20Nikos%20Komodakis%20and%20Matthieu%20Cord%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20Self-supervised%20learning%20can%20be%20used%20for%20mitigating%20the%20greedy%20needs%20of%0AVision%20Transformer%20networks%20for%20very%20large%20fully-annotated%20datasets.%20Different%0Aclasses%20of%20self-supervised%20learning%20offer%20representations%20with%20either%20good%0Acontextual%20reasoning%20properties%2C%20e.g.%2C%20using%20masked%20image%20modeling%20strategies%2C%0Aor%20invariance%20to%20image%20perturbations%2C%20e.g.%2C%20with%20contrastive%20methods.%20In%20this%0Awork%2C%20we%20propose%20a%20single-stage%20and%20standalone%20method%2C%20MOCA%2C%20which%20unifies%20both%0Adesired%20properties%20using%20novel%20mask-and-predict%20objectives%20defined%20with%0Ahigh-level%20features%20%28instead%20of%20pixel-level%20details%29.%20Moreover%2C%20we%20show%20how%20to%0Aeffectively%20employ%20both%20learning%20paradigms%20in%20a%20synergistic%20and%0Acomputation-efficient%20way.%20Doing%20so%2C%20we%20achieve%20new%20state-of-the-art%20results%20on%0Alow-shot%20settings%20and%20strong%20experimental%20results%20in%20various%20evaluation%0Aprotocols%20with%20a%20training%20that%20is%20at%20least%203%20times%20faster%20than%20prior%20methods.%0AWe%20provide%20the%20implementation%20code%20at%20https%3A//github.com/valeoai/MOCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09361v2&entry.124074799=Read"},
{"title": "Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data", "author": "Tim Elsner and Paula Usinger and Victor Czech and Gregor Kobsik and Yanjiang He and Isaak Lim and Leif Kobbelt", "abstract": "  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n", "link": "http://arxiv.org/abs/2407.11913v1", "date": "2024-07-16", "relevancy": 2.7556, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5809}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data&body=Title%3A%20Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data%0AAuthor%3A%20Tim%20Elsner%20and%20Paula%20Usinger%20and%20Victor%20Czech%20and%20Gregor%20Kobsik%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt%0AAbstract%3A%20%20%20In%20quantised%20autoencoders%2C%20images%20are%20usually%20split%20into%20local%20patches%2C%20each%0Aencoded%20by%20one%20token.%20This%20representation%20is%20redundant%20in%20the%20sense%20that%20the%0Asame%20number%20of%20tokens%20is%20spend%20per%20region%2C%20regardless%20of%20the%20visual%20information%0Acontent%20in%20that%20region.%20Adaptive%20discretisation%20schemes%20like%20quadtrees%20are%0Aapplied%20to%20allocate%20tokens%20for%20patches%20with%20varying%20sizes%2C%20but%20this%20just%20varies%0Athe%20region%20of%20influence%20for%20a%20token%20which%20nevertheless%20remains%20a%20local%0Adescriptor.%20Modern%20architectures%20add%20an%20attention%20mechanism%20to%20the%20autoencoder%0Awhich%20infuses%20some%20degree%20of%20global%20information%20into%20the%20local%20tokens.%20Despite%0Athe%20global%20context%2C%20tokens%20are%20still%20associated%20with%20a%20local%20image%20region.%20In%0Acontrast%2C%20our%20method%20is%20inspired%20by%20spectral%20decompositions%20which%20transform%20an%0Ainput%20signal%20into%20a%20superposition%20of%20global%20frequencies.%20Taking%20the%20data-driven%0Aperspective%2C%20we%20learn%20custom%20basis%20functions%20corresponding%20to%20the%20codebook%0Aentries%20in%20our%20VQ-VAE%20setup.%20Furthermore%2C%20a%20decoder%20combines%20these%20basis%0Afunctions%20in%20a%20non-linear%20fashion%2C%20going%20beyond%20the%20simple%20linear%20superposition%0Aof%20spectral%20decompositions.%20We%20can%20achieve%20this%20global%20description%20with%20an%0Aefficient%20transpose%20operation%20between%20features%20and%20channels%20and%20demonstrate%20our%0Aperformance%20on%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantised%2520Global%2520Autoencoder%253A%2520A%2520Holistic%2520Approach%2520to%2520Representing%2520Visual%250A%2520%2520Data%26entry.906535625%3DTim%2520Elsner%2520and%2520Paula%2520Usinger%2520and%2520Victor%2520Czech%2520and%2520Gregor%2520Kobsik%2520and%2520Yanjiang%2520He%2520and%2520Isaak%2520Lim%2520and%2520Leif%2520Kobbelt%26entry.1292438233%3D%2520%2520In%2520quantised%2520autoencoders%252C%2520images%2520are%2520usually%2520split%2520into%2520local%2520patches%252C%2520each%250Aencoded%2520by%2520one%2520token.%2520This%2520representation%2520is%2520redundant%2520in%2520the%2520sense%2520that%2520the%250Asame%2520number%2520of%2520tokens%2520is%2520spend%2520per%2520region%252C%2520regardless%2520of%2520the%2520visual%2520information%250Acontent%2520in%2520that%2520region.%2520Adaptive%2520discretisation%2520schemes%2520like%2520quadtrees%2520are%250Aapplied%2520to%2520allocate%2520tokens%2520for%2520patches%2520with%2520varying%2520sizes%252C%2520but%2520this%2520just%2520varies%250Athe%2520region%2520of%2520influence%2520for%2520a%2520token%2520which%2520nevertheless%2520remains%2520a%2520local%250Adescriptor.%2520Modern%2520architectures%2520add%2520an%2520attention%2520mechanism%2520to%2520the%2520autoencoder%250Awhich%2520infuses%2520some%2520degree%2520of%2520global%2520information%2520into%2520the%2520local%2520tokens.%2520Despite%250Athe%2520global%2520context%252C%2520tokens%2520are%2520still%2520associated%2520with%2520a%2520local%2520image%2520region.%2520In%250Acontrast%252C%2520our%2520method%2520is%2520inspired%2520by%2520spectral%2520decompositions%2520which%2520transform%2520an%250Ainput%2520signal%2520into%2520a%2520superposition%2520of%2520global%2520frequencies.%2520Taking%2520the%2520data-driven%250Aperspective%252C%2520we%2520learn%2520custom%2520basis%2520functions%2520corresponding%2520to%2520the%2520codebook%250Aentries%2520in%2520our%2520VQ-VAE%2520setup.%2520Furthermore%252C%2520a%2520decoder%2520combines%2520these%2520basis%250Afunctions%2520in%2520a%2520non-linear%2520fashion%252C%2520going%2520beyond%2520the%2520simple%2520linear%2520superposition%250Aof%2520spectral%2520decompositions.%2520We%2520can%2520achieve%2520this%2520global%2520description%2520with%2520an%250Aefficient%2520transpose%2520operation%2520between%2520features%2520and%2520channels%2520and%2520demonstrate%2520our%250Aperformance%2520on%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantised%20Global%20Autoencoder%3A%20A%20Holistic%20Approach%20to%20Representing%20Visual%0A%20%20Data&entry.906535625=Tim%20Elsner%20and%20Paula%20Usinger%20and%20Victor%20Czech%20and%20Gregor%20Kobsik%20and%20Yanjiang%20He%20and%20Isaak%20Lim%20and%20Leif%20Kobbelt&entry.1292438233=%20%20In%20quantised%20autoencoders%2C%20images%20are%20usually%20split%20into%20local%20patches%2C%20each%0Aencoded%20by%20one%20token.%20This%20representation%20is%20redundant%20in%20the%20sense%20that%20the%0Asame%20number%20of%20tokens%20is%20spend%20per%20region%2C%20regardless%20of%20the%20visual%20information%0Acontent%20in%20that%20region.%20Adaptive%20discretisation%20schemes%20like%20quadtrees%20are%0Aapplied%20to%20allocate%20tokens%20for%20patches%20with%20varying%20sizes%2C%20but%20this%20just%20varies%0Athe%20region%20of%20influence%20for%20a%20token%20which%20nevertheless%20remains%20a%20local%0Adescriptor.%20Modern%20architectures%20add%20an%20attention%20mechanism%20to%20the%20autoencoder%0Awhich%20infuses%20some%20degree%20of%20global%20information%20into%20the%20local%20tokens.%20Despite%0Athe%20global%20context%2C%20tokens%20are%20still%20associated%20with%20a%20local%20image%20region.%20In%0Acontrast%2C%20our%20method%20is%20inspired%20by%20spectral%20decompositions%20which%20transform%20an%0Ainput%20signal%20into%20a%20superposition%20of%20global%20frequencies.%20Taking%20the%20data-driven%0Aperspective%2C%20we%20learn%20custom%20basis%20functions%20corresponding%20to%20the%20codebook%0Aentries%20in%20our%20VQ-VAE%20setup.%20Furthermore%2C%20a%20decoder%20combines%20these%20basis%0Afunctions%20in%20a%20non-linear%20fashion%2C%20going%20beyond%20the%20simple%20linear%20superposition%0Aof%20spectral%20decompositions.%20We%20can%20achieve%20this%20global%20description%20with%20an%0Aefficient%20transpose%20operation%20between%20features%20and%20channels%20and%20demonstrate%20our%0Aperformance%20on%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11913v1&entry.124074799=Read"},
{"title": "Are Synthetic Data Useful for Egocentric Hand-Object Interaction\n  Detection?", "author": "Rosario Leonardi and Antonino Furnari and Francesco Ragusa and Giovanni Maria Farinella", "abstract": "  In this study, we investigate the effectiveness of synthetic data in\nenhancing egocentric hand-object interaction detection. Via extensive\nexperiments and comparative analyses on three egocentric datasets, VISOR,\nEgoHOS, and ENIGMA-51, our findings reveal how to exploit synthetic data for\nthe HOI detection task when real labeled data are scarce or unavailable.\nSpecifically, by leveraging only 10% of real labeled data, we achieve\nimprovements in Overall AP compared to baselines trained exclusively on real\ndata of: +5.67% on EPIC-KITCHENS VISOR, +8.24% on EgoHOS, and +11.69% on\nENIGMA-51. Our analysis is supported by a novel data generation pipeline and\nthe newly introduced HOI-Synth benchmark which augments existing datasets with\nsynthetic images of hand-object interactions automatically labeled with\nhand-object contact states, bounding boxes, and pixel-wise segmentation masks.\nData, code, and data generation tools to support future research are released\nat: https://fpv-iplab.github.io/HOI-Synth/.\n", "link": "http://arxiv.org/abs/2312.02672v3", "date": "2024-07-16", "relevancy": 2.7523, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5982}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5273}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Synthetic%20Data%20Useful%20for%20Egocentric%20Hand-Object%20Interaction%0A%20%20Detection%3F&body=Title%3A%20Are%20Synthetic%20Data%20Useful%20for%20Egocentric%20Hand-Object%20Interaction%0A%20%20Detection%3F%0AAuthor%3A%20Rosario%20Leonardi%20and%20Antonino%20Furnari%20and%20Francesco%20Ragusa%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigate%20the%20effectiveness%20of%20synthetic%20data%20in%0Aenhancing%20egocentric%20hand-object%20interaction%20detection.%20Via%20extensive%0Aexperiments%20and%20comparative%20analyses%20on%20three%20egocentric%20datasets%2C%20VISOR%2C%0AEgoHOS%2C%20and%20ENIGMA-51%2C%20our%20findings%20reveal%20how%20to%20exploit%20synthetic%20data%20for%0Athe%20HOI%20detection%20task%20when%20real%20labeled%20data%20are%20scarce%20or%20unavailable.%0ASpecifically%2C%20by%20leveraging%20only%2010%25%20of%20real%20labeled%20data%2C%20we%20achieve%0Aimprovements%20in%20Overall%20AP%20compared%20to%20baselines%20trained%20exclusively%20on%20real%0Adata%20of%3A%20%2B5.67%25%20on%20EPIC-KITCHENS%20VISOR%2C%20%2B8.24%25%20on%20EgoHOS%2C%20and%20%2B11.69%25%20on%0AENIGMA-51.%20Our%20analysis%20is%20supported%20by%20a%20novel%20data%20generation%20pipeline%20and%0Athe%20newly%20introduced%20HOI-Synth%20benchmark%20which%20augments%20existing%20datasets%20with%0Asynthetic%20images%20of%20hand-object%20interactions%20automatically%20labeled%20with%0Ahand-object%20contact%20states%2C%20bounding%20boxes%2C%20and%20pixel-wise%20segmentation%20masks.%0AData%2C%20code%2C%20and%20data%20generation%20tools%20to%20support%20future%20research%20are%20released%0Aat%3A%20https%3A//fpv-iplab.github.io/HOI-Synth/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Synthetic%2520Data%2520Useful%2520for%2520Egocentric%2520Hand-Object%2520Interaction%250A%2520%2520Detection%253F%26entry.906535625%3DRosario%2520Leonardi%2520and%2520Antonino%2520Furnari%2520and%2520Francesco%2520Ragusa%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520effectiveness%2520of%2520synthetic%2520data%2520in%250Aenhancing%2520egocentric%2520hand-object%2520interaction%2520detection.%2520Via%2520extensive%250Aexperiments%2520and%2520comparative%2520analyses%2520on%2520three%2520egocentric%2520datasets%252C%2520VISOR%252C%250AEgoHOS%252C%2520and%2520ENIGMA-51%252C%2520our%2520findings%2520reveal%2520how%2520to%2520exploit%2520synthetic%2520data%2520for%250Athe%2520HOI%2520detection%2520task%2520when%2520real%2520labeled%2520data%2520are%2520scarce%2520or%2520unavailable.%250ASpecifically%252C%2520by%2520leveraging%2520only%252010%2525%2520of%2520real%2520labeled%2520data%252C%2520we%2520achieve%250Aimprovements%2520in%2520Overall%2520AP%2520compared%2520to%2520baselines%2520trained%2520exclusively%2520on%2520real%250Adata%2520of%253A%2520%252B5.67%2525%2520on%2520EPIC-KITCHENS%2520VISOR%252C%2520%252B8.24%2525%2520on%2520EgoHOS%252C%2520and%2520%252B11.69%2525%2520on%250AENIGMA-51.%2520Our%2520analysis%2520is%2520supported%2520by%2520a%2520novel%2520data%2520generation%2520pipeline%2520and%250Athe%2520newly%2520introduced%2520HOI-Synth%2520benchmark%2520which%2520augments%2520existing%2520datasets%2520with%250Asynthetic%2520images%2520of%2520hand-object%2520interactions%2520automatically%2520labeled%2520with%250Ahand-object%2520contact%2520states%252C%2520bounding%2520boxes%252C%2520and%2520pixel-wise%2520segmentation%2520masks.%250AData%252C%2520code%252C%2520and%2520data%2520generation%2520tools%2520to%2520support%2520future%2520research%2520are%2520released%250Aat%253A%2520https%253A//fpv-iplab.github.io/HOI-Synth/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Synthetic%20Data%20Useful%20for%20Egocentric%20Hand-Object%20Interaction%0A%20%20Detection%3F&entry.906535625=Rosario%20Leonardi%20and%20Antonino%20Furnari%20and%20Francesco%20Ragusa%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigate%20the%20effectiveness%20of%20synthetic%20data%20in%0Aenhancing%20egocentric%20hand-object%20interaction%20detection.%20Via%20extensive%0Aexperiments%20and%20comparative%20analyses%20on%20three%20egocentric%20datasets%2C%20VISOR%2C%0AEgoHOS%2C%20and%20ENIGMA-51%2C%20our%20findings%20reveal%20how%20to%20exploit%20synthetic%20data%20for%0Athe%20HOI%20detection%20task%20when%20real%20labeled%20data%20are%20scarce%20or%20unavailable.%0ASpecifically%2C%20by%20leveraging%20only%2010%25%20of%20real%20labeled%20data%2C%20we%20achieve%0Aimprovements%20in%20Overall%20AP%20compared%20to%20baselines%20trained%20exclusively%20on%20real%0Adata%20of%3A%20%2B5.67%25%20on%20EPIC-KITCHENS%20VISOR%2C%20%2B8.24%25%20on%20EgoHOS%2C%20and%20%2B11.69%25%20on%0AENIGMA-51.%20Our%20analysis%20is%20supported%20by%20a%20novel%20data%20generation%20pipeline%20and%0Athe%20newly%20introduced%20HOI-Synth%20benchmark%20which%20augments%20existing%20datasets%20with%0Asynthetic%20images%20of%20hand-object%20interactions%20automatically%20labeled%20with%0Ahand-object%20contact%20states%2C%20bounding%20boxes%2C%20and%20pixel-wise%20segmentation%20masks.%0AData%2C%20code%2C%20and%20data%20generation%20tools%20to%20support%20future%20research%20are%20released%0Aat%3A%20https%3A//fpv-iplab.github.io/HOI-Synth/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02672v3&entry.124074799=Read"},
{"title": "Rethinking LiDAR Domain Generalization: Single Source as Multiple\n  Density Domains", "author": "Jaeyeul Kim and Jungwan Woo and Jeonghoon Kim and Sunghoon Im", "abstract": "  In the realm of LiDAR-based perception, significant strides have been made,\nyet domain generalization remains a substantial challenge. The performance\noften deteriorates when models are applied to unfamiliar datasets with\ndifferent LiDAR sensors or deployed in new environments, primarily due to\nvariations in point cloud density distributions. To tackle this challenge, we\npropose a Density Discriminative Feature Embedding (DDFE) module, capitalizing\non the observation that a single source LiDAR point cloud encompasses a\nspectrum of densities. The DDFE module is meticulously designed to extract\ndensity-specific features within a single source domain, facilitating the\nrecognition of objects sharing similar density characteristics across different\nLiDAR sensors. In addition, we introduce a simple yet effective density\naugmentation technique aimed at expanding the spectrum of density in source\ndata, thereby enhancing the capabilities of the DDFE. Our DDFE stands out as a\nversatile and lightweight domain generalization module. It can be seamlessly\nintegrated into various 3D backbone networks, where it has demonstrated\nsuperior performance over current state-of-the-art domain generalization\nmethods. Code is available at https://github.com/dgist-cvlab/MultiDensityDG.\n", "link": "http://arxiv.org/abs/2312.12098v2", "date": "2024-07-16", "relevancy": 2.7247, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5404}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20LiDAR%20Domain%20Generalization%3A%20Single%20Source%20as%20Multiple%0A%20%20Density%20Domains&body=Title%3A%20Rethinking%20LiDAR%20Domain%20Generalization%3A%20Single%20Source%20as%20Multiple%0A%20%20Density%20Domains%0AAuthor%3A%20Jaeyeul%20Kim%20and%20Jungwan%20Woo%20and%20Jeonghoon%20Kim%20and%20Sunghoon%20Im%0AAbstract%3A%20%20%20In%20the%20realm%20of%20LiDAR-based%20perception%2C%20significant%20strides%20have%20been%20made%2C%0Ayet%20domain%20generalization%20remains%20a%20substantial%20challenge.%20The%20performance%0Aoften%20deteriorates%20when%20models%20are%20applied%20to%20unfamiliar%20datasets%20with%0Adifferent%20LiDAR%20sensors%20or%20deployed%20in%20new%20environments%2C%20primarily%20due%20to%0Avariations%20in%20point%20cloud%20density%20distributions.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20a%20Density%20Discriminative%20Feature%20Embedding%20%28DDFE%29%20module%2C%20capitalizing%0Aon%20the%20observation%20that%20a%20single%20source%20LiDAR%20point%20cloud%20encompasses%20a%0Aspectrum%20of%20densities.%20The%20DDFE%20module%20is%20meticulously%20designed%20to%20extract%0Adensity-specific%20features%20within%20a%20single%20source%20domain%2C%20facilitating%20the%0Arecognition%20of%20objects%20sharing%20similar%20density%20characteristics%20across%20different%0ALiDAR%20sensors.%20In%20addition%2C%20we%20introduce%20a%20simple%20yet%20effective%20density%0Aaugmentation%20technique%20aimed%20at%20expanding%20the%20spectrum%20of%20density%20in%20source%0Adata%2C%20thereby%20enhancing%20the%20capabilities%20of%20the%20DDFE.%20Our%20DDFE%20stands%20out%20as%20a%0Aversatile%20and%20lightweight%20domain%20generalization%20module.%20It%20can%20be%20seamlessly%0Aintegrated%20into%20various%203D%20backbone%20networks%2C%20where%20it%20has%20demonstrated%0Asuperior%20performance%20over%20current%20state-of-the-art%20domain%20generalization%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/dgist-cvlab/MultiDensityDG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520LiDAR%2520Domain%2520Generalization%253A%2520Single%2520Source%2520as%2520Multiple%250A%2520%2520Density%2520Domains%26entry.906535625%3DJaeyeul%2520Kim%2520and%2520Jungwan%2520Woo%2520and%2520Jeonghoon%2520Kim%2520and%2520Sunghoon%2520Im%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520LiDAR-based%2520perception%252C%2520significant%2520strides%2520have%2520been%2520made%252C%250Ayet%2520domain%2520generalization%2520remains%2520a%2520substantial%2520challenge.%2520The%2520performance%250Aoften%2520deteriorates%2520when%2520models%2520are%2520applied%2520to%2520unfamiliar%2520datasets%2520with%250Adifferent%2520LiDAR%2520sensors%2520or%2520deployed%2520in%2520new%2520environments%252C%2520primarily%2520due%2520to%250Avariations%2520in%2520point%2520cloud%2520density%2520distributions.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520Density%2520Discriminative%2520Feature%2520Embedding%2520%2528DDFE%2529%2520module%252C%2520capitalizing%250Aon%2520the%2520observation%2520that%2520a%2520single%2520source%2520LiDAR%2520point%2520cloud%2520encompasses%2520a%250Aspectrum%2520of%2520densities.%2520The%2520DDFE%2520module%2520is%2520meticulously%2520designed%2520to%2520extract%250Adensity-specific%2520features%2520within%2520a%2520single%2520source%2520domain%252C%2520facilitating%2520the%250Arecognition%2520of%2520objects%2520sharing%2520similar%2520density%2520characteristics%2520across%2520different%250ALiDAR%2520sensors.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520density%250Aaugmentation%2520technique%2520aimed%2520at%2520expanding%2520the%2520spectrum%2520of%2520density%2520in%2520source%250Adata%252C%2520thereby%2520enhancing%2520the%2520capabilities%2520of%2520the%2520DDFE.%2520Our%2520DDFE%2520stands%2520out%2520as%2520a%250Aversatile%2520and%2520lightweight%2520domain%2520generalization%2520module.%2520It%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520various%25203D%2520backbone%2520networks%252C%2520where%2520it%2520has%2520demonstrated%250Asuperior%2520performance%2520over%2520current%2520state-of-the-art%2520domain%2520generalization%250Amethods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/dgist-cvlab/MultiDensityDG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20LiDAR%20Domain%20Generalization%3A%20Single%20Source%20as%20Multiple%0A%20%20Density%20Domains&entry.906535625=Jaeyeul%20Kim%20and%20Jungwan%20Woo%20and%20Jeonghoon%20Kim%20and%20Sunghoon%20Im&entry.1292438233=%20%20In%20the%20realm%20of%20LiDAR-based%20perception%2C%20significant%20strides%20have%20been%20made%2C%0Ayet%20domain%20generalization%20remains%20a%20substantial%20challenge.%20The%20performance%0Aoften%20deteriorates%20when%20models%20are%20applied%20to%20unfamiliar%20datasets%20with%0Adifferent%20LiDAR%20sensors%20or%20deployed%20in%20new%20environments%2C%20primarily%20due%20to%0Avariations%20in%20point%20cloud%20density%20distributions.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20a%20Density%20Discriminative%20Feature%20Embedding%20%28DDFE%29%20module%2C%20capitalizing%0Aon%20the%20observation%20that%20a%20single%20source%20LiDAR%20point%20cloud%20encompasses%20a%0Aspectrum%20of%20densities.%20The%20DDFE%20module%20is%20meticulously%20designed%20to%20extract%0Adensity-specific%20features%20within%20a%20single%20source%20domain%2C%20facilitating%20the%0Arecognition%20of%20objects%20sharing%20similar%20density%20characteristics%20across%20different%0ALiDAR%20sensors.%20In%20addition%2C%20we%20introduce%20a%20simple%20yet%20effective%20density%0Aaugmentation%20technique%20aimed%20at%20expanding%20the%20spectrum%20of%20density%20in%20source%0Adata%2C%20thereby%20enhancing%20the%20capabilities%20of%20the%20DDFE.%20Our%20DDFE%20stands%20out%20as%20a%0Aversatile%20and%20lightweight%20domain%20generalization%20module.%20It%20can%20be%20seamlessly%0Aintegrated%20into%20various%203D%20backbone%20networks%2C%20where%20it%20has%20demonstrated%0Asuperior%20performance%20over%20current%20state-of-the-art%20domain%20generalization%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/dgist-cvlab/MultiDensityDG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12098v2&entry.124074799=Read"},
{"title": "Multimodal Cross-Domain Few-Shot Learning for Egocentric Action\n  Recognition", "author": "Masashi Hatano and Ryo Hachiuma and Ryo Fujii and Hideo Saito", "abstract": "  We address a novel cross-domain few-shot learning task (CD-FSL) with\nmultimodal input and unlabeled target data for egocentric action recognition.\nThis paper simultaneously tackles two critical challenges associated with\negocentric action recognition in CD-FSL settings: (1) the extreme domain gap in\negocentric videos (e.g., daily life vs. industrial domain) and (2) the\ncomputational cost for real-world applications. We propose MM-CDFSL, a\ndomain-adaptive and computationally efficient approach designed to enhance\nadaptability to the target domain and improve inference cost. To address the\nfirst challenge, we propose the incorporation of multimodal distillation into\nthe student RGB model using teacher models. Each teacher model is trained\nindependently on source and target data for its respective modality. Leveraging\nonly unlabeled target data during multimodal distillation enhances the student\nmodel's adaptability to the target domain. We further introduce ensemble masked\ninference, a technique that reduces the number of input tokens through masking.\nIn this approach, ensemble prediction mitigates the performance degradation\ncaused by masking, effectively addressing the second issue. Our approach\noutperformed the state-of-the-art CD-FSL approaches with a substantial margin\non multiple egocentric datasets, improving by an average of 6.12/6.10 points\nfor 1-shot/5-shot settings while achieving $2.2$ times faster inference speed.\nProject page: https://masashi-hatano.github.io/MM-CDFSL/\n", "link": "http://arxiv.org/abs/2405.19917v3", "date": "2024-07-16", "relevancy": 2.7126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5706}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5339}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition&body=Title%3A%20Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition%0AAuthor%3A%20Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Ryo%20Fujii%20and%20Hideo%20Saito%0AAbstract%3A%20%20%20We%20address%20a%20novel%20cross-domain%20few-shot%20learning%20task%20%28CD-FSL%29%20with%0Amultimodal%20input%20and%20unlabeled%20target%20data%20for%20egocentric%20action%20recognition.%0AThis%20paper%20simultaneously%20tackles%20two%20critical%20challenges%20associated%20with%0Aegocentric%20action%20recognition%20in%20CD-FSL%20settings%3A%20%281%29%20the%20extreme%20domain%20gap%20in%0Aegocentric%20videos%20%28e.g.%2C%20daily%20life%20vs.%20industrial%20domain%29%20and%20%282%29%20the%0Acomputational%20cost%20for%20real-world%20applications.%20We%20propose%20MM-CDFSL%2C%20a%0Adomain-adaptive%20and%20computationally%20efficient%20approach%20designed%20to%20enhance%0Aadaptability%20to%20the%20target%20domain%20and%20improve%20inference%20cost.%20To%20address%20the%0Afirst%20challenge%2C%20we%20propose%20the%20incorporation%20of%20multimodal%20distillation%20into%0Athe%20student%20RGB%20model%20using%20teacher%20models.%20Each%20teacher%20model%20is%20trained%0Aindependently%20on%20source%20and%20target%20data%20for%20its%20respective%20modality.%20Leveraging%0Aonly%20unlabeled%20target%20data%20during%20multimodal%20distillation%20enhances%20the%20student%0Amodel%27s%20adaptability%20to%20the%20target%20domain.%20We%20further%20introduce%20ensemble%20masked%0Ainference%2C%20a%20technique%20that%20reduces%20the%20number%20of%20input%20tokens%20through%20masking.%0AIn%20this%20approach%2C%20ensemble%20prediction%20mitigates%20the%20performance%20degradation%0Acaused%20by%20masking%2C%20effectively%20addressing%20the%20second%20issue.%20Our%20approach%0Aoutperformed%20the%20state-of-the-art%20CD-FSL%20approaches%20with%20a%20substantial%20margin%0Aon%20multiple%20egocentric%20datasets%2C%20improving%20by%20an%20average%20of%206.12/6.10%20points%0Afor%201-shot/5-shot%20settings%20while%20achieving%20%242.2%24%20times%20faster%20inference%20speed.%0AProject%20page%3A%20https%3A//masashi-hatano.github.io/MM-CDFSL/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Cross-Domain%2520Few-Shot%2520Learning%2520for%2520Egocentric%2520Action%250A%2520%2520Recognition%26entry.906535625%3DMasashi%2520Hatano%2520and%2520Ryo%2520Hachiuma%2520and%2520Ryo%2520Fujii%2520and%2520Hideo%2520Saito%26entry.1292438233%3D%2520%2520We%2520address%2520a%2520novel%2520cross-domain%2520few-shot%2520learning%2520task%2520%2528CD-FSL%2529%2520with%250Amultimodal%2520input%2520and%2520unlabeled%2520target%2520data%2520for%2520egocentric%2520action%2520recognition.%250AThis%2520paper%2520simultaneously%2520tackles%2520two%2520critical%2520challenges%2520associated%2520with%250Aegocentric%2520action%2520recognition%2520in%2520CD-FSL%2520settings%253A%2520%25281%2529%2520the%2520extreme%2520domain%2520gap%2520in%250Aegocentric%2520videos%2520%2528e.g.%252C%2520daily%2520life%2520vs.%2520industrial%2520domain%2529%2520and%2520%25282%2529%2520the%250Acomputational%2520cost%2520for%2520real-world%2520applications.%2520We%2520propose%2520MM-CDFSL%252C%2520a%250Adomain-adaptive%2520and%2520computationally%2520efficient%2520approach%2520designed%2520to%2520enhance%250Aadaptability%2520to%2520the%2520target%2520domain%2520and%2520improve%2520inference%2520cost.%2520To%2520address%2520the%250Afirst%2520challenge%252C%2520we%2520propose%2520the%2520incorporation%2520of%2520multimodal%2520distillation%2520into%250Athe%2520student%2520RGB%2520model%2520using%2520teacher%2520models.%2520Each%2520teacher%2520model%2520is%2520trained%250Aindependently%2520on%2520source%2520and%2520target%2520data%2520for%2520its%2520respective%2520modality.%2520Leveraging%250Aonly%2520unlabeled%2520target%2520data%2520during%2520multimodal%2520distillation%2520enhances%2520the%2520student%250Amodel%2527s%2520adaptability%2520to%2520the%2520target%2520domain.%2520We%2520further%2520introduce%2520ensemble%2520masked%250Ainference%252C%2520a%2520technique%2520that%2520reduces%2520the%2520number%2520of%2520input%2520tokens%2520through%2520masking.%250AIn%2520this%2520approach%252C%2520ensemble%2520prediction%2520mitigates%2520the%2520performance%2520degradation%250Acaused%2520by%2520masking%252C%2520effectively%2520addressing%2520the%2520second%2520issue.%2520Our%2520approach%250Aoutperformed%2520the%2520state-of-the-art%2520CD-FSL%2520approaches%2520with%2520a%2520substantial%2520margin%250Aon%2520multiple%2520egocentric%2520datasets%252C%2520improving%2520by%2520an%2520average%2520of%25206.12/6.10%2520points%250Afor%25201-shot/5-shot%2520settings%2520while%2520achieving%2520%25242.2%2524%2520times%2520faster%2520inference%2520speed.%250AProject%2520page%253A%2520https%253A//masashi-hatano.github.io/MM-CDFSL/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Cross-Domain%20Few-Shot%20Learning%20for%20Egocentric%20Action%0A%20%20Recognition&entry.906535625=Masashi%20Hatano%20and%20Ryo%20Hachiuma%20and%20Ryo%20Fujii%20and%20Hideo%20Saito&entry.1292438233=%20%20We%20address%20a%20novel%20cross-domain%20few-shot%20learning%20task%20%28CD-FSL%29%20with%0Amultimodal%20input%20and%20unlabeled%20target%20data%20for%20egocentric%20action%20recognition.%0AThis%20paper%20simultaneously%20tackles%20two%20critical%20challenges%20associated%20with%0Aegocentric%20action%20recognition%20in%20CD-FSL%20settings%3A%20%281%29%20the%20extreme%20domain%20gap%20in%0Aegocentric%20videos%20%28e.g.%2C%20daily%20life%20vs.%20industrial%20domain%29%20and%20%282%29%20the%0Acomputational%20cost%20for%20real-world%20applications.%20We%20propose%20MM-CDFSL%2C%20a%0Adomain-adaptive%20and%20computationally%20efficient%20approach%20designed%20to%20enhance%0Aadaptability%20to%20the%20target%20domain%20and%20improve%20inference%20cost.%20To%20address%20the%0Afirst%20challenge%2C%20we%20propose%20the%20incorporation%20of%20multimodal%20distillation%20into%0Athe%20student%20RGB%20model%20using%20teacher%20models.%20Each%20teacher%20model%20is%20trained%0Aindependently%20on%20source%20and%20target%20data%20for%20its%20respective%20modality.%20Leveraging%0Aonly%20unlabeled%20target%20data%20during%20multimodal%20distillation%20enhances%20the%20student%0Amodel%27s%20adaptability%20to%20the%20target%20domain.%20We%20further%20introduce%20ensemble%20masked%0Ainference%2C%20a%20technique%20that%20reduces%20the%20number%20of%20input%20tokens%20through%20masking.%0AIn%20this%20approach%2C%20ensemble%20prediction%20mitigates%20the%20performance%20degradation%0Acaused%20by%20masking%2C%20effectively%20addressing%20the%20second%20issue.%20Our%20approach%0Aoutperformed%20the%20state-of-the-art%20CD-FSL%20approaches%20with%20a%20substantial%20margin%0Aon%20multiple%20egocentric%20datasets%2C%20improving%20by%20an%20average%20of%206.12/6.10%20points%0Afor%201-shot/5-shot%20settings%20while%20achieving%20%242.2%24%20times%20faster%20inference%20speed.%0AProject%20page%3A%20https%3A//masashi-hatano.github.io/MM-CDFSL/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19917v3&entry.124074799=Read"},
{"title": "SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint\n  Alignment of Images", "author": "Nir Barel and Ron Shapira Weber and Nir Mualem and Shahaf E. Finder and Oren Freifeld", "abstract": "  The unsupervised task of Joint Alignment (JA) of images is beset by\nchallenges such as high complexity, geometric distortions, and convergence to\npoor local or even global optima. Although Vision Transformers (ViT) have\nrecently provided valuable features for JA, they fall short of fully addressing\nthese issues. Consequently, researchers frequently depend on expensive models\nand numerous regularization terms, resulting in long training times and\nchallenging hyperparameter tuning. We introduce the Spatial Joint Alignment\nModel (SpaceJAM), a novel approach that addresses the JA task with efficiency\nand simplicity. SpaceJAM leverages a compact architecture with only 16K\ntrainable parameters and uniquely operates without the need for regularization\nor atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate\nthat SpaceJAM matches the alignment capabilities of existing methods while\nsignificantly reducing computational demands and achieving at least a 10x\nspeedup. SpaceJAM sets a new standard for rapid and effective image alignment,\nmaking the process more accessible and efficient. Our code is available at:\nhttps://bgu-cs-vil.github.io/SpaceJAM/.\n", "link": "http://arxiv.org/abs/2407.11850v1", "date": "2024-07-16", "relevancy": 2.6908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5356}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images&body=Title%3A%20SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images%0AAuthor%3A%20Nir%20Barel%20and%20Ron%20Shapira%20Weber%20and%20Nir%20Mualem%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20The%20unsupervised%20task%20of%20Joint%20Alignment%20%28JA%29%20of%20images%20is%20beset%20by%0Achallenges%20such%20as%20high%20complexity%2C%20geometric%20distortions%2C%20and%20convergence%20to%0Apoor%20local%20or%20even%20global%20optima.%20Although%20Vision%20Transformers%20%28ViT%29%20have%0Arecently%20provided%20valuable%20features%20for%20JA%2C%20they%20fall%20short%20of%20fully%20addressing%0Athese%20issues.%20Consequently%2C%20researchers%20frequently%20depend%20on%20expensive%20models%0Aand%20numerous%20regularization%20terms%2C%20resulting%20in%20long%20training%20times%20and%0Achallenging%20hyperparameter%20tuning.%20We%20introduce%20the%20Spatial%20Joint%20Alignment%0AModel%20%28SpaceJAM%29%2C%20a%20novel%20approach%20that%20addresses%20the%20JA%20task%20with%20efficiency%0Aand%20simplicity.%20SpaceJAM%20leverages%20a%20compact%20architecture%20with%20only%2016K%0Atrainable%20parameters%20and%20uniquely%20operates%20without%20the%20need%20for%20regularization%0Aor%20atlas%20maintenance.%20Evaluations%20on%20SPair-71K%20and%20CUB%20datasets%20demonstrate%0Athat%20SpaceJAM%20matches%20the%20alignment%20capabilities%20of%20existing%20methods%20while%0Asignificantly%20reducing%20computational%20demands%20and%20achieving%20at%20least%20a%2010x%0Aspeedup.%20SpaceJAM%20sets%20a%20new%20standard%20for%20rapid%20and%20effective%20image%20alignment%2C%0Amaking%20the%20process%20more%20accessible%20and%20efficient.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/SpaceJAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceJAM%253A%2520a%2520Lightweight%2520and%2520Regularization-free%2520Method%2520for%2520Fast%2520Joint%250A%2520%2520Alignment%2520of%2520Images%26entry.906535625%3DNir%2520Barel%2520and%2520Ron%2520Shapira%2520Weber%2520and%2520Nir%2520Mualem%2520and%2520Shahaf%2520E.%2520Finder%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520The%2520unsupervised%2520task%2520of%2520Joint%2520Alignment%2520%2528JA%2529%2520of%2520images%2520is%2520beset%2520by%250Achallenges%2520such%2520as%2520high%2520complexity%252C%2520geometric%2520distortions%252C%2520and%2520convergence%2520to%250Apoor%2520local%2520or%2520even%2520global%2520optima.%2520Although%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%250Arecently%2520provided%2520valuable%2520features%2520for%2520JA%252C%2520they%2520fall%2520short%2520of%2520fully%2520addressing%250Athese%2520issues.%2520Consequently%252C%2520researchers%2520frequently%2520depend%2520on%2520expensive%2520models%250Aand%2520numerous%2520regularization%2520terms%252C%2520resulting%2520in%2520long%2520training%2520times%2520and%250Achallenging%2520hyperparameter%2520tuning.%2520We%2520introduce%2520the%2520Spatial%2520Joint%2520Alignment%250AModel%2520%2528SpaceJAM%2529%252C%2520a%2520novel%2520approach%2520that%2520addresses%2520the%2520JA%2520task%2520with%2520efficiency%250Aand%2520simplicity.%2520SpaceJAM%2520leverages%2520a%2520compact%2520architecture%2520with%2520only%252016K%250Atrainable%2520parameters%2520and%2520uniquely%2520operates%2520without%2520the%2520need%2520for%2520regularization%250Aor%2520atlas%2520maintenance.%2520Evaluations%2520on%2520SPair-71K%2520and%2520CUB%2520datasets%2520demonstrate%250Athat%2520SpaceJAM%2520matches%2520the%2520alignment%2520capabilities%2520of%2520existing%2520methods%2520while%250Asignificantly%2520reducing%2520computational%2520demands%2520and%2520achieving%2520at%2520least%2520a%252010x%250Aspeedup.%2520SpaceJAM%2520sets%2520a%2520new%2520standard%2520for%2520rapid%2520and%2520effective%2520image%2520alignment%252C%250Amaking%2520the%2520process%2520more%2520accessible%2520and%2520efficient.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//bgu-cs-vil.github.io/SpaceJAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images&entry.906535625=Nir%20Barel%20and%20Ron%20Shapira%20Weber%20and%20Nir%20Mualem%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld&entry.1292438233=%20%20The%20unsupervised%20task%20of%20Joint%20Alignment%20%28JA%29%20of%20images%20is%20beset%20by%0Achallenges%20such%20as%20high%20complexity%2C%20geometric%20distortions%2C%20and%20convergence%20to%0Apoor%20local%20or%20even%20global%20optima.%20Although%20Vision%20Transformers%20%28ViT%29%20have%0Arecently%20provided%20valuable%20features%20for%20JA%2C%20they%20fall%20short%20of%20fully%20addressing%0Athese%20issues.%20Consequently%2C%20researchers%20frequently%20depend%20on%20expensive%20models%0Aand%20numerous%20regularization%20terms%2C%20resulting%20in%20long%20training%20times%20and%0Achallenging%20hyperparameter%20tuning.%20We%20introduce%20the%20Spatial%20Joint%20Alignment%0AModel%20%28SpaceJAM%29%2C%20a%20novel%20approach%20that%20addresses%20the%20JA%20task%20with%20efficiency%0Aand%20simplicity.%20SpaceJAM%20leverages%20a%20compact%20architecture%20with%20only%2016K%0Atrainable%20parameters%20and%20uniquely%20operates%20without%20the%20need%20for%20regularization%0Aor%20atlas%20maintenance.%20Evaluations%20on%20SPair-71K%20and%20CUB%20datasets%20demonstrate%0Athat%20SpaceJAM%20matches%20the%20alignment%20capabilities%20of%20existing%20methods%20while%0Asignificantly%20reducing%20computational%20demands%20and%20achieving%20at%20least%20a%2010x%0Aspeedup.%20SpaceJAM%20sets%20a%20new%20standard%20for%20rapid%20and%20effective%20image%20alignment%2C%0Amaking%20the%20process%20more%20accessible%20and%20efficient.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/SpaceJAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11850v1&entry.124074799=Read"},
{"title": "GTPT: Group-based Token Pruning Transformer for Efficient Human Pose\n  Estimation", "author": "Haonan Wang and Jie Liu and Jie Tang and Gangshan Wu and Bo Xu and Yanbing Chou and Yong Wang", "abstract": "  In recent years, 2D human pose estimation has made significant progress on\npublic benchmarks. However, many of these approaches face challenges of less\napplicability in the industrial community due to the large number of parametric\nquantities and computational overhead. Efficient human pose estimation remains\na hurdle, especially for whole-body pose estimation with numerous keypoints.\nWhile most current methods for efficient human pose estimation primarily rely\non CNNs, we propose the Group-based Token Pruning Transformer (GTPT) that fully\nharnesses the advantages of the Transformer. GTPT alleviates the computational\nburden by gradually introducing keypoints in a coarse-to-fine manner. It\nminimizes the computation overhead while ensuring high performance. Besides,\nGTPT groups keypoint tokens and prunes visual tokens to improve model\nperformance while reducing redundancy. We propose the Multi-Head Group\nAttention (MHGA) between different groups to achieve global interaction with\nlittle computational overhead. We conducted experiments on COCO and\nCOCO-WholeBody. Compared to other methods, the experimental results show that\nGTPT can achieve higher performance with less computation, especially in\nwhole-body with numerous keypoints.\n", "link": "http://arxiv.org/abs/2407.10756v2", "date": "2024-07-16", "relevancy": 2.6803, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.56}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5301}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Haonan%20Wang%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Bo%20Xu%20and%20Yanbing%20Chou%20and%20Yong%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%202D%20human%20pose%20estimation%20has%20made%20significant%20progress%20on%0Apublic%20benchmarks.%20However%2C%20many%20of%20these%20approaches%20face%20challenges%20of%20less%0Aapplicability%20in%20the%20industrial%20community%20due%20to%20the%20large%20number%20of%20parametric%0Aquantities%20and%20computational%20overhead.%20Efficient%20human%20pose%20estimation%20remains%0Aa%20hurdle%2C%20especially%20for%20whole-body%20pose%20estimation%20with%20numerous%20keypoints.%0AWhile%20most%20current%20methods%20for%20efficient%20human%20pose%20estimation%20primarily%20rely%0Aon%20CNNs%2C%20we%20propose%20the%20Group-based%20Token%20Pruning%20Transformer%20%28GTPT%29%20that%20fully%0Aharnesses%20the%20advantages%20of%20the%20Transformer.%20GTPT%20alleviates%20the%20computational%0Aburden%20by%20gradually%20introducing%20keypoints%20in%20a%20coarse-to-fine%20manner.%20It%0Aminimizes%20the%20computation%20overhead%20while%20ensuring%20high%20performance.%20Besides%2C%0AGTPT%20groups%20keypoint%20tokens%20and%20prunes%20visual%20tokens%20to%20improve%20model%0Aperformance%20while%20reducing%20redundancy.%20We%20propose%20the%20Multi-Head%20Group%0AAttention%20%28MHGA%29%20between%20different%20groups%20to%20achieve%20global%20interaction%20with%0Alittle%20computational%20overhead.%20We%20conducted%20experiments%20on%20COCO%20and%0ACOCO-WholeBody.%20Compared%20to%20other%20methods%2C%20the%20experimental%20results%20show%20that%0AGTPT%20can%20achieve%20higher%20performance%20with%20less%20computation%2C%20especially%20in%0Awhole-body%20with%20numerous%20keypoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTPT%253A%2520Group-based%2520Token%2520Pruning%2520Transformer%2520for%2520Efficient%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DHaonan%2520Wang%2520and%2520Jie%2520Liu%2520and%2520Jie%2520Tang%2520and%2520Gangshan%2520Wu%2520and%2520Bo%2520Xu%2520and%2520Yanbing%2520Chou%2520and%2520Yong%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25202D%2520human%2520pose%2520estimation%2520has%2520made%2520significant%2520progress%2520on%250Apublic%2520benchmarks.%2520However%252C%2520many%2520of%2520these%2520approaches%2520face%2520challenges%2520of%2520less%250Aapplicability%2520in%2520the%2520industrial%2520community%2520due%2520to%2520the%2520large%2520number%2520of%2520parametric%250Aquantities%2520and%2520computational%2520overhead.%2520Efficient%2520human%2520pose%2520estimation%2520remains%250Aa%2520hurdle%252C%2520especially%2520for%2520whole-body%2520pose%2520estimation%2520with%2520numerous%2520keypoints.%250AWhile%2520most%2520current%2520methods%2520for%2520efficient%2520human%2520pose%2520estimation%2520primarily%2520rely%250Aon%2520CNNs%252C%2520we%2520propose%2520the%2520Group-based%2520Token%2520Pruning%2520Transformer%2520%2528GTPT%2529%2520that%2520fully%250Aharnesses%2520the%2520advantages%2520of%2520the%2520Transformer.%2520GTPT%2520alleviates%2520the%2520computational%250Aburden%2520by%2520gradually%2520introducing%2520keypoints%2520in%2520a%2520coarse-to-fine%2520manner.%2520It%250Aminimizes%2520the%2520computation%2520overhead%2520while%2520ensuring%2520high%2520performance.%2520Besides%252C%250AGTPT%2520groups%2520keypoint%2520tokens%2520and%2520prunes%2520visual%2520tokens%2520to%2520improve%2520model%250Aperformance%2520while%2520reducing%2520redundancy.%2520We%2520propose%2520the%2520Multi-Head%2520Group%250AAttention%2520%2528MHGA%2529%2520between%2520different%2520groups%2520to%2520achieve%2520global%2520interaction%2520with%250Alittle%2520computational%2520overhead.%2520We%2520conducted%2520experiments%2520on%2520COCO%2520and%250ACOCO-WholeBody.%2520Compared%2520to%2520other%2520methods%252C%2520the%2520experimental%2520results%2520show%2520that%250AGTPT%2520can%2520achieve%2520higher%2520performance%2520with%2520less%2520computation%252C%2520especially%2520in%250Awhole-body%2520with%2520numerous%2520keypoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTPT%3A%20Group-based%20Token%20Pruning%20Transformer%20for%20Efficient%20Human%20Pose%0A%20%20Estimation&entry.906535625=Haonan%20Wang%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Bo%20Xu%20and%20Yanbing%20Chou%20and%20Yong%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%202D%20human%20pose%20estimation%20has%20made%20significant%20progress%20on%0Apublic%20benchmarks.%20However%2C%20many%20of%20these%20approaches%20face%20challenges%20of%20less%0Aapplicability%20in%20the%20industrial%20community%20due%20to%20the%20large%20number%20of%20parametric%0Aquantities%20and%20computational%20overhead.%20Efficient%20human%20pose%20estimation%20remains%0Aa%20hurdle%2C%20especially%20for%20whole-body%20pose%20estimation%20with%20numerous%20keypoints.%0AWhile%20most%20current%20methods%20for%20efficient%20human%20pose%20estimation%20primarily%20rely%0Aon%20CNNs%2C%20we%20propose%20the%20Group-based%20Token%20Pruning%20Transformer%20%28GTPT%29%20that%20fully%0Aharnesses%20the%20advantages%20of%20the%20Transformer.%20GTPT%20alleviates%20the%20computational%0Aburden%20by%20gradually%20introducing%20keypoints%20in%20a%20coarse-to-fine%20manner.%20It%0Aminimizes%20the%20computation%20overhead%20while%20ensuring%20high%20performance.%20Besides%2C%0AGTPT%20groups%20keypoint%20tokens%20and%20prunes%20visual%20tokens%20to%20improve%20model%0Aperformance%20while%20reducing%20redundancy.%20We%20propose%20the%20Multi-Head%20Group%0AAttention%20%28MHGA%29%20between%20different%20groups%20to%20achieve%20global%20interaction%20with%0Alittle%20computational%20overhead.%20We%20conducted%20experiments%20on%20COCO%20and%0ACOCO-WholeBody.%20Compared%20to%20other%20methods%2C%20the%20experimental%20results%20show%20that%0AGTPT%20can%20achieve%20higher%20performance%20with%20less%20computation%2C%20especially%20in%0Awhole-body%20with%20numerous%20keypoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10756v2&entry.124074799=Read"},
{"title": "Learning Network Representations with Disentangled Graph Auto-Encoder", "author": "Di Fan and Chuanhou Gao", "abstract": "  The (variational) graph auto-encoder is widely used to learn representations\nfor graph-structured data. However, the formation of real-world graphs is a\ncomplicated and heterogeneous process influenced by latent factors. Existing\nencoders are fundamentally holistic, neglecting the entanglement of latent\nfactors. This reduces the effectiveness of graph analysis tasks, while also\nmaking it more difficult to explain the learned representations. As a result,\nlearning disentangled graph representations with the (variational) graph\nauto-encoder poses significant challenges and remains largely unexplored in the\ncurrent research. In this paper, we introduce the Disentangled Graph\nAuto-Encoder (DGA) and the Disentangled Variational Graph Auto-Encoder (DVGA)\nto learn disentangled representations. Specifically, we first design a\ndisentangled graph convolutional network with multi-channel message-passing\nlayers to serve as the encoder. This allows each channel to aggregate\ninformation about each latent factor. The disentangled variational graph\nauto-encoder's expressive capability is then enhanced by applying a\ncomponent-wise flow to each channel. In addition, we construct a factor-wise\ndecoder that takes into account the characteristics of disentangled\nrepresentations. We improve the independence of representations by imposing\nindependence constraints on the mapping channels for distinct latent factors.\nEmpirical experiments on both synthetic and real-world datasets demonstrate the\nsuperiority of our proposed method compared to several state-of-the-art\nbaselines.\n", "link": "http://arxiv.org/abs/2402.01143v2", "date": "2024-07-16", "relevancy": 2.6677, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.569}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5474}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Network%20Representations%20with%20Disentangled%20Graph%20Auto-Encoder&body=Title%3A%20Learning%20Network%20Representations%20with%20Disentangled%20Graph%20Auto-Encoder%0AAuthor%3A%20Di%20Fan%20and%20Chuanhou%20Gao%0AAbstract%3A%20%20%20The%20%28variational%29%20graph%20auto-encoder%20is%20widely%20used%20to%20learn%20representations%0Afor%20graph-structured%20data.%20However%2C%20the%20formation%20of%20real-world%20graphs%20is%20a%0Acomplicated%20and%20heterogeneous%20process%20influenced%20by%20latent%20factors.%20Existing%0Aencoders%20are%20fundamentally%20holistic%2C%20neglecting%20the%20entanglement%20of%20latent%0Afactors.%20This%20reduces%20the%20effectiveness%20of%20graph%20analysis%20tasks%2C%20while%20also%0Amaking%20it%20more%20difficult%20to%20explain%20the%20learned%20representations.%20As%20a%20result%2C%0Alearning%20disentangled%20graph%20representations%20with%20the%20%28variational%29%20graph%0Aauto-encoder%20poses%20significant%20challenges%20and%20remains%20largely%20unexplored%20in%20the%0Acurrent%20research.%20In%20this%20paper%2C%20we%20introduce%20the%20Disentangled%20Graph%0AAuto-Encoder%20%28DGA%29%20and%20the%20Disentangled%20Variational%20Graph%20Auto-Encoder%20%28DVGA%29%0Ato%20learn%20disentangled%20representations.%20Specifically%2C%20we%20first%20design%20a%0Adisentangled%20graph%20convolutional%20network%20with%20multi-channel%20message-passing%0Alayers%20to%20serve%20as%20the%20encoder.%20This%20allows%20each%20channel%20to%20aggregate%0Ainformation%20about%20each%20latent%20factor.%20The%20disentangled%20variational%20graph%0Aauto-encoder%27s%20expressive%20capability%20is%20then%20enhanced%20by%20applying%20a%0Acomponent-wise%20flow%20to%20each%20channel.%20In%20addition%2C%20we%20construct%20a%20factor-wise%0Adecoder%20that%20takes%20into%20account%20the%20characteristics%20of%20disentangled%0Arepresentations.%20We%20improve%20the%20independence%20of%20representations%20by%20imposing%0Aindependence%20constraints%20on%20the%20mapping%20channels%20for%20distinct%20latent%20factors.%0AEmpirical%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%20compared%20to%20several%20state-of-the-art%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Network%2520Representations%2520with%2520Disentangled%2520Graph%2520Auto-Encoder%26entry.906535625%3DDi%2520Fan%2520and%2520Chuanhou%2520Gao%26entry.1292438233%3D%2520%2520The%2520%2528variational%2529%2520graph%2520auto-encoder%2520is%2520widely%2520used%2520to%2520learn%2520representations%250Afor%2520graph-structured%2520data.%2520However%252C%2520the%2520formation%2520of%2520real-world%2520graphs%2520is%2520a%250Acomplicated%2520and%2520heterogeneous%2520process%2520influenced%2520by%2520latent%2520factors.%2520Existing%250Aencoders%2520are%2520fundamentally%2520holistic%252C%2520neglecting%2520the%2520entanglement%2520of%2520latent%250Afactors.%2520This%2520reduces%2520the%2520effectiveness%2520of%2520graph%2520analysis%2520tasks%252C%2520while%2520also%250Amaking%2520it%2520more%2520difficult%2520to%2520explain%2520the%2520learned%2520representations.%2520As%2520a%2520result%252C%250Alearning%2520disentangled%2520graph%2520representations%2520with%2520the%2520%2528variational%2529%2520graph%250Aauto-encoder%2520poses%2520significant%2520challenges%2520and%2520remains%2520largely%2520unexplored%2520in%2520the%250Acurrent%2520research.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Disentangled%2520Graph%250AAuto-Encoder%2520%2528DGA%2529%2520and%2520the%2520Disentangled%2520Variational%2520Graph%2520Auto-Encoder%2520%2528DVGA%2529%250Ato%2520learn%2520disentangled%2520representations.%2520Specifically%252C%2520we%2520first%2520design%2520a%250Adisentangled%2520graph%2520convolutional%2520network%2520with%2520multi-channel%2520message-passing%250Alayers%2520to%2520serve%2520as%2520the%2520encoder.%2520This%2520allows%2520each%2520channel%2520to%2520aggregate%250Ainformation%2520about%2520each%2520latent%2520factor.%2520The%2520disentangled%2520variational%2520graph%250Aauto-encoder%2527s%2520expressive%2520capability%2520is%2520then%2520enhanced%2520by%2520applying%2520a%250Acomponent-wise%2520flow%2520to%2520each%2520channel.%2520In%2520addition%252C%2520we%2520construct%2520a%2520factor-wise%250Adecoder%2520that%2520takes%2520into%2520account%2520the%2520characteristics%2520of%2520disentangled%250Arepresentations.%2520We%2520improve%2520the%2520independence%2520of%2520representations%2520by%2520imposing%250Aindependence%2520constraints%2520on%2520the%2520mapping%2520channels%2520for%2520distinct%2520latent%2520factors.%250AEmpirical%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520method%2520compared%2520to%2520several%2520state-of-the-art%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Network%20Representations%20with%20Disentangled%20Graph%20Auto-Encoder&entry.906535625=Di%20Fan%20and%20Chuanhou%20Gao&entry.1292438233=%20%20The%20%28variational%29%20graph%20auto-encoder%20is%20widely%20used%20to%20learn%20representations%0Afor%20graph-structured%20data.%20However%2C%20the%20formation%20of%20real-world%20graphs%20is%20a%0Acomplicated%20and%20heterogeneous%20process%20influenced%20by%20latent%20factors.%20Existing%0Aencoders%20are%20fundamentally%20holistic%2C%20neglecting%20the%20entanglement%20of%20latent%0Afactors.%20This%20reduces%20the%20effectiveness%20of%20graph%20analysis%20tasks%2C%20while%20also%0Amaking%20it%20more%20difficult%20to%20explain%20the%20learned%20representations.%20As%20a%20result%2C%0Alearning%20disentangled%20graph%20representations%20with%20the%20%28variational%29%20graph%0Aauto-encoder%20poses%20significant%20challenges%20and%20remains%20largely%20unexplored%20in%20the%0Acurrent%20research.%20In%20this%20paper%2C%20we%20introduce%20the%20Disentangled%20Graph%0AAuto-Encoder%20%28DGA%29%20and%20the%20Disentangled%20Variational%20Graph%20Auto-Encoder%20%28DVGA%29%0Ato%20learn%20disentangled%20representations.%20Specifically%2C%20we%20first%20design%20a%0Adisentangled%20graph%20convolutional%20network%20with%20multi-channel%20message-passing%0Alayers%20to%20serve%20as%20the%20encoder.%20This%20allows%20each%20channel%20to%20aggregate%0Ainformation%20about%20each%20latent%20factor.%20The%20disentangled%20variational%20graph%0Aauto-encoder%27s%20expressive%20capability%20is%20then%20enhanced%20by%20applying%20a%0Acomponent-wise%20flow%20to%20each%20channel.%20In%20addition%2C%20we%20construct%20a%20factor-wise%0Adecoder%20that%20takes%20into%20account%20the%20characteristics%20of%20disentangled%0Arepresentations.%20We%20improve%20the%20independence%20of%20representations%20by%20imposing%0Aindependence%20constraints%20on%20the%20mapping%20channels%20for%20distinct%20latent%20factors.%0AEmpirical%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%20compared%20to%20several%20state-of-the-art%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01143v2&entry.124074799=Read"},
{"title": "DepGAN: Leveraging Depth Maps for Handling Occlusions and Transparency\n  in Image Composition", "author": "Amr Ghoneim and Jiju Poovvancheri and Yasushi Akiyama and Dong Chen", "abstract": "  Image composition is a complex task which requires a lot of information about\nthe scene for an accurate and realistic composition, such as perspective,\nlighting, shadows, occlusions, and object interactions. Previous methods have\npredominantly used 2D information for image composition, neglecting the\npotentials of 3D spatial information. In this work, we propose DepGAN, a\nGenerative Adversarial Network that utilizes depth maps and alpha channels to\nrectify inaccurate occlusions and enhance transparency effects in image\ncomposition. Central to our network is a novel loss function called Depth Aware\nLoss which quantifies the pixel wise depth difference to accurately delineate\nocclusion boundaries while compositing objects at different depth levels.\nFurthermore, we enhance our network's learning process by utilizing opacity\ndata, enabling it to effectively manage compositions involving transparent and\nsemi-transparent objects. We tested our model against state-of-the-art image\ncomposition GANs on benchmark (both real and synthetic) datasets. The results\nreveal that DepGAN significantly outperforms existing methods in terms of\naccuracy of object placement semantics, transparency and occlusion handling,\nboth visually and quantitatively. Our code is available at\nhttps://amrtsg.github.io/DepGAN/.\n", "link": "http://arxiv.org/abs/2407.11890v1", "date": "2024-07-16", "relevancy": 2.6442, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepGAN%3A%20Leveraging%20Depth%20Maps%20for%20Handling%20Occlusions%20and%20Transparency%0A%20%20in%20Image%20Composition&body=Title%3A%20DepGAN%3A%20Leveraging%20Depth%20Maps%20for%20Handling%20Occlusions%20and%20Transparency%0A%20%20in%20Image%20Composition%0AAuthor%3A%20Amr%20Ghoneim%20and%20Jiju%20Poovvancheri%20and%20Yasushi%20Akiyama%20and%20Dong%20Chen%0AAbstract%3A%20%20%20Image%20composition%20is%20a%20complex%20task%20which%20requires%20a%20lot%20of%20information%20about%0Athe%20scene%20for%20an%20accurate%20and%20realistic%20composition%2C%20such%20as%20perspective%2C%0Alighting%2C%20shadows%2C%20occlusions%2C%20and%20object%20interactions.%20Previous%20methods%20have%0Apredominantly%20used%202D%20information%20for%20image%20composition%2C%20neglecting%20the%0Apotentials%20of%203D%20spatial%20information.%20In%20this%20work%2C%20we%20propose%20DepGAN%2C%20a%0AGenerative%20Adversarial%20Network%20that%20utilizes%20depth%20maps%20and%20alpha%20channels%20to%0Arectify%20inaccurate%20occlusions%20and%20enhance%20transparency%20effects%20in%20image%0Acomposition.%20Central%20to%20our%20network%20is%20a%20novel%20loss%20function%20called%20Depth%20Aware%0ALoss%20which%20quantifies%20the%20pixel%20wise%20depth%20difference%20to%20accurately%20delineate%0Aocclusion%20boundaries%20while%20compositing%20objects%20at%20different%20depth%20levels.%0AFurthermore%2C%20we%20enhance%20our%20network%27s%20learning%20process%20by%20utilizing%20opacity%0Adata%2C%20enabling%20it%20to%20effectively%20manage%20compositions%20involving%20transparent%20and%0Asemi-transparent%20objects.%20We%20tested%20our%20model%20against%20state-of-the-art%20image%0Acomposition%20GANs%20on%20benchmark%20%28both%20real%20and%20synthetic%29%20datasets.%20The%20results%0Areveal%20that%20DepGAN%20significantly%20outperforms%20existing%20methods%20in%20terms%20of%0Aaccuracy%20of%20object%20placement%20semantics%2C%20transparency%20and%20occlusion%20handling%2C%0Aboth%20visually%20and%20quantitatively.%20Our%20code%20is%20available%20at%0Ahttps%3A//amrtsg.github.io/DepGAN/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepGAN%253A%2520Leveraging%2520Depth%2520Maps%2520for%2520Handling%2520Occlusions%2520and%2520Transparency%250A%2520%2520in%2520Image%2520Composition%26entry.906535625%3DAmr%2520Ghoneim%2520and%2520Jiju%2520Poovvancheri%2520and%2520Yasushi%2520Akiyama%2520and%2520Dong%2520Chen%26entry.1292438233%3D%2520%2520Image%2520composition%2520is%2520a%2520complex%2520task%2520which%2520requires%2520a%2520lot%2520of%2520information%2520about%250Athe%2520scene%2520for%2520an%2520accurate%2520and%2520realistic%2520composition%252C%2520such%2520as%2520perspective%252C%250Alighting%252C%2520shadows%252C%2520occlusions%252C%2520and%2520object%2520interactions.%2520Previous%2520methods%2520have%250Apredominantly%2520used%25202D%2520information%2520for%2520image%2520composition%252C%2520neglecting%2520the%250Apotentials%2520of%25203D%2520spatial%2520information.%2520In%2520this%2520work%252C%2520we%2520propose%2520DepGAN%252C%2520a%250AGenerative%2520Adversarial%2520Network%2520that%2520utilizes%2520depth%2520maps%2520and%2520alpha%2520channels%2520to%250Arectify%2520inaccurate%2520occlusions%2520and%2520enhance%2520transparency%2520effects%2520in%2520image%250Acomposition.%2520Central%2520to%2520our%2520network%2520is%2520a%2520novel%2520loss%2520function%2520called%2520Depth%2520Aware%250ALoss%2520which%2520quantifies%2520the%2520pixel%2520wise%2520depth%2520difference%2520to%2520accurately%2520delineate%250Aocclusion%2520boundaries%2520while%2520compositing%2520objects%2520at%2520different%2520depth%2520levels.%250AFurthermore%252C%2520we%2520enhance%2520our%2520network%2527s%2520learning%2520process%2520by%2520utilizing%2520opacity%250Adata%252C%2520enabling%2520it%2520to%2520effectively%2520manage%2520compositions%2520involving%2520transparent%2520and%250Asemi-transparent%2520objects.%2520We%2520tested%2520our%2520model%2520against%2520state-of-the-art%2520image%250Acomposition%2520GANs%2520on%2520benchmark%2520%2528both%2520real%2520and%2520synthetic%2529%2520datasets.%2520The%2520results%250Areveal%2520that%2520DepGAN%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%250Aaccuracy%2520of%2520object%2520placement%2520semantics%252C%2520transparency%2520and%2520occlusion%2520handling%252C%250Aboth%2520visually%2520and%2520quantitatively.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//amrtsg.github.io/DepGAN/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepGAN%3A%20Leveraging%20Depth%20Maps%20for%20Handling%20Occlusions%20and%20Transparency%0A%20%20in%20Image%20Composition&entry.906535625=Amr%20Ghoneim%20and%20Jiju%20Poovvancheri%20and%20Yasushi%20Akiyama%20and%20Dong%20Chen&entry.1292438233=%20%20Image%20composition%20is%20a%20complex%20task%20which%20requires%20a%20lot%20of%20information%20about%0Athe%20scene%20for%20an%20accurate%20and%20realistic%20composition%2C%20such%20as%20perspective%2C%0Alighting%2C%20shadows%2C%20occlusions%2C%20and%20object%20interactions.%20Previous%20methods%20have%0Apredominantly%20used%202D%20information%20for%20image%20composition%2C%20neglecting%20the%0Apotentials%20of%203D%20spatial%20information.%20In%20this%20work%2C%20we%20propose%20DepGAN%2C%20a%0AGenerative%20Adversarial%20Network%20that%20utilizes%20depth%20maps%20and%20alpha%20channels%20to%0Arectify%20inaccurate%20occlusions%20and%20enhance%20transparency%20effects%20in%20image%0Acomposition.%20Central%20to%20our%20network%20is%20a%20novel%20loss%20function%20called%20Depth%20Aware%0ALoss%20which%20quantifies%20the%20pixel%20wise%20depth%20difference%20to%20accurately%20delineate%0Aocclusion%20boundaries%20while%20compositing%20objects%20at%20different%20depth%20levels.%0AFurthermore%2C%20we%20enhance%20our%20network%27s%20learning%20process%20by%20utilizing%20opacity%0Adata%2C%20enabling%20it%20to%20effectively%20manage%20compositions%20involving%20transparent%20and%0Asemi-transparent%20objects.%20We%20tested%20our%20model%20against%20state-of-the-art%20image%0Acomposition%20GANs%20on%20benchmark%20%28both%20real%20and%20synthetic%29%20datasets.%20The%20results%0Areveal%20that%20DepGAN%20significantly%20outperforms%20existing%20methods%20in%20terms%20of%0Aaccuracy%20of%20object%20placement%20semantics%2C%20transparency%20and%20occlusion%20handling%2C%0Aboth%20visually%20and%20quantitatively.%20Our%20code%20is%20available%20at%0Ahttps%3A//amrtsg.github.io/DepGAN/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11890v1&entry.124074799=Read"},
{"title": "Temporally Consistent Stereo Matching", "author": "Jiaxi Zeng and Chengtang Yao and Yuwei Wu and Yunde Jia", "abstract": "  Stereo matching provides depth estimation from binocular images for\ndownstream applications. These applications mostly take video streams as input\nand require temporally consistent depth maps. However, existing methods mainly\nfocus on the estimation at the single-frame level. This commonly leads to\ntemporally inconsistent results, especially in ill-posed regions. In this\npaper, we aim to leverage temporal information to improve the temporal\nconsistency, accuracy, and efficiency of stereo matching. To achieve this, we\nformulate video stereo matching as a process of temporal disparity completion\nfollowed by continuous iterative refinements. Specifically, we first project\nthe disparity of the previous timestamp to the current viewpoint, obtaining a\nsemi-dense disparity map. Then, we complete this map through a disparity\ncompletion module to obtain a well-initialized disparity map. The state\nfeatures from the current completion module and from the past refinement are\nfused together, providing a temporally coherent state for subsequent\nrefinement. Based on this coherent state, we introduce a dual-space refinement\nmodule to iteratively refine the initialized result in both disparity and\ndisparity gradient spaces, improving estimations in ill-posed regions.\nExtensive experiments demonstrate that our method effectively alleviates\ntemporal inconsistency while enhancing both accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2407.11950v1", "date": "2024-07-16", "relevancy": 2.6148, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5451}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporally%20Consistent%20Stereo%20Matching&body=Title%3A%20Temporally%20Consistent%20Stereo%20Matching%0AAuthor%3A%20Jiaxi%20Zeng%20and%20Chengtang%20Yao%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%0AAbstract%3A%20%20%20Stereo%20matching%20provides%20depth%20estimation%20from%20binocular%20images%20for%0Adownstream%20applications.%20These%20applications%20mostly%20take%20video%20streams%20as%20input%0Aand%20require%20temporally%20consistent%20depth%20maps.%20However%2C%20existing%20methods%20mainly%0Afocus%20on%20the%20estimation%20at%20the%20single-frame%20level.%20This%20commonly%20leads%20to%0Atemporally%20inconsistent%20results%2C%20especially%20in%20ill-posed%20regions.%20In%20this%0Apaper%2C%20we%20aim%20to%20leverage%20temporal%20information%20to%20improve%20the%20temporal%0Aconsistency%2C%20accuracy%2C%20and%20efficiency%20of%20stereo%20matching.%20To%20achieve%20this%2C%20we%0Aformulate%20video%20stereo%20matching%20as%20a%20process%20of%20temporal%20disparity%20completion%0Afollowed%20by%20continuous%20iterative%20refinements.%20Specifically%2C%20we%20first%20project%0Athe%20disparity%20of%20the%20previous%20timestamp%20to%20the%20current%20viewpoint%2C%20obtaining%20a%0Asemi-dense%20disparity%20map.%20Then%2C%20we%20complete%20this%20map%20through%20a%20disparity%0Acompletion%20module%20to%20obtain%20a%20well-initialized%20disparity%20map.%20The%20state%0Afeatures%20from%20the%20current%20completion%20module%20and%20from%20the%20past%20refinement%20are%0Afused%20together%2C%20providing%20a%20temporally%20coherent%20state%20for%20subsequent%0Arefinement.%20Based%20on%20this%20coherent%20state%2C%20we%20introduce%20a%20dual-space%20refinement%0Amodule%20to%20iteratively%20refine%20the%20initialized%20result%20in%20both%20disparity%20and%0Adisparity%20gradient%20spaces%2C%20improving%20estimations%20in%20ill-posed%20regions.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20effectively%20alleviates%0Atemporal%20inconsistency%20while%20enhancing%20both%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporally%2520Consistent%2520Stereo%2520Matching%26entry.906535625%3DJiaxi%2520Zeng%2520and%2520Chengtang%2520Yao%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520provides%2520depth%2520estimation%2520from%2520binocular%2520images%2520for%250Adownstream%2520applications.%2520These%2520applications%2520mostly%2520take%2520video%2520streams%2520as%2520input%250Aand%2520require%2520temporally%2520consistent%2520depth%2520maps.%2520However%252C%2520existing%2520methods%2520mainly%250Afocus%2520on%2520the%2520estimation%2520at%2520the%2520single-frame%2520level.%2520This%2520commonly%2520leads%2520to%250Atemporally%2520inconsistent%2520results%252C%2520especially%2520in%2520ill-posed%2520regions.%2520In%2520this%250Apaper%252C%2520we%2520aim%2520to%2520leverage%2520temporal%2520information%2520to%2520improve%2520the%2520temporal%250Aconsistency%252C%2520accuracy%252C%2520and%2520efficiency%2520of%2520stereo%2520matching.%2520To%2520achieve%2520this%252C%2520we%250Aformulate%2520video%2520stereo%2520matching%2520as%2520a%2520process%2520of%2520temporal%2520disparity%2520completion%250Afollowed%2520by%2520continuous%2520iterative%2520refinements.%2520Specifically%252C%2520we%2520first%2520project%250Athe%2520disparity%2520of%2520the%2520previous%2520timestamp%2520to%2520the%2520current%2520viewpoint%252C%2520obtaining%2520a%250Asemi-dense%2520disparity%2520map.%2520Then%252C%2520we%2520complete%2520this%2520map%2520through%2520a%2520disparity%250Acompletion%2520module%2520to%2520obtain%2520a%2520well-initialized%2520disparity%2520map.%2520The%2520state%250Afeatures%2520from%2520the%2520current%2520completion%2520module%2520and%2520from%2520the%2520past%2520refinement%2520are%250Afused%2520together%252C%2520providing%2520a%2520temporally%2520coherent%2520state%2520for%2520subsequent%250Arefinement.%2520Based%2520on%2520this%2520coherent%2520state%252C%2520we%2520introduce%2520a%2520dual-space%2520refinement%250Amodule%2520to%2520iteratively%2520refine%2520the%2520initialized%2520result%2520in%2520both%2520disparity%2520and%250Adisparity%2520gradient%2520spaces%252C%2520improving%2520estimations%2520in%2520ill-posed%2520regions.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520effectively%2520alleviates%250Atemporal%2520inconsistency%2520while%2520enhancing%2520both%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporally%20Consistent%20Stereo%20Matching&entry.906535625=Jiaxi%20Zeng%20and%20Chengtang%20Yao%20and%20Yuwei%20Wu%20and%20Yunde%20Jia&entry.1292438233=%20%20Stereo%20matching%20provides%20depth%20estimation%20from%20binocular%20images%20for%0Adownstream%20applications.%20These%20applications%20mostly%20take%20video%20streams%20as%20input%0Aand%20require%20temporally%20consistent%20depth%20maps.%20However%2C%20existing%20methods%20mainly%0Afocus%20on%20the%20estimation%20at%20the%20single-frame%20level.%20This%20commonly%20leads%20to%0Atemporally%20inconsistent%20results%2C%20especially%20in%20ill-posed%20regions.%20In%20this%0Apaper%2C%20we%20aim%20to%20leverage%20temporal%20information%20to%20improve%20the%20temporal%0Aconsistency%2C%20accuracy%2C%20and%20efficiency%20of%20stereo%20matching.%20To%20achieve%20this%2C%20we%0Aformulate%20video%20stereo%20matching%20as%20a%20process%20of%20temporal%20disparity%20completion%0Afollowed%20by%20continuous%20iterative%20refinements.%20Specifically%2C%20we%20first%20project%0Athe%20disparity%20of%20the%20previous%20timestamp%20to%20the%20current%20viewpoint%2C%20obtaining%20a%0Asemi-dense%20disparity%20map.%20Then%2C%20we%20complete%20this%20map%20through%20a%20disparity%0Acompletion%20module%20to%20obtain%20a%20well-initialized%20disparity%20map.%20The%20state%0Afeatures%20from%20the%20current%20completion%20module%20and%20from%20the%20past%20refinement%20are%0Afused%20together%2C%20providing%20a%20temporally%20coherent%20state%20for%20subsequent%0Arefinement.%20Based%20on%20this%20coherent%20state%2C%20we%20introduce%20a%20dual-space%20refinement%0Amodule%20to%20iteratively%20refine%20the%20initialized%20result%20in%20both%20disparity%20and%0Adisparity%20gradient%20spaces%2C%20improving%20estimations%20in%20ill-posed%20regions.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20effectively%20alleviates%0Atemporal%20inconsistency%20while%20enhancing%20both%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11950v1&entry.124074799=Read"},
{"title": "Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based\n  Approach", "author": "Tanvir Hossain and Khaled Mohammed Saifuddin and Muhammad Ifte Khairul Islam and Farhan Tanvir and Esra Akbas", "abstract": "  Graph Neural Network (GNN) achieves great success for node-level and\ngraph-level tasks via encoding meaningful topological structures of networks in\nvarious domains, ranging from social to biological networks. However, repeated\naggregation operations lead to excessive mixing of node representations,\nparticularly in dense regions with multiple GNN layers, resulting in nearly\nindistinguishable embeddings. This phenomenon leads to the oversmoothing\nproblem that hampers downstream graph analytics tasks. To overcome this issue,\nwe propose a novel and flexible truss-based graph sparsification model that\nprunes edges from dense regions of the graph. Pruning redundant edges in dense\nregions helps to prevent the aggregation of excessive neighborhood information\nduring hierarchical message passing and pooling in GNN models. We then utilize\nour sparsification model in the state-of-the-art baseline GNNs and pooling\nmodels, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and\nAdamGNN. Extensive experiments on different real-world datasets show that our\nmodel significantly improves the performance of the baseline GNN models in the\ngraph classification task.\n", "link": "http://arxiv.org/abs/2407.11928v1", "date": "2024-07-16", "relevancy": 2.6069, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5325}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5169}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20Oversmoothing%20in%20GNN%20via%20Graph%20Sparsification%3A%20A%20Truss-based%0A%20%20Approach&body=Title%3A%20Tackling%20Oversmoothing%20in%20GNN%20via%20Graph%20Sparsification%3A%20A%20Truss-based%0A%20%20Approach%0AAuthor%3A%20Tanvir%20Hossain%20and%20Khaled%20Mohammed%20Saifuddin%20and%20Muhammad%20Ifte%20Khairul%20Islam%20and%20Farhan%20Tanvir%20and%20Esra%20Akbas%0AAbstract%3A%20%20%20Graph%20Neural%20Network%20%28GNN%29%20achieves%20great%20success%20for%20node-level%20and%0Agraph-level%20tasks%20via%20encoding%20meaningful%20topological%20structures%20of%20networks%20in%0Avarious%20domains%2C%20ranging%20from%20social%20to%20biological%20networks.%20However%2C%20repeated%0Aaggregation%20operations%20lead%20to%20excessive%20mixing%20of%20node%20representations%2C%0Aparticularly%20in%20dense%20regions%20with%20multiple%20GNN%20layers%2C%20resulting%20in%20nearly%0Aindistinguishable%20embeddings.%20This%20phenomenon%20leads%20to%20the%20oversmoothing%0Aproblem%20that%20hampers%20downstream%20graph%20analytics%20tasks.%20To%20overcome%20this%20issue%2C%0Awe%20propose%20a%20novel%20and%20flexible%20truss-based%20graph%20sparsification%20model%20that%0Aprunes%20edges%20from%20dense%20regions%20of%20the%20graph.%20Pruning%20redundant%20edges%20in%20dense%0Aregions%20helps%20to%20prevent%20the%20aggregation%20of%20excessive%20neighborhood%20information%0Aduring%20hierarchical%20message%20passing%20and%20pooling%20in%20GNN%20models.%20We%20then%20utilize%0Aour%20sparsification%20model%20in%20the%20state-of-the-art%20baseline%20GNNs%20and%20pooling%0Amodels%2C%20such%20as%20GIN%2C%20SAGPool%2C%20GMT%2C%20DiffPool%2C%20MinCutPool%2C%20HGP-SL%2C%20DMonPool%2C%20and%0AAdamGNN.%20Extensive%20experiments%20on%20different%20real-world%20datasets%20show%20that%20our%0Amodel%20significantly%20improves%20the%20performance%20of%20the%20baseline%20GNN%20models%20in%20the%0Agraph%20classification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520Oversmoothing%2520in%2520GNN%2520via%2520Graph%2520Sparsification%253A%2520A%2520Truss-based%250A%2520%2520Approach%26entry.906535625%3DTanvir%2520Hossain%2520and%2520Khaled%2520Mohammed%2520Saifuddin%2520and%2520Muhammad%2520Ifte%2520Khairul%2520Islam%2520and%2520Farhan%2520Tanvir%2520and%2520Esra%2520Akbas%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520achieves%2520great%2520success%2520for%2520node-level%2520and%250Agraph-level%2520tasks%2520via%2520encoding%2520meaningful%2520topological%2520structures%2520of%2520networks%2520in%250Avarious%2520domains%252C%2520ranging%2520from%2520social%2520to%2520biological%2520networks.%2520However%252C%2520repeated%250Aaggregation%2520operations%2520lead%2520to%2520excessive%2520mixing%2520of%2520node%2520representations%252C%250Aparticularly%2520in%2520dense%2520regions%2520with%2520multiple%2520GNN%2520layers%252C%2520resulting%2520in%2520nearly%250Aindistinguishable%2520embeddings.%2520This%2520phenomenon%2520leads%2520to%2520the%2520oversmoothing%250Aproblem%2520that%2520hampers%2520downstream%2520graph%2520analytics%2520tasks.%2520To%2520overcome%2520this%2520issue%252C%250Awe%2520propose%2520a%2520novel%2520and%2520flexible%2520truss-based%2520graph%2520sparsification%2520model%2520that%250Aprunes%2520edges%2520from%2520dense%2520regions%2520of%2520the%2520graph.%2520Pruning%2520redundant%2520edges%2520in%2520dense%250Aregions%2520helps%2520to%2520prevent%2520the%2520aggregation%2520of%2520excessive%2520neighborhood%2520information%250Aduring%2520hierarchical%2520message%2520passing%2520and%2520pooling%2520in%2520GNN%2520models.%2520We%2520then%2520utilize%250Aour%2520sparsification%2520model%2520in%2520the%2520state-of-the-art%2520baseline%2520GNNs%2520and%2520pooling%250Amodels%252C%2520such%2520as%2520GIN%252C%2520SAGPool%252C%2520GMT%252C%2520DiffPool%252C%2520MinCutPool%252C%2520HGP-SL%252C%2520DMonPool%252C%2520and%250AAdamGNN.%2520Extensive%2520experiments%2520on%2520different%2520real-world%2520datasets%2520show%2520that%2520our%250Amodel%2520significantly%2520improves%2520the%2520performance%2520of%2520the%2520baseline%2520GNN%2520models%2520in%2520the%250Agraph%2520classification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20Oversmoothing%20in%20GNN%20via%20Graph%20Sparsification%3A%20A%20Truss-based%0A%20%20Approach&entry.906535625=Tanvir%20Hossain%20and%20Khaled%20Mohammed%20Saifuddin%20and%20Muhammad%20Ifte%20Khairul%20Islam%20and%20Farhan%20Tanvir%20and%20Esra%20Akbas&entry.1292438233=%20%20Graph%20Neural%20Network%20%28GNN%29%20achieves%20great%20success%20for%20node-level%20and%0Agraph-level%20tasks%20via%20encoding%20meaningful%20topological%20structures%20of%20networks%20in%0Avarious%20domains%2C%20ranging%20from%20social%20to%20biological%20networks.%20However%2C%20repeated%0Aaggregation%20operations%20lead%20to%20excessive%20mixing%20of%20node%20representations%2C%0Aparticularly%20in%20dense%20regions%20with%20multiple%20GNN%20layers%2C%20resulting%20in%20nearly%0Aindistinguishable%20embeddings.%20This%20phenomenon%20leads%20to%20the%20oversmoothing%0Aproblem%20that%20hampers%20downstream%20graph%20analytics%20tasks.%20To%20overcome%20this%20issue%2C%0Awe%20propose%20a%20novel%20and%20flexible%20truss-based%20graph%20sparsification%20model%20that%0Aprunes%20edges%20from%20dense%20regions%20of%20the%20graph.%20Pruning%20redundant%20edges%20in%20dense%0Aregions%20helps%20to%20prevent%20the%20aggregation%20of%20excessive%20neighborhood%20information%0Aduring%20hierarchical%20message%20passing%20and%20pooling%20in%20GNN%20models.%20We%20then%20utilize%0Aour%20sparsification%20model%20in%20the%20state-of-the-art%20baseline%20GNNs%20and%20pooling%0Amodels%2C%20such%20as%20GIN%2C%20SAGPool%2C%20GMT%2C%20DiffPool%2C%20MinCutPool%2C%20HGP-SL%2C%20DMonPool%2C%20and%0AAdamGNN.%20Extensive%20experiments%20on%20different%20real-world%20datasets%20show%20that%20our%0Amodel%20significantly%20improves%20the%20performance%20of%20the%20baseline%20GNN%20models%20in%20the%0Agraph%20classification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11928v1&entry.124074799=Read"},
{"title": "MEVG: Multi-event Video Generation with Text-to-Video Models", "author": "Gyeongrok Oh and Jaehwan Jeong and Sieun Kim and Wonmin Byeon and Jinkyu Kim and Sungwoong Kim and Sangpil Kim", "abstract": "  We introduce a novel diffusion-based video generation method, generating a\nvideo showing multiple events given multiple individual sentences from the\nuser. Our method does not require a large-scale video dataset since our method\nuses a pre-trained diffusion-based text-to-video generative model without a\nfine-tuning process. Specifically, we propose a last frame-aware diffusion\nprocess to preserve visual coherence between consecutive videos where each\nvideo consists of different events by initializing the latent and\nsimultaneously adjusting noise in the latent to enhance the motion dynamic in a\ngenerated video. Furthermore, we find that the iterative update of latent\nvectors by referring to all the preceding frames maintains the global\nappearance across the frames in a video clip. To handle dynamic text input for\nvideo generation, we utilize a novel prompt generator that transfers course\ntext messages from the user into the multiple optimal prompts for the\ntext-to-video diffusion model. Extensive experiments and user studies show that\nour proposed method is superior to other video-generative models in terms of\ntemporal coherency of content and semantics. Video examples are available on\nour project page: https://kuai-lab.github.io/eccv2024mevg.\n", "link": "http://arxiv.org/abs/2312.04086v2", "date": "2024-07-16", "relevancy": 2.562, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.694}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6712}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEVG%3A%20Multi-event%20Video%20Generation%20with%20Text-to-Video%20Models&body=Title%3A%20MEVG%3A%20Multi-event%20Video%20Generation%20with%20Text-to-Video%20Models%0AAuthor%3A%20Gyeongrok%20Oh%20and%20Jaehwan%20Jeong%20and%20Sieun%20Kim%20and%20Wonmin%20Byeon%20and%20Jinkyu%20Kim%20and%20Sungwoong%20Kim%20and%20Sangpil%20Kim%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20diffusion-based%20video%20generation%20method%2C%20generating%20a%0Avideo%20showing%20multiple%20events%20given%20multiple%20individual%20sentences%20from%20the%0Auser.%20Our%20method%20does%20not%20require%20a%20large-scale%20video%20dataset%20since%20our%20method%0Auses%20a%20pre-trained%20diffusion-based%20text-to-video%20generative%20model%20without%20a%0Afine-tuning%20process.%20Specifically%2C%20we%20propose%20a%20last%20frame-aware%20diffusion%0Aprocess%20to%20preserve%20visual%20coherence%20between%20consecutive%20videos%20where%20each%0Avideo%20consists%20of%20different%20events%20by%20initializing%20the%20latent%20and%0Asimultaneously%20adjusting%20noise%20in%20the%20latent%20to%20enhance%20the%20motion%20dynamic%20in%20a%0Agenerated%20video.%20Furthermore%2C%20we%20find%20that%20the%20iterative%20update%20of%20latent%0Avectors%20by%20referring%20to%20all%20the%20preceding%20frames%20maintains%20the%20global%0Aappearance%20across%20the%20frames%20in%20a%20video%20clip.%20To%20handle%20dynamic%20text%20input%20for%0Avideo%20generation%2C%20we%20utilize%20a%20novel%20prompt%20generator%20that%20transfers%20course%0Atext%20messages%20from%20the%20user%20into%20the%20multiple%20optimal%20prompts%20for%20the%0Atext-to-video%20diffusion%20model.%20Extensive%20experiments%20and%20user%20studies%20show%20that%0Aour%20proposed%20method%20is%20superior%20to%20other%20video-generative%20models%20in%20terms%20of%0Atemporal%20coherency%20of%20content%20and%20semantics.%20Video%20examples%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//kuai-lab.github.io/eccv2024mevg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEVG%253A%2520Multi-event%2520Video%2520Generation%2520with%2520Text-to-Video%2520Models%26entry.906535625%3DGyeongrok%2520Oh%2520and%2520Jaehwan%2520Jeong%2520and%2520Sieun%2520Kim%2520and%2520Wonmin%2520Byeon%2520and%2520Jinkyu%2520Kim%2520and%2520Sungwoong%2520Kim%2520and%2520Sangpil%2520Kim%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520diffusion-based%2520video%2520generation%2520method%252C%2520generating%2520a%250Avideo%2520showing%2520multiple%2520events%2520given%2520multiple%2520individual%2520sentences%2520from%2520the%250Auser.%2520Our%2520method%2520does%2520not%2520require%2520a%2520large-scale%2520video%2520dataset%2520since%2520our%2520method%250Auses%2520a%2520pre-trained%2520diffusion-based%2520text-to-video%2520generative%2520model%2520without%2520a%250Afine-tuning%2520process.%2520Specifically%252C%2520we%2520propose%2520a%2520last%2520frame-aware%2520diffusion%250Aprocess%2520to%2520preserve%2520visual%2520coherence%2520between%2520consecutive%2520videos%2520where%2520each%250Avideo%2520consists%2520of%2520different%2520events%2520by%2520initializing%2520the%2520latent%2520and%250Asimultaneously%2520adjusting%2520noise%2520in%2520the%2520latent%2520to%2520enhance%2520the%2520motion%2520dynamic%2520in%2520a%250Agenerated%2520video.%2520Furthermore%252C%2520we%2520find%2520that%2520the%2520iterative%2520update%2520of%2520latent%250Avectors%2520by%2520referring%2520to%2520all%2520the%2520preceding%2520frames%2520maintains%2520the%2520global%250Aappearance%2520across%2520the%2520frames%2520in%2520a%2520video%2520clip.%2520To%2520handle%2520dynamic%2520text%2520input%2520for%250Avideo%2520generation%252C%2520we%2520utilize%2520a%2520novel%2520prompt%2520generator%2520that%2520transfers%2520course%250Atext%2520messages%2520from%2520the%2520user%2520into%2520the%2520multiple%2520optimal%2520prompts%2520for%2520the%250Atext-to-video%2520diffusion%2520model.%2520Extensive%2520experiments%2520and%2520user%2520studies%2520show%2520that%250Aour%2520proposed%2520method%2520is%2520superior%2520to%2520other%2520video-generative%2520models%2520in%2520terms%2520of%250Atemporal%2520coherency%2520of%2520content%2520and%2520semantics.%2520Video%2520examples%2520are%2520available%2520on%250Aour%2520project%2520page%253A%2520https%253A//kuai-lab.github.io/eccv2024mevg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEVG%3A%20Multi-event%20Video%20Generation%20with%20Text-to-Video%20Models&entry.906535625=Gyeongrok%20Oh%20and%20Jaehwan%20Jeong%20and%20Sieun%20Kim%20and%20Wonmin%20Byeon%20and%20Jinkyu%20Kim%20and%20Sungwoong%20Kim%20and%20Sangpil%20Kim&entry.1292438233=%20%20We%20introduce%20a%20novel%20diffusion-based%20video%20generation%20method%2C%20generating%20a%0Avideo%20showing%20multiple%20events%20given%20multiple%20individual%20sentences%20from%20the%0Auser.%20Our%20method%20does%20not%20require%20a%20large-scale%20video%20dataset%20since%20our%20method%0Auses%20a%20pre-trained%20diffusion-based%20text-to-video%20generative%20model%20without%20a%0Afine-tuning%20process.%20Specifically%2C%20we%20propose%20a%20last%20frame-aware%20diffusion%0Aprocess%20to%20preserve%20visual%20coherence%20between%20consecutive%20videos%20where%20each%0Avideo%20consists%20of%20different%20events%20by%20initializing%20the%20latent%20and%0Asimultaneously%20adjusting%20noise%20in%20the%20latent%20to%20enhance%20the%20motion%20dynamic%20in%20a%0Agenerated%20video.%20Furthermore%2C%20we%20find%20that%20the%20iterative%20update%20of%20latent%0Avectors%20by%20referring%20to%20all%20the%20preceding%20frames%20maintains%20the%20global%0Aappearance%20across%20the%20frames%20in%20a%20video%20clip.%20To%20handle%20dynamic%20text%20input%20for%0Avideo%20generation%2C%20we%20utilize%20a%20novel%20prompt%20generator%20that%20transfers%20course%0Atext%20messages%20from%20the%20user%20into%20the%20multiple%20optimal%20prompts%20for%20the%0Atext-to-video%20diffusion%20model.%20Extensive%20experiments%20and%20user%20studies%20show%20that%0Aour%20proposed%20method%20is%20superior%20to%20other%20video-generative%20models%20in%20terms%20of%0Atemporal%20coherency%20of%20content%20and%20semantics.%20Video%20examples%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//kuai-lab.github.io/eccv2024mevg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04086v2&entry.124074799=Read"},
{"title": "OAM-TCD: A globally diverse dataset of high-resolution tree cover maps", "author": "Josh Veitch-Michaelis and Andrew Cottam and Daniella Schweizer and Eben N. Broadbent and David Dao and Ce Zhang and Angelica Almeyda Zambrano and Simeon Max", "abstract": "  Accurately quantifying tree cover is an important metric for ecosystem\nmonitoring and for assessing progress in restored sites. Recent works have\nshown that deep learning-based segmentation algorithms are capable of\naccurately mapping trees at country and continental scales using\nhigh-resolution aerial and satellite imagery. Mapping at high (ideally\nsub-meter) resolution is necessary to identify individual trees, however there\nare few open-access datasets containing instance level annotations and those\nthat exist are small or not geographically diverse. We present a novel\nopen-access dataset for individual tree crown delineation (TCD) in\nhigh-resolution aerial imagery sourced from OpenAerialMap (OAM). Our dataset,\nOAM-TCD, comprises 5072 2048x2048 px images at 10 cm/px resolution with\nassociated human-labeled instance masks for over 280k individual and 56k groups\nof trees. By sampling imagery from around the world, we are able to better\ncapture the diversity and morphology of trees in different terrestrial biomes\nand in both urban and natural environments. Using our dataset, we train\nreference instance and semantic segmentation models that compare favorably to\nexisting state-of-the-art models. We assess performance through k-fold\ncross-validation and comparison with existing datasets; additionally we\ndemonstrate compelling results on independent aerial imagery captured over\nSwitzerland and compare to municipal tree inventories and LIDAR-derived canopy\nmaps in the city of Zurich. Our dataset, models and training/benchmark code are\npublicly released under permissive open-source licenses: Creative Commons\n(majority CC BY 4.0), and Apache 2.0 respectively.\n", "link": "http://arxiv.org/abs/2407.11743v1", "date": "2024-07-16", "relevancy": 2.5615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OAM-TCD%3A%20A%20globally%20diverse%20dataset%20of%20high-resolution%20tree%20cover%20maps&body=Title%3A%20OAM-TCD%3A%20A%20globally%20diverse%20dataset%20of%20high-resolution%20tree%20cover%20maps%0AAuthor%3A%20Josh%20Veitch-Michaelis%20and%20Andrew%20Cottam%20and%20Daniella%20Schweizer%20and%20Eben%20N.%20Broadbent%20and%20David%20Dao%20and%20Ce%20Zhang%20and%20Angelica%20Almeyda%20Zambrano%20and%20Simeon%20Max%0AAbstract%3A%20%20%20Accurately%20quantifying%20tree%20cover%20is%20an%20important%20metric%20for%20ecosystem%0Amonitoring%20and%20for%20assessing%20progress%20in%20restored%20sites.%20Recent%20works%20have%0Ashown%20that%20deep%20learning-based%20segmentation%20algorithms%20are%20capable%20of%0Aaccurately%20mapping%20trees%20at%20country%20and%20continental%20scales%20using%0Ahigh-resolution%20aerial%20and%20satellite%20imagery.%20Mapping%20at%20high%20%28ideally%0Asub-meter%29%20resolution%20is%20necessary%20to%20identify%20individual%20trees%2C%20however%20there%0Aare%20few%20open-access%20datasets%20containing%20instance%20level%20annotations%20and%20those%0Athat%20exist%20are%20small%20or%20not%20geographically%20diverse.%20We%20present%20a%20novel%0Aopen-access%20dataset%20for%20individual%20tree%20crown%20delineation%20%28TCD%29%20in%0Ahigh-resolution%20aerial%20imagery%20sourced%20from%20OpenAerialMap%20%28OAM%29.%20Our%20dataset%2C%0AOAM-TCD%2C%20comprises%205072%202048x2048%20px%20images%20at%2010%20cm/px%20resolution%20with%0Aassociated%20human-labeled%20instance%20masks%20for%20over%20280k%20individual%20and%2056k%20groups%0Aof%20trees.%20By%20sampling%20imagery%20from%20around%20the%20world%2C%20we%20are%20able%20to%20better%0Acapture%20the%20diversity%20and%20morphology%20of%20trees%20in%20different%20terrestrial%20biomes%0Aand%20in%20both%20urban%20and%20natural%20environments.%20Using%20our%20dataset%2C%20we%20train%0Areference%20instance%20and%20semantic%20segmentation%20models%20that%20compare%20favorably%20to%0Aexisting%20state-of-the-art%20models.%20We%20assess%20performance%20through%20k-fold%0Across-validation%20and%20comparison%20with%20existing%20datasets%3B%20additionally%20we%0Ademonstrate%20compelling%20results%20on%20independent%20aerial%20imagery%20captured%20over%0ASwitzerland%20and%20compare%20to%20municipal%20tree%20inventories%20and%20LIDAR-derived%20canopy%0Amaps%20in%20the%20city%20of%20Zurich.%20Our%20dataset%2C%20models%20and%20training/benchmark%20code%20are%0Apublicly%20released%20under%20permissive%20open-source%20licenses%3A%20Creative%20Commons%0A%28majority%20CC%20BY%204.0%29%2C%20and%20Apache%202.0%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOAM-TCD%253A%2520A%2520globally%2520diverse%2520dataset%2520of%2520high-resolution%2520tree%2520cover%2520maps%26entry.906535625%3DJosh%2520Veitch-Michaelis%2520and%2520Andrew%2520Cottam%2520and%2520Daniella%2520Schweizer%2520and%2520Eben%2520N.%2520Broadbent%2520and%2520David%2520Dao%2520and%2520Ce%2520Zhang%2520and%2520Angelica%2520Almeyda%2520Zambrano%2520and%2520Simeon%2520Max%26entry.1292438233%3D%2520%2520Accurately%2520quantifying%2520tree%2520cover%2520is%2520an%2520important%2520metric%2520for%2520ecosystem%250Amonitoring%2520and%2520for%2520assessing%2520progress%2520in%2520restored%2520sites.%2520Recent%2520works%2520have%250Ashown%2520that%2520deep%2520learning-based%2520segmentation%2520algorithms%2520are%2520capable%2520of%250Aaccurately%2520mapping%2520trees%2520at%2520country%2520and%2520continental%2520scales%2520using%250Ahigh-resolution%2520aerial%2520and%2520satellite%2520imagery.%2520Mapping%2520at%2520high%2520%2528ideally%250Asub-meter%2529%2520resolution%2520is%2520necessary%2520to%2520identify%2520individual%2520trees%252C%2520however%2520there%250Aare%2520few%2520open-access%2520datasets%2520containing%2520instance%2520level%2520annotations%2520and%2520those%250Athat%2520exist%2520are%2520small%2520or%2520not%2520geographically%2520diverse.%2520We%2520present%2520a%2520novel%250Aopen-access%2520dataset%2520for%2520individual%2520tree%2520crown%2520delineation%2520%2528TCD%2529%2520in%250Ahigh-resolution%2520aerial%2520imagery%2520sourced%2520from%2520OpenAerialMap%2520%2528OAM%2529.%2520Our%2520dataset%252C%250AOAM-TCD%252C%2520comprises%25205072%25202048x2048%2520px%2520images%2520at%252010%2520cm/px%2520resolution%2520with%250Aassociated%2520human-labeled%2520instance%2520masks%2520for%2520over%2520280k%2520individual%2520and%252056k%2520groups%250Aof%2520trees.%2520By%2520sampling%2520imagery%2520from%2520around%2520the%2520world%252C%2520we%2520are%2520able%2520to%2520better%250Acapture%2520the%2520diversity%2520and%2520morphology%2520of%2520trees%2520in%2520different%2520terrestrial%2520biomes%250Aand%2520in%2520both%2520urban%2520and%2520natural%2520environments.%2520Using%2520our%2520dataset%252C%2520we%2520train%250Areference%2520instance%2520and%2520semantic%2520segmentation%2520models%2520that%2520compare%2520favorably%2520to%250Aexisting%2520state-of-the-art%2520models.%2520We%2520assess%2520performance%2520through%2520k-fold%250Across-validation%2520and%2520comparison%2520with%2520existing%2520datasets%253B%2520additionally%2520we%250Ademonstrate%2520compelling%2520results%2520on%2520independent%2520aerial%2520imagery%2520captured%2520over%250ASwitzerland%2520and%2520compare%2520to%2520municipal%2520tree%2520inventories%2520and%2520LIDAR-derived%2520canopy%250Amaps%2520in%2520the%2520city%2520of%2520Zurich.%2520Our%2520dataset%252C%2520models%2520and%2520training/benchmark%2520code%2520are%250Apublicly%2520released%2520under%2520permissive%2520open-source%2520licenses%253A%2520Creative%2520Commons%250A%2528majority%2520CC%2520BY%25204.0%2529%252C%2520and%2520Apache%25202.0%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OAM-TCD%3A%20A%20globally%20diverse%20dataset%20of%20high-resolution%20tree%20cover%20maps&entry.906535625=Josh%20Veitch-Michaelis%20and%20Andrew%20Cottam%20and%20Daniella%20Schweizer%20and%20Eben%20N.%20Broadbent%20and%20David%20Dao%20and%20Ce%20Zhang%20and%20Angelica%20Almeyda%20Zambrano%20and%20Simeon%20Max&entry.1292438233=%20%20Accurately%20quantifying%20tree%20cover%20is%20an%20important%20metric%20for%20ecosystem%0Amonitoring%20and%20for%20assessing%20progress%20in%20restored%20sites.%20Recent%20works%20have%0Ashown%20that%20deep%20learning-based%20segmentation%20algorithms%20are%20capable%20of%0Aaccurately%20mapping%20trees%20at%20country%20and%20continental%20scales%20using%0Ahigh-resolution%20aerial%20and%20satellite%20imagery.%20Mapping%20at%20high%20%28ideally%0Asub-meter%29%20resolution%20is%20necessary%20to%20identify%20individual%20trees%2C%20however%20there%0Aare%20few%20open-access%20datasets%20containing%20instance%20level%20annotations%20and%20those%0Athat%20exist%20are%20small%20or%20not%20geographically%20diverse.%20We%20present%20a%20novel%0Aopen-access%20dataset%20for%20individual%20tree%20crown%20delineation%20%28TCD%29%20in%0Ahigh-resolution%20aerial%20imagery%20sourced%20from%20OpenAerialMap%20%28OAM%29.%20Our%20dataset%2C%0AOAM-TCD%2C%20comprises%205072%202048x2048%20px%20images%20at%2010%20cm/px%20resolution%20with%0Aassociated%20human-labeled%20instance%20masks%20for%20over%20280k%20individual%20and%2056k%20groups%0Aof%20trees.%20By%20sampling%20imagery%20from%20around%20the%20world%2C%20we%20are%20able%20to%20better%0Acapture%20the%20diversity%20and%20morphology%20of%20trees%20in%20different%20terrestrial%20biomes%0Aand%20in%20both%20urban%20and%20natural%20environments.%20Using%20our%20dataset%2C%20we%20train%0Areference%20instance%20and%20semantic%20segmentation%20models%20that%20compare%20favorably%20to%0Aexisting%20state-of-the-art%20models.%20We%20assess%20performance%20through%20k-fold%0Across-validation%20and%20comparison%20with%20existing%20datasets%3B%20additionally%20we%0Ademonstrate%20compelling%20results%20on%20independent%20aerial%20imagery%20captured%20over%0ASwitzerland%20and%20compare%20to%20municipal%20tree%20inventories%20and%20LIDAR-derived%20canopy%0Amaps%20in%20the%20city%20of%20Zurich.%20Our%20dataset%2C%20models%20and%20training/benchmark%20code%20are%0Apublicly%20released%20under%20permissive%20open-source%20licenses%3A%20Creative%20Commons%0A%28majority%20CC%20BY%204.0%29%2C%20and%20Apache%202.0%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11743v1&entry.124074799=Read"},
{"title": "Latent Spectral Regularization for Continual Learning", "author": "Emanuele Frascaroli and Riccardo Benaglia and Matteo Boschini and Luca Moschella and Cosimo Fiorini and Emanuele Rodol\u00e0 and Simone Calderara", "abstract": "  While biological intelligence grows organically as new knowledge is gathered\nthroughout life, Artificial Neural Networks forget catastrophically whenever\nthey face a changing training data distribution. Rehearsal-based Continual\nLearning (CL) approaches have been established as a versatile and reliable\nsolution to overcome this limitation; however, sudden input disruptions and\nmemory constraints are known to alter the consistency of their predictions. We\nstudy this phenomenon by investigating the geometric characteristics of the\nlearner's latent space and find that replayed data points of different classes\nincreasingly mix up, interfering with classification. Hence, we propose a\ngeometric regularizer that enforces weak requirements on the Laplacian spectrum\nof the latent space, promoting a partitioning behavior. Our proposal, called\nContinual Spectral Regularizer for Incremental Learning (CaSpeR-IL), can be\neasily combined with any rehearsal-based CL approach and improves the\nperformance of SOTA methods on standard benchmarks.\n", "link": "http://arxiv.org/abs/2301.03345v4", "date": "2024-07-16", "relevancy": 2.5562, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5301}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Spectral%20Regularization%20for%20Continual%20Learning&body=Title%3A%20Latent%20Spectral%20Regularization%20for%20Continual%20Learning%0AAuthor%3A%20Emanuele%20Frascaroli%20and%20Riccardo%20Benaglia%20and%20Matteo%20Boschini%20and%20Luca%20Moschella%20and%20Cosimo%20Fiorini%20and%20Emanuele%20Rodol%C3%A0%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20While%20biological%20intelligence%20grows%20organically%20as%20new%20knowledge%20is%20gathered%0Athroughout%20life%2C%20Artificial%20Neural%20Networks%20forget%20catastrophically%20whenever%0Athey%20face%20a%20changing%20training%20data%20distribution.%20Rehearsal-based%20Continual%0ALearning%20%28CL%29%20approaches%20have%20been%20established%20as%20a%20versatile%20and%20reliable%0Asolution%20to%20overcome%20this%20limitation%3B%20however%2C%20sudden%20input%20disruptions%20and%0Amemory%20constraints%20are%20known%20to%20alter%20the%20consistency%20of%20their%20predictions.%20We%0Astudy%20this%20phenomenon%20by%20investigating%20the%20geometric%20characteristics%20of%20the%0Alearner%27s%20latent%20space%20and%20find%20that%20replayed%20data%20points%20of%20different%20classes%0Aincreasingly%20mix%20up%2C%20interfering%20with%20classification.%20Hence%2C%20we%20propose%20a%0Ageometric%20regularizer%20that%20enforces%20weak%20requirements%20on%20the%20Laplacian%20spectrum%0Aof%20the%20latent%20space%2C%20promoting%20a%20partitioning%20behavior.%20Our%20proposal%2C%20called%0AContinual%20Spectral%20Regularizer%20for%20Incremental%20Learning%20%28CaSpeR-IL%29%2C%20can%20be%0Aeasily%20combined%20with%20any%20rehearsal-based%20CL%20approach%20and%20improves%20the%0Aperformance%20of%20SOTA%20methods%20on%20standard%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.03345v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Spectral%2520Regularization%2520for%2520Continual%2520Learning%26entry.906535625%3DEmanuele%2520Frascaroli%2520and%2520Riccardo%2520Benaglia%2520and%2520Matteo%2520Boschini%2520and%2520Luca%2520Moschella%2520and%2520Cosimo%2520Fiorini%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520While%2520biological%2520intelligence%2520grows%2520organically%2520as%2520new%2520knowledge%2520is%2520gathered%250Athroughout%2520life%252C%2520Artificial%2520Neural%2520Networks%2520forget%2520catastrophically%2520whenever%250Athey%2520face%2520a%2520changing%2520training%2520data%2520distribution.%2520Rehearsal-based%2520Continual%250ALearning%2520%2528CL%2529%2520approaches%2520have%2520been%2520established%2520as%2520a%2520versatile%2520and%2520reliable%250Asolution%2520to%2520overcome%2520this%2520limitation%253B%2520however%252C%2520sudden%2520input%2520disruptions%2520and%250Amemory%2520constraints%2520are%2520known%2520to%2520alter%2520the%2520consistency%2520of%2520their%2520predictions.%2520We%250Astudy%2520this%2520phenomenon%2520by%2520investigating%2520the%2520geometric%2520characteristics%2520of%2520the%250Alearner%2527s%2520latent%2520space%2520and%2520find%2520that%2520replayed%2520data%2520points%2520of%2520different%2520classes%250Aincreasingly%2520mix%2520up%252C%2520interfering%2520with%2520classification.%2520Hence%252C%2520we%2520propose%2520a%250Ageometric%2520regularizer%2520that%2520enforces%2520weak%2520requirements%2520on%2520the%2520Laplacian%2520spectrum%250Aof%2520the%2520latent%2520space%252C%2520promoting%2520a%2520partitioning%2520behavior.%2520Our%2520proposal%252C%2520called%250AContinual%2520Spectral%2520Regularizer%2520for%2520Incremental%2520Learning%2520%2528CaSpeR-IL%2529%252C%2520can%2520be%250Aeasily%2520combined%2520with%2520any%2520rehearsal-based%2520CL%2520approach%2520and%2520improves%2520the%250Aperformance%2520of%2520SOTA%2520methods%2520on%2520standard%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.03345v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Spectral%20Regularization%20for%20Continual%20Learning&entry.906535625=Emanuele%20Frascaroli%20and%20Riccardo%20Benaglia%20and%20Matteo%20Boschini%20and%20Luca%20Moschella%20and%20Cosimo%20Fiorini%20and%20Emanuele%20Rodol%C3%A0%20and%20Simone%20Calderara&entry.1292438233=%20%20While%20biological%20intelligence%20grows%20organically%20as%20new%20knowledge%20is%20gathered%0Athroughout%20life%2C%20Artificial%20Neural%20Networks%20forget%20catastrophically%20whenever%0Athey%20face%20a%20changing%20training%20data%20distribution.%20Rehearsal-based%20Continual%0ALearning%20%28CL%29%20approaches%20have%20been%20established%20as%20a%20versatile%20and%20reliable%0Asolution%20to%20overcome%20this%20limitation%3B%20however%2C%20sudden%20input%20disruptions%20and%0Amemory%20constraints%20are%20known%20to%20alter%20the%20consistency%20of%20their%20predictions.%20We%0Astudy%20this%20phenomenon%20by%20investigating%20the%20geometric%20characteristics%20of%20the%0Alearner%27s%20latent%20space%20and%20find%20that%20replayed%20data%20points%20of%20different%20classes%0Aincreasingly%20mix%20up%2C%20interfering%20with%20classification.%20Hence%2C%20we%20propose%20a%0Ageometric%20regularizer%20that%20enforces%20weak%20requirements%20on%20the%20Laplacian%20spectrum%0Aof%20the%20latent%20space%2C%20promoting%20a%20partitioning%20behavior.%20Our%20proposal%2C%20called%0AContinual%20Spectral%20Regularizer%20for%20Incremental%20Learning%20%28CaSpeR-IL%29%2C%20can%20be%0Aeasily%20combined%20with%20any%20rehearsal-based%20CL%20approach%20and%20improves%20the%0Aperformance%20of%20SOTA%20methods%20on%20standard%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.03345v4&entry.124074799=Read"},
{"title": "PerlDiff: Controllable Street View Synthesis Using Perspective-Layout\n  Diffusion Models", "author": "Jinhua Zhang and Hualian Sheng and Sijia Cai and Bing Deng and Qiao Liang and Wen Li and Ying Fu and Jieping Ye and Shuhang Gu", "abstract": "  Controllable generation is considered a potentially vital approach to address\nthe challenge of annotating 3D data, and the precision of such controllable\ngeneration becomes particularly imperative in the context of data production\nfor autonomous driving. Existing methods focus on the integration of diverse\ngenerative information into controlling inputs, utilizing frameworks such as\nGLIGEN or ControlNet, to produce commendable outcomes in controllable\ngeneration. However, such approaches intrinsically restrict generation\nperformance to the learning capacities of predefined network architectures. In\nthis paper, we explore the integration of controlling information and introduce\nPerlDiff (Perspective-Layout Diffusion Models), a method for effective street\nview image generation that fully leverages perspective 3D geometric\ninformation. Our PerlDiff employs 3D geometric priors to guide the generation\nof street view images with precise object-level control within the network\nlearning process, resulting in a more robust and controllable output. Moreover,\nit demonstrates superior controllability compared to alternative layout control\nmethods. Empirical results justify that our PerlDiff markedly enhances the\nprecision of generation on the NuScenes and KITTI datasets. Our codes and\nmodels are publicly available at https://github.com/LabShuHangGU/PerlDiff.\n", "link": "http://arxiv.org/abs/2407.06109v2", "date": "2024-07-16", "relevancy": 2.4671, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6259}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6259}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models&body=Title%3A%20PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models%0AAuthor%3A%20Jinhua%20Zhang%20and%20Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Wen%20Li%20and%20Ying%20Fu%20and%20Jieping%20Ye%20and%20Shuhang%20Gu%0AAbstract%3A%20%20%20Controllable%20generation%20is%20considered%20a%20potentially%20vital%20approach%20to%20address%0Athe%20challenge%20of%20annotating%203D%20data%2C%20and%20the%20precision%20of%20such%20controllable%0Ageneration%20becomes%20particularly%20imperative%20in%20the%20context%20of%20data%20production%0Afor%20autonomous%20driving.%20Existing%20methods%20focus%20on%20the%20integration%20of%20diverse%0Agenerative%20information%20into%20controlling%20inputs%2C%20utilizing%20frameworks%20such%20as%0AGLIGEN%20or%20ControlNet%2C%20to%20produce%20commendable%20outcomes%20in%20controllable%0Ageneration.%20However%2C%20such%20approaches%20intrinsically%20restrict%20generation%0Aperformance%20to%20the%20learning%20capacities%20of%20predefined%20network%20architectures.%20In%0Athis%20paper%2C%20we%20explore%20the%20integration%20of%20controlling%20information%20and%20introduce%0APerlDiff%20%28Perspective-Layout%20Diffusion%20Models%29%2C%20a%20method%20for%20effective%20street%0Aview%20image%20generation%20that%20fully%20leverages%20perspective%203D%20geometric%0Ainformation.%20Our%20PerlDiff%20employs%203D%20geometric%20priors%20to%20guide%20the%20generation%0Aof%20street%20view%20images%20with%20precise%20object-level%20control%20within%20the%20network%0Alearning%20process%2C%20resulting%20in%20a%20more%20robust%20and%20controllable%20output.%20Moreover%2C%0Ait%20demonstrates%20superior%20controllability%20compared%20to%20alternative%20layout%20control%0Amethods.%20Empirical%20results%20justify%20that%20our%20PerlDiff%20markedly%20enhances%20the%0Aprecision%20of%20generation%20on%20the%20NuScenes%20and%20KITTI%20datasets.%20Our%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/LabShuHangGU/PerlDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerlDiff%253A%2520Controllable%2520Street%2520View%2520Synthesis%2520Using%2520Perspective-Layout%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DJinhua%2520Zhang%2520and%2520Hualian%2520Sheng%2520and%2520Sijia%2520Cai%2520and%2520Bing%2520Deng%2520and%2520Qiao%2520Liang%2520and%2520Wen%2520Li%2520and%2520Ying%2520Fu%2520and%2520Jieping%2520Ye%2520and%2520Shuhang%2520Gu%26entry.1292438233%3D%2520%2520Controllable%2520generation%2520is%2520considered%2520a%2520potentially%2520vital%2520approach%2520to%2520address%250Athe%2520challenge%2520of%2520annotating%25203D%2520data%252C%2520and%2520the%2520precision%2520of%2520such%2520controllable%250Ageneration%2520becomes%2520particularly%2520imperative%2520in%2520the%2520context%2520of%2520data%2520production%250Afor%2520autonomous%2520driving.%2520Existing%2520methods%2520focus%2520on%2520the%2520integration%2520of%2520diverse%250Agenerative%2520information%2520into%2520controlling%2520inputs%252C%2520utilizing%2520frameworks%2520such%2520as%250AGLIGEN%2520or%2520ControlNet%252C%2520to%2520produce%2520commendable%2520outcomes%2520in%2520controllable%250Ageneration.%2520However%252C%2520such%2520approaches%2520intrinsically%2520restrict%2520generation%250Aperformance%2520to%2520the%2520learning%2520capacities%2520of%2520predefined%2520network%2520architectures.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520the%2520integration%2520of%2520controlling%2520information%2520and%2520introduce%250APerlDiff%2520%2528Perspective-Layout%2520Diffusion%2520Models%2529%252C%2520a%2520method%2520for%2520effective%2520street%250Aview%2520image%2520generation%2520that%2520fully%2520leverages%2520perspective%25203D%2520geometric%250Ainformation.%2520Our%2520PerlDiff%2520employs%25203D%2520geometric%2520priors%2520to%2520guide%2520the%2520generation%250Aof%2520street%2520view%2520images%2520with%2520precise%2520object-level%2520control%2520within%2520the%2520network%250Alearning%2520process%252C%2520resulting%2520in%2520a%2520more%2520robust%2520and%2520controllable%2520output.%2520Moreover%252C%250Ait%2520demonstrates%2520superior%2520controllability%2520compared%2520to%2520alternative%2520layout%2520control%250Amethods.%2520Empirical%2520results%2520justify%2520that%2520our%2520PerlDiff%2520markedly%2520enhances%2520the%250Aprecision%2520of%2520generation%2520on%2520the%2520NuScenes%2520and%2520KITTI%2520datasets.%2520Our%2520codes%2520and%250Amodels%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/LabShuHangGU/PerlDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models&entry.906535625=Jinhua%20Zhang%20and%20Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Wen%20Li%20and%20Ying%20Fu%20and%20Jieping%20Ye%20and%20Shuhang%20Gu&entry.1292438233=%20%20Controllable%20generation%20is%20considered%20a%20potentially%20vital%20approach%20to%20address%0Athe%20challenge%20of%20annotating%203D%20data%2C%20and%20the%20precision%20of%20such%20controllable%0Ageneration%20becomes%20particularly%20imperative%20in%20the%20context%20of%20data%20production%0Afor%20autonomous%20driving.%20Existing%20methods%20focus%20on%20the%20integration%20of%20diverse%0Agenerative%20information%20into%20controlling%20inputs%2C%20utilizing%20frameworks%20such%20as%0AGLIGEN%20or%20ControlNet%2C%20to%20produce%20commendable%20outcomes%20in%20controllable%0Ageneration.%20However%2C%20such%20approaches%20intrinsically%20restrict%20generation%0Aperformance%20to%20the%20learning%20capacities%20of%20predefined%20network%20architectures.%20In%0Athis%20paper%2C%20we%20explore%20the%20integration%20of%20controlling%20information%20and%20introduce%0APerlDiff%20%28Perspective-Layout%20Diffusion%20Models%29%2C%20a%20method%20for%20effective%20street%0Aview%20image%20generation%20that%20fully%20leverages%20perspective%203D%20geometric%0Ainformation.%20Our%20PerlDiff%20employs%203D%20geometric%20priors%20to%20guide%20the%20generation%0Aof%20street%20view%20images%20with%20precise%20object-level%20control%20within%20the%20network%0Alearning%20process%2C%20resulting%20in%20a%20more%20robust%20and%20controllable%20output.%20Moreover%2C%0Ait%20demonstrates%20superior%20controllability%20compared%20to%20alternative%20layout%20control%0Amethods.%20Empirical%20results%20justify%20that%20our%20PerlDiff%20markedly%20enhances%20the%0Aprecision%20of%20generation%20on%20the%20NuScenes%20and%20KITTI%20datasets.%20Our%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/LabShuHangGU/PerlDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06109v2&entry.124074799=Read"},
{"title": "Global atmospheric data assimilation with multi-modal masked\n  autoencoders", "author": "Thomas J. Vandal and Kate Duffy and Daniel McDuff and Yoni Nachmany and Chris Hartshorn", "abstract": "  Global data assimilation enables weather forecasting at all scales and\nprovides valuable data for studying the Earth system. However, the\ncomputational demands of physics-based algorithms used in operational systems\nlimits the volume and diversity of observations that are assimilated. Here, we\npresent \"EarthNet\", a multi-modal foundation model for data assimilation that\nlearns to predict a global gap-filled atmospheric state solely from satellite\nobservations. EarthNet is trained as a masked autoencoder that ingests a 12\nhour sequence of observations and learns to fill missing data from other\nsensors. We show that EarthNet performs a form of data assimilation producing a\nglobal 0.16 degree reanalysis dataset of 3D atmospheric temperature and\nhumidity at a fraction of the time compared to operational systems. It is shown\nthat the resulting reanalysis dataset reproduces climatology by evaluating a 1\nhour forecast background state against observations. We also show that our 3D\nhumidity predictions outperform MERRA-2 and ERA5 reanalyses by 10% to 60%\nbetween the middle troposphere and lower stratosphere (5 to 20 km altitude) and\nour 3D temperature and humidity are statistically equivalent to the Microwave\nintegrated Retrieval System (MiRS) observations at nearly every level of the\natmosphere. Our results indicate significant promise in using EarthNet for\nhigh-frequency data assimilation and global weather forecasting.\n", "link": "http://arxiv.org/abs/2407.11696v1", "date": "2024-07-16", "relevancy": 2.4667, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5269}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20atmospheric%20data%20assimilation%20with%20multi-modal%20masked%0A%20%20autoencoders&body=Title%3A%20Global%20atmospheric%20data%20assimilation%20with%20multi-modal%20masked%0A%20%20autoencoders%0AAuthor%3A%20Thomas%20J.%20Vandal%20and%20Kate%20Duffy%20and%20Daniel%20McDuff%20and%20Yoni%20Nachmany%20and%20Chris%20Hartshorn%0AAbstract%3A%20%20%20Global%20data%20assimilation%20enables%20weather%20forecasting%20at%20all%20scales%20and%0Aprovides%20valuable%20data%20for%20studying%20the%20Earth%20system.%20However%2C%20the%0Acomputational%20demands%20of%20physics-based%20algorithms%20used%20in%20operational%20systems%0Alimits%20the%20volume%20and%20diversity%20of%20observations%20that%20are%20assimilated.%20Here%2C%20we%0Apresent%20%22EarthNet%22%2C%20a%20multi-modal%20foundation%20model%20for%20data%20assimilation%20that%0Alearns%20to%20predict%20a%20global%20gap-filled%20atmospheric%20state%20solely%20from%20satellite%0Aobservations.%20EarthNet%20is%20trained%20as%20a%20masked%20autoencoder%20that%20ingests%20a%2012%0Ahour%20sequence%20of%20observations%20and%20learns%20to%20fill%20missing%20data%20from%20other%0Asensors.%20We%20show%20that%20EarthNet%20performs%20a%20form%20of%20data%20assimilation%20producing%20a%0Aglobal%200.16%20degree%20reanalysis%20dataset%20of%203D%20atmospheric%20temperature%20and%0Ahumidity%20at%20a%20fraction%20of%20the%20time%20compared%20to%20operational%20systems.%20It%20is%20shown%0Athat%20the%20resulting%20reanalysis%20dataset%20reproduces%20climatology%20by%20evaluating%20a%201%0Ahour%20forecast%20background%20state%20against%20observations.%20We%20also%20show%20that%20our%203D%0Ahumidity%20predictions%20outperform%20MERRA-2%20and%20ERA5%20reanalyses%20by%2010%25%20to%2060%25%0Abetween%20the%20middle%20troposphere%20and%20lower%20stratosphere%20%285%20to%2020%20km%20altitude%29%20and%0Aour%203D%20temperature%20and%20humidity%20are%20statistically%20equivalent%20to%20the%20Microwave%0Aintegrated%20Retrieval%20System%20%28MiRS%29%20observations%20at%20nearly%20every%20level%20of%20the%0Aatmosphere.%20Our%20results%20indicate%20significant%20promise%20in%20using%20EarthNet%20for%0Ahigh-frequency%20data%20assimilation%20and%20global%20weather%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520atmospheric%2520data%2520assimilation%2520with%2520multi-modal%2520masked%250A%2520%2520autoencoders%26entry.906535625%3DThomas%2520J.%2520Vandal%2520and%2520Kate%2520Duffy%2520and%2520Daniel%2520McDuff%2520and%2520Yoni%2520Nachmany%2520and%2520Chris%2520Hartshorn%26entry.1292438233%3D%2520%2520Global%2520data%2520assimilation%2520enables%2520weather%2520forecasting%2520at%2520all%2520scales%2520and%250Aprovides%2520valuable%2520data%2520for%2520studying%2520the%2520Earth%2520system.%2520However%252C%2520the%250Acomputational%2520demands%2520of%2520physics-based%2520algorithms%2520used%2520in%2520operational%2520systems%250Alimits%2520the%2520volume%2520and%2520diversity%2520of%2520observations%2520that%2520are%2520assimilated.%2520Here%252C%2520we%250Apresent%2520%2522EarthNet%2522%252C%2520a%2520multi-modal%2520foundation%2520model%2520for%2520data%2520assimilation%2520that%250Alearns%2520to%2520predict%2520a%2520global%2520gap-filled%2520atmospheric%2520state%2520solely%2520from%2520satellite%250Aobservations.%2520EarthNet%2520is%2520trained%2520as%2520a%2520masked%2520autoencoder%2520that%2520ingests%2520a%252012%250Ahour%2520sequence%2520of%2520observations%2520and%2520learns%2520to%2520fill%2520missing%2520data%2520from%2520other%250Asensors.%2520We%2520show%2520that%2520EarthNet%2520performs%2520a%2520form%2520of%2520data%2520assimilation%2520producing%2520a%250Aglobal%25200.16%2520degree%2520reanalysis%2520dataset%2520of%25203D%2520atmospheric%2520temperature%2520and%250Ahumidity%2520at%2520a%2520fraction%2520of%2520the%2520time%2520compared%2520to%2520operational%2520systems.%2520It%2520is%2520shown%250Athat%2520the%2520resulting%2520reanalysis%2520dataset%2520reproduces%2520climatology%2520by%2520evaluating%2520a%25201%250Ahour%2520forecast%2520background%2520state%2520against%2520observations.%2520We%2520also%2520show%2520that%2520our%25203D%250Ahumidity%2520predictions%2520outperform%2520MERRA-2%2520and%2520ERA5%2520reanalyses%2520by%252010%2525%2520to%252060%2525%250Abetween%2520the%2520middle%2520troposphere%2520and%2520lower%2520stratosphere%2520%25285%2520to%252020%2520km%2520altitude%2529%2520and%250Aour%25203D%2520temperature%2520and%2520humidity%2520are%2520statistically%2520equivalent%2520to%2520the%2520Microwave%250Aintegrated%2520Retrieval%2520System%2520%2528MiRS%2529%2520observations%2520at%2520nearly%2520every%2520level%2520of%2520the%250Aatmosphere.%2520Our%2520results%2520indicate%2520significant%2520promise%2520in%2520using%2520EarthNet%2520for%250Ahigh-frequency%2520data%2520assimilation%2520and%2520global%2520weather%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20atmospheric%20data%20assimilation%20with%20multi-modal%20masked%0A%20%20autoencoders&entry.906535625=Thomas%20J.%20Vandal%20and%20Kate%20Duffy%20and%20Daniel%20McDuff%20and%20Yoni%20Nachmany%20and%20Chris%20Hartshorn&entry.1292438233=%20%20Global%20data%20assimilation%20enables%20weather%20forecasting%20at%20all%20scales%20and%0Aprovides%20valuable%20data%20for%20studying%20the%20Earth%20system.%20However%2C%20the%0Acomputational%20demands%20of%20physics-based%20algorithms%20used%20in%20operational%20systems%0Alimits%20the%20volume%20and%20diversity%20of%20observations%20that%20are%20assimilated.%20Here%2C%20we%0Apresent%20%22EarthNet%22%2C%20a%20multi-modal%20foundation%20model%20for%20data%20assimilation%20that%0Alearns%20to%20predict%20a%20global%20gap-filled%20atmospheric%20state%20solely%20from%20satellite%0Aobservations.%20EarthNet%20is%20trained%20as%20a%20masked%20autoencoder%20that%20ingests%20a%2012%0Ahour%20sequence%20of%20observations%20and%20learns%20to%20fill%20missing%20data%20from%20other%0Asensors.%20We%20show%20that%20EarthNet%20performs%20a%20form%20of%20data%20assimilation%20producing%20a%0Aglobal%200.16%20degree%20reanalysis%20dataset%20of%203D%20atmospheric%20temperature%20and%0Ahumidity%20at%20a%20fraction%20of%20the%20time%20compared%20to%20operational%20systems.%20It%20is%20shown%0Athat%20the%20resulting%20reanalysis%20dataset%20reproduces%20climatology%20by%20evaluating%20a%201%0Ahour%20forecast%20background%20state%20against%20observations.%20We%20also%20show%20that%20our%203D%0Ahumidity%20predictions%20outperform%20MERRA-2%20and%20ERA5%20reanalyses%20by%2010%25%20to%2060%25%0Abetween%20the%20middle%20troposphere%20and%20lower%20stratosphere%20%285%20to%2020%20km%20altitude%29%20and%0Aour%203D%20temperature%20and%20humidity%20are%20statistically%20equivalent%20to%20the%20Microwave%0Aintegrated%20Retrieval%20System%20%28MiRS%29%20observations%20at%20nearly%20every%20level%20of%20the%0Aatmosphere.%20Our%20results%20indicate%20significant%20promise%20in%20using%20EarthNet%20for%0Ahigh-frequency%20data%20assimilation%20and%20global%20weather%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11696v1&entry.124074799=Read"},
{"title": "Global Optimisation of Black-Box Functions with Generative Models in the\n  Wasserstein Space", "author": "Tigran Ramazyan and Mikhail Hushchyn and Denis Derkach", "abstract": "  We propose a new uncertainty estimator for gradient-free optimisation of\nblack-box simulators using deep generative surrogate models. Optimisation of\nthese simulators is especially challenging for stochastic simulators and higher\ndimensions. To address these issues, we utilise a deep generative surrogate\napproach to model the black box response for the entire parameter space. We\nthen leverage this knowledge to estimate the proposed uncertainty based on the\nWasserstein distance - the Wasserstein uncertainty. This approach is employed\nin a posterior agnostic gradient-free optimisation algorithm that minimises\nregret over the entire parameter space. A series of tests were conducted to\ndemonstrate that our method is more robust to the shape of both the black box\nfunction and the stochastic response of the black box than state-of-the-art\nmethods, such as efficient global optimisation with a deep Gaussian process\nsurrogate.\n", "link": "http://arxiv.org/abs/2407.11917v1", "date": "2024-07-16", "relevancy": 2.4655, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5058}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4904}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space&body=Title%3A%20Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space%0AAuthor%3A%20Tigran%20Ramazyan%20and%20Mikhail%20Hushchyn%20and%20Denis%20Derkach%0AAbstract%3A%20%20%20We%20propose%20a%20new%20uncertainty%20estimator%20for%20gradient-free%20optimisation%20of%0Ablack-box%20simulators%20using%20deep%20generative%20surrogate%20models.%20Optimisation%20of%0Athese%20simulators%20is%20especially%20challenging%20for%20stochastic%20simulators%20and%20higher%0Adimensions.%20To%20address%20these%20issues%2C%20we%20utilise%20a%20deep%20generative%20surrogate%0Aapproach%20to%20model%20the%20black%20box%20response%20for%20the%20entire%20parameter%20space.%20We%0Athen%20leverage%20this%20knowledge%20to%20estimate%20the%20proposed%20uncertainty%20based%20on%20the%0AWasserstein%20distance%20-%20the%20Wasserstein%20uncertainty.%20This%20approach%20is%20employed%0Ain%20a%20posterior%20agnostic%20gradient-free%20optimisation%20algorithm%20that%20minimises%0Aregret%20over%20the%20entire%20parameter%20space.%20A%20series%20of%20tests%20were%20conducted%20to%0Ademonstrate%20that%20our%20method%20is%20more%20robust%20to%20the%20shape%20of%20both%20the%20black%20box%0Afunction%20and%20the%20stochastic%20response%20of%20the%20black%20box%20than%20state-of-the-art%0Amethods%2C%20such%20as%20efficient%20global%20optimisation%20with%20a%20deep%20Gaussian%20process%0Asurrogate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Optimisation%2520of%2520Black-Box%2520Functions%2520with%2520Generative%2520Models%2520in%2520the%250A%2520%2520Wasserstein%2520Space%26entry.906535625%3DTigran%2520Ramazyan%2520and%2520Mikhail%2520Hushchyn%2520and%2520Denis%2520Derkach%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520uncertainty%2520estimator%2520for%2520gradient-free%2520optimisation%2520of%250Ablack-box%2520simulators%2520using%2520deep%2520generative%2520surrogate%2520models.%2520Optimisation%2520of%250Athese%2520simulators%2520is%2520especially%2520challenging%2520for%2520stochastic%2520simulators%2520and%2520higher%250Adimensions.%2520To%2520address%2520these%2520issues%252C%2520we%2520utilise%2520a%2520deep%2520generative%2520surrogate%250Aapproach%2520to%2520model%2520the%2520black%2520box%2520response%2520for%2520the%2520entire%2520parameter%2520space.%2520We%250Athen%2520leverage%2520this%2520knowledge%2520to%2520estimate%2520the%2520proposed%2520uncertainty%2520based%2520on%2520the%250AWasserstein%2520distance%2520-%2520the%2520Wasserstein%2520uncertainty.%2520This%2520approach%2520is%2520employed%250Ain%2520a%2520posterior%2520agnostic%2520gradient-free%2520optimisation%2520algorithm%2520that%2520minimises%250Aregret%2520over%2520the%2520entire%2520parameter%2520space.%2520A%2520series%2520of%2520tests%2520were%2520conducted%2520to%250Ademonstrate%2520that%2520our%2520method%2520is%2520more%2520robust%2520to%2520the%2520shape%2520of%2520both%2520the%2520black%2520box%250Afunction%2520and%2520the%2520stochastic%2520response%2520of%2520the%2520black%2520box%2520than%2520state-of-the-art%250Amethods%252C%2520such%2520as%2520efficient%2520global%2520optimisation%2520with%2520a%2520deep%2520Gaussian%2520process%250Asurrogate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space&entry.906535625=Tigran%20Ramazyan%20and%20Mikhail%20Hushchyn%20and%20Denis%20Derkach&entry.1292438233=%20%20We%20propose%20a%20new%20uncertainty%20estimator%20for%20gradient-free%20optimisation%20of%0Ablack-box%20simulators%20using%20deep%20generative%20surrogate%20models.%20Optimisation%20of%0Athese%20simulators%20is%20especially%20challenging%20for%20stochastic%20simulators%20and%20higher%0Adimensions.%20To%20address%20these%20issues%2C%20we%20utilise%20a%20deep%20generative%20surrogate%0Aapproach%20to%20model%20the%20black%20box%20response%20for%20the%20entire%20parameter%20space.%20We%0Athen%20leverage%20this%20knowledge%20to%20estimate%20the%20proposed%20uncertainty%20based%20on%20the%0AWasserstein%20distance%20-%20the%20Wasserstein%20uncertainty.%20This%20approach%20is%20employed%0Ain%20a%20posterior%20agnostic%20gradient-free%20optimisation%20algorithm%20that%20minimises%0Aregret%20over%20the%20entire%20parameter%20space.%20A%20series%20of%20tests%20were%20conducted%20to%0Ademonstrate%20that%20our%20method%20is%20more%20robust%20to%20the%20shape%20of%20both%20the%20black%20box%0Afunction%20and%20the%20stochastic%20response%20of%20the%20black%20box%20than%20state-of-the-art%0Amethods%2C%20such%20as%20efficient%20global%20optimisation%20with%20a%20deep%20Gaussian%20process%0Asurrogate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11917v1&entry.124074799=Read"},
{"title": "QVD: Post-training Quantization for Video Diffusion Models", "author": "Shilong Tian and Hong Chen and Chengtao Lv and Yu Liu and Jinyang Guo and Xianglong Liu and Shengxi Li and Hao Yang and Tao Xie", "abstract": "  Recently, video diffusion models (VDMs) have garnered significant attention\ndue to their notable advancements in generating coherent and realistic video\ncontent. However, processing multiple frame features concurrently, coupled with\nthe considerable model size, results in high latency and extensive memory\nconsumption, hindering their broader application. Post-training quantization\n(PTQ) is an effective technique to reduce memory footprint and improve\ncomputational efficiency. Unlike image diffusion, we observe that the temporal\nfeatures, which are integrated into all frame features, exhibit pronounced\nskewness. Furthermore, we investigate significant inter-channel disparities and\nasymmetries in the activation of video diffusion models, resulting in low\ncoverage of quantization levels by individual channels and increasing the\nchallenge of quantization. To address these issues, we introduce the first PTQ\nstrategy tailored for video diffusion models, dubbed QVD. Specifically, we\npropose the High Temporal Discriminability Quantization (HTDQ) method, designed\nfor temporal features, which retains the high discriminability of quantized\nfeatures, providing precise temporal guidance for all video frames. In\naddition, we present the Scattered Channel Range Integration (SCRI) method\nwhich aims to improve the coverage of quantization levels across individual\nchannels. Experimental validations across various models, datasets, and\nbit-width settings demonstrate the effectiveness of our QVD in terms of diverse\nmetrics. In particular, we achieve near-lossless performance degradation on\nW8A8, outperforming the current methods by 205.12 in FVD.\n", "link": "http://arxiv.org/abs/2407.11585v1", "date": "2024-07-16", "relevancy": 2.4538, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6386}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6132}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QVD%3A%20Post-training%20Quantization%20for%20Video%20Diffusion%20Models&body=Title%3A%20QVD%3A%20Post-training%20Quantization%20for%20Video%20Diffusion%20Models%0AAuthor%3A%20Shilong%20Tian%20and%20Hong%20Chen%20and%20Chengtao%20Lv%20and%20Yu%20Liu%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Shengxi%20Li%20and%20Hao%20Yang%20and%20Tao%20Xie%0AAbstract%3A%20%20%20Recently%2C%20video%20diffusion%20models%20%28VDMs%29%20have%20garnered%20significant%20attention%0Adue%20to%20their%20notable%20advancements%20in%20generating%20coherent%20and%20realistic%20video%0Acontent.%20However%2C%20processing%20multiple%20frame%20features%20concurrently%2C%20coupled%20with%0Athe%20considerable%20model%20size%2C%20results%20in%20high%20latency%20and%20extensive%20memory%0Aconsumption%2C%20hindering%20their%20broader%20application.%20Post-training%20quantization%0A%28PTQ%29%20is%20an%20effective%20technique%20to%20reduce%20memory%20footprint%20and%20improve%0Acomputational%20efficiency.%20Unlike%20image%20diffusion%2C%20we%20observe%20that%20the%20temporal%0Afeatures%2C%20which%20are%20integrated%20into%20all%20frame%20features%2C%20exhibit%20pronounced%0Askewness.%20Furthermore%2C%20we%20investigate%20significant%20inter-channel%20disparities%20and%0Aasymmetries%20in%20the%20activation%20of%20video%20diffusion%20models%2C%20resulting%20in%20low%0Acoverage%20of%20quantization%20levels%20by%20individual%20channels%20and%20increasing%20the%0Achallenge%20of%20quantization.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20PTQ%0Astrategy%20tailored%20for%20video%20diffusion%20models%2C%20dubbed%20QVD.%20Specifically%2C%20we%0Apropose%20the%20High%20Temporal%20Discriminability%20Quantization%20%28HTDQ%29%20method%2C%20designed%0Afor%20temporal%20features%2C%20which%20retains%20the%20high%20discriminability%20of%20quantized%0Afeatures%2C%20providing%20precise%20temporal%20guidance%20for%20all%20video%20frames.%20In%0Aaddition%2C%20we%20present%20the%20Scattered%20Channel%20Range%20Integration%20%28SCRI%29%20method%0Awhich%20aims%20to%20improve%20the%20coverage%20of%20quantization%20levels%20across%20individual%0Achannels.%20Experimental%20validations%20across%20various%20models%2C%20datasets%2C%20and%0Abit-width%20settings%20demonstrate%20the%20effectiveness%20of%20our%20QVD%20in%20terms%20of%20diverse%0Ametrics.%20In%20particular%2C%20we%20achieve%20near-lossless%20performance%20degradation%20on%0AW8A8%2C%20outperforming%20the%20current%20methods%20by%20205.12%20in%20FVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQVD%253A%2520Post-training%2520Quantization%2520for%2520Video%2520Diffusion%2520Models%26entry.906535625%3DShilong%2520Tian%2520and%2520Hong%2520Chen%2520and%2520Chengtao%2520Lv%2520and%2520Yu%2520Liu%2520and%2520Jinyang%2520Guo%2520and%2520Xianglong%2520Liu%2520and%2520Shengxi%2520Li%2520and%2520Hao%2520Yang%2520and%2520Tao%2520Xie%26entry.1292438233%3D%2520%2520Recently%252C%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520have%2520garnered%2520significant%2520attention%250Adue%2520to%2520their%2520notable%2520advancements%2520in%2520generating%2520coherent%2520and%2520realistic%2520video%250Acontent.%2520However%252C%2520processing%2520multiple%2520frame%2520features%2520concurrently%252C%2520coupled%2520with%250Athe%2520considerable%2520model%2520size%252C%2520results%2520in%2520high%2520latency%2520and%2520extensive%2520memory%250Aconsumption%252C%2520hindering%2520their%2520broader%2520application.%2520Post-training%2520quantization%250A%2528PTQ%2529%2520is%2520an%2520effective%2520technique%2520to%2520reduce%2520memory%2520footprint%2520and%2520improve%250Acomputational%2520efficiency.%2520Unlike%2520image%2520diffusion%252C%2520we%2520observe%2520that%2520the%2520temporal%250Afeatures%252C%2520which%2520are%2520integrated%2520into%2520all%2520frame%2520features%252C%2520exhibit%2520pronounced%250Askewness.%2520Furthermore%252C%2520we%2520investigate%2520significant%2520inter-channel%2520disparities%2520and%250Aasymmetries%2520in%2520the%2520activation%2520of%2520video%2520diffusion%2520models%252C%2520resulting%2520in%2520low%250Acoverage%2520of%2520quantization%2520levels%2520by%2520individual%2520channels%2520and%2520increasing%2520the%250Achallenge%2520of%2520quantization.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520first%2520PTQ%250Astrategy%2520tailored%2520for%2520video%2520diffusion%2520models%252C%2520dubbed%2520QVD.%2520Specifically%252C%2520we%250Apropose%2520the%2520High%2520Temporal%2520Discriminability%2520Quantization%2520%2528HTDQ%2529%2520method%252C%2520designed%250Afor%2520temporal%2520features%252C%2520which%2520retains%2520the%2520high%2520discriminability%2520of%2520quantized%250Afeatures%252C%2520providing%2520precise%2520temporal%2520guidance%2520for%2520all%2520video%2520frames.%2520In%250Aaddition%252C%2520we%2520present%2520the%2520Scattered%2520Channel%2520Range%2520Integration%2520%2528SCRI%2529%2520method%250Awhich%2520aims%2520to%2520improve%2520the%2520coverage%2520of%2520quantization%2520levels%2520across%2520individual%250Achannels.%2520Experimental%2520validations%2520across%2520various%2520models%252C%2520datasets%252C%2520and%250Abit-width%2520settings%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520QVD%2520in%2520terms%2520of%2520diverse%250Ametrics.%2520In%2520particular%252C%2520we%2520achieve%2520near-lossless%2520performance%2520degradation%2520on%250AW8A8%252C%2520outperforming%2520the%2520current%2520methods%2520by%2520205.12%2520in%2520FVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QVD%3A%20Post-training%20Quantization%20for%20Video%20Diffusion%20Models&entry.906535625=Shilong%20Tian%20and%20Hong%20Chen%20and%20Chengtao%20Lv%20and%20Yu%20Liu%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Shengxi%20Li%20and%20Hao%20Yang%20and%20Tao%20Xie&entry.1292438233=%20%20Recently%2C%20video%20diffusion%20models%20%28VDMs%29%20have%20garnered%20significant%20attention%0Adue%20to%20their%20notable%20advancements%20in%20generating%20coherent%20and%20realistic%20video%0Acontent.%20However%2C%20processing%20multiple%20frame%20features%20concurrently%2C%20coupled%20with%0Athe%20considerable%20model%20size%2C%20results%20in%20high%20latency%20and%20extensive%20memory%0Aconsumption%2C%20hindering%20their%20broader%20application.%20Post-training%20quantization%0A%28PTQ%29%20is%20an%20effective%20technique%20to%20reduce%20memory%20footprint%20and%20improve%0Acomputational%20efficiency.%20Unlike%20image%20diffusion%2C%20we%20observe%20that%20the%20temporal%0Afeatures%2C%20which%20are%20integrated%20into%20all%20frame%20features%2C%20exhibit%20pronounced%0Askewness.%20Furthermore%2C%20we%20investigate%20significant%20inter-channel%20disparities%20and%0Aasymmetries%20in%20the%20activation%20of%20video%20diffusion%20models%2C%20resulting%20in%20low%0Acoverage%20of%20quantization%20levels%20by%20individual%20channels%20and%20increasing%20the%0Achallenge%20of%20quantization.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20PTQ%0Astrategy%20tailored%20for%20video%20diffusion%20models%2C%20dubbed%20QVD.%20Specifically%2C%20we%0Apropose%20the%20High%20Temporal%20Discriminability%20Quantization%20%28HTDQ%29%20method%2C%20designed%0Afor%20temporal%20features%2C%20which%20retains%20the%20high%20discriminability%20of%20quantized%0Afeatures%2C%20providing%20precise%20temporal%20guidance%20for%20all%20video%20frames.%20In%0Aaddition%2C%20we%20present%20the%20Scattered%20Channel%20Range%20Integration%20%28SCRI%29%20method%0Awhich%20aims%20to%20improve%20the%20coverage%20of%20quantization%20levels%20across%20individual%0Achannels.%20Experimental%20validations%20across%20various%20models%2C%20datasets%2C%20and%0Abit-width%20settings%20demonstrate%20the%20effectiveness%20of%20our%20QVD%20in%20terms%20of%20diverse%0Ametrics.%20In%20particular%2C%20we%20achieve%20near-lossless%20performance%20degradation%20on%0AW8A8%2C%20outperforming%20the%20current%20methods%20by%20205.12%20in%20FVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11585v1&entry.124074799=Read"},
{"title": "Fusion LiDAR-Inertial-Encoder data for High-Accuracy SLAM", "author": "Manh Do Duc and Thanh Nguyen Canh and Minh DoNgoc and Xiem HoangVan", "abstract": "  In the realm of robotics, achieving simultaneous localization and mapping\n(SLAM) is paramount for autonomous navigation, especially in challenging\nenvironments like texture-less structures. This paper proposed a\nfactor-graph-based model that tightly integrates IMU and encoder sensors to\nenhance positioning in such environments. The system operates by meticulously\nevaluating the data from each sensor. Based on these evaluations, weights are\ndynamically adjusted to prioritize the more reliable source of information at\nany given moment. The robot's state is initialized using IMU data, while the\nencoder aids motion estimation in long corridors. Discrepancies between the two\nstates are used to correct IMU drift. The effectiveness of this method is\ndemonstrably validated through experimentation. Compared to Karto SLAM, a\nwidely used SLAM algorithm, this approach achieves an improvement of 26.98% in\nrotation angle error and 67.68% reduction in position error. These results\nconvincingly demonstrate the method's superior accuracy and robustness in\ntexture-less environments.\n", "link": "http://arxiv.org/abs/2407.11870v1", "date": "2024-07-16", "relevancy": 2.4331, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6225}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20LiDAR-Inertial-Encoder%20data%20for%20High-Accuracy%20SLAM&body=Title%3A%20Fusion%20LiDAR-Inertial-Encoder%20data%20for%20High-Accuracy%20SLAM%0AAuthor%3A%20Manh%20Do%20Duc%20and%20Thanh%20Nguyen%20Canh%20and%20Minh%20DoNgoc%20and%20Xiem%20HoangVan%0AAbstract%3A%20%20%20In%20the%20realm%20of%20robotics%2C%20achieving%20simultaneous%20localization%20and%20mapping%0A%28SLAM%29%20is%20paramount%20for%20autonomous%20navigation%2C%20especially%20in%20challenging%0Aenvironments%20like%20texture-less%20structures.%20This%20paper%20proposed%20a%0Afactor-graph-based%20model%20that%20tightly%20integrates%20IMU%20and%20encoder%20sensors%20to%0Aenhance%20positioning%20in%20such%20environments.%20The%20system%20operates%20by%20meticulously%0Aevaluating%20the%20data%20from%20each%20sensor.%20Based%20on%20these%20evaluations%2C%20weights%20are%0Adynamically%20adjusted%20to%20prioritize%20the%20more%20reliable%20source%20of%20information%20at%0Aany%20given%20moment.%20The%20robot%27s%20state%20is%20initialized%20using%20IMU%20data%2C%20while%20the%0Aencoder%20aids%20motion%20estimation%20in%20long%20corridors.%20Discrepancies%20between%20the%20two%0Astates%20are%20used%20to%20correct%20IMU%20drift.%20The%20effectiveness%20of%20this%20method%20is%0Ademonstrably%20validated%20through%20experimentation.%20Compared%20to%20Karto%20SLAM%2C%20a%0Awidely%20used%20SLAM%20algorithm%2C%20this%20approach%20achieves%20an%20improvement%20of%2026.98%25%20in%0Arotation%20angle%20error%20and%2067.68%25%20reduction%20in%20position%20error.%20These%20results%0Aconvincingly%20demonstrate%20the%20method%27s%20superior%20accuracy%20and%20robustness%20in%0Atexture-less%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520LiDAR-Inertial-Encoder%2520data%2520for%2520High-Accuracy%2520SLAM%26entry.906535625%3DManh%2520Do%2520Duc%2520and%2520Thanh%2520Nguyen%2520Canh%2520and%2520Minh%2520DoNgoc%2520and%2520Xiem%2520HoangVan%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520robotics%252C%2520achieving%2520simultaneous%2520localization%2520and%2520mapping%250A%2528SLAM%2529%2520is%2520paramount%2520for%2520autonomous%2520navigation%252C%2520especially%2520in%2520challenging%250Aenvironments%2520like%2520texture-less%2520structures.%2520This%2520paper%2520proposed%2520a%250Afactor-graph-based%2520model%2520that%2520tightly%2520integrates%2520IMU%2520and%2520encoder%2520sensors%2520to%250Aenhance%2520positioning%2520in%2520such%2520environments.%2520The%2520system%2520operates%2520by%2520meticulously%250Aevaluating%2520the%2520data%2520from%2520each%2520sensor.%2520Based%2520on%2520these%2520evaluations%252C%2520weights%2520are%250Adynamically%2520adjusted%2520to%2520prioritize%2520the%2520more%2520reliable%2520source%2520of%2520information%2520at%250Aany%2520given%2520moment.%2520The%2520robot%2527s%2520state%2520is%2520initialized%2520using%2520IMU%2520data%252C%2520while%2520the%250Aencoder%2520aids%2520motion%2520estimation%2520in%2520long%2520corridors.%2520Discrepancies%2520between%2520the%2520two%250Astates%2520are%2520used%2520to%2520correct%2520IMU%2520drift.%2520The%2520effectiveness%2520of%2520this%2520method%2520is%250Ademonstrably%2520validated%2520through%2520experimentation.%2520Compared%2520to%2520Karto%2520SLAM%252C%2520a%250Awidely%2520used%2520SLAM%2520algorithm%252C%2520this%2520approach%2520achieves%2520an%2520improvement%2520of%252026.98%2525%2520in%250Arotation%2520angle%2520error%2520and%252067.68%2525%2520reduction%2520in%2520position%2520error.%2520These%2520results%250Aconvincingly%2520demonstrate%2520the%2520method%2527s%2520superior%2520accuracy%2520and%2520robustness%2520in%250Atexture-less%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20LiDAR-Inertial-Encoder%20data%20for%20High-Accuracy%20SLAM&entry.906535625=Manh%20Do%20Duc%20and%20Thanh%20Nguyen%20Canh%20and%20Minh%20DoNgoc%20and%20Xiem%20HoangVan&entry.1292438233=%20%20In%20the%20realm%20of%20robotics%2C%20achieving%20simultaneous%20localization%20and%20mapping%0A%28SLAM%29%20is%20paramount%20for%20autonomous%20navigation%2C%20especially%20in%20challenging%0Aenvironments%20like%20texture-less%20structures.%20This%20paper%20proposed%20a%0Afactor-graph-based%20model%20that%20tightly%20integrates%20IMU%20and%20encoder%20sensors%20to%0Aenhance%20positioning%20in%20such%20environments.%20The%20system%20operates%20by%20meticulously%0Aevaluating%20the%20data%20from%20each%20sensor.%20Based%20on%20these%20evaluations%2C%20weights%20are%0Adynamically%20adjusted%20to%20prioritize%20the%20more%20reliable%20source%20of%20information%20at%0Aany%20given%20moment.%20The%20robot%27s%20state%20is%20initialized%20using%20IMU%20data%2C%20while%20the%0Aencoder%20aids%20motion%20estimation%20in%20long%20corridors.%20Discrepancies%20between%20the%20two%0Astates%20are%20used%20to%20correct%20IMU%20drift.%20The%20effectiveness%20of%20this%20method%20is%0Ademonstrably%20validated%20through%20experimentation.%20Compared%20to%20Karto%20SLAM%2C%20a%0Awidely%20used%20SLAM%20algorithm%2C%20this%20approach%20achieves%20an%20improvement%20of%2026.98%25%20in%0Arotation%20angle%20error%20and%2067.68%25%20reduction%20in%20position%20error.%20These%20results%0Aconvincingly%20demonstrate%20the%20method%27s%20superior%20accuracy%20and%20robustness%20in%0Atexture-less%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11870v1&entry.124074799=Read"},
{"title": "HyperAggregation: Aggregating over Graph Edges with Hypernetworks", "author": "Nicolas Lell and Ansgar Scherp", "abstract": "  HyperAggregation is a hypernetwork-based aggregation function for Graph\nNeural Networks. It uses a hypernetwork to dynamically generate weights in the\nsize of the current neighborhood, which are then used to aggregate this\nneighborhood. This aggregation with the generated weights is done like an\nMLP-Mixer channel mixing over variable-sized vertex neighborhoods. We\ndemonstrate HyperAggregation in two models, GraphHyperMixer is a model based on\nMLP-Mixer while GraphHyperConv is derived from a GCN but with a\nhypernetwork-based aggregation function. We perform experiments on diverse\nbenchmark datasets for the vertex classification, graph classification, and\ngraph regression tasks. The results show that HyperAggregation can be\neffectively used for homophilic and heterophilic datasets in both inductive and\ntransductive settings. GraphHyperConv performs better than GraphHyperMixer and\nis especially strong in the transductive setting. On the heterophilic dataset\nRoman-Empire it reaches a new state of the art. On the graph-level tasks our\nmodels perform in line with similarly sized models. Ablation studies\ninvestigate the robustness against various hyperparameter choices. The\nimplementation of HyperAggregation as well code to reproduce all experiments is\navailable under https://github.com/Foisunt/HyperAggregation .\n", "link": "http://arxiv.org/abs/2407.11596v1", "date": "2024-07-16", "relevancy": 2.4196, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5323}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4641}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperAggregation%3A%20Aggregating%20over%20Graph%20Edges%20with%20Hypernetworks&body=Title%3A%20HyperAggregation%3A%20Aggregating%20over%20Graph%20Edges%20with%20Hypernetworks%0AAuthor%3A%20Nicolas%20Lell%20and%20Ansgar%20Scherp%0AAbstract%3A%20%20%20HyperAggregation%20is%20a%20hypernetwork-based%20aggregation%20function%20for%20Graph%0ANeural%20Networks.%20It%20uses%20a%20hypernetwork%20to%20dynamically%20generate%20weights%20in%20the%0Asize%20of%20the%20current%20neighborhood%2C%20which%20are%20then%20used%20to%20aggregate%20this%0Aneighborhood.%20This%20aggregation%20with%20the%20generated%20weights%20is%20done%20like%20an%0AMLP-Mixer%20channel%20mixing%20over%20variable-sized%20vertex%20neighborhoods.%20We%0Ademonstrate%20HyperAggregation%20in%20two%20models%2C%20GraphHyperMixer%20is%20a%20model%20based%20on%0AMLP-Mixer%20while%20GraphHyperConv%20is%20derived%20from%20a%20GCN%20but%20with%20a%0Ahypernetwork-based%20aggregation%20function.%20We%20perform%20experiments%20on%20diverse%0Abenchmark%20datasets%20for%20the%20vertex%20classification%2C%20graph%20classification%2C%20and%0Agraph%20regression%20tasks.%20The%20results%20show%20that%20HyperAggregation%20can%20be%0Aeffectively%20used%20for%20homophilic%20and%20heterophilic%20datasets%20in%20both%20inductive%20and%0Atransductive%20settings.%20GraphHyperConv%20performs%20better%20than%20GraphHyperMixer%20and%0Ais%20especially%20strong%20in%20the%20transductive%20setting.%20On%20the%20heterophilic%20dataset%0ARoman-Empire%20it%20reaches%20a%20new%20state%20of%20the%20art.%20On%20the%20graph-level%20tasks%20our%0Amodels%20perform%20in%20line%20with%20similarly%20sized%20models.%20Ablation%20studies%0Ainvestigate%20the%20robustness%20against%20various%20hyperparameter%20choices.%20The%0Aimplementation%20of%20HyperAggregation%20as%20well%20code%20to%20reproduce%20all%20experiments%20is%0Aavailable%20under%20https%3A//github.com/Foisunt/HyperAggregation%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperAggregation%253A%2520Aggregating%2520over%2520Graph%2520Edges%2520with%2520Hypernetworks%26entry.906535625%3DNicolas%2520Lell%2520and%2520Ansgar%2520Scherp%26entry.1292438233%3D%2520%2520HyperAggregation%2520is%2520a%2520hypernetwork-based%2520aggregation%2520function%2520for%2520Graph%250ANeural%2520Networks.%2520It%2520uses%2520a%2520hypernetwork%2520to%2520dynamically%2520generate%2520weights%2520in%2520the%250Asize%2520of%2520the%2520current%2520neighborhood%252C%2520which%2520are%2520then%2520used%2520to%2520aggregate%2520this%250Aneighborhood.%2520This%2520aggregation%2520with%2520the%2520generated%2520weights%2520is%2520done%2520like%2520an%250AMLP-Mixer%2520channel%2520mixing%2520over%2520variable-sized%2520vertex%2520neighborhoods.%2520We%250Ademonstrate%2520HyperAggregation%2520in%2520two%2520models%252C%2520GraphHyperMixer%2520is%2520a%2520model%2520based%2520on%250AMLP-Mixer%2520while%2520GraphHyperConv%2520is%2520derived%2520from%2520a%2520GCN%2520but%2520with%2520a%250Ahypernetwork-based%2520aggregation%2520function.%2520We%2520perform%2520experiments%2520on%2520diverse%250Abenchmark%2520datasets%2520for%2520the%2520vertex%2520classification%252C%2520graph%2520classification%252C%2520and%250Agraph%2520regression%2520tasks.%2520The%2520results%2520show%2520that%2520HyperAggregation%2520can%2520be%250Aeffectively%2520used%2520for%2520homophilic%2520and%2520heterophilic%2520datasets%2520in%2520both%2520inductive%2520and%250Atransductive%2520settings.%2520GraphHyperConv%2520performs%2520better%2520than%2520GraphHyperMixer%2520and%250Ais%2520especially%2520strong%2520in%2520the%2520transductive%2520setting.%2520On%2520the%2520heterophilic%2520dataset%250ARoman-Empire%2520it%2520reaches%2520a%2520new%2520state%2520of%2520the%2520art.%2520On%2520the%2520graph-level%2520tasks%2520our%250Amodels%2520perform%2520in%2520line%2520with%2520similarly%2520sized%2520models.%2520Ablation%2520studies%250Ainvestigate%2520the%2520robustness%2520against%2520various%2520hyperparameter%2520choices.%2520The%250Aimplementation%2520of%2520HyperAggregation%2520as%2520well%2520code%2520to%2520reproduce%2520all%2520experiments%2520is%250Aavailable%2520under%2520https%253A//github.com/Foisunt/HyperAggregation%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperAggregation%3A%20Aggregating%20over%20Graph%20Edges%20with%20Hypernetworks&entry.906535625=Nicolas%20Lell%20and%20Ansgar%20Scherp&entry.1292438233=%20%20HyperAggregation%20is%20a%20hypernetwork-based%20aggregation%20function%20for%20Graph%0ANeural%20Networks.%20It%20uses%20a%20hypernetwork%20to%20dynamically%20generate%20weights%20in%20the%0Asize%20of%20the%20current%20neighborhood%2C%20which%20are%20then%20used%20to%20aggregate%20this%0Aneighborhood.%20This%20aggregation%20with%20the%20generated%20weights%20is%20done%20like%20an%0AMLP-Mixer%20channel%20mixing%20over%20variable-sized%20vertex%20neighborhoods.%20We%0Ademonstrate%20HyperAggregation%20in%20two%20models%2C%20GraphHyperMixer%20is%20a%20model%20based%20on%0AMLP-Mixer%20while%20GraphHyperConv%20is%20derived%20from%20a%20GCN%20but%20with%20a%0Ahypernetwork-based%20aggregation%20function.%20We%20perform%20experiments%20on%20diverse%0Abenchmark%20datasets%20for%20the%20vertex%20classification%2C%20graph%20classification%2C%20and%0Agraph%20regression%20tasks.%20The%20results%20show%20that%20HyperAggregation%20can%20be%0Aeffectively%20used%20for%20homophilic%20and%20heterophilic%20datasets%20in%20both%20inductive%20and%0Atransductive%20settings.%20GraphHyperConv%20performs%20better%20than%20GraphHyperMixer%20and%0Ais%20especially%20strong%20in%20the%20transductive%20setting.%20On%20the%20heterophilic%20dataset%0ARoman-Empire%20it%20reaches%20a%20new%20state%20of%20the%20art.%20On%20the%20graph-level%20tasks%20our%0Amodels%20perform%20in%20line%20with%20similarly%20sized%20models.%20Ablation%20studies%0Ainvestigate%20the%20robustness%20against%20various%20hyperparameter%20choices.%20The%0Aimplementation%20of%20HyperAggregation%20as%20well%20code%20to%20reproduce%20all%20experiments%20is%0Aavailable%20under%20https%3A//github.com/Foisunt/HyperAggregation%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11596v1&entry.124074799=Read"},
{"title": "Personalized Conversational Travel Assistant powered by Generative AI", "author": "Alexio Cassani and Michele Ruberl and Antonio Salis and Giacomo Giannese and Gianluca Boanelli", "abstract": "  The Tourism and Destination Management Organization (DMO) industry is rapidly\nevolving to adapt to new technologies and traveler expectations. Generative\nArtificial Intelligence (AI) offers an astonishing and innovative opportunity\nto enhance the tourism experience by providing personalized, interactive and\nengaging assistance. In this article, we propose a generative AI-based chatbot\nfor tourism assistance. The chatbot leverages AI ability to generate realistic\nand creative texts, adopting the friendly persona of the well-known Italian\nall-knowledgeable aunties, to provide tourists with personalized information,\ntailored and dynamic pre, during and post recommendations and trip plans and\npersonalized itineraries, using both text and voice commands, and supporting\ndifferent languages to satisfy Italian and foreign tourists expectations. This\nwork is under development in the Molise CTE research project, funded by the\nItalian Minister of the Economic Growth (MIMIT), with the aim to leverage the\nbest emerging technologies available, such as Cloud and AI to produce state of\nthe art solutions in the Smart City environment.\n", "link": "http://arxiv.org/abs/2407.11830v1", "date": "2024-07-16", "relevancy": 2.4069, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.491}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4844}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Conversational%20Travel%20Assistant%20powered%20by%20Generative%20AI&body=Title%3A%20Personalized%20Conversational%20Travel%20Assistant%20powered%20by%20Generative%20AI%0AAuthor%3A%20Alexio%20Cassani%20and%20Michele%20Ruberl%20and%20Antonio%20Salis%20and%20Giacomo%20Giannese%20and%20Gianluca%20Boanelli%0AAbstract%3A%20%20%20The%20Tourism%20and%20Destination%20Management%20Organization%20%28DMO%29%20industry%20is%20rapidly%0Aevolving%20to%20adapt%20to%20new%20technologies%20and%20traveler%20expectations.%20Generative%0AArtificial%20Intelligence%20%28AI%29%20offers%20an%20astonishing%20and%20innovative%20opportunity%0Ato%20enhance%20the%20tourism%20experience%20by%20providing%20personalized%2C%20interactive%20and%0Aengaging%20assistance.%20In%20this%20article%2C%20we%20propose%20a%20generative%20AI-based%20chatbot%0Afor%20tourism%20assistance.%20The%20chatbot%20leverages%20AI%20ability%20to%20generate%20realistic%0Aand%20creative%20texts%2C%20adopting%20the%20friendly%20persona%20of%20the%20well-known%20Italian%0Aall-knowledgeable%20aunties%2C%20to%20provide%20tourists%20with%20personalized%20information%2C%0Atailored%20and%20dynamic%20pre%2C%20during%20and%20post%20recommendations%20and%20trip%20plans%20and%0Apersonalized%20itineraries%2C%20using%20both%20text%20and%20voice%20commands%2C%20and%20supporting%0Adifferent%20languages%20to%20satisfy%20Italian%20and%20foreign%20tourists%20expectations.%20This%0Awork%20is%20under%20development%20in%20the%20Molise%20CTE%20research%20project%2C%20funded%20by%20the%0AItalian%20Minister%20of%20the%20Economic%20Growth%20%28MIMIT%29%2C%20with%20the%20aim%20to%20leverage%20the%0Abest%20emerging%20technologies%20available%2C%20such%20as%20Cloud%20and%20AI%20to%20produce%20state%20of%0Athe%20art%20solutions%20in%20the%20Smart%20City%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Conversational%2520Travel%2520Assistant%2520powered%2520by%2520Generative%2520AI%26entry.906535625%3DAlexio%2520Cassani%2520and%2520Michele%2520Ruberl%2520and%2520Antonio%2520Salis%2520and%2520Giacomo%2520Giannese%2520and%2520Gianluca%2520Boanelli%26entry.1292438233%3D%2520%2520The%2520Tourism%2520and%2520Destination%2520Management%2520Organization%2520%2528DMO%2529%2520industry%2520is%2520rapidly%250Aevolving%2520to%2520adapt%2520to%2520new%2520technologies%2520and%2520traveler%2520expectations.%2520Generative%250AArtificial%2520Intelligence%2520%2528AI%2529%2520offers%2520an%2520astonishing%2520and%2520innovative%2520opportunity%250Ato%2520enhance%2520the%2520tourism%2520experience%2520by%2520providing%2520personalized%252C%2520interactive%2520and%250Aengaging%2520assistance.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520generative%2520AI-based%2520chatbot%250Afor%2520tourism%2520assistance.%2520The%2520chatbot%2520leverages%2520AI%2520ability%2520to%2520generate%2520realistic%250Aand%2520creative%2520texts%252C%2520adopting%2520the%2520friendly%2520persona%2520of%2520the%2520well-known%2520Italian%250Aall-knowledgeable%2520aunties%252C%2520to%2520provide%2520tourists%2520with%2520personalized%2520information%252C%250Atailored%2520and%2520dynamic%2520pre%252C%2520during%2520and%2520post%2520recommendations%2520and%2520trip%2520plans%2520and%250Apersonalized%2520itineraries%252C%2520using%2520both%2520text%2520and%2520voice%2520commands%252C%2520and%2520supporting%250Adifferent%2520languages%2520to%2520satisfy%2520Italian%2520and%2520foreign%2520tourists%2520expectations.%2520This%250Awork%2520is%2520under%2520development%2520in%2520the%2520Molise%2520CTE%2520research%2520project%252C%2520funded%2520by%2520the%250AItalian%2520Minister%2520of%2520the%2520Economic%2520Growth%2520%2528MIMIT%2529%252C%2520with%2520the%2520aim%2520to%2520leverage%2520the%250Abest%2520emerging%2520technologies%2520available%252C%2520such%2520as%2520Cloud%2520and%2520AI%2520to%2520produce%2520state%2520of%250Athe%2520art%2520solutions%2520in%2520the%2520Smart%2520City%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Conversational%20Travel%20Assistant%20powered%20by%20Generative%20AI&entry.906535625=Alexio%20Cassani%20and%20Michele%20Ruberl%20and%20Antonio%20Salis%20and%20Giacomo%20Giannese%20and%20Gianluca%20Boanelli&entry.1292438233=%20%20The%20Tourism%20and%20Destination%20Management%20Organization%20%28DMO%29%20industry%20is%20rapidly%0Aevolving%20to%20adapt%20to%20new%20technologies%20and%20traveler%20expectations.%20Generative%0AArtificial%20Intelligence%20%28AI%29%20offers%20an%20astonishing%20and%20innovative%20opportunity%0Ato%20enhance%20the%20tourism%20experience%20by%20providing%20personalized%2C%20interactive%20and%0Aengaging%20assistance.%20In%20this%20article%2C%20we%20propose%20a%20generative%20AI-based%20chatbot%0Afor%20tourism%20assistance.%20The%20chatbot%20leverages%20AI%20ability%20to%20generate%20realistic%0Aand%20creative%20texts%2C%20adopting%20the%20friendly%20persona%20of%20the%20well-known%20Italian%0Aall-knowledgeable%20aunties%2C%20to%20provide%20tourists%20with%20personalized%20information%2C%0Atailored%20and%20dynamic%20pre%2C%20during%20and%20post%20recommendations%20and%20trip%20plans%20and%0Apersonalized%20itineraries%2C%20using%20both%20text%20and%20voice%20commands%2C%20and%20supporting%0Adifferent%20languages%20to%20satisfy%20Italian%20and%20foreign%20tourists%20expectations.%20This%0Awork%20is%20under%20development%20in%20the%20Molise%20CTE%20research%20project%2C%20funded%20by%20the%0AItalian%20Minister%20of%20the%20Economic%20Growth%20%28MIMIT%29%2C%20with%20the%20aim%20to%20leverage%20the%0Abest%20emerging%20technologies%20available%2C%20such%20as%20Cloud%20and%20AI%20to%20produce%20state%20of%0Athe%20art%20solutions%20in%20the%20Smart%20City%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11830v1&entry.124074799=Read"},
{"title": "Graph Dimension Attention Networks for Enterprise Credit Assessment", "author": "Shaopeng Wei and Beni Egressy and Xingyan Chen and Yu Zhao and Fuzhen Zhuang and Roger Wattenhofer and Gang Kou", "abstract": "  Enterprise credit assessment is critical for evaluating financial risk, and\nGraph Neural Networks (GNNs), with their advanced capability to model\ninter-entity relationships, are a natural tool to get a deeper understanding of\nthese financial networks. However, existing GNN-based methodologies\npredominantly emphasize entity-level attention mechanisms for contagion risk\naggregation, often overlooking the heterogeneous importance of different\nfeature dimensions, thus falling short in adequately modeling credit risk\nlevels. To address this issue, we propose a novel architecture named Graph\nDimension Attention Network (GDAN), which incorporates a dimension-level\nattention mechanism to capture fine-grained risk-related characteristics.\nFurthermore, we explore the interpretability of the GNN-based method in\nfinancial scenarios and propose a simple but effective data-centric explainer\nfor GDAN, called GDAN-DistShift. DistShift provides edge-level interpretability\nby quantifying distribution shifts during the message-passing process.\nMoreover, we collected a real-world, multi-source Enterprise Credit Assessment\nDataset (ECAD) and have made it accessible to the research community since\nhigh-quality datasets are lacking in this field. Extensive experiments\nconducted on ECAD demonstrate the effectiveness of our methods. In addition, we\nran GDAN on the well-known datasets SMEsD and DBLP, also with excellent\nresults.\n", "link": "http://arxiv.org/abs/2407.11615v1", "date": "2024-07-16", "relevancy": 2.396, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.521}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4598}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Dimension%20Attention%20Networks%20for%20Enterprise%20Credit%20Assessment&body=Title%3A%20Graph%20Dimension%20Attention%20Networks%20for%20Enterprise%20Credit%20Assessment%0AAuthor%3A%20Shaopeng%20Wei%20and%20Beni%20Egressy%20and%20Xingyan%20Chen%20and%20Yu%20Zhao%20and%20Fuzhen%20Zhuang%20and%20Roger%20Wattenhofer%20and%20Gang%20Kou%0AAbstract%3A%20%20%20Enterprise%20credit%20assessment%20is%20critical%20for%20evaluating%20financial%20risk%2C%20and%0AGraph%20Neural%20Networks%20%28GNNs%29%2C%20with%20their%20advanced%20capability%20to%20model%0Ainter-entity%20relationships%2C%20are%20a%20natural%20tool%20to%20get%20a%20deeper%20understanding%20of%0Athese%20financial%20networks.%20However%2C%20existing%20GNN-based%20methodologies%0Apredominantly%20emphasize%20entity-level%20attention%20mechanisms%20for%20contagion%20risk%0Aaggregation%2C%20often%20overlooking%20the%20heterogeneous%20importance%20of%20different%0Afeature%20dimensions%2C%20thus%20falling%20short%20in%20adequately%20modeling%20credit%20risk%0Alevels.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20architecture%20named%20Graph%0ADimension%20Attention%20Network%20%28GDAN%29%2C%20which%20incorporates%20a%20dimension-level%0Aattention%20mechanism%20to%20capture%20fine-grained%20risk-related%20characteristics.%0AFurthermore%2C%20we%20explore%20the%20interpretability%20of%20the%20GNN-based%20method%20in%0Afinancial%20scenarios%20and%20propose%20a%20simple%20but%20effective%20data-centric%20explainer%0Afor%20GDAN%2C%20called%20GDAN-DistShift.%20DistShift%20provides%20edge-level%20interpretability%0Aby%20quantifying%20distribution%20shifts%20during%20the%20message-passing%20process.%0AMoreover%2C%20we%20collected%20a%20real-world%2C%20multi-source%20Enterprise%20Credit%20Assessment%0ADataset%20%28ECAD%29%20and%20have%20made%20it%20accessible%20to%20the%20research%20community%20since%0Ahigh-quality%20datasets%20are%20lacking%20in%20this%20field.%20Extensive%20experiments%0Aconducted%20on%20ECAD%20demonstrate%20the%20effectiveness%20of%20our%20methods.%20In%20addition%2C%20we%0Aran%20GDAN%20on%20the%20well-known%20datasets%20SMEsD%20and%20DBLP%2C%20also%20with%20excellent%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Dimension%2520Attention%2520Networks%2520for%2520Enterprise%2520Credit%2520Assessment%26entry.906535625%3DShaopeng%2520Wei%2520and%2520Beni%2520Egressy%2520and%2520Xingyan%2520Chen%2520and%2520Yu%2520Zhao%2520and%2520Fuzhen%2520Zhuang%2520and%2520Roger%2520Wattenhofer%2520and%2520Gang%2520Kou%26entry.1292438233%3D%2520%2520Enterprise%2520credit%2520assessment%2520is%2520critical%2520for%2520evaluating%2520financial%2520risk%252C%2520and%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520with%2520their%2520advanced%2520capability%2520to%2520model%250Ainter-entity%2520relationships%252C%2520are%2520a%2520natural%2520tool%2520to%2520get%2520a%2520deeper%2520understanding%2520of%250Athese%2520financial%2520networks.%2520However%252C%2520existing%2520GNN-based%2520methodologies%250Apredominantly%2520emphasize%2520entity-level%2520attention%2520mechanisms%2520for%2520contagion%2520risk%250Aaggregation%252C%2520often%2520overlooking%2520the%2520heterogeneous%2520importance%2520of%2520different%250Afeature%2520dimensions%252C%2520thus%2520falling%2520short%2520in%2520adequately%2520modeling%2520credit%2520risk%250Alevels.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520architecture%2520named%2520Graph%250ADimension%2520Attention%2520Network%2520%2528GDAN%2529%252C%2520which%2520incorporates%2520a%2520dimension-level%250Aattention%2520mechanism%2520to%2520capture%2520fine-grained%2520risk-related%2520characteristics.%250AFurthermore%252C%2520we%2520explore%2520the%2520interpretability%2520of%2520the%2520GNN-based%2520method%2520in%250Afinancial%2520scenarios%2520and%2520propose%2520a%2520simple%2520but%2520effective%2520data-centric%2520explainer%250Afor%2520GDAN%252C%2520called%2520GDAN-DistShift.%2520DistShift%2520provides%2520edge-level%2520interpretability%250Aby%2520quantifying%2520distribution%2520shifts%2520during%2520the%2520message-passing%2520process.%250AMoreover%252C%2520we%2520collected%2520a%2520real-world%252C%2520multi-source%2520Enterprise%2520Credit%2520Assessment%250ADataset%2520%2528ECAD%2529%2520and%2520have%2520made%2520it%2520accessible%2520to%2520the%2520research%2520community%2520since%250Ahigh-quality%2520datasets%2520are%2520lacking%2520in%2520this%2520field.%2520Extensive%2520experiments%250Aconducted%2520on%2520ECAD%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520methods.%2520In%2520addition%252C%2520we%250Aran%2520GDAN%2520on%2520the%2520well-known%2520datasets%2520SMEsD%2520and%2520DBLP%252C%2520also%2520with%2520excellent%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Dimension%20Attention%20Networks%20for%20Enterprise%20Credit%20Assessment&entry.906535625=Shaopeng%20Wei%20and%20Beni%20Egressy%20and%20Xingyan%20Chen%20and%20Yu%20Zhao%20and%20Fuzhen%20Zhuang%20and%20Roger%20Wattenhofer%20and%20Gang%20Kou&entry.1292438233=%20%20Enterprise%20credit%20assessment%20is%20critical%20for%20evaluating%20financial%20risk%2C%20and%0AGraph%20Neural%20Networks%20%28GNNs%29%2C%20with%20their%20advanced%20capability%20to%20model%0Ainter-entity%20relationships%2C%20are%20a%20natural%20tool%20to%20get%20a%20deeper%20understanding%20of%0Athese%20financial%20networks.%20However%2C%20existing%20GNN-based%20methodologies%0Apredominantly%20emphasize%20entity-level%20attention%20mechanisms%20for%20contagion%20risk%0Aaggregation%2C%20often%20overlooking%20the%20heterogeneous%20importance%20of%20different%0Afeature%20dimensions%2C%20thus%20falling%20short%20in%20adequately%20modeling%20credit%20risk%0Alevels.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20architecture%20named%20Graph%0ADimension%20Attention%20Network%20%28GDAN%29%2C%20which%20incorporates%20a%20dimension-level%0Aattention%20mechanism%20to%20capture%20fine-grained%20risk-related%20characteristics.%0AFurthermore%2C%20we%20explore%20the%20interpretability%20of%20the%20GNN-based%20method%20in%0Afinancial%20scenarios%20and%20propose%20a%20simple%20but%20effective%20data-centric%20explainer%0Afor%20GDAN%2C%20called%20GDAN-DistShift.%20DistShift%20provides%20edge-level%20interpretability%0Aby%20quantifying%20distribution%20shifts%20during%20the%20message-passing%20process.%0AMoreover%2C%20we%20collected%20a%20real-world%2C%20multi-source%20Enterprise%20Credit%20Assessment%0ADataset%20%28ECAD%29%20and%20have%20made%20it%20accessible%20to%20the%20research%20community%20since%0Ahigh-quality%20datasets%20are%20lacking%20in%20this%20field.%20Extensive%20experiments%0Aconducted%20on%20ECAD%20demonstrate%20the%20effectiveness%20of%20our%20methods.%20In%20addition%2C%20we%0Aran%20GDAN%20on%20the%20well-known%20datasets%20SMEsD%20and%20DBLP%2C%20also%20with%20excellent%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11615v1&entry.124074799=Read"},
{"title": "Mask-guided cross-image attention for zero-shot in-silico\n  histopathologic image generation with a diffusion model", "author": "Dominik Winter and Nicolas Triltsch and Marco Rosati and Anatoliy Shumilov and Ziya Kokaragac and Yuri Popov and Thomas Padel and Laura Sebastian Monasor and Ross Hill and Markus Schick and Nicolas Brieu", "abstract": "  Creating in-silico data with generative AI promises a cost-effective\nalternative to staining, imaging, and annotating whole slide images in\ncomputational pathology. Diffusion models are the state-of-the-art solution for\ngenerating in-silico images, offering unparalleled fidelity and realism. Using\nappearance transfer diffusion models allows for zero-shot image generation,\nfacilitating fast application and making model training unnecessary. However\ncurrent appearance transfer diffusion models are designed for natural images,\nwhere the main task is to transfer the foreground object from an origin to a\ntarget domain, while the background is of insignificant importance. In\ncomputational pathology, specifically in oncology, it is however not\nstraightforward to define which objects in an image should be classified as\nforeground and background, as all objects in an image may be of critical\nimportance for the detailed understanding the tumor micro-environment. We\ncontribute to the applicability of appearance transfer diffusion models to\nimmunohistochemistry-stained images by modifying the appearance transfer\nguidance to alternate between class-specific AdaIN feature statistics matchings\nusing existing segmentation masks. The performance of the proposed method is\ndemonstrated on the downstream task of supervised epithelium segmentation,\nshowing that the number of manual annotations required for model training can\nbe reduced by 75%, outperforming the baseline approach. Additionally, we\nconsulted with a certified pathologist to investigate future improvements. We\nanticipate this work to inspire the application of zero-shot diffusion models\nin computational pathology, providing an efficient method to generate in-silico\nimages with unmatched fidelity and realism, which prove meaningful for\ndownstream tasks, such as training existing deep learning models or finetuning\nfoundation models.\n", "link": "http://arxiv.org/abs/2407.11664v1", "date": "2024-07-16", "relevancy": 2.3833, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6218}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5982}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model&body=Title%3A%20Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model%0AAuthor%3A%20Dominik%20Winter%20and%20Nicolas%20Triltsch%20and%20Marco%20Rosati%20and%20Anatoliy%20Shumilov%20and%20Ziya%20Kokaragac%20and%20Yuri%20Popov%20and%20Thomas%20Padel%20and%20Laura%20Sebastian%20Monasor%20and%20Ross%20Hill%20and%20Markus%20Schick%20and%20Nicolas%20Brieu%0AAbstract%3A%20%20%20Creating%20in-silico%20data%20with%20generative%20AI%20promises%20a%20cost-effective%0Aalternative%20to%20staining%2C%20imaging%2C%20and%20annotating%20whole%20slide%20images%20in%0Acomputational%20pathology.%20Diffusion%20models%20are%20the%20state-of-the-art%20solution%20for%0Agenerating%20in-silico%20images%2C%20offering%20unparalleled%20fidelity%20and%20realism.%20Using%0Aappearance%20transfer%20diffusion%20models%20allows%20for%20zero-shot%20image%20generation%2C%0Afacilitating%20fast%20application%20and%20making%20model%20training%20unnecessary.%20However%0Acurrent%20appearance%20transfer%20diffusion%20models%20are%20designed%20for%20natural%20images%2C%0Awhere%20the%20main%20task%20is%20to%20transfer%20the%20foreground%20object%20from%20an%20origin%20to%20a%0Atarget%20domain%2C%20while%20the%20background%20is%20of%20insignificant%20importance.%20In%0Acomputational%20pathology%2C%20specifically%20in%20oncology%2C%20it%20is%20however%20not%0Astraightforward%20to%20define%20which%20objects%20in%20an%20image%20should%20be%20classified%20as%0Aforeground%20and%20background%2C%20as%20all%20objects%20in%20an%20image%20may%20be%20of%20critical%0Aimportance%20for%20the%20detailed%20understanding%20the%20tumor%20micro-environment.%20We%0Acontribute%20to%20the%20applicability%20of%20appearance%20transfer%20diffusion%20models%20to%0Aimmunohistochemistry-stained%20images%20by%20modifying%20the%20appearance%20transfer%0Aguidance%20to%20alternate%20between%20class-specific%20AdaIN%20feature%20statistics%20matchings%0Ausing%20existing%20segmentation%20masks.%20The%20performance%20of%20the%20proposed%20method%20is%0Ademonstrated%20on%20the%20downstream%20task%20of%20supervised%20epithelium%20segmentation%2C%0Ashowing%20that%20the%20number%20of%20manual%20annotations%20required%20for%20model%20training%20can%0Abe%20reduced%20by%2075%25%2C%20outperforming%20the%20baseline%20approach.%20Additionally%2C%20we%0Aconsulted%20with%20a%20certified%20pathologist%20to%20investigate%20future%20improvements.%20We%0Aanticipate%20this%20work%20to%20inspire%20the%20application%20of%20zero-shot%20diffusion%20models%0Ain%20computational%20pathology%2C%20providing%20an%20efficient%20method%20to%20generate%20in-silico%0Aimages%20with%20unmatched%20fidelity%20and%20realism%2C%20which%20prove%20meaningful%20for%0Adownstream%20tasks%2C%20such%20as%20training%20existing%20deep%20learning%20models%20or%20finetuning%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-guided%2520cross-image%2520attention%2520for%2520zero-shot%2520in-silico%250A%2520%2520histopathologic%2520image%2520generation%2520with%2520a%2520diffusion%2520model%26entry.906535625%3DDominik%2520Winter%2520and%2520Nicolas%2520Triltsch%2520and%2520Marco%2520Rosati%2520and%2520Anatoliy%2520Shumilov%2520and%2520Ziya%2520Kokaragac%2520and%2520Yuri%2520Popov%2520and%2520Thomas%2520Padel%2520and%2520Laura%2520Sebastian%2520Monasor%2520and%2520Ross%2520Hill%2520and%2520Markus%2520Schick%2520and%2520Nicolas%2520Brieu%26entry.1292438233%3D%2520%2520Creating%2520in-silico%2520data%2520with%2520generative%2520AI%2520promises%2520a%2520cost-effective%250Aalternative%2520to%2520staining%252C%2520imaging%252C%2520and%2520annotating%2520whole%2520slide%2520images%2520in%250Acomputational%2520pathology.%2520Diffusion%2520models%2520are%2520the%2520state-of-the-art%2520solution%2520for%250Agenerating%2520in-silico%2520images%252C%2520offering%2520unparalleled%2520fidelity%2520and%2520realism.%2520Using%250Aappearance%2520transfer%2520diffusion%2520models%2520allows%2520for%2520zero-shot%2520image%2520generation%252C%250Afacilitating%2520fast%2520application%2520and%2520making%2520model%2520training%2520unnecessary.%2520However%250Acurrent%2520appearance%2520transfer%2520diffusion%2520models%2520are%2520designed%2520for%2520natural%2520images%252C%250Awhere%2520the%2520main%2520task%2520is%2520to%2520transfer%2520the%2520foreground%2520object%2520from%2520an%2520origin%2520to%2520a%250Atarget%2520domain%252C%2520while%2520the%2520background%2520is%2520of%2520insignificant%2520importance.%2520In%250Acomputational%2520pathology%252C%2520specifically%2520in%2520oncology%252C%2520it%2520is%2520however%2520not%250Astraightforward%2520to%2520define%2520which%2520objects%2520in%2520an%2520image%2520should%2520be%2520classified%2520as%250Aforeground%2520and%2520background%252C%2520as%2520all%2520objects%2520in%2520an%2520image%2520may%2520be%2520of%2520critical%250Aimportance%2520for%2520the%2520detailed%2520understanding%2520the%2520tumor%2520micro-environment.%2520We%250Acontribute%2520to%2520the%2520applicability%2520of%2520appearance%2520transfer%2520diffusion%2520models%2520to%250Aimmunohistochemistry-stained%2520images%2520by%2520modifying%2520the%2520appearance%2520transfer%250Aguidance%2520to%2520alternate%2520between%2520class-specific%2520AdaIN%2520feature%2520statistics%2520matchings%250Ausing%2520existing%2520segmentation%2520masks.%2520The%2520performance%2520of%2520the%2520proposed%2520method%2520is%250Ademonstrated%2520on%2520the%2520downstream%2520task%2520of%2520supervised%2520epithelium%2520segmentation%252C%250Ashowing%2520that%2520the%2520number%2520of%2520manual%2520annotations%2520required%2520for%2520model%2520training%2520can%250Abe%2520reduced%2520by%252075%2525%252C%2520outperforming%2520the%2520baseline%2520approach.%2520Additionally%252C%2520we%250Aconsulted%2520with%2520a%2520certified%2520pathologist%2520to%2520investigate%2520future%2520improvements.%2520We%250Aanticipate%2520this%2520work%2520to%2520inspire%2520the%2520application%2520of%2520zero-shot%2520diffusion%2520models%250Ain%2520computational%2520pathology%252C%2520providing%2520an%2520efficient%2520method%2520to%2520generate%2520in-silico%250Aimages%2520with%2520unmatched%2520fidelity%2520and%2520realism%252C%2520which%2520prove%2520meaningful%2520for%250Adownstream%2520tasks%252C%2520such%2520as%2520training%2520existing%2520deep%2520learning%2520models%2520or%2520finetuning%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-guided%20cross-image%20attention%20for%20zero-shot%20in-silico%0A%20%20histopathologic%20image%20generation%20with%20a%20diffusion%20model&entry.906535625=Dominik%20Winter%20and%20Nicolas%20Triltsch%20and%20Marco%20Rosati%20and%20Anatoliy%20Shumilov%20and%20Ziya%20Kokaragac%20and%20Yuri%20Popov%20and%20Thomas%20Padel%20and%20Laura%20Sebastian%20Monasor%20and%20Ross%20Hill%20and%20Markus%20Schick%20and%20Nicolas%20Brieu&entry.1292438233=%20%20Creating%20in-silico%20data%20with%20generative%20AI%20promises%20a%20cost-effective%0Aalternative%20to%20staining%2C%20imaging%2C%20and%20annotating%20whole%20slide%20images%20in%0Acomputational%20pathology.%20Diffusion%20models%20are%20the%20state-of-the-art%20solution%20for%0Agenerating%20in-silico%20images%2C%20offering%20unparalleled%20fidelity%20and%20realism.%20Using%0Aappearance%20transfer%20diffusion%20models%20allows%20for%20zero-shot%20image%20generation%2C%0Afacilitating%20fast%20application%20and%20making%20model%20training%20unnecessary.%20However%0Acurrent%20appearance%20transfer%20diffusion%20models%20are%20designed%20for%20natural%20images%2C%0Awhere%20the%20main%20task%20is%20to%20transfer%20the%20foreground%20object%20from%20an%20origin%20to%20a%0Atarget%20domain%2C%20while%20the%20background%20is%20of%20insignificant%20importance.%20In%0Acomputational%20pathology%2C%20specifically%20in%20oncology%2C%20it%20is%20however%20not%0Astraightforward%20to%20define%20which%20objects%20in%20an%20image%20should%20be%20classified%20as%0Aforeground%20and%20background%2C%20as%20all%20objects%20in%20an%20image%20may%20be%20of%20critical%0Aimportance%20for%20the%20detailed%20understanding%20the%20tumor%20micro-environment.%20We%0Acontribute%20to%20the%20applicability%20of%20appearance%20transfer%20diffusion%20models%20to%0Aimmunohistochemistry-stained%20images%20by%20modifying%20the%20appearance%20transfer%0Aguidance%20to%20alternate%20between%20class-specific%20AdaIN%20feature%20statistics%20matchings%0Ausing%20existing%20segmentation%20masks.%20The%20performance%20of%20the%20proposed%20method%20is%0Ademonstrated%20on%20the%20downstream%20task%20of%20supervised%20epithelium%20segmentation%2C%0Ashowing%20that%20the%20number%20of%20manual%20annotations%20required%20for%20model%20training%20can%0Abe%20reduced%20by%2075%25%2C%20outperforming%20the%20baseline%20approach.%20Additionally%2C%20we%0Aconsulted%20with%20a%20certified%20pathologist%20to%20investigate%20future%20improvements.%20We%0Aanticipate%20this%20work%20to%20inspire%20the%20application%20of%20zero-shot%20diffusion%20models%0Ain%20computational%20pathology%2C%20providing%20an%20efficient%20method%20to%20generate%20in-silico%0Aimages%20with%20unmatched%20fidelity%20and%20realism%2C%20which%20prove%20meaningful%20for%0Adownstream%20tasks%2C%20such%20as%20training%20existing%20deep%20learning%20models%20or%20finetuning%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11664v1&entry.124074799=Read"},
{"title": "Sparse Training for Federated Learning with Regularized Error Correction", "author": "Ran Greidi and Kobi Cohen", "abstract": "  Federated Learning (FL) has attracted much interest due to the significant\nadvantages it brings to training deep neural network (DNN) models. However,\nsince communications and computation resources are limited, training DNN models\nin FL systems face challenges such as elevated computational and communication\ncosts in complex tasks. Sparse training schemes gain increasing attention in\norder to scale down the dimensionality of each client (i.e., node)\ntransmission. Specifically, sparsification with error correction methods is a\npromising technique, where only important updates are sent to the parameter\nserver (PS) and the rest are accumulated locally. While error correction\nmethods have shown to achieve a significant sparsification level of the\nclient-to-PS message without harming convergence, pushing sparsity further\nremains unresolved due to the staleness effect. In this paper, we propose a\nnovel algorithm, dubbed Federated Learning with Accumulated Regularized\nEmbeddings (FLARE), to overcome this challenge. FLARE presents a novel sparse\ntraining approach via accumulated pulling of the updated models with\nregularization on the embeddings in the FL process, providing a powerful\nsolution to the staleness effect, and pushing sparsity to an exceptional level.\nThe performance of FLARE is validated through extensive experiments on diverse\nand complex models, achieving a remarkable sparsity level (10 times and more\nbeyond the current state-of-the-art) along with significantly improved\naccuracy. Additionally, an open-source software package has been developed for\nthe benefit of researchers and developers in related fields.\n", "link": "http://arxiv.org/abs/2312.13795v2", "date": "2024-07-16", "relevancy": 2.3486, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4738}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Training%20for%20Federated%20Learning%20with%20Regularized%20Error%20Correction&body=Title%3A%20Sparse%20Training%20for%20Federated%20Learning%20with%20Regularized%20Error%20Correction%0AAuthor%3A%20Ran%20Greidi%20and%20Kobi%20Cohen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20attracted%20much%20interest%20due%20to%20the%20significant%0Aadvantages%20it%20brings%20to%20training%20deep%20neural%20network%20%28DNN%29%20models.%20However%2C%0Asince%20communications%20and%20computation%20resources%20are%20limited%2C%20training%20DNN%20models%0Ain%20FL%20systems%20face%20challenges%20such%20as%20elevated%20computational%20and%20communication%0Acosts%20in%20complex%20tasks.%20Sparse%20training%20schemes%20gain%20increasing%20attention%20in%0Aorder%20to%20scale%20down%20the%20dimensionality%20of%20each%20client%20%28i.e.%2C%20node%29%0Atransmission.%20Specifically%2C%20sparsification%20with%20error%20correction%20methods%20is%20a%0Apromising%20technique%2C%20where%20only%20important%20updates%20are%20sent%20to%20the%20parameter%0Aserver%20%28PS%29%20and%20the%20rest%20are%20accumulated%20locally.%20While%20error%20correction%0Amethods%20have%20shown%20to%20achieve%20a%20significant%20sparsification%20level%20of%20the%0Aclient-to-PS%20message%20without%20harming%20convergence%2C%20pushing%20sparsity%20further%0Aremains%20unresolved%20due%20to%20the%20staleness%20effect.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20algorithm%2C%20dubbed%20Federated%20Learning%20with%20Accumulated%20Regularized%0AEmbeddings%20%28FLARE%29%2C%20to%20overcome%20this%20challenge.%20FLARE%20presents%20a%20novel%20sparse%0Atraining%20approach%20via%20accumulated%20pulling%20of%20the%20updated%20models%20with%0Aregularization%20on%20the%20embeddings%20in%20the%20FL%20process%2C%20providing%20a%20powerful%0Asolution%20to%20the%20staleness%20effect%2C%20and%20pushing%20sparsity%20to%20an%20exceptional%20level.%0AThe%20performance%20of%20FLARE%20is%20validated%20through%20extensive%20experiments%20on%20diverse%0Aand%20complex%20models%2C%20achieving%20a%20remarkable%20sparsity%20level%20%2810%20times%20and%20more%0Abeyond%20the%20current%20state-of-the-art%29%20along%20with%20significantly%20improved%0Aaccuracy.%20Additionally%2C%20an%20open-source%20software%20package%20has%20been%20developed%20for%0Athe%20benefit%20of%20researchers%20and%20developers%20in%20related%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Training%2520for%2520Federated%2520Learning%2520with%2520Regularized%2520Error%2520Correction%26entry.906535625%3DRan%2520Greidi%2520and%2520Kobi%2520Cohen%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520attracted%2520much%2520interest%2520due%2520to%2520the%2520significant%250Aadvantages%2520it%2520brings%2520to%2520training%2520deep%2520neural%2520network%2520%2528DNN%2529%2520models.%2520However%252C%250Asince%2520communications%2520and%2520computation%2520resources%2520are%2520limited%252C%2520training%2520DNN%2520models%250Ain%2520FL%2520systems%2520face%2520challenges%2520such%2520as%2520elevated%2520computational%2520and%2520communication%250Acosts%2520in%2520complex%2520tasks.%2520Sparse%2520training%2520schemes%2520gain%2520increasing%2520attention%2520in%250Aorder%2520to%2520scale%2520down%2520the%2520dimensionality%2520of%2520each%2520client%2520%2528i.e.%252C%2520node%2529%250Atransmission.%2520Specifically%252C%2520sparsification%2520with%2520error%2520correction%2520methods%2520is%2520a%250Apromising%2520technique%252C%2520where%2520only%2520important%2520updates%2520are%2520sent%2520to%2520the%2520parameter%250Aserver%2520%2528PS%2529%2520and%2520the%2520rest%2520are%2520accumulated%2520locally.%2520While%2520error%2520correction%250Amethods%2520have%2520shown%2520to%2520achieve%2520a%2520significant%2520sparsification%2520level%2520of%2520the%250Aclient-to-PS%2520message%2520without%2520harming%2520convergence%252C%2520pushing%2520sparsity%2520further%250Aremains%2520unresolved%2520due%2520to%2520the%2520staleness%2520effect.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520algorithm%252C%2520dubbed%2520Federated%2520Learning%2520with%2520Accumulated%2520Regularized%250AEmbeddings%2520%2528FLARE%2529%252C%2520to%2520overcome%2520this%2520challenge.%2520FLARE%2520presents%2520a%2520novel%2520sparse%250Atraining%2520approach%2520via%2520accumulated%2520pulling%2520of%2520the%2520updated%2520models%2520with%250Aregularization%2520on%2520the%2520embeddings%2520in%2520the%2520FL%2520process%252C%2520providing%2520a%2520powerful%250Asolution%2520to%2520the%2520staleness%2520effect%252C%2520and%2520pushing%2520sparsity%2520to%2520an%2520exceptional%2520level.%250AThe%2520performance%2520of%2520FLARE%2520is%2520validated%2520through%2520extensive%2520experiments%2520on%2520diverse%250Aand%2520complex%2520models%252C%2520achieving%2520a%2520remarkable%2520sparsity%2520level%2520%252810%2520times%2520and%2520more%250Abeyond%2520the%2520current%2520state-of-the-art%2529%2520along%2520with%2520significantly%2520improved%250Aaccuracy.%2520Additionally%252C%2520an%2520open-source%2520software%2520package%2520has%2520been%2520developed%2520for%250Athe%2520benefit%2520of%2520researchers%2520and%2520developers%2520in%2520related%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Training%20for%20Federated%20Learning%20with%20Regularized%20Error%20Correction&entry.906535625=Ran%20Greidi%20and%20Kobi%20Cohen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20attracted%20much%20interest%20due%20to%20the%20significant%0Aadvantages%20it%20brings%20to%20training%20deep%20neural%20network%20%28DNN%29%20models.%20However%2C%0Asince%20communications%20and%20computation%20resources%20are%20limited%2C%20training%20DNN%20models%0Ain%20FL%20systems%20face%20challenges%20such%20as%20elevated%20computational%20and%20communication%0Acosts%20in%20complex%20tasks.%20Sparse%20training%20schemes%20gain%20increasing%20attention%20in%0Aorder%20to%20scale%20down%20the%20dimensionality%20of%20each%20client%20%28i.e.%2C%20node%29%0Atransmission.%20Specifically%2C%20sparsification%20with%20error%20correction%20methods%20is%20a%0Apromising%20technique%2C%20where%20only%20important%20updates%20are%20sent%20to%20the%20parameter%0Aserver%20%28PS%29%20and%20the%20rest%20are%20accumulated%20locally.%20While%20error%20correction%0Amethods%20have%20shown%20to%20achieve%20a%20significant%20sparsification%20level%20of%20the%0Aclient-to-PS%20message%20without%20harming%20convergence%2C%20pushing%20sparsity%20further%0Aremains%20unresolved%20due%20to%20the%20staleness%20effect.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20algorithm%2C%20dubbed%20Federated%20Learning%20with%20Accumulated%20Regularized%0AEmbeddings%20%28FLARE%29%2C%20to%20overcome%20this%20challenge.%20FLARE%20presents%20a%20novel%20sparse%0Atraining%20approach%20via%20accumulated%20pulling%20of%20the%20updated%20models%20with%0Aregularization%20on%20the%20embeddings%20in%20the%20FL%20process%2C%20providing%20a%20powerful%0Asolution%20to%20the%20staleness%20effect%2C%20and%20pushing%20sparsity%20to%20an%20exceptional%20level.%0AThe%20performance%20of%20FLARE%20is%20validated%20through%20extensive%20experiments%20on%20diverse%0Aand%20complex%20models%2C%20achieving%20a%20remarkable%20sparsity%20level%20%2810%20times%20and%20more%0Abeyond%20the%20current%20state-of-the-art%29%20along%20with%20significantly%20improved%0Aaccuracy.%20Additionally%2C%20an%20open-source%20software%20package%20has%20been%20developed%20for%0Athe%20benefit%20of%20researchers%20and%20developers%20in%20related%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13795v2&entry.124074799=Read"},
{"title": "MotionCtrl: A Unified and Flexible Motion Controller for Video\n  Generation", "author": "Zhouxia Wang and Ziyang Yuan and Xintao Wang and Tianshui Chen and Menghan Xia and Ping Luo and Ying Shan", "abstract": "  Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods. Project Page: https://wzhouxiff.github.io/projects/MotionCtrl/\n", "link": "http://arxiv.org/abs/2312.03641v2", "date": "2024-07-16", "relevancy": 2.3324, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.636}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5739}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionCtrl%3A%20A%20Unified%20and%20Flexible%20Motion%20Controller%20for%20Video%0A%20%20Generation&body=Title%3A%20MotionCtrl%3A%20A%20Unified%20and%20Flexible%20Motion%20Controller%20for%20Video%0A%20%20Generation%0AAuthor%3A%20Zhouxia%20Wang%20and%20Ziyang%20Yuan%20and%20Xintao%20Wang%20and%20Tianshui%20Chen%20and%20Menghan%20Xia%20and%20Ping%20Luo%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Motions%20in%20a%20video%20primarily%20consist%20of%20camera%20motion%2C%20induced%20by%20camera%0Amovement%2C%20and%20object%20motion%2C%20resulting%20from%20object%20movement.%20Accurate%20control%0Aof%20both%20camera%20and%20object%20motion%20is%20essential%20for%20video%20generation.%20However%2C%0Aexisting%20works%20either%20mainly%20focus%20on%20one%20type%20of%20motion%20or%20do%20not%20clearly%0Adistinguish%20between%20the%20two%2C%20limiting%20their%20control%20capabilities%20and%20diversity.%0ATherefore%2C%20this%20paper%20presents%20MotionCtrl%2C%20a%20unified%20and%20flexible%20motion%0Acontroller%20for%20video%20generation%20designed%20to%20effectively%20and%20independently%0Acontrol%20camera%20and%20object%20motion.%20The%20architecture%20and%20training%20strategy%20of%0AMotionCtrl%20are%20carefully%20devised%2C%20taking%20into%20account%20the%20inherent%20properties%0Aof%20camera%20motion%2C%20object%20motion%2C%20and%20imperfect%20training%20data.%20Compared%20to%0Aprevious%20methods%2C%20MotionCtrl%20offers%20three%20main%20advantages%3A%201%29%20It%20effectively%0Aand%20independently%20controls%20camera%20motion%20and%20object%20motion%2C%20enabling%20more%0Afine-grained%20motion%20control%20and%20facilitating%20flexible%20and%20diverse%20combinations%0Aof%20both%20types%20of%20motion.%202%29%20Its%20motion%20conditions%20are%20determined%20by%20camera%0Aposes%20and%20trajectories%2C%20which%20are%20appearance-free%20and%20minimally%20impact%20the%0Aappearance%20or%20shape%20of%20objects%20in%20generated%20videos.%203%29%20It%20is%20a%20relatively%0Ageneralizable%20model%20that%20can%20adapt%20to%20a%20wide%20array%20of%20camera%20poses%20and%0Atrajectories%20once%20trained.%20Extensive%20qualitative%20and%20quantitative%20experiments%0Ahave%20been%20conducted%20to%20demonstrate%20the%20superiority%20of%20MotionCtrl%20over%20existing%0Amethods.%20Project%20Page%3A%20https%3A//wzhouxiff.github.io/projects/MotionCtrl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionCtrl%253A%2520A%2520Unified%2520and%2520Flexible%2520Motion%2520Controller%2520for%2520Video%250A%2520%2520Generation%26entry.906535625%3DZhouxia%2520Wang%2520and%2520Ziyang%2520Yuan%2520and%2520Xintao%2520Wang%2520and%2520Tianshui%2520Chen%2520and%2520Menghan%2520Xia%2520and%2520Ping%2520Luo%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Motions%2520in%2520a%2520video%2520primarily%2520consist%2520of%2520camera%2520motion%252C%2520induced%2520by%2520camera%250Amovement%252C%2520and%2520object%2520motion%252C%2520resulting%2520from%2520object%2520movement.%2520Accurate%2520control%250Aof%2520both%2520camera%2520and%2520object%2520motion%2520is%2520essential%2520for%2520video%2520generation.%2520However%252C%250Aexisting%2520works%2520either%2520mainly%2520focus%2520on%2520one%2520type%2520of%2520motion%2520or%2520do%2520not%2520clearly%250Adistinguish%2520between%2520the%2520two%252C%2520limiting%2520their%2520control%2520capabilities%2520and%2520diversity.%250ATherefore%252C%2520this%2520paper%2520presents%2520MotionCtrl%252C%2520a%2520unified%2520and%2520flexible%2520motion%250Acontroller%2520for%2520video%2520generation%2520designed%2520to%2520effectively%2520and%2520independently%250Acontrol%2520camera%2520and%2520object%2520motion.%2520The%2520architecture%2520and%2520training%2520strategy%2520of%250AMotionCtrl%2520are%2520carefully%2520devised%252C%2520taking%2520into%2520account%2520the%2520inherent%2520properties%250Aof%2520camera%2520motion%252C%2520object%2520motion%252C%2520and%2520imperfect%2520training%2520data.%2520Compared%2520to%250Aprevious%2520methods%252C%2520MotionCtrl%2520offers%2520three%2520main%2520advantages%253A%25201%2529%2520It%2520effectively%250Aand%2520independently%2520controls%2520camera%2520motion%2520and%2520object%2520motion%252C%2520enabling%2520more%250Afine-grained%2520motion%2520control%2520and%2520facilitating%2520flexible%2520and%2520diverse%2520combinations%250Aof%2520both%2520types%2520of%2520motion.%25202%2529%2520Its%2520motion%2520conditions%2520are%2520determined%2520by%2520camera%250Aposes%2520and%2520trajectories%252C%2520which%2520are%2520appearance-free%2520and%2520minimally%2520impact%2520the%250Aappearance%2520or%2520shape%2520of%2520objects%2520in%2520generated%2520videos.%25203%2529%2520It%2520is%2520a%2520relatively%250Ageneralizable%2520model%2520that%2520can%2520adapt%2520to%2520a%2520wide%2520array%2520of%2520camera%2520poses%2520and%250Atrajectories%2520once%2520trained.%2520Extensive%2520qualitative%2520and%2520quantitative%2520experiments%250Ahave%2520been%2520conducted%2520to%2520demonstrate%2520the%2520superiority%2520of%2520MotionCtrl%2520over%2520existing%250Amethods.%2520Project%2520Page%253A%2520https%253A//wzhouxiff.github.io/projects/MotionCtrl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionCtrl%3A%20A%20Unified%20and%20Flexible%20Motion%20Controller%20for%20Video%0A%20%20Generation&entry.906535625=Zhouxia%20Wang%20and%20Ziyang%20Yuan%20and%20Xintao%20Wang%20and%20Tianshui%20Chen%20and%20Menghan%20Xia%20and%20Ping%20Luo%20and%20Ying%20Shan&entry.1292438233=%20%20Motions%20in%20a%20video%20primarily%20consist%20of%20camera%20motion%2C%20induced%20by%20camera%0Amovement%2C%20and%20object%20motion%2C%20resulting%20from%20object%20movement.%20Accurate%20control%0Aof%20both%20camera%20and%20object%20motion%20is%20essential%20for%20video%20generation.%20However%2C%0Aexisting%20works%20either%20mainly%20focus%20on%20one%20type%20of%20motion%20or%20do%20not%20clearly%0Adistinguish%20between%20the%20two%2C%20limiting%20their%20control%20capabilities%20and%20diversity.%0ATherefore%2C%20this%20paper%20presents%20MotionCtrl%2C%20a%20unified%20and%20flexible%20motion%0Acontroller%20for%20video%20generation%20designed%20to%20effectively%20and%20independently%0Acontrol%20camera%20and%20object%20motion.%20The%20architecture%20and%20training%20strategy%20of%0AMotionCtrl%20are%20carefully%20devised%2C%20taking%20into%20account%20the%20inherent%20properties%0Aof%20camera%20motion%2C%20object%20motion%2C%20and%20imperfect%20training%20data.%20Compared%20to%0Aprevious%20methods%2C%20MotionCtrl%20offers%20three%20main%20advantages%3A%201%29%20It%20effectively%0Aand%20independently%20controls%20camera%20motion%20and%20object%20motion%2C%20enabling%20more%0Afine-grained%20motion%20control%20and%20facilitating%20flexible%20and%20diverse%20combinations%0Aof%20both%20types%20of%20motion.%202%29%20Its%20motion%20conditions%20are%20determined%20by%20camera%0Aposes%20and%20trajectories%2C%20which%20are%20appearance-free%20and%20minimally%20impact%20the%0Aappearance%20or%20shape%20of%20objects%20in%20generated%20videos.%203%29%20It%20is%20a%20relatively%0Ageneralizable%20model%20that%20can%20adapt%20to%20a%20wide%20array%20of%20camera%20poses%20and%0Atrajectories%20once%20trained.%20Extensive%20qualitative%20and%20quantitative%20experiments%0Ahave%20been%20conducted%20to%20demonstrate%20the%20superiority%20of%20MotionCtrl%20over%20existing%0Amethods.%20Project%20Page%3A%20https%3A//wzhouxiff.github.io/projects/MotionCtrl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03641v2&entry.124074799=Read"},
{"title": "Improving Unsupervised Video Object Segmentation via Fake Flow\n  Generation", "author": "Suhwan Cho and Minhyeok Lee and Jungho Lee and Donghyeong Kim and Seunghoon Lee and Sungmin Woo and Sangyoun Lee", "abstract": "  Unsupervised video object segmentation (VOS), also known as video salient\nobject detection, aims to detect the most prominent object in a video at the\npixel level. Recently, two-stream approaches that leverage both RGB images and\noptical flow maps have gained significant attention. However, the limited\namount of training data remains a substantial challenge. In this study, we\npropose a novel data generation method that simulates fake optical flows from\nsingle images, thereby creating large-scale training data for stable network\nlearning. Inspired by the observation that optical flow maps are highly\ndependent on depth maps, we generate fake optical flows by refining and\naugmenting the estimated depth maps of each image. By incorporating our\nsimulated image-flow pairs, we achieve new state-of-the-art performance on all\npublic benchmark datasets without relying on complex modules. We believe that\nour data generation method represents a potential breakthrough for future VOS\nresearch.\n", "link": "http://arxiv.org/abs/2407.11714v1", "date": "2024-07-16", "relevancy": 2.2934, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5939}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5699}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Unsupervised%20Video%20Object%20Segmentation%20via%20Fake%20Flow%0A%20%20Generation&body=Title%3A%20Improving%20Unsupervised%20Video%20Object%20Segmentation%20via%20Fake%20Flow%0A%20%20Generation%0AAuthor%3A%20Suhwan%20Cho%20and%20Minhyeok%20Lee%20and%20Jungho%20Lee%20and%20Donghyeong%20Kim%20and%20Seunghoon%20Lee%20and%20Sungmin%20Woo%20and%20Sangyoun%20Lee%0AAbstract%3A%20%20%20Unsupervised%20video%20object%20segmentation%20%28VOS%29%2C%20also%20known%20as%20video%20salient%0Aobject%20detection%2C%20aims%20to%20detect%20the%20most%20prominent%20object%20in%20a%20video%20at%20the%0Apixel%20level.%20Recently%2C%20two-stream%20approaches%20that%20leverage%20both%20RGB%20images%20and%0Aoptical%20flow%20maps%20have%20gained%20significant%20attention.%20However%2C%20the%20limited%0Aamount%20of%20training%20data%20remains%20a%20substantial%20challenge.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20data%20generation%20method%20that%20simulates%20fake%20optical%20flows%20from%0Asingle%20images%2C%20thereby%20creating%20large-scale%20training%20data%20for%20stable%20network%0Alearning.%20Inspired%20by%20the%20observation%20that%20optical%20flow%20maps%20are%20highly%0Adependent%20on%20depth%20maps%2C%20we%20generate%20fake%20optical%20flows%20by%20refining%20and%0Aaugmenting%20the%20estimated%20depth%20maps%20of%20each%20image.%20By%20incorporating%20our%0Asimulated%20image-flow%20pairs%2C%20we%20achieve%20new%20state-of-the-art%20performance%20on%20all%0Apublic%20benchmark%20datasets%20without%20relying%20on%20complex%20modules.%20We%20believe%20that%0Aour%20data%20generation%20method%20represents%20a%20potential%20breakthrough%20for%20future%20VOS%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Unsupervised%2520Video%2520Object%2520Segmentation%2520via%2520Fake%2520Flow%250A%2520%2520Generation%26entry.906535625%3DSuhwan%2520Cho%2520and%2520Minhyeok%2520Lee%2520and%2520Jungho%2520Lee%2520and%2520Donghyeong%2520Kim%2520and%2520Seunghoon%2520Lee%2520and%2520Sungmin%2520Woo%2520and%2520Sangyoun%2520Lee%26entry.1292438233%3D%2520%2520Unsupervised%2520video%2520object%2520segmentation%2520%2528VOS%2529%252C%2520also%2520known%2520as%2520video%2520salient%250Aobject%2520detection%252C%2520aims%2520to%2520detect%2520the%2520most%2520prominent%2520object%2520in%2520a%2520video%2520at%2520the%250Apixel%2520level.%2520Recently%252C%2520two-stream%2520approaches%2520that%2520leverage%2520both%2520RGB%2520images%2520and%250Aoptical%2520flow%2520maps%2520have%2520gained%2520significant%2520attention.%2520However%252C%2520the%2520limited%250Aamount%2520of%2520training%2520data%2520remains%2520a%2520substantial%2520challenge.%2520In%2520this%2520study%252C%2520we%250Apropose%2520a%2520novel%2520data%2520generation%2520method%2520that%2520simulates%2520fake%2520optical%2520flows%2520from%250Asingle%2520images%252C%2520thereby%2520creating%2520large-scale%2520training%2520data%2520for%2520stable%2520network%250Alearning.%2520Inspired%2520by%2520the%2520observation%2520that%2520optical%2520flow%2520maps%2520are%2520highly%250Adependent%2520on%2520depth%2520maps%252C%2520we%2520generate%2520fake%2520optical%2520flows%2520by%2520refining%2520and%250Aaugmenting%2520the%2520estimated%2520depth%2520maps%2520of%2520each%2520image.%2520By%2520incorporating%2520our%250Asimulated%2520image-flow%2520pairs%252C%2520we%2520achieve%2520new%2520state-of-the-art%2520performance%2520on%2520all%250Apublic%2520benchmark%2520datasets%2520without%2520relying%2520on%2520complex%2520modules.%2520We%2520believe%2520that%250Aour%2520data%2520generation%2520method%2520represents%2520a%2520potential%2520breakthrough%2520for%2520future%2520VOS%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Unsupervised%20Video%20Object%20Segmentation%20via%20Fake%20Flow%0A%20%20Generation&entry.906535625=Suhwan%20Cho%20and%20Minhyeok%20Lee%20and%20Jungho%20Lee%20and%20Donghyeong%20Kim%20and%20Seunghoon%20Lee%20and%20Sungmin%20Woo%20and%20Sangyoun%20Lee&entry.1292438233=%20%20Unsupervised%20video%20object%20segmentation%20%28VOS%29%2C%20also%20known%20as%20video%20salient%0Aobject%20detection%2C%20aims%20to%20detect%20the%20most%20prominent%20object%20in%20a%20video%20at%20the%0Apixel%20level.%20Recently%2C%20two-stream%20approaches%20that%20leverage%20both%20RGB%20images%20and%0Aoptical%20flow%20maps%20have%20gained%20significant%20attention.%20However%2C%20the%20limited%0Aamount%20of%20training%20data%20remains%20a%20substantial%20challenge.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20data%20generation%20method%20that%20simulates%20fake%20optical%20flows%20from%0Asingle%20images%2C%20thereby%20creating%20large-scale%20training%20data%20for%20stable%20network%0Alearning.%20Inspired%20by%20the%20observation%20that%20optical%20flow%20maps%20are%20highly%0Adependent%20on%20depth%20maps%2C%20we%20generate%20fake%20optical%20flows%20by%20refining%20and%0Aaugmenting%20the%20estimated%20depth%20maps%20of%20each%20image.%20By%20incorporating%20our%0Asimulated%20image-flow%20pairs%2C%20we%20achieve%20new%20state-of-the-art%20performance%20on%20all%0Apublic%20benchmark%20datasets%20without%20relying%20on%20complex%20modules.%20We%20believe%20that%0Aour%20data%20generation%20method%20represents%20a%20potential%20breakthrough%20for%20future%20VOS%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11714v1&entry.124074799=Read"},
{"title": "Diff-Reg v1: Diffusion Matching Model for Registration Problem", "author": "Qianliang Wu and Haobo Jiang and Lei Luo and Jun Li and Yaqing Ding and Jin Xie and Jian Yang", "abstract": "  Establishing reliable correspondences is essential for registration tasks\nsuch as 3D and 2D3D registration. Existing methods commonly leverage geometric\nor semantic point features to generate potential correspondences. However,\nthese features may face challenges such as large deformation, scale\ninconsistency, and ambiguous matching problems (e.g., symmetry). Additionally,\nmany previous methods, which rely on single-pass prediction, may struggle with\nlocal minima in complex scenarios. To mitigate these challenges, we introduce a\ndiffusion matching model for robust correspondence construction. Our approach\ntreats correspondence estimation as a denoising diffusion process within the\ndoubly stochastic matrix space, which gradually denoises (refines) a doubly\nstochastic matching matrix to the ground-truth one for high-quality\ncorrespondence estimation. It involves a forward diffusion process that\ngradually introduces Gaussian noise into the ground truth matching matrix and a\nreverse denoising process that iteratively refines the noisy matching matrix.\nIn particular, the feature extraction from the backbone occurs only once during\nthe inference phase. Our lightweight denoising module utilizes the same feature\nat each reverse sampling step. Evaluation of our method on both 3D and 2D3D\nregistration tasks confirms its effectiveness. The code is available at\nhttps://github.com/wuqianliang/Diff-Reg.\n", "link": "http://arxiv.org/abs/2403.19919v2", "date": "2024-07-16", "relevancy": 2.284, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.61}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5644}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Reg%20v1%3A%20Diffusion%20Matching%20Model%20for%20Registration%20Problem&body=Title%3A%20Diff-Reg%20v1%3A%20Diffusion%20Matching%20Model%20for%20Registration%20Problem%0AAuthor%3A%20Qianliang%20Wu%20and%20Haobo%20Jiang%20and%20Lei%20Luo%20and%20Jun%20Li%20and%20Yaqing%20Ding%20and%20Jin%20Xie%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Establishing%20reliable%20correspondences%20is%20essential%20for%20registration%20tasks%0Asuch%20as%203D%20and%202D3D%20registration.%20Existing%20methods%20commonly%20leverage%20geometric%0Aor%20semantic%20point%20features%20to%20generate%20potential%20correspondences.%20However%2C%0Athese%20features%20may%20face%20challenges%20such%20as%20large%20deformation%2C%20scale%0Ainconsistency%2C%20and%20ambiguous%20matching%20problems%20%28e.g.%2C%20symmetry%29.%20Additionally%2C%0Amany%20previous%20methods%2C%20which%20rely%20on%20single-pass%20prediction%2C%20may%20struggle%20with%0Alocal%20minima%20in%20complex%20scenarios.%20To%20mitigate%20these%20challenges%2C%20we%20introduce%20a%0Adiffusion%20matching%20model%20for%20robust%20correspondence%20construction.%20Our%20approach%0Atreats%20correspondence%20estimation%20as%20a%20denoising%20diffusion%20process%20within%20the%0Adoubly%20stochastic%20matrix%20space%2C%20which%20gradually%20denoises%20%28refines%29%20a%20doubly%0Astochastic%20matching%20matrix%20to%20the%20ground-truth%20one%20for%20high-quality%0Acorrespondence%20estimation.%20It%20involves%20a%20forward%20diffusion%20process%20that%0Agradually%20introduces%20Gaussian%20noise%20into%20the%20ground%20truth%20matching%20matrix%20and%20a%0Areverse%20denoising%20process%20that%20iteratively%20refines%20the%20noisy%20matching%20matrix.%0AIn%20particular%2C%20the%20feature%20extraction%20from%20the%20backbone%20occurs%20only%20once%20during%0Athe%20inference%20phase.%20Our%20lightweight%20denoising%20module%20utilizes%20the%20same%20feature%0Aat%20each%20reverse%20sampling%20step.%20Evaluation%20of%20our%20method%20on%20both%203D%20and%202D3D%0Aregistration%20tasks%20confirms%20its%20effectiveness.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/wuqianliang/Diff-Reg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Reg%2520v1%253A%2520Diffusion%2520Matching%2520Model%2520for%2520Registration%2520Problem%26entry.906535625%3DQianliang%2520Wu%2520and%2520Haobo%2520Jiang%2520and%2520Lei%2520Luo%2520and%2520Jun%2520Li%2520and%2520Yaqing%2520Ding%2520and%2520Jin%2520Xie%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Establishing%2520reliable%2520correspondences%2520is%2520essential%2520for%2520registration%2520tasks%250Asuch%2520as%25203D%2520and%25202D3D%2520registration.%2520Existing%2520methods%2520commonly%2520leverage%2520geometric%250Aor%2520semantic%2520point%2520features%2520to%2520generate%2520potential%2520correspondences.%2520However%252C%250Athese%2520features%2520may%2520face%2520challenges%2520such%2520as%2520large%2520deformation%252C%2520scale%250Ainconsistency%252C%2520and%2520ambiguous%2520matching%2520problems%2520%2528e.g.%252C%2520symmetry%2529.%2520Additionally%252C%250Amany%2520previous%2520methods%252C%2520which%2520rely%2520on%2520single-pass%2520prediction%252C%2520may%2520struggle%2520with%250Alocal%2520minima%2520in%2520complex%2520scenarios.%2520To%2520mitigate%2520these%2520challenges%252C%2520we%2520introduce%2520a%250Adiffusion%2520matching%2520model%2520for%2520robust%2520correspondence%2520construction.%2520Our%2520approach%250Atreats%2520correspondence%2520estimation%2520as%2520a%2520denoising%2520diffusion%2520process%2520within%2520the%250Adoubly%2520stochastic%2520matrix%2520space%252C%2520which%2520gradually%2520denoises%2520%2528refines%2529%2520a%2520doubly%250Astochastic%2520matching%2520matrix%2520to%2520the%2520ground-truth%2520one%2520for%2520high-quality%250Acorrespondence%2520estimation.%2520It%2520involves%2520a%2520forward%2520diffusion%2520process%2520that%250Agradually%2520introduces%2520Gaussian%2520noise%2520into%2520the%2520ground%2520truth%2520matching%2520matrix%2520and%2520a%250Areverse%2520denoising%2520process%2520that%2520iteratively%2520refines%2520the%2520noisy%2520matching%2520matrix.%250AIn%2520particular%252C%2520the%2520feature%2520extraction%2520from%2520the%2520backbone%2520occurs%2520only%2520once%2520during%250Athe%2520inference%2520phase.%2520Our%2520lightweight%2520denoising%2520module%2520utilizes%2520the%2520same%2520feature%250Aat%2520each%2520reverse%2520sampling%2520step.%2520Evaluation%2520of%2520our%2520method%2520on%2520both%25203D%2520and%25202D3D%250Aregistration%2520tasks%2520confirms%2520its%2520effectiveness.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/wuqianliang/Diff-Reg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Reg%20v1%3A%20Diffusion%20Matching%20Model%20for%20Registration%20Problem&entry.906535625=Qianliang%20Wu%20and%20Haobo%20Jiang%20and%20Lei%20Luo%20and%20Jun%20Li%20and%20Yaqing%20Ding%20and%20Jin%20Xie%20and%20Jian%20Yang&entry.1292438233=%20%20Establishing%20reliable%20correspondences%20is%20essential%20for%20registration%20tasks%0Asuch%20as%203D%20and%202D3D%20registration.%20Existing%20methods%20commonly%20leverage%20geometric%0Aor%20semantic%20point%20features%20to%20generate%20potential%20correspondences.%20However%2C%0Athese%20features%20may%20face%20challenges%20such%20as%20large%20deformation%2C%20scale%0Ainconsistency%2C%20and%20ambiguous%20matching%20problems%20%28e.g.%2C%20symmetry%29.%20Additionally%2C%0Amany%20previous%20methods%2C%20which%20rely%20on%20single-pass%20prediction%2C%20may%20struggle%20with%0Alocal%20minima%20in%20complex%20scenarios.%20To%20mitigate%20these%20challenges%2C%20we%20introduce%20a%0Adiffusion%20matching%20model%20for%20robust%20correspondence%20construction.%20Our%20approach%0Atreats%20correspondence%20estimation%20as%20a%20denoising%20diffusion%20process%20within%20the%0Adoubly%20stochastic%20matrix%20space%2C%20which%20gradually%20denoises%20%28refines%29%20a%20doubly%0Astochastic%20matching%20matrix%20to%20the%20ground-truth%20one%20for%20high-quality%0Acorrespondence%20estimation.%20It%20involves%20a%20forward%20diffusion%20process%20that%0Agradually%20introduces%20Gaussian%20noise%20into%20the%20ground%20truth%20matching%20matrix%20and%20a%0Areverse%20denoising%20process%20that%20iteratively%20refines%20the%20noisy%20matching%20matrix.%0AIn%20particular%2C%20the%20feature%20extraction%20from%20the%20backbone%20occurs%20only%20once%20during%0Athe%20inference%20phase.%20Our%20lightweight%20denoising%20module%20utilizes%20the%20same%20feature%0Aat%20each%20reverse%20sampling%20step.%20Evaluation%20of%20our%20method%20on%20both%203D%20and%202D3D%0Aregistration%20tasks%20confirms%20its%20effectiveness.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/wuqianliang/Diff-Reg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19919v2&entry.124074799=Read"},
{"title": "Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for\n  Binary Images Denoising", "author": "Luca Tirel and Ali Mohamed Ali and Hashim A. Hashim", "abstract": "  This paper introduces a novel approach to image denoising that leverages the\nadvantages of Generative Adversarial Networks (GANs). Specifically, we propose\na model that combines elements of the Pix2Pix model and the Wasserstein GAN\n(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks to\ncapitalize on the denoising capabilities of conditional GANs, as demonstrated\nin the Pix2Pix model, while mitigating the need for an exhaustive search for\noptimal hyperparameters that could potentially ruin the stability of the\nlearning process. In the proposed method, the GAN's generator is employed to\nproduce denoised images, harnessing the power of a conditional GAN for noise\nreduction. Simultaneously, the implementation of the Lipschitz continuity\nconstraint during updates, as featured in WGAN-GP, aids in reducing\nsusceptibility to mode collapse. This innovative design allows the proposed\nmodel to benefit from the strong points of both Pix2Pix and WGAN-GP, generating\nsuperior denoising results while ensuring training stability. Drawing on\nprevious work on image-to-image translation and GAN stabilization techniques,\nthe proposed research highlights the potential of GANs as a general-purpose\nsolution for denoising. The paper details the development and testing of this\nmodel, showcasing its effectiveness through numerical experiments. The dataset\nwas created by adding synthetic noise to clean images. Numerical results based\non real-world dataset validation underscore the efficacy of this approach in\nimage-denoising tasks, exhibiting significant enhancements over traditional\ntechniques. Notably, the proposed model demonstrates strong generalization\ncapabilities, performing effectively even when trained with synthetic noise.\n", "link": "http://arxiv.org/abs/2407.11865v1", "date": "2024-07-16", "relevancy": 2.2821, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.558}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising&body=Title%3A%20Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising%0AAuthor%3A%20Luca%20Tirel%20and%20Ali%20Mohamed%20Ali%20and%20Hashim%20A.%20Hashim%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20image%20denoising%20that%20leverages%20the%0Aadvantages%20of%20Generative%20Adversarial%20Networks%20%28GANs%29.%20Specifically%2C%20we%20propose%0Aa%20model%20that%20combines%20elements%20of%20the%20Pix2Pix%20model%20and%20the%20Wasserstein%20GAN%0A%28WGAN%29%20with%20Gradient%20Penalty%20%28WGAN-GP%29.%20This%20hybrid%20framework%20seeks%20to%0Acapitalize%20on%20the%20denoising%20capabilities%20of%20conditional%20GANs%2C%20as%20demonstrated%0Ain%20the%20Pix2Pix%20model%2C%20while%20mitigating%20the%20need%20for%20an%20exhaustive%20search%20for%0Aoptimal%20hyperparameters%20that%20could%20potentially%20ruin%20the%20stability%20of%20the%0Alearning%20process.%20In%20the%20proposed%20method%2C%20the%20GAN%27s%20generator%20is%20employed%20to%0Aproduce%20denoised%20images%2C%20harnessing%20the%20power%20of%20a%20conditional%20GAN%20for%20noise%0Areduction.%20Simultaneously%2C%20the%20implementation%20of%20the%20Lipschitz%20continuity%0Aconstraint%20during%20updates%2C%20as%20featured%20in%20WGAN-GP%2C%20aids%20in%20reducing%0Asusceptibility%20to%20mode%20collapse.%20This%20innovative%20design%20allows%20the%20proposed%0Amodel%20to%20benefit%20from%20the%20strong%20points%20of%20both%20Pix2Pix%20and%20WGAN-GP%2C%20generating%0Asuperior%20denoising%20results%20while%20ensuring%20training%20stability.%20Drawing%20on%0Aprevious%20work%20on%20image-to-image%20translation%20and%20GAN%20stabilization%20techniques%2C%0Athe%20proposed%20research%20highlights%20the%20potential%20of%20GANs%20as%20a%20general-purpose%0Asolution%20for%20denoising.%20The%20paper%20details%20the%20development%20and%20testing%20of%20this%0Amodel%2C%20showcasing%20its%20effectiveness%20through%20numerical%20experiments.%20The%20dataset%0Awas%20created%20by%20adding%20synthetic%20noise%20to%20clean%20images.%20Numerical%20results%20based%0Aon%20real-world%20dataset%20validation%20underscore%20the%20efficacy%20of%20this%20approach%20in%0Aimage-denoising%20tasks%2C%20exhibiting%20significant%20enhancements%20over%20traditional%0Atechniques.%20Notably%2C%20the%20proposed%20model%20demonstrates%20strong%20generalization%0Acapabilities%2C%20performing%20effectively%20even%20when%20trained%20with%20synthetic%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Hybrid%2520Integrated%2520Pix2Pix%2520and%2520WGAN%2520Model%2520with%2520Gradient%2520Penalty%2520for%250A%2520%2520Binary%2520Images%2520Denoising%26entry.906535625%3DLuca%2520Tirel%2520and%2520Ali%2520Mohamed%2520Ali%2520and%2520Hashim%2520A.%2520Hashim%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520image%2520denoising%2520that%2520leverages%2520the%250Aadvantages%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529.%2520Specifically%252C%2520we%2520propose%250Aa%2520model%2520that%2520combines%2520elements%2520of%2520the%2520Pix2Pix%2520model%2520and%2520the%2520Wasserstein%2520GAN%250A%2528WGAN%2529%2520with%2520Gradient%2520Penalty%2520%2528WGAN-GP%2529.%2520This%2520hybrid%2520framework%2520seeks%2520to%250Acapitalize%2520on%2520the%2520denoising%2520capabilities%2520of%2520conditional%2520GANs%252C%2520as%2520demonstrated%250Ain%2520the%2520Pix2Pix%2520model%252C%2520while%2520mitigating%2520the%2520need%2520for%2520an%2520exhaustive%2520search%2520for%250Aoptimal%2520hyperparameters%2520that%2520could%2520potentially%2520ruin%2520the%2520stability%2520of%2520the%250Alearning%2520process.%2520In%2520the%2520proposed%2520method%252C%2520the%2520GAN%2527s%2520generator%2520is%2520employed%2520to%250Aproduce%2520denoised%2520images%252C%2520harnessing%2520the%2520power%2520of%2520a%2520conditional%2520GAN%2520for%2520noise%250Areduction.%2520Simultaneously%252C%2520the%2520implementation%2520of%2520the%2520Lipschitz%2520continuity%250Aconstraint%2520during%2520updates%252C%2520as%2520featured%2520in%2520WGAN-GP%252C%2520aids%2520in%2520reducing%250Asusceptibility%2520to%2520mode%2520collapse.%2520This%2520innovative%2520design%2520allows%2520the%2520proposed%250Amodel%2520to%2520benefit%2520from%2520the%2520strong%2520points%2520of%2520both%2520Pix2Pix%2520and%2520WGAN-GP%252C%2520generating%250Asuperior%2520denoising%2520results%2520while%2520ensuring%2520training%2520stability.%2520Drawing%2520on%250Aprevious%2520work%2520on%2520image-to-image%2520translation%2520and%2520GAN%2520stabilization%2520techniques%252C%250Athe%2520proposed%2520research%2520highlights%2520the%2520potential%2520of%2520GANs%2520as%2520a%2520general-purpose%250Asolution%2520for%2520denoising.%2520The%2520paper%2520details%2520the%2520development%2520and%2520testing%2520of%2520this%250Amodel%252C%2520showcasing%2520its%2520effectiveness%2520through%2520numerical%2520experiments.%2520The%2520dataset%250Awas%2520created%2520by%2520adding%2520synthetic%2520noise%2520to%2520clean%2520images.%2520Numerical%2520results%2520based%250Aon%2520real-world%2520dataset%2520validation%2520underscore%2520the%2520efficacy%2520of%2520this%2520approach%2520in%250Aimage-denoising%2520tasks%252C%2520exhibiting%2520significant%2520enhancements%2520over%2520traditional%250Atechniques.%2520Notably%252C%2520the%2520proposed%2520model%2520demonstrates%2520strong%2520generalization%250Acapabilities%252C%2520performing%2520effectively%2520even%2520when%2520trained%2520with%2520synthetic%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Hybrid%20Integrated%20Pix2Pix%20and%20WGAN%20Model%20with%20Gradient%20Penalty%20for%0A%20%20Binary%20Images%20Denoising&entry.906535625=Luca%20Tirel%20and%20Ali%20Mohamed%20Ali%20and%20Hashim%20A.%20Hashim&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20image%20denoising%20that%20leverages%20the%0Aadvantages%20of%20Generative%20Adversarial%20Networks%20%28GANs%29.%20Specifically%2C%20we%20propose%0Aa%20model%20that%20combines%20elements%20of%20the%20Pix2Pix%20model%20and%20the%20Wasserstein%20GAN%0A%28WGAN%29%20with%20Gradient%20Penalty%20%28WGAN-GP%29.%20This%20hybrid%20framework%20seeks%20to%0Acapitalize%20on%20the%20denoising%20capabilities%20of%20conditional%20GANs%2C%20as%20demonstrated%0Ain%20the%20Pix2Pix%20model%2C%20while%20mitigating%20the%20need%20for%20an%20exhaustive%20search%20for%0Aoptimal%20hyperparameters%20that%20could%20potentially%20ruin%20the%20stability%20of%20the%0Alearning%20process.%20In%20the%20proposed%20method%2C%20the%20GAN%27s%20generator%20is%20employed%20to%0Aproduce%20denoised%20images%2C%20harnessing%20the%20power%20of%20a%20conditional%20GAN%20for%20noise%0Areduction.%20Simultaneously%2C%20the%20implementation%20of%20the%20Lipschitz%20continuity%0Aconstraint%20during%20updates%2C%20as%20featured%20in%20WGAN-GP%2C%20aids%20in%20reducing%0Asusceptibility%20to%20mode%20collapse.%20This%20innovative%20design%20allows%20the%20proposed%0Amodel%20to%20benefit%20from%20the%20strong%20points%20of%20both%20Pix2Pix%20and%20WGAN-GP%2C%20generating%0Asuperior%20denoising%20results%20while%20ensuring%20training%20stability.%20Drawing%20on%0Aprevious%20work%20on%20image-to-image%20translation%20and%20GAN%20stabilization%20techniques%2C%0Athe%20proposed%20research%20highlights%20the%20potential%20of%20GANs%20as%20a%20general-purpose%0Asolution%20for%20denoising.%20The%20paper%20details%20the%20development%20and%20testing%20of%20this%0Amodel%2C%20showcasing%20its%20effectiveness%20through%20numerical%20experiments.%20The%20dataset%0Awas%20created%20by%20adding%20synthetic%20noise%20to%20clean%20images.%20Numerical%20results%20based%0Aon%20real-world%20dataset%20validation%20underscore%20the%20efficacy%20of%20this%20approach%20in%0Aimage-denoising%20tasks%2C%20exhibiting%20significant%20enhancements%20over%20traditional%0Atechniques.%20Notably%2C%20the%20proposed%20model%20demonstrates%20strong%20generalization%0Acapabilities%2C%20performing%20effectively%20even%20when%20trained%20with%20synthetic%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11865v1&entry.124074799=Read"},
{"title": "SFPNet: Sparse Focal Point Network for Semantic Segmentation on General\n  LiDAR Point Clouds", "author": "Yanbo Wang and Wentao Zhao and Chuan Cao and Tianchen Deng and Jingchuan Wang and Weidong Chen", "abstract": "  Although LiDAR semantic segmentation advances rapidly, state-of-the-art\nmethods often incorporate specifically designed inductive bias derived from\nbenchmarks originating from mechanical spinning LiDAR. This can limit model\ngeneralizability to other kinds of LiDAR technologies and make hyperparameter\ntuning more complex. To tackle these issues, we propose a generalized framework\nto accommodate various types of LiDAR prevalent in the market by replacing\nwindow-attention with our sparse focal point modulation. Our SFPNet is capable\nof extracting multi-level contexts and dynamically aggregating them using a\ngate mechanism. By implementing a channel-wise information query, features that\nincorporate both local and global contexts are encoded. We also introduce a\nnovel large-scale hybrid-solid LiDAR semantic segmentation dataset for robotic\napplications. SFPNet demonstrates competitive performance on conventional\nbenchmarks derived from mechanical spinning LiDAR, while achieving\nstate-of-the-art results on benchmark derived from solid-state LiDAR.\nAdditionally, it outperforms existing methods on our novel dataset sourced from\nhybrid-solid LiDAR. Code and dataset are available at\nhttps://github.com/Cavendish518/SFPNet and https://www.semanticindustry.top.\n", "link": "http://arxiv.org/abs/2407.11569v1", "date": "2024-07-16", "relevancy": 2.2772, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5859}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5821}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFPNet%3A%20Sparse%20Focal%20Point%20Network%20for%20Semantic%20Segmentation%20on%20General%0A%20%20LiDAR%20Point%20Clouds&body=Title%3A%20SFPNet%3A%20Sparse%20Focal%20Point%20Network%20for%20Semantic%20Segmentation%20on%20General%0A%20%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Yanbo%20Wang%20and%20Wentao%20Zhao%20and%20Chuan%20Cao%20and%20Tianchen%20Deng%20and%20Jingchuan%20Wang%20and%20Weidong%20Chen%0AAbstract%3A%20%20%20Although%20LiDAR%20semantic%20segmentation%20advances%20rapidly%2C%20state-of-the-art%0Amethods%20often%20incorporate%20specifically%20designed%20inductive%20bias%20derived%20from%0Abenchmarks%20originating%20from%20mechanical%20spinning%20LiDAR.%20This%20can%20limit%20model%0Ageneralizability%20to%20other%20kinds%20of%20LiDAR%20technologies%20and%20make%20hyperparameter%0Atuning%20more%20complex.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20generalized%20framework%0Ato%20accommodate%20various%20types%20of%20LiDAR%20prevalent%20in%20the%20market%20by%20replacing%0Awindow-attention%20with%20our%20sparse%20focal%20point%20modulation.%20Our%20SFPNet%20is%20capable%0Aof%20extracting%20multi-level%20contexts%20and%20dynamically%20aggregating%20them%20using%20a%0Agate%20mechanism.%20By%20implementing%20a%20channel-wise%20information%20query%2C%20features%20that%0Aincorporate%20both%20local%20and%20global%20contexts%20are%20encoded.%20We%20also%20introduce%20a%0Anovel%20large-scale%20hybrid-solid%20LiDAR%20semantic%20segmentation%20dataset%20for%20robotic%0Aapplications.%20SFPNet%20demonstrates%20competitive%20performance%20on%20conventional%0Abenchmarks%20derived%20from%20mechanical%20spinning%20LiDAR%2C%20while%20achieving%0Astate-of-the-art%20results%20on%20benchmark%20derived%20from%20solid-state%20LiDAR.%0AAdditionally%2C%20it%20outperforms%20existing%20methods%20on%20our%20novel%20dataset%20sourced%20from%0Ahybrid-solid%20LiDAR.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Cavendish518/SFPNet%20and%20https%3A//www.semanticindustry.top.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFPNet%253A%2520Sparse%2520Focal%2520Point%2520Network%2520for%2520Semantic%2520Segmentation%2520on%2520General%250A%2520%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DYanbo%2520Wang%2520and%2520Wentao%2520Zhao%2520and%2520Chuan%2520Cao%2520and%2520Tianchen%2520Deng%2520and%2520Jingchuan%2520Wang%2520and%2520Weidong%2520Chen%26entry.1292438233%3D%2520%2520Although%2520LiDAR%2520semantic%2520segmentation%2520advances%2520rapidly%252C%2520state-of-the-art%250Amethods%2520often%2520incorporate%2520specifically%2520designed%2520inductive%2520bias%2520derived%2520from%250Abenchmarks%2520originating%2520from%2520mechanical%2520spinning%2520LiDAR.%2520This%2520can%2520limit%2520model%250Ageneralizability%2520to%2520other%2520kinds%2520of%2520LiDAR%2520technologies%2520and%2520make%2520hyperparameter%250Atuning%2520more%2520complex.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520generalized%2520framework%250Ato%2520accommodate%2520various%2520types%2520of%2520LiDAR%2520prevalent%2520in%2520the%2520market%2520by%2520replacing%250Awindow-attention%2520with%2520our%2520sparse%2520focal%2520point%2520modulation.%2520Our%2520SFPNet%2520is%2520capable%250Aof%2520extracting%2520multi-level%2520contexts%2520and%2520dynamically%2520aggregating%2520them%2520using%2520a%250Agate%2520mechanism.%2520By%2520implementing%2520a%2520channel-wise%2520information%2520query%252C%2520features%2520that%250Aincorporate%2520both%2520local%2520and%2520global%2520contexts%2520are%2520encoded.%2520We%2520also%2520introduce%2520a%250Anovel%2520large-scale%2520hybrid-solid%2520LiDAR%2520semantic%2520segmentation%2520dataset%2520for%2520robotic%250Aapplications.%2520SFPNet%2520demonstrates%2520competitive%2520performance%2520on%2520conventional%250Abenchmarks%2520derived%2520from%2520mechanical%2520spinning%2520LiDAR%252C%2520while%2520achieving%250Astate-of-the-art%2520results%2520on%2520benchmark%2520derived%2520from%2520solid-state%2520LiDAR.%250AAdditionally%252C%2520it%2520outperforms%2520existing%2520methods%2520on%2520our%2520novel%2520dataset%2520sourced%2520from%250Ahybrid-solid%2520LiDAR.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/Cavendish518/SFPNet%2520and%2520https%253A//www.semanticindustry.top.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFPNet%3A%20Sparse%20Focal%20Point%20Network%20for%20Semantic%20Segmentation%20on%20General%0A%20%20LiDAR%20Point%20Clouds&entry.906535625=Yanbo%20Wang%20and%20Wentao%20Zhao%20and%20Chuan%20Cao%20and%20Tianchen%20Deng%20and%20Jingchuan%20Wang%20and%20Weidong%20Chen&entry.1292438233=%20%20Although%20LiDAR%20semantic%20segmentation%20advances%20rapidly%2C%20state-of-the-art%0Amethods%20often%20incorporate%20specifically%20designed%20inductive%20bias%20derived%20from%0Abenchmarks%20originating%20from%20mechanical%20spinning%20LiDAR.%20This%20can%20limit%20model%0Ageneralizability%20to%20other%20kinds%20of%20LiDAR%20technologies%20and%20make%20hyperparameter%0Atuning%20more%20complex.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20generalized%20framework%0Ato%20accommodate%20various%20types%20of%20LiDAR%20prevalent%20in%20the%20market%20by%20replacing%0Awindow-attention%20with%20our%20sparse%20focal%20point%20modulation.%20Our%20SFPNet%20is%20capable%0Aof%20extracting%20multi-level%20contexts%20and%20dynamically%20aggregating%20them%20using%20a%0Agate%20mechanism.%20By%20implementing%20a%20channel-wise%20information%20query%2C%20features%20that%0Aincorporate%20both%20local%20and%20global%20contexts%20are%20encoded.%20We%20also%20introduce%20a%0Anovel%20large-scale%20hybrid-solid%20LiDAR%20semantic%20segmentation%20dataset%20for%20robotic%0Aapplications.%20SFPNet%20demonstrates%20competitive%20performance%20on%20conventional%0Abenchmarks%20derived%20from%20mechanical%20spinning%20LiDAR%2C%20while%20achieving%0Astate-of-the-art%20results%20on%20benchmark%20derived%20from%20solid-state%20LiDAR.%0AAdditionally%2C%20it%20outperforms%20existing%20methods%20on%20our%20novel%20dataset%20sourced%20from%0Ahybrid-solid%20LiDAR.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Cavendish518/SFPNet%20and%20https%3A//www.semanticindustry.top.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11569v1&entry.124074799=Read"},
{"title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular\n  Dynamic Human Modeling", "author": "Jaehyeok Kim and Dongyoon Wee and Dan Xu", "abstract": "  This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.\n", "link": "http://arxiv.org/abs/2407.11962v1", "date": "2024-07-16", "relevancy": 2.2715, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5887}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5534}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Oriented%20Compositional%20Neural%20Radiance%20Fields%20for%20Monocular%0A%20%20Dynamic%20Human%20Modeling&body=Title%3A%20Motion-Oriented%20Compositional%20Neural%20Radiance%20Fields%20for%20Monocular%0A%20%20Dynamic%20Human%20Modeling%0AAuthor%3A%20Jaehyeok%20Kim%20and%20Dongyoon%20Wee%20and%20Dan%20Xu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Motion-oriented%20Compositional%20Neural%20Radiance%20Fields%0A%28MoCo-NeRF%29%2C%20a%20framework%20designed%20to%20perform%20free-viewpoint%20rendering%20of%0Amonocular%20human%20videos%20via%20novel%20non-rigid%20motion%20modeling%20approach.%20In%20the%0Acontext%20of%20dynamic%20clothed%20humans%2C%20complex%20cloth%20dynamics%20generate%20non-rigid%0Amotions%20that%20are%20intrinsically%20distinct%20from%20skeletal%20articulations%20and%0Acritically%20important%20for%20the%20rendering%20quality.%20The%20conventional%20approach%0Amodels%20non-rigid%20motions%20as%20spatial%20%283D%29%20deviations%20in%20addition%20to%20skeletal%0Atransformations.%20However%2C%20it%20is%20either%20time-consuming%20or%20challenging%20to%20achieve%0Aoptimal%20quality%20due%20to%20its%20high%20learning%20complexity%20without%20a%20direct%0Asupervision.%20To%20target%20this%20problem%2C%20we%20propose%20a%20novel%20approach%20of%20modeling%0Anon-rigid%20motions%20as%20radiance%20residual%20fields%20to%20benefit%20from%20more%20direct%20color%0Asupervision%20in%20the%20rendering%20and%20utilize%20the%20rigid%20radiance%20fields%20as%20a%20prior%0Ato%20reduce%20the%20complexity%20of%20the%20learning%20process.%20Our%20approach%20utilizes%20a%0Asingle%20multiresolution%20hash%20encoding%20%28MHE%29%20to%20concurrently%20learn%20the%20canonical%0AT-pose%20representation%20from%20rigid%20skeletal%20motions%20and%20the%20radiance%20residual%0Afield%20for%20non-rigid%20motions.%20Additionally%2C%20to%20further%20improve%20both%20training%0Aefficiency%20and%20usability%2C%20we%20extend%20MoCo-NeRF%20to%20support%20simultaneous%20training%0Aof%20multiple%20subjects%20within%20a%20single%20framework%2C%20thanks%20to%20our%20effective%20design%0Afor%20modeling%20non-rigid%20motions.%20This%20scalability%20is%20achieved%20through%20the%0Aintegration%20of%20a%20global%20MHE%20and%20learnable%20identity%20codes%20in%20addition%20to%0Amultiple%20local%20MHEs.%20We%20present%20extensive%20results%20on%20ZJU-MoCap%20and%20MonoCap%2C%0Aclearly%20demonstrating%20state-of-the-art%20performance%20in%20both%20single-%20and%0Amulti-subject%20settings.%20The%20code%20and%20model%20will%20be%20made%20publicly%20available%20at%0Athe%20project%20page%3A%20https%3A//stevejaehyeok.github.io/publications/moco-nerf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Oriented%2520Compositional%2520Neural%2520Radiance%2520Fields%2520for%2520Monocular%250A%2520%2520Dynamic%2520Human%2520Modeling%26entry.906535625%3DJaehyeok%2520Kim%2520and%2520Dongyoon%2520Wee%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Motion-oriented%2520Compositional%2520Neural%2520Radiance%2520Fields%250A%2528MoCo-NeRF%2529%252C%2520a%2520framework%2520designed%2520to%2520perform%2520free-viewpoint%2520rendering%2520of%250Amonocular%2520human%2520videos%2520via%2520novel%2520non-rigid%2520motion%2520modeling%2520approach.%2520In%2520the%250Acontext%2520of%2520dynamic%2520clothed%2520humans%252C%2520complex%2520cloth%2520dynamics%2520generate%2520non-rigid%250Amotions%2520that%2520are%2520intrinsically%2520distinct%2520from%2520skeletal%2520articulations%2520and%250Acritically%2520important%2520for%2520the%2520rendering%2520quality.%2520The%2520conventional%2520approach%250Amodels%2520non-rigid%2520motions%2520as%2520spatial%2520%25283D%2529%2520deviations%2520in%2520addition%2520to%2520skeletal%250Atransformations.%2520However%252C%2520it%2520is%2520either%2520time-consuming%2520or%2520challenging%2520to%2520achieve%250Aoptimal%2520quality%2520due%2520to%2520its%2520high%2520learning%2520complexity%2520without%2520a%2520direct%250Asupervision.%2520To%2520target%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520approach%2520of%2520modeling%250Anon-rigid%2520motions%2520as%2520radiance%2520residual%2520fields%2520to%2520benefit%2520from%2520more%2520direct%2520color%250Asupervision%2520in%2520the%2520rendering%2520and%2520utilize%2520the%2520rigid%2520radiance%2520fields%2520as%2520a%2520prior%250Ato%2520reduce%2520the%2520complexity%2520of%2520the%2520learning%2520process.%2520Our%2520approach%2520utilizes%2520a%250Asingle%2520multiresolution%2520hash%2520encoding%2520%2528MHE%2529%2520to%2520concurrently%2520learn%2520the%2520canonical%250AT-pose%2520representation%2520from%2520rigid%2520skeletal%2520motions%2520and%2520the%2520radiance%2520residual%250Afield%2520for%2520non-rigid%2520motions.%2520Additionally%252C%2520to%2520further%2520improve%2520both%2520training%250Aefficiency%2520and%2520usability%252C%2520we%2520extend%2520MoCo-NeRF%2520to%2520support%2520simultaneous%2520training%250Aof%2520multiple%2520subjects%2520within%2520a%2520single%2520framework%252C%2520thanks%2520to%2520our%2520effective%2520design%250Afor%2520modeling%2520non-rigid%2520motions.%2520This%2520scalability%2520is%2520achieved%2520through%2520the%250Aintegration%2520of%2520a%2520global%2520MHE%2520and%2520learnable%2520identity%2520codes%2520in%2520addition%2520to%250Amultiple%2520local%2520MHEs.%2520We%2520present%2520extensive%2520results%2520on%2520ZJU-MoCap%2520and%2520MonoCap%252C%250Aclearly%2520demonstrating%2520state-of-the-art%2520performance%2520in%2520both%2520single-%2520and%250Amulti-subject%2520settings.%2520The%2520code%2520and%2520model%2520will%2520be%2520made%2520publicly%2520available%2520at%250Athe%2520project%2520page%253A%2520https%253A//stevejaehyeok.github.io/publications/moco-nerf.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Oriented%20Compositional%20Neural%20Radiance%20Fields%20for%20Monocular%0A%20%20Dynamic%20Human%20Modeling&entry.906535625=Jaehyeok%20Kim%20and%20Dongyoon%20Wee%20and%20Dan%20Xu&entry.1292438233=%20%20This%20paper%20introduces%20Motion-oriented%20Compositional%20Neural%20Radiance%20Fields%0A%28MoCo-NeRF%29%2C%20a%20framework%20designed%20to%20perform%20free-viewpoint%20rendering%20of%0Amonocular%20human%20videos%20via%20novel%20non-rigid%20motion%20modeling%20approach.%20In%20the%0Acontext%20of%20dynamic%20clothed%20humans%2C%20complex%20cloth%20dynamics%20generate%20non-rigid%0Amotions%20that%20are%20intrinsically%20distinct%20from%20skeletal%20articulations%20and%0Acritically%20important%20for%20the%20rendering%20quality.%20The%20conventional%20approach%0Amodels%20non-rigid%20motions%20as%20spatial%20%283D%29%20deviations%20in%20addition%20to%20skeletal%0Atransformations.%20However%2C%20it%20is%20either%20time-consuming%20or%20challenging%20to%20achieve%0Aoptimal%20quality%20due%20to%20its%20high%20learning%20complexity%20without%20a%20direct%0Asupervision.%20To%20target%20this%20problem%2C%20we%20propose%20a%20novel%20approach%20of%20modeling%0Anon-rigid%20motions%20as%20radiance%20residual%20fields%20to%20benefit%20from%20more%20direct%20color%0Asupervision%20in%20the%20rendering%20and%20utilize%20the%20rigid%20radiance%20fields%20as%20a%20prior%0Ato%20reduce%20the%20complexity%20of%20the%20learning%20process.%20Our%20approach%20utilizes%20a%0Asingle%20multiresolution%20hash%20encoding%20%28MHE%29%20to%20concurrently%20learn%20the%20canonical%0AT-pose%20representation%20from%20rigid%20skeletal%20motions%20and%20the%20radiance%20residual%0Afield%20for%20non-rigid%20motions.%20Additionally%2C%20to%20further%20improve%20both%20training%0Aefficiency%20and%20usability%2C%20we%20extend%20MoCo-NeRF%20to%20support%20simultaneous%20training%0Aof%20multiple%20subjects%20within%20a%20single%20framework%2C%20thanks%20to%20our%20effective%20design%0Afor%20modeling%20non-rigid%20motions.%20This%20scalability%20is%20achieved%20through%20the%0Aintegration%20of%20a%20global%20MHE%20and%20learnable%20identity%20codes%20in%20addition%20to%0Amultiple%20local%20MHEs.%20We%20present%20extensive%20results%20on%20ZJU-MoCap%20and%20MonoCap%2C%0Aclearly%20demonstrating%20state-of-the-art%20performance%20in%20both%20single-%20and%0Amulti-subject%20settings.%20The%20code%20and%20model%20will%20be%20made%20publicly%20available%20at%0Athe%20project%20page%3A%20https%3A//stevejaehyeok.github.io/publications/moco-nerf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11962v1&entry.124074799=Read"},
{"title": "DataDream: Few-shot Guided Dataset Generation", "author": "Jae Myung Kim and Jessica Bader and Stephan Alaniz and Cordelia Schmid and Zeynep Akata", "abstract": "  While text-to-image diffusion models have been shown to achieve\nstate-of-the-art results in image synthesis, they have yet to prove their\neffectiveness in downstream applications. Previous work has proposed to\ngenerate data for image classifier training given limited real data access.\nHowever, these methods struggle to generate in-distribution images or depict\nfine-grained features, thereby hindering the generalization of classification\nmodels trained on synthetic datasets. We propose DataDream, a framework for\nsynthesizing classification datasets that more faithfully represents the real\ndata distribution when guided by few-shot examples of the target classes.\nDataDream fine-tunes LoRA weights for the image generation model on the few\nreal images before generating the training data using the adapted model. We\nthen fine-tune LoRA weights for CLIP using the synthetic data to improve\ndownstream image classification over previous approaches on a large variety of\ndatasets. We demonstrate the efficacy of DataDream through extensive\nexperiments, surpassing state-of-the-art classification accuracy with few-shot\ndata across 7 out of 10 datasets, while being competitive on the other 3.\nAdditionally, we provide insights into the impact of various factors, such as\nthe number of real-shot and generated images as well as the fine-tuning compute\non model performance. The code is available at\nhttps://github.com/ExplainableML/DataDream.\n", "link": "http://arxiv.org/abs/2407.10910v2", "date": "2024-07-16", "relevancy": 2.2687, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation&body=Title%3A%20DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation%0AAuthor%3A%20Jae%20Myung%20Kim%20and%20Jessica%20Bader%20and%20Stephan%20Alaniz%20and%20Cordelia%20Schmid%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20While%20text-to-image%20diffusion%20models%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20results%20in%20image%20synthesis%2C%20they%20have%20yet%20to%20prove%20their%0Aeffectiveness%20in%20downstream%20applications.%20Previous%20work%20has%20proposed%20to%0Agenerate%20data%20for%20image%20classifier%20training%20given%20limited%20real%20data%20access.%0AHowever%2C%20these%20methods%20struggle%20to%20generate%20in-distribution%20images%20or%20depict%0Afine-grained%20features%2C%20thereby%20hindering%20the%20generalization%20of%20classification%0Amodels%20trained%20on%20synthetic%20datasets.%20We%20propose%20DataDream%2C%20a%20framework%20for%0Asynthesizing%20classification%20datasets%20that%20more%20faithfully%20represents%20the%20real%0Adata%20distribution%20when%20guided%20by%20few-shot%20examples%20of%20the%20target%20classes.%0ADataDream%20fine-tunes%20LoRA%20weights%20for%20the%20image%20generation%20model%20on%20the%20few%0Areal%20images%20before%20generating%20the%20training%20data%20using%20the%20adapted%20model.%20We%0Athen%20fine-tune%20LoRA%20weights%20for%20CLIP%20using%20the%20synthetic%20data%20to%20improve%0Adownstream%20image%20classification%20over%20previous%20approaches%20on%20a%20large%20variety%20of%0Adatasets.%20We%20demonstrate%20the%20efficacy%20of%20DataDream%20through%20extensive%0Aexperiments%2C%20surpassing%20state-of-the-art%20classification%20accuracy%20with%20few-shot%0Adata%20across%207%20out%20of%2010%20datasets%2C%20while%20being%20competitive%20on%20the%20other%203.%0AAdditionally%2C%20we%20provide%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%0Athe%20number%20of%20real-shot%20and%20generated%20images%20as%20well%20as%20the%20fine-tuning%20compute%0Aon%20model%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/DataDream.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataDream%253A%2520Few-shot%2520Guided%2520Dataset%2520Generation%26entry.906535625%3DJae%2520Myung%2520Kim%2520and%2520Jessica%2520Bader%2520and%2520Stephan%2520Alaniz%2520and%2520Cordelia%2520Schmid%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520While%2520text-to-image%2520diffusion%2520models%2520have%2520been%2520shown%2520to%2520achieve%250Astate-of-the-art%2520results%2520in%2520image%2520synthesis%252C%2520they%2520have%2520yet%2520to%2520prove%2520their%250Aeffectiveness%2520in%2520downstream%2520applications.%2520Previous%2520work%2520has%2520proposed%2520to%250Agenerate%2520data%2520for%2520image%2520classifier%2520training%2520given%2520limited%2520real%2520data%2520access.%250AHowever%252C%2520these%2520methods%2520struggle%2520to%2520generate%2520in-distribution%2520images%2520or%2520depict%250Afine-grained%2520features%252C%2520thereby%2520hindering%2520the%2520generalization%2520of%2520classification%250Amodels%2520trained%2520on%2520synthetic%2520datasets.%2520We%2520propose%2520DataDream%252C%2520a%2520framework%2520for%250Asynthesizing%2520classification%2520datasets%2520that%2520more%2520faithfully%2520represents%2520the%2520real%250Adata%2520distribution%2520when%2520guided%2520by%2520few-shot%2520examples%2520of%2520the%2520target%2520classes.%250ADataDream%2520fine-tunes%2520LoRA%2520weights%2520for%2520the%2520image%2520generation%2520model%2520on%2520the%2520few%250Areal%2520images%2520before%2520generating%2520the%2520training%2520data%2520using%2520the%2520adapted%2520model.%2520We%250Athen%2520fine-tune%2520LoRA%2520weights%2520for%2520CLIP%2520using%2520the%2520synthetic%2520data%2520to%2520improve%250Adownstream%2520image%2520classification%2520over%2520previous%2520approaches%2520on%2520a%2520large%2520variety%2520of%250Adatasets.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520DataDream%2520through%2520extensive%250Aexperiments%252C%2520surpassing%2520state-of-the-art%2520classification%2520accuracy%2520with%2520few-shot%250Adata%2520across%25207%2520out%2520of%252010%2520datasets%252C%2520while%2520being%2520competitive%2520on%2520the%2520other%25203.%250AAdditionally%252C%2520we%2520provide%2520insights%2520into%2520the%2520impact%2520of%2520various%2520factors%252C%2520such%2520as%250Athe%2520number%2520of%2520real-shot%2520and%2520generated%2520images%2520as%2520well%2520as%2520the%2520fine-tuning%2520compute%250Aon%2520model%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ExplainableML/DataDream.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataDream%3A%20Few-shot%20Guided%20Dataset%20Generation&entry.906535625=Jae%20Myung%20Kim%20and%20Jessica%20Bader%20and%20Stephan%20Alaniz%20and%20Cordelia%20Schmid%20and%20Zeynep%20Akata&entry.1292438233=%20%20While%20text-to-image%20diffusion%20models%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20results%20in%20image%20synthesis%2C%20they%20have%20yet%20to%20prove%20their%0Aeffectiveness%20in%20downstream%20applications.%20Previous%20work%20has%20proposed%20to%0Agenerate%20data%20for%20image%20classifier%20training%20given%20limited%20real%20data%20access.%0AHowever%2C%20these%20methods%20struggle%20to%20generate%20in-distribution%20images%20or%20depict%0Afine-grained%20features%2C%20thereby%20hindering%20the%20generalization%20of%20classification%0Amodels%20trained%20on%20synthetic%20datasets.%20We%20propose%20DataDream%2C%20a%20framework%20for%0Asynthesizing%20classification%20datasets%20that%20more%20faithfully%20represents%20the%20real%0Adata%20distribution%20when%20guided%20by%20few-shot%20examples%20of%20the%20target%20classes.%0ADataDream%20fine-tunes%20LoRA%20weights%20for%20the%20image%20generation%20model%20on%20the%20few%0Areal%20images%20before%20generating%20the%20training%20data%20using%20the%20adapted%20model.%20We%0Athen%20fine-tune%20LoRA%20weights%20for%20CLIP%20using%20the%20synthetic%20data%20to%20improve%0Adownstream%20image%20classification%20over%20previous%20approaches%20on%20a%20large%20variety%20of%0Adatasets.%20We%20demonstrate%20the%20efficacy%20of%20DataDream%20through%20extensive%0Aexperiments%2C%20surpassing%20state-of-the-art%20classification%20accuracy%20with%20few-shot%0Adata%20across%207%20out%20of%2010%20datasets%2C%20while%20being%20competitive%20on%20the%20other%203.%0AAdditionally%2C%20we%20provide%20insights%20into%20the%20impact%20of%20various%20factors%2C%20such%20as%0Athe%20number%20of%20real-shot%20and%20generated%20images%20as%20well%20as%20the%20fine-tuning%20compute%0Aon%20model%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/DataDream.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10910v2&entry.124074799=Read"},
{"title": "OmniRace: 6D Hand Pose Estimation for Intuitive Guidance of Racing Drone", "author": "Valerii Serpiva and Aleksey Fedoseev and Sausar Karaf and Ali Alridha Abdulkarim and Dzmitry Tsetserukou", "abstract": "  This paper presents the OmniRace approach to controlling a racing drone with\n6-degree of freedom (DoF) hand pose estimation and gesture recognition. To our\nknowledge, it is the first-ever technology that allows for low-level control of\nhigh-speed drones using gestures. OmniRace employs a gesture interface based on\ncomputer vision and a deep neural network to estimate a 6-DoF hand pose. The\nadvanced machine learning algorithm robustly interprets human gestures,\nallowing users to control drone motion intuitively. Real-time control of a\nracing drone demonstrates the effectiveness of the system, validating its\npotential to revolutionize drone racing and other applications. Experimental\nresults conducted in the Gazebo simulation environment revealed that OmniRace\nallows the users to complite the UAV race track significantly (by 25.1%) faster\nand to decrease the length of the test drone path (from 102.9 to 83.7 m). Users\npreferred the gesture interface for attractiveness (1.57 UEQ score), hedonic\nquality (1.56 UEQ score), and lower perceived temporal demand (32.0 score in\nNASA-TLX), while noting the high efficiency (0.75 UEQ score) and low physical\ndemand (19.0 score in NASA-TLX) of the baseline remote controller. The deep\nneural network attains an average accuracy of 99.75% when applied to both\nnormalized datasets and raw datasets. OmniRace can potentially change the way\nhumans interact with and navigate racing drones in dynamic and complex\nenvironments. The source code is available at\nhttps://github.com/SerValera/OmniRace.git.\n", "link": "http://arxiv.org/abs/2407.09841v2", "date": "2024-07-16", "relevancy": 2.2658, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5987}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniRace%3A%206D%20Hand%20Pose%20Estimation%20for%20Intuitive%20Guidance%20of%20Racing%20Drone&body=Title%3A%20OmniRace%3A%206D%20Hand%20Pose%20Estimation%20for%20Intuitive%20Guidance%20of%20Racing%20Drone%0AAuthor%3A%20Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Sausar%20Karaf%20and%20Ali%20Alridha%20Abdulkarim%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20OmniRace%20approach%20to%20controlling%20a%20racing%20drone%20with%0A6-degree%20of%20freedom%20%28DoF%29%20hand%20pose%20estimation%20and%20gesture%20recognition.%20To%20our%0Aknowledge%2C%20it%20is%20the%20first-ever%20technology%20that%20allows%20for%20low-level%20control%20of%0Ahigh-speed%20drones%20using%20gestures.%20OmniRace%20employs%20a%20gesture%20interface%20based%20on%0Acomputer%20vision%20and%20a%20deep%20neural%20network%20to%20estimate%20a%206-DoF%20hand%20pose.%20The%0Aadvanced%20machine%20learning%20algorithm%20robustly%20interprets%20human%20gestures%2C%0Aallowing%20users%20to%20control%20drone%20motion%20intuitively.%20Real-time%20control%20of%20a%0Aracing%20drone%20demonstrates%20the%20effectiveness%20of%20the%20system%2C%20validating%20its%0Apotential%20to%20revolutionize%20drone%20racing%20and%20other%20applications.%20Experimental%0Aresults%20conducted%20in%20the%20Gazebo%20simulation%20environment%20revealed%20that%20OmniRace%0Aallows%20the%20users%20to%20complite%20the%20UAV%20race%20track%20significantly%20%28by%2025.1%25%29%20faster%0Aand%20to%20decrease%20the%20length%20of%20the%20test%20drone%20path%20%28from%20102.9%20to%2083.7%20m%29.%20Users%0Apreferred%20the%20gesture%20interface%20for%20attractiveness%20%281.57%20UEQ%20score%29%2C%20hedonic%0Aquality%20%281.56%20UEQ%20score%29%2C%20and%20lower%20perceived%20temporal%20demand%20%2832.0%20score%20in%0ANASA-TLX%29%2C%20while%20noting%20the%20high%20efficiency%20%280.75%20UEQ%20score%29%20and%20low%20physical%0Ademand%20%2819.0%20score%20in%20NASA-TLX%29%20of%20the%20baseline%20remote%20controller.%20The%20deep%0Aneural%20network%20attains%20an%20average%20accuracy%20of%2099.75%25%20when%20applied%20to%20both%0Anormalized%20datasets%20and%20raw%20datasets.%20OmniRace%20can%20potentially%20change%20the%20way%0Ahumans%20interact%20with%20and%20navigate%20racing%20drones%20in%20dynamic%20and%20complex%0Aenvironments.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/SerValera/OmniRace.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniRace%253A%25206D%2520Hand%2520Pose%2520Estimation%2520for%2520Intuitive%2520Guidance%2520of%2520Racing%2520Drone%26entry.906535625%3DValerii%2520Serpiva%2520and%2520Aleksey%2520Fedoseev%2520and%2520Sausar%2520Karaf%2520and%2520Ali%2520Alridha%2520Abdulkarim%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520OmniRace%2520approach%2520to%2520controlling%2520a%2520racing%2520drone%2520with%250A6-degree%2520of%2520freedom%2520%2528DoF%2529%2520hand%2520pose%2520estimation%2520and%2520gesture%2520recognition.%2520To%2520our%250Aknowledge%252C%2520it%2520is%2520the%2520first-ever%2520technology%2520that%2520allows%2520for%2520low-level%2520control%2520of%250Ahigh-speed%2520drones%2520using%2520gestures.%2520OmniRace%2520employs%2520a%2520gesture%2520interface%2520based%2520on%250Acomputer%2520vision%2520and%2520a%2520deep%2520neural%2520network%2520to%2520estimate%2520a%25206-DoF%2520hand%2520pose.%2520The%250Aadvanced%2520machine%2520learning%2520algorithm%2520robustly%2520interprets%2520human%2520gestures%252C%250Aallowing%2520users%2520to%2520control%2520drone%2520motion%2520intuitively.%2520Real-time%2520control%2520of%2520a%250Aracing%2520drone%2520demonstrates%2520the%2520effectiveness%2520of%2520the%2520system%252C%2520validating%2520its%250Apotential%2520to%2520revolutionize%2520drone%2520racing%2520and%2520other%2520applications.%2520Experimental%250Aresults%2520conducted%2520in%2520the%2520Gazebo%2520simulation%2520environment%2520revealed%2520that%2520OmniRace%250Aallows%2520the%2520users%2520to%2520complite%2520the%2520UAV%2520race%2520track%2520significantly%2520%2528by%252025.1%2525%2529%2520faster%250Aand%2520to%2520decrease%2520the%2520length%2520of%2520the%2520test%2520drone%2520path%2520%2528from%2520102.9%2520to%252083.7%2520m%2529.%2520Users%250Apreferred%2520the%2520gesture%2520interface%2520for%2520attractiveness%2520%25281.57%2520UEQ%2520score%2529%252C%2520hedonic%250Aquality%2520%25281.56%2520UEQ%2520score%2529%252C%2520and%2520lower%2520perceived%2520temporal%2520demand%2520%252832.0%2520score%2520in%250ANASA-TLX%2529%252C%2520while%2520noting%2520the%2520high%2520efficiency%2520%25280.75%2520UEQ%2520score%2529%2520and%2520low%2520physical%250Ademand%2520%252819.0%2520score%2520in%2520NASA-TLX%2529%2520of%2520the%2520baseline%2520remote%2520controller.%2520The%2520deep%250Aneural%2520network%2520attains%2520an%2520average%2520accuracy%2520of%252099.75%2525%2520when%2520applied%2520to%2520both%250Anormalized%2520datasets%2520and%2520raw%2520datasets.%2520OmniRace%2520can%2520potentially%2520change%2520the%2520way%250Ahumans%2520interact%2520with%2520and%2520navigate%2520racing%2520drones%2520in%2520dynamic%2520and%2520complex%250Aenvironments.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/SerValera/OmniRace.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniRace%3A%206D%20Hand%20Pose%20Estimation%20for%20Intuitive%20Guidance%20of%20Racing%20Drone&entry.906535625=Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Sausar%20Karaf%20and%20Ali%20Alridha%20Abdulkarim%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20presents%20the%20OmniRace%20approach%20to%20controlling%20a%20racing%20drone%20with%0A6-degree%20of%20freedom%20%28DoF%29%20hand%20pose%20estimation%20and%20gesture%20recognition.%20To%20our%0Aknowledge%2C%20it%20is%20the%20first-ever%20technology%20that%20allows%20for%20low-level%20control%20of%0Ahigh-speed%20drones%20using%20gestures.%20OmniRace%20employs%20a%20gesture%20interface%20based%20on%0Acomputer%20vision%20and%20a%20deep%20neural%20network%20to%20estimate%20a%206-DoF%20hand%20pose.%20The%0Aadvanced%20machine%20learning%20algorithm%20robustly%20interprets%20human%20gestures%2C%0Aallowing%20users%20to%20control%20drone%20motion%20intuitively.%20Real-time%20control%20of%20a%0Aracing%20drone%20demonstrates%20the%20effectiveness%20of%20the%20system%2C%20validating%20its%0Apotential%20to%20revolutionize%20drone%20racing%20and%20other%20applications.%20Experimental%0Aresults%20conducted%20in%20the%20Gazebo%20simulation%20environment%20revealed%20that%20OmniRace%0Aallows%20the%20users%20to%20complite%20the%20UAV%20race%20track%20significantly%20%28by%2025.1%25%29%20faster%0Aand%20to%20decrease%20the%20length%20of%20the%20test%20drone%20path%20%28from%20102.9%20to%2083.7%20m%29.%20Users%0Apreferred%20the%20gesture%20interface%20for%20attractiveness%20%281.57%20UEQ%20score%29%2C%20hedonic%0Aquality%20%281.56%20UEQ%20score%29%2C%20and%20lower%20perceived%20temporal%20demand%20%2832.0%20score%20in%0ANASA-TLX%29%2C%20while%20noting%20the%20high%20efficiency%20%280.75%20UEQ%20score%29%20and%20low%20physical%0Ademand%20%2819.0%20score%20in%20NASA-TLX%29%20of%20the%20baseline%20remote%20controller.%20The%20deep%0Aneural%20network%20attains%20an%20average%20accuracy%20of%2099.75%25%20when%20applied%20to%20both%0Anormalized%20datasets%20and%20raw%20datasets.%20OmniRace%20can%20potentially%20change%20the%20way%0Ahumans%20interact%20with%20and%20navigate%20racing%20drones%20in%20dynamic%20and%20complex%0Aenvironments.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/SerValera/OmniRace.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09841v2&entry.124074799=Read"},
{"title": "Hierarchical Separable Video Transformer for Snapshot Compressive\n  Imaging", "author": "Ping Wang and Yulun Zhang and Lishun Wang and Xin Yuan", "abstract": "  Transformers have achieved the state-of-the-art performance on solving the\ninverse problem of Snapshot Compressive Imaging (SCI) for video, whose\nill-posedness is rooted in the mixed degradation of spatial masking and\ntemporal aliasing. However, previous Transformers lack an insight into the\ndegradation and thus have limited performance and efficiency. In this work, we\ntailor an efficient reconstruction architecture without temporal aggregation in\nearly layers and Hierarchical Separable Video Transformer (HiSViT) as building\nblock. HiSViT is built by multiple groups of Cross-Scale Separable Multi-head\nSelf-Attention (CSS-MSA) and Gated Self-Modulated Feed-Forward Network\n(GSM-FFN) with dense connections, each of which is conducted within a separate\nchannel portions at a different scale, for multi-scale interactions and\nlong-range modeling. By separating spatial operations from temporal ones,\nCSS-MSA introduces an inductive bias of paying more attention within frames\ninstead of between frames while saving computational overheads. GSM-FFN is\ndesign to enhance the locality via gated mechanism and factorized\nspatial-temporal convolutions. Extensive experiments demonstrate that our\nmethod outperforms previous methods by $>\\!0.5$ dB with comparable or fewer\ncomplexity and parameters. The source codes and pretrained models are released\nat https://github.com/pwangcs/HiSViT.\n", "link": "http://arxiv.org/abs/2407.11946v1", "date": "2024-07-16", "relevancy": 2.2635, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6123}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5633}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Separable%20Video%20Transformer%20for%20Snapshot%20Compressive%0A%20%20Imaging&body=Title%3A%20Hierarchical%20Separable%20Video%20Transformer%20for%20Snapshot%20Compressive%0A%20%20Imaging%0AAuthor%3A%20Ping%20Wang%20and%20Yulun%20Zhang%20and%20Lishun%20Wang%20and%20Xin%20Yuan%0AAbstract%3A%20%20%20Transformers%20have%20achieved%20the%20state-of-the-art%20performance%20on%20solving%20the%0Ainverse%20problem%20of%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20for%20video%2C%20whose%0Aill-posedness%20is%20rooted%20in%20the%20mixed%20degradation%20of%20spatial%20masking%20and%0Atemporal%20aliasing.%20However%2C%20previous%20Transformers%20lack%20an%20insight%20into%20the%0Adegradation%20and%20thus%20have%20limited%20performance%20and%20efficiency.%20In%20this%20work%2C%20we%0Atailor%20an%20efficient%20reconstruction%20architecture%20without%20temporal%20aggregation%20in%0Aearly%20layers%20and%20Hierarchical%20Separable%20Video%20Transformer%20%28HiSViT%29%20as%20building%0Ablock.%20HiSViT%20is%20built%20by%20multiple%20groups%20of%20Cross-Scale%20Separable%20Multi-head%0ASelf-Attention%20%28CSS-MSA%29%20and%20Gated%20Self-Modulated%20Feed-Forward%20Network%0A%28GSM-FFN%29%20with%20dense%20connections%2C%20each%20of%20which%20is%20conducted%20within%20a%20separate%0Achannel%20portions%20at%20a%20different%20scale%2C%20for%20multi-scale%20interactions%20and%0Along-range%20modeling.%20By%20separating%20spatial%20operations%20from%20temporal%20ones%2C%0ACSS-MSA%20introduces%20an%20inductive%20bias%20of%20paying%20more%20attention%20within%20frames%0Ainstead%20of%20between%20frames%20while%20saving%20computational%20overheads.%20GSM-FFN%20is%0Adesign%20to%20enhance%20the%20locality%20via%20gated%20mechanism%20and%20factorized%0Aspatial-temporal%20convolutions.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20previous%20methods%20by%20%24%3E%5C%210.5%24%20dB%20with%20comparable%20or%20fewer%0Acomplexity%20and%20parameters.%20The%20source%20codes%20and%20pretrained%20models%20are%20released%0Aat%20https%3A//github.com/pwangcs/HiSViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Separable%2520Video%2520Transformer%2520for%2520Snapshot%2520Compressive%250A%2520%2520Imaging%26entry.906535625%3DPing%2520Wang%2520and%2520Yulun%2520Zhang%2520and%2520Lishun%2520Wang%2520and%2520Xin%2520Yuan%26entry.1292438233%3D%2520%2520Transformers%2520have%2520achieved%2520the%2520state-of-the-art%2520performance%2520on%2520solving%2520the%250Ainverse%2520problem%2520of%2520Snapshot%2520Compressive%2520Imaging%2520%2528SCI%2529%2520for%2520video%252C%2520whose%250Aill-posedness%2520is%2520rooted%2520in%2520the%2520mixed%2520degradation%2520of%2520spatial%2520masking%2520and%250Atemporal%2520aliasing.%2520However%252C%2520previous%2520Transformers%2520lack%2520an%2520insight%2520into%2520the%250Adegradation%2520and%2520thus%2520have%2520limited%2520performance%2520and%2520efficiency.%2520In%2520this%2520work%252C%2520we%250Atailor%2520an%2520efficient%2520reconstruction%2520architecture%2520without%2520temporal%2520aggregation%2520in%250Aearly%2520layers%2520and%2520Hierarchical%2520Separable%2520Video%2520Transformer%2520%2528HiSViT%2529%2520as%2520building%250Ablock.%2520HiSViT%2520is%2520built%2520by%2520multiple%2520groups%2520of%2520Cross-Scale%2520Separable%2520Multi-head%250ASelf-Attention%2520%2528CSS-MSA%2529%2520and%2520Gated%2520Self-Modulated%2520Feed-Forward%2520Network%250A%2528GSM-FFN%2529%2520with%2520dense%2520connections%252C%2520each%2520of%2520which%2520is%2520conducted%2520within%2520a%2520separate%250Achannel%2520portions%2520at%2520a%2520different%2520scale%252C%2520for%2520multi-scale%2520interactions%2520and%250Along-range%2520modeling.%2520By%2520separating%2520spatial%2520operations%2520from%2520temporal%2520ones%252C%250ACSS-MSA%2520introduces%2520an%2520inductive%2520bias%2520of%2520paying%2520more%2520attention%2520within%2520frames%250Ainstead%2520of%2520between%2520frames%2520while%2520saving%2520computational%2520overheads.%2520GSM-FFN%2520is%250Adesign%2520to%2520enhance%2520the%2520locality%2520via%2520gated%2520mechanism%2520and%2520factorized%250Aspatial-temporal%2520convolutions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520previous%2520methods%2520by%2520%2524%253E%255C%25210.5%2524%2520dB%2520with%2520comparable%2520or%2520fewer%250Acomplexity%2520and%2520parameters.%2520The%2520source%2520codes%2520and%2520pretrained%2520models%2520are%2520released%250Aat%2520https%253A//github.com/pwangcs/HiSViT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Separable%20Video%20Transformer%20for%20Snapshot%20Compressive%0A%20%20Imaging&entry.906535625=Ping%20Wang%20and%20Yulun%20Zhang%20and%20Lishun%20Wang%20and%20Xin%20Yuan&entry.1292438233=%20%20Transformers%20have%20achieved%20the%20state-of-the-art%20performance%20on%20solving%20the%0Ainverse%20problem%20of%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20for%20video%2C%20whose%0Aill-posedness%20is%20rooted%20in%20the%20mixed%20degradation%20of%20spatial%20masking%20and%0Atemporal%20aliasing.%20However%2C%20previous%20Transformers%20lack%20an%20insight%20into%20the%0Adegradation%20and%20thus%20have%20limited%20performance%20and%20efficiency.%20In%20this%20work%2C%20we%0Atailor%20an%20efficient%20reconstruction%20architecture%20without%20temporal%20aggregation%20in%0Aearly%20layers%20and%20Hierarchical%20Separable%20Video%20Transformer%20%28HiSViT%29%20as%20building%0Ablock.%20HiSViT%20is%20built%20by%20multiple%20groups%20of%20Cross-Scale%20Separable%20Multi-head%0ASelf-Attention%20%28CSS-MSA%29%20and%20Gated%20Self-Modulated%20Feed-Forward%20Network%0A%28GSM-FFN%29%20with%20dense%20connections%2C%20each%20of%20which%20is%20conducted%20within%20a%20separate%0Achannel%20portions%20at%20a%20different%20scale%2C%20for%20multi-scale%20interactions%20and%0Along-range%20modeling.%20By%20separating%20spatial%20operations%20from%20temporal%20ones%2C%0ACSS-MSA%20introduces%20an%20inductive%20bias%20of%20paying%20more%20attention%20within%20frames%0Ainstead%20of%20between%20frames%20while%20saving%20computational%20overheads.%20GSM-FFN%20is%0Adesign%20to%20enhance%20the%20locality%20via%20gated%20mechanism%20and%20factorized%0Aspatial-temporal%20convolutions.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20previous%20methods%20by%20%24%3E%5C%210.5%24%20dB%20with%20comparable%20or%20fewer%0Acomplexity%20and%20parameters.%20The%20source%20codes%20and%20pretrained%20models%20are%20released%0Aat%20https%3A//github.com/pwangcs/HiSViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11946v1&entry.124074799=Read"},
{"title": "Dynamic Dimension Wrapping (DDW) Algorithm: A Novel Approach for\n  Efficient Cross-Dimensional Search in Dynamic Multidimensional Spaces", "author": "Dongnan Jin and Yali Liu and Qiuzhi Song and Xunju Ma and Yue Liu and Dehao Wu", "abstract": "  In the real world, as the complexity of optimization problems continues to\nincrease, there is an urgent need to research more efficient optimization\nmethods. Current optimization algorithms excel in solving problems with a fixed\nnumber of dimensions. However, their efficiency in searching dynamic\nmulti-dimensional spaces is unsatisfactory. In response to the challenge of\ncross-dimensional search in multi-dimensional spaces with varying numbers of\ndimensions, this study proposes a new optimization algorithm-Dynamic Dimension\nWrapping (DDW) algorithm. Firstly, by utilizing the Dynamic Time Warping (DTW)\nalgorithm and Euclidean distance, a mapping relationship between different time\nseries across dimensions is established, thus creating a fitness function\nsuitable for dimensionally dynamic multi-dimensional space. Additionally, DDW\nintroduces a novel, more efficient cross-dimensional search mechanism for\ndynamic multidimensional spaces. Finally, through comparative tests with 31\noptimization algorithms in dynamic multidimensional space search, the results\ndemonstrate that DDW exhibits outstanding search efficiency and provides search\nresults closest to the actual optimal solution.\n", "link": "http://arxiv.org/abs/2407.11626v1", "date": "2024-07-16", "relevancy": 2.2603, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4611}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Dimension%20Wrapping%20%28DDW%29%20Algorithm%3A%20A%20Novel%20Approach%20for%0A%20%20Efficient%20Cross-Dimensional%20Search%20in%20Dynamic%20Multidimensional%20Spaces&body=Title%3A%20Dynamic%20Dimension%20Wrapping%20%28DDW%29%20Algorithm%3A%20A%20Novel%20Approach%20for%0A%20%20Efficient%20Cross-Dimensional%20Search%20in%20Dynamic%20Multidimensional%20Spaces%0AAuthor%3A%20Dongnan%20Jin%20and%20Yali%20Liu%20and%20Qiuzhi%20Song%20and%20Xunju%20Ma%20and%20Yue%20Liu%20and%20Dehao%20Wu%0AAbstract%3A%20%20%20In%20the%20real%20world%2C%20as%20the%20complexity%20of%20optimization%20problems%20continues%20to%0Aincrease%2C%20there%20is%20an%20urgent%20need%20to%20research%20more%20efficient%20optimization%0Amethods.%20Current%20optimization%20algorithms%20excel%20in%20solving%20problems%20with%20a%20fixed%0Anumber%20of%20dimensions.%20However%2C%20their%20efficiency%20in%20searching%20dynamic%0Amulti-dimensional%20spaces%20is%20unsatisfactory.%20In%20response%20to%20the%20challenge%20of%0Across-dimensional%20search%20in%20multi-dimensional%20spaces%20with%20varying%20numbers%20of%0Adimensions%2C%20this%20study%20proposes%20a%20new%20optimization%20algorithm-Dynamic%20Dimension%0AWrapping%20%28DDW%29%20algorithm.%20Firstly%2C%20by%20utilizing%20the%20Dynamic%20Time%20Warping%20%28DTW%29%0Aalgorithm%20and%20Euclidean%20distance%2C%20a%20mapping%20relationship%20between%20different%20time%0Aseries%20across%20dimensions%20is%20established%2C%20thus%20creating%20a%20fitness%20function%0Asuitable%20for%20dimensionally%20dynamic%20multi-dimensional%20space.%20Additionally%2C%20DDW%0Aintroduces%20a%20novel%2C%20more%20efficient%20cross-dimensional%20search%20mechanism%20for%0Adynamic%20multidimensional%20spaces.%20Finally%2C%20through%20comparative%20tests%20with%2031%0Aoptimization%20algorithms%20in%20dynamic%20multidimensional%20space%20search%2C%20the%20results%0Ademonstrate%20that%20DDW%20exhibits%20outstanding%20search%20efficiency%20and%20provides%20search%0Aresults%20closest%20to%20the%20actual%20optimal%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Dimension%2520Wrapping%2520%2528DDW%2529%2520Algorithm%253A%2520A%2520Novel%2520Approach%2520for%250A%2520%2520Efficient%2520Cross-Dimensional%2520Search%2520in%2520Dynamic%2520Multidimensional%2520Spaces%26entry.906535625%3DDongnan%2520Jin%2520and%2520Yali%2520Liu%2520and%2520Qiuzhi%2520Song%2520and%2520Xunju%2520Ma%2520and%2520Yue%2520Liu%2520and%2520Dehao%2520Wu%26entry.1292438233%3D%2520%2520In%2520the%2520real%2520world%252C%2520as%2520the%2520complexity%2520of%2520optimization%2520problems%2520continues%2520to%250Aincrease%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%2520research%2520more%2520efficient%2520optimization%250Amethods.%2520Current%2520optimization%2520algorithms%2520excel%2520in%2520solving%2520problems%2520with%2520a%2520fixed%250Anumber%2520of%2520dimensions.%2520However%252C%2520their%2520efficiency%2520in%2520searching%2520dynamic%250Amulti-dimensional%2520spaces%2520is%2520unsatisfactory.%2520In%2520response%2520to%2520the%2520challenge%2520of%250Across-dimensional%2520search%2520in%2520multi-dimensional%2520spaces%2520with%2520varying%2520numbers%2520of%250Adimensions%252C%2520this%2520study%2520proposes%2520a%2520new%2520optimization%2520algorithm-Dynamic%2520Dimension%250AWrapping%2520%2528DDW%2529%2520algorithm.%2520Firstly%252C%2520by%2520utilizing%2520the%2520Dynamic%2520Time%2520Warping%2520%2528DTW%2529%250Aalgorithm%2520and%2520Euclidean%2520distance%252C%2520a%2520mapping%2520relationship%2520between%2520different%2520time%250Aseries%2520across%2520dimensions%2520is%2520established%252C%2520thus%2520creating%2520a%2520fitness%2520function%250Asuitable%2520for%2520dimensionally%2520dynamic%2520multi-dimensional%2520space.%2520Additionally%252C%2520DDW%250Aintroduces%2520a%2520novel%252C%2520more%2520efficient%2520cross-dimensional%2520search%2520mechanism%2520for%250Adynamic%2520multidimensional%2520spaces.%2520Finally%252C%2520through%2520comparative%2520tests%2520with%252031%250Aoptimization%2520algorithms%2520in%2520dynamic%2520multidimensional%2520space%2520search%252C%2520the%2520results%250Ademonstrate%2520that%2520DDW%2520exhibits%2520outstanding%2520search%2520efficiency%2520and%2520provides%2520search%250Aresults%2520closest%2520to%2520the%2520actual%2520optimal%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Dimension%20Wrapping%20%28DDW%29%20Algorithm%3A%20A%20Novel%20Approach%20for%0A%20%20Efficient%20Cross-Dimensional%20Search%20in%20Dynamic%20Multidimensional%20Spaces&entry.906535625=Dongnan%20Jin%20and%20Yali%20Liu%20and%20Qiuzhi%20Song%20and%20Xunju%20Ma%20and%20Yue%20Liu%20and%20Dehao%20Wu&entry.1292438233=%20%20In%20the%20real%20world%2C%20as%20the%20complexity%20of%20optimization%20problems%20continues%20to%0Aincrease%2C%20there%20is%20an%20urgent%20need%20to%20research%20more%20efficient%20optimization%0Amethods.%20Current%20optimization%20algorithms%20excel%20in%20solving%20problems%20with%20a%20fixed%0Anumber%20of%20dimensions.%20However%2C%20their%20efficiency%20in%20searching%20dynamic%0Amulti-dimensional%20spaces%20is%20unsatisfactory.%20In%20response%20to%20the%20challenge%20of%0Across-dimensional%20search%20in%20multi-dimensional%20spaces%20with%20varying%20numbers%20of%0Adimensions%2C%20this%20study%20proposes%20a%20new%20optimization%20algorithm-Dynamic%20Dimension%0AWrapping%20%28DDW%29%20algorithm.%20Firstly%2C%20by%20utilizing%20the%20Dynamic%20Time%20Warping%20%28DTW%29%0Aalgorithm%20and%20Euclidean%20distance%2C%20a%20mapping%20relationship%20between%20different%20time%0Aseries%20across%20dimensions%20is%20established%2C%20thus%20creating%20a%20fitness%20function%0Asuitable%20for%20dimensionally%20dynamic%20multi-dimensional%20space.%20Additionally%2C%20DDW%0Aintroduces%20a%20novel%2C%20more%20efficient%20cross-dimensional%20search%20mechanism%20for%0Adynamic%20multidimensional%20spaces.%20Finally%2C%20through%20comparative%20tests%20with%2031%0Aoptimization%20algorithms%20in%20dynamic%20multidimensional%20space%20search%2C%20the%20results%0Ademonstrate%20that%20DDW%20exhibits%20outstanding%20search%20efficiency%20and%20provides%20search%0Aresults%20closest%20to%20the%20actual%20optimal%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11626v1&entry.124074799=Read"},
{"title": "Decomposition Betters Tracking Everything Everywhere", "author": "Rui Li and Dong Liu", "abstract": "  Recent studies on motion estimation have advocated an optimized motion\nrepresentation that is globally consistent across the entire video, preferably\nfor every pixel. This is challenging as a uniform representation may not\naccount for the complex and diverse motion and appearance of natural videos. We\naddress this problem and propose a new test-time optimization method, named\nDecoMotion, for estimating per-pixel and long-range motion. DecoMotion\nexplicitly decomposes video content into static scenes and dynamic objects,\neither of which uses a quasi-3D canonical volume to represent. DecoMotion\nseparately coordinates the transformations between local and canonical spaces,\nfacilitating an affine transformation for the static scene that corresponds to\ncamera motion. For the dynamic volume, DecoMotion leverages discriminative and\ntemporally consistent features to rectify the non-rigid transformation. The two\nvolumes are finally fused to fully represent motion and appearance. This\ndivide-and-conquer strategy leads to more robust tracking through occlusions\nand deformations and meanwhile obtains decomposed appearances. We conduct\nevaluations on the TAP-Vid benchmark. The results demonstrate our method boosts\nthe point-tracking accuracy by a large margin and performs on par with some\nstate-of-the-art dedicated point-tracking solutions.\n", "link": "http://arxiv.org/abs/2407.06531v2", "date": "2024-07-16", "relevancy": 2.2568, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5919}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5706}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposition%20Betters%20Tracking%20Everything%20Everywhere&body=Title%3A%20Decomposition%20Betters%20Tracking%20Everything%20Everywhere%0AAuthor%3A%20Rui%20Li%20and%20Dong%20Liu%0AAbstract%3A%20%20%20Recent%20studies%20on%20motion%20estimation%20have%20advocated%20an%20optimized%20motion%0Arepresentation%20that%20is%20globally%20consistent%20across%20the%20entire%20video%2C%20preferably%0Afor%20every%20pixel.%20This%20is%20challenging%20as%20a%20uniform%20representation%20may%20not%0Aaccount%20for%20the%20complex%20and%20diverse%20motion%20and%20appearance%20of%20natural%20videos.%20We%0Aaddress%20this%20problem%20and%20propose%20a%20new%20test-time%20optimization%20method%2C%20named%0ADecoMotion%2C%20for%20estimating%20per-pixel%20and%20long-range%20motion.%20DecoMotion%0Aexplicitly%20decomposes%20video%20content%20into%20static%20scenes%20and%20dynamic%20objects%2C%0Aeither%20of%20which%20uses%20a%20quasi-3D%20canonical%20volume%20to%20represent.%20DecoMotion%0Aseparately%20coordinates%20the%20transformations%20between%20local%20and%20canonical%20spaces%2C%0Afacilitating%20an%20affine%20transformation%20for%20the%20static%20scene%20that%20corresponds%20to%0Acamera%20motion.%20For%20the%20dynamic%20volume%2C%20DecoMotion%20leverages%20discriminative%20and%0Atemporally%20consistent%20features%20to%20rectify%20the%20non-rigid%20transformation.%20The%20two%0Avolumes%20are%20finally%20fused%20to%20fully%20represent%20motion%20and%20appearance.%20This%0Adivide-and-conquer%20strategy%20leads%20to%20more%20robust%20tracking%20through%20occlusions%0Aand%20deformations%20and%20meanwhile%20obtains%20decomposed%20appearances.%20We%20conduct%0Aevaluations%20on%20the%20TAP-Vid%20benchmark.%20The%20results%20demonstrate%20our%20method%20boosts%0Athe%20point-tracking%20accuracy%20by%20a%20large%20margin%20and%20performs%20on%20par%20with%20some%0Astate-of-the-art%20dedicated%20point-tracking%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposition%2520Betters%2520Tracking%2520Everything%2520Everywhere%26entry.906535625%3DRui%2520Li%2520and%2520Dong%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520on%2520motion%2520estimation%2520have%2520advocated%2520an%2520optimized%2520motion%250Arepresentation%2520that%2520is%2520globally%2520consistent%2520across%2520the%2520entire%2520video%252C%2520preferably%250Afor%2520every%2520pixel.%2520This%2520is%2520challenging%2520as%2520a%2520uniform%2520representation%2520may%2520not%250Aaccount%2520for%2520the%2520complex%2520and%2520diverse%2520motion%2520and%2520appearance%2520of%2520natural%2520videos.%2520We%250Aaddress%2520this%2520problem%2520and%2520propose%2520a%2520new%2520test-time%2520optimization%2520method%252C%2520named%250ADecoMotion%252C%2520for%2520estimating%2520per-pixel%2520and%2520long-range%2520motion.%2520DecoMotion%250Aexplicitly%2520decomposes%2520video%2520content%2520into%2520static%2520scenes%2520and%2520dynamic%2520objects%252C%250Aeither%2520of%2520which%2520uses%2520a%2520quasi-3D%2520canonical%2520volume%2520to%2520represent.%2520DecoMotion%250Aseparately%2520coordinates%2520the%2520transformations%2520between%2520local%2520and%2520canonical%2520spaces%252C%250Afacilitating%2520an%2520affine%2520transformation%2520for%2520the%2520static%2520scene%2520that%2520corresponds%2520to%250Acamera%2520motion.%2520For%2520the%2520dynamic%2520volume%252C%2520DecoMotion%2520leverages%2520discriminative%2520and%250Atemporally%2520consistent%2520features%2520to%2520rectify%2520the%2520non-rigid%2520transformation.%2520The%2520two%250Avolumes%2520are%2520finally%2520fused%2520to%2520fully%2520represent%2520motion%2520and%2520appearance.%2520This%250Adivide-and-conquer%2520strategy%2520leads%2520to%2520more%2520robust%2520tracking%2520through%2520occlusions%250Aand%2520deformations%2520and%2520meanwhile%2520obtains%2520decomposed%2520appearances.%2520We%2520conduct%250Aevaluations%2520on%2520the%2520TAP-Vid%2520benchmark.%2520The%2520results%2520demonstrate%2520our%2520method%2520boosts%250Athe%2520point-tracking%2520accuracy%2520by%2520a%2520large%2520margin%2520and%2520performs%2520on%2520par%2520with%2520some%250Astate-of-the-art%2520dedicated%2520point-tracking%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposition%20Betters%20Tracking%20Everything%20Everywhere&entry.906535625=Rui%20Li%20and%20Dong%20Liu&entry.1292438233=%20%20Recent%20studies%20on%20motion%20estimation%20have%20advocated%20an%20optimized%20motion%0Arepresentation%20that%20is%20globally%20consistent%20across%20the%20entire%20video%2C%20preferably%0Afor%20every%20pixel.%20This%20is%20challenging%20as%20a%20uniform%20representation%20may%20not%0Aaccount%20for%20the%20complex%20and%20diverse%20motion%20and%20appearance%20of%20natural%20videos.%20We%0Aaddress%20this%20problem%20and%20propose%20a%20new%20test-time%20optimization%20method%2C%20named%0ADecoMotion%2C%20for%20estimating%20per-pixel%20and%20long-range%20motion.%20DecoMotion%0Aexplicitly%20decomposes%20video%20content%20into%20static%20scenes%20and%20dynamic%20objects%2C%0Aeither%20of%20which%20uses%20a%20quasi-3D%20canonical%20volume%20to%20represent.%20DecoMotion%0Aseparately%20coordinates%20the%20transformations%20between%20local%20and%20canonical%20spaces%2C%0Afacilitating%20an%20affine%20transformation%20for%20the%20static%20scene%20that%20corresponds%20to%0Acamera%20motion.%20For%20the%20dynamic%20volume%2C%20DecoMotion%20leverages%20discriminative%20and%0Atemporally%20consistent%20features%20to%20rectify%20the%20non-rigid%20transformation.%20The%20two%0Avolumes%20are%20finally%20fused%20to%20fully%20represent%20motion%20and%20appearance.%20This%0Adivide-and-conquer%20strategy%20leads%20to%20more%20robust%20tracking%20through%20occlusions%0Aand%20deformations%20and%20meanwhile%20obtains%20decomposed%20appearances.%20We%20conduct%0Aevaluations%20on%20the%20TAP-Vid%20benchmark.%20The%20results%20demonstrate%20our%20method%20boosts%0Athe%20point-tracking%20accuracy%20by%20a%20large%20margin%20and%20performs%20on%20par%20with%20some%0Astate-of-the-art%20dedicated%20point-tracking%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06531v2&entry.124074799=Read"},
{"title": "Rate-Distortion-Cognition Controllable Versatile Neural Image\n  Compression", "author": "Jinming Liu and Ruoyu Feng and Yunpeng Qi and Qiuyu Chen and Zhibo Chen and Wenjun Zeng and Xin Jin", "abstract": "  Recently, the field of Image Coding for Machines (ICM) has garnered\nheightened interest and significant advances thanks to the rapid progress of\nlearning-based techniques for image compression and analysis. Previous studies\noften require training separate codecs to support various bitrate levels,\nmachine tasks, and networks, thus lacking both flexibility and practicality. To\naddress these challenges, we propose a rate-distortion-cognition controllable\nversatile image compression, which method allows the users to adjust the\nbitrate (i.e., Rate), image reconstruction quality (i.e., Distortion), and\nmachine task accuracy (i.e., Cognition) with a single neural model, achieving\nultra-controllability. Specifically, we first introduce a cognition-oriented\nloss in the primary compression branch to train a codec for diverse machine\ntasks. This branch attains variable bitrate by regulating quantization degree\nthrough the latent code channels. To further enhance the quality of the\nreconstructed images, we employ an auxiliary branch to supplement residual\ninformation with a scalable bitstream. Ultimately, two branches use a `$\\beta x\n+ (1 - \\beta) y$' interpolation strategy to achieve a balanced\ncognition-distortion trade-off. Extensive experiments demonstrate that our\nmethod yields satisfactory ICM performance and flexible\nRate-Distortion-Cognition controlling.\n", "link": "http://arxiv.org/abs/2407.11700v1", "date": "2024-07-16", "relevancy": 2.2521, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5904}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rate-Distortion-Cognition%20Controllable%20Versatile%20Neural%20Image%0A%20%20Compression&body=Title%3A%20Rate-Distortion-Cognition%20Controllable%20Versatile%20Neural%20Image%0A%20%20Compression%0AAuthor%3A%20Jinming%20Liu%20and%20Ruoyu%20Feng%20and%20Yunpeng%20Qi%20and%20Qiuyu%20Chen%20and%20Zhibo%20Chen%20and%20Wenjun%20Zeng%20and%20Xin%20Jin%0AAbstract%3A%20%20%20Recently%2C%20the%20field%20of%20Image%20Coding%20for%20Machines%20%28ICM%29%20has%20garnered%0Aheightened%20interest%20and%20significant%20advances%20thanks%20to%20the%20rapid%20progress%20of%0Alearning-based%20techniques%20for%20image%20compression%20and%20analysis.%20Previous%20studies%0Aoften%20require%20training%20separate%20codecs%20to%20support%20various%20bitrate%20levels%2C%0Amachine%20tasks%2C%20and%20networks%2C%20thus%20lacking%20both%20flexibility%20and%20practicality.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20rate-distortion-cognition%20controllable%0Aversatile%20image%20compression%2C%20which%20method%20allows%20the%20users%20to%20adjust%20the%0Abitrate%20%28i.e.%2C%20Rate%29%2C%20image%20reconstruction%20quality%20%28i.e.%2C%20Distortion%29%2C%20and%0Amachine%20task%20accuracy%20%28i.e.%2C%20Cognition%29%20with%20a%20single%20neural%20model%2C%20achieving%0Aultra-controllability.%20Specifically%2C%20we%20first%20introduce%20a%20cognition-oriented%0Aloss%20in%20the%20primary%20compression%20branch%20to%20train%20a%20codec%20for%20diverse%20machine%0Atasks.%20This%20branch%20attains%20variable%20bitrate%20by%20regulating%20quantization%20degree%0Athrough%20the%20latent%20code%20channels.%20To%20further%20enhance%20the%20quality%20of%20the%0Areconstructed%20images%2C%20we%20employ%20an%20auxiliary%20branch%20to%20supplement%20residual%0Ainformation%20with%20a%20scalable%20bitstream.%20Ultimately%2C%20two%20branches%20use%20a%20%60%24%5Cbeta%20x%0A%2B%20%281%20-%20%5Cbeta%29%20y%24%27%20interpolation%20strategy%20to%20achieve%20a%20balanced%0Acognition-distortion%20trade-off.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20yields%20satisfactory%20ICM%20performance%20and%20flexible%0ARate-Distortion-Cognition%20controlling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRate-Distortion-Cognition%2520Controllable%2520Versatile%2520Neural%2520Image%250A%2520%2520Compression%26entry.906535625%3DJinming%2520Liu%2520and%2520Ruoyu%2520Feng%2520and%2520Yunpeng%2520Qi%2520and%2520Qiuyu%2520Chen%2520and%2520Zhibo%2520Chen%2520and%2520Wenjun%2520Zeng%2520and%2520Xin%2520Jin%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520field%2520of%2520Image%2520Coding%2520for%2520Machines%2520%2528ICM%2529%2520has%2520garnered%250Aheightened%2520interest%2520and%2520significant%2520advances%2520thanks%2520to%2520the%2520rapid%2520progress%2520of%250Alearning-based%2520techniques%2520for%2520image%2520compression%2520and%2520analysis.%2520Previous%2520studies%250Aoften%2520require%2520training%2520separate%2520codecs%2520to%2520support%2520various%2520bitrate%2520levels%252C%250Amachine%2520tasks%252C%2520and%2520networks%252C%2520thus%2520lacking%2520both%2520flexibility%2520and%2520practicality.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520rate-distortion-cognition%2520controllable%250Aversatile%2520image%2520compression%252C%2520which%2520method%2520allows%2520the%2520users%2520to%2520adjust%2520the%250Abitrate%2520%2528i.e.%252C%2520Rate%2529%252C%2520image%2520reconstruction%2520quality%2520%2528i.e.%252C%2520Distortion%2529%252C%2520and%250Amachine%2520task%2520accuracy%2520%2528i.e.%252C%2520Cognition%2529%2520with%2520a%2520single%2520neural%2520model%252C%2520achieving%250Aultra-controllability.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520cognition-oriented%250Aloss%2520in%2520the%2520primary%2520compression%2520branch%2520to%2520train%2520a%2520codec%2520for%2520diverse%2520machine%250Atasks.%2520This%2520branch%2520attains%2520variable%2520bitrate%2520by%2520regulating%2520quantization%2520degree%250Athrough%2520the%2520latent%2520code%2520channels.%2520To%2520further%2520enhance%2520the%2520quality%2520of%2520the%250Areconstructed%2520images%252C%2520we%2520employ%2520an%2520auxiliary%2520branch%2520to%2520supplement%2520residual%250Ainformation%2520with%2520a%2520scalable%2520bitstream.%2520Ultimately%252C%2520two%2520branches%2520use%2520a%2520%2560%2524%255Cbeta%2520x%250A%252B%2520%25281%2520-%2520%255Cbeta%2529%2520y%2524%2527%2520interpolation%2520strategy%2520to%2520achieve%2520a%2520balanced%250Acognition-distortion%2520trade-off.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520yields%2520satisfactory%2520ICM%2520performance%2520and%2520flexible%250ARate-Distortion-Cognition%2520controlling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rate-Distortion-Cognition%20Controllable%20Versatile%20Neural%20Image%0A%20%20Compression&entry.906535625=Jinming%20Liu%20and%20Ruoyu%20Feng%20and%20Yunpeng%20Qi%20and%20Qiuyu%20Chen%20and%20Zhibo%20Chen%20and%20Wenjun%20Zeng%20and%20Xin%20Jin&entry.1292438233=%20%20Recently%2C%20the%20field%20of%20Image%20Coding%20for%20Machines%20%28ICM%29%20has%20garnered%0Aheightened%20interest%20and%20significant%20advances%20thanks%20to%20the%20rapid%20progress%20of%0Alearning-based%20techniques%20for%20image%20compression%20and%20analysis.%20Previous%20studies%0Aoften%20require%20training%20separate%20codecs%20to%20support%20various%20bitrate%20levels%2C%0Amachine%20tasks%2C%20and%20networks%2C%20thus%20lacking%20both%20flexibility%20and%20practicality.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20rate-distortion-cognition%20controllable%0Aversatile%20image%20compression%2C%20which%20method%20allows%20the%20users%20to%20adjust%20the%0Abitrate%20%28i.e.%2C%20Rate%29%2C%20image%20reconstruction%20quality%20%28i.e.%2C%20Distortion%29%2C%20and%0Amachine%20task%20accuracy%20%28i.e.%2C%20Cognition%29%20with%20a%20single%20neural%20model%2C%20achieving%0Aultra-controllability.%20Specifically%2C%20we%20first%20introduce%20a%20cognition-oriented%0Aloss%20in%20the%20primary%20compression%20branch%20to%20train%20a%20codec%20for%20diverse%20machine%0Atasks.%20This%20branch%20attains%20variable%20bitrate%20by%20regulating%20quantization%20degree%0Athrough%20the%20latent%20code%20channels.%20To%20further%20enhance%20the%20quality%20of%20the%0Areconstructed%20images%2C%20we%20employ%20an%20auxiliary%20branch%20to%20supplement%20residual%0Ainformation%20with%20a%20scalable%20bitstream.%20Ultimately%2C%20two%20branches%20use%20a%20%60%24%5Cbeta%20x%0A%2B%20%281%20-%20%5Cbeta%29%20y%24%27%20interpolation%20strategy%20to%20achieve%20a%20balanced%0Acognition-distortion%20trade-off.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20yields%20satisfactory%20ICM%20performance%20and%20flexible%0ARate-Distortion-Cognition%20controlling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11700v1&entry.124074799=Read"},
{"title": "Lost and Found: Overcoming Detector Failures in Online Multi-Object\n  Tracking", "author": "Lorenzo Vaquero and Yihong Xu and Xavier Alameda-Pineda and Victor M. Brea and Manuel Mucientes", "abstract": "  Multi-object tracking (MOT) endeavors to precisely estimate the positions and\nidentities of multiple objects over time. The prevailing approach,\ntracking-by-detection (TbD), first detects objects and then links detections,\nresulting in a simple yet effective method. However, contemporary detectors may\noccasionally miss some objects in certain frames, causing trackers to cease\ntracking prematurely. To tackle this issue, we propose BUSCA, meaning `to\nsearch', a versatile framework compatible with any online TbD system, enhancing\nits ability to persistently track those objects missed by the detector,\nprimarily due to occlusions. Remarkably, this is accomplished without modifying\npast tracking results or accessing future frames, i.e., in a fully online\nmanner. BUSCA generates proposals based on neighboring tracks, motion, and\nlearned tokens. Utilizing a decision Transformer that integrates multimodal\nvisual and spatiotemporal information, it addresses the object-proposal\nassociation as a multi-choice question-answering task. BUSCA is trained\nindependently of the underlying tracker, solely on synthetic data, without\nrequiring fine-tuning. Through BUSCA, we showcase consistent performance\nenhancements across five different trackers and establish a new\nstate-of-the-art baseline across three different benchmarks. Code available at:\nhttps://github.com/lorenzovaquero/BUSCA.\n", "link": "http://arxiv.org/abs/2407.10151v2", "date": "2024-07-16", "relevancy": 2.2389, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5663}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20and%20Found%3A%20Overcoming%20Detector%20Failures%20in%20Online%20Multi-Object%0A%20%20Tracking&body=Title%3A%20Lost%20and%20Found%3A%20Overcoming%20Detector%20Failures%20in%20Online%20Multi-Object%0A%20%20Tracking%0AAuthor%3A%20Lorenzo%20Vaquero%20and%20Yihong%20Xu%20and%20Xavier%20Alameda-Pineda%20and%20Victor%20M.%20Brea%20and%20Manuel%20Mucientes%0AAbstract%3A%20%20%20Multi-object%20tracking%20%28MOT%29%20endeavors%20to%20precisely%20estimate%20the%20positions%20and%0Aidentities%20of%20multiple%20objects%20over%20time.%20The%20prevailing%20approach%2C%0Atracking-by-detection%20%28TbD%29%2C%20first%20detects%20objects%20and%20then%20links%20detections%2C%0Aresulting%20in%20a%20simple%20yet%20effective%20method.%20However%2C%20contemporary%20detectors%20may%0Aoccasionally%20miss%20some%20objects%20in%20certain%20frames%2C%20causing%20trackers%20to%20cease%0Atracking%20prematurely.%20To%20tackle%20this%20issue%2C%20we%20propose%20BUSCA%2C%20meaning%20%60to%0Asearch%27%2C%20a%20versatile%20framework%20compatible%20with%20any%20online%20TbD%20system%2C%20enhancing%0Aits%20ability%20to%20persistently%20track%20those%20objects%20missed%20by%20the%20detector%2C%0Aprimarily%20due%20to%20occlusions.%20Remarkably%2C%20this%20is%20accomplished%20without%20modifying%0Apast%20tracking%20results%20or%20accessing%20future%20frames%2C%20i.e.%2C%20in%20a%20fully%20online%0Amanner.%20BUSCA%20generates%20proposals%20based%20on%20neighboring%20tracks%2C%20motion%2C%20and%0Alearned%20tokens.%20Utilizing%20a%20decision%20Transformer%20that%20integrates%20multimodal%0Avisual%20and%20spatiotemporal%20information%2C%20it%20addresses%20the%20object-proposal%0Aassociation%20as%20a%20multi-choice%20question-answering%20task.%20BUSCA%20is%20trained%0Aindependently%20of%20the%20underlying%20tracker%2C%20solely%20on%20synthetic%20data%2C%20without%0Arequiring%20fine-tuning.%20Through%20BUSCA%2C%20we%20showcase%20consistent%20performance%0Aenhancements%20across%20five%20different%20trackers%20and%20establish%20a%20new%0Astate-of-the-art%20baseline%20across%20three%20different%20benchmarks.%20Code%20available%20at%3A%0Ahttps%3A//github.com/lorenzovaquero/BUSCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520and%2520Found%253A%2520Overcoming%2520Detector%2520Failures%2520in%2520Online%2520Multi-Object%250A%2520%2520Tracking%26entry.906535625%3DLorenzo%2520Vaquero%2520and%2520Yihong%2520Xu%2520and%2520Xavier%2520Alameda-Pineda%2520and%2520Victor%2520M.%2520Brea%2520and%2520Manuel%2520Mucientes%26entry.1292438233%3D%2520%2520Multi-object%2520tracking%2520%2528MOT%2529%2520endeavors%2520to%2520precisely%2520estimate%2520the%2520positions%2520and%250Aidentities%2520of%2520multiple%2520objects%2520over%2520time.%2520The%2520prevailing%2520approach%252C%250Atracking-by-detection%2520%2528TbD%2529%252C%2520first%2520detects%2520objects%2520and%2520then%2520links%2520detections%252C%250Aresulting%2520in%2520a%2520simple%2520yet%2520effective%2520method.%2520However%252C%2520contemporary%2520detectors%2520may%250Aoccasionally%2520miss%2520some%2520objects%2520in%2520certain%2520frames%252C%2520causing%2520trackers%2520to%2520cease%250Atracking%2520prematurely.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520BUSCA%252C%2520meaning%2520%2560to%250Asearch%2527%252C%2520a%2520versatile%2520framework%2520compatible%2520with%2520any%2520online%2520TbD%2520system%252C%2520enhancing%250Aits%2520ability%2520to%2520persistently%2520track%2520those%2520objects%2520missed%2520by%2520the%2520detector%252C%250Aprimarily%2520due%2520to%2520occlusions.%2520Remarkably%252C%2520this%2520is%2520accomplished%2520without%2520modifying%250Apast%2520tracking%2520results%2520or%2520accessing%2520future%2520frames%252C%2520i.e.%252C%2520in%2520a%2520fully%2520online%250Amanner.%2520BUSCA%2520generates%2520proposals%2520based%2520on%2520neighboring%2520tracks%252C%2520motion%252C%2520and%250Alearned%2520tokens.%2520Utilizing%2520a%2520decision%2520Transformer%2520that%2520integrates%2520multimodal%250Avisual%2520and%2520spatiotemporal%2520information%252C%2520it%2520addresses%2520the%2520object-proposal%250Aassociation%2520as%2520a%2520multi-choice%2520question-answering%2520task.%2520BUSCA%2520is%2520trained%250Aindependently%2520of%2520the%2520underlying%2520tracker%252C%2520solely%2520on%2520synthetic%2520data%252C%2520without%250Arequiring%2520fine-tuning.%2520Through%2520BUSCA%252C%2520we%2520showcase%2520consistent%2520performance%250Aenhancements%2520across%2520five%2520different%2520trackers%2520and%2520establish%2520a%2520new%250Astate-of-the-art%2520baseline%2520across%2520three%2520different%2520benchmarks.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/lorenzovaquero/BUSCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20and%20Found%3A%20Overcoming%20Detector%20Failures%20in%20Online%20Multi-Object%0A%20%20Tracking&entry.906535625=Lorenzo%20Vaquero%20and%20Yihong%20Xu%20and%20Xavier%20Alameda-Pineda%20and%20Victor%20M.%20Brea%20and%20Manuel%20Mucientes&entry.1292438233=%20%20Multi-object%20tracking%20%28MOT%29%20endeavors%20to%20precisely%20estimate%20the%20positions%20and%0Aidentities%20of%20multiple%20objects%20over%20time.%20The%20prevailing%20approach%2C%0Atracking-by-detection%20%28TbD%29%2C%20first%20detects%20objects%20and%20then%20links%20detections%2C%0Aresulting%20in%20a%20simple%20yet%20effective%20method.%20However%2C%20contemporary%20detectors%20may%0Aoccasionally%20miss%20some%20objects%20in%20certain%20frames%2C%20causing%20trackers%20to%20cease%0Atracking%20prematurely.%20To%20tackle%20this%20issue%2C%20we%20propose%20BUSCA%2C%20meaning%20%60to%0Asearch%27%2C%20a%20versatile%20framework%20compatible%20with%20any%20online%20TbD%20system%2C%20enhancing%0Aits%20ability%20to%20persistently%20track%20those%20objects%20missed%20by%20the%20detector%2C%0Aprimarily%20due%20to%20occlusions.%20Remarkably%2C%20this%20is%20accomplished%20without%20modifying%0Apast%20tracking%20results%20or%20accessing%20future%20frames%2C%20i.e.%2C%20in%20a%20fully%20online%0Amanner.%20BUSCA%20generates%20proposals%20based%20on%20neighboring%20tracks%2C%20motion%2C%20and%0Alearned%20tokens.%20Utilizing%20a%20decision%20Transformer%20that%20integrates%20multimodal%0Avisual%20and%20spatiotemporal%20information%2C%20it%20addresses%20the%20object-proposal%0Aassociation%20as%20a%20multi-choice%20question-answering%20task.%20BUSCA%20is%20trained%0Aindependently%20of%20the%20underlying%20tracker%2C%20solely%20on%20synthetic%20data%2C%20without%0Arequiring%20fine-tuning.%20Through%20BUSCA%2C%20we%20showcase%20consistent%20performance%0Aenhancements%20across%20five%20different%20trackers%20and%20establish%20a%20new%0Astate-of-the-art%20baseline%20across%20three%20different%20benchmarks.%20Code%20available%20at%3A%0Ahttps%3A//github.com/lorenzovaquero/BUSCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10151v2&entry.124074799=Read"},
{"title": "Distractors-Immune Representation Learning with Cross-modal Contrastive\n  Regularization for Change Captioning", "author": "Yunbin Tu and Liang Li and Li Su and Chenggang Yan and Qingming Huang", "abstract": "  Change captioning aims to succinctly describe the semantic change between a\npair of similar images, while being immune to distractors (illumination and\nviewpoint changes). Under these distractors, unchanged objects often appear\npseudo changes about location and scale, and certain objects might overlap\nothers, resulting in perturbational and discrimination-degraded features\nbetween two images. However, most existing methods directly capture the\ndifference between them, which risk obtaining error-prone difference features.\nIn this paper, we propose a distractors-immune representation learning network\nthat correlates the corresponding channels of two image representations and\ndecorrelates different ones in a self-supervised manner, thus attaining a pair\nof stable image representations under distractors. Then, the model can better\ninteract them to capture the reliable difference features for caption\ngeneration. To yield words based on the most related difference features, we\nfurther design a cross-modal contrastive regularization, which regularizes the\ncross-modal alignment by maximizing the contrastive alignment between the\nattended difference features and generated words. Extensive experiments show\nthat our method outperforms the state-of-the-art methods on four public\ndatasets. The code is available at https://github.com/tuyunbin/DIRL.\n", "link": "http://arxiv.org/abs/2407.11683v1", "date": "2024-07-16", "relevancy": 2.223, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5714}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5531}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distractors-Immune%20Representation%20Learning%20with%20Cross-modal%20Contrastive%0A%20%20Regularization%20for%20Change%20Captioning&body=Title%3A%20Distractors-Immune%20Representation%20Learning%20with%20Cross-modal%20Contrastive%0A%20%20Regularization%20for%20Change%20Captioning%0AAuthor%3A%20Yunbin%20Tu%20and%20Liang%20Li%20and%20Li%20Su%20and%20Chenggang%20Yan%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Change%20captioning%20aims%20to%20succinctly%20describe%20the%20semantic%20change%20between%20a%0Apair%20of%20similar%20images%2C%20while%20being%20immune%20to%20distractors%20%28illumination%20and%0Aviewpoint%20changes%29.%20Under%20these%20distractors%2C%20unchanged%20objects%20often%20appear%0Apseudo%20changes%20about%20location%20and%20scale%2C%20and%20certain%20objects%20might%20overlap%0Aothers%2C%20resulting%20in%20perturbational%20and%20discrimination-degraded%20features%0Abetween%20two%20images.%20However%2C%20most%20existing%20methods%20directly%20capture%20the%0Adifference%20between%20them%2C%20which%20risk%20obtaining%20error-prone%20difference%20features.%0AIn%20this%20paper%2C%20we%20propose%20a%20distractors-immune%20representation%20learning%20network%0Athat%20correlates%20the%20corresponding%20channels%20of%20two%20image%20representations%20and%0Adecorrelates%20different%20ones%20in%20a%20self-supervised%20manner%2C%20thus%20attaining%20a%20pair%0Aof%20stable%20image%20representations%20under%20distractors.%20Then%2C%20the%20model%20can%20better%0Ainteract%20them%20to%20capture%20the%20reliable%20difference%20features%20for%20caption%0Ageneration.%20To%20yield%20words%20based%20on%20the%20most%20related%20difference%20features%2C%20we%0Afurther%20design%20a%20cross-modal%20contrastive%20regularization%2C%20which%20regularizes%20the%0Across-modal%20alignment%20by%20maximizing%20the%20contrastive%20alignment%20between%20the%0Aattended%20difference%20features%20and%20generated%20words.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20four%20public%0Adatasets.%20The%20code%20is%20available%20at%20https%3A//github.com/tuyunbin/DIRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistractors-Immune%2520Representation%2520Learning%2520with%2520Cross-modal%2520Contrastive%250A%2520%2520Regularization%2520for%2520Change%2520Captioning%26entry.906535625%3DYunbin%2520Tu%2520and%2520Liang%2520Li%2520and%2520Li%2520Su%2520and%2520Chenggang%2520Yan%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Change%2520captioning%2520aims%2520to%2520succinctly%2520describe%2520the%2520semantic%2520change%2520between%2520a%250Apair%2520of%2520similar%2520images%252C%2520while%2520being%2520immune%2520to%2520distractors%2520%2528illumination%2520and%250Aviewpoint%2520changes%2529.%2520Under%2520these%2520distractors%252C%2520unchanged%2520objects%2520often%2520appear%250Apseudo%2520changes%2520about%2520location%2520and%2520scale%252C%2520and%2520certain%2520objects%2520might%2520overlap%250Aothers%252C%2520resulting%2520in%2520perturbational%2520and%2520discrimination-degraded%2520features%250Abetween%2520two%2520images.%2520However%252C%2520most%2520existing%2520methods%2520directly%2520capture%2520the%250Adifference%2520between%2520them%252C%2520which%2520risk%2520obtaining%2520error-prone%2520difference%2520features.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520distractors-immune%2520representation%2520learning%2520network%250Athat%2520correlates%2520the%2520corresponding%2520channels%2520of%2520two%2520image%2520representations%2520and%250Adecorrelates%2520different%2520ones%2520in%2520a%2520self-supervised%2520manner%252C%2520thus%2520attaining%2520a%2520pair%250Aof%2520stable%2520image%2520representations%2520under%2520distractors.%2520Then%252C%2520the%2520model%2520can%2520better%250Ainteract%2520them%2520to%2520capture%2520the%2520reliable%2520difference%2520features%2520for%2520caption%250Ageneration.%2520To%2520yield%2520words%2520based%2520on%2520the%2520most%2520related%2520difference%2520features%252C%2520we%250Afurther%2520design%2520a%2520cross-modal%2520contrastive%2520regularization%252C%2520which%2520regularizes%2520the%250Across-modal%2520alignment%2520by%2520maximizing%2520the%2520contrastive%2520alignment%2520between%2520the%250Aattended%2520difference%2520features%2520and%2520generated%2520words.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%2520four%2520public%250Adatasets.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/tuyunbin/DIRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distractors-Immune%20Representation%20Learning%20with%20Cross-modal%20Contrastive%0A%20%20Regularization%20for%20Change%20Captioning&entry.906535625=Yunbin%20Tu%20and%20Liang%20Li%20and%20Li%20Su%20and%20Chenggang%20Yan%20and%20Qingming%20Huang&entry.1292438233=%20%20Change%20captioning%20aims%20to%20succinctly%20describe%20the%20semantic%20change%20between%20a%0Apair%20of%20similar%20images%2C%20while%20being%20immune%20to%20distractors%20%28illumination%20and%0Aviewpoint%20changes%29.%20Under%20these%20distractors%2C%20unchanged%20objects%20often%20appear%0Apseudo%20changes%20about%20location%20and%20scale%2C%20and%20certain%20objects%20might%20overlap%0Aothers%2C%20resulting%20in%20perturbational%20and%20discrimination-degraded%20features%0Abetween%20two%20images.%20However%2C%20most%20existing%20methods%20directly%20capture%20the%0Adifference%20between%20them%2C%20which%20risk%20obtaining%20error-prone%20difference%20features.%0AIn%20this%20paper%2C%20we%20propose%20a%20distractors-immune%20representation%20learning%20network%0Athat%20correlates%20the%20corresponding%20channels%20of%20two%20image%20representations%20and%0Adecorrelates%20different%20ones%20in%20a%20self-supervised%20manner%2C%20thus%20attaining%20a%20pair%0Aof%20stable%20image%20representations%20under%20distractors.%20Then%2C%20the%20model%20can%20better%0Ainteract%20them%20to%20capture%20the%20reliable%20difference%20features%20for%20caption%0Ageneration.%20To%20yield%20words%20based%20on%20the%20most%20related%20difference%20features%2C%20we%0Afurther%20design%20a%20cross-modal%20contrastive%20regularization%2C%20which%20regularizes%20the%0Across-modal%20alignment%20by%20maximizing%20the%20contrastive%20alignment%20between%20the%0Aattended%20difference%20features%20and%20generated%20words.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20four%20public%0Adatasets.%20The%20code%20is%20available%20at%20https%3A//github.com/tuyunbin/DIRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11683v1&entry.124074799=Read"},
{"title": "Encapsulating Knowledge in One Prompt", "author": "Qi Li and Runpeng Yu and Xinchao Wang", "abstract": "  This paradigm encapsulates knowledge from various models into a solitary\nprompt without altering the original models or requiring access to the training\ndata, which enables us to achieve efficient and convenient knowledge transfer\nin more realistic scenarios. From a practicality standpoint, this paradigm not\nonly for the first time proves the effectiveness of Visual Prompt in data\ninaccessible contexts, but also solves the problems of low model reusability\nand high storage resource consumption faced by traditional Data-Free Knowledge\nTransfer, which means that we can realize the parallel knowledge transfer of\nmultiple models without modifying any source model. Extensive experiments\nacross various datasets and models demonstrate the efficacy of the proposed\nKiOP knowledge transfer paradigm. Without access to real training data and with\nrigorous storage capacity constraints, it is also capable of yielding\nconsiderable outcomes when dealing with cross-model backbone setups and\nhandling parallel knowledge transfer processing requests with multiple (more\nthan 2) models.\n", "link": "http://arxiv.org/abs/2407.11902v1", "date": "2024-07-16", "relevancy": 2.2211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4498}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4479}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encapsulating%20Knowledge%20in%20One%20Prompt&body=Title%3A%20Encapsulating%20Knowledge%20in%20One%20Prompt%0AAuthor%3A%20Qi%20Li%20and%20Runpeng%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20This%20paradigm%20encapsulates%20knowledge%20from%20various%20models%20into%20a%20solitary%0Aprompt%20without%20altering%20the%20original%20models%20or%20requiring%20access%20to%20the%20training%0Adata%2C%20which%20enables%20us%20to%20achieve%20efficient%20and%20convenient%20knowledge%20transfer%0Ain%20more%20realistic%20scenarios.%20From%20a%20practicality%20standpoint%2C%20this%20paradigm%20not%0Aonly%20for%20the%20first%20time%20proves%20the%20effectiveness%20of%20Visual%20Prompt%20in%20data%0Ainaccessible%20contexts%2C%20but%20also%20solves%20the%20problems%20of%20low%20model%20reusability%0Aand%20high%20storage%20resource%20consumption%20faced%20by%20traditional%20Data-Free%20Knowledge%0ATransfer%2C%20which%20means%20that%20we%20can%20realize%20the%20parallel%20knowledge%20transfer%20of%0Amultiple%20models%20without%20modifying%20any%20source%20model.%20Extensive%20experiments%0Aacross%20various%20datasets%20and%20models%20demonstrate%20the%20efficacy%20of%20the%20proposed%0AKiOP%20knowledge%20transfer%20paradigm.%20Without%20access%20to%20real%20training%20data%20and%20with%0Arigorous%20storage%20capacity%20constraints%2C%20it%20is%20also%20capable%20of%20yielding%0Aconsiderable%20outcomes%20when%20dealing%20with%20cross-model%20backbone%20setups%20and%0Ahandling%20parallel%20knowledge%20transfer%20processing%20requests%20with%20multiple%20%28more%0Athan%202%29%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncapsulating%2520Knowledge%2520in%2520One%2520Prompt%26entry.906535625%3DQi%2520Li%2520and%2520Runpeng%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520This%2520paradigm%2520encapsulates%2520knowledge%2520from%2520various%2520models%2520into%2520a%2520solitary%250Aprompt%2520without%2520altering%2520the%2520original%2520models%2520or%2520requiring%2520access%2520to%2520the%2520training%250Adata%252C%2520which%2520enables%2520us%2520to%2520achieve%2520efficient%2520and%2520convenient%2520knowledge%2520transfer%250Ain%2520more%2520realistic%2520scenarios.%2520From%2520a%2520practicality%2520standpoint%252C%2520this%2520paradigm%2520not%250Aonly%2520for%2520the%2520first%2520time%2520proves%2520the%2520effectiveness%2520of%2520Visual%2520Prompt%2520in%2520data%250Ainaccessible%2520contexts%252C%2520but%2520also%2520solves%2520the%2520problems%2520of%2520low%2520model%2520reusability%250Aand%2520high%2520storage%2520resource%2520consumption%2520faced%2520by%2520traditional%2520Data-Free%2520Knowledge%250ATransfer%252C%2520which%2520means%2520that%2520we%2520can%2520realize%2520the%2520parallel%2520knowledge%2520transfer%2520of%250Amultiple%2520models%2520without%2520modifying%2520any%2520source%2520model.%2520Extensive%2520experiments%250Aacross%2520various%2520datasets%2520and%2520models%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%250AKiOP%2520knowledge%2520transfer%2520paradigm.%2520Without%2520access%2520to%2520real%2520training%2520data%2520and%2520with%250Arigorous%2520storage%2520capacity%2520constraints%252C%2520it%2520is%2520also%2520capable%2520of%2520yielding%250Aconsiderable%2520outcomes%2520when%2520dealing%2520with%2520cross-model%2520backbone%2520setups%2520and%250Ahandling%2520parallel%2520knowledge%2520transfer%2520processing%2520requests%2520with%2520multiple%2520%2528more%250Athan%25202%2529%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encapsulating%20Knowledge%20in%20One%20Prompt&entry.906535625=Qi%20Li%20and%20Runpeng%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20This%20paradigm%20encapsulates%20knowledge%20from%20various%20models%20into%20a%20solitary%0Aprompt%20without%20altering%20the%20original%20models%20or%20requiring%20access%20to%20the%20training%0Adata%2C%20which%20enables%20us%20to%20achieve%20efficient%20and%20convenient%20knowledge%20transfer%0Ain%20more%20realistic%20scenarios.%20From%20a%20practicality%20standpoint%2C%20this%20paradigm%20not%0Aonly%20for%20the%20first%20time%20proves%20the%20effectiveness%20of%20Visual%20Prompt%20in%20data%0Ainaccessible%20contexts%2C%20but%20also%20solves%20the%20problems%20of%20low%20model%20reusability%0Aand%20high%20storage%20resource%20consumption%20faced%20by%20traditional%20Data-Free%20Knowledge%0ATransfer%2C%20which%20means%20that%20we%20can%20realize%20the%20parallel%20knowledge%20transfer%20of%0Amultiple%20models%20without%20modifying%20any%20source%20model.%20Extensive%20experiments%0Aacross%20various%20datasets%20and%20models%20demonstrate%20the%20efficacy%20of%20the%20proposed%0AKiOP%20knowledge%20transfer%20paradigm.%20Without%20access%20to%20real%20training%20data%20and%20with%0Arigorous%20storage%20capacity%20constraints%2C%20it%20is%20also%20capable%20of%20yielding%0Aconsiderable%20outcomes%20when%20dealing%20with%20cross-model%20backbone%20setups%20and%0Ahandling%20parallel%20knowledge%20transfer%20processing%20requests%20with%20multiple%20%28more%0Athan%202%29%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11902v1&entry.124074799=Read"},
{"title": "Perception Helps Planning: Facilitating Multi-Stage Lane-Level\n  Integration via Double-Edge Structures", "author": "Guoliang You and Xiaomeng Chu and Yifan Duan and Wenyu Zhang and Xingchen Li and Sha Zhang and Yao Li and Jianmin Ji and Yanyong Zhang", "abstract": "  When planning for autonomous driving, it is crucial to consider essential\ntraffic elements such as lanes, intersections, traffic regulations, and dynamic\nagents. However, they are often overlooked by the traditional end-to-end\nplanning methods, likely leading to inefficiencies and non-compliance with\ntraffic regulations. In this work, we endeavor to integrate the perception of\nthese elements into the planning task. To this end, we propose Perception Helps\nPlanning (PHP), a novel framework that reconciles lane-level planning with\nperception. This integration ensures that planning is inherently aligned with\ntraffic constraints, thus facilitating safe and efficient driving.\nSpecifically, PHP focuses on both edges of a lane for planning and perception\npurposes, taking into consideration the 3D positions of both lane edges and\nattributes for lane intersections, lane directions, lane occupancy, and\nplanning. In the algorithmic design, the process begins with the transformer\nencoding multi-camera images to extract the above features and predicting\nlane-level perception results. Next, the hierarchical feature early fusion\nmodule refines the features for predicting planning attributes. Finally, the\ndouble-edge interpreter utilizes a late-fusion process specifically designed to\nintegrate lane-level perception and planning information, culminating in the\ngeneration of vehicle control signals. Experiments on three Carla benchmarks\nshow significant improvements in driving score of 27.20%, 33.47%, and 15.54%\nover existing algorithms, respectively, achieving the state-of-the-art\nperformance, with the system operating up to 22.57 FPS.\n", "link": "http://arxiv.org/abs/2407.11644v1", "date": "2024-07-16", "relevancy": 2.2198, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5966}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5659}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Helps%20Planning%3A%20Facilitating%20Multi-Stage%20Lane-Level%0A%20%20Integration%20via%20Double-Edge%20Structures&body=Title%3A%20Perception%20Helps%20Planning%3A%20Facilitating%20Multi-Stage%20Lane-Level%0A%20%20Integration%20via%20Double-Edge%20Structures%0AAuthor%3A%20Guoliang%20You%20and%20Xiaomeng%20Chu%20and%20Yifan%20Duan%20and%20Wenyu%20Zhang%20and%20Xingchen%20Li%20and%20Sha%20Zhang%20and%20Yao%20Li%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20When%20planning%20for%20autonomous%20driving%2C%20it%20is%20crucial%20to%20consider%20essential%0Atraffic%20elements%20such%20as%20lanes%2C%20intersections%2C%20traffic%20regulations%2C%20and%20dynamic%0Aagents.%20However%2C%20they%20are%20often%20overlooked%20by%20the%20traditional%20end-to-end%0Aplanning%20methods%2C%20likely%20leading%20to%20inefficiencies%20and%20non-compliance%20with%0Atraffic%20regulations.%20In%20this%20work%2C%20we%20endeavor%20to%20integrate%20the%20perception%20of%0Athese%20elements%20into%20the%20planning%20task.%20To%20this%20end%2C%20we%20propose%20Perception%20Helps%0APlanning%20%28PHP%29%2C%20a%20novel%20framework%20that%20reconciles%20lane-level%20planning%20with%0Aperception.%20This%20integration%20ensures%20that%20planning%20is%20inherently%20aligned%20with%0Atraffic%20constraints%2C%20thus%20facilitating%20safe%20and%20efficient%20driving.%0ASpecifically%2C%20PHP%20focuses%20on%20both%20edges%20of%20a%20lane%20for%20planning%20and%20perception%0Apurposes%2C%20taking%20into%20consideration%20the%203D%20positions%20of%20both%20lane%20edges%20and%0Aattributes%20for%20lane%20intersections%2C%20lane%20directions%2C%20lane%20occupancy%2C%20and%0Aplanning.%20In%20the%20algorithmic%20design%2C%20the%20process%20begins%20with%20the%20transformer%0Aencoding%20multi-camera%20images%20to%20extract%20the%20above%20features%20and%20predicting%0Alane-level%20perception%20results.%20Next%2C%20the%20hierarchical%20feature%20early%20fusion%0Amodule%20refines%20the%20features%20for%20predicting%20planning%20attributes.%20Finally%2C%20the%0Adouble-edge%20interpreter%20utilizes%20a%20late-fusion%20process%20specifically%20designed%20to%0Aintegrate%20lane-level%20perception%20and%20planning%20information%2C%20culminating%20in%20the%0Ageneration%20of%20vehicle%20control%20signals.%20Experiments%20on%20three%20Carla%20benchmarks%0Ashow%20significant%20improvements%20in%20driving%20score%20of%2027.20%25%2C%2033.47%25%2C%20and%2015.54%25%0Aover%20existing%20algorithms%2C%20respectively%2C%20achieving%20the%20state-of-the-art%0Aperformance%2C%20with%20the%20system%20operating%20up%20to%2022.57%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Helps%2520Planning%253A%2520Facilitating%2520Multi-Stage%2520Lane-Level%250A%2520%2520Integration%2520via%2520Double-Edge%2520Structures%26entry.906535625%3DGuoliang%2520You%2520and%2520Xiaomeng%2520Chu%2520and%2520Yifan%2520Duan%2520and%2520Wenyu%2520Zhang%2520and%2520Xingchen%2520Li%2520and%2520Sha%2520Zhang%2520and%2520Yao%2520Li%2520and%2520Jianmin%2520Ji%2520and%2520Yanyong%2520Zhang%26entry.1292438233%3D%2520%2520When%2520planning%2520for%2520autonomous%2520driving%252C%2520it%2520is%2520crucial%2520to%2520consider%2520essential%250Atraffic%2520elements%2520such%2520as%2520lanes%252C%2520intersections%252C%2520traffic%2520regulations%252C%2520and%2520dynamic%250Aagents.%2520However%252C%2520they%2520are%2520often%2520overlooked%2520by%2520the%2520traditional%2520end-to-end%250Aplanning%2520methods%252C%2520likely%2520leading%2520to%2520inefficiencies%2520and%2520non-compliance%2520with%250Atraffic%2520regulations.%2520In%2520this%2520work%252C%2520we%2520endeavor%2520to%2520integrate%2520the%2520perception%2520of%250Athese%2520elements%2520into%2520the%2520planning%2520task.%2520To%2520this%2520end%252C%2520we%2520propose%2520Perception%2520Helps%250APlanning%2520%2528PHP%2529%252C%2520a%2520novel%2520framework%2520that%2520reconciles%2520lane-level%2520planning%2520with%250Aperception.%2520This%2520integration%2520ensures%2520that%2520planning%2520is%2520inherently%2520aligned%2520with%250Atraffic%2520constraints%252C%2520thus%2520facilitating%2520safe%2520and%2520efficient%2520driving.%250ASpecifically%252C%2520PHP%2520focuses%2520on%2520both%2520edges%2520of%2520a%2520lane%2520for%2520planning%2520and%2520perception%250Apurposes%252C%2520taking%2520into%2520consideration%2520the%25203D%2520positions%2520of%2520both%2520lane%2520edges%2520and%250Aattributes%2520for%2520lane%2520intersections%252C%2520lane%2520directions%252C%2520lane%2520occupancy%252C%2520and%250Aplanning.%2520In%2520the%2520algorithmic%2520design%252C%2520the%2520process%2520begins%2520with%2520the%2520transformer%250Aencoding%2520multi-camera%2520images%2520to%2520extract%2520the%2520above%2520features%2520and%2520predicting%250Alane-level%2520perception%2520results.%2520Next%252C%2520the%2520hierarchical%2520feature%2520early%2520fusion%250Amodule%2520refines%2520the%2520features%2520for%2520predicting%2520planning%2520attributes.%2520Finally%252C%2520the%250Adouble-edge%2520interpreter%2520utilizes%2520a%2520late-fusion%2520process%2520specifically%2520designed%2520to%250Aintegrate%2520lane-level%2520perception%2520and%2520planning%2520information%252C%2520culminating%2520in%2520the%250Ageneration%2520of%2520vehicle%2520control%2520signals.%2520Experiments%2520on%2520three%2520Carla%2520benchmarks%250Ashow%2520significant%2520improvements%2520in%2520driving%2520score%2520of%252027.20%2525%252C%252033.47%2525%252C%2520and%252015.54%2525%250Aover%2520existing%2520algorithms%252C%2520respectively%252C%2520achieving%2520the%2520state-of-the-art%250Aperformance%252C%2520with%2520the%2520system%2520operating%2520up%2520to%252022.57%2520FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Helps%20Planning%3A%20Facilitating%20Multi-Stage%20Lane-Level%0A%20%20Integration%20via%20Double-Edge%20Structures&entry.906535625=Guoliang%20You%20and%20Xiaomeng%20Chu%20and%20Yifan%20Duan%20and%20Wenyu%20Zhang%20and%20Xingchen%20Li%20and%20Sha%20Zhang%20and%20Yao%20Li%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang&entry.1292438233=%20%20When%20planning%20for%20autonomous%20driving%2C%20it%20is%20crucial%20to%20consider%20essential%0Atraffic%20elements%20such%20as%20lanes%2C%20intersections%2C%20traffic%20regulations%2C%20and%20dynamic%0Aagents.%20However%2C%20they%20are%20often%20overlooked%20by%20the%20traditional%20end-to-end%0Aplanning%20methods%2C%20likely%20leading%20to%20inefficiencies%20and%20non-compliance%20with%0Atraffic%20regulations.%20In%20this%20work%2C%20we%20endeavor%20to%20integrate%20the%20perception%20of%0Athese%20elements%20into%20the%20planning%20task.%20To%20this%20end%2C%20we%20propose%20Perception%20Helps%0APlanning%20%28PHP%29%2C%20a%20novel%20framework%20that%20reconciles%20lane-level%20planning%20with%0Aperception.%20This%20integration%20ensures%20that%20planning%20is%20inherently%20aligned%20with%0Atraffic%20constraints%2C%20thus%20facilitating%20safe%20and%20efficient%20driving.%0ASpecifically%2C%20PHP%20focuses%20on%20both%20edges%20of%20a%20lane%20for%20planning%20and%20perception%0Apurposes%2C%20taking%20into%20consideration%20the%203D%20positions%20of%20both%20lane%20edges%20and%0Aattributes%20for%20lane%20intersections%2C%20lane%20directions%2C%20lane%20occupancy%2C%20and%0Aplanning.%20In%20the%20algorithmic%20design%2C%20the%20process%20begins%20with%20the%20transformer%0Aencoding%20multi-camera%20images%20to%20extract%20the%20above%20features%20and%20predicting%0Alane-level%20perception%20results.%20Next%2C%20the%20hierarchical%20feature%20early%20fusion%0Amodule%20refines%20the%20features%20for%20predicting%20planning%20attributes.%20Finally%2C%20the%0Adouble-edge%20interpreter%20utilizes%20a%20late-fusion%20process%20specifically%20designed%20to%0Aintegrate%20lane-level%20perception%20and%20planning%20information%2C%20culminating%20in%20the%0Ageneration%20of%20vehicle%20control%20signals.%20Experiments%20on%20three%20Carla%20benchmarks%0Ashow%20significant%20improvements%20in%20driving%20score%20of%2027.20%25%2C%2033.47%25%2C%20and%2015.54%25%0Aover%20existing%20algorithms%2C%20respectively%2C%20achieving%20the%20state-of-the-art%0Aperformance%2C%20with%20the%20system%20operating%20up%20to%2022.57%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11644v1&entry.124074799=Read"},
{"title": "MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based\n  Exact IMU Pre-integration", "author": "Yifu Wang and Yonhon Ng and Inkyu Sa and Alvaro Parra and Cristian Rodriguez and Tao Jun Lin and Hongdong Li", "abstract": "  We present a novel optimization-based Visual-Inertial SLAM system designed\nfor multiple partially overlapped camera systems, named MAVIS. Our framework\nfully exploits the benefits of wide field-of-view from multi-camera systems,\nand the metric scale measurements provided by an inertial measurement unit\n(IMU). We introduce an improved IMU pre-integration formulation based on the\nexponential function of an automorphism of SE_2(3), which can effectively\nenhance tracking performance under fast rotational motion and extended\nintegration time. Furthermore, we extend conventional front-end tracking and\nback-end optimization module designed for monocular or stereo setup towards\nmulti-camera systems, and introduce implementation details that contribute to\nthe performance of our system in challenging scenarios. The practical validity\nof our approach is supported by our experiments on public datasets. Our MAVIS\nwon the first place in all the vision-IMU tracks (single and multi-session\nSLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the\nsecond place.\n", "link": "http://arxiv.org/abs/2309.08142v5", "date": "2024-07-16", "relevancy": 2.2137, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5597}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAVIS%3A%20Multi-Camera%20Augmented%20Visual-Inertial%20SLAM%20using%20SE2%283%29%20Based%0A%20%20Exact%20IMU%20Pre-integration&body=Title%3A%20MAVIS%3A%20Multi-Camera%20Augmented%20Visual-Inertial%20SLAM%20using%20SE2%283%29%20Based%0A%20%20Exact%20IMU%20Pre-integration%0AAuthor%3A%20Yifu%20Wang%20and%20Yonhon%20Ng%20and%20Inkyu%20Sa%20and%20Alvaro%20Parra%20and%20Cristian%20Rodriguez%20and%20Tao%20Jun%20Lin%20and%20Hongdong%20Li%0AAbstract%3A%20%20%20We%20present%20a%20novel%20optimization-based%20Visual-Inertial%20SLAM%20system%20designed%0Afor%20multiple%20partially%20overlapped%20camera%20systems%2C%20named%20MAVIS.%20Our%20framework%0Afully%20exploits%20the%20benefits%20of%20wide%20field-of-view%20from%20multi-camera%20systems%2C%0Aand%20the%20metric%20scale%20measurements%20provided%20by%20an%20inertial%20measurement%20unit%0A%28IMU%29.%20We%20introduce%20an%20improved%20IMU%20pre-integration%20formulation%20based%20on%20the%0Aexponential%20function%20of%20an%20automorphism%20of%20SE_2%283%29%2C%20which%20can%20effectively%0Aenhance%20tracking%20performance%20under%20fast%20rotational%20motion%20and%20extended%0Aintegration%20time.%20Furthermore%2C%20we%20extend%20conventional%20front-end%20tracking%20and%0Aback-end%20optimization%20module%20designed%20for%20monocular%20or%20stereo%20setup%20towards%0Amulti-camera%20systems%2C%20and%20introduce%20implementation%20details%20that%20contribute%20to%0Athe%20performance%20of%20our%20system%20in%20challenging%20scenarios.%20The%20practical%20validity%0Aof%20our%20approach%20is%20supported%20by%20our%20experiments%20on%20public%20datasets.%20Our%20MAVIS%0Awon%20the%20first%20place%20in%20all%20the%20vision-IMU%20tracks%20%28single%20and%20multi-session%0ASLAM%29%20on%20Hilti%20SLAM%20Challenge%202023%20with%201.7%20times%20the%20score%20compared%20to%20the%0Asecond%20place.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08142v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAVIS%253A%2520Multi-Camera%2520Augmented%2520Visual-Inertial%2520SLAM%2520using%2520SE2%25283%2529%2520Based%250A%2520%2520Exact%2520IMU%2520Pre-integration%26entry.906535625%3DYifu%2520Wang%2520and%2520Yonhon%2520Ng%2520and%2520Inkyu%2520Sa%2520and%2520Alvaro%2520Parra%2520and%2520Cristian%2520Rodriguez%2520and%2520Tao%2520Jun%2520Lin%2520and%2520Hongdong%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520optimization-based%2520Visual-Inertial%2520SLAM%2520system%2520designed%250Afor%2520multiple%2520partially%2520overlapped%2520camera%2520systems%252C%2520named%2520MAVIS.%2520Our%2520framework%250Afully%2520exploits%2520the%2520benefits%2520of%2520wide%2520field-of-view%2520from%2520multi-camera%2520systems%252C%250Aand%2520the%2520metric%2520scale%2520measurements%2520provided%2520by%2520an%2520inertial%2520measurement%2520unit%250A%2528IMU%2529.%2520We%2520introduce%2520an%2520improved%2520IMU%2520pre-integration%2520formulation%2520based%2520on%2520the%250Aexponential%2520function%2520of%2520an%2520automorphism%2520of%2520SE_2%25283%2529%252C%2520which%2520can%2520effectively%250Aenhance%2520tracking%2520performance%2520under%2520fast%2520rotational%2520motion%2520and%2520extended%250Aintegration%2520time.%2520Furthermore%252C%2520we%2520extend%2520conventional%2520front-end%2520tracking%2520and%250Aback-end%2520optimization%2520module%2520designed%2520for%2520monocular%2520or%2520stereo%2520setup%2520towards%250Amulti-camera%2520systems%252C%2520and%2520introduce%2520implementation%2520details%2520that%2520contribute%2520to%250Athe%2520performance%2520of%2520our%2520system%2520in%2520challenging%2520scenarios.%2520The%2520practical%2520validity%250Aof%2520our%2520approach%2520is%2520supported%2520by%2520our%2520experiments%2520on%2520public%2520datasets.%2520Our%2520MAVIS%250Awon%2520the%2520first%2520place%2520in%2520all%2520the%2520vision-IMU%2520tracks%2520%2528single%2520and%2520multi-session%250ASLAM%2529%2520on%2520Hilti%2520SLAM%2520Challenge%25202023%2520with%25201.7%2520times%2520the%2520score%2520compared%2520to%2520the%250Asecond%2520place.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08142v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAVIS%3A%20Multi-Camera%20Augmented%20Visual-Inertial%20SLAM%20using%20SE2%283%29%20Based%0A%20%20Exact%20IMU%20Pre-integration&entry.906535625=Yifu%20Wang%20and%20Yonhon%20Ng%20and%20Inkyu%20Sa%20and%20Alvaro%20Parra%20and%20Cristian%20Rodriguez%20and%20Tao%20Jun%20Lin%20and%20Hongdong%20Li&entry.1292438233=%20%20We%20present%20a%20novel%20optimization-based%20Visual-Inertial%20SLAM%20system%20designed%0Afor%20multiple%20partially%20overlapped%20camera%20systems%2C%20named%20MAVIS.%20Our%20framework%0Afully%20exploits%20the%20benefits%20of%20wide%20field-of-view%20from%20multi-camera%20systems%2C%0Aand%20the%20metric%20scale%20measurements%20provided%20by%20an%20inertial%20measurement%20unit%0A%28IMU%29.%20We%20introduce%20an%20improved%20IMU%20pre-integration%20formulation%20based%20on%20the%0Aexponential%20function%20of%20an%20automorphism%20of%20SE_2%283%29%2C%20which%20can%20effectively%0Aenhance%20tracking%20performance%20under%20fast%20rotational%20motion%20and%20extended%0Aintegration%20time.%20Furthermore%2C%20we%20extend%20conventional%20front-end%20tracking%20and%0Aback-end%20optimization%20module%20designed%20for%20monocular%20or%20stereo%20setup%20towards%0Amulti-camera%20systems%2C%20and%20introduce%20implementation%20details%20that%20contribute%20to%0Athe%20performance%20of%20our%20system%20in%20challenging%20scenarios.%20The%20practical%20validity%0Aof%20our%20approach%20is%20supported%20by%20our%20experiments%20on%20public%20datasets.%20Our%20MAVIS%0Awon%20the%20first%20place%20in%20all%20the%20vision-IMU%20tracks%20%28single%20and%20multi-session%0ASLAM%29%20on%20Hilti%20SLAM%20Challenge%202023%20with%201.7%20times%20the%20score%20compared%20to%20the%0Asecond%20place.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08142v5&entry.124074799=Read"},
{"title": "Monocular Occupancy Prediction for Scalable Indoor Scenes", "author": "Hongxiao Yu and Yuqi Wang and Yuntao Chen and Zhaoxiang Zhang", "abstract": "  Camera-based 3D occupancy prediction has recently garnered increasing\nattention in outdoor driving scenes. However, research in indoor scenes remains\nrelatively unexplored. The core differences in indoor scenes lie in the\ncomplexity of scene scale and the variance in object size. In this paper, we\npropose a novel method, named ISO, for predicting indoor scene occupancy using\nmonocular images. ISO harnesses the advantages of a pretrained depth model to\nachieve accurate depth predictions. Furthermore, we introduce the Dual Feature\nLine of Sight Projection (D-FLoSP) module within ISO, which enhances the\nlearning of 3D voxel features. To foster further research in this domain, we\nintroduce Occ-ScanNet, a large-scale occupancy benchmark for indoor scenes.\nWith a dataset size 40 times larger than the NYUv2 dataset, it facilitates\nfuture scalable research in indoor scene analysis. Experimental results on both\nNYUv2 and Occ-ScanNet demonstrate that our method achieves state-of-the-art\nperformance. The dataset and code are made publicly at\nhttps://github.com/hongxiaoy/ISO.git.\n", "link": "http://arxiv.org/abs/2407.11730v1", "date": "2024-07-16", "relevancy": 2.2103, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.551}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Occupancy%20Prediction%20for%20Scalable%20Indoor%20Scenes&body=Title%3A%20Monocular%20Occupancy%20Prediction%20for%20Scalable%20Indoor%20Scenes%0AAuthor%3A%20Hongxiao%20Yu%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Camera-based%203D%20occupancy%20prediction%20has%20recently%20garnered%20increasing%0Aattention%20in%20outdoor%20driving%20scenes.%20However%2C%20research%20in%20indoor%20scenes%20remains%0Arelatively%20unexplored.%20The%20core%20differences%20in%20indoor%20scenes%20lie%20in%20the%0Acomplexity%20of%20scene%20scale%20and%20the%20variance%20in%20object%20size.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%2C%20named%20ISO%2C%20for%20predicting%20indoor%20scene%20occupancy%20using%0Amonocular%20images.%20ISO%20harnesses%20the%20advantages%20of%20a%20pretrained%20depth%20model%20to%0Aachieve%20accurate%20depth%20predictions.%20Furthermore%2C%20we%20introduce%20the%20Dual%20Feature%0ALine%20of%20Sight%20Projection%20%28D-FLoSP%29%20module%20within%20ISO%2C%20which%20enhances%20the%0Alearning%20of%203D%20voxel%20features.%20To%20foster%20further%20research%20in%20this%20domain%2C%20we%0Aintroduce%20Occ-ScanNet%2C%20a%20large-scale%20occupancy%20benchmark%20for%20indoor%20scenes.%0AWith%20a%20dataset%20size%2040%20times%20larger%20than%20the%20NYUv2%20dataset%2C%20it%20facilitates%0Afuture%20scalable%20research%20in%20indoor%20scene%20analysis.%20Experimental%20results%20on%20both%0ANYUv2%20and%20Occ-ScanNet%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%20The%20dataset%20and%20code%20are%20made%20publicly%20at%0Ahttps%3A//github.com/hongxiaoy/ISO.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Occupancy%2520Prediction%2520for%2520Scalable%2520Indoor%2520Scenes%26entry.906535625%3DHongxiao%2520Yu%2520and%2520Yuqi%2520Wang%2520and%2520Yuntao%2520Chen%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Camera-based%25203D%2520occupancy%2520prediction%2520has%2520recently%2520garnered%2520increasing%250Aattention%2520in%2520outdoor%2520driving%2520scenes.%2520However%252C%2520research%2520in%2520indoor%2520scenes%2520remains%250Arelatively%2520unexplored.%2520The%2520core%2520differences%2520in%2520indoor%2520scenes%2520lie%2520in%2520the%250Acomplexity%2520of%2520scene%2520scale%2520and%2520the%2520variance%2520in%2520object%2520size.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520method%252C%2520named%2520ISO%252C%2520for%2520predicting%2520indoor%2520scene%2520occupancy%2520using%250Amonocular%2520images.%2520ISO%2520harnesses%2520the%2520advantages%2520of%2520a%2520pretrained%2520depth%2520model%2520to%250Aachieve%2520accurate%2520depth%2520predictions.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Dual%2520Feature%250ALine%2520of%2520Sight%2520Projection%2520%2528D-FLoSP%2529%2520module%2520within%2520ISO%252C%2520which%2520enhances%2520the%250Alearning%2520of%25203D%2520voxel%2520features.%2520To%2520foster%2520further%2520research%2520in%2520this%2520domain%252C%2520we%250Aintroduce%2520Occ-ScanNet%252C%2520a%2520large-scale%2520occupancy%2520benchmark%2520for%2520indoor%2520scenes.%250AWith%2520a%2520dataset%2520size%252040%2520times%2520larger%2520than%2520the%2520NYUv2%2520dataset%252C%2520it%2520facilitates%250Afuture%2520scalable%2520research%2520in%2520indoor%2520scene%2520analysis.%2520Experimental%2520results%2520on%2520both%250ANYUv2%2520and%2520Occ-ScanNet%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance.%2520The%2520dataset%2520and%2520code%2520are%2520made%2520publicly%2520at%250Ahttps%253A//github.com/hongxiaoy/ISO.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Occupancy%20Prediction%20for%20Scalable%20Indoor%20Scenes&entry.906535625=Hongxiao%20Yu%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Camera-based%203D%20occupancy%20prediction%20has%20recently%20garnered%20increasing%0Aattention%20in%20outdoor%20driving%20scenes.%20However%2C%20research%20in%20indoor%20scenes%20remains%0Arelatively%20unexplored.%20The%20core%20differences%20in%20indoor%20scenes%20lie%20in%20the%0Acomplexity%20of%20scene%20scale%20and%20the%20variance%20in%20object%20size.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%2C%20named%20ISO%2C%20for%20predicting%20indoor%20scene%20occupancy%20using%0Amonocular%20images.%20ISO%20harnesses%20the%20advantages%20of%20a%20pretrained%20depth%20model%20to%0Aachieve%20accurate%20depth%20predictions.%20Furthermore%2C%20we%20introduce%20the%20Dual%20Feature%0ALine%20of%20Sight%20Projection%20%28D-FLoSP%29%20module%20within%20ISO%2C%20which%20enhances%20the%0Alearning%20of%203D%20voxel%20features.%20To%20foster%20further%20research%20in%20this%20domain%2C%20we%0Aintroduce%20Occ-ScanNet%2C%20a%20large-scale%20occupancy%20benchmark%20for%20indoor%20scenes.%0AWith%20a%20dataset%20size%2040%20times%20larger%20than%20the%20NYUv2%20dataset%2C%20it%20facilitates%0Afuture%20scalable%20research%20in%20indoor%20scene%20analysis.%20Experimental%20results%20on%20both%0ANYUv2%20and%20Occ-ScanNet%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%20The%20dataset%20and%20code%20are%20made%20publicly%20at%0Ahttps%3A//github.com/hongxiaoy/ISO.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11730v1&entry.124074799=Read"},
{"title": "MapDistill: Boosting Efficient Camera-based HD Map Construction via\n  Camera-LiDAR Fusion Model Distillation", "author": "Xiaoshuai Hao and Ruikai Li and Hui Zhang and Dingzhe Li and Rong Yin and Sangil Jung and Seung-In Park and ByungIn Yoo and Haimei Zhao and Jing Zhang", "abstract": "  Online high-definition (HD) map construction is an important and challenging\ntask in autonomous driving. Recently, there has been a growing interest in\ncost-effective multi-view camera-based methods without relying on other sensors\nlike LiDAR. However, these methods suffer from a lack of explicit depth\ninformation, necessitating the use of large models to achieve satisfactory\nperformance. To address this, we employ the Knowledge Distillation (KD) idea\nfor efficient HD map construction for the first time and introduce a novel\nKD-based approach called MapDistill to transfer knowledge from a\nhigh-performance camera-LiDAR fusion model to a lightweight camera-only model.\nSpecifically, we adopt the teacher-student architecture, i.e., a camera-LiDAR\nfusion model as the teacher and a lightweight camera model as the student, and\ndevise a dual BEV transform module to facilitate cross-modal knowledge\ndistillation while maintaining cost-effective camera-only deployment.\nAdditionally, we present a comprehensive distillation scheme encompassing\ncross-modal relation distillation, dual-level feature distillation, and map\nhead distillation. This approach alleviates knowledge transfer challenges\nbetween modalities, enabling the student model to learn improved feature\nrepresentations for HD map construction. Experimental results on the\nchallenging nuScenes dataset demonstrate the effectiveness of MapDistill,\nsurpassing existing competitors by over 7.7 mAP or 4.5X speedup.\n", "link": "http://arxiv.org/abs/2407.11682v1", "date": "2024-07-16", "relevancy": 2.2049, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.55}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapDistill%3A%20Boosting%20Efficient%20Camera-based%20HD%20Map%20Construction%20via%0A%20%20Camera-LiDAR%20Fusion%20Model%20Distillation&body=Title%3A%20MapDistill%3A%20Boosting%20Efficient%20Camera-based%20HD%20Map%20Construction%20via%0A%20%20Camera-LiDAR%20Fusion%20Model%20Distillation%0AAuthor%3A%20Xiaoshuai%20Hao%20and%20Ruikai%20Li%20and%20Hui%20Zhang%20and%20Dingzhe%20Li%20and%20Rong%20Yin%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo%20and%20Haimei%20Zhao%20and%20Jing%20Zhang%0AAbstract%3A%20%20%20Online%20high-definition%20%28HD%29%20map%20construction%20is%20an%20important%20and%20challenging%0Atask%20in%20autonomous%20driving.%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%0Acost-effective%20multi-view%20camera-based%20methods%20without%20relying%20on%20other%20sensors%0Alike%20LiDAR.%20However%2C%20these%20methods%20suffer%20from%20a%20lack%20of%20explicit%20depth%0Ainformation%2C%20necessitating%20the%20use%20of%20large%20models%20to%20achieve%20satisfactory%0Aperformance.%20To%20address%20this%2C%20we%20employ%20the%20Knowledge%20Distillation%20%28KD%29%20idea%0Afor%20efficient%20HD%20map%20construction%20for%20the%20first%20time%20and%20introduce%20a%20novel%0AKD-based%20approach%20called%20MapDistill%20to%20transfer%20knowledge%20from%20a%0Ahigh-performance%20camera-LiDAR%20fusion%20model%20to%20a%20lightweight%20camera-only%20model.%0ASpecifically%2C%20we%20adopt%20the%20teacher-student%20architecture%2C%20i.e.%2C%20a%20camera-LiDAR%0Afusion%20model%20as%20the%20teacher%20and%20a%20lightweight%20camera%20model%20as%20the%20student%2C%20and%0Adevise%20a%20dual%20BEV%20transform%20module%20to%20facilitate%20cross-modal%20knowledge%0Adistillation%20while%20maintaining%20cost-effective%20camera-only%20deployment.%0AAdditionally%2C%20we%20present%20a%20comprehensive%20distillation%20scheme%20encompassing%0Across-modal%20relation%20distillation%2C%20dual-level%20feature%20distillation%2C%20and%20map%0Ahead%20distillation.%20This%20approach%20alleviates%20knowledge%20transfer%20challenges%0Abetween%20modalities%2C%20enabling%20the%20student%20model%20to%20learn%20improved%20feature%0Arepresentations%20for%20HD%20map%20construction.%20Experimental%20results%20on%20the%0Achallenging%20nuScenes%20dataset%20demonstrate%20the%20effectiveness%20of%20MapDistill%2C%0Asurpassing%20existing%20competitors%20by%20over%207.7%20mAP%20or%204.5X%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapDistill%253A%2520Boosting%2520Efficient%2520Camera-based%2520HD%2520Map%2520Construction%2520via%250A%2520%2520Camera-LiDAR%2520Fusion%2520Model%2520Distillation%26entry.906535625%3DXiaoshuai%2520Hao%2520and%2520Ruikai%2520Li%2520and%2520Hui%2520Zhang%2520and%2520Dingzhe%2520Li%2520and%2520Rong%2520Yin%2520and%2520Sangil%2520Jung%2520and%2520Seung-In%2520Park%2520and%2520ByungIn%2520Yoo%2520and%2520Haimei%2520Zhao%2520and%2520Jing%2520Zhang%26entry.1292438233%3D%2520%2520Online%2520high-definition%2520%2528HD%2529%2520map%2520construction%2520is%2520an%2520important%2520and%2520challenging%250Atask%2520in%2520autonomous%2520driving.%2520Recently%252C%2520there%2520has%2520been%2520a%2520growing%2520interest%2520in%250Acost-effective%2520multi-view%2520camera-based%2520methods%2520without%2520relying%2520on%2520other%2520sensors%250Alike%2520LiDAR.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520a%2520lack%2520of%2520explicit%2520depth%250Ainformation%252C%2520necessitating%2520the%2520use%2520of%2520large%2520models%2520to%2520achieve%2520satisfactory%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520employ%2520the%2520Knowledge%2520Distillation%2520%2528KD%2529%2520idea%250Afor%2520efficient%2520HD%2520map%2520construction%2520for%2520the%2520first%2520time%2520and%2520introduce%2520a%2520novel%250AKD-based%2520approach%2520called%2520MapDistill%2520to%2520transfer%2520knowledge%2520from%2520a%250Ahigh-performance%2520camera-LiDAR%2520fusion%2520model%2520to%2520a%2520lightweight%2520camera-only%2520model.%250ASpecifically%252C%2520we%2520adopt%2520the%2520teacher-student%2520architecture%252C%2520i.e.%252C%2520a%2520camera-LiDAR%250Afusion%2520model%2520as%2520the%2520teacher%2520and%2520a%2520lightweight%2520camera%2520model%2520as%2520the%2520student%252C%2520and%250Adevise%2520a%2520dual%2520BEV%2520transform%2520module%2520to%2520facilitate%2520cross-modal%2520knowledge%250Adistillation%2520while%2520maintaining%2520cost-effective%2520camera-only%2520deployment.%250AAdditionally%252C%2520we%2520present%2520a%2520comprehensive%2520distillation%2520scheme%2520encompassing%250Across-modal%2520relation%2520distillation%252C%2520dual-level%2520feature%2520distillation%252C%2520and%2520map%250Ahead%2520distillation.%2520This%2520approach%2520alleviates%2520knowledge%2520transfer%2520challenges%250Abetween%2520modalities%252C%2520enabling%2520the%2520student%2520model%2520to%2520learn%2520improved%2520feature%250Arepresentations%2520for%2520HD%2520map%2520construction.%2520Experimental%2520results%2520on%2520the%250Achallenging%2520nuScenes%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520MapDistill%252C%250Asurpassing%2520existing%2520competitors%2520by%2520over%25207.7%2520mAP%2520or%25204.5X%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapDistill%3A%20Boosting%20Efficient%20Camera-based%20HD%20Map%20Construction%20via%0A%20%20Camera-LiDAR%20Fusion%20Model%20Distillation&entry.906535625=Xiaoshuai%20Hao%20and%20Ruikai%20Li%20and%20Hui%20Zhang%20and%20Dingzhe%20Li%20and%20Rong%20Yin%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo%20and%20Haimei%20Zhao%20and%20Jing%20Zhang&entry.1292438233=%20%20Online%20high-definition%20%28HD%29%20map%20construction%20is%20an%20important%20and%20challenging%0Atask%20in%20autonomous%20driving.%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%0Acost-effective%20multi-view%20camera-based%20methods%20without%20relying%20on%20other%20sensors%0Alike%20LiDAR.%20However%2C%20these%20methods%20suffer%20from%20a%20lack%20of%20explicit%20depth%0Ainformation%2C%20necessitating%20the%20use%20of%20large%20models%20to%20achieve%20satisfactory%0Aperformance.%20To%20address%20this%2C%20we%20employ%20the%20Knowledge%20Distillation%20%28KD%29%20idea%0Afor%20efficient%20HD%20map%20construction%20for%20the%20first%20time%20and%20introduce%20a%20novel%0AKD-based%20approach%20called%20MapDistill%20to%20transfer%20knowledge%20from%20a%0Ahigh-performance%20camera-LiDAR%20fusion%20model%20to%20a%20lightweight%20camera-only%20model.%0ASpecifically%2C%20we%20adopt%20the%20teacher-student%20architecture%2C%20i.e.%2C%20a%20camera-LiDAR%0Afusion%20model%20as%20the%20teacher%20and%20a%20lightweight%20camera%20model%20as%20the%20student%2C%20and%0Adevise%20a%20dual%20BEV%20transform%20module%20to%20facilitate%20cross-modal%20knowledge%0Adistillation%20while%20maintaining%20cost-effective%20camera-only%20deployment.%0AAdditionally%2C%20we%20present%20a%20comprehensive%20distillation%20scheme%20encompassing%0Across-modal%20relation%20distillation%2C%20dual-level%20feature%20distillation%2C%20and%20map%0Ahead%20distillation.%20This%20approach%20alleviates%20knowledge%20transfer%20challenges%0Abetween%20modalities%2C%20enabling%20the%20student%20model%20to%20learn%20improved%20feature%0Arepresentations%20for%20HD%20map%20construction.%20Experimental%20results%20on%20the%0Achallenging%20nuScenes%20dataset%20demonstrate%20the%20effectiveness%20of%20MapDistill%2C%0Asurpassing%20existing%20competitors%20by%20over%207.7%20mAP%20or%204.5X%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11682v1&entry.124074799=Read"},
{"title": "Enhanced Safety in Autonomous Driving: Integrating Latent State\n  Diffusion Model for End-to-End Navigation", "author": "Detian Chu and Linyuan Bai and Jianuo Huang and Zhenlong Fang and Peng Zhang and Wei Kang", "abstract": "  With the advancement of autonomous driving, ensuring safety during motion\nplanning and navigation is becoming more and more important. However, most\nend-to-end planning methods suffer from a lack of safety. This research\naddresses the safety issue in the control optimization problem of autonomous\ndriving, formulated as Constrained Markov Decision Processes (CMDPs). We\npropose a novel, model-based approach for policy optimization, utilizing a\nconditional Value-at-Risk based Soft Actor Critic to manage constraints in\ncomplex, high-dimensional state spaces effectively. Our method introduces a\nworst-case actor to guide safe exploration, ensuring rigorous adherence to\nsafety requirements even in unpredictable scenarios. The policy optimization\nemploys the Augmented Lagrangian method and leverages latent diffusion models\nto predict and simulate future trajectories. This dual approach not only aids\nin navigating environments safely but also refines the policy's performance by\nintegrating distribution modeling to account for environmental uncertainties.\nEmpirical evaluations conducted in both simulated and real environment\ndemonstrate that our approach outperforms existing methods in terms of safety,\nefficiency, and decision-making capabilities.\n", "link": "http://arxiv.org/abs/2407.06317v3", "date": "2024-07-16", "relevancy": 2.1992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5575}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Safety%20in%20Autonomous%20Driving%3A%20Integrating%20Latent%20State%0A%20%20Diffusion%20Model%20for%20End-to-End%20Navigation&body=Title%3A%20Enhanced%20Safety%20in%20Autonomous%20Driving%3A%20Integrating%20Latent%20State%0A%20%20Diffusion%20Model%20for%20End-to-End%20Navigation%0AAuthor%3A%20Detian%20Chu%20and%20Linyuan%20Bai%20and%20Jianuo%20Huang%20and%20Zhenlong%20Fang%20and%20Peng%20Zhang%20and%20Wei%20Kang%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20autonomous%20driving%2C%20ensuring%20safety%20during%20motion%0Aplanning%20and%20navigation%20is%20becoming%20more%20and%20more%20important.%20However%2C%20most%0Aend-to-end%20planning%20methods%20suffer%20from%20a%20lack%20of%20safety.%20This%20research%0Aaddresses%20the%20safety%20issue%20in%20the%20control%20optimization%20problem%20of%20autonomous%0Adriving%2C%20formulated%20as%20Constrained%20Markov%20Decision%20Processes%20%28CMDPs%29.%20We%0Apropose%20a%20novel%2C%20model-based%20approach%20for%20policy%20optimization%2C%20utilizing%20a%0Aconditional%20Value-at-Risk%20based%20Soft%20Actor%20Critic%20to%20manage%20constraints%20in%0Acomplex%2C%20high-dimensional%20state%20spaces%20effectively.%20Our%20method%20introduces%20a%0Aworst-case%20actor%20to%20guide%20safe%20exploration%2C%20ensuring%20rigorous%20adherence%20to%0Asafety%20requirements%20even%20in%20unpredictable%20scenarios.%20The%20policy%20optimization%0Aemploys%20the%20Augmented%20Lagrangian%20method%20and%20leverages%20latent%20diffusion%20models%0Ato%20predict%20and%20simulate%20future%20trajectories.%20This%20dual%20approach%20not%20only%20aids%0Ain%20navigating%20environments%20safely%20but%20also%20refines%20the%20policy%27s%20performance%20by%0Aintegrating%20distribution%20modeling%20to%20account%20for%20environmental%20uncertainties.%0AEmpirical%20evaluations%20conducted%20in%20both%20simulated%20and%20real%20environment%0Ademonstrate%20that%20our%20approach%20outperforms%20existing%20methods%20in%20terms%20of%20safety%2C%0Aefficiency%2C%20and%20decision-making%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06317v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Safety%2520in%2520Autonomous%2520Driving%253A%2520Integrating%2520Latent%2520State%250A%2520%2520Diffusion%2520Model%2520for%2520End-to-End%2520Navigation%26entry.906535625%3DDetian%2520Chu%2520and%2520Linyuan%2520Bai%2520and%2520Jianuo%2520Huang%2520and%2520Zhenlong%2520Fang%2520and%2520Peng%2520Zhang%2520and%2520Wei%2520Kang%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520autonomous%2520driving%252C%2520ensuring%2520safety%2520during%2520motion%250Aplanning%2520and%2520navigation%2520is%2520becoming%2520more%2520and%2520more%2520important.%2520However%252C%2520most%250Aend-to-end%2520planning%2520methods%2520suffer%2520from%2520a%2520lack%2520of%2520safety.%2520This%2520research%250Aaddresses%2520the%2520safety%2520issue%2520in%2520the%2520control%2520optimization%2520problem%2520of%2520autonomous%250Adriving%252C%2520formulated%2520as%2520Constrained%2520Markov%2520Decision%2520Processes%2520%2528CMDPs%2529.%2520We%250Apropose%2520a%2520novel%252C%2520model-based%2520approach%2520for%2520policy%2520optimization%252C%2520utilizing%2520a%250Aconditional%2520Value-at-Risk%2520based%2520Soft%2520Actor%2520Critic%2520to%2520manage%2520constraints%2520in%250Acomplex%252C%2520high-dimensional%2520state%2520spaces%2520effectively.%2520Our%2520method%2520introduces%2520a%250Aworst-case%2520actor%2520to%2520guide%2520safe%2520exploration%252C%2520ensuring%2520rigorous%2520adherence%2520to%250Asafety%2520requirements%2520even%2520in%2520unpredictable%2520scenarios.%2520The%2520policy%2520optimization%250Aemploys%2520the%2520Augmented%2520Lagrangian%2520method%2520and%2520leverages%2520latent%2520diffusion%2520models%250Ato%2520predict%2520and%2520simulate%2520future%2520trajectories.%2520This%2520dual%2520approach%2520not%2520only%2520aids%250Ain%2520navigating%2520environments%2520safely%2520but%2520also%2520refines%2520the%2520policy%2527s%2520performance%2520by%250Aintegrating%2520distribution%2520modeling%2520to%2520account%2520for%2520environmental%2520uncertainties.%250AEmpirical%2520evaluations%2520conducted%2520in%2520both%2520simulated%2520and%2520real%2520environment%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520safety%252C%250Aefficiency%252C%2520and%2520decision-making%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06317v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Safety%20in%20Autonomous%20Driving%3A%20Integrating%20Latent%20State%0A%20%20Diffusion%20Model%20for%20End-to-End%20Navigation&entry.906535625=Detian%20Chu%20and%20Linyuan%20Bai%20and%20Jianuo%20Huang%20and%20Zhenlong%20Fang%20and%20Peng%20Zhang%20and%20Wei%20Kang&entry.1292438233=%20%20With%20the%20advancement%20of%20autonomous%20driving%2C%20ensuring%20safety%20during%20motion%0Aplanning%20and%20navigation%20is%20becoming%20more%20and%20more%20important.%20However%2C%20most%0Aend-to-end%20planning%20methods%20suffer%20from%20a%20lack%20of%20safety.%20This%20research%0Aaddresses%20the%20safety%20issue%20in%20the%20control%20optimization%20problem%20of%20autonomous%0Adriving%2C%20formulated%20as%20Constrained%20Markov%20Decision%20Processes%20%28CMDPs%29.%20We%0Apropose%20a%20novel%2C%20model-based%20approach%20for%20policy%20optimization%2C%20utilizing%20a%0Aconditional%20Value-at-Risk%20based%20Soft%20Actor%20Critic%20to%20manage%20constraints%20in%0Acomplex%2C%20high-dimensional%20state%20spaces%20effectively.%20Our%20method%20introduces%20a%0Aworst-case%20actor%20to%20guide%20safe%20exploration%2C%20ensuring%20rigorous%20adherence%20to%0Asafety%20requirements%20even%20in%20unpredictable%20scenarios.%20The%20policy%20optimization%0Aemploys%20the%20Augmented%20Lagrangian%20method%20and%20leverages%20latent%20diffusion%20models%0Ato%20predict%20and%20simulate%20future%20trajectories.%20This%20dual%20approach%20not%20only%20aids%0Ain%20navigating%20environments%20safely%20but%20also%20refines%20the%20policy%27s%20performance%20by%0Aintegrating%20distribution%20modeling%20to%20account%20for%20environmental%20uncertainties.%0AEmpirical%20evaluations%20conducted%20in%20both%20simulated%20and%20real%20environment%0Ademonstrate%20that%20our%20approach%20outperforms%20existing%20methods%20in%20terms%20of%20safety%2C%0Aefficiency%2C%20and%20decision-making%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06317v3&entry.124074799=Read"},
{"title": "Video-Language Alignment Pre-training via Spatio-Temporal Graph\n  Transformer", "author": "Shi-Xue Zhang and Hongfa Wang and Xiaobin Zhu and Weibo Gu and Tianjin Zhang and Chun Yang and Wei Liu and Xu-Cheng Yin", "abstract": "  Video-language alignment is a crucial multi-modal task that benefits various\ndownstream applications, e.g., video-text retrieval and video question\nanswering. Existing methods either utilize multi-modal information in\nvideo-text pairs or apply global and local alignment techniques to promote\nalignment precision. However, these methods often fail to fully explore the\nspatio-temporal relationships among vision tokens within video and across\ndifferent video-text pairs. In this paper, we propose a novel Spatio-Temporal\nGraph Transformer module to uniformly learn spatial and temporal contexts for\nvideo-language alignment pre-training (dubbed STGT). Specifically, our STGT\ncombines spatio-temporal graph structure information with attention in\ntransformer block, effectively utilizing the spatio-temporal contexts. In this\nway, we can model the relationships between vision tokens, promoting video-text\nalignment precision for benefiting downstream tasks. In addition, we propose a\nself-similarity alignment loss to explore the inherent self-similarity in the\nvideo and text. With the initial optimization achieved by contrastive learning,\nit can further promote the alignment accuracy between video and text.\nExperimental results on challenging downstream tasks, including video-text\nretrieval and video question answering, verify the superior performance of our\nmethod.\n", "link": "http://arxiv.org/abs/2407.11677v1", "date": "2024-07-16", "relevancy": 2.1908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5672}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5565}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Language%20Alignment%20Pre-training%20via%20Spatio-Temporal%20Graph%0A%20%20Transformer&body=Title%3A%20Video-Language%20Alignment%20Pre-training%20via%20Spatio-Temporal%20Graph%0A%20%20Transformer%0AAuthor%3A%20Shi-Xue%20Zhang%20and%20Hongfa%20Wang%20and%20Xiaobin%20Zhu%20and%20Weibo%20Gu%20and%20Tianjin%20Zhang%20and%20Chun%20Yang%20and%20Wei%20Liu%20and%20Xu-Cheng%20Yin%0AAbstract%3A%20%20%20Video-language%20alignment%20is%20a%20crucial%20multi-modal%20task%20that%20benefits%20various%0Adownstream%20applications%2C%20e.g.%2C%20video-text%20retrieval%20and%20video%20question%0Aanswering.%20Existing%20methods%20either%20utilize%20multi-modal%20information%20in%0Avideo-text%20pairs%20or%20apply%20global%20and%20local%20alignment%20techniques%20to%20promote%0Aalignment%20precision.%20However%2C%20these%20methods%20often%20fail%20to%20fully%20explore%20the%0Aspatio-temporal%20relationships%20among%20vision%20tokens%20within%20video%20and%20across%0Adifferent%20video-text%20pairs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Spatio-Temporal%0AGraph%20Transformer%20module%20to%20uniformly%20learn%20spatial%20and%20temporal%20contexts%20for%0Avideo-language%20alignment%20pre-training%20%28dubbed%20STGT%29.%20Specifically%2C%20our%20STGT%0Acombines%20spatio-temporal%20graph%20structure%20information%20with%20attention%20in%0Atransformer%20block%2C%20effectively%20utilizing%20the%20spatio-temporal%20contexts.%20In%20this%0Away%2C%20we%20can%20model%20the%20relationships%20between%20vision%20tokens%2C%20promoting%20video-text%0Aalignment%20precision%20for%20benefiting%20downstream%20tasks.%20In%20addition%2C%20we%20propose%20a%0Aself-similarity%20alignment%20loss%20to%20explore%20the%20inherent%20self-similarity%20in%20the%0Avideo%20and%20text.%20With%20the%20initial%20optimization%20achieved%20by%20contrastive%20learning%2C%0Ait%20can%20further%20promote%20the%20alignment%20accuracy%20between%20video%20and%20text.%0AExperimental%20results%20on%20challenging%20downstream%20tasks%2C%20including%20video-text%0Aretrieval%20and%20video%20question%20answering%2C%20verify%20the%20superior%20performance%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Language%2520Alignment%2520Pre-training%2520via%2520Spatio-Temporal%2520Graph%250A%2520%2520Transformer%26entry.906535625%3DShi-Xue%2520Zhang%2520and%2520Hongfa%2520Wang%2520and%2520Xiaobin%2520Zhu%2520and%2520Weibo%2520Gu%2520and%2520Tianjin%2520Zhang%2520and%2520Chun%2520Yang%2520and%2520Wei%2520Liu%2520and%2520Xu-Cheng%2520Yin%26entry.1292438233%3D%2520%2520Video-language%2520alignment%2520is%2520a%2520crucial%2520multi-modal%2520task%2520that%2520benefits%2520various%250Adownstream%2520applications%252C%2520e.g.%252C%2520video-text%2520retrieval%2520and%2520video%2520question%250Aanswering.%2520Existing%2520methods%2520either%2520utilize%2520multi-modal%2520information%2520in%250Avideo-text%2520pairs%2520or%2520apply%2520global%2520and%2520local%2520alignment%2520techniques%2520to%2520promote%250Aalignment%2520precision.%2520However%252C%2520these%2520methods%2520often%2520fail%2520to%2520fully%2520explore%2520the%250Aspatio-temporal%2520relationships%2520among%2520vision%2520tokens%2520within%2520video%2520and%2520across%250Adifferent%2520video-text%2520pairs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Spatio-Temporal%250AGraph%2520Transformer%2520module%2520to%2520uniformly%2520learn%2520spatial%2520and%2520temporal%2520contexts%2520for%250Avideo-language%2520alignment%2520pre-training%2520%2528dubbed%2520STGT%2529.%2520Specifically%252C%2520our%2520STGT%250Acombines%2520spatio-temporal%2520graph%2520structure%2520information%2520with%2520attention%2520in%250Atransformer%2520block%252C%2520effectively%2520utilizing%2520the%2520spatio-temporal%2520contexts.%2520In%2520this%250Away%252C%2520we%2520can%2520model%2520the%2520relationships%2520between%2520vision%2520tokens%252C%2520promoting%2520video-text%250Aalignment%2520precision%2520for%2520benefiting%2520downstream%2520tasks.%2520In%2520addition%252C%2520we%2520propose%2520a%250Aself-similarity%2520alignment%2520loss%2520to%2520explore%2520the%2520inherent%2520self-similarity%2520in%2520the%250Avideo%2520and%2520text.%2520With%2520the%2520initial%2520optimization%2520achieved%2520by%2520contrastive%2520learning%252C%250Ait%2520can%2520further%2520promote%2520the%2520alignment%2520accuracy%2520between%2520video%2520and%2520text.%250AExperimental%2520results%2520on%2520challenging%2520downstream%2520tasks%252C%2520including%2520video-text%250Aretrieval%2520and%2520video%2520question%2520answering%252C%2520verify%2520the%2520superior%2520performance%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Language%20Alignment%20Pre-training%20via%20Spatio-Temporal%20Graph%0A%20%20Transformer&entry.906535625=Shi-Xue%20Zhang%20and%20Hongfa%20Wang%20and%20Xiaobin%20Zhu%20and%20Weibo%20Gu%20and%20Tianjin%20Zhang%20and%20Chun%20Yang%20and%20Wei%20Liu%20and%20Xu-Cheng%20Yin&entry.1292438233=%20%20Video-language%20alignment%20is%20a%20crucial%20multi-modal%20task%20that%20benefits%20various%0Adownstream%20applications%2C%20e.g.%2C%20video-text%20retrieval%20and%20video%20question%0Aanswering.%20Existing%20methods%20either%20utilize%20multi-modal%20information%20in%0Avideo-text%20pairs%20or%20apply%20global%20and%20local%20alignment%20techniques%20to%20promote%0Aalignment%20precision.%20However%2C%20these%20methods%20often%20fail%20to%20fully%20explore%20the%0Aspatio-temporal%20relationships%20among%20vision%20tokens%20within%20video%20and%20across%0Adifferent%20video-text%20pairs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Spatio-Temporal%0AGraph%20Transformer%20module%20to%20uniformly%20learn%20spatial%20and%20temporal%20contexts%20for%0Avideo-language%20alignment%20pre-training%20%28dubbed%20STGT%29.%20Specifically%2C%20our%20STGT%0Acombines%20spatio-temporal%20graph%20structure%20information%20with%20attention%20in%0Atransformer%20block%2C%20effectively%20utilizing%20the%20spatio-temporal%20contexts.%20In%20this%0Away%2C%20we%20can%20model%20the%20relationships%20between%20vision%20tokens%2C%20promoting%20video-text%0Aalignment%20precision%20for%20benefiting%20downstream%20tasks.%20In%20addition%2C%20we%20propose%20a%0Aself-similarity%20alignment%20loss%20to%20explore%20the%20inherent%20self-similarity%20in%20the%0Avideo%20and%20text.%20With%20the%20initial%20optimization%20achieved%20by%20contrastive%20learning%2C%0Ait%20can%20further%20promote%20the%20alignment%20accuracy%20between%20video%20and%20text.%0AExperimental%20results%20on%20challenging%20downstream%20tasks%2C%20including%20video-text%0Aretrieval%20and%20video%20question%20answering%2C%20verify%20the%20superior%20performance%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11677v1&entry.124074799=Read"},
{"title": "Fairly Accurate: Optimizing Accuracy Parity in Fair Target-Group\n  Detection", "author": "Soumyajit Gupta and Venelin Kovatchev and Maria De-Arteaga and Matthew Lease", "abstract": "  In algorithmic toxicity detection pipelines, it is important to identify\nwhich demographic group(s) are the subject of a post, a task commonly known as\n\\textit{target (group) detection}. While accurate detection is clearly\nimportant, we further advocate a fairness objective: to provide equal\nprotection to all groups who may be targeted. To this end, we adopt\n\\textit{Accuracy Parity} (AP) -- balanced detection accuracy across groups --\nas our fairness objective. However, in order to align model training with our\nAP fairness objective, we require an equivalent loss function. Moreover, for\ngradient-based models such as neural networks, this loss function needs to be\ndifferentiable. Because no such loss function exists today for AP, we propose\n\\emph{Group Accuracy Parity} (GAP): the first differentiable loss function\nhaving a one-on-one mapping to AP. We empirically show that GAP addresses\ndisparate impact on groups for target detection. Furthermore, because a single\npost often targets multiple groups in practice, we also provide a mathematical\nextension of GAP to larger multi-group settings, something typically requiring\nheuristics in prior work. Our findings show that by optimizing AP, GAP better\nmitigates bias in comparison with other commonly employed loss functions.\n", "link": "http://arxiv.org/abs/2407.11933v1", "date": "2024-07-16", "relevancy": 2.1883, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4316}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairly%20Accurate%3A%20Optimizing%20Accuracy%20Parity%20in%20Fair%20Target-Group%0A%20%20Detection&body=Title%3A%20Fairly%20Accurate%3A%20Optimizing%20Accuracy%20Parity%20in%20Fair%20Target-Group%0A%20%20Detection%0AAuthor%3A%20Soumyajit%20Gupta%20and%20Venelin%20Kovatchev%20and%20Maria%20De-Arteaga%20and%20Matthew%20Lease%0AAbstract%3A%20%20%20In%20algorithmic%20toxicity%20detection%20pipelines%2C%20it%20is%20important%20to%20identify%0Awhich%20demographic%20group%28s%29%20are%20the%20subject%20of%20a%20post%2C%20a%20task%20commonly%20known%20as%0A%5Ctextit%7Btarget%20%28group%29%20detection%7D.%20While%20accurate%20detection%20is%20clearly%0Aimportant%2C%20we%20further%20advocate%20a%20fairness%20objective%3A%20to%20provide%20equal%0Aprotection%20to%20all%20groups%20who%20may%20be%20targeted.%20To%20this%20end%2C%20we%20adopt%0A%5Ctextit%7BAccuracy%20Parity%7D%20%28AP%29%20--%20balanced%20detection%20accuracy%20across%20groups%20--%0Aas%20our%20fairness%20objective.%20However%2C%20in%20order%20to%20align%20model%20training%20with%20our%0AAP%20fairness%20objective%2C%20we%20require%20an%20equivalent%20loss%20function.%20Moreover%2C%20for%0Agradient-based%20models%20such%20as%20neural%20networks%2C%20this%20loss%20function%20needs%20to%20be%0Adifferentiable.%20Because%20no%20such%20loss%20function%20exists%20today%20for%20AP%2C%20we%20propose%0A%5Cemph%7BGroup%20Accuracy%20Parity%7D%20%28GAP%29%3A%20the%20first%20differentiable%20loss%20function%0Ahaving%20a%20one-on-one%20mapping%20to%20AP.%20We%20empirically%20show%20that%20GAP%20addresses%0Adisparate%20impact%20on%20groups%20for%20target%20detection.%20Furthermore%2C%20because%20a%20single%0Apost%20often%20targets%20multiple%20groups%20in%20practice%2C%20we%20also%20provide%20a%20mathematical%0Aextension%20of%20GAP%20to%20larger%20multi-group%20settings%2C%20something%20typically%20requiring%0Aheuristics%20in%20prior%20work.%20Our%20findings%20show%20that%20by%20optimizing%20AP%2C%20GAP%20better%0Amitigates%20bias%20in%20comparison%20with%20other%20commonly%20employed%20loss%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairly%2520Accurate%253A%2520Optimizing%2520Accuracy%2520Parity%2520in%2520Fair%2520Target-Group%250A%2520%2520Detection%26entry.906535625%3DSoumyajit%2520Gupta%2520and%2520Venelin%2520Kovatchev%2520and%2520Maria%2520De-Arteaga%2520and%2520Matthew%2520Lease%26entry.1292438233%3D%2520%2520In%2520algorithmic%2520toxicity%2520detection%2520pipelines%252C%2520it%2520is%2520important%2520to%2520identify%250Awhich%2520demographic%2520group%2528s%2529%2520are%2520the%2520subject%2520of%2520a%2520post%252C%2520a%2520task%2520commonly%2520known%2520as%250A%255Ctextit%257Btarget%2520%2528group%2529%2520detection%257D.%2520While%2520accurate%2520detection%2520is%2520clearly%250Aimportant%252C%2520we%2520further%2520advocate%2520a%2520fairness%2520objective%253A%2520to%2520provide%2520equal%250Aprotection%2520to%2520all%2520groups%2520who%2520may%2520be%2520targeted.%2520To%2520this%2520end%252C%2520we%2520adopt%250A%255Ctextit%257BAccuracy%2520Parity%257D%2520%2528AP%2529%2520--%2520balanced%2520detection%2520accuracy%2520across%2520groups%2520--%250Aas%2520our%2520fairness%2520objective.%2520However%252C%2520in%2520order%2520to%2520align%2520model%2520training%2520with%2520our%250AAP%2520fairness%2520objective%252C%2520we%2520require%2520an%2520equivalent%2520loss%2520function.%2520Moreover%252C%2520for%250Agradient-based%2520models%2520such%2520as%2520neural%2520networks%252C%2520this%2520loss%2520function%2520needs%2520to%2520be%250Adifferentiable.%2520Because%2520no%2520such%2520loss%2520function%2520exists%2520today%2520for%2520AP%252C%2520we%2520propose%250A%255Cemph%257BGroup%2520Accuracy%2520Parity%257D%2520%2528GAP%2529%253A%2520the%2520first%2520differentiable%2520loss%2520function%250Ahaving%2520a%2520one-on-one%2520mapping%2520to%2520AP.%2520We%2520empirically%2520show%2520that%2520GAP%2520addresses%250Adisparate%2520impact%2520on%2520groups%2520for%2520target%2520detection.%2520Furthermore%252C%2520because%2520a%2520single%250Apost%2520often%2520targets%2520multiple%2520groups%2520in%2520practice%252C%2520we%2520also%2520provide%2520a%2520mathematical%250Aextension%2520of%2520GAP%2520to%2520larger%2520multi-group%2520settings%252C%2520something%2520typically%2520requiring%250Aheuristics%2520in%2520prior%2520work.%2520Our%2520findings%2520show%2520that%2520by%2520optimizing%2520AP%252C%2520GAP%2520better%250Amitigates%2520bias%2520in%2520comparison%2520with%2520other%2520commonly%2520employed%2520loss%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairly%20Accurate%3A%20Optimizing%20Accuracy%20Parity%20in%20Fair%20Target-Group%0A%20%20Detection&entry.906535625=Soumyajit%20Gupta%20and%20Venelin%20Kovatchev%20and%20Maria%20De-Arteaga%20and%20Matthew%20Lease&entry.1292438233=%20%20In%20algorithmic%20toxicity%20detection%20pipelines%2C%20it%20is%20important%20to%20identify%0Awhich%20demographic%20group%28s%29%20are%20the%20subject%20of%20a%20post%2C%20a%20task%20commonly%20known%20as%0A%5Ctextit%7Btarget%20%28group%29%20detection%7D.%20While%20accurate%20detection%20is%20clearly%0Aimportant%2C%20we%20further%20advocate%20a%20fairness%20objective%3A%20to%20provide%20equal%0Aprotection%20to%20all%20groups%20who%20may%20be%20targeted.%20To%20this%20end%2C%20we%20adopt%0A%5Ctextit%7BAccuracy%20Parity%7D%20%28AP%29%20--%20balanced%20detection%20accuracy%20across%20groups%20--%0Aas%20our%20fairness%20objective.%20However%2C%20in%20order%20to%20align%20model%20training%20with%20our%0AAP%20fairness%20objective%2C%20we%20require%20an%20equivalent%20loss%20function.%20Moreover%2C%20for%0Agradient-based%20models%20such%20as%20neural%20networks%2C%20this%20loss%20function%20needs%20to%20be%0Adifferentiable.%20Because%20no%20such%20loss%20function%20exists%20today%20for%20AP%2C%20we%20propose%0A%5Cemph%7BGroup%20Accuracy%20Parity%7D%20%28GAP%29%3A%20the%20first%20differentiable%20loss%20function%0Ahaving%20a%20one-on-one%20mapping%20to%20AP.%20We%20empirically%20show%20that%20GAP%20addresses%0Adisparate%20impact%20on%20groups%20for%20target%20detection.%20Furthermore%2C%20because%20a%20single%0Apost%20often%20targets%20multiple%20groups%20in%20practice%2C%20we%20also%20provide%20a%20mathematical%0Aextension%20of%20GAP%20to%20larger%20multi-group%20settings%2C%20something%20typically%20requiring%0Aheuristics%20in%20prior%20work.%20Our%20findings%20show%20that%20by%20optimizing%20AP%2C%20GAP%20better%0Amitigates%20bias%20in%20comparison%20with%20other%20commonly%20employed%20loss%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11933v1&entry.124074799=Read"},
{"title": "Relation DETR: Exploring Explicit Position Relation Prior for Object\n  Detection", "author": "Xiuquan Hou and Meiqin Liu and Senlin Zhang and Ping Wei and Badong Chen and Xuguang Lan", "abstract": "  This paper presents a general scheme for enhancing the convergence and\nperformance of DETR (DEtection TRansformer). We investigate the slow\nconvergence problem in transformers from a new perspective, suggesting that it\narises from the self-attention that introduces no structural bias over inputs.\nTo address this issue, we explore incorporating position relation prior as\nattention bias to augment object detection, following the verification of its\nstatistical significance using a proposed quantitative macroscopic correlation\n(MC) metric. Our approach, termed Relation-DETR, introduces an encoder to\nconstruct position relation embeddings for progressive attention refinement,\nwhich further extends the traditional streaming pipeline of DETR into a\ncontrastive relation pipeline to address the conflicts between non-duplicate\npredictions and positive supervision. Extensive experiments on both generic and\ntask-specific datasets demonstrate the effectiveness of our approach. Under the\nsame configurations, Relation-DETR achieves a significant improvement (+2.0% AP\ncompared to DINO), state-of-the-art performance (51.7% AP for 1x and 52.1% AP\nfor 2x settings), and a remarkably faster convergence speed (over 40% AP with\nonly 2 training epochs) than existing DETR detectors on COCO val2017. Moreover,\nthe proposed relation encoder serves as a universal plug-in-and-play component,\nbringing clear improvements for theoretically any DETR-like methods.\nFurthermore, we introduce a class-agnostic detection dataset, SA-Det-100k. The\nexperimental results on the dataset illustrate that the proposed explicit\nposition relation achieves a clear improvement of 1.3% AP, highlighting its\npotential towards universal object detection. The code and dataset are\navailable at https://github.com/xiuqhou/Relation-DETR.\n", "link": "http://arxiv.org/abs/2407.11699v1", "date": "2024-07-16", "relevancy": 2.1808, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5548}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5443}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relation%20DETR%3A%20Exploring%20Explicit%20Position%20Relation%20Prior%20for%20Object%0A%20%20Detection&body=Title%3A%20Relation%20DETR%3A%20Exploring%20Explicit%20Position%20Relation%20Prior%20for%20Object%0A%20%20Detection%0AAuthor%3A%20Xiuquan%20Hou%20and%20Meiqin%20Liu%20and%20Senlin%20Zhang%20and%20Ping%20Wei%20and%20Badong%20Chen%20and%20Xuguang%20Lan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20general%20scheme%20for%20enhancing%20the%20convergence%20and%0Aperformance%20of%20DETR%20%28DEtection%20TRansformer%29.%20We%20investigate%20the%20slow%0Aconvergence%20problem%20in%20transformers%20from%20a%20new%20perspective%2C%20suggesting%20that%20it%0Aarises%20from%20the%20self-attention%20that%20introduces%20no%20structural%20bias%20over%20inputs.%0ATo%20address%20this%20issue%2C%20we%20explore%20incorporating%20position%20relation%20prior%20as%0Aattention%20bias%20to%20augment%20object%20detection%2C%20following%20the%20verification%20of%20its%0Astatistical%20significance%20using%20a%20proposed%20quantitative%20macroscopic%20correlation%0A%28MC%29%20metric.%20Our%20approach%2C%20termed%20Relation-DETR%2C%20introduces%20an%20encoder%20to%0Aconstruct%20position%20relation%20embeddings%20for%20progressive%20attention%20refinement%2C%0Awhich%20further%20extends%20the%20traditional%20streaming%20pipeline%20of%20DETR%20into%20a%0Acontrastive%20relation%20pipeline%20to%20address%20the%20conflicts%20between%20non-duplicate%0Apredictions%20and%20positive%20supervision.%20Extensive%20experiments%20on%20both%20generic%20and%0Atask-specific%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Under%20the%0Asame%20configurations%2C%20Relation-DETR%20achieves%20a%20significant%20improvement%20%28%2B2.0%25%20AP%0Acompared%20to%20DINO%29%2C%20state-of-the-art%20performance%20%2851.7%25%20AP%20for%201x%20and%2052.1%25%20AP%0Afor%202x%20settings%29%2C%20and%20a%20remarkably%20faster%20convergence%20speed%20%28over%2040%25%20AP%20with%0Aonly%202%20training%20epochs%29%20than%20existing%20DETR%20detectors%20on%20COCO%20val2017.%20Moreover%2C%0Athe%20proposed%20relation%20encoder%20serves%20as%20a%20universal%20plug-in-and-play%20component%2C%0Abringing%20clear%20improvements%20for%20theoretically%20any%20DETR-like%20methods.%0AFurthermore%2C%20we%20introduce%20a%20class-agnostic%20detection%20dataset%2C%20SA-Det-100k.%20The%0Aexperimental%20results%20on%20the%20dataset%20illustrate%20that%20the%20proposed%20explicit%0Aposition%20relation%20achieves%20a%20clear%20improvement%20of%201.3%25%20AP%2C%20highlighting%20its%0Apotential%20towards%20universal%20object%20detection.%20The%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//github.com/xiuqhou/Relation-DETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelation%2520DETR%253A%2520Exploring%2520Explicit%2520Position%2520Relation%2520Prior%2520for%2520Object%250A%2520%2520Detection%26entry.906535625%3DXiuquan%2520Hou%2520and%2520Meiqin%2520Liu%2520and%2520Senlin%2520Zhang%2520and%2520Ping%2520Wei%2520and%2520Badong%2520Chen%2520and%2520Xuguang%2520Lan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520general%2520scheme%2520for%2520enhancing%2520the%2520convergence%2520and%250Aperformance%2520of%2520DETR%2520%2528DEtection%2520TRansformer%2529.%2520We%2520investigate%2520the%2520slow%250Aconvergence%2520problem%2520in%2520transformers%2520from%2520a%2520new%2520perspective%252C%2520suggesting%2520that%2520it%250Aarises%2520from%2520the%2520self-attention%2520that%2520introduces%2520no%2520structural%2520bias%2520over%2520inputs.%250ATo%2520address%2520this%2520issue%252C%2520we%2520explore%2520incorporating%2520position%2520relation%2520prior%2520as%250Aattention%2520bias%2520to%2520augment%2520object%2520detection%252C%2520following%2520the%2520verification%2520of%2520its%250Astatistical%2520significance%2520using%2520a%2520proposed%2520quantitative%2520macroscopic%2520correlation%250A%2528MC%2529%2520metric.%2520Our%2520approach%252C%2520termed%2520Relation-DETR%252C%2520introduces%2520an%2520encoder%2520to%250Aconstruct%2520position%2520relation%2520embeddings%2520for%2520progressive%2520attention%2520refinement%252C%250Awhich%2520further%2520extends%2520the%2520traditional%2520streaming%2520pipeline%2520of%2520DETR%2520into%2520a%250Acontrastive%2520relation%2520pipeline%2520to%2520address%2520the%2520conflicts%2520between%2520non-duplicate%250Apredictions%2520and%2520positive%2520supervision.%2520Extensive%2520experiments%2520on%2520both%2520generic%2520and%250Atask-specific%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Under%2520the%250Asame%2520configurations%252C%2520Relation-DETR%2520achieves%2520a%2520significant%2520improvement%2520%2528%252B2.0%2525%2520AP%250Acompared%2520to%2520DINO%2529%252C%2520state-of-the-art%2520performance%2520%252851.7%2525%2520AP%2520for%25201x%2520and%252052.1%2525%2520AP%250Afor%25202x%2520settings%2529%252C%2520and%2520a%2520remarkably%2520faster%2520convergence%2520speed%2520%2528over%252040%2525%2520AP%2520with%250Aonly%25202%2520training%2520epochs%2529%2520than%2520existing%2520DETR%2520detectors%2520on%2520COCO%2520val2017.%2520Moreover%252C%250Athe%2520proposed%2520relation%2520encoder%2520serves%2520as%2520a%2520universal%2520plug-in-and-play%2520component%252C%250Abringing%2520clear%2520improvements%2520for%2520theoretically%2520any%2520DETR-like%2520methods.%250AFurthermore%252C%2520we%2520introduce%2520a%2520class-agnostic%2520detection%2520dataset%252C%2520SA-Det-100k.%2520The%250Aexperimental%2520results%2520on%2520the%2520dataset%2520illustrate%2520that%2520the%2520proposed%2520explicit%250Aposition%2520relation%2520achieves%2520a%2520clear%2520improvement%2520of%25201.3%2525%2520AP%252C%2520highlighting%2520its%250Apotential%2520towards%2520universal%2520object%2520detection.%2520The%2520code%2520and%2520dataset%2520are%250Aavailable%2520at%2520https%253A//github.com/xiuqhou/Relation-DETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relation%20DETR%3A%20Exploring%20Explicit%20Position%20Relation%20Prior%20for%20Object%0A%20%20Detection&entry.906535625=Xiuquan%20Hou%20and%20Meiqin%20Liu%20and%20Senlin%20Zhang%20and%20Ping%20Wei%20and%20Badong%20Chen%20and%20Xuguang%20Lan&entry.1292438233=%20%20This%20paper%20presents%20a%20general%20scheme%20for%20enhancing%20the%20convergence%20and%0Aperformance%20of%20DETR%20%28DEtection%20TRansformer%29.%20We%20investigate%20the%20slow%0Aconvergence%20problem%20in%20transformers%20from%20a%20new%20perspective%2C%20suggesting%20that%20it%0Aarises%20from%20the%20self-attention%20that%20introduces%20no%20structural%20bias%20over%20inputs.%0ATo%20address%20this%20issue%2C%20we%20explore%20incorporating%20position%20relation%20prior%20as%0Aattention%20bias%20to%20augment%20object%20detection%2C%20following%20the%20verification%20of%20its%0Astatistical%20significance%20using%20a%20proposed%20quantitative%20macroscopic%20correlation%0A%28MC%29%20metric.%20Our%20approach%2C%20termed%20Relation-DETR%2C%20introduces%20an%20encoder%20to%0Aconstruct%20position%20relation%20embeddings%20for%20progressive%20attention%20refinement%2C%0Awhich%20further%20extends%20the%20traditional%20streaming%20pipeline%20of%20DETR%20into%20a%0Acontrastive%20relation%20pipeline%20to%20address%20the%20conflicts%20between%20non-duplicate%0Apredictions%20and%20positive%20supervision.%20Extensive%20experiments%20on%20both%20generic%20and%0Atask-specific%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Under%20the%0Asame%20configurations%2C%20Relation-DETR%20achieves%20a%20significant%20improvement%20%28%2B2.0%25%20AP%0Acompared%20to%20DINO%29%2C%20state-of-the-art%20performance%20%2851.7%25%20AP%20for%201x%20and%2052.1%25%20AP%0Afor%202x%20settings%29%2C%20and%20a%20remarkably%20faster%20convergence%20speed%20%28over%2040%25%20AP%20with%0Aonly%202%20training%20epochs%29%20than%20existing%20DETR%20detectors%20on%20COCO%20val2017.%20Moreover%2C%0Athe%20proposed%20relation%20encoder%20serves%20as%20a%20universal%20plug-in-and-play%20component%2C%0Abringing%20clear%20improvements%20for%20theoretically%20any%20DETR-like%20methods.%0AFurthermore%2C%20we%20introduce%20a%20class-agnostic%20detection%20dataset%2C%20SA-Det-100k.%20The%0Aexperimental%20results%20on%20the%20dataset%20illustrate%20that%20the%20proposed%20explicit%0Aposition%20relation%20achieves%20a%20clear%20improvement%20of%201.3%25%20AP%2C%20highlighting%20its%0Apotential%20towards%20universal%20object%20detection.%20The%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//github.com/xiuqhou/Relation-DETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11699v1&entry.124074799=Read"},
{"title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language\n  Large Models", "author": "Chen Ju and Haicheng Wang and Haozhe Cheng and Xu Chen and Zhonghua Zhai and Weilin Huang and Jinsong Lan and Shuai Xiao and Bo Zheng", "abstract": "  Vision-Language Large Models (VLMs) recently become primary backbone of AI,\ndue to the impressive performance. However, their expensive computation costs,\ni.e., throughput and delay, impede potentials in the real-world scenarios. To\nachieve acceleration for VLMs, most existing methods focus on the model\nperspective: pruning, distillation, quantization, but completely overlook the\ndata-perspective redundancy. To fill the overlook, this paper pioneers the\nseverity of data redundancy, and designs one plug-and-play Turbo module guided\nby information degree to prune inefficient tokens from visual or textual data.\nIn pursuit of efficiency-performance trade-offs, information degree takes two\ncrucial factors into consideration: mutual redundancy and semantic value.\nConcretely, the former evaluates data duplication between sequential tokens;\nwhile the latter evaluates each token by its contribution to the overall\nsemantics. As a result, tokens with high information degree carry less\nredundancy and stronger semantics. For VLMs' calculation, Turbo works as a\nuser-friendly plug-in that sorts data referring to information degree,\nutilizing only top-level ones to save costs. Its advantages are multifaceted,\ne.g., being generally compatible to various VLMs across understanding and\ngeneration, simple use without re-training and trivial engineering efforts. On\nmultiple VLMs benchmarks, we fully experiment to demonstrate the good\nacceleration of Turbo, under negligible performance drop.\n", "link": "http://arxiv.org/abs/2407.11717v1", "date": "2024-07-16", "relevancy": 2.1806, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turbo%3A%20Informativity-Driven%20Acceleration%20Plug-In%20for%20Vision-Language%0A%20%20Large%20Models&body=Title%3A%20Turbo%3A%20Informativity-Driven%20Acceleration%20Plug-In%20for%20Vision-Language%0A%20%20Large%20Models%0AAuthor%3A%20Chen%20Ju%20and%20Haicheng%20Wang%20and%20Haozhe%20Cheng%20and%20Xu%20Chen%20and%20Zhonghua%20Zhai%20and%20Weilin%20Huang%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Vision-Language%20Large%20Models%20%28VLMs%29%20recently%20become%20primary%20backbone%20of%20AI%2C%0Adue%20to%20the%20impressive%20performance.%20However%2C%20their%20expensive%20computation%20costs%2C%0Ai.e.%2C%20throughput%20and%20delay%2C%20impede%20potentials%20in%20the%20real-world%20scenarios.%20To%0Aachieve%20acceleration%20for%20VLMs%2C%20most%20existing%20methods%20focus%20on%20the%20model%0Aperspective%3A%20pruning%2C%20distillation%2C%20quantization%2C%20but%20completely%20overlook%20the%0Adata-perspective%20redundancy.%20To%20fill%20the%20overlook%2C%20this%20paper%20pioneers%20the%0Aseverity%20of%20data%20redundancy%2C%20and%20designs%20one%20plug-and-play%20Turbo%20module%20guided%0Aby%20information%20degree%20to%20prune%20inefficient%20tokens%20from%20visual%20or%20textual%20data.%0AIn%20pursuit%20of%20efficiency-performance%20trade-offs%2C%20information%20degree%20takes%20two%0Acrucial%20factors%20into%20consideration%3A%20mutual%20redundancy%20and%20semantic%20value.%0AConcretely%2C%20the%20former%20evaluates%20data%20duplication%20between%20sequential%20tokens%3B%0Awhile%20the%20latter%20evaluates%20each%20token%20by%20its%20contribution%20to%20the%20overall%0Asemantics.%20As%20a%20result%2C%20tokens%20with%20high%20information%20degree%20carry%20less%0Aredundancy%20and%20stronger%20semantics.%20For%20VLMs%27%20calculation%2C%20Turbo%20works%20as%20a%0Auser-friendly%20plug-in%20that%20sorts%20data%20referring%20to%20information%20degree%2C%0Autilizing%20only%20top-level%20ones%20to%20save%20costs.%20Its%20advantages%20are%20multifaceted%2C%0Ae.g.%2C%20being%20generally%20compatible%20to%20various%20VLMs%20across%20understanding%20and%0Ageneration%2C%20simple%20use%20without%20re-training%20and%20trivial%20engineering%20efforts.%20On%0Amultiple%20VLMs%20benchmarks%2C%20we%20fully%20experiment%20to%20demonstrate%20the%20good%0Aacceleration%20of%20Turbo%2C%20under%20negligible%20performance%20drop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurbo%253A%2520Informativity-Driven%2520Acceleration%2520Plug-In%2520for%2520Vision-Language%250A%2520%2520Large%2520Models%26entry.906535625%3DChen%2520Ju%2520and%2520Haicheng%2520Wang%2520and%2520Haozhe%2520Cheng%2520and%2520Xu%2520Chen%2520and%2520Zhonghua%2520Zhai%2520and%2520Weilin%2520Huang%2520and%2520Jinsong%2520Lan%2520and%2520Shuai%2520Xiao%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Vision-Language%2520Large%2520Models%2520%2528VLMs%2529%2520recently%2520become%2520primary%2520backbone%2520of%2520AI%252C%250Adue%2520to%2520the%2520impressive%2520performance.%2520However%252C%2520their%2520expensive%2520computation%2520costs%252C%250Ai.e.%252C%2520throughput%2520and%2520delay%252C%2520impede%2520potentials%2520in%2520the%2520real-world%2520scenarios.%2520To%250Aachieve%2520acceleration%2520for%2520VLMs%252C%2520most%2520existing%2520methods%2520focus%2520on%2520the%2520model%250Aperspective%253A%2520pruning%252C%2520distillation%252C%2520quantization%252C%2520but%2520completely%2520overlook%2520the%250Adata-perspective%2520redundancy.%2520To%2520fill%2520the%2520overlook%252C%2520this%2520paper%2520pioneers%2520the%250Aseverity%2520of%2520data%2520redundancy%252C%2520and%2520designs%2520one%2520plug-and-play%2520Turbo%2520module%2520guided%250Aby%2520information%2520degree%2520to%2520prune%2520inefficient%2520tokens%2520from%2520visual%2520or%2520textual%2520data.%250AIn%2520pursuit%2520of%2520efficiency-performance%2520trade-offs%252C%2520information%2520degree%2520takes%2520two%250Acrucial%2520factors%2520into%2520consideration%253A%2520mutual%2520redundancy%2520and%2520semantic%2520value.%250AConcretely%252C%2520the%2520former%2520evaluates%2520data%2520duplication%2520between%2520sequential%2520tokens%253B%250Awhile%2520the%2520latter%2520evaluates%2520each%2520token%2520by%2520its%2520contribution%2520to%2520the%2520overall%250Asemantics.%2520As%2520a%2520result%252C%2520tokens%2520with%2520high%2520information%2520degree%2520carry%2520less%250Aredundancy%2520and%2520stronger%2520semantics.%2520For%2520VLMs%2527%2520calculation%252C%2520Turbo%2520works%2520as%2520a%250Auser-friendly%2520plug-in%2520that%2520sorts%2520data%2520referring%2520to%2520information%2520degree%252C%250Autilizing%2520only%2520top-level%2520ones%2520to%2520save%2520costs.%2520Its%2520advantages%2520are%2520multifaceted%252C%250Ae.g.%252C%2520being%2520generally%2520compatible%2520to%2520various%2520VLMs%2520across%2520understanding%2520and%250Ageneration%252C%2520simple%2520use%2520without%2520re-training%2520and%2520trivial%2520engineering%2520efforts.%2520On%250Amultiple%2520VLMs%2520benchmarks%252C%2520we%2520fully%2520experiment%2520to%2520demonstrate%2520the%2520good%250Aacceleration%2520of%2520Turbo%252C%2520under%2520negligible%2520performance%2520drop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turbo%3A%20Informativity-Driven%20Acceleration%20Plug-In%20for%20Vision-Language%0A%20%20Large%20Models&entry.906535625=Chen%20Ju%20and%20Haicheng%20Wang%20and%20Haozhe%20Cheng%20and%20Xu%20Chen%20and%20Zhonghua%20Zhai%20and%20Weilin%20Huang%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao%20and%20Bo%20Zheng&entry.1292438233=%20%20Vision-Language%20Large%20Models%20%28VLMs%29%20recently%20become%20primary%20backbone%20of%20AI%2C%0Adue%20to%20the%20impressive%20performance.%20However%2C%20their%20expensive%20computation%20costs%2C%0Ai.e.%2C%20throughput%20and%20delay%2C%20impede%20potentials%20in%20the%20real-world%20scenarios.%20To%0Aachieve%20acceleration%20for%20VLMs%2C%20most%20existing%20methods%20focus%20on%20the%20model%0Aperspective%3A%20pruning%2C%20distillation%2C%20quantization%2C%20but%20completely%20overlook%20the%0Adata-perspective%20redundancy.%20To%20fill%20the%20overlook%2C%20this%20paper%20pioneers%20the%0Aseverity%20of%20data%20redundancy%2C%20and%20designs%20one%20plug-and-play%20Turbo%20module%20guided%0Aby%20information%20degree%20to%20prune%20inefficient%20tokens%20from%20visual%20or%20textual%20data.%0AIn%20pursuit%20of%20efficiency-performance%20trade-offs%2C%20information%20degree%20takes%20two%0Acrucial%20factors%20into%20consideration%3A%20mutual%20redundancy%20and%20semantic%20value.%0AConcretely%2C%20the%20former%20evaluates%20data%20duplication%20between%20sequential%20tokens%3B%0Awhile%20the%20latter%20evaluates%20each%20token%20by%20its%20contribution%20to%20the%20overall%0Asemantics.%20As%20a%20result%2C%20tokens%20with%20high%20information%20degree%20carry%20less%0Aredundancy%20and%20stronger%20semantics.%20For%20VLMs%27%20calculation%2C%20Turbo%20works%20as%20a%0Auser-friendly%20plug-in%20that%20sorts%20data%20referring%20to%20information%20degree%2C%0Autilizing%20only%20top-level%20ones%20to%20save%20costs.%20Its%20advantages%20are%20multifaceted%2C%0Ae.g.%2C%20being%20generally%20compatible%20to%20various%20VLMs%20across%20understanding%20and%0Ageneration%2C%20simple%20use%20without%20re-training%20and%20trivial%20engineering%20efforts.%20On%0Amultiple%20VLMs%20benchmarks%2C%20we%20fully%20experiment%20to%20demonstrate%20the%20good%0Aacceleration%20of%20Turbo%2C%20under%20negligible%20performance%20drop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11717v1&entry.124074799=Read"},
{"title": "MergeNet: Explicit Mesh Reconstruction from Sparse Point Clouds via Edge\n  Prediction", "author": "Weimin Wang and Yingxu Deng and Zezeng Li and Yu Liu and Na Lei", "abstract": "  This paper introduces a novel method for reconstructing meshes from sparse\npoint clouds by predicting edge connection. Existing implicit methods usually\nproduce superior smooth and watertight meshes due to the isosurface extraction\nalgorithms~(e.g., Marching Cubes). However, these methods become memory and\ncomputationally intensive with increasing resolution. Explicit methods are more\nefficient by directly forming the face from points. Nevertheless, the challenge\nof selecting appropriate faces from enormous candidates often leads to\nundesirable faces and holes. Moreover, the reconstruction performance of both\napproaches tends to degrade when the point cloud gets sparse. To this end, we\npropose MEsh Reconstruction via edGE~(MergeNet), which converts mesh\nreconstruction into local connectivity prediction problems. Specifically,\nMergeNet learns to extract the features of candidate edges and regress their\ndistances to the underlying surface. Consequently, the predicted distance is\nutilized to filter out edges that lay on surfaces. Finally, the meshes are\nreconstructed by refining the triangulations formed by these edges. Extensive\nexperiments on synthetic and real-scanned datasets demonstrate the superiority\nof MergeNet to SoTA explicit methods.\n", "link": "http://arxiv.org/abs/2407.11610v1", "date": "2024-07-16", "relevancy": 2.172, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5782}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5264}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MergeNet%3A%20Explicit%20Mesh%20Reconstruction%20from%20Sparse%20Point%20Clouds%20via%20Edge%0A%20%20Prediction&body=Title%3A%20MergeNet%3A%20Explicit%20Mesh%20Reconstruction%20from%20Sparse%20Point%20Clouds%20via%20Edge%0A%20%20Prediction%0AAuthor%3A%20Weimin%20Wang%20and%20Yingxu%20Deng%20and%20Zezeng%20Li%20and%20Yu%20Liu%20and%20Na%20Lei%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20method%20for%20reconstructing%20meshes%20from%20sparse%0Apoint%20clouds%20by%20predicting%20edge%20connection.%20Existing%20implicit%20methods%20usually%0Aproduce%20superior%20smooth%20and%20watertight%20meshes%20due%20to%20the%20isosurface%20extraction%0Aalgorithms~%28e.g.%2C%20Marching%20Cubes%29.%20However%2C%20these%20methods%20become%20memory%20and%0Acomputationally%20intensive%20with%20increasing%20resolution.%20Explicit%20methods%20are%20more%0Aefficient%20by%20directly%20forming%20the%20face%20from%20points.%20Nevertheless%2C%20the%20challenge%0Aof%20selecting%20appropriate%20faces%20from%20enormous%20candidates%20often%20leads%20to%0Aundesirable%20faces%20and%20holes.%20Moreover%2C%20the%20reconstruction%20performance%20of%20both%0Aapproaches%20tends%20to%20degrade%20when%20the%20point%20cloud%20gets%20sparse.%20To%20this%20end%2C%20we%0Apropose%20MEsh%20Reconstruction%20via%20edGE~%28MergeNet%29%2C%20which%20converts%20mesh%0Areconstruction%20into%20local%20connectivity%20prediction%20problems.%20Specifically%2C%0AMergeNet%20learns%20to%20extract%20the%20features%20of%20candidate%20edges%20and%20regress%20their%0Adistances%20to%20the%20underlying%20surface.%20Consequently%2C%20the%20predicted%20distance%20is%0Autilized%20to%20filter%20out%20edges%20that%20lay%20on%20surfaces.%20Finally%2C%20the%20meshes%20are%0Areconstructed%20by%20refining%20the%20triangulations%20formed%20by%20these%20edges.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-scanned%20datasets%20demonstrate%20the%20superiority%0Aof%20MergeNet%20to%20SoTA%20explicit%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMergeNet%253A%2520Explicit%2520Mesh%2520Reconstruction%2520from%2520Sparse%2520Point%2520Clouds%2520via%2520Edge%250A%2520%2520Prediction%26entry.906535625%3DWeimin%2520Wang%2520and%2520Yingxu%2520Deng%2520and%2520Zezeng%2520Li%2520and%2520Yu%2520Liu%2520and%2520Na%2520Lei%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520for%2520reconstructing%2520meshes%2520from%2520sparse%250Apoint%2520clouds%2520by%2520predicting%2520edge%2520connection.%2520Existing%2520implicit%2520methods%2520usually%250Aproduce%2520superior%2520smooth%2520and%2520watertight%2520meshes%2520due%2520to%2520the%2520isosurface%2520extraction%250Aalgorithms~%2528e.g.%252C%2520Marching%2520Cubes%2529.%2520However%252C%2520these%2520methods%2520become%2520memory%2520and%250Acomputationally%2520intensive%2520with%2520increasing%2520resolution.%2520Explicit%2520methods%2520are%2520more%250Aefficient%2520by%2520directly%2520forming%2520the%2520face%2520from%2520points.%2520Nevertheless%252C%2520the%2520challenge%250Aof%2520selecting%2520appropriate%2520faces%2520from%2520enormous%2520candidates%2520often%2520leads%2520to%250Aundesirable%2520faces%2520and%2520holes.%2520Moreover%252C%2520the%2520reconstruction%2520performance%2520of%2520both%250Aapproaches%2520tends%2520to%2520degrade%2520when%2520the%2520point%2520cloud%2520gets%2520sparse.%2520To%2520this%2520end%252C%2520we%250Apropose%2520MEsh%2520Reconstruction%2520via%2520edGE~%2528MergeNet%2529%252C%2520which%2520converts%2520mesh%250Areconstruction%2520into%2520local%2520connectivity%2520prediction%2520problems.%2520Specifically%252C%250AMergeNet%2520learns%2520to%2520extract%2520the%2520features%2520of%2520candidate%2520edges%2520and%2520regress%2520their%250Adistances%2520to%2520the%2520underlying%2520surface.%2520Consequently%252C%2520the%2520predicted%2520distance%2520is%250Autilized%2520to%2520filter%2520out%2520edges%2520that%2520lay%2520on%2520surfaces.%2520Finally%252C%2520the%2520meshes%2520are%250Areconstructed%2520by%2520refining%2520the%2520triangulations%2520formed%2520by%2520these%2520edges.%2520Extensive%250Aexperiments%2520on%2520synthetic%2520and%2520real-scanned%2520datasets%2520demonstrate%2520the%2520superiority%250Aof%2520MergeNet%2520to%2520SoTA%2520explicit%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MergeNet%3A%20Explicit%20Mesh%20Reconstruction%20from%20Sparse%20Point%20Clouds%20via%20Edge%0A%20%20Prediction&entry.906535625=Weimin%20Wang%20and%20Yingxu%20Deng%20and%20Zezeng%20Li%20and%20Yu%20Liu%20and%20Na%20Lei&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20method%20for%20reconstructing%20meshes%20from%20sparse%0Apoint%20clouds%20by%20predicting%20edge%20connection.%20Existing%20implicit%20methods%20usually%0Aproduce%20superior%20smooth%20and%20watertight%20meshes%20due%20to%20the%20isosurface%20extraction%0Aalgorithms~%28e.g.%2C%20Marching%20Cubes%29.%20However%2C%20these%20methods%20become%20memory%20and%0Acomputationally%20intensive%20with%20increasing%20resolution.%20Explicit%20methods%20are%20more%0Aefficient%20by%20directly%20forming%20the%20face%20from%20points.%20Nevertheless%2C%20the%20challenge%0Aof%20selecting%20appropriate%20faces%20from%20enormous%20candidates%20often%20leads%20to%0Aundesirable%20faces%20and%20holes.%20Moreover%2C%20the%20reconstruction%20performance%20of%20both%0Aapproaches%20tends%20to%20degrade%20when%20the%20point%20cloud%20gets%20sparse.%20To%20this%20end%2C%20we%0Apropose%20MEsh%20Reconstruction%20via%20edGE~%28MergeNet%29%2C%20which%20converts%20mesh%0Areconstruction%20into%20local%20connectivity%20prediction%20problems.%20Specifically%2C%0AMergeNet%20learns%20to%20extract%20the%20features%20of%20candidate%20edges%20and%20regress%20their%0Adistances%20to%20the%20underlying%20surface.%20Consequently%2C%20the%20predicted%20distance%20is%0Autilized%20to%20filter%20out%20edges%20that%20lay%20on%20surfaces.%20Finally%2C%20the%20meshes%20are%0Areconstructed%20by%20refining%20the%20triangulations%20formed%20by%20these%20edges.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-scanned%20datasets%20demonstrate%20the%20superiority%0Aof%20MergeNet%20to%20SoTA%20explicit%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11610v1&entry.124074799=Read"},
{"title": "Progressive Pretext Task Learning for Human Trajectory Prediction", "author": "Xiaotong Lin and Tianming Liang and Jianhuang Lai and Jian-Fang Hu", "abstract": "  Human trajectory prediction is a practical task of predicting the future\npositions of pedestrians on the road, which typically covers all temporal\nranges from short-term to long-term within a trajectory. However, existing\nworks attempt to address the entire trajectory prediction with a singular,\nuniform training paradigm, neglecting the distinction between short-term and\nlong-term dynamics in human trajectories. To overcome this limitation, we\nintroduce a novel Progressive Pretext Task learning (PPT) framework, which\nprogressively enhances the model's capacity of capturing short-term dynamics\nand long-term dependencies for the final entire trajectory prediction.\nSpecifically, we elaborately design three stages of training tasks in the PPT\nframework. In the first stage, the model learns to comprehend the short-term\ndynamics through a stepwise next-position prediction task. In the second stage,\nthe model is further enhanced to understand long-term dependencies through a\ndestination prediction task. In the final stage, the model aims to address the\nentire future trajectory task by taking full advantage of the knowledge from\nprevious stages. To alleviate the knowledge forgetting, we further apply a\ncross-task knowledge distillation. Additionally, we design a Transformer-based\ntrajectory predictor, which is able to achieve highly efficient two-step\nreasoning by integrating a destination-driven prediction strategy and a group\nof learnable prompt embeddings. Extensive experiments on popular benchmarks\nhave demonstrated that our proposed approach achieves state-of-the-art\nperformance with high efficiency. Code is available at\nhttps://github.com/iSEE-Laboratory/PPT.\n", "link": "http://arxiv.org/abs/2407.11588v1", "date": "2024-07-16", "relevancy": 2.1582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5635}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Pretext%20Task%20Learning%20for%20Human%20Trajectory%20Prediction&body=Title%3A%20Progressive%20Pretext%20Task%20Learning%20for%20Human%20Trajectory%20Prediction%0AAuthor%3A%20Xiaotong%20Lin%20and%20Tianming%20Liang%20and%20Jianhuang%20Lai%20and%20Jian-Fang%20Hu%0AAbstract%3A%20%20%20Human%20trajectory%20prediction%20is%20a%20practical%20task%20of%20predicting%20the%20future%0Apositions%20of%20pedestrians%20on%20the%20road%2C%20which%20typically%20covers%20all%20temporal%0Aranges%20from%20short-term%20to%20long-term%20within%20a%20trajectory.%20However%2C%20existing%0Aworks%20attempt%20to%20address%20the%20entire%20trajectory%20prediction%20with%20a%20singular%2C%0Auniform%20training%20paradigm%2C%20neglecting%20the%20distinction%20between%20short-term%20and%0Along-term%20dynamics%20in%20human%20trajectories.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20Progressive%20Pretext%20Task%20learning%20%28PPT%29%20framework%2C%20which%0Aprogressively%20enhances%20the%20model%27s%20capacity%20of%20capturing%20short-term%20dynamics%0Aand%20long-term%20dependencies%20for%20the%20final%20entire%20trajectory%20prediction.%0ASpecifically%2C%20we%20elaborately%20design%20three%20stages%20of%20training%20tasks%20in%20the%20PPT%0Aframework.%20In%20the%20first%20stage%2C%20the%20model%20learns%20to%20comprehend%20the%20short-term%0Adynamics%20through%20a%20stepwise%20next-position%20prediction%20task.%20In%20the%20second%20stage%2C%0Athe%20model%20is%20further%20enhanced%20to%20understand%20long-term%20dependencies%20through%20a%0Adestination%20prediction%20task.%20In%20the%20final%20stage%2C%20the%20model%20aims%20to%20address%20the%0Aentire%20future%20trajectory%20task%20by%20taking%20full%20advantage%20of%20the%20knowledge%20from%0Aprevious%20stages.%20To%20alleviate%20the%20knowledge%20forgetting%2C%20we%20further%20apply%20a%0Across-task%20knowledge%20distillation.%20Additionally%2C%20we%20design%20a%20Transformer-based%0Atrajectory%20predictor%2C%20which%20is%20able%20to%20achieve%20highly%20efficient%20two-step%0Areasoning%20by%20integrating%20a%20destination-driven%20prediction%20strategy%20and%20a%20group%0Aof%20learnable%20prompt%20embeddings.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ahave%20demonstrated%20that%20our%20proposed%20approach%20achieves%20state-of-the-art%0Aperformance%20with%20high%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/iSEE-Laboratory/PPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Pretext%2520Task%2520Learning%2520for%2520Human%2520Trajectory%2520Prediction%26entry.906535625%3DXiaotong%2520Lin%2520and%2520Tianming%2520Liang%2520and%2520Jianhuang%2520Lai%2520and%2520Jian-Fang%2520Hu%26entry.1292438233%3D%2520%2520Human%2520trajectory%2520prediction%2520is%2520a%2520practical%2520task%2520of%2520predicting%2520the%2520future%250Apositions%2520of%2520pedestrians%2520on%2520the%2520road%252C%2520which%2520typically%2520covers%2520all%2520temporal%250Aranges%2520from%2520short-term%2520to%2520long-term%2520within%2520a%2520trajectory.%2520However%252C%2520existing%250Aworks%2520attempt%2520to%2520address%2520the%2520entire%2520trajectory%2520prediction%2520with%2520a%2520singular%252C%250Auniform%2520training%2520paradigm%252C%2520neglecting%2520the%2520distinction%2520between%2520short-term%2520and%250Along-term%2520dynamics%2520in%2520human%2520trajectories.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520a%2520novel%2520Progressive%2520Pretext%2520Task%2520learning%2520%2528PPT%2529%2520framework%252C%2520which%250Aprogressively%2520enhances%2520the%2520model%2527s%2520capacity%2520of%2520capturing%2520short-term%2520dynamics%250Aand%2520long-term%2520dependencies%2520for%2520the%2520final%2520entire%2520trajectory%2520prediction.%250ASpecifically%252C%2520we%2520elaborately%2520design%2520three%2520stages%2520of%2520training%2520tasks%2520in%2520the%2520PPT%250Aframework.%2520In%2520the%2520first%2520stage%252C%2520the%2520model%2520learns%2520to%2520comprehend%2520the%2520short-term%250Adynamics%2520through%2520a%2520stepwise%2520next-position%2520prediction%2520task.%2520In%2520the%2520second%2520stage%252C%250Athe%2520model%2520is%2520further%2520enhanced%2520to%2520understand%2520long-term%2520dependencies%2520through%2520a%250Adestination%2520prediction%2520task.%2520In%2520the%2520final%2520stage%252C%2520the%2520model%2520aims%2520to%2520address%2520the%250Aentire%2520future%2520trajectory%2520task%2520by%2520taking%2520full%2520advantage%2520of%2520the%2520knowledge%2520from%250Aprevious%2520stages.%2520To%2520alleviate%2520the%2520knowledge%2520forgetting%252C%2520we%2520further%2520apply%2520a%250Across-task%2520knowledge%2520distillation.%2520Additionally%252C%2520we%2520design%2520a%2520Transformer-based%250Atrajectory%2520predictor%252C%2520which%2520is%2520able%2520to%2520achieve%2520highly%2520efficient%2520two-step%250Areasoning%2520by%2520integrating%2520a%2520destination-driven%2520prediction%2520strategy%2520and%2520a%2520group%250Aof%2520learnable%2520prompt%2520embeddings.%2520Extensive%2520experiments%2520on%2520popular%2520benchmarks%250Ahave%2520demonstrated%2520that%2520our%2520proposed%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520high%2520efficiency.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/iSEE-Laboratory/PPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Pretext%20Task%20Learning%20for%20Human%20Trajectory%20Prediction&entry.906535625=Xiaotong%20Lin%20and%20Tianming%20Liang%20and%20Jianhuang%20Lai%20and%20Jian-Fang%20Hu&entry.1292438233=%20%20Human%20trajectory%20prediction%20is%20a%20practical%20task%20of%20predicting%20the%20future%0Apositions%20of%20pedestrians%20on%20the%20road%2C%20which%20typically%20covers%20all%20temporal%0Aranges%20from%20short-term%20to%20long-term%20within%20a%20trajectory.%20However%2C%20existing%0Aworks%20attempt%20to%20address%20the%20entire%20trajectory%20prediction%20with%20a%20singular%2C%0Auniform%20training%20paradigm%2C%20neglecting%20the%20distinction%20between%20short-term%20and%0Along-term%20dynamics%20in%20human%20trajectories.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20Progressive%20Pretext%20Task%20learning%20%28PPT%29%20framework%2C%20which%0Aprogressively%20enhances%20the%20model%27s%20capacity%20of%20capturing%20short-term%20dynamics%0Aand%20long-term%20dependencies%20for%20the%20final%20entire%20trajectory%20prediction.%0ASpecifically%2C%20we%20elaborately%20design%20three%20stages%20of%20training%20tasks%20in%20the%20PPT%0Aframework.%20In%20the%20first%20stage%2C%20the%20model%20learns%20to%20comprehend%20the%20short-term%0Adynamics%20through%20a%20stepwise%20next-position%20prediction%20task.%20In%20the%20second%20stage%2C%0Athe%20model%20is%20further%20enhanced%20to%20understand%20long-term%20dependencies%20through%20a%0Adestination%20prediction%20task.%20In%20the%20final%20stage%2C%20the%20model%20aims%20to%20address%20the%0Aentire%20future%20trajectory%20task%20by%20taking%20full%20advantage%20of%20the%20knowledge%20from%0Aprevious%20stages.%20To%20alleviate%20the%20knowledge%20forgetting%2C%20we%20further%20apply%20a%0Across-task%20knowledge%20distillation.%20Additionally%2C%20we%20design%20a%20Transformer-based%0Atrajectory%20predictor%2C%20which%20is%20able%20to%20achieve%20highly%20efficient%20two-step%0Areasoning%20by%20integrating%20a%20destination-driven%20prediction%20strategy%20and%20a%20group%0Aof%20learnable%20prompt%20embeddings.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ahave%20demonstrated%20that%20our%20proposed%20approach%20achieves%20state-of-the-art%0Aperformance%20with%20high%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/iSEE-Laboratory/PPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11588v1&entry.124074799=Read"},
{"title": "Sailing Through Point Clouds: Safe Navigation Using Point Cloud Based\n  Control Barrier Functions", "author": "Bolun Dai and Rooholla Khorrambakht and Prashanth Krishnamurthy and Farshad Khorrami", "abstract": "  The capability to navigate safely in an unstructured environment is crucial\nwhen deploying robotic systems in real-world scenarios. Recently, control\nbarrier function (CBF) based approaches have been highly effective in\nsynthesizing safety-critical controllers. In this work, we propose a novel\nCBF-based local planner comprised of two components: Vessel and Mariner. The\nVessel is a novel scaling factor based CBF formulation that synthesizes CBFs\nusing only point cloud data. The Mariner is a CBF-based preview control\nframework that is used to mitigate getting stuck in spurious equilibria during\nnavigation. To demonstrate the efficacy of our proposed approach, we first\ncompare the proposed point cloud based CBF formulation with other point cloud\nbased CBF formulations. Then, we demonstrate the performance of our proposed\napproach and its integration with global planners using experimental studies on\nthe Unitree B1 and Unitree Go2 quadruped robots in various environments.\n", "link": "http://arxiv.org/abs/2403.18206v2", "date": "2024-07-16", "relevancy": 2.1546, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sailing%20Through%20Point%20Clouds%3A%20Safe%20Navigation%20Using%20Point%20Cloud%20Based%0A%20%20Control%20Barrier%20Functions&body=Title%3A%20Sailing%20Through%20Point%20Clouds%3A%20Safe%20Navigation%20Using%20Point%20Cloud%20Based%0A%20%20Control%20Barrier%20Functions%0AAuthor%3A%20Bolun%20Dai%20and%20Rooholla%20Khorrambakht%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20The%20capability%20to%20navigate%20safely%20in%20an%20unstructured%20environment%20is%20crucial%0Awhen%20deploying%20robotic%20systems%20in%20real-world%20scenarios.%20Recently%2C%20control%0Abarrier%20function%20%28CBF%29%20based%20approaches%20have%20been%20highly%20effective%20in%0Asynthesizing%20safety-critical%20controllers.%20In%20this%20work%2C%20we%20propose%20a%20novel%0ACBF-based%20local%20planner%20comprised%20of%20two%20components%3A%20Vessel%20and%20Mariner.%20The%0AVessel%20is%20a%20novel%20scaling%20factor%20based%20CBF%20formulation%20that%20synthesizes%20CBFs%0Ausing%20only%20point%20cloud%20data.%20The%20Mariner%20is%20a%20CBF-based%20preview%20control%0Aframework%20that%20is%20used%20to%20mitigate%20getting%20stuck%20in%20spurious%20equilibria%20during%0Anavigation.%20To%20demonstrate%20the%20efficacy%20of%20our%20proposed%20approach%2C%20we%20first%0Acompare%20the%20proposed%20point%20cloud%20based%20CBF%20formulation%20with%20other%20point%20cloud%0Abased%20CBF%20formulations.%20Then%2C%20we%20demonstrate%20the%20performance%20of%20our%20proposed%0Aapproach%20and%20its%20integration%20with%20global%20planners%20using%20experimental%20studies%20on%0Athe%20Unitree%20B1%20and%20Unitree%20Go2%20quadruped%20robots%20in%20various%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSailing%2520Through%2520Point%2520Clouds%253A%2520Safe%2520Navigation%2520Using%2520Point%2520Cloud%2520Based%250A%2520%2520Control%2520Barrier%2520Functions%26entry.906535625%3DBolun%2520Dai%2520and%2520Rooholla%2520Khorrambakht%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520The%2520capability%2520to%2520navigate%2520safely%2520in%2520an%2520unstructured%2520environment%2520is%2520crucial%250Awhen%2520deploying%2520robotic%2520systems%2520in%2520real-world%2520scenarios.%2520Recently%252C%2520control%250Abarrier%2520function%2520%2528CBF%2529%2520based%2520approaches%2520have%2520been%2520highly%2520effective%2520in%250Asynthesizing%2520safety-critical%2520controllers.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250ACBF-based%2520local%2520planner%2520comprised%2520of%2520two%2520components%253A%2520Vessel%2520and%2520Mariner.%2520The%250AVessel%2520is%2520a%2520novel%2520scaling%2520factor%2520based%2520CBF%2520formulation%2520that%2520synthesizes%2520CBFs%250Ausing%2520only%2520point%2520cloud%2520data.%2520The%2520Mariner%2520is%2520a%2520CBF-based%2520preview%2520control%250Aframework%2520that%2520is%2520used%2520to%2520mitigate%2520getting%2520stuck%2520in%2520spurious%2520equilibria%2520during%250Anavigation.%2520To%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520approach%252C%2520we%2520first%250Acompare%2520the%2520proposed%2520point%2520cloud%2520based%2520CBF%2520formulation%2520with%2520other%2520point%2520cloud%250Abased%2520CBF%2520formulations.%2520Then%252C%2520we%2520demonstrate%2520the%2520performance%2520of%2520our%2520proposed%250Aapproach%2520and%2520its%2520integration%2520with%2520global%2520planners%2520using%2520experimental%2520studies%2520on%250Athe%2520Unitree%2520B1%2520and%2520Unitree%2520Go2%2520quadruped%2520robots%2520in%2520various%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sailing%20Through%20Point%20Clouds%3A%20Safe%20Navigation%20Using%20Point%20Cloud%20Based%0A%20%20Control%20Barrier%20Functions&entry.906535625=Bolun%20Dai%20and%20Rooholla%20Khorrambakht%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami&entry.1292438233=%20%20The%20capability%20to%20navigate%20safely%20in%20an%20unstructured%20environment%20is%20crucial%0Awhen%20deploying%20robotic%20systems%20in%20real-world%20scenarios.%20Recently%2C%20control%0Abarrier%20function%20%28CBF%29%20based%20approaches%20have%20been%20highly%20effective%20in%0Asynthesizing%20safety-critical%20controllers.%20In%20this%20work%2C%20we%20propose%20a%20novel%0ACBF-based%20local%20planner%20comprised%20of%20two%20components%3A%20Vessel%20and%20Mariner.%20The%0AVessel%20is%20a%20novel%20scaling%20factor%20based%20CBF%20formulation%20that%20synthesizes%20CBFs%0Ausing%20only%20point%20cloud%20data.%20The%20Mariner%20is%20a%20CBF-based%20preview%20control%0Aframework%20that%20is%20used%20to%20mitigate%20getting%20stuck%20in%20spurious%20equilibria%20during%0Anavigation.%20To%20demonstrate%20the%20efficacy%20of%20our%20proposed%20approach%2C%20we%20first%0Acompare%20the%20proposed%20point%20cloud%20based%20CBF%20formulation%20with%20other%20point%20cloud%0Abased%20CBF%20formulations.%20Then%2C%20we%20demonstrate%20the%20performance%20of%20our%20proposed%0Aapproach%20and%20its%20integration%20with%20global%20planners%20using%20experimental%20studies%20on%0Athe%20Unitree%20B1%20and%20Unitree%20Go2%20quadruped%20robots%20in%20various%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18206v2&entry.124074799=Read"},
{"title": "Characteristic Learning for Provable One Step Generation", "author": "Zhao Ding and Chenguang Duan and Yuling Jiao and Ruoxuan Li and Jerry Zhijian Yang and Pingwen Zhang", "abstract": "  We propose the characteristic generator, a novel one-step generative model\nthat combines the efficiency of sampling in Generative Adversarial Networks\n(GANs) with the stable performance of flow-based models. Our model is driven by\ncharacteristics, along which the probability density transport can be described\nby ordinary differential equations (ODEs). Specifically, We estimate the\nvelocity field through nonparametric regression and utilize Euler method to\nsolve the probability flow ODE, generating a series of discrete approximations\nto the characteristics. We then use a deep neural network to fit these\ncharacteristics, ensuring a one-step mapping that effectively pushes the prior\ndistribution towards the target distribution. In the theoretical aspect, we\nanalyze the errors in velocity matching, Euler discretization, and\ncharacteristic fitting to establish a non-asymptotic convergence rate for the\ncharacteristic generator in 2-Wasserstein distance. To the best of our\nknowledge, this is the first thorough analysis for simulation-free one step\ngenerative models. Additionally, our analysis refines the error analysis of\nflow-based generative models in prior works. We apply our method on both\nsynthetic and real datasets, and the results demonstrate that the\ncharacteristic generator achieves high generation quality with just a single\nevaluation of neural network.\n", "link": "http://arxiv.org/abs/2405.05512v4", "date": "2024-07-16", "relevancy": 2.1518, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5713}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation&body=Title%3A%20Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation%0AAuthor%3A%20Zhao%20Ding%20and%20Chenguang%20Duan%20and%20Yuling%20Jiao%20and%20Ruoxuan%20Li%20and%20Jerry%20Zhijian%20Yang%20and%20Pingwen%20Zhang%0AAbstract%3A%20%20%20We%20propose%20the%20characteristic%20generator%2C%20a%20novel%20one-step%20generative%20model%0Athat%20combines%20the%20efficiency%20of%20sampling%20in%20Generative%20Adversarial%20Networks%0A%28GANs%29%20with%20the%20stable%20performance%20of%20flow-based%20models.%20Our%20model%20is%20driven%20by%0Acharacteristics%2C%20along%20which%20the%20probability%20density%20transport%20can%20be%20described%0Aby%20ordinary%20differential%20equations%20%28ODEs%29.%20Specifically%2C%20We%20estimate%20the%0Avelocity%20field%20through%20nonparametric%20regression%20and%20utilize%20Euler%20method%20to%0Asolve%20the%20probability%20flow%20ODE%2C%20generating%20a%20series%20of%20discrete%20approximations%0Ato%20the%20characteristics.%20We%20then%20use%20a%20deep%20neural%20network%20to%20fit%20these%0Acharacteristics%2C%20ensuring%20a%20one-step%20mapping%20that%20effectively%20pushes%20the%20prior%0Adistribution%20towards%20the%20target%20distribution.%20In%20the%20theoretical%20aspect%2C%20we%0Aanalyze%20the%20errors%20in%20velocity%20matching%2C%20Euler%20discretization%2C%20and%0Acharacteristic%20fitting%20to%20establish%20a%20non-asymptotic%20convergence%20rate%20for%20the%0Acharacteristic%20generator%20in%202-Wasserstein%20distance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20thorough%20analysis%20for%20simulation-free%20one%20step%0Agenerative%20models.%20Additionally%2C%20our%20analysis%20refines%20the%20error%20analysis%20of%0Aflow-based%20generative%20models%20in%20prior%20works.%20We%20apply%20our%20method%20on%20both%0Asynthetic%20and%20real%20datasets%2C%20and%20the%20results%20demonstrate%20that%20the%0Acharacteristic%20generator%20achieves%20high%20generation%20quality%20with%20just%20a%20single%0Aevaluation%20of%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05512v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacteristic%2520Learning%2520for%2520Provable%2520One%2520Step%2520Generation%26entry.906535625%3DZhao%2520Ding%2520and%2520Chenguang%2520Duan%2520and%2520Yuling%2520Jiao%2520and%2520Ruoxuan%2520Li%2520and%2520Jerry%2520Zhijian%2520Yang%2520and%2520Pingwen%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520characteristic%2520generator%252C%2520a%2520novel%2520one-step%2520generative%2520model%250Athat%2520combines%2520the%2520efficiency%2520of%2520sampling%2520in%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%2520with%2520the%2520stable%2520performance%2520of%2520flow-based%2520models.%2520Our%2520model%2520is%2520driven%2520by%250Acharacteristics%252C%2520along%2520which%2520the%2520probability%2520density%2520transport%2520can%2520be%2520described%250Aby%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529.%2520Specifically%252C%2520We%2520estimate%2520the%250Avelocity%2520field%2520through%2520nonparametric%2520regression%2520and%2520utilize%2520Euler%2520method%2520to%250Asolve%2520the%2520probability%2520flow%2520ODE%252C%2520generating%2520a%2520series%2520of%2520discrete%2520approximations%250Ato%2520the%2520characteristics.%2520We%2520then%2520use%2520a%2520deep%2520neural%2520network%2520to%2520fit%2520these%250Acharacteristics%252C%2520ensuring%2520a%2520one-step%2520mapping%2520that%2520effectively%2520pushes%2520the%2520prior%250Adistribution%2520towards%2520the%2520target%2520distribution.%2520In%2520the%2520theoretical%2520aspect%252C%2520we%250Aanalyze%2520the%2520errors%2520in%2520velocity%2520matching%252C%2520Euler%2520discretization%252C%2520and%250Acharacteristic%2520fitting%2520to%2520establish%2520a%2520non-asymptotic%2520convergence%2520rate%2520for%2520the%250Acharacteristic%2520generator%2520in%25202-Wasserstein%2520distance.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520thorough%2520analysis%2520for%2520simulation-free%2520one%2520step%250Agenerative%2520models.%2520Additionally%252C%2520our%2520analysis%2520refines%2520the%2520error%2520analysis%2520of%250Aflow-based%2520generative%2520models%2520in%2520prior%2520works.%2520We%2520apply%2520our%2520method%2520on%2520both%250Asynthetic%2520and%2520real%2520datasets%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520the%250Acharacteristic%2520generator%2520achieves%2520high%2520generation%2520quality%2520with%2520just%2520a%2520single%250Aevaluation%2520of%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05512v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation&entry.906535625=Zhao%20Ding%20and%20Chenguang%20Duan%20and%20Yuling%20Jiao%20and%20Ruoxuan%20Li%20and%20Jerry%20Zhijian%20Yang%20and%20Pingwen%20Zhang&entry.1292438233=%20%20We%20propose%20the%20characteristic%20generator%2C%20a%20novel%20one-step%20generative%20model%0Athat%20combines%20the%20efficiency%20of%20sampling%20in%20Generative%20Adversarial%20Networks%0A%28GANs%29%20with%20the%20stable%20performance%20of%20flow-based%20models.%20Our%20model%20is%20driven%20by%0Acharacteristics%2C%20along%20which%20the%20probability%20density%20transport%20can%20be%20described%0Aby%20ordinary%20differential%20equations%20%28ODEs%29.%20Specifically%2C%20We%20estimate%20the%0Avelocity%20field%20through%20nonparametric%20regression%20and%20utilize%20Euler%20method%20to%0Asolve%20the%20probability%20flow%20ODE%2C%20generating%20a%20series%20of%20discrete%20approximations%0Ato%20the%20characteristics.%20We%20then%20use%20a%20deep%20neural%20network%20to%20fit%20these%0Acharacteristics%2C%20ensuring%20a%20one-step%20mapping%20that%20effectively%20pushes%20the%20prior%0Adistribution%20towards%20the%20target%20distribution.%20In%20the%20theoretical%20aspect%2C%20we%0Aanalyze%20the%20errors%20in%20velocity%20matching%2C%20Euler%20discretization%2C%20and%0Acharacteristic%20fitting%20to%20establish%20a%20non-asymptotic%20convergence%20rate%20for%20the%0Acharacteristic%20generator%20in%202-Wasserstein%20distance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20thorough%20analysis%20for%20simulation-free%20one%20step%0Agenerative%20models.%20Additionally%2C%20our%20analysis%20refines%20the%20error%20analysis%20of%0Aflow-based%20generative%20models%20in%20prior%20works.%20We%20apply%20our%20method%20on%20both%0Asynthetic%20and%20real%20datasets%2C%20and%20the%20results%20demonstrate%20that%20the%0Acharacteristic%20generator%20achieves%20high%20generation%20quality%20with%20just%20a%20single%0Aevaluation%20of%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05512v4&entry.124074799=Read"},
{"title": "Diffusion-based Graph Generative Methods", "author": "Hongyang Chen and Can Xu and Lingyu Zheng and Qiang Zhang and Xuemin Lin", "abstract": "  Being the most cutting-edge generative methods, diffusion methods have shown\ngreat advances in wide generation tasks. Among them, graph generation attracts\nsignificant research attention for its broad application in real life. In our\nsurvey, we systematically and comprehensively review on diffusion-based graph\ngenerative methods. We first make a review on three mainstream paradigms of\ndiffusion methods, which are denoising diffusion probabilistic models,\nscore-based genrative models, and stochastic differential equations. Then we\nfurther categorize and introduce the latest applications of diffusion models on\ngraphs. In the end, we point out some limitations of current studies and future\ndirections of future explorations. The summary of existing methods metioned in\nthis survey is in\nhttps://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.\n", "link": "http://arxiv.org/abs/2401.15617v2", "date": "2024-07-16", "relevancy": 2.148, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5384}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Graph%20Generative%20Methods&body=Title%3A%20Diffusion-based%20Graph%20Generative%20Methods%0AAuthor%3A%20Hongyang%20Chen%20and%20Can%20Xu%20and%20Lingyu%20Zheng%20and%20Qiang%20Zhang%20and%20Xuemin%20Lin%0AAbstract%3A%20%20%20Being%20the%20most%20cutting-edge%20generative%20methods%2C%20diffusion%20methods%20have%20shown%0Agreat%20advances%20in%20wide%20generation%20tasks.%20Among%20them%2C%20graph%20generation%20attracts%0Asignificant%20research%20attention%20for%20its%20broad%20application%20in%20real%20life.%20In%20our%0Asurvey%2C%20we%20systematically%20and%20comprehensively%20review%20on%20diffusion-based%20graph%0Agenerative%20methods.%20We%20first%20make%20a%20review%20on%20three%20mainstream%20paradigms%20of%0Adiffusion%20methods%2C%20which%20are%20denoising%20diffusion%20probabilistic%20models%2C%0Ascore-based%20genrative%20models%2C%20and%20stochastic%20differential%20equations.%20Then%20we%0Afurther%20categorize%20and%20introduce%20the%20latest%20applications%20of%20diffusion%20models%20on%0Agraphs.%20In%20the%20end%2C%20we%20point%20out%20some%20limitations%20of%20current%20studies%20and%20future%0Adirections%20of%20future%20explorations.%20The%20summary%20of%20existing%20methods%20metioned%20in%0Athis%20survey%20is%20in%0Ahttps%3A//github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Graph%2520Generative%2520Methods%26entry.906535625%3DHongyang%2520Chen%2520and%2520Can%2520Xu%2520and%2520Lingyu%2520Zheng%2520and%2520Qiang%2520Zhang%2520and%2520Xuemin%2520Lin%26entry.1292438233%3D%2520%2520Being%2520the%2520most%2520cutting-edge%2520generative%2520methods%252C%2520diffusion%2520methods%2520have%2520shown%250Agreat%2520advances%2520in%2520wide%2520generation%2520tasks.%2520Among%2520them%252C%2520graph%2520generation%2520attracts%250Asignificant%2520research%2520attention%2520for%2520its%2520broad%2520application%2520in%2520real%2520life.%2520In%2520our%250Asurvey%252C%2520we%2520systematically%2520and%2520comprehensively%2520review%2520on%2520diffusion-based%2520graph%250Agenerative%2520methods.%2520We%2520first%2520make%2520a%2520review%2520on%2520three%2520mainstream%2520paradigms%2520of%250Adiffusion%2520methods%252C%2520which%2520are%2520denoising%2520diffusion%2520probabilistic%2520models%252C%250Ascore-based%2520genrative%2520models%252C%2520and%2520stochastic%2520differential%2520equations.%2520Then%2520we%250Afurther%2520categorize%2520and%2520introduce%2520the%2520latest%2520applications%2520of%2520diffusion%2520models%2520on%250Agraphs.%2520In%2520the%2520end%252C%2520we%2520point%2520out%2520some%2520limitations%2520of%2520current%2520studies%2520and%2520future%250Adirections%2520of%2520future%2520explorations.%2520The%2520summary%2520of%2520existing%2520methods%2520metioned%2520in%250Athis%2520survey%2520is%2520in%250Ahttps%253A//github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Graph%20Generative%20Methods&entry.906535625=Hongyang%20Chen%20and%20Can%20Xu%20and%20Lingyu%20Zheng%20and%20Qiang%20Zhang%20and%20Xuemin%20Lin&entry.1292438233=%20%20Being%20the%20most%20cutting-edge%20generative%20methods%2C%20diffusion%20methods%20have%20shown%0Agreat%20advances%20in%20wide%20generation%20tasks.%20Among%20them%2C%20graph%20generation%20attracts%0Asignificant%20research%20attention%20for%20its%20broad%20application%20in%20real%20life.%20In%20our%0Asurvey%2C%20we%20systematically%20and%20comprehensively%20review%20on%20diffusion-based%20graph%0Agenerative%20methods.%20We%20first%20make%20a%20review%20on%20three%20mainstream%20paradigms%20of%0Adiffusion%20methods%2C%20which%20are%20denoising%20diffusion%20probabilistic%20models%2C%0Ascore-based%20genrative%20models%2C%20and%20stochastic%20differential%20equations.%20Then%20we%0Afurther%20categorize%20and%20introduce%20the%20latest%20applications%20of%20diffusion%20models%20on%0Agraphs.%20In%20the%20end%2C%20we%20point%20out%20some%20limitations%20of%20current%20studies%20and%20future%0Adirections%20of%20future%20explorations.%20The%20summary%20of%20existing%20methods%20metioned%20in%0Athis%20survey%20is%20in%0Ahttps%3A//github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15617v2&entry.124074799=Read"},
{"title": "Self-Guided Generation of Minority Samples Using Diffusion Models", "author": "Soobin Um and Jong Chul Ye", "abstract": "  We present a novel approach for generating minority samples that live on\nlow-density regions of a data manifold. Our framework is built upon diffusion\nmodels, leveraging the principle of guided sampling that incorporates an\narbitrary energy-based guidance during inference time. The key defining feature\nof our sampler lies in its \\emph{self-contained} nature, \\ie, implementable\nsolely with a pretrained model. This distinguishes our sampler from existing\ntechniques that require expensive additional components (like external\nclassifiers) for minority generation. Specifically, we first estimate the\nlikelihood of features within an intermediate latent sample by evaluating a\nreconstruction loss w.r.t. its posterior mean. The generation then proceeds\nwith the minimization of the estimated likelihood, thereby encouraging the\nemergence of minority features in the latent samples of subsequent timesteps.\nTo further improve the performance of our sampler, we provide several\ntime-scheduling techniques that properly manage the influence of guidance over\ninference steps. Experiments on benchmark real datasets demonstrate that our\napproach can greatly improve the capability of creating realistic\nlow-likelihood minority instances over the existing techniques without the\nreliance on costly additional elements. Code is available at\n\\url{https://github.com/soobin-um/sg-minority}.\n", "link": "http://arxiv.org/abs/2407.11555v1", "date": "2024-07-16", "relevancy": 2.1305, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5613}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5346}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Guided%20Generation%20of%20Minority%20Samples%20Using%20Diffusion%20Models&body=Title%3A%20Self-Guided%20Generation%20of%20Minority%20Samples%20Using%20Diffusion%20Models%0AAuthor%3A%20Soobin%20Um%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20generating%20minority%20samples%20that%20live%20on%0Alow-density%20regions%20of%20a%20data%20manifold.%20Our%20framework%20is%20built%20upon%20diffusion%0Amodels%2C%20leveraging%20the%20principle%20of%20guided%20sampling%20that%20incorporates%20an%0Aarbitrary%20energy-based%20guidance%20during%20inference%20time.%20The%20key%20defining%20feature%0Aof%20our%20sampler%20lies%20in%20its%20%5Cemph%7Bself-contained%7D%20nature%2C%20%5Cie%2C%20implementable%0Asolely%20with%20a%20pretrained%20model.%20This%20distinguishes%20our%20sampler%20from%20existing%0Atechniques%20that%20require%20expensive%20additional%20components%20%28like%20external%0Aclassifiers%29%20for%20minority%20generation.%20Specifically%2C%20we%20first%20estimate%20the%0Alikelihood%20of%20features%20within%20an%20intermediate%20latent%20sample%20by%20evaluating%20a%0Areconstruction%20loss%20w.r.t.%20its%20posterior%20mean.%20The%20generation%20then%20proceeds%0Awith%20the%20minimization%20of%20the%20estimated%20likelihood%2C%20thereby%20encouraging%20the%0Aemergence%20of%20minority%20features%20in%20the%20latent%20samples%20of%20subsequent%20timesteps.%0ATo%20further%20improve%20the%20performance%20of%20our%20sampler%2C%20we%20provide%20several%0Atime-scheduling%20techniques%20that%20properly%20manage%20the%20influence%20of%20guidance%20over%0Ainference%20steps.%20Experiments%20on%20benchmark%20real%20datasets%20demonstrate%20that%20our%0Aapproach%20can%20greatly%20improve%20the%20capability%20of%20creating%20realistic%0Alow-likelihood%20minority%20instances%20over%20the%20existing%20techniques%20without%20the%0Areliance%20on%20costly%20additional%20elements.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/soobin-um/sg-minority%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Guided%2520Generation%2520of%2520Minority%2520Samples%2520Using%2520Diffusion%2520Models%26entry.906535625%3DSoobin%2520Um%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520generating%2520minority%2520samples%2520that%2520live%2520on%250Alow-density%2520regions%2520of%2520a%2520data%2520manifold.%2520Our%2520framework%2520is%2520built%2520upon%2520diffusion%250Amodels%252C%2520leveraging%2520the%2520principle%2520of%2520guided%2520sampling%2520that%2520incorporates%2520an%250Aarbitrary%2520energy-based%2520guidance%2520during%2520inference%2520time.%2520The%2520key%2520defining%2520feature%250Aof%2520our%2520sampler%2520lies%2520in%2520its%2520%255Cemph%257Bself-contained%257D%2520nature%252C%2520%255Cie%252C%2520implementable%250Asolely%2520with%2520a%2520pretrained%2520model.%2520This%2520distinguishes%2520our%2520sampler%2520from%2520existing%250Atechniques%2520that%2520require%2520expensive%2520additional%2520components%2520%2528like%2520external%250Aclassifiers%2529%2520for%2520minority%2520generation.%2520Specifically%252C%2520we%2520first%2520estimate%2520the%250Alikelihood%2520of%2520features%2520within%2520an%2520intermediate%2520latent%2520sample%2520by%2520evaluating%2520a%250Areconstruction%2520loss%2520w.r.t.%2520its%2520posterior%2520mean.%2520The%2520generation%2520then%2520proceeds%250Awith%2520the%2520minimization%2520of%2520the%2520estimated%2520likelihood%252C%2520thereby%2520encouraging%2520the%250Aemergence%2520of%2520minority%2520features%2520in%2520the%2520latent%2520samples%2520of%2520subsequent%2520timesteps.%250ATo%2520further%2520improve%2520the%2520performance%2520of%2520our%2520sampler%252C%2520we%2520provide%2520several%250Atime-scheduling%2520techniques%2520that%2520properly%2520manage%2520the%2520influence%2520of%2520guidance%2520over%250Ainference%2520steps.%2520Experiments%2520on%2520benchmark%2520real%2520datasets%2520demonstrate%2520that%2520our%250Aapproach%2520can%2520greatly%2520improve%2520the%2520capability%2520of%2520creating%2520realistic%250Alow-likelihood%2520minority%2520instances%2520over%2520the%2520existing%2520techniques%2520without%2520the%250Areliance%2520on%2520costly%2520additional%2520elements.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/soobin-um/sg-minority%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Guided%20Generation%20of%20Minority%20Samples%20Using%20Diffusion%20Models&entry.906535625=Soobin%20Um%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20generating%20minority%20samples%20that%20live%20on%0Alow-density%20regions%20of%20a%20data%20manifold.%20Our%20framework%20is%20built%20upon%20diffusion%0Amodels%2C%20leveraging%20the%20principle%20of%20guided%20sampling%20that%20incorporates%20an%0Aarbitrary%20energy-based%20guidance%20during%20inference%20time.%20The%20key%20defining%20feature%0Aof%20our%20sampler%20lies%20in%20its%20%5Cemph%7Bself-contained%7D%20nature%2C%20%5Cie%2C%20implementable%0Asolely%20with%20a%20pretrained%20model.%20This%20distinguishes%20our%20sampler%20from%20existing%0Atechniques%20that%20require%20expensive%20additional%20components%20%28like%20external%0Aclassifiers%29%20for%20minority%20generation.%20Specifically%2C%20we%20first%20estimate%20the%0Alikelihood%20of%20features%20within%20an%20intermediate%20latent%20sample%20by%20evaluating%20a%0Areconstruction%20loss%20w.r.t.%20its%20posterior%20mean.%20The%20generation%20then%20proceeds%0Awith%20the%20minimization%20of%20the%20estimated%20likelihood%2C%20thereby%20encouraging%20the%0Aemergence%20of%20minority%20features%20in%20the%20latent%20samples%20of%20subsequent%20timesteps.%0ATo%20further%20improve%20the%20performance%20of%20our%20sampler%2C%20we%20provide%20several%0Atime-scheduling%20techniques%20that%20properly%20manage%20the%20influence%20of%20guidance%20over%0Ainference%20steps.%20Experiments%20on%20benchmark%20real%20datasets%20demonstrate%20that%20our%0Aapproach%20can%20greatly%20improve%20the%20capability%20of%20creating%20realistic%0Alow-likelihood%20minority%20instances%20over%20the%20existing%20techniques%20without%20the%0Areliance%20on%20costly%20additional%20elements.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/soobin-um/sg-minority%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11555v1&entry.124074799=Read"},
{"title": "Imitation of human motion achieves natural head movements for humanoid\n  robots in an active-speaker detection task", "author": "Bosong Ding and Murat Kirtay and Giacomo Spigler", "abstract": "  Head movements are crucial for social human-human interaction. They can\ntransmit important cues (e.g., joint attention, speaker detection) that cannot\nbe achieved with verbal interaction alone. This advantage also holds for\nhuman-robot interaction. Even though modeling human motions through generative\nAI models has become an active research area within robotics in recent years,\nthe use of these methods for producing head movements in human-robot\ninteraction remains underexplored. In this work, we employed a generative AI\npipeline to produce human-like head movements for a Nao humanoid robot. In\naddition, we tested the system on a real-time active-speaker tracking task in a\ngroup conversation setting. Overall, the results show that the Nao robot\nsuccessfully imitates human head movements in a natural manner while actively\ntracking the speakers during the conversation. Code and data from this study\nare available at https://github.com/dingdingding60/Humanoids2024HRI\n", "link": "http://arxiv.org/abs/2407.11915v1", "date": "2024-07-16", "relevancy": 2.1272, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5327}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20of%20human%20motion%20achieves%20natural%20head%20movements%20for%20humanoid%0A%20%20robots%20in%20an%20active-speaker%20detection%20task&body=Title%3A%20Imitation%20of%20human%20motion%20achieves%20natural%20head%20movements%20for%20humanoid%0A%20%20robots%20in%20an%20active-speaker%20detection%20task%0AAuthor%3A%20Bosong%20Ding%20and%20Murat%20Kirtay%20and%20Giacomo%20Spigler%0AAbstract%3A%20%20%20Head%20movements%20are%20crucial%20for%20social%20human-human%20interaction.%20They%20can%0Atransmit%20important%20cues%20%28e.g.%2C%20joint%20attention%2C%20speaker%20detection%29%20that%20cannot%0Abe%20achieved%20with%20verbal%20interaction%20alone.%20This%20advantage%20also%20holds%20for%0Ahuman-robot%20interaction.%20Even%20though%20modeling%20human%20motions%20through%20generative%0AAI%20models%20has%20become%20an%20active%20research%20area%20within%20robotics%20in%20recent%20years%2C%0Athe%20use%20of%20these%20methods%20for%20producing%20head%20movements%20in%20human-robot%0Ainteraction%20remains%20underexplored.%20In%20this%20work%2C%20we%20employed%20a%20generative%20AI%0Apipeline%20to%20produce%20human-like%20head%20movements%20for%20a%20Nao%20humanoid%20robot.%20In%0Aaddition%2C%20we%20tested%20the%20system%20on%20a%20real-time%20active-speaker%20tracking%20task%20in%20a%0Agroup%20conversation%20setting.%20Overall%2C%20the%20results%20show%20that%20the%20Nao%20robot%0Asuccessfully%20imitates%20human%20head%20movements%20in%20a%20natural%20manner%20while%20actively%0Atracking%20the%20speakers%20during%20the%20conversation.%20Code%20and%20data%20from%20this%20study%0Aare%20available%20at%20https%3A//github.com/dingdingding60/Humanoids2024HRI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520of%2520human%2520motion%2520achieves%2520natural%2520head%2520movements%2520for%2520humanoid%250A%2520%2520robots%2520in%2520an%2520active-speaker%2520detection%2520task%26entry.906535625%3DBosong%2520Ding%2520and%2520Murat%2520Kirtay%2520and%2520Giacomo%2520Spigler%26entry.1292438233%3D%2520%2520Head%2520movements%2520are%2520crucial%2520for%2520social%2520human-human%2520interaction.%2520They%2520can%250Atransmit%2520important%2520cues%2520%2528e.g.%252C%2520joint%2520attention%252C%2520speaker%2520detection%2529%2520that%2520cannot%250Abe%2520achieved%2520with%2520verbal%2520interaction%2520alone.%2520This%2520advantage%2520also%2520holds%2520for%250Ahuman-robot%2520interaction.%2520Even%2520though%2520modeling%2520human%2520motions%2520through%2520generative%250AAI%2520models%2520has%2520become%2520an%2520active%2520research%2520area%2520within%2520robotics%2520in%2520recent%2520years%252C%250Athe%2520use%2520of%2520these%2520methods%2520for%2520producing%2520head%2520movements%2520in%2520human-robot%250Ainteraction%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520employed%2520a%2520generative%2520AI%250Apipeline%2520to%2520produce%2520human-like%2520head%2520movements%2520for%2520a%2520Nao%2520humanoid%2520robot.%2520In%250Aaddition%252C%2520we%2520tested%2520the%2520system%2520on%2520a%2520real-time%2520active-speaker%2520tracking%2520task%2520in%2520a%250Agroup%2520conversation%2520setting.%2520Overall%252C%2520the%2520results%2520show%2520that%2520the%2520Nao%2520robot%250Asuccessfully%2520imitates%2520human%2520head%2520movements%2520in%2520a%2520natural%2520manner%2520while%2520actively%250Atracking%2520the%2520speakers%2520during%2520the%2520conversation.%2520Code%2520and%2520data%2520from%2520this%2520study%250Aare%2520available%2520at%2520https%253A//github.com/dingdingding60/Humanoids2024HRI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20of%20human%20motion%20achieves%20natural%20head%20movements%20for%20humanoid%0A%20%20robots%20in%20an%20active-speaker%20detection%20task&entry.906535625=Bosong%20Ding%20and%20Murat%20Kirtay%20and%20Giacomo%20Spigler&entry.1292438233=%20%20Head%20movements%20are%20crucial%20for%20social%20human-human%20interaction.%20They%20can%0Atransmit%20important%20cues%20%28e.g.%2C%20joint%20attention%2C%20speaker%20detection%29%20that%20cannot%0Abe%20achieved%20with%20verbal%20interaction%20alone.%20This%20advantage%20also%20holds%20for%0Ahuman-robot%20interaction.%20Even%20though%20modeling%20human%20motions%20through%20generative%0AAI%20models%20has%20become%20an%20active%20research%20area%20within%20robotics%20in%20recent%20years%2C%0Athe%20use%20of%20these%20methods%20for%20producing%20head%20movements%20in%20human-robot%0Ainteraction%20remains%20underexplored.%20In%20this%20work%2C%20we%20employed%20a%20generative%20AI%0Apipeline%20to%20produce%20human-like%20head%20movements%20for%20a%20Nao%20humanoid%20robot.%20In%0Aaddition%2C%20we%20tested%20the%20system%20on%20a%20real-time%20active-speaker%20tracking%20task%20in%20a%0Agroup%20conversation%20setting.%20Overall%2C%20the%20results%20show%20that%20the%20Nao%20robot%0Asuccessfully%20imitates%20human%20head%20movements%20in%20a%20natural%20manner%20while%20actively%0Atracking%20the%20speakers%20during%20the%20conversation.%20Code%20and%20data%20from%20this%20study%0Aare%20available%20at%20https%3A//github.com/dingdingding60/Humanoids2024HRI%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11915v1&entry.124074799=Read"},
{"title": "Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model\n  Co-development", "author": "Daoyuan Chen and Haibin Wang and Yilun Huang and Ce Ge and Yaliang Li and Bolin Ding and Jingren Zhou", "abstract": "  The emergence of large-scale multi-modal generative models has drastically\nadvanced artificial intelligence, introducing unprecedented levels of\nperformance and functionality. However, optimizing these models remains\nchallenging due to historically isolated paths of model-centric and\ndata-centric developments, leading to suboptimal outcomes and inefficient\nresource utilization. In response, we present a novel sandbox suite tailored\nfor integrated data-model co-development. This sandbox provides a comprehensive\nexperimental platform, enabling rapid iteration and insight-driven refinement\nof both data and models. Our proposed \"Probe-Analyze-Refine\" workflow,\nvalidated through applications on state-of-the-art LLaVA-like and DiT based\nmodels, yields significant performance boosts, such as topping the VBench\nleaderboard. We also uncover fruitful insights gleaned from exhaustive\nbenchmarks, shedding light on the critical interplay between data quality,\ndiversity, and model behavior. With the hope of fostering deeper understanding\nand future progress in multi-modal data and generative modeling, our codes,\ndatasets, and models are maintained and accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.\n", "link": "http://arxiv.org/abs/2407.11784v1", "date": "2024-07-16", "relevancy": 2.1185, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5296}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Juicer%20Sandbox%3A%20A%20Comprehensive%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development&body=Title%3A%20Data-Juicer%20Sandbox%3A%20A%20Comprehensive%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development%0AAuthor%3A%20Daoyuan%20Chen%20and%20Haibin%20Wang%20and%20Yilun%20Huang%20and%20Ce%20Ge%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20The%20emergence%20of%20large-scale%20multi-modal%20generative%20models%20has%20drastically%0Aadvanced%20artificial%20intelligence%2C%20introducing%20unprecedented%20levels%20of%0Aperformance%20and%20functionality.%20However%2C%20optimizing%20these%20models%20remains%0Achallenging%20due%20to%20historically%20isolated%20paths%20of%20model-centric%20and%0Adata-centric%20developments%2C%20leading%20to%20suboptimal%20outcomes%20and%20inefficient%0Aresource%20utilization.%20In%20response%2C%20we%20present%20a%20novel%20sandbox%20suite%20tailored%0Afor%20integrated%20data-model%20co-development.%20This%20sandbox%20provides%20a%20comprehensive%0Aexperimental%20platform%2C%20enabling%20rapid%20iteration%20and%20insight-driven%20refinement%0Aof%20both%20data%20and%20models.%20Our%20proposed%20%22Probe-Analyze-Refine%22%20workflow%2C%0Avalidated%20through%20applications%20on%20state-of-the-art%20LLaVA-like%20and%20DiT%20based%0Amodels%2C%20yields%20significant%20performance%20boosts%2C%20such%20as%20topping%20the%20VBench%0Aleaderboard.%20We%20also%20uncover%20fruitful%20insights%20gleaned%20from%20exhaustive%0Abenchmarks%2C%20shedding%20light%20on%20the%20critical%20interplay%20between%20data%20quality%2C%0Adiversity%2C%20and%20model%20behavior.%20With%20the%20hope%20of%20fostering%20deeper%20understanding%0Aand%20future%20progress%20in%20multi-modal%20data%20and%20generative%20modeling%2C%20our%20codes%2C%0Adatasets%2C%20and%20models%20are%20maintained%20and%20accessible%20at%0Ahttps%3A//github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Juicer%2520Sandbox%253A%2520A%2520Comprehensive%2520Suite%2520for%2520Multimodal%2520Data-Model%250A%2520%2520Co-development%26entry.906535625%3DDaoyuan%2520Chen%2520and%2520Haibin%2520Wang%2520and%2520Yilun%2520Huang%2520and%2520Ce%2520Ge%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large-scale%2520multi-modal%2520generative%2520models%2520has%2520drastically%250Aadvanced%2520artificial%2520intelligence%252C%2520introducing%2520unprecedented%2520levels%2520of%250Aperformance%2520and%2520functionality.%2520However%252C%2520optimizing%2520these%2520models%2520remains%250Achallenging%2520due%2520to%2520historically%2520isolated%2520paths%2520of%2520model-centric%2520and%250Adata-centric%2520developments%252C%2520leading%2520to%2520suboptimal%2520outcomes%2520and%2520inefficient%250Aresource%2520utilization.%2520In%2520response%252C%2520we%2520present%2520a%2520novel%2520sandbox%2520suite%2520tailored%250Afor%2520integrated%2520data-model%2520co-development.%2520This%2520sandbox%2520provides%2520a%2520comprehensive%250Aexperimental%2520platform%252C%2520enabling%2520rapid%2520iteration%2520and%2520insight-driven%2520refinement%250Aof%2520both%2520data%2520and%2520models.%2520Our%2520proposed%2520%2522Probe-Analyze-Refine%2522%2520workflow%252C%250Avalidated%2520through%2520applications%2520on%2520state-of-the-art%2520LLaVA-like%2520and%2520DiT%2520based%250Amodels%252C%2520yields%2520significant%2520performance%2520boosts%252C%2520such%2520as%2520topping%2520the%2520VBench%250Aleaderboard.%2520We%2520also%2520uncover%2520fruitful%2520insights%2520gleaned%2520from%2520exhaustive%250Abenchmarks%252C%2520shedding%2520light%2520on%2520the%2520critical%2520interplay%2520between%2520data%2520quality%252C%250Adiversity%252C%2520and%2520model%2520behavior.%2520With%2520the%2520hope%2520of%2520fostering%2520deeper%2520understanding%250Aand%2520future%2520progress%2520in%2520multi-modal%2520data%2520and%2520generative%2520modeling%252C%2520our%2520codes%252C%250Adatasets%252C%2520and%2520models%2520are%2520maintained%2520and%2520accessible%2520at%250Ahttps%253A//github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Juicer%20Sandbox%3A%20A%20Comprehensive%20Suite%20for%20Multimodal%20Data-Model%0A%20%20Co-development&entry.906535625=Daoyuan%20Chen%20and%20Haibin%20Wang%20and%20Yilun%20Huang%20and%20Ce%20Ge%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20The%20emergence%20of%20large-scale%20multi-modal%20generative%20models%20has%20drastically%0Aadvanced%20artificial%20intelligence%2C%20introducing%20unprecedented%20levels%20of%0Aperformance%20and%20functionality.%20However%2C%20optimizing%20these%20models%20remains%0Achallenging%20due%20to%20historically%20isolated%20paths%20of%20model-centric%20and%0Adata-centric%20developments%2C%20leading%20to%20suboptimal%20outcomes%20and%20inefficient%0Aresource%20utilization.%20In%20response%2C%20we%20present%20a%20novel%20sandbox%20suite%20tailored%0Afor%20integrated%20data-model%20co-development.%20This%20sandbox%20provides%20a%20comprehensive%0Aexperimental%20platform%2C%20enabling%20rapid%20iteration%20and%20insight-driven%20refinement%0Aof%20both%20data%20and%20models.%20Our%20proposed%20%22Probe-Analyze-Refine%22%20workflow%2C%0Avalidated%20through%20applications%20on%20state-of-the-art%20LLaVA-like%20and%20DiT%20based%0Amodels%2C%20yields%20significant%20performance%20boosts%2C%20such%20as%20topping%20the%20VBench%0Aleaderboard.%20We%20also%20uncover%20fruitful%20insights%20gleaned%20from%20exhaustive%0Abenchmarks%2C%20shedding%20light%20on%20the%20critical%20interplay%20between%20data%20quality%2C%0Adiversity%2C%20and%20model%20behavior.%20With%20the%20hope%20of%20fostering%20deeper%20understanding%0Aand%20future%20progress%20in%20multi-modal%20data%20and%20generative%20modeling%2C%20our%20codes%2C%0Adatasets%2C%20and%20models%20are%20maintained%20and%20accessible%20at%0Ahttps%3A//github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11784v1&entry.124074799=Read"},
{"title": "PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient\n  Task Adaptation", "author": "Yizhe Xiong and Hui Chen and Tianxiang Hao and Zijia Lin and Jungong Han and Yuesong Zhang and Guoxin Wang and Yongjun Bao and Guiguang Ding", "abstract": "  Recently, the scale of transformers has grown rapidly, which introduces\nconsiderable challenges in terms of training overhead and inference efficiency\nin the scope of task adaptation. Existing works, namely Parameter-Efficient\nFine-Tuning (PEFT) and model compression, have separately investigated the\nchallenges. However, PEFT cannot guarantee the inference efficiency of the\noriginal backbone, especially for large-scale models. Model compression\nrequires significant training costs for structure searching and re-training.\nConsequently, a simple combination of them cannot guarantee accomplishing both\ntraining efficiency and inference efficiency with minimal costs. In this paper,\nwe propose a novel Parallel Yielding Re-Activation (PYRA) method for such a\nchallenge of training-inference efficient task adaptation. PYRA first utilizes\nparallel yielding adaptive weights to comprehensively perceive the data\ndistribution in downstream tasks. A re-activation strategy for token modulation\nis then applied for tokens to be merged, leading to calibrated token features.\nExtensive experiments demonstrate that PYRA outperforms all competing methods\nunder both low compression rate and high compression rate, demonstrating its\neffectiveness and superiority in maintaining both training efficiency and\ninference efficiency for large-scale foundation models. Our code is available\nat https://github.com/THU-MIG/PYRA.\n", "link": "http://arxiv.org/abs/2403.09192v3", "date": "2024-07-16", "relevancy": 2.1185, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5556}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PYRA%3A%20Parallel%20Yielding%20Re-Activation%20for%20Training-Inference%20Efficient%0A%20%20Task%20Adaptation&body=Title%3A%20PYRA%3A%20Parallel%20Yielding%20Re-Activation%20for%20Training-Inference%20Efficient%0A%20%20Task%20Adaptation%0AAuthor%3A%20Yizhe%20Xiong%20and%20Hui%20Chen%20and%20Tianxiang%20Hao%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Yuesong%20Zhang%20and%20Guoxin%20Wang%20and%20Yongjun%20Bao%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Recently%2C%20the%20scale%20of%20transformers%20has%20grown%20rapidly%2C%20which%20introduces%0Aconsiderable%20challenges%20in%20terms%20of%20training%20overhead%20and%20inference%20efficiency%0Ain%20the%20scope%20of%20task%20adaptation.%20Existing%20works%2C%20namely%20Parameter-Efficient%0AFine-Tuning%20%28PEFT%29%20and%20model%20compression%2C%20have%20separately%20investigated%20the%0Achallenges.%20However%2C%20PEFT%20cannot%20guarantee%20the%20inference%20efficiency%20of%20the%0Aoriginal%20backbone%2C%20especially%20for%20large-scale%20models.%20Model%20compression%0Arequires%20significant%20training%20costs%20for%20structure%20searching%20and%20re-training.%0AConsequently%2C%20a%20simple%20combination%20of%20them%20cannot%20guarantee%20accomplishing%20both%0Atraining%20efficiency%20and%20inference%20efficiency%20with%20minimal%20costs.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Parallel%20Yielding%20Re-Activation%20%28PYRA%29%20method%20for%20such%20a%0Achallenge%20of%20training-inference%20efficient%20task%20adaptation.%20PYRA%20first%20utilizes%0Aparallel%20yielding%20adaptive%20weights%20to%20comprehensively%20perceive%20the%20data%0Adistribution%20in%20downstream%20tasks.%20A%20re-activation%20strategy%20for%20token%20modulation%0Ais%20then%20applied%20for%20tokens%20to%20be%20merged%2C%20leading%20to%20calibrated%20token%20features.%0AExtensive%20experiments%20demonstrate%20that%20PYRA%20outperforms%20all%20competing%20methods%0Aunder%20both%20low%20compression%20rate%20and%20high%20compression%20rate%2C%20demonstrating%20its%0Aeffectiveness%20and%20superiority%20in%20maintaining%20both%20training%20efficiency%20and%0Ainference%20efficiency%20for%20large-scale%20foundation%20models.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/THU-MIG/PYRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09192v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPYRA%253A%2520Parallel%2520Yielding%2520Re-Activation%2520for%2520Training-Inference%2520Efficient%250A%2520%2520Task%2520Adaptation%26entry.906535625%3DYizhe%2520Xiong%2520and%2520Hui%2520Chen%2520and%2520Tianxiang%2520Hao%2520and%2520Zijia%2520Lin%2520and%2520Jungong%2520Han%2520and%2520Yuesong%2520Zhang%2520and%2520Guoxin%2520Wang%2520and%2520Yongjun%2520Bao%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520scale%2520of%2520transformers%2520has%2520grown%2520rapidly%252C%2520which%2520introduces%250Aconsiderable%2520challenges%2520in%2520terms%2520of%2520training%2520overhead%2520and%2520inference%2520efficiency%250Ain%2520the%2520scope%2520of%2520task%2520adaptation.%2520Existing%2520works%252C%2520namely%2520Parameter-Efficient%250AFine-Tuning%2520%2528PEFT%2529%2520and%2520model%2520compression%252C%2520have%2520separately%2520investigated%2520the%250Achallenges.%2520However%252C%2520PEFT%2520cannot%2520guarantee%2520the%2520inference%2520efficiency%2520of%2520the%250Aoriginal%2520backbone%252C%2520especially%2520for%2520large-scale%2520models.%2520Model%2520compression%250Arequires%2520significant%2520training%2520costs%2520for%2520structure%2520searching%2520and%2520re-training.%250AConsequently%252C%2520a%2520simple%2520combination%2520of%2520them%2520cannot%2520guarantee%2520accomplishing%2520both%250Atraining%2520efficiency%2520and%2520inference%2520efficiency%2520with%2520minimal%2520costs.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520Parallel%2520Yielding%2520Re-Activation%2520%2528PYRA%2529%2520method%2520for%2520such%2520a%250Achallenge%2520of%2520training-inference%2520efficient%2520task%2520adaptation.%2520PYRA%2520first%2520utilizes%250Aparallel%2520yielding%2520adaptive%2520weights%2520to%2520comprehensively%2520perceive%2520the%2520data%250Adistribution%2520in%2520downstream%2520tasks.%2520A%2520re-activation%2520strategy%2520for%2520token%2520modulation%250Ais%2520then%2520applied%2520for%2520tokens%2520to%2520be%2520merged%252C%2520leading%2520to%2520calibrated%2520token%2520features.%250AExtensive%2520experiments%2520demonstrate%2520that%2520PYRA%2520outperforms%2520all%2520competing%2520methods%250Aunder%2520both%2520low%2520compression%2520rate%2520and%2520high%2520compression%2520rate%252C%2520demonstrating%2520its%250Aeffectiveness%2520and%2520superiority%2520in%2520maintaining%2520both%2520training%2520efficiency%2520and%250Ainference%2520efficiency%2520for%2520large-scale%2520foundation%2520models.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/THU-MIG/PYRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09192v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PYRA%3A%20Parallel%20Yielding%20Re-Activation%20for%20Training-Inference%20Efficient%0A%20%20Task%20Adaptation&entry.906535625=Yizhe%20Xiong%20and%20Hui%20Chen%20and%20Tianxiang%20Hao%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Yuesong%20Zhang%20and%20Guoxin%20Wang%20and%20Yongjun%20Bao%20and%20Guiguang%20Ding&entry.1292438233=%20%20Recently%2C%20the%20scale%20of%20transformers%20has%20grown%20rapidly%2C%20which%20introduces%0Aconsiderable%20challenges%20in%20terms%20of%20training%20overhead%20and%20inference%20efficiency%0Ain%20the%20scope%20of%20task%20adaptation.%20Existing%20works%2C%20namely%20Parameter-Efficient%0AFine-Tuning%20%28PEFT%29%20and%20model%20compression%2C%20have%20separately%20investigated%20the%0Achallenges.%20However%2C%20PEFT%20cannot%20guarantee%20the%20inference%20efficiency%20of%20the%0Aoriginal%20backbone%2C%20especially%20for%20large-scale%20models.%20Model%20compression%0Arequires%20significant%20training%20costs%20for%20structure%20searching%20and%20re-training.%0AConsequently%2C%20a%20simple%20combination%20of%20them%20cannot%20guarantee%20accomplishing%20both%0Atraining%20efficiency%20and%20inference%20efficiency%20with%20minimal%20costs.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Parallel%20Yielding%20Re-Activation%20%28PYRA%29%20method%20for%20such%20a%0Achallenge%20of%20training-inference%20efficient%20task%20adaptation.%20PYRA%20first%20utilizes%0Aparallel%20yielding%20adaptive%20weights%20to%20comprehensively%20perceive%20the%20data%0Adistribution%20in%20downstream%20tasks.%20A%20re-activation%20strategy%20for%20token%20modulation%0Ais%20then%20applied%20for%20tokens%20to%20be%20merged%2C%20leading%20to%20calibrated%20token%20features.%0AExtensive%20experiments%20demonstrate%20that%20PYRA%20outperforms%20all%20competing%20methods%0Aunder%20both%20low%20compression%20rate%20and%20high%20compression%20rate%2C%20demonstrating%20its%0Aeffectiveness%20and%20superiority%20in%20maintaining%20both%20training%20efficiency%20and%0Ainference%20efficiency%20for%20large-scale%20foundation%20models.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/THU-MIG/PYRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09192v3&entry.124074799=Read"},
{"title": "Learning Confidence Bounds for Classification with Imbalanced Data", "author": "Matt Clifford and Jonathan Erskine and Alexander Hepburn and Ra\u00fal Santos-Rodr\u00edguez and Dario Garcia-Garcia", "abstract": "  Class imbalance poses a significant challenge in classification tasks, where\ntraditional approaches often lead to biased models and unreliable predictions.\nUndersampling and oversampling techniques have been commonly employed to\naddress this issue, yet they suffer from inherent limitations stemming from\ntheir simplistic approach such as loss of information and additional biases\nrespectively. In this paper, we propose a novel framework that leverages\nlearning theory and concentration inequalities to overcome the shortcomings of\ntraditional solutions. We focus on understanding the uncertainty in a\nclass-dependent manner, as captured by confidence bounds that we directly embed\ninto the learning process. By incorporating class-dependent estimates, our\nmethod can effectively adapt to the varying degrees of imbalance across\ndifferent classes, resulting in more robust and reliable classification\noutcomes. We empirically show how our framework provides a promising direction\nfor handling imbalanced data in classification tasks, offering practitioners a\nvaluable tool for building more accurate and trustworthy models.\n", "link": "http://arxiv.org/abs/2407.11878v1", "date": "2024-07-16", "relevancy": 2.1091, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5559}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data&body=Title%3A%20Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data%0AAuthor%3A%20Matt%20Clifford%20and%20Jonathan%20Erskine%20and%20Alexander%20Hepburn%20and%20Ra%C3%BAl%20Santos-Rodr%C3%ADguez%20and%20Dario%20Garcia-Garcia%0AAbstract%3A%20%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20in%20classification%20tasks%2C%20where%0Atraditional%20approaches%20often%20lead%20to%20biased%20models%20and%20unreliable%20predictions.%0AUndersampling%20and%20oversampling%20techniques%20have%20been%20commonly%20employed%20to%0Aaddress%20this%20issue%2C%20yet%20they%20suffer%20from%20inherent%20limitations%20stemming%20from%0Atheir%20simplistic%20approach%20such%20as%20loss%20of%20information%20and%20additional%20biases%0Arespectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20leverages%0Alearning%20theory%20and%20concentration%20inequalities%20to%20overcome%20the%20shortcomings%20of%0Atraditional%20solutions.%20We%20focus%20on%20understanding%20the%20uncertainty%20in%20a%0Aclass-dependent%20manner%2C%20as%20captured%20by%20confidence%20bounds%20that%20we%20directly%20embed%0Ainto%20the%20learning%20process.%20By%20incorporating%20class-dependent%20estimates%2C%20our%0Amethod%20can%20effectively%20adapt%20to%20the%20varying%20degrees%20of%20imbalance%20across%0Adifferent%20classes%2C%20resulting%20in%20more%20robust%20and%20reliable%20classification%0Aoutcomes.%20We%20empirically%20show%20how%20our%20framework%20provides%20a%20promising%20direction%0Afor%20handling%20imbalanced%20data%20in%20classification%20tasks%2C%20offering%20practitioners%20a%0Avaluable%20tool%20for%20building%20more%20accurate%20and%20trustworthy%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Confidence%2520Bounds%2520for%2520Classification%2520with%2520Imbalanced%2520Data%26entry.906535625%3DMatt%2520Clifford%2520and%2520Jonathan%2520Erskine%2520and%2520Alexander%2520Hepburn%2520and%2520Ra%25C3%25BAl%2520Santos-Rodr%25C3%25ADguez%2520and%2520Dario%2520Garcia-Garcia%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520poses%2520a%2520significant%2520challenge%2520in%2520classification%2520tasks%252C%2520where%250Atraditional%2520approaches%2520often%2520lead%2520to%2520biased%2520models%2520and%2520unreliable%2520predictions.%250AUndersampling%2520and%2520oversampling%2520techniques%2520have%2520been%2520commonly%2520employed%2520to%250Aaddress%2520this%2520issue%252C%2520yet%2520they%2520suffer%2520from%2520inherent%2520limitations%2520stemming%2520from%250Atheir%2520simplistic%2520approach%2520such%2520as%2520loss%2520of%2520information%2520and%2520additional%2520biases%250Arespectively.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520leverages%250Alearning%2520theory%2520and%2520concentration%2520inequalities%2520to%2520overcome%2520the%2520shortcomings%2520of%250Atraditional%2520solutions.%2520We%2520focus%2520on%2520understanding%2520the%2520uncertainty%2520in%2520a%250Aclass-dependent%2520manner%252C%2520as%2520captured%2520by%2520confidence%2520bounds%2520that%2520we%2520directly%2520embed%250Ainto%2520the%2520learning%2520process.%2520By%2520incorporating%2520class-dependent%2520estimates%252C%2520our%250Amethod%2520can%2520effectively%2520adapt%2520to%2520the%2520varying%2520degrees%2520of%2520imbalance%2520across%250Adifferent%2520classes%252C%2520resulting%2520in%2520more%2520robust%2520and%2520reliable%2520classification%250Aoutcomes.%2520We%2520empirically%2520show%2520how%2520our%2520framework%2520provides%2520a%2520promising%2520direction%250Afor%2520handling%2520imbalanced%2520data%2520in%2520classification%2520tasks%252C%2520offering%2520practitioners%2520a%250Avaluable%2520tool%2520for%2520building%2520more%2520accurate%2520and%2520trustworthy%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data&entry.906535625=Matt%20Clifford%20and%20Jonathan%20Erskine%20and%20Alexander%20Hepburn%20and%20Ra%C3%BAl%20Santos-Rodr%C3%ADguez%20and%20Dario%20Garcia-Garcia&entry.1292438233=%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20in%20classification%20tasks%2C%20where%0Atraditional%20approaches%20often%20lead%20to%20biased%20models%20and%20unreliable%20predictions.%0AUndersampling%20and%20oversampling%20techniques%20have%20been%20commonly%20employed%20to%0Aaddress%20this%20issue%2C%20yet%20they%20suffer%20from%20inherent%20limitations%20stemming%20from%0Atheir%20simplistic%20approach%20such%20as%20loss%20of%20information%20and%20additional%20biases%0Arespectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20leverages%0Alearning%20theory%20and%20concentration%20inequalities%20to%20overcome%20the%20shortcomings%20of%0Atraditional%20solutions.%20We%20focus%20on%20understanding%20the%20uncertainty%20in%20a%0Aclass-dependent%20manner%2C%20as%20captured%20by%20confidence%20bounds%20that%20we%20directly%20embed%0Ainto%20the%20learning%20process.%20By%20incorporating%20class-dependent%20estimates%2C%20our%0Amethod%20can%20effectively%20adapt%20to%20the%20varying%20degrees%20of%20imbalance%20across%0Adifferent%20classes%2C%20resulting%20in%20more%20robust%20and%20reliable%20classification%0Aoutcomes.%20We%20empirically%20show%20how%20our%20framework%20provides%20a%20promising%20direction%0Afor%20handling%20imbalanced%20data%20in%20classification%20tasks%2C%20offering%20practitioners%20a%0Avaluable%20tool%20for%20building%20more%20accurate%20and%20trustworthy%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11878v1&entry.124074799=Read"},
{"title": "Cycle Contrastive Adversarial Learning for Unsupervised image Deraining", "author": "Chen Zhao and Weiling Cai and ChengWei Hu and Zheng Yuan", "abstract": "  To tackle the difficulties in fitting paired real-world data for single image\nderaining (SID), recent unsupervised methods have achieved notable success.\nHowever, these methods often struggle to generate high-quality, rain-free\nimages due to a lack of attention to semantic representation and image content,\nresulting in ineffective separation of content from the rain layer. In this\npaper, we propose a novel cycle contrastive generative adversarial network for\nunsupervised SID, called CCLGAN. This framework combines cycle contrastive\nlearning (CCL) and location contrastive learning (LCL). CCL improves image\nreconstruction and rain-layer removal by bringing similar features closer and\npushing dissimilar features apart in both semantic and discriminative spaces.\nAt the same time, LCL preserves content information by constraining mutual\ninformation at the same location across different exemplars. CCLGAN shows\nsuperior performance, as extensive experiments demonstrate the benefits of\nCCLGAN and the effectiveness of its components.\n", "link": "http://arxiv.org/abs/2407.11750v1", "date": "2024-07-16", "relevancy": 2.1009, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5431}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycle%20Contrastive%20Adversarial%20Learning%20for%20Unsupervised%20image%20Deraining&body=Title%3A%20Cycle%20Contrastive%20Adversarial%20Learning%20for%20Unsupervised%20image%20Deraining%0AAuthor%3A%20Chen%20Zhao%20and%20Weiling%20Cai%20and%20ChengWei%20Hu%20and%20Zheng%20Yuan%0AAbstract%3A%20%20%20To%20tackle%20the%20difficulties%20in%20fitting%20paired%20real-world%20data%20for%20single%20image%0Aderaining%20%28SID%29%2C%20recent%20unsupervised%20methods%20have%20achieved%20notable%20success.%0AHowever%2C%20these%20methods%20often%20struggle%20to%20generate%20high-quality%2C%20rain-free%0Aimages%20due%20to%20a%20lack%20of%20attention%20to%20semantic%20representation%20and%20image%20content%2C%0Aresulting%20in%20ineffective%20separation%20of%20content%20from%20the%20rain%20layer.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20cycle%20contrastive%20generative%20adversarial%20network%20for%0Aunsupervised%20SID%2C%20called%20CCLGAN.%20This%20framework%20combines%20cycle%20contrastive%0Alearning%20%28CCL%29%20and%20location%20contrastive%20learning%20%28LCL%29.%20CCL%20improves%20image%0Areconstruction%20and%20rain-layer%20removal%20by%20bringing%20similar%20features%20closer%20and%0Apushing%20dissimilar%20features%20apart%20in%20both%20semantic%20and%20discriminative%20spaces.%0AAt%20the%20same%20time%2C%20LCL%20preserves%20content%20information%20by%20constraining%20mutual%0Ainformation%20at%20the%20same%20location%20across%20different%20exemplars.%20CCLGAN%20shows%0Asuperior%20performance%2C%20as%20extensive%20experiments%20demonstrate%20the%20benefits%20of%0ACCLGAN%20and%20the%20effectiveness%20of%20its%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycle%2520Contrastive%2520Adversarial%2520Learning%2520for%2520Unsupervised%2520image%2520Deraining%26entry.906535625%3DChen%2520Zhao%2520and%2520Weiling%2520Cai%2520and%2520ChengWei%2520Hu%2520and%2520Zheng%2520Yuan%26entry.1292438233%3D%2520%2520To%2520tackle%2520the%2520difficulties%2520in%2520fitting%2520paired%2520real-world%2520data%2520for%2520single%2520image%250Aderaining%2520%2528SID%2529%252C%2520recent%2520unsupervised%2520methods%2520have%2520achieved%2520notable%2520success.%250AHowever%252C%2520these%2520methods%2520often%2520struggle%2520to%2520generate%2520high-quality%252C%2520rain-free%250Aimages%2520due%2520to%2520a%2520lack%2520of%2520attention%2520to%2520semantic%2520representation%2520and%2520image%2520content%252C%250Aresulting%2520in%2520ineffective%2520separation%2520of%2520content%2520from%2520the%2520rain%2520layer.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520cycle%2520contrastive%2520generative%2520adversarial%2520network%2520for%250Aunsupervised%2520SID%252C%2520called%2520CCLGAN.%2520This%2520framework%2520combines%2520cycle%2520contrastive%250Alearning%2520%2528CCL%2529%2520and%2520location%2520contrastive%2520learning%2520%2528LCL%2529.%2520CCL%2520improves%2520image%250Areconstruction%2520and%2520rain-layer%2520removal%2520by%2520bringing%2520similar%2520features%2520closer%2520and%250Apushing%2520dissimilar%2520features%2520apart%2520in%2520both%2520semantic%2520and%2520discriminative%2520spaces.%250AAt%2520the%2520same%2520time%252C%2520LCL%2520preserves%2520content%2520information%2520by%2520constraining%2520mutual%250Ainformation%2520at%2520the%2520same%2520location%2520across%2520different%2520exemplars.%2520CCLGAN%2520shows%250Asuperior%2520performance%252C%2520as%2520extensive%2520experiments%2520demonstrate%2520the%2520benefits%2520of%250ACCLGAN%2520and%2520the%2520effectiveness%2520of%2520its%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle%20Contrastive%20Adversarial%20Learning%20for%20Unsupervised%20image%20Deraining&entry.906535625=Chen%20Zhao%20and%20Weiling%20Cai%20and%20ChengWei%20Hu%20and%20Zheng%20Yuan&entry.1292438233=%20%20To%20tackle%20the%20difficulties%20in%20fitting%20paired%20real-world%20data%20for%20single%20image%0Aderaining%20%28SID%29%2C%20recent%20unsupervised%20methods%20have%20achieved%20notable%20success.%0AHowever%2C%20these%20methods%20often%20struggle%20to%20generate%20high-quality%2C%20rain-free%0Aimages%20due%20to%20a%20lack%20of%20attention%20to%20semantic%20representation%20and%20image%20content%2C%0Aresulting%20in%20ineffective%20separation%20of%20content%20from%20the%20rain%20layer.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20cycle%20contrastive%20generative%20adversarial%20network%20for%0Aunsupervised%20SID%2C%20called%20CCLGAN.%20This%20framework%20combines%20cycle%20contrastive%0Alearning%20%28CCL%29%20and%20location%20contrastive%20learning%20%28LCL%29.%20CCL%20improves%20image%0Areconstruction%20and%20rain-layer%20removal%20by%20bringing%20similar%20features%20closer%20and%0Apushing%20dissimilar%20features%20apart%20in%20both%20semantic%20and%20discriminative%20spaces.%0AAt%20the%20same%20time%2C%20LCL%20preserves%20content%20information%20by%20constraining%20mutual%0Ainformation%20at%20the%20same%20location%20across%20different%20exemplars.%20CCLGAN%20shows%0Asuperior%20performance%2C%20as%20extensive%20experiments%20demonstrate%20the%20benefits%20of%0ACCLGAN%20and%20the%20effectiveness%20of%20its%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11750v1&entry.124074799=Read"},
{"title": "Controllable Navigation Instruction Generation with Chain of Thought\n  Prompting", "author": "Xianghao Kong and Jinyu Chen and Wenguan Wang and Hang Su and Xiaolin Hu and Yi Yang and Si Liu", "abstract": "  Instruction generation is a vital and multidisciplinary research area with\nbroad applications. Existing instruction generation models are limited to\ngenerating instructions in a single style from a particular dataset, and the\nstyle and content of generated instructions cannot be controlled. Moreover,\nmost existing instruction generation methods also disregard the spatial\nmodeling of the navigation environment. Leveraging the capabilities of Large\nLanguage Models (LLMs), we propose C-Instructor, which utilizes the\nchain-of-thought-style prompt for style-controllable and content-controllable\ninstruction generation. Firstly, we propose a Chain of Thought with Landmarks\n(CoTL) mechanism, which guides the LLM to identify key landmarks and then\ngenerate complete instructions. CoTL renders generated instructions more\naccessible to follow and offers greater controllability over the manipulation\nof landmark objects. Furthermore, we present a Spatial Topology Modeling Task\nto facilitate the understanding of the spatial structure of the environment.\nFinally, we introduce a Style-Mixed Training policy, harnessing the prior\nknowledge of LLMs to enable style control for instruction generation based on\ndifferent prompts within a single model instance. Extensive experiments\ndemonstrate that instructions generated by C-Instructor outperform those\ngenerated by previous methods in text metrics, navigation guidance evaluation,\nand user studies.\n", "link": "http://arxiv.org/abs/2407.07433v2", "date": "2024-07-16", "relevancy": 2.1002, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5372}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5339}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Navigation%20Instruction%20Generation%20with%20Chain%20of%20Thought%0A%20%20Prompting&body=Title%3A%20Controllable%20Navigation%20Instruction%20Generation%20with%20Chain%20of%20Thought%0A%20%20Prompting%0AAuthor%3A%20Xianghao%20Kong%20and%20Jinyu%20Chen%20and%20Wenguan%20Wang%20and%20Hang%20Su%20and%20Xiaolin%20Hu%20and%20Yi%20Yang%20and%20Si%20Liu%0AAbstract%3A%20%20%20Instruction%20generation%20is%20a%20vital%20and%20multidisciplinary%20research%20area%20with%0Abroad%20applications.%20Existing%20instruction%20generation%20models%20are%20limited%20to%0Agenerating%20instructions%20in%20a%20single%20style%20from%20a%20particular%20dataset%2C%20and%20the%0Astyle%20and%20content%20of%20generated%20instructions%20cannot%20be%20controlled.%20Moreover%2C%0Amost%20existing%20instruction%20generation%20methods%20also%20disregard%20the%20spatial%0Amodeling%20of%20the%20navigation%20environment.%20Leveraging%20the%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20propose%20C-Instructor%2C%20which%20utilizes%20the%0Achain-of-thought-style%20prompt%20for%20style-controllable%20and%20content-controllable%0Ainstruction%20generation.%20Firstly%2C%20we%20propose%20a%20Chain%20of%20Thought%20with%20Landmarks%0A%28CoTL%29%20mechanism%2C%20which%20guides%20the%20LLM%20to%20identify%20key%20landmarks%20and%20then%0Agenerate%20complete%20instructions.%20CoTL%20renders%20generated%20instructions%20more%0Aaccessible%20to%20follow%20and%20offers%20greater%20controllability%20over%20the%20manipulation%0Aof%20landmark%20objects.%20Furthermore%2C%20we%20present%20a%20Spatial%20Topology%20Modeling%20Task%0Ato%20facilitate%20the%20understanding%20of%20the%20spatial%20structure%20of%20the%20environment.%0AFinally%2C%20we%20introduce%20a%20Style-Mixed%20Training%20policy%2C%20harnessing%20the%20prior%0Aknowledge%20of%20LLMs%20to%20enable%20style%20control%20for%20instruction%20generation%20based%20on%0Adifferent%20prompts%20within%20a%20single%20model%20instance.%20Extensive%20experiments%0Ademonstrate%20that%20instructions%20generated%20by%20C-Instructor%20outperform%20those%0Agenerated%20by%20previous%20methods%20in%20text%20metrics%2C%20navigation%20guidance%20evaluation%2C%0Aand%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Navigation%2520Instruction%2520Generation%2520with%2520Chain%2520of%2520Thought%250A%2520%2520Prompting%26entry.906535625%3DXianghao%2520Kong%2520and%2520Jinyu%2520Chen%2520and%2520Wenguan%2520Wang%2520and%2520Hang%2520Su%2520and%2520Xiaolin%2520Hu%2520and%2520Yi%2520Yang%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520Instruction%2520generation%2520is%2520a%2520vital%2520and%2520multidisciplinary%2520research%2520area%2520with%250Abroad%2520applications.%2520Existing%2520instruction%2520generation%2520models%2520are%2520limited%2520to%250Agenerating%2520instructions%2520in%2520a%2520single%2520style%2520from%2520a%2520particular%2520dataset%252C%2520and%2520the%250Astyle%2520and%2520content%2520of%2520generated%2520instructions%2520cannot%2520be%2520controlled.%2520Moreover%252C%250Amost%2520existing%2520instruction%2520generation%2520methods%2520also%2520disregard%2520the%2520spatial%250Amodeling%2520of%2520the%2520navigation%2520environment.%2520Leveraging%2520the%2520capabilities%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520we%2520propose%2520C-Instructor%252C%2520which%2520utilizes%2520the%250Achain-of-thought-style%2520prompt%2520for%2520style-controllable%2520and%2520content-controllable%250Ainstruction%2520generation.%2520Firstly%252C%2520we%2520propose%2520a%2520Chain%2520of%2520Thought%2520with%2520Landmarks%250A%2528CoTL%2529%2520mechanism%252C%2520which%2520guides%2520the%2520LLM%2520to%2520identify%2520key%2520landmarks%2520and%2520then%250Agenerate%2520complete%2520instructions.%2520CoTL%2520renders%2520generated%2520instructions%2520more%250Aaccessible%2520to%2520follow%2520and%2520offers%2520greater%2520controllability%2520over%2520the%2520manipulation%250Aof%2520landmark%2520objects.%2520Furthermore%252C%2520we%2520present%2520a%2520Spatial%2520Topology%2520Modeling%2520Task%250Ato%2520facilitate%2520the%2520understanding%2520of%2520the%2520spatial%2520structure%2520of%2520the%2520environment.%250AFinally%252C%2520we%2520introduce%2520a%2520Style-Mixed%2520Training%2520policy%252C%2520harnessing%2520the%2520prior%250Aknowledge%2520of%2520LLMs%2520to%2520enable%2520style%2520control%2520for%2520instruction%2520generation%2520based%2520on%250Adifferent%2520prompts%2520within%2520a%2520single%2520model%2520instance.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520instructions%2520generated%2520by%2520C-Instructor%2520outperform%2520those%250Agenerated%2520by%2520previous%2520methods%2520in%2520text%2520metrics%252C%2520navigation%2520guidance%2520evaluation%252C%250Aand%2520user%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Navigation%20Instruction%20Generation%20with%20Chain%20of%20Thought%0A%20%20Prompting&entry.906535625=Xianghao%20Kong%20and%20Jinyu%20Chen%20and%20Wenguan%20Wang%20and%20Hang%20Su%20and%20Xiaolin%20Hu%20and%20Yi%20Yang%20and%20Si%20Liu&entry.1292438233=%20%20Instruction%20generation%20is%20a%20vital%20and%20multidisciplinary%20research%20area%20with%0Abroad%20applications.%20Existing%20instruction%20generation%20models%20are%20limited%20to%0Agenerating%20instructions%20in%20a%20single%20style%20from%20a%20particular%20dataset%2C%20and%20the%0Astyle%20and%20content%20of%20generated%20instructions%20cannot%20be%20controlled.%20Moreover%2C%0Amost%20existing%20instruction%20generation%20methods%20also%20disregard%20the%20spatial%0Amodeling%20of%20the%20navigation%20environment.%20Leveraging%20the%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20propose%20C-Instructor%2C%20which%20utilizes%20the%0Achain-of-thought-style%20prompt%20for%20style-controllable%20and%20content-controllable%0Ainstruction%20generation.%20Firstly%2C%20we%20propose%20a%20Chain%20of%20Thought%20with%20Landmarks%0A%28CoTL%29%20mechanism%2C%20which%20guides%20the%20LLM%20to%20identify%20key%20landmarks%20and%20then%0Agenerate%20complete%20instructions.%20CoTL%20renders%20generated%20instructions%20more%0Aaccessible%20to%20follow%20and%20offers%20greater%20controllability%20over%20the%20manipulation%0Aof%20landmark%20objects.%20Furthermore%2C%20we%20present%20a%20Spatial%20Topology%20Modeling%20Task%0Ato%20facilitate%20the%20understanding%20of%20the%20spatial%20structure%20of%20the%20environment.%0AFinally%2C%20we%20introduce%20a%20Style-Mixed%20Training%20policy%2C%20harnessing%20the%20prior%0Aknowledge%20of%20LLMs%20to%20enable%20style%20control%20for%20instruction%20generation%20based%20on%0Adifferent%20prompts%20within%20a%20single%20model%20instance.%20Extensive%20experiments%0Ademonstrate%20that%20instructions%20generated%20by%20C-Instructor%20outperform%20those%0Agenerated%20by%20previous%20methods%20in%20text%20metrics%2C%20navigation%20guidance%20evaluation%2C%0Aand%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07433v2&entry.124074799=Read"},
{"title": "Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object\n  Detector", "author": "Yuqian Fu and Yu Wang and Yixuan Pan and Lian Huai and Xingyu Qiu and Zeyu Shangguan and Tong Liu and Yanwei Fu and Luc Van Gool and Xingqun Jiang", "abstract": "  This paper studies the challenging cross-domain few-shot object detection\n(CD-FSOD), aiming to develop an accurate object detector for novel domains with\nminimal labeled examples. While transformer-based open-set detectors, such as\nDE-ViT, show promise in traditional few-shot object detection, their\ngeneralization to CD-FSOD remains unclear: 1) can such open-set detection\nmethods easily generalize to CD-FSOD? 2) If not, how can models be enhanced\nwhen facing huge domain gaps? To answer the first question, we employ measures\nincluding style, inter-class variance (ICV), and indefinable boundaries (IB) to\nunderstand the domain gap. Based on these measures, we establish a new\nbenchmark named CD-FSOD to evaluate object detection methods, revealing that\nmost of the current approaches fail to generalize across domains. Technically,\nwe observe that the performance decline is associated with our proposed\nmeasures: style, ICV, and IB. Consequently, we propose several novel modules to\naddress these issues. First, the learnable instance features align initial\nfixed instances with target categories, enhancing feature distinctiveness.\nSecond, the instance reweighting module assigns higher importance to\nhigh-quality instances with slight IB. Third, the domain prompter encourages\nfeatures resilient to different styles by synthesizing imaginary domains\nwithout altering semantic contents. These techniques collectively contribute to\nthe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),\nsignificantly improving upon the base DE-ViT. Experimental results validate the\nefficacy of our model.\n", "link": "http://arxiv.org/abs/2402.03094v3", "date": "2024-07-16", "relevancy": 2.073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector&body=Title%3A%20Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector%0AAuthor%3A%20Yuqian%20Fu%20and%20Yu%20Wang%20and%20Yixuan%20Pan%20and%20Lian%20Huai%20and%20Xingyu%20Qiu%20and%20Zeyu%20Shangguan%20and%20Tong%20Liu%20and%20Yanwei%20Fu%20and%20Luc%20Van%20Gool%20and%20Xingqun%20Jiang%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20challenging%20cross-domain%20few-shot%20object%20detection%0A%28CD-FSOD%29%2C%20aiming%20to%20develop%20an%20accurate%20object%20detector%20for%20novel%20domains%20with%0Aminimal%20labeled%20examples.%20While%20transformer-based%20open-set%20detectors%2C%20such%20as%0ADE-ViT%2C%20show%20promise%20in%20traditional%20few-shot%20object%20detection%2C%20their%0Ageneralization%20to%20CD-FSOD%20remains%20unclear%3A%201%29%20can%20such%20open-set%20detection%0Amethods%20easily%20generalize%20to%20CD-FSOD%3F%202%29%20If%20not%2C%20how%20can%20models%20be%20enhanced%0Awhen%20facing%20huge%20domain%20gaps%3F%20To%20answer%20the%20first%20question%2C%20we%20employ%20measures%0Aincluding%20style%2C%20inter-class%20variance%20%28ICV%29%2C%20and%20indefinable%20boundaries%20%28IB%29%20to%0Aunderstand%20the%20domain%20gap.%20Based%20on%20these%20measures%2C%20we%20establish%20a%20new%0Abenchmark%20named%20CD-FSOD%20to%20evaluate%20object%20detection%20methods%2C%20revealing%20that%0Amost%20of%20the%20current%20approaches%20fail%20to%20generalize%20across%20domains.%20Technically%2C%0Awe%20observe%20that%20the%20performance%20decline%20is%20associated%20with%20our%20proposed%0Ameasures%3A%20style%2C%20ICV%2C%20and%20IB.%20Consequently%2C%20we%20propose%20several%20novel%20modules%20to%0Aaddress%20these%20issues.%20First%2C%20the%20learnable%20instance%20features%20align%20initial%0Afixed%20instances%20with%20target%20categories%2C%20enhancing%20feature%20distinctiveness.%0ASecond%2C%20the%20instance%20reweighting%20module%20assigns%20higher%20importance%20to%0Ahigh-quality%20instances%20with%20slight%20IB.%20Third%2C%20the%20domain%20prompter%20encourages%0Afeatures%20resilient%20to%20different%20styles%20by%20synthesizing%20imaginary%20domains%0Awithout%20altering%20semantic%20contents.%20These%20techniques%20collectively%20contribute%20to%0Athe%20development%20of%20the%20Cross-Domain%20Vision%20Transformer%20for%20CD-FSOD%20%28CD-ViTO%29%2C%0Asignificantly%20improving%20upon%20the%20base%20DE-ViT.%20Experimental%20results%20validate%20the%0Aefficacy%20of%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03094v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Few-Shot%2520Object%2520Detection%2520via%2520Enhanced%2520Open-Set%2520Object%250A%2520%2520Detector%26entry.906535625%3DYuqian%2520Fu%2520and%2520Yu%2520Wang%2520and%2520Yixuan%2520Pan%2520and%2520Lian%2520Huai%2520and%2520Xingyu%2520Qiu%2520and%2520Zeyu%2520Shangguan%2520and%2520Tong%2520Liu%2520and%2520Yanwei%2520Fu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xingqun%2520Jiang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520challenging%2520cross-domain%2520few-shot%2520object%2520detection%250A%2528CD-FSOD%2529%252C%2520aiming%2520to%2520develop%2520an%2520accurate%2520object%2520detector%2520for%2520novel%2520domains%2520with%250Aminimal%2520labeled%2520examples.%2520While%2520transformer-based%2520open-set%2520detectors%252C%2520such%2520as%250ADE-ViT%252C%2520show%2520promise%2520in%2520traditional%2520few-shot%2520object%2520detection%252C%2520their%250Ageneralization%2520to%2520CD-FSOD%2520remains%2520unclear%253A%25201%2529%2520can%2520such%2520open-set%2520detection%250Amethods%2520easily%2520generalize%2520to%2520CD-FSOD%253F%25202%2529%2520If%2520not%252C%2520how%2520can%2520models%2520be%2520enhanced%250Awhen%2520facing%2520huge%2520domain%2520gaps%253F%2520To%2520answer%2520the%2520first%2520question%252C%2520we%2520employ%2520measures%250Aincluding%2520style%252C%2520inter-class%2520variance%2520%2528ICV%2529%252C%2520and%2520indefinable%2520boundaries%2520%2528IB%2529%2520to%250Aunderstand%2520the%2520domain%2520gap.%2520Based%2520on%2520these%2520measures%252C%2520we%2520establish%2520a%2520new%250Abenchmark%2520named%2520CD-FSOD%2520to%2520evaluate%2520object%2520detection%2520methods%252C%2520revealing%2520that%250Amost%2520of%2520the%2520current%2520approaches%2520fail%2520to%2520generalize%2520across%2520domains.%2520Technically%252C%250Awe%2520observe%2520that%2520the%2520performance%2520decline%2520is%2520associated%2520with%2520our%2520proposed%250Ameasures%253A%2520style%252C%2520ICV%252C%2520and%2520IB.%2520Consequently%252C%2520we%2520propose%2520several%2520novel%2520modules%2520to%250Aaddress%2520these%2520issues.%2520First%252C%2520the%2520learnable%2520instance%2520features%2520align%2520initial%250Afixed%2520instances%2520with%2520target%2520categories%252C%2520enhancing%2520feature%2520distinctiveness.%250ASecond%252C%2520the%2520instance%2520reweighting%2520module%2520assigns%2520higher%2520importance%2520to%250Ahigh-quality%2520instances%2520with%2520slight%2520IB.%2520Third%252C%2520the%2520domain%2520prompter%2520encourages%250Afeatures%2520resilient%2520to%2520different%2520styles%2520by%2520synthesizing%2520imaginary%2520domains%250Awithout%2520altering%2520semantic%2520contents.%2520These%2520techniques%2520collectively%2520contribute%2520to%250Athe%2520development%2520of%2520the%2520Cross-Domain%2520Vision%2520Transformer%2520for%2520CD-FSOD%2520%2528CD-ViTO%2529%252C%250Asignificantly%2520improving%2520upon%2520the%2520base%2520DE-ViT.%2520Experimental%2520results%2520validate%2520the%250Aefficacy%2520of%2520our%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03094v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector&entry.906535625=Yuqian%20Fu%20and%20Yu%20Wang%20and%20Yixuan%20Pan%20and%20Lian%20Huai%20and%20Xingyu%20Qiu%20and%20Zeyu%20Shangguan%20and%20Tong%20Liu%20and%20Yanwei%20Fu%20and%20Luc%20Van%20Gool%20and%20Xingqun%20Jiang&entry.1292438233=%20%20This%20paper%20studies%20the%20challenging%20cross-domain%20few-shot%20object%20detection%0A%28CD-FSOD%29%2C%20aiming%20to%20develop%20an%20accurate%20object%20detector%20for%20novel%20domains%20with%0Aminimal%20labeled%20examples.%20While%20transformer-based%20open-set%20detectors%2C%20such%20as%0ADE-ViT%2C%20show%20promise%20in%20traditional%20few-shot%20object%20detection%2C%20their%0Ageneralization%20to%20CD-FSOD%20remains%20unclear%3A%201%29%20can%20such%20open-set%20detection%0Amethods%20easily%20generalize%20to%20CD-FSOD%3F%202%29%20If%20not%2C%20how%20can%20models%20be%20enhanced%0Awhen%20facing%20huge%20domain%20gaps%3F%20To%20answer%20the%20first%20question%2C%20we%20employ%20measures%0Aincluding%20style%2C%20inter-class%20variance%20%28ICV%29%2C%20and%20indefinable%20boundaries%20%28IB%29%20to%0Aunderstand%20the%20domain%20gap.%20Based%20on%20these%20measures%2C%20we%20establish%20a%20new%0Abenchmark%20named%20CD-FSOD%20to%20evaluate%20object%20detection%20methods%2C%20revealing%20that%0Amost%20of%20the%20current%20approaches%20fail%20to%20generalize%20across%20domains.%20Technically%2C%0Awe%20observe%20that%20the%20performance%20decline%20is%20associated%20with%20our%20proposed%0Ameasures%3A%20style%2C%20ICV%2C%20and%20IB.%20Consequently%2C%20we%20propose%20several%20novel%20modules%20to%0Aaddress%20these%20issues.%20First%2C%20the%20learnable%20instance%20features%20align%20initial%0Afixed%20instances%20with%20target%20categories%2C%20enhancing%20feature%20distinctiveness.%0ASecond%2C%20the%20instance%20reweighting%20module%20assigns%20higher%20importance%20to%0Ahigh-quality%20instances%20with%20slight%20IB.%20Third%2C%20the%20domain%20prompter%20encourages%0Afeatures%20resilient%20to%20different%20styles%20by%20synthesizing%20imaginary%20domains%0Awithout%20altering%20semantic%20contents.%20These%20techniques%20collectively%20contribute%20to%0Athe%20development%20of%20the%20Cross-Domain%20Vision%20Transformer%20for%20CD-FSOD%20%28CD-ViTO%29%2C%0Asignificantly%20improving%20upon%20the%20base%20DE-ViT.%20Experimental%20results%20validate%20the%0Aefficacy%20of%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03094v3&entry.124074799=Read"},
{"title": "Enhancing Split Computing and Early Exit Applications through Predefined\n  Sparsity", "author": "Luigi Capogrosso and Enrico Fraccaroli and Giulio Petrozziello and Francesco Setti and Samarjit Chakraborty and Franco Fummi and Marco Cristani", "abstract": "  In the past decade, Deep Neural Networks (DNNs) achieved state-of-the-art\nperformance in a broad range of problems, spanning from object classification\nand action recognition to smart building and healthcare. The flexibility that\nmakes DNNs such a pervasive technology comes at a price: the computational\nrequirements preclude their deployment on most of the resource-constrained edge\ndevices available today to solve real-time and real-world tasks. This paper\nintroduces a novel approach to address this challenge by combining the concept\nof predefined sparsity with Split Computing (SC) and Early Exit (EE). In\nparticular, SC aims at splitting a DNN with a part of it deployed on an edge\ndevice and the rest on a remote server. Instead, EE allows the system to stop\nusing the remote server and rely solely on the edge device's computation if the\nanswer is already good enough. Specifically, how to apply such a predefined\nsparsity to a SC and EE paradigm has never been studied. This paper studies\nthis problem and shows how predefined sparsity significantly reduces the\ncomputational, storage, and energy burdens during the training and inference\nphases, regardless of the hardware platform. This makes it a valuable approach\nfor enhancing the performance of SC and EE applications. Experimental results\nshowcase reductions exceeding 4x in storage and computational complexity\nwithout compromising performance. The source code is available at\nhttps://github.com/intelligolabs/sparsity_sc_ee.\n", "link": "http://arxiv.org/abs/2407.11763v1", "date": "2024-07-16", "relevancy": 2.0713, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5098}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Split%20Computing%20and%20Early%20Exit%20Applications%20through%20Predefined%0A%20%20Sparsity&body=Title%3A%20Enhancing%20Split%20Computing%20and%20Early%20Exit%20Applications%20through%20Predefined%0A%20%20Sparsity%0AAuthor%3A%20Luigi%20Capogrosso%20and%20Enrico%20Fraccaroli%20and%20Giulio%20Petrozziello%20and%20Francesco%20Setti%20and%20Samarjit%20Chakraborty%20and%20Franco%20Fummi%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20In%20the%20past%20decade%2C%20Deep%20Neural%20Networks%20%28DNNs%29%20achieved%20state-of-the-art%0Aperformance%20in%20a%20broad%20range%20of%20problems%2C%20spanning%20from%20object%20classification%0Aand%20action%20recognition%20to%20smart%20building%20and%20healthcare.%20The%20flexibility%20that%0Amakes%20DNNs%20such%20a%20pervasive%20technology%20comes%20at%20a%20price%3A%20the%20computational%0Arequirements%20preclude%20their%20deployment%20on%20most%20of%20the%20resource-constrained%20edge%0Adevices%20available%20today%20to%20solve%20real-time%20and%20real-world%20tasks.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20address%20this%20challenge%20by%20combining%20the%20concept%0Aof%20predefined%20sparsity%20with%20Split%20Computing%20%28SC%29%20and%20Early%20Exit%20%28EE%29.%20In%0Aparticular%2C%20SC%20aims%20at%20splitting%20a%20DNN%20with%20a%20part%20of%20it%20deployed%20on%20an%20edge%0Adevice%20and%20the%20rest%20on%20a%20remote%20server.%20Instead%2C%20EE%20allows%20the%20system%20to%20stop%0Ausing%20the%20remote%20server%20and%20rely%20solely%20on%20the%20edge%20device%27s%20computation%20if%20the%0Aanswer%20is%20already%20good%20enough.%20Specifically%2C%20how%20to%20apply%20such%20a%20predefined%0Asparsity%20to%20a%20SC%20and%20EE%20paradigm%20has%20never%20been%20studied.%20This%20paper%20studies%0Athis%20problem%20and%20shows%20how%20predefined%20sparsity%20significantly%20reduces%20the%0Acomputational%2C%20storage%2C%20and%20energy%20burdens%20during%20the%20training%20and%20inference%0Aphases%2C%20regardless%20of%20the%20hardware%20platform.%20This%20makes%20it%20a%20valuable%20approach%0Afor%20enhancing%20the%20performance%20of%20SC%20and%20EE%20applications.%20Experimental%20results%0Ashowcase%20reductions%20exceeding%204x%20in%20storage%20and%20computational%20complexity%0Awithout%20compromising%20performance.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/sparsity_sc_ee.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Split%2520Computing%2520and%2520Early%2520Exit%2520Applications%2520through%2520Predefined%250A%2520%2520Sparsity%26entry.906535625%3DLuigi%2520Capogrosso%2520and%2520Enrico%2520Fraccaroli%2520and%2520Giulio%2520Petrozziello%2520and%2520Francesco%2520Setti%2520and%2520Samarjit%2520Chakraborty%2520and%2520Franco%2520Fummi%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520decade%252C%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520achieved%2520state-of-the-art%250Aperformance%2520in%2520a%2520broad%2520range%2520of%2520problems%252C%2520spanning%2520from%2520object%2520classification%250Aand%2520action%2520recognition%2520to%2520smart%2520building%2520and%2520healthcare.%2520The%2520flexibility%2520that%250Amakes%2520DNNs%2520such%2520a%2520pervasive%2520technology%2520comes%2520at%2520a%2520price%253A%2520the%2520computational%250Arequirements%2520preclude%2520their%2520deployment%2520on%2520most%2520of%2520the%2520resource-constrained%2520edge%250Adevices%2520available%2520today%2520to%2520solve%2520real-time%2520and%2520real-world%2520tasks.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520approach%2520to%2520address%2520this%2520challenge%2520by%2520combining%2520the%2520concept%250Aof%2520predefined%2520sparsity%2520with%2520Split%2520Computing%2520%2528SC%2529%2520and%2520Early%2520Exit%2520%2528EE%2529.%2520In%250Aparticular%252C%2520SC%2520aims%2520at%2520splitting%2520a%2520DNN%2520with%2520a%2520part%2520of%2520it%2520deployed%2520on%2520an%2520edge%250Adevice%2520and%2520the%2520rest%2520on%2520a%2520remote%2520server.%2520Instead%252C%2520EE%2520allows%2520the%2520system%2520to%2520stop%250Ausing%2520the%2520remote%2520server%2520and%2520rely%2520solely%2520on%2520the%2520edge%2520device%2527s%2520computation%2520if%2520the%250Aanswer%2520is%2520already%2520good%2520enough.%2520Specifically%252C%2520how%2520to%2520apply%2520such%2520a%2520predefined%250Asparsity%2520to%2520a%2520SC%2520and%2520EE%2520paradigm%2520has%2520never%2520been%2520studied.%2520This%2520paper%2520studies%250Athis%2520problem%2520and%2520shows%2520how%2520predefined%2520sparsity%2520significantly%2520reduces%2520the%250Acomputational%252C%2520storage%252C%2520and%2520energy%2520burdens%2520during%2520the%2520training%2520and%2520inference%250Aphases%252C%2520regardless%2520of%2520the%2520hardware%2520platform.%2520This%2520makes%2520it%2520a%2520valuable%2520approach%250Afor%2520enhancing%2520the%2520performance%2520of%2520SC%2520and%2520EE%2520applications.%2520Experimental%2520results%250Ashowcase%2520reductions%2520exceeding%25204x%2520in%2520storage%2520and%2520computational%2520complexity%250Awithout%2520compromising%2520performance.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/intelligolabs/sparsity_sc_ee.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Split%20Computing%20and%20Early%20Exit%20Applications%20through%20Predefined%0A%20%20Sparsity&entry.906535625=Luigi%20Capogrosso%20and%20Enrico%20Fraccaroli%20and%20Giulio%20Petrozziello%20and%20Francesco%20Setti%20and%20Samarjit%20Chakraborty%20and%20Franco%20Fummi%20and%20Marco%20Cristani&entry.1292438233=%20%20In%20the%20past%20decade%2C%20Deep%20Neural%20Networks%20%28DNNs%29%20achieved%20state-of-the-art%0Aperformance%20in%20a%20broad%20range%20of%20problems%2C%20spanning%20from%20object%20classification%0Aand%20action%20recognition%20to%20smart%20building%20and%20healthcare.%20The%20flexibility%20that%0Amakes%20DNNs%20such%20a%20pervasive%20technology%20comes%20at%20a%20price%3A%20the%20computational%0Arequirements%20preclude%20their%20deployment%20on%20most%20of%20the%20resource-constrained%20edge%0Adevices%20available%20today%20to%20solve%20real-time%20and%20real-world%20tasks.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20address%20this%20challenge%20by%20combining%20the%20concept%0Aof%20predefined%20sparsity%20with%20Split%20Computing%20%28SC%29%20and%20Early%20Exit%20%28EE%29.%20In%0Aparticular%2C%20SC%20aims%20at%20splitting%20a%20DNN%20with%20a%20part%20of%20it%20deployed%20on%20an%20edge%0Adevice%20and%20the%20rest%20on%20a%20remote%20server.%20Instead%2C%20EE%20allows%20the%20system%20to%20stop%0Ausing%20the%20remote%20server%20and%20rely%20solely%20on%20the%20edge%20device%27s%20computation%20if%20the%0Aanswer%20is%20already%20good%20enough.%20Specifically%2C%20how%20to%20apply%20such%20a%20predefined%0Asparsity%20to%20a%20SC%20and%20EE%20paradigm%20has%20never%20been%20studied.%20This%20paper%20studies%0Athis%20problem%20and%20shows%20how%20predefined%20sparsity%20significantly%20reduces%20the%0Acomputational%2C%20storage%2C%20and%20energy%20burdens%20during%20the%20training%20and%20inference%0Aphases%2C%20regardless%20of%20the%20hardware%20platform.%20This%20makes%20it%20a%20valuable%20approach%0Afor%20enhancing%20the%20performance%20of%20SC%20and%20EE%20applications.%20Experimental%20results%0Ashowcase%20reductions%20exceeding%204x%20in%20storage%20and%20computational%20complexity%0Awithout%20compromising%20performance.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/sparsity_sc_ee.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11763v1&entry.124074799=Read"},
{"title": "Affective Behavior Analysis using Task-adaptive and AU-assisted Graph\n  Network", "author": "Xiaodong Li and Wenchao Du and Hongyu Yang", "abstract": "  In this paper, we present our solution and experiment result for the\nMulti-Task Learning Challenge of the 7th Affective Behavior Analysis\nin-the-wild(ABAW7) Competition. This challenge consists of three tasks: action\nunit detection, facial expression recognition, and valance-arousal estimation.\nWe address the research problems of this challenge from three aspects: 1)For\nlearning robust visual feature representations, we introduce the pre-trained\nlarge model Dinov2. 2) To adaptively extract the required features of eack\ntask, we design a task-adaptive block that performs cross-attention between a\nset of learnable query vectors and pre-extracted features. 3) By proposing the\nAU-assisted Graph Convolutional Network(AU-GCN), we make full use of the\ncorrelation information between AUs to assist in solving the EXPR and VA tasks.\nFinally, we achieve the evaluation measure of \\textbf{1.2542} on the validation\nset provided by the organizers.\n", "link": "http://arxiv.org/abs/2407.11663v1", "date": "2024-07-16", "relevancy": 2.0531, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affective%20Behavior%20Analysis%20using%20Task-adaptive%20and%20AU-assisted%20Graph%0A%20%20Network&body=Title%3A%20Affective%20Behavior%20Analysis%20using%20Task-adaptive%20and%20AU-assisted%20Graph%0A%20%20Network%0AAuthor%3A%20Xiaodong%20Li%20and%20Wenchao%20Du%20and%20Hongyu%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20solution%20and%20experiment%20result%20for%20the%0AMulti-Task%20Learning%20Challenge%20of%20the%207th%20Affective%20Behavior%20Analysis%0Ain-the-wild%28ABAW7%29%20Competition.%20This%20challenge%20consists%20of%20three%20tasks%3A%20action%0Aunit%20detection%2C%20facial%20expression%20recognition%2C%20and%20valance-arousal%20estimation.%0AWe%20address%20the%20research%20problems%20of%20this%20challenge%20from%20three%20aspects%3A%201%29For%0Alearning%20robust%20visual%20feature%20representations%2C%20we%20introduce%20the%20pre-trained%0Alarge%20model%20Dinov2.%202%29%20To%20adaptively%20extract%20the%20required%20features%20of%20eack%0Atask%2C%20we%20design%20a%20task-adaptive%20block%20that%20performs%20cross-attention%20between%20a%0Aset%20of%20learnable%20query%20vectors%20and%20pre-extracted%20features.%203%29%20By%20proposing%20the%0AAU-assisted%20Graph%20Convolutional%20Network%28AU-GCN%29%2C%20we%20make%20full%20use%20of%20the%0Acorrelation%20information%20between%20AUs%20to%20assist%20in%20solving%20the%20EXPR%20and%20VA%20tasks.%0AFinally%2C%20we%20achieve%20the%20evaluation%20measure%20of%20%5Ctextbf%7B1.2542%7D%20on%20the%20validation%0Aset%20provided%20by%20the%20organizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffective%2520Behavior%2520Analysis%2520using%2520Task-adaptive%2520and%2520AU-assisted%2520Graph%250A%2520%2520Network%26entry.906535625%3DXiaodong%2520Li%2520and%2520Wenchao%2520Du%2520and%2520Hongyu%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520solution%2520and%2520experiment%2520result%2520for%2520the%250AMulti-Task%2520Learning%2520Challenge%2520of%2520the%25207th%2520Affective%2520Behavior%2520Analysis%250Ain-the-wild%2528ABAW7%2529%2520Competition.%2520This%2520challenge%2520consists%2520of%2520three%2520tasks%253A%2520action%250Aunit%2520detection%252C%2520facial%2520expression%2520recognition%252C%2520and%2520valance-arousal%2520estimation.%250AWe%2520address%2520the%2520research%2520problems%2520of%2520this%2520challenge%2520from%2520three%2520aspects%253A%25201%2529For%250Alearning%2520robust%2520visual%2520feature%2520representations%252C%2520we%2520introduce%2520the%2520pre-trained%250Alarge%2520model%2520Dinov2.%25202%2529%2520To%2520adaptively%2520extract%2520the%2520required%2520features%2520of%2520eack%250Atask%252C%2520we%2520design%2520a%2520task-adaptive%2520block%2520that%2520performs%2520cross-attention%2520between%2520a%250Aset%2520of%2520learnable%2520query%2520vectors%2520and%2520pre-extracted%2520features.%25203%2529%2520By%2520proposing%2520the%250AAU-assisted%2520Graph%2520Convolutional%2520Network%2528AU-GCN%2529%252C%2520we%2520make%2520full%2520use%2520of%2520the%250Acorrelation%2520information%2520between%2520AUs%2520to%2520assist%2520in%2520solving%2520the%2520EXPR%2520and%2520VA%2520tasks.%250AFinally%252C%2520we%2520achieve%2520the%2520evaluation%2520measure%2520of%2520%255Ctextbf%257B1.2542%257D%2520on%2520the%2520validation%250Aset%2520provided%2520by%2520the%2520organizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affective%20Behavior%20Analysis%20using%20Task-adaptive%20and%20AU-assisted%20Graph%0A%20%20Network&entry.906535625=Xiaodong%20Li%20and%20Wenchao%20Du%20and%20Hongyu%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20solution%20and%20experiment%20result%20for%20the%0AMulti-Task%20Learning%20Challenge%20of%20the%207th%20Affective%20Behavior%20Analysis%0Ain-the-wild%28ABAW7%29%20Competition.%20This%20challenge%20consists%20of%20three%20tasks%3A%20action%0Aunit%20detection%2C%20facial%20expression%20recognition%2C%20and%20valance-arousal%20estimation.%0AWe%20address%20the%20research%20problems%20of%20this%20challenge%20from%20three%20aspects%3A%201%29For%0Alearning%20robust%20visual%20feature%20representations%2C%20we%20introduce%20the%20pre-trained%0Alarge%20model%20Dinov2.%202%29%20To%20adaptively%20extract%20the%20required%20features%20of%20eack%0Atask%2C%20we%20design%20a%20task-adaptive%20block%20that%20performs%20cross-attention%20between%20a%0Aset%20of%20learnable%20query%20vectors%20and%20pre-extracted%20features.%203%29%20By%20proposing%20the%0AAU-assisted%20Graph%20Convolutional%20Network%28AU-GCN%29%2C%20we%20make%20full%20use%20of%20the%0Acorrelation%20information%20between%20AUs%20to%20assist%20in%20solving%20the%20EXPR%20and%20VA%20tasks.%0AFinally%2C%20we%20achieve%20the%20evaluation%20measure%20of%20%5Ctextbf%7B1.2542%7D%20on%20the%20validation%0Aset%20provided%20by%20the%20organizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11663v1&entry.124074799=Read"},
{"title": "Human-Machine Shared Control Approach for the Takeover of Cooperative\n  Adaptive Cruise Control", "author": "Haoran Wang and Zhenning Li and Arno Eichberger and Jia Hu", "abstract": "  Cooperative Adaptive Cruise Control (CACC) often requires human takeover for\ntasks such as exiting a freeway. Direct human takeover can pose significant\nrisks, especially given the close-following strategy employed by CACC, which\nmight cause drivers to feel unsafe and execute hard braking, potentially\nleading to collisions. This research aims to develop a CACC takeover controller\nthat ensures a smooth transition from automated to human control. The proposed\nCACC takeover maneuver employs an indirect human-machine shared control\napproach, modeled as a Stackelberg competition where the machine acts as the\nleader and the human as the follower. The machine guides the human to respond\nin a manner that aligns with the machine's expectations, aiding in maintaining\nfollowing stability. Additionally, the human reaction function is integrated\ninto the machine's predictive control system, moving beyond a simple\n\"prediction-planning\" pipeline to enhance planning optimality. The controller\nhas been verified to i) enable a smooth takeover maneuver of CACC; ii) ensure\nstring stability within a specific Operational Design Domain (ODD) when human\ncontrol authority is below 32.7%; iii) enhance both perceived and actual safety\nthrough machine interventions; and iv) reduce the impact on upstream traffic by\nup to 60%.\n", "link": "http://arxiv.org/abs/2407.11551v1", "date": "2024-07-16", "relevancy": 2.0517, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5295}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5182}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Machine%20Shared%20Control%20Approach%20for%20the%20Takeover%20of%20Cooperative%0A%20%20Adaptive%20Cruise%20Control&body=Title%3A%20Human-Machine%20Shared%20Control%20Approach%20for%20the%20Takeover%20of%20Cooperative%0A%20%20Adaptive%20Cruise%20Control%0AAuthor%3A%20Haoran%20Wang%20and%20Zhenning%20Li%20and%20Arno%20Eichberger%20and%20Jia%20Hu%0AAbstract%3A%20%20%20Cooperative%20Adaptive%20Cruise%20Control%20%28CACC%29%20often%20requires%20human%20takeover%20for%0Atasks%20such%20as%20exiting%20a%20freeway.%20Direct%20human%20takeover%20can%20pose%20significant%0Arisks%2C%20especially%20given%20the%20close-following%20strategy%20employed%20by%20CACC%2C%20which%0Amight%20cause%20drivers%20to%20feel%20unsafe%20and%20execute%20hard%20braking%2C%20potentially%0Aleading%20to%20collisions.%20This%20research%20aims%20to%20develop%20a%20CACC%20takeover%20controller%0Athat%20ensures%20a%20smooth%20transition%20from%20automated%20to%20human%20control.%20The%20proposed%0ACACC%20takeover%20maneuver%20employs%20an%20indirect%20human-machine%20shared%20control%0Aapproach%2C%20modeled%20as%20a%20Stackelberg%20competition%20where%20the%20machine%20acts%20as%20the%0Aleader%20and%20the%20human%20as%20the%20follower.%20The%20machine%20guides%20the%20human%20to%20respond%0Ain%20a%20manner%20that%20aligns%20with%20the%20machine%27s%20expectations%2C%20aiding%20in%20maintaining%0Afollowing%20stability.%20Additionally%2C%20the%20human%20reaction%20function%20is%20integrated%0Ainto%20the%20machine%27s%20predictive%20control%20system%2C%20moving%20beyond%20a%20simple%0A%22prediction-planning%22%20pipeline%20to%20enhance%20planning%20optimality.%20The%20controller%0Ahas%20been%20verified%20to%20i%29%20enable%20a%20smooth%20takeover%20maneuver%20of%20CACC%3B%20ii%29%20ensure%0Astring%20stability%20within%20a%20specific%20Operational%20Design%20Domain%20%28ODD%29%20when%20human%0Acontrol%20authority%20is%20below%2032.7%25%3B%20iii%29%20enhance%20both%20perceived%20and%20actual%20safety%0Athrough%20machine%20interventions%3B%20and%20iv%29%20reduce%20the%20impact%20on%20upstream%20traffic%20by%0Aup%20to%2060%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Machine%2520Shared%2520Control%2520Approach%2520for%2520the%2520Takeover%2520of%2520Cooperative%250A%2520%2520Adaptive%2520Cruise%2520Control%26entry.906535625%3DHaoran%2520Wang%2520and%2520Zhenning%2520Li%2520and%2520Arno%2520Eichberger%2520and%2520Jia%2520Hu%26entry.1292438233%3D%2520%2520Cooperative%2520Adaptive%2520Cruise%2520Control%2520%2528CACC%2529%2520often%2520requires%2520human%2520takeover%2520for%250Atasks%2520such%2520as%2520exiting%2520a%2520freeway.%2520Direct%2520human%2520takeover%2520can%2520pose%2520significant%250Arisks%252C%2520especially%2520given%2520the%2520close-following%2520strategy%2520employed%2520by%2520CACC%252C%2520which%250Amight%2520cause%2520drivers%2520to%2520feel%2520unsafe%2520and%2520execute%2520hard%2520braking%252C%2520potentially%250Aleading%2520to%2520collisions.%2520This%2520research%2520aims%2520to%2520develop%2520a%2520CACC%2520takeover%2520controller%250Athat%2520ensures%2520a%2520smooth%2520transition%2520from%2520automated%2520to%2520human%2520control.%2520The%2520proposed%250ACACC%2520takeover%2520maneuver%2520employs%2520an%2520indirect%2520human-machine%2520shared%2520control%250Aapproach%252C%2520modeled%2520as%2520a%2520Stackelberg%2520competition%2520where%2520the%2520machine%2520acts%2520as%2520the%250Aleader%2520and%2520the%2520human%2520as%2520the%2520follower.%2520The%2520machine%2520guides%2520the%2520human%2520to%2520respond%250Ain%2520a%2520manner%2520that%2520aligns%2520with%2520the%2520machine%2527s%2520expectations%252C%2520aiding%2520in%2520maintaining%250Afollowing%2520stability.%2520Additionally%252C%2520the%2520human%2520reaction%2520function%2520is%2520integrated%250Ainto%2520the%2520machine%2527s%2520predictive%2520control%2520system%252C%2520moving%2520beyond%2520a%2520simple%250A%2522prediction-planning%2522%2520pipeline%2520to%2520enhance%2520planning%2520optimality.%2520The%2520controller%250Ahas%2520been%2520verified%2520to%2520i%2529%2520enable%2520a%2520smooth%2520takeover%2520maneuver%2520of%2520CACC%253B%2520ii%2529%2520ensure%250Astring%2520stability%2520within%2520a%2520specific%2520Operational%2520Design%2520Domain%2520%2528ODD%2529%2520when%2520human%250Acontrol%2520authority%2520is%2520below%252032.7%2525%253B%2520iii%2529%2520enhance%2520both%2520perceived%2520and%2520actual%2520safety%250Athrough%2520machine%2520interventions%253B%2520and%2520iv%2529%2520reduce%2520the%2520impact%2520on%2520upstream%2520traffic%2520by%250Aup%2520to%252060%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Machine%20Shared%20Control%20Approach%20for%20the%20Takeover%20of%20Cooperative%0A%20%20Adaptive%20Cruise%20Control&entry.906535625=Haoran%20Wang%20and%20Zhenning%20Li%20and%20Arno%20Eichberger%20and%20Jia%20Hu&entry.1292438233=%20%20Cooperative%20Adaptive%20Cruise%20Control%20%28CACC%29%20often%20requires%20human%20takeover%20for%0Atasks%20such%20as%20exiting%20a%20freeway.%20Direct%20human%20takeover%20can%20pose%20significant%0Arisks%2C%20especially%20given%20the%20close-following%20strategy%20employed%20by%20CACC%2C%20which%0Amight%20cause%20drivers%20to%20feel%20unsafe%20and%20execute%20hard%20braking%2C%20potentially%0Aleading%20to%20collisions.%20This%20research%20aims%20to%20develop%20a%20CACC%20takeover%20controller%0Athat%20ensures%20a%20smooth%20transition%20from%20automated%20to%20human%20control.%20The%20proposed%0ACACC%20takeover%20maneuver%20employs%20an%20indirect%20human-machine%20shared%20control%0Aapproach%2C%20modeled%20as%20a%20Stackelberg%20competition%20where%20the%20machine%20acts%20as%20the%0Aleader%20and%20the%20human%20as%20the%20follower.%20The%20machine%20guides%20the%20human%20to%20respond%0Ain%20a%20manner%20that%20aligns%20with%20the%20machine%27s%20expectations%2C%20aiding%20in%20maintaining%0Afollowing%20stability.%20Additionally%2C%20the%20human%20reaction%20function%20is%20integrated%0Ainto%20the%20machine%27s%20predictive%20control%20system%2C%20moving%20beyond%20a%20simple%0A%22prediction-planning%22%20pipeline%20to%20enhance%20planning%20optimality.%20The%20controller%0Ahas%20been%20verified%20to%20i%29%20enable%20a%20smooth%20takeover%20maneuver%20of%20CACC%3B%20ii%29%20ensure%0Astring%20stability%20within%20a%20specific%20Operational%20Design%20Domain%20%28ODD%29%20when%20human%0Acontrol%20authority%20is%20below%2032.7%25%3B%20iii%29%20enhance%20both%20perceived%20and%20actual%20safety%0Athrough%20machine%20interventions%3B%20and%20iv%29%20reduce%20the%20impact%20on%20upstream%20traffic%20by%0Aup%20to%2060%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11551v1&entry.124074799=Read"},
{"title": "OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces", "author": "Zehan Wang and Ziang Zhang and Hang Zhang and Luping Liu and Rongjie Huang and Xize Cheng and Hengshuang Zhao and Zhou Zhao", "abstract": "  Recently, human-computer interaction with various modalities has shown\npromising applications, like GPT-4o and Gemini. Given the foundational role of\nmultimodal joint representation in understanding and generation pipelines,\nhigh-quality omni joint representations would be a step toward co-processing\nmore diverse multimodal information. In this work, we present OmniBind,\nlarge-scale multimodal joint representation models ranging in scale from 7\nbillion to 30 billion parameters, which support 3D, audio, image, and language\ninputs. Due to the scarcity of data pairs across all modalities, instead of\ntraining large models from scratch, we propose remapping and binding the spaces\nof various pre-trained specialist models together. This approach enables\n\"scaling up\" by indirectly increasing the model parameters and the amount of\nseen data. To effectively integrate various spaces, we dynamically assign\nweights to different spaces by learning routers with two objectives:\ncross-modal overall alignment and language representation decoupling. Notably,\nsince binding and routing spaces both only require lightweight networks,\nOmniBind is extremely training-efficient. Learning the largest 30B model\nrequires merely unpaired unimodal data and approximately 3 days on a single\n8-4090 node. Extensive experiments demonstrate the versatility and superiority\nof OmniBind as an omni representation model, highlighting its great potential\nfor diverse applications, such as any-query and composable multimodal\nunderstanding.\n", "link": "http://arxiv.org/abs/2407.11895v1", "date": "2024-07-16", "relevancy": 2.043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniBind%3A%20Large-scale%20Omni%20Multimodal%20Representation%20via%20Binding%20Spaces&body=Title%3A%20OmniBind%3A%20Large-scale%20Omni%20Multimodal%20Representation%20via%20Binding%20Spaces%0AAuthor%3A%20Zehan%20Wang%20and%20Ziang%20Zhang%20and%20Hang%20Zhang%20and%20Luping%20Liu%20and%20Rongjie%20Huang%20and%20Xize%20Cheng%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Recently%2C%20human-computer%20interaction%20with%20various%20modalities%20has%20shown%0Apromising%20applications%2C%20like%20GPT-4o%20and%20Gemini.%20Given%20the%20foundational%20role%20of%0Amultimodal%20joint%20representation%20in%20understanding%20and%20generation%20pipelines%2C%0Ahigh-quality%20omni%20joint%20representations%20would%20be%20a%20step%20toward%20co-processing%0Amore%20diverse%20multimodal%20information.%20In%20this%20work%2C%20we%20present%20OmniBind%2C%0Alarge-scale%20multimodal%20joint%20representation%20models%20ranging%20in%20scale%20from%207%0Abillion%20to%2030%20billion%20parameters%2C%20which%20support%203D%2C%20audio%2C%20image%2C%20and%20language%0Ainputs.%20Due%20to%20the%20scarcity%20of%20data%20pairs%20across%20all%20modalities%2C%20instead%20of%0Atraining%20large%20models%20from%20scratch%2C%20we%20propose%20remapping%20and%20binding%20the%20spaces%0Aof%20various%20pre-trained%20specialist%20models%20together.%20This%20approach%20enables%0A%22scaling%20up%22%20by%20indirectly%20increasing%20the%20model%20parameters%20and%20the%20amount%20of%0Aseen%20data.%20To%20effectively%20integrate%20various%20spaces%2C%20we%20dynamically%20assign%0Aweights%20to%20different%20spaces%20by%20learning%20routers%20with%20two%20objectives%3A%0Across-modal%20overall%20alignment%20and%20language%20representation%20decoupling.%20Notably%2C%0Asince%20binding%20and%20routing%20spaces%20both%20only%20require%20lightweight%20networks%2C%0AOmniBind%20is%20extremely%20training-efficient.%20Learning%20the%20largest%2030B%20model%0Arequires%20merely%20unpaired%20unimodal%20data%20and%20approximately%203%20days%20on%20a%20single%0A8-4090%20node.%20Extensive%20experiments%20demonstrate%20the%20versatility%20and%20superiority%0Aof%20OmniBind%20as%20an%20omni%20representation%20model%2C%20highlighting%20its%20great%20potential%0Afor%20diverse%20applications%2C%20such%20as%20any-query%20and%20composable%20multimodal%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniBind%253A%2520Large-scale%2520Omni%2520Multimodal%2520Representation%2520via%2520Binding%2520Spaces%26entry.906535625%3DZehan%2520Wang%2520and%2520Ziang%2520Zhang%2520and%2520Hang%2520Zhang%2520and%2520Luping%2520Liu%2520and%2520Rongjie%2520Huang%2520and%2520Xize%2520Cheng%2520and%2520Hengshuang%2520Zhao%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Recently%252C%2520human-computer%2520interaction%2520with%2520various%2520modalities%2520has%2520shown%250Apromising%2520applications%252C%2520like%2520GPT-4o%2520and%2520Gemini.%2520Given%2520the%2520foundational%2520role%2520of%250Amultimodal%2520joint%2520representation%2520in%2520understanding%2520and%2520generation%2520pipelines%252C%250Ahigh-quality%2520omni%2520joint%2520representations%2520would%2520be%2520a%2520step%2520toward%2520co-processing%250Amore%2520diverse%2520multimodal%2520information.%2520In%2520this%2520work%252C%2520we%2520present%2520OmniBind%252C%250Alarge-scale%2520multimodal%2520joint%2520representation%2520models%2520ranging%2520in%2520scale%2520from%25207%250Abillion%2520to%252030%2520billion%2520parameters%252C%2520which%2520support%25203D%252C%2520audio%252C%2520image%252C%2520and%2520language%250Ainputs.%2520Due%2520to%2520the%2520scarcity%2520of%2520data%2520pairs%2520across%2520all%2520modalities%252C%2520instead%2520of%250Atraining%2520large%2520models%2520from%2520scratch%252C%2520we%2520propose%2520remapping%2520and%2520binding%2520the%2520spaces%250Aof%2520various%2520pre-trained%2520specialist%2520models%2520together.%2520This%2520approach%2520enables%250A%2522scaling%2520up%2522%2520by%2520indirectly%2520increasing%2520the%2520model%2520parameters%2520and%2520the%2520amount%2520of%250Aseen%2520data.%2520To%2520effectively%2520integrate%2520various%2520spaces%252C%2520we%2520dynamically%2520assign%250Aweights%2520to%2520different%2520spaces%2520by%2520learning%2520routers%2520with%2520two%2520objectives%253A%250Across-modal%2520overall%2520alignment%2520and%2520language%2520representation%2520decoupling.%2520Notably%252C%250Asince%2520binding%2520and%2520routing%2520spaces%2520both%2520only%2520require%2520lightweight%2520networks%252C%250AOmniBind%2520is%2520extremely%2520training-efficient.%2520Learning%2520the%2520largest%252030B%2520model%250Arequires%2520merely%2520unpaired%2520unimodal%2520data%2520and%2520approximately%25203%2520days%2520on%2520a%2520single%250A8-4090%2520node.%2520Extensive%2520experiments%2520demonstrate%2520the%2520versatility%2520and%2520superiority%250Aof%2520OmniBind%2520as%2520an%2520omni%2520representation%2520model%252C%2520highlighting%2520its%2520great%2520potential%250Afor%2520diverse%2520applications%252C%2520such%2520as%2520any-query%2520and%2520composable%2520multimodal%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniBind%3A%20Large-scale%20Omni%20Multimodal%20Representation%20via%20Binding%20Spaces&entry.906535625=Zehan%20Wang%20and%20Ziang%20Zhang%20and%20Hang%20Zhang%20and%20Luping%20Liu%20and%20Rongjie%20Huang%20and%20Xize%20Cheng%20and%20Hengshuang%20Zhao%20and%20Zhou%20Zhao&entry.1292438233=%20%20Recently%2C%20human-computer%20interaction%20with%20various%20modalities%20has%20shown%0Apromising%20applications%2C%20like%20GPT-4o%20and%20Gemini.%20Given%20the%20foundational%20role%20of%0Amultimodal%20joint%20representation%20in%20understanding%20and%20generation%20pipelines%2C%0Ahigh-quality%20omni%20joint%20representations%20would%20be%20a%20step%20toward%20co-processing%0Amore%20diverse%20multimodal%20information.%20In%20this%20work%2C%20we%20present%20OmniBind%2C%0Alarge-scale%20multimodal%20joint%20representation%20models%20ranging%20in%20scale%20from%207%0Abillion%20to%2030%20billion%20parameters%2C%20which%20support%203D%2C%20audio%2C%20image%2C%20and%20language%0Ainputs.%20Due%20to%20the%20scarcity%20of%20data%20pairs%20across%20all%20modalities%2C%20instead%20of%0Atraining%20large%20models%20from%20scratch%2C%20we%20propose%20remapping%20and%20binding%20the%20spaces%0Aof%20various%20pre-trained%20specialist%20models%20together.%20This%20approach%20enables%0A%22scaling%20up%22%20by%20indirectly%20increasing%20the%20model%20parameters%20and%20the%20amount%20of%0Aseen%20data.%20To%20effectively%20integrate%20various%20spaces%2C%20we%20dynamically%20assign%0Aweights%20to%20different%20spaces%20by%20learning%20routers%20with%20two%20objectives%3A%0Across-modal%20overall%20alignment%20and%20language%20representation%20decoupling.%20Notably%2C%0Asince%20binding%20and%20routing%20spaces%20both%20only%20require%20lightweight%20networks%2C%0AOmniBind%20is%20extremely%20training-efficient.%20Learning%20the%20largest%2030B%20model%0Arequires%20merely%20unpaired%20unimodal%20data%20and%20approximately%203%20days%20on%20a%20single%0A8-4090%20node.%20Extensive%20experiments%20demonstrate%20the%20versatility%20and%20superiority%0Aof%20OmniBind%20as%20an%20omni%20representation%20model%2C%20highlighting%20its%20great%20potential%0Afor%20diverse%20applications%2C%20such%20as%20any-query%20and%20composable%20multimodal%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11895v1&entry.124074799=Read"},
{"title": "Improving Explainability of Softmax Classifiers Using a Prototype-Based\n  Joint Embedding Method", "author": "Hilarie Sit and Brendan Keith and Karianne Bergen", "abstract": "  We propose a prototype-based approach for improving explainability of softmax\nclassifiers that provides an understandable prediction confidence, generated\nthrough stochastic sampling of prototypes, and demonstrates potential for out\nof distribution detection (OOD). By modifying the model architecture and\ntraining to make predictions using similarities to any set of class examples\nfrom the training dataset, we acquire the ability to sample for prototypical\nexamples that contributed to the prediction, which provide an instance-based\nexplanation for the model's decision. Furthermore, by learning relationships\nbetween images from the training dataset through relative distances within the\nmodel's latent space, we obtain a metric for uncertainty that is better able to\ndetect out of distribution data than softmax confidence.\n", "link": "http://arxiv.org/abs/2407.02271v2", "date": "2024-07-16", "relevancy": 2.0394, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5088}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method&body=Title%3A%20Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method%0AAuthor%3A%20Hilarie%20Sit%20and%20Brendan%20Keith%20and%20Karianne%20Bergen%0AAbstract%3A%20%20%20We%20propose%20a%20prototype-based%20approach%20for%20improving%20explainability%20of%20softmax%0Aclassifiers%20that%20provides%20an%20understandable%20prediction%20confidence%2C%20generated%0Athrough%20stochastic%20sampling%20of%20prototypes%2C%20and%20demonstrates%20potential%20for%20out%0Aof%20distribution%20detection%20%28OOD%29.%20By%20modifying%20the%20model%20architecture%20and%0Atraining%20to%20make%20predictions%20using%20similarities%20to%20any%20set%20of%20class%20examples%0Afrom%20the%20training%20dataset%2C%20we%20acquire%20the%20ability%20to%20sample%20for%20prototypical%0Aexamples%20that%20contributed%20to%20the%20prediction%2C%20which%20provide%20an%20instance-based%0Aexplanation%20for%20the%20model%27s%20decision.%20Furthermore%2C%20by%20learning%20relationships%0Abetween%20images%20from%20the%20training%20dataset%20through%20relative%20distances%20within%20the%0Amodel%27s%20latent%20space%2C%20we%20obtain%20a%20metric%20for%20uncertainty%20that%20is%20better%20able%20to%0Adetect%20out%20of%20distribution%20data%20than%20softmax%20confidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Explainability%2520of%2520Softmax%2520Classifiers%2520Using%2520a%2520Prototype-Based%250A%2520%2520Joint%2520Embedding%2520Method%26entry.906535625%3DHilarie%2520Sit%2520and%2520Brendan%2520Keith%2520and%2520Karianne%2520Bergen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520prototype-based%2520approach%2520for%2520improving%2520explainability%2520of%2520softmax%250Aclassifiers%2520that%2520provides%2520an%2520understandable%2520prediction%2520confidence%252C%2520generated%250Athrough%2520stochastic%2520sampling%2520of%2520prototypes%252C%2520and%2520demonstrates%2520potential%2520for%2520out%250Aof%2520distribution%2520detection%2520%2528OOD%2529.%2520By%2520modifying%2520the%2520model%2520architecture%2520and%250Atraining%2520to%2520make%2520predictions%2520using%2520similarities%2520to%2520any%2520set%2520of%2520class%2520examples%250Afrom%2520the%2520training%2520dataset%252C%2520we%2520acquire%2520the%2520ability%2520to%2520sample%2520for%2520prototypical%250Aexamples%2520that%2520contributed%2520to%2520the%2520prediction%252C%2520which%2520provide%2520an%2520instance-based%250Aexplanation%2520for%2520the%2520model%2527s%2520decision.%2520Furthermore%252C%2520by%2520learning%2520relationships%250Abetween%2520images%2520from%2520the%2520training%2520dataset%2520through%2520relative%2520distances%2520within%2520the%250Amodel%2527s%2520latent%2520space%252C%2520we%2520obtain%2520a%2520metric%2520for%2520uncertainty%2520that%2520is%2520better%2520able%2520to%250Adetect%2520out%2520of%2520distribution%2520data%2520than%2520softmax%2520confidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method&entry.906535625=Hilarie%20Sit%20and%20Brendan%20Keith%20and%20Karianne%20Bergen&entry.1292438233=%20%20We%20propose%20a%20prototype-based%20approach%20for%20improving%20explainability%20of%20softmax%0Aclassifiers%20that%20provides%20an%20understandable%20prediction%20confidence%2C%20generated%0Athrough%20stochastic%20sampling%20of%20prototypes%2C%20and%20demonstrates%20potential%20for%20out%0Aof%20distribution%20detection%20%28OOD%29.%20By%20modifying%20the%20model%20architecture%20and%0Atraining%20to%20make%20predictions%20using%20similarities%20to%20any%20set%20of%20class%20examples%0Afrom%20the%20training%20dataset%2C%20we%20acquire%20the%20ability%20to%20sample%20for%20prototypical%0Aexamples%20that%20contributed%20to%20the%20prediction%2C%20which%20provide%20an%20instance-based%0Aexplanation%20for%20the%20model%27s%20decision.%20Furthermore%2C%20by%20learning%20relationships%0Abetween%20images%20from%20the%20training%20dataset%20through%20relative%20distances%20within%20the%0Amodel%27s%20latent%20space%2C%20we%20obtain%20a%20metric%20for%20uncertainty%20that%20is%20better%20able%20to%0Adetect%20out%20of%20distribution%20data%20than%20softmax%20confidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02271v2&entry.124074799=Read"},
{"title": "CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging", "author": "Sunny Gupta and Amit Sethi", "abstract": "  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n", "link": "http://arxiv.org/abs/2407.11652v1", "date": "2024-07-16", "relevancy": 2.0364, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5009}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging&body=Title%3A%20CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging%0AAuthor%3A%20Sunny%20Gupta%20and%20Amit%20Sethi%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20privacy-preserving%20approach%20to%20train%20models%0Aon%20decentralized%20data.%20Its%20potential%20in%20healthcare%20is%20significant%2C%20but%0Achallenges%20arise%20due%20to%20cross-client%20variations%20in%20medical%20image%20data%2C%0Aexacerbated%20by%20limited%20annotations.%20This%20paper%20introduces%20Cross-Client%0AVariations%20Adaptive%20Federated%20Learning%20%28CCVA-FL%29%20to%20address%20these%20issues.%0ACCVA-FL%20aims%20to%20minimize%20cross-client%20variations%20by%20transforming%20images%20into%20a%0Acommon%20feature%20space.%20It%20involves%20expert%20annotation%20of%20a%20subset%20of%20images%20from%0Aeach%20client%2C%20followed%20by%20the%20selection%20of%20a%20client%20with%20the%20least%20data%0Acomplexity%20as%20the%20target.%20Synthetic%20medical%20images%20are%20then%20generated%20using%0AScalable%20Diffusion%20Models%20with%20Transformers%20%28DiT%29%20based%20on%20the%20target%20client%27s%0Aannotated%20images.%20These%20synthetic%20images%2C%20capturing%20diversity%20and%20representing%0Athe%20original%20data%2C%20are%20shared%20with%20other%20clients.%20Each%20client%20then%20translates%0Aits%20local%20images%20into%20the%20target%20image%20space%20using%20image-to-image%20translation.%0AThe%20translated%20images%20are%20subsequently%20used%20in%20a%20federated%20learning%20setting%20to%0Adevelop%20a%20server%20model.%20Our%20results%20demonstrate%20that%20CCVA-FL%20outperforms%0AVanilla%20Federated%20Averaging%20by%20effectively%20addressing%20data%20distribution%0Adifferences%20across%20clients%20without%20compromising%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCVA-FL%253A%2520Cross-Client%2520Variations%2520Adaptive%2520Federated%2520Learning%2520for%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DSunny%2520Gupta%2520and%2520Amit%2520Sethi%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520privacy-preserving%2520approach%2520to%2520train%2520models%250Aon%2520decentralized%2520data.%2520Its%2520potential%2520in%2520healthcare%2520is%2520significant%252C%2520but%250Achallenges%2520arise%2520due%2520to%2520cross-client%2520variations%2520in%2520medical%2520image%2520data%252C%250Aexacerbated%2520by%2520limited%2520annotations.%2520This%2520paper%2520introduces%2520Cross-Client%250AVariations%2520Adaptive%2520Federated%2520Learning%2520%2528CCVA-FL%2529%2520to%2520address%2520these%2520issues.%250ACCVA-FL%2520aims%2520to%2520minimize%2520cross-client%2520variations%2520by%2520transforming%2520images%2520into%2520a%250Acommon%2520feature%2520space.%2520It%2520involves%2520expert%2520annotation%2520of%2520a%2520subset%2520of%2520images%2520from%250Aeach%2520client%252C%2520followed%2520by%2520the%2520selection%2520of%2520a%2520client%2520with%2520the%2520least%2520data%250Acomplexity%2520as%2520the%2520target.%2520Synthetic%2520medical%2520images%2520are%2520then%2520generated%2520using%250AScalable%2520Diffusion%2520Models%2520with%2520Transformers%2520%2528DiT%2529%2520based%2520on%2520the%2520target%2520client%2527s%250Aannotated%2520images.%2520These%2520synthetic%2520images%252C%2520capturing%2520diversity%2520and%2520representing%250Athe%2520original%2520data%252C%2520are%2520shared%2520with%2520other%2520clients.%2520Each%2520client%2520then%2520translates%250Aits%2520local%2520images%2520into%2520the%2520target%2520image%2520space%2520using%2520image-to-image%2520translation.%250AThe%2520translated%2520images%2520are%2520subsequently%2520used%2520in%2520a%2520federated%2520learning%2520setting%2520to%250Adevelop%2520a%2520server%2520model.%2520Our%2520results%2520demonstrate%2520that%2520CCVA-FL%2520outperforms%250AVanilla%2520Federated%2520Averaging%2520by%2520effectively%2520addressing%2520data%2520distribution%250Adifferences%2520across%2520clients%2520without%2520compromising%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging&entry.906535625=Sunny%20Gupta%20and%20Amit%20Sethi&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20privacy-preserving%20approach%20to%20train%20models%0Aon%20decentralized%20data.%20Its%20potential%20in%20healthcare%20is%20significant%2C%20but%0Achallenges%20arise%20due%20to%20cross-client%20variations%20in%20medical%20image%20data%2C%0Aexacerbated%20by%20limited%20annotations.%20This%20paper%20introduces%20Cross-Client%0AVariations%20Adaptive%20Federated%20Learning%20%28CCVA-FL%29%20to%20address%20these%20issues.%0ACCVA-FL%20aims%20to%20minimize%20cross-client%20variations%20by%20transforming%20images%20into%20a%0Acommon%20feature%20space.%20It%20involves%20expert%20annotation%20of%20a%20subset%20of%20images%20from%0Aeach%20client%2C%20followed%20by%20the%20selection%20of%20a%20client%20with%20the%20least%20data%0Acomplexity%20as%20the%20target.%20Synthetic%20medical%20images%20are%20then%20generated%20using%0AScalable%20Diffusion%20Models%20with%20Transformers%20%28DiT%29%20based%20on%20the%20target%20client%27s%0Aannotated%20images.%20These%20synthetic%20images%2C%20capturing%20diversity%20and%20representing%0Athe%20original%20data%2C%20are%20shared%20with%20other%20clients.%20Each%20client%20then%20translates%0Aits%20local%20images%20into%20the%20target%20image%20space%20using%20image-to-image%20translation.%0AThe%20translated%20images%20are%20subsequently%20used%20in%20a%20federated%20learning%20setting%20to%0Adevelop%20a%20server%20model.%20Our%20results%20demonstrate%20that%20CCVA-FL%20outperforms%0AVanilla%20Federated%20Averaging%20by%20effectively%20addressing%20data%20distribution%0Adifferences%20across%20clients%20without%20compromising%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11652v1&entry.124074799=Read"},
{"title": "Image Clustering with External Guidance", "author": "Yunfan Li and Peng Hu and Dezhong Peng and Jiancheng Lv and Jianping Fan and Xi Peng", "abstract": "  The core of clustering is incorporating prior knowledge to construct\nsupervision signals. From classic k-means based on data compactness to recent\ncontrastive clustering guided by self-supervision, the evolution of clustering\nmethods intrinsically corresponds to the progression of supervision signals. At\npresent, substantial efforts have been devoted to mining internal supervision\nsignals from data. Nevertheless, the abundant external knowledge such as\nsemantic descriptions, which naturally conduces to clustering, is regrettably\noverlooked. In this work, we propose leveraging external knowledge as a new\nsupervision signal to guide clustering, even though it seems irrelevant to the\ngiven data. To implement and validate our idea, we design an externally guided\nclustering method (Text-Aided Clustering, TAC), which leverages the textual\nsemantics of WordNet to facilitate image clustering. Specifically, TAC first\nselects and retrieves WordNet nouns that best distinguish images to enhance the\nfeature discriminability. Then, to improve image clustering performance, TAC\ncollaborates text and image modalities by mutually distilling cross-modal\nneighborhood information. Experiments demonstrate that TAC achieves\nstate-of-the-art performance on five widely used and three more challenging\nimage clustering benchmarks, including the full ImageNet-1K dataset.\n", "link": "http://arxiv.org/abs/2310.11989v3", "date": "2024-07-16", "relevancy": 2.0344, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4987}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Clustering%20with%20External%20Guidance&body=Title%3A%20Image%20Clustering%20with%20External%20Guidance%0AAuthor%3A%20Yunfan%20Li%20and%20Peng%20Hu%20and%20Dezhong%20Peng%20and%20Jiancheng%20Lv%20and%20Jianping%20Fan%20and%20Xi%20Peng%0AAbstract%3A%20%20%20The%20core%20of%20clustering%20is%20incorporating%20prior%20knowledge%20to%20construct%0Asupervision%20signals.%20From%20classic%20k-means%20based%20on%20data%20compactness%20to%20recent%0Acontrastive%20clustering%20guided%20by%20self-supervision%2C%20the%20evolution%20of%20clustering%0Amethods%20intrinsically%20corresponds%20to%20the%20progression%20of%20supervision%20signals.%20At%0Apresent%2C%20substantial%20efforts%20have%20been%20devoted%20to%20mining%20internal%20supervision%0Asignals%20from%20data.%20Nevertheless%2C%20the%20abundant%20external%20knowledge%20such%20as%0Asemantic%20descriptions%2C%20which%20naturally%20conduces%20to%20clustering%2C%20is%20regrettably%0Aoverlooked.%20In%20this%20work%2C%20we%20propose%20leveraging%20external%20knowledge%20as%20a%20new%0Asupervision%20signal%20to%20guide%20clustering%2C%20even%20though%20it%20seems%20irrelevant%20to%20the%0Agiven%20data.%20To%20implement%20and%20validate%20our%20idea%2C%20we%20design%20an%20externally%20guided%0Aclustering%20method%20%28Text-Aided%20Clustering%2C%20TAC%29%2C%20which%20leverages%20the%20textual%0Asemantics%20of%20WordNet%20to%20facilitate%20image%20clustering.%20Specifically%2C%20TAC%20first%0Aselects%20and%20retrieves%20WordNet%20nouns%20that%20best%20distinguish%20images%20to%20enhance%20the%0Afeature%20discriminability.%20Then%2C%20to%20improve%20image%20clustering%20performance%2C%20TAC%0Acollaborates%20text%20and%20image%20modalities%20by%20mutually%20distilling%20cross-modal%0Aneighborhood%20information.%20Experiments%20demonstrate%20that%20TAC%20achieves%0Astate-of-the-art%20performance%20on%20five%20widely%20used%20and%20three%20more%20challenging%0Aimage%20clustering%20benchmarks%2C%20including%20the%20full%20ImageNet-1K%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Clustering%2520with%2520External%2520Guidance%26entry.906535625%3DYunfan%2520Li%2520and%2520Peng%2520Hu%2520and%2520Dezhong%2520Peng%2520and%2520Jiancheng%2520Lv%2520and%2520Jianping%2520Fan%2520and%2520Xi%2520Peng%26entry.1292438233%3D%2520%2520The%2520core%2520of%2520clustering%2520is%2520incorporating%2520prior%2520knowledge%2520to%2520construct%250Asupervision%2520signals.%2520From%2520classic%2520k-means%2520based%2520on%2520data%2520compactness%2520to%2520recent%250Acontrastive%2520clustering%2520guided%2520by%2520self-supervision%252C%2520the%2520evolution%2520of%2520clustering%250Amethods%2520intrinsically%2520corresponds%2520to%2520the%2520progression%2520of%2520supervision%2520signals.%2520At%250Apresent%252C%2520substantial%2520efforts%2520have%2520been%2520devoted%2520to%2520mining%2520internal%2520supervision%250Asignals%2520from%2520data.%2520Nevertheless%252C%2520the%2520abundant%2520external%2520knowledge%2520such%2520as%250Asemantic%2520descriptions%252C%2520which%2520naturally%2520conduces%2520to%2520clustering%252C%2520is%2520regrettably%250Aoverlooked.%2520In%2520this%2520work%252C%2520we%2520propose%2520leveraging%2520external%2520knowledge%2520as%2520a%2520new%250Asupervision%2520signal%2520to%2520guide%2520clustering%252C%2520even%2520though%2520it%2520seems%2520irrelevant%2520to%2520the%250Agiven%2520data.%2520To%2520implement%2520and%2520validate%2520our%2520idea%252C%2520we%2520design%2520an%2520externally%2520guided%250Aclustering%2520method%2520%2528Text-Aided%2520Clustering%252C%2520TAC%2529%252C%2520which%2520leverages%2520the%2520textual%250Asemantics%2520of%2520WordNet%2520to%2520facilitate%2520image%2520clustering.%2520Specifically%252C%2520TAC%2520first%250Aselects%2520and%2520retrieves%2520WordNet%2520nouns%2520that%2520best%2520distinguish%2520images%2520to%2520enhance%2520the%250Afeature%2520discriminability.%2520Then%252C%2520to%2520improve%2520image%2520clustering%2520performance%252C%2520TAC%250Acollaborates%2520text%2520and%2520image%2520modalities%2520by%2520mutually%2520distilling%2520cross-modal%250Aneighborhood%2520information.%2520Experiments%2520demonstrate%2520that%2520TAC%2520achieves%250Astate-of-the-art%2520performance%2520on%2520five%2520widely%2520used%2520and%2520three%2520more%2520challenging%250Aimage%2520clustering%2520benchmarks%252C%2520including%2520the%2520full%2520ImageNet-1K%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Clustering%20with%20External%20Guidance&entry.906535625=Yunfan%20Li%20and%20Peng%20Hu%20and%20Dezhong%20Peng%20and%20Jiancheng%20Lv%20and%20Jianping%20Fan%20and%20Xi%20Peng&entry.1292438233=%20%20The%20core%20of%20clustering%20is%20incorporating%20prior%20knowledge%20to%20construct%0Asupervision%20signals.%20From%20classic%20k-means%20based%20on%20data%20compactness%20to%20recent%0Acontrastive%20clustering%20guided%20by%20self-supervision%2C%20the%20evolution%20of%20clustering%0Amethods%20intrinsically%20corresponds%20to%20the%20progression%20of%20supervision%20signals.%20At%0Apresent%2C%20substantial%20efforts%20have%20been%20devoted%20to%20mining%20internal%20supervision%0Asignals%20from%20data.%20Nevertheless%2C%20the%20abundant%20external%20knowledge%20such%20as%0Asemantic%20descriptions%2C%20which%20naturally%20conduces%20to%20clustering%2C%20is%20regrettably%0Aoverlooked.%20In%20this%20work%2C%20we%20propose%20leveraging%20external%20knowledge%20as%20a%20new%0Asupervision%20signal%20to%20guide%20clustering%2C%20even%20though%20it%20seems%20irrelevant%20to%20the%0Agiven%20data.%20To%20implement%20and%20validate%20our%20idea%2C%20we%20design%20an%20externally%20guided%0Aclustering%20method%20%28Text-Aided%20Clustering%2C%20TAC%29%2C%20which%20leverages%20the%20textual%0Asemantics%20of%20WordNet%20to%20facilitate%20image%20clustering.%20Specifically%2C%20TAC%20first%0Aselects%20and%20retrieves%20WordNet%20nouns%20that%20best%20distinguish%20images%20to%20enhance%20the%0Afeature%20discriminability.%20Then%2C%20to%20improve%20image%20clustering%20performance%2C%20TAC%0Acollaborates%20text%20and%20image%20modalities%20by%20mutually%20distilling%20cross-modal%0Aneighborhood%20information.%20Experiments%20demonstrate%20that%20TAC%20achieves%0Astate-of-the-art%20performance%20on%20five%20widely%20used%20and%20three%20more%20challenging%0Aimage%20clustering%20benchmarks%2C%20including%20the%20full%20ImageNet-1K%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11989v3&entry.124074799=Read"},
{"title": "Rethinking Fair Graph Neural Networks from Re-balancing", "author": "Zhixun Li and Yushun Dong and Qiang Liu and Jeffrey Xu Yu", "abstract": "  Driven by the powerful representation ability of Graph Neural Networks\n(GNNs), plentiful GNN models have been widely deployed in many real-world\napplications. Nevertheless, due to distribution disparities between different\ndemographic groups, fairness in high-stake decision-making systems is receiving\nincreasing attention. Although lots of recent works devoted to improving the\nfairness of GNNs and achieved considerable success, they all require\nsignificant architectural changes or additional loss functions requiring more\nhyper-parameter tuning. Surprisingly, we find that simple re-balancing methods\ncan easily match or surpass existing fair GNN methods. We claim that the\nimbalance across different demographic groups is a significant source of\nunfairness, resulting in imbalanced contributions from each group to the\nparameters updating. However, these simple re-balancing methods have their own\nshortcomings during training. In this paper, we propose FairGB, Fair Graph\nNeural Network via re-Balancing, which mitigates the unfairness of GNNs by\ngroup balancing. Technically, FairGB consists of two modules: counterfactual\nnode mixup and contribution alignment loss. Firstly, we select counterfactual\npairs across inter-domain and inter-class, and interpolate the ego-networks to\ngenerate new samples. Guided by analysis, we can reveal the debiasing mechanism\nof our model by the causal view and prove that our strategy can make sensitive\nattributes statistically independent from target labels. Secondly, we reweigh\nthe contribution of each group according to gradients. By combining these two\nmodules, they can mutually promote each other. Experimental results on\nbenchmark datasets show that our method can achieve state-of-the-art results\nconcerning both utility and fairness metrics. Code is available at\nhttps://github.com/ZhixunLEE/FairGB.\n", "link": "http://arxiv.org/abs/2407.11624v1", "date": "2024-07-16", "relevancy": 2.0318, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5306}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.494}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Fair%20Graph%20Neural%20Networks%20from%20Re-balancing&body=Title%3A%20Rethinking%20Fair%20Graph%20Neural%20Networks%20from%20Re-balancing%0AAuthor%3A%20Zhixun%20Li%20and%20Yushun%20Dong%20and%20Qiang%20Liu%20and%20Jeffrey%20Xu%20Yu%0AAbstract%3A%20%20%20Driven%20by%20the%20powerful%20representation%20ability%20of%20Graph%20Neural%20Networks%0A%28GNNs%29%2C%20plentiful%20GNN%20models%20have%20been%20widely%20deployed%20in%20many%20real-world%0Aapplications.%20Nevertheless%2C%20due%20to%20distribution%20disparities%20between%20different%0Ademographic%20groups%2C%20fairness%20in%20high-stake%20decision-making%20systems%20is%20receiving%0Aincreasing%20attention.%20Although%20lots%20of%20recent%20works%20devoted%20to%20improving%20the%0Afairness%20of%20GNNs%20and%20achieved%20considerable%20success%2C%20they%20all%20require%0Asignificant%20architectural%20changes%20or%20additional%20loss%20functions%20requiring%20more%0Ahyper-parameter%20tuning.%20Surprisingly%2C%20we%20find%20that%20simple%20re-balancing%20methods%0Acan%20easily%20match%20or%20surpass%20existing%20fair%20GNN%20methods.%20We%20claim%20that%20the%0Aimbalance%20across%20different%20demographic%20groups%20is%20a%20significant%20source%20of%0Aunfairness%2C%20resulting%20in%20imbalanced%20contributions%20from%20each%20group%20to%20the%0Aparameters%20updating.%20However%2C%20these%20simple%20re-balancing%20methods%20have%20their%20own%0Ashortcomings%20during%20training.%20In%20this%20paper%2C%20we%20propose%20FairGB%2C%20Fair%20Graph%0ANeural%20Network%20via%20re-Balancing%2C%20which%20mitigates%20the%20unfairness%20of%20GNNs%20by%0Agroup%20balancing.%20Technically%2C%20FairGB%20consists%20of%20two%20modules%3A%20counterfactual%0Anode%20mixup%20and%20contribution%20alignment%20loss.%20Firstly%2C%20we%20select%20counterfactual%0Apairs%20across%20inter-domain%20and%20inter-class%2C%20and%20interpolate%20the%20ego-networks%20to%0Agenerate%20new%20samples.%20Guided%20by%20analysis%2C%20we%20can%20reveal%20the%20debiasing%20mechanism%0Aof%20our%20model%20by%20the%20causal%20view%20and%20prove%20that%20our%20strategy%20can%20make%20sensitive%0Aattributes%20statistically%20independent%20from%20target%20labels.%20Secondly%2C%20we%20reweigh%0Athe%20contribution%20of%20each%20group%20according%20to%20gradients.%20By%20combining%20these%20two%0Amodules%2C%20they%20can%20mutually%20promote%20each%20other.%20Experimental%20results%20on%0Abenchmark%20datasets%20show%20that%20our%20method%20can%20achieve%20state-of-the-art%20results%0Aconcerning%20both%20utility%20and%20fairness%20metrics.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ZhixunLEE/FairGB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Fair%2520Graph%2520Neural%2520Networks%2520from%2520Re-balancing%26entry.906535625%3DZhixun%2520Li%2520and%2520Yushun%2520Dong%2520and%2520Qiang%2520Liu%2520and%2520Jeffrey%2520Xu%2520Yu%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520powerful%2520representation%2520ability%2520of%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%252C%2520plentiful%2520GNN%2520models%2520have%2520been%2520widely%2520deployed%2520in%2520many%2520real-world%250Aapplications.%2520Nevertheless%252C%2520due%2520to%2520distribution%2520disparities%2520between%2520different%250Ademographic%2520groups%252C%2520fairness%2520in%2520high-stake%2520decision-making%2520systems%2520is%2520receiving%250Aincreasing%2520attention.%2520Although%2520lots%2520of%2520recent%2520works%2520devoted%2520to%2520improving%2520the%250Afairness%2520of%2520GNNs%2520and%2520achieved%2520considerable%2520success%252C%2520they%2520all%2520require%250Asignificant%2520architectural%2520changes%2520or%2520additional%2520loss%2520functions%2520requiring%2520more%250Ahyper-parameter%2520tuning.%2520Surprisingly%252C%2520we%2520find%2520that%2520simple%2520re-balancing%2520methods%250Acan%2520easily%2520match%2520or%2520surpass%2520existing%2520fair%2520GNN%2520methods.%2520We%2520claim%2520that%2520the%250Aimbalance%2520across%2520different%2520demographic%2520groups%2520is%2520a%2520significant%2520source%2520of%250Aunfairness%252C%2520resulting%2520in%2520imbalanced%2520contributions%2520from%2520each%2520group%2520to%2520the%250Aparameters%2520updating.%2520However%252C%2520these%2520simple%2520re-balancing%2520methods%2520have%2520their%2520own%250Ashortcomings%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FairGB%252C%2520Fair%2520Graph%250ANeural%2520Network%2520via%2520re-Balancing%252C%2520which%2520mitigates%2520the%2520unfairness%2520of%2520GNNs%2520by%250Agroup%2520balancing.%2520Technically%252C%2520FairGB%2520consists%2520of%2520two%2520modules%253A%2520counterfactual%250Anode%2520mixup%2520and%2520contribution%2520alignment%2520loss.%2520Firstly%252C%2520we%2520select%2520counterfactual%250Apairs%2520across%2520inter-domain%2520and%2520inter-class%252C%2520and%2520interpolate%2520the%2520ego-networks%2520to%250Agenerate%2520new%2520samples.%2520Guided%2520by%2520analysis%252C%2520we%2520can%2520reveal%2520the%2520debiasing%2520mechanism%250Aof%2520our%2520model%2520by%2520the%2520causal%2520view%2520and%2520prove%2520that%2520our%2520strategy%2520can%2520make%2520sensitive%250Aattributes%2520statistically%2520independent%2520from%2520target%2520labels.%2520Secondly%252C%2520we%2520reweigh%250Athe%2520contribution%2520of%2520each%2520group%2520according%2520to%2520gradients.%2520By%2520combining%2520these%2520two%250Amodules%252C%2520they%2520can%2520mutually%2520promote%2520each%2520other.%2520Experimental%2520results%2520on%250Abenchmark%2520datasets%2520show%2520that%2520our%2520method%2520can%2520achieve%2520state-of-the-art%2520results%250Aconcerning%2520both%2520utility%2520and%2520fairness%2520metrics.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZhixunLEE/FairGB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Fair%20Graph%20Neural%20Networks%20from%20Re-balancing&entry.906535625=Zhixun%20Li%20and%20Yushun%20Dong%20and%20Qiang%20Liu%20and%20Jeffrey%20Xu%20Yu&entry.1292438233=%20%20Driven%20by%20the%20powerful%20representation%20ability%20of%20Graph%20Neural%20Networks%0A%28GNNs%29%2C%20plentiful%20GNN%20models%20have%20been%20widely%20deployed%20in%20many%20real-world%0Aapplications.%20Nevertheless%2C%20due%20to%20distribution%20disparities%20between%20different%0Ademographic%20groups%2C%20fairness%20in%20high-stake%20decision-making%20systems%20is%20receiving%0Aincreasing%20attention.%20Although%20lots%20of%20recent%20works%20devoted%20to%20improving%20the%0Afairness%20of%20GNNs%20and%20achieved%20considerable%20success%2C%20they%20all%20require%0Asignificant%20architectural%20changes%20or%20additional%20loss%20functions%20requiring%20more%0Ahyper-parameter%20tuning.%20Surprisingly%2C%20we%20find%20that%20simple%20re-balancing%20methods%0Acan%20easily%20match%20or%20surpass%20existing%20fair%20GNN%20methods.%20We%20claim%20that%20the%0Aimbalance%20across%20different%20demographic%20groups%20is%20a%20significant%20source%20of%0Aunfairness%2C%20resulting%20in%20imbalanced%20contributions%20from%20each%20group%20to%20the%0Aparameters%20updating.%20However%2C%20these%20simple%20re-balancing%20methods%20have%20their%20own%0Ashortcomings%20during%20training.%20In%20this%20paper%2C%20we%20propose%20FairGB%2C%20Fair%20Graph%0ANeural%20Network%20via%20re-Balancing%2C%20which%20mitigates%20the%20unfairness%20of%20GNNs%20by%0Agroup%20balancing.%20Technically%2C%20FairGB%20consists%20of%20two%20modules%3A%20counterfactual%0Anode%20mixup%20and%20contribution%20alignment%20loss.%20Firstly%2C%20we%20select%20counterfactual%0Apairs%20across%20inter-domain%20and%20inter-class%2C%20and%20interpolate%20the%20ego-networks%20to%0Agenerate%20new%20samples.%20Guided%20by%20analysis%2C%20we%20can%20reveal%20the%20debiasing%20mechanism%0Aof%20our%20model%20by%20the%20causal%20view%20and%20prove%20that%20our%20strategy%20can%20make%20sensitive%0Aattributes%20statistically%20independent%20from%20target%20labels.%20Secondly%2C%20we%20reweigh%0Athe%20contribution%20of%20each%20group%20according%20to%20gradients.%20By%20combining%20these%20two%0Amodules%2C%20they%20can%20mutually%20promote%20each%20other.%20Experimental%20results%20on%0Abenchmark%20datasets%20show%20that%20our%20method%20can%20achieve%20state-of-the-art%20results%0Aconcerning%20both%20utility%20and%20fairness%20metrics.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ZhixunLEE/FairGB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11624v1&entry.124074799=Read"},
{"title": "Statistics-aware Audio-visual Deepfake Detector", "author": "Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  In this paper, we propose an enhanced audio-visual deep detection method.\nRecent methods in audio-visual deepfake detection mostly assess the\nsynchronization between audio and visual features. Although they have shown\npromising results, they are based on the maximization/minimization of isolated\nfeature distances without considering feature statistics. Moreover, they rely\non cumbersome deep learning architectures and are heavily dependent on\nempirically fixed hyperparameters. Herein, to overcome these limitations, we\npropose: (1) a statistical feature loss to enhance the discrimination\ncapability of the model, instead of relying solely on feature distances; (2)\nusing the waveform for describing the audio as a replacement of frequency-based\nrepresentations; (3) a post-processing normalization of the fakeness score; (4)\nthe use of shallower network for reducing the computational complexity.\nExperiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of\nthe proposed method.\n", "link": "http://arxiv.org/abs/2407.11650v1", "date": "2024-07-16", "relevancy": 2.03, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5184}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4998}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistics-aware%20Audio-visual%20Deepfake%20Detector&body=Title%3A%20Statistics-aware%20Audio-visual%20Deepfake%20Detector%0AAuthor%3A%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20enhanced%20audio-visual%20deep%20detection%20method.%0ARecent%20methods%20in%20audio-visual%20deepfake%20detection%20mostly%20assess%20the%0Asynchronization%20between%20audio%20and%20visual%20features.%20Although%20they%20have%20shown%0Apromising%20results%2C%20they%20are%20based%20on%20the%20maximization/minimization%20of%20isolated%0Afeature%20distances%20without%20considering%20feature%20statistics.%20Moreover%2C%20they%20rely%0Aon%20cumbersome%20deep%20learning%20architectures%20and%20are%20heavily%20dependent%20on%0Aempirically%20fixed%20hyperparameters.%20Herein%2C%20to%20overcome%20these%20limitations%2C%20we%0Apropose%3A%20%281%29%20a%20statistical%20feature%20loss%20to%20enhance%20the%20discrimination%0Acapability%20of%20the%20model%2C%20instead%20of%20relying%20solely%20on%20feature%20distances%3B%20%282%29%0Ausing%20the%20waveform%20for%20describing%20the%20audio%20as%20a%20replacement%20of%20frequency-based%0Arepresentations%3B%20%283%29%20a%20post-processing%20normalization%20of%20the%20fakeness%20score%3B%20%284%29%0Athe%20use%20of%20shallower%20network%20for%20reducing%20the%20computational%20complexity.%0AExperiments%20on%20the%20DFDC%20and%20FakeAVCeleb%20datasets%20demonstrate%20the%20relevance%20of%0Athe%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistics-aware%2520Audio-visual%2520Deepfake%2520Detector%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520enhanced%2520audio-visual%2520deep%2520detection%2520method.%250ARecent%2520methods%2520in%2520audio-visual%2520deepfake%2520detection%2520mostly%2520assess%2520the%250Asynchronization%2520between%2520audio%2520and%2520visual%2520features.%2520Although%2520they%2520have%2520shown%250Apromising%2520results%252C%2520they%2520are%2520based%2520on%2520the%2520maximization/minimization%2520of%2520isolated%250Afeature%2520distances%2520without%2520considering%2520feature%2520statistics.%2520Moreover%252C%2520they%2520rely%250Aon%2520cumbersome%2520deep%2520learning%2520architectures%2520and%2520are%2520heavily%2520dependent%2520on%250Aempirically%2520fixed%2520hyperparameters.%2520Herein%252C%2520to%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%253A%2520%25281%2529%2520a%2520statistical%2520feature%2520loss%2520to%2520enhance%2520the%2520discrimination%250Acapability%2520of%2520the%2520model%252C%2520instead%2520of%2520relying%2520solely%2520on%2520feature%2520distances%253B%2520%25282%2529%250Ausing%2520the%2520waveform%2520for%2520describing%2520the%2520audio%2520as%2520a%2520replacement%2520of%2520frequency-based%250Arepresentations%253B%2520%25283%2529%2520a%2520post-processing%2520normalization%2520of%2520the%2520fakeness%2520score%253B%2520%25284%2529%250Athe%2520use%2520of%2520shallower%2520network%2520for%2520reducing%2520the%2520computational%2520complexity.%250AExperiments%2520on%2520the%2520DFDC%2520and%2520FakeAVCeleb%2520datasets%2520demonstrate%2520the%2520relevance%2520of%250Athe%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistics-aware%20Audio-visual%20Deepfake%20Detector&entry.906535625=Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20enhanced%20audio-visual%20deep%20detection%20method.%0ARecent%20methods%20in%20audio-visual%20deepfake%20detection%20mostly%20assess%20the%0Asynchronization%20between%20audio%20and%20visual%20features.%20Although%20they%20have%20shown%0Apromising%20results%2C%20they%20are%20based%20on%20the%20maximization/minimization%20of%20isolated%0Afeature%20distances%20without%20considering%20feature%20statistics.%20Moreover%2C%20they%20rely%0Aon%20cumbersome%20deep%20learning%20architectures%20and%20are%20heavily%20dependent%20on%0Aempirically%20fixed%20hyperparameters.%20Herein%2C%20to%20overcome%20these%20limitations%2C%20we%0Apropose%3A%20%281%29%20a%20statistical%20feature%20loss%20to%20enhance%20the%20discrimination%0Acapability%20of%20the%20model%2C%20instead%20of%20relying%20solely%20on%20feature%20distances%3B%20%282%29%0Ausing%20the%20waveform%20for%20describing%20the%20audio%20as%20a%20replacement%20of%20frequency-based%0Arepresentations%3B%20%283%29%20a%20post-processing%20normalization%20of%20the%20fakeness%20score%3B%20%284%29%0Athe%20use%20of%20shallower%20network%20for%20reducing%20the%20computational%20complexity.%0AExperiments%20on%20the%20DFDC%20and%20FakeAVCeleb%20datasets%20demonstrate%20the%20relevance%20of%0Athe%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11650v1&entry.124074799=Read"},
{"title": "Single Layer Single Gradient Unlearning", "author": "Zikui Cai and Yaoteng Tan and M. Salman Asif", "abstract": "  Machine unlearning methods seek to revise pretrained models such that effects\nof certain training samples can be removed. In addition to effective erasure,\nlow computational cost and general utility retention are also highly desirable.\nExisting unlearning methods usually involve iterative updates over the model\nparameters, which incurs a high computational cost. In this work, we propose an\nefficient method that only requires a one-time gradient computation, with which\nwe modify only a single layer of model parameters. Specifically, we first\nidentify a small number of model layers that lie on the Pareto front of high\nforget importance and low retain influence as critical layers. Then we search\nfor a suitable step size and take a step along the gradient direction of a\nsingle critical layer while keeping other layers frozen. This method is highly\nmodular and can be used to unlearn multiple concepts simultaneously in a\ncontrollable manner. We demonstrate the effectiveness and efficiency of this\nmethod on various models including CLIP, stable diffusion, and VLMs, surpassing\nother state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.11867v1", "date": "2024-07-16", "relevancy": 2.0254, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Layer%20Single%20Gradient%20Unlearning&body=Title%3A%20Single%20Layer%20Single%20Gradient%20Unlearning%0AAuthor%3A%20Zikui%20Cai%20and%20Yaoteng%20Tan%20and%20M.%20Salman%20Asif%0AAbstract%3A%20%20%20Machine%20unlearning%20methods%20seek%20to%20revise%20pretrained%20models%20such%20that%20effects%0Aof%20certain%20training%20samples%20can%20be%20removed.%20In%20addition%20to%20effective%20erasure%2C%0Alow%20computational%20cost%20and%20general%20utility%20retention%20are%20also%20highly%20desirable.%0AExisting%20unlearning%20methods%20usually%20involve%20iterative%20updates%20over%20the%20model%0Aparameters%2C%20which%20incurs%20a%20high%20computational%20cost.%20In%20this%20work%2C%20we%20propose%20an%0Aefficient%20method%20that%20only%20requires%20a%20one-time%20gradient%20computation%2C%20with%20which%0Awe%20modify%20only%20a%20single%20layer%20of%20model%20parameters.%20Specifically%2C%20we%20first%0Aidentify%20a%20small%20number%20of%20model%20layers%20that%20lie%20on%20the%20Pareto%20front%20of%20high%0Aforget%20importance%20and%20low%20retain%20influence%20as%20critical%20layers.%20Then%20we%20search%0Afor%20a%20suitable%20step%20size%20and%20take%20a%20step%20along%20the%20gradient%20direction%20of%20a%0Asingle%20critical%20layer%20while%20keeping%20other%20layers%20frozen.%20This%20method%20is%20highly%0Amodular%20and%20can%20be%20used%20to%20unlearn%20multiple%20concepts%20simultaneously%20in%20a%0Acontrollable%20manner.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20this%0Amethod%20on%20various%20models%20including%20CLIP%2C%20stable%20diffusion%2C%20and%20VLMs%2C%20surpassing%0Aother%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Layer%2520Single%2520Gradient%2520Unlearning%26entry.906535625%3DZikui%2520Cai%2520and%2520Yaoteng%2520Tan%2520and%2520M.%2520Salman%2520Asif%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520methods%2520seek%2520to%2520revise%2520pretrained%2520models%2520such%2520that%2520effects%250Aof%2520certain%2520training%2520samples%2520can%2520be%2520removed.%2520In%2520addition%2520to%2520effective%2520erasure%252C%250Alow%2520computational%2520cost%2520and%2520general%2520utility%2520retention%2520are%2520also%2520highly%2520desirable.%250AExisting%2520unlearning%2520methods%2520usually%2520involve%2520iterative%2520updates%2520over%2520the%2520model%250Aparameters%252C%2520which%2520incurs%2520a%2520high%2520computational%2520cost.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250Aefficient%2520method%2520that%2520only%2520requires%2520a%2520one-time%2520gradient%2520computation%252C%2520with%2520which%250Awe%2520modify%2520only%2520a%2520single%2520layer%2520of%2520model%2520parameters.%2520Specifically%252C%2520we%2520first%250Aidentify%2520a%2520small%2520number%2520of%2520model%2520layers%2520that%2520lie%2520on%2520the%2520Pareto%2520front%2520of%2520high%250Aforget%2520importance%2520and%2520low%2520retain%2520influence%2520as%2520critical%2520layers.%2520Then%2520we%2520search%250Afor%2520a%2520suitable%2520step%2520size%2520and%2520take%2520a%2520step%2520along%2520the%2520gradient%2520direction%2520of%2520a%250Asingle%2520critical%2520layer%2520while%2520keeping%2520other%2520layers%2520frozen.%2520This%2520method%2520is%2520highly%250Amodular%2520and%2520can%2520be%2520used%2520to%2520unlearn%2520multiple%2520concepts%2520simultaneously%2520in%2520a%250Acontrollable%2520manner.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520this%250Amethod%2520on%2520various%2520models%2520including%2520CLIP%252C%2520stable%2520diffusion%252C%2520and%2520VLMs%252C%2520surpassing%250Aother%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Layer%20Single%20Gradient%20Unlearning&entry.906535625=Zikui%20Cai%20and%20Yaoteng%20Tan%20and%20M.%20Salman%20Asif&entry.1292438233=%20%20Machine%20unlearning%20methods%20seek%20to%20revise%20pretrained%20models%20such%20that%20effects%0Aof%20certain%20training%20samples%20can%20be%20removed.%20In%20addition%20to%20effective%20erasure%2C%0Alow%20computational%20cost%20and%20general%20utility%20retention%20are%20also%20highly%20desirable.%0AExisting%20unlearning%20methods%20usually%20involve%20iterative%20updates%20over%20the%20model%0Aparameters%2C%20which%20incurs%20a%20high%20computational%20cost.%20In%20this%20work%2C%20we%20propose%20an%0Aefficient%20method%20that%20only%20requires%20a%20one-time%20gradient%20computation%2C%20with%20which%0Awe%20modify%20only%20a%20single%20layer%20of%20model%20parameters.%20Specifically%2C%20we%20first%0Aidentify%20a%20small%20number%20of%20model%20layers%20that%20lie%20on%20the%20Pareto%20front%20of%20high%0Aforget%20importance%20and%20low%20retain%20influence%20as%20critical%20layers.%20Then%20we%20search%0Afor%20a%20suitable%20step%20size%20and%20take%20a%20step%20along%20the%20gradient%20direction%20of%20a%0Asingle%20critical%20layer%20while%20keeping%20other%20layers%20frozen.%20This%20method%20is%20highly%0Amodular%20and%20can%20be%20used%20to%20unlearn%20multiple%20concepts%20simultaneously%20in%20a%0Acontrollable%20manner.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20this%0Amethod%20on%20various%20models%20including%20CLIP%2C%20stable%20diffusion%2C%20and%20VLMs%2C%20surpassing%0Aother%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11867v1&entry.124074799=Read"},
{"title": "Formal Verification of Unknown Dynamical Systems via Gaussian Process\n  Regression", "author": "John Skovbekk and Luca Laurenti and Eric Frew and Morteza Lahijanian", "abstract": "  Leveraging autonomous systems in safety-critical scenarios requires verifying\ntheir behaviors in the presence of uncertainties and black-box components that\ninfluence the system dynamics. In this work, we develop a framework for\nverifying discrete-time dynamical systems with unmodelled dynamics and noisy\nmeasurements against temporal logic specifications from an input-output\ndataset. The verification framework employs Gaussian process (GP) regression to\nlearn the unknown dynamics from the dataset and abstracts the continuous-space\nsystem as a finite-state, uncertain Markov decision process (MDP). This\nabstraction relies on space discretization and transition probability intervals\nthat capture the uncertainty due to the error in GP regression by using\nreproducible kernel Hilbert space analysis as well as the uncertainty induced\nby discretization. The framework utilizes existing model checking tools for\nverification of the uncertain MDP abstraction against a given temporal logic\nspecification. We establish the correctness of extending the verification\nresults on the abstraction created from noisy measurements to the underlying\nsystem. We show that the computational complexity of the framework is\npolynomial in the size of the dataset and discrete abstraction. The complexity\nanalysis illustrates a trade-off between the quality of the verification\nresults and the computational burden to handle larger datasets and finer\nabstractions. Finally, we demonstrate the efficacy of our learning and\nverification framework on several case studies with linear, nonlinear, and\nswitched dynamical systems.\n", "link": "http://arxiv.org/abs/2201.00655v2", "date": "2024-07-16", "relevancy": 2.0196, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Formal%20Verification%20of%20Unknown%20Dynamical%20Systems%20via%20Gaussian%20Process%0A%20%20Regression&body=Title%3A%20Formal%20Verification%20of%20Unknown%20Dynamical%20Systems%20via%20Gaussian%20Process%0A%20%20Regression%0AAuthor%3A%20John%20Skovbekk%20and%20Luca%20Laurenti%20and%20Eric%20Frew%20and%20Morteza%20Lahijanian%0AAbstract%3A%20%20%20Leveraging%20autonomous%20systems%20in%20safety-critical%20scenarios%20requires%20verifying%0Atheir%20behaviors%20in%20the%20presence%20of%20uncertainties%20and%20black-box%20components%20that%0Ainfluence%20the%20system%20dynamics.%20In%20this%20work%2C%20we%20develop%20a%20framework%20for%0Averifying%20discrete-time%20dynamical%20systems%20with%20unmodelled%20dynamics%20and%20noisy%0Ameasurements%20against%20temporal%20logic%20specifications%20from%20an%20input-output%0Adataset.%20The%20verification%20framework%20employs%20Gaussian%20process%20%28GP%29%20regression%20to%0Alearn%20the%20unknown%20dynamics%20from%20the%20dataset%20and%20abstracts%20the%20continuous-space%0Asystem%20as%20a%20finite-state%2C%20uncertain%20Markov%20decision%20process%20%28MDP%29.%20This%0Aabstraction%20relies%20on%20space%20discretization%20and%20transition%20probability%20intervals%0Athat%20capture%20the%20uncertainty%20due%20to%20the%20error%20in%20GP%20regression%20by%20using%0Areproducible%20kernel%20Hilbert%20space%20analysis%20as%20well%20as%20the%20uncertainty%20induced%0Aby%20discretization.%20The%20framework%20utilizes%20existing%20model%20checking%20tools%20for%0Averification%20of%20the%20uncertain%20MDP%20abstraction%20against%20a%20given%20temporal%20logic%0Aspecification.%20We%20establish%20the%20correctness%20of%20extending%20the%20verification%0Aresults%20on%20the%20abstraction%20created%20from%20noisy%20measurements%20to%20the%20underlying%0Asystem.%20We%20show%20that%20the%20computational%20complexity%20of%20the%20framework%20is%0Apolynomial%20in%20the%20size%20of%20the%20dataset%20and%20discrete%20abstraction.%20The%20complexity%0Aanalysis%20illustrates%20a%20trade-off%20between%20the%20quality%20of%20the%20verification%0Aresults%20and%20the%20computational%20burden%20to%20handle%20larger%20datasets%20and%20finer%0Aabstractions.%20Finally%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20learning%20and%0Averification%20framework%20on%20several%20case%20studies%20with%20linear%2C%20nonlinear%2C%20and%0Aswitched%20dynamical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.00655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFormal%2520Verification%2520of%2520Unknown%2520Dynamical%2520Systems%2520via%2520Gaussian%2520Process%250A%2520%2520Regression%26entry.906535625%3DJohn%2520Skovbekk%2520and%2520Luca%2520Laurenti%2520and%2520Eric%2520Frew%2520and%2520Morteza%2520Lahijanian%26entry.1292438233%3D%2520%2520Leveraging%2520autonomous%2520systems%2520in%2520safety-critical%2520scenarios%2520requires%2520verifying%250Atheir%2520behaviors%2520in%2520the%2520presence%2520of%2520uncertainties%2520and%2520black-box%2520components%2520that%250Ainfluence%2520the%2520system%2520dynamics.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520framework%2520for%250Averifying%2520discrete-time%2520dynamical%2520systems%2520with%2520unmodelled%2520dynamics%2520and%2520noisy%250Ameasurements%2520against%2520temporal%2520logic%2520specifications%2520from%2520an%2520input-output%250Adataset.%2520The%2520verification%2520framework%2520employs%2520Gaussian%2520process%2520%2528GP%2529%2520regression%2520to%250Alearn%2520the%2520unknown%2520dynamics%2520from%2520the%2520dataset%2520and%2520abstracts%2520the%2520continuous-space%250Asystem%2520as%2520a%2520finite-state%252C%2520uncertain%2520Markov%2520decision%2520process%2520%2528MDP%2529.%2520This%250Aabstraction%2520relies%2520on%2520space%2520discretization%2520and%2520transition%2520probability%2520intervals%250Athat%2520capture%2520the%2520uncertainty%2520due%2520to%2520the%2520error%2520in%2520GP%2520regression%2520by%2520using%250Areproducible%2520kernel%2520Hilbert%2520space%2520analysis%2520as%2520well%2520as%2520the%2520uncertainty%2520induced%250Aby%2520discretization.%2520The%2520framework%2520utilizes%2520existing%2520model%2520checking%2520tools%2520for%250Averification%2520of%2520the%2520uncertain%2520MDP%2520abstraction%2520against%2520a%2520given%2520temporal%2520logic%250Aspecification.%2520We%2520establish%2520the%2520correctness%2520of%2520extending%2520the%2520verification%250Aresults%2520on%2520the%2520abstraction%2520created%2520from%2520noisy%2520measurements%2520to%2520the%2520underlying%250Asystem.%2520We%2520show%2520that%2520the%2520computational%2520complexity%2520of%2520the%2520framework%2520is%250Apolynomial%2520in%2520the%2520size%2520of%2520the%2520dataset%2520and%2520discrete%2520abstraction.%2520The%2520complexity%250Aanalysis%2520illustrates%2520a%2520trade-off%2520between%2520the%2520quality%2520of%2520the%2520verification%250Aresults%2520and%2520the%2520computational%2520burden%2520to%2520handle%2520larger%2520datasets%2520and%2520finer%250Aabstractions.%2520Finally%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520learning%2520and%250Averification%2520framework%2520on%2520several%2520case%2520studies%2520with%2520linear%252C%2520nonlinear%252C%2520and%250Aswitched%2520dynamical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.00655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Formal%20Verification%20of%20Unknown%20Dynamical%20Systems%20via%20Gaussian%20Process%0A%20%20Regression&entry.906535625=John%20Skovbekk%20and%20Luca%20Laurenti%20and%20Eric%20Frew%20and%20Morteza%20Lahijanian&entry.1292438233=%20%20Leveraging%20autonomous%20systems%20in%20safety-critical%20scenarios%20requires%20verifying%0Atheir%20behaviors%20in%20the%20presence%20of%20uncertainties%20and%20black-box%20components%20that%0Ainfluence%20the%20system%20dynamics.%20In%20this%20work%2C%20we%20develop%20a%20framework%20for%0Averifying%20discrete-time%20dynamical%20systems%20with%20unmodelled%20dynamics%20and%20noisy%0Ameasurements%20against%20temporal%20logic%20specifications%20from%20an%20input-output%0Adataset.%20The%20verification%20framework%20employs%20Gaussian%20process%20%28GP%29%20regression%20to%0Alearn%20the%20unknown%20dynamics%20from%20the%20dataset%20and%20abstracts%20the%20continuous-space%0Asystem%20as%20a%20finite-state%2C%20uncertain%20Markov%20decision%20process%20%28MDP%29.%20This%0Aabstraction%20relies%20on%20space%20discretization%20and%20transition%20probability%20intervals%0Athat%20capture%20the%20uncertainty%20due%20to%20the%20error%20in%20GP%20regression%20by%20using%0Areproducible%20kernel%20Hilbert%20space%20analysis%20as%20well%20as%20the%20uncertainty%20induced%0Aby%20discretization.%20The%20framework%20utilizes%20existing%20model%20checking%20tools%20for%0Averification%20of%20the%20uncertain%20MDP%20abstraction%20against%20a%20given%20temporal%20logic%0Aspecification.%20We%20establish%20the%20correctness%20of%20extending%20the%20verification%0Aresults%20on%20the%20abstraction%20created%20from%20noisy%20measurements%20to%20the%20underlying%0Asystem.%20We%20show%20that%20the%20computational%20complexity%20of%20the%20framework%20is%0Apolynomial%20in%20the%20size%20of%20the%20dataset%20and%20discrete%20abstraction.%20The%20complexity%0Aanalysis%20illustrates%20a%20trade-off%20between%20the%20quality%20of%20the%20verification%0Aresults%20and%20the%20computational%20burden%20to%20handle%20larger%20datasets%20and%20finer%0Aabstractions.%20Finally%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20learning%20and%0Averification%20framework%20on%20several%20case%20studies%20with%20linear%2C%20nonlinear%2C%20and%0Aswitched%20dynamical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.00655v2&entry.124074799=Read"},
{"title": "V2X-M2C: Efficient Multi-Module Collaborative Perception with Two\n  Connections", "author": "Hyunchul Bae and Minhee Kang and Heejin Ahn", "abstract": "  In this paper, we investigate improving the perception performance of\nautonomous vehicles through communication with other vehicles and road\ninfrastructures. To this end, we introduce a collaborative perception model\n$\\textbf{V2X-M2C}$, consisting of multiple modules, each generating inter-agent\ncomplementary information, spatial global context, and spatial local\ninformation. Inspired by the question of why most existing architectures are\nsequential, we analyze both the $\\textit{sequential}$ and $\\textit{parallel}$\nconnections of the modules. The sequential connection synergizes the modules,\nwhereas the parallel connection independently improves each module. Extensive\nexperiments demonstrate that V2X-M2C achieves state-of-the-art perception\nperformance, increasing the detection accuracy by 8.00% to 10.87% and\ndecreasing the FLOPs by 42.81% to 52.64%.\n", "link": "http://arxiv.org/abs/2407.11546v1", "date": "2024-07-16", "relevancy": 2.019, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2X-M2C%3A%20Efficient%20Multi-Module%20Collaborative%20Perception%20with%20Two%0A%20%20Connections&body=Title%3A%20V2X-M2C%3A%20Efficient%20Multi-Module%20Collaborative%20Perception%20with%20Two%0A%20%20Connections%0AAuthor%3A%20Hyunchul%20Bae%20and%20Minhee%20Kang%20and%20Heejin%20Ahn%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20improving%20the%20perception%20performance%20of%0Aautonomous%20vehicles%20through%20communication%20with%20other%20vehicles%20and%20road%0Ainfrastructures.%20To%20this%20end%2C%20we%20introduce%20a%20collaborative%20perception%20model%0A%24%5Ctextbf%7BV2X-M2C%7D%24%2C%20consisting%20of%20multiple%20modules%2C%20each%20generating%20inter-agent%0Acomplementary%20information%2C%20spatial%20global%20context%2C%20and%20spatial%20local%0Ainformation.%20Inspired%20by%20the%20question%20of%20why%20most%20existing%20architectures%20are%0Asequential%2C%20we%20analyze%20both%20the%20%24%5Ctextit%7Bsequential%7D%24%20and%20%24%5Ctextit%7Bparallel%7D%24%0Aconnections%20of%20the%20modules.%20The%20sequential%20connection%20synergizes%20the%20modules%2C%0Awhereas%20the%20parallel%20connection%20independently%20improves%20each%20module.%20Extensive%0Aexperiments%20demonstrate%20that%20V2X-M2C%20achieves%20state-of-the-art%20perception%0Aperformance%2C%20increasing%20the%20detection%20accuracy%20by%208.00%25%20to%2010.87%25%20and%0Adecreasing%20the%20FLOPs%20by%2042.81%25%20to%2052.64%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2X-M2C%253A%2520Efficient%2520Multi-Module%2520Collaborative%2520Perception%2520with%2520Two%250A%2520%2520Connections%26entry.906535625%3DHyunchul%2520Bae%2520and%2520Minhee%2520Kang%2520and%2520Heejin%2520Ahn%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520improving%2520the%2520perception%2520performance%2520of%250Aautonomous%2520vehicles%2520through%2520communication%2520with%2520other%2520vehicles%2520and%2520road%250Ainfrastructures.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520collaborative%2520perception%2520model%250A%2524%255Ctextbf%257BV2X-M2C%257D%2524%252C%2520consisting%2520of%2520multiple%2520modules%252C%2520each%2520generating%2520inter-agent%250Acomplementary%2520information%252C%2520spatial%2520global%2520context%252C%2520and%2520spatial%2520local%250Ainformation.%2520Inspired%2520by%2520the%2520question%2520of%2520why%2520most%2520existing%2520architectures%2520are%250Asequential%252C%2520we%2520analyze%2520both%2520the%2520%2524%255Ctextit%257Bsequential%257D%2524%2520and%2520%2524%255Ctextit%257Bparallel%257D%2524%250Aconnections%2520of%2520the%2520modules.%2520The%2520sequential%2520connection%2520synergizes%2520the%2520modules%252C%250Awhereas%2520the%2520parallel%2520connection%2520independently%2520improves%2520each%2520module.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520V2X-M2C%2520achieves%2520state-of-the-art%2520perception%250Aperformance%252C%2520increasing%2520the%2520detection%2520accuracy%2520by%25208.00%2525%2520to%252010.87%2525%2520and%250Adecreasing%2520the%2520FLOPs%2520by%252042.81%2525%2520to%252052.64%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X-M2C%3A%20Efficient%20Multi-Module%20Collaborative%20Perception%20with%20Two%0A%20%20Connections&entry.906535625=Hyunchul%20Bae%20and%20Minhee%20Kang%20and%20Heejin%20Ahn&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20improving%20the%20perception%20performance%20of%0Aautonomous%20vehicles%20through%20communication%20with%20other%20vehicles%20and%20road%0Ainfrastructures.%20To%20this%20end%2C%20we%20introduce%20a%20collaborative%20perception%20model%0A%24%5Ctextbf%7BV2X-M2C%7D%24%2C%20consisting%20of%20multiple%20modules%2C%20each%20generating%20inter-agent%0Acomplementary%20information%2C%20spatial%20global%20context%2C%20and%20spatial%20local%0Ainformation.%20Inspired%20by%20the%20question%20of%20why%20most%20existing%20architectures%20are%0Asequential%2C%20we%20analyze%20both%20the%20%24%5Ctextit%7Bsequential%7D%24%20and%20%24%5Ctextit%7Bparallel%7D%24%0Aconnections%20of%20the%20modules.%20The%20sequential%20connection%20synergizes%20the%20modules%2C%0Awhereas%20the%20parallel%20connection%20independently%20improves%20each%20module.%20Extensive%0Aexperiments%20demonstrate%20that%20V2X-M2C%20achieves%20state-of-the-art%20perception%0Aperformance%2C%20increasing%20the%20detection%20accuracy%20by%208.00%25%20to%2010.87%25%20and%0Adecreasing%20the%20FLOPs%20by%2042.81%25%20to%2052.64%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11546v1&entry.124074799=Read"},
{"title": "Scaling Sign Language Translation", "author": "Biao Zhang and Garrett Tanzer and Orhan Firat", "abstract": "  Sign language translation (SLT) addresses the problem of translating\ninformation from a sign language in video to a spoken language in text.\nExisting studies, while showing progress, are often limited to narrow domains\nand/or few sign languages and struggle with open-domain tasks. In this paper,\nwe push forward the frontier of SLT by scaling pretraining data, model size,\nand number of translation directions. We perform large-scale SLT pretraining on\ndifferent data including 1) noisy multilingual YouTube SLT data, 2) parallel\ntext corpora, and 3) SLT data augmented by translating video captions to other\nlanguages with off-the-shelf machine translation models. We unify different\npretraining tasks with task-specific prompts under the encoder-decoder\narchitecture, and initialize the SLT model with pretrained (m/By)T5 models\nacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL\nto 42 spoken languages) demonstrate the significance of data/model scaling and\ncross-lingual cross-modal transfer, as well as the feasibility of zero-shot\nSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT\nbenchmarks covering 5 sign languages. Experiments show substantial quality\nimprovements over the vanilla baselines, surpassing the previous\nstate-of-the-art (SOTA) by wide margins.\n", "link": "http://arxiv.org/abs/2407.11855v1", "date": "2024-07-16", "relevancy": 2.0149, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5594}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Sign%20Language%20Translation&body=Title%3A%20Scaling%20Sign%20Language%20Translation%0AAuthor%3A%20Biao%20Zhang%20and%20Garrett%20Tanzer%20and%20Orhan%20Firat%0AAbstract%3A%20%20%20Sign%20language%20translation%20%28SLT%29%20addresses%20the%20problem%20of%20translating%0Ainformation%20from%20a%20sign%20language%20in%20video%20to%20a%20spoken%20language%20in%20text.%0AExisting%20studies%2C%20while%20showing%20progress%2C%20are%20often%20limited%20to%20narrow%20domains%0Aand/or%20few%20sign%20languages%20and%20struggle%20with%20open-domain%20tasks.%20In%20this%20paper%2C%0Awe%20push%20forward%20the%20frontier%20of%20SLT%20by%20scaling%20pretraining%20data%2C%20model%20size%2C%0Aand%20number%20of%20translation%20directions.%20We%20perform%20large-scale%20SLT%20pretraining%20on%0Adifferent%20data%20including%201%29%20noisy%20multilingual%20YouTube%20SLT%20data%2C%202%29%20parallel%0Atext%20corpora%2C%20and%203%29%20SLT%20data%20augmented%20by%20translating%20video%20captions%20to%20other%0Alanguages%20with%20off-the-shelf%20machine%20translation%20models.%20We%20unify%20different%0Apretraining%20tasks%20with%20task-specific%20prompts%20under%20the%20encoder-decoder%0Aarchitecture%2C%20and%20initialize%20the%20SLT%20model%20with%20pretrained%20%28m/By%29T5%20models%0Aacross%20model%20sizes.%20SLT%20pretraining%20results%20on%20How2Sign%20and%20FLEURS-ASL%230%20%28ASL%0Ato%2042%20spoken%20languages%29%20demonstrate%20the%20significance%20of%20data/model%20scaling%20and%0Across-lingual%20cross-modal%20transfer%2C%20as%20well%20as%20the%20feasibility%20of%20zero-shot%0ASLT.%20We%20finetune%20the%20pretrained%20SLT%20models%20on%205%20downstream%20open-domain%20SLT%0Abenchmarks%20covering%205%20sign%20languages.%20Experiments%20show%20substantial%20quality%0Aimprovements%20over%20the%20vanilla%20baselines%2C%20surpassing%20the%20previous%0Astate-of-the-art%20%28SOTA%29%20by%20wide%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Sign%2520Language%2520Translation%26entry.906535625%3DBiao%2520Zhang%2520and%2520Garrett%2520Tanzer%2520and%2520Orhan%2520Firat%26entry.1292438233%3D%2520%2520Sign%2520language%2520translation%2520%2528SLT%2529%2520addresses%2520the%2520problem%2520of%2520translating%250Ainformation%2520from%2520a%2520sign%2520language%2520in%2520video%2520to%2520a%2520spoken%2520language%2520in%2520text.%250AExisting%2520studies%252C%2520while%2520showing%2520progress%252C%2520are%2520often%2520limited%2520to%2520narrow%2520domains%250Aand/or%2520few%2520sign%2520languages%2520and%2520struggle%2520with%2520open-domain%2520tasks.%2520In%2520this%2520paper%252C%250Awe%2520push%2520forward%2520the%2520frontier%2520of%2520SLT%2520by%2520scaling%2520pretraining%2520data%252C%2520model%2520size%252C%250Aand%2520number%2520of%2520translation%2520directions.%2520We%2520perform%2520large-scale%2520SLT%2520pretraining%2520on%250Adifferent%2520data%2520including%25201%2529%2520noisy%2520multilingual%2520YouTube%2520SLT%2520data%252C%25202%2529%2520parallel%250Atext%2520corpora%252C%2520and%25203%2529%2520SLT%2520data%2520augmented%2520by%2520translating%2520video%2520captions%2520to%2520other%250Alanguages%2520with%2520off-the-shelf%2520machine%2520translation%2520models.%2520We%2520unify%2520different%250Apretraining%2520tasks%2520with%2520task-specific%2520prompts%2520under%2520the%2520encoder-decoder%250Aarchitecture%252C%2520and%2520initialize%2520the%2520SLT%2520model%2520with%2520pretrained%2520%2528m/By%2529T5%2520models%250Aacross%2520model%2520sizes.%2520SLT%2520pretraining%2520results%2520on%2520How2Sign%2520and%2520FLEURS-ASL%25230%2520%2528ASL%250Ato%252042%2520spoken%2520languages%2529%2520demonstrate%2520the%2520significance%2520of%2520data/model%2520scaling%2520and%250Across-lingual%2520cross-modal%2520transfer%252C%2520as%2520well%2520as%2520the%2520feasibility%2520of%2520zero-shot%250ASLT.%2520We%2520finetune%2520the%2520pretrained%2520SLT%2520models%2520on%25205%2520downstream%2520open-domain%2520SLT%250Abenchmarks%2520covering%25205%2520sign%2520languages.%2520Experiments%2520show%2520substantial%2520quality%250Aimprovements%2520over%2520the%2520vanilla%2520baselines%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520by%2520wide%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Sign%20Language%20Translation&entry.906535625=Biao%20Zhang%20and%20Garrett%20Tanzer%20and%20Orhan%20Firat&entry.1292438233=%20%20Sign%20language%20translation%20%28SLT%29%20addresses%20the%20problem%20of%20translating%0Ainformation%20from%20a%20sign%20language%20in%20video%20to%20a%20spoken%20language%20in%20text.%0AExisting%20studies%2C%20while%20showing%20progress%2C%20are%20often%20limited%20to%20narrow%20domains%0Aand/or%20few%20sign%20languages%20and%20struggle%20with%20open-domain%20tasks.%20In%20this%20paper%2C%0Awe%20push%20forward%20the%20frontier%20of%20SLT%20by%20scaling%20pretraining%20data%2C%20model%20size%2C%0Aand%20number%20of%20translation%20directions.%20We%20perform%20large-scale%20SLT%20pretraining%20on%0Adifferent%20data%20including%201%29%20noisy%20multilingual%20YouTube%20SLT%20data%2C%202%29%20parallel%0Atext%20corpora%2C%20and%203%29%20SLT%20data%20augmented%20by%20translating%20video%20captions%20to%20other%0Alanguages%20with%20off-the-shelf%20machine%20translation%20models.%20We%20unify%20different%0Apretraining%20tasks%20with%20task-specific%20prompts%20under%20the%20encoder-decoder%0Aarchitecture%2C%20and%20initialize%20the%20SLT%20model%20with%20pretrained%20%28m/By%29T5%20models%0Aacross%20model%20sizes.%20SLT%20pretraining%20results%20on%20How2Sign%20and%20FLEURS-ASL%230%20%28ASL%0Ato%2042%20spoken%20languages%29%20demonstrate%20the%20significance%20of%20data/model%20scaling%20and%0Across-lingual%20cross-modal%20transfer%2C%20as%20well%20as%20the%20feasibility%20of%20zero-shot%0ASLT.%20We%20finetune%20the%20pretrained%20SLT%20models%20on%205%20downstream%20open-domain%20SLT%0Abenchmarks%20covering%205%20sign%20languages.%20Experiments%20show%20substantial%20quality%0Aimprovements%20over%20the%20vanilla%20baselines%2C%20surpassing%20the%20previous%0Astate-of-the-art%20%28SOTA%29%20by%20wide%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11855v1&entry.124074799=Read"},
{"title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in\n  Grammatical Error Detection", "author": "Gaetan Lopez Latouche and Marc-Andr\u00e9 Carbonneau and Ben Swanson", "abstract": "  Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.\n", "link": "http://arxiv.org/abs/2407.11854v1", "date": "2024-07-16", "relevancy": 1.9289, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4862}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Cross-Lingual%20Transfer%20for%20Synthetic%20Data%20Generation%20in%0A%20%20Grammatical%20Error%20Detection&body=Title%3A%20Zero-shot%20Cross-Lingual%20Transfer%20for%20Synthetic%20Data%20Generation%20in%0A%20%20Grammatical%20Error%20Detection%0AAuthor%3A%20Gaetan%20Lopez%20Latouche%20and%20Marc-Andr%C3%A9%20Carbonneau%20and%20Ben%20Swanson%0AAbstract%3A%20%20%20Grammatical%20Error%20Detection%20%28GED%29%20methods%20rely%20heavily%20on%20human%20annotated%0Aerror%20corpora.%20However%2C%20these%20annotations%20are%20unavailable%20in%20many%20low-resource%0Alanguages.%20In%20this%20paper%2C%20we%20investigate%20GED%20in%20this%20context.%20Leveraging%20the%0Azero-shot%20cross-lingual%20transfer%20capabilities%20of%20multilingual%20pre-trained%0Alanguage%20models%2C%20we%20train%20a%20model%20using%20data%20from%20a%20diverse%20set%20of%20languages%20to%0Agenerate%20synthetic%20errors%20in%20other%20languages.%20These%20synthetic%20error%20corpora%20are%0Athen%20used%20to%20train%20a%20GED%20model.%20Specifically%20we%20propose%20a%20two-stage%20fine-tuning%0Apipeline%20where%20the%20GED%20model%20is%20first%20fine-tuned%20on%20multilingual%20synthetic%20data%0Afrom%20target%20languages%20followed%20by%20fine-tuning%20on%20human-annotated%20GED%20corpora%0Afrom%20source%20languages.%20This%20approach%20outperforms%20current%20state-of-the-art%0Aannotation-free%20GED%20methods.%20We%20also%20analyse%20the%20errors%20produced%20by%20our%20method%0Aand%20other%20strong%20baselines%2C%20finding%20that%20our%20approach%20produces%20errors%20that%20are%0Amore%20diverse%20and%20more%20similar%20to%20human%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Cross-Lingual%2520Transfer%2520for%2520Synthetic%2520Data%2520Generation%2520in%250A%2520%2520Grammatical%2520Error%2520Detection%26entry.906535625%3DGaetan%2520Lopez%2520Latouche%2520and%2520Marc-Andr%25C3%25A9%2520Carbonneau%2520and%2520Ben%2520Swanson%26entry.1292438233%3D%2520%2520Grammatical%2520Error%2520Detection%2520%2528GED%2529%2520methods%2520rely%2520heavily%2520on%2520human%2520annotated%250Aerror%2520corpora.%2520However%252C%2520these%2520annotations%2520are%2520unavailable%2520in%2520many%2520low-resource%250Alanguages.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520GED%2520in%2520this%2520context.%2520Leveraging%2520the%250Azero-shot%2520cross-lingual%2520transfer%2520capabilities%2520of%2520multilingual%2520pre-trained%250Alanguage%2520models%252C%2520we%2520train%2520a%2520model%2520using%2520data%2520from%2520a%2520diverse%2520set%2520of%2520languages%2520to%250Agenerate%2520synthetic%2520errors%2520in%2520other%2520languages.%2520These%2520synthetic%2520error%2520corpora%2520are%250Athen%2520used%2520to%2520train%2520a%2520GED%2520model.%2520Specifically%2520we%2520propose%2520a%2520two-stage%2520fine-tuning%250Apipeline%2520where%2520the%2520GED%2520model%2520is%2520first%2520fine-tuned%2520on%2520multilingual%2520synthetic%2520data%250Afrom%2520target%2520languages%2520followed%2520by%2520fine-tuning%2520on%2520human-annotated%2520GED%2520corpora%250Afrom%2520source%2520languages.%2520This%2520approach%2520outperforms%2520current%2520state-of-the-art%250Aannotation-free%2520GED%2520methods.%2520We%2520also%2520analyse%2520the%2520errors%2520produced%2520by%2520our%2520method%250Aand%2520other%2520strong%2520baselines%252C%2520finding%2520that%2520our%2520approach%2520produces%2520errors%2520that%2520are%250Amore%2520diverse%2520and%2520more%2520similar%2520to%2520human%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Cross-Lingual%20Transfer%20for%20Synthetic%20Data%20Generation%20in%0A%20%20Grammatical%20Error%20Detection&entry.906535625=Gaetan%20Lopez%20Latouche%20and%20Marc-Andr%C3%A9%20Carbonneau%20and%20Ben%20Swanson&entry.1292438233=%20%20Grammatical%20Error%20Detection%20%28GED%29%20methods%20rely%20heavily%20on%20human%20annotated%0Aerror%20corpora.%20However%2C%20these%20annotations%20are%20unavailable%20in%20many%20low-resource%0Alanguages.%20In%20this%20paper%2C%20we%20investigate%20GED%20in%20this%20context.%20Leveraging%20the%0Azero-shot%20cross-lingual%20transfer%20capabilities%20of%20multilingual%20pre-trained%0Alanguage%20models%2C%20we%20train%20a%20model%20using%20data%20from%20a%20diverse%20set%20of%20languages%20to%0Agenerate%20synthetic%20errors%20in%20other%20languages.%20These%20synthetic%20error%20corpora%20are%0Athen%20used%20to%20train%20a%20GED%20model.%20Specifically%20we%20propose%20a%20two-stage%20fine-tuning%0Apipeline%20where%20the%20GED%20model%20is%20first%20fine-tuned%20on%20multilingual%20synthetic%20data%0Afrom%20target%20languages%20followed%20by%20fine-tuning%20on%20human-annotated%20GED%20corpora%0Afrom%20source%20languages.%20This%20approach%20outperforms%20current%20state-of-the-art%0Aannotation-free%20GED%20methods.%20We%20also%20analyse%20the%20errors%20produced%20by%20our%20method%0Aand%20other%20strong%20baselines%2C%20finding%20that%20our%20approach%20produces%20errors%20that%20are%0Amore%20diverse%20and%20more%20similar%20to%20human%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11854v1&entry.124074799=Read"},
{"title": "Novel Artistic Scene-Centric Datasets for Effective Transfer Learning in\n  Fragrant Spaces", "author": "Shumei Liu and Haiting Huang and Mathias Zinnen and Andreas Maier and Vincent Christlein", "abstract": "  Olfaction, often overlooked in cultural heritage studies, holds profound\nsignificance in shaping human experiences and identities. Examining historical\ndepictions of olfactory scenes can offer valuable insights into the role of\nsmells in history. We show that a transfer-learning approach using weakly\nlabeled training data can remarkably improve the classification of fragrant\nspaces and, more generally, artistic scene depictions. We fine-tune\nPlaces365-pre-trained models by querying two cultural heritage data sources and\nusing the search terms as supervision signal. The models are evaluated on two\nmanually corrected test splits. This work lays a foundation for further\nexploration of fragrant spaces recognition and artistic scene classification.\nAll images and labels are released as the ArtPlaces dataset at\nhttps://zenodo.org/doi/10.5281/zenodo.11584328.\n", "link": "http://arxiv.org/abs/2407.11701v1", "date": "2024-07-16", "relevancy": 1.9774, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.503}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Artistic%20Scene-Centric%20Datasets%20for%20Effective%20Transfer%20Learning%20in%0A%20%20Fragrant%20Spaces&body=Title%3A%20Novel%20Artistic%20Scene-Centric%20Datasets%20for%20Effective%20Transfer%20Learning%20in%0A%20%20Fragrant%20Spaces%0AAuthor%3A%20Shumei%20Liu%20and%20Haiting%20Huang%20and%20Mathias%20Zinnen%20and%20Andreas%20Maier%20and%20Vincent%20Christlein%0AAbstract%3A%20%20%20Olfaction%2C%20often%20overlooked%20in%20cultural%20heritage%20studies%2C%20holds%20profound%0Asignificance%20in%20shaping%20human%20experiences%20and%20identities.%20Examining%20historical%0Adepictions%20of%20olfactory%20scenes%20can%20offer%20valuable%20insights%20into%20the%20role%20of%0Asmells%20in%20history.%20We%20show%20that%20a%20transfer-learning%20approach%20using%20weakly%0Alabeled%20training%20data%20can%20remarkably%20improve%20the%20classification%20of%20fragrant%0Aspaces%20and%2C%20more%20generally%2C%20artistic%20scene%20depictions.%20We%20fine-tune%0APlaces365-pre-trained%20models%20by%20querying%20two%20cultural%20heritage%20data%20sources%20and%0Ausing%20the%20search%20terms%20as%20supervision%20signal.%20The%20models%20are%20evaluated%20on%20two%0Amanually%20corrected%20test%20splits.%20This%20work%20lays%20a%20foundation%20for%20further%0Aexploration%20of%20fragrant%20spaces%20recognition%20and%20artistic%20scene%20classification.%0AAll%20images%20and%20labels%20are%20released%20as%20the%20ArtPlaces%20dataset%20at%0Ahttps%3A//zenodo.org/doi/10.5281/zenodo.11584328.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Artistic%2520Scene-Centric%2520Datasets%2520for%2520Effective%2520Transfer%2520Learning%2520in%250A%2520%2520Fragrant%2520Spaces%26entry.906535625%3DShumei%2520Liu%2520and%2520Haiting%2520Huang%2520and%2520Mathias%2520Zinnen%2520and%2520Andreas%2520Maier%2520and%2520Vincent%2520Christlein%26entry.1292438233%3D%2520%2520Olfaction%252C%2520often%2520overlooked%2520in%2520cultural%2520heritage%2520studies%252C%2520holds%2520profound%250Asignificance%2520in%2520shaping%2520human%2520experiences%2520and%2520identities.%2520Examining%2520historical%250Adepictions%2520of%2520olfactory%2520scenes%2520can%2520offer%2520valuable%2520insights%2520into%2520the%2520role%2520of%250Asmells%2520in%2520history.%2520We%2520show%2520that%2520a%2520transfer-learning%2520approach%2520using%2520weakly%250Alabeled%2520training%2520data%2520can%2520remarkably%2520improve%2520the%2520classification%2520of%2520fragrant%250Aspaces%2520and%252C%2520more%2520generally%252C%2520artistic%2520scene%2520depictions.%2520We%2520fine-tune%250APlaces365-pre-trained%2520models%2520by%2520querying%2520two%2520cultural%2520heritage%2520data%2520sources%2520and%250Ausing%2520the%2520search%2520terms%2520as%2520supervision%2520signal.%2520The%2520models%2520are%2520evaluated%2520on%2520two%250Amanually%2520corrected%2520test%2520splits.%2520This%2520work%2520lays%2520a%2520foundation%2520for%2520further%250Aexploration%2520of%2520fragrant%2520spaces%2520recognition%2520and%2520artistic%2520scene%2520classification.%250AAll%2520images%2520and%2520labels%2520are%2520released%2520as%2520the%2520ArtPlaces%2520dataset%2520at%250Ahttps%253A//zenodo.org/doi/10.5281/zenodo.11584328.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Artistic%20Scene-Centric%20Datasets%20for%20Effective%20Transfer%20Learning%20in%0A%20%20Fragrant%20Spaces&entry.906535625=Shumei%20Liu%20and%20Haiting%20Huang%20and%20Mathias%20Zinnen%20and%20Andreas%20Maier%20and%20Vincent%20Christlein&entry.1292438233=%20%20Olfaction%2C%20often%20overlooked%20in%20cultural%20heritage%20studies%2C%20holds%20profound%0Asignificance%20in%20shaping%20human%20experiences%20and%20identities.%20Examining%20historical%0Adepictions%20of%20olfactory%20scenes%20can%20offer%20valuable%20insights%20into%20the%20role%20of%0Asmells%20in%20history.%20We%20show%20that%20a%20transfer-learning%20approach%20using%20weakly%0Alabeled%20training%20data%20can%20remarkably%20improve%20the%20classification%20of%20fragrant%0Aspaces%20and%2C%20more%20generally%2C%20artistic%20scene%20depictions.%20We%20fine-tune%0APlaces365-pre-trained%20models%20by%20querying%20two%20cultural%20heritage%20data%20sources%20and%0Ausing%20the%20search%20terms%20as%20supervision%20signal.%20The%20models%20are%20evaluated%20on%20two%0Amanually%20corrected%20test%20splits.%20This%20work%20lays%20a%20foundation%20for%20further%0Aexploration%20of%20fragrant%20spaces%20recognition%20and%20artistic%20scene%20classification.%0AAll%20images%20and%20labels%20are%20released%20as%20the%20ArtPlaces%20dataset%20at%0Ahttps%3A//zenodo.org/doi/10.5281/zenodo.11584328.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11701v1&entry.124074799=Read"},
{"title": "Effective Bayesian Causal Inference via Structural Marginalisation and\n  Autoregressive Orders", "author": "Christian Toth and Christian Knoll and Franz Pernkopf and Robert Peharz", "abstract": "  Bayesian causal inference (BCI) naturally incorporates epistemic uncertainty\nabout the true causal model into down-stream causal reasoning tasks by\nposterior averaging over causal models. However, this poses a tremendously hard\ncomputational problem due to the intractable number of causal structures to\nmarginalise over. In this work, we decompose the structure learning problem\ninto inferring (i) a causal order and (ii) a parent set for each variable given\na causal order. By limiting the number of parents per variable, we can exactly\nmarginalise over the parent sets in polynomial time, which leaves only the\ncausal order to be marginalised. To this end, we propose a novel autoregressive\nmodel over causal orders (ARCO) learnable with gradient-based methods. Our\nmethod yields state-of-the-art in structure learning on simulated non-linear\nadditive noise benchmarks with scale-free and Erdos-Renyi graph structures, and\ncompetitive results on real-world data. Moreover, we illustrate that our method\naccurately infers interventional distributions, which allows us to estimate\nposterior average causal effects and many other causal quantities of interest.\n", "link": "http://arxiv.org/abs/2402.14781v2", "date": "2024-07-16", "relevancy": 1.2897, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4131}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Bayesian%20Causal%20Inference%20via%20Structural%20Marginalisation%20and%0A%20%20Autoregressive%20Orders&body=Title%3A%20Effective%20Bayesian%20Causal%20Inference%20via%20Structural%20Marginalisation%20and%0A%20%20Autoregressive%20Orders%0AAuthor%3A%20Christian%20Toth%20and%20Christian%20Knoll%20and%20Franz%20Pernkopf%20and%20Robert%20Peharz%0AAbstract%3A%20%20%20Bayesian%20causal%20inference%20%28BCI%29%20naturally%20incorporates%20epistemic%20uncertainty%0Aabout%20the%20true%20causal%20model%20into%20down-stream%20causal%20reasoning%20tasks%20by%0Aposterior%20averaging%20over%20causal%20models.%20However%2C%20this%20poses%20a%20tremendously%20hard%0Acomputational%20problem%20due%20to%20the%20intractable%20number%20of%20causal%20structures%20to%0Amarginalise%20over.%20In%20this%20work%2C%20we%20decompose%20the%20structure%20learning%20problem%0Ainto%20inferring%20%28i%29%20a%20causal%20order%20and%20%28ii%29%20a%20parent%20set%20for%20each%20variable%20given%0Aa%20causal%20order.%20By%20limiting%20the%20number%20of%20parents%20per%20variable%2C%20we%20can%20exactly%0Amarginalise%20over%20the%20parent%20sets%20in%20polynomial%20time%2C%20which%20leaves%20only%20the%0Acausal%20order%20to%20be%20marginalised.%20To%20this%20end%2C%20we%20propose%20a%20novel%20autoregressive%0Amodel%20over%20causal%20orders%20%28ARCO%29%20learnable%20with%20gradient-based%20methods.%20Our%0Amethod%20yields%20state-of-the-art%20in%20structure%20learning%20on%20simulated%20non-linear%0Aadditive%20noise%20benchmarks%20with%20scale-free%20and%20Erdos-Renyi%20graph%20structures%2C%20and%0Acompetitive%20results%20on%20real-world%20data.%20Moreover%2C%20we%20illustrate%20that%20our%20method%0Aaccurately%20infers%20interventional%20distributions%2C%20which%20allows%20us%20to%20estimate%0Aposterior%20average%20causal%20effects%20and%20many%20other%20causal%20quantities%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14781v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Bayesian%2520Causal%2520Inference%2520via%2520Structural%2520Marginalisation%2520and%250A%2520%2520Autoregressive%2520Orders%26entry.906535625%3DChristian%2520Toth%2520and%2520Christian%2520Knoll%2520and%2520Franz%2520Pernkopf%2520and%2520Robert%2520Peharz%26entry.1292438233%3D%2520%2520Bayesian%2520causal%2520inference%2520%2528BCI%2529%2520naturally%2520incorporates%2520epistemic%2520uncertainty%250Aabout%2520the%2520true%2520causal%2520model%2520into%2520down-stream%2520causal%2520reasoning%2520tasks%2520by%250Aposterior%2520averaging%2520over%2520causal%2520models.%2520However%252C%2520this%2520poses%2520a%2520tremendously%2520hard%250Acomputational%2520problem%2520due%2520to%2520the%2520intractable%2520number%2520of%2520causal%2520structures%2520to%250Amarginalise%2520over.%2520In%2520this%2520work%252C%2520we%2520decompose%2520the%2520structure%2520learning%2520problem%250Ainto%2520inferring%2520%2528i%2529%2520a%2520causal%2520order%2520and%2520%2528ii%2529%2520a%2520parent%2520set%2520for%2520each%2520variable%2520given%250Aa%2520causal%2520order.%2520By%2520limiting%2520the%2520number%2520of%2520parents%2520per%2520variable%252C%2520we%2520can%2520exactly%250Amarginalise%2520over%2520the%2520parent%2520sets%2520in%2520polynomial%2520time%252C%2520which%2520leaves%2520only%2520the%250Acausal%2520order%2520to%2520be%2520marginalised.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520autoregressive%250Amodel%2520over%2520causal%2520orders%2520%2528ARCO%2529%2520learnable%2520with%2520gradient-based%2520methods.%2520Our%250Amethod%2520yields%2520state-of-the-art%2520in%2520structure%2520learning%2520on%2520simulated%2520non-linear%250Aadditive%2520noise%2520benchmarks%2520with%2520scale-free%2520and%2520Erdos-Renyi%2520graph%2520structures%252C%2520and%250Acompetitive%2520results%2520on%2520real-world%2520data.%2520Moreover%252C%2520we%2520illustrate%2520that%2520our%2520method%250Aaccurately%2520infers%2520interventional%2520distributions%252C%2520which%2520allows%2520us%2520to%2520estimate%250Aposterior%2520average%2520causal%2520effects%2520and%2520many%2520other%2520causal%2520quantities%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14781v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Bayesian%20Causal%20Inference%20via%20Structural%20Marginalisation%20and%0A%20%20Autoregressive%20Orders&entry.906535625=Christian%20Toth%20and%20Christian%20Knoll%20and%20Franz%20Pernkopf%20and%20Robert%20Peharz&entry.1292438233=%20%20Bayesian%20causal%20inference%20%28BCI%29%20naturally%20incorporates%20epistemic%20uncertainty%0Aabout%20the%20true%20causal%20model%20into%20down-stream%20causal%20reasoning%20tasks%20by%0Aposterior%20averaging%20over%20causal%20models.%20However%2C%20this%20poses%20a%20tremendously%20hard%0Acomputational%20problem%20due%20to%20the%20intractable%20number%20of%20causal%20structures%20to%0Amarginalise%20over.%20In%20this%20work%2C%20we%20decompose%20the%20structure%20learning%20problem%0Ainto%20inferring%20%28i%29%20a%20causal%20order%20and%20%28ii%29%20a%20parent%20set%20for%20each%20variable%20given%0Aa%20causal%20order.%20By%20limiting%20the%20number%20of%20parents%20per%20variable%2C%20we%20can%20exactly%0Amarginalise%20over%20the%20parent%20sets%20in%20polynomial%20time%2C%20which%20leaves%20only%20the%0Acausal%20order%20to%20be%20marginalised.%20To%20this%20end%2C%20we%20propose%20a%20novel%20autoregressive%0Amodel%20over%20causal%20orders%20%28ARCO%29%20learnable%20with%20gradient-based%20methods.%20Our%0Amethod%20yields%20state-of-the-art%20in%20structure%20learning%20on%20simulated%20non-linear%0Aadditive%20noise%20benchmarks%20with%20scale-free%20and%20Erdos-Renyi%20graph%20structures%2C%20and%0Acompetitive%20results%20on%20real-world%20data.%20Moreover%2C%20we%20illustrate%20that%20our%20method%0Aaccurately%20infers%20interventional%20distributions%2C%20which%20allows%20us%20to%20estimate%0Aposterior%20average%20causal%20effects%20and%20many%20other%20causal%20quantities%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14781v2&entry.124074799=Read"},
{"title": "Invariant Consistency for Knowledge Distillation", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.\n", "link": "http://arxiv.org/abs/2407.11802v1", "date": "2024-07-16", "relevancy": 1.9181, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4772}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Consistency%20for%20Knowledge%20Distillation&body=Title%3A%20Invariant%20Consistency%20for%20Knowledge%20Distillation%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20involves%20transferring%20the%20knowledge%20from%20one%0Aneural%20network%20to%20another%2C%20often%20from%20a%20larger%2C%20well-trained%20model%20%28teacher%29%20to%0Aa%20smaller%2C%20more%20efficient%20model%20%28student%29.%20Traditional%20KD%20methods%20minimize%20the%0AKullback-Leibler%20%28KL%29%20divergence%20between%20the%20probabilistic%20outputs%20of%20the%0Ateacher%20and%20student%20networks.%20However%2C%20this%20approach%20often%20overlooks%20crucial%0Astructural%20knowledge%20embedded%20within%20the%20teacher%27s%20network.%20In%20this%20paper%2C%20we%0Aintroduce%20Invariant%20Consistency%20Distillation%20%28ICD%29%2C%20a%20novel%20methodology%0Adesigned%20to%20enhance%20KD%20by%20ensuring%20that%20the%20student%20model%27s%20representations%20are%0Aconsistent%20with%20those%20of%20the%20teacher.%20Our%20approach%20combines%20contrastive%0Alearning%20with%20an%20explicit%20invariance%20penalty%2C%20capturing%20significantly%20more%0Ainformation%20from%20the%20teacher%27s%20representation%20of%20the%20data.%20Our%20results%20on%0ACIFAR-100%20demonstrate%20that%20ICD%20outperforms%20traditional%20KD%20techniques%20and%0Asurpasses%2013%20state-of-the-art%20methods.%20In%20some%20cases%2C%20the%20student%20model%20even%0Aexceeds%20the%20teacher%20model%20in%20terms%20of%20accuracy.%20Furthermore%2C%20we%20successfully%0Atransfer%20our%20method%20to%20other%20datasets%2C%20including%20Tiny%20ImageNet%20and%20STL-10.%20The%0Acode%20will%20be%20made%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Consistency%2520for%2520Knowledge%2520Distillation%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520involves%2520transferring%2520the%2520knowledge%2520from%2520one%250Aneural%2520network%2520to%2520another%252C%2520often%2520from%2520a%2520larger%252C%2520well-trained%2520model%2520%2528teacher%2529%2520to%250Aa%2520smaller%252C%2520more%2520efficient%2520model%2520%2528student%2529.%2520Traditional%2520KD%2520methods%2520minimize%2520the%250AKullback-Leibler%2520%2528KL%2529%2520divergence%2520between%2520the%2520probabilistic%2520outputs%2520of%2520the%250Ateacher%2520and%2520student%2520networks.%2520However%252C%2520this%2520approach%2520often%2520overlooks%2520crucial%250Astructural%2520knowledge%2520embedded%2520within%2520the%2520teacher%2527s%2520network.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Invariant%2520Consistency%2520Distillation%2520%2528ICD%2529%252C%2520a%2520novel%2520methodology%250Adesigned%2520to%2520enhance%2520KD%2520by%2520ensuring%2520that%2520the%2520student%2520model%2527s%2520representations%2520are%250Aconsistent%2520with%2520those%2520of%2520the%2520teacher.%2520Our%2520approach%2520combines%2520contrastive%250Alearning%2520with%2520an%2520explicit%2520invariance%2520penalty%252C%2520capturing%2520significantly%2520more%250Ainformation%2520from%2520the%2520teacher%2527s%2520representation%2520of%2520the%2520data.%2520Our%2520results%2520on%250ACIFAR-100%2520demonstrate%2520that%2520ICD%2520outperforms%2520traditional%2520KD%2520techniques%2520and%250Asurpasses%252013%2520state-of-the-art%2520methods.%2520In%2520some%2520cases%252C%2520the%2520student%2520model%2520even%250Aexceeds%2520the%2520teacher%2520model%2520in%2520terms%2520of%2520accuracy.%2520Furthermore%252C%2520we%2520successfully%250Atransfer%2520our%2520method%2520to%2520other%2520datasets%252C%2520including%2520Tiny%2520ImageNet%2520and%2520STL-10.%2520The%250Acode%2520will%2520be%2520made%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Consistency%20for%20Knowledge%20Distillation&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20involves%20transferring%20the%20knowledge%20from%20one%0Aneural%20network%20to%20another%2C%20often%20from%20a%20larger%2C%20well-trained%20model%20%28teacher%29%20to%0Aa%20smaller%2C%20more%20efficient%20model%20%28student%29.%20Traditional%20KD%20methods%20minimize%20the%0AKullback-Leibler%20%28KL%29%20divergence%20between%20the%20probabilistic%20outputs%20of%20the%0Ateacher%20and%20student%20networks.%20However%2C%20this%20approach%20often%20overlooks%20crucial%0Astructural%20knowledge%20embedded%20within%20the%20teacher%27s%20network.%20In%20this%20paper%2C%20we%0Aintroduce%20Invariant%20Consistency%20Distillation%20%28ICD%29%2C%20a%20novel%20methodology%0Adesigned%20to%20enhance%20KD%20by%20ensuring%20that%20the%20student%20model%27s%20representations%20are%0Aconsistent%20with%20those%20of%20the%20teacher.%20Our%20approach%20combines%20contrastive%0Alearning%20with%20an%20explicit%20invariance%20penalty%2C%20capturing%20significantly%20more%0Ainformation%20from%20the%20teacher%27s%20representation%20of%20the%20data.%20Our%20results%20on%0ACIFAR-100%20demonstrate%20that%20ICD%20outperforms%20traditional%20KD%20techniques%20and%0Asurpasses%2013%20state-of-the-art%20methods.%20In%20some%20cases%2C%20the%20student%20model%20even%0Aexceeds%20the%20teacher%20model%20in%20terms%20of%20accuracy.%20Furthermore%2C%20we%20successfully%0Atransfer%20our%20method%20to%20other%20datasets%2C%20including%20Tiny%20ImageNet%20and%20STL-10.%20The%0Acode%20will%20be%20made%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11802v1&entry.124074799=Read"},
{"title": "Towards a Benchmark for Causal Business Process Reasoning with LLMs", "author": "Fabiana Fournier and Lior Limonad and Inna Skarbovsky", "abstract": "  Large Language Models (LLMs) are increasingly used for boosting\norganizational efficiency and automating tasks. While not originally designed\nfor complex cognitive processes, recent efforts have further extended to employ\nLLMs in activities such as reasoning, planning, and decision-making. In\nbusiness processes, such abilities could be invaluable for leveraging on the\nmassive corpora LLMs have been trained on for gaining deep understanding of\nsuch processes. In this work, we plant the seeds for the development of a\nbenchmark to assess the ability of LLMs to reason about causal and process\nperspectives of business operations. We refer to this view as\nCausally-augmented Business Processes (BP^C). The core of the benchmark\ncomprises a set of BP^C related situations, a set of questions about these\nsituations, and a set of deductive rules employed to systematically resolve the\nground truth answers to these questions. Also with the power of LLMs, the seed\nis then instantiated into a larger-scale set of domain-specific situations and\nquestions. Reasoning on BP^C is of crucial importance for process interventions\nand process improvement. Our benchmark, accessible at\nhttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possible\nmodalities: testing the performance of any target LLM and training an LLM to\nadvance its capability to reason about BP^C.\n", "link": "http://arxiv.org/abs/2406.05506v2", "date": "2024-07-16", "relevancy": 1.2627, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4401}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.419}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Benchmark%20for%20Causal%20Business%20Process%20Reasoning%20with%20LLMs&body=Title%3A%20Towards%20a%20Benchmark%20for%20Causal%20Business%20Process%20Reasoning%20with%20LLMs%0AAuthor%3A%20Fabiana%20Fournier%20and%20Lior%20Limonad%20and%20Inna%20Skarbovsky%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20boosting%0Aorganizational%20efficiency%20and%20automating%20tasks.%20While%20not%20originally%20designed%0Afor%20complex%20cognitive%20processes%2C%20recent%20efforts%20have%20further%20extended%20to%20employ%0ALLMs%20in%20activities%20such%20as%20reasoning%2C%20planning%2C%20and%20decision-making.%20In%0Abusiness%20processes%2C%20such%20abilities%20could%20be%20invaluable%20for%20leveraging%20on%20the%0Amassive%20corpora%20LLMs%20have%20been%20trained%20on%20for%20gaining%20deep%20understanding%20of%0Asuch%20processes.%20In%20this%20work%2C%20we%20plant%20the%20seeds%20for%20the%20development%20of%20a%0Abenchmark%20to%20assess%20the%20ability%20of%20LLMs%20to%20reason%20about%20causal%20and%20process%0Aperspectives%20of%20business%20operations.%20We%20refer%20to%20this%20view%20as%0ACausally-augmented%20Business%20Processes%20%28BP%5EC%29.%20The%20core%20of%20the%20benchmark%0Acomprises%20a%20set%20of%20BP%5EC%20related%20situations%2C%20a%20set%20of%20questions%20about%20these%0Asituations%2C%20and%20a%20set%20of%20deductive%20rules%20employed%20to%20systematically%20resolve%20the%0Aground%20truth%20answers%20to%20these%20questions.%20Also%20with%20the%20power%20of%20LLMs%2C%20the%20seed%0Ais%20then%20instantiated%20into%20a%20larger-scale%20set%20of%20domain-specific%20situations%20and%0Aquestions.%20Reasoning%20on%20BP%5EC%20is%20of%20crucial%20importance%20for%20process%20interventions%0Aand%20process%20improvement.%20Our%20benchmark%2C%20accessible%20at%0Ahttps%3A//huggingface.co/datasets/ibm/BPC%2C%20can%20be%20used%20in%20one%20of%20two%20possible%0Amodalities%3A%20testing%20the%20performance%20of%20any%20target%20LLM%20and%20training%20an%20LLM%20to%0Aadvance%20its%20capability%20to%20reason%20about%20BP%5EC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Benchmark%2520for%2520Causal%2520Business%2520Process%2520Reasoning%2520with%2520LLMs%26entry.906535625%3DFabiana%2520Fournier%2520and%2520Lior%2520Limonad%2520and%2520Inna%2520Skarbovsky%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520boosting%250Aorganizational%2520efficiency%2520and%2520automating%2520tasks.%2520While%2520not%2520originally%2520designed%250Afor%2520complex%2520cognitive%2520processes%252C%2520recent%2520efforts%2520have%2520further%2520extended%2520to%2520employ%250ALLMs%2520in%2520activities%2520such%2520as%2520reasoning%252C%2520planning%252C%2520and%2520decision-making.%2520In%250Abusiness%2520processes%252C%2520such%2520abilities%2520could%2520be%2520invaluable%2520for%2520leveraging%2520on%2520the%250Amassive%2520corpora%2520LLMs%2520have%2520been%2520trained%2520on%2520for%2520gaining%2520deep%2520understanding%2520of%250Asuch%2520processes.%2520In%2520this%2520work%252C%2520we%2520plant%2520the%2520seeds%2520for%2520the%2520development%2520of%2520a%250Abenchmark%2520to%2520assess%2520the%2520ability%2520of%2520LLMs%2520to%2520reason%2520about%2520causal%2520and%2520process%250Aperspectives%2520of%2520business%2520operations.%2520We%2520refer%2520to%2520this%2520view%2520as%250ACausally-augmented%2520Business%2520Processes%2520%2528BP%255EC%2529.%2520The%2520core%2520of%2520the%2520benchmark%250Acomprises%2520a%2520set%2520of%2520BP%255EC%2520related%2520situations%252C%2520a%2520set%2520of%2520questions%2520about%2520these%250Asituations%252C%2520and%2520a%2520set%2520of%2520deductive%2520rules%2520employed%2520to%2520systematically%2520resolve%2520the%250Aground%2520truth%2520answers%2520to%2520these%2520questions.%2520Also%2520with%2520the%2520power%2520of%2520LLMs%252C%2520the%2520seed%250Ais%2520then%2520instantiated%2520into%2520a%2520larger-scale%2520set%2520of%2520domain-specific%2520situations%2520and%250Aquestions.%2520Reasoning%2520on%2520BP%255EC%2520is%2520of%2520crucial%2520importance%2520for%2520process%2520interventions%250Aand%2520process%2520improvement.%2520Our%2520benchmark%252C%2520accessible%2520at%250Ahttps%253A//huggingface.co/datasets/ibm/BPC%252C%2520can%2520be%2520used%2520in%2520one%2520of%2520two%2520possible%250Amodalities%253A%2520testing%2520the%2520performance%2520of%2520any%2520target%2520LLM%2520and%2520training%2520an%2520LLM%2520to%250Aadvance%2520its%2520capability%2520to%2520reason%2520about%2520BP%255EC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Benchmark%20for%20Causal%20Business%20Process%20Reasoning%20with%20LLMs&entry.906535625=Fabiana%20Fournier%20and%20Lior%20Limonad%20and%20Inna%20Skarbovsky&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20boosting%0Aorganizational%20efficiency%20and%20automating%20tasks.%20While%20not%20originally%20designed%0Afor%20complex%20cognitive%20processes%2C%20recent%20efforts%20have%20further%20extended%20to%20employ%0ALLMs%20in%20activities%20such%20as%20reasoning%2C%20planning%2C%20and%20decision-making.%20In%0Abusiness%20processes%2C%20such%20abilities%20could%20be%20invaluable%20for%20leveraging%20on%20the%0Amassive%20corpora%20LLMs%20have%20been%20trained%20on%20for%20gaining%20deep%20understanding%20of%0Asuch%20processes.%20In%20this%20work%2C%20we%20plant%20the%20seeds%20for%20the%20development%20of%20a%0Abenchmark%20to%20assess%20the%20ability%20of%20LLMs%20to%20reason%20about%20causal%20and%20process%0Aperspectives%20of%20business%20operations.%20We%20refer%20to%20this%20view%20as%0ACausally-augmented%20Business%20Processes%20%28BP%5EC%29.%20The%20core%20of%20the%20benchmark%0Acomprises%20a%20set%20of%20BP%5EC%20related%20situations%2C%20a%20set%20of%20questions%20about%20these%0Asituations%2C%20and%20a%20set%20of%20deductive%20rules%20employed%20to%20systematically%20resolve%20the%0Aground%20truth%20answers%20to%20these%20questions.%20Also%20with%20the%20power%20of%20LLMs%2C%20the%20seed%0Ais%20then%20instantiated%20into%20a%20larger-scale%20set%20of%20domain-specific%20situations%20and%0Aquestions.%20Reasoning%20on%20BP%5EC%20is%20of%20crucial%20importance%20for%20process%20interventions%0Aand%20process%20improvement.%20Our%20benchmark%2C%20accessible%20at%0Ahttps%3A//huggingface.co/datasets/ibm/BPC%2C%20can%20be%20used%20in%20one%20of%20two%20possible%0Amodalities%3A%20testing%20the%20performance%20of%20any%20target%20LLM%20and%20training%20an%20LLM%20to%0Aadvance%20its%20capability%20to%20reason%20about%20BP%5EC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05506v2&entry.124074799=Read"},
{"title": "Does Refusal Training in LLMs Generalize to the Past Tense?", "author": "Maksym Andriushchenko and Nicolas Flammarion", "abstract": "  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,\nGemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a\nreformulation model. For example, the success rate of this simple attack on\nGPT-4o increases from 1% using direct requests to 88% using 20 past tense\nreformulation attempts on harmful requests from JailbreakBench with GPT-4 as a\njailbreak judge. Interestingly, we also find that reformulations in the future\ntense are less effective, suggesting that refusal guardrails tend to consider\npast historical questions more benign than hypothetical future questions.\nMoreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending\nagainst past reformulations is feasible when past tense examples are explicitly\nincluded in the fine-tuning data. Overall, our findings highlight that the\nwidely used alignment techniques -- such as SFT, RLHF, and adversarial training\n-- employed to align the studied models can be brittle and do not always\ngeneralize as intended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.\n", "link": "http://arxiv.org/abs/2407.11969v1", "date": "2024-07-16", "relevancy": 1.5834, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Refusal%20Training%20in%20LLMs%20Generalize%20to%20the%20Past%20Tense%3F&body=Title%3A%20Does%20Refusal%20Training%20in%20LLMs%20Generalize%20to%20the%20Past%20Tense%3F%0AAuthor%3A%20Maksym%20Andriushchenko%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20Refusal%20training%20is%20widely%20used%20to%20prevent%20LLMs%20from%20generating%20harmful%2C%0Aundesirable%2C%20or%20illegal%20outputs.%20We%20reveal%20a%20curious%20generalization%20gap%20in%20the%0Acurrent%20refusal%20training%20approaches%3A%20simply%20reformulating%20a%20harmful%20request%20in%0Athe%20past%20tense%20%28e.g.%2C%20%22How%20to%20make%20a%20Molotov%20cocktail%3F%22%20to%20%22How%20did%20people%20make%0Aa%20Molotov%20cocktail%3F%22%29%20is%20often%20sufficient%20to%20jailbreak%20many%20state-of-the-art%0ALLMs.%20We%20systematically%20evaluate%20this%20method%20on%20Llama-3%208B%2C%20GPT-3.5%20Turbo%2C%0AGemma-2%209B%2C%20Phi-3-Mini%2C%20GPT-4o%2C%20and%20R2D2%20models%20using%20GPT-3.5%20Turbo%20as%20a%0Areformulation%20model.%20For%20example%2C%20the%20success%20rate%20of%20this%20simple%20attack%20on%0AGPT-4o%20increases%20from%201%25%20using%20direct%20requests%20to%2088%25%20using%2020%20past%20tense%0Areformulation%20attempts%20on%20harmful%20requests%20from%20JailbreakBench%20with%20GPT-4%20as%20a%0Ajailbreak%20judge.%20Interestingly%2C%20we%20also%20find%20that%20reformulations%20in%20the%20future%0Atense%20are%20less%20effective%2C%20suggesting%20that%20refusal%20guardrails%20tend%20to%20consider%0Apast%20historical%20questions%20more%20benign%20than%20hypothetical%20future%20questions.%0AMoreover%2C%20our%20experiments%20on%20fine-tuning%20GPT-3.5%20Turbo%20show%20that%20defending%0Aagainst%20past%20reformulations%20is%20feasible%20when%20past%20tense%20examples%20are%20explicitly%0Aincluded%20in%20the%20fine-tuning%20data.%20Overall%2C%20our%20findings%20highlight%20that%20the%0Awidely%20used%20alignment%20techniques%20--%20such%20as%20SFT%2C%20RLHF%2C%20and%20adversarial%20training%0A--%20employed%20to%20align%20the%20studied%20models%20can%20be%20brittle%20and%20do%20not%20always%0Ageneralize%20as%20intended.%20We%20provide%20code%20and%20jailbreak%20artifacts%20at%0Ahttps%3A//github.com/tml-epfl/llm-past-tense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Refusal%2520Training%2520in%2520LLMs%2520Generalize%2520to%2520the%2520Past%2520Tense%253F%26entry.906535625%3DMaksym%2520Andriushchenko%2520and%2520Nicolas%2520Flammarion%26entry.1292438233%3D%2520%2520Refusal%2520training%2520is%2520widely%2520used%2520to%2520prevent%2520LLMs%2520from%2520generating%2520harmful%252C%250Aundesirable%252C%2520or%2520illegal%2520outputs.%2520We%2520reveal%2520a%2520curious%2520generalization%2520gap%2520in%2520the%250Acurrent%2520refusal%2520training%2520approaches%253A%2520simply%2520reformulating%2520a%2520harmful%2520request%2520in%250Athe%2520past%2520tense%2520%2528e.g.%252C%2520%2522How%2520to%2520make%2520a%2520Molotov%2520cocktail%253F%2522%2520to%2520%2522How%2520did%2520people%2520make%250Aa%2520Molotov%2520cocktail%253F%2522%2529%2520is%2520often%2520sufficient%2520to%2520jailbreak%2520many%2520state-of-the-art%250ALLMs.%2520We%2520systematically%2520evaluate%2520this%2520method%2520on%2520Llama-3%25208B%252C%2520GPT-3.5%2520Turbo%252C%250AGemma-2%25209B%252C%2520Phi-3-Mini%252C%2520GPT-4o%252C%2520and%2520R2D2%2520models%2520using%2520GPT-3.5%2520Turbo%2520as%2520a%250Areformulation%2520model.%2520For%2520example%252C%2520the%2520success%2520rate%2520of%2520this%2520simple%2520attack%2520on%250AGPT-4o%2520increases%2520from%25201%2525%2520using%2520direct%2520requests%2520to%252088%2525%2520using%252020%2520past%2520tense%250Areformulation%2520attempts%2520on%2520harmful%2520requests%2520from%2520JailbreakBench%2520with%2520GPT-4%2520as%2520a%250Ajailbreak%2520judge.%2520Interestingly%252C%2520we%2520also%2520find%2520that%2520reformulations%2520in%2520the%2520future%250Atense%2520are%2520less%2520effective%252C%2520suggesting%2520that%2520refusal%2520guardrails%2520tend%2520to%2520consider%250Apast%2520historical%2520questions%2520more%2520benign%2520than%2520hypothetical%2520future%2520questions.%250AMoreover%252C%2520our%2520experiments%2520on%2520fine-tuning%2520GPT-3.5%2520Turbo%2520show%2520that%2520defending%250Aagainst%2520past%2520reformulations%2520is%2520feasible%2520when%2520past%2520tense%2520examples%2520are%2520explicitly%250Aincluded%2520in%2520the%2520fine-tuning%2520data.%2520Overall%252C%2520our%2520findings%2520highlight%2520that%2520the%250Awidely%2520used%2520alignment%2520techniques%2520--%2520such%2520as%2520SFT%252C%2520RLHF%252C%2520and%2520adversarial%2520training%250A--%2520employed%2520to%2520align%2520the%2520studied%2520models%2520can%2520be%2520brittle%2520and%2520do%2520not%2520always%250Ageneralize%2520as%2520intended.%2520We%2520provide%2520code%2520and%2520jailbreak%2520artifacts%2520at%250Ahttps%253A//github.com/tml-epfl/llm-past-tense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Refusal%20Training%20in%20LLMs%20Generalize%20to%20the%20Past%20Tense%3F&entry.906535625=Maksym%20Andriushchenko%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20Refusal%20training%20is%20widely%20used%20to%20prevent%20LLMs%20from%20generating%20harmful%2C%0Aundesirable%2C%20or%20illegal%20outputs.%20We%20reveal%20a%20curious%20generalization%20gap%20in%20the%0Acurrent%20refusal%20training%20approaches%3A%20simply%20reformulating%20a%20harmful%20request%20in%0Athe%20past%20tense%20%28e.g.%2C%20%22How%20to%20make%20a%20Molotov%20cocktail%3F%22%20to%20%22How%20did%20people%20make%0Aa%20Molotov%20cocktail%3F%22%29%20is%20often%20sufficient%20to%20jailbreak%20many%20state-of-the-art%0ALLMs.%20We%20systematically%20evaluate%20this%20method%20on%20Llama-3%208B%2C%20GPT-3.5%20Turbo%2C%0AGemma-2%209B%2C%20Phi-3-Mini%2C%20GPT-4o%2C%20and%20R2D2%20models%20using%20GPT-3.5%20Turbo%20as%20a%0Areformulation%20model.%20For%20example%2C%20the%20success%20rate%20of%20this%20simple%20attack%20on%0AGPT-4o%20increases%20from%201%25%20using%20direct%20requests%20to%2088%25%20using%2020%20past%20tense%0Areformulation%20attempts%20on%20harmful%20requests%20from%20JailbreakBench%20with%20GPT-4%20as%20a%0Ajailbreak%20judge.%20Interestingly%2C%20we%20also%20find%20that%20reformulations%20in%20the%20future%0Atense%20are%20less%20effective%2C%20suggesting%20that%20refusal%20guardrails%20tend%20to%20consider%0Apast%20historical%20questions%20more%20benign%20than%20hypothetical%20future%20questions.%0AMoreover%2C%20our%20experiments%20on%20fine-tuning%20GPT-3.5%20Turbo%20show%20that%20defending%0Aagainst%20past%20reformulations%20is%20feasible%20when%20past%20tense%20examples%20are%20explicitly%0Aincluded%20in%20the%20fine-tuning%20data.%20Overall%2C%20our%20findings%20highlight%20that%20the%0Awidely%20used%20alignment%20techniques%20--%20such%20as%20SFT%2C%20RLHF%2C%20and%20adversarial%20training%0A--%20employed%20to%20align%20the%20studied%20models%20can%20be%20brittle%20and%20do%20not%20always%0Ageneralize%20as%20intended.%20We%20provide%20code%20and%20jailbreak%20artifacts%20at%0Ahttps%3A//github.com/tml-epfl/llm-past-tense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11969v1&entry.124074799=Read"},
{"title": "Local Feature Selection without Label or Feature Leakage for\n  Interpretable Machine Learning Predictions", "author": "Harrie Oosterhuis and Lijun Lyu and Avishek Anand", "abstract": "  Local feature selection in machine learning provides instance-specific\nexplanations by focusing on the most relevant features for each prediction,\nenhancing the interpretability of complex models. However, such methods tend to\nproduce misleading explanations by encoding additional information in their\nselections. In this work, we attribute the problem of misleading selections by\nformalizing the concepts of label and feature leakage. We rigorously derive the\nnecessary and sufficient conditions under which we can guarantee no leakage,\nand show existing methods do not meet these conditions. Furthermore, we propose\nthe first local feature selection method that is proven to have no leakage\ncalled SUWR. Our experimental results indicate that SUWR is less prone to\noverfitting and combines state-of-the-art predictive performance with high\nfeature-selection sparsity. Our generic and easily extendable formal approach\nprovides a strong theoretical basis for future work on interpretability with\nreliable explanations.\n", "link": "http://arxiv.org/abs/2407.11778v1", "date": "2024-07-16", "relevancy": 1.8624, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4705}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Feature%20Selection%20without%20Label%20or%20Feature%20Leakage%20for%0A%20%20Interpretable%20Machine%20Learning%20Predictions&body=Title%3A%20Local%20Feature%20Selection%20without%20Label%20or%20Feature%20Leakage%20for%0A%20%20Interpretable%20Machine%20Learning%20Predictions%0AAuthor%3A%20Harrie%20Oosterhuis%20and%20Lijun%20Lyu%20and%20Avishek%20Anand%0AAbstract%3A%20%20%20Local%20feature%20selection%20in%20machine%20learning%20provides%20instance-specific%0Aexplanations%20by%20focusing%20on%20the%20most%20relevant%20features%20for%20each%20prediction%2C%0Aenhancing%20the%20interpretability%20of%20complex%20models.%20However%2C%20such%20methods%20tend%20to%0Aproduce%20misleading%20explanations%20by%20encoding%20additional%20information%20in%20their%0Aselections.%20In%20this%20work%2C%20we%20attribute%20the%20problem%20of%20misleading%20selections%20by%0Aformalizing%20the%20concepts%20of%20label%20and%20feature%20leakage.%20We%20rigorously%20derive%20the%0Anecessary%20and%20sufficient%20conditions%20under%20which%20we%20can%20guarantee%20no%20leakage%2C%0Aand%20show%20existing%20methods%20do%20not%20meet%20these%20conditions.%20Furthermore%2C%20we%20propose%0Athe%20first%20local%20feature%20selection%20method%20that%20is%20proven%20to%20have%20no%20leakage%0Acalled%20SUWR.%20Our%20experimental%20results%20indicate%20that%20SUWR%20is%20less%20prone%20to%0Aoverfitting%20and%20combines%20state-of-the-art%20predictive%20performance%20with%20high%0Afeature-selection%20sparsity.%20Our%20generic%20and%20easily%20extendable%20formal%20approach%0Aprovides%20a%20strong%20theoretical%20basis%20for%20future%20work%20on%20interpretability%20with%0Areliable%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Feature%2520Selection%2520without%2520Label%2520or%2520Feature%2520Leakage%2520for%250A%2520%2520Interpretable%2520Machine%2520Learning%2520Predictions%26entry.906535625%3DHarrie%2520Oosterhuis%2520and%2520Lijun%2520Lyu%2520and%2520Avishek%2520Anand%26entry.1292438233%3D%2520%2520Local%2520feature%2520selection%2520in%2520machine%2520learning%2520provides%2520instance-specific%250Aexplanations%2520by%2520focusing%2520on%2520the%2520most%2520relevant%2520features%2520for%2520each%2520prediction%252C%250Aenhancing%2520the%2520interpretability%2520of%2520complex%2520models.%2520However%252C%2520such%2520methods%2520tend%2520to%250Aproduce%2520misleading%2520explanations%2520by%2520encoding%2520additional%2520information%2520in%2520their%250Aselections.%2520In%2520this%2520work%252C%2520we%2520attribute%2520the%2520problem%2520of%2520misleading%2520selections%2520by%250Aformalizing%2520the%2520concepts%2520of%2520label%2520and%2520feature%2520leakage.%2520We%2520rigorously%2520derive%2520the%250Anecessary%2520and%2520sufficient%2520conditions%2520under%2520which%2520we%2520can%2520guarantee%2520no%2520leakage%252C%250Aand%2520show%2520existing%2520methods%2520do%2520not%2520meet%2520these%2520conditions.%2520Furthermore%252C%2520we%2520propose%250Athe%2520first%2520local%2520feature%2520selection%2520method%2520that%2520is%2520proven%2520to%2520have%2520no%2520leakage%250Acalled%2520SUWR.%2520Our%2520experimental%2520results%2520indicate%2520that%2520SUWR%2520is%2520less%2520prone%2520to%250Aoverfitting%2520and%2520combines%2520state-of-the-art%2520predictive%2520performance%2520with%2520high%250Afeature-selection%2520sparsity.%2520Our%2520generic%2520and%2520easily%2520extendable%2520formal%2520approach%250Aprovides%2520a%2520strong%2520theoretical%2520basis%2520for%2520future%2520work%2520on%2520interpretability%2520with%250Areliable%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Feature%20Selection%20without%20Label%20or%20Feature%20Leakage%20for%0A%20%20Interpretable%20Machine%20Learning%20Predictions&entry.906535625=Harrie%20Oosterhuis%20and%20Lijun%20Lyu%20and%20Avishek%20Anand&entry.1292438233=%20%20Local%20feature%20selection%20in%20machine%20learning%20provides%20instance-specific%0Aexplanations%20by%20focusing%20on%20the%20most%20relevant%20features%20for%20each%20prediction%2C%0Aenhancing%20the%20interpretability%20of%20complex%20models.%20However%2C%20such%20methods%20tend%20to%0Aproduce%20misleading%20explanations%20by%20encoding%20additional%20information%20in%20their%0Aselections.%20In%20this%20work%2C%20we%20attribute%20the%20problem%20of%20misleading%20selections%20by%0Aformalizing%20the%20concepts%20of%20label%20and%20feature%20leakage.%20We%20rigorously%20derive%20the%0Anecessary%20and%20sufficient%20conditions%20under%20which%20we%20can%20guarantee%20no%20leakage%2C%0Aand%20show%20existing%20methods%20do%20not%20meet%20these%20conditions.%20Furthermore%2C%20we%20propose%0Athe%20first%20local%20feature%20selection%20method%20that%20is%20proven%20to%20have%20no%20leakage%0Acalled%20SUWR.%20Our%20experimental%20results%20indicate%20that%20SUWR%20is%20less%20prone%20to%0Aoverfitting%20and%20combines%20state-of-the-art%20predictive%20performance%20with%20high%0Afeature-selection%20sparsity.%20Our%20generic%20and%20easily%20extendable%20formal%20approach%0Aprovides%20a%20strong%20theoretical%20basis%20for%20future%20work%20on%20interpretability%20with%0Areliable%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11778v1&entry.124074799=Read"},
{"title": "Simplifying the Theory on Over-Smoothing", "author": "Andreas Roth", "abstract": "  Graph convolutions have gained popularity due to their ability to efficiently\noperate on data with an irregular geometric structure. However, graph\nconvolutions cause over-smoothing, which refers to representations becoming\nmore similar with increased depth. However, many different definitions and\nintuitions currently coexist, leading to research efforts focusing on\nincompatible directions. This paper attempts to align these directions by\nshowing that over-smoothing is merely a special case of power iteration. This\ngreatly simplifies the existing theory on over-smoothing, making it more\naccessible. Based on the theory, we provide a novel comprehensive definition of\nrank collapse as a generalized form of over-smoothing and introduce the\nrank-one distance as a corresponding metric. Our empirical evaluation of 14\ncommonly used methods shows that more models than were previously known suffer\nfrom this issue.\n", "link": "http://arxiv.org/abs/2407.11876v1", "date": "2024-07-16", "relevancy": 1.9443, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4991}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20the%20Theory%20on%20Over-Smoothing&body=Title%3A%20Simplifying%20the%20Theory%20on%20Over-Smoothing%0AAuthor%3A%20Andreas%20Roth%0AAbstract%3A%20%20%20Graph%20convolutions%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20efficiently%0Aoperate%20on%20data%20with%20an%20irregular%20geometric%20structure.%20However%2C%20graph%0Aconvolutions%20cause%20over-smoothing%2C%20which%20refers%20to%20representations%20becoming%0Amore%20similar%20with%20increased%20depth.%20However%2C%20many%20different%20definitions%20and%0Aintuitions%20currently%20coexist%2C%20leading%20to%20research%20efforts%20focusing%20on%0Aincompatible%20directions.%20This%20paper%20attempts%20to%20align%20these%20directions%20by%0Ashowing%20that%20over-smoothing%20is%20merely%20a%20special%20case%20of%20power%20iteration.%20This%0Agreatly%20simplifies%20the%20existing%20theory%20on%20over-smoothing%2C%20making%20it%20more%0Aaccessible.%20Based%20on%20the%20theory%2C%20we%20provide%20a%20novel%20comprehensive%20definition%20of%0Arank%20collapse%20as%20a%20generalized%20form%20of%20over-smoothing%20and%20introduce%20the%0Arank-one%20distance%20as%20a%20corresponding%20metric.%20Our%20empirical%20evaluation%20of%2014%0Acommonly%20used%20methods%20shows%20that%20more%20models%20than%20were%20previously%20known%20suffer%0Afrom%20this%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520the%2520Theory%2520on%2520Over-Smoothing%26entry.906535625%3DAndreas%2520Roth%26entry.1292438233%3D%2520%2520Graph%2520convolutions%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520ability%2520to%2520efficiently%250Aoperate%2520on%2520data%2520with%2520an%2520irregular%2520geometric%2520structure.%2520However%252C%2520graph%250Aconvolutions%2520cause%2520over-smoothing%252C%2520which%2520refers%2520to%2520representations%2520becoming%250Amore%2520similar%2520with%2520increased%2520depth.%2520However%252C%2520many%2520different%2520definitions%2520and%250Aintuitions%2520currently%2520coexist%252C%2520leading%2520to%2520research%2520efforts%2520focusing%2520on%250Aincompatible%2520directions.%2520This%2520paper%2520attempts%2520to%2520align%2520these%2520directions%2520by%250Ashowing%2520that%2520over-smoothing%2520is%2520merely%2520a%2520special%2520case%2520of%2520power%2520iteration.%2520This%250Agreatly%2520simplifies%2520the%2520existing%2520theory%2520on%2520over-smoothing%252C%2520making%2520it%2520more%250Aaccessible.%2520Based%2520on%2520the%2520theory%252C%2520we%2520provide%2520a%2520novel%2520comprehensive%2520definition%2520of%250Arank%2520collapse%2520as%2520a%2520generalized%2520form%2520of%2520over-smoothing%2520and%2520introduce%2520the%250Arank-one%2520distance%2520as%2520a%2520corresponding%2520metric.%2520Our%2520empirical%2520evaluation%2520of%252014%250Acommonly%2520used%2520methods%2520shows%2520that%2520more%2520models%2520than%2520were%2520previously%2520known%2520suffer%250Afrom%2520this%2520issue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20the%20Theory%20on%20Over-Smoothing&entry.906535625=Andreas%20Roth&entry.1292438233=%20%20Graph%20convolutions%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20efficiently%0Aoperate%20on%20data%20with%20an%20irregular%20geometric%20structure.%20However%2C%20graph%0Aconvolutions%20cause%20over-smoothing%2C%20which%20refers%20to%20representations%20becoming%0Amore%20similar%20with%20increased%20depth.%20However%2C%20many%20different%20definitions%20and%0Aintuitions%20currently%20coexist%2C%20leading%20to%20research%20efforts%20focusing%20on%0Aincompatible%20directions.%20This%20paper%20attempts%20to%20align%20these%20directions%20by%0Ashowing%20that%20over-smoothing%20is%20merely%20a%20special%20case%20of%20power%20iteration.%20This%0Agreatly%20simplifies%20the%20existing%20theory%20on%20over-smoothing%2C%20making%20it%20more%0Aaccessible.%20Based%20on%20the%20theory%2C%20we%20provide%20a%20novel%20comprehensive%20definition%20of%0Arank%20collapse%20as%20a%20generalized%20form%20of%20over-smoothing%20and%20introduce%20the%0Arank-one%20distance%20as%20a%20corresponding%20metric.%20Our%20empirical%20evaluation%20of%2014%0Acommonly%20used%20methods%20shows%20that%20more%20models%20than%20were%20previously%20known%20suffer%0Afrom%20this%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11876v1&entry.124074799=Read"},
{"title": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games", "author": "Nathan Herr and Fernando Acero and Roberta Raileanu and Mar\u00eda P\u00e9rez-Ortiz and Zhibin Li", "abstract": "  Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic abilities remain largely unexplored. Game theory\nprovides a good framework for assessing the decision-making abilities of LLMs\nin interactions with other agents. Although prior studies have shown that LLMs\ncan solve these tasks with carefully curated prompts, they fail when the\nproblem setting or prompt changes. In this work we investigate LLMs' behaviour\nin strategic games, Stag Hunt and Prisoner Dilemma, analyzing performance\nvariations under different settings and prompts. Our results show that the\ntested state-of-the-art LLMs exhibit at least one of the following systematic\nbiases: (1) positional bias, (2) payoff bias, or (3) behavioural bias.\nSubsequently, we observed that the LLMs' performance drops when the game\nconfiguration is misaligned with the affecting biases. Performance is assessed\nbased on the selection of the correct action, one which agrees with the\nprompted preferred behaviours of both players. Alignment refers to whether the\nLLM's bias aligns with the correct action. For example, GPT-4o's average\nperformance drops by 34% when misaligned. Additionally, the current trend of\n\"bigger and newer is better\" does not hold for the above, where GPT-4o (the\ncurrent best-performing LLM) suffers the most substantial performance drop.\nLastly, we note that while chain-of-thought prompting does reduce the effect of\nthe biases on most models, it is far from solving the problem at the\nfundamental level.\n", "link": "http://arxiv.org/abs/2407.04467v2", "date": "2024-07-16", "relevancy": 0.8896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4542}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games&body=Title%3A%20Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games%0AAuthor%3A%20Nathan%20Herr%20and%20Fernando%20Acero%20and%20Roberta%20Raileanu%20and%20Mar%C3%ADa%20P%C3%A9rez-Ortiz%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20used%20in%20real-world%0Asettings%2C%20yet%20their%20strategic%20abilities%20remain%20largely%20unexplored.%20Game%20theory%0Aprovides%20a%20good%20framework%20for%20assessing%20the%20decision-making%20abilities%20of%20LLMs%0Ain%20interactions%20with%20other%20agents.%20Although%20prior%20studies%20have%20shown%20that%20LLMs%0Acan%20solve%20these%20tasks%20with%20carefully%20curated%20prompts%2C%20they%20fail%20when%20the%0Aproblem%20setting%20or%20prompt%20changes.%20In%20this%20work%20we%20investigate%20LLMs%27%20behaviour%0Ain%20strategic%20games%2C%20Stag%20Hunt%20and%20Prisoner%20Dilemma%2C%20analyzing%20performance%0Avariations%20under%20different%20settings%20and%20prompts.%20Our%20results%20show%20that%20the%0Atested%20state-of-the-art%20LLMs%20exhibit%20at%20least%20one%20of%20the%20following%20systematic%0Abiases%3A%20%281%29%20positional%20bias%2C%20%282%29%20payoff%20bias%2C%20or%20%283%29%20behavioural%20bias.%0ASubsequently%2C%20we%20observed%20that%20the%20LLMs%27%20performance%20drops%20when%20the%20game%0Aconfiguration%20is%20misaligned%20with%20the%20affecting%20biases.%20Performance%20is%20assessed%0Abased%20on%20the%20selection%20of%20the%20correct%20action%2C%20one%20which%20agrees%20with%20the%0Aprompted%20preferred%20behaviours%20of%20both%20players.%20Alignment%20refers%20to%20whether%20the%0ALLM%27s%20bias%20aligns%20with%20the%20correct%20action.%20For%20example%2C%20GPT-4o%27s%20average%0Aperformance%20drops%20by%2034%25%20when%20misaligned.%20Additionally%2C%20the%20current%20trend%20of%0A%22bigger%20and%20newer%20is%20better%22%20does%20not%20hold%20for%20the%20above%2C%20where%20GPT-4o%20%28the%0Acurrent%20best-performing%20LLM%29%20suffers%20the%20most%20substantial%20performance%20drop.%0ALastly%2C%20we%20note%20that%20while%20chain-of-thought%20prompting%20does%20reduce%20the%20effect%20of%0Athe%20biases%20on%20most%20models%2C%20it%20is%20far%20from%20solving%20the%20problem%20at%20the%0Afundamental%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Strategic%2520Decision%2520Makers%253F%2520A%2520Study%2520of%250A%2520%2520Performance%2520and%2520Bias%2520in%2520Two-Player%2520Non-Zero-Sum%2520Games%26entry.906535625%3DNathan%2520Herr%2520and%2520Fernando%2520Acero%2520and%2520Roberta%2520Raileanu%2520and%2520Mar%25C3%25ADa%2520P%25C3%25A9rez-Ortiz%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520increasingly%2520used%2520in%2520real-world%250Asettings%252C%2520yet%2520their%2520strategic%2520abilities%2520remain%2520largely%2520unexplored.%2520Game%2520theory%250Aprovides%2520a%2520good%2520framework%2520for%2520assessing%2520the%2520decision-making%2520abilities%2520of%2520LLMs%250Ain%2520interactions%2520with%2520other%2520agents.%2520Although%2520prior%2520studies%2520have%2520shown%2520that%2520LLMs%250Acan%2520solve%2520these%2520tasks%2520with%2520carefully%2520curated%2520prompts%252C%2520they%2520fail%2520when%2520the%250Aproblem%2520setting%2520or%2520prompt%2520changes.%2520In%2520this%2520work%2520we%2520investigate%2520LLMs%2527%2520behaviour%250Ain%2520strategic%2520games%252C%2520Stag%2520Hunt%2520and%2520Prisoner%2520Dilemma%252C%2520analyzing%2520performance%250Avariations%2520under%2520different%2520settings%2520and%2520prompts.%2520Our%2520results%2520show%2520that%2520the%250Atested%2520state-of-the-art%2520LLMs%2520exhibit%2520at%2520least%2520one%2520of%2520the%2520following%2520systematic%250Abiases%253A%2520%25281%2529%2520positional%2520bias%252C%2520%25282%2529%2520payoff%2520bias%252C%2520or%2520%25283%2529%2520behavioural%2520bias.%250ASubsequently%252C%2520we%2520observed%2520that%2520the%2520LLMs%2527%2520performance%2520drops%2520when%2520the%2520game%250Aconfiguration%2520is%2520misaligned%2520with%2520the%2520affecting%2520biases.%2520Performance%2520is%2520assessed%250Abased%2520on%2520the%2520selection%2520of%2520the%2520correct%2520action%252C%2520one%2520which%2520agrees%2520with%2520the%250Aprompted%2520preferred%2520behaviours%2520of%2520both%2520players.%2520Alignment%2520refers%2520to%2520whether%2520the%250ALLM%2527s%2520bias%2520aligns%2520with%2520the%2520correct%2520action.%2520For%2520example%252C%2520GPT-4o%2527s%2520average%250Aperformance%2520drops%2520by%252034%2525%2520when%2520misaligned.%2520Additionally%252C%2520the%2520current%2520trend%2520of%250A%2522bigger%2520and%2520newer%2520is%2520better%2522%2520does%2520not%2520hold%2520for%2520the%2520above%252C%2520where%2520GPT-4o%2520%2528the%250Acurrent%2520best-performing%2520LLM%2529%2520suffers%2520the%2520most%2520substantial%2520performance%2520drop.%250ALastly%252C%2520we%2520note%2520that%2520while%2520chain-of-thought%2520prompting%2520does%2520reduce%2520the%2520effect%2520of%250Athe%2520biases%2520on%2520most%2520models%252C%2520it%2520is%2520far%2520from%2520solving%2520the%2520problem%2520at%2520the%250Afundamental%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games&entry.906535625=Nathan%20Herr%20and%20Fernando%20Acero%20and%20Roberta%20Raileanu%20and%20Mar%C3%ADa%20P%C3%A9rez-Ortiz%20and%20Zhibin%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20used%20in%20real-world%0Asettings%2C%20yet%20their%20strategic%20abilities%20remain%20largely%20unexplored.%20Game%20theory%0Aprovides%20a%20good%20framework%20for%20assessing%20the%20decision-making%20abilities%20of%20LLMs%0Ain%20interactions%20with%20other%20agents.%20Although%20prior%20studies%20have%20shown%20that%20LLMs%0Acan%20solve%20these%20tasks%20with%20carefully%20curated%20prompts%2C%20they%20fail%20when%20the%0Aproblem%20setting%20or%20prompt%20changes.%20In%20this%20work%20we%20investigate%20LLMs%27%20behaviour%0Ain%20strategic%20games%2C%20Stag%20Hunt%20and%20Prisoner%20Dilemma%2C%20analyzing%20performance%0Avariations%20under%20different%20settings%20and%20prompts.%20Our%20results%20show%20that%20the%0Atested%20state-of-the-art%20LLMs%20exhibit%20at%20least%20one%20of%20the%20following%20systematic%0Abiases%3A%20%281%29%20positional%20bias%2C%20%282%29%20payoff%20bias%2C%20or%20%283%29%20behavioural%20bias.%0ASubsequently%2C%20we%20observed%20that%20the%20LLMs%27%20performance%20drops%20when%20the%20game%0Aconfiguration%20is%20misaligned%20with%20the%20affecting%20biases.%20Performance%20is%20assessed%0Abased%20on%20the%20selection%20of%20the%20correct%20action%2C%20one%20which%20agrees%20with%20the%0Aprompted%20preferred%20behaviours%20of%20both%20players.%20Alignment%20refers%20to%20whether%20the%0ALLM%27s%20bias%20aligns%20with%20the%20correct%20action.%20For%20example%2C%20GPT-4o%27s%20average%0Aperformance%20drops%20by%2034%25%20when%20misaligned.%20Additionally%2C%20the%20current%20trend%20of%0A%22bigger%20and%20newer%20is%20better%22%20does%20not%20hold%20for%20the%20above%2C%20where%20GPT-4o%20%28the%0Acurrent%20best-performing%20LLM%29%20suffers%20the%20most%20substantial%20performance%20drop.%0ALastly%2C%20we%20note%20that%20while%20chain-of-thought%20prompting%20does%20reduce%20the%20effect%20of%0Athe%20biases%20on%20most%20models%2C%20it%20is%20far%20from%20solving%20the%20problem%20at%20the%0Afundamental%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04467v2&entry.124074799=Read"},
{"title": "Neural Compression of Atmospheric States", "author": "Piotr Mirowski and David Warde-Farley and Mihaela Rosca and Matthew Koichi Grimes and Yana Hasson and Hyunjik Kim and M\u00e9lanie Rey and Simon Osindero and Suman Ravuri and Shakir Mohamed", "abstract": "  Atmospheric states derived from reanalysis comprise a substantial portion of\nweather and climate simulation outputs. Many stakeholders -- such as\nresearchers, policy makers, and insurers -- use this data to better understand\nthe earth system and guide policy decisions. Atmospheric states have also\nreceived increased interest as machine learning approaches to weather\nprediction have shown promising results. A key issue for all audiences is that\ndense time series of these high-dimensional states comprise an enormous amount\nof data, precluding all but the most well resourced groups from accessing and\nusing historical data and future projections. To address this problem, we\npropose a method for compressing atmospheric states using methods from the\nneural network literature, adapting spherical data to processing by\nconventional neural architectures through the use of the area-preserving\nHEALPix projection. We investigate two model classes for building neural\ncompressors: the hyperprior model from the neural image compression literature\nand recent vector-quantised models. We show that both families of models\nsatisfy the desiderata of small average error, a small number of high-error\nreconstructed pixels, faithful reproduction of extreme events such as\nhurricanes and heatwaves, preservation of the spectral power distribution\nacross spatial scales. We demonstrate compression ratios in excess of 1000x,\nwith compression and decompression at a rate of approximately one second per\nglobal atmospheric state.\n", "link": "http://arxiv.org/abs/2407.11666v1", "date": "2024-07-16", "relevancy": 1.4947, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5195}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4946}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Compression%20of%20Atmospheric%20States&body=Title%3A%20Neural%20Compression%20of%20Atmospheric%20States%0AAuthor%3A%20Piotr%20Mirowski%20and%20David%20Warde-Farley%20and%20Mihaela%20Rosca%20and%20Matthew%20Koichi%20Grimes%20and%20Yana%20Hasson%20and%20Hyunjik%20Kim%20and%20M%C3%A9lanie%20Rey%20and%20Simon%20Osindero%20and%20Suman%20Ravuri%20and%20Shakir%20Mohamed%0AAbstract%3A%20%20%20Atmospheric%20states%20derived%20from%20reanalysis%20comprise%20a%20substantial%20portion%20of%0Aweather%20and%20climate%20simulation%20outputs.%20Many%20stakeholders%20--%20such%20as%0Aresearchers%2C%20policy%20makers%2C%20and%20insurers%20--%20use%20this%20data%20to%20better%20understand%0Athe%20earth%20system%20and%20guide%20policy%20decisions.%20Atmospheric%20states%20have%20also%0Areceived%20increased%20interest%20as%20machine%20learning%20approaches%20to%20weather%0Aprediction%20have%20shown%20promising%20results.%20A%20key%20issue%20for%20all%20audiences%20is%20that%0Adense%20time%20series%20of%20these%20high-dimensional%20states%20comprise%20an%20enormous%20amount%0Aof%20data%2C%20precluding%20all%20but%20the%20most%20well%20resourced%20groups%20from%20accessing%20and%0Ausing%20historical%20data%20and%20future%20projections.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20method%20for%20compressing%20atmospheric%20states%20using%20methods%20from%20the%0Aneural%20network%20literature%2C%20adapting%20spherical%20data%20to%20processing%20by%0Aconventional%20neural%20architectures%20through%20the%20use%20of%20the%20area-preserving%0AHEALPix%20projection.%20We%20investigate%20two%20model%20classes%20for%20building%20neural%0Acompressors%3A%20the%20hyperprior%20model%20from%20the%20neural%20image%20compression%20literature%0Aand%20recent%20vector-quantised%20models.%20We%20show%20that%20both%20families%20of%20models%0Asatisfy%20the%20desiderata%20of%20small%20average%20error%2C%20a%20small%20number%20of%20high-error%0Areconstructed%20pixels%2C%20faithful%20reproduction%20of%20extreme%20events%20such%20as%0Ahurricanes%20and%20heatwaves%2C%20preservation%20of%20the%20spectral%20power%20distribution%0Aacross%20spatial%20scales.%20We%20demonstrate%20compression%20ratios%20in%20excess%20of%201000x%2C%0Awith%20compression%20and%20decompression%20at%20a%20rate%20of%20approximately%20one%20second%20per%0Aglobal%20atmospheric%20state.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Compression%2520of%2520Atmospheric%2520States%26entry.906535625%3DPiotr%2520Mirowski%2520and%2520David%2520Warde-Farley%2520and%2520Mihaela%2520Rosca%2520and%2520Matthew%2520Koichi%2520Grimes%2520and%2520Yana%2520Hasson%2520and%2520Hyunjik%2520Kim%2520and%2520M%25C3%25A9lanie%2520Rey%2520and%2520Simon%2520Osindero%2520and%2520Suman%2520Ravuri%2520and%2520Shakir%2520Mohamed%26entry.1292438233%3D%2520%2520Atmospheric%2520states%2520derived%2520from%2520reanalysis%2520comprise%2520a%2520substantial%2520portion%2520of%250Aweather%2520and%2520climate%2520simulation%2520outputs.%2520Many%2520stakeholders%2520--%2520such%2520as%250Aresearchers%252C%2520policy%2520makers%252C%2520and%2520insurers%2520--%2520use%2520this%2520data%2520to%2520better%2520understand%250Athe%2520earth%2520system%2520and%2520guide%2520policy%2520decisions.%2520Atmospheric%2520states%2520have%2520also%250Areceived%2520increased%2520interest%2520as%2520machine%2520learning%2520approaches%2520to%2520weather%250Aprediction%2520have%2520shown%2520promising%2520results.%2520A%2520key%2520issue%2520for%2520all%2520audiences%2520is%2520that%250Adense%2520time%2520series%2520of%2520these%2520high-dimensional%2520states%2520comprise%2520an%2520enormous%2520amount%250Aof%2520data%252C%2520precluding%2520all%2520but%2520the%2520most%2520well%2520resourced%2520groups%2520from%2520accessing%2520and%250Ausing%2520historical%2520data%2520and%2520future%2520projections.%2520To%2520address%2520this%2520problem%252C%2520we%250Apropose%2520a%2520method%2520for%2520compressing%2520atmospheric%2520states%2520using%2520methods%2520from%2520the%250Aneural%2520network%2520literature%252C%2520adapting%2520spherical%2520data%2520to%2520processing%2520by%250Aconventional%2520neural%2520architectures%2520through%2520the%2520use%2520of%2520the%2520area-preserving%250AHEALPix%2520projection.%2520We%2520investigate%2520two%2520model%2520classes%2520for%2520building%2520neural%250Acompressors%253A%2520the%2520hyperprior%2520model%2520from%2520the%2520neural%2520image%2520compression%2520literature%250Aand%2520recent%2520vector-quantised%2520models.%2520We%2520show%2520that%2520both%2520families%2520of%2520models%250Asatisfy%2520the%2520desiderata%2520of%2520small%2520average%2520error%252C%2520a%2520small%2520number%2520of%2520high-error%250Areconstructed%2520pixels%252C%2520faithful%2520reproduction%2520of%2520extreme%2520events%2520such%2520as%250Ahurricanes%2520and%2520heatwaves%252C%2520preservation%2520of%2520the%2520spectral%2520power%2520distribution%250Aacross%2520spatial%2520scales.%2520We%2520demonstrate%2520compression%2520ratios%2520in%2520excess%2520of%25201000x%252C%250Awith%2520compression%2520and%2520decompression%2520at%2520a%2520rate%2520of%2520approximately%2520one%2520second%2520per%250Aglobal%2520atmospheric%2520state.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Compression%20of%20Atmospheric%20States&entry.906535625=Piotr%20Mirowski%20and%20David%20Warde-Farley%20and%20Mihaela%20Rosca%20and%20Matthew%20Koichi%20Grimes%20and%20Yana%20Hasson%20and%20Hyunjik%20Kim%20and%20M%C3%A9lanie%20Rey%20and%20Simon%20Osindero%20and%20Suman%20Ravuri%20and%20Shakir%20Mohamed&entry.1292438233=%20%20Atmospheric%20states%20derived%20from%20reanalysis%20comprise%20a%20substantial%20portion%20of%0Aweather%20and%20climate%20simulation%20outputs.%20Many%20stakeholders%20--%20such%20as%0Aresearchers%2C%20policy%20makers%2C%20and%20insurers%20--%20use%20this%20data%20to%20better%20understand%0Athe%20earth%20system%20and%20guide%20policy%20decisions.%20Atmospheric%20states%20have%20also%0Areceived%20increased%20interest%20as%20machine%20learning%20approaches%20to%20weather%0Aprediction%20have%20shown%20promising%20results.%20A%20key%20issue%20for%20all%20audiences%20is%20that%0Adense%20time%20series%20of%20these%20high-dimensional%20states%20comprise%20an%20enormous%20amount%0Aof%20data%2C%20precluding%20all%20but%20the%20most%20well%20resourced%20groups%20from%20accessing%20and%0Ausing%20historical%20data%20and%20future%20projections.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20method%20for%20compressing%20atmospheric%20states%20using%20methods%20from%20the%0Aneural%20network%20literature%2C%20adapting%20spherical%20data%20to%20processing%20by%0Aconventional%20neural%20architectures%20through%20the%20use%20of%20the%20area-preserving%0AHEALPix%20projection.%20We%20investigate%20two%20model%20classes%20for%20building%20neural%0Acompressors%3A%20the%20hyperprior%20model%20from%20the%20neural%20image%20compression%20literature%0Aand%20recent%20vector-quantised%20models.%20We%20show%20that%20both%20families%20of%20models%0Asatisfy%20the%20desiderata%20of%20small%20average%20error%2C%20a%20small%20number%20of%20high-error%0Areconstructed%20pixels%2C%20faithful%20reproduction%20of%20extreme%20events%20such%20as%0Ahurricanes%20and%20heatwaves%2C%20preservation%20of%20the%20spectral%20power%20distribution%0Aacross%20spatial%20scales.%20We%20demonstrate%20compression%20ratios%20in%20excess%20of%201000x%2C%0Awith%20compression%20and%20decompression%20at%20a%20rate%20of%20approximately%20one%20second%20per%0Aglobal%20atmospheric%20state.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11666v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


