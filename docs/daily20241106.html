<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241105.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human\n  Features", "author": "Arnab Dey and Cheng-You Lu and Andrew I. Comport and Srinath Sridhar and Chin-Teng Lin and Jean Martinet", "abstract": "  Recent advancements in radiance field rendering show promising results in 3D\nscene representation, where Gaussian splatting-based techniques emerge as\nstate-of-the-art due to their quality and efficiency. Gaussian splatting is\nwidely used for various applications, including 3D human representation.\nHowever, previous 3D Gaussian splatting methods either use parametric body\nmodels as additional information or fail to provide any underlying structure,\nlike human biomechanical features, which are essential for different\napplications. In this paper, we present a novel approach called HFGaussian that\ncan estimate novel views and human features, such as the 3D skeleton, 3D key\npoints, and dense pose, from sparse input images in real time at 25 FPS. The\nproposed method leverages generalizable Gaussian splatting technique to\nrepresent the human subject and its associated features, enabling efficient and\ngeneralizable reconstruction. By incorporating a pose regression network and\nthe feature splatting technique with Gaussian splatting, HFGaussian\ndemonstrates improved capabilities over existing 3D human methods, showcasing\nthe potential of 3D human representations with integrated biomechanics. We\nthoroughly evaluate our HFGaussian method against the latest state-of-the-art\ntechniques in human Gaussian splatting and pose estimation, demonstrating its\nreal-time, state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2411.03086v1", "date": "2024-11-05", "relevancy": 3.4891, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7501}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.676}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HFGaussian%3A%20Learning%20Generalizable%20Gaussian%20Human%20with%20Integrated%20Human%0A%20%20Features&body=Title%3A%20HFGaussian%3A%20Learning%20Generalizable%20Gaussian%20Human%20with%20Integrated%20Human%0A%20%20Features%0AAuthor%3A%20Arnab%20Dey%20and%20Cheng-You%20Lu%20and%20Andrew%20I.%20Comport%20and%20Srinath%20Sridhar%20and%20Chin-Teng%20Lin%20and%20Jean%20Martinet%0AAbstract%3A%20%20%20Recent%20advancements%20in%20radiance%20field%20rendering%20show%20promising%20results%20in%203D%0Ascene%20representation%2C%20where%20Gaussian%20splatting-based%20techniques%20emerge%20as%0Astate-of-the-art%20due%20to%20their%20quality%20and%20efficiency.%20Gaussian%20splatting%20is%0Awidely%20used%20for%20various%20applications%2C%20including%203D%20human%20representation.%0AHowever%2C%20previous%203D%20Gaussian%20splatting%20methods%20either%20use%20parametric%20body%0Amodels%20as%20additional%20information%20or%20fail%20to%20provide%20any%20underlying%20structure%2C%0Alike%20human%20biomechanical%20features%2C%20which%20are%20essential%20for%20different%0Aapplications.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20called%20HFGaussian%20that%0Acan%20estimate%20novel%20views%20and%20human%20features%2C%20such%20as%20the%203D%20skeleton%2C%203D%20key%0Apoints%2C%20and%20dense%20pose%2C%20from%20sparse%20input%20images%20in%20real%20time%20at%2025%20FPS.%20The%0Aproposed%20method%20leverages%20generalizable%20Gaussian%20splatting%20technique%20to%0Arepresent%20the%20human%20subject%20and%20its%20associated%20features%2C%20enabling%20efficient%20and%0Ageneralizable%20reconstruction.%20By%20incorporating%20a%20pose%20regression%20network%20and%0Athe%20feature%20splatting%20technique%20with%20Gaussian%20splatting%2C%20HFGaussian%0Ademonstrates%20improved%20capabilities%20over%20existing%203D%20human%20methods%2C%20showcasing%0Athe%20potential%20of%203D%20human%20representations%20with%20integrated%20biomechanics.%20We%0Athoroughly%20evaluate%20our%20HFGaussian%20method%20against%20the%20latest%20state-of-the-art%0Atechniques%20in%20human%20Gaussian%20splatting%20and%20pose%20estimation%2C%20demonstrating%20its%0Areal-time%2C%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHFGaussian%253A%2520Learning%2520Generalizable%2520Gaussian%2520Human%2520with%2520Integrated%2520Human%250A%2520%2520Features%26entry.906535625%3DArnab%2520Dey%2520and%2520Cheng-You%2520Lu%2520and%2520Andrew%2520I.%2520Comport%2520and%2520Srinath%2520Sridhar%2520and%2520Chin-Teng%2520Lin%2520and%2520Jean%2520Martinet%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520radiance%2520field%2520rendering%2520show%2520promising%2520results%2520in%25203D%250Ascene%2520representation%252C%2520where%2520Gaussian%2520splatting-based%2520techniques%2520emerge%2520as%250Astate-of-the-art%2520due%2520to%2520their%2520quality%2520and%2520efficiency.%2520Gaussian%2520splatting%2520is%250Awidely%2520used%2520for%2520various%2520applications%252C%2520including%25203D%2520human%2520representation.%250AHowever%252C%2520previous%25203D%2520Gaussian%2520splatting%2520methods%2520either%2520use%2520parametric%2520body%250Amodels%2520as%2520additional%2520information%2520or%2520fail%2520to%2520provide%2520any%2520underlying%2520structure%252C%250Alike%2520human%2520biomechanical%2520features%252C%2520which%2520are%2520essential%2520for%2520different%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520called%2520HFGaussian%2520that%250Acan%2520estimate%2520novel%2520views%2520and%2520human%2520features%252C%2520such%2520as%2520the%25203D%2520skeleton%252C%25203D%2520key%250Apoints%252C%2520and%2520dense%2520pose%252C%2520from%2520sparse%2520input%2520images%2520in%2520real%2520time%2520at%252025%2520FPS.%2520The%250Aproposed%2520method%2520leverages%2520generalizable%2520Gaussian%2520splatting%2520technique%2520to%250Arepresent%2520the%2520human%2520subject%2520and%2520its%2520associated%2520features%252C%2520enabling%2520efficient%2520and%250Ageneralizable%2520reconstruction.%2520By%2520incorporating%2520a%2520pose%2520regression%2520network%2520and%250Athe%2520feature%2520splatting%2520technique%2520with%2520Gaussian%2520splatting%252C%2520HFGaussian%250Ademonstrates%2520improved%2520capabilities%2520over%2520existing%25203D%2520human%2520methods%252C%2520showcasing%250Athe%2520potential%2520of%25203D%2520human%2520representations%2520with%2520integrated%2520biomechanics.%2520We%250Athoroughly%2520evaluate%2520our%2520HFGaussian%2520method%2520against%2520the%2520latest%2520state-of-the-art%250Atechniques%2520in%2520human%2520Gaussian%2520splatting%2520and%2520pose%2520estimation%252C%2520demonstrating%2520its%250Areal-time%252C%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HFGaussian%3A%20Learning%20Generalizable%20Gaussian%20Human%20with%20Integrated%20Human%0A%20%20Features&entry.906535625=Arnab%20Dey%20and%20Cheng-You%20Lu%20and%20Andrew%20I.%20Comport%20and%20Srinath%20Sridhar%20and%20Chin-Teng%20Lin%20and%20Jean%20Martinet&entry.1292438233=%20%20Recent%20advancements%20in%20radiance%20field%20rendering%20show%20promising%20results%20in%203D%0Ascene%20representation%2C%20where%20Gaussian%20splatting-based%20techniques%20emerge%20as%0Astate-of-the-art%20due%20to%20their%20quality%20and%20efficiency.%20Gaussian%20splatting%20is%0Awidely%20used%20for%20various%20applications%2C%20including%203D%20human%20representation.%0AHowever%2C%20previous%203D%20Gaussian%20splatting%20methods%20either%20use%20parametric%20body%0Amodels%20as%20additional%20information%20or%20fail%20to%20provide%20any%20underlying%20structure%2C%0Alike%20human%20biomechanical%20features%2C%20which%20are%20essential%20for%20different%0Aapplications.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20called%20HFGaussian%20that%0Acan%20estimate%20novel%20views%20and%20human%20features%2C%20such%20as%20the%203D%20skeleton%2C%203D%20key%0Apoints%2C%20and%20dense%20pose%2C%20from%20sparse%20input%20images%20in%20real%20time%20at%2025%20FPS.%20The%0Aproposed%20method%20leverages%20generalizable%20Gaussian%20splatting%20technique%20to%0Arepresent%20the%20human%20subject%20and%20its%20associated%20features%2C%20enabling%20efficient%20and%0Ageneralizable%20reconstruction.%20By%20incorporating%20a%20pose%20regression%20network%20and%0Athe%20feature%20splatting%20technique%20with%20Gaussian%20splatting%2C%20HFGaussian%0Ademonstrates%20improved%20capabilities%20over%20existing%203D%20human%20methods%2C%20showcasing%0Athe%20potential%20of%203D%20human%20representations%20with%20integrated%20biomechanics.%20We%0Athoroughly%20evaluate%20our%20HFGaussian%20method%20against%20the%20latest%20state-of-the-art%0Atechniques%20in%20human%20Gaussian%20splatting%20and%20pose%20estimation%2C%20demonstrating%20its%0Areal-time%2C%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03086v1&entry.124074799=Read"},
{"title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "author": "Dawei Dai and Xu Long and Li Yutang and Zhang Yuanhui and Shuyin Xia", "abstract": "  Human-scene vision-language tasks are increasingly prevalent in diverse\nsocial applications, yet recent advancements predominantly rely on models\nspecifically tailored to individual tasks. Emerging research indicates that\nlarge vision-language models (VLMs) can enhance performance across various\ndownstream vision-language understanding tasks. However, general-domain models\noften underperform in specialized fields. This study introduces a\ndomain-specific Large Vision-Language Model, Human-Scene Vision-Language Model\n(HumanVLM), designed to provide a foundation for human-scene Vision-Language\ntasks. Specifically, (1) we create a large-scale human-scene multimodal\nimage-text dataset (HumanCaption-10M) sourced from the Internet to facilitate\ndomain-specific alignment; (2) develop a captioning approach for human-centered\nimages, capturing human faces, bodies, and backgrounds, and construct a\nhigh-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs)\nthat contain as much detailed information as possible about human; (3) Using\nHumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments,\nwe then evaluate our HumanVLM across varous downstream tasks, where it\ndemonstrates superior overall performance among multimodal models of comparable\nscale, particularly excelling in human-related tasks and significantly\noutperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM,\nalongside the data introduced, will stimulate the research in human-around\nfields.\n", "link": "http://arxiv.org/abs/2411.03034v1", "date": "2024-11-05", "relevancy": 3.165, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanVLM%3A%20Foundation%20for%20Human-Scene%20Vision-Language%20Model&body=Title%3A%20HumanVLM%3A%20Foundation%20for%20Human-Scene%20Vision-Language%20Model%0AAuthor%3A%20Dawei%20Dai%20and%20Xu%20Long%20and%20Li%20Yutang%20and%20Zhang%20Yuanhui%20and%20Shuyin%20Xia%0AAbstract%3A%20%20%20Human-scene%20vision-language%20tasks%20are%20increasingly%20prevalent%20in%20diverse%0Asocial%20applications%2C%20yet%20recent%20advancements%20predominantly%20rely%20on%20models%0Aspecifically%20tailored%20to%20individual%20tasks.%20Emerging%20research%20indicates%20that%0Alarge%20vision-language%20models%20%28VLMs%29%20can%20enhance%20performance%20across%20various%0Adownstream%20vision-language%20understanding%20tasks.%20However%2C%20general-domain%20models%0Aoften%20underperform%20in%20specialized%20fields.%20This%20study%20introduces%20a%0Adomain-specific%20Large%20Vision-Language%20Model%2C%20Human-Scene%20Vision-Language%20Model%0A%28HumanVLM%29%2C%20designed%20to%20provide%20a%20foundation%20for%20human-scene%20Vision-Language%0Atasks.%20Specifically%2C%20%281%29%20we%20create%20a%20large-scale%20human-scene%20multimodal%0Aimage-text%20dataset%20%28HumanCaption-10M%29%20sourced%20from%20the%20Internet%20to%20facilitate%0Adomain-specific%20alignment%3B%20%282%29%20develop%20a%20captioning%20approach%20for%20human-centered%0Aimages%2C%20capturing%20human%20faces%2C%20bodies%2C%20and%20backgrounds%2C%20and%20construct%20a%0Ahigh-quality%20Human-Scene%20image-text%20dataset%20%28HumanCaptionHQ%2C%20about%20311k%20pairs%29%0Athat%20contain%20as%20much%20detailed%20information%20as%20possible%20about%20human%3B%20%283%29%20Using%0AHumanCaption-10M%20and%20HumanCaptionHQ%2C%20we%20train%20a%20HumanVLM.%20In%20the%20experiments%2C%0Awe%20then%20evaluate%20our%20HumanVLM%20across%20varous%20downstream%20tasks%2C%20where%20it%0Ademonstrates%20superior%20overall%20performance%20among%20multimodal%20models%20of%20comparable%0Ascale%2C%20particularly%20excelling%20in%20human-related%20tasks%20and%20significantly%0Aoutperforming%20similar%20models%2C%20including%20Qwen2VL%20and%20ChatGPT-4o.%20HumanVLM%2C%0Aalongside%20the%20data%20introduced%2C%20will%20stimulate%20the%20research%20in%20human-around%0Afields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanVLM%253A%2520Foundation%2520for%2520Human-Scene%2520Vision-Language%2520Model%26entry.906535625%3DDawei%2520Dai%2520and%2520Xu%2520Long%2520and%2520Li%2520Yutang%2520and%2520Zhang%2520Yuanhui%2520and%2520Shuyin%2520Xia%26entry.1292438233%3D%2520%2520Human-scene%2520vision-language%2520tasks%2520are%2520increasingly%2520prevalent%2520in%2520diverse%250Asocial%2520applications%252C%2520yet%2520recent%2520advancements%2520predominantly%2520rely%2520on%2520models%250Aspecifically%2520tailored%2520to%2520individual%2520tasks.%2520Emerging%2520research%2520indicates%2520that%250Alarge%2520vision-language%2520models%2520%2528VLMs%2529%2520can%2520enhance%2520performance%2520across%2520various%250Adownstream%2520vision-language%2520understanding%2520tasks.%2520However%252C%2520general-domain%2520models%250Aoften%2520underperform%2520in%2520specialized%2520fields.%2520This%2520study%2520introduces%2520a%250Adomain-specific%2520Large%2520Vision-Language%2520Model%252C%2520Human-Scene%2520Vision-Language%2520Model%250A%2528HumanVLM%2529%252C%2520designed%2520to%2520provide%2520a%2520foundation%2520for%2520human-scene%2520Vision-Language%250Atasks.%2520Specifically%252C%2520%25281%2529%2520we%2520create%2520a%2520large-scale%2520human-scene%2520multimodal%250Aimage-text%2520dataset%2520%2528HumanCaption-10M%2529%2520sourced%2520from%2520the%2520Internet%2520to%2520facilitate%250Adomain-specific%2520alignment%253B%2520%25282%2529%2520develop%2520a%2520captioning%2520approach%2520for%2520human-centered%250Aimages%252C%2520capturing%2520human%2520faces%252C%2520bodies%252C%2520and%2520backgrounds%252C%2520and%2520construct%2520a%250Ahigh-quality%2520Human-Scene%2520image-text%2520dataset%2520%2528HumanCaptionHQ%252C%2520about%2520311k%2520pairs%2529%250Athat%2520contain%2520as%2520much%2520detailed%2520information%2520as%2520possible%2520about%2520human%253B%2520%25283%2529%2520Using%250AHumanCaption-10M%2520and%2520HumanCaptionHQ%252C%2520we%2520train%2520a%2520HumanVLM.%2520In%2520the%2520experiments%252C%250Awe%2520then%2520evaluate%2520our%2520HumanVLM%2520across%2520varous%2520downstream%2520tasks%252C%2520where%2520it%250Ademonstrates%2520superior%2520overall%2520performance%2520among%2520multimodal%2520models%2520of%2520comparable%250Ascale%252C%2520particularly%2520excelling%2520in%2520human-related%2520tasks%2520and%2520significantly%250Aoutperforming%2520similar%2520models%252C%2520including%2520Qwen2VL%2520and%2520ChatGPT-4o.%2520HumanVLM%252C%250Aalongside%2520the%2520data%2520introduced%252C%2520will%2520stimulate%2520the%2520research%2520in%2520human-around%250Afields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanVLM%3A%20Foundation%20for%20Human-Scene%20Vision-Language%20Model&entry.906535625=Dawei%20Dai%20and%20Xu%20Long%20and%20Li%20Yutang%20and%20Zhang%20Yuanhui%20and%20Shuyin%20Xia&entry.1292438233=%20%20Human-scene%20vision-language%20tasks%20are%20increasingly%20prevalent%20in%20diverse%0Asocial%20applications%2C%20yet%20recent%20advancements%20predominantly%20rely%20on%20models%0Aspecifically%20tailored%20to%20individual%20tasks.%20Emerging%20research%20indicates%20that%0Alarge%20vision-language%20models%20%28VLMs%29%20can%20enhance%20performance%20across%20various%0Adownstream%20vision-language%20understanding%20tasks.%20However%2C%20general-domain%20models%0Aoften%20underperform%20in%20specialized%20fields.%20This%20study%20introduces%20a%0Adomain-specific%20Large%20Vision-Language%20Model%2C%20Human-Scene%20Vision-Language%20Model%0A%28HumanVLM%29%2C%20designed%20to%20provide%20a%20foundation%20for%20human-scene%20Vision-Language%0Atasks.%20Specifically%2C%20%281%29%20we%20create%20a%20large-scale%20human-scene%20multimodal%0Aimage-text%20dataset%20%28HumanCaption-10M%29%20sourced%20from%20the%20Internet%20to%20facilitate%0Adomain-specific%20alignment%3B%20%282%29%20develop%20a%20captioning%20approach%20for%20human-centered%0Aimages%2C%20capturing%20human%20faces%2C%20bodies%2C%20and%20backgrounds%2C%20and%20construct%20a%0Ahigh-quality%20Human-Scene%20image-text%20dataset%20%28HumanCaptionHQ%2C%20about%20311k%20pairs%29%0Athat%20contain%20as%20much%20detailed%20information%20as%20possible%20about%20human%3B%20%283%29%20Using%0AHumanCaption-10M%20and%20HumanCaptionHQ%2C%20we%20train%20a%20HumanVLM.%20In%20the%20experiments%2C%0Awe%20then%20evaluate%20our%20HumanVLM%20across%20varous%20downstream%20tasks%2C%20where%20it%0Ademonstrates%20superior%20overall%20performance%20among%20multimodal%20models%20of%20comparable%0Ascale%2C%20particularly%20excelling%20in%20human-related%20tasks%20and%20significantly%0Aoutperforming%20similar%20models%2C%20including%20Qwen2VL%20and%20ChatGPT-4o.%20HumanVLM%2C%0Aalongside%20the%20data%20introduced%2C%20will%20stimulate%20the%20research%20in%20human-around%0Afields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03034v1&entry.124074799=Read"},
{"title": "GlobalDoc: A Cross-Modal Vision-Language Framework for Real-World\n  Document Image Retrieval and Classification", "author": "Souhail Bakkali and Sanket Biswas and Zuheng Ming and Micka\u00ebl Coustaty and Mar\u00e7al Rusi\u00f1ol and Oriol Ramos Terrades and Josep Llad\u00f3s", "abstract": "  Visual document understanding (VDU) has rapidly advanced with the development\nof powerful multi-modal language models. However, these models typically\nrequire extensive document pre-training data to learn intermediate\nrepresentations and often suffer a significant performance drop in real-world\nonline industrial settings. A primary issue is their heavy reliance on OCR\nengines to extract local positional information within document pages, which\nlimits the models' ability to capture global information and hinders their\ngeneralizability, flexibility, and robustness. In this paper, we introduce\nGlobalDoc, a cross-modal transformer-based architecture pre-trained in a\nself-supervised manner using three novel pretext objective tasks. GlobalDoc\nimproves the learning of richer semantic concepts by unifying language and\nvisual representations, resulting in more transferable models. For proper\nevaluation, we also propose two novel document-level downstream VDU tasks,\nFew-Shot Document Image Classification (DIC) and Content-based Document Image\nRetrieval (DIR), designed to simulate industrial scenarios more closely.\nExtensive experimentation has been conducted to demonstrate GlobalDoc's\neffectiveness in practical settings.\n", "link": "http://arxiv.org/abs/2309.05756v3", "date": "2024-11-05", "relevancy": 3.0684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlobalDoc%3A%20A%20Cross-Modal%20Vision-Language%20Framework%20for%20Real-World%0A%20%20Document%20Image%20Retrieval%20and%20Classification&body=Title%3A%20GlobalDoc%3A%20A%20Cross-Modal%20Vision-Language%20Framework%20for%20Real-World%0A%20%20Document%20Image%20Retrieval%20and%20Classification%0AAuthor%3A%20Souhail%20Bakkali%20and%20Sanket%20Biswas%20and%20Zuheng%20Ming%20and%20Micka%C3%ABl%20Coustaty%20and%20Mar%C3%A7al%20Rusi%C3%B1ol%20and%20Oriol%20Ramos%20Terrades%20and%20Josep%20Llad%C3%B3s%0AAbstract%3A%20%20%20Visual%20document%20understanding%20%28VDU%29%20has%20rapidly%20advanced%20with%20the%20development%0Aof%20powerful%20multi-modal%20language%20models.%20However%2C%20these%20models%20typically%0Arequire%20extensive%20document%20pre-training%20data%20to%20learn%20intermediate%0Arepresentations%20and%20often%20suffer%20a%20significant%20performance%20drop%20in%20real-world%0Aonline%20industrial%20settings.%20A%20primary%20issue%20is%20their%20heavy%20reliance%20on%20OCR%0Aengines%20to%20extract%20local%20positional%20information%20within%20document%20pages%2C%20which%0Alimits%20the%20models%27%20ability%20to%20capture%20global%20information%20and%20hinders%20their%0Ageneralizability%2C%20flexibility%2C%20and%20robustness.%20In%20this%20paper%2C%20we%20introduce%0AGlobalDoc%2C%20a%20cross-modal%20transformer-based%20architecture%20pre-trained%20in%20a%0Aself-supervised%20manner%20using%20three%20novel%20pretext%20objective%20tasks.%20GlobalDoc%0Aimproves%20the%20learning%20of%20richer%20semantic%20concepts%20by%20unifying%20language%20and%0Avisual%20representations%2C%20resulting%20in%20more%20transferable%20models.%20For%20proper%0Aevaluation%2C%20we%20also%20propose%20two%20novel%20document-level%20downstream%20VDU%20tasks%2C%0AFew-Shot%20Document%20Image%20Classification%20%28DIC%29%20and%20Content-based%20Document%20Image%0ARetrieval%20%28DIR%29%2C%20designed%20to%20simulate%20industrial%20scenarios%20more%20closely.%0AExtensive%20experimentation%20has%20been%20conducted%20to%20demonstrate%20GlobalDoc%27s%0Aeffectiveness%20in%20practical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05756v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobalDoc%253A%2520A%2520Cross-Modal%2520Vision-Language%2520Framework%2520for%2520Real-World%250A%2520%2520Document%2520Image%2520Retrieval%2520and%2520Classification%26entry.906535625%3DSouhail%2520Bakkali%2520and%2520Sanket%2520Biswas%2520and%2520Zuheng%2520Ming%2520and%2520Micka%25C3%25ABl%2520Coustaty%2520and%2520Mar%25C3%25A7al%2520Rusi%25C3%25B1ol%2520and%2520Oriol%2520Ramos%2520Terrades%2520and%2520Josep%2520Llad%25C3%25B3s%26entry.1292438233%3D%2520%2520Visual%2520document%2520understanding%2520%2528VDU%2529%2520has%2520rapidly%2520advanced%2520with%2520the%2520development%250Aof%2520powerful%2520multi-modal%2520language%2520models.%2520However%252C%2520these%2520models%2520typically%250Arequire%2520extensive%2520document%2520pre-training%2520data%2520to%2520learn%2520intermediate%250Arepresentations%2520and%2520often%2520suffer%2520a%2520significant%2520performance%2520drop%2520in%2520real-world%250Aonline%2520industrial%2520settings.%2520A%2520primary%2520issue%2520is%2520their%2520heavy%2520reliance%2520on%2520OCR%250Aengines%2520to%2520extract%2520local%2520positional%2520information%2520within%2520document%2520pages%252C%2520which%250Alimits%2520the%2520models%2527%2520ability%2520to%2520capture%2520global%2520information%2520and%2520hinders%2520their%250Ageneralizability%252C%2520flexibility%252C%2520and%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AGlobalDoc%252C%2520a%2520cross-modal%2520transformer-based%2520architecture%2520pre-trained%2520in%2520a%250Aself-supervised%2520manner%2520using%2520three%2520novel%2520pretext%2520objective%2520tasks.%2520GlobalDoc%250Aimproves%2520the%2520learning%2520of%2520richer%2520semantic%2520concepts%2520by%2520unifying%2520language%2520and%250Avisual%2520representations%252C%2520resulting%2520in%2520more%2520transferable%2520models.%2520For%2520proper%250Aevaluation%252C%2520we%2520also%2520propose%2520two%2520novel%2520document-level%2520downstream%2520VDU%2520tasks%252C%250AFew-Shot%2520Document%2520Image%2520Classification%2520%2528DIC%2529%2520and%2520Content-based%2520Document%2520Image%250ARetrieval%2520%2528DIR%2529%252C%2520designed%2520to%2520simulate%2520industrial%2520scenarios%2520more%2520closely.%250AExtensive%2520experimentation%2520has%2520been%2520conducted%2520to%2520demonstrate%2520GlobalDoc%2527s%250Aeffectiveness%2520in%2520practical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05756v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlobalDoc%3A%20A%20Cross-Modal%20Vision-Language%20Framework%20for%20Real-World%0A%20%20Document%20Image%20Retrieval%20and%20Classification&entry.906535625=Souhail%20Bakkali%20and%20Sanket%20Biswas%20and%20Zuheng%20Ming%20and%20Micka%C3%ABl%20Coustaty%20and%20Mar%C3%A7al%20Rusi%C3%B1ol%20and%20Oriol%20Ramos%20Terrades%20and%20Josep%20Llad%C3%B3s&entry.1292438233=%20%20Visual%20document%20understanding%20%28VDU%29%20has%20rapidly%20advanced%20with%20the%20development%0Aof%20powerful%20multi-modal%20language%20models.%20However%2C%20these%20models%20typically%0Arequire%20extensive%20document%20pre-training%20data%20to%20learn%20intermediate%0Arepresentations%20and%20often%20suffer%20a%20significant%20performance%20drop%20in%20real-world%0Aonline%20industrial%20settings.%20A%20primary%20issue%20is%20their%20heavy%20reliance%20on%20OCR%0Aengines%20to%20extract%20local%20positional%20information%20within%20document%20pages%2C%20which%0Alimits%20the%20models%27%20ability%20to%20capture%20global%20information%20and%20hinders%20their%0Ageneralizability%2C%20flexibility%2C%20and%20robustness.%20In%20this%20paper%2C%20we%20introduce%0AGlobalDoc%2C%20a%20cross-modal%20transformer-based%20architecture%20pre-trained%20in%20a%0Aself-supervised%20manner%20using%20three%20novel%20pretext%20objective%20tasks.%20GlobalDoc%0Aimproves%20the%20learning%20of%20richer%20semantic%20concepts%20by%20unifying%20language%20and%0Avisual%20representations%2C%20resulting%20in%20more%20transferable%20models.%20For%20proper%0Aevaluation%2C%20we%20also%20propose%20two%20novel%20document-level%20downstream%20VDU%20tasks%2C%0AFew-Shot%20Document%20Image%20Classification%20%28DIC%29%20and%20Content-based%20Document%20Image%0ARetrieval%20%28DIR%29%2C%20designed%20to%20simulate%20industrial%20scenarios%20more%20closely.%0AExtensive%20experimentation%20has%20been%20conducted%20to%20demonstrate%20GlobalDoc%27s%0Aeffectiveness%20in%20practical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05756v3&entry.124074799=Read"},
{"title": "Decoupling Fine Detail and Global Geometry for Compressed Depth Map\n  Super-Resolution", "author": "Huan Zheng and Wencheng Han and Jianbing Shen", "abstract": "  Recovering high-quality depth maps from compressed sources has gained\nsignificant attention due to the limitations of consumer-grade depth cameras\nand the bandwidth restrictions during data transmission. However, current\nmethods still suffer from two challenges. First, bit-depth compression produces\na uniform depth representation in regions with subtle variations, hindering the\nrecovery of detailed information. Second, densely distributed random noise\nreduces the accuracy of estimating the global geometric structure of the scene.\nTo address these challenges, we propose a novel framework, termed\ngeometry-decoupled network (GDNet), for compressed depth map super-resolution\nthat decouples the high-quality depth map reconstruction process by handling\nglobal and detailed geometric features separately. To be specific, we propose\nthe fine geometry detail encoder (FGDE), which is designed to aggregate fine\ngeometry details in high-resolution low-level image features while\nsimultaneously enriching them with complementary information from\nlow-resolution context-level image features. In addition, we develop the global\ngeometry encoder (GGE) that aims at suppressing noise and extracting global\ngeometric information effectively via constructing compact feature\nrepresentation in a low-rank space. We conduct experiments on multiple\nbenchmark datasets, demonstrating that our GDNet significantly outperforms\ncurrent methods in terms of geometric consistency and detail recovery. In the\nECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st\nplace award. Our codes will be available.\n", "link": "http://arxiv.org/abs/2411.03239v1", "date": "2024-11-05", "relevancy": 2.8763, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5921}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5884}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Fine%20Detail%20and%20Global%20Geometry%20for%20Compressed%20Depth%20Map%0A%20%20Super-Resolution&body=Title%3A%20Decoupling%20Fine%20Detail%20and%20Global%20Geometry%20for%20Compressed%20Depth%20Map%0A%20%20Super-Resolution%0AAuthor%3A%20Huan%20Zheng%20and%20Wencheng%20Han%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Recovering%20high-quality%20depth%20maps%20from%20compressed%20sources%20has%20gained%0Asignificant%20attention%20due%20to%20the%20limitations%20of%20consumer-grade%20depth%20cameras%0Aand%20the%20bandwidth%20restrictions%20during%20data%20transmission.%20However%2C%20current%0Amethods%20still%20suffer%20from%20two%20challenges.%20First%2C%20bit-depth%20compression%20produces%0Aa%20uniform%20depth%20representation%20in%20regions%20with%20subtle%20variations%2C%20hindering%20the%0Arecovery%20of%20detailed%20information.%20Second%2C%20densely%20distributed%20random%20noise%0Areduces%20the%20accuracy%20of%20estimating%20the%20global%20geometric%20structure%20of%20the%20scene.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20termed%0Ageometry-decoupled%20network%20%28GDNet%29%2C%20for%20compressed%20depth%20map%20super-resolution%0Athat%20decouples%20the%20high-quality%20depth%20map%20reconstruction%20process%20by%20handling%0Aglobal%20and%20detailed%20geometric%20features%20separately.%20To%20be%20specific%2C%20we%20propose%0Athe%20fine%20geometry%20detail%20encoder%20%28FGDE%29%2C%20which%20is%20designed%20to%20aggregate%20fine%0Ageometry%20details%20in%20high-resolution%20low-level%20image%20features%20while%0Asimultaneously%20enriching%20them%20with%20complementary%20information%20from%0Alow-resolution%20context-level%20image%20features.%20In%20addition%2C%20we%20develop%20the%20global%0Ageometry%20encoder%20%28GGE%29%20that%20aims%20at%20suppressing%20noise%20and%20extracting%20global%0Ageometric%20information%20effectively%20via%20constructing%20compact%20feature%0Arepresentation%20in%20a%20low-rank%20space.%20We%20conduct%20experiments%20on%20multiple%0Abenchmark%20datasets%2C%20demonstrating%20that%20our%20GDNet%20significantly%20outperforms%0Acurrent%20methods%20in%20terms%20of%20geometric%20consistency%20and%20detail%20recovery.%20In%20the%0AECCV%202024%20AIM%20Compressed%20Depth%20Upsampling%20Challenge%2C%20our%20solution%20won%20the%201st%0Aplace%20award.%20Our%20codes%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Fine%2520Detail%2520and%2520Global%2520Geometry%2520for%2520Compressed%2520Depth%2520Map%250A%2520%2520Super-Resolution%26entry.906535625%3DHuan%2520Zheng%2520and%2520Wencheng%2520Han%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Recovering%2520high-quality%2520depth%2520maps%2520from%2520compressed%2520sources%2520has%2520gained%250Asignificant%2520attention%2520due%2520to%2520the%2520limitations%2520of%2520consumer-grade%2520depth%2520cameras%250Aand%2520the%2520bandwidth%2520restrictions%2520during%2520data%2520transmission.%2520However%252C%2520current%250Amethods%2520still%2520suffer%2520from%2520two%2520challenges.%2520First%252C%2520bit-depth%2520compression%2520produces%250Aa%2520uniform%2520depth%2520representation%2520in%2520regions%2520with%2520subtle%2520variations%252C%2520hindering%2520the%250Arecovery%2520of%2520detailed%2520information.%2520Second%252C%2520densely%2520distributed%2520random%2520noise%250Areduces%2520the%2520accuracy%2520of%2520estimating%2520the%2520global%2520geometric%2520structure%2520of%2520the%2520scene.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520termed%250Ageometry-decoupled%2520network%2520%2528GDNet%2529%252C%2520for%2520compressed%2520depth%2520map%2520super-resolution%250Athat%2520decouples%2520the%2520high-quality%2520depth%2520map%2520reconstruction%2520process%2520by%2520handling%250Aglobal%2520and%2520detailed%2520geometric%2520features%2520separately.%2520To%2520be%2520specific%252C%2520we%2520propose%250Athe%2520fine%2520geometry%2520detail%2520encoder%2520%2528FGDE%2529%252C%2520which%2520is%2520designed%2520to%2520aggregate%2520fine%250Ageometry%2520details%2520in%2520high-resolution%2520low-level%2520image%2520features%2520while%250Asimultaneously%2520enriching%2520them%2520with%2520complementary%2520information%2520from%250Alow-resolution%2520context-level%2520image%2520features.%2520In%2520addition%252C%2520we%2520develop%2520the%2520global%250Ageometry%2520encoder%2520%2528GGE%2529%2520that%2520aims%2520at%2520suppressing%2520noise%2520and%2520extracting%2520global%250Ageometric%2520information%2520effectively%2520via%2520constructing%2520compact%2520feature%250Arepresentation%2520in%2520a%2520low-rank%2520space.%2520We%2520conduct%2520experiments%2520on%2520multiple%250Abenchmark%2520datasets%252C%2520demonstrating%2520that%2520our%2520GDNet%2520significantly%2520outperforms%250Acurrent%2520methods%2520in%2520terms%2520of%2520geometric%2520consistency%2520and%2520detail%2520recovery.%2520In%2520the%250AECCV%25202024%2520AIM%2520Compressed%2520Depth%2520Upsampling%2520Challenge%252C%2520our%2520solution%2520won%2520the%25201st%250Aplace%2520award.%2520Our%2520codes%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Fine%20Detail%20and%20Global%20Geometry%20for%20Compressed%20Depth%20Map%0A%20%20Super-Resolution&entry.906535625=Huan%20Zheng%20and%20Wencheng%20Han%20and%20Jianbing%20Shen&entry.1292438233=%20%20Recovering%20high-quality%20depth%20maps%20from%20compressed%20sources%20has%20gained%0Asignificant%20attention%20due%20to%20the%20limitations%20of%20consumer-grade%20depth%20cameras%0Aand%20the%20bandwidth%20restrictions%20during%20data%20transmission.%20However%2C%20current%0Amethods%20still%20suffer%20from%20two%20challenges.%20First%2C%20bit-depth%20compression%20produces%0Aa%20uniform%20depth%20representation%20in%20regions%20with%20subtle%20variations%2C%20hindering%20the%0Arecovery%20of%20detailed%20information.%20Second%2C%20densely%20distributed%20random%20noise%0Areduces%20the%20accuracy%20of%20estimating%20the%20global%20geometric%20structure%20of%20the%20scene.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20termed%0Ageometry-decoupled%20network%20%28GDNet%29%2C%20for%20compressed%20depth%20map%20super-resolution%0Athat%20decouples%20the%20high-quality%20depth%20map%20reconstruction%20process%20by%20handling%0Aglobal%20and%20detailed%20geometric%20features%20separately.%20To%20be%20specific%2C%20we%20propose%0Athe%20fine%20geometry%20detail%20encoder%20%28FGDE%29%2C%20which%20is%20designed%20to%20aggregate%20fine%0Ageometry%20details%20in%20high-resolution%20low-level%20image%20features%20while%0Asimultaneously%20enriching%20them%20with%20complementary%20information%20from%0Alow-resolution%20context-level%20image%20features.%20In%20addition%2C%20we%20develop%20the%20global%0Ageometry%20encoder%20%28GGE%29%20that%20aims%20at%20suppressing%20noise%20and%20extracting%20global%0Ageometric%20information%20effectively%20via%20constructing%20compact%20feature%0Arepresentation%20in%20a%20low-rank%20space.%20We%20conduct%20experiments%20on%20multiple%0Abenchmark%20datasets%2C%20demonstrating%20that%20our%20GDNet%20significantly%20outperforms%0Acurrent%20methods%20in%20terms%20of%20geometric%20consistency%20and%20detail%20recovery.%20In%20the%0AECCV%202024%20AIM%20Compressed%20Depth%20Upsampling%20Challenge%2C%20our%20solution%20won%20the%201st%0Aplace%20award.%20Our%20codes%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03239v1&entry.124074799=Read"},
{"title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single\n  In-the-Wild Image using a Dataset with Levels of Details", "author": "Zhongjin Luo and Haolin Liu and Chenghong Li and Wanghao Du and Zirong Jin and Wanhu Sun and Yinyu Nie and Weikai Chen and Xiaoguang Han", "abstract": "  Neural implicit functions have brought impressive advances to the\nstate-of-the-art of clothed human digitization from multiple or even single\nimages. However, despite the progress, current arts still have difficulty\ngeneralizing to unseen images with complex cloth deformation and body poses. In\nthis work, we present GarVerseLOD, a new dataset and framework that paves the\nway to achieving unprecedented robustness in high-fidelity 3D garment\nreconstruction from a single unconstrained image. Inspired by the recent\nsuccess of large generative models, we believe that one key to addressing the\ngeneralization challenge lies in the quantity and quality of 3D garment data.\nTowards this end, GarVerseLOD collects 6,000 high-quality cloth models with\nfine-grained geometry details manually created by professional artists. In\naddition to the scale of training data, we observe that having disentangled\ngranularities of geometry can play an important role in boosting the\ngeneralization capability and inference accuracy of the learned model. We hence\ncraft GarVerseLOD as a hierarchical dataset with levels of details (LOD),\nspanning from detail-free stylized shape to pose-blended garment with\npixel-aligned details. This allows us to make this highly under-constrained\nproblem tractable by factorizing the inference into easier tasks, each narrowed\ndown with smaller searching space. To ensure GarVerseLOD can generalize well to\nin-the-wild images, we propose a novel labeling paradigm based on conditional\ndiffusion models to generate extensive paired images for each garment model\nwith high photorealism. We evaluate our method on a massive amount of\nin-the-wild images. Experimental results demonstrate that GarVerseLOD can\ngenerate standalone garment pieces with significantly better quality than prior\napproaches. Project page: https://garverselod.github.io/\n", "link": "http://arxiv.org/abs/2411.03047v1", "date": "2024-11-05", "relevancy": 2.7645, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7177}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6937}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarVerseLOD%3A%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%0A%20%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details&body=Title%3A%20GarVerseLOD%3A%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%0A%20%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details%0AAuthor%3A%20Zhongjin%20Luo%20and%20Haolin%20Liu%20and%20Chenghong%20Li%20and%20Wanghao%20Du%20and%20Zirong%20Jin%20and%20Wanhu%20Sun%20and%20Yinyu%20Nie%20and%20Weikai%20Chen%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20Neural%20implicit%20functions%20have%20brought%20impressive%20advances%20to%20the%0Astate-of-the-art%20of%20clothed%20human%20digitization%20from%20multiple%20or%20even%20single%0Aimages.%20However%2C%20despite%20the%20progress%2C%20current%20arts%20still%20have%20difficulty%0Ageneralizing%20to%20unseen%20images%20with%20complex%20cloth%20deformation%20and%20body%20poses.%20In%0Athis%20work%2C%20we%20present%20GarVerseLOD%2C%20a%20new%20dataset%20and%20framework%20that%20paves%20the%0Away%20to%20achieving%20unprecedented%20robustness%20in%20high-fidelity%203D%20garment%0Areconstruction%20from%20a%20single%20unconstrained%20image.%20Inspired%20by%20the%20recent%0Asuccess%20of%20large%20generative%20models%2C%20we%20believe%20that%20one%20key%20to%20addressing%20the%0Ageneralization%20challenge%20lies%20in%20the%20quantity%20and%20quality%20of%203D%20garment%20data.%0ATowards%20this%20end%2C%20GarVerseLOD%20collects%206%2C000%20high-quality%20cloth%20models%20with%0Afine-grained%20geometry%20details%20manually%20created%20by%20professional%20artists.%20In%0Aaddition%20to%20the%20scale%20of%20training%20data%2C%20we%20observe%20that%20having%20disentangled%0Agranularities%20of%20geometry%20can%20play%20an%20important%20role%20in%20boosting%20the%0Ageneralization%20capability%20and%20inference%20accuracy%20of%20the%20learned%20model.%20We%20hence%0Acraft%20GarVerseLOD%20as%20a%20hierarchical%20dataset%20with%20levels%20of%20details%20%28LOD%29%2C%0Aspanning%20from%20detail-free%20stylized%20shape%20to%20pose-blended%20garment%20with%0Apixel-aligned%20details.%20This%20allows%20us%20to%20make%20this%20highly%20under-constrained%0Aproblem%20tractable%20by%20factorizing%20the%20inference%20into%20easier%20tasks%2C%20each%20narrowed%0Adown%20with%20smaller%20searching%20space.%20To%20ensure%20GarVerseLOD%20can%20generalize%20well%20to%0Ain-the-wild%20images%2C%20we%20propose%20a%20novel%20labeling%20paradigm%20based%20on%20conditional%0Adiffusion%20models%20to%20generate%20extensive%20paired%20images%20for%20each%20garment%20model%0Awith%20high%20photorealism.%20We%20evaluate%20our%20method%20on%20a%20massive%20amount%20of%0Ain-the-wild%20images.%20Experimental%20results%20demonstrate%20that%20GarVerseLOD%20can%0Agenerate%20standalone%20garment%20pieces%20with%20significantly%20better%20quality%20than%20prior%0Aapproaches.%20Project%20page%3A%20https%3A//garverselod.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarVerseLOD%253A%2520High-Fidelity%25203D%2520Garment%2520Reconstruction%2520from%2520a%2520Single%250A%2520%2520In-the-Wild%2520Image%2520using%2520a%2520Dataset%2520with%2520Levels%2520of%2520Details%26entry.906535625%3DZhongjin%2520Luo%2520and%2520Haolin%2520Liu%2520and%2520Chenghong%2520Li%2520and%2520Wanghao%2520Du%2520and%2520Zirong%2520Jin%2520and%2520Wanhu%2520Sun%2520and%2520Yinyu%2520Nie%2520and%2520Weikai%2520Chen%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520functions%2520have%2520brought%2520impressive%2520advances%2520to%2520the%250Astate-of-the-art%2520of%2520clothed%2520human%2520digitization%2520from%2520multiple%2520or%2520even%2520single%250Aimages.%2520However%252C%2520despite%2520the%2520progress%252C%2520current%2520arts%2520still%2520have%2520difficulty%250Ageneralizing%2520to%2520unseen%2520images%2520with%2520complex%2520cloth%2520deformation%2520and%2520body%2520poses.%2520In%250Athis%2520work%252C%2520we%2520present%2520GarVerseLOD%252C%2520a%2520new%2520dataset%2520and%2520framework%2520that%2520paves%2520the%250Away%2520to%2520achieving%2520unprecedented%2520robustness%2520in%2520high-fidelity%25203D%2520garment%250Areconstruction%2520from%2520a%2520single%2520unconstrained%2520image.%2520Inspired%2520by%2520the%2520recent%250Asuccess%2520of%2520large%2520generative%2520models%252C%2520we%2520believe%2520that%2520one%2520key%2520to%2520addressing%2520the%250Ageneralization%2520challenge%2520lies%2520in%2520the%2520quantity%2520and%2520quality%2520of%25203D%2520garment%2520data.%250ATowards%2520this%2520end%252C%2520GarVerseLOD%2520collects%25206%252C000%2520high-quality%2520cloth%2520models%2520with%250Afine-grained%2520geometry%2520details%2520manually%2520created%2520by%2520professional%2520artists.%2520In%250Aaddition%2520to%2520the%2520scale%2520of%2520training%2520data%252C%2520we%2520observe%2520that%2520having%2520disentangled%250Agranularities%2520of%2520geometry%2520can%2520play%2520an%2520important%2520role%2520in%2520boosting%2520the%250Ageneralization%2520capability%2520and%2520inference%2520accuracy%2520of%2520the%2520learned%2520model.%2520We%2520hence%250Acraft%2520GarVerseLOD%2520as%2520a%2520hierarchical%2520dataset%2520with%2520levels%2520of%2520details%2520%2528LOD%2529%252C%250Aspanning%2520from%2520detail-free%2520stylized%2520shape%2520to%2520pose-blended%2520garment%2520with%250Apixel-aligned%2520details.%2520This%2520allows%2520us%2520to%2520make%2520this%2520highly%2520under-constrained%250Aproblem%2520tractable%2520by%2520factorizing%2520the%2520inference%2520into%2520easier%2520tasks%252C%2520each%2520narrowed%250Adown%2520with%2520smaller%2520searching%2520space.%2520To%2520ensure%2520GarVerseLOD%2520can%2520generalize%2520well%2520to%250Ain-the-wild%2520images%252C%2520we%2520propose%2520a%2520novel%2520labeling%2520paradigm%2520based%2520on%2520conditional%250Adiffusion%2520models%2520to%2520generate%2520extensive%2520paired%2520images%2520for%2520each%2520garment%2520model%250Awith%2520high%2520photorealism.%2520We%2520evaluate%2520our%2520method%2520on%2520a%2520massive%2520amount%2520of%250Ain-the-wild%2520images.%2520Experimental%2520results%2520demonstrate%2520that%2520GarVerseLOD%2520can%250Agenerate%2520standalone%2520garment%2520pieces%2520with%2520significantly%2520better%2520quality%2520than%2520prior%250Aapproaches.%2520Project%2520page%253A%2520https%253A//garverselod.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarVerseLOD%3A%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%0A%20%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details&entry.906535625=Zhongjin%20Luo%20and%20Haolin%20Liu%20and%20Chenghong%20Li%20and%20Wanghao%20Du%20and%20Zirong%20Jin%20and%20Wanhu%20Sun%20and%20Yinyu%20Nie%20and%20Weikai%20Chen%20and%20Xiaoguang%20Han&entry.1292438233=%20%20Neural%20implicit%20functions%20have%20brought%20impressive%20advances%20to%20the%0Astate-of-the-art%20of%20clothed%20human%20digitization%20from%20multiple%20or%20even%20single%0Aimages.%20However%2C%20despite%20the%20progress%2C%20current%20arts%20still%20have%20difficulty%0Ageneralizing%20to%20unseen%20images%20with%20complex%20cloth%20deformation%20and%20body%20poses.%20In%0Athis%20work%2C%20we%20present%20GarVerseLOD%2C%20a%20new%20dataset%20and%20framework%20that%20paves%20the%0Away%20to%20achieving%20unprecedented%20robustness%20in%20high-fidelity%203D%20garment%0Areconstruction%20from%20a%20single%20unconstrained%20image.%20Inspired%20by%20the%20recent%0Asuccess%20of%20large%20generative%20models%2C%20we%20believe%20that%20one%20key%20to%20addressing%20the%0Ageneralization%20challenge%20lies%20in%20the%20quantity%20and%20quality%20of%203D%20garment%20data.%0ATowards%20this%20end%2C%20GarVerseLOD%20collects%206%2C000%20high-quality%20cloth%20models%20with%0Afine-grained%20geometry%20details%20manually%20created%20by%20professional%20artists.%20In%0Aaddition%20to%20the%20scale%20of%20training%20data%2C%20we%20observe%20that%20having%20disentangled%0Agranularities%20of%20geometry%20can%20play%20an%20important%20role%20in%20boosting%20the%0Ageneralization%20capability%20and%20inference%20accuracy%20of%20the%20learned%20model.%20We%20hence%0Acraft%20GarVerseLOD%20as%20a%20hierarchical%20dataset%20with%20levels%20of%20details%20%28LOD%29%2C%0Aspanning%20from%20detail-free%20stylized%20shape%20to%20pose-blended%20garment%20with%0Apixel-aligned%20details.%20This%20allows%20us%20to%20make%20this%20highly%20under-constrained%0Aproblem%20tractable%20by%20factorizing%20the%20inference%20into%20easier%20tasks%2C%20each%20narrowed%0Adown%20with%20smaller%20searching%20space.%20To%20ensure%20GarVerseLOD%20can%20generalize%20well%20to%0Ain-the-wild%20images%2C%20we%20propose%20a%20novel%20labeling%20paradigm%20based%20on%20conditional%0Adiffusion%20models%20to%20generate%20extensive%20paired%20images%20for%20each%20garment%20model%0Awith%20high%20photorealism.%20We%20evaluate%20our%20method%20on%20a%20massive%20amount%20of%0Ain-the-wild%20images.%20Experimental%20results%20demonstrate%20that%20GarVerseLOD%20can%0Agenerate%20standalone%20garment%20pieces%20with%20significantly%20better%20quality%20than%20prior%0Aapproaches.%20Project%20page%3A%20https%3A//garverselod.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03047v1&entry.124074799=Read"},
{"title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models", "author": "Kevin Y. Li and Sachin Goyal and Joao D. Semedo and J. Zico Kolter", "abstract": "  Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.\n", "link": "http://arxiv.org/abs/2411.03312v1", "date": "2024-11-05", "relevancy": 2.7452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20Optimal%20VLMs%20Need%20Only%20One%20Visual%20Token%20but%20Larger%20Models&body=Title%3A%20Inference%20Optimal%20VLMs%20Need%20Only%20One%20Visual%20Token%20but%20Larger%20Models%0AAuthor%3A%20Kevin%20Y.%20Li%20and%20Sachin%20Goyal%20and%20Joao%20D.%20Semedo%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20across%0Avarious%20visual%20understanding%20and%20reasoning%20tasks.%20However%2C%20their%20real-world%0Adeployment%20is%20often%20constrained%20by%20high%20latency%20during%20inference%20due%20to%0Asubstantial%20compute%20required%20to%20process%20the%20large%20number%20of%20input%20tokens%0A%28predominantly%20from%20the%20image%29%20by%20the%20LLM.%20To%20reduce%20inference%20costs%2C%20one%20can%0Aeither%20downsize%20the%20LLM%20or%20reduce%20the%20number%20of%20input%20image-tokens%2C%20the%20latter%0Aof%20which%20has%20been%20the%20focus%20of%20many%20recent%20works%20around%20token%20compression.%0AHowever%2C%20it%20is%20unclear%20what%20the%20optimal%20trade-off%20is%2C%20as%20both%20the%20factors%0Adirectly%20affect%20the%20VLM%20performance.%20We%20first%20characterize%20this%20optimal%0Atrade-off%20between%20the%20number%20of%20visual%20tokens%20and%20LLM%20parameters%20by%0Aestablishing%20scaling%20laws%20that%20capture%20variations%20in%20performance%20with%20these%20two%0Afactors.%20Our%20results%20reveal%20a%20surprising%20trend%3A%20for%20visual%20reasoning%20tasks%2C%20the%0Ainference-optimal%20behavior%20in%20VLMs%2C%20i.e.%2C%20minimum%20downstream%20error%20at%20any%20given%0Afixed%20inference%20compute%2C%20is%20achieved%20when%20using%20the%20largest%20LLM%20that%20fits%0Awithin%20the%20inference%20budget%20while%20minimizing%20visual%20token%20count%20-%20often%20to%20a%0Asingle%20token.%20While%20the%20token%20reduction%20literature%20has%20mainly%20focused%20on%0Amaintaining%20base%20model%20performance%20by%20modestly%20reducing%20the%20token%20count%20%28e.g.%2C%0A%245-10%5Ctimes%24%29%2C%20our%20results%20indicate%20that%20the%20compute-optimal%20inference%20regime%0Arequires%20operating%20under%20even%20higher%20token%20compression%20ratios.%20Based%20on%20these%0Ainsights%2C%20we%20take%20some%20initial%20steps%20towards%20building%20approaches%20tailored%20for%0Ahigh%20token%20compression%20settings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/locuslab/llava-token-compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520Optimal%2520VLMs%2520Need%2520Only%2520One%2520Visual%2520Token%2520but%2520Larger%2520Models%26entry.906535625%3DKevin%2520Y.%2520Li%2520and%2520Sachin%2520Goyal%2520and%2520Joao%2520D.%2520Semedo%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520across%250Avarious%2520visual%2520understanding%2520and%2520reasoning%2520tasks.%2520However%252C%2520their%2520real-world%250Adeployment%2520is%2520often%2520constrained%2520by%2520high%2520latency%2520during%2520inference%2520due%2520to%250Asubstantial%2520compute%2520required%2520to%2520process%2520the%2520large%2520number%2520of%2520input%2520tokens%250A%2528predominantly%2520from%2520the%2520image%2529%2520by%2520the%2520LLM.%2520To%2520reduce%2520inference%2520costs%252C%2520one%2520can%250Aeither%2520downsize%2520the%2520LLM%2520or%2520reduce%2520the%2520number%2520of%2520input%2520image-tokens%252C%2520the%2520latter%250Aof%2520which%2520has%2520been%2520the%2520focus%2520of%2520many%2520recent%2520works%2520around%2520token%2520compression.%250AHowever%252C%2520it%2520is%2520unclear%2520what%2520the%2520optimal%2520trade-off%2520is%252C%2520as%2520both%2520the%2520factors%250Adirectly%2520affect%2520the%2520VLM%2520performance.%2520We%2520first%2520characterize%2520this%2520optimal%250Atrade-off%2520between%2520the%2520number%2520of%2520visual%2520tokens%2520and%2520LLM%2520parameters%2520by%250Aestablishing%2520scaling%2520laws%2520that%2520capture%2520variations%2520in%2520performance%2520with%2520these%2520two%250Afactors.%2520Our%2520results%2520reveal%2520a%2520surprising%2520trend%253A%2520for%2520visual%2520reasoning%2520tasks%252C%2520the%250Ainference-optimal%2520behavior%2520in%2520VLMs%252C%2520i.e.%252C%2520minimum%2520downstream%2520error%2520at%2520any%2520given%250Afixed%2520inference%2520compute%252C%2520is%2520achieved%2520when%2520using%2520the%2520largest%2520LLM%2520that%2520fits%250Awithin%2520the%2520inference%2520budget%2520while%2520minimizing%2520visual%2520token%2520count%2520-%2520often%2520to%2520a%250Asingle%2520token.%2520While%2520the%2520token%2520reduction%2520literature%2520has%2520mainly%2520focused%2520on%250Amaintaining%2520base%2520model%2520performance%2520by%2520modestly%2520reducing%2520the%2520token%2520count%2520%2528e.g.%252C%250A%25245-10%255Ctimes%2524%2529%252C%2520our%2520results%2520indicate%2520that%2520the%2520compute-optimal%2520inference%2520regime%250Arequires%2520operating%2520under%2520even%2520higher%2520token%2520compression%2520ratios.%2520Based%2520on%2520these%250Ainsights%252C%2520we%2520take%2520some%2520initial%2520steps%2520towards%2520building%2520approaches%2520tailored%2520for%250Ahigh%2520token%2520compression%2520settings.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/locuslab/llava-token-compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20Optimal%20VLMs%20Need%20Only%20One%20Visual%20Token%20but%20Larger%20Models&entry.906535625=Kevin%20Y.%20Li%20and%20Sachin%20Goyal%20and%20Joao%20D.%20Semedo%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20across%0Avarious%20visual%20understanding%20and%20reasoning%20tasks.%20However%2C%20their%20real-world%0Adeployment%20is%20often%20constrained%20by%20high%20latency%20during%20inference%20due%20to%0Asubstantial%20compute%20required%20to%20process%20the%20large%20number%20of%20input%20tokens%0A%28predominantly%20from%20the%20image%29%20by%20the%20LLM.%20To%20reduce%20inference%20costs%2C%20one%20can%0Aeither%20downsize%20the%20LLM%20or%20reduce%20the%20number%20of%20input%20image-tokens%2C%20the%20latter%0Aof%20which%20has%20been%20the%20focus%20of%20many%20recent%20works%20around%20token%20compression.%0AHowever%2C%20it%20is%20unclear%20what%20the%20optimal%20trade-off%20is%2C%20as%20both%20the%20factors%0Adirectly%20affect%20the%20VLM%20performance.%20We%20first%20characterize%20this%20optimal%0Atrade-off%20between%20the%20number%20of%20visual%20tokens%20and%20LLM%20parameters%20by%0Aestablishing%20scaling%20laws%20that%20capture%20variations%20in%20performance%20with%20these%20two%0Afactors.%20Our%20results%20reveal%20a%20surprising%20trend%3A%20for%20visual%20reasoning%20tasks%2C%20the%0Ainference-optimal%20behavior%20in%20VLMs%2C%20i.e.%2C%20minimum%20downstream%20error%20at%20any%20given%0Afixed%20inference%20compute%2C%20is%20achieved%20when%20using%20the%20largest%20LLM%20that%20fits%0Awithin%20the%20inference%20budget%20while%20minimizing%20visual%20token%20count%20-%20often%20to%20a%0Asingle%20token.%20While%20the%20token%20reduction%20literature%20has%20mainly%20focused%20on%0Amaintaining%20base%20model%20performance%20by%20modestly%20reducing%20the%20token%20count%20%28e.g.%2C%0A%245-10%5Ctimes%24%29%2C%20our%20results%20indicate%20that%20the%20compute-optimal%20inference%20regime%0Arequires%20operating%20under%20even%20higher%20token%20compression%20ratios.%20Based%20on%20these%0Ainsights%2C%20we%20take%20some%20initial%20steps%20towards%20building%20approaches%20tailored%20for%0Ahigh%20token%20compression%20settings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/locuslab/llava-token-compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03312v1&entry.124074799=Read"},
{"title": "Asynchronous Perception Machine For Efficient Test-Time-Training", "author": "Rajat Modi and Yogesh Singh Rawat", "abstract": "  In this work, we propose Asynchronous Perception Machine (APM), a\ncomputationally-efficient architecture for test-time-training (TTT). APM can\nprocess patches of an image one at a time in any order asymmetrically and still\nencode semantic-awareness in the net. We demonstrate APM's ability to recognize\nout-of-distribution images without dataset-specific pre-training, augmentation\nor any-pretext task. APM offers competitive performance over existing TTT\napproaches. To perform TTT, APM just distills test sample's representation\nonce. APM possesses a unique property: it can learn using just this single\nrepresentation and starts predicting semantically-aware features.\n  APM demostrates potential applications beyond test-time-training: APM can\nscale up to a dataset of 2D images and yield semantic-clusterings in a single\nforward pass. APM also provides first empirical evidence towards validating\nGLOM's insight, i.e. input percept is a field. Therefore, APM helps us converge\ntowards an implementation which can do both interpolation and perception on a\nshared-connectionist hardware. Our code is publicly available at this link:\nhttps://rajatmodi62.github.io/apm_project_page/.\n", "link": "http://arxiv.org/abs/2410.20535v3", "date": "2024-11-05", "relevancy": 2.722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Perception%20Machine%20For%20Efficient%20Test-Time-Training&body=Title%3A%20Asynchronous%20Perception%20Machine%20For%20Efficient%20Test-Time-Training%0AAuthor%3A%20Rajat%20Modi%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20Asynchronous%20Perception%20Machine%20%28APM%29%2C%20a%0Acomputationally-efficient%20architecture%20for%20test-time-training%20%28TTT%29.%20APM%20can%0Aprocess%20patches%20of%20an%20image%20one%20at%20a%20time%20in%20any%20order%20asymmetrically%20and%20still%0Aencode%20semantic-awareness%20in%20the%20net.%20We%20demonstrate%20APM%27s%20ability%20to%20recognize%0Aout-of-distribution%20images%20without%20dataset-specific%20pre-training%2C%20augmentation%0Aor%20any-pretext%20task.%20APM%20offers%20competitive%20performance%20over%20existing%20TTT%0Aapproaches.%20To%20perform%20TTT%2C%20APM%20just%20distills%20test%20sample%27s%20representation%0Aonce.%20APM%20possesses%20a%20unique%20property%3A%20it%20can%20learn%20using%20just%20this%20single%0Arepresentation%20and%20starts%20predicting%20semantically-aware%20features.%0A%20%20APM%20demostrates%20potential%20applications%20beyond%20test-time-training%3A%20APM%20can%0Ascale%20up%20to%20a%20dataset%20of%202D%20images%20and%20yield%20semantic-clusterings%20in%20a%20single%0Aforward%20pass.%20APM%20also%20provides%20first%20empirical%20evidence%20towards%20validating%0AGLOM%27s%20insight%2C%20i.e.%20input%20percept%20is%20a%20field.%20Therefore%2C%20APM%20helps%20us%20converge%0Atowards%20an%20implementation%20which%20can%20do%20both%20interpolation%20and%20perception%20on%20a%0Ashared-connectionist%20hardware.%20Our%20code%20is%20publicly%20available%20at%20this%20link%3A%0Ahttps%3A//rajatmodi62.github.io/apm_project_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20535v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Perception%2520Machine%2520For%2520Efficient%2520Test-Time-Training%26entry.906535625%3DRajat%2520Modi%2520and%2520Yogesh%2520Singh%2520Rawat%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520Asynchronous%2520Perception%2520Machine%2520%2528APM%2529%252C%2520a%250Acomputationally-efficient%2520architecture%2520for%2520test-time-training%2520%2528TTT%2529.%2520APM%2520can%250Aprocess%2520patches%2520of%2520an%2520image%2520one%2520at%2520a%2520time%2520in%2520any%2520order%2520asymmetrically%2520and%2520still%250Aencode%2520semantic-awareness%2520in%2520the%2520net.%2520We%2520demonstrate%2520APM%2527s%2520ability%2520to%2520recognize%250Aout-of-distribution%2520images%2520without%2520dataset-specific%2520pre-training%252C%2520augmentation%250Aor%2520any-pretext%2520task.%2520APM%2520offers%2520competitive%2520performance%2520over%2520existing%2520TTT%250Aapproaches.%2520To%2520perform%2520TTT%252C%2520APM%2520just%2520distills%2520test%2520sample%2527s%2520representation%250Aonce.%2520APM%2520possesses%2520a%2520unique%2520property%253A%2520it%2520can%2520learn%2520using%2520just%2520this%2520single%250Arepresentation%2520and%2520starts%2520predicting%2520semantically-aware%2520features.%250A%2520%2520APM%2520demostrates%2520potential%2520applications%2520beyond%2520test-time-training%253A%2520APM%2520can%250Ascale%2520up%2520to%2520a%2520dataset%2520of%25202D%2520images%2520and%2520yield%2520semantic-clusterings%2520in%2520a%2520single%250Aforward%2520pass.%2520APM%2520also%2520provides%2520first%2520empirical%2520evidence%2520towards%2520validating%250AGLOM%2527s%2520insight%252C%2520i.e.%2520input%2520percept%2520is%2520a%2520field.%2520Therefore%252C%2520APM%2520helps%2520us%2520converge%250Atowards%2520an%2520implementation%2520which%2520can%2520do%2520both%2520interpolation%2520and%2520perception%2520on%2520a%250Ashared-connectionist%2520hardware.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520this%2520link%253A%250Ahttps%253A//rajatmodi62.github.io/apm_project_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20535v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Perception%20Machine%20For%20Efficient%20Test-Time-Training&entry.906535625=Rajat%20Modi%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20Asynchronous%20Perception%20Machine%20%28APM%29%2C%20a%0Acomputationally-efficient%20architecture%20for%20test-time-training%20%28TTT%29.%20APM%20can%0Aprocess%20patches%20of%20an%20image%20one%20at%20a%20time%20in%20any%20order%20asymmetrically%20and%20still%0Aencode%20semantic-awareness%20in%20the%20net.%20We%20demonstrate%20APM%27s%20ability%20to%20recognize%0Aout-of-distribution%20images%20without%20dataset-specific%20pre-training%2C%20augmentation%0Aor%20any-pretext%20task.%20APM%20offers%20competitive%20performance%20over%20existing%20TTT%0Aapproaches.%20To%20perform%20TTT%2C%20APM%20just%20distills%20test%20sample%27s%20representation%0Aonce.%20APM%20possesses%20a%20unique%20property%3A%20it%20can%20learn%20using%20just%20this%20single%0Arepresentation%20and%20starts%20predicting%20semantically-aware%20features.%0A%20%20APM%20demostrates%20potential%20applications%20beyond%20test-time-training%3A%20APM%20can%0Ascale%20up%20to%20a%20dataset%20of%202D%20images%20and%20yield%20semantic-clusterings%20in%20a%20single%0Aforward%20pass.%20APM%20also%20provides%20first%20empirical%20evidence%20towards%20validating%0AGLOM%27s%20insight%2C%20i.e.%20input%20percept%20is%20a%20field.%20Therefore%2C%20APM%20helps%20us%20converge%0Atowards%20an%20implementation%20which%20can%20do%20both%20interpolation%20and%20perception%20on%20a%0Ashared-connectionist%20hardware.%20Our%20code%20is%20publicly%20available%20at%20this%20link%3A%0Ahttps%3A//rajatmodi62.github.io/apm_project_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20535v3&entry.124074799=Read"},
{"title": "Online Analytic Exemplar-Free Continual Learning with Large Models for\n  Imbalanced Autonomous Driving Task", "author": "Huiping Zhuang and Di Fang and Kai Tong and Yuchen Liu and Ziqian Zeng and Xu Zhou and Cen Chen", "abstract": "  In autonomous driving, even a meticulously trained model can encounter\nfailures when facing unfamiliar scenarios. One of these scenarios can be\nformulated as an online continual learning (OCL) problem. That is, data come in\nan online fashion, and models are updated according to these streaming data.\nTwo major OCL challenges are catastrophic forgetting and data imbalance. To\naddress these challenges, in this paper, we propose an Analytic Exemplar-Free\nOnline Continual Learning algorithm (AEF-OCL). The AEF-OCL leverages analytic\ncontinual learning principles and employs ridge regression as a classifier for\nfeatures extracted by a large backbone network. It solves the OCL problem by\nrecursively calculating the analytical solution, ensuring an equalization\nbetween the continual learning and its joint-learning counterpart, and works\nwithout the need to save any used samples (i.e., exemplar-free). Additionally,\nwe introduce a Pseudo-Features Generator (PFG) module that recursively\nestimates the mean and the variance of real features for each class. It\nover-samples offset pseudo-features from the same normal distribution as the\nreal features, thereby addressing the data imbalance issue. Experimental\nresults demonstrate that despite being an exemplar-free strategy, our method\noutperforms various methods on the autonomous driving SODA10M dataset. Source\ncode is available at https://github.com/ZHUANGHP/Analytic-continual-learning.\n", "link": "http://arxiv.org/abs/2405.17779v2", "date": "2024-11-05", "relevancy": 2.7124, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.574}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5285}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Analytic%20Exemplar-Free%20Continual%20Learning%20with%20Large%20Models%20for%0A%20%20Imbalanced%20Autonomous%20Driving%20Task&body=Title%3A%20Online%20Analytic%20Exemplar-Free%20Continual%20Learning%20with%20Large%20Models%20for%0A%20%20Imbalanced%20Autonomous%20Driving%20Task%0AAuthor%3A%20Huiping%20Zhuang%20and%20Di%20Fang%20and%20Kai%20Tong%20and%20Yuchen%20Liu%20and%20Ziqian%20Zeng%20and%20Xu%20Zhou%20and%20Cen%20Chen%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20even%20a%20meticulously%20trained%20model%20can%20encounter%0Afailures%20when%20facing%20unfamiliar%20scenarios.%20One%20of%20these%20scenarios%20can%20be%0Aformulated%20as%20an%20online%20continual%20learning%20%28OCL%29%20problem.%20That%20is%2C%20data%20come%20in%0Aan%20online%20fashion%2C%20and%20models%20are%20updated%20according%20to%20these%20streaming%20data.%0ATwo%20major%20OCL%20challenges%20are%20catastrophic%20forgetting%20and%20data%20imbalance.%20To%0Aaddress%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20an%20Analytic%20Exemplar-Free%0AOnline%20Continual%20Learning%20algorithm%20%28AEF-OCL%29.%20The%20AEF-OCL%20leverages%20analytic%0Acontinual%20learning%20principles%20and%20employs%20ridge%20regression%20as%20a%20classifier%20for%0Afeatures%20extracted%20by%20a%20large%20backbone%20network.%20It%20solves%20the%20OCL%20problem%20by%0Arecursively%20calculating%20the%20analytical%20solution%2C%20ensuring%20an%20equalization%0Abetween%20the%20continual%20learning%20and%20its%20joint-learning%20counterpart%2C%20and%20works%0Awithout%20the%20need%20to%20save%20any%20used%20samples%20%28i.e.%2C%20exemplar-free%29.%20Additionally%2C%0Awe%20introduce%20a%20Pseudo-Features%20Generator%20%28PFG%29%20module%20that%20recursively%0Aestimates%20the%20mean%20and%20the%20variance%20of%20real%20features%20for%20each%20class.%20It%0Aover-samples%20offset%20pseudo-features%20from%20the%20same%20normal%20distribution%20as%20the%0Areal%20features%2C%20thereby%20addressing%20the%20data%20imbalance%20issue.%20Experimental%0Aresults%20demonstrate%20that%20despite%20being%20an%20exemplar-free%20strategy%2C%20our%20method%0Aoutperforms%20various%20methods%20on%20the%20autonomous%20driving%20SODA10M%20dataset.%20Source%0Acode%20is%20available%20at%20https%3A//github.com/ZHUANGHP/Analytic-continual-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17779v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Analytic%2520Exemplar-Free%2520Continual%2520Learning%2520with%2520Large%2520Models%2520for%250A%2520%2520Imbalanced%2520Autonomous%2520Driving%2520Task%26entry.906535625%3DHuiping%2520Zhuang%2520and%2520Di%2520Fang%2520and%2520Kai%2520Tong%2520and%2520Yuchen%2520Liu%2520and%2520Ziqian%2520Zeng%2520and%2520Xu%2520Zhou%2520and%2520Cen%2520Chen%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520even%2520a%2520meticulously%2520trained%2520model%2520can%2520encounter%250Afailures%2520when%2520facing%2520unfamiliar%2520scenarios.%2520One%2520of%2520these%2520scenarios%2520can%2520be%250Aformulated%2520as%2520an%2520online%2520continual%2520learning%2520%2528OCL%2529%2520problem.%2520That%2520is%252C%2520data%2520come%2520in%250Aan%2520online%2520fashion%252C%2520and%2520models%2520are%2520updated%2520according%2520to%2520these%2520streaming%2520data.%250ATwo%2520major%2520OCL%2520challenges%2520are%2520catastrophic%2520forgetting%2520and%2520data%2520imbalance.%2520To%250Aaddress%2520these%2520challenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520an%2520Analytic%2520Exemplar-Free%250AOnline%2520Continual%2520Learning%2520algorithm%2520%2528AEF-OCL%2529.%2520The%2520AEF-OCL%2520leverages%2520analytic%250Acontinual%2520learning%2520principles%2520and%2520employs%2520ridge%2520regression%2520as%2520a%2520classifier%2520for%250Afeatures%2520extracted%2520by%2520a%2520large%2520backbone%2520network.%2520It%2520solves%2520the%2520OCL%2520problem%2520by%250Arecursively%2520calculating%2520the%2520analytical%2520solution%252C%2520ensuring%2520an%2520equalization%250Abetween%2520the%2520continual%2520learning%2520and%2520its%2520joint-learning%2520counterpart%252C%2520and%2520works%250Awithout%2520the%2520need%2520to%2520save%2520any%2520used%2520samples%2520%2528i.e.%252C%2520exemplar-free%2529.%2520Additionally%252C%250Awe%2520introduce%2520a%2520Pseudo-Features%2520Generator%2520%2528PFG%2529%2520module%2520that%2520recursively%250Aestimates%2520the%2520mean%2520and%2520the%2520variance%2520of%2520real%2520features%2520for%2520each%2520class.%2520It%250Aover-samples%2520offset%2520pseudo-features%2520from%2520the%2520same%2520normal%2520distribution%2520as%2520the%250Areal%2520features%252C%2520thereby%2520addressing%2520the%2520data%2520imbalance%2520issue.%2520Experimental%250Aresults%2520demonstrate%2520that%2520despite%2520being%2520an%2520exemplar-free%2520strategy%252C%2520our%2520method%250Aoutperforms%2520various%2520methods%2520on%2520the%2520autonomous%2520driving%2520SODA10M%2520dataset.%2520Source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/ZHUANGHP/Analytic-continual-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17779v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Analytic%20Exemplar-Free%20Continual%20Learning%20with%20Large%20Models%20for%0A%20%20Imbalanced%20Autonomous%20Driving%20Task&entry.906535625=Huiping%20Zhuang%20and%20Di%20Fang%20and%20Kai%20Tong%20and%20Yuchen%20Liu%20and%20Ziqian%20Zeng%20and%20Xu%20Zhou%20and%20Cen%20Chen&entry.1292438233=%20%20In%20autonomous%20driving%2C%20even%20a%20meticulously%20trained%20model%20can%20encounter%0Afailures%20when%20facing%20unfamiliar%20scenarios.%20One%20of%20these%20scenarios%20can%20be%0Aformulated%20as%20an%20online%20continual%20learning%20%28OCL%29%20problem.%20That%20is%2C%20data%20come%20in%0Aan%20online%20fashion%2C%20and%20models%20are%20updated%20according%20to%20these%20streaming%20data.%0ATwo%20major%20OCL%20challenges%20are%20catastrophic%20forgetting%20and%20data%20imbalance.%20To%0Aaddress%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20an%20Analytic%20Exemplar-Free%0AOnline%20Continual%20Learning%20algorithm%20%28AEF-OCL%29.%20The%20AEF-OCL%20leverages%20analytic%0Acontinual%20learning%20principles%20and%20employs%20ridge%20regression%20as%20a%20classifier%20for%0Afeatures%20extracted%20by%20a%20large%20backbone%20network.%20It%20solves%20the%20OCL%20problem%20by%0Arecursively%20calculating%20the%20analytical%20solution%2C%20ensuring%20an%20equalization%0Abetween%20the%20continual%20learning%20and%20its%20joint-learning%20counterpart%2C%20and%20works%0Awithout%20the%20need%20to%20save%20any%20used%20samples%20%28i.e.%2C%20exemplar-free%29.%20Additionally%2C%0Awe%20introduce%20a%20Pseudo-Features%20Generator%20%28PFG%29%20module%20that%20recursively%0Aestimates%20the%20mean%20and%20the%20variance%20of%20real%20features%20for%20each%20class.%20It%0Aover-samples%20offset%20pseudo-features%20from%20the%20same%20normal%20distribution%20as%20the%0Areal%20features%2C%20thereby%20addressing%20the%20data%20imbalance%20issue.%20Experimental%0Aresults%20demonstrate%20that%20despite%20being%20an%20exemplar-free%20strategy%2C%20our%20method%0Aoutperforms%20various%20methods%20on%20the%20autonomous%20driving%20SODA10M%20dataset.%20Source%0Acode%20is%20available%20at%20https%3A//github.com/ZHUANGHP/Analytic-continual-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17779v2&entry.124074799=Read"},
{"title": "Attention-based Class-Conditioned Alignment for Multi-Source Domain\n  Adaptation of Object Detectors", "author": "Atif Belal and Akhil Meethal and Francisco Perdigon Romero and Marco Pedersoli and Eric Granger", "abstract": "  Domain adaptation methods for object detection (OD) strive to mitigate the\nimpact of distribution shifts by promoting feature alignment across source and\ntarget domains. Multi-source domain adaptation (MSDA) allows leveraging\nmultiple annotated source datasets and unlabeled target data to improve the\naccuracy and robustness of the detection model. Most state-of-the-art MSDA\nmethods for OD perform feature alignment in a class-agnostic manner. This is\nchallenging since the objects have unique modal information due to variations\nin object appearance across domains. A recent prototype-based approach proposed\na class-wise alignment, yet it suffers from error accumulation due to noisy\npseudo-labels that can negatively affect adaptation with imbalanced data. To\novercome these limitations, we propose an attention-based class-conditioned\nalignment method for MSDA that aligns instances of each object category across\ndomains. In particular, an attention module coupled with an adversarial domain\nclassifier allows learning domain-invariant and class-specific instance\nrepresentations. Experimental results on multiple benchmarking MSDA datasets\nindicate that our method outperforms the state-of-the-art methods and is robust\nto class imbalance using a conceptually simple class-conditioning method. Our\ncode is available at https://github.com/imatif17/ACIA.\n", "link": "http://arxiv.org/abs/2403.09918v4", "date": "2024-11-05", "relevancy": 2.6844, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5234}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-based%20Class-Conditioned%20Alignment%20for%20Multi-Source%20Domain%0A%20%20Adaptation%20of%20Object%20Detectors&body=Title%3A%20Attention-based%20Class-Conditioned%20Alignment%20for%20Multi-Source%20Domain%0A%20%20Adaptation%20of%20Object%20Detectors%0AAuthor%3A%20Atif%20Belal%20and%20Akhil%20Meethal%20and%20Francisco%20Perdigon%20Romero%20and%20Marco%20Pedersoli%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Domain%20adaptation%20methods%20for%20object%20detection%20%28OD%29%20strive%20to%20mitigate%20the%0Aimpact%20of%20distribution%20shifts%20by%20promoting%20feature%20alignment%20across%20source%20and%0Atarget%20domains.%20Multi-source%20domain%20adaptation%20%28MSDA%29%20allows%20leveraging%0Amultiple%20annotated%20source%20datasets%20and%20unlabeled%20target%20data%20to%20improve%20the%0Aaccuracy%20and%20robustness%20of%20the%20detection%20model.%20Most%20state-of-the-art%20MSDA%0Amethods%20for%20OD%20perform%20feature%20alignment%20in%20a%20class-agnostic%20manner.%20This%20is%0Achallenging%20since%20the%20objects%20have%20unique%20modal%20information%20due%20to%20variations%0Ain%20object%20appearance%20across%20domains.%20A%20recent%20prototype-based%20approach%20proposed%0Aa%20class-wise%20alignment%2C%20yet%20it%20suffers%20from%20error%20accumulation%20due%20to%20noisy%0Apseudo-labels%20that%20can%20negatively%20affect%20adaptation%20with%20imbalanced%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20an%20attention-based%20class-conditioned%0Aalignment%20method%20for%20MSDA%20that%20aligns%20instances%20of%20each%20object%20category%20across%0Adomains.%20In%20particular%2C%20an%20attention%20module%20coupled%20with%20an%20adversarial%20domain%0Aclassifier%20allows%20learning%20domain-invariant%20and%20class-specific%20instance%0Arepresentations.%20Experimental%20results%20on%20multiple%20benchmarking%20MSDA%20datasets%0Aindicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20and%20is%20robust%0Ato%20class%20imbalance%20using%20a%20conceptually%20simple%20class-conditioning%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/imatif17/ACIA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09918v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-based%2520Class-Conditioned%2520Alignment%2520for%2520Multi-Source%2520Domain%250A%2520%2520Adaptation%2520of%2520Object%2520Detectors%26entry.906535625%3DAtif%2520Belal%2520and%2520Akhil%2520Meethal%2520and%2520Francisco%2520Perdigon%2520Romero%2520and%2520Marco%2520Pedersoli%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520methods%2520for%2520object%2520detection%2520%2528OD%2529%2520strive%2520to%2520mitigate%2520the%250Aimpact%2520of%2520distribution%2520shifts%2520by%2520promoting%2520feature%2520alignment%2520across%2520source%2520and%250Atarget%2520domains.%2520Multi-source%2520domain%2520adaptation%2520%2528MSDA%2529%2520allows%2520leveraging%250Amultiple%2520annotated%2520source%2520datasets%2520and%2520unlabeled%2520target%2520data%2520to%2520improve%2520the%250Aaccuracy%2520and%2520robustness%2520of%2520the%2520detection%2520model.%2520Most%2520state-of-the-art%2520MSDA%250Amethods%2520for%2520OD%2520perform%2520feature%2520alignment%2520in%2520a%2520class-agnostic%2520manner.%2520This%2520is%250Achallenging%2520since%2520the%2520objects%2520have%2520unique%2520modal%2520information%2520due%2520to%2520variations%250Ain%2520object%2520appearance%2520across%2520domains.%2520A%2520recent%2520prototype-based%2520approach%2520proposed%250Aa%2520class-wise%2520alignment%252C%2520yet%2520it%2520suffers%2520from%2520error%2520accumulation%2520due%2520to%2520noisy%250Apseudo-labels%2520that%2520can%2520negatively%2520affect%2520adaptation%2520with%2520imbalanced%2520data.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520an%2520attention-based%2520class-conditioned%250Aalignment%2520method%2520for%2520MSDA%2520that%2520aligns%2520instances%2520of%2520each%2520object%2520category%2520across%250Adomains.%2520In%2520particular%252C%2520an%2520attention%2520module%2520coupled%2520with%2520an%2520adversarial%2520domain%250Aclassifier%2520allows%2520learning%2520domain-invariant%2520and%2520class-specific%2520instance%250Arepresentations.%2520Experimental%2520results%2520on%2520multiple%2520benchmarking%2520MSDA%2520datasets%250Aindicate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods%2520and%2520is%2520robust%250Ato%2520class%2520imbalance%2520using%2520a%2520conceptually%2520simple%2520class-conditioning%2520method.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/imatif17/ACIA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09918v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-based%20Class-Conditioned%20Alignment%20for%20Multi-Source%20Domain%0A%20%20Adaptation%20of%20Object%20Detectors&entry.906535625=Atif%20Belal%20and%20Akhil%20Meethal%20and%20Francisco%20Perdigon%20Romero%20and%20Marco%20Pedersoli%20and%20Eric%20Granger&entry.1292438233=%20%20Domain%20adaptation%20methods%20for%20object%20detection%20%28OD%29%20strive%20to%20mitigate%20the%0Aimpact%20of%20distribution%20shifts%20by%20promoting%20feature%20alignment%20across%20source%20and%0Atarget%20domains.%20Multi-source%20domain%20adaptation%20%28MSDA%29%20allows%20leveraging%0Amultiple%20annotated%20source%20datasets%20and%20unlabeled%20target%20data%20to%20improve%20the%0Aaccuracy%20and%20robustness%20of%20the%20detection%20model.%20Most%20state-of-the-art%20MSDA%0Amethods%20for%20OD%20perform%20feature%20alignment%20in%20a%20class-agnostic%20manner.%20This%20is%0Achallenging%20since%20the%20objects%20have%20unique%20modal%20information%20due%20to%20variations%0Ain%20object%20appearance%20across%20domains.%20A%20recent%20prototype-based%20approach%20proposed%0Aa%20class-wise%20alignment%2C%20yet%20it%20suffers%20from%20error%20accumulation%20due%20to%20noisy%0Apseudo-labels%20that%20can%20negatively%20affect%20adaptation%20with%20imbalanced%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20an%20attention-based%20class-conditioned%0Aalignment%20method%20for%20MSDA%20that%20aligns%20instances%20of%20each%20object%20category%20across%0Adomains.%20In%20particular%2C%20an%20attention%20module%20coupled%20with%20an%20adversarial%20domain%0Aclassifier%20allows%20learning%20domain-invariant%20and%20class-specific%20instance%0Arepresentations.%20Experimental%20results%20on%20multiple%20benchmarking%20MSDA%20datasets%0Aindicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20and%20is%20robust%0Ato%20class%20imbalance%20using%20a%20conceptually%20simple%20class-conditioning%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/imatif17/ACIA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09918v4&entry.124074799=Read"},
{"title": "Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and\n  Image-to-3D Generation", "author": "Xianghui Yang and Huiwen Shi and Bowen Zhang and Fan Yang and Jiacheng Wang and Hongxu Zhao and Xinhai Liu and Xinzhou Wang and Qingxiang Lin and Jiaao Yu and Lifu Wang and Zhuo Chen and Sicong Liu and Yuhong Liu and Yong Yang and Di Wang and Jie Jiang and Chunchao Guo", "abstract": "  While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. Our framework involves\nthe text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to\nsupport both text- and image-conditioned 3D generation. Our standard version\nhas 3x more parameters than our lite and other existing model. Our\nHunyuan3D-1.0 achieves an impressive balance between speed and quality,\nsignificantly reducing generation time while maintaining the quality and\ndiversity of the produced assets.\n", "link": "http://arxiv.org/abs/2411.02293v2", "date": "2024-11-05", "relevancy": 2.6835, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6763}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tencent%20Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%0A%20%20Image-to-3D%20Generation&body=Title%3A%20Tencent%20Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%0A%20%20Image-to-3D%20Generation%0AAuthor%3A%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Bowen%20Zhang%20and%20Fan%20Yang%20and%20Jiacheng%20Wang%20and%20Hongxu%20Zhao%20and%20Xinhai%20Liu%20and%20Xinzhou%20Wang%20and%20Qingxiang%20Lin%20and%20Jiaao%20Yu%20and%20Lifu%20Wang%20and%20Zhuo%20Chen%20and%20Sicong%20Liu%20and%20Yuhong%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Jie%20Jiang%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20While%203D%20generative%20models%20have%20greatly%20improved%20artists%27%20workflows%2C%20the%0Aexisting%20diffusion%20models%20for%203D%20generation%20suffer%20from%20slow%20generation%20and%0Apoor%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%20approach%0Anamed%20Hunyuan3D-1.0%20including%20a%20lite%20version%20and%20a%20standard%20version%2C%20that%20both%0Asupport%20text-%20and%20image-conditioned%20generation.%20In%20the%20first%20stage%2C%20we%20employ%20a%0Amulti-view%20diffusion%20model%20that%20efficiently%20generates%20multi-view%20RGB%20in%0Aapproximately%204%20seconds.%20These%20multi-view%20images%20capture%20rich%20details%20of%20the%203D%0Aasset%20from%20different%20viewpoints%2C%20relaxing%20the%20tasks%20from%20single-view%20to%0Amulti-view%20reconstruction.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20feed-forward%0Areconstruction%20model%20that%20rapidly%20and%20faithfully%20reconstructs%20the%203D%20asset%0Agiven%20the%20generated%20multi-view%20images%20in%20approximately%207%20seconds.%20The%0Areconstruction%20network%20learns%20to%20handle%20noises%20and%20in-consistency%20introduced%20by%0Athe%20multi-view%20diffusion%20and%20leverages%20the%20available%20information%20from%20the%0Acondition%20image%20to%20efficiently%20recover%20the%203D%20structure.%20Our%20framework%20involves%0Athe%20text-to-image%20model%2C%20i.e.%2C%20Hunyuan-DiT%2C%20making%20it%20a%20unified%20framework%20to%0Asupport%20both%20text-%20and%20image-conditioned%203D%20generation.%20Our%20standard%20version%0Ahas%203x%20more%20parameters%20than%20our%20lite%20and%20other%20existing%20model.%20Our%0AHunyuan3D-1.0%20achieves%20an%20impressive%20balance%20between%20speed%20and%20quality%2C%0Asignificantly%20reducing%20generation%20time%20while%20maintaining%20the%20quality%20and%0Adiversity%20of%20the%20produced%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTencent%2520Hunyuan3D-1.0%253A%2520A%2520Unified%2520Framework%2520for%2520Text-to-3D%2520and%250A%2520%2520Image-to-3D%2520Generation%26entry.906535625%3DXianghui%2520Yang%2520and%2520Huiwen%2520Shi%2520and%2520Bowen%2520Zhang%2520and%2520Fan%2520Yang%2520and%2520Jiacheng%2520Wang%2520and%2520Hongxu%2520Zhao%2520and%2520Xinhai%2520Liu%2520and%2520Xinzhou%2520Wang%2520and%2520Qingxiang%2520Lin%2520and%2520Jiaao%2520Yu%2520and%2520Lifu%2520Wang%2520and%2520Zhuo%2520Chen%2520and%2520Sicong%2520Liu%2520and%2520Yuhong%2520Liu%2520and%2520Yong%2520Yang%2520and%2520Di%2520Wang%2520and%2520Jie%2520Jiang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520While%25203D%2520generative%2520models%2520have%2520greatly%2520improved%2520artists%2527%2520workflows%252C%2520the%250Aexisting%2520diffusion%2520models%2520for%25203D%2520generation%2520suffer%2520from%2520slow%2520generation%2520and%250Apoor%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520two-stage%2520approach%250Anamed%2520Hunyuan3D-1.0%2520including%2520a%2520lite%2520version%2520and%2520a%2520standard%2520version%252C%2520that%2520both%250Asupport%2520text-%2520and%2520image-conditioned%2520generation.%2520In%2520the%2520first%2520stage%252C%2520we%2520employ%2520a%250Amulti-view%2520diffusion%2520model%2520that%2520efficiently%2520generates%2520multi-view%2520RGB%2520in%250Aapproximately%25204%2520seconds.%2520These%2520multi-view%2520images%2520capture%2520rich%2520details%2520of%2520the%25203D%250Aasset%2520from%2520different%2520viewpoints%252C%2520relaxing%2520the%2520tasks%2520from%2520single-view%2520to%250Amulti-view%2520reconstruction.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520feed-forward%250Areconstruction%2520model%2520that%2520rapidly%2520and%2520faithfully%2520reconstructs%2520the%25203D%2520asset%250Agiven%2520the%2520generated%2520multi-view%2520images%2520in%2520approximately%25207%2520seconds.%2520The%250Areconstruction%2520network%2520learns%2520to%2520handle%2520noises%2520and%2520in-consistency%2520introduced%2520by%250Athe%2520multi-view%2520diffusion%2520and%2520leverages%2520the%2520available%2520information%2520from%2520the%250Acondition%2520image%2520to%2520efficiently%2520recover%2520the%25203D%2520structure.%2520Our%2520framework%2520involves%250Athe%2520text-to-image%2520model%252C%2520i.e.%252C%2520Hunyuan-DiT%252C%2520making%2520it%2520a%2520unified%2520framework%2520to%250Asupport%2520both%2520text-%2520and%2520image-conditioned%25203D%2520generation.%2520Our%2520standard%2520version%250Ahas%25203x%2520more%2520parameters%2520than%2520our%2520lite%2520and%2520other%2520existing%2520model.%2520Our%250AHunyuan3D-1.0%2520achieves%2520an%2520impressive%2520balance%2520between%2520speed%2520and%2520quality%252C%250Asignificantly%2520reducing%2520generation%2520time%2520while%2520maintaining%2520the%2520quality%2520and%250Adiversity%2520of%2520the%2520produced%2520assets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tencent%20Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%0A%20%20Image-to-3D%20Generation&entry.906535625=Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Bowen%20Zhang%20and%20Fan%20Yang%20and%20Jiacheng%20Wang%20and%20Hongxu%20Zhao%20and%20Xinhai%20Liu%20and%20Xinzhou%20Wang%20and%20Qingxiang%20Lin%20and%20Jiaao%20Yu%20and%20Lifu%20Wang%20and%20Zhuo%20Chen%20and%20Sicong%20Liu%20and%20Yuhong%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Jie%20Jiang%20and%20Chunchao%20Guo&entry.1292438233=%20%20While%203D%20generative%20models%20have%20greatly%20improved%20artists%27%20workflows%2C%20the%0Aexisting%20diffusion%20models%20for%203D%20generation%20suffer%20from%20slow%20generation%20and%0Apoor%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%20approach%0Anamed%20Hunyuan3D-1.0%20including%20a%20lite%20version%20and%20a%20standard%20version%2C%20that%20both%0Asupport%20text-%20and%20image-conditioned%20generation.%20In%20the%20first%20stage%2C%20we%20employ%20a%0Amulti-view%20diffusion%20model%20that%20efficiently%20generates%20multi-view%20RGB%20in%0Aapproximately%204%20seconds.%20These%20multi-view%20images%20capture%20rich%20details%20of%20the%203D%0Aasset%20from%20different%20viewpoints%2C%20relaxing%20the%20tasks%20from%20single-view%20to%0Amulti-view%20reconstruction.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20feed-forward%0Areconstruction%20model%20that%20rapidly%20and%20faithfully%20reconstructs%20the%203D%20asset%0Agiven%20the%20generated%20multi-view%20images%20in%20approximately%207%20seconds.%20The%0Areconstruction%20network%20learns%20to%20handle%20noises%20and%20in-consistency%20introduced%20by%0Athe%20multi-view%20diffusion%20and%20leverages%20the%20available%20information%20from%20the%0Acondition%20image%20to%20efficiently%20recover%20the%203D%20structure.%20Our%20framework%20involves%0Athe%20text-to-image%20model%2C%20i.e.%2C%20Hunyuan-DiT%2C%20making%20it%20a%20unified%20framework%20to%0Asupport%20both%20text-%20and%20image-conditioned%203D%20generation.%20Our%20standard%20version%0Ahas%203x%20more%20parameters%20than%20our%20lite%20and%20other%20existing%20model.%20Our%0AHunyuan3D-1.0%20achieves%20an%20impressive%20balance%20between%20speed%20and%20quality%2C%0Asignificantly%20reducing%20generation%20time%20while%20maintaining%20the%20quality%20and%0Adiversity%20of%20the%20produced%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02293v2&entry.124074799=Read"},
{"title": "DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback\n  Alignment", "author": "Gongpei Zhao and Tao Wang and Congyan Lang and Yi Jin and Yidong Li and Haibin Ling", "abstract": "  Graph neural networks are recognized for their strong performance across\nvarious applications, with the backpropagation algorithm playing a central role\nin the development of most GNN models. However, despite its effectiveness, BP\nhas limitations that challenge its biological plausibility and affect the\nefficiency, scalability and parallelism of training neural networks for\ngraph-based tasks. While several non-BP training algorithms, such as the direct\nfeedback alignment, have been successfully applied to fully-connected and\nconvolutional network components for handling Euclidean data, directly adapting\nthese non-BP frameworks to manage non-Euclidean graph data in GNN models\npresents significant challenges. These challenges primarily arise from the\nviolation of the i.i.d. assumption in graph data and the difficulty in\naccessing prediction errors for all samples (nodes) within the graph. To\novercome these obstacles, in this paper we propose DFA-GNN, a novel forward\nlearning framework tailored for GNNs with a case study of semi-supervised\nlearning. The proposed method breaks the limitations of BP by using a dedicated\nforward training mechanism. Specifically, DFA-GNN extends the principles of DFA\nto adapt to graph data and unique architecture of GNNs, which incorporates the\ninformation of graph topology into the feedback links to accommodate the\nnon-Euclidean characteristics of graph data. Additionally, for semi-supervised\ngraph learning tasks, we developed a pseudo error generator that spreads\nresidual errors from training data to create a pseudo error for each unlabeled\nnode. These pseudo errors are then utilized to train GNNs using DFA. Extensive\nexperiments on 10 public benchmarks reveal that our learning framework\noutperforms not only previous non-BP methods but also the standard BP methods,\nand it exhibits excellent robustness against various types of noise and\nattacks.\n", "link": "http://arxiv.org/abs/2406.02040v2", "date": "2024-11-05", "relevancy": 2.626, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5335}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFA-GNN%3A%20Forward%20Learning%20of%20Graph%20Neural%20Networks%20by%20Direct%20Feedback%0A%20%20Alignment&body=Title%3A%20DFA-GNN%3A%20Forward%20Learning%20of%20Graph%20Neural%20Networks%20by%20Direct%20Feedback%0A%20%20Alignment%0AAuthor%3A%20Gongpei%20Zhao%20and%20Tao%20Wang%20and%20Congyan%20Lang%20and%20Yi%20Jin%20and%20Yidong%20Li%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20Graph%20neural%20networks%20are%20recognized%20for%20their%20strong%20performance%20across%0Avarious%20applications%2C%20with%20the%20backpropagation%20algorithm%20playing%20a%20central%20role%0Ain%20the%20development%20of%20most%20GNN%20models.%20However%2C%20despite%20its%20effectiveness%2C%20BP%0Ahas%20limitations%20that%20challenge%20its%20biological%20plausibility%20and%20affect%20the%0Aefficiency%2C%20scalability%20and%20parallelism%20of%20training%20neural%20networks%20for%0Agraph-based%20tasks.%20While%20several%20non-BP%20training%20algorithms%2C%20such%20as%20the%20direct%0Afeedback%20alignment%2C%20have%20been%20successfully%20applied%20to%20fully-connected%20and%0Aconvolutional%20network%20components%20for%20handling%20Euclidean%20data%2C%20directly%20adapting%0Athese%20non-BP%20frameworks%20to%20manage%20non-Euclidean%20graph%20data%20in%20GNN%20models%0Apresents%20significant%20challenges.%20These%20challenges%20primarily%20arise%20from%20the%0Aviolation%20of%20the%20i.i.d.%20assumption%20in%20graph%20data%20and%20the%20difficulty%20in%0Aaccessing%20prediction%20errors%20for%20all%20samples%20%28nodes%29%20within%20the%20graph.%20To%0Aovercome%20these%20obstacles%2C%20in%20this%20paper%20we%20propose%20DFA-GNN%2C%20a%20novel%20forward%0Alearning%20framework%20tailored%20for%20GNNs%20with%20a%20case%20study%20of%20semi-supervised%0Alearning.%20The%20proposed%20method%20breaks%20the%20limitations%20of%20BP%20by%20using%20a%20dedicated%0Aforward%20training%20mechanism.%20Specifically%2C%20DFA-GNN%20extends%20the%20principles%20of%20DFA%0Ato%20adapt%20to%20graph%20data%20and%20unique%20architecture%20of%20GNNs%2C%20which%20incorporates%20the%0Ainformation%20of%20graph%20topology%20into%20the%20feedback%20links%20to%20accommodate%20the%0Anon-Euclidean%20characteristics%20of%20graph%20data.%20Additionally%2C%20for%20semi-supervised%0Agraph%20learning%20tasks%2C%20we%20developed%20a%20pseudo%20error%20generator%20that%20spreads%0Aresidual%20errors%20from%20training%20data%20to%20create%20a%20pseudo%20error%20for%20each%20unlabeled%0Anode.%20These%20pseudo%20errors%20are%20then%20utilized%20to%20train%20GNNs%20using%20DFA.%20Extensive%0Aexperiments%20on%2010%20public%20benchmarks%20reveal%20that%20our%20learning%20framework%0Aoutperforms%20not%20only%20previous%20non-BP%20methods%20but%20also%20the%20standard%20BP%20methods%2C%0Aand%20it%20exhibits%20excellent%20robustness%20against%20various%20types%20of%20noise%20and%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFA-GNN%253A%2520Forward%2520Learning%2520of%2520Graph%2520Neural%2520Networks%2520by%2520Direct%2520Feedback%250A%2520%2520Alignment%26entry.906535625%3DGongpei%2520Zhao%2520and%2520Tao%2520Wang%2520and%2520Congyan%2520Lang%2520and%2520Yi%2520Jin%2520and%2520Yidong%2520Li%2520and%2520Haibin%2520Ling%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520are%2520recognized%2520for%2520their%2520strong%2520performance%2520across%250Avarious%2520applications%252C%2520with%2520the%2520backpropagation%2520algorithm%2520playing%2520a%2520central%2520role%250Ain%2520the%2520development%2520of%2520most%2520GNN%2520models.%2520However%252C%2520despite%2520its%2520effectiveness%252C%2520BP%250Ahas%2520limitations%2520that%2520challenge%2520its%2520biological%2520plausibility%2520and%2520affect%2520the%250Aefficiency%252C%2520scalability%2520and%2520parallelism%2520of%2520training%2520neural%2520networks%2520for%250Agraph-based%2520tasks.%2520While%2520several%2520non-BP%2520training%2520algorithms%252C%2520such%2520as%2520the%2520direct%250Afeedback%2520alignment%252C%2520have%2520been%2520successfully%2520applied%2520to%2520fully-connected%2520and%250Aconvolutional%2520network%2520components%2520for%2520handling%2520Euclidean%2520data%252C%2520directly%2520adapting%250Athese%2520non-BP%2520frameworks%2520to%2520manage%2520non-Euclidean%2520graph%2520data%2520in%2520GNN%2520models%250Apresents%2520significant%2520challenges.%2520These%2520challenges%2520primarily%2520arise%2520from%2520the%250Aviolation%2520of%2520the%2520i.i.d.%2520assumption%2520in%2520graph%2520data%2520and%2520the%2520difficulty%2520in%250Aaccessing%2520prediction%2520errors%2520for%2520all%2520samples%2520%2528nodes%2529%2520within%2520the%2520graph.%2520To%250Aovercome%2520these%2520obstacles%252C%2520in%2520this%2520paper%2520we%2520propose%2520DFA-GNN%252C%2520a%2520novel%2520forward%250Alearning%2520framework%2520tailored%2520for%2520GNNs%2520with%2520a%2520case%2520study%2520of%2520semi-supervised%250Alearning.%2520The%2520proposed%2520method%2520breaks%2520the%2520limitations%2520of%2520BP%2520by%2520using%2520a%2520dedicated%250Aforward%2520training%2520mechanism.%2520Specifically%252C%2520DFA-GNN%2520extends%2520the%2520principles%2520of%2520DFA%250Ato%2520adapt%2520to%2520graph%2520data%2520and%2520unique%2520architecture%2520of%2520GNNs%252C%2520which%2520incorporates%2520the%250Ainformation%2520of%2520graph%2520topology%2520into%2520the%2520feedback%2520links%2520to%2520accommodate%2520the%250Anon-Euclidean%2520characteristics%2520of%2520graph%2520data.%2520Additionally%252C%2520for%2520semi-supervised%250Agraph%2520learning%2520tasks%252C%2520we%2520developed%2520a%2520pseudo%2520error%2520generator%2520that%2520spreads%250Aresidual%2520errors%2520from%2520training%2520data%2520to%2520create%2520a%2520pseudo%2520error%2520for%2520each%2520unlabeled%250Anode.%2520These%2520pseudo%2520errors%2520are%2520then%2520utilized%2520to%2520train%2520GNNs%2520using%2520DFA.%2520Extensive%250Aexperiments%2520on%252010%2520public%2520benchmarks%2520reveal%2520that%2520our%2520learning%2520framework%250Aoutperforms%2520not%2520only%2520previous%2520non-BP%2520methods%2520but%2520also%2520the%2520standard%2520BP%2520methods%252C%250Aand%2520it%2520exhibits%2520excellent%2520robustness%2520against%2520various%2520types%2520of%2520noise%2520and%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFA-GNN%3A%20Forward%20Learning%20of%20Graph%20Neural%20Networks%20by%20Direct%20Feedback%0A%20%20Alignment&entry.906535625=Gongpei%20Zhao%20and%20Tao%20Wang%20and%20Congyan%20Lang%20and%20Yi%20Jin%20and%20Yidong%20Li%20and%20Haibin%20Ling&entry.1292438233=%20%20Graph%20neural%20networks%20are%20recognized%20for%20their%20strong%20performance%20across%0Avarious%20applications%2C%20with%20the%20backpropagation%20algorithm%20playing%20a%20central%20role%0Ain%20the%20development%20of%20most%20GNN%20models.%20However%2C%20despite%20its%20effectiveness%2C%20BP%0Ahas%20limitations%20that%20challenge%20its%20biological%20plausibility%20and%20affect%20the%0Aefficiency%2C%20scalability%20and%20parallelism%20of%20training%20neural%20networks%20for%0Agraph-based%20tasks.%20While%20several%20non-BP%20training%20algorithms%2C%20such%20as%20the%20direct%0Afeedback%20alignment%2C%20have%20been%20successfully%20applied%20to%20fully-connected%20and%0Aconvolutional%20network%20components%20for%20handling%20Euclidean%20data%2C%20directly%20adapting%0Athese%20non-BP%20frameworks%20to%20manage%20non-Euclidean%20graph%20data%20in%20GNN%20models%0Apresents%20significant%20challenges.%20These%20challenges%20primarily%20arise%20from%20the%0Aviolation%20of%20the%20i.i.d.%20assumption%20in%20graph%20data%20and%20the%20difficulty%20in%0Aaccessing%20prediction%20errors%20for%20all%20samples%20%28nodes%29%20within%20the%20graph.%20To%0Aovercome%20these%20obstacles%2C%20in%20this%20paper%20we%20propose%20DFA-GNN%2C%20a%20novel%20forward%0Alearning%20framework%20tailored%20for%20GNNs%20with%20a%20case%20study%20of%20semi-supervised%0Alearning.%20The%20proposed%20method%20breaks%20the%20limitations%20of%20BP%20by%20using%20a%20dedicated%0Aforward%20training%20mechanism.%20Specifically%2C%20DFA-GNN%20extends%20the%20principles%20of%20DFA%0Ato%20adapt%20to%20graph%20data%20and%20unique%20architecture%20of%20GNNs%2C%20which%20incorporates%20the%0Ainformation%20of%20graph%20topology%20into%20the%20feedback%20links%20to%20accommodate%20the%0Anon-Euclidean%20characteristics%20of%20graph%20data.%20Additionally%2C%20for%20semi-supervised%0Agraph%20learning%20tasks%2C%20we%20developed%20a%20pseudo%20error%20generator%20that%20spreads%0Aresidual%20errors%20from%20training%20data%20to%20create%20a%20pseudo%20error%20for%20each%20unlabeled%0Anode.%20These%20pseudo%20errors%20are%20then%20utilized%20to%20train%20GNNs%20using%20DFA.%20Extensive%0Aexperiments%20on%2010%20public%20benchmarks%20reveal%20that%20our%20learning%20framework%0Aoutperforms%20not%20only%20previous%20non-BP%20methods%20but%20also%20the%20standard%20BP%20methods%2C%0Aand%20it%20exhibits%20excellent%20robustness%20against%20various%20types%20of%20noise%20and%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02040v2&entry.124074799=Read"},
{"title": "Topograph: An efficient Graph-Based Framework for Strictly Topology\n  Preserving Image Segmentation", "author": "Laurin Lux and Alexander H. Berger and Alexander Weers and Nico Stucki and Daniel Rueckert and Ulrich Bauer and Johannes C. Paetzold", "abstract": "  Topological correctness plays a critical role in many image segmentation\ntasks, yet most networks are trained using pixel-wise loss functions, such as\nDice, neglecting topological accuracy. Existing topology-aware methods often\nlack robust topological guarantees, are limited to specific use cases, or\nimpose high computational costs. In this work, we propose a novel, graph-based\nframework for topologically accurate image segmentation that is both\ncomputationally efficient and generally applicable. Our method constructs a\ncomponent graph that fully encodes the topological information of both the\nprediction and ground truth, allowing us to efficiently identify topologically\ncritical regions and aggregate a loss based on local neighborhood information.\nFurthermore, we introduce a strict topological metric capturing the homotopy\nequivalence between the union and intersection of prediction-label pairs. We\nformally prove the topological guarantees of our approach and empirically\nvalidate its effectiveness on binary and multi-class datasets. Our loss\ndemonstrates state-of-the-art performance with up to fivefold faster loss\ncomputation compared to persistent homology methods.\n", "link": "http://arxiv.org/abs/2411.03228v1", "date": "2024-11-05", "relevancy": 2.6222, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5361}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.536}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topograph%3A%20An%20efficient%20Graph-Based%20Framework%20for%20Strictly%20Topology%0A%20%20Preserving%20Image%20Segmentation&body=Title%3A%20Topograph%3A%20An%20efficient%20Graph-Based%20Framework%20for%20Strictly%20Topology%0A%20%20Preserving%20Image%20Segmentation%0AAuthor%3A%20Laurin%20Lux%20and%20Alexander%20H.%20Berger%20and%20Alexander%20Weers%20and%20Nico%20Stucki%20and%20Daniel%20Rueckert%20and%20Ulrich%20Bauer%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20%20%20Topological%20correctness%20plays%20a%20critical%20role%20in%20many%20image%20segmentation%0Atasks%2C%20yet%20most%20networks%20are%20trained%20using%20pixel-wise%20loss%20functions%2C%20such%20as%0ADice%2C%20neglecting%20topological%20accuracy.%20Existing%20topology-aware%20methods%20often%0Alack%20robust%20topological%20guarantees%2C%20are%20limited%20to%20specific%20use%20cases%2C%20or%0Aimpose%20high%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%20graph-based%0Aframework%20for%20topologically%20accurate%20image%20segmentation%20that%20is%20both%0Acomputationally%20efficient%20and%20generally%20applicable.%20Our%20method%20constructs%20a%0Acomponent%20graph%20that%20fully%20encodes%20the%20topological%20information%20of%20both%20the%0Aprediction%20and%20ground%20truth%2C%20allowing%20us%20to%20efficiently%20identify%20topologically%0Acritical%20regions%20and%20aggregate%20a%20loss%20based%20on%20local%20neighborhood%20information.%0AFurthermore%2C%20we%20introduce%20a%20strict%20topological%20metric%20capturing%20the%20homotopy%0Aequivalence%20between%20the%20union%20and%20intersection%20of%20prediction-label%20pairs.%20We%0Aformally%20prove%20the%20topological%20guarantees%20of%20our%20approach%20and%20empirically%0Avalidate%20its%20effectiveness%20on%20binary%20and%20multi-class%20datasets.%20Our%20loss%0Ademonstrates%20state-of-the-art%20performance%20with%20up%20to%20fivefold%20faster%20loss%0Acomputation%20compared%20to%20persistent%20homology%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopograph%253A%2520An%2520efficient%2520Graph-Based%2520Framework%2520for%2520Strictly%2520Topology%250A%2520%2520Preserving%2520Image%2520Segmentation%26entry.906535625%3DLaurin%2520Lux%2520and%2520Alexander%2520H.%2520Berger%2520and%2520Alexander%2520Weers%2520and%2520Nico%2520Stucki%2520and%2520Daniel%2520Rueckert%2520and%2520Ulrich%2520Bauer%2520and%2520Johannes%2520C.%2520Paetzold%26entry.1292438233%3D%2520%2520Topological%2520correctness%2520plays%2520a%2520critical%2520role%2520in%2520many%2520image%2520segmentation%250Atasks%252C%2520yet%2520most%2520networks%2520are%2520trained%2520using%2520pixel-wise%2520loss%2520functions%252C%2520such%2520as%250ADice%252C%2520neglecting%2520topological%2520accuracy.%2520Existing%2520topology-aware%2520methods%2520often%250Alack%2520robust%2520topological%2520guarantees%252C%2520are%2520limited%2520to%2520specific%2520use%2520cases%252C%2520or%250Aimpose%2520high%2520computational%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%252C%2520graph-based%250Aframework%2520for%2520topologically%2520accurate%2520image%2520segmentation%2520that%2520is%2520both%250Acomputationally%2520efficient%2520and%2520generally%2520applicable.%2520Our%2520method%2520constructs%2520a%250Acomponent%2520graph%2520that%2520fully%2520encodes%2520the%2520topological%2520information%2520of%2520both%2520the%250Aprediction%2520and%2520ground%2520truth%252C%2520allowing%2520us%2520to%2520efficiently%2520identify%2520topologically%250Acritical%2520regions%2520and%2520aggregate%2520a%2520loss%2520based%2520on%2520local%2520neighborhood%2520information.%250AFurthermore%252C%2520we%2520introduce%2520a%2520strict%2520topological%2520metric%2520capturing%2520the%2520homotopy%250Aequivalence%2520between%2520the%2520union%2520and%2520intersection%2520of%2520prediction-label%2520pairs.%2520We%250Aformally%2520prove%2520the%2520topological%2520guarantees%2520of%2520our%2520approach%2520and%2520empirically%250Avalidate%2520its%2520effectiveness%2520on%2520binary%2520and%2520multi-class%2520datasets.%2520Our%2520loss%250Ademonstrates%2520state-of-the-art%2520performance%2520with%2520up%2520to%2520fivefold%2520faster%2520loss%250Acomputation%2520compared%2520to%2520persistent%2520homology%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topograph%3A%20An%20efficient%20Graph-Based%20Framework%20for%20Strictly%20Topology%0A%20%20Preserving%20Image%20Segmentation&entry.906535625=Laurin%20Lux%20and%20Alexander%20H.%20Berger%20and%20Alexander%20Weers%20and%20Nico%20Stucki%20and%20Daniel%20Rueckert%20and%20Ulrich%20Bauer%20and%20Johannes%20C.%20Paetzold&entry.1292438233=%20%20Topological%20correctness%20plays%20a%20critical%20role%20in%20many%20image%20segmentation%0Atasks%2C%20yet%20most%20networks%20are%20trained%20using%20pixel-wise%20loss%20functions%2C%20such%20as%0ADice%2C%20neglecting%20topological%20accuracy.%20Existing%20topology-aware%20methods%20often%0Alack%20robust%20topological%20guarantees%2C%20are%20limited%20to%20specific%20use%20cases%2C%20or%0Aimpose%20high%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%20graph-based%0Aframework%20for%20topologically%20accurate%20image%20segmentation%20that%20is%20both%0Acomputationally%20efficient%20and%20generally%20applicable.%20Our%20method%20constructs%20a%0Acomponent%20graph%20that%20fully%20encodes%20the%20topological%20information%20of%20both%20the%0Aprediction%20and%20ground%20truth%2C%20allowing%20us%20to%20efficiently%20identify%20topologically%0Acritical%20regions%20and%20aggregate%20a%20loss%20based%20on%20local%20neighborhood%20information.%0AFurthermore%2C%20we%20introduce%20a%20strict%20topological%20metric%20capturing%20the%20homotopy%0Aequivalence%20between%20the%20union%20and%20intersection%20of%20prediction-label%20pairs.%20We%0Aformally%20prove%20the%20topological%20guarantees%20of%20our%20approach%20and%20empirically%0Avalidate%20its%20effectiveness%20on%20binary%20and%20multi-class%20datasets.%20Our%20loss%0Ademonstrates%20state-of-the-art%20performance%20with%20up%20to%20fivefold%20faster%20loss%0Acomputation%20compared%20to%20persistent%20homology%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03228v1&entry.124074799=Read"},
{"title": "Discovering Data Structures: Nearest Neighbor Search and Beyond", "author": "Omar Salemohamed and Laurent Charlin and Shivam Garg and Vatsal Sharan and Gregory Valiant", "abstract": "  We propose a general framework for end-to-end learning of data structures.\nOur framework adapts to the underlying data distribution and provides\nfine-grained control over query and space complexity. Crucially, the data\nstructure is learned from scratch, and does not require careful initialization\nor seeding with candidate data structures/algorithms. We first apply this\nframework to the problem of nearest neighbor search. In several settings, we\nare able to reverse-engineer the learned data structures and query algorithms.\nFor 1D nearest neighbor search, the model discovers optimal distribution\n(in)dependent algorithms such as binary search and variants of interpolation\nsearch. In higher dimensions, the model learns solutions that resemble k-d\ntrees in some regimes, while in others, they have elements of\nlocality-sensitive hashing. The model can also learn useful representations of\nhigh-dimensional data and exploit them to design effective data structures. We\nalso adapt our framework to the problem of estimating frequencies over a data\nstream, and believe it could also be a powerful discovery tool for new\nproblems.\n", "link": "http://arxiv.org/abs/2411.03253v1", "date": "2024-11-05", "relevancy": 2.6134, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5201}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Data%20Structures%3A%20Nearest%20Neighbor%20Search%20and%20Beyond&body=Title%3A%20Discovering%20Data%20Structures%3A%20Nearest%20Neighbor%20Search%20and%20Beyond%0AAuthor%3A%20Omar%20Salemohamed%20and%20Laurent%20Charlin%20and%20Shivam%20Garg%20and%20Vatsal%20Sharan%20and%20Gregory%20Valiant%0AAbstract%3A%20%20%20We%20propose%20a%20general%20framework%20for%20end-to-end%20learning%20of%20data%20structures.%0AOur%20framework%20adapts%20to%20the%20underlying%20data%20distribution%20and%20provides%0Afine-grained%20control%20over%20query%20and%20space%20complexity.%20Crucially%2C%20the%20data%0Astructure%20is%20learned%20from%20scratch%2C%20and%20does%20not%20require%20careful%20initialization%0Aor%20seeding%20with%20candidate%20data%20structures/algorithms.%20We%20first%20apply%20this%0Aframework%20to%20the%20problem%20of%20nearest%20neighbor%20search.%20In%20several%20settings%2C%20we%0Aare%20able%20to%20reverse-engineer%20the%20learned%20data%20structures%20and%20query%20algorithms.%0AFor%201D%20nearest%20neighbor%20search%2C%20the%20model%20discovers%20optimal%20distribution%0A%28in%29dependent%20algorithms%20such%20as%20binary%20search%20and%20variants%20of%20interpolation%0Asearch.%20In%20higher%20dimensions%2C%20the%20model%20learns%20solutions%20that%20resemble%20k-d%0Atrees%20in%20some%20regimes%2C%20while%20in%20others%2C%20they%20have%20elements%20of%0Alocality-sensitive%20hashing.%20The%20model%20can%20also%20learn%20useful%20representations%20of%0Ahigh-dimensional%20data%20and%20exploit%20them%20to%20design%20effective%20data%20structures.%20We%0Aalso%20adapt%20our%20framework%20to%20the%20problem%20of%20estimating%20frequencies%20over%20a%20data%0Astream%2C%20and%20believe%20it%20could%20also%20be%20a%20powerful%20discovery%20tool%20for%20new%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Data%2520Structures%253A%2520Nearest%2520Neighbor%2520Search%2520and%2520Beyond%26entry.906535625%3DOmar%2520Salemohamed%2520and%2520Laurent%2520Charlin%2520and%2520Shivam%2520Garg%2520and%2520Vatsal%2520Sharan%2520and%2520Gregory%2520Valiant%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520general%2520framework%2520for%2520end-to-end%2520learning%2520of%2520data%2520structures.%250AOur%2520framework%2520adapts%2520to%2520the%2520underlying%2520data%2520distribution%2520and%2520provides%250Afine-grained%2520control%2520over%2520query%2520and%2520space%2520complexity.%2520Crucially%252C%2520the%2520data%250Astructure%2520is%2520learned%2520from%2520scratch%252C%2520and%2520does%2520not%2520require%2520careful%2520initialization%250Aor%2520seeding%2520with%2520candidate%2520data%2520structures/algorithms.%2520We%2520first%2520apply%2520this%250Aframework%2520to%2520the%2520problem%2520of%2520nearest%2520neighbor%2520search.%2520In%2520several%2520settings%252C%2520we%250Aare%2520able%2520to%2520reverse-engineer%2520the%2520learned%2520data%2520structures%2520and%2520query%2520algorithms.%250AFor%25201D%2520nearest%2520neighbor%2520search%252C%2520the%2520model%2520discovers%2520optimal%2520distribution%250A%2528in%2529dependent%2520algorithms%2520such%2520as%2520binary%2520search%2520and%2520variants%2520of%2520interpolation%250Asearch.%2520In%2520higher%2520dimensions%252C%2520the%2520model%2520learns%2520solutions%2520that%2520resemble%2520k-d%250Atrees%2520in%2520some%2520regimes%252C%2520while%2520in%2520others%252C%2520they%2520have%2520elements%2520of%250Alocality-sensitive%2520hashing.%2520The%2520model%2520can%2520also%2520learn%2520useful%2520representations%2520of%250Ahigh-dimensional%2520data%2520and%2520exploit%2520them%2520to%2520design%2520effective%2520data%2520structures.%2520We%250Aalso%2520adapt%2520our%2520framework%2520to%2520the%2520problem%2520of%2520estimating%2520frequencies%2520over%2520a%2520data%250Astream%252C%2520and%2520believe%2520it%2520could%2520also%2520be%2520a%2520powerful%2520discovery%2520tool%2520for%2520new%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Data%20Structures%3A%20Nearest%20Neighbor%20Search%20and%20Beyond&entry.906535625=Omar%20Salemohamed%20and%20Laurent%20Charlin%20and%20Shivam%20Garg%20and%20Vatsal%20Sharan%20and%20Gregory%20Valiant&entry.1292438233=%20%20We%20propose%20a%20general%20framework%20for%20end-to-end%20learning%20of%20data%20structures.%0AOur%20framework%20adapts%20to%20the%20underlying%20data%20distribution%20and%20provides%0Afine-grained%20control%20over%20query%20and%20space%20complexity.%20Crucially%2C%20the%20data%0Astructure%20is%20learned%20from%20scratch%2C%20and%20does%20not%20require%20careful%20initialization%0Aor%20seeding%20with%20candidate%20data%20structures/algorithms.%20We%20first%20apply%20this%0Aframework%20to%20the%20problem%20of%20nearest%20neighbor%20search.%20In%20several%20settings%2C%20we%0Aare%20able%20to%20reverse-engineer%20the%20learned%20data%20structures%20and%20query%20algorithms.%0AFor%201D%20nearest%20neighbor%20search%2C%20the%20model%20discovers%20optimal%20distribution%0A%28in%29dependent%20algorithms%20such%20as%20binary%20search%20and%20variants%20of%20interpolation%0Asearch.%20In%20higher%20dimensions%2C%20the%20model%20learns%20solutions%20that%20resemble%20k-d%0Atrees%20in%20some%20regimes%2C%20while%20in%20others%2C%20they%20have%20elements%20of%0Alocality-sensitive%20hashing.%20The%20model%20can%20also%20learn%20useful%20representations%20of%0Ahigh-dimensional%20data%20and%20exploit%20them%20to%20design%20effective%20data%20structures.%20We%0Aalso%20adapt%20our%20framework%20to%20the%20problem%20of%20estimating%20frequencies%20over%20a%20data%0Astream%2C%20and%20believe%20it%20could%20also%20be%20a%20powerful%20discovery%20tool%20for%20new%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03253v1&entry.124074799=Read"},
{"title": "Classification Done Right for Vision-Language Pre-Training", "author": "Huang Zilong and Ye Qinghao and Kang Bingyi and Feng Jiashi and Fan Haoqi", "abstract": "  We introduce SuperClass, a super simple classification method for\nvision-language pre-training on image-text data. Unlike its contrastive\ncounterpart CLIP who contrast with a text encoder, SuperClass directly utilizes\ntokenized raw text as supervised classification labels, without the need for\nadditional text filtering or selection. Due to the absence of the text encoding\nas contrastive target, SuperClass does not require a text encoder and does not\nneed to maintain a large batch size as CLIP does. SuperClass demonstrated\nsuperior performance on various downstream tasks, including classic computer\nvision benchmarks and vision language downstream tasks. We further explored the\nscaling behavior of SuperClass on model size, training length, or data size,\nand reported encouraging results and comparisons to CLIP.\nhttps://github.com/x-cls/superclass\n", "link": "http://arxiv.org/abs/2411.03313v1", "date": "2024-11-05", "relevancy": 2.6078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training&body=Title%3A%20Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training%0AAuthor%3A%20Huang%20Zilong%20and%20Ye%20Qinghao%20and%20Kang%20Bingyi%20and%20Feng%20Jiashi%20and%20Fan%20Haoqi%0AAbstract%3A%20%20%20We%20introduce%20SuperClass%2C%20a%20super%20simple%20classification%20method%20for%0Avision-language%20pre-training%20on%20image-text%20data.%20Unlike%20its%20contrastive%0Acounterpart%20CLIP%20who%20contrast%20with%20a%20text%20encoder%2C%20SuperClass%20directly%20utilizes%0Atokenized%20raw%20text%20as%20supervised%20classification%20labels%2C%20without%20the%20need%20for%0Aadditional%20text%20filtering%20or%20selection.%20Due%20to%20the%20absence%20of%20the%20text%20encoding%0Aas%20contrastive%20target%2C%20SuperClass%20does%20not%20require%20a%20text%20encoder%20and%20does%20not%0Aneed%20to%20maintain%20a%20large%20batch%20size%20as%20CLIP%20does.%20SuperClass%20demonstrated%0Asuperior%20performance%20on%20various%20downstream%20tasks%2C%20including%20classic%20computer%0Avision%20benchmarks%20and%20vision%20language%20downstream%20tasks.%20We%20further%20explored%20the%0Ascaling%20behavior%20of%20SuperClass%20on%20model%20size%2C%20training%20length%2C%20or%20data%20size%2C%0Aand%20reported%20encouraging%20results%20and%20comparisons%20to%20CLIP.%0Ahttps%3A//github.com/x-cls/superclass%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520Done%2520Right%2520for%2520Vision-Language%2520Pre-Training%26entry.906535625%3DHuang%2520Zilong%2520and%2520Ye%2520Qinghao%2520and%2520Kang%2520Bingyi%2520and%2520Feng%2520Jiashi%2520and%2520Fan%2520Haoqi%26entry.1292438233%3D%2520%2520We%2520introduce%2520SuperClass%252C%2520a%2520super%2520simple%2520classification%2520method%2520for%250Avision-language%2520pre-training%2520on%2520image-text%2520data.%2520Unlike%2520its%2520contrastive%250Acounterpart%2520CLIP%2520who%2520contrast%2520with%2520a%2520text%2520encoder%252C%2520SuperClass%2520directly%2520utilizes%250Atokenized%2520raw%2520text%2520as%2520supervised%2520classification%2520labels%252C%2520without%2520the%2520need%2520for%250Aadditional%2520text%2520filtering%2520or%2520selection.%2520Due%2520to%2520the%2520absence%2520of%2520the%2520text%2520encoding%250Aas%2520contrastive%2520target%252C%2520SuperClass%2520does%2520not%2520require%2520a%2520text%2520encoder%2520and%2520does%2520not%250Aneed%2520to%2520maintain%2520a%2520large%2520batch%2520size%2520as%2520CLIP%2520does.%2520SuperClass%2520demonstrated%250Asuperior%2520performance%2520on%2520various%2520downstream%2520tasks%252C%2520including%2520classic%2520computer%250Avision%2520benchmarks%2520and%2520vision%2520language%2520downstream%2520tasks.%2520We%2520further%2520explored%2520the%250Ascaling%2520behavior%2520of%2520SuperClass%2520on%2520model%2520size%252C%2520training%2520length%252C%2520or%2520data%2520size%252C%250Aand%2520reported%2520encouraging%2520results%2520and%2520comparisons%2520to%2520CLIP.%250Ahttps%253A//github.com/x-cls/superclass%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training&entry.906535625=Huang%20Zilong%20and%20Ye%20Qinghao%20and%20Kang%20Bingyi%20and%20Feng%20Jiashi%20and%20Fan%20Haoqi&entry.1292438233=%20%20We%20introduce%20SuperClass%2C%20a%20super%20simple%20classification%20method%20for%0Avision-language%20pre-training%20on%20image-text%20data.%20Unlike%20its%20contrastive%0Acounterpart%20CLIP%20who%20contrast%20with%20a%20text%20encoder%2C%20SuperClass%20directly%20utilizes%0Atokenized%20raw%20text%20as%20supervised%20classification%20labels%2C%20without%20the%20need%20for%0Aadditional%20text%20filtering%20or%20selection.%20Due%20to%20the%20absence%20of%20the%20text%20encoding%0Aas%20contrastive%20target%2C%20SuperClass%20does%20not%20require%20a%20text%20encoder%20and%20does%20not%0Aneed%20to%20maintain%20a%20large%20batch%20size%20as%20CLIP%20does.%20SuperClass%20demonstrated%0Asuperior%20performance%20on%20various%20downstream%20tasks%2C%20including%20classic%20computer%0Avision%20benchmarks%20and%20vision%20language%20downstream%20tasks.%20We%20further%20explored%20the%0Ascaling%20behavior%20of%20SuperClass%20on%20model%20size%2C%20training%20length%2C%20or%20data%20size%2C%0Aand%20reported%20encouraging%20results%20and%20comparisons%20to%20CLIP.%0Ahttps%3A//github.com/x-cls/superclass%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03313v1&entry.124074799=Read"},
{"title": "Better, Not Just More: Data-Centric Machine Learning for Earth\n  Observation", "author": "Ribana Roscher and Marc Ru\u00dfwurm and Caroline Gevaert and Michael Kampffmeyer and Jefersson A. dos Santos and Maria Vakalopoulou and Ronny H\u00e4nsch and Stine Hansen and Keiller Nogueira and Jonathan Prexl and Devis Tuia", "abstract": "  Recent developments and research in modern machine learning have led to\nsubstantial improvements in the geospatial field. Although numerous deep\nlearning architectures and models have been proposed, the majority of them have\nbeen solely developed on benchmark datasets that lack strong real-world\nrelevance. Furthermore, the performance of many methods has already saturated\non these datasets. We argue that a shift from a model-centric view to a\ncomplementary data-centric perspective is necessary for further improvements in\naccuracy, generalization ability, and real impact on end-user applications.\nFurthermore, considering the entire machine learning cycle-from problem\ndefinition to model deployment with feedback-is crucial for enhancing machine\nlearning models that can be reliable in unforeseen situations. This work\npresents a definition as well as a precise categorization and overview of\nautomated data-centric learning approaches for geospatial data. It highlights\nthe complementary role of data-centric learning with respect to model-centric\nin the larger machine learning deployment cycle. We review papers across the\nentire geospatial field and categorize them into different groups. A set of\nrepresentative experiments shows concrete implementation examples. These\nexamples provide concrete steps to act on geospatial data with data-centric\nmachine learning approaches.\n", "link": "http://arxiv.org/abs/2312.05327v3", "date": "2024-11-05", "relevancy": 2.5981, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%2C%20Not%20Just%20More%3A%20Data-Centric%20Machine%20Learning%20for%20Earth%0A%20%20Observation&body=Title%3A%20Better%2C%20Not%20Just%20More%3A%20Data-Centric%20Machine%20Learning%20for%20Earth%0A%20%20Observation%0AAuthor%3A%20Ribana%20Roscher%20and%20Marc%20Ru%C3%9Fwurm%20and%20Caroline%20Gevaert%20and%20Michael%20Kampffmeyer%20and%20Jefersson%20A.%20dos%20Santos%20and%20Maria%20Vakalopoulou%20and%20Ronny%20H%C3%A4nsch%20and%20Stine%20Hansen%20and%20Keiller%20Nogueira%20and%20Jonathan%20Prexl%20and%20Devis%20Tuia%0AAbstract%3A%20%20%20Recent%20developments%20and%20research%20in%20modern%20machine%20learning%20have%20led%20to%0Asubstantial%20improvements%20in%20the%20geospatial%20field.%20Although%20numerous%20deep%0Alearning%20architectures%20and%20models%20have%20been%20proposed%2C%20the%20majority%20of%20them%20have%0Abeen%20solely%20developed%20on%20benchmark%20datasets%20that%20lack%20strong%20real-world%0Arelevance.%20Furthermore%2C%20the%20performance%20of%20many%20methods%20has%20already%20saturated%0Aon%20these%20datasets.%20We%20argue%20that%20a%20shift%20from%20a%20model-centric%20view%20to%20a%0Acomplementary%20data-centric%20perspective%20is%20necessary%20for%20further%20improvements%20in%0Aaccuracy%2C%20generalization%20ability%2C%20and%20real%20impact%20on%20end-user%20applications.%0AFurthermore%2C%20considering%20the%20entire%20machine%20learning%20cycle-from%20problem%0Adefinition%20to%20model%20deployment%20with%20feedback-is%20crucial%20for%20enhancing%20machine%0Alearning%20models%20that%20can%20be%20reliable%20in%20unforeseen%20situations.%20This%20work%0Apresents%20a%20definition%20as%20well%20as%20a%20precise%20categorization%20and%20overview%20of%0Aautomated%20data-centric%20learning%20approaches%20for%20geospatial%20data.%20It%20highlights%0Athe%20complementary%20role%20of%20data-centric%20learning%20with%20respect%20to%20model-centric%0Ain%20the%20larger%20machine%20learning%20deployment%20cycle.%20We%20review%20papers%20across%20the%0Aentire%20geospatial%20field%20and%20categorize%20them%20into%20different%20groups.%20A%20set%20of%0Arepresentative%20experiments%20shows%20concrete%20implementation%20examples.%20These%0Aexamples%20provide%20concrete%20steps%20to%20act%20on%20geospatial%20data%20with%20data-centric%0Amachine%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05327v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%252C%2520Not%2520Just%2520More%253A%2520Data-Centric%2520Machine%2520Learning%2520for%2520Earth%250A%2520%2520Observation%26entry.906535625%3DRibana%2520Roscher%2520and%2520Marc%2520Ru%25C3%259Fwurm%2520and%2520Caroline%2520Gevaert%2520and%2520Michael%2520Kampffmeyer%2520and%2520Jefersson%2520A.%2520dos%2520Santos%2520and%2520Maria%2520Vakalopoulou%2520and%2520Ronny%2520H%25C3%25A4nsch%2520and%2520Stine%2520Hansen%2520and%2520Keiller%2520Nogueira%2520and%2520Jonathan%2520Prexl%2520and%2520Devis%2520Tuia%26entry.1292438233%3D%2520%2520Recent%2520developments%2520and%2520research%2520in%2520modern%2520machine%2520learning%2520have%2520led%2520to%250Asubstantial%2520improvements%2520in%2520the%2520geospatial%2520field.%2520Although%2520numerous%2520deep%250Alearning%2520architectures%2520and%2520models%2520have%2520been%2520proposed%252C%2520the%2520majority%2520of%2520them%2520have%250Abeen%2520solely%2520developed%2520on%2520benchmark%2520datasets%2520that%2520lack%2520strong%2520real-world%250Arelevance.%2520Furthermore%252C%2520the%2520performance%2520of%2520many%2520methods%2520has%2520already%2520saturated%250Aon%2520these%2520datasets.%2520We%2520argue%2520that%2520a%2520shift%2520from%2520a%2520model-centric%2520view%2520to%2520a%250Acomplementary%2520data-centric%2520perspective%2520is%2520necessary%2520for%2520further%2520improvements%2520in%250Aaccuracy%252C%2520generalization%2520ability%252C%2520and%2520real%2520impact%2520on%2520end-user%2520applications.%250AFurthermore%252C%2520considering%2520the%2520entire%2520machine%2520learning%2520cycle-from%2520problem%250Adefinition%2520to%2520model%2520deployment%2520with%2520feedback-is%2520crucial%2520for%2520enhancing%2520machine%250Alearning%2520models%2520that%2520can%2520be%2520reliable%2520in%2520unforeseen%2520situations.%2520This%2520work%250Apresents%2520a%2520definition%2520as%2520well%2520as%2520a%2520precise%2520categorization%2520and%2520overview%2520of%250Aautomated%2520data-centric%2520learning%2520approaches%2520for%2520geospatial%2520data.%2520It%2520highlights%250Athe%2520complementary%2520role%2520of%2520data-centric%2520learning%2520with%2520respect%2520to%2520model-centric%250Ain%2520the%2520larger%2520machine%2520learning%2520deployment%2520cycle.%2520We%2520review%2520papers%2520across%2520the%250Aentire%2520geospatial%2520field%2520and%2520categorize%2520them%2520into%2520different%2520groups.%2520A%2520set%2520of%250Arepresentative%2520experiments%2520shows%2520concrete%2520implementation%2520examples.%2520These%250Aexamples%2520provide%2520concrete%2520steps%2520to%2520act%2520on%2520geospatial%2520data%2520with%2520data-centric%250Amachine%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05327v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%2C%20Not%20Just%20More%3A%20Data-Centric%20Machine%20Learning%20for%20Earth%0A%20%20Observation&entry.906535625=Ribana%20Roscher%20and%20Marc%20Ru%C3%9Fwurm%20and%20Caroline%20Gevaert%20and%20Michael%20Kampffmeyer%20and%20Jefersson%20A.%20dos%20Santos%20and%20Maria%20Vakalopoulou%20and%20Ronny%20H%C3%A4nsch%20and%20Stine%20Hansen%20and%20Keiller%20Nogueira%20and%20Jonathan%20Prexl%20and%20Devis%20Tuia&entry.1292438233=%20%20Recent%20developments%20and%20research%20in%20modern%20machine%20learning%20have%20led%20to%0Asubstantial%20improvements%20in%20the%20geospatial%20field.%20Although%20numerous%20deep%0Alearning%20architectures%20and%20models%20have%20been%20proposed%2C%20the%20majority%20of%20them%20have%0Abeen%20solely%20developed%20on%20benchmark%20datasets%20that%20lack%20strong%20real-world%0Arelevance.%20Furthermore%2C%20the%20performance%20of%20many%20methods%20has%20already%20saturated%0Aon%20these%20datasets.%20We%20argue%20that%20a%20shift%20from%20a%20model-centric%20view%20to%20a%0Acomplementary%20data-centric%20perspective%20is%20necessary%20for%20further%20improvements%20in%0Aaccuracy%2C%20generalization%20ability%2C%20and%20real%20impact%20on%20end-user%20applications.%0AFurthermore%2C%20considering%20the%20entire%20machine%20learning%20cycle-from%20problem%0Adefinition%20to%20model%20deployment%20with%20feedback-is%20crucial%20for%20enhancing%20machine%0Alearning%20models%20that%20can%20be%20reliable%20in%20unforeseen%20situations.%20This%20work%0Apresents%20a%20definition%20as%20well%20as%20a%20precise%20categorization%20and%20overview%20of%0Aautomated%20data-centric%20learning%20approaches%20for%20geospatial%20data.%20It%20highlights%0Athe%20complementary%20role%20of%20data-centric%20learning%20with%20respect%20to%20model-centric%0Ain%20the%20larger%20machine%20learning%20deployment%20cycle.%20We%20review%20papers%20across%20the%0Aentire%20geospatial%20field%20and%20categorize%20them%20into%20different%20groups.%20A%20set%20of%0Arepresentative%20experiments%20shows%20concrete%20implementation%20examples.%20These%0Aexamples%20provide%20concrete%20steps%20to%20act%20on%20geospatial%20data%20with%20data-centric%0Amachine%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05327v3&entry.124074799=Read"},
{"title": "DA-MoE: Addressing Depth-Sensitivity in Graph-Level Analysis through\n  Mixture of Experts", "author": "Zelin Yao and Chuang Liu and Xianke Meng and Yibing Zhan and Jia Wu and Shirui Pan and Wenbin Hu", "abstract": "  Graph neural networks (GNNs) are gaining popularity for processing\ngraph-structured data. In real-world scenarios, graph data within the same\ndataset can vary significantly in scale. This variability leads to\ndepth-sensitivity, where the optimal depth of GNN layers depends on the scale\nof the graph data. Empirically, fewer layers are sufficient for message passing\nin smaller graphs, while larger graphs typically require deeper networks to\ncapture long-range dependencies and global features. However, existing methods\ngenerally use a fixed number of GNN layers to generate representations for all\ngraphs, overlooking the depth-sensitivity issue in graph structure data. To\naddress this challenge, we propose the depth adaptive mixture of expert\n(DA-MoE) method, which incorporates two main improvements to GNN backbone:\n\\textbf{1)} DA-MoE employs different GNN layers, each considered an expert with\nits own parameters. Such a design allows the model to flexibly aggregate\ninformation at different scales, effectively addressing the depth-sensitivity\nissue in graph data. \\textbf{2)} DA-MoE utilizes GNN to capture the structural\ninformation instead of the linear projections in the gating network. Thus, the\ngating network enables the model to capture complex patterns and dependencies\nwithin the data. By leveraging these improvements, each expert in DA-MoE\nspecifically learns distinct graph patterns at different scales. Furthermore,\ncomprehensive experiments on the TU dataset and open graph benchmark (OGB) have\nshown that DA-MoE consistently surpasses existing baselines on various tasks,\nincluding graph, node, and link-level analyses. The code are available at\n\\url{https://github.com/Celin-Yao/DA-MoE}.\n", "link": "http://arxiv.org/abs/2411.03025v1", "date": "2024-11-05", "relevancy": 2.5525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5403}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5004}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-MoE%3A%20Addressing%20Depth-Sensitivity%20in%20Graph-Level%20Analysis%20through%0A%20%20Mixture%20of%20Experts&body=Title%3A%20DA-MoE%3A%20Addressing%20Depth-Sensitivity%20in%20Graph-Level%20Analysis%20through%0A%20%20Mixture%20of%20Experts%0AAuthor%3A%20Zelin%20Yao%20and%20Chuang%20Liu%20and%20Xianke%20Meng%20and%20Yibing%20Zhan%20and%20Jia%20Wu%20and%20Shirui%20Pan%20and%20Wenbin%20Hu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20gaining%20popularity%20for%20processing%0Agraph-structured%20data.%20In%20real-world%20scenarios%2C%20graph%20data%20within%20the%20same%0Adataset%20can%20vary%20significantly%20in%20scale.%20This%20variability%20leads%20to%0Adepth-sensitivity%2C%20where%20the%20optimal%20depth%20of%20GNN%20layers%20depends%20on%20the%20scale%0Aof%20the%20graph%20data.%20Empirically%2C%20fewer%20layers%20are%20sufficient%20for%20message%20passing%0Ain%20smaller%20graphs%2C%20while%20larger%20graphs%20typically%20require%20deeper%20networks%20to%0Acapture%20long-range%20dependencies%20and%20global%20features.%20However%2C%20existing%20methods%0Agenerally%20use%20a%20fixed%20number%20of%20GNN%20layers%20to%20generate%20representations%20for%20all%0Agraphs%2C%20overlooking%20the%20depth-sensitivity%20issue%20in%20graph%20structure%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20depth%20adaptive%20mixture%20of%20expert%0A%28DA-MoE%29%20method%2C%20which%20incorporates%20two%20main%20improvements%20to%20GNN%20backbone%3A%0A%5Ctextbf%7B1%29%7D%20DA-MoE%20employs%20different%20GNN%20layers%2C%20each%20considered%20an%20expert%20with%0Aits%20own%20parameters.%20Such%20a%20design%20allows%20the%20model%20to%20flexibly%20aggregate%0Ainformation%20at%20different%20scales%2C%20effectively%20addressing%20the%20depth-sensitivity%0Aissue%20in%20graph%20data.%20%5Ctextbf%7B2%29%7D%20DA-MoE%20utilizes%20GNN%20to%20capture%20the%20structural%0Ainformation%20instead%20of%20the%20linear%20projections%20in%20the%20gating%20network.%20Thus%2C%20the%0Agating%20network%20enables%20the%20model%20to%20capture%20complex%20patterns%20and%20dependencies%0Awithin%20the%20data.%20By%20leveraging%20these%20improvements%2C%20each%20expert%20in%20DA-MoE%0Aspecifically%20learns%20distinct%20graph%20patterns%20at%20different%20scales.%20Furthermore%2C%0Acomprehensive%20experiments%20on%20the%20TU%20dataset%20and%20open%20graph%20benchmark%20%28OGB%29%20have%0Ashown%20that%20DA-MoE%20consistently%20surpasses%20existing%20baselines%20on%20various%20tasks%2C%0Aincluding%20graph%2C%20node%2C%20and%20link-level%20analyses.%20The%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Celin-Yao/DA-MoE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-MoE%253A%2520Addressing%2520Depth-Sensitivity%2520in%2520Graph-Level%2520Analysis%2520through%250A%2520%2520Mixture%2520of%2520Experts%26entry.906535625%3DZelin%2520Yao%2520and%2520Chuang%2520Liu%2520and%2520Xianke%2520Meng%2520and%2520Yibing%2520Zhan%2520and%2520Jia%2520Wu%2520and%2520Shirui%2520Pan%2520and%2520Wenbin%2520Hu%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520gaining%2520popularity%2520for%2520processing%250Agraph-structured%2520data.%2520In%2520real-world%2520scenarios%252C%2520graph%2520data%2520within%2520the%2520same%250Adataset%2520can%2520vary%2520significantly%2520in%2520scale.%2520This%2520variability%2520leads%2520to%250Adepth-sensitivity%252C%2520where%2520the%2520optimal%2520depth%2520of%2520GNN%2520layers%2520depends%2520on%2520the%2520scale%250Aof%2520the%2520graph%2520data.%2520Empirically%252C%2520fewer%2520layers%2520are%2520sufficient%2520for%2520message%2520passing%250Ain%2520smaller%2520graphs%252C%2520while%2520larger%2520graphs%2520typically%2520require%2520deeper%2520networks%2520to%250Acapture%2520long-range%2520dependencies%2520and%2520global%2520features.%2520However%252C%2520existing%2520methods%250Agenerally%2520use%2520a%2520fixed%2520number%2520of%2520GNN%2520layers%2520to%2520generate%2520representations%2520for%2520all%250Agraphs%252C%2520overlooking%2520the%2520depth-sensitivity%2520issue%2520in%2520graph%2520structure%2520data.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520the%2520depth%2520adaptive%2520mixture%2520of%2520expert%250A%2528DA-MoE%2529%2520method%252C%2520which%2520incorporates%2520two%2520main%2520improvements%2520to%2520GNN%2520backbone%253A%250A%255Ctextbf%257B1%2529%257D%2520DA-MoE%2520employs%2520different%2520GNN%2520layers%252C%2520each%2520considered%2520an%2520expert%2520with%250Aits%2520own%2520parameters.%2520Such%2520a%2520design%2520allows%2520the%2520model%2520to%2520flexibly%2520aggregate%250Ainformation%2520at%2520different%2520scales%252C%2520effectively%2520addressing%2520the%2520depth-sensitivity%250Aissue%2520in%2520graph%2520data.%2520%255Ctextbf%257B2%2529%257D%2520DA-MoE%2520utilizes%2520GNN%2520to%2520capture%2520the%2520structural%250Ainformation%2520instead%2520of%2520the%2520linear%2520projections%2520in%2520the%2520gating%2520network.%2520Thus%252C%2520the%250Agating%2520network%2520enables%2520the%2520model%2520to%2520capture%2520complex%2520patterns%2520and%2520dependencies%250Awithin%2520the%2520data.%2520By%2520leveraging%2520these%2520improvements%252C%2520each%2520expert%2520in%2520DA-MoE%250Aspecifically%2520learns%2520distinct%2520graph%2520patterns%2520at%2520different%2520scales.%2520Furthermore%252C%250Acomprehensive%2520experiments%2520on%2520the%2520TU%2520dataset%2520and%2520open%2520graph%2520benchmark%2520%2528OGB%2529%2520have%250Ashown%2520that%2520DA-MoE%2520consistently%2520surpasses%2520existing%2520baselines%2520on%2520various%2520tasks%252C%250Aincluding%2520graph%252C%2520node%252C%2520and%2520link-level%2520analyses.%2520The%2520code%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Celin-Yao/DA-MoE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-MoE%3A%20Addressing%20Depth-Sensitivity%20in%20Graph-Level%20Analysis%20through%0A%20%20Mixture%20of%20Experts&entry.906535625=Zelin%20Yao%20and%20Chuang%20Liu%20and%20Xianke%20Meng%20and%20Yibing%20Zhan%20and%20Jia%20Wu%20and%20Shirui%20Pan%20and%20Wenbin%20Hu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20gaining%20popularity%20for%20processing%0Agraph-structured%20data.%20In%20real-world%20scenarios%2C%20graph%20data%20within%20the%20same%0Adataset%20can%20vary%20significantly%20in%20scale.%20This%20variability%20leads%20to%0Adepth-sensitivity%2C%20where%20the%20optimal%20depth%20of%20GNN%20layers%20depends%20on%20the%20scale%0Aof%20the%20graph%20data.%20Empirically%2C%20fewer%20layers%20are%20sufficient%20for%20message%20passing%0Ain%20smaller%20graphs%2C%20while%20larger%20graphs%20typically%20require%20deeper%20networks%20to%0Acapture%20long-range%20dependencies%20and%20global%20features.%20However%2C%20existing%20methods%0Agenerally%20use%20a%20fixed%20number%20of%20GNN%20layers%20to%20generate%20representations%20for%20all%0Agraphs%2C%20overlooking%20the%20depth-sensitivity%20issue%20in%20graph%20structure%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20depth%20adaptive%20mixture%20of%20expert%0A%28DA-MoE%29%20method%2C%20which%20incorporates%20two%20main%20improvements%20to%20GNN%20backbone%3A%0A%5Ctextbf%7B1%29%7D%20DA-MoE%20employs%20different%20GNN%20layers%2C%20each%20considered%20an%20expert%20with%0Aits%20own%20parameters.%20Such%20a%20design%20allows%20the%20model%20to%20flexibly%20aggregate%0Ainformation%20at%20different%20scales%2C%20effectively%20addressing%20the%20depth-sensitivity%0Aissue%20in%20graph%20data.%20%5Ctextbf%7B2%29%7D%20DA-MoE%20utilizes%20GNN%20to%20capture%20the%20structural%0Ainformation%20instead%20of%20the%20linear%20projections%20in%20the%20gating%20network.%20Thus%2C%20the%0Agating%20network%20enables%20the%20model%20to%20capture%20complex%20patterns%20and%20dependencies%0Awithin%20the%20data.%20By%20leveraging%20these%20improvements%2C%20each%20expert%20in%20DA-MoE%0Aspecifically%20learns%20distinct%20graph%20patterns%20at%20different%20scales.%20Furthermore%2C%0Acomprehensive%20experiments%20on%20the%20TU%20dataset%20and%20open%20graph%20benchmark%20%28OGB%29%20have%0Ashown%20that%20DA-MoE%20consistently%20surpasses%20existing%20baselines%20on%20various%20tasks%2C%0Aincluding%20graph%2C%20node%2C%20and%20link-level%20analyses.%20The%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Celin-Yao/DA-MoE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03025v1&entry.124074799=Read"},
{"title": "A Framework for Real-Time Volcano-Seismic Event Recognition Based on\n  Multi-Station Seismograms and Semantic Segmentation Models", "author": "Camilo Espinosa-Curilem and Millaray Curilem and Daniel Basualto", "abstract": "  In volcano monitoring, effective recognition of seismic events is essential\nfor understanding volcanic activity and raising timely warning alerts.\nTraditional methods rely on manual analysis, which can be subjective and\nlabor-intensive. Furthermore, current automatic approaches often tackle\ndetection and classification separately, mostly rely on single station\ninformation and generally require tailored preprocessing and representations to\nperform predictions. These limitations often hinder their application to\nreal-time monitoring and utilization across different volcano conditions. This\nstudy introduces a novel approach that utilizes Semantic Segmentation models to\nautomate seismic event recognition by applying a straight forward\ntransformation of multi-channel 1D signals into 2D representations, enabling\ntheir use as images. Our framework employs a data-driven, end-to-end design\nthat integrates multi-station seismic data with minimal preprocessing,\nperforming both detection and classification simultaneously for five seismic\nevent classes. We evaluated four state-of-the-art segmentation models (UNet,\nUNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events\nrecorded at four different Chilean volcanoes: Nevados del Chill\\'an Volcanic\nComplex, Laguna del Maule, Villarrica and Puyehue-Cord\\'on Caulle. Among these\nmodels, the UNet architecture was identified as the most effective model,\nachieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and\n0.88, respectively, and demonstrating superior noise robustness and model\nflexibility to unseen volcano datasets.\n", "link": "http://arxiv.org/abs/2410.20595v3", "date": "2024-11-05", "relevancy": 2.5416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Real-Time%20Volcano-Seismic%20Event%20Recognition%20Based%20on%0A%20%20Multi-Station%20Seismograms%20and%20Semantic%20Segmentation%20Models&body=Title%3A%20A%20Framework%20for%20Real-Time%20Volcano-Seismic%20Event%20Recognition%20Based%20on%0A%20%20Multi-Station%20Seismograms%20and%20Semantic%20Segmentation%20Models%0AAuthor%3A%20Camilo%20Espinosa-Curilem%20and%20Millaray%20Curilem%20and%20Daniel%20Basualto%0AAbstract%3A%20%20%20In%20volcano%20monitoring%2C%20effective%20recognition%20of%20seismic%20events%20is%20essential%0Afor%20understanding%20volcanic%20activity%20and%20raising%20timely%20warning%20alerts.%0ATraditional%20methods%20rely%20on%20manual%20analysis%2C%20which%20can%20be%20subjective%20and%0Alabor-intensive.%20Furthermore%2C%20current%20automatic%20approaches%20often%20tackle%0Adetection%20and%20classification%20separately%2C%20mostly%20rely%20on%20single%20station%0Ainformation%20and%20generally%20require%20tailored%20preprocessing%20and%20representations%20to%0Aperform%20predictions.%20These%20limitations%20often%20hinder%20their%20application%20to%0Areal-time%20monitoring%20and%20utilization%20across%20different%20volcano%20conditions.%20This%0Astudy%20introduces%20a%20novel%20approach%20that%20utilizes%20Semantic%20Segmentation%20models%20to%0Aautomate%20seismic%20event%20recognition%20by%20applying%20a%20straight%20forward%0Atransformation%20of%20multi-channel%201D%20signals%20into%202D%20representations%2C%20enabling%0Atheir%20use%20as%20images.%20Our%20framework%20employs%20a%20data-driven%2C%20end-to-end%20design%0Athat%20integrates%20multi-station%20seismic%20data%20with%20minimal%20preprocessing%2C%0Aperforming%20both%20detection%20and%20classification%20simultaneously%20for%20five%20seismic%0Aevent%20classes.%20We%20evaluated%20four%20state-of-the-art%20segmentation%20models%20%28UNet%2C%0AUNet%2B%2B%2C%20DeepLabV3%2B%20and%20SwinUNet%29%20on%20approximately%2025.000%20seismic%20events%0Arecorded%20at%20four%20different%20Chilean%20volcanoes%3A%20Nevados%20del%20Chill%5C%27an%20Volcanic%0AComplex%2C%20Laguna%20del%20Maule%2C%20Villarrica%20and%20Puyehue-Cord%5C%27on%20Caulle.%20Among%20these%0Amodels%2C%20the%20UNet%20architecture%20was%20identified%20as%20the%20most%20effective%20model%2C%0Aachieving%20mean%20F1%20and%20Intersection%20over%20Union%20%28IoU%29%20scores%20of%20up%20to%200.91%20and%0A0.88%2C%20respectively%2C%20and%20demonstrating%20superior%20noise%20robustness%20and%20model%0Aflexibility%20to%20unseen%20volcano%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20595v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Real-Time%2520Volcano-Seismic%2520Event%2520Recognition%2520Based%2520on%250A%2520%2520Multi-Station%2520Seismograms%2520and%2520Semantic%2520Segmentation%2520Models%26entry.906535625%3DCamilo%2520Espinosa-Curilem%2520and%2520Millaray%2520Curilem%2520and%2520Daniel%2520Basualto%26entry.1292438233%3D%2520%2520In%2520volcano%2520monitoring%252C%2520effective%2520recognition%2520of%2520seismic%2520events%2520is%2520essential%250Afor%2520understanding%2520volcanic%2520activity%2520and%2520raising%2520timely%2520warning%2520alerts.%250ATraditional%2520methods%2520rely%2520on%2520manual%2520analysis%252C%2520which%2520can%2520be%2520subjective%2520and%250Alabor-intensive.%2520Furthermore%252C%2520current%2520automatic%2520approaches%2520often%2520tackle%250Adetection%2520and%2520classification%2520separately%252C%2520mostly%2520rely%2520on%2520single%2520station%250Ainformation%2520and%2520generally%2520require%2520tailored%2520preprocessing%2520and%2520representations%2520to%250Aperform%2520predictions.%2520These%2520limitations%2520often%2520hinder%2520their%2520application%2520to%250Areal-time%2520monitoring%2520and%2520utilization%2520across%2520different%2520volcano%2520conditions.%2520This%250Astudy%2520introduces%2520a%2520novel%2520approach%2520that%2520utilizes%2520Semantic%2520Segmentation%2520models%2520to%250Aautomate%2520seismic%2520event%2520recognition%2520by%2520applying%2520a%2520straight%2520forward%250Atransformation%2520of%2520multi-channel%25201D%2520signals%2520into%25202D%2520representations%252C%2520enabling%250Atheir%2520use%2520as%2520images.%2520Our%2520framework%2520employs%2520a%2520data-driven%252C%2520end-to-end%2520design%250Athat%2520integrates%2520multi-station%2520seismic%2520data%2520with%2520minimal%2520preprocessing%252C%250Aperforming%2520both%2520detection%2520and%2520classification%2520simultaneously%2520for%2520five%2520seismic%250Aevent%2520classes.%2520We%2520evaluated%2520four%2520state-of-the-art%2520segmentation%2520models%2520%2528UNet%252C%250AUNet%252B%252B%252C%2520DeepLabV3%252B%2520and%2520SwinUNet%2529%2520on%2520approximately%252025.000%2520seismic%2520events%250Arecorded%2520at%2520four%2520different%2520Chilean%2520volcanoes%253A%2520Nevados%2520del%2520Chill%255C%2527an%2520Volcanic%250AComplex%252C%2520Laguna%2520del%2520Maule%252C%2520Villarrica%2520and%2520Puyehue-Cord%255C%2527on%2520Caulle.%2520Among%2520these%250Amodels%252C%2520the%2520UNet%2520architecture%2520was%2520identified%2520as%2520the%2520most%2520effective%2520model%252C%250Aachieving%2520mean%2520F1%2520and%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520scores%2520of%2520up%2520to%25200.91%2520and%250A0.88%252C%2520respectively%252C%2520and%2520demonstrating%2520superior%2520noise%2520robustness%2520and%2520model%250Aflexibility%2520to%2520unseen%2520volcano%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20595v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Real-Time%20Volcano-Seismic%20Event%20Recognition%20Based%20on%0A%20%20Multi-Station%20Seismograms%20and%20Semantic%20Segmentation%20Models&entry.906535625=Camilo%20Espinosa-Curilem%20and%20Millaray%20Curilem%20and%20Daniel%20Basualto&entry.1292438233=%20%20In%20volcano%20monitoring%2C%20effective%20recognition%20of%20seismic%20events%20is%20essential%0Afor%20understanding%20volcanic%20activity%20and%20raising%20timely%20warning%20alerts.%0ATraditional%20methods%20rely%20on%20manual%20analysis%2C%20which%20can%20be%20subjective%20and%0Alabor-intensive.%20Furthermore%2C%20current%20automatic%20approaches%20often%20tackle%0Adetection%20and%20classification%20separately%2C%20mostly%20rely%20on%20single%20station%0Ainformation%20and%20generally%20require%20tailored%20preprocessing%20and%20representations%20to%0Aperform%20predictions.%20These%20limitations%20often%20hinder%20their%20application%20to%0Areal-time%20monitoring%20and%20utilization%20across%20different%20volcano%20conditions.%20This%0Astudy%20introduces%20a%20novel%20approach%20that%20utilizes%20Semantic%20Segmentation%20models%20to%0Aautomate%20seismic%20event%20recognition%20by%20applying%20a%20straight%20forward%0Atransformation%20of%20multi-channel%201D%20signals%20into%202D%20representations%2C%20enabling%0Atheir%20use%20as%20images.%20Our%20framework%20employs%20a%20data-driven%2C%20end-to-end%20design%0Athat%20integrates%20multi-station%20seismic%20data%20with%20minimal%20preprocessing%2C%0Aperforming%20both%20detection%20and%20classification%20simultaneously%20for%20five%20seismic%0Aevent%20classes.%20We%20evaluated%20four%20state-of-the-art%20segmentation%20models%20%28UNet%2C%0AUNet%2B%2B%2C%20DeepLabV3%2B%20and%20SwinUNet%29%20on%20approximately%2025.000%20seismic%20events%0Arecorded%20at%20four%20different%20Chilean%20volcanoes%3A%20Nevados%20del%20Chill%5C%27an%20Volcanic%0AComplex%2C%20Laguna%20del%20Maule%2C%20Villarrica%20and%20Puyehue-Cord%5C%27on%20Caulle.%20Among%20these%0Amodels%2C%20the%20UNet%20architecture%20was%20identified%20as%20the%20most%20effective%20model%2C%0Aachieving%20mean%20F1%20and%20Intersection%20over%20Union%20%28IoU%29%20scores%20of%20up%20to%200.91%20and%0A0.88%2C%20respectively%2C%20and%20demonstrating%20superior%20noise%20robustness%20and%20model%0Aflexibility%20to%20unseen%20volcano%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20595v3&entry.124074799=Read"},
{"title": "PaCE: Parsimonious Concept Engineering for Large Language Models", "author": "Jinqi Luo and Tianjiao Ding and Kwan Ho Ryan Chan and Darshan Thaker and Aditya Chattopadhyay and Chris Callison-Burch and Ren\u00e9 Vidal", "abstract": "  Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.\n", "link": "http://arxiv.org/abs/2406.04331v2", "date": "2024-11-05", "relevancy": 2.5246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaCE%3A%20Parsimonious%20Concept%20Engineering%20for%20Large%20Language%20Models&body=Title%3A%20PaCE%3A%20Parsimonious%20Concept%20Engineering%20for%20Large%20Language%20Models%0AAuthor%3A%20Jinqi%20Luo%20and%20Tianjiao%20Ding%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Darshan%20Thaker%20and%20Aditya%20Chattopadhyay%20and%20Chris%20Callison-Burch%20and%20Ren%C3%A9%20Vidal%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20used%20for%20a%20wide%20variety%20of%20tasks.%0AWhile%20they%20are%20capable%20of%20generating%20human-like%20responses%2C%20they%20can%20also%0Aproduce%20undesirable%20output%20including%20potentially%20harmful%20information%2C%20racist%20or%0Asexist%20language%2C%20and%20hallucinations.%20Alignment%20methods%20are%20designed%20to%20reduce%0Asuch%20undesirable%20outputs%20via%20techniques%20such%20as%20fine-tuning%2C%20prompt%0Aengineering%2C%20and%20representation%20engineering.%20However%2C%20existing%20methods%20face%0Aseveral%20challenges%3A%20some%20require%20costly%20fine-tuning%20for%20every%20alignment%20task%3B%0Asome%20do%20not%20adequately%20remove%20undesirable%20concepts%2C%20failing%20alignment%3B%20some%0Aremove%20benign%20concepts%2C%20lowering%20the%20linguistic%20capabilities%20of%20LLMs.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Parsimonious%20Concept%20Engineering%20%28PaCE%29%2C%20a%0Anovel%20activation%20engineering%20framework%20for%20alignment.%20First%2C%20to%20sufficiently%0Amodel%20the%20concepts%2C%20we%20construct%20a%20large-scale%20concept%20dictionary%20in%20the%0Aactivation%20space%2C%20in%20which%20each%20atom%20corresponds%20to%20a%20semantic%20concept.%20Given%0Aany%20alignment%20task%2C%20we%20instruct%20a%20concept%20partitioner%20to%20efficiently%20annotate%0Athe%20concepts%20as%20benign%20or%20undesirable.%20Then%2C%20at%20inference%20time%2C%20we%20decompose%0Athe%20LLM%20activations%20along%20the%20concept%20dictionary%20via%20sparse%20coding%2C%20to%0Aaccurately%20represent%20the%20activations%20as%20linear%20combinations%20of%20benign%20and%0Aundesirable%20components.%20By%20removing%20the%20latter%20ones%20from%20the%20activations%2C%20we%0Areorient%20the%20behavior%20of%20the%20LLM%20towards%20the%20alignment%20goal.%20We%20conduct%0Aexperiments%20on%20tasks%20such%20as%20response%20detoxification%2C%20faithfulness%20enhancement%2C%0Aand%20sentiment%20revising%2C%20and%20show%20that%20PaCE%20achieves%20state-of-the-art%20alignment%0Aperformance%20while%20maintaining%20linguistic%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaCE%253A%2520Parsimonious%2520Concept%2520Engineering%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJinqi%2520Luo%2520and%2520Tianjiao%2520Ding%2520and%2520Kwan%2520Ho%2520Ryan%2520Chan%2520and%2520Darshan%2520Thaker%2520and%2520Aditya%2520Chattopadhyay%2520and%2520Chris%2520Callison-Burch%2520and%2520Ren%25C3%25A9%2520Vidal%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520used%2520for%2520a%2520wide%2520variety%2520of%2520tasks.%250AWhile%2520they%2520are%2520capable%2520of%2520generating%2520human-like%2520responses%252C%2520they%2520can%2520also%250Aproduce%2520undesirable%2520output%2520including%2520potentially%2520harmful%2520information%252C%2520racist%2520or%250Asexist%2520language%252C%2520and%2520hallucinations.%2520Alignment%2520methods%2520are%2520designed%2520to%2520reduce%250Asuch%2520undesirable%2520outputs%2520via%2520techniques%2520such%2520as%2520fine-tuning%252C%2520prompt%250Aengineering%252C%2520and%2520representation%2520engineering.%2520However%252C%2520existing%2520methods%2520face%250Aseveral%2520challenges%253A%2520some%2520require%2520costly%2520fine-tuning%2520for%2520every%2520alignment%2520task%253B%250Asome%2520do%2520not%2520adequately%2520remove%2520undesirable%2520concepts%252C%2520failing%2520alignment%253B%2520some%250Aremove%2520benign%2520concepts%252C%2520lowering%2520the%2520linguistic%2520capabilities%2520of%2520LLMs.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520Parsimonious%2520Concept%2520Engineering%2520%2528PaCE%2529%252C%2520a%250Anovel%2520activation%2520engineering%2520framework%2520for%2520alignment.%2520First%252C%2520to%2520sufficiently%250Amodel%2520the%2520concepts%252C%2520we%2520construct%2520a%2520large-scale%2520concept%2520dictionary%2520in%2520the%250Aactivation%2520space%252C%2520in%2520which%2520each%2520atom%2520corresponds%2520to%2520a%2520semantic%2520concept.%2520Given%250Aany%2520alignment%2520task%252C%2520we%2520instruct%2520a%2520concept%2520partitioner%2520to%2520efficiently%2520annotate%250Athe%2520concepts%2520as%2520benign%2520or%2520undesirable.%2520Then%252C%2520at%2520inference%2520time%252C%2520we%2520decompose%250Athe%2520LLM%2520activations%2520along%2520the%2520concept%2520dictionary%2520via%2520sparse%2520coding%252C%2520to%250Aaccurately%2520represent%2520the%2520activations%2520as%2520linear%2520combinations%2520of%2520benign%2520and%250Aundesirable%2520components.%2520By%2520removing%2520the%2520latter%2520ones%2520from%2520the%2520activations%252C%2520we%250Areorient%2520the%2520behavior%2520of%2520the%2520LLM%2520towards%2520the%2520alignment%2520goal.%2520We%2520conduct%250Aexperiments%2520on%2520tasks%2520such%2520as%2520response%2520detoxification%252C%2520faithfulness%2520enhancement%252C%250Aand%2520sentiment%2520revising%252C%2520and%2520show%2520that%2520PaCE%2520achieves%2520state-of-the-art%2520alignment%250Aperformance%2520while%2520maintaining%2520linguistic%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaCE%3A%20Parsimonious%20Concept%20Engineering%20for%20Large%20Language%20Models&entry.906535625=Jinqi%20Luo%20and%20Tianjiao%20Ding%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Darshan%20Thaker%20and%20Aditya%20Chattopadhyay%20and%20Chris%20Callison-Burch%20and%20Ren%C3%A9%20Vidal&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20used%20for%20a%20wide%20variety%20of%20tasks.%0AWhile%20they%20are%20capable%20of%20generating%20human-like%20responses%2C%20they%20can%20also%0Aproduce%20undesirable%20output%20including%20potentially%20harmful%20information%2C%20racist%20or%0Asexist%20language%2C%20and%20hallucinations.%20Alignment%20methods%20are%20designed%20to%20reduce%0Asuch%20undesirable%20outputs%20via%20techniques%20such%20as%20fine-tuning%2C%20prompt%0Aengineering%2C%20and%20representation%20engineering.%20However%2C%20existing%20methods%20face%0Aseveral%20challenges%3A%20some%20require%20costly%20fine-tuning%20for%20every%20alignment%20task%3B%0Asome%20do%20not%20adequately%20remove%20undesirable%20concepts%2C%20failing%20alignment%3B%20some%0Aremove%20benign%20concepts%2C%20lowering%20the%20linguistic%20capabilities%20of%20LLMs.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Parsimonious%20Concept%20Engineering%20%28PaCE%29%2C%20a%0Anovel%20activation%20engineering%20framework%20for%20alignment.%20First%2C%20to%20sufficiently%0Amodel%20the%20concepts%2C%20we%20construct%20a%20large-scale%20concept%20dictionary%20in%20the%0Aactivation%20space%2C%20in%20which%20each%20atom%20corresponds%20to%20a%20semantic%20concept.%20Given%0Aany%20alignment%20task%2C%20we%20instruct%20a%20concept%20partitioner%20to%20efficiently%20annotate%0Athe%20concepts%20as%20benign%20or%20undesirable.%20Then%2C%20at%20inference%20time%2C%20we%20decompose%0Athe%20LLM%20activations%20along%20the%20concept%20dictionary%20via%20sparse%20coding%2C%20to%0Aaccurately%20represent%20the%20activations%20as%20linear%20combinations%20of%20benign%20and%0Aundesirable%20components.%20By%20removing%20the%20latter%20ones%20from%20the%20activations%2C%20we%0Areorient%20the%20behavior%20of%20the%20LLM%20towards%20the%20alignment%20goal.%20We%20conduct%0Aexperiments%20on%20tasks%20such%20as%20response%20detoxification%2C%20faithfulness%20enhancement%2C%0Aand%20sentiment%20revising%2C%20and%20show%20that%20PaCE%20achieves%20state-of-the-art%20alignment%0Aperformance%20while%20maintaining%20linguistic%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04331v2&entry.124074799=Read"},
{"title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues", "author": "Ge Bai and Jie Liu and Xingyuan Bu and Yancheng He and Jiaheng Liu and Zhanhui Zhou and Zhuoran Lin and Wenbo Su and Tiezheng Ge and Bo Zheng and Wanli Ouyang", "abstract": "  The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.\n", "link": "http://arxiv.org/abs/2402.14762v3", "date": "2024-11-05", "relevancy": 2.5033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues&body=Title%3A%20MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues%0AAuthor%3A%20Ge%20Bai%20and%20Jie%20Liu%20and%20Xingyuan%20Bu%20and%20Yancheng%20He%20and%20Jiaheng%20Liu%20and%20Zhanhui%20Zhou%20and%20Zhuoran%20Lin%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20drastically%20enhanced%20dialogue%0Asystems.%20However%2C%20comprehensively%20evaluating%20the%20dialogue%20abilities%20of%20LLMs%0Aremains%20a%20challenge.%20Previous%20benchmarks%20have%20primarily%20focused%20on%20single-turn%0Adialogues%20or%20provided%20coarse-grained%20and%20incomplete%20assessments%20of%20multi-turn%0Adialogues%2C%20overlooking%20the%20complexity%20and%20fine-grained%20nuances%20of%20real-life%0Adialogues.%20To%20address%20this%20issue%2C%20we%20introduce%20MT-Bench-101%2C%20specifically%0Adesigned%20to%20evaluate%20the%20fine-grained%20abilities%20of%20LLMs%20in%20multi-turn%0Adialogues.%20By%20conducting%20a%20detailed%20analysis%20of%20real%20multi-turn%20dialogue%20data%2C%0Awe%20construct%20a%20three-tier%20hierarchical%20ability%20taxonomy%20comprising%204208%20turns%0Aacross%201388%20multi-turn%20dialogues%20in%2013%20distinct%20tasks.%20We%20then%20evaluate%2021%0Apopular%20LLMs%20based%20on%20MT-Bench-101%2C%20conducting%20comprehensive%20analyses%20from%20both%0Aability%20and%20task%20perspectives%20and%20observing%20differing%20trends%20in%20LLMs%0Aperformance%20across%20dialogue%20turns%20within%20various%20tasks.%20Further%20analysis%0Aindicates%20that%20neither%20utilizing%20common%20alignment%20techniques%20nor%20chat-specific%0Adesigns%20has%20led%20to%20obvious%20enhancements%20in%20the%20multi-turn%20abilities%20of%20LLMs.%0AExtensive%20case%20studies%20suggest%20that%20our%20designed%20tasks%20accurately%20assess%20the%0Acorresponding%20multi-turn%20abilities.%20The%20data%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mtbench101/mt-bench-101%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Bench-101%253A%2520A%2520Fine-Grained%2520Benchmark%2520for%2520Evaluating%2520Large%2520Language%250A%2520%2520Models%2520in%2520Multi-Turn%2520Dialogues%26entry.906535625%3DGe%2520Bai%2520and%2520Jie%2520Liu%2520and%2520Xingyuan%2520Bu%2520and%2520Yancheng%2520He%2520and%2520Jiaheng%2520Liu%2520and%2520Zhanhui%2520Zhou%2520and%2520Zhuoran%2520Lin%2520and%2520Wenbo%2520Su%2520and%2520Tiezheng%2520Ge%2520and%2520Bo%2520Zheng%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520drastically%2520enhanced%2520dialogue%250Asystems.%2520However%252C%2520comprehensively%2520evaluating%2520the%2520dialogue%2520abilities%2520of%2520LLMs%250Aremains%2520a%2520challenge.%2520Previous%2520benchmarks%2520have%2520primarily%2520focused%2520on%2520single-turn%250Adialogues%2520or%2520provided%2520coarse-grained%2520and%2520incomplete%2520assessments%2520of%2520multi-turn%250Adialogues%252C%2520overlooking%2520the%2520complexity%2520and%2520fine-grained%2520nuances%2520of%2520real-life%250Adialogues.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MT-Bench-101%252C%2520specifically%250Adesigned%2520to%2520evaluate%2520the%2520fine-grained%2520abilities%2520of%2520LLMs%2520in%2520multi-turn%250Adialogues.%2520By%2520conducting%2520a%2520detailed%2520analysis%2520of%2520real%2520multi-turn%2520dialogue%2520data%252C%250Awe%2520construct%2520a%2520three-tier%2520hierarchical%2520ability%2520taxonomy%2520comprising%25204208%2520turns%250Aacross%25201388%2520multi-turn%2520dialogues%2520in%252013%2520distinct%2520tasks.%2520We%2520then%2520evaluate%252021%250Apopular%2520LLMs%2520based%2520on%2520MT-Bench-101%252C%2520conducting%2520comprehensive%2520analyses%2520from%2520both%250Aability%2520and%2520task%2520perspectives%2520and%2520observing%2520differing%2520trends%2520in%2520LLMs%250Aperformance%2520across%2520dialogue%2520turns%2520within%2520various%2520tasks.%2520Further%2520analysis%250Aindicates%2520that%2520neither%2520utilizing%2520common%2520alignment%2520techniques%2520nor%2520chat-specific%250Adesigns%2520has%2520led%2520to%2520obvious%2520enhancements%2520in%2520the%2520multi-turn%2520abilities%2520of%2520LLMs.%250AExtensive%2520case%2520studies%2520suggest%2520that%2520our%2520designed%2520tasks%2520accurately%2520assess%2520the%250Acorresponding%2520multi-turn%2520abilities.%2520The%2520data%2520and%2520code%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/mtbench101/mt-bench-101%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues&entry.906535625=Ge%20Bai%20and%20Jie%20Liu%20and%20Xingyuan%20Bu%20and%20Yancheng%20He%20and%20Jiaheng%20Liu%20and%20Zhanhui%20Zhou%20and%20Zhuoran%20Lin%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng%20and%20Wanli%20Ouyang&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20drastically%20enhanced%20dialogue%0Asystems.%20However%2C%20comprehensively%20evaluating%20the%20dialogue%20abilities%20of%20LLMs%0Aremains%20a%20challenge.%20Previous%20benchmarks%20have%20primarily%20focused%20on%20single-turn%0Adialogues%20or%20provided%20coarse-grained%20and%20incomplete%20assessments%20of%20multi-turn%0Adialogues%2C%20overlooking%20the%20complexity%20and%20fine-grained%20nuances%20of%20real-life%0Adialogues.%20To%20address%20this%20issue%2C%20we%20introduce%20MT-Bench-101%2C%20specifically%0Adesigned%20to%20evaluate%20the%20fine-grained%20abilities%20of%20LLMs%20in%20multi-turn%0Adialogues.%20By%20conducting%20a%20detailed%20analysis%20of%20real%20multi-turn%20dialogue%20data%2C%0Awe%20construct%20a%20three-tier%20hierarchical%20ability%20taxonomy%20comprising%204208%20turns%0Aacross%201388%20multi-turn%20dialogues%20in%2013%20distinct%20tasks.%20We%20then%20evaluate%2021%0Apopular%20LLMs%20based%20on%20MT-Bench-101%2C%20conducting%20comprehensive%20analyses%20from%20both%0Aability%20and%20task%20perspectives%20and%20observing%20differing%20trends%20in%20LLMs%0Aperformance%20across%20dialogue%20turns%20within%20various%20tasks.%20Further%20analysis%0Aindicates%20that%20neither%20utilizing%20common%20alignment%20techniques%20nor%20chat-specific%0Adesigns%20has%20led%20to%20obvious%20enhancements%20in%20the%20multi-turn%20abilities%20of%20LLMs.%0AExtensive%20case%20studies%20suggest%20that%20our%20designed%20tasks%20accurately%20assess%20the%0Acorresponding%20multi-turn%20abilities.%20The%20data%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mtbench101/mt-bench-101%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14762v3&entry.124074799=Read"},
{"title": "Self-supervised cross-modality learning for uncertainty-aware object\n  detection and recognition in applications which lack pre-labelled training\n  data", "author": "Irum Mehboob and Li Sun and Alireza Astegarpanah and Rustam Stolkin", "abstract": "  This paper shows how an uncertainty-aware, deep neural network can be trained\nto detect, recognise and localise objects in 2D RGB images, in applications\nlacking annotated train-ng datasets. We propose a self-supervising\nteacher-student pipeline, in which a relatively simple teacher classifier,\ntrained with only a few labelled 2D thumbnails, automatically processes a\nlarger body of unlabelled RGB-D data to teach a student network based on a\nmodified YOLOv3 architecture. Firstly, 3D object detection with back projection\nis used to automatically extract and teach 2D detection and localisation\ninformation to the student network. Secondly, a weakly supervised 2D thumbnail\nclassifier, with minimal training on a small number of hand-labelled images, is\nused to teach object category recognition. Thirdly, we use a Gaussian Process\nGP to encode and teach a robust uncertainty estimation functionality, so that\nthe student can output confidence scores with each categorization. The\nresulting student significantly outperforms the same YOLO architecture trained\ndirectly on the same amount of labelled data. Our GP-based approach yields\nrobust and meaningful uncertainty estimations for complex industrial object\nclassifications. The end-to-end network is also capable of real-time\nprocessing, needed for robotics applications. Our method can be applied to many\nimportant industrial tasks, where labelled datasets are typically unavailable.\nIn this paper, we demonstrate an example of detection, localisation, and object\ncategory recognition of nuclear mixed-waste materials in highly cluttered and\nunstructured scenes. This is critical for robotic sorting and handling of\nlegacy nuclear waste, which poses complex environmental remediation challenges\nin many nuclearised nations.\n", "link": "http://arxiv.org/abs/2411.03082v1", "date": "2024-11-05", "relevancy": 2.4259, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6046}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20cross-modality%20learning%20for%20uncertainty-aware%20object%0A%20%20detection%20and%20recognition%20in%20applications%20which%20lack%20pre-labelled%20training%0A%20%20data&body=Title%3A%20Self-supervised%20cross-modality%20learning%20for%20uncertainty-aware%20object%0A%20%20detection%20and%20recognition%20in%20applications%20which%20lack%20pre-labelled%20training%0A%20%20data%0AAuthor%3A%20Irum%20Mehboob%20and%20Li%20Sun%20and%20Alireza%20Astegarpanah%20and%20Rustam%20Stolkin%0AAbstract%3A%20%20%20This%20paper%20shows%20how%20an%20uncertainty-aware%2C%20deep%20neural%20network%20can%20be%20trained%0Ato%20detect%2C%20recognise%20and%20localise%20objects%20in%202D%20RGB%20images%2C%20in%20applications%0Alacking%20annotated%20train-ng%20datasets.%20We%20propose%20a%20self-supervising%0Ateacher-student%20pipeline%2C%20in%20which%20a%20relatively%20simple%20teacher%20classifier%2C%0Atrained%20with%20only%20a%20few%20labelled%202D%20thumbnails%2C%20automatically%20processes%20a%0Alarger%20body%20of%20unlabelled%20RGB-D%20data%20to%20teach%20a%20student%20network%20based%20on%20a%0Amodified%20YOLOv3%20architecture.%20Firstly%2C%203D%20object%20detection%20with%20back%20projection%0Ais%20used%20to%20automatically%20extract%20and%20teach%202D%20detection%20and%20localisation%0Ainformation%20to%20the%20student%20network.%20Secondly%2C%20a%20weakly%20supervised%202D%20thumbnail%0Aclassifier%2C%20with%20minimal%20training%20on%20a%20small%20number%20of%20hand-labelled%20images%2C%20is%0Aused%20to%20teach%20object%20category%20recognition.%20Thirdly%2C%20we%20use%20a%20Gaussian%20Process%0AGP%20to%20encode%20and%20teach%20a%20robust%20uncertainty%20estimation%20functionality%2C%20so%20that%0Athe%20student%20can%20output%20confidence%20scores%20with%20each%20categorization.%20The%0Aresulting%20student%20significantly%20outperforms%20the%20same%20YOLO%20architecture%20trained%0Adirectly%20on%20the%20same%20amount%20of%20labelled%20data.%20Our%20GP-based%20approach%20yields%0Arobust%20and%20meaningful%20uncertainty%20estimations%20for%20complex%20industrial%20object%0Aclassifications.%20The%20end-to-end%20network%20is%20also%20capable%20of%20real-time%0Aprocessing%2C%20needed%20for%20robotics%20applications.%20Our%20method%20can%20be%20applied%20to%20many%0Aimportant%20industrial%20tasks%2C%20where%20labelled%20datasets%20are%20typically%20unavailable.%0AIn%20this%20paper%2C%20we%20demonstrate%20an%20example%20of%20detection%2C%20localisation%2C%20and%20object%0Acategory%20recognition%20of%20nuclear%20mixed-waste%20materials%20in%20highly%20cluttered%20and%0Aunstructured%20scenes.%20This%20is%20critical%20for%20robotic%20sorting%20and%20handling%20of%0Alegacy%20nuclear%20waste%2C%20which%20poses%20complex%20environmental%20remediation%20challenges%0Ain%20many%20nuclearised%20nations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520cross-modality%2520learning%2520for%2520uncertainty-aware%2520object%250A%2520%2520detection%2520and%2520recognition%2520in%2520applications%2520which%2520lack%2520pre-labelled%2520training%250A%2520%2520data%26entry.906535625%3DIrum%2520Mehboob%2520and%2520Li%2520Sun%2520and%2520Alireza%2520Astegarpanah%2520and%2520Rustam%2520Stolkin%26entry.1292438233%3D%2520%2520This%2520paper%2520shows%2520how%2520an%2520uncertainty-aware%252C%2520deep%2520neural%2520network%2520can%2520be%2520trained%250Ato%2520detect%252C%2520recognise%2520and%2520localise%2520objects%2520in%25202D%2520RGB%2520images%252C%2520in%2520applications%250Alacking%2520annotated%2520train-ng%2520datasets.%2520We%2520propose%2520a%2520self-supervising%250Ateacher-student%2520pipeline%252C%2520in%2520which%2520a%2520relatively%2520simple%2520teacher%2520classifier%252C%250Atrained%2520with%2520only%2520a%2520few%2520labelled%25202D%2520thumbnails%252C%2520automatically%2520processes%2520a%250Alarger%2520body%2520of%2520unlabelled%2520RGB-D%2520data%2520to%2520teach%2520a%2520student%2520network%2520based%2520on%2520a%250Amodified%2520YOLOv3%2520architecture.%2520Firstly%252C%25203D%2520object%2520detection%2520with%2520back%2520projection%250Ais%2520used%2520to%2520automatically%2520extract%2520and%2520teach%25202D%2520detection%2520and%2520localisation%250Ainformation%2520to%2520the%2520student%2520network.%2520Secondly%252C%2520a%2520weakly%2520supervised%25202D%2520thumbnail%250Aclassifier%252C%2520with%2520minimal%2520training%2520on%2520a%2520small%2520number%2520of%2520hand-labelled%2520images%252C%2520is%250Aused%2520to%2520teach%2520object%2520category%2520recognition.%2520Thirdly%252C%2520we%2520use%2520a%2520Gaussian%2520Process%250AGP%2520to%2520encode%2520and%2520teach%2520a%2520robust%2520uncertainty%2520estimation%2520functionality%252C%2520so%2520that%250Athe%2520student%2520can%2520output%2520confidence%2520scores%2520with%2520each%2520categorization.%2520The%250Aresulting%2520student%2520significantly%2520outperforms%2520the%2520same%2520YOLO%2520architecture%2520trained%250Adirectly%2520on%2520the%2520same%2520amount%2520of%2520labelled%2520data.%2520Our%2520GP-based%2520approach%2520yields%250Arobust%2520and%2520meaningful%2520uncertainty%2520estimations%2520for%2520complex%2520industrial%2520object%250Aclassifications.%2520The%2520end-to-end%2520network%2520is%2520also%2520capable%2520of%2520real-time%250Aprocessing%252C%2520needed%2520for%2520robotics%2520applications.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520many%250Aimportant%2520industrial%2520tasks%252C%2520where%2520labelled%2520datasets%2520are%2520typically%2520unavailable.%250AIn%2520this%2520paper%252C%2520we%2520demonstrate%2520an%2520example%2520of%2520detection%252C%2520localisation%252C%2520and%2520object%250Acategory%2520recognition%2520of%2520nuclear%2520mixed-waste%2520materials%2520in%2520highly%2520cluttered%2520and%250Aunstructured%2520scenes.%2520This%2520is%2520critical%2520for%2520robotic%2520sorting%2520and%2520handling%2520of%250Alegacy%2520nuclear%2520waste%252C%2520which%2520poses%2520complex%2520environmental%2520remediation%2520challenges%250Ain%2520many%2520nuclearised%2520nations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20cross-modality%20learning%20for%20uncertainty-aware%20object%0A%20%20detection%20and%20recognition%20in%20applications%20which%20lack%20pre-labelled%20training%0A%20%20data&entry.906535625=Irum%20Mehboob%20and%20Li%20Sun%20and%20Alireza%20Astegarpanah%20and%20Rustam%20Stolkin&entry.1292438233=%20%20This%20paper%20shows%20how%20an%20uncertainty-aware%2C%20deep%20neural%20network%20can%20be%20trained%0Ato%20detect%2C%20recognise%20and%20localise%20objects%20in%202D%20RGB%20images%2C%20in%20applications%0Alacking%20annotated%20train-ng%20datasets.%20We%20propose%20a%20self-supervising%0Ateacher-student%20pipeline%2C%20in%20which%20a%20relatively%20simple%20teacher%20classifier%2C%0Atrained%20with%20only%20a%20few%20labelled%202D%20thumbnails%2C%20automatically%20processes%20a%0Alarger%20body%20of%20unlabelled%20RGB-D%20data%20to%20teach%20a%20student%20network%20based%20on%20a%0Amodified%20YOLOv3%20architecture.%20Firstly%2C%203D%20object%20detection%20with%20back%20projection%0Ais%20used%20to%20automatically%20extract%20and%20teach%202D%20detection%20and%20localisation%0Ainformation%20to%20the%20student%20network.%20Secondly%2C%20a%20weakly%20supervised%202D%20thumbnail%0Aclassifier%2C%20with%20minimal%20training%20on%20a%20small%20number%20of%20hand-labelled%20images%2C%20is%0Aused%20to%20teach%20object%20category%20recognition.%20Thirdly%2C%20we%20use%20a%20Gaussian%20Process%0AGP%20to%20encode%20and%20teach%20a%20robust%20uncertainty%20estimation%20functionality%2C%20so%20that%0Athe%20student%20can%20output%20confidence%20scores%20with%20each%20categorization.%20The%0Aresulting%20student%20significantly%20outperforms%20the%20same%20YOLO%20architecture%20trained%0Adirectly%20on%20the%20same%20amount%20of%20labelled%20data.%20Our%20GP-based%20approach%20yields%0Arobust%20and%20meaningful%20uncertainty%20estimations%20for%20complex%20industrial%20object%0Aclassifications.%20The%20end-to-end%20network%20is%20also%20capable%20of%20real-time%0Aprocessing%2C%20needed%20for%20robotics%20applications.%20Our%20method%20can%20be%20applied%20to%20many%0Aimportant%20industrial%20tasks%2C%20where%20labelled%20datasets%20are%20typically%20unavailable.%0AIn%20this%20paper%2C%20we%20demonstrate%20an%20example%20of%20detection%2C%20localisation%2C%20and%20object%0Acategory%20recognition%20of%20nuclear%20mixed-waste%20materials%20in%20highly%20cluttered%20and%0Aunstructured%20scenes.%20This%20is%20critical%20for%20robotic%20sorting%20and%20handling%20of%0Alegacy%20nuclear%20waste%2C%20which%20poses%20complex%20environmental%20remediation%20challenges%0Ain%20many%20nuclearised%20nations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03082v1&entry.124074799=Read"},
{"title": "Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation", "author": "Shan Zhao and Zhaiyu Chen and Zhitong Xiong and Yilei Shi and Sudipan Saha and Xiao Xiang Zhu", "abstract": "  Earth Observation (EO) data analysis has been significantly revolutionized by\ndeep learning (DL), with applications typically limited to grid-like data\nstructures. Graph Neural Networks (GNNs) emerge as an important innovation,\npropelling DL into the non-Euclidean domain. Naturally, GNNs can effectively\ntackle the challenges posed by diverse modalities, multiple sensors, and the\nheterogeneous nature of EO data. To introduce GNNs in the related domains, our\nreview begins by offering fundamental knowledge on GNNs. Then, we summarize the\ngeneric problems in EO, to which GNNs can offer potential solutions. Following\nthis, we explore a broad spectrum of GNNs' applications to scientific problems\nin Earth systems, covering areas such as weather and climate analysis, disaster\nmanagement, air quality monitoring, agriculture, land cover classification,\nhydrological process modeling, and urban modeling. The rationale behind\nadopting GNNs in these fields is explained, alongside methodologies for\norganizing graphs and designing favorable architectures for various tasks.\nFurthermore, we highlight methodological challenges of implementing GNNs in\nthese domains and possible solutions that could guide future research. While\nacknowledging that GNNs are not a universal solution, we conclude the paper by\ncomparing them with other popular architectures like transformers and analyzing\ntheir potential synergies.\n", "link": "http://arxiv.org/abs/2411.03223v1", "date": "2024-11-05", "relevancy": 2.4183, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4895}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4817}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Grid%20Data%3A%20Exploring%20Graph%20Neural%20Networks%20for%20Earth%20Observation&body=Title%3A%20Beyond%20Grid%20Data%3A%20Exploring%20Graph%20Neural%20Networks%20for%20Earth%20Observation%0AAuthor%3A%20Shan%20Zhao%20and%20Zhaiyu%20Chen%20and%20Zhitong%20Xiong%20and%20Yilei%20Shi%20and%20Sudipan%20Saha%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Earth%20Observation%20%28EO%29%20data%20analysis%20has%20been%20significantly%20revolutionized%20by%0Adeep%20learning%20%28DL%29%2C%20with%20applications%20typically%20limited%20to%20grid-like%20data%0Astructures.%20Graph%20Neural%20Networks%20%28GNNs%29%20emerge%20as%20an%20important%20innovation%2C%0Apropelling%20DL%20into%20the%20non-Euclidean%20domain.%20Naturally%2C%20GNNs%20can%20effectively%0Atackle%20the%20challenges%20posed%20by%20diverse%20modalities%2C%20multiple%20sensors%2C%20and%20the%0Aheterogeneous%20nature%20of%20EO%20data.%20To%20introduce%20GNNs%20in%20the%20related%20domains%2C%20our%0Areview%20begins%20by%20offering%20fundamental%20knowledge%20on%20GNNs.%20Then%2C%20we%20summarize%20the%0Ageneric%20problems%20in%20EO%2C%20to%20which%20GNNs%20can%20offer%20potential%20solutions.%20Following%0Athis%2C%20we%20explore%20a%20broad%20spectrum%20of%20GNNs%27%20applications%20to%20scientific%20problems%0Ain%20Earth%20systems%2C%20covering%20areas%20such%20as%20weather%20and%20climate%20analysis%2C%20disaster%0Amanagement%2C%20air%20quality%20monitoring%2C%20agriculture%2C%20land%20cover%20classification%2C%0Ahydrological%20process%20modeling%2C%20and%20urban%20modeling.%20The%20rationale%20behind%0Aadopting%20GNNs%20in%20these%20fields%20is%20explained%2C%20alongside%20methodologies%20for%0Aorganizing%20graphs%20and%20designing%20favorable%20architectures%20for%20various%20tasks.%0AFurthermore%2C%20we%20highlight%20methodological%20challenges%20of%20implementing%20GNNs%20in%0Athese%20domains%20and%20possible%20solutions%20that%20could%20guide%20future%20research.%20While%0Aacknowledging%20that%20GNNs%20are%20not%20a%20universal%20solution%2C%20we%20conclude%20the%20paper%20by%0Acomparing%20them%20with%20other%20popular%20architectures%20like%20transformers%20and%20analyzing%0Atheir%20potential%20synergies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Grid%2520Data%253A%2520Exploring%2520Graph%2520Neural%2520Networks%2520for%2520Earth%2520Observation%26entry.906535625%3DShan%2520Zhao%2520and%2520Zhaiyu%2520Chen%2520and%2520Zhitong%2520Xiong%2520and%2520Yilei%2520Shi%2520and%2520Sudipan%2520Saha%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Earth%2520Observation%2520%2528EO%2529%2520data%2520analysis%2520has%2520been%2520significantly%2520revolutionized%2520by%250Adeep%2520learning%2520%2528DL%2529%252C%2520with%2520applications%2520typically%2520limited%2520to%2520grid-like%2520data%250Astructures.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520emerge%2520as%2520an%2520important%2520innovation%252C%250Apropelling%2520DL%2520into%2520the%2520non-Euclidean%2520domain.%2520Naturally%252C%2520GNNs%2520can%2520effectively%250Atackle%2520the%2520challenges%2520posed%2520by%2520diverse%2520modalities%252C%2520multiple%2520sensors%252C%2520and%2520the%250Aheterogeneous%2520nature%2520of%2520EO%2520data.%2520To%2520introduce%2520GNNs%2520in%2520the%2520related%2520domains%252C%2520our%250Areview%2520begins%2520by%2520offering%2520fundamental%2520knowledge%2520on%2520GNNs.%2520Then%252C%2520we%2520summarize%2520the%250Ageneric%2520problems%2520in%2520EO%252C%2520to%2520which%2520GNNs%2520can%2520offer%2520potential%2520solutions.%2520Following%250Athis%252C%2520we%2520explore%2520a%2520broad%2520spectrum%2520of%2520GNNs%2527%2520applications%2520to%2520scientific%2520problems%250Ain%2520Earth%2520systems%252C%2520covering%2520areas%2520such%2520as%2520weather%2520and%2520climate%2520analysis%252C%2520disaster%250Amanagement%252C%2520air%2520quality%2520monitoring%252C%2520agriculture%252C%2520land%2520cover%2520classification%252C%250Ahydrological%2520process%2520modeling%252C%2520and%2520urban%2520modeling.%2520The%2520rationale%2520behind%250Aadopting%2520GNNs%2520in%2520these%2520fields%2520is%2520explained%252C%2520alongside%2520methodologies%2520for%250Aorganizing%2520graphs%2520and%2520designing%2520favorable%2520architectures%2520for%2520various%2520tasks.%250AFurthermore%252C%2520we%2520highlight%2520methodological%2520challenges%2520of%2520implementing%2520GNNs%2520in%250Athese%2520domains%2520and%2520possible%2520solutions%2520that%2520could%2520guide%2520future%2520research.%2520While%250Aacknowledging%2520that%2520GNNs%2520are%2520not%2520a%2520universal%2520solution%252C%2520we%2520conclude%2520the%2520paper%2520by%250Acomparing%2520them%2520with%2520other%2520popular%2520architectures%2520like%2520transformers%2520and%2520analyzing%250Atheir%2520potential%2520synergies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Grid%20Data%3A%20Exploring%20Graph%20Neural%20Networks%20for%20Earth%20Observation&entry.906535625=Shan%20Zhao%20and%20Zhaiyu%20Chen%20and%20Zhitong%20Xiong%20and%20Yilei%20Shi%20and%20Sudipan%20Saha%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Earth%20Observation%20%28EO%29%20data%20analysis%20has%20been%20significantly%20revolutionized%20by%0Adeep%20learning%20%28DL%29%2C%20with%20applications%20typically%20limited%20to%20grid-like%20data%0Astructures.%20Graph%20Neural%20Networks%20%28GNNs%29%20emerge%20as%20an%20important%20innovation%2C%0Apropelling%20DL%20into%20the%20non-Euclidean%20domain.%20Naturally%2C%20GNNs%20can%20effectively%0Atackle%20the%20challenges%20posed%20by%20diverse%20modalities%2C%20multiple%20sensors%2C%20and%20the%0Aheterogeneous%20nature%20of%20EO%20data.%20To%20introduce%20GNNs%20in%20the%20related%20domains%2C%20our%0Areview%20begins%20by%20offering%20fundamental%20knowledge%20on%20GNNs.%20Then%2C%20we%20summarize%20the%0Ageneric%20problems%20in%20EO%2C%20to%20which%20GNNs%20can%20offer%20potential%20solutions.%20Following%0Athis%2C%20we%20explore%20a%20broad%20spectrum%20of%20GNNs%27%20applications%20to%20scientific%20problems%0Ain%20Earth%20systems%2C%20covering%20areas%20such%20as%20weather%20and%20climate%20analysis%2C%20disaster%0Amanagement%2C%20air%20quality%20monitoring%2C%20agriculture%2C%20land%20cover%20classification%2C%0Ahydrological%20process%20modeling%2C%20and%20urban%20modeling.%20The%20rationale%20behind%0Aadopting%20GNNs%20in%20these%20fields%20is%20explained%2C%20alongside%20methodologies%20for%0Aorganizing%20graphs%20and%20designing%20favorable%20architectures%20for%20various%20tasks.%0AFurthermore%2C%20we%20highlight%20methodological%20challenges%20of%20implementing%20GNNs%20in%0Athese%20domains%20and%20possible%20solutions%20that%20could%20guide%20future%20research.%20While%0Aacknowledging%20that%20GNNs%20are%20not%20a%20universal%20solution%2C%20we%20conclude%20the%20paper%20by%0Acomparing%20them%20with%20other%20popular%20architectures%20like%20transformers%20and%20analyzing%0Atheir%20potential%20synergies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03223v1&entry.124074799=Read"},
{"title": "Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object\n  Detection", "author": "Jiacheng Zhang and Jiaming Li and Xiangru Lin and Wei Zhang and Xiao Tan and Junyu Han and Errui Ding and Jingdong Wang and Guanbin Li", "abstract": "  We delve into pseudo-labeling for semi-supervised monocular 3D object\ndetection (SSM3OD) and discover two primary issues: a misalignment between the\nprediction quality of 3D and 2D attributes and the tendency of depth\nsupervision derived from pseudo-labels to be noisy, leading to significant\noptimization conflicts with other reliable forms of supervision. We introduce a\nnovel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach\nfeatures a Decoupled Pseudo-label Generation (DPG) module, designed to\nefficiently generate pseudo-labels by separately processing 2D and 3D\nattributes. This module incorporates a unique homography-based method for\nidentifying dependable pseudo-labels in BEV space, specifically for 3D\nattributes. Additionally, we present a DepthGradient Projection (DGP) module to\nmitigate optimization conflicts caused by noisy depth supervision of\npseudo-labels, effectively decoupling the depth gradient and removing\nconflicting gradients. This dual decoupling strategy-at both the pseudo-label\ngeneration and gradient levels-significantly improves the utilization of\npseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark\ndemonstrate the superiority of our method over existing approaches.\n", "link": "http://arxiv.org/abs/2403.17387v3", "date": "2024-11-05", "relevancy": 2.4099, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6587}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6043}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Pseudo-labeling%20for%20Semi-Supervised%20Monocular%203D%20Object%0A%20%20Detection&body=Title%3A%20Decoupled%20Pseudo-labeling%20for%20Semi-Supervised%20Monocular%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Jiacheng%20Zhang%20and%20Jiaming%20Li%20and%20Xiangru%20Lin%20and%20Wei%20Zhang%20and%20Xiao%20Tan%20and%20Junyu%20Han%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20We%20delve%20into%20pseudo-labeling%20for%20semi-supervised%20monocular%203D%20object%0Adetection%20%28SSM3OD%29%20and%20discover%20two%20primary%20issues%3A%20a%20misalignment%20between%20the%0Aprediction%20quality%20of%203D%20and%202D%20attributes%20and%20the%20tendency%20of%20depth%0Asupervision%20derived%20from%20pseudo-labels%20to%20be%20noisy%2C%20leading%20to%20significant%0Aoptimization%20conflicts%20with%20other%20reliable%20forms%20of%20supervision.%20We%20introduce%20a%0Anovel%20decoupled%20pseudo-labeling%20%28DPL%29%20approach%20for%20SSM3OD.%20Our%20approach%0Afeatures%20a%20Decoupled%20Pseudo-label%20Generation%20%28DPG%29%20module%2C%20designed%20to%0Aefficiently%20generate%20pseudo-labels%20by%20separately%20processing%202D%20and%203D%0Aattributes.%20This%20module%20incorporates%20a%20unique%20homography-based%20method%20for%0Aidentifying%20dependable%20pseudo-labels%20in%20BEV%20space%2C%20specifically%20for%203D%0Aattributes.%20Additionally%2C%20we%20present%20a%20DepthGradient%20Projection%20%28DGP%29%20module%20to%0Amitigate%20optimization%20conflicts%20caused%20by%20noisy%20depth%20supervision%20of%0Apseudo-labels%2C%20effectively%20decoupling%20the%20depth%20gradient%20and%20removing%0Aconflicting%20gradients.%20This%20dual%20decoupling%20strategy-at%20both%20the%20pseudo-label%0Ageneration%20and%20gradient%20levels-significantly%20improves%20the%20utilization%20of%0Apseudo-labels%20in%20SSM3OD.%20Our%20comprehensive%20experiments%20on%20the%20KITTI%20benchmark%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Pseudo-labeling%2520for%2520Semi-Supervised%2520Monocular%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DJiacheng%2520Zhang%2520and%2520Jiaming%2520Li%2520and%2520Xiangru%2520Lin%2520and%2520Wei%2520Zhang%2520and%2520Xiao%2520Tan%2520and%2520Junyu%2520Han%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%2520and%2520Guanbin%2520Li%26entry.1292438233%3D%2520%2520We%2520delve%2520into%2520pseudo-labeling%2520for%2520semi-supervised%2520monocular%25203D%2520object%250Adetection%2520%2528SSM3OD%2529%2520and%2520discover%2520two%2520primary%2520issues%253A%2520a%2520misalignment%2520between%2520the%250Aprediction%2520quality%2520of%25203D%2520and%25202D%2520attributes%2520and%2520the%2520tendency%2520of%2520depth%250Asupervision%2520derived%2520from%2520pseudo-labels%2520to%2520be%2520noisy%252C%2520leading%2520to%2520significant%250Aoptimization%2520conflicts%2520with%2520other%2520reliable%2520forms%2520of%2520supervision.%2520We%2520introduce%2520a%250Anovel%2520decoupled%2520pseudo-labeling%2520%2528DPL%2529%2520approach%2520for%2520SSM3OD.%2520Our%2520approach%250Afeatures%2520a%2520Decoupled%2520Pseudo-label%2520Generation%2520%2528DPG%2529%2520module%252C%2520designed%2520to%250Aefficiently%2520generate%2520pseudo-labels%2520by%2520separately%2520processing%25202D%2520and%25203D%250Aattributes.%2520This%2520module%2520incorporates%2520a%2520unique%2520homography-based%2520method%2520for%250Aidentifying%2520dependable%2520pseudo-labels%2520in%2520BEV%2520space%252C%2520specifically%2520for%25203D%250Aattributes.%2520Additionally%252C%2520we%2520present%2520a%2520DepthGradient%2520Projection%2520%2528DGP%2529%2520module%2520to%250Amitigate%2520optimization%2520conflicts%2520caused%2520by%2520noisy%2520depth%2520supervision%2520of%250Apseudo-labels%252C%2520effectively%2520decoupling%2520the%2520depth%2520gradient%2520and%2520removing%250Aconflicting%2520gradients.%2520This%2520dual%2520decoupling%2520strategy-at%2520both%2520the%2520pseudo-label%250Ageneration%2520and%2520gradient%2520levels-significantly%2520improves%2520the%2520utilization%2520of%250Apseudo-labels%2520in%2520SSM3OD.%2520Our%2520comprehensive%2520experiments%2520on%2520the%2520KITTI%2520benchmark%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Pseudo-labeling%20for%20Semi-Supervised%20Monocular%203D%20Object%0A%20%20Detection&entry.906535625=Jiacheng%20Zhang%20and%20Jiaming%20Li%20and%20Xiangru%20Lin%20and%20Wei%20Zhang%20and%20Xiao%20Tan%20and%20Junyu%20Han%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Guanbin%20Li&entry.1292438233=%20%20We%20delve%20into%20pseudo-labeling%20for%20semi-supervised%20monocular%203D%20object%0Adetection%20%28SSM3OD%29%20and%20discover%20two%20primary%20issues%3A%20a%20misalignment%20between%20the%0Aprediction%20quality%20of%203D%20and%202D%20attributes%20and%20the%20tendency%20of%20depth%0Asupervision%20derived%20from%20pseudo-labels%20to%20be%20noisy%2C%20leading%20to%20significant%0Aoptimization%20conflicts%20with%20other%20reliable%20forms%20of%20supervision.%20We%20introduce%20a%0Anovel%20decoupled%20pseudo-labeling%20%28DPL%29%20approach%20for%20SSM3OD.%20Our%20approach%0Afeatures%20a%20Decoupled%20Pseudo-label%20Generation%20%28DPG%29%20module%2C%20designed%20to%0Aefficiently%20generate%20pseudo-labels%20by%20separately%20processing%202D%20and%203D%0Aattributes.%20This%20module%20incorporates%20a%20unique%20homography-based%20method%20for%0Aidentifying%20dependable%20pseudo-labels%20in%20BEV%20space%2C%20specifically%20for%203D%0Aattributes.%20Additionally%2C%20we%20present%20a%20DepthGradient%20Projection%20%28DGP%29%20module%20to%0Amitigate%20optimization%20conflicts%20caused%20by%20noisy%20depth%20supervision%20of%0Apseudo-labels%2C%20effectively%20decoupling%20the%20depth%20gradient%20and%20removing%0Aconflicting%20gradients.%20This%20dual%20decoupling%20strategy-at%20both%20the%20pseudo-label%0Ageneration%20and%20gradient%20levels-significantly%20improves%20the%20utilization%20of%0Apseudo-labels%20in%20SSM3OD.%20Our%20comprehensive%20experiments%20on%20the%20KITTI%20benchmark%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17387v3&entry.124074799=Read"},
{"title": "Kernel Orthogonality does not necessarily imply a Decrease in Feature\n  Map Redundancy in CNNs: Convolutional Similarity Minimization", "author": "Zakariae Belmekki and Jun Li and Patrick Reuter and David Antonio G\u00f3mez J\u00e1uregui and Karl Jenkins", "abstract": "  Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning\ndue to their success in various tasks. Nonetheless, it has been observed that\nCNNs suffer from redundancy in feature maps, leading to inefficient capacity\nutilization. Efforts to mitigate and solve this problem led to the emergence of\nmultiple methods, amongst which is kernel orthogonality through variant means.\nIn this work, we challenge the common belief that kernel orthogonality leads to\na decrease in feature map redundancy, which is, supposedly, the ultimate\nobjective behind kernel orthogonality. We prove, theoretically and empirically,\nthat kernel orthogonality has an unpredictable effect on feature map similarity\nand does not necessarily decrease it. Based on our theoretical result, we\npropose an effective method to reduce feature map similarity independently of\nthe input of the CNN. This is done by minimizing a novel loss function we call\nConvolutional Similarity. Empirical results show that minimizing the\nConvolutional Similarity increases the performance of classification models and\ncan accelerate their convergence. Furthermore, using our proposed method pushes\ntowards a more efficient use of the capacity of models, allowing the use of\nsignificantly smaller models to achieve the same levels of performance.\n", "link": "http://arxiv.org/abs/2411.03226v1", "date": "2024-11-05", "relevancy": 2.4046, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4874}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Orthogonality%20does%20not%20necessarily%20imply%20a%20Decrease%20in%20Feature%0A%20%20Map%20Redundancy%20in%20CNNs%3A%20Convolutional%20Similarity%20Minimization&body=Title%3A%20Kernel%20Orthogonality%20does%20not%20necessarily%20imply%20a%20Decrease%20in%20Feature%0A%20%20Map%20Redundancy%20in%20CNNs%3A%20Convolutional%20Similarity%20Minimization%0AAuthor%3A%20Zakariae%20Belmekki%20and%20Jun%20Li%20and%20Patrick%20Reuter%20and%20David%20Antonio%20G%C3%B3mez%20J%C3%A1uregui%20and%20Karl%20Jenkins%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20heavily%20used%20in%20Deep%20Learning%0Adue%20to%20their%20success%20in%20various%20tasks.%20Nonetheless%2C%20it%20has%20been%20observed%20that%0ACNNs%20suffer%20from%20redundancy%20in%20feature%20maps%2C%20leading%20to%20inefficient%20capacity%0Autilization.%20Efforts%20to%20mitigate%20and%20solve%20this%20problem%20led%20to%20the%20emergence%20of%0Amultiple%20methods%2C%20amongst%20which%20is%20kernel%20orthogonality%20through%20variant%20means.%0AIn%20this%20work%2C%20we%20challenge%20the%20common%20belief%20that%20kernel%20orthogonality%20leads%20to%0Aa%20decrease%20in%20feature%20map%20redundancy%2C%20which%20is%2C%20supposedly%2C%20the%20ultimate%0Aobjective%20behind%20kernel%20orthogonality.%20We%20prove%2C%20theoretically%20and%20empirically%2C%0Athat%20kernel%20orthogonality%20has%20an%20unpredictable%20effect%20on%20feature%20map%20similarity%0Aand%20does%20not%20necessarily%20decrease%20it.%20Based%20on%20our%20theoretical%20result%2C%20we%0Apropose%20an%20effective%20method%20to%20reduce%20feature%20map%20similarity%20independently%20of%0Athe%20input%20of%20the%20CNN.%20This%20is%20done%20by%20minimizing%20a%20novel%20loss%20function%20we%20call%0AConvolutional%20Similarity.%20Empirical%20results%20show%20that%20minimizing%20the%0AConvolutional%20Similarity%20increases%20the%20performance%20of%20classification%20models%20and%0Acan%20accelerate%20their%20convergence.%20Furthermore%2C%20using%20our%20proposed%20method%20pushes%0Atowards%20a%20more%20efficient%20use%20of%20the%20capacity%20of%20models%2C%20allowing%20the%20use%20of%0Asignificantly%20smaller%20models%20to%20achieve%20the%20same%20levels%20of%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Orthogonality%2520does%2520not%2520necessarily%2520imply%2520a%2520Decrease%2520in%2520Feature%250A%2520%2520Map%2520Redundancy%2520in%2520CNNs%253A%2520Convolutional%2520Similarity%2520Minimization%26entry.906535625%3DZakariae%2520Belmekki%2520and%2520Jun%2520Li%2520and%2520Patrick%2520Reuter%2520and%2520David%2520Antonio%2520G%25C3%25B3mez%2520J%25C3%25A1uregui%2520and%2520Karl%2520Jenkins%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520been%2520heavily%2520used%2520in%2520Deep%2520Learning%250Adue%2520to%2520their%2520success%2520in%2520various%2520tasks.%2520Nonetheless%252C%2520it%2520has%2520been%2520observed%2520that%250ACNNs%2520suffer%2520from%2520redundancy%2520in%2520feature%2520maps%252C%2520leading%2520to%2520inefficient%2520capacity%250Autilization.%2520Efforts%2520to%2520mitigate%2520and%2520solve%2520this%2520problem%2520led%2520to%2520the%2520emergence%2520of%250Amultiple%2520methods%252C%2520amongst%2520which%2520is%2520kernel%2520orthogonality%2520through%2520variant%2520means.%250AIn%2520this%2520work%252C%2520we%2520challenge%2520the%2520common%2520belief%2520that%2520kernel%2520orthogonality%2520leads%2520to%250Aa%2520decrease%2520in%2520feature%2520map%2520redundancy%252C%2520which%2520is%252C%2520supposedly%252C%2520the%2520ultimate%250Aobjective%2520behind%2520kernel%2520orthogonality.%2520We%2520prove%252C%2520theoretically%2520and%2520empirically%252C%250Athat%2520kernel%2520orthogonality%2520has%2520an%2520unpredictable%2520effect%2520on%2520feature%2520map%2520similarity%250Aand%2520does%2520not%2520necessarily%2520decrease%2520it.%2520Based%2520on%2520our%2520theoretical%2520result%252C%2520we%250Apropose%2520an%2520effective%2520method%2520to%2520reduce%2520feature%2520map%2520similarity%2520independently%2520of%250Athe%2520input%2520of%2520the%2520CNN.%2520This%2520is%2520done%2520by%2520minimizing%2520a%2520novel%2520loss%2520function%2520we%2520call%250AConvolutional%2520Similarity.%2520Empirical%2520results%2520show%2520that%2520minimizing%2520the%250AConvolutional%2520Similarity%2520increases%2520the%2520performance%2520of%2520classification%2520models%2520and%250Acan%2520accelerate%2520their%2520convergence.%2520Furthermore%252C%2520using%2520our%2520proposed%2520method%2520pushes%250Atowards%2520a%2520more%2520efficient%2520use%2520of%2520the%2520capacity%2520of%2520models%252C%2520allowing%2520the%2520use%2520of%250Asignificantly%2520smaller%2520models%2520to%2520achieve%2520the%2520same%2520levels%2520of%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Orthogonality%20does%20not%20necessarily%20imply%20a%20Decrease%20in%20Feature%0A%20%20Map%20Redundancy%20in%20CNNs%3A%20Convolutional%20Similarity%20Minimization&entry.906535625=Zakariae%20Belmekki%20and%20Jun%20Li%20and%20Patrick%20Reuter%20and%20David%20Antonio%20G%C3%B3mez%20J%C3%A1uregui%20and%20Karl%20Jenkins&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20heavily%20used%20in%20Deep%20Learning%0Adue%20to%20their%20success%20in%20various%20tasks.%20Nonetheless%2C%20it%20has%20been%20observed%20that%0ACNNs%20suffer%20from%20redundancy%20in%20feature%20maps%2C%20leading%20to%20inefficient%20capacity%0Autilization.%20Efforts%20to%20mitigate%20and%20solve%20this%20problem%20led%20to%20the%20emergence%20of%0Amultiple%20methods%2C%20amongst%20which%20is%20kernel%20orthogonality%20through%20variant%20means.%0AIn%20this%20work%2C%20we%20challenge%20the%20common%20belief%20that%20kernel%20orthogonality%20leads%20to%0Aa%20decrease%20in%20feature%20map%20redundancy%2C%20which%20is%2C%20supposedly%2C%20the%20ultimate%0Aobjective%20behind%20kernel%20orthogonality.%20We%20prove%2C%20theoretically%20and%20empirically%2C%0Athat%20kernel%20orthogonality%20has%20an%20unpredictable%20effect%20on%20feature%20map%20similarity%0Aand%20does%20not%20necessarily%20decrease%20it.%20Based%20on%20our%20theoretical%20result%2C%20we%0Apropose%20an%20effective%20method%20to%20reduce%20feature%20map%20similarity%20independently%20of%0Athe%20input%20of%20the%20CNN.%20This%20is%20done%20by%20minimizing%20a%20novel%20loss%20function%20we%20call%0AConvolutional%20Similarity.%20Empirical%20results%20show%20that%20minimizing%20the%0AConvolutional%20Similarity%20increases%20the%20performance%20of%20classification%20models%20and%0Acan%20accelerate%20their%20convergence.%20Furthermore%2C%20using%20our%20proposed%20method%20pushes%0Atowards%20a%20more%20efficient%20use%20of%20the%20capacity%20of%20models%2C%20allowing%20the%20use%20of%0Asignificantly%20smaller%20models%20to%20achieve%20the%20same%20levels%20of%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03226v1&entry.124074799=Read"},
{"title": "Contextual Knowledge Pursuit for Faithful Visual Synthesis", "author": "Jinqi Luo and Kwan Ho Ryan Chan and Dimitris Dimos and Ren\u00e9 Vidal", "abstract": "  Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.\n", "link": "http://arxiv.org/abs/2311.17898v3", "date": "2024-11-05", "relevancy": 2.3934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Knowledge%20Pursuit%20for%20Faithful%20Visual%20Synthesis&body=Title%3A%20Contextual%20Knowledge%20Pursuit%20for%20Faithful%20Visual%20Synthesis%0AAuthor%3A%20Jinqi%20Luo%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Dimitris%20Dimos%20and%20Ren%C3%A9%20Vidal%0AAbstract%3A%20%20%20Modern%20text-to-vision%20generative%20models%20often%20hallucinate%20when%20the%20prompt%0Adescribing%20the%20scene%20to%20be%20generated%20is%20underspecified.%20In%20large%20language%0Amodels%20%28LLMs%29%2C%20a%20prevalent%20strategy%20to%20reduce%20hallucinations%20is%20to%20retrieve%0Afactual%20knowledge%20from%20an%20external%20database.%20While%20such%20retrieval%20augmentation%0Astrategies%20have%20great%20potential%20to%20enhance%20text-to-vision%20generators%2C%20existing%0Astatic%20top-K%20retrieval%20methods%20explore%20the%20knowledge%20pool%20once%2C%20missing%20the%0Abroader%20context%20necessary%20for%20high-quality%20generation.%20Furthermore%2C%20LLMs%0Ainternally%20possess%20rich%20world%20knowledge%20learned%20during%20large-scale%20training%0A%28parametric%20knowledge%29%20that%20could%20mitigate%20the%20need%20for%20external%20data%0Aretrieval.%20This%20paper%20proposes%20Contextual%20Knowledge%20Pursuit%20%28CKPT%29%2C%20a%20framework%0Athat%20leverages%20the%20complementary%20strengths%20of%20external%20and%20parametric%20knowledge%0Ato%20help%20generators%20produce%20reliable%20visual%20content.%20Instead%20of%20the%20one-time%0Aretrieval%20of%20facts%20from%20an%20external%20database%20to%20improve%20a%20given%20prompt%2C%20CKPT%0Auses%20%281%29%20an%20LLM%20to%20decide%20whether%20to%20seek%20external%20knowledge%20or%20to%20self-elicit%0Adescriptions%20from%20LLM%20parametric%20knowledge%2C%20%282%29%20a%20knowledge%20pursuit%20process%20to%0Acontextually%20seek%20and%20sequentially%20gather%20most%20relevant%20facts%2C%20%283%29%20a%20knowledge%0Aaggregator%20for%20prompt%20enhancement%20with%20the%20gathered%20fact%20context%2C%20and%20%284%29%20a%0Afiltered%20fine-tuning%20objective%20to%20improve%20visual%20synthesis%20with%20richer%20prompts.%0AWe%20evaluate%20CKPT%20across%20multiple%20text-driven%20generative%20tasks%20%28image%2C%203D%0Arendering%2C%20and%20video%29%20on%20datasets%20of%20rare%20objects%20and%20daily%20scenarios.%20Our%0Aresults%20show%20that%20CKPT%20is%20capable%20of%20generating%20faithful%20and%20semantically%20rich%0Acontent%20across%20diverse%20visual%20domains%2C%20offering%20a%20promising%20data%20source%20for%0Azero-shot%20synthesis%20and%20filtered%20fine-tuning%20of%20text-to-vision%20generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17898v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Knowledge%2520Pursuit%2520for%2520Faithful%2520Visual%2520Synthesis%26entry.906535625%3DJinqi%2520Luo%2520and%2520Kwan%2520Ho%2520Ryan%2520Chan%2520and%2520Dimitris%2520Dimos%2520and%2520Ren%25C3%25A9%2520Vidal%26entry.1292438233%3D%2520%2520Modern%2520text-to-vision%2520generative%2520models%2520often%2520hallucinate%2520when%2520the%2520prompt%250Adescribing%2520the%2520scene%2520to%2520be%2520generated%2520is%2520underspecified.%2520In%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520a%2520prevalent%2520strategy%2520to%2520reduce%2520hallucinations%2520is%2520to%2520retrieve%250Afactual%2520knowledge%2520from%2520an%2520external%2520database.%2520While%2520such%2520retrieval%2520augmentation%250Astrategies%2520have%2520great%2520potential%2520to%2520enhance%2520text-to-vision%2520generators%252C%2520existing%250Astatic%2520top-K%2520retrieval%2520methods%2520explore%2520the%2520knowledge%2520pool%2520once%252C%2520missing%2520the%250Abroader%2520context%2520necessary%2520for%2520high-quality%2520generation.%2520Furthermore%252C%2520LLMs%250Ainternally%2520possess%2520rich%2520world%2520knowledge%2520learned%2520during%2520large-scale%2520training%250A%2528parametric%2520knowledge%2529%2520that%2520could%2520mitigate%2520the%2520need%2520for%2520external%2520data%250Aretrieval.%2520This%2520paper%2520proposes%2520Contextual%2520Knowledge%2520Pursuit%2520%2528CKPT%2529%252C%2520a%2520framework%250Athat%2520leverages%2520the%2520complementary%2520strengths%2520of%2520external%2520and%2520parametric%2520knowledge%250Ato%2520help%2520generators%2520produce%2520reliable%2520visual%2520content.%2520Instead%2520of%2520the%2520one-time%250Aretrieval%2520of%2520facts%2520from%2520an%2520external%2520database%2520to%2520improve%2520a%2520given%2520prompt%252C%2520CKPT%250Auses%2520%25281%2529%2520an%2520LLM%2520to%2520decide%2520whether%2520to%2520seek%2520external%2520knowledge%2520or%2520to%2520self-elicit%250Adescriptions%2520from%2520LLM%2520parametric%2520knowledge%252C%2520%25282%2529%2520a%2520knowledge%2520pursuit%2520process%2520to%250Acontextually%2520seek%2520and%2520sequentially%2520gather%2520most%2520relevant%2520facts%252C%2520%25283%2529%2520a%2520knowledge%250Aaggregator%2520for%2520prompt%2520enhancement%2520with%2520the%2520gathered%2520fact%2520context%252C%2520and%2520%25284%2529%2520a%250Afiltered%2520fine-tuning%2520objective%2520to%2520improve%2520visual%2520synthesis%2520with%2520richer%2520prompts.%250AWe%2520evaluate%2520CKPT%2520across%2520multiple%2520text-driven%2520generative%2520tasks%2520%2528image%252C%25203D%250Arendering%252C%2520and%2520video%2529%2520on%2520datasets%2520of%2520rare%2520objects%2520and%2520daily%2520scenarios.%2520Our%250Aresults%2520show%2520that%2520CKPT%2520is%2520capable%2520of%2520generating%2520faithful%2520and%2520semantically%2520rich%250Acontent%2520across%2520diverse%2520visual%2520domains%252C%2520offering%2520a%2520promising%2520data%2520source%2520for%250Azero-shot%2520synthesis%2520and%2520filtered%2520fine-tuning%2520of%2520text-to-vision%2520generative%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17898v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Knowledge%20Pursuit%20for%20Faithful%20Visual%20Synthesis&entry.906535625=Jinqi%20Luo%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Dimitris%20Dimos%20and%20Ren%C3%A9%20Vidal&entry.1292438233=%20%20Modern%20text-to-vision%20generative%20models%20often%20hallucinate%20when%20the%20prompt%0Adescribing%20the%20scene%20to%20be%20generated%20is%20underspecified.%20In%20large%20language%0Amodels%20%28LLMs%29%2C%20a%20prevalent%20strategy%20to%20reduce%20hallucinations%20is%20to%20retrieve%0Afactual%20knowledge%20from%20an%20external%20database.%20While%20such%20retrieval%20augmentation%0Astrategies%20have%20great%20potential%20to%20enhance%20text-to-vision%20generators%2C%20existing%0Astatic%20top-K%20retrieval%20methods%20explore%20the%20knowledge%20pool%20once%2C%20missing%20the%0Abroader%20context%20necessary%20for%20high-quality%20generation.%20Furthermore%2C%20LLMs%0Ainternally%20possess%20rich%20world%20knowledge%20learned%20during%20large-scale%20training%0A%28parametric%20knowledge%29%20that%20could%20mitigate%20the%20need%20for%20external%20data%0Aretrieval.%20This%20paper%20proposes%20Contextual%20Knowledge%20Pursuit%20%28CKPT%29%2C%20a%20framework%0Athat%20leverages%20the%20complementary%20strengths%20of%20external%20and%20parametric%20knowledge%0Ato%20help%20generators%20produce%20reliable%20visual%20content.%20Instead%20of%20the%20one-time%0Aretrieval%20of%20facts%20from%20an%20external%20database%20to%20improve%20a%20given%20prompt%2C%20CKPT%0Auses%20%281%29%20an%20LLM%20to%20decide%20whether%20to%20seek%20external%20knowledge%20or%20to%20self-elicit%0Adescriptions%20from%20LLM%20parametric%20knowledge%2C%20%282%29%20a%20knowledge%20pursuit%20process%20to%0Acontextually%20seek%20and%20sequentially%20gather%20most%20relevant%20facts%2C%20%283%29%20a%20knowledge%0Aaggregator%20for%20prompt%20enhancement%20with%20the%20gathered%20fact%20context%2C%20and%20%284%29%20a%0Afiltered%20fine-tuning%20objective%20to%20improve%20visual%20synthesis%20with%20richer%20prompts.%0AWe%20evaluate%20CKPT%20across%20multiple%20text-driven%20generative%20tasks%20%28image%2C%203D%0Arendering%2C%20and%20video%29%20on%20datasets%20of%20rare%20objects%20and%20daily%20scenarios.%20Our%0Aresults%20show%20that%20CKPT%20is%20capable%20of%20generating%20faithful%20and%20semantically%20rich%0Acontent%20across%20diverse%20visual%20domains%2C%20offering%20a%20promising%20data%20source%20for%0Azero-shot%20synthesis%20and%20filtered%20fine-tuning%20of%20text-to-vision%20generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17898v3&entry.124074799=Read"},
{"title": "Dynamic Typography: Bringing Text to Life via Video Diffusion Prior", "author": "Zichen Liu and Yihao Meng and Hao Ouyang and Yue Yu and Bolin Zhao and Daniel Cohen-Or and Huamin Qu", "abstract": "  Text animation serves as an expressive medium, transforming static\ncommunication into dynamic experiences by infusing words with motion to evoke\nemotions, emphasize meanings, and construct compelling narratives. Crafting\nanimations that are semantically aware poses significant challenges, demanding\nexpertise in graphic design and animation. We present an automated text\nanimation scheme, termed \"Dynamic Typography\", which combines two challenging\ntasks. It deforms letters to convey semantic meaning and infuses them with\nvibrant movements based on user prompts. Our technique harnesses vector\ngraphics representations and an end-to-end optimization-based framework. This\nframework employs neural displacement fields to convert letters into base\nshapes and applies per-frame motion, encouraging coherence with the intended\ntextual concept. Shape preservation techniques and perceptual loss\nregularization are employed to maintain legibility and structural integrity\nthroughout the animation process. We demonstrate the generalizability of our\napproach across various text-to-video models and highlight the superiority of\nour end-to-end methodology over baseline methods, which might comprise separate\ntasks. Through quantitative and qualitative evaluations, we demonstrate the\neffectiveness of our framework in generating coherent text animations that\nfaithfully interpret user prompts while maintaining readability. Our code is\navailable at: https://animate-your-word.github.io/demo/.\n", "link": "http://arxiv.org/abs/2404.11614v3", "date": "2024-11-05", "relevancy": 2.3817, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6679}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5884}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Typography%3A%20Bringing%20Text%20to%20Life%20via%20Video%20Diffusion%20Prior&body=Title%3A%20Dynamic%20Typography%3A%20Bringing%20Text%20to%20Life%20via%20Video%20Diffusion%20Prior%0AAuthor%3A%20Zichen%20Liu%20and%20Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Bolin%20Zhao%20and%20Daniel%20Cohen-Or%20and%20Huamin%20Qu%0AAbstract%3A%20%20%20Text%20animation%20serves%20as%20an%20expressive%20medium%2C%20transforming%20static%0Acommunication%20into%20dynamic%20experiences%20by%20infusing%20words%20with%20motion%20to%20evoke%0Aemotions%2C%20emphasize%20meanings%2C%20and%20construct%20compelling%20narratives.%20Crafting%0Aanimations%20that%20are%20semantically%20aware%20poses%20significant%20challenges%2C%20demanding%0Aexpertise%20in%20graphic%20design%20and%20animation.%20We%20present%20an%20automated%20text%0Aanimation%20scheme%2C%20termed%20%22Dynamic%20Typography%22%2C%20which%20combines%20two%20challenging%0Atasks.%20It%20deforms%20letters%20to%20convey%20semantic%20meaning%20and%20infuses%20them%20with%0Avibrant%20movements%20based%20on%20user%20prompts.%20Our%20technique%20harnesses%20vector%0Agraphics%20representations%20and%20an%20end-to-end%20optimization-based%20framework.%20This%0Aframework%20employs%20neural%20displacement%20fields%20to%20convert%20letters%20into%20base%0Ashapes%20and%20applies%20per-frame%20motion%2C%20encouraging%20coherence%20with%20the%20intended%0Atextual%20concept.%20Shape%20preservation%20techniques%20and%20perceptual%20loss%0Aregularization%20are%20employed%20to%20maintain%20legibility%20and%20structural%20integrity%0Athroughout%20the%20animation%20process.%20We%20demonstrate%20the%20generalizability%20of%20our%0Aapproach%20across%20various%20text-to-video%20models%20and%20highlight%20the%20superiority%20of%0Aour%20end-to-end%20methodology%20over%20baseline%20methods%2C%20which%20might%20comprise%20separate%0Atasks.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20generating%20coherent%20text%20animations%20that%0Afaithfully%20interpret%20user%20prompts%20while%20maintaining%20readability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//animate-your-word.github.io/demo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Typography%253A%2520Bringing%2520Text%2520to%2520Life%2520via%2520Video%2520Diffusion%2520Prior%26entry.906535625%3DZichen%2520Liu%2520and%2520Yihao%2520Meng%2520and%2520Hao%2520Ouyang%2520and%2520Yue%2520Yu%2520and%2520Bolin%2520Zhao%2520and%2520Daniel%2520Cohen-Or%2520and%2520Huamin%2520Qu%26entry.1292438233%3D%2520%2520Text%2520animation%2520serves%2520as%2520an%2520expressive%2520medium%252C%2520transforming%2520static%250Acommunication%2520into%2520dynamic%2520experiences%2520by%2520infusing%2520words%2520with%2520motion%2520to%2520evoke%250Aemotions%252C%2520emphasize%2520meanings%252C%2520and%2520construct%2520compelling%2520narratives.%2520Crafting%250Aanimations%2520that%2520are%2520semantically%2520aware%2520poses%2520significant%2520challenges%252C%2520demanding%250Aexpertise%2520in%2520graphic%2520design%2520and%2520animation.%2520We%2520present%2520an%2520automated%2520text%250Aanimation%2520scheme%252C%2520termed%2520%2522Dynamic%2520Typography%2522%252C%2520which%2520combines%2520two%2520challenging%250Atasks.%2520It%2520deforms%2520letters%2520to%2520convey%2520semantic%2520meaning%2520and%2520infuses%2520them%2520with%250Avibrant%2520movements%2520based%2520on%2520user%2520prompts.%2520Our%2520technique%2520harnesses%2520vector%250Agraphics%2520representations%2520and%2520an%2520end-to-end%2520optimization-based%2520framework.%2520This%250Aframework%2520employs%2520neural%2520displacement%2520fields%2520to%2520convert%2520letters%2520into%2520base%250Ashapes%2520and%2520applies%2520per-frame%2520motion%252C%2520encouraging%2520coherence%2520with%2520the%2520intended%250Atextual%2520concept.%2520Shape%2520preservation%2520techniques%2520and%2520perceptual%2520loss%250Aregularization%2520are%2520employed%2520to%2520maintain%2520legibility%2520and%2520structural%2520integrity%250Athroughout%2520the%2520animation%2520process.%2520We%2520demonstrate%2520the%2520generalizability%2520of%2520our%250Aapproach%2520across%2520various%2520text-to-video%2520models%2520and%2520highlight%2520the%2520superiority%2520of%250Aour%2520end-to-end%2520methodology%2520over%2520baseline%2520methods%252C%2520which%2520might%2520comprise%2520separate%250Atasks.%2520Through%2520quantitative%2520and%2520qualitative%2520evaluations%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520framework%2520in%2520generating%2520coherent%2520text%2520animations%2520that%250Afaithfully%2520interpret%2520user%2520prompts%2520while%2520maintaining%2520readability.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//animate-your-word.github.io/demo/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Typography%3A%20Bringing%20Text%20to%20Life%20via%20Video%20Diffusion%20Prior&entry.906535625=Zichen%20Liu%20and%20Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Bolin%20Zhao%20and%20Daniel%20Cohen-Or%20and%20Huamin%20Qu&entry.1292438233=%20%20Text%20animation%20serves%20as%20an%20expressive%20medium%2C%20transforming%20static%0Acommunication%20into%20dynamic%20experiences%20by%20infusing%20words%20with%20motion%20to%20evoke%0Aemotions%2C%20emphasize%20meanings%2C%20and%20construct%20compelling%20narratives.%20Crafting%0Aanimations%20that%20are%20semantically%20aware%20poses%20significant%20challenges%2C%20demanding%0Aexpertise%20in%20graphic%20design%20and%20animation.%20We%20present%20an%20automated%20text%0Aanimation%20scheme%2C%20termed%20%22Dynamic%20Typography%22%2C%20which%20combines%20two%20challenging%0Atasks.%20It%20deforms%20letters%20to%20convey%20semantic%20meaning%20and%20infuses%20them%20with%0Avibrant%20movements%20based%20on%20user%20prompts.%20Our%20technique%20harnesses%20vector%0Agraphics%20representations%20and%20an%20end-to-end%20optimization-based%20framework.%20This%0Aframework%20employs%20neural%20displacement%20fields%20to%20convert%20letters%20into%20base%0Ashapes%20and%20applies%20per-frame%20motion%2C%20encouraging%20coherence%20with%20the%20intended%0Atextual%20concept.%20Shape%20preservation%20techniques%20and%20perceptual%20loss%0Aregularization%20are%20employed%20to%20maintain%20legibility%20and%20structural%20integrity%0Athroughout%20the%20animation%20process.%20We%20demonstrate%20the%20generalizability%20of%20our%0Aapproach%20across%20various%20text-to-video%20models%20and%20highlight%20the%20superiority%20of%0Aour%20end-to-end%20methodology%20over%20baseline%20methods%2C%20which%20might%20comprise%20separate%0Atasks.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20generating%20coherent%20text%20animations%20that%0Afaithfully%20interpret%20user%20prompts%20while%20maintaining%20readability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//animate-your-word.github.io/demo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11614v3&entry.124074799=Read"},
{"title": "Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot\n  Navigation", "author": "Ananya Trivedi and Sarvesh Prajapati and Anway Shirgaonkar and Mark Zolotas and Taskin Padir", "abstract": "  Traditional approaches to motion modeling for skid-steer robots struggle with\ncapturing nonlinear tire-terrain dynamics, especially during high-speed\nmaneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic\nunicycle model with Gaussian Process (GP) regression outputs. This enables us\nto develop an adaptive, uncertainty-informed navigation formulation. We solve\nthe resultant stochastic optimal control problem using a chance-constrained\nModel Predictive Path Integral (MPPI) control method. This approach formulates\nboth obstacle avoidance and path-following as chance constraints, accounting\nfor residual uncertainties from the GP to ensure safety and reliability in\ncontrol. Leveraging GPU acceleration, we efficiently manage the non-convex\nnature of the problem, ensuring real-time performance. Our approach unifies\npath-following and obstacle avoidance across different terrains, unlike prior\nworks which typically focus on one or the other. We compare our GP-MPPI method\nagainst unicycle and data-driven kinematic models within the MPPI framework. In\nsimulations, our approach shows superior tracking accuracy and obstacle\navoidance. We further validate our approach through hardware experiments on a\nskid-steer robot platform, demonstrating its effectiveness in high-speed\nnavigation. The GPU implementation of the proposed method and supplementary\nvideo footage are available at https: //stochasticmppi.github.io.\n", "link": "http://arxiv.org/abs/2411.03289v1", "date": "2024-11-05", "relevancy": 2.3467, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6202}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Sampling%20Based%20Stochastic%20MPC%20for%20Skid-Steer%20Mobile%20Robot%0A%20%20Navigation&body=Title%3A%20Data-Driven%20Sampling%20Based%20Stochastic%20MPC%20for%20Skid-Steer%20Mobile%20Robot%0A%20%20Navigation%0AAuthor%3A%20Ananya%20Trivedi%20and%20Sarvesh%20Prajapati%20and%20Anway%20Shirgaonkar%20and%20Mark%20Zolotas%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Traditional%20approaches%20to%20motion%20modeling%20for%20skid-steer%20robots%20struggle%20with%0Acapturing%20nonlinear%20tire-terrain%20dynamics%2C%20especially%20during%20high-speed%0Amaneuvers.%20In%20this%20paper%2C%20we%20tackle%20such%20nonlinearities%20by%20enhancing%20a%20dynamic%0Aunicycle%20model%20with%20Gaussian%20Process%20%28GP%29%20regression%20outputs.%20This%20enables%20us%0Ato%20develop%20an%20adaptive%2C%20uncertainty-informed%20navigation%20formulation.%20We%20solve%0Athe%20resultant%20stochastic%20optimal%20control%20problem%20using%20a%20chance-constrained%0AModel%20Predictive%20Path%20Integral%20%28MPPI%29%20control%20method.%20This%20approach%20formulates%0Aboth%20obstacle%20avoidance%20and%20path-following%20as%20chance%20constraints%2C%20accounting%0Afor%20residual%20uncertainties%20from%20the%20GP%20to%20ensure%20safety%20and%20reliability%20in%0Acontrol.%20Leveraging%20GPU%20acceleration%2C%20we%20efficiently%20manage%20the%20non-convex%0Anature%20of%20the%20problem%2C%20ensuring%20real-time%20performance.%20Our%20approach%20unifies%0Apath-following%20and%20obstacle%20avoidance%20across%20different%20terrains%2C%20unlike%20prior%0Aworks%20which%20typically%20focus%20on%20one%20or%20the%20other.%20We%20compare%20our%20GP-MPPI%20method%0Aagainst%20unicycle%20and%20data-driven%20kinematic%20models%20within%20the%20MPPI%20framework.%20In%0Asimulations%2C%20our%20approach%20shows%20superior%20tracking%20accuracy%20and%20obstacle%0Aavoidance.%20We%20further%20validate%20our%20approach%20through%20hardware%20experiments%20on%20a%0Askid-steer%20robot%20platform%2C%20demonstrating%20its%20effectiveness%20in%20high-speed%0Anavigation.%20The%20GPU%20implementation%20of%20the%20proposed%20method%20and%20supplementary%0Avideo%20footage%20are%20available%20at%20https%3A%20//stochasticmppi.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Sampling%2520Based%2520Stochastic%2520MPC%2520for%2520Skid-Steer%2520Mobile%2520Robot%250A%2520%2520Navigation%26entry.906535625%3DAnanya%2520Trivedi%2520and%2520Sarvesh%2520Prajapati%2520and%2520Anway%2520Shirgaonkar%2520and%2520Mark%2520Zolotas%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Traditional%2520approaches%2520to%2520motion%2520modeling%2520for%2520skid-steer%2520robots%2520struggle%2520with%250Acapturing%2520nonlinear%2520tire-terrain%2520dynamics%252C%2520especially%2520during%2520high-speed%250Amaneuvers.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520such%2520nonlinearities%2520by%2520enhancing%2520a%2520dynamic%250Aunicycle%2520model%2520with%2520Gaussian%2520Process%2520%2528GP%2529%2520regression%2520outputs.%2520This%2520enables%2520us%250Ato%2520develop%2520an%2520adaptive%252C%2520uncertainty-informed%2520navigation%2520formulation.%2520We%2520solve%250Athe%2520resultant%2520stochastic%2520optimal%2520control%2520problem%2520using%2520a%2520chance-constrained%250AModel%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520control%2520method.%2520This%2520approach%2520formulates%250Aboth%2520obstacle%2520avoidance%2520and%2520path-following%2520as%2520chance%2520constraints%252C%2520accounting%250Afor%2520residual%2520uncertainties%2520from%2520the%2520GP%2520to%2520ensure%2520safety%2520and%2520reliability%2520in%250Acontrol.%2520Leveraging%2520GPU%2520acceleration%252C%2520we%2520efficiently%2520manage%2520the%2520non-convex%250Anature%2520of%2520the%2520problem%252C%2520ensuring%2520real-time%2520performance.%2520Our%2520approach%2520unifies%250Apath-following%2520and%2520obstacle%2520avoidance%2520across%2520different%2520terrains%252C%2520unlike%2520prior%250Aworks%2520which%2520typically%2520focus%2520on%2520one%2520or%2520the%2520other.%2520We%2520compare%2520our%2520GP-MPPI%2520method%250Aagainst%2520unicycle%2520and%2520data-driven%2520kinematic%2520models%2520within%2520the%2520MPPI%2520framework.%2520In%250Asimulations%252C%2520our%2520approach%2520shows%2520superior%2520tracking%2520accuracy%2520and%2520obstacle%250Aavoidance.%2520We%2520further%2520validate%2520our%2520approach%2520through%2520hardware%2520experiments%2520on%2520a%250Askid-steer%2520robot%2520platform%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520high-speed%250Anavigation.%2520The%2520GPU%2520implementation%2520of%2520the%2520proposed%2520method%2520and%2520supplementary%250Avideo%2520footage%2520are%2520available%2520at%2520https%253A%2520//stochasticmppi.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Sampling%20Based%20Stochastic%20MPC%20for%20Skid-Steer%20Mobile%20Robot%0A%20%20Navigation&entry.906535625=Ananya%20Trivedi%20and%20Sarvesh%20Prajapati%20and%20Anway%20Shirgaonkar%20and%20Mark%20Zolotas%20and%20Taskin%20Padir&entry.1292438233=%20%20Traditional%20approaches%20to%20motion%20modeling%20for%20skid-steer%20robots%20struggle%20with%0Acapturing%20nonlinear%20tire-terrain%20dynamics%2C%20especially%20during%20high-speed%0Amaneuvers.%20In%20this%20paper%2C%20we%20tackle%20such%20nonlinearities%20by%20enhancing%20a%20dynamic%0Aunicycle%20model%20with%20Gaussian%20Process%20%28GP%29%20regression%20outputs.%20This%20enables%20us%0Ato%20develop%20an%20adaptive%2C%20uncertainty-informed%20navigation%20formulation.%20We%20solve%0Athe%20resultant%20stochastic%20optimal%20control%20problem%20using%20a%20chance-constrained%0AModel%20Predictive%20Path%20Integral%20%28MPPI%29%20control%20method.%20This%20approach%20formulates%0Aboth%20obstacle%20avoidance%20and%20path-following%20as%20chance%20constraints%2C%20accounting%0Afor%20residual%20uncertainties%20from%20the%20GP%20to%20ensure%20safety%20and%20reliability%20in%0Acontrol.%20Leveraging%20GPU%20acceleration%2C%20we%20efficiently%20manage%20the%20non-convex%0Anature%20of%20the%20problem%2C%20ensuring%20real-time%20performance.%20Our%20approach%20unifies%0Apath-following%20and%20obstacle%20avoidance%20across%20different%20terrains%2C%20unlike%20prior%0Aworks%20which%20typically%20focus%20on%20one%20or%20the%20other.%20We%20compare%20our%20GP-MPPI%20method%0Aagainst%20unicycle%20and%20data-driven%20kinematic%20models%20within%20the%20MPPI%20framework.%20In%0Asimulations%2C%20our%20approach%20shows%20superior%20tracking%20accuracy%20and%20obstacle%0Aavoidance.%20We%20further%20validate%20our%20approach%20through%20hardware%20experiments%20on%20a%0Askid-steer%20robot%20platform%2C%20demonstrating%20its%20effectiveness%20in%20high-speed%0Anavigation.%20The%20GPU%20implementation%20of%20the%20proposed%20method%20and%20supplementary%0Avideo%20footage%20are%20available%20at%20https%3A%20//stochasticmppi.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03289v1&entry.124074799=Read"},
{"title": "DeBaRA: Denoising-Based 3D Room Arrangement Generation", "author": "L\u00e9opold Maillard and Nicolas Sereyjol-Garros and Tom Durand and Maks Ovsjanikov", "abstract": "  Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios.\n", "link": "http://arxiv.org/abs/2409.18336v2", "date": "2024-11-05", "relevancy": 2.2872, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeBaRA%3A%20Denoising-Based%203D%20Room%20Arrangement%20Generation&body=Title%3A%20DeBaRA%3A%20Denoising-Based%203D%20Room%20Arrangement%20Generation%0AAuthor%3A%20L%C3%A9opold%20Maillard%20and%20Nicolas%20Sereyjol-Garros%20and%20Tom%20Durand%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Generating%20realistic%20and%20diverse%20layouts%20of%20furnished%20indoor%203D%20scenes%0Aunlocks%20multiple%20interactive%20applications%20impacting%20a%20wide%20range%20of%20industries.%0AThe%20inherent%20complexity%20of%20object%20interactions%2C%20the%20limited%20amount%20of%20available%0Adata%20and%20the%20requirement%20to%20fulfill%20spatial%20constraints%20all%20make%20generative%0Amodeling%20for%203D%20scene%20synthesis%20and%20arrangement%20challenging.%20Current%20methods%0Aaddress%20these%20challenges%20autoregressively%20or%20by%20using%20off-the-shelf%20diffusion%0Aobjectives%20by%20simultaneously%20predicting%20all%20attributes%20without%203D%20reasoning%0Aconsiderations.%20In%20this%20paper%2C%20we%20introduce%20DeBaRA%2C%20a%20score-based%20model%0Aspecifically%20tailored%20for%20precise%2C%20controllable%20and%20flexible%20arrangement%0Ageneration%20in%20a%20bounded%20environment.%20We%20argue%20that%20the%20most%20critical%20component%0Aof%20a%20scene%20synthesis%20system%20is%20to%20accurately%20establish%20the%20size%20and%20position%20of%0Avarious%20objects%20within%20a%20restricted%20area.%20Based%20on%20this%20insight%2C%20we%20propose%20a%0Alightweight%20conditional%20score-based%20model%20designed%20with%203D%20spatial%20awareness%20at%0Aits%20core.%20We%20demonstrate%20that%20by%20focusing%20on%20spatial%20attributes%20of%20objects%2C%20a%0Asingle%20trained%20DeBaRA%20model%20can%20be%20leveraged%20at%20test%20time%20to%20perform%20several%0Adownstream%20applications%20such%20as%20scene%20synthesis%2C%20completion%20and%20re-arrangement.%0AFurther%2C%20we%20introduce%20a%20novel%20Self%20Score%20Evaluation%20procedure%20so%20it%20can%20be%0Aoptimally%20employed%20alongside%20external%20LLM%20models.%20We%20evaluate%20our%20approach%0Athrough%20extensive%20experiments%20and%20demonstrate%20significant%20improvement%20upon%0Astate-of-the-art%20approaches%20in%20a%20range%20of%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeBaRA%253A%2520Denoising-Based%25203D%2520Room%2520Arrangement%2520Generation%26entry.906535625%3DL%25C3%25A9opold%2520Maillard%2520and%2520Nicolas%2520Sereyjol-Garros%2520and%2520Tom%2520Durand%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520and%2520diverse%2520layouts%2520of%2520furnished%2520indoor%25203D%2520scenes%250Aunlocks%2520multiple%2520interactive%2520applications%2520impacting%2520a%2520wide%2520range%2520of%2520industries.%250AThe%2520inherent%2520complexity%2520of%2520object%2520interactions%252C%2520the%2520limited%2520amount%2520of%2520available%250Adata%2520and%2520the%2520requirement%2520to%2520fulfill%2520spatial%2520constraints%2520all%2520make%2520generative%250Amodeling%2520for%25203D%2520scene%2520synthesis%2520and%2520arrangement%2520challenging.%2520Current%2520methods%250Aaddress%2520these%2520challenges%2520autoregressively%2520or%2520by%2520using%2520off-the-shelf%2520diffusion%250Aobjectives%2520by%2520simultaneously%2520predicting%2520all%2520attributes%2520without%25203D%2520reasoning%250Aconsiderations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DeBaRA%252C%2520a%2520score-based%2520model%250Aspecifically%2520tailored%2520for%2520precise%252C%2520controllable%2520and%2520flexible%2520arrangement%250Ageneration%2520in%2520a%2520bounded%2520environment.%2520We%2520argue%2520that%2520the%2520most%2520critical%2520component%250Aof%2520a%2520scene%2520synthesis%2520system%2520is%2520to%2520accurately%2520establish%2520the%2520size%2520and%2520position%2520of%250Avarious%2520objects%2520within%2520a%2520restricted%2520area.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%250Alightweight%2520conditional%2520score-based%2520model%2520designed%2520with%25203D%2520spatial%2520awareness%2520at%250Aits%2520core.%2520We%2520demonstrate%2520that%2520by%2520focusing%2520on%2520spatial%2520attributes%2520of%2520objects%252C%2520a%250Asingle%2520trained%2520DeBaRA%2520model%2520can%2520be%2520leveraged%2520at%2520test%2520time%2520to%2520perform%2520several%250Adownstream%2520applications%2520such%2520as%2520scene%2520synthesis%252C%2520completion%2520and%2520re-arrangement.%250AFurther%252C%2520we%2520introduce%2520a%2520novel%2520Self%2520Score%2520Evaluation%2520procedure%2520so%2520it%2520can%2520be%250Aoptimally%2520employed%2520alongside%2520external%2520LLM%2520models.%2520We%2520evaluate%2520our%2520approach%250Athrough%2520extensive%2520experiments%2520and%2520demonstrate%2520significant%2520improvement%2520upon%250Astate-of-the-art%2520approaches%2520in%2520a%2520range%2520of%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeBaRA%3A%20Denoising-Based%203D%20Room%20Arrangement%20Generation&entry.906535625=L%C3%A9opold%20Maillard%20and%20Nicolas%20Sereyjol-Garros%20and%20Tom%20Durand%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Generating%20realistic%20and%20diverse%20layouts%20of%20furnished%20indoor%203D%20scenes%0Aunlocks%20multiple%20interactive%20applications%20impacting%20a%20wide%20range%20of%20industries.%0AThe%20inherent%20complexity%20of%20object%20interactions%2C%20the%20limited%20amount%20of%20available%0Adata%20and%20the%20requirement%20to%20fulfill%20spatial%20constraints%20all%20make%20generative%0Amodeling%20for%203D%20scene%20synthesis%20and%20arrangement%20challenging.%20Current%20methods%0Aaddress%20these%20challenges%20autoregressively%20or%20by%20using%20off-the-shelf%20diffusion%0Aobjectives%20by%20simultaneously%20predicting%20all%20attributes%20without%203D%20reasoning%0Aconsiderations.%20In%20this%20paper%2C%20we%20introduce%20DeBaRA%2C%20a%20score-based%20model%0Aspecifically%20tailored%20for%20precise%2C%20controllable%20and%20flexible%20arrangement%0Ageneration%20in%20a%20bounded%20environment.%20We%20argue%20that%20the%20most%20critical%20component%0Aof%20a%20scene%20synthesis%20system%20is%20to%20accurately%20establish%20the%20size%20and%20position%20of%0Avarious%20objects%20within%20a%20restricted%20area.%20Based%20on%20this%20insight%2C%20we%20propose%20a%0Alightweight%20conditional%20score-based%20model%20designed%20with%203D%20spatial%20awareness%20at%0Aits%20core.%20We%20demonstrate%20that%20by%20focusing%20on%20spatial%20attributes%20of%20objects%2C%20a%0Asingle%20trained%20DeBaRA%20model%20can%20be%20leveraged%20at%20test%20time%20to%20perform%20several%0Adownstream%20applications%20such%20as%20scene%20synthesis%2C%20completion%20and%20re-arrangement.%0AFurther%2C%20we%20introduce%20a%20novel%20Self%20Score%20Evaluation%20procedure%20so%20it%20can%20be%0Aoptimally%20employed%20alongside%20external%20LLM%20models.%20We%20evaluate%20our%20approach%0Athrough%20extensive%20experiments%20and%20demonstrate%20significant%20improvement%20upon%0Astate-of-the-art%20approaches%20in%20a%20range%20of%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18336v2&entry.124074799=Read"},
{"title": "OCCAM: Online Continuous Controller Adaptation with Meta-Learned Models", "author": "Hersh Sanghvi and Spencer Folk and Camillo Jose Taylor", "abstract": "  Control tuning and adaptation present a significant challenge to the usage of\nrobots in diverse environments. It is often nontrivial to find a single set of\ncontrol parameters by hand that work well across the broad array of\nenvironments and conditions that a robot might encounter. Automated adaptation\napproaches must utilize prior knowledge about the system while adapting to\nsignificant domain shifts to find new control parameters quickly. In this work,\nwe present a general framework for online controller adaptation that deals with\nthese challenges. We combine meta-learning with Bayesian recursive estimation\nto learn prior predictive models of system performance that quickly adapt to\nonline data, even when there is significant domain shift. These predictive\nmodels can be used as cost functions within efficient sampling-based\noptimization routines to find new control parameters online that maximize\nsystem performance. Our framework is powerful and flexible enough to adapt\ncontrollers for four diverse systems: a simulated race car, a simulated\nquadrupedal robot, and a simulated and physical quadrotor. The video and code\ncan be found at https://hersh500.github.io/occam.\n", "link": "http://arxiv.org/abs/2406.17620v2", "date": "2024-11-05", "relevancy": 2.286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6187}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5696}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCCAM%3A%20Online%20Continuous%20Controller%20Adaptation%20with%20Meta-Learned%20Models&body=Title%3A%20OCCAM%3A%20Online%20Continuous%20Controller%20Adaptation%20with%20Meta-Learned%20Models%0AAuthor%3A%20Hersh%20Sanghvi%20and%20Spencer%20Folk%20and%20Camillo%20Jose%20Taylor%0AAbstract%3A%20%20%20Control%20tuning%20and%20adaptation%20present%20a%20significant%20challenge%20to%20the%20usage%20of%0Arobots%20in%20diverse%20environments.%20It%20is%20often%20nontrivial%20to%20find%20a%20single%20set%20of%0Acontrol%20parameters%20by%20hand%20that%20work%20well%20across%20the%20broad%20array%20of%0Aenvironments%20and%20conditions%20that%20a%20robot%20might%20encounter.%20Automated%20adaptation%0Aapproaches%20must%20utilize%20prior%20knowledge%20about%20the%20system%20while%20adapting%20to%0Asignificant%20domain%20shifts%20to%20find%20new%20control%20parameters%20quickly.%20In%20this%20work%2C%0Awe%20present%20a%20general%20framework%20for%20online%20controller%20adaptation%20that%20deals%20with%0Athese%20challenges.%20We%20combine%20meta-learning%20with%20Bayesian%20recursive%20estimation%0Ato%20learn%20prior%20predictive%20models%20of%20system%20performance%20that%20quickly%20adapt%20to%0Aonline%20data%2C%20even%20when%20there%20is%20significant%20domain%20shift.%20These%20predictive%0Amodels%20can%20be%20used%20as%20cost%20functions%20within%20efficient%20sampling-based%0Aoptimization%20routines%20to%20find%20new%20control%20parameters%20online%20that%20maximize%0Asystem%20performance.%20Our%20framework%20is%20powerful%20and%20flexible%20enough%20to%20adapt%0Acontrollers%20for%20four%20diverse%20systems%3A%20a%20simulated%20race%20car%2C%20a%20simulated%0Aquadrupedal%20robot%2C%20and%20a%20simulated%20and%20physical%20quadrotor.%20The%20video%20and%20code%0Acan%20be%20found%20at%20https%3A//hersh500.github.io/occam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCCAM%253A%2520Online%2520Continuous%2520Controller%2520Adaptation%2520with%2520Meta-Learned%2520Models%26entry.906535625%3DHersh%2520Sanghvi%2520and%2520Spencer%2520Folk%2520and%2520Camillo%2520Jose%2520Taylor%26entry.1292438233%3D%2520%2520Control%2520tuning%2520and%2520adaptation%2520present%2520a%2520significant%2520challenge%2520to%2520the%2520usage%2520of%250Arobots%2520in%2520diverse%2520environments.%2520It%2520is%2520often%2520nontrivial%2520to%2520find%2520a%2520single%2520set%2520of%250Acontrol%2520parameters%2520by%2520hand%2520that%2520work%2520well%2520across%2520the%2520broad%2520array%2520of%250Aenvironments%2520and%2520conditions%2520that%2520a%2520robot%2520might%2520encounter.%2520Automated%2520adaptation%250Aapproaches%2520must%2520utilize%2520prior%2520knowledge%2520about%2520the%2520system%2520while%2520adapting%2520to%250Asignificant%2520domain%2520shifts%2520to%2520find%2520new%2520control%2520parameters%2520quickly.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520general%2520framework%2520for%2520online%2520controller%2520adaptation%2520that%2520deals%2520with%250Athese%2520challenges.%2520We%2520combine%2520meta-learning%2520with%2520Bayesian%2520recursive%2520estimation%250Ato%2520learn%2520prior%2520predictive%2520models%2520of%2520system%2520performance%2520that%2520quickly%2520adapt%2520to%250Aonline%2520data%252C%2520even%2520when%2520there%2520is%2520significant%2520domain%2520shift.%2520These%2520predictive%250Amodels%2520can%2520be%2520used%2520as%2520cost%2520functions%2520within%2520efficient%2520sampling-based%250Aoptimization%2520routines%2520to%2520find%2520new%2520control%2520parameters%2520online%2520that%2520maximize%250Asystem%2520performance.%2520Our%2520framework%2520is%2520powerful%2520and%2520flexible%2520enough%2520to%2520adapt%250Acontrollers%2520for%2520four%2520diverse%2520systems%253A%2520a%2520simulated%2520race%2520car%252C%2520a%2520simulated%250Aquadrupedal%2520robot%252C%2520and%2520a%2520simulated%2520and%2520physical%2520quadrotor.%2520The%2520video%2520and%2520code%250Acan%2520be%2520found%2520at%2520https%253A//hersh500.github.io/occam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCCAM%3A%20Online%20Continuous%20Controller%20Adaptation%20with%20Meta-Learned%20Models&entry.906535625=Hersh%20Sanghvi%20and%20Spencer%20Folk%20and%20Camillo%20Jose%20Taylor&entry.1292438233=%20%20Control%20tuning%20and%20adaptation%20present%20a%20significant%20challenge%20to%20the%20usage%20of%0Arobots%20in%20diverse%20environments.%20It%20is%20often%20nontrivial%20to%20find%20a%20single%20set%20of%0Acontrol%20parameters%20by%20hand%20that%20work%20well%20across%20the%20broad%20array%20of%0Aenvironments%20and%20conditions%20that%20a%20robot%20might%20encounter.%20Automated%20adaptation%0Aapproaches%20must%20utilize%20prior%20knowledge%20about%20the%20system%20while%20adapting%20to%0Asignificant%20domain%20shifts%20to%20find%20new%20control%20parameters%20quickly.%20In%20this%20work%2C%0Awe%20present%20a%20general%20framework%20for%20online%20controller%20adaptation%20that%20deals%20with%0Athese%20challenges.%20We%20combine%20meta-learning%20with%20Bayesian%20recursive%20estimation%0Ato%20learn%20prior%20predictive%20models%20of%20system%20performance%20that%20quickly%20adapt%20to%0Aonline%20data%2C%20even%20when%20there%20is%20significant%20domain%20shift.%20These%20predictive%0Amodels%20can%20be%20used%20as%20cost%20functions%20within%20efficient%20sampling-based%0Aoptimization%20routines%20to%20find%20new%20control%20parameters%20online%20that%20maximize%0Asystem%20performance.%20Our%20framework%20is%20powerful%20and%20flexible%20enough%20to%20adapt%0Acontrollers%20for%20four%20diverse%20systems%3A%20a%20simulated%20race%20car%2C%20a%20simulated%0Aquadrupedal%20robot%2C%20and%20a%20simulated%20and%20physical%20quadrotor.%20The%20video%20and%20code%0Acan%20be%20found%20at%20https%3A//hersh500.github.io/occam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17620v2&entry.124074799=Read"},
{"title": "Rethinking Decoders for Transformer-based Semantic Segmentation:\n  Compression is All You Need", "author": "Qishuai Wen and Chun-Guang Li", "abstract": "  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n", "link": "http://arxiv.org/abs/2411.03033v1", "date": "2024-11-05", "relevancy": 2.2705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%0A%20%20Compression%20is%20All%20You%20Need&body=Title%3A%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%0A%20%20Compression%20is%20All%20You%20Need%0AAuthor%3A%20Qishuai%20Wen%20and%20Chun-Guang%20Li%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Decoders%2520for%2520Transformer-based%2520Semantic%2520Segmentation%253A%250A%2520%2520Compression%2520is%2520All%2520You%2520Need%26entry.906535625%3DQishuai%2520Wen%2520and%2520Chun-Guang%2520Li%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520Transformer-based%2520semantic%2520segmentation%250Atypically%2520adopt%2520Transformer%2520decoders%2520that%2520are%2520used%2520to%2520extract%2520additional%250Aembeddings%2520from%2520image%2520embeddings%2520via%2520cross-attention%252C%2520refine%2520either%2520or%2520both%250Atypes%2520of%2520embeddings%2520via%2520self-attention%252C%2520and%2520project%2520image%2520embeddings%2520onto%2520the%250Aadditional%2520embeddings%2520via%2520dot-product.%2520Despite%2520their%2520remarkable%2520success%252C%2520these%250Aempirical%2520designs%2520still%2520lack%2520theoretical%2520justifications%2520or%2520interpretations%252C%250Athus%2520hindering%2520potentially%2520principled%2520improvements.%2520In%2520this%2520paper%252C%2520we%2520argue%250Athat%2520there%2520are%2520fundamental%2520connections%2520between%2520semantic%2520segmentation%2520and%250Acompression%252C%2520especially%2520between%2520the%2520Transformer%2520decoders%2520and%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529.%2520From%2520such%2520a%2520perspective%252C%2520we%2520derive%2520a%2520white-box%252C%2520fully%250Aattentional%2520DEcoder%2520for%2520PrIncipled%2520semantiC%2520segemenTation%2520%2528DEPICT%2529%252C%2520with%2520the%250Ainterpretations%2520as%2520follows%253A%25201%2529%2520the%2520self-attention%2520operator%2520refines%2520image%250Aembeddings%2520to%2520construct%2520an%2520ideal%2520principal%2520subspace%2520that%2520aligns%2520with%2520the%250Asupervision%2520and%2520retains%2520most%2520information%253B%25202%2529%2520the%2520cross-attention%2520operator%2520seeks%250Ato%2520find%2520a%2520low-rank%2520approximation%2520of%2520the%2520refined%2520image%2520embeddings%252C%2520which%2520is%250Aexpected%2520to%2520be%2520a%2520set%2520of%2520orthonormal%2520bases%2520of%2520the%2520principal%2520subspace%2520and%250Acorresponds%2520to%2520the%2520predefined%2520classes%253B%25203%2529%2520the%2520dot-product%2520operation%2520yields%250Acompact%2520representation%2520for%2520image%2520embeddings%2520as%2520segmentation%2520masks.%2520Experiments%250Aconducted%2520on%2520dataset%2520ADE20K%2520find%2520that%2520DEPICT%2520consistently%2520outperforms%2520its%250Ablack-box%2520counterpart%252C%2520Segmenter%252C%2520and%2520it%2520is%2520light%2520weight%2520and%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%0A%20%20Compression%20is%20All%20You%20Need&entry.906535625=Qishuai%20Wen%20and%20Chun-Guang%20Li&entry.1292438233=%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03033v1&entry.124074799=Read"},
{"title": "Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor", "author": "Anish Bhattacharya and Marco Cannici and Nishanth Rao and Yuezhan Tao and Vijay Kumar and Nikolai Matni and Davide Scaramuzza", "abstract": "  We present the first static-obstacle avoidance method for quadrotors using\njust an onboard, monocular event camera. Quadrotors are capable of fast and\nagile flight in cluttered environments when piloted manually, but vision-based\nautonomous flight in unknown environments is difficult in part due to the\nsensor limitations of traditional onboard cameras. Event cameras, however,\npromise nearly zero motion blur and high dynamic range, but produce a very\nlarge volume of events under significant ego-motion and further lack a\ncontinuous-time sensor model in simulation, making direct sim-to-real transfer\nnot possible. By leveraging depth prediction as a pretext task in our learning\nframework, we can pre-train a reactive obstacle avoidance events-to-control\npolicy with approximated, simulated events and then fine-tune the perception\ncomponent with limited events-and-depth real-world data to achieve obstacle\navoidance in indoor and outdoor settings. We demonstrate this across two\nquadrotor-event camera platforms in multiple settings and find, contrary to\ntraditional vision-based works, that low speeds (1m/s) make the task harder and\nmore prone to collisions, while high speeds (5m/s) result in better event-based\ndepth estimation and avoidance. We also find that success rates in outdoor\nscenes can be significantly higher than in certain indoor scenes.\n", "link": "http://arxiv.org/abs/2411.03303v1", "date": "2024-11-05", "relevancy": 2.2628, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5793}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Event-Based%20Vision%20for%20Obstacle%20Avoidance%20with%20a%20Quadrotor&body=Title%3A%20Monocular%20Event-Based%20Vision%20for%20Obstacle%20Avoidance%20with%20a%20Quadrotor%0AAuthor%3A%20Anish%20Bhattacharya%20and%20Marco%20Cannici%20and%20Nishanth%20Rao%20and%20Yuezhan%20Tao%20and%20Vijay%20Kumar%20and%20Nikolai%20Matni%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20We%20present%20the%20first%20static-obstacle%20avoidance%20method%20for%20quadrotors%20using%0Ajust%20an%20onboard%2C%20monocular%20event%20camera.%20Quadrotors%20are%20capable%20of%20fast%20and%0Aagile%20flight%20in%20cluttered%20environments%20when%20piloted%20manually%2C%20but%20vision-based%0Aautonomous%20flight%20in%20unknown%20environments%20is%20difficult%20in%20part%20due%20to%20the%0Asensor%20limitations%20of%20traditional%20onboard%20cameras.%20Event%20cameras%2C%20however%2C%0Apromise%20nearly%20zero%20motion%20blur%20and%20high%20dynamic%20range%2C%20but%20produce%20a%20very%0Alarge%20volume%20of%20events%20under%20significant%20ego-motion%20and%20further%20lack%20a%0Acontinuous-time%20sensor%20model%20in%20simulation%2C%20making%20direct%20sim-to-real%20transfer%0Anot%20possible.%20By%20leveraging%20depth%20prediction%20as%20a%20pretext%20task%20in%20our%20learning%0Aframework%2C%20we%20can%20pre-train%20a%20reactive%20obstacle%20avoidance%20events-to-control%0Apolicy%20with%20approximated%2C%20simulated%20events%20and%20then%20fine-tune%20the%20perception%0Acomponent%20with%20limited%20events-and-depth%20real-world%20data%20to%20achieve%20obstacle%0Aavoidance%20in%20indoor%20and%20outdoor%20settings.%20We%20demonstrate%20this%20across%20two%0Aquadrotor-event%20camera%20platforms%20in%20multiple%20settings%20and%20find%2C%20contrary%20to%0Atraditional%20vision-based%20works%2C%20that%20low%20speeds%20%281m/s%29%20make%20the%20task%20harder%20and%0Amore%20prone%20to%20collisions%2C%20while%20high%20speeds%20%285m/s%29%20result%20in%20better%20event-based%0Adepth%20estimation%20and%20avoidance.%20We%20also%20find%20that%20success%20rates%20in%20outdoor%0Ascenes%20can%20be%20significantly%20higher%20than%20in%20certain%20indoor%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Event-Based%2520Vision%2520for%2520Obstacle%2520Avoidance%2520with%2520a%2520Quadrotor%26entry.906535625%3DAnish%2520Bhattacharya%2520and%2520Marco%2520Cannici%2520and%2520Nishanth%2520Rao%2520and%2520Yuezhan%2520Tao%2520and%2520Vijay%2520Kumar%2520and%2520Nikolai%2520Matni%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520static-obstacle%2520avoidance%2520method%2520for%2520quadrotors%2520using%250Ajust%2520an%2520onboard%252C%2520monocular%2520event%2520camera.%2520Quadrotors%2520are%2520capable%2520of%2520fast%2520and%250Aagile%2520flight%2520in%2520cluttered%2520environments%2520when%2520piloted%2520manually%252C%2520but%2520vision-based%250Aautonomous%2520flight%2520in%2520unknown%2520environments%2520is%2520difficult%2520in%2520part%2520due%2520to%2520the%250Asensor%2520limitations%2520of%2520traditional%2520onboard%2520cameras.%2520Event%2520cameras%252C%2520however%252C%250Apromise%2520nearly%2520zero%2520motion%2520blur%2520and%2520high%2520dynamic%2520range%252C%2520but%2520produce%2520a%2520very%250Alarge%2520volume%2520of%2520events%2520under%2520significant%2520ego-motion%2520and%2520further%2520lack%2520a%250Acontinuous-time%2520sensor%2520model%2520in%2520simulation%252C%2520making%2520direct%2520sim-to-real%2520transfer%250Anot%2520possible.%2520By%2520leveraging%2520depth%2520prediction%2520as%2520a%2520pretext%2520task%2520in%2520our%2520learning%250Aframework%252C%2520we%2520can%2520pre-train%2520a%2520reactive%2520obstacle%2520avoidance%2520events-to-control%250Apolicy%2520with%2520approximated%252C%2520simulated%2520events%2520and%2520then%2520fine-tune%2520the%2520perception%250Acomponent%2520with%2520limited%2520events-and-depth%2520real-world%2520data%2520to%2520achieve%2520obstacle%250Aavoidance%2520in%2520indoor%2520and%2520outdoor%2520settings.%2520We%2520demonstrate%2520this%2520across%2520two%250Aquadrotor-event%2520camera%2520platforms%2520in%2520multiple%2520settings%2520and%2520find%252C%2520contrary%2520to%250Atraditional%2520vision-based%2520works%252C%2520that%2520low%2520speeds%2520%25281m/s%2529%2520make%2520the%2520task%2520harder%2520and%250Amore%2520prone%2520to%2520collisions%252C%2520while%2520high%2520speeds%2520%25285m/s%2529%2520result%2520in%2520better%2520event-based%250Adepth%2520estimation%2520and%2520avoidance.%2520We%2520also%2520find%2520that%2520success%2520rates%2520in%2520outdoor%250Ascenes%2520can%2520be%2520significantly%2520higher%2520than%2520in%2520certain%2520indoor%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Event-Based%20Vision%20for%20Obstacle%20Avoidance%20with%20a%20Quadrotor&entry.906535625=Anish%20Bhattacharya%20and%20Marco%20Cannici%20and%20Nishanth%20Rao%20and%20Yuezhan%20Tao%20and%20Vijay%20Kumar%20and%20Nikolai%20Matni%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20We%20present%20the%20first%20static-obstacle%20avoidance%20method%20for%20quadrotors%20using%0Ajust%20an%20onboard%2C%20monocular%20event%20camera.%20Quadrotors%20are%20capable%20of%20fast%20and%0Aagile%20flight%20in%20cluttered%20environments%20when%20piloted%20manually%2C%20but%20vision-based%0Aautonomous%20flight%20in%20unknown%20environments%20is%20difficult%20in%20part%20due%20to%20the%0Asensor%20limitations%20of%20traditional%20onboard%20cameras.%20Event%20cameras%2C%20however%2C%0Apromise%20nearly%20zero%20motion%20blur%20and%20high%20dynamic%20range%2C%20but%20produce%20a%20very%0Alarge%20volume%20of%20events%20under%20significant%20ego-motion%20and%20further%20lack%20a%0Acontinuous-time%20sensor%20model%20in%20simulation%2C%20making%20direct%20sim-to-real%20transfer%0Anot%20possible.%20By%20leveraging%20depth%20prediction%20as%20a%20pretext%20task%20in%20our%20learning%0Aframework%2C%20we%20can%20pre-train%20a%20reactive%20obstacle%20avoidance%20events-to-control%0Apolicy%20with%20approximated%2C%20simulated%20events%20and%20then%20fine-tune%20the%20perception%0Acomponent%20with%20limited%20events-and-depth%20real-world%20data%20to%20achieve%20obstacle%0Aavoidance%20in%20indoor%20and%20outdoor%20settings.%20We%20demonstrate%20this%20across%20two%0Aquadrotor-event%20camera%20platforms%20in%20multiple%20settings%20and%20find%2C%20contrary%20to%0Atraditional%20vision-based%20works%2C%20that%20low%20speeds%20%281m/s%29%20make%20the%20task%20harder%20and%0Amore%20prone%20to%20collisions%2C%20while%20high%20speeds%20%285m/s%29%20result%20in%20better%20event-based%0Adepth%20estimation%20and%20avoidance.%20We%20also%20find%20that%20success%20rates%20in%20outdoor%0Ascenes%20can%20be%20significantly%20higher%20than%20in%20certain%20indoor%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03303v1&entry.124074799=Read"},
{"title": "Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities\n  of Neurosymbolic AI", "author": "Ruwan Wickramarachchi and Cory Henson and Amit Sheth", "abstract": "  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful\napproach for tasks spanning from perception to cognition. The use of\nNeurosymbolic AI has been shown to achieve enhanced capabilities, including\nimproved grounding, alignment, explainability, and reliability. However, due to\nits nascent stage, there is a lack of widely available real-world benchmark\ndatasets tailored to Neurosymbolic AI tasks. To address this gap and support\nthe evaluation of current and future methods, we introduce DSceneKG -- a suite\nof knowledge graphs of driving scenes built from real-world, high-quality\nscenes from multiple open autonomous driving datasets. In this article, we\ndetail the construction process of DSceneKG and highlight its application in\nseven different tasks. DSceneKG is publicly accessible at:\nhttps://github.com/ruwantw/DSceneKG\n", "link": "http://arxiv.org/abs/2411.03225v1", "date": "2024-11-05", "relevancy": 2.2422, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI&body=Title%3A%20Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI%0AAuthor%3A%20Ruwan%20Wickramarachchi%20and%20Cory%20Henson%20and%20Amit%20Sheth%0AAbstract%3A%20%20%20In%20the%20era%20of%20Generative%20AI%2C%20Neurosymbolic%20AI%20is%20emerging%20as%20a%20powerful%0Aapproach%20for%20tasks%20spanning%20from%20perception%20to%20cognition.%20The%20use%20of%0ANeurosymbolic%20AI%20has%20been%20shown%20to%20achieve%20enhanced%20capabilities%2C%20including%0Aimproved%20grounding%2C%20alignment%2C%20explainability%2C%20and%20reliability.%20However%2C%20due%20to%0Aits%20nascent%20stage%2C%20there%20is%20a%20lack%20of%20widely%20available%20real-world%20benchmark%0Adatasets%20tailored%20to%20Neurosymbolic%20AI%20tasks.%20To%20address%20this%20gap%20and%20support%0Athe%20evaluation%20of%20current%20and%20future%20methods%2C%20we%20introduce%20DSceneKG%20--%20a%20suite%0Aof%20knowledge%20graphs%20of%20driving%20scenes%20built%20from%20real-world%2C%20high-quality%0Ascenes%20from%20multiple%20open%20autonomous%20driving%20datasets.%20In%20this%20article%2C%20we%0Adetail%20the%20construction%20process%20of%20DSceneKG%20and%20highlight%20its%20application%20in%0Aseven%20different%20tasks.%20DSceneKG%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/ruwantw/DSceneKG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graphs%2520of%2520Driving%2520Scenes%2520to%2520Empower%2520the%2520Emerging%2520Capabilities%250A%2520%2520of%2520Neurosymbolic%2520AI%26entry.906535625%3DRuwan%2520Wickramarachchi%2520and%2520Cory%2520Henson%2520and%2520Amit%2520Sheth%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520Generative%2520AI%252C%2520Neurosymbolic%2520AI%2520is%2520emerging%2520as%2520a%2520powerful%250Aapproach%2520for%2520tasks%2520spanning%2520from%2520perception%2520to%2520cognition.%2520The%2520use%2520of%250ANeurosymbolic%2520AI%2520has%2520been%2520shown%2520to%2520achieve%2520enhanced%2520capabilities%252C%2520including%250Aimproved%2520grounding%252C%2520alignment%252C%2520explainability%252C%2520and%2520reliability.%2520However%252C%2520due%2520to%250Aits%2520nascent%2520stage%252C%2520there%2520is%2520a%2520lack%2520of%2520widely%2520available%2520real-world%2520benchmark%250Adatasets%2520tailored%2520to%2520Neurosymbolic%2520AI%2520tasks.%2520To%2520address%2520this%2520gap%2520and%2520support%250Athe%2520evaluation%2520of%2520current%2520and%2520future%2520methods%252C%2520we%2520introduce%2520DSceneKG%2520--%2520a%2520suite%250Aof%2520knowledge%2520graphs%2520of%2520driving%2520scenes%2520built%2520from%2520real-world%252C%2520high-quality%250Ascenes%2520from%2520multiple%2520open%2520autonomous%2520driving%2520datasets.%2520In%2520this%2520article%252C%2520we%250Adetail%2520the%2520construction%2520process%2520of%2520DSceneKG%2520and%2520highlight%2520its%2520application%2520in%250Aseven%2520different%2520tasks.%2520DSceneKG%2520is%2520publicly%2520accessible%2520at%253A%250Ahttps%253A//github.com/ruwantw/DSceneKG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI&entry.906535625=Ruwan%20Wickramarachchi%20and%20Cory%20Henson%20and%20Amit%20Sheth&entry.1292438233=%20%20In%20the%20era%20of%20Generative%20AI%2C%20Neurosymbolic%20AI%20is%20emerging%20as%20a%20powerful%0Aapproach%20for%20tasks%20spanning%20from%20perception%20to%20cognition.%20The%20use%20of%0ANeurosymbolic%20AI%20has%20been%20shown%20to%20achieve%20enhanced%20capabilities%2C%20including%0Aimproved%20grounding%2C%20alignment%2C%20explainability%2C%20and%20reliability.%20However%2C%20due%20to%0Aits%20nascent%20stage%2C%20there%20is%20a%20lack%20of%20widely%20available%20real-world%20benchmark%0Adatasets%20tailored%20to%20Neurosymbolic%20AI%20tasks.%20To%20address%20this%20gap%20and%20support%0Athe%20evaluation%20of%20current%20and%20future%20methods%2C%20we%20introduce%20DSceneKG%20--%20a%20suite%0Aof%20knowledge%20graphs%20of%20driving%20scenes%20built%20from%20real-world%2C%20high-quality%0Ascenes%20from%20multiple%20open%20autonomous%20driving%20datasets.%20In%20this%20article%2C%20we%0Adetail%20the%20construction%20process%20of%20DSceneKG%20and%20highlight%20its%20application%20in%0Aseven%20different%20tasks.%20DSceneKG%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/ruwantw/DSceneKG%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03225v1&entry.124074799=Read"},
{"title": "Enhancing Transformer Training Efficiency with Dynamic Dropout", "author": "Hanrui Yan and Dan Shao", "abstract": "  We introduce Dynamic Dropout, a novel regularization technique designed to\nenhance the training efficiency of Transformer models by dynamically adjusting\nthe dropout rate based on training epochs or validation loss improvements. This\napproach addresses the challenge of balancing regularization and model\ncapacity, which is crucial for achieving fast convergence and high performance.\nOur method involves modifying the GPT model to accept a variable dropout rate\nand updating dropout layers during training using schedules such as linear\ndecay, exponential decay, and validation loss-based adjustments. Extensive\nexperiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout\nsignificantly accelerates training and improves inference efficiency compared\nto a baseline model with a fixed dropout rate. The validation loss-based\nadjustment schedule provided the best overall performance, highlighting the\npotential of Dynamic Dropout as a valuable technique for training large-scale\nTransformer models.\n", "link": "http://arxiv.org/abs/2411.03236v1", "date": "2024-11-05", "relevancy": 2.1959, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5629}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5456}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Transformer%20Training%20Efficiency%20with%20Dynamic%20Dropout&body=Title%3A%20Enhancing%20Transformer%20Training%20Efficiency%20with%20Dynamic%20Dropout%0AAuthor%3A%20Hanrui%20Yan%20and%20Dan%20Shao%0AAbstract%3A%20%20%20We%20introduce%20Dynamic%20Dropout%2C%20a%20novel%20regularization%20technique%20designed%20to%0Aenhance%20the%20training%20efficiency%20of%20Transformer%20models%20by%20dynamically%20adjusting%0Athe%20dropout%20rate%20based%20on%20training%20epochs%20or%20validation%20loss%20improvements.%20This%0Aapproach%20addresses%20the%20challenge%20of%20balancing%20regularization%20and%20model%0Acapacity%2C%20which%20is%20crucial%20for%20achieving%20fast%20convergence%20and%20high%20performance.%0AOur%20method%20involves%20modifying%20the%20GPT%20model%20to%20accept%20a%20variable%20dropout%20rate%0Aand%20updating%20dropout%20layers%20during%20training%20using%20schedules%20such%20as%20linear%0Adecay%2C%20exponential%20decay%2C%20and%20validation%20loss-based%20adjustments.%20Extensive%0Aexperiments%20on%20the%20Shakespeare%5C_char%20dataset%20demonstrate%20that%20Dynamic%20Dropout%0Asignificantly%20accelerates%20training%20and%20improves%20inference%20efficiency%20compared%0Ato%20a%20baseline%20model%20with%20a%20fixed%20dropout%20rate.%20The%20validation%20loss-based%0Aadjustment%20schedule%20provided%20the%20best%20overall%20performance%2C%20highlighting%20the%0Apotential%20of%20Dynamic%20Dropout%20as%20a%20valuable%20technique%20for%20training%20large-scale%0ATransformer%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Transformer%2520Training%2520Efficiency%2520with%2520Dynamic%2520Dropout%26entry.906535625%3DHanrui%2520Yan%2520and%2520Dan%2520Shao%26entry.1292438233%3D%2520%2520We%2520introduce%2520Dynamic%2520Dropout%252C%2520a%2520novel%2520regularization%2520technique%2520designed%2520to%250Aenhance%2520the%2520training%2520efficiency%2520of%2520Transformer%2520models%2520by%2520dynamically%2520adjusting%250Athe%2520dropout%2520rate%2520based%2520on%2520training%2520epochs%2520or%2520validation%2520loss%2520improvements.%2520This%250Aapproach%2520addresses%2520the%2520challenge%2520of%2520balancing%2520regularization%2520and%2520model%250Acapacity%252C%2520which%2520is%2520crucial%2520for%2520achieving%2520fast%2520convergence%2520and%2520high%2520performance.%250AOur%2520method%2520involves%2520modifying%2520the%2520GPT%2520model%2520to%2520accept%2520a%2520variable%2520dropout%2520rate%250Aand%2520updating%2520dropout%2520layers%2520during%2520training%2520using%2520schedules%2520such%2520as%2520linear%250Adecay%252C%2520exponential%2520decay%252C%2520and%2520validation%2520loss-based%2520adjustments.%2520Extensive%250Aexperiments%2520on%2520the%2520Shakespeare%255C_char%2520dataset%2520demonstrate%2520that%2520Dynamic%2520Dropout%250Asignificantly%2520accelerates%2520training%2520and%2520improves%2520inference%2520efficiency%2520compared%250Ato%2520a%2520baseline%2520model%2520with%2520a%2520fixed%2520dropout%2520rate.%2520The%2520validation%2520loss-based%250Aadjustment%2520schedule%2520provided%2520the%2520best%2520overall%2520performance%252C%2520highlighting%2520the%250Apotential%2520of%2520Dynamic%2520Dropout%2520as%2520a%2520valuable%2520technique%2520for%2520training%2520large-scale%250ATransformer%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Transformer%20Training%20Efficiency%20with%20Dynamic%20Dropout&entry.906535625=Hanrui%20Yan%20and%20Dan%20Shao&entry.1292438233=%20%20We%20introduce%20Dynamic%20Dropout%2C%20a%20novel%20regularization%20technique%20designed%20to%0Aenhance%20the%20training%20efficiency%20of%20Transformer%20models%20by%20dynamically%20adjusting%0Athe%20dropout%20rate%20based%20on%20training%20epochs%20or%20validation%20loss%20improvements.%20This%0Aapproach%20addresses%20the%20challenge%20of%20balancing%20regularization%20and%20model%0Acapacity%2C%20which%20is%20crucial%20for%20achieving%20fast%20convergence%20and%20high%20performance.%0AOur%20method%20involves%20modifying%20the%20GPT%20model%20to%20accept%20a%20variable%20dropout%20rate%0Aand%20updating%20dropout%20layers%20during%20training%20using%20schedules%20such%20as%20linear%0Adecay%2C%20exponential%20decay%2C%20and%20validation%20loss-based%20adjustments.%20Extensive%0Aexperiments%20on%20the%20Shakespeare%5C_char%20dataset%20demonstrate%20that%20Dynamic%20Dropout%0Asignificantly%20accelerates%20training%20and%20improves%20inference%20efficiency%20compared%0Ato%20a%20baseline%20model%20with%20a%20fixed%20dropout%20rate.%20The%20validation%20loss-based%0Aadjustment%20schedule%20provided%20the%20best%20overall%20performance%2C%20highlighting%20the%0Apotential%20of%20Dynamic%20Dropout%20as%20a%20valuable%20technique%20for%20training%20large-scale%0ATransformer%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03236v1&entry.124074799=Read"},
{"title": "Investigating the Applicability of a Snapshot Computed Tomography\n  Imaging Spectrometer for the Prediction of Brix and pH of Grapes", "author": "Mads Svanborg Peters and Mads Juul Ahleb\u00e6k and Mads Toudal Frandsen and Bjarke J\u00f8rgensen and Christian Hald Jessen and Andreas Krogh Carlsen and Wei-Chih Huang and Ren\u00e9 Lynge Eriksen", "abstract": "  In this paper, a recently developed snapshot hyperspectral imaging (HSI)\nsystem based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to\ndetermine Brix and pH values in Sheegene 20 table grapes through Partial Least\nSquares Regression (PLSR) modeling. The performance of the CTIS system is\ncompared with that of a state-of-the-art line scan HSI system by imaging 100\ngrapes across both platforms. Reference measurements of Brix and pH values are\nobtained directly using a refractometer and a pH meter, as these parameters are\nessential for assessing the quality of table and wine grapes. The findings\nindicate that the spectra captured by the CTIS camera correlate well with the\nreference measurements, despite the system's narrower spectral range. The CTIS\ncamera's advantages, including its lower cost, portability, and reduced\nsusceptibility to motion errors, highlight its potential for promising in-field\napplications in grape quality assessment.\n", "link": "http://arxiv.org/abs/2411.03114v1", "date": "2024-11-05", "relevancy": 2.1844, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4544}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4394}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Applicability%20of%20a%20Snapshot%20Computed%20Tomography%0A%20%20Imaging%20Spectrometer%20for%20the%20Prediction%20of%20Brix%20and%20pH%20of%20Grapes&body=Title%3A%20Investigating%20the%20Applicability%20of%20a%20Snapshot%20Computed%20Tomography%0A%20%20Imaging%20Spectrometer%20for%20the%20Prediction%20of%20Brix%20and%20pH%20of%20Grapes%0AAuthor%3A%20Mads%20Svanborg%20Peters%20and%20Mads%20Juul%20Ahleb%C3%A6k%20and%20Mads%20Toudal%20Frandsen%20and%20Bjarke%20J%C3%B8rgensen%20and%20Christian%20Hald%20Jessen%20and%20Andreas%20Krogh%20Carlsen%20and%20Wei-Chih%20Huang%20and%20Ren%C3%A9%20Lynge%20Eriksen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20recently%20developed%20snapshot%20hyperspectral%20imaging%20%28HSI%29%0Asystem%20based%20on%20Computed%20Tomography%20Imaging%20Spectroscopy%20%28CTIS%29%20is%20utilized%20to%0Adetermine%20Brix%20and%20pH%20values%20in%20Sheegene%2020%20table%20grapes%20through%20Partial%20Least%0ASquares%20Regression%20%28PLSR%29%20modeling.%20The%20performance%20of%20the%20CTIS%20system%20is%0Acompared%20with%20that%20of%20a%20state-of-the-art%20line%20scan%20HSI%20system%20by%20imaging%20100%0Agrapes%20across%20both%20platforms.%20Reference%20measurements%20of%20Brix%20and%20pH%20values%20are%0Aobtained%20directly%20using%20a%20refractometer%20and%20a%20pH%20meter%2C%20as%20these%20parameters%20are%0Aessential%20for%20assessing%20the%20quality%20of%20table%20and%20wine%20grapes.%20The%20findings%0Aindicate%20that%20the%20spectra%20captured%20by%20the%20CTIS%20camera%20correlate%20well%20with%20the%0Areference%20measurements%2C%20despite%20the%20system%27s%20narrower%20spectral%20range.%20The%20CTIS%0Acamera%27s%20advantages%2C%20including%20its%20lower%20cost%2C%20portability%2C%20and%20reduced%0Asusceptibility%20to%20motion%20errors%2C%20highlight%20its%20potential%20for%20promising%20in-field%0Aapplications%20in%20grape%20quality%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Applicability%2520of%2520a%2520Snapshot%2520Computed%2520Tomography%250A%2520%2520Imaging%2520Spectrometer%2520for%2520the%2520Prediction%2520of%2520Brix%2520and%2520pH%2520of%2520Grapes%26entry.906535625%3DMads%2520Svanborg%2520Peters%2520and%2520Mads%2520Juul%2520Ahleb%25C3%25A6k%2520and%2520Mads%2520Toudal%2520Frandsen%2520and%2520Bjarke%2520J%25C3%25B8rgensen%2520and%2520Christian%2520Hald%2520Jessen%2520and%2520Andreas%2520Krogh%2520Carlsen%2520and%2520Wei-Chih%2520Huang%2520and%2520Ren%25C3%25A9%2520Lynge%2520Eriksen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520recently%2520developed%2520snapshot%2520hyperspectral%2520imaging%2520%2528HSI%2529%250Asystem%2520based%2520on%2520Computed%2520Tomography%2520Imaging%2520Spectroscopy%2520%2528CTIS%2529%2520is%2520utilized%2520to%250Adetermine%2520Brix%2520and%2520pH%2520values%2520in%2520Sheegene%252020%2520table%2520grapes%2520through%2520Partial%2520Least%250ASquares%2520Regression%2520%2528PLSR%2529%2520modeling.%2520The%2520performance%2520of%2520the%2520CTIS%2520system%2520is%250Acompared%2520with%2520that%2520of%2520a%2520state-of-the-art%2520line%2520scan%2520HSI%2520system%2520by%2520imaging%2520100%250Agrapes%2520across%2520both%2520platforms.%2520Reference%2520measurements%2520of%2520Brix%2520and%2520pH%2520values%2520are%250Aobtained%2520directly%2520using%2520a%2520refractometer%2520and%2520a%2520pH%2520meter%252C%2520as%2520these%2520parameters%2520are%250Aessential%2520for%2520assessing%2520the%2520quality%2520of%2520table%2520and%2520wine%2520grapes.%2520The%2520findings%250Aindicate%2520that%2520the%2520spectra%2520captured%2520by%2520the%2520CTIS%2520camera%2520correlate%2520well%2520with%2520the%250Areference%2520measurements%252C%2520despite%2520the%2520system%2527s%2520narrower%2520spectral%2520range.%2520The%2520CTIS%250Acamera%2527s%2520advantages%252C%2520including%2520its%2520lower%2520cost%252C%2520portability%252C%2520and%2520reduced%250Asusceptibility%2520to%2520motion%2520errors%252C%2520highlight%2520its%2520potential%2520for%2520promising%2520in-field%250Aapplications%2520in%2520grape%2520quality%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Applicability%20of%20a%20Snapshot%20Computed%20Tomography%0A%20%20Imaging%20Spectrometer%20for%20the%20Prediction%20of%20Brix%20and%20pH%20of%20Grapes&entry.906535625=Mads%20Svanborg%20Peters%20and%20Mads%20Juul%20Ahleb%C3%A6k%20and%20Mads%20Toudal%20Frandsen%20and%20Bjarke%20J%C3%B8rgensen%20and%20Christian%20Hald%20Jessen%20and%20Andreas%20Krogh%20Carlsen%20and%20Wei-Chih%20Huang%20and%20Ren%C3%A9%20Lynge%20Eriksen&entry.1292438233=%20%20In%20this%20paper%2C%20a%20recently%20developed%20snapshot%20hyperspectral%20imaging%20%28HSI%29%0Asystem%20based%20on%20Computed%20Tomography%20Imaging%20Spectroscopy%20%28CTIS%29%20is%20utilized%20to%0Adetermine%20Brix%20and%20pH%20values%20in%20Sheegene%2020%20table%20grapes%20through%20Partial%20Least%0ASquares%20Regression%20%28PLSR%29%20modeling.%20The%20performance%20of%20the%20CTIS%20system%20is%0Acompared%20with%20that%20of%20a%20state-of-the-art%20line%20scan%20HSI%20system%20by%20imaging%20100%0Agrapes%20across%20both%20platforms.%20Reference%20measurements%20of%20Brix%20and%20pH%20values%20are%0Aobtained%20directly%20using%20a%20refractometer%20and%20a%20pH%20meter%2C%20as%20these%20parameters%20are%0Aessential%20for%20assessing%20the%20quality%20of%20table%20and%20wine%20grapes.%20The%20findings%0Aindicate%20that%20the%20spectra%20captured%20by%20the%20CTIS%20camera%20correlate%20well%20with%20the%0Areference%20measurements%2C%20despite%20the%20system%27s%20narrower%20spectral%20range.%20The%20CTIS%0Acamera%27s%20advantages%2C%20including%20its%20lower%20cost%2C%20portability%2C%20and%20reduced%0Asusceptibility%20to%20motion%20errors%2C%20highlight%20its%20potential%20for%20promising%20in-field%0Aapplications%20in%20grape%20quality%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03114v1&entry.124074799=Read"},
{"title": "Efficient Hamiltonian, structure and trace distance learning of Gaussian\n  states", "author": "Marco Fanizza and Cambyse Rouz\u00e9 and Daniel Stilck Fran\u00e7a", "abstract": "  In this work, we initiate the study of Hamiltonian learning for positive\ntemperature bosonic Gaussian states, the quantum generalization of the widely\nstudied problem of learning Gaussian graphical models. We obtain efficient\nprotocols, both in sample and computational complexity, for the task of\ninferring the parameters of their underlying quadratic Hamiltonian under the\nassumption of bounded temperature, squeezing, displacement and maximal degree\nof the interaction graph. Our protocol only requires heterodyne measurements,\nwhich are often experimentally feasible, and has a sample complexity that\nscales logarithmically with the number of modes. Furthermore, we show that it\nis possible to learn the underlying interaction graph in a similar setting and\nsample complexity. Taken together, our results put the status of the quantum\nHamiltonian learning problem for continuous variable systems in a much more\nadvanced state when compared to spins, where state-of-the-art results are\neither unavailable or quantitatively inferior to ours. In addition, we use our\ntechniques to obtain the first results on learning Gaussian states in trace\ndistance with a quadratic scaling in precision and polynomial in the number of\nmodes, albeit imposing certain restrictions on the Gaussian states. Our main\ntechnical innovations are several continuity bounds for the covariance and\nHamiltonian matrix of a Gaussian state, which are of independent interest,\ncombined with what we call the local inversion technique. In essence, the local\ninversion technique allows us to reliably infer the Hamiltonian of a Gaussian\nstate by only estimating in parallel submatrices of the covariance matrix whose\nsize scales with the desired precision, but not the number of modes. This way\nwe bypass the need to obtain precise global estimates of the covariance matrix,\ncontrolling the sample complexity.\n", "link": "http://arxiv.org/abs/2411.03163v1", "date": "2024-11-05", "relevancy": 2.166, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4546}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4227}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states&body=Title%3A%20Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states%0AAuthor%3A%20Marco%20Fanizza%20and%20Cambyse%20Rouz%C3%A9%20and%20Daniel%20Stilck%20Fran%C3%A7a%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20initiate%20the%20study%20of%20Hamiltonian%20learning%20for%20positive%0Atemperature%20bosonic%20Gaussian%20states%2C%20the%20quantum%20generalization%20of%20the%20widely%0Astudied%20problem%20of%20learning%20Gaussian%20graphical%20models.%20We%20obtain%20efficient%0Aprotocols%2C%20both%20in%20sample%20and%20computational%20complexity%2C%20for%20the%20task%20of%0Ainferring%20the%20parameters%20of%20their%20underlying%20quadratic%20Hamiltonian%20under%20the%0Aassumption%20of%20bounded%20temperature%2C%20squeezing%2C%20displacement%20and%20maximal%20degree%0Aof%20the%20interaction%20graph.%20Our%20protocol%20only%20requires%20heterodyne%20measurements%2C%0Awhich%20are%20often%20experimentally%20feasible%2C%20and%20has%20a%20sample%20complexity%20that%0Ascales%20logarithmically%20with%20the%20number%20of%20modes.%20Furthermore%2C%20we%20show%20that%20it%0Ais%20possible%20to%20learn%20the%20underlying%20interaction%20graph%20in%20a%20similar%20setting%20and%0Asample%20complexity.%20Taken%20together%2C%20our%20results%20put%20the%20status%20of%20the%20quantum%0AHamiltonian%20learning%20problem%20for%20continuous%20variable%20systems%20in%20a%20much%20more%0Aadvanced%20state%20when%20compared%20to%20spins%2C%20where%20state-of-the-art%20results%20are%0Aeither%20unavailable%20or%20quantitatively%20inferior%20to%20ours.%20In%20addition%2C%20we%20use%20our%0Atechniques%20to%20obtain%20the%20first%20results%20on%20learning%20Gaussian%20states%20in%20trace%0Adistance%20with%20a%20quadratic%20scaling%20in%20precision%20and%20polynomial%20in%20the%20number%20of%0Amodes%2C%20albeit%20imposing%20certain%20restrictions%20on%20the%20Gaussian%20states.%20Our%20main%0Atechnical%20innovations%20are%20several%20continuity%20bounds%20for%20the%20covariance%20and%0AHamiltonian%20matrix%20of%20a%20Gaussian%20state%2C%20which%20are%20of%20independent%20interest%2C%0Acombined%20with%20what%20we%20call%20the%20local%20inversion%20technique.%20In%20essence%2C%20the%20local%0Ainversion%20technique%20allows%20us%20to%20reliably%20infer%20the%20Hamiltonian%20of%20a%20Gaussian%0Astate%20by%20only%20estimating%20in%20parallel%20submatrices%20of%20the%20covariance%20matrix%20whose%0Asize%20scales%20with%20the%20desired%20precision%2C%20but%20not%20the%20number%20of%20modes.%20This%20way%0Awe%20bypass%20the%20need%20to%20obtain%20precise%20global%20estimates%20of%20the%20covariance%20matrix%2C%0Acontrolling%20the%20sample%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Hamiltonian%252C%2520structure%2520and%2520trace%2520distance%2520learning%2520of%2520Gaussian%250A%2520%2520states%26entry.906535625%3DMarco%2520Fanizza%2520and%2520Cambyse%2520Rouz%25C3%25A9%2520and%2520Daniel%2520Stilck%2520Fran%25C3%25A7a%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520initiate%2520the%2520study%2520of%2520Hamiltonian%2520learning%2520for%2520positive%250Atemperature%2520bosonic%2520Gaussian%2520states%252C%2520the%2520quantum%2520generalization%2520of%2520the%2520widely%250Astudied%2520problem%2520of%2520learning%2520Gaussian%2520graphical%2520models.%2520We%2520obtain%2520efficient%250Aprotocols%252C%2520both%2520in%2520sample%2520and%2520computational%2520complexity%252C%2520for%2520the%2520task%2520of%250Ainferring%2520the%2520parameters%2520of%2520their%2520underlying%2520quadratic%2520Hamiltonian%2520under%2520the%250Aassumption%2520of%2520bounded%2520temperature%252C%2520squeezing%252C%2520displacement%2520and%2520maximal%2520degree%250Aof%2520the%2520interaction%2520graph.%2520Our%2520protocol%2520only%2520requires%2520heterodyne%2520measurements%252C%250Awhich%2520are%2520often%2520experimentally%2520feasible%252C%2520and%2520has%2520a%2520sample%2520complexity%2520that%250Ascales%2520logarithmically%2520with%2520the%2520number%2520of%2520modes.%2520Furthermore%252C%2520we%2520show%2520that%2520it%250Ais%2520possible%2520to%2520learn%2520the%2520underlying%2520interaction%2520graph%2520in%2520a%2520similar%2520setting%2520and%250Asample%2520complexity.%2520Taken%2520together%252C%2520our%2520results%2520put%2520the%2520status%2520of%2520the%2520quantum%250AHamiltonian%2520learning%2520problem%2520for%2520continuous%2520variable%2520systems%2520in%2520a%2520much%2520more%250Aadvanced%2520state%2520when%2520compared%2520to%2520spins%252C%2520where%2520state-of-the-art%2520results%2520are%250Aeither%2520unavailable%2520or%2520quantitatively%2520inferior%2520to%2520ours.%2520In%2520addition%252C%2520we%2520use%2520our%250Atechniques%2520to%2520obtain%2520the%2520first%2520results%2520on%2520learning%2520Gaussian%2520states%2520in%2520trace%250Adistance%2520with%2520a%2520quadratic%2520scaling%2520in%2520precision%2520and%2520polynomial%2520in%2520the%2520number%2520of%250Amodes%252C%2520albeit%2520imposing%2520certain%2520restrictions%2520on%2520the%2520Gaussian%2520states.%2520Our%2520main%250Atechnical%2520innovations%2520are%2520several%2520continuity%2520bounds%2520for%2520the%2520covariance%2520and%250AHamiltonian%2520matrix%2520of%2520a%2520Gaussian%2520state%252C%2520which%2520are%2520of%2520independent%2520interest%252C%250Acombined%2520with%2520what%2520we%2520call%2520the%2520local%2520inversion%2520technique.%2520In%2520essence%252C%2520the%2520local%250Ainversion%2520technique%2520allows%2520us%2520to%2520reliably%2520infer%2520the%2520Hamiltonian%2520of%2520a%2520Gaussian%250Astate%2520by%2520only%2520estimating%2520in%2520parallel%2520submatrices%2520of%2520the%2520covariance%2520matrix%2520whose%250Asize%2520scales%2520with%2520the%2520desired%2520precision%252C%2520but%2520not%2520the%2520number%2520of%2520modes.%2520This%2520way%250Awe%2520bypass%2520the%2520need%2520to%2520obtain%2520precise%2520global%2520estimates%2520of%2520the%2520covariance%2520matrix%252C%250Acontrolling%2520the%2520sample%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states&entry.906535625=Marco%20Fanizza%20and%20Cambyse%20Rouz%C3%A9%20and%20Daniel%20Stilck%20Fran%C3%A7a&entry.1292438233=%20%20In%20this%20work%2C%20we%20initiate%20the%20study%20of%20Hamiltonian%20learning%20for%20positive%0Atemperature%20bosonic%20Gaussian%20states%2C%20the%20quantum%20generalization%20of%20the%20widely%0Astudied%20problem%20of%20learning%20Gaussian%20graphical%20models.%20We%20obtain%20efficient%0Aprotocols%2C%20both%20in%20sample%20and%20computational%20complexity%2C%20for%20the%20task%20of%0Ainferring%20the%20parameters%20of%20their%20underlying%20quadratic%20Hamiltonian%20under%20the%0Aassumption%20of%20bounded%20temperature%2C%20squeezing%2C%20displacement%20and%20maximal%20degree%0Aof%20the%20interaction%20graph.%20Our%20protocol%20only%20requires%20heterodyne%20measurements%2C%0Awhich%20are%20often%20experimentally%20feasible%2C%20and%20has%20a%20sample%20complexity%20that%0Ascales%20logarithmically%20with%20the%20number%20of%20modes.%20Furthermore%2C%20we%20show%20that%20it%0Ais%20possible%20to%20learn%20the%20underlying%20interaction%20graph%20in%20a%20similar%20setting%20and%0Asample%20complexity.%20Taken%20together%2C%20our%20results%20put%20the%20status%20of%20the%20quantum%0AHamiltonian%20learning%20problem%20for%20continuous%20variable%20systems%20in%20a%20much%20more%0Aadvanced%20state%20when%20compared%20to%20spins%2C%20where%20state-of-the-art%20results%20are%0Aeither%20unavailable%20or%20quantitatively%20inferior%20to%20ours.%20In%20addition%2C%20we%20use%20our%0Atechniques%20to%20obtain%20the%20first%20results%20on%20learning%20Gaussian%20states%20in%20trace%0Adistance%20with%20a%20quadratic%20scaling%20in%20precision%20and%20polynomial%20in%20the%20number%20of%0Amodes%2C%20albeit%20imposing%20certain%20restrictions%20on%20the%20Gaussian%20states.%20Our%20main%0Atechnical%20innovations%20are%20several%20continuity%20bounds%20for%20the%20covariance%20and%0AHamiltonian%20matrix%20of%20a%20Gaussian%20state%2C%20which%20are%20of%20independent%20interest%2C%0Acombined%20with%20what%20we%20call%20the%20local%20inversion%20technique.%20In%20essence%2C%20the%20local%0Ainversion%20technique%20allows%20us%20to%20reliably%20infer%20the%20Hamiltonian%20of%20a%20Gaussian%0Astate%20by%20only%20estimating%20in%20parallel%20submatrices%20of%20the%20covariance%20matrix%20whose%0Asize%20scales%20with%20the%20desired%20precision%2C%20but%20not%20the%20number%20of%20modes.%20This%20way%0Awe%20bypass%20the%20need%20to%20obtain%20precise%20global%20estimates%20of%20the%20covariance%20matrix%2C%0Acontrolling%20the%20sample%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03163v1&entry.124074799=Read"},
{"title": "Navigating Extremes: Dynamic Sparsity in Large Output Space", "author": "Nasib Ullah and Erik Schultheis and Mike Lasby and Yani Ioannou and Rohit Babbar", "abstract": "  In recent years, Dynamic Sparse Training (DST) has emerged as an alternative\nto post-training pruning for generating efficient models. In principle, DST\nallows for a more memory efficient training process, as it maintains sparsity\nthroughout the entire training run. However, current DST implementations fail\nto capitalize on this in practice. Because sparse matrix multiplication is much\nless efficient than dense matrix multiplication on GPUs, most implementations\nsimulate sparsity by masking weights. In this paper, we leverage recent\nadvances in semi-structured sparse training to apply DST in the domain of\nclassification with large output spaces, where memory-efficiency is paramount.\nWith a label space of possibly millions of candidates, the classification layer\nalone will consume several gigabytes of memory. Switching from a dense to a\nfixed fan-in sparse layer updated with sparse evolutionary training (SET);\nhowever, severely hampers training convergence, especially at the largest label\nspaces. We find that poor gradient flow from the sparse classifier to the dense\ntext encoder make it difficult to learn good input representations. By\nemploying an intermediate layer or adding an auxiliary training objective, we\nrecover most of the generalisation performance of the dense model. Overall, we\ndemonstrate the applicability and practical benefits of DST in a challenging\ndomain -- characterized by a highly skewed label distribution that differs\nsubstantially from typical DST benchmark datasets -- which enables end-to-end\ntraining with millions of labels on commodity hardware.\n", "link": "http://arxiv.org/abs/2411.03171v1", "date": "2024-11-05", "relevancy": 2.1656, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5461}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5451}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space&body=Title%3A%20Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space%0AAuthor%3A%20Nasib%20Ullah%20and%20Erik%20Schultheis%20and%20Mike%20Lasby%20and%20Yani%20Ioannou%20and%20Rohit%20Babbar%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Dynamic%20Sparse%20Training%20%28DST%29%20has%20emerged%20as%20an%20alternative%0Ato%20post-training%20pruning%20for%20generating%20efficient%20models.%20In%20principle%2C%20DST%0Aallows%20for%20a%20more%20memory%20efficient%20training%20process%2C%20as%20it%20maintains%20sparsity%0Athroughout%20the%20entire%20training%20run.%20However%2C%20current%20DST%20implementations%20fail%0Ato%20capitalize%20on%20this%20in%20practice.%20Because%20sparse%20matrix%20multiplication%20is%20much%0Aless%20efficient%20than%20dense%20matrix%20multiplication%20on%20GPUs%2C%20most%20implementations%0Asimulate%20sparsity%20by%20masking%20weights.%20In%20this%20paper%2C%20we%20leverage%20recent%0Aadvances%20in%20semi-structured%20sparse%20training%20to%20apply%20DST%20in%20the%20domain%20of%0Aclassification%20with%20large%20output%20spaces%2C%20where%20memory-efficiency%20is%20paramount.%0AWith%20a%20label%20space%20of%20possibly%20millions%20of%20candidates%2C%20the%20classification%20layer%0Aalone%20will%20consume%20several%20gigabytes%20of%20memory.%20Switching%20from%20a%20dense%20to%20a%0Afixed%20fan-in%20sparse%20layer%20updated%20with%20sparse%20evolutionary%20training%20%28SET%29%3B%0Ahowever%2C%20severely%20hampers%20training%20convergence%2C%20especially%20at%20the%20largest%20label%0Aspaces.%20We%20find%20that%20poor%20gradient%20flow%20from%20the%20sparse%20classifier%20to%20the%20dense%0Atext%20encoder%20make%20it%20difficult%20to%20learn%20good%20input%20representations.%20By%0Aemploying%20an%20intermediate%20layer%20or%20adding%20an%20auxiliary%20training%20objective%2C%20we%0Arecover%20most%20of%20the%20generalisation%20performance%20of%20the%20dense%20model.%20Overall%2C%20we%0Ademonstrate%20the%20applicability%20and%20practical%20benefits%20of%20DST%20in%20a%20challenging%0Adomain%20--%20characterized%20by%20a%20highly%20skewed%20label%20distribution%20that%20differs%0Asubstantially%20from%20typical%20DST%20benchmark%20datasets%20--%20which%20enables%20end-to-end%0Atraining%20with%20millions%20of%20labels%20on%20commodity%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Extremes%253A%2520Dynamic%2520Sparsity%2520in%2520Large%2520Output%2520Space%26entry.906535625%3DNasib%2520Ullah%2520and%2520Erik%2520Schultheis%2520and%2520Mike%2520Lasby%2520and%2520Yani%2520Ioannou%2520and%2520Rohit%2520Babbar%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Dynamic%2520Sparse%2520Training%2520%2528DST%2529%2520has%2520emerged%2520as%2520an%2520alternative%250Ato%2520post-training%2520pruning%2520for%2520generating%2520efficient%2520models.%2520In%2520principle%252C%2520DST%250Aallows%2520for%2520a%2520more%2520memory%2520efficient%2520training%2520process%252C%2520as%2520it%2520maintains%2520sparsity%250Athroughout%2520the%2520entire%2520training%2520run.%2520However%252C%2520current%2520DST%2520implementations%2520fail%250Ato%2520capitalize%2520on%2520this%2520in%2520practice.%2520Because%2520sparse%2520matrix%2520multiplication%2520is%2520much%250Aless%2520efficient%2520than%2520dense%2520matrix%2520multiplication%2520on%2520GPUs%252C%2520most%2520implementations%250Asimulate%2520sparsity%2520by%2520masking%2520weights.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520recent%250Aadvances%2520in%2520semi-structured%2520sparse%2520training%2520to%2520apply%2520DST%2520in%2520the%2520domain%2520of%250Aclassification%2520with%2520large%2520output%2520spaces%252C%2520where%2520memory-efficiency%2520is%2520paramount.%250AWith%2520a%2520label%2520space%2520of%2520possibly%2520millions%2520of%2520candidates%252C%2520the%2520classification%2520layer%250Aalone%2520will%2520consume%2520several%2520gigabytes%2520of%2520memory.%2520Switching%2520from%2520a%2520dense%2520to%2520a%250Afixed%2520fan-in%2520sparse%2520layer%2520updated%2520with%2520sparse%2520evolutionary%2520training%2520%2528SET%2529%253B%250Ahowever%252C%2520severely%2520hampers%2520training%2520convergence%252C%2520especially%2520at%2520the%2520largest%2520label%250Aspaces.%2520We%2520find%2520that%2520poor%2520gradient%2520flow%2520from%2520the%2520sparse%2520classifier%2520to%2520the%2520dense%250Atext%2520encoder%2520make%2520it%2520difficult%2520to%2520learn%2520good%2520input%2520representations.%2520By%250Aemploying%2520an%2520intermediate%2520layer%2520or%2520adding%2520an%2520auxiliary%2520training%2520objective%252C%2520we%250Arecover%2520most%2520of%2520the%2520generalisation%2520performance%2520of%2520the%2520dense%2520model.%2520Overall%252C%2520we%250Ademonstrate%2520the%2520applicability%2520and%2520practical%2520benefits%2520of%2520DST%2520in%2520a%2520challenging%250Adomain%2520--%2520characterized%2520by%2520a%2520highly%2520skewed%2520label%2520distribution%2520that%2520differs%250Asubstantially%2520from%2520typical%2520DST%2520benchmark%2520datasets%2520--%2520which%2520enables%2520end-to-end%250Atraining%2520with%2520millions%2520of%2520labels%2520on%2520commodity%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space&entry.906535625=Nasib%20Ullah%20and%20Erik%20Schultheis%20and%20Mike%20Lasby%20and%20Yani%20Ioannou%20and%20Rohit%20Babbar&entry.1292438233=%20%20In%20recent%20years%2C%20Dynamic%20Sparse%20Training%20%28DST%29%20has%20emerged%20as%20an%20alternative%0Ato%20post-training%20pruning%20for%20generating%20efficient%20models.%20In%20principle%2C%20DST%0Aallows%20for%20a%20more%20memory%20efficient%20training%20process%2C%20as%20it%20maintains%20sparsity%0Athroughout%20the%20entire%20training%20run.%20However%2C%20current%20DST%20implementations%20fail%0Ato%20capitalize%20on%20this%20in%20practice.%20Because%20sparse%20matrix%20multiplication%20is%20much%0Aless%20efficient%20than%20dense%20matrix%20multiplication%20on%20GPUs%2C%20most%20implementations%0Asimulate%20sparsity%20by%20masking%20weights.%20In%20this%20paper%2C%20we%20leverage%20recent%0Aadvances%20in%20semi-structured%20sparse%20training%20to%20apply%20DST%20in%20the%20domain%20of%0Aclassification%20with%20large%20output%20spaces%2C%20where%20memory-efficiency%20is%20paramount.%0AWith%20a%20label%20space%20of%20possibly%20millions%20of%20candidates%2C%20the%20classification%20layer%0Aalone%20will%20consume%20several%20gigabytes%20of%20memory.%20Switching%20from%20a%20dense%20to%20a%0Afixed%20fan-in%20sparse%20layer%20updated%20with%20sparse%20evolutionary%20training%20%28SET%29%3B%0Ahowever%2C%20severely%20hampers%20training%20convergence%2C%20especially%20at%20the%20largest%20label%0Aspaces.%20We%20find%20that%20poor%20gradient%20flow%20from%20the%20sparse%20classifier%20to%20the%20dense%0Atext%20encoder%20make%20it%20difficult%20to%20learn%20good%20input%20representations.%20By%0Aemploying%20an%20intermediate%20layer%20or%20adding%20an%20auxiliary%20training%20objective%2C%20we%0Arecover%20most%20of%20the%20generalisation%20performance%20of%20the%20dense%20model.%20Overall%2C%20we%0Ademonstrate%20the%20applicability%20and%20practical%20benefits%20of%20DST%20in%20a%20challenging%0Adomain%20--%20characterized%20by%20a%20highly%20skewed%20label%20distribution%20that%20differs%0Asubstantially%20from%20typical%20DST%20benchmark%20datasets%20--%20which%20enables%20end-to-end%0Atraining%20with%20millions%20of%20labels%20on%20commodity%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03171v1&entry.124074799=Read"},
{"title": "VERITAS: A Unified Approach to Reliability Evaluation", "author": "Rajkumar Ramamurthy and Meghana Arakkal Rajeev and Oliver Molenschot and James Zou and Nazneen Rajani", "abstract": "  Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting.\n", "link": "http://arxiv.org/abs/2411.03300v1", "date": "2024-11-05", "relevancy": 2.1622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERITAS%3A%20A%20Unified%20Approach%20to%20Reliability%20Evaluation&body=Title%3A%20VERITAS%3A%20A%20Unified%20Approach%20to%20Reliability%20Evaluation%0AAuthor%3A%20Rajkumar%20Ramamurthy%20and%20Meghana%20Arakkal%20Rajeev%20and%20Oliver%20Molenschot%20and%20James%20Zou%20and%20Nazneen%20Rajani%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20synthesize%20information%20from%20their%0Acontext%20to%20generate%20an%20accurate%20response.%20This%20renders%20them%20unreliable%20in%0Aknowledge%20intensive%20settings%20where%20reliability%20of%20the%20output%20is%20key.%20A%20critical%0Acomponent%20for%20reliable%20LLMs%20is%20the%20integration%20of%20a%20robust%20fact-checking%20system%0Athat%20can%20detect%20hallucinations%20across%20various%20formats.%20While%20several%0Aopen-access%20fact-checking%20models%20are%20available%2C%20their%20functionality%20is%20often%0Alimited%20to%20specific%20tasks%2C%20such%20as%20grounded%20question-answering%20or%20entailment%0Averification%2C%20and%20they%20perform%20less%20effectively%20in%20conversational%20settings.%20On%0Athe%20other%20hand%2C%20closed-access%20models%20like%20GPT-4%20and%20Claude%20offer%20greater%0Aflexibility%20across%20different%20contexts%2C%20including%20grounded%20dialogue%0Averification%2C%20but%20are%20hindered%20by%20high%20costs%20and%20latency.%20In%20this%20work%2C%20we%0Aintroduce%20VERITAS%2C%20a%20family%20of%20hallucination%20detection%20models%20designed%20to%0Aoperate%20flexibly%20across%20diverse%20contexts%20while%20minimizing%20latency%20and%20costs.%0AVERITAS%20achieves%20state-of-the-art%20results%20considering%20average%20performance%20on%0Aall%20major%20hallucination%20detection%20benchmarks%2C%20with%20%2410%5C%25%24%20increase%20in%20average%0Aperformance%20when%20compared%20to%20similar-sized%20models%20and%20get%20close%20to%20the%0Aperformance%20of%20GPT4%20turbo%20with%20LLM-as-a-judge%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERITAS%253A%2520A%2520Unified%2520Approach%2520to%2520Reliability%2520Evaluation%26entry.906535625%3DRajkumar%2520Ramamurthy%2520and%2520Meghana%2520Arakkal%2520Rajeev%2520and%2520Oliver%2520Molenschot%2520and%2520James%2520Zou%2520and%2520Nazneen%2520Rajani%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520fail%2520to%2520synthesize%2520information%2520from%2520their%250Acontext%2520to%2520generate%2520an%2520accurate%2520response.%2520This%2520renders%2520them%2520unreliable%2520in%250Aknowledge%2520intensive%2520settings%2520where%2520reliability%2520of%2520the%2520output%2520is%2520key.%2520A%2520critical%250Acomponent%2520for%2520reliable%2520LLMs%2520is%2520the%2520integration%2520of%2520a%2520robust%2520fact-checking%2520system%250Athat%2520can%2520detect%2520hallucinations%2520across%2520various%2520formats.%2520While%2520several%250Aopen-access%2520fact-checking%2520models%2520are%2520available%252C%2520their%2520functionality%2520is%2520often%250Alimited%2520to%2520specific%2520tasks%252C%2520such%2520as%2520grounded%2520question-answering%2520or%2520entailment%250Averification%252C%2520and%2520they%2520perform%2520less%2520effectively%2520in%2520conversational%2520settings.%2520On%250Athe%2520other%2520hand%252C%2520closed-access%2520models%2520like%2520GPT-4%2520and%2520Claude%2520offer%2520greater%250Aflexibility%2520across%2520different%2520contexts%252C%2520including%2520grounded%2520dialogue%250Averification%252C%2520but%2520are%2520hindered%2520by%2520high%2520costs%2520and%2520latency.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520VERITAS%252C%2520a%2520family%2520of%2520hallucination%2520detection%2520models%2520designed%2520to%250Aoperate%2520flexibly%2520across%2520diverse%2520contexts%2520while%2520minimizing%2520latency%2520and%2520costs.%250AVERITAS%2520achieves%2520state-of-the-art%2520results%2520considering%2520average%2520performance%2520on%250Aall%2520major%2520hallucination%2520detection%2520benchmarks%252C%2520with%2520%252410%255C%2525%2524%2520increase%2520in%2520average%250Aperformance%2520when%2520compared%2520to%2520similar-sized%2520models%2520and%2520get%2520close%2520to%2520the%250Aperformance%2520of%2520GPT4%2520turbo%2520with%2520LLM-as-a-judge%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERITAS%3A%20A%20Unified%20Approach%20to%20Reliability%20Evaluation&entry.906535625=Rajkumar%20Ramamurthy%20and%20Meghana%20Arakkal%20Rajeev%20and%20Oliver%20Molenschot%20and%20James%20Zou%20and%20Nazneen%20Rajani&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20synthesize%20information%20from%20their%0Acontext%20to%20generate%20an%20accurate%20response.%20This%20renders%20them%20unreliable%20in%0Aknowledge%20intensive%20settings%20where%20reliability%20of%20the%20output%20is%20key.%20A%20critical%0Acomponent%20for%20reliable%20LLMs%20is%20the%20integration%20of%20a%20robust%20fact-checking%20system%0Athat%20can%20detect%20hallucinations%20across%20various%20formats.%20While%20several%0Aopen-access%20fact-checking%20models%20are%20available%2C%20their%20functionality%20is%20often%0Alimited%20to%20specific%20tasks%2C%20such%20as%20grounded%20question-answering%20or%20entailment%0Averification%2C%20and%20they%20perform%20less%20effectively%20in%20conversational%20settings.%20On%0Athe%20other%20hand%2C%20closed-access%20models%20like%20GPT-4%20and%20Claude%20offer%20greater%0Aflexibility%20across%20different%20contexts%2C%20including%20grounded%20dialogue%0Averification%2C%20but%20are%20hindered%20by%20high%20costs%20and%20latency.%20In%20this%20work%2C%20we%0Aintroduce%20VERITAS%2C%20a%20family%20of%20hallucination%20detection%20models%20designed%20to%0Aoperate%20flexibly%20across%20diverse%20contexts%20while%20minimizing%20latency%20and%20costs.%0AVERITAS%20achieves%20state-of-the-art%20results%20considering%20average%20performance%20on%0Aall%20major%20hallucination%20detection%20benchmarks%2C%20with%20%2410%5C%25%24%20increase%20in%20average%0Aperformance%20when%20compared%20to%20similar-sized%20models%20and%20get%20close%20to%20the%0Aperformance%20of%20GPT4%20turbo%20with%20LLM-as-a-judge%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03300v1&entry.124074799=Read"},
{"title": "Seeing Eye to AI: Comparing Human Gaze and Model Attention in Video\n  Memorability", "author": "Prajneya Kumar and Eshika Khandelwal and Makarand Tapaswi and Vishnu Sreekumar", "abstract": "  Understanding what makes a video memorable has important applications in\nadvertising or education technology. Towards this goal, we investigate\nspatio-temporal attention mechanisms underlying video memorability. Different\nfrom previous works that fuse multiple features, we adopt a simple\nCNN+Transformer architecture that enables analysis of spatio-temporal attention\nwhile matching state-of-the-art (SoTA) performance on video memorability\nprediction. We compare model attention against human gaze fixations collected\nthrough a small-scale eye-tracking study where humans perform the video memory\ntask. We uncover the following insights: (i) Quantitative saliency metrics show\nthat our model, trained only to predict a memorability score, exhibits similar\nspatial attention patterns to human gaze, especially for more memorable videos.\n(ii) The model assigns greater importance to initial frames in a video,\nmimicking human attention patterns. (iii) Panoptic segmentation reveals that\nboth (model and humans) assign a greater share of attention to things and less\nattention to stuff as compared to their occurrence probability.\n", "link": "http://arxiv.org/abs/2311.16484v2", "date": "2024-11-05", "relevancy": 2.146, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Eye%20to%20AI%3A%20Comparing%20Human%20Gaze%20and%20Model%20Attention%20in%20Video%0A%20%20Memorability&body=Title%3A%20Seeing%20Eye%20to%20AI%3A%20Comparing%20Human%20Gaze%20and%20Model%20Attention%20in%20Video%0A%20%20Memorability%0AAuthor%3A%20Prajneya%20Kumar%20and%20Eshika%20Khandelwal%20and%20Makarand%20Tapaswi%20and%20Vishnu%20Sreekumar%0AAbstract%3A%20%20%20Understanding%20what%20makes%20a%20video%20memorable%20has%20important%20applications%20in%0Aadvertising%20or%20education%20technology.%20Towards%20this%20goal%2C%20we%20investigate%0Aspatio-temporal%20attention%20mechanisms%20underlying%20video%20memorability.%20Different%0Afrom%20previous%20works%20that%20fuse%20multiple%20features%2C%20we%20adopt%20a%20simple%0ACNN%2BTransformer%20architecture%20that%20enables%20analysis%20of%20spatio-temporal%20attention%0Awhile%20matching%20state-of-the-art%20%28SoTA%29%20performance%20on%20video%20memorability%0Aprediction.%20We%20compare%20model%20attention%20against%20human%20gaze%20fixations%20collected%0Athrough%20a%20small-scale%20eye-tracking%20study%20where%20humans%20perform%20the%20video%20memory%0Atask.%20We%20uncover%20the%20following%20insights%3A%20%28i%29%20Quantitative%20saliency%20metrics%20show%0Athat%20our%20model%2C%20trained%20only%20to%20predict%20a%20memorability%20score%2C%20exhibits%20similar%0Aspatial%20attention%20patterns%20to%20human%20gaze%2C%20especially%20for%20more%20memorable%20videos.%0A%28ii%29%20The%20model%20assigns%20greater%20importance%20to%20initial%20frames%20in%20a%20video%2C%0Amimicking%20human%20attention%20patterns.%20%28iii%29%20Panoptic%20segmentation%20reveals%20that%0Aboth%20%28model%20and%20humans%29%20assign%20a%20greater%20share%20of%20attention%20to%20things%20and%20less%0Aattention%20to%20stuff%20as%20compared%20to%20their%20occurrence%20probability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Eye%2520to%2520AI%253A%2520Comparing%2520Human%2520Gaze%2520and%2520Model%2520Attention%2520in%2520Video%250A%2520%2520Memorability%26entry.906535625%3DPrajneya%2520Kumar%2520and%2520Eshika%2520Khandelwal%2520and%2520Makarand%2520Tapaswi%2520and%2520Vishnu%2520Sreekumar%26entry.1292438233%3D%2520%2520Understanding%2520what%2520makes%2520a%2520video%2520memorable%2520has%2520important%2520applications%2520in%250Aadvertising%2520or%2520education%2520technology.%2520Towards%2520this%2520goal%252C%2520we%2520investigate%250Aspatio-temporal%2520attention%2520mechanisms%2520underlying%2520video%2520memorability.%2520Different%250Afrom%2520previous%2520works%2520that%2520fuse%2520multiple%2520features%252C%2520we%2520adopt%2520a%2520simple%250ACNN%252BTransformer%2520architecture%2520that%2520enables%2520analysis%2520of%2520spatio-temporal%2520attention%250Awhile%2520matching%2520state-of-the-art%2520%2528SoTA%2529%2520performance%2520on%2520video%2520memorability%250Aprediction.%2520We%2520compare%2520model%2520attention%2520against%2520human%2520gaze%2520fixations%2520collected%250Athrough%2520a%2520small-scale%2520eye-tracking%2520study%2520where%2520humans%2520perform%2520the%2520video%2520memory%250Atask.%2520We%2520uncover%2520the%2520following%2520insights%253A%2520%2528i%2529%2520Quantitative%2520saliency%2520metrics%2520show%250Athat%2520our%2520model%252C%2520trained%2520only%2520to%2520predict%2520a%2520memorability%2520score%252C%2520exhibits%2520similar%250Aspatial%2520attention%2520patterns%2520to%2520human%2520gaze%252C%2520especially%2520for%2520more%2520memorable%2520videos.%250A%2528ii%2529%2520The%2520model%2520assigns%2520greater%2520importance%2520to%2520initial%2520frames%2520in%2520a%2520video%252C%250Amimicking%2520human%2520attention%2520patterns.%2520%2528iii%2529%2520Panoptic%2520segmentation%2520reveals%2520that%250Aboth%2520%2528model%2520and%2520humans%2529%2520assign%2520a%2520greater%2520share%2520of%2520attention%2520to%2520things%2520and%2520less%250Aattention%2520to%2520stuff%2520as%2520compared%2520to%2520their%2520occurrence%2520probability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Eye%20to%20AI%3A%20Comparing%20Human%20Gaze%20and%20Model%20Attention%20in%20Video%0A%20%20Memorability&entry.906535625=Prajneya%20Kumar%20and%20Eshika%20Khandelwal%20and%20Makarand%20Tapaswi%20and%20Vishnu%20Sreekumar&entry.1292438233=%20%20Understanding%20what%20makes%20a%20video%20memorable%20has%20important%20applications%20in%0Aadvertising%20or%20education%20technology.%20Towards%20this%20goal%2C%20we%20investigate%0Aspatio-temporal%20attention%20mechanisms%20underlying%20video%20memorability.%20Different%0Afrom%20previous%20works%20that%20fuse%20multiple%20features%2C%20we%20adopt%20a%20simple%0ACNN%2BTransformer%20architecture%20that%20enables%20analysis%20of%20spatio-temporal%20attention%0Awhile%20matching%20state-of-the-art%20%28SoTA%29%20performance%20on%20video%20memorability%0Aprediction.%20We%20compare%20model%20attention%20against%20human%20gaze%20fixations%20collected%0Athrough%20a%20small-scale%20eye-tracking%20study%20where%20humans%20perform%20the%20video%20memory%0Atask.%20We%20uncover%20the%20following%20insights%3A%20%28i%29%20Quantitative%20saliency%20metrics%20show%0Athat%20our%20model%2C%20trained%20only%20to%20predict%20a%20memorability%20score%2C%20exhibits%20similar%0Aspatial%20attention%20patterns%20to%20human%20gaze%2C%20especially%20for%20more%20memorable%20videos.%0A%28ii%29%20The%20model%20assigns%20greater%20importance%20to%20initial%20frames%20in%20a%20video%2C%0Amimicking%20human%20attention%20patterns.%20%28iii%29%20Panoptic%20segmentation%20reveals%20that%0Aboth%20%28model%20and%20humans%29%20assign%20a%20greater%20share%20of%20attention%20to%20things%20and%20less%0Aattention%20to%20stuff%20as%20compared%20to%20their%20occurrence%20probability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16484v2&entry.124074799=Read"},
{"title": "FakeShield: Explainable Image Forgery Detection and Localization via\n  Multi-modal Large Language Models", "author": "Zhipei Xu and Xuanyu Zhang and Runyi Li and Zecheng Tang and Qing Huang and Jian Zhang", "abstract": "  The rapid development of generative AI is a double-edged sword, which not\nonly facilitates content creation but also makes image manipulation easier and\nmore difficult to detect. Although current image forgery detection and\nlocalization (IFDL) methods are generally effective, they tend to face two\nchallenges: \\textbf{1)} black-box nature with unknown detection principle,\n\\textbf{2)} limited generalization across diverse tampering methods (e.g.,\nPhotoshop, DeepFake, AIGC-Editing). To address these issues, we propose the\nexplainable IFDL task and design FakeShield, a multi-modal framework capable of\nevaluating image authenticity, generating tampered region masks, and providing\na judgment basis based on pixel-level and image-level tampering clues.\nAdditionally, we leverage GPT-4o to enhance existing IFDL datasets, creating\nthe Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's\ntampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided\nExplainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery\nLocalization Module (MFLM) to address various types of tamper detection\ninterpretation and achieve forgery localization guided by detailed textual\ndescriptions. Extensive experiments demonstrate that FakeShield effectively\ndetects and localizes various tampering techniques, offering an explainable and\nsuperior solution compared to previous IFDL methods.\n", "link": "http://arxiv.org/abs/2410.02761v3", "date": "2024-11-05", "relevancy": 2.1412, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeShield%3A%20Explainable%20Image%20Forgery%20Detection%20and%20Localization%20via%0A%20%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20FakeShield%3A%20Explainable%20Image%20Forgery%20Detection%20and%20Localization%20via%0A%20%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Zhipei%20Xu%20and%20Xuanyu%20Zhang%20and%20Runyi%20Li%20and%20Zecheng%20Tang%20and%20Qing%20Huang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20generative%20AI%20is%20a%20double-edged%20sword%2C%20which%20not%0Aonly%20facilitates%20content%20creation%20but%20also%20makes%20image%20manipulation%20easier%20and%0Amore%20difficult%20to%20detect.%20Although%20current%20image%20forgery%20detection%20and%0Alocalization%20%28IFDL%29%20methods%20are%20generally%20effective%2C%20they%20tend%20to%20face%20two%0Achallenges%3A%20%5Ctextbf%7B1%29%7D%20black-box%20nature%20with%20unknown%20detection%20principle%2C%0A%5Ctextbf%7B2%29%7D%20limited%20generalization%20across%20diverse%20tampering%20methods%20%28e.g.%2C%0APhotoshop%2C%20DeepFake%2C%20AIGC-Editing%29.%20To%20address%20these%20issues%2C%20we%20propose%20the%0Aexplainable%20IFDL%20task%20and%20design%20FakeShield%2C%20a%20multi-modal%20framework%20capable%20of%0Aevaluating%20image%20authenticity%2C%20generating%20tampered%20region%20masks%2C%20and%20providing%0Aa%20judgment%20basis%20based%20on%20pixel-level%20and%20image-level%20tampering%20clues.%0AAdditionally%2C%20we%20leverage%20GPT-4o%20to%20enhance%20existing%20IFDL%20datasets%2C%20creating%0Athe%20Multi-Modal%20Tamper%20Description%20dataSet%20%28MMTD-Set%29%20for%20training%20FakeShield%27s%0Atampering%20analysis%20capabilities.%20Meanwhile%2C%20we%20incorporate%20a%20Domain%20Tag-guided%0AExplainable%20Forgery%20Detection%20Module%20%28DTE-FDM%29%20and%20a%20Multi-modal%20Forgery%0ALocalization%20Module%20%28MFLM%29%20to%20address%20various%20types%20of%20tamper%20detection%0Ainterpretation%20and%20achieve%20forgery%20localization%20guided%20by%20detailed%20textual%0Adescriptions.%20Extensive%20experiments%20demonstrate%20that%20FakeShield%20effectively%0Adetects%20and%20localizes%20various%20tampering%20techniques%2C%20offering%20an%20explainable%20and%0Asuperior%20solution%20compared%20to%20previous%20IFDL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeShield%253A%2520Explainable%2520Image%2520Forgery%2520Detection%2520and%2520Localization%2520via%250A%2520%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DZhipei%2520Xu%2520and%2520Xuanyu%2520Zhang%2520and%2520Runyi%2520Li%2520and%2520Zecheng%2520Tang%2520and%2520Qing%2520Huang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520generative%2520AI%2520is%2520a%2520double-edged%2520sword%252C%2520which%2520not%250Aonly%2520facilitates%2520content%2520creation%2520but%2520also%2520makes%2520image%2520manipulation%2520easier%2520and%250Amore%2520difficult%2520to%2520detect.%2520Although%2520current%2520image%2520forgery%2520detection%2520and%250Alocalization%2520%2528IFDL%2529%2520methods%2520are%2520generally%2520effective%252C%2520they%2520tend%2520to%2520face%2520two%250Achallenges%253A%2520%255Ctextbf%257B1%2529%257D%2520black-box%2520nature%2520with%2520unknown%2520detection%2520principle%252C%250A%255Ctextbf%257B2%2529%257D%2520limited%2520generalization%2520across%2520diverse%2520tampering%2520methods%2520%2528e.g.%252C%250APhotoshop%252C%2520DeepFake%252C%2520AIGC-Editing%2529.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%250Aexplainable%2520IFDL%2520task%2520and%2520design%2520FakeShield%252C%2520a%2520multi-modal%2520framework%2520capable%2520of%250Aevaluating%2520image%2520authenticity%252C%2520generating%2520tampered%2520region%2520masks%252C%2520and%2520providing%250Aa%2520judgment%2520basis%2520based%2520on%2520pixel-level%2520and%2520image-level%2520tampering%2520clues.%250AAdditionally%252C%2520we%2520leverage%2520GPT-4o%2520to%2520enhance%2520existing%2520IFDL%2520datasets%252C%2520creating%250Athe%2520Multi-Modal%2520Tamper%2520Description%2520dataSet%2520%2528MMTD-Set%2529%2520for%2520training%2520FakeShield%2527s%250Atampering%2520analysis%2520capabilities.%2520Meanwhile%252C%2520we%2520incorporate%2520a%2520Domain%2520Tag-guided%250AExplainable%2520Forgery%2520Detection%2520Module%2520%2528DTE-FDM%2529%2520and%2520a%2520Multi-modal%2520Forgery%250ALocalization%2520Module%2520%2528MFLM%2529%2520to%2520address%2520various%2520types%2520of%2520tamper%2520detection%250Ainterpretation%2520and%2520achieve%2520forgery%2520localization%2520guided%2520by%2520detailed%2520textual%250Adescriptions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FakeShield%2520effectively%250Adetects%2520and%2520localizes%2520various%2520tampering%2520techniques%252C%2520offering%2520an%2520explainable%2520and%250Asuperior%2520solution%2520compared%2520to%2520previous%2520IFDL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeShield%3A%20Explainable%20Image%20Forgery%20Detection%20and%20Localization%20via%0A%20%20Multi-modal%20Large%20Language%20Models&entry.906535625=Zhipei%20Xu%20and%20Xuanyu%20Zhang%20and%20Runyi%20Li%20and%20Zecheng%20Tang%20and%20Qing%20Huang%20and%20Jian%20Zhang&entry.1292438233=%20%20The%20rapid%20development%20of%20generative%20AI%20is%20a%20double-edged%20sword%2C%20which%20not%0Aonly%20facilitates%20content%20creation%20but%20also%20makes%20image%20manipulation%20easier%20and%0Amore%20difficult%20to%20detect.%20Although%20current%20image%20forgery%20detection%20and%0Alocalization%20%28IFDL%29%20methods%20are%20generally%20effective%2C%20they%20tend%20to%20face%20two%0Achallenges%3A%20%5Ctextbf%7B1%29%7D%20black-box%20nature%20with%20unknown%20detection%20principle%2C%0A%5Ctextbf%7B2%29%7D%20limited%20generalization%20across%20diverse%20tampering%20methods%20%28e.g.%2C%0APhotoshop%2C%20DeepFake%2C%20AIGC-Editing%29.%20To%20address%20these%20issues%2C%20we%20propose%20the%0Aexplainable%20IFDL%20task%20and%20design%20FakeShield%2C%20a%20multi-modal%20framework%20capable%20of%0Aevaluating%20image%20authenticity%2C%20generating%20tampered%20region%20masks%2C%20and%20providing%0Aa%20judgment%20basis%20based%20on%20pixel-level%20and%20image-level%20tampering%20clues.%0AAdditionally%2C%20we%20leverage%20GPT-4o%20to%20enhance%20existing%20IFDL%20datasets%2C%20creating%0Athe%20Multi-Modal%20Tamper%20Description%20dataSet%20%28MMTD-Set%29%20for%20training%20FakeShield%27s%0Atampering%20analysis%20capabilities.%20Meanwhile%2C%20we%20incorporate%20a%20Domain%20Tag-guided%0AExplainable%20Forgery%20Detection%20Module%20%28DTE-FDM%29%20and%20a%20Multi-modal%20Forgery%0ALocalization%20Module%20%28MFLM%29%20to%20address%20various%20types%20of%20tamper%20detection%0Ainterpretation%20and%20achieve%20forgery%20localization%20guided%20by%20detailed%20textual%0Adescriptions.%20Extensive%20experiments%20demonstrate%20that%20FakeShield%20effectively%0Adetects%20and%20localizes%20various%20tampering%20techniques%2C%20offering%20an%20explainable%20and%0Asuperior%20solution%20compared%20to%20previous%20IFDL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02761v3&entry.124074799=Read"},
{"title": "OxonFair: A Flexible Toolkit for Algorithmic Fairness", "author": "Eoin Delaney and Zihao Fu and Sandra Wachter and Brent Mittelstadt and Chris Russell", "abstract": "  We present OxonFair, a new open source toolkit for enforcing fairness in\nbinary classification. Compared to existing toolkits: (i) We support NLP and\nComputer Vision classification as well as standard tabular problems. (ii) We\nsupport enforcing fairness on validation data, making us robust to a wide range\nof overfitting challenges. (iii) Our approach can optimize any measure based on\nTrue Positives, False Positive, False Negatives, and True Negatives. This makes\nit easily extensible and much more expressive than existing toolkits. It\nsupports all 9 and all 10 of the decision-based group metrics of two popular\nreview articles. (iv) We jointly optimize a performance objective alongside\nfairness constraints. This minimizes degradation while enforcing fairness, and\neven improves the performance of inadequately tuned unfair baselines. OxonFair\nis compatible with standard ML toolkits, including sklearn, Autogluon, and\nPyTorch and is available at https://github.com/oxfordinternetinstitute/oxonfair\n", "link": "http://arxiv.org/abs/2407.13710v2", "date": "2024-11-05", "relevancy": 2.1402, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4367}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4303}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OxonFair%3A%20A%20Flexible%20Toolkit%20for%20Algorithmic%20Fairness&body=Title%3A%20OxonFair%3A%20A%20Flexible%20Toolkit%20for%20Algorithmic%20Fairness%0AAuthor%3A%20Eoin%20Delaney%20and%20Zihao%20Fu%20and%20Sandra%20Wachter%20and%20Brent%20Mittelstadt%20and%20Chris%20Russell%0AAbstract%3A%20%20%20We%20present%20OxonFair%2C%20a%20new%20open%20source%20toolkit%20for%20enforcing%20fairness%20in%0Abinary%20classification.%20Compared%20to%20existing%20toolkits%3A%20%28i%29%20We%20support%20NLP%20and%0AComputer%20Vision%20classification%20as%20well%20as%20standard%20tabular%20problems.%20%28ii%29%20We%0Asupport%20enforcing%20fairness%20on%20validation%20data%2C%20making%20us%20robust%20to%20a%20wide%20range%0Aof%20overfitting%20challenges.%20%28iii%29%20Our%20approach%20can%20optimize%20any%20measure%20based%20on%0ATrue%20Positives%2C%20False%20Positive%2C%20False%20Negatives%2C%20and%20True%20Negatives.%20This%20makes%0Ait%20easily%20extensible%20and%20much%20more%20expressive%20than%20existing%20toolkits.%20It%0Asupports%20all%209%20and%20all%2010%20of%20the%20decision-based%20group%20metrics%20of%20two%20popular%0Areview%20articles.%20%28iv%29%20We%20jointly%20optimize%20a%20performance%20objective%20alongside%0Afairness%20constraints.%20This%20minimizes%20degradation%20while%20enforcing%20fairness%2C%20and%0Aeven%20improves%20the%20performance%20of%20inadequately%20tuned%20unfair%20baselines.%20OxonFair%0Ais%20compatible%20with%20standard%20ML%20toolkits%2C%20including%20sklearn%2C%20Autogluon%2C%20and%0APyTorch%20and%20is%20available%20at%20https%3A//github.com/oxfordinternetinstitute/oxonfair%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOxonFair%253A%2520A%2520Flexible%2520Toolkit%2520for%2520Algorithmic%2520Fairness%26entry.906535625%3DEoin%2520Delaney%2520and%2520Zihao%2520Fu%2520and%2520Sandra%2520Wachter%2520and%2520Brent%2520Mittelstadt%2520and%2520Chris%2520Russell%26entry.1292438233%3D%2520%2520We%2520present%2520OxonFair%252C%2520a%2520new%2520open%2520source%2520toolkit%2520for%2520enforcing%2520fairness%2520in%250Abinary%2520classification.%2520Compared%2520to%2520existing%2520toolkits%253A%2520%2528i%2529%2520We%2520support%2520NLP%2520and%250AComputer%2520Vision%2520classification%2520as%2520well%2520as%2520standard%2520tabular%2520problems.%2520%2528ii%2529%2520We%250Asupport%2520enforcing%2520fairness%2520on%2520validation%2520data%252C%2520making%2520us%2520robust%2520to%2520a%2520wide%2520range%250Aof%2520overfitting%2520challenges.%2520%2528iii%2529%2520Our%2520approach%2520can%2520optimize%2520any%2520measure%2520based%2520on%250ATrue%2520Positives%252C%2520False%2520Positive%252C%2520False%2520Negatives%252C%2520and%2520True%2520Negatives.%2520This%2520makes%250Ait%2520easily%2520extensible%2520and%2520much%2520more%2520expressive%2520than%2520existing%2520toolkits.%2520It%250Asupports%2520all%25209%2520and%2520all%252010%2520of%2520the%2520decision-based%2520group%2520metrics%2520of%2520two%2520popular%250Areview%2520articles.%2520%2528iv%2529%2520We%2520jointly%2520optimize%2520a%2520performance%2520objective%2520alongside%250Afairness%2520constraints.%2520This%2520minimizes%2520degradation%2520while%2520enforcing%2520fairness%252C%2520and%250Aeven%2520improves%2520the%2520performance%2520of%2520inadequately%2520tuned%2520unfair%2520baselines.%2520OxonFair%250Ais%2520compatible%2520with%2520standard%2520ML%2520toolkits%252C%2520including%2520sklearn%252C%2520Autogluon%252C%2520and%250APyTorch%2520and%2520is%2520available%2520at%2520https%253A//github.com/oxfordinternetinstitute/oxonfair%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OxonFair%3A%20A%20Flexible%20Toolkit%20for%20Algorithmic%20Fairness&entry.906535625=Eoin%20Delaney%20and%20Zihao%20Fu%20and%20Sandra%20Wachter%20and%20Brent%20Mittelstadt%20and%20Chris%20Russell&entry.1292438233=%20%20We%20present%20OxonFair%2C%20a%20new%20open%20source%20toolkit%20for%20enforcing%20fairness%20in%0Abinary%20classification.%20Compared%20to%20existing%20toolkits%3A%20%28i%29%20We%20support%20NLP%20and%0AComputer%20Vision%20classification%20as%20well%20as%20standard%20tabular%20problems.%20%28ii%29%20We%0Asupport%20enforcing%20fairness%20on%20validation%20data%2C%20making%20us%20robust%20to%20a%20wide%20range%0Aof%20overfitting%20challenges.%20%28iii%29%20Our%20approach%20can%20optimize%20any%20measure%20based%20on%0ATrue%20Positives%2C%20False%20Positive%2C%20False%20Negatives%2C%20and%20True%20Negatives.%20This%20makes%0Ait%20easily%20extensible%20and%20much%20more%20expressive%20than%20existing%20toolkits.%20It%0Asupports%20all%209%20and%20all%2010%20of%20the%20decision-based%20group%20metrics%20of%20two%20popular%0Areview%20articles.%20%28iv%29%20We%20jointly%20optimize%20a%20performance%20objective%20alongside%0Afairness%20constraints.%20This%20minimizes%20degradation%20while%20enforcing%20fairness%2C%20and%0Aeven%20improves%20the%20performance%20of%20inadequately%20tuned%20unfair%20baselines.%20OxonFair%0Ais%20compatible%20with%20standard%20ML%20toolkits%2C%20including%20sklearn%2C%20Autogluon%2C%20and%0APyTorch%20and%20is%20available%20at%20https%3A//github.com/oxfordinternetinstitute/oxonfair%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13710v2&entry.124074799=Read"},
{"title": "Interaction2Code: How Far Are We From Automatic Interactive Webpage\n  Generation?", "author": "Jingyu Xiao and Yuxuan Wan and Yintong Huo and Zhiyao Xu and Michael R. Lyu", "abstract": "  Converting webpage design into functional UI code is a critical step for\nbuilding websites, which can be labor-intensive and time-consuming. To automate\nthis design-to-code transformation process, various automated methods using\nlearning-based networks and multi-modal large language models (MLLMs) have been\nproposed. However, these studies were merely evaluated on a narrow range of\nstatic web pages and ignored dynamic interaction elements, making them less\npractical for real-world website deployment.\n  To fill in the blank, we present the first systematic investigation of MLLMs\nin generating interactive webpages. Specifically, we first formulate the\nInteraction-to-Code task and build the Interaction2Code benchmark that contains\n97 unique web pages and 213 distinct interactions, spanning 15 webpage types\nand 30 interaction categories. We then conduct comprehensive experiments on\nthree state-of-the-art (SOTA) MLLMs using both automatic metrics and human\nevaluations, thereby summarizing six findings accordingly. Our experimental\nresults highlight the limitations of MLLMs in generating fine-grained\ninteractive features and managing interactions with complex transformations and\nsubtle visual modifications. We further analyze failure cases and their\nunderlying causes, identifying 10 common failure types and assessing their\nseverity. Additionally, our findings reveal three critical influencing factors,\ni.e., prompts, visual saliency, and textual descriptions, that can enhance the\ninteraction generation performance of MLLMs. Based on these findings, we elicit\nimplications for researchers and developers, providing a foundation for future\nadvancements in this field. Datasets and source code are available at\nhttps://github.com/WebPAI/Interaction2Code.\n", "link": "http://arxiv.org/abs/2411.03292v1", "date": "2024-11-05", "relevancy": 2.1363, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5411}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5304}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interaction2Code%3A%20How%20Far%20Are%20We%20From%20Automatic%20Interactive%20Webpage%0A%20%20Generation%3F&body=Title%3A%20Interaction2Code%3A%20How%20Far%20Are%20We%20From%20Automatic%20Interactive%20Webpage%0A%20%20Generation%3F%0AAuthor%3A%20Jingyu%20Xiao%20and%20Yuxuan%20Wan%20and%20Yintong%20Huo%20and%20Zhiyao%20Xu%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Converting%20webpage%20design%20into%20functional%20UI%20code%20is%20a%20critical%20step%20for%0Abuilding%20websites%2C%20which%20can%20be%20labor-intensive%20and%20time-consuming.%20To%20automate%0Athis%20design-to-code%20transformation%20process%2C%20various%20automated%20methods%20using%0Alearning-based%20networks%20and%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20been%0Aproposed.%20However%2C%20these%20studies%20were%20merely%20evaluated%20on%20a%20narrow%20range%20of%0Astatic%20web%20pages%20and%20ignored%20dynamic%20interaction%20elements%2C%20making%20them%20less%0Apractical%20for%20real-world%20website%20deployment.%0A%20%20To%20fill%20in%20the%20blank%2C%20we%20present%20the%20first%20systematic%20investigation%20of%20MLLMs%0Ain%20generating%20interactive%20webpages.%20Specifically%2C%20we%20first%20formulate%20the%0AInteraction-to-Code%20task%20and%20build%20the%20Interaction2Code%20benchmark%20that%20contains%0A97%20unique%20web%20pages%20and%20213%20distinct%20interactions%2C%20spanning%2015%20webpage%20types%0Aand%2030%20interaction%20categories.%20We%20then%20conduct%20comprehensive%20experiments%20on%0Athree%20state-of-the-art%20%28SOTA%29%20MLLMs%20using%20both%20automatic%20metrics%20and%20human%0Aevaluations%2C%20thereby%20summarizing%20six%20findings%20accordingly.%20Our%20experimental%0Aresults%20highlight%20the%20limitations%20of%20MLLMs%20in%20generating%20fine-grained%0Ainteractive%20features%20and%20managing%20interactions%20with%20complex%20transformations%20and%0Asubtle%20visual%20modifications.%20We%20further%20analyze%20failure%20cases%20and%20their%0Aunderlying%20causes%2C%20identifying%2010%20common%20failure%20types%20and%20assessing%20their%0Aseverity.%20Additionally%2C%20our%20findings%20reveal%20three%20critical%20influencing%20factors%2C%0Ai.e.%2C%20prompts%2C%20visual%20saliency%2C%20and%20textual%20descriptions%2C%20that%20can%20enhance%20the%0Ainteraction%20generation%20performance%20of%20MLLMs.%20Based%20on%20these%20findings%2C%20we%20elicit%0Aimplications%20for%20researchers%20and%20developers%2C%20providing%20a%20foundation%20for%20future%0Aadvancements%20in%20this%20field.%20Datasets%20and%20source%20code%20are%20available%20at%0Ahttps%3A//github.com/WebPAI/Interaction2Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteraction2Code%253A%2520How%2520Far%2520Are%2520We%2520From%2520Automatic%2520Interactive%2520Webpage%250A%2520%2520Generation%253F%26entry.906535625%3DJingyu%2520Xiao%2520and%2520Yuxuan%2520Wan%2520and%2520Yintong%2520Huo%2520and%2520Zhiyao%2520Xu%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Converting%2520webpage%2520design%2520into%2520functional%2520UI%2520code%2520is%2520a%2520critical%2520step%2520for%250Abuilding%2520websites%252C%2520which%2520can%2520be%2520labor-intensive%2520and%2520time-consuming.%2520To%2520automate%250Athis%2520design-to-code%2520transformation%2520process%252C%2520various%2520automated%2520methods%2520using%250Alearning-based%2520networks%2520and%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520been%250Aproposed.%2520However%252C%2520these%2520studies%2520were%2520merely%2520evaluated%2520on%2520a%2520narrow%2520range%2520of%250Astatic%2520web%2520pages%2520and%2520ignored%2520dynamic%2520interaction%2520elements%252C%2520making%2520them%2520less%250Apractical%2520for%2520real-world%2520website%2520deployment.%250A%2520%2520To%2520fill%2520in%2520the%2520blank%252C%2520we%2520present%2520the%2520first%2520systematic%2520investigation%2520of%2520MLLMs%250Ain%2520generating%2520interactive%2520webpages.%2520Specifically%252C%2520we%2520first%2520formulate%2520the%250AInteraction-to-Code%2520task%2520and%2520build%2520the%2520Interaction2Code%2520benchmark%2520that%2520contains%250A97%2520unique%2520web%2520pages%2520and%2520213%2520distinct%2520interactions%252C%2520spanning%252015%2520webpage%2520types%250Aand%252030%2520interaction%2520categories.%2520We%2520then%2520conduct%2520comprehensive%2520experiments%2520on%250Athree%2520state-of-the-art%2520%2528SOTA%2529%2520MLLMs%2520using%2520both%2520automatic%2520metrics%2520and%2520human%250Aevaluations%252C%2520thereby%2520summarizing%2520six%2520findings%2520accordingly.%2520Our%2520experimental%250Aresults%2520highlight%2520the%2520limitations%2520of%2520MLLMs%2520in%2520generating%2520fine-grained%250Ainteractive%2520features%2520and%2520managing%2520interactions%2520with%2520complex%2520transformations%2520and%250Asubtle%2520visual%2520modifications.%2520We%2520further%2520analyze%2520failure%2520cases%2520and%2520their%250Aunderlying%2520causes%252C%2520identifying%252010%2520common%2520failure%2520types%2520and%2520assessing%2520their%250Aseverity.%2520Additionally%252C%2520our%2520findings%2520reveal%2520three%2520critical%2520influencing%2520factors%252C%250Ai.e.%252C%2520prompts%252C%2520visual%2520saliency%252C%2520and%2520textual%2520descriptions%252C%2520that%2520can%2520enhance%2520the%250Ainteraction%2520generation%2520performance%2520of%2520MLLMs.%2520Based%2520on%2520these%2520findings%252C%2520we%2520elicit%250Aimplications%2520for%2520researchers%2520and%2520developers%252C%2520providing%2520a%2520foundation%2520for%2520future%250Aadvancements%2520in%2520this%2520field.%2520Datasets%2520and%2520source%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/WebPAI/Interaction2Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interaction2Code%3A%20How%20Far%20Are%20We%20From%20Automatic%20Interactive%20Webpage%0A%20%20Generation%3F&entry.906535625=Jingyu%20Xiao%20and%20Yuxuan%20Wan%20and%20Yintong%20Huo%20and%20Zhiyao%20Xu%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Converting%20webpage%20design%20into%20functional%20UI%20code%20is%20a%20critical%20step%20for%0Abuilding%20websites%2C%20which%20can%20be%20labor-intensive%20and%20time-consuming.%20To%20automate%0Athis%20design-to-code%20transformation%20process%2C%20various%20automated%20methods%20using%0Alearning-based%20networks%20and%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20been%0Aproposed.%20However%2C%20these%20studies%20were%20merely%20evaluated%20on%20a%20narrow%20range%20of%0Astatic%20web%20pages%20and%20ignored%20dynamic%20interaction%20elements%2C%20making%20them%20less%0Apractical%20for%20real-world%20website%20deployment.%0A%20%20To%20fill%20in%20the%20blank%2C%20we%20present%20the%20first%20systematic%20investigation%20of%20MLLMs%0Ain%20generating%20interactive%20webpages.%20Specifically%2C%20we%20first%20formulate%20the%0AInteraction-to-Code%20task%20and%20build%20the%20Interaction2Code%20benchmark%20that%20contains%0A97%20unique%20web%20pages%20and%20213%20distinct%20interactions%2C%20spanning%2015%20webpage%20types%0Aand%2030%20interaction%20categories.%20We%20then%20conduct%20comprehensive%20experiments%20on%0Athree%20state-of-the-art%20%28SOTA%29%20MLLMs%20using%20both%20automatic%20metrics%20and%20human%0Aevaluations%2C%20thereby%20summarizing%20six%20findings%20accordingly.%20Our%20experimental%0Aresults%20highlight%20the%20limitations%20of%20MLLMs%20in%20generating%20fine-grained%0Ainteractive%20features%20and%20managing%20interactions%20with%20complex%20transformations%20and%0Asubtle%20visual%20modifications.%20We%20further%20analyze%20failure%20cases%20and%20their%0Aunderlying%20causes%2C%20identifying%2010%20common%20failure%20types%20and%20assessing%20their%0Aseverity.%20Additionally%2C%20our%20findings%20reveal%20three%20critical%20influencing%20factors%2C%0Ai.e.%2C%20prompts%2C%20visual%20saliency%2C%20and%20textual%20descriptions%2C%20that%20can%20enhance%20the%0Ainteraction%20generation%20performance%20of%20MLLMs.%20Based%20on%20these%20findings%2C%20we%20elicit%0Aimplications%20for%20researchers%20and%20developers%2C%20providing%20a%20foundation%20for%20future%0Aadvancements%20in%20this%20field.%20Datasets%20and%20source%20code%20are%20available%20at%0Ahttps%3A//github.com/WebPAI/Interaction2Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03292v1&entry.124074799=Read"},
{"title": "SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning. Hard negatives - samples closely resembling the anchor\n- are key to enhancing learned representations' discriminative power. However,\nefficiently leveraging hard negatives remains challenging. We introduce SynCo\n(Synthetic Negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and better representation learning,\nreaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after\n200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50\nencoder. It also transfers more effectively to detection tasks: on PASCAL VOC,\nit outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it\nsets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for\ninstance segmentation. Our synthetic hard negative generation approach\nsignificantly enhances visual representations learned through self-supervised\ncontrastive learning. Code is available at\nhttps://github.com/giakoumoglou/synco.\n", "link": "http://arxiv.org/abs/2410.02401v5", "date": "2024-11-05", "relevancy": 2.1256, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5626}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations&body=Title%3A%20SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20become%20a%20dominant%20approach%20in%20self-supervised%20visual%0Arepresentation%20learning.%20Hard%20negatives%20-%20samples%20closely%20resembling%20the%20anchor%0A-%20are%20key%20to%20enhancing%20learned%20representations%27%20discriminative%20power.%20However%2C%0Aefficiently%20leveraging%20hard%20negatives%20remains%20challenging.%20We%20introduce%20SynCo%0A%28Synthetic%20Negatives%20in%20Contrastive%20learning%29%2C%20a%20novel%20approach%20that%20improves%0Amodel%20performance%20by%20generating%20synthetic%20hard%20negatives%20on%20the%20representation%0Aspace.%20Building%20on%20the%20MoCo%20framework%2C%20SynCo%20introduces%20six%20strategies%20for%0Acreating%20diverse%20synthetic%20hard%20negatives%20on-the-fly%20with%20minimal%20computational%0Aoverhead.%20SynCo%20achieves%20faster%20training%20and%20better%20representation%20learning%2C%0Areaching%2067.9%25%20top-1%20accuracy%20on%20ImageNet%20ILSVRC-2012%20linear%20evaluation%20after%0A200%20pretraining%20epochs%2C%20surpassing%20MoCo%27s%2067.5%25%20using%20the%20same%20ResNet-50%0Aencoder.%20It%20also%20transfers%20more%20effectively%20to%20detection%20tasks%3A%20on%20PASCAL%20VOC%2C%0Ait%20outperforms%20both%20the%20supervised%20baseline%20and%20MoCo%20with%2082.5%25%20AP%3B%20on%20COCO%2C%20it%0Asets%20new%20benchmarks%20with%2040.9%25%20AP%20for%20bounding%20box%20detection%20and%2035.5%25%20AP%20for%0Ainstance%20segmentation.%20Our%20synthetic%20hard%20negative%20generation%20approach%0Asignificantly%20enhances%20visual%20representations%20learned%20through%20self-supervised%0Acontrastive%20learning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/giakoumoglou/synco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02401v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCo%253A%2520Synthetic%2520Hard%2520Negatives%2520in%2520Contrastive%2520Learning%2520for%2520Better%250A%2520%2520Unsupervised%2520Visual%2520Representations%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520become%2520a%2520dominant%2520approach%2520in%2520self-supervised%2520visual%250Arepresentation%2520learning.%2520Hard%2520negatives%2520-%2520samples%2520closely%2520resembling%2520the%2520anchor%250A-%2520are%2520key%2520to%2520enhancing%2520learned%2520representations%2527%2520discriminative%2520power.%2520However%252C%250Aefficiently%2520leveraging%2520hard%2520negatives%2520remains%2520challenging.%2520We%2520introduce%2520SynCo%250A%2528Synthetic%2520Negatives%2520in%2520Contrastive%2520learning%2529%252C%2520a%2520novel%2520approach%2520that%2520improves%250Amodel%2520performance%2520by%2520generating%2520synthetic%2520hard%2520negatives%2520on%2520the%2520representation%250Aspace.%2520Building%2520on%2520the%2520MoCo%2520framework%252C%2520SynCo%2520introduces%2520six%2520strategies%2520for%250Acreating%2520diverse%2520synthetic%2520hard%2520negatives%2520on-the-fly%2520with%2520minimal%2520computational%250Aoverhead.%2520SynCo%2520achieves%2520faster%2520training%2520and%2520better%2520representation%2520learning%252C%250Areaching%252067.9%2525%2520top-1%2520accuracy%2520on%2520ImageNet%2520ILSVRC-2012%2520linear%2520evaluation%2520after%250A200%2520pretraining%2520epochs%252C%2520surpassing%2520MoCo%2527s%252067.5%2525%2520using%2520the%2520same%2520ResNet-50%250Aencoder.%2520It%2520also%2520transfers%2520more%2520effectively%2520to%2520detection%2520tasks%253A%2520on%2520PASCAL%2520VOC%252C%250Ait%2520outperforms%2520both%2520the%2520supervised%2520baseline%2520and%2520MoCo%2520with%252082.5%2525%2520AP%253B%2520on%2520COCO%252C%2520it%250Asets%2520new%2520benchmarks%2520with%252040.9%2525%2520AP%2520for%2520bounding%2520box%2520detection%2520and%252035.5%2525%2520AP%2520for%250Ainstance%2520segmentation.%2520Our%2520synthetic%2520hard%2520negative%2520generation%2520approach%250Asignificantly%2520enhances%2520visual%2520representations%2520learned%2520through%2520self-supervised%250Acontrastive%2520learning.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/giakoumoglou/synco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02401v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCo%3A%20Synthetic%20Hard%20Negatives%20in%20Contrastive%20Learning%20for%20Better%0A%20%20Unsupervised%20Visual%20Representations&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Contrastive%20learning%20has%20become%20a%20dominant%20approach%20in%20self-supervised%20visual%0Arepresentation%20learning.%20Hard%20negatives%20-%20samples%20closely%20resembling%20the%20anchor%0A-%20are%20key%20to%20enhancing%20learned%20representations%27%20discriminative%20power.%20However%2C%0Aefficiently%20leveraging%20hard%20negatives%20remains%20challenging.%20We%20introduce%20SynCo%0A%28Synthetic%20Negatives%20in%20Contrastive%20learning%29%2C%20a%20novel%20approach%20that%20improves%0Amodel%20performance%20by%20generating%20synthetic%20hard%20negatives%20on%20the%20representation%0Aspace.%20Building%20on%20the%20MoCo%20framework%2C%20SynCo%20introduces%20six%20strategies%20for%0Acreating%20diverse%20synthetic%20hard%20negatives%20on-the-fly%20with%20minimal%20computational%0Aoverhead.%20SynCo%20achieves%20faster%20training%20and%20better%20representation%20learning%2C%0Areaching%2067.9%25%20top-1%20accuracy%20on%20ImageNet%20ILSVRC-2012%20linear%20evaluation%20after%0A200%20pretraining%20epochs%2C%20surpassing%20MoCo%27s%2067.5%25%20using%20the%20same%20ResNet-50%0Aencoder.%20It%20also%20transfers%20more%20effectively%20to%20detection%20tasks%3A%20on%20PASCAL%20VOC%2C%0Ait%20outperforms%20both%20the%20supervised%20baseline%20and%20MoCo%20with%2082.5%25%20AP%3B%20on%20COCO%2C%20it%0Asets%20new%20benchmarks%20with%2040.9%25%20AP%20for%20bounding%20box%20detection%20and%2035.5%25%20AP%20for%0Ainstance%20segmentation.%20Our%20synthetic%20hard%20negative%20generation%20approach%0Asignificantly%20enhances%20visual%20representations%20learned%20through%20self-supervised%0Acontrastive%20learning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/giakoumoglou/synco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02401v5&entry.124074799=Read"},
{"title": "Local Lesion Generation is Effective for Capsule Endoscopy Image Data\n  Augmentation in a Limited Data Setting", "author": "Adrian B. Ch\u0142opowiec and Adam R. Ch\u0142opowiec and Krzysztof Galus and Wojciech Cebula and Martin Tabakov", "abstract": "  Limited medical imaging datasets challenge deep learning models by increasing\nrisks of overfitting and reduced generalization, particularly in Generative\nAdversarial Networks (GANs), where discriminators may overfit, leading to\ntraining divergence. This constraint also impairs classification models trained\non small datasets. Generative Data Augmentation (GDA) addresses this by\nexpanding training datasets with synthetic data, although it requires training\na generative model. We propose and evaluate two local lesion generation\napproaches to address the challenge of augmenting small medical image datasets.\nThe first approach employs the Poisson Image Editing algorithm, a classical\nimage processing technique, to create realistic image composites that\noutperform current state-of-the-art methods. The second approach introduces a\nnovel generative method, leveraging a fine-tuned Image Inpainting GAN to\nsynthesize realistic lesions within specified regions of real training images.\nA comprehensive comparison of the two proposed methods demonstrates that\neffective local lesion generation in a data-constrained setting allows for\nreaching new state-of-the-art results in capsule endoscopy lesion\nclassification. Combination of our techniques achieves a macro F1-score of\n33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on\nthe highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule\nendoscopy. To the best of our knowledge, this work is the first to apply a\nfine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that\nan image-conditional GAN can be adapted effectively to limited datasets to\ngenerate high-quality examples, facilitating effective data augmentation.\nAdditionally, we show that combining this GAN-based approach with classical\nimage processing techniques further enhances the results.\n", "link": "http://arxiv.org/abs/2411.03098v1", "date": "2024-11-05", "relevancy": 2.1225, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5338}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting&body=Title%3A%20Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting%0AAuthor%3A%20Adrian%20B.%20Ch%C5%82opowiec%20and%20Adam%20R.%20Ch%C5%82opowiec%20and%20Krzysztof%20Galus%20and%20Wojciech%20Cebula%20and%20Martin%20Tabakov%0AAbstract%3A%20%20%20Limited%20medical%20imaging%20datasets%20challenge%20deep%20learning%20models%20by%20increasing%0Arisks%20of%20overfitting%20and%20reduced%20generalization%2C%20particularly%20in%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20where%20discriminators%20may%20overfit%2C%20leading%20to%0Atraining%20divergence.%20This%20constraint%20also%20impairs%20classification%20models%20trained%0Aon%20small%20datasets.%20Generative%20Data%20Augmentation%20%28GDA%29%20addresses%20this%20by%0Aexpanding%20training%20datasets%20with%20synthetic%20data%2C%20although%20it%20requires%20training%0Aa%20generative%20model.%20We%20propose%20and%20evaluate%20two%20local%20lesion%20generation%0Aapproaches%20to%20address%20the%20challenge%20of%20augmenting%20small%20medical%20image%20datasets.%0AThe%20first%20approach%20employs%20the%20Poisson%20Image%20Editing%20algorithm%2C%20a%20classical%0Aimage%20processing%20technique%2C%20to%20create%20realistic%20image%20composites%20that%0Aoutperform%20current%20state-of-the-art%20methods.%20The%20second%20approach%20introduces%20a%0Anovel%20generative%20method%2C%20leveraging%20a%20fine-tuned%20Image%20Inpainting%20GAN%20to%0Asynthesize%20realistic%20lesions%20within%20specified%20regions%20of%20real%20training%20images.%0AA%20comprehensive%20comparison%20of%20the%20two%20proposed%20methods%20demonstrates%20that%0Aeffective%20local%20lesion%20generation%20in%20a%20data-constrained%20setting%20allows%20for%0Areaching%20new%20state-of-the-art%20results%20in%20capsule%20endoscopy%20lesion%0Aclassification.%20Combination%20of%20our%20techniques%20achieves%20a%20macro%20F1-score%20of%0A33.07%25%2C%20surpassing%20the%20previous%20best%20result%20by%207.84%20percentage%20points%20%28p.p.%29%20on%0Athe%20highly%20imbalanced%20Kvasir%20Capsule%20Dataset%2C%20a%20benchmark%20for%20capsule%0Aendoscopy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20apply%20a%0Afine-tuned%20Image%20Inpainting%20GAN%20for%20GDA%20in%20medical%20imaging%2C%20demonstrating%20that%0Aan%20image-conditional%20GAN%20can%20be%20adapted%20effectively%20to%20limited%20datasets%20to%0Agenerate%20high-quality%20examples%2C%20facilitating%20effective%20data%20augmentation.%0AAdditionally%2C%20we%20show%20that%20combining%20this%20GAN-based%20approach%20with%20classical%0Aimage%20processing%20techniques%20further%20enhances%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Lesion%2520Generation%2520is%2520Effective%2520for%2520Capsule%2520Endoscopy%2520Image%2520Data%250A%2520%2520Augmentation%2520in%2520a%2520Limited%2520Data%2520Setting%26entry.906535625%3DAdrian%2520B.%2520Ch%25C5%2582opowiec%2520and%2520Adam%2520R.%2520Ch%25C5%2582opowiec%2520and%2520Krzysztof%2520Galus%2520and%2520Wojciech%2520Cebula%2520and%2520Martin%2520Tabakov%26entry.1292438233%3D%2520%2520Limited%2520medical%2520imaging%2520datasets%2520challenge%2520deep%2520learning%2520models%2520by%2520increasing%250Arisks%2520of%2520overfitting%2520and%2520reduced%2520generalization%252C%2520particularly%2520in%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%252C%2520where%2520discriminators%2520may%2520overfit%252C%2520leading%2520to%250Atraining%2520divergence.%2520This%2520constraint%2520also%2520impairs%2520classification%2520models%2520trained%250Aon%2520small%2520datasets.%2520Generative%2520Data%2520Augmentation%2520%2528GDA%2529%2520addresses%2520this%2520by%250Aexpanding%2520training%2520datasets%2520with%2520synthetic%2520data%252C%2520although%2520it%2520requires%2520training%250Aa%2520generative%2520model.%2520We%2520propose%2520and%2520evaluate%2520two%2520local%2520lesion%2520generation%250Aapproaches%2520to%2520address%2520the%2520challenge%2520of%2520augmenting%2520small%2520medical%2520image%2520datasets.%250AThe%2520first%2520approach%2520employs%2520the%2520Poisson%2520Image%2520Editing%2520algorithm%252C%2520a%2520classical%250Aimage%2520processing%2520technique%252C%2520to%2520create%2520realistic%2520image%2520composites%2520that%250Aoutperform%2520current%2520state-of-the-art%2520methods.%2520The%2520second%2520approach%2520introduces%2520a%250Anovel%2520generative%2520method%252C%2520leveraging%2520a%2520fine-tuned%2520Image%2520Inpainting%2520GAN%2520to%250Asynthesize%2520realistic%2520lesions%2520within%2520specified%2520regions%2520of%2520real%2520training%2520images.%250AA%2520comprehensive%2520comparison%2520of%2520the%2520two%2520proposed%2520methods%2520demonstrates%2520that%250Aeffective%2520local%2520lesion%2520generation%2520in%2520a%2520data-constrained%2520setting%2520allows%2520for%250Areaching%2520new%2520state-of-the-art%2520results%2520in%2520capsule%2520endoscopy%2520lesion%250Aclassification.%2520Combination%2520of%2520our%2520techniques%2520achieves%2520a%2520macro%2520F1-score%2520of%250A33.07%2525%252C%2520surpassing%2520the%2520previous%2520best%2520result%2520by%25207.84%2520percentage%2520points%2520%2528p.p.%2529%2520on%250Athe%2520highly%2520imbalanced%2520Kvasir%2520Capsule%2520Dataset%252C%2520a%2520benchmark%2520for%2520capsule%250Aendoscopy.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%2520apply%2520a%250Afine-tuned%2520Image%2520Inpainting%2520GAN%2520for%2520GDA%2520in%2520medical%2520imaging%252C%2520demonstrating%2520that%250Aan%2520image-conditional%2520GAN%2520can%2520be%2520adapted%2520effectively%2520to%2520limited%2520datasets%2520to%250Agenerate%2520high-quality%2520examples%252C%2520facilitating%2520effective%2520data%2520augmentation.%250AAdditionally%252C%2520we%2520show%2520that%2520combining%2520this%2520GAN-based%2520approach%2520with%2520classical%250Aimage%2520processing%2520techniques%2520further%2520enhances%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting&entry.906535625=Adrian%20B.%20Ch%C5%82opowiec%20and%20Adam%20R.%20Ch%C5%82opowiec%20and%20Krzysztof%20Galus%20and%20Wojciech%20Cebula%20and%20Martin%20Tabakov&entry.1292438233=%20%20Limited%20medical%20imaging%20datasets%20challenge%20deep%20learning%20models%20by%20increasing%0Arisks%20of%20overfitting%20and%20reduced%20generalization%2C%20particularly%20in%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20where%20discriminators%20may%20overfit%2C%20leading%20to%0Atraining%20divergence.%20This%20constraint%20also%20impairs%20classification%20models%20trained%0Aon%20small%20datasets.%20Generative%20Data%20Augmentation%20%28GDA%29%20addresses%20this%20by%0Aexpanding%20training%20datasets%20with%20synthetic%20data%2C%20although%20it%20requires%20training%0Aa%20generative%20model.%20We%20propose%20and%20evaluate%20two%20local%20lesion%20generation%0Aapproaches%20to%20address%20the%20challenge%20of%20augmenting%20small%20medical%20image%20datasets.%0AThe%20first%20approach%20employs%20the%20Poisson%20Image%20Editing%20algorithm%2C%20a%20classical%0Aimage%20processing%20technique%2C%20to%20create%20realistic%20image%20composites%20that%0Aoutperform%20current%20state-of-the-art%20methods.%20The%20second%20approach%20introduces%20a%0Anovel%20generative%20method%2C%20leveraging%20a%20fine-tuned%20Image%20Inpainting%20GAN%20to%0Asynthesize%20realistic%20lesions%20within%20specified%20regions%20of%20real%20training%20images.%0AA%20comprehensive%20comparison%20of%20the%20two%20proposed%20methods%20demonstrates%20that%0Aeffective%20local%20lesion%20generation%20in%20a%20data-constrained%20setting%20allows%20for%0Areaching%20new%20state-of-the-art%20results%20in%20capsule%20endoscopy%20lesion%0Aclassification.%20Combination%20of%20our%20techniques%20achieves%20a%20macro%20F1-score%20of%0A33.07%25%2C%20surpassing%20the%20previous%20best%20result%20by%207.84%20percentage%20points%20%28p.p.%29%20on%0Athe%20highly%20imbalanced%20Kvasir%20Capsule%20Dataset%2C%20a%20benchmark%20for%20capsule%0Aendoscopy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20apply%20a%0Afine-tuned%20Image%20Inpainting%20GAN%20for%20GDA%20in%20medical%20imaging%2C%20demonstrating%20that%0Aan%20image-conditional%20GAN%20can%20be%20adapted%20effectively%20to%20limited%20datasets%20to%0Agenerate%20high-quality%20examples%2C%20facilitating%20effective%20data%20augmentation.%0AAdditionally%2C%20we%20show%20that%20combining%20this%20GAN-based%20approach%20with%20classical%0Aimage%20processing%20techniques%20further%20enhances%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03098v1&entry.124074799=Read"},
{"title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis", "author": "Temitope Akinboyewa and Zhenlong Li and Huan Ning and M. Naser Lessani", "abstract": "  Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.\n", "link": "http://arxiv.org/abs/2411.03205v1", "date": "2024-11-05", "relevancy": 2.1201, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5466}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis&body=Title%3A%20GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis%0AAuthor%3A%20Temitope%20Akinboyewa%20and%20Zhenlong%20Li%20and%20Huan%20Ning%20and%20M.%20Naser%20Lessani%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Generative%20AI%20offer%20promising%20capabilities%20for%20spatial%0Aanalysis.%20Despite%20their%20potential%2C%20the%20integration%20of%20generative%20AI%20with%0Aestablished%20GIS%20platforms%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Aframework%20for%20integrating%20LLMs%20directly%20into%20existing%20GIS%20platforms%2C%20using%20QGIS%0Aas%20an%20example.%20Our%20approach%20leverages%20the%20reasoning%20and%20programming%0Acapabilities%20of%20LLMs%20to%20autonomously%20generate%20spatial%20analysis%20workflows%20and%0Acode%20through%20an%20informed%20agent%20that%20has%20comprehensive%20documentation%20of%20key%20GIS%0Atools%20and%20parameters.%20The%20implementation%20of%20this%20framework%20resulted%20in%20the%0Adevelopment%20of%20a%20%22GIS%20Copilot%22%20that%20allows%20GIS%20users%20to%20interact%20with%20QGIS%0Ausing%20natural%20language%20commands%20for%20spatial%20analysis.%20The%20GIS%20Copilot%20was%0Aevaluated%20based%20on%20three%20complexity%20levels%3A%20basic%20tasks%20that%20require%20one%20GIS%0Atool%20and%20typically%20involve%20one%20data%20layer%20to%20perform%20simple%20operations%3B%0Aintermediate%20tasks%20involving%20multi-step%20processes%20with%20multiple%20tools%2C%20guided%0Aby%20user%20instructions%3B%20and%20advanced%20tasks%20which%20involve%20multi-step%20processes%0Athat%20require%20multiple%20tools%20but%20not%20guided%20by%20user%20instructions%2C%20necessitating%0Athe%20agent%20to%20independently%20decide%20on%20and%20executes%20the%20necessary%20steps.%20The%0Aevaluation%20reveals%20that%20the%20GIS%20Copilot%20demonstrates%20strong%20potential%20in%0Aautomating%20foundational%20GIS%20operations%2C%20with%20a%20high%20success%20rate%20in%20tool%0Aselection%20and%20code%20generation%20for%20basic%20and%20intermediate%20tasks%2C%20while%0Achallenges%20remain%20in%20achieving%20full%20autonomy%20for%20more%20complex%20tasks.%20This%20study%0Acontributes%20to%20the%20emerging%20vision%20of%20Autonomous%20GIS%2C%20providing%20a%20pathway%20for%0Anon-experts%20to%20engage%20with%20geospatial%20analysis%20with%20minimal%20prior%20expertise.%0AWhile%20full%20autonomy%20is%20yet%20to%20be%20achieved%2C%20the%20GIS%20Copilot%20demonstrates%0Asignificant%20potential%20for%20simplifying%20GIS%20workflows%20and%20enhancing%0Adecision-making%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIS%2520Copilot%253A%2520Towards%2520an%2520Autonomous%2520GIS%2520Agent%2520for%2520Spatial%2520Analysis%26entry.906535625%3DTemitope%2520Akinboyewa%2520and%2520Zhenlong%2520Li%2520and%2520Huan%2520Ning%2520and%2520M.%2520Naser%2520Lessani%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Generative%2520AI%2520offer%2520promising%2520capabilities%2520for%2520spatial%250Aanalysis.%2520Despite%2520their%2520potential%252C%2520the%2520integration%2520of%2520generative%2520AI%2520with%250Aestablished%2520GIS%2520platforms%2520remains%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Aframework%2520for%2520integrating%2520LLMs%2520directly%2520into%2520existing%2520GIS%2520platforms%252C%2520using%2520QGIS%250Aas%2520an%2520example.%2520Our%2520approach%2520leverages%2520the%2520reasoning%2520and%2520programming%250Acapabilities%2520of%2520LLMs%2520to%2520autonomously%2520generate%2520spatial%2520analysis%2520workflows%2520and%250Acode%2520through%2520an%2520informed%2520agent%2520that%2520has%2520comprehensive%2520documentation%2520of%2520key%2520GIS%250Atools%2520and%2520parameters.%2520The%2520implementation%2520of%2520this%2520framework%2520resulted%2520in%2520the%250Adevelopment%2520of%2520a%2520%2522GIS%2520Copilot%2522%2520that%2520allows%2520GIS%2520users%2520to%2520interact%2520with%2520QGIS%250Ausing%2520natural%2520language%2520commands%2520for%2520spatial%2520analysis.%2520The%2520GIS%2520Copilot%2520was%250Aevaluated%2520based%2520on%2520three%2520complexity%2520levels%253A%2520basic%2520tasks%2520that%2520require%2520one%2520GIS%250Atool%2520and%2520typically%2520involve%2520one%2520data%2520layer%2520to%2520perform%2520simple%2520operations%253B%250Aintermediate%2520tasks%2520involving%2520multi-step%2520processes%2520with%2520multiple%2520tools%252C%2520guided%250Aby%2520user%2520instructions%253B%2520and%2520advanced%2520tasks%2520which%2520involve%2520multi-step%2520processes%250Athat%2520require%2520multiple%2520tools%2520but%2520not%2520guided%2520by%2520user%2520instructions%252C%2520necessitating%250Athe%2520agent%2520to%2520independently%2520decide%2520on%2520and%2520executes%2520the%2520necessary%2520steps.%2520The%250Aevaluation%2520reveals%2520that%2520the%2520GIS%2520Copilot%2520demonstrates%2520strong%2520potential%2520in%250Aautomating%2520foundational%2520GIS%2520operations%252C%2520with%2520a%2520high%2520success%2520rate%2520in%2520tool%250Aselection%2520and%2520code%2520generation%2520for%2520basic%2520and%2520intermediate%2520tasks%252C%2520while%250Achallenges%2520remain%2520in%2520achieving%2520full%2520autonomy%2520for%2520more%2520complex%2520tasks.%2520This%2520study%250Acontributes%2520to%2520the%2520emerging%2520vision%2520of%2520Autonomous%2520GIS%252C%2520providing%2520a%2520pathway%2520for%250Anon-experts%2520to%2520engage%2520with%2520geospatial%2520analysis%2520with%2520minimal%2520prior%2520expertise.%250AWhile%2520full%2520autonomy%2520is%2520yet%2520to%2520be%2520achieved%252C%2520the%2520GIS%2520Copilot%2520demonstrates%250Asignificant%2520potential%2520for%2520simplifying%2520GIS%2520workflows%2520and%2520enhancing%250Adecision-making%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis&entry.906535625=Temitope%20Akinboyewa%20and%20Zhenlong%20Li%20and%20Huan%20Ning%20and%20M.%20Naser%20Lessani&entry.1292438233=%20%20Recent%20advancements%20in%20Generative%20AI%20offer%20promising%20capabilities%20for%20spatial%0Aanalysis.%20Despite%20their%20potential%2C%20the%20integration%20of%20generative%20AI%20with%0Aestablished%20GIS%20platforms%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Aframework%20for%20integrating%20LLMs%20directly%20into%20existing%20GIS%20platforms%2C%20using%20QGIS%0Aas%20an%20example.%20Our%20approach%20leverages%20the%20reasoning%20and%20programming%0Acapabilities%20of%20LLMs%20to%20autonomously%20generate%20spatial%20analysis%20workflows%20and%0Acode%20through%20an%20informed%20agent%20that%20has%20comprehensive%20documentation%20of%20key%20GIS%0Atools%20and%20parameters.%20The%20implementation%20of%20this%20framework%20resulted%20in%20the%0Adevelopment%20of%20a%20%22GIS%20Copilot%22%20that%20allows%20GIS%20users%20to%20interact%20with%20QGIS%0Ausing%20natural%20language%20commands%20for%20spatial%20analysis.%20The%20GIS%20Copilot%20was%0Aevaluated%20based%20on%20three%20complexity%20levels%3A%20basic%20tasks%20that%20require%20one%20GIS%0Atool%20and%20typically%20involve%20one%20data%20layer%20to%20perform%20simple%20operations%3B%0Aintermediate%20tasks%20involving%20multi-step%20processes%20with%20multiple%20tools%2C%20guided%0Aby%20user%20instructions%3B%20and%20advanced%20tasks%20which%20involve%20multi-step%20processes%0Athat%20require%20multiple%20tools%20but%20not%20guided%20by%20user%20instructions%2C%20necessitating%0Athe%20agent%20to%20independently%20decide%20on%20and%20executes%20the%20necessary%20steps.%20The%0Aevaluation%20reveals%20that%20the%20GIS%20Copilot%20demonstrates%20strong%20potential%20in%0Aautomating%20foundational%20GIS%20operations%2C%20with%20a%20high%20success%20rate%20in%20tool%0Aselection%20and%20code%20generation%20for%20basic%20and%20intermediate%20tasks%2C%20while%0Achallenges%20remain%20in%20achieving%20full%20autonomy%20for%20more%20complex%20tasks.%20This%20study%0Acontributes%20to%20the%20emerging%20vision%20of%20Autonomous%20GIS%2C%20providing%20a%20pathway%20for%0Anon-experts%20to%20engage%20with%20geospatial%20analysis%20with%20minimal%20prior%20expertise.%0AWhile%20full%20autonomy%20is%20yet%20to%20be%20achieved%2C%20the%20GIS%20Copilot%20demonstrates%0Asignificant%20potential%20for%20simplifying%20GIS%20workflows%20and%20enhancing%0Adecision-making%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03205v1&entry.124074799=Read"},
{"title": "Taming the Long Tail in Human Mobility Prediction", "author": "Xiaohang Xu and Renhe Jiang and Chuang Yang and Zipei Fan and Kaoru Sezaki", "abstract": "  With the popularity of location-based services, human mobility prediction\nplays a key role in enhancing personalized navigation, optimizing\nrecommendation systems, and facilitating urban mobility and planning. This\ninvolves predicting a user's next POI (point-of-interest) visit using their\npast visit history. However, the uneven distribution of visitations over time\nand space, namely the long-tail problem in spatial distribution, makes it\ndifficult for AI models to predict those POIs that are less visited by humans.\nIn light of this issue, we propose the Long-Tail Adjusted Next POI Prediction\n(LoTNext) framework for mobility prediction, combining a Long-Tailed Graph\nAdjustment module to reduce the impact of the long-tailed nodes in the user-POI\ninteraction graph and a novel Long-Tailed Loss Adjustment module to adjust loss\nby logit score and sample weight adjustment strategy. Also, we employ the\nauxiliary prediction task to enhance generalization and accuracy. Our\nexperiments with two real-world trajectory datasets demonstrate that LoTNext\nsignificantly surpasses existing state-of-the-art works. Our code is available\nat https://github.com/Yukayo/LoTNext.\n", "link": "http://arxiv.org/abs/2410.14970v2", "date": "2024-11-05", "relevancy": 2.0497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5077}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction&body=Title%3A%20Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction%0AAuthor%3A%20Xiaohang%20Xu%20and%20Renhe%20Jiang%20and%20Chuang%20Yang%20and%20Zipei%20Fan%20and%20Kaoru%20Sezaki%0AAbstract%3A%20%20%20With%20the%20popularity%20of%20location-based%20services%2C%20human%20mobility%20prediction%0Aplays%20a%20key%20role%20in%20enhancing%20personalized%20navigation%2C%20optimizing%0Arecommendation%20systems%2C%20and%20facilitating%20urban%20mobility%20and%20planning.%20This%0Ainvolves%20predicting%20a%20user%27s%20next%20POI%20%28point-of-interest%29%20visit%20using%20their%0Apast%20visit%20history.%20However%2C%20the%20uneven%20distribution%20of%20visitations%20over%20time%0Aand%20space%2C%20namely%20the%20long-tail%20problem%20in%20spatial%20distribution%2C%20makes%20it%0Adifficult%20for%20AI%20models%20to%20predict%20those%20POIs%20that%20are%20less%20visited%20by%20humans.%0AIn%20light%20of%20this%20issue%2C%20we%20propose%20the%20Long-Tail%20Adjusted%20Next%20POI%20Prediction%0A%28LoTNext%29%20framework%20for%20mobility%20prediction%2C%20combining%20a%20Long-Tailed%20Graph%0AAdjustment%20module%20to%20reduce%20the%20impact%20of%20the%20long-tailed%20nodes%20in%20the%20user-POI%0Ainteraction%20graph%20and%20a%20novel%20Long-Tailed%20Loss%20Adjustment%20module%20to%20adjust%20loss%0Aby%20logit%20score%20and%20sample%20weight%20adjustment%20strategy.%20Also%2C%20we%20employ%20the%0Aauxiliary%20prediction%20task%20to%20enhance%20generalization%20and%20accuracy.%20Our%0Aexperiments%20with%20two%20real-world%20trajectory%20datasets%20demonstrate%20that%20LoTNext%0Asignificantly%20surpasses%20existing%20state-of-the-art%20works.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/Yukayo/LoTNext.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520the%2520Long%2520Tail%2520in%2520Human%2520Mobility%2520Prediction%26entry.906535625%3DXiaohang%2520Xu%2520and%2520Renhe%2520Jiang%2520and%2520Chuang%2520Yang%2520and%2520Zipei%2520Fan%2520and%2520Kaoru%2520Sezaki%26entry.1292438233%3D%2520%2520With%2520the%2520popularity%2520of%2520location-based%2520services%252C%2520human%2520mobility%2520prediction%250Aplays%2520a%2520key%2520role%2520in%2520enhancing%2520personalized%2520navigation%252C%2520optimizing%250Arecommendation%2520systems%252C%2520and%2520facilitating%2520urban%2520mobility%2520and%2520planning.%2520This%250Ainvolves%2520predicting%2520a%2520user%2527s%2520next%2520POI%2520%2528point-of-interest%2529%2520visit%2520using%2520their%250Apast%2520visit%2520history.%2520However%252C%2520the%2520uneven%2520distribution%2520of%2520visitations%2520over%2520time%250Aand%2520space%252C%2520namely%2520the%2520long-tail%2520problem%2520in%2520spatial%2520distribution%252C%2520makes%2520it%250Adifficult%2520for%2520AI%2520models%2520to%2520predict%2520those%2520POIs%2520that%2520are%2520less%2520visited%2520by%2520humans.%250AIn%2520light%2520of%2520this%2520issue%252C%2520we%2520propose%2520the%2520Long-Tail%2520Adjusted%2520Next%2520POI%2520Prediction%250A%2528LoTNext%2529%2520framework%2520for%2520mobility%2520prediction%252C%2520combining%2520a%2520Long-Tailed%2520Graph%250AAdjustment%2520module%2520to%2520reduce%2520the%2520impact%2520of%2520the%2520long-tailed%2520nodes%2520in%2520the%2520user-POI%250Ainteraction%2520graph%2520and%2520a%2520novel%2520Long-Tailed%2520Loss%2520Adjustment%2520module%2520to%2520adjust%2520loss%250Aby%2520logit%2520score%2520and%2520sample%2520weight%2520adjustment%2520strategy.%2520Also%252C%2520we%2520employ%2520the%250Aauxiliary%2520prediction%2520task%2520to%2520enhance%2520generalization%2520and%2520accuracy.%2520Our%250Aexperiments%2520with%2520two%2520real-world%2520trajectory%2520datasets%2520demonstrate%2520that%2520LoTNext%250Asignificantly%2520surpasses%2520existing%2520state-of-the-art%2520works.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/Yukayo/LoTNext.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20the%20Long%20Tail%20in%20Human%20Mobility%20Prediction&entry.906535625=Xiaohang%20Xu%20and%20Renhe%20Jiang%20and%20Chuang%20Yang%20and%20Zipei%20Fan%20and%20Kaoru%20Sezaki&entry.1292438233=%20%20With%20the%20popularity%20of%20location-based%20services%2C%20human%20mobility%20prediction%0Aplays%20a%20key%20role%20in%20enhancing%20personalized%20navigation%2C%20optimizing%0Arecommendation%20systems%2C%20and%20facilitating%20urban%20mobility%20and%20planning.%20This%0Ainvolves%20predicting%20a%20user%27s%20next%20POI%20%28point-of-interest%29%20visit%20using%20their%0Apast%20visit%20history.%20However%2C%20the%20uneven%20distribution%20of%20visitations%20over%20time%0Aand%20space%2C%20namely%20the%20long-tail%20problem%20in%20spatial%20distribution%2C%20makes%20it%0Adifficult%20for%20AI%20models%20to%20predict%20those%20POIs%20that%20are%20less%20visited%20by%20humans.%0AIn%20light%20of%20this%20issue%2C%20we%20propose%20the%20Long-Tail%20Adjusted%20Next%20POI%20Prediction%0A%28LoTNext%29%20framework%20for%20mobility%20prediction%2C%20combining%20a%20Long-Tailed%20Graph%0AAdjustment%20module%20to%20reduce%20the%20impact%20of%20the%20long-tailed%20nodes%20in%20the%20user-POI%0Ainteraction%20graph%20and%20a%20novel%20Long-Tailed%20Loss%20Adjustment%20module%20to%20adjust%20loss%0Aby%20logit%20score%20and%20sample%20weight%20adjustment%20strategy.%20Also%2C%20we%20employ%20the%0Aauxiliary%20prediction%20task%20to%20enhance%20generalization%20and%20accuracy.%20Our%0Aexperiments%20with%20two%20real-world%20trajectory%20datasets%20demonstrate%20that%20LoTNext%0Asignificantly%20surpasses%20existing%20state-of-the-art%20works.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/Yukayo/LoTNext.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14970v2&entry.124074799=Read"},
{"title": "MME-Finance: A Multimodal Finance Benchmark for Expert-level\n  Understanding and Reasoning", "author": "Ziliang Gan and Yu Lu and Dong Zhang and Haohan Li and Che Liu and Jian Liu and Ji Liu and Haipang Wu and Chaoyou Fu and Zenglin Xu and Rongjunchen Zhang and Yong Dai", "abstract": "  In recent years, multimodal benchmarks for general domains have guided the\nrapid development of multimodal models on general tasks. However, the financial\nfield has its peculiarities. It features unique graphical images (e.g.,\ncandlestick charts, technical indicator charts) and possesses a wealth of\nspecialized financial knowledge (e.g., futures, turnover rate). Therefore,\nbenchmarks from general fields often fail to measure the performance of\nmultimodal models in the financial domain, and thus cannot effectively guide\nthe rapid development of large financial models. To promote the development of\nlarge financial multimodal models, we propose MME-Finance, an bilingual\nopen-ended and practical usage-oriented Visual Question Answering (VQA)\nbenchmark. The characteristics of our benchmark are finance and expertise,\nwhich include constructing charts that reflect the actual usage needs of users\n(e.g., computer screenshots and mobile photography), creating questions\naccording to the preferences in financial domain inquiries, and annotating\nquestions by experts with 10+ years of experience in the financial industry.\nAdditionally, we have developed a custom-designed financial evaluation system\nin which visual information is first introduced in the multi-modal evaluation\nprocess. Extensive experimental evaluations of 19 mainstream MLLMs are\nconducted to test their perception, reasoning, and cognition capabilities. The\nresults indicate that models performing well on general benchmarks cannot do\nwell on MME-Finance; for instance, the top-performing open-source and\nclosed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),\nrespectively. Their performance is particularly poor in categories most\nrelevant to finance, such as candlestick charts and technical indicator charts.\nIn addition, we propose a Chinese version, which helps compare performance of\nMLLMs under a Chinese context.\n", "link": "http://arxiv.org/abs/2411.03314v1", "date": "2024-11-05", "relevancy": 2.0485, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-Finance%3A%20A%20Multimodal%20Finance%20Benchmark%20for%20Expert-level%0A%20%20Understanding%20and%20Reasoning&body=Title%3A%20MME-Finance%3A%20A%20Multimodal%20Finance%20Benchmark%20for%20Expert-level%0A%20%20Understanding%20and%20Reasoning%0AAuthor%3A%20Ziliang%20Gan%20and%20Yu%20Lu%20and%20Dong%20Zhang%20and%20Haohan%20Li%20and%20Che%20Liu%20and%20Jian%20Liu%20and%20Ji%20Liu%20and%20Haipang%20Wu%20and%20Chaoyou%20Fu%20and%20Zenglin%20Xu%20and%20Rongjunchen%20Zhang%20and%20Yong%20Dai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20multimodal%20benchmarks%20for%20general%20domains%20have%20guided%20the%0Arapid%20development%20of%20multimodal%20models%20on%20general%20tasks.%20However%2C%20the%20financial%0Afield%20has%20its%20peculiarities.%20It%20features%20unique%20graphical%20images%20%28e.g.%2C%0Acandlestick%20charts%2C%20technical%20indicator%20charts%29%20and%20possesses%20a%20wealth%20of%0Aspecialized%20financial%20knowledge%20%28e.g.%2C%20futures%2C%20turnover%20rate%29.%20Therefore%2C%0Abenchmarks%20from%20general%20fields%20often%20fail%20to%20measure%20the%20performance%20of%0Amultimodal%20models%20in%20the%20financial%20domain%2C%20and%20thus%20cannot%20effectively%20guide%0Athe%20rapid%20development%20of%20large%20financial%20models.%20To%20promote%20the%20development%20of%0Alarge%20financial%20multimodal%20models%2C%20we%20propose%20MME-Finance%2C%20an%20bilingual%0Aopen-ended%20and%20practical%20usage-oriented%20Visual%20Question%20Answering%20%28VQA%29%0Abenchmark.%20The%20characteristics%20of%20our%20benchmark%20are%20finance%20and%20expertise%2C%0Awhich%20include%20constructing%20charts%20that%20reflect%20the%20actual%20usage%20needs%20of%20users%0A%28e.g.%2C%20computer%20screenshots%20and%20mobile%20photography%29%2C%20creating%20questions%0Aaccording%20to%20the%20preferences%20in%20financial%20domain%20inquiries%2C%20and%20annotating%0Aquestions%20by%20experts%20with%2010%2B%20years%20of%20experience%20in%20the%20financial%20industry.%0AAdditionally%2C%20we%20have%20developed%20a%20custom-designed%20financial%20evaluation%20system%0Ain%20which%20visual%20information%20is%20first%20introduced%20in%20the%20multi-modal%20evaluation%0Aprocess.%20Extensive%20experimental%20evaluations%20of%2019%20mainstream%20MLLMs%20are%0Aconducted%20to%20test%20their%20perception%2C%20reasoning%2C%20and%20cognition%20capabilities.%20The%0Aresults%20indicate%20that%20models%20performing%20well%20on%20general%20benchmarks%20cannot%20do%0Awell%20on%20MME-Finance%3B%20for%20instance%2C%20the%20top-performing%20open-source%20and%0Aclosed-source%20models%20obtain%2065.69%20%28Qwen2VL-72B%29%20and%2063.18%20%28GPT-4o%29%2C%0Arespectively.%20Their%20performance%20is%20particularly%20poor%20in%20categories%20most%0Arelevant%20to%20finance%2C%20such%20as%20candlestick%20charts%20and%20technical%20indicator%20charts.%0AIn%20addition%2C%20we%20propose%20a%20Chinese%20version%2C%20which%20helps%20compare%20performance%20of%0AMLLMs%20under%20a%20Chinese%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-Finance%253A%2520A%2520Multimodal%2520Finance%2520Benchmark%2520for%2520Expert-level%250A%2520%2520Understanding%2520and%2520Reasoning%26entry.906535625%3DZiliang%2520Gan%2520and%2520Yu%2520Lu%2520and%2520Dong%2520Zhang%2520and%2520Haohan%2520Li%2520and%2520Che%2520Liu%2520and%2520Jian%2520Liu%2520and%2520Ji%2520Liu%2520and%2520Haipang%2520Wu%2520and%2520Chaoyou%2520Fu%2520and%2520Zenglin%2520Xu%2520and%2520Rongjunchen%2520Zhang%2520and%2520Yong%2520Dai%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520multimodal%2520benchmarks%2520for%2520general%2520domains%2520have%2520guided%2520the%250Arapid%2520development%2520of%2520multimodal%2520models%2520on%2520general%2520tasks.%2520However%252C%2520the%2520financial%250Afield%2520has%2520its%2520peculiarities.%2520It%2520features%2520unique%2520graphical%2520images%2520%2528e.g.%252C%250Acandlestick%2520charts%252C%2520technical%2520indicator%2520charts%2529%2520and%2520possesses%2520a%2520wealth%2520of%250Aspecialized%2520financial%2520knowledge%2520%2528e.g.%252C%2520futures%252C%2520turnover%2520rate%2529.%2520Therefore%252C%250Abenchmarks%2520from%2520general%2520fields%2520often%2520fail%2520to%2520measure%2520the%2520performance%2520of%250Amultimodal%2520models%2520in%2520the%2520financial%2520domain%252C%2520and%2520thus%2520cannot%2520effectively%2520guide%250Athe%2520rapid%2520development%2520of%2520large%2520financial%2520models.%2520To%2520promote%2520the%2520development%2520of%250Alarge%2520financial%2520multimodal%2520models%252C%2520we%2520propose%2520MME-Finance%252C%2520an%2520bilingual%250Aopen-ended%2520and%2520practical%2520usage-oriented%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Abenchmark.%2520The%2520characteristics%2520of%2520our%2520benchmark%2520are%2520finance%2520and%2520expertise%252C%250Awhich%2520include%2520constructing%2520charts%2520that%2520reflect%2520the%2520actual%2520usage%2520needs%2520of%2520users%250A%2528e.g.%252C%2520computer%2520screenshots%2520and%2520mobile%2520photography%2529%252C%2520creating%2520questions%250Aaccording%2520to%2520the%2520preferences%2520in%2520financial%2520domain%2520inquiries%252C%2520and%2520annotating%250Aquestions%2520by%2520experts%2520with%252010%252B%2520years%2520of%2520experience%2520in%2520the%2520financial%2520industry.%250AAdditionally%252C%2520we%2520have%2520developed%2520a%2520custom-designed%2520financial%2520evaluation%2520system%250Ain%2520which%2520visual%2520information%2520is%2520first%2520introduced%2520in%2520the%2520multi-modal%2520evaluation%250Aprocess.%2520Extensive%2520experimental%2520evaluations%2520of%252019%2520mainstream%2520MLLMs%2520are%250Aconducted%2520to%2520test%2520their%2520perception%252C%2520reasoning%252C%2520and%2520cognition%2520capabilities.%2520The%250Aresults%2520indicate%2520that%2520models%2520performing%2520well%2520on%2520general%2520benchmarks%2520cannot%2520do%250Awell%2520on%2520MME-Finance%253B%2520for%2520instance%252C%2520the%2520top-performing%2520open-source%2520and%250Aclosed-source%2520models%2520obtain%252065.69%2520%2528Qwen2VL-72B%2529%2520and%252063.18%2520%2528GPT-4o%2529%252C%250Arespectively.%2520Their%2520performance%2520is%2520particularly%2520poor%2520in%2520categories%2520most%250Arelevant%2520to%2520finance%252C%2520such%2520as%2520candlestick%2520charts%2520and%2520technical%2520indicator%2520charts.%250AIn%2520addition%252C%2520we%2520propose%2520a%2520Chinese%2520version%252C%2520which%2520helps%2520compare%2520performance%2520of%250AMLLMs%2520under%2520a%2520Chinese%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-Finance%3A%20A%20Multimodal%20Finance%20Benchmark%20for%20Expert-level%0A%20%20Understanding%20and%20Reasoning&entry.906535625=Ziliang%20Gan%20and%20Yu%20Lu%20and%20Dong%20Zhang%20and%20Haohan%20Li%20and%20Che%20Liu%20and%20Jian%20Liu%20and%20Ji%20Liu%20and%20Haipang%20Wu%20and%20Chaoyou%20Fu%20and%20Zenglin%20Xu%20and%20Rongjunchen%20Zhang%20and%20Yong%20Dai&entry.1292438233=%20%20In%20recent%20years%2C%20multimodal%20benchmarks%20for%20general%20domains%20have%20guided%20the%0Arapid%20development%20of%20multimodal%20models%20on%20general%20tasks.%20However%2C%20the%20financial%0Afield%20has%20its%20peculiarities.%20It%20features%20unique%20graphical%20images%20%28e.g.%2C%0Acandlestick%20charts%2C%20technical%20indicator%20charts%29%20and%20possesses%20a%20wealth%20of%0Aspecialized%20financial%20knowledge%20%28e.g.%2C%20futures%2C%20turnover%20rate%29.%20Therefore%2C%0Abenchmarks%20from%20general%20fields%20often%20fail%20to%20measure%20the%20performance%20of%0Amultimodal%20models%20in%20the%20financial%20domain%2C%20and%20thus%20cannot%20effectively%20guide%0Athe%20rapid%20development%20of%20large%20financial%20models.%20To%20promote%20the%20development%20of%0Alarge%20financial%20multimodal%20models%2C%20we%20propose%20MME-Finance%2C%20an%20bilingual%0Aopen-ended%20and%20practical%20usage-oriented%20Visual%20Question%20Answering%20%28VQA%29%0Abenchmark.%20The%20characteristics%20of%20our%20benchmark%20are%20finance%20and%20expertise%2C%0Awhich%20include%20constructing%20charts%20that%20reflect%20the%20actual%20usage%20needs%20of%20users%0A%28e.g.%2C%20computer%20screenshots%20and%20mobile%20photography%29%2C%20creating%20questions%0Aaccording%20to%20the%20preferences%20in%20financial%20domain%20inquiries%2C%20and%20annotating%0Aquestions%20by%20experts%20with%2010%2B%20years%20of%20experience%20in%20the%20financial%20industry.%0AAdditionally%2C%20we%20have%20developed%20a%20custom-designed%20financial%20evaluation%20system%0Ain%20which%20visual%20information%20is%20first%20introduced%20in%20the%20multi-modal%20evaluation%0Aprocess.%20Extensive%20experimental%20evaluations%20of%2019%20mainstream%20MLLMs%20are%0Aconducted%20to%20test%20their%20perception%2C%20reasoning%2C%20and%20cognition%20capabilities.%20The%0Aresults%20indicate%20that%20models%20performing%20well%20on%20general%20benchmarks%20cannot%20do%0Awell%20on%20MME-Finance%3B%20for%20instance%2C%20the%20top-performing%20open-source%20and%0Aclosed-source%20models%20obtain%2065.69%20%28Qwen2VL-72B%29%20and%2063.18%20%28GPT-4o%29%2C%0Arespectively.%20Their%20performance%20is%20particularly%20poor%20in%20categories%20most%0Arelevant%20to%20finance%2C%20such%20as%20candlestick%20charts%20and%20technical%20indicator%20charts.%0AIn%20addition%2C%20we%20propose%20a%20Chinese%20version%2C%20which%20helps%20compare%20performance%20of%0AMLLMs%20under%20a%20Chinese%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03314v1&entry.124074799=Read"},
{"title": "ATM: Improving Model Merging by Alternating Tuning and Merging", "author": "Luca Zhou and Daniele Solombrino and Donato Crisostomi and Maria Sofia Bucarelli and Fabrizio Silvestri and Emanuele Rodol\u00e0", "abstract": "  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, task vectors are mathematically equivalent to the\ngradients obtained via gradient descent in a multi-task setting, and still\napproximate these gradients in subsequent epochs. Furthermore, we show that\ntask vectors perform optimally when equality is maintained, and their\neffectiveness is largely driven by the first epoch's gradient. Building on this\ninsight, we propose viewing model merging as a single step in an iterative\nprocess that Alternates between Tuning and Merging (ATM). This method acts as a\nbridge between model merging and multi-task gradient descent, achieving\nstate-of-the-art results with the same data and computational requirements. We\nextensively evaluate ATM across diverse settings, achieving up to 20% higher\naccuracy in computer vision and NLP tasks, compared to the best\nbaselines.Finally, we provide both empirical and theoretical support for its\neffectiveness, demonstrating increased orthogonality between task vectors and\nproving that ATM minimizes an upper bound on the loss obtained by jointly\nfinetuning all tasks.\n", "link": "http://arxiv.org/abs/2411.03055v1", "date": "2024-11-05", "relevancy": 2.047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&body=Title%3A%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging%0AAuthor%3A%20Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Model%20merging%20has%20recently%20emerged%20as%20a%20cost-efficient%20paradigm%20for%0Amulti-task%20learning.%20Among%20current%20approaches%2C%20task%20arithmetic%20stands%20out%20for%0Aits%20simplicity%20and%20effectiveness.%20In%20this%20paper%2C%20we%20motivate%20the%20effectiveness%0Aof%20task%20vectors%20by%20linking%20them%20to%20multi-task%20gradients.%20We%20show%20that%20in%20a%0Asingle-epoch%20scenario%2C%20task%20vectors%20are%20mathematically%20equivalent%20to%20the%0Agradients%20obtained%20via%20gradient%20descent%20in%20a%20multi-task%20setting%2C%20and%20still%0Aapproximate%20these%20gradients%20in%20subsequent%20epochs.%20Furthermore%2C%20we%20show%20that%0Atask%20vectors%20perform%20optimally%20when%20equality%20is%20maintained%2C%20and%20their%0Aeffectiveness%20is%20largely%20driven%20by%20the%20first%20epoch%27s%20gradient.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20viewing%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%0Aprocess%20that%20Alternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20This%20method%20acts%20as%20a%0Abridge%20between%20model%20merging%20and%20multi-task%20gradient%20descent%2C%20achieving%0Astate-of-the-art%20results%20with%20the%20same%20data%20and%20computational%20requirements.%20We%0Aextensively%20evaluate%20ATM%20across%20diverse%20settings%2C%20achieving%20up%20to%2020%25%20higher%0Aaccuracy%20in%20computer%20vision%20and%20NLP%20tasks%2C%20compared%20to%20the%20best%0Abaselines.Finally%2C%20we%20provide%20both%20empirical%20and%20theoretical%20support%20for%20its%0Aeffectiveness%2C%20demonstrating%20increased%20orthogonality%20between%20task%20vectors%20and%0Aproving%20that%20ATM%20minimizes%20an%20upper%20bound%20on%20the%20loss%20obtained%20by%20jointly%0Afinetuning%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATM%253A%2520Improving%2520Model%2520Merging%2520by%2520Alternating%2520Tuning%2520and%2520Merging%26entry.906535625%3DLuca%2520Zhou%2520and%2520Daniele%2520Solombrino%2520and%2520Donato%2520Crisostomi%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Fabrizio%2520Silvestri%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520recently%2520emerged%2520as%2520a%2520cost-efficient%2520paradigm%2520for%250Amulti-task%2520learning.%2520Among%2520current%2520approaches%252C%2520task%2520arithmetic%2520stands%2520out%2520for%250Aits%2520simplicity%2520and%2520effectiveness.%2520In%2520this%2520paper%252C%2520we%2520motivate%2520the%2520effectiveness%250Aof%2520task%2520vectors%2520by%2520linking%2520them%2520to%2520multi-task%2520gradients.%2520We%2520show%2520that%2520in%2520a%250Asingle-epoch%2520scenario%252C%2520task%2520vectors%2520are%2520mathematically%2520equivalent%2520to%2520the%250Agradients%2520obtained%2520via%2520gradient%2520descent%2520in%2520a%2520multi-task%2520setting%252C%2520and%2520still%250Aapproximate%2520these%2520gradients%2520in%2520subsequent%2520epochs.%2520Furthermore%252C%2520we%2520show%2520that%250Atask%2520vectors%2520perform%2520optimally%2520when%2520equality%2520is%2520maintained%252C%2520and%2520their%250Aeffectiveness%2520is%2520largely%2520driven%2520by%2520the%2520first%2520epoch%2527s%2520gradient.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520viewing%2520model%2520merging%2520as%2520a%2520single%2520step%2520in%2520an%2520iterative%250Aprocess%2520that%2520Alternates%2520between%2520Tuning%2520and%2520Merging%2520%2528ATM%2529.%2520This%2520method%2520acts%2520as%2520a%250Abridge%2520between%2520model%2520merging%2520and%2520multi-task%2520gradient%2520descent%252C%2520achieving%250Astate-of-the-art%2520results%2520with%2520the%2520same%2520data%2520and%2520computational%2520requirements.%2520We%250Aextensively%2520evaluate%2520ATM%2520across%2520diverse%2520settings%252C%2520achieving%2520up%2520to%252020%2525%2520higher%250Aaccuracy%2520in%2520computer%2520vision%2520and%2520NLP%2520tasks%252C%2520compared%2520to%2520the%2520best%250Abaselines.Finally%252C%2520we%2520provide%2520both%2520empirical%2520and%2520theoretical%2520support%2520for%2520its%250Aeffectiveness%252C%2520demonstrating%2520increased%2520orthogonality%2520between%2520task%2520vectors%2520and%250Aproving%2520that%2520ATM%2520minimizes%2520an%2520upper%2520bound%2520on%2520the%2520loss%2520obtained%2520by%2520jointly%250Afinetuning%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&entry.906535625=Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Model%20merging%20has%20recently%20emerged%20as%20a%20cost-efficient%20paradigm%20for%0Amulti-task%20learning.%20Among%20current%20approaches%2C%20task%20arithmetic%20stands%20out%20for%0Aits%20simplicity%20and%20effectiveness.%20In%20this%20paper%2C%20we%20motivate%20the%20effectiveness%0Aof%20task%20vectors%20by%20linking%20them%20to%20multi-task%20gradients.%20We%20show%20that%20in%20a%0Asingle-epoch%20scenario%2C%20task%20vectors%20are%20mathematically%20equivalent%20to%20the%0Agradients%20obtained%20via%20gradient%20descent%20in%20a%20multi-task%20setting%2C%20and%20still%0Aapproximate%20these%20gradients%20in%20subsequent%20epochs.%20Furthermore%2C%20we%20show%20that%0Atask%20vectors%20perform%20optimally%20when%20equality%20is%20maintained%2C%20and%20their%0Aeffectiveness%20is%20largely%20driven%20by%20the%20first%20epoch%27s%20gradient.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20viewing%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%0Aprocess%20that%20Alternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20This%20method%20acts%20as%20a%0Abridge%20between%20model%20merging%20and%20multi-task%20gradient%20descent%2C%20achieving%0Astate-of-the-art%20results%20with%20the%20same%20data%20and%20computational%20requirements.%20We%0Aextensively%20evaluate%20ATM%20across%20diverse%20settings%2C%20achieving%20up%20to%2020%25%20higher%0Aaccuracy%20in%20computer%20vision%20and%20NLP%20tasks%2C%20compared%20to%20the%20best%0Abaselines.Finally%2C%20we%20provide%20both%20empirical%20and%20theoretical%20support%20for%20its%0Aeffectiveness%2C%20demonstrating%20increased%20orthogonality%20between%20task%20vectors%20and%0Aproving%20that%20ATM%20minimizes%20an%20upper%20bound%20on%20the%20loss%20obtained%20by%20jointly%0Afinetuning%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03055v1&entry.124074799=Read"},
{"title": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models", "author": "Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng", "abstract": "  Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.\n", "link": "http://arxiv.org/abs/2406.14550v2", "date": "2024-11-05", "relevancy": 2.0327, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models&body=Title%3A%20GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Shilong%20Li%20and%20Yancheng%20He%20and%20Hangyu%20Guo%20and%20Xingyuan%20Bu%20and%20Ge%20Bai%20and%20Jie%20Liu%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yangguang%20Li%20and%20Wanli%20Ouyang%20and%20Wenbo%20Su%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Long-context%20capabilities%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%0Atackle%20complex%20and%20long-input%20tasks.%20Despite%20numerous%20efforts%20made%20to%20optimize%0ALLMs%20for%20long%20contexts%2C%20challenges%20persist%20in%20robustly%20processing%20long%20inputs.%0AIn%20this%20paper%2C%20we%20introduce%20GraphReader%2C%20a%20graph-based%20agent%20system%20designed%20to%0Ahandle%20long%20texts%20by%20structuring%20them%20into%20a%20graph%20and%20employing%20an%20agent%20to%0Aexplore%20this%20graph%20autonomously.%20Upon%20receiving%20a%20question%2C%20the%20agent%20first%0Aundertakes%20a%20step-by-step%20analysis%20and%20devises%20a%20rational%20plan.%20It%20then%20invokes%0Aa%20set%20of%20predefined%20functions%20to%20read%20node%20content%20and%20neighbors%2C%20facilitating%0Aa%20coarse-to-fine%20exploration%20of%20the%20graph.%20Throughout%20the%20exploration%2C%20the%0Aagent%20continuously%20records%20new%20insights%20and%20reflects%20on%20current%20circumstances%0Ato%20optimize%20the%20process%20until%20it%20has%20gathered%20sufficient%20information%20to%0Agenerate%20an%20answer.%20Experimental%20results%20on%20the%20LV-Eval%20dataset%20reveal%20that%0AGraphReader%2C%20using%20a%204k%20context%20window%2C%20consistently%20outperforms%20GPT-4-128k%0Aacross%20context%20lengths%20from%2016k%20to%20256k%20by%20a%20large%20margin.%20Additionally%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20on%20four%20challenging%20single-hop%20and%0Amulti-hop%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphReader%253A%2520Building%2520Graph-based%2520Agent%2520to%2520Enhance%2520Long-Context%250A%2520%2520Abilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DShilong%2520Li%2520and%2520Yancheng%2520He%2520and%2520Hangyu%2520Guo%2520and%2520Xingyuan%2520Bu%2520and%2520Ge%2520Bai%2520and%2520Jie%2520Liu%2520and%2520Jiaheng%2520Liu%2520and%2520Xingwei%2520Qu%2520and%2520Yangguang%2520Li%2520and%2520Wanli%2520Ouyang%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Long-context%2520capabilities%2520are%2520essential%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Atackle%2520complex%2520and%2520long-input%2520tasks.%2520Despite%2520numerous%2520efforts%2520made%2520to%2520optimize%250ALLMs%2520for%2520long%2520contexts%252C%2520challenges%2520persist%2520in%2520robustly%2520processing%2520long%2520inputs.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520GraphReader%252C%2520a%2520graph-based%2520agent%2520system%2520designed%2520to%250Ahandle%2520long%2520texts%2520by%2520structuring%2520them%2520into%2520a%2520graph%2520and%2520employing%2520an%2520agent%2520to%250Aexplore%2520this%2520graph%2520autonomously.%2520Upon%2520receiving%2520a%2520question%252C%2520the%2520agent%2520first%250Aundertakes%2520a%2520step-by-step%2520analysis%2520and%2520devises%2520a%2520rational%2520plan.%2520It%2520then%2520invokes%250Aa%2520set%2520of%2520predefined%2520functions%2520to%2520read%2520node%2520content%2520and%2520neighbors%252C%2520facilitating%250Aa%2520coarse-to-fine%2520exploration%2520of%2520the%2520graph.%2520Throughout%2520the%2520exploration%252C%2520the%250Aagent%2520continuously%2520records%2520new%2520insights%2520and%2520reflects%2520on%2520current%2520circumstances%250Ato%2520optimize%2520the%2520process%2520until%2520it%2520has%2520gathered%2520sufficient%2520information%2520to%250Agenerate%2520an%2520answer.%2520Experimental%2520results%2520on%2520the%2520LV-Eval%2520dataset%2520reveal%2520that%250AGraphReader%252C%2520using%2520a%25204k%2520context%2520window%252C%2520consistently%2520outperforms%2520GPT-4-128k%250Aacross%2520context%2520lengths%2520from%252016k%2520to%2520256k%2520by%2520a%2520large%2520margin.%2520Additionally%252C%2520our%250Aapproach%2520demonstrates%2520superior%2520performance%2520on%2520four%2520challenging%2520single-hop%2520and%250Amulti-hop%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models&entry.906535625=Shilong%20Li%20and%20Yancheng%20He%20and%20Hangyu%20Guo%20and%20Xingyuan%20Bu%20and%20Ge%20Bai%20and%20Jie%20Liu%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yangguang%20Li%20and%20Wanli%20Ouyang%20and%20Wenbo%20Su%20and%20Bo%20Zheng&entry.1292438233=%20%20Long-context%20capabilities%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%0Atackle%20complex%20and%20long-input%20tasks.%20Despite%20numerous%20efforts%20made%20to%20optimize%0ALLMs%20for%20long%20contexts%2C%20challenges%20persist%20in%20robustly%20processing%20long%20inputs.%0AIn%20this%20paper%2C%20we%20introduce%20GraphReader%2C%20a%20graph-based%20agent%20system%20designed%20to%0Ahandle%20long%20texts%20by%20structuring%20them%20into%20a%20graph%20and%20employing%20an%20agent%20to%0Aexplore%20this%20graph%20autonomously.%20Upon%20receiving%20a%20question%2C%20the%20agent%20first%0Aundertakes%20a%20step-by-step%20analysis%20and%20devises%20a%20rational%20plan.%20It%20then%20invokes%0Aa%20set%20of%20predefined%20functions%20to%20read%20node%20content%20and%20neighbors%2C%20facilitating%0Aa%20coarse-to-fine%20exploration%20of%20the%20graph.%20Throughout%20the%20exploration%2C%20the%0Aagent%20continuously%20records%20new%20insights%20and%20reflects%20on%20current%20circumstances%0Ato%20optimize%20the%20process%20until%20it%20has%20gathered%20sufficient%20information%20to%0Agenerate%20an%20answer.%20Experimental%20results%20on%20the%20LV-Eval%20dataset%20reveal%20that%0AGraphReader%2C%20using%20a%204k%20context%20window%2C%20consistently%20outperforms%20GPT-4-128k%0Aacross%20context%20lengths%20from%2016k%20to%20256k%20by%20a%20large%20margin.%20Additionally%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20on%20four%20challenging%20single-hop%20and%0Amulti-hop%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14550v2&entry.124074799=Read"},
{"title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale", "author": "Huy Nhat Phan and Tien N. Nguyen and Phong X. Nguyen and Nghi D. Q. Bui", "abstract": "  Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2409.16299v2", "date": "2024-11-05", "relevancy": 2.0174, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5198}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5014}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperAgent%3A%20Generalist%20Software%20Engineering%20Agents%20to%20Solve%20Coding%20Tasks%0A%20%20at%20Scale&body=Title%3A%20HyperAgent%3A%20Generalist%20Software%20Engineering%20Agents%20to%20Solve%20Coding%20Tasks%0A%20%20at%20Scale%0AAuthor%3A%20Huy%20Nhat%20Phan%20and%20Tien%20N.%20Nguyen%20and%20Phong%20X.%20Nguyen%20and%20Nghi%20D.%20Q.%20Bui%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20software%20engineering%20%28SE%29%2C%0Ashowcasing%20remarkable%20proficiency%20in%20various%20coding%20tasks.%20Despite%20recent%0Aadvancements%20that%20have%20enabled%20the%20creation%20of%20autonomous%20software%20agents%0Autilizing%20LLMs%20for%20end-to-end%20development%20tasks%2C%20these%20systems%20are%20typically%0Adesigned%20for%20specific%20SE%20functions.%20We%20introduce%20HyperAgent%2C%20an%20innovative%0Ageneralist%20multi-agent%20system%20designed%20to%20tackle%20a%20wide%20range%20of%20SE%20tasks%0Aacross%20different%20programming%20languages%20by%20mimicking%20the%20workflows%20of%20human%0Adevelopers.%20HyperAgent%20features%20four%20specialized%20agents-Planner%2C%20Navigator%2C%0ACode%20Editor%2C%20and%20Executor-capable%20of%20handling%20the%20entire%20lifecycle%20of%20SE%20tasks%2C%0Afrom%20initial%20planning%20to%20final%20verification.%20HyperAgent%20sets%20new%20benchmarks%20in%0Adiverse%20SE%20tasks%2C%20including%20GitHub%20issue%20resolution%20on%20the%20renowned%20SWE-Bench%0Abenchmark%2C%20outperforming%20robust%20baselines.%20Furthermore%2C%20HyperAgent%20demonstrates%0Aexceptional%20performance%20in%20repository-level%20code%20generation%20%28RepoExec%29%20and%0Afault%20localization%20and%20program%20repair%20%28Defects4J%29%2C%20often%20surpassing%0Astate-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperAgent%253A%2520Generalist%2520Software%2520Engineering%2520Agents%2520to%2520Solve%2520Coding%2520Tasks%250A%2520%2520at%2520Scale%26entry.906535625%3DHuy%2520Nhat%2520Phan%2520and%2520Tien%2520N.%2520Nguyen%2520and%2520Phong%2520X.%2520Nguyen%2520and%2520Nghi%2520D.%2520Q.%2520Bui%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520software%2520engineering%2520%2528SE%2529%252C%250Ashowcasing%2520remarkable%2520proficiency%2520in%2520various%2520coding%2520tasks.%2520Despite%2520recent%250Aadvancements%2520that%2520have%2520enabled%2520the%2520creation%2520of%2520autonomous%2520software%2520agents%250Autilizing%2520LLMs%2520for%2520end-to-end%2520development%2520tasks%252C%2520these%2520systems%2520are%2520typically%250Adesigned%2520for%2520specific%2520SE%2520functions.%2520We%2520introduce%2520HyperAgent%252C%2520an%2520innovative%250Ageneralist%2520multi-agent%2520system%2520designed%2520to%2520tackle%2520a%2520wide%2520range%2520of%2520SE%2520tasks%250Aacross%2520different%2520programming%2520languages%2520by%2520mimicking%2520the%2520workflows%2520of%2520human%250Adevelopers.%2520HyperAgent%2520features%2520four%2520specialized%2520agents-Planner%252C%2520Navigator%252C%250ACode%2520Editor%252C%2520and%2520Executor-capable%2520of%2520handling%2520the%2520entire%2520lifecycle%2520of%2520SE%2520tasks%252C%250Afrom%2520initial%2520planning%2520to%2520final%2520verification.%2520HyperAgent%2520sets%2520new%2520benchmarks%2520in%250Adiverse%2520SE%2520tasks%252C%2520including%2520GitHub%2520issue%2520resolution%2520on%2520the%2520renowned%2520SWE-Bench%250Abenchmark%252C%2520outperforming%2520robust%2520baselines.%2520Furthermore%252C%2520HyperAgent%2520demonstrates%250Aexceptional%2520performance%2520in%2520repository-level%2520code%2520generation%2520%2528RepoExec%2529%2520and%250Afault%2520localization%2520and%2520program%2520repair%2520%2528Defects4J%2529%252C%2520often%2520surpassing%250Astate-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperAgent%3A%20Generalist%20Software%20Engineering%20Agents%20to%20Solve%20Coding%20Tasks%0A%20%20at%20Scale&entry.906535625=Huy%20Nhat%20Phan%20and%20Tien%20N.%20Nguyen%20and%20Phong%20X.%20Nguyen%20and%20Nghi%20D.%20Q.%20Bui&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20software%20engineering%20%28SE%29%2C%0Ashowcasing%20remarkable%20proficiency%20in%20various%20coding%20tasks.%20Despite%20recent%0Aadvancements%20that%20have%20enabled%20the%20creation%20of%20autonomous%20software%20agents%0Autilizing%20LLMs%20for%20end-to-end%20development%20tasks%2C%20these%20systems%20are%20typically%0Adesigned%20for%20specific%20SE%20functions.%20We%20introduce%20HyperAgent%2C%20an%20innovative%0Ageneralist%20multi-agent%20system%20designed%20to%20tackle%20a%20wide%20range%20of%20SE%20tasks%0Aacross%20different%20programming%20languages%20by%20mimicking%20the%20workflows%20of%20human%0Adevelopers.%20HyperAgent%20features%20four%20specialized%20agents-Planner%2C%20Navigator%2C%0ACode%20Editor%2C%20and%20Executor-capable%20of%20handling%20the%20entire%20lifecycle%20of%20SE%20tasks%2C%0Afrom%20initial%20planning%20to%20final%20verification.%20HyperAgent%20sets%20new%20benchmarks%20in%0Adiverse%20SE%20tasks%2C%20including%20GitHub%20issue%20resolution%20on%20the%20renowned%20SWE-Bench%0Abenchmark%2C%20outperforming%20robust%20baselines.%20Furthermore%2C%20HyperAgent%20demonstrates%0Aexceptional%20performance%20in%20repository-level%20code%20generation%20%28RepoExec%29%20and%0Afault%20localization%20and%20program%20repair%20%28Defects4J%29%2C%20often%20surpassing%0Astate-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16299v2&entry.124074799=Read"},
{"title": "Can Transformers Smell Like Humans?", "author": "Farzaneh Taleb and Miguel Vasco and Ant\u00f4nio H. Ribeiro and M\u00e5rten Bj\u00f6rkman and Danica Kragic", "abstract": "  The human brain encodes stimuli from the environment into representations\nthat form a sensory perception of the world. Despite recent advances in\nunderstanding visual and auditory perception, olfactory perception remains an\nunder-explored topic in the machine learning community due to the lack of\nlarge-scale datasets annotated with labels of human olfactory perception. In\nthis work, we ask the question of whether pre-trained transformer models of\nchemical structures encode representations that are aligned with human\nolfactory perception, i.e., can transformers smell like humans? We demonstrate\nthat representations encoded from transformers pre-trained on general chemical\nstructures are highly aligned with human olfactory perception. We use multiple\ndatasets and different types of perceptual representations to show that the\nrepresentations encoded by transformer models are able to predict: (i) labels\nassociated with odorants provided by experts; (ii) continuous ratings provided\nby human participants with respect to pre-defined descriptors; and (iii)\nsimilarity ratings between odorants provided by human participants. Finally, we\nevaluate the extent to which this alignment is associated with physicochemical\nfeatures of odorants known to be relevant for olfactory decoding.\n", "link": "http://arxiv.org/abs/2411.03038v1", "date": "2024-11-05", "relevancy": 2.0147, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Transformers%20Smell%20Like%20Humans%3F&body=Title%3A%20Can%20Transformers%20Smell%20Like%20Humans%3F%0AAuthor%3A%20Farzaneh%20Taleb%20and%20Miguel%20Vasco%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20The%20human%20brain%20encodes%20stimuli%20from%20the%20environment%20into%20representations%0Athat%20form%20a%20sensory%20perception%20of%20the%20world.%20Despite%20recent%20advances%20in%0Aunderstanding%20visual%20and%20auditory%20perception%2C%20olfactory%20perception%20remains%20an%0Aunder-explored%20topic%20in%20the%20machine%20learning%20community%20due%20to%20the%20lack%20of%0Alarge-scale%20datasets%20annotated%20with%20labels%20of%20human%20olfactory%20perception.%20In%0Athis%20work%2C%20we%20ask%20the%20question%20of%20whether%20pre-trained%20transformer%20models%20of%0Achemical%20structures%20encode%20representations%20that%20are%20aligned%20with%20human%0Aolfactory%20perception%2C%20i.e.%2C%20can%20transformers%20smell%20like%20humans%3F%20We%20demonstrate%0Athat%20representations%20encoded%20from%20transformers%20pre-trained%20on%20general%20chemical%0Astructures%20are%20highly%20aligned%20with%20human%20olfactory%20perception.%20We%20use%20multiple%0Adatasets%20and%20different%20types%20of%20perceptual%20representations%20to%20show%20that%20the%0Arepresentations%20encoded%20by%20transformer%20models%20are%20able%20to%20predict%3A%20%28i%29%20labels%0Aassociated%20with%20odorants%20provided%20by%20experts%3B%20%28ii%29%20continuous%20ratings%20provided%0Aby%20human%20participants%20with%20respect%20to%20pre-defined%20descriptors%3B%20and%20%28iii%29%0Asimilarity%20ratings%20between%20odorants%20provided%20by%20human%20participants.%20Finally%2C%20we%0Aevaluate%20the%20extent%20to%20which%20this%20alignment%20is%20associated%20with%20physicochemical%0Afeatures%20of%20odorants%20known%20to%20be%20relevant%20for%20olfactory%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Transformers%2520Smell%2520Like%2520Humans%253F%26entry.906535625%3DFarzaneh%2520Taleb%2520and%2520Miguel%2520Vasco%2520and%2520Ant%25C3%25B4nio%2520H.%2520Ribeiro%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520encodes%2520stimuli%2520from%2520the%2520environment%2520into%2520representations%250Athat%2520form%2520a%2520sensory%2520perception%2520of%2520the%2520world.%2520Despite%2520recent%2520advances%2520in%250Aunderstanding%2520visual%2520and%2520auditory%2520perception%252C%2520olfactory%2520perception%2520remains%2520an%250Aunder-explored%2520topic%2520in%2520the%2520machine%2520learning%2520community%2520due%2520to%2520the%2520lack%2520of%250Alarge-scale%2520datasets%2520annotated%2520with%2520labels%2520of%2520human%2520olfactory%2520perception.%2520In%250Athis%2520work%252C%2520we%2520ask%2520the%2520question%2520of%2520whether%2520pre-trained%2520transformer%2520models%2520of%250Achemical%2520structures%2520encode%2520representations%2520that%2520are%2520aligned%2520with%2520human%250Aolfactory%2520perception%252C%2520i.e.%252C%2520can%2520transformers%2520smell%2520like%2520humans%253F%2520We%2520demonstrate%250Athat%2520representations%2520encoded%2520from%2520transformers%2520pre-trained%2520on%2520general%2520chemical%250Astructures%2520are%2520highly%2520aligned%2520with%2520human%2520olfactory%2520perception.%2520We%2520use%2520multiple%250Adatasets%2520and%2520different%2520types%2520of%2520perceptual%2520representations%2520to%2520show%2520that%2520the%250Arepresentations%2520encoded%2520by%2520transformer%2520models%2520are%2520able%2520to%2520predict%253A%2520%2528i%2529%2520labels%250Aassociated%2520with%2520odorants%2520provided%2520by%2520experts%253B%2520%2528ii%2529%2520continuous%2520ratings%2520provided%250Aby%2520human%2520participants%2520with%2520respect%2520to%2520pre-defined%2520descriptors%253B%2520and%2520%2528iii%2529%250Asimilarity%2520ratings%2520between%2520odorants%2520provided%2520by%2520human%2520participants.%2520Finally%252C%2520we%250Aevaluate%2520the%2520extent%2520to%2520which%2520this%2520alignment%2520is%2520associated%2520with%2520physicochemical%250Afeatures%2520of%2520odorants%2520known%2520to%2520be%2520relevant%2520for%2520olfactory%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Transformers%20Smell%20Like%20Humans%3F&entry.906535625=Farzaneh%20Taleb%20and%20Miguel%20Vasco%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic&entry.1292438233=%20%20The%20human%20brain%20encodes%20stimuli%20from%20the%20environment%20into%20representations%0Athat%20form%20a%20sensory%20perception%20of%20the%20world.%20Despite%20recent%20advances%20in%0Aunderstanding%20visual%20and%20auditory%20perception%2C%20olfactory%20perception%20remains%20an%0Aunder-explored%20topic%20in%20the%20machine%20learning%20community%20due%20to%20the%20lack%20of%0Alarge-scale%20datasets%20annotated%20with%20labels%20of%20human%20olfactory%20perception.%20In%0Athis%20work%2C%20we%20ask%20the%20question%20of%20whether%20pre-trained%20transformer%20models%20of%0Achemical%20structures%20encode%20representations%20that%20are%20aligned%20with%20human%0Aolfactory%20perception%2C%20i.e.%2C%20can%20transformers%20smell%20like%20humans%3F%20We%20demonstrate%0Athat%20representations%20encoded%20from%20transformers%20pre-trained%20on%20general%20chemical%0Astructures%20are%20highly%20aligned%20with%20human%20olfactory%20perception.%20We%20use%20multiple%0Adatasets%20and%20different%20types%20of%20perceptual%20representations%20to%20show%20that%20the%0Arepresentations%20encoded%20by%20transformer%20models%20are%20able%20to%20predict%3A%20%28i%29%20labels%0Aassociated%20with%20odorants%20provided%20by%20experts%3B%20%28ii%29%20continuous%20ratings%20provided%0Aby%20human%20participants%20with%20respect%20to%20pre-defined%20descriptors%3B%20and%20%28iii%29%0Asimilarity%20ratings%20between%20odorants%20provided%20by%20human%20participants.%20Finally%2C%20we%0Aevaluate%20the%20extent%20to%20which%20this%20alignment%20is%20associated%20with%20physicochemical%0Afeatures%20of%20odorants%20known%20to%20be%20relevant%20for%20olfactory%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03038v1&entry.124074799=Read"},
{"title": "KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold\n  Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection", "author": "Md Abrar Jahin and Md. Akmol Masud and M. F. Mridha and Zeyar Aung and Nilanjan Dey", "abstract": "  Heart failure remains a major global health challenge, contributing\nsignificantly to the 17.8 million annual deaths from cardiovascular disease,\nhighlighting the need for improved diagnostic tools. Current heart disease\nprediction models based on classical machine learning face limitations,\nincluding poor handling of high-dimensional, imbalanced data, limited\nperformance on small datasets, and a lack of uncertainty quantification, while\nalso being difficult for healthcare professionals to interpret. To address\nthese issues, we introduce KACQ-DCNN, a novel classical-quantum hybrid\ndual-channel neural network that replaces traditional multilayer perceptrons\nand convolutional layers with Kolmogorov-Arnold Networks (KANs). This approach\nenhances function approximation with learnable univariate activation functions,\nreducing model complexity and improving generalization. The KACQ-DCNN 4-qubit\n1-layered model significantly outperforms 37 benchmark models across multiple\nmetrics, achieving an accuracy of 92.03%, a macro-average precision, recall,\nand F1 score of 92.00%, and an ROC-AUC score of 94.77%. Ablation studies\ndemonstrate the synergistic benefits of combining classical and quantum\ncomponents with KAN. Additionally, explainability techniques like LIME and SHAP\nprovide feature-level insights, improving model transparency, while uncertainty\nquantification via conformal prediction ensures robust probability estimates.\nThese results suggest that KACQ-DCNN offers a promising path toward more\naccurate, interpretable, and reliable heart disease predictions, paving the way\nfor advancements in cardiovascular healthcare.\n", "link": "http://arxiv.org/abs/2410.07446v2", "date": "2024-11-05", "relevancy": 2.0123, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5129}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection&body=Title%3A%20KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection%0AAuthor%3A%20Md%20Abrar%20Jahin%20and%20Md.%20Akmol%20Masud%20and%20M.%20F.%20Mridha%20and%20Zeyar%20Aung%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20Heart%20failure%20remains%20a%20major%20global%20health%20challenge%2C%20contributing%0Asignificantly%20to%20the%2017.8%20million%20annual%20deaths%20from%20cardiovascular%20disease%2C%0Ahighlighting%20the%20need%20for%20improved%20diagnostic%20tools.%20Current%20heart%20disease%0Aprediction%20models%20based%20on%20classical%20machine%20learning%20face%20limitations%2C%0Aincluding%20poor%20handling%20of%20high-dimensional%2C%20imbalanced%20data%2C%20limited%0Aperformance%20on%20small%20datasets%2C%20and%20a%20lack%20of%20uncertainty%20quantification%2C%20while%0Aalso%20being%20difficult%20for%20healthcare%20professionals%20to%20interpret.%20To%20address%0Athese%20issues%2C%20we%20introduce%20KACQ-DCNN%2C%20a%20novel%20classical-quantum%20hybrid%0Adual-channel%20neural%20network%20that%20replaces%20traditional%20multilayer%20perceptrons%0Aand%20convolutional%20layers%20with%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20This%20approach%0Aenhances%20function%20approximation%20with%20learnable%20univariate%20activation%20functions%2C%0Areducing%20model%20complexity%20and%20improving%20generalization.%20The%20KACQ-DCNN%204-qubit%0A1-layered%20model%20significantly%20outperforms%2037%20benchmark%20models%20across%20multiple%0Ametrics%2C%20achieving%20an%20accuracy%20of%2092.03%25%2C%20a%20macro-average%20precision%2C%20recall%2C%0Aand%20F1%20score%20of%2092.00%25%2C%20and%20an%20ROC-AUC%20score%20of%2094.77%25.%20Ablation%20studies%0Ademonstrate%20the%20synergistic%20benefits%20of%20combining%20classical%20and%20quantum%0Acomponents%20with%20KAN.%20Additionally%2C%20explainability%20techniques%20like%20LIME%20and%20SHAP%0Aprovide%20feature-level%20insights%2C%20improving%20model%20transparency%2C%20while%20uncertainty%0Aquantification%20via%20conformal%20prediction%20ensures%20robust%20probability%20estimates.%0AThese%20results%20suggest%20that%20KACQ-DCNN%20offers%20a%20promising%20path%20toward%20more%0Aaccurate%2C%20interpretable%2C%20and%20reliable%20heart%20disease%20predictions%2C%20paving%20the%20way%0Afor%20advancements%20in%20cardiovascular%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKACQ-DCNN%253A%2520Uncertainty-Aware%2520Interpretable%2520Kolmogorov-Arnold%250A%2520%2520Classical-Quantum%2520Dual-Channel%2520Neural%2520Network%2520for%2520Heart%2520Disease%2520Detection%26entry.906535625%3DMd%2520Abrar%2520Jahin%2520and%2520Md.%2520Akmol%2520Masud%2520and%2520M.%2520F.%2520Mridha%2520and%2520Zeyar%2520Aung%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520Heart%2520failure%2520remains%2520a%2520major%2520global%2520health%2520challenge%252C%2520contributing%250Asignificantly%2520to%2520the%252017.8%2520million%2520annual%2520deaths%2520from%2520cardiovascular%2520disease%252C%250Ahighlighting%2520the%2520need%2520for%2520improved%2520diagnostic%2520tools.%2520Current%2520heart%2520disease%250Aprediction%2520models%2520based%2520on%2520classical%2520machine%2520learning%2520face%2520limitations%252C%250Aincluding%2520poor%2520handling%2520of%2520high-dimensional%252C%2520imbalanced%2520data%252C%2520limited%250Aperformance%2520on%2520small%2520datasets%252C%2520and%2520a%2520lack%2520of%2520uncertainty%2520quantification%252C%2520while%250Aalso%2520being%2520difficult%2520for%2520healthcare%2520professionals%2520to%2520interpret.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520KACQ-DCNN%252C%2520a%2520novel%2520classical-quantum%2520hybrid%250Adual-channel%2520neural%2520network%2520that%2520replaces%2520traditional%2520multilayer%2520perceptrons%250Aand%2520convolutional%2520layers%2520with%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529.%2520This%2520approach%250Aenhances%2520function%2520approximation%2520with%2520learnable%2520univariate%2520activation%2520functions%252C%250Areducing%2520model%2520complexity%2520and%2520improving%2520generalization.%2520The%2520KACQ-DCNN%25204-qubit%250A1-layered%2520model%2520significantly%2520outperforms%252037%2520benchmark%2520models%2520across%2520multiple%250Ametrics%252C%2520achieving%2520an%2520accuracy%2520of%252092.03%2525%252C%2520a%2520macro-average%2520precision%252C%2520recall%252C%250Aand%2520F1%2520score%2520of%252092.00%2525%252C%2520and%2520an%2520ROC-AUC%2520score%2520of%252094.77%2525.%2520Ablation%2520studies%250Ademonstrate%2520the%2520synergistic%2520benefits%2520of%2520combining%2520classical%2520and%2520quantum%250Acomponents%2520with%2520KAN.%2520Additionally%252C%2520explainability%2520techniques%2520like%2520LIME%2520and%2520SHAP%250Aprovide%2520feature-level%2520insights%252C%2520improving%2520model%2520transparency%252C%2520while%2520uncertainty%250Aquantification%2520via%2520conformal%2520prediction%2520ensures%2520robust%2520probability%2520estimates.%250AThese%2520results%2520suggest%2520that%2520KACQ-DCNN%2520offers%2520a%2520promising%2520path%2520toward%2520more%250Aaccurate%252C%2520interpretable%252C%2520and%2520reliable%2520heart%2520disease%2520predictions%252C%2520paving%2520the%2520way%250Afor%2520advancements%2520in%2520cardiovascular%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection&entry.906535625=Md%20Abrar%20Jahin%20and%20Md.%20Akmol%20Masud%20and%20M.%20F.%20Mridha%20and%20Zeyar%20Aung%20and%20Nilanjan%20Dey&entry.1292438233=%20%20Heart%20failure%20remains%20a%20major%20global%20health%20challenge%2C%20contributing%0Asignificantly%20to%20the%2017.8%20million%20annual%20deaths%20from%20cardiovascular%20disease%2C%0Ahighlighting%20the%20need%20for%20improved%20diagnostic%20tools.%20Current%20heart%20disease%0Aprediction%20models%20based%20on%20classical%20machine%20learning%20face%20limitations%2C%0Aincluding%20poor%20handling%20of%20high-dimensional%2C%20imbalanced%20data%2C%20limited%0Aperformance%20on%20small%20datasets%2C%20and%20a%20lack%20of%20uncertainty%20quantification%2C%20while%0Aalso%20being%20difficult%20for%20healthcare%20professionals%20to%20interpret.%20To%20address%0Athese%20issues%2C%20we%20introduce%20KACQ-DCNN%2C%20a%20novel%20classical-quantum%20hybrid%0Adual-channel%20neural%20network%20that%20replaces%20traditional%20multilayer%20perceptrons%0Aand%20convolutional%20layers%20with%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20This%20approach%0Aenhances%20function%20approximation%20with%20learnable%20univariate%20activation%20functions%2C%0Areducing%20model%20complexity%20and%20improving%20generalization.%20The%20KACQ-DCNN%204-qubit%0A1-layered%20model%20significantly%20outperforms%2037%20benchmark%20models%20across%20multiple%0Ametrics%2C%20achieving%20an%20accuracy%20of%2092.03%25%2C%20a%20macro-average%20precision%2C%20recall%2C%0Aand%20F1%20score%20of%2092.00%25%2C%20and%20an%20ROC-AUC%20score%20of%2094.77%25.%20Ablation%20studies%0Ademonstrate%20the%20synergistic%20benefits%20of%20combining%20classical%20and%20quantum%0Acomponents%20with%20KAN.%20Additionally%2C%20explainability%20techniques%20like%20LIME%20and%20SHAP%0Aprovide%20feature-level%20insights%2C%20improving%20model%20transparency%2C%20while%20uncertainty%0Aquantification%20via%20conformal%20prediction%20ensures%20robust%20probability%20estimates.%0AThese%20results%20suggest%20that%20KACQ-DCNN%20offers%20a%20promising%20path%20toward%20more%0Aaccurate%2C%20interpretable%2C%20and%20reliable%20heart%20disease%20predictions%2C%20paving%20the%20way%0Afor%20advancements%20in%20cardiovascular%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07446v2&entry.124074799=Read"},
{"title": "Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for\n  Semi-supervised Medical Image Classification", "author": "Zhang Qixiang and Yang Yuxiang and Zu Chen and Zhang Jianjia and Wu Xi and Zhou Jiliu and Wang Yan", "abstract": "  Semi-supervised learning (SSL) is a popular solution to alleviate the high\nannotation cost in medical image classification. As a main branch of SSL,\nconsistency regularization engages in imposing consensus between the\npredictions of a single sample from different views, termed as Absolute\nLocation consistency (AL-c). However, only AL-c may be insufficient. Just like\nwhen diagnosing a case in practice, besides the case itself, the doctor usually\nrefers to certain related trustworthy cases to make more reliable\ndecisions.Therefore, we argue that solely relying on AL-c may ignore the\nrelative differences across samples, which we interpret as relative locations,\nand only exploit limited information from one perspective. To address this\nissue, we propose a Sample Consistency Mean Teacher (SCMT) which not only\nincorporates AL c but also additionally enforces consistency between the\nsamples' relative similarities to its related samples, called Relative Location\nconsistency (RL c). AL c and RL c conduct consistency regularization from two\ndifferent perspectives, jointly extracting more diverse semantic information\nfor classification. On the other hand, due to the highly similar structures in\nmedical images, the sample distribution could be overly dense in feature space,\nmaking their relative locations susceptible to noise. To tackle this problem,\nwe further develop a Sample Scatter Mean Teacher (SSMT) by utilizing\ncontrastive learning to sparsify the sample distribution and obtain robust and\neffective relative locations. Extensive experiments on different datasets\ndemonstrate the superiority of our method.\n", "link": "http://arxiv.org/abs/2411.03041v1", "date": "2024-11-05", "relevancy": 2.0073, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5261}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Judge%20Like%20a%20Real%20Doctor%3A%20Dual%20Teacher%20Sample%20Consistency%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Classification&body=Title%3A%20Judge%20Like%20a%20Real%20Doctor%3A%20Dual%20Teacher%20Sample%20Consistency%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Classification%0AAuthor%3A%20Zhang%20Qixiang%20and%20Yang%20Yuxiang%20and%20Zu%20Chen%20and%20Zhang%20Jianjia%20and%20Wu%20Xi%20and%20Zhou%20Jiliu%20and%20Wang%20Yan%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20is%20a%20popular%20solution%20to%20alleviate%20the%20high%0Aannotation%20cost%20in%20medical%20image%20classification.%20As%20a%20main%20branch%20of%20SSL%2C%0Aconsistency%20regularization%20engages%20in%20imposing%20consensus%20between%20the%0Apredictions%20of%20a%20single%20sample%20from%20different%20views%2C%20termed%20as%20Absolute%0ALocation%20consistency%20%28AL-c%29.%20However%2C%20only%20AL-c%20may%20be%20insufficient.%20Just%20like%0Awhen%20diagnosing%20a%20case%20in%20practice%2C%20besides%20the%20case%20itself%2C%20the%20doctor%20usually%0Arefers%20to%20certain%20related%20trustworthy%20cases%20to%20make%20more%20reliable%0Adecisions.Therefore%2C%20we%20argue%20that%20solely%20relying%20on%20AL-c%20may%20ignore%20the%0Arelative%20differences%20across%20samples%2C%20which%20we%20interpret%20as%20relative%20locations%2C%0Aand%20only%20exploit%20limited%20information%20from%20one%20perspective.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20Sample%20Consistency%20Mean%20Teacher%20%28SCMT%29%20which%20not%20only%0Aincorporates%20AL%20c%20but%20also%20additionally%20enforces%20consistency%20between%20the%0Asamples%27%20relative%20similarities%20to%20its%20related%20samples%2C%20called%20Relative%20Location%0Aconsistency%20%28RL%20c%29.%20AL%20c%20and%20RL%20c%20conduct%20consistency%20regularization%20from%20two%0Adifferent%20perspectives%2C%20jointly%20extracting%20more%20diverse%20semantic%20information%0Afor%20classification.%20On%20the%20other%20hand%2C%20due%20to%20the%20highly%20similar%20structures%20in%0Amedical%20images%2C%20the%20sample%20distribution%20could%20be%20overly%20dense%20in%20feature%20space%2C%0Amaking%20their%20relative%20locations%20susceptible%20to%20noise.%20To%20tackle%20this%20problem%2C%0Awe%20further%20develop%20a%20Sample%20Scatter%20Mean%20Teacher%20%28SSMT%29%20by%20utilizing%0Acontrastive%20learning%20to%20sparsify%20the%20sample%20distribution%20and%20obtain%20robust%20and%0Aeffective%20relative%20locations.%20Extensive%20experiments%20on%20different%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudge%2520Like%2520a%2520Real%2520Doctor%253A%2520Dual%2520Teacher%2520Sample%2520Consistency%2520Framework%2520for%250A%2520%2520Semi-supervised%2520Medical%2520Image%2520Classification%26entry.906535625%3DZhang%2520Qixiang%2520and%2520Yang%2520Yuxiang%2520and%2520Zu%2520Chen%2520and%2520Zhang%2520Jianjia%2520and%2520Wu%2520Xi%2520and%2520Zhou%2520Jiliu%2520and%2520Wang%2520Yan%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520is%2520a%2520popular%2520solution%2520to%2520alleviate%2520the%2520high%250Aannotation%2520cost%2520in%2520medical%2520image%2520classification.%2520As%2520a%2520main%2520branch%2520of%2520SSL%252C%250Aconsistency%2520regularization%2520engages%2520in%2520imposing%2520consensus%2520between%2520the%250Apredictions%2520of%2520a%2520single%2520sample%2520from%2520different%2520views%252C%2520termed%2520as%2520Absolute%250ALocation%2520consistency%2520%2528AL-c%2529.%2520However%252C%2520only%2520AL-c%2520may%2520be%2520insufficient.%2520Just%2520like%250Awhen%2520diagnosing%2520a%2520case%2520in%2520practice%252C%2520besides%2520the%2520case%2520itself%252C%2520the%2520doctor%2520usually%250Arefers%2520to%2520certain%2520related%2520trustworthy%2520cases%2520to%2520make%2520more%2520reliable%250Adecisions.Therefore%252C%2520we%2520argue%2520that%2520solely%2520relying%2520on%2520AL-c%2520may%2520ignore%2520the%250Arelative%2520differences%2520across%2520samples%252C%2520which%2520we%2520interpret%2520as%2520relative%2520locations%252C%250Aand%2520only%2520exploit%2520limited%2520information%2520from%2520one%2520perspective.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520Sample%2520Consistency%2520Mean%2520Teacher%2520%2528SCMT%2529%2520which%2520not%2520only%250Aincorporates%2520AL%2520c%2520but%2520also%2520additionally%2520enforces%2520consistency%2520between%2520the%250Asamples%2527%2520relative%2520similarities%2520to%2520its%2520related%2520samples%252C%2520called%2520Relative%2520Location%250Aconsistency%2520%2528RL%2520c%2529.%2520AL%2520c%2520and%2520RL%2520c%2520conduct%2520consistency%2520regularization%2520from%2520two%250Adifferent%2520perspectives%252C%2520jointly%2520extracting%2520more%2520diverse%2520semantic%2520information%250Afor%2520classification.%2520On%2520the%2520other%2520hand%252C%2520due%2520to%2520the%2520highly%2520similar%2520structures%2520in%250Amedical%2520images%252C%2520the%2520sample%2520distribution%2520could%2520be%2520overly%2520dense%2520in%2520feature%2520space%252C%250Amaking%2520their%2520relative%2520locations%2520susceptible%2520to%2520noise.%2520To%2520tackle%2520this%2520problem%252C%250Awe%2520further%2520develop%2520a%2520Sample%2520Scatter%2520Mean%2520Teacher%2520%2528SSMT%2529%2520by%2520utilizing%250Acontrastive%2520learning%2520to%2520sparsify%2520the%2520sample%2520distribution%2520and%2520obtain%2520robust%2520and%250Aeffective%2520relative%2520locations.%2520Extensive%2520experiments%2520on%2520different%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Judge%20Like%20a%20Real%20Doctor%3A%20Dual%20Teacher%20Sample%20Consistency%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Classification&entry.906535625=Zhang%20Qixiang%20and%20Yang%20Yuxiang%20and%20Zu%20Chen%20and%20Zhang%20Jianjia%20and%20Wu%20Xi%20and%20Zhou%20Jiliu%20and%20Wang%20Yan&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20is%20a%20popular%20solution%20to%20alleviate%20the%20high%0Aannotation%20cost%20in%20medical%20image%20classification.%20As%20a%20main%20branch%20of%20SSL%2C%0Aconsistency%20regularization%20engages%20in%20imposing%20consensus%20between%20the%0Apredictions%20of%20a%20single%20sample%20from%20different%20views%2C%20termed%20as%20Absolute%0ALocation%20consistency%20%28AL-c%29.%20However%2C%20only%20AL-c%20may%20be%20insufficient.%20Just%20like%0Awhen%20diagnosing%20a%20case%20in%20practice%2C%20besides%20the%20case%20itself%2C%20the%20doctor%20usually%0Arefers%20to%20certain%20related%20trustworthy%20cases%20to%20make%20more%20reliable%0Adecisions.Therefore%2C%20we%20argue%20that%20solely%20relying%20on%20AL-c%20may%20ignore%20the%0Arelative%20differences%20across%20samples%2C%20which%20we%20interpret%20as%20relative%20locations%2C%0Aand%20only%20exploit%20limited%20information%20from%20one%20perspective.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20Sample%20Consistency%20Mean%20Teacher%20%28SCMT%29%20which%20not%20only%0Aincorporates%20AL%20c%20but%20also%20additionally%20enforces%20consistency%20between%20the%0Asamples%27%20relative%20similarities%20to%20its%20related%20samples%2C%20called%20Relative%20Location%0Aconsistency%20%28RL%20c%29.%20AL%20c%20and%20RL%20c%20conduct%20consistency%20regularization%20from%20two%0Adifferent%20perspectives%2C%20jointly%20extracting%20more%20diverse%20semantic%20information%0Afor%20classification.%20On%20the%20other%20hand%2C%20due%20to%20the%20highly%20similar%20structures%20in%0Amedical%20images%2C%20the%20sample%20distribution%20could%20be%20overly%20dense%20in%20feature%20space%2C%0Amaking%20their%20relative%20locations%20susceptible%20to%20noise.%20To%20tackle%20this%20problem%2C%0Awe%20further%20develop%20a%20Sample%20Scatter%20Mean%20Teacher%20%28SSMT%29%20by%20utilizing%0Acontrastive%20learning%20to%20sparsify%20the%20sample%20distribution%20and%20obtain%20robust%20and%0Aeffective%20relative%20locations.%20Extensive%20experiments%20on%20different%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03041v1&entry.124074799=Read"},
{"title": "Energy-Aware Predictive Motion Planning for Autonomous Vehicles Using a\n  Hybrid Zonotope Constraint Representation", "author": "Joshua A. Robbins and Andrew F. Thompson and Sean Brennan and Herschel C. Pangborn", "abstract": "  Uncrewed aerial systems have tightly coupled energy and motion dynamics which\nmust be accounted for by onboard planning algorithms. This work proposes a\nstrategy for coupled motion and energy planning using model predictive control\n(MPC). A reduced-order linear time-invariant model of coupled energy and motion\ndynamics is presented. Constrained zonotopes are used to represent state and\ninput constraints, and hybrid zonotopes are used to represent non-convex\nconstraints tied to a map of the environment. The structures of these\nconstraint representations are exploited within a mixed-integer quadratic\nprogram solver tailored to MPC motion planning problems. Results apply the\nproposed methodology to coupled motion and energy utilization planning problems\nfor 1) a hybrid-electric vehicle that must restrict engine usage when flying\nover regions with noise restrictions, and 2) an electric package delivery drone\nthat must track waysets with both position and battery state of charge\nrequirements. By leveraging the structure-exploiting solver, the proposed\nmixed-integer MPC formulations can be implemented in real time.\n", "link": "http://arxiv.org/abs/2411.03189v1", "date": "2024-11-05", "relevancy": 2.0033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4941}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation&body=Title%3A%20Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation%0AAuthor%3A%20Joshua%20A.%20Robbins%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn%0AAbstract%3A%20%20%20Uncrewed%20aerial%20systems%20have%20tightly%20coupled%20energy%20and%20motion%20dynamics%20which%0Amust%20be%20accounted%20for%20by%20onboard%20planning%20algorithms.%20This%20work%20proposes%20a%0Astrategy%20for%20coupled%20motion%20and%20energy%20planning%20using%20model%20predictive%20control%0A%28MPC%29.%20A%20reduced-order%20linear%20time-invariant%20model%20of%20coupled%20energy%20and%20motion%0Adynamics%20is%20presented.%20Constrained%20zonotopes%20are%20used%20to%20represent%20state%20and%0Ainput%20constraints%2C%20and%20hybrid%20zonotopes%20are%20used%20to%20represent%20non-convex%0Aconstraints%20tied%20to%20a%20map%20of%20the%20environment.%20The%20structures%20of%20these%0Aconstraint%20representations%20are%20exploited%20within%20a%20mixed-integer%20quadratic%0Aprogram%20solver%20tailored%20to%20MPC%20motion%20planning%20problems.%20Results%20apply%20the%0Aproposed%20methodology%20to%20coupled%20motion%20and%20energy%20utilization%20planning%20problems%0Afor%201%29%20a%20hybrid-electric%20vehicle%20that%20must%20restrict%20engine%20usage%20when%20flying%0Aover%20regions%20with%20noise%20restrictions%2C%20and%202%29%20an%20electric%20package%20delivery%20drone%0Athat%20must%20track%20waysets%20with%20both%20position%20and%20battery%20state%20of%20charge%0Arequirements.%20By%20leveraging%20the%20structure-exploiting%20solver%2C%20the%20proposed%0Amixed-integer%20MPC%20formulations%20can%20be%20implemented%20in%20real%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Aware%2520Predictive%2520Motion%2520Planning%2520for%2520Autonomous%2520Vehicles%2520Using%2520a%250A%2520%2520Hybrid%2520Zonotope%2520Constraint%2520Representation%26entry.906535625%3DJoshua%2520A.%2520Robbins%2520and%2520Andrew%2520F.%2520Thompson%2520and%2520Sean%2520Brennan%2520and%2520Herschel%2520C.%2520Pangborn%26entry.1292438233%3D%2520%2520Uncrewed%2520aerial%2520systems%2520have%2520tightly%2520coupled%2520energy%2520and%2520motion%2520dynamics%2520which%250Amust%2520be%2520accounted%2520for%2520by%2520onboard%2520planning%2520algorithms.%2520This%2520work%2520proposes%2520a%250Astrategy%2520for%2520coupled%2520motion%2520and%2520energy%2520planning%2520using%2520model%2520predictive%2520control%250A%2528MPC%2529.%2520A%2520reduced-order%2520linear%2520time-invariant%2520model%2520of%2520coupled%2520energy%2520and%2520motion%250Adynamics%2520is%2520presented.%2520Constrained%2520zonotopes%2520are%2520used%2520to%2520represent%2520state%2520and%250Ainput%2520constraints%252C%2520and%2520hybrid%2520zonotopes%2520are%2520used%2520to%2520represent%2520non-convex%250Aconstraints%2520tied%2520to%2520a%2520map%2520of%2520the%2520environment.%2520The%2520structures%2520of%2520these%250Aconstraint%2520representations%2520are%2520exploited%2520within%2520a%2520mixed-integer%2520quadratic%250Aprogram%2520solver%2520tailored%2520to%2520MPC%2520motion%2520planning%2520problems.%2520Results%2520apply%2520the%250Aproposed%2520methodology%2520to%2520coupled%2520motion%2520and%2520energy%2520utilization%2520planning%2520problems%250Afor%25201%2529%2520a%2520hybrid-electric%2520vehicle%2520that%2520must%2520restrict%2520engine%2520usage%2520when%2520flying%250Aover%2520regions%2520with%2520noise%2520restrictions%252C%2520and%25202%2529%2520an%2520electric%2520package%2520delivery%2520drone%250Athat%2520must%2520track%2520waysets%2520with%2520both%2520position%2520and%2520battery%2520state%2520of%2520charge%250Arequirements.%2520By%2520leveraging%2520the%2520structure-exploiting%2520solver%252C%2520the%2520proposed%250Amixed-integer%2520MPC%2520formulations%2520can%2520be%2520implemented%2520in%2520real%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Aware%20Predictive%20Motion%20Planning%20for%20Autonomous%20Vehicles%20Using%20a%0A%20%20Hybrid%20Zonotope%20Constraint%20Representation&entry.906535625=Joshua%20A.%20Robbins%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn&entry.1292438233=%20%20Uncrewed%20aerial%20systems%20have%20tightly%20coupled%20energy%20and%20motion%20dynamics%20which%0Amust%20be%20accounted%20for%20by%20onboard%20planning%20algorithms.%20This%20work%20proposes%20a%0Astrategy%20for%20coupled%20motion%20and%20energy%20planning%20using%20model%20predictive%20control%0A%28MPC%29.%20A%20reduced-order%20linear%20time-invariant%20model%20of%20coupled%20energy%20and%20motion%0Adynamics%20is%20presented.%20Constrained%20zonotopes%20are%20used%20to%20represent%20state%20and%0Ainput%20constraints%2C%20and%20hybrid%20zonotopes%20are%20used%20to%20represent%20non-convex%0Aconstraints%20tied%20to%20a%20map%20of%20the%20environment.%20The%20structures%20of%20these%0Aconstraint%20representations%20are%20exploited%20within%20a%20mixed-integer%20quadratic%0Aprogram%20solver%20tailored%20to%20MPC%20motion%20planning%20problems.%20Results%20apply%20the%0Aproposed%20methodology%20to%20coupled%20motion%20and%20energy%20utilization%20planning%20problems%0Afor%201%29%20a%20hybrid-electric%20vehicle%20that%20must%20restrict%20engine%20usage%20when%20flying%0Aover%20regions%20with%20noise%20restrictions%2C%20and%202%29%20an%20electric%20package%20delivery%20drone%0Athat%20must%20track%20waysets%20with%20both%20position%20and%20battery%20state%20of%20charge%0Arequirements.%20By%20leveraging%20the%20structure-exploiting%20solver%2C%20the%20proposed%0Amixed-integer%20MPC%20formulations%20can%20be%20implemented%20in%20real%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03189v1&entry.124074799=Read"},
{"title": "Exploiting the Segment Anything Model (SAM) for Lung Segmentation in\n  Chest X-ray Images", "author": "Gabriel Bellon de Carvalho and Jurandy Almeida", "abstract": "  Segment Anything Model (SAM), a new AI model from Meta AI released in April\n2023, is an ambitious tool designed to identify and separate individual objects\nwithin a given image through semantic interpretation. The advanced capabilities\nof SAM are the result of its training with millions of images and masks, and a\nfew days after its release, several researchers began testing the model on\nmedical images to evaluate its performance in this domain. With this\nperspective in focus -- i.e., optimizing work in the healthcare field -- this\nwork proposes the use of this new technology to evaluate and study chest X-ray\nimages. The approach adopted for this work, with the aim of improving the\nmodel's performance for lung segmentation, involved a transfer learning\nprocess, specifically the fine-tuning technique. After applying this\nadjustment, a substantial improvement was observed in the evaluation metrics\nused to assess SAM's performance compared to the masks provided by the\ndatasets. The results obtained by the model after the adjustments were\nsatisfactory and similar to cutting-edge neural networks, such as U-Net.\n", "link": "http://arxiv.org/abs/2411.03064v1", "date": "2024-11-05", "relevancy": 2.0, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4957}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20Lung%20Segmentation%20in%0A%20%20Chest%20X-ray%20Images&body=Title%3A%20Exploiting%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20Lung%20Segmentation%20in%0A%20%20Chest%20X-ray%20Images%0AAuthor%3A%20Gabriel%20Bellon%20de%20Carvalho%20and%20Jurandy%20Almeida%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20new%20AI%20model%20from%20Meta%20AI%20released%20in%20April%0A2023%2C%20is%20an%20ambitious%20tool%20designed%20to%20identify%20and%20separate%20individual%20objects%0Awithin%20a%20given%20image%20through%20semantic%20interpretation.%20The%20advanced%20capabilities%0Aof%20SAM%20are%20the%20result%20of%20its%20training%20with%20millions%20of%20images%20and%20masks%2C%20and%20a%0Afew%20days%20after%20its%20release%2C%20several%20researchers%20began%20testing%20the%20model%20on%0Amedical%20images%20to%20evaluate%20its%20performance%20in%20this%20domain.%20With%20this%0Aperspective%20in%20focus%20--%20i.e.%2C%20optimizing%20work%20in%20the%20healthcare%20field%20--%20this%0Awork%20proposes%20the%20use%20of%20this%20new%20technology%20to%20evaluate%20and%20study%20chest%20X-ray%0Aimages.%20The%20approach%20adopted%20for%20this%20work%2C%20with%20the%20aim%20of%20improving%20the%0Amodel%27s%20performance%20for%20lung%20segmentation%2C%20involved%20a%20transfer%20learning%0Aprocess%2C%20specifically%20the%20fine-tuning%20technique.%20After%20applying%20this%0Aadjustment%2C%20a%20substantial%20improvement%20was%20observed%20in%20the%20evaluation%20metrics%0Aused%20to%20assess%20SAM%27s%20performance%20compared%20to%20the%20masks%20provided%20by%20the%0Adatasets.%20The%20results%20obtained%20by%20the%20model%20after%20the%20adjustments%20were%0Asatisfactory%20and%20similar%20to%20cutting-edge%20neural%20networks%2C%20such%20as%20U-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520for%2520Lung%2520Segmentation%2520in%250A%2520%2520Chest%2520X-ray%2520Images%26entry.906535625%3DGabriel%2520Bellon%2520de%2520Carvalho%2520and%2520Jurandy%2520Almeida%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520a%2520new%2520AI%2520model%2520from%2520Meta%2520AI%2520released%2520in%2520April%250A2023%252C%2520is%2520an%2520ambitious%2520tool%2520designed%2520to%2520identify%2520and%2520separate%2520individual%2520objects%250Awithin%2520a%2520given%2520image%2520through%2520semantic%2520interpretation.%2520The%2520advanced%2520capabilities%250Aof%2520SAM%2520are%2520the%2520result%2520of%2520its%2520training%2520with%2520millions%2520of%2520images%2520and%2520masks%252C%2520and%2520a%250Afew%2520days%2520after%2520its%2520release%252C%2520several%2520researchers%2520began%2520testing%2520the%2520model%2520on%250Amedical%2520images%2520to%2520evaluate%2520its%2520performance%2520in%2520this%2520domain.%2520With%2520this%250Aperspective%2520in%2520focus%2520--%2520i.e.%252C%2520optimizing%2520work%2520in%2520the%2520healthcare%2520field%2520--%2520this%250Awork%2520proposes%2520the%2520use%2520of%2520this%2520new%2520technology%2520to%2520evaluate%2520and%2520study%2520chest%2520X-ray%250Aimages.%2520The%2520approach%2520adopted%2520for%2520this%2520work%252C%2520with%2520the%2520aim%2520of%2520improving%2520the%250Amodel%2527s%2520performance%2520for%2520lung%2520segmentation%252C%2520involved%2520a%2520transfer%2520learning%250Aprocess%252C%2520specifically%2520the%2520fine-tuning%2520technique.%2520After%2520applying%2520this%250Aadjustment%252C%2520a%2520substantial%2520improvement%2520was%2520observed%2520in%2520the%2520evaluation%2520metrics%250Aused%2520to%2520assess%2520SAM%2527s%2520performance%2520compared%2520to%2520the%2520masks%2520provided%2520by%2520the%250Adatasets.%2520The%2520results%2520obtained%2520by%2520the%2520model%2520after%2520the%2520adjustments%2520were%250Asatisfactory%2520and%2520similar%2520to%2520cutting-edge%2520neural%2520networks%252C%2520such%2520as%2520U-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%20Lung%20Segmentation%20in%0A%20%20Chest%20X-ray%20Images&entry.906535625=Gabriel%20Bellon%20de%20Carvalho%20and%20Jurandy%20Almeida&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20new%20AI%20model%20from%20Meta%20AI%20released%20in%20April%0A2023%2C%20is%20an%20ambitious%20tool%20designed%20to%20identify%20and%20separate%20individual%20objects%0Awithin%20a%20given%20image%20through%20semantic%20interpretation.%20The%20advanced%20capabilities%0Aof%20SAM%20are%20the%20result%20of%20its%20training%20with%20millions%20of%20images%20and%20masks%2C%20and%20a%0Afew%20days%20after%20its%20release%2C%20several%20researchers%20began%20testing%20the%20model%20on%0Amedical%20images%20to%20evaluate%20its%20performance%20in%20this%20domain.%20With%20this%0Aperspective%20in%20focus%20--%20i.e.%2C%20optimizing%20work%20in%20the%20healthcare%20field%20--%20this%0Awork%20proposes%20the%20use%20of%20this%20new%20technology%20to%20evaluate%20and%20study%20chest%20X-ray%0Aimages.%20The%20approach%20adopted%20for%20this%20work%2C%20with%20the%20aim%20of%20improving%20the%0Amodel%27s%20performance%20for%20lung%20segmentation%2C%20involved%20a%20transfer%20learning%0Aprocess%2C%20specifically%20the%20fine-tuning%20technique.%20After%20applying%20this%0Aadjustment%2C%20a%20substantial%20improvement%20was%20observed%20in%20the%20evaluation%20metrics%0Aused%20to%20assess%20SAM%27s%20performance%20compared%20to%20the%20masks%20provided%20by%20the%0Adatasets.%20The%20results%20obtained%20by%20the%20model%20after%20the%20adjustments%20were%0Asatisfactory%20and%20similar%20to%20cutting-edge%20neural%20networks%2C%20such%20as%20U-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03064v1&entry.124074799=Read"},
{"title": "Re-assembling the past: The RePAIR dataset and benchmark for real world\n  2D and 3D puzzle solving", "author": "Theodore Tsesmelis and Luca Palmieri and Marina Khoroshiltseva and Adeela Islam and Gur Elkin and Ofir Itzhak Shahar and Gianluca Scarpellini and Stefano Fiorini and Yaniv Ohayon and Nadav Alali and Sinem Aslan and Pietro Morerio and Sebastiano Vascon and Elena Gravina and Maria Cristina Napolitano and Giuseppe Scarpati and Gabriel Zuchtriegel and Alexandra Sp\u00fchler and Michel E. Fuchs and Stuart James and Ohad Ben-Shahar and Marcello Pelillo and Alessio Del Bue", "abstract": "  This paper proposes the RePAIR dataset that represents a challenging\nbenchmark to test modern computational and data driven methods for\npuzzle-solving and reassembly tasks. Our dataset has unique properties that are\nuncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and\nfractures are realistic, caused by a collapse of a fresco during a World War II\nbombing at the Pompeii archaeological park. The fragments are also eroded and\nhave missing pieces with irregular shapes and different dimensions, challenging\nfurther the reassembly algorithms. The dataset is multi-modal providing high\nresolution images with characteristic pictorial elements, detailed 3D scans of\nthe fragments and meta-data annotated by the archaeologists. Ground truth has\nbeen generated through several years of unceasing fieldwork, including the\nexcavation and cleaning of each fragment, followed by manual puzzle solving by\narchaeologists of a subset of approx. 1000 pieces among the 16000 available.\nAfter digitizing all the fragments in 3D, a benchmark was prepared to challenge\ncurrent reassembly and puzzle-solving methods that often solve more simplistic\nsynthetic scenarios. The tested baselines show that there clearly exists a gap\nto fill in solving this computationally complex problem.\n", "link": "http://arxiv.org/abs/2410.24010v2", "date": "2024-11-05", "relevancy": 1.9968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-assembling%20the%20past%3A%20The%20RePAIR%20dataset%20and%20benchmark%20for%20real%20world%0A%20%202D%20and%203D%20puzzle%20solving&body=Title%3A%20Re-assembling%20the%20past%3A%20The%20RePAIR%20dataset%20and%20benchmark%20for%20real%20world%0A%20%202D%20and%203D%20puzzle%20solving%0AAuthor%3A%20Theodore%20Tsesmelis%20and%20Luca%20Palmieri%20and%20Marina%20Khoroshiltseva%20and%20Adeela%20Islam%20and%20Gur%20Elkin%20and%20Ofir%20Itzhak%20Shahar%20and%20Gianluca%20Scarpellini%20and%20Stefano%20Fiorini%20and%20Yaniv%20Ohayon%20and%20Nadav%20Alali%20and%20Sinem%20Aslan%20and%20Pietro%20Morerio%20and%20Sebastiano%20Vascon%20and%20Elena%20Gravina%20and%20Maria%20Cristina%20Napolitano%20and%20Giuseppe%20Scarpati%20and%20Gabriel%20Zuchtriegel%20and%20Alexandra%20Sp%C3%BChler%20and%20Michel%20E.%20Fuchs%20and%20Stuart%20James%20and%20Ohad%20Ben-Shahar%20and%20Marcello%20Pelillo%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20This%20paper%20proposes%20the%20RePAIR%20dataset%20that%20represents%20a%20challenging%0Abenchmark%20to%20test%20modern%20computational%20and%20data%20driven%20methods%20for%0Apuzzle-solving%20and%20reassembly%20tasks.%20Our%20dataset%20has%20unique%20properties%20that%20are%0Auncommon%20to%20current%20benchmarks%20for%202D%20and%203D%20puzzle%20solving.%20The%20fragments%20and%0Afractures%20are%20realistic%2C%20caused%20by%20a%20collapse%20of%20a%20fresco%20during%20a%20World%20War%20II%0Abombing%20at%20the%20Pompeii%20archaeological%20park.%20The%20fragments%20are%20also%20eroded%20and%0Ahave%20missing%20pieces%20with%20irregular%20shapes%20and%20different%20dimensions%2C%20challenging%0Afurther%20the%20reassembly%20algorithms.%20The%20dataset%20is%20multi-modal%20providing%20high%0Aresolution%20images%20with%20characteristic%20pictorial%20elements%2C%20detailed%203D%20scans%20of%0Athe%20fragments%20and%20meta-data%20annotated%20by%20the%20archaeologists.%20Ground%20truth%20has%0Abeen%20generated%20through%20several%20years%20of%20unceasing%20fieldwork%2C%20including%20the%0Aexcavation%20and%20cleaning%20of%20each%20fragment%2C%20followed%20by%20manual%20puzzle%20solving%20by%0Aarchaeologists%20of%20a%20subset%20of%20approx.%201000%20pieces%20among%20the%2016000%20available.%0AAfter%20digitizing%20all%20the%20fragments%20in%203D%2C%20a%20benchmark%20was%20prepared%20to%20challenge%0Acurrent%20reassembly%20and%20puzzle-solving%20methods%20that%20often%20solve%20more%20simplistic%0Asynthetic%20scenarios.%20The%20tested%20baselines%20show%20that%20there%20clearly%20exists%20a%20gap%0Ato%20fill%20in%20solving%20this%20computationally%20complex%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-assembling%2520the%2520past%253A%2520The%2520RePAIR%2520dataset%2520and%2520benchmark%2520for%2520real%2520world%250A%2520%25202D%2520and%25203D%2520puzzle%2520solving%26entry.906535625%3DTheodore%2520Tsesmelis%2520and%2520Luca%2520Palmieri%2520and%2520Marina%2520Khoroshiltseva%2520and%2520Adeela%2520Islam%2520and%2520Gur%2520Elkin%2520and%2520Ofir%2520Itzhak%2520Shahar%2520and%2520Gianluca%2520Scarpellini%2520and%2520Stefano%2520Fiorini%2520and%2520Yaniv%2520Ohayon%2520and%2520Nadav%2520Alali%2520and%2520Sinem%2520Aslan%2520and%2520Pietro%2520Morerio%2520and%2520Sebastiano%2520Vascon%2520and%2520Elena%2520Gravina%2520and%2520Maria%2520Cristina%2520Napolitano%2520and%2520Giuseppe%2520Scarpati%2520and%2520Gabriel%2520Zuchtriegel%2520and%2520Alexandra%2520Sp%25C3%25BChler%2520and%2520Michel%2520E.%2520Fuchs%2520and%2520Stuart%2520James%2520and%2520Ohad%2520Ben-Shahar%2520and%2520Marcello%2520Pelillo%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520the%2520RePAIR%2520dataset%2520that%2520represents%2520a%2520challenging%250Abenchmark%2520to%2520test%2520modern%2520computational%2520and%2520data%2520driven%2520methods%2520for%250Apuzzle-solving%2520and%2520reassembly%2520tasks.%2520Our%2520dataset%2520has%2520unique%2520properties%2520that%2520are%250Auncommon%2520to%2520current%2520benchmarks%2520for%25202D%2520and%25203D%2520puzzle%2520solving.%2520The%2520fragments%2520and%250Afractures%2520are%2520realistic%252C%2520caused%2520by%2520a%2520collapse%2520of%2520a%2520fresco%2520during%2520a%2520World%2520War%2520II%250Abombing%2520at%2520the%2520Pompeii%2520archaeological%2520park.%2520The%2520fragments%2520are%2520also%2520eroded%2520and%250Ahave%2520missing%2520pieces%2520with%2520irregular%2520shapes%2520and%2520different%2520dimensions%252C%2520challenging%250Afurther%2520the%2520reassembly%2520algorithms.%2520The%2520dataset%2520is%2520multi-modal%2520providing%2520high%250Aresolution%2520images%2520with%2520characteristic%2520pictorial%2520elements%252C%2520detailed%25203D%2520scans%2520of%250Athe%2520fragments%2520and%2520meta-data%2520annotated%2520by%2520the%2520archaeologists.%2520Ground%2520truth%2520has%250Abeen%2520generated%2520through%2520several%2520years%2520of%2520unceasing%2520fieldwork%252C%2520including%2520the%250Aexcavation%2520and%2520cleaning%2520of%2520each%2520fragment%252C%2520followed%2520by%2520manual%2520puzzle%2520solving%2520by%250Aarchaeologists%2520of%2520a%2520subset%2520of%2520approx.%25201000%2520pieces%2520among%2520the%252016000%2520available.%250AAfter%2520digitizing%2520all%2520the%2520fragments%2520in%25203D%252C%2520a%2520benchmark%2520was%2520prepared%2520to%2520challenge%250Acurrent%2520reassembly%2520and%2520puzzle-solving%2520methods%2520that%2520often%2520solve%2520more%2520simplistic%250Asynthetic%2520scenarios.%2520The%2520tested%2520baselines%2520show%2520that%2520there%2520clearly%2520exists%2520a%2520gap%250Ato%2520fill%2520in%2520solving%2520this%2520computationally%2520complex%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-assembling%20the%20past%3A%20The%20RePAIR%20dataset%20and%20benchmark%20for%20real%20world%0A%20%202D%20and%203D%20puzzle%20solving&entry.906535625=Theodore%20Tsesmelis%20and%20Luca%20Palmieri%20and%20Marina%20Khoroshiltseva%20and%20Adeela%20Islam%20and%20Gur%20Elkin%20and%20Ofir%20Itzhak%20Shahar%20and%20Gianluca%20Scarpellini%20and%20Stefano%20Fiorini%20and%20Yaniv%20Ohayon%20and%20Nadav%20Alali%20and%20Sinem%20Aslan%20and%20Pietro%20Morerio%20and%20Sebastiano%20Vascon%20and%20Elena%20Gravina%20and%20Maria%20Cristina%20Napolitano%20and%20Giuseppe%20Scarpati%20and%20Gabriel%20Zuchtriegel%20and%20Alexandra%20Sp%C3%BChler%20and%20Michel%20E.%20Fuchs%20and%20Stuart%20James%20and%20Ohad%20Ben-Shahar%20and%20Marcello%20Pelillo%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20This%20paper%20proposes%20the%20RePAIR%20dataset%20that%20represents%20a%20challenging%0Abenchmark%20to%20test%20modern%20computational%20and%20data%20driven%20methods%20for%0Apuzzle-solving%20and%20reassembly%20tasks.%20Our%20dataset%20has%20unique%20properties%20that%20are%0Auncommon%20to%20current%20benchmarks%20for%202D%20and%203D%20puzzle%20solving.%20The%20fragments%20and%0Afractures%20are%20realistic%2C%20caused%20by%20a%20collapse%20of%20a%20fresco%20during%20a%20World%20War%20II%0Abombing%20at%20the%20Pompeii%20archaeological%20park.%20The%20fragments%20are%20also%20eroded%20and%0Ahave%20missing%20pieces%20with%20irregular%20shapes%20and%20different%20dimensions%2C%20challenging%0Afurther%20the%20reassembly%20algorithms.%20The%20dataset%20is%20multi-modal%20providing%20high%0Aresolution%20images%20with%20characteristic%20pictorial%20elements%2C%20detailed%203D%20scans%20of%0Athe%20fragments%20and%20meta-data%20annotated%20by%20the%20archaeologists.%20Ground%20truth%20has%0Abeen%20generated%20through%20several%20years%20of%20unceasing%20fieldwork%2C%20including%20the%0Aexcavation%20and%20cleaning%20of%20each%20fragment%2C%20followed%20by%20manual%20puzzle%20solving%20by%0Aarchaeologists%20of%20a%20subset%20of%20approx.%201000%20pieces%20among%20the%2016000%20available.%0AAfter%20digitizing%20all%20the%20fragments%20in%203D%2C%20a%20benchmark%20was%20prepared%20to%20challenge%0Acurrent%20reassembly%20and%20puzzle-solving%20methods%20that%20often%20solve%20more%20simplistic%0Asynthetic%20scenarios.%20The%20tested%20baselines%20show%20that%20there%20clearly%20exists%20a%20gap%0Ato%20fill%20in%20solving%20this%20computationally%20complex%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24010v2&entry.124074799=Read"},
{"title": "On Improved Conditioning Mechanisms and Pre-training Strategies for\n  Diffusion Models", "author": "Tariq Berrada Ifriqi and Pietro Astolfi and Melissa Hall and Reyhane Askari-Hemmat and Yohann Benchetrit and Marton Havasi and Matthew Muckley and Karteek Alahari and Adriana Romero-Soriano and Jakob Verbeek and Michal Drozdzal", "abstract": "  Large-scale training of latent diffusion models (LDMs) has enabled\nunprecedented quality in image generation. However, the key components of the\nbest performing LDM training recipes are oftentimes not available to the\nresearch community, preventing apple-to-apple comparisons and hindering the\nvalidation of progress in the field. In this work, we perform an in-depth study\nof LDM training recipes focusing on the performance of models and their\ntraining efficiency. To ensure apple-to-apple comparisons, we re-implement five\npreviously published models with their corresponding recipes. Through our\nstudy, we explore the effects of (i)~the mechanisms used to condition the\ngenerative model on semantic information (e.g., text prompt) and control\nmetadata (e.g., crop size, random flip flag, etc.) on the model performance,\nand (ii)~the transfer of the representations learned on smaller and\nlower-resolution datasets to larger ones on the training efficiency and model\nperformance. We then propose a novel conditioning mechanism that disentangles\nsemantic and control metadata conditionings and sets a new state-of-the-art in\nclass-conditional generation on the ImageNet-1k dataset -- with FID\nimprovements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image\ngeneration on the CC12M dataset -- with FID improvements of 8% on 256 and 23%\non 512 resolution.\n", "link": "http://arxiv.org/abs/2411.03177v1", "date": "2024-11-05", "relevancy": 1.9891, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7316}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6534}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Improved%20Conditioning%20Mechanisms%20and%20Pre-training%20Strategies%20for%0A%20%20Diffusion%20Models&body=Title%3A%20On%20Improved%20Conditioning%20Mechanisms%20and%20Pre-training%20Strategies%20for%0A%20%20Diffusion%20Models%0AAuthor%3A%20Tariq%20Berrada%20Ifriqi%20and%20Pietro%20Astolfi%20and%20Melissa%20Hall%20and%20Reyhane%20Askari-Hemmat%20and%20Yohann%20Benchetrit%20and%20Marton%20Havasi%20and%20Matthew%20Muckley%20and%20Karteek%20Alahari%20and%20Adriana%20Romero-Soriano%20and%20Jakob%20Verbeek%20and%20Michal%20Drozdzal%0AAbstract%3A%20%20%20Large-scale%20training%20of%20latent%20diffusion%20models%20%28LDMs%29%20has%20enabled%0Aunprecedented%20quality%20in%20image%20generation.%20However%2C%20the%20key%20components%20of%20the%0Abest%20performing%20LDM%20training%20recipes%20are%20oftentimes%20not%20available%20to%20the%0Aresearch%20community%2C%20preventing%20apple-to-apple%20comparisons%20and%20hindering%20the%0Avalidation%20of%20progress%20in%20the%20field.%20In%20this%20work%2C%20we%20perform%20an%20in-depth%20study%0Aof%20LDM%20training%20recipes%20focusing%20on%20the%20performance%20of%20models%20and%20their%0Atraining%20efficiency.%20To%20ensure%20apple-to-apple%20comparisons%2C%20we%20re-implement%20five%0Apreviously%20published%20models%20with%20their%20corresponding%20recipes.%20Through%20our%0Astudy%2C%20we%20explore%20the%20effects%20of%20%28i%29~the%20mechanisms%20used%20to%20condition%20the%0Agenerative%20model%20on%20semantic%20information%20%28e.g.%2C%20text%20prompt%29%20and%20control%0Ametadata%20%28e.g.%2C%20crop%20size%2C%20random%20flip%20flag%2C%20etc.%29%20on%20the%20model%20performance%2C%0Aand%20%28ii%29~the%20transfer%20of%20the%20representations%20learned%20on%20smaller%20and%0Alower-resolution%20datasets%20to%20larger%20ones%20on%20the%20training%20efficiency%20and%20model%0Aperformance.%20We%20then%20propose%20a%20novel%20conditioning%20mechanism%20that%20disentangles%0Asemantic%20and%20control%20metadata%20conditionings%20and%20sets%20a%20new%20state-of-the-art%20in%0Aclass-conditional%20generation%20on%20the%20ImageNet-1k%20dataset%20--%20with%20FID%0Aimprovements%20of%207%25%20on%20256%20and%208%25%20on%20512%20resolutions%20--%20as%20well%20as%20text-to-image%0Ageneration%20on%20the%20CC12M%20dataset%20--%20with%20FID%20improvements%20of%208%25%20on%20256%20and%2023%25%0Aon%20512%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Improved%2520Conditioning%2520Mechanisms%2520and%2520Pre-training%2520Strategies%2520for%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DTariq%2520Berrada%2520Ifriqi%2520and%2520Pietro%2520Astolfi%2520and%2520Melissa%2520Hall%2520and%2520Reyhane%2520Askari-Hemmat%2520and%2520Yohann%2520Benchetrit%2520and%2520Marton%2520Havasi%2520and%2520Matthew%2520Muckley%2520and%2520Karteek%2520Alahari%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Jakob%2520Verbeek%2520and%2520Michal%2520Drozdzal%26entry.1292438233%3D%2520%2520Large-scale%2520training%2520of%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520has%2520enabled%250Aunprecedented%2520quality%2520in%2520image%2520generation.%2520However%252C%2520the%2520key%2520components%2520of%2520the%250Abest%2520performing%2520LDM%2520training%2520recipes%2520are%2520oftentimes%2520not%2520available%2520to%2520the%250Aresearch%2520community%252C%2520preventing%2520apple-to-apple%2520comparisons%2520and%2520hindering%2520the%250Avalidation%2520of%2520progress%2520in%2520the%2520field.%2520In%2520this%2520work%252C%2520we%2520perform%2520an%2520in-depth%2520study%250Aof%2520LDM%2520training%2520recipes%2520focusing%2520on%2520the%2520performance%2520of%2520models%2520and%2520their%250Atraining%2520efficiency.%2520To%2520ensure%2520apple-to-apple%2520comparisons%252C%2520we%2520re-implement%2520five%250Apreviously%2520published%2520models%2520with%2520their%2520corresponding%2520recipes.%2520Through%2520our%250Astudy%252C%2520we%2520explore%2520the%2520effects%2520of%2520%2528i%2529~the%2520mechanisms%2520used%2520to%2520condition%2520the%250Agenerative%2520model%2520on%2520semantic%2520information%2520%2528e.g.%252C%2520text%2520prompt%2529%2520and%2520control%250Ametadata%2520%2528e.g.%252C%2520crop%2520size%252C%2520random%2520flip%2520flag%252C%2520etc.%2529%2520on%2520the%2520model%2520performance%252C%250Aand%2520%2528ii%2529~the%2520transfer%2520of%2520the%2520representations%2520learned%2520on%2520smaller%2520and%250Alower-resolution%2520datasets%2520to%2520larger%2520ones%2520on%2520the%2520training%2520efficiency%2520and%2520model%250Aperformance.%2520We%2520then%2520propose%2520a%2520novel%2520conditioning%2520mechanism%2520that%2520disentangles%250Asemantic%2520and%2520control%2520metadata%2520conditionings%2520and%2520sets%2520a%2520new%2520state-of-the-art%2520in%250Aclass-conditional%2520generation%2520on%2520the%2520ImageNet-1k%2520dataset%2520--%2520with%2520FID%250Aimprovements%2520of%25207%2525%2520on%2520256%2520and%25208%2525%2520on%2520512%2520resolutions%2520--%2520as%2520well%2520as%2520text-to-image%250Ageneration%2520on%2520the%2520CC12M%2520dataset%2520--%2520with%2520FID%2520improvements%2520of%25208%2525%2520on%2520256%2520and%252023%2525%250Aon%2520512%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Improved%20Conditioning%20Mechanisms%20and%20Pre-training%20Strategies%20for%0A%20%20Diffusion%20Models&entry.906535625=Tariq%20Berrada%20Ifriqi%20and%20Pietro%20Astolfi%20and%20Melissa%20Hall%20and%20Reyhane%20Askari-Hemmat%20and%20Yohann%20Benchetrit%20and%20Marton%20Havasi%20and%20Matthew%20Muckley%20and%20Karteek%20Alahari%20and%20Adriana%20Romero-Soriano%20and%20Jakob%20Verbeek%20and%20Michal%20Drozdzal&entry.1292438233=%20%20Large-scale%20training%20of%20latent%20diffusion%20models%20%28LDMs%29%20has%20enabled%0Aunprecedented%20quality%20in%20image%20generation.%20However%2C%20the%20key%20components%20of%20the%0Abest%20performing%20LDM%20training%20recipes%20are%20oftentimes%20not%20available%20to%20the%0Aresearch%20community%2C%20preventing%20apple-to-apple%20comparisons%20and%20hindering%20the%0Avalidation%20of%20progress%20in%20the%20field.%20In%20this%20work%2C%20we%20perform%20an%20in-depth%20study%0Aof%20LDM%20training%20recipes%20focusing%20on%20the%20performance%20of%20models%20and%20their%0Atraining%20efficiency.%20To%20ensure%20apple-to-apple%20comparisons%2C%20we%20re-implement%20five%0Apreviously%20published%20models%20with%20their%20corresponding%20recipes.%20Through%20our%0Astudy%2C%20we%20explore%20the%20effects%20of%20%28i%29~the%20mechanisms%20used%20to%20condition%20the%0Agenerative%20model%20on%20semantic%20information%20%28e.g.%2C%20text%20prompt%29%20and%20control%0Ametadata%20%28e.g.%2C%20crop%20size%2C%20random%20flip%20flag%2C%20etc.%29%20on%20the%20model%20performance%2C%0Aand%20%28ii%29~the%20transfer%20of%20the%20representations%20learned%20on%20smaller%20and%0Alower-resolution%20datasets%20to%20larger%20ones%20on%20the%20training%20efficiency%20and%20model%0Aperformance.%20We%20then%20propose%20a%20novel%20conditioning%20mechanism%20that%20disentangles%0Asemantic%20and%20control%20metadata%20conditionings%20and%20sets%20a%20new%20state-of-the-art%20in%0Aclass-conditional%20generation%20on%20the%20ImageNet-1k%20dataset%20--%20with%20FID%0Aimprovements%20of%207%25%20on%20256%20and%208%25%20on%20512%20resolutions%20--%20as%20well%20as%20text-to-image%0Ageneration%20on%20the%20CC12M%20dataset%20--%20with%20FID%20improvements%20of%208%25%20on%20256%20and%2023%25%0Aon%20512%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03177v1&entry.124074799=Read"},
{"title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content", "author": "Joao Monteiro and Pierre-Andre Noel and Etienne Marcotte and Sai Rajeswar and Valentina Zantedeschi and David Vazquez and Nicolas Chapados and Christopher Pal and Perouz Taslakian", "abstract": "  Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.\n", "link": "http://arxiv.org/abs/2406.11811v2", "date": "2024-11-05", "relevancy": 1.9885, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepLiQA%3A%20A%20Question-Answering%20Dataset%20for%20Benchmarking%20LLMs%20on%20Unseen%0A%20%20Reference%20Content&body=Title%3A%20RepLiQA%3A%20A%20Question-Answering%20Dataset%20for%20Benchmarking%20LLMs%20on%20Unseen%0A%20%20Reference%20Content%0AAuthor%3A%20Joao%20Monteiro%20and%20Pierre-Andre%20Noel%20and%20Etienne%20Marcotte%20and%20Sai%20Rajeswar%20and%20Valentina%20Zantedeschi%20and%20David%20Vazquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20vast%20amounts%20of%20data%2C%20most%20of%0Awhich%20is%20automatically%20scraped%20from%20the%20internet.%20This%20data%20includes%0Aencyclopedic%20documents%20that%20harbor%20a%20vast%20amount%20of%20general%20knowledge%20%28e.g.%2C%0AWikipedia%29%20but%20also%20potentially%20overlap%20with%20benchmark%20datasets%20used%20for%0Aevaluating%20LLMs.%20Consequently%2C%20evaluating%20models%20on%20test%20splits%20that%20might%20have%0Aleaked%20into%20the%20training%20set%20is%20prone%20to%20misleading%20conclusions.%20To%20foster%0Asound%20evaluation%20of%20language%20models%2C%20we%20introduce%20a%20new%20test%20dataset%20named%0ARepLiQA%2C%20suited%20for%20question-answering%20and%20topic%20retrieval%20tasks.%20RepLiQA%20is%20a%0Acollection%20of%20five%20splits%20of%20test%20sets%2C%20four%20of%20which%20have%20not%20been%20released%20to%0Athe%20internet%20or%20exposed%20to%20LLM%20APIs%20prior%20to%20this%20publication.%20Each%20sample%20in%0ARepLiQA%20comprises%20%281%29%20a%20reference%20document%20crafted%20by%20a%20human%20annotator%20and%0Adepicting%20an%20imaginary%20scenario%20%28e.g.%2C%20a%20news%20article%29%20absent%20from%20the%0Ainternet%3B%20%282%29%20a%20question%20about%20the%20document%27s%20topic%3B%20%283%29%20a%20ground-truth%20answer%0Aderived%20directly%20from%20the%20information%20in%20the%20document%3B%20and%20%284%29%20the%20paragraph%0Aextracted%20from%20the%20reference%20document%20containing%20the%20answer.%20As%20such%2C%20accurate%0Aanswers%20can%20only%20be%20generated%20if%20a%20model%20can%20find%20relevant%20content%20within%20the%0Aprovided%20document.%20We%20run%20a%20large-scale%20benchmark%20comprising%20several%0Astate-of-the-art%20LLMs%20to%20uncover%20differences%20in%20performance%20across%20models%20of%0Avarious%20types%20and%20sizes%20in%20a%20context-conditional%20language%20modeling%20setting.%0AReleased%20splits%20of%20RepLiQA%20can%20be%20found%20here%3A%0Ahttps%3A//huggingface.co/datasets/ServiceNow/repliqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepLiQA%253A%2520A%2520Question-Answering%2520Dataset%2520for%2520Benchmarking%2520LLMs%2520on%2520Unseen%250A%2520%2520Reference%2520Content%26entry.906535625%3DJoao%2520Monteiro%2520and%2520Pierre-Andre%2520Noel%2520and%2520Etienne%2520Marcotte%2520and%2520Sai%2520Rajeswar%2520and%2520Valentina%2520Zantedeschi%2520and%2520David%2520Vazquez%2520and%2520Nicolas%2520Chapados%2520and%2520Christopher%2520Pal%2520and%2520Perouz%2520Taslakian%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520vast%2520amounts%2520of%2520data%252C%2520most%2520of%250Awhich%2520is%2520automatically%2520scraped%2520from%2520the%2520internet.%2520This%2520data%2520includes%250Aencyclopedic%2520documents%2520that%2520harbor%2520a%2520vast%2520amount%2520of%2520general%2520knowledge%2520%2528e.g.%252C%250AWikipedia%2529%2520but%2520also%2520potentially%2520overlap%2520with%2520benchmark%2520datasets%2520used%2520for%250Aevaluating%2520LLMs.%2520Consequently%252C%2520evaluating%2520models%2520on%2520test%2520splits%2520that%2520might%2520have%250Aleaked%2520into%2520the%2520training%2520set%2520is%2520prone%2520to%2520misleading%2520conclusions.%2520To%2520foster%250Asound%2520evaluation%2520of%2520language%2520models%252C%2520we%2520introduce%2520a%2520new%2520test%2520dataset%2520named%250ARepLiQA%252C%2520suited%2520for%2520question-answering%2520and%2520topic%2520retrieval%2520tasks.%2520RepLiQA%2520is%2520a%250Acollection%2520of%2520five%2520splits%2520of%2520test%2520sets%252C%2520four%2520of%2520which%2520have%2520not%2520been%2520released%2520to%250Athe%2520internet%2520or%2520exposed%2520to%2520LLM%2520APIs%2520prior%2520to%2520this%2520publication.%2520Each%2520sample%2520in%250ARepLiQA%2520comprises%2520%25281%2529%2520a%2520reference%2520document%2520crafted%2520by%2520a%2520human%2520annotator%2520and%250Adepicting%2520an%2520imaginary%2520scenario%2520%2528e.g.%252C%2520a%2520news%2520article%2529%2520absent%2520from%2520the%250Ainternet%253B%2520%25282%2529%2520a%2520question%2520about%2520the%2520document%2527s%2520topic%253B%2520%25283%2529%2520a%2520ground-truth%2520answer%250Aderived%2520directly%2520from%2520the%2520information%2520in%2520the%2520document%253B%2520and%2520%25284%2529%2520the%2520paragraph%250Aextracted%2520from%2520the%2520reference%2520document%2520containing%2520the%2520answer.%2520As%2520such%252C%2520accurate%250Aanswers%2520can%2520only%2520be%2520generated%2520if%2520a%2520model%2520can%2520find%2520relevant%2520content%2520within%2520the%250Aprovided%2520document.%2520We%2520run%2520a%2520large-scale%2520benchmark%2520comprising%2520several%250Astate-of-the-art%2520LLMs%2520to%2520uncover%2520differences%2520in%2520performance%2520across%2520models%2520of%250Avarious%2520types%2520and%2520sizes%2520in%2520a%2520context-conditional%2520language%2520modeling%2520setting.%250AReleased%2520splits%2520of%2520RepLiQA%2520can%2520be%2520found%2520here%253A%250Ahttps%253A//huggingface.co/datasets/ServiceNow/repliqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepLiQA%3A%20A%20Question-Answering%20Dataset%20for%20Benchmarking%20LLMs%20on%20Unseen%0A%20%20Reference%20Content&entry.906535625=Joao%20Monteiro%20and%20Pierre-Andre%20Noel%20and%20Etienne%20Marcotte%20and%20Sai%20Rajeswar%20and%20Valentina%20Zantedeschi%20and%20David%20Vazquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20vast%20amounts%20of%20data%2C%20most%20of%0Awhich%20is%20automatically%20scraped%20from%20the%20internet.%20This%20data%20includes%0Aencyclopedic%20documents%20that%20harbor%20a%20vast%20amount%20of%20general%20knowledge%20%28e.g.%2C%0AWikipedia%29%20but%20also%20potentially%20overlap%20with%20benchmark%20datasets%20used%20for%0Aevaluating%20LLMs.%20Consequently%2C%20evaluating%20models%20on%20test%20splits%20that%20might%20have%0Aleaked%20into%20the%20training%20set%20is%20prone%20to%20misleading%20conclusions.%20To%20foster%0Asound%20evaluation%20of%20language%20models%2C%20we%20introduce%20a%20new%20test%20dataset%20named%0ARepLiQA%2C%20suited%20for%20question-answering%20and%20topic%20retrieval%20tasks.%20RepLiQA%20is%20a%0Acollection%20of%20five%20splits%20of%20test%20sets%2C%20four%20of%20which%20have%20not%20been%20released%20to%0Athe%20internet%20or%20exposed%20to%20LLM%20APIs%20prior%20to%20this%20publication.%20Each%20sample%20in%0ARepLiQA%20comprises%20%281%29%20a%20reference%20document%20crafted%20by%20a%20human%20annotator%20and%0Adepicting%20an%20imaginary%20scenario%20%28e.g.%2C%20a%20news%20article%29%20absent%20from%20the%0Ainternet%3B%20%282%29%20a%20question%20about%20the%20document%27s%20topic%3B%20%283%29%20a%20ground-truth%20answer%0Aderived%20directly%20from%20the%20information%20in%20the%20document%3B%20and%20%284%29%20the%20paragraph%0Aextracted%20from%20the%20reference%20document%20containing%20the%20answer.%20As%20such%2C%20accurate%0Aanswers%20can%20only%20be%20generated%20if%20a%20model%20can%20find%20relevant%20content%20within%20the%0Aprovided%20document.%20We%20run%20a%20large-scale%20benchmark%20comprising%20several%0Astate-of-the-art%20LLMs%20to%20uncover%20differences%20in%20performance%20across%20models%20of%0Avarious%20types%20and%20sizes%20in%20a%20context-conditional%20language%20modeling%20setting.%0AReleased%20splits%20of%20RepLiQA%20can%20be%20found%20here%3A%0Ahttps%3A//huggingface.co/datasets/ServiceNow/repliqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11811v2&entry.124074799=Read"},
{"title": "Unleashing the power of novel conditional generative approaches for new\n  materials discovery", "author": "Lev Novitskiy and Vladimir Lazarev and Mikhail Tiutiulnikov and Nikita Vakhrameev and Roman Eremin and Innokentiy Humonen and Andrey Kuznetsov and Denis Dimitrov and Semen Budennyy", "abstract": "  For a very long time, computational approaches to the design of new materials\nhave relied on an iterative process of finding a candidate material and\nmodeling its properties. AI has played a crucial role in this regard, helping\nto accelerate the discovery and optimization of crystal properties and\nstructures through advanced computational methodologies and data-driven\napproaches. To address the problem of new materials design and fasten the\nprocess of new materials search, we have applied latest generative approaches\nto the problem of crystal structure design, trying to solve the inverse\nproblem: by given properties generate a structure that satisfies them without\nutilizing supercomputer powers. In our work we propose two approaches: 1)\nconditional structure modification: optimization of the stability of an\narbitrary atomic configuration, using the energy difference between the most\nenergetically favorable structure and all its less stable polymorphs and 2)\nconditional structure generation. We used a representation for materials that\nincludes the following information: lattice, atom coordinates, atom types,\nchemical features, space group and formation energy of the structure. The loss\nfunction was optimized to take into account the periodic boundary conditions of\ncrystal structures. We have applied Diffusion models approach, Flow matching,\nusual Autoencoder (AE) and compared the results of the models and approaches.\nAs a metric for the study, physical PyMatGen matcher was employed: we compare\ntarget structure with generated one using default tolerances. So far, our\nmodifier and generator produce structures with needed properties with accuracy\n41% and 82% respectively. To prove the offered methodology efficiency,\ninference have been carried out, resulting in several potentially new\nstructures with formation energy below the AFLOW-derived convex hulls.\n", "link": "http://arxiv.org/abs/2411.03156v1", "date": "2024-11-05", "relevancy": 1.9818, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5364}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4908}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20power%20of%20novel%20conditional%20generative%20approaches%20for%20new%0A%20%20materials%20discovery&body=Title%3A%20Unleashing%20the%20power%20of%20novel%20conditional%20generative%20approaches%20for%20new%0A%20%20materials%20discovery%0AAuthor%3A%20Lev%20Novitskiy%20and%20Vladimir%20Lazarev%20and%20Mikhail%20Tiutiulnikov%20and%20Nikita%20Vakhrameev%20and%20Roman%20Eremin%20and%20Innokentiy%20Humonen%20and%20Andrey%20Kuznetsov%20and%20Denis%20Dimitrov%20and%20Semen%20Budennyy%0AAbstract%3A%20%20%20For%20a%20very%20long%20time%2C%20computational%20approaches%20to%20the%20design%20of%20new%20materials%0Ahave%20relied%20on%20an%20iterative%20process%20of%20finding%20a%20candidate%20material%20and%0Amodeling%20its%20properties.%20AI%20has%20played%20a%20crucial%20role%20in%20this%20regard%2C%20helping%0Ato%20accelerate%20the%20discovery%20and%20optimization%20of%20crystal%20properties%20and%0Astructures%20through%20advanced%20computational%20methodologies%20and%20data-driven%0Aapproaches.%20To%20address%20the%20problem%20of%20new%20materials%20design%20and%20fasten%20the%0Aprocess%20of%20new%20materials%20search%2C%20we%20have%20applied%20latest%20generative%20approaches%0Ato%20the%20problem%20of%20crystal%20structure%20design%2C%20trying%20to%20solve%20the%20inverse%0Aproblem%3A%20by%20given%20properties%20generate%20a%20structure%20that%20satisfies%20them%20without%0Autilizing%20supercomputer%20powers.%20In%20our%20work%20we%20propose%20two%20approaches%3A%201%29%0Aconditional%20structure%20modification%3A%20optimization%20of%20the%20stability%20of%20an%0Aarbitrary%20atomic%20configuration%2C%20using%20the%20energy%20difference%20between%20the%20most%0Aenergetically%20favorable%20structure%20and%20all%20its%20less%20stable%20polymorphs%20and%202%29%0Aconditional%20structure%20generation.%20We%20used%20a%20representation%20for%20materials%20that%0Aincludes%20the%20following%20information%3A%20lattice%2C%20atom%20coordinates%2C%20atom%20types%2C%0Achemical%20features%2C%20space%20group%20and%20formation%20energy%20of%20the%20structure.%20The%20loss%0Afunction%20was%20optimized%20to%20take%20into%20account%20the%20periodic%20boundary%20conditions%20of%0Acrystal%20structures.%20We%20have%20applied%20Diffusion%20models%20approach%2C%20Flow%20matching%2C%0Ausual%20Autoencoder%20%28AE%29%20and%20compared%20the%20results%20of%20the%20models%20and%20approaches.%0AAs%20a%20metric%20for%20the%20study%2C%20physical%20PyMatGen%20matcher%20was%20employed%3A%20we%20compare%0Atarget%20structure%20with%20generated%20one%20using%20default%20tolerances.%20So%20far%2C%20our%0Amodifier%20and%20generator%20produce%20structures%20with%20needed%20properties%20with%20accuracy%0A41%25%20and%2082%25%20respectively.%20To%20prove%20the%20offered%20methodology%20efficiency%2C%0Ainference%20have%20been%20carried%20out%2C%20resulting%20in%20several%20potentially%20new%0Astructures%20with%20formation%20energy%20below%20the%20AFLOW-derived%20convex%20hulls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520power%2520of%2520novel%2520conditional%2520generative%2520approaches%2520for%2520new%250A%2520%2520materials%2520discovery%26entry.906535625%3DLev%2520Novitskiy%2520and%2520Vladimir%2520Lazarev%2520and%2520Mikhail%2520Tiutiulnikov%2520and%2520Nikita%2520Vakhrameev%2520and%2520Roman%2520Eremin%2520and%2520Innokentiy%2520Humonen%2520and%2520Andrey%2520Kuznetsov%2520and%2520Denis%2520Dimitrov%2520and%2520Semen%2520Budennyy%26entry.1292438233%3D%2520%2520For%2520a%2520very%2520long%2520time%252C%2520computational%2520approaches%2520to%2520the%2520design%2520of%2520new%2520materials%250Ahave%2520relied%2520on%2520an%2520iterative%2520process%2520of%2520finding%2520a%2520candidate%2520material%2520and%250Amodeling%2520its%2520properties.%2520AI%2520has%2520played%2520a%2520crucial%2520role%2520in%2520this%2520regard%252C%2520helping%250Ato%2520accelerate%2520the%2520discovery%2520and%2520optimization%2520of%2520crystal%2520properties%2520and%250Astructures%2520through%2520advanced%2520computational%2520methodologies%2520and%2520data-driven%250Aapproaches.%2520To%2520address%2520the%2520problem%2520of%2520new%2520materials%2520design%2520and%2520fasten%2520the%250Aprocess%2520of%2520new%2520materials%2520search%252C%2520we%2520have%2520applied%2520latest%2520generative%2520approaches%250Ato%2520the%2520problem%2520of%2520crystal%2520structure%2520design%252C%2520trying%2520to%2520solve%2520the%2520inverse%250Aproblem%253A%2520by%2520given%2520properties%2520generate%2520a%2520structure%2520that%2520satisfies%2520them%2520without%250Autilizing%2520supercomputer%2520powers.%2520In%2520our%2520work%2520we%2520propose%2520two%2520approaches%253A%25201%2529%250Aconditional%2520structure%2520modification%253A%2520optimization%2520of%2520the%2520stability%2520of%2520an%250Aarbitrary%2520atomic%2520configuration%252C%2520using%2520the%2520energy%2520difference%2520between%2520the%2520most%250Aenergetically%2520favorable%2520structure%2520and%2520all%2520its%2520less%2520stable%2520polymorphs%2520and%25202%2529%250Aconditional%2520structure%2520generation.%2520We%2520used%2520a%2520representation%2520for%2520materials%2520that%250Aincludes%2520the%2520following%2520information%253A%2520lattice%252C%2520atom%2520coordinates%252C%2520atom%2520types%252C%250Achemical%2520features%252C%2520space%2520group%2520and%2520formation%2520energy%2520of%2520the%2520structure.%2520The%2520loss%250Afunction%2520was%2520optimized%2520to%2520take%2520into%2520account%2520the%2520periodic%2520boundary%2520conditions%2520of%250Acrystal%2520structures.%2520We%2520have%2520applied%2520Diffusion%2520models%2520approach%252C%2520Flow%2520matching%252C%250Ausual%2520Autoencoder%2520%2528AE%2529%2520and%2520compared%2520the%2520results%2520of%2520the%2520models%2520and%2520approaches.%250AAs%2520a%2520metric%2520for%2520the%2520study%252C%2520physical%2520PyMatGen%2520matcher%2520was%2520employed%253A%2520we%2520compare%250Atarget%2520structure%2520with%2520generated%2520one%2520using%2520default%2520tolerances.%2520So%2520far%252C%2520our%250Amodifier%2520and%2520generator%2520produce%2520structures%2520with%2520needed%2520properties%2520with%2520accuracy%250A41%2525%2520and%252082%2525%2520respectively.%2520To%2520prove%2520the%2520offered%2520methodology%2520efficiency%252C%250Ainference%2520have%2520been%2520carried%2520out%252C%2520resulting%2520in%2520several%2520potentially%2520new%250Astructures%2520with%2520formation%2520energy%2520below%2520the%2520AFLOW-derived%2520convex%2520hulls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20power%20of%20novel%20conditional%20generative%20approaches%20for%20new%0A%20%20materials%20discovery&entry.906535625=Lev%20Novitskiy%20and%20Vladimir%20Lazarev%20and%20Mikhail%20Tiutiulnikov%20and%20Nikita%20Vakhrameev%20and%20Roman%20Eremin%20and%20Innokentiy%20Humonen%20and%20Andrey%20Kuznetsov%20and%20Denis%20Dimitrov%20and%20Semen%20Budennyy&entry.1292438233=%20%20For%20a%20very%20long%20time%2C%20computational%20approaches%20to%20the%20design%20of%20new%20materials%0Ahave%20relied%20on%20an%20iterative%20process%20of%20finding%20a%20candidate%20material%20and%0Amodeling%20its%20properties.%20AI%20has%20played%20a%20crucial%20role%20in%20this%20regard%2C%20helping%0Ato%20accelerate%20the%20discovery%20and%20optimization%20of%20crystal%20properties%20and%0Astructures%20through%20advanced%20computational%20methodologies%20and%20data-driven%0Aapproaches.%20To%20address%20the%20problem%20of%20new%20materials%20design%20and%20fasten%20the%0Aprocess%20of%20new%20materials%20search%2C%20we%20have%20applied%20latest%20generative%20approaches%0Ato%20the%20problem%20of%20crystal%20structure%20design%2C%20trying%20to%20solve%20the%20inverse%0Aproblem%3A%20by%20given%20properties%20generate%20a%20structure%20that%20satisfies%20them%20without%0Autilizing%20supercomputer%20powers.%20In%20our%20work%20we%20propose%20two%20approaches%3A%201%29%0Aconditional%20structure%20modification%3A%20optimization%20of%20the%20stability%20of%20an%0Aarbitrary%20atomic%20configuration%2C%20using%20the%20energy%20difference%20between%20the%20most%0Aenergetically%20favorable%20structure%20and%20all%20its%20less%20stable%20polymorphs%20and%202%29%0Aconditional%20structure%20generation.%20We%20used%20a%20representation%20for%20materials%20that%0Aincludes%20the%20following%20information%3A%20lattice%2C%20atom%20coordinates%2C%20atom%20types%2C%0Achemical%20features%2C%20space%20group%20and%20formation%20energy%20of%20the%20structure.%20The%20loss%0Afunction%20was%20optimized%20to%20take%20into%20account%20the%20periodic%20boundary%20conditions%20of%0Acrystal%20structures.%20We%20have%20applied%20Diffusion%20models%20approach%2C%20Flow%20matching%2C%0Ausual%20Autoencoder%20%28AE%29%20and%20compared%20the%20results%20of%20the%20models%20and%20approaches.%0AAs%20a%20metric%20for%20the%20study%2C%20physical%20PyMatGen%20matcher%20was%20employed%3A%20we%20compare%0Atarget%20structure%20with%20generated%20one%20using%20default%20tolerances.%20So%20far%2C%20our%0Amodifier%20and%20generator%20produce%20structures%20with%20needed%20properties%20with%20accuracy%0A41%25%20and%2082%25%20respectively.%20To%20prove%20the%20offered%20methodology%20efficiency%2C%0Ainference%20have%20been%20carried%20out%2C%20resulting%20in%20several%20potentially%20new%0Astructures%20with%20formation%20energy%20below%20the%20AFLOW-derived%20convex%20hulls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03156v1&entry.124074799=Read"},
{"title": "Graph-Based Semi-Supervised Segregated Lipschitz Learning", "author": "Farid Bozorgnia and Yassine Belkheiri and Abderrahim Elmoataz", "abstract": "  This paper presents an approach to semi-supervised learning for the\nclassification of data using the Lipschitz Learning on graphs. We develop a\ngraph-based semi-supervised learning framework that leverages the properties of\nthe infinity Laplacian to propagate labels in a dataset where only a few\nsamples are labeled. By extending the theory of spatial segregation from the\nLaplace operator to the infinity Laplace operator, both in continuum and\ndiscrete settings, our approach provides a robust method for dealing with class\nimbalance, a common challenge in machine learning. Experimental validation on\nseveral benchmark datasets demonstrates that our method not only improves\nclassification accuracy compared to existing methods but also ensures efficient\nlabel propagation in scenarios with limited labeled data.\n", "link": "http://arxiv.org/abs/2411.03273v1", "date": "2024-11-05", "relevancy": 1.9813, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5443}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.464}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Semi-Supervised%20Segregated%20Lipschitz%20Learning&body=Title%3A%20Graph-Based%20Semi-Supervised%20Segregated%20Lipschitz%20Learning%0AAuthor%3A%20Farid%20Bozorgnia%20and%20Yassine%20Belkheiri%20and%20Abderrahim%20Elmoataz%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20approach%20to%20semi-supervised%20learning%20for%20the%0Aclassification%20of%20data%20using%20the%20Lipschitz%20Learning%20on%20graphs.%20We%20develop%20a%0Agraph-based%20semi-supervised%20learning%20framework%20that%20leverages%20the%20properties%20of%0Athe%20infinity%20Laplacian%20to%20propagate%20labels%20in%20a%20dataset%20where%20only%20a%20few%0Asamples%20are%20labeled.%20By%20extending%20the%20theory%20of%20spatial%20segregation%20from%20the%0ALaplace%20operator%20to%20the%20infinity%20Laplace%20operator%2C%20both%20in%20continuum%20and%0Adiscrete%20settings%2C%20our%20approach%20provides%20a%20robust%20method%20for%20dealing%20with%20class%0Aimbalance%2C%20a%20common%20challenge%20in%20machine%20learning.%20Experimental%20validation%20on%0Aseveral%20benchmark%20datasets%20demonstrates%20that%20our%20method%20not%20only%20improves%0Aclassification%20accuracy%20compared%20to%20existing%20methods%20but%20also%20ensures%20efficient%0Alabel%20propagation%20in%20scenarios%20with%20limited%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Semi-Supervised%2520Segregated%2520Lipschitz%2520Learning%26entry.906535625%3DFarid%2520Bozorgnia%2520and%2520Yassine%2520Belkheiri%2520and%2520Abderrahim%2520Elmoataz%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520approach%2520to%2520semi-supervised%2520learning%2520for%2520the%250Aclassification%2520of%2520data%2520using%2520the%2520Lipschitz%2520Learning%2520on%2520graphs.%2520We%2520develop%2520a%250Agraph-based%2520semi-supervised%2520learning%2520framework%2520that%2520leverages%2520the%2520properties%2520of%250Athe%2520infinity%2520Laplacian%2520to%2520propagate%2520labels%2520in%2520a%2520dataset%2520where%2520only%2520a%2520few%250Asamples%2520are%2520labeled.%2520By%2520extending%2520the%2520theory%2520of%2520spatial%2520segregation%2520from%2520the%250ALaplace%2520operator%2520to%2520the%2520infinity%2520Laplace%2520operator%252C%2520both%2520in%2520continuum%2520and%250Adiscrete%2520settings%252C%2520our%2520approach%2520provides%2520a%2520robust%2520method%2520for%2520dealing%2520with%2520class%250Aimbalance%252C%2520a%2520common%2520challenge%2520in%2520machine%2520learning.%2520Experimental%2520validation%2520on%250Aseveral%2520benchmark%2520datasets%2520demonstrates%2520that%2520our%2520method%2520not%2520only%2520improves%250Aclassification%2520accuracy%2520compared%2520to%2520existing%2520methods%2520but%2520also%2520ensures%2520efficient%250Alabel%2520propagation%2520in%2520scenarios%2520with%2520limited%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Semi-Supervised%20Segregated%20Lipschitz%20Learning&entry.906535625=Farid%20Bozorgnia%20and%20Yassine%20Belkheiri%20and%20Abderrahim%20Elmoataz&entry.1292438233=%20%20This%20paper%20presents%20an%20approach%20to%20semi-supervised%20learning%20for%20the%0Aclassification%20of%20data%20using%20the%20Lipschitz%20Learning%20on%20graphs.%20We%20develop%20a%0Agraph-based%20semi-supervised%20learning%20framework%20that%20leverages%20the%20properties%20of%0Athe%20infinity%20Laplacian%20to%20propagate%20labels%20in%20a%20dataset%20where%20only%20a%20few%0Asamples%20are%20labeled.%20By%20extending%20the%20theory%20of%20spatial%20segregation%20from%20the%0ALaplace%20operator%20to%20the%20infinity%20Laplace%20operator%2C%20both%20in%20continuum%20and%0Adiscrete%20settings%2C%20our%20approach%20provides%20a%20robust%20method%20for%20dealing%20with%20class%0Aimbalance%2C%20a%20common%20challenge%20in%20machine%20learning.%20Experimental%20validation%20on%0Aseveral%20benchmark%20datasets%20demonstrates%20that%20our%20method%20not%20only%20improves%0Aclassification%20accuracy%20compared%20to%20existing%20methods%20but%20also%20ensures%20efficient%0Alabel%20propagation%20in%20scenarios%20with%20limited%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03273v1&entry.124074799=Read"},
{"title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and\n  Evaluation Metrics", "author": "Nicol\u00f2 Romandini and Alessio Mora and Carlo Mazzocca and Rebecca Montanari and Paolo Bellavista", "abstract": "  Federated learning (FL) enables collaborative training of a machine learning\n(ML) model across multiple parties, facilitating the preservation of users' and\ninstitutions' privacy by maintaining data stored locally. Instead of\ncentralizing raw data, FL exchanges locally refined model parameters to build a\nglobal model incrementally. While FL is more compliant with emerging\nregulations such as the European General Data Protection Regulation (GDPR),\nensuring the right to be forgotten in this context - allowing FL participants\nto remove their data contributions from the learned model - remains unclear. In\naddition, it is recognized that malicious clients may inject backdoors into the\nglobal model through updates, e.g., to generate mispredictions on specially\ncrafted data examples. Consequently, there is the need for mechanisms that can\nguarantee individuals the possibility to remove their data and erase malicious\ncontributions even after aggregation, without compromising the already acquired\n\"good\" knowledge. This highlights the necessity for novel federated unlearning\n(FU) algorithms, which can efficiently remove specific clients' contributions\nwithout full model retraining. This article provides background concepts,\nempirical evidence, and practical guidelines to design/implement efficient FU\nschemes. This study includes a detailed analysis of the metrics for evaluating\nunlearning in FL and presents an in-depth literature review categorizing\nstate-of-the-art FU contributions under a novel taxonomy. Finally, we outline\nthe most relevant and still open technical challenges, by identifying the most\npromising research directions in the field.\n", "link": "http://arxiv.org/abs/2401.05146v3", "date": "2024-11-05", "relevancy": 1.9783, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Unlearning%3A%20A%20Survey%20on%20Methods%2C%20Design%20Guidelines%2C%20and%0A%20%20Evaluation%20Metrics&body=Title%3A%20Federated%20Unlearning%3A%20A%20Survey%20on%20Methods%2C%20Design%20Guidelines%2C%20and%0A%20%20Evaluation%20Metrics%0AAuthor%3A%20Nicol%C3%B2%20Romandini%20and%20Alessio%20Mora%20and%20Carlo%20Mazzocca%20and%20Rebecca%20Montanari%20and%20Paolo%20Bellavista%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20training%20of%20a%20machine%20learning%0A%28ML%29%20model%20across%20multiple%20parties%2C%20facilitating%20the%20preservation%20of%20users%27%20and%0Ainstitutions%27%20privacy%20by%20maintaining%20data%20stored%20locally.%20Instead%20of%0Acentralizing%20raw%20data%2C%20FL%20exchanges%20locally%20refined%20model%20parameters%20to%20build%20a%0Aglobal%20model%20incrementally.%20While%20FL%20is%20more%20compliant%20with%20emerging%0Aregulations%20such%20as%20the%20European%20General%20Data%20Protection%20Regulation%20%28GDPR%29%2C%0Aensuring%20the%20right%20to%20be%20forgotten%20in%20this%20context%20-%20allowing%20FL%20participants%0Ato%20remove%20their%20data%20contributions%20from%20the%20learned%20model%20-%20remains%20unclear.%20In%0Aaddition%2C%20it%20is%20recognized%20that%20malicious%20clients%20may%20inject%20backdoors%20into%20the%0Aglobal%20model%20through%20updates%2C%20e.g.%2C%20to%20generate%20mispredictions%20on%20specially%0Acrafted%20data%20examples.%20Consequently%2C%20there%20is%20the%20need%20for%20mechanisms%20that%20can%0Aguarantee%20individuals%20the%20possibility%20to%20remove%20their%20data%20and%20erase%20malicious%0Acontributions%20even%20after%20aggregation%2C%20without%20compromising%20the%20already%20acquired%0A%22good%22%20knowledge.%20This%20highlights%20the%20necessity%20for%20novel%20federated%20unlearning%0A%28FU%29%20algorithms%2C%20which%20can%20efficiently%20remove%20specific%20clients%27%20contributions%0Awithout%20full%20model%20retraining.%20This%20article%20provides%20background%20concepts%2C%0Aempirical%20evidence%2C%20and%20practical%20guidelines%20to%20design/implement%20efficient%20FU%0Aschemes.%20This%20study%20includes%20a%20detailed%20analysis%20of%20the%20metrics%20for%20evaluating%0Aunlearning%20in%20FL%20and%20presents%20an%20in-depth%20literature%20review%20categorizing%0Astate-of-the-art%20FU%20contributions%20under%20a%20novel%20taxonomy.%20Finally%2C%20we%20outline%0Athe%20most%20relevant%20and%20still%20open%20technical%20challenges%2C%20by%20identifying%20the%20most%0Apromising%20research%20directions%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05146v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Unlearning%253A%2520A%2520Survey%2520on%2520Methods%252C%2520Design%2520Guidelines%252C%2520and%250A%2520%2520Evaluation%2520Metrics%26entry.906535625%3DNicol%25C3%25B2%2520Romandini%2520and%2520Alessio%2520Mora%2520and%2520Carlo%2520Mazzocca%2520and%2520Rebecca%2520Montanari%2520and%2520Paolo%2520Bellavista%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520of%2520a%2520machine%2520learning%250A%2528ML%2529%2520model%2520across%2520multiple%2520parties%252C%2520facilitating%2520the%2520preservation%2520of%2520users%2527%2520and%250Ainstitutions%2527%2520privacy%2520by%2520maintaining%2520data%2520stored%2520locally.%2520Instead%2520of%250Acentralizing%2520raw%2520data%252C%2520FL%2520exchanges%2520locally%2520refined%2520model%2520parameters%2520to%2520build%2520a%250Aglobal%2520model%2520incrementally.%2520While%2520FL%2520is%2520more%2520compliant%2520with%2520emerging%250Aregulations%2520such%2520as%2520the%2520European%2520General%2520Data%2520Protection%2520Regulation%2520%2528GDPR%2529%252C%250Aensuring%2520the%2520right%2520to%2520be%2520forgotten%2520in%2520this%2520context%2520-%2520allowing%2520FL%2520participants%250Ato%2520remove%2520their%2520data%2520contributions%2520from%2520the%2520learned%2520model%2520-%2520remains%2520unclear.%2520In%250Aaddition%252C%2520it%2520is%2520recognized%2520that%2520malicious%2520clients%2520may%2520inject%2520backdoors%2520into%2520the%250Aglobal%2520model%2520through%2520updates%252C%2520e.g.%252C%2520to%2520generate%2520mispredictions%2520on%2520specially%250Acrafted%2520data%2520examples.%2520Consequently%252C%2520there%2520is%2520the%2520need%2520for%2520mechanisms%2520that%2520can%250Aguarantee%2520individuals%2520the%2520possibility%2520to%2520remove%2520their%2520data%2520and%2520erase%2520malicious%250Acontributions%2520even%2520after%2520aggregation%252C%2520without%2520compromising%2520the%2520already%2520acquired%250A%2522good%2522%2520knowledge.%2520This%2520highlights%2520the%2520necessity%2520for%2520novel%2520federated%2520unlearning%250A%2528FU%2529%2520algorithms%252C%2520which%2520can%2520efficiently%2520remove%2520specific%2520clients%2527%2520contributions%250Awithout%2520full%2520model%2520retraining.%2520This%2520article%2520provides%2520background%2520concepts%252C%250Aempirical%2520evidence%252C%2520and%2520practical%2520guidelines%2520to%2520design/implement%2520efficient%2520FU%250Aschemes.%2520This%2520study%2520includes%2520a%2520detailed%2520analysis%2520of%2520the%2520metrics%2520for%2520evaluating%250Aunlearning%2520in%2520FL%2520and%2520presents%2520an%2520in-depth%2520literature%2520review%2520categorizing%250Astate-of-the-art%2520FU%2520contributions%2520under%2520a%2520novel%2520taxonomy.%2520Finally%252C%2520we%2520outline%250Athe%2520most%2520relevant%2520and%2520still%2520open%2520technical%2520challenges%252C%2520by%2520identifying%2520the%2520most%250Apromising%2520research%2520directions%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05146v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Unlearning%3A%20A%20Survey%20on%20Methods%2C%20Design%20Guidelines%2C%20and%0A%20%20Evaluation%20Metrics&entry.906535625=Nicol%C3%B2%20Romandini%20and%20Alessio%20Mora%20and%20Carlo%20Mazzocca%20and%20Rebecca%20Montanari%20and%20Paolo%20Bellavista&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20training%20of%20a%20machine%20learning%0A%28ML%29%20model%20across%20multiple%20parties%2C%20facilitating%20the%20preservation%20of%20users%27%20and%0Ainstitutions%27%20privacy%20by%20maintaining%20data%20stored%20locally.%20Instead%20of%0Acentralizing%20raw%20data%2C%20FL%20exchanges%20locally%20refined%20model%20parameters%20to%20build%20a%0Aglobal%20model%20incrementally.%20While%20FL%20is%20more%20compliant%20with%20emerging%0Aregulations%20such%20as%20the%20European%20General%20Data%20Protection%20Regulation%20%28GDPR%29%2C%0Aensuring%20the%20right%20to%20be%20forgotten%20in%20this%20context%20-%20allowing%20FL%20participants%0Ato%20remove%20their%20data%20contributions%20from%20the%20learned%20model%20-%20remains%20unclear.%20In%0Aaddition%2C%20it%20is%20recognized%20that%20malicious%20clients%20may%20inject%20backdoors%20into%20the%0Aglobal%20model%20through%20updates%2C%20e.g.%2C%20to%20generate%20mispredictions%20on%20specially%0Acrafted%20data%20examples.%20Consequently%2C%20there%20is%20the%20need%20for%20mechanisms%20that%20can%0Aguarantee%20individuals%20the%20possibility%20to%20remove%20their%20data%20and%20erase%20malicious%0Acontributions%20even%20after%20aggregation%2C%20without%20compromising%20the%20already%20acquired%0A%22good%22%20knowledge.%20This%20highlights%20the%20necessity%20for%20novel%20federated%20unlearning%0A%28FU%29%20algorithms%2C%20which%20can%20efficiently%20remove%20specific%20clients%27%20contributions%0Awithout%20full%20model%20retraining.%20This%20article%20provides%20background%20concepts%2C%0Aempirical%20evidence%2C%20and%20practical%20guidelines%20to%20design/implement%20efficient%20FU%0Aschemes.%20This%20study%20includes%20a%20detailed%20analysis%20of%20the%20metrics%20for%20evaluating%0Aunlearning%20in%20FL%20and%20presents%20an%20in-depth%20literature%20review%20categorizing%0Astate-of-the-art%20FU%20contributions%20under%20a%20novel%20taxonomy.%20Finally%2C%20we%20outline%0Athe%20most%20relevant%20and%20still%20open%20technical%20challenges%2C%20by%20identifying%20the%20most%0Apromising%20research%20directions%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05146v3&entry.124074799=Read"},
{"title": "MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and\n  Provable Convergence", "author": "Ionut-Vlad Modoranu and Mher Safaryan and Grigory Malinovsky and Eldar Kurtic and Thomas Robert and Peter Richtarik and Dan Alistarh", "abstract": "  We propose a new variant of the Adam optimizer called MicroAdam that\nspecifically minimizes memory overheads, while maintaining theoretical\nconvergence guarantees. We achieve this by compressing the gradient information\nbefore it is fed into the optimizer state, thereby reducing its memory\nfootprint significantly. We control the resulting compression error via a novel\ninstance of the classical \\emph{error feedback} mechanism from distributed\noptimization in which *the error correction information is itself compressed*\nto allow for practical memory gains. We prove that the resulting approach\nmaintains theoretical convergence guarantees competitive to those of AMSGrad,\nwhile providing good practical performance. Specifically, we show that\nMicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT)\nand billion-scale (LLaMA) models, MicroAdam provides practical convergence\ncompetitive to that of the uncompressed Adam baseline, with lower memory usage\nand similar running time. Our code is available at\nhttps://github.com/IST-DASLab/MicroAdam.\n", "link": "http://arxiv.org/abs/2405.15593v2", "date": "2024-11-05", "relevancy": 1.9769, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence&body=Title%3A%20MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence%0AAuthor%3A%20Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Grigory%20Malinovsky%20and%20Eldar%20Kurtic%20and%20Thomas%20Robert%20and%20Peter%20Richtarik%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20We%20propose%20a%20new%20variant%20of%20the%20Adam%20optimizer%20called%20MicroAdam%20that%0Aspecifically%20minimizes%20memory%20overheads%2C%20while%20maintaining%20theoretical%0Aconvergence%20guarantees.%20We%20achieve%20this%20by%20compressing%20the%20gradient%20information%0Abefore%20it%20is%20fed%20into%20the%20optimizer%20state%2C%20thereby%20reducing%20its%20memory%0Afootprint%20significantly.%20We%20control%20the%20resulting%20compression%20error%20via%20a%20novel%0Ainstance%20of%20the%20classical%20%5Cemph%7Berror%20feedback%7D%20mechanism%20from%20distributed%0Aoptimization%20in%20which%20%2Athe%20error%20correction%20information%20is%20itself%20compressed%2A%0Ato%20allow%20for%20practical%20memory%20gains.%20We%20prove%20that%20the%20resulting%20approach%0Amaintains%20theoretical%20convergence%20guarantees%20competitive%20to%20those%20of%20AMSGrad%2C%0Awhile%20providing%20good%20practical%20performance.%20Specifically%2C%20we%20show%20that%0AMicroAdam%20can%20be%20implemented%20efficiently%20on%20GPUs%3A%20on%20both%20million-scale%20%28BERT%29%0Aand%20billion-scale%20%28LLaMA%29%20models%2C%20MicroAdam%20provides%20practical%20convergence%0Acompetitive%20to%20that%20of%20the%20uncompressed%20Adam%20baseline%2C%20with%20lower%20memory%20usage%0Aand%20similar%20running%20time.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IST-DASLab/MicroAdam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroAdam%253A%2520Accurate%2520Adaptive%2520Optimization%2520with%2520Low%2520Space%2520Overhead%2520and%250A%2520%2520Provable%2520Convergence%26entry.906535625%3DIonut-Vlad%2520Modoranu%2520and%2520Mher%2520Safaryan%2520and%2520Grigory%2520Malinovsky%2520and%2520Eldar%2520Kurtic%2520and%2520Thomas%2520Robert%2520and%2520Peter%2520Richtarik%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520variant%2520of%2520the%2520Adam%2520optimizer%2520called%2520MicroAdam%2520that%250Aspecifically%2520minimizes%2520memory%2520overheads%252C%2520while%2520maintaining%2520theoretical%250Aconvergence%2520guarantees.%2520We%2520achieve%2520this%2520by%2520compressing%2520the%2520gradient%2520information%250Abefore%2520it%2520is%2520fed%2520into%2520the%2520optimizer%2520state%252C%2520thereby%2520reducing%2520its%2520memory%250Afootprint%2520significantly.%2520We%2520control%2520the%2520resulting%2520compression%2520error%2520via%2520a%2520novel%250Ainstance%2520of%2520the%2520classical%2520%255Cemph%257Berror%2520feedback%257D%2520mechanism%2520from%2520distributed%250Aoptimization%2520in%2520which%2520%252Athe%2520error%2520correction%2520information%2520is%2520itself%2520compressed%252A%250Ato%2520allow%2520for%2520practical%2520memory%2520gains.%2520We%2520prove%2520that%2520the%2520resulting%2520approach%250Amaintains%2520theoretical%2520convergence%2520guarantees%2520competitive%2520to%2520those%2520of%2520AMSGrad%252C%250Awhile%2520providing%2520good%2520practical%2520performance.%2520Specifically%252C%2520we%2520show%2520that%250AMicroAdam%2520can%2520be%2520implemented%2520efficiently%2520on%2520GPUs%253A%2520on%2520both%2520million-scale%2520%2528BERT%2529%250Aand%2520billion-scale%2520%2528LLaMA%2529%2520models%252C%2520MicroAdam%2520provides%2520practical%2520convergence%250Acompetitive%2520to%2520that%2520of%2520the%2520uncompressed%2520Adam%2520baseline%252C%2520with%2520lower%2520memory%2520usage%250Aand%2520similar%2520running%2520time.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IST-DASLab/MicroAdam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence&entry.906535625=Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Grigory%20Malinovsky%20and%20Eldar%20Kurtic%20and%20Thomas%20Robert%20and%20Peter%20Richtarik%20and%20Dan%20Alistarh&entry.1292438233=%20%20We%20propose%20a%20new%20variant%20of%20the%20Adam%20optimizer%20called%20MicroAdam%20that%0Aspecifically%20minimizes%20memory%20overheads%2C%20while%20maintaining%20theoretical%0Aconvergence%20guarantees.%20We%20achieve%20this%20by%20compressing%20the%20gradient%20information%0Abefore%20it%20is%20fed%20into%20the%20optimizer%20state%2C%20thereby%20reducing%20its%20memory%0Afootprint%20significantly.%20We%20control%20the%20resulting%20compression%20error%20via%20a%20novel%0Ainstance%20of%20the%20classical%20%5Cemph%7Berror%20feedback%7D%20mechanism%20from%20distributed%0Aoptimization%20in%20which%20%2Athe%20error%20correction%20information%20is%20itself%20compressed%2A%0Ato%20allow%20for%20practical%20memory%20gains.%20We%20prove%20that%20the%20resulting%20approach%0Amaintains%20theoretical%20convergence%20guarantees%20competitive%20to%20those%20of%20AMSGrad%2C%0Awhile%20providing%20good%20practical%20performance.%20Specifically%2C%20we%20show%20that%0AMicroAdam%20can%20be%20implemented%20efficiently%20on%20GPUs%3A%20on%20both%20million-scale%20%28BERT%29%0Aand%20billion-scale%20%28LLaMA%29%20models%2C%20MicroAdam%20provides%20practical%20convergence%0Acompetitive%20to%20that%20of%20the%20uncompressed%20Adam%20baseline%2C%20with%20lower%20memory%20usage%0Aand%20similar%20running%20time.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IST-DASLab/MicroAdam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15593v2&entry.124074799=Read"},
{"title": "Decision-focused predictions via pessimistic bilevel optimization: a\n  computational study", "author": "V\u00edctor Bucarey and Sophia Calder\u00f3n and Gonzalo Mu\u00f1oz and Frederic Semet", "abstract": "  Dealing with uncertainty in optimization parameters is an important and\nlongstanding challenge. Typically, uncertain parameters are predicted\naccurately, and then a deterministic optimization problem is solved. However,\nthe decisions produced by this so-called \\emph{predict-then-optimize} procedure\ncan be highly sensitive to uncertain parameters. In this work, we contribute to\nrecent efforts in producing \\emph{decision-focused} predictions, i.e., to build\npredictive models that are constructed with the goal of minimizing a\n\\emph{regret} measure on the decisions taken with them. We begin by formulating\nthe exact expected regret minimization as a pessimistic bilevel optimization\nmodel. Then, we establish NP-completeness of this problem, even in a heavily\nrestricted case. Using duality arguments, we reformulate it as a non-convex\nquadratic optimization problem. Finally, we show various computational\ntechniques to achieve tractability. We report extensive computational results\non shortest-path instances with uncertain cost vectors. Our results indicate\nthat our approach can improve training performance over the approach of\nElmachtoub and Grigas (2022), a state-of-the-art method for decision-focused\nlearning.\n", "link": "http://arxiv.org/abs/2312.17640v4", "date": "2024-11-05", "relevancy": 1.9738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4903}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision-focused%20predictions%20via%20pessimistic%20bilevel%20optimization%3A%20a%0A%20%20computational%20study&body=Title%3A%20Decision-focused%20predictions%20via%20pessimistic%20bilevel%20optimization%3A%20a%0A%20%20computational%20study%0AAuthor%3A%20V%C3%ADctor%20Bucarey%20and%20Sophia%20Calder%C3%B3n%20and%20Gonzalo%20Mu%C3%B1oz%20and%20Frederic%20Semet%0AAbstract%3A%20%20%20Dealing%20with%20uncertainty%20in%20optimization%20parameters%20is%20an%20important%20and%0Alongstanding%20challenge.%20Typically%2C%20uncertain%20parameters%20are%20predicted%0Aaccurately%2C%20and%20then%20a%20deterministic%20optimization%20problem%20is%20solved.%20However%2C%0Athe%20decisions%20produced%20by%20this%20so-called%20%5Cemph%7Bpredict-then-optimize%7D%20procedure%0Acan%20be%20highly%20sensitive%20to%20uncertain%20parameters.%20In%20this%20work%2C%20we%20contribute%20to%0Arecent%20efforts%20in%20producing%20%5Cemph%7Bdecision-focused%7D%20predictions%2C%20i.e.%2C%20to%20build%0Apredictive%20models%20that%20are%20constructed%20with%20the%20goal%20of%20minimizing%20a%0A%5Cemph%7Bregret%7D%20measure%20on%20the%20decisions%20taken%20with%20them.%20We%20begin%20by%20formulating%0Athe%20exact%20expected%20regret%20minimization%20as%20a%20pessimistic%20bilevel%20optimization%0Amodel.%20Then%2C%20we%20establish%20NP-completeness%20of%20this%20problem%2C%20even%20in%20a%20heavily%0Arestricted%20case.%20Using%20duality%20arguments%2C%20we%20reformulate%20it%20as%20a%20non-convex%0Aquadratic%20optimization%20problem.%20Finally%2C%20we%20show%20various%20computational%0Atechniques%20to%20achieve%20tractability.%20We%20report%20extensive%20computational%20results%0Aon%20shortest-path%20instances%20with%20uncertain%20cost%20vectors.%20Our%20results%20indicate%0Athat%20our%20approach%20can%20improve%20training%20performance%20over%20the%20approach%20of%0AElmachtoub%20and%20Grigas%20%282022%29%2C%20a%20state-of-the-art%20method%20for%20decision-focused%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17640v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision-focused%2520predictions%2520via%2520pessimistic%2520bilevel%2520optimization%253A%2520a%250A%2520%2520computational%2520study%26entry.906535625%3DV%25C3%25ADctor%2520Bucarey%2520and%2520Sophia%2520Calder%25C3%25B3n%2520and%2520Gonzalo%2520Mu%25C3%25B1oz%2520and%2520Frederic%2520Semet%26entry.1292438233%3D%2520%2520Dealing%2520with%2520uncertainty%2520in%2520optimization%2520parameters%2520is%2520an%2520important%2520and%250Alongstanding%2520challenge.%2520Typically%252C%2520uncertain%2520parameters%2520are%2520predicted%250Aaccurately%252C%2520and%2520then%2520a%2520deterministic%2520optimization%2520problem%2520is%2520solved.%2520However%252C%250Athe%2520decisions%2520produced%2520by%2520this%2520so-called%2520%255Cemph%257Bpredict-then-optimize%257D%2520procedure%250Acan%2520be%2520highly%2520sensitive%2520to%2520uncertain%2520parameters.%2520In%2520this%2520work%252C%2520we%2520contribute%2520to%250Arecent%2520efforts%2520in%2520producing%2520%255Cemph%257Bdecision-focused%257D%2520predictions%252C%2520i.e.%252C%2520to%2520build%250Apredictive%2520models%2520that%2520are%2520constructed%2520with%2520the%2520goal%2520of%2520minimizing%2520a%250A%255Cemph%257Bregret%257D%2520measure%2520on%2520the%2520decisions%2520taken%2520with%2520them.%2520We%2520begin%2520by%2520formulating%250Athe%2520exact%2520expected%2520regret%2520minimization%2520as%2520a%2520pessimistic%2520bilevel%2520optimization%250Amodel.%2520Then%252C%2520we%2520establish%2520NP-completeness%2520of%2520this%2520problem%252C%2520even%2520in%2520a%2520heavily%250Arestricted%2520case.%2520Using%2520duality%2520arguments%252C%2520we%2520reformulate%2520it%2520as%2520a%2520non-convex%250Aquadratic%2520optimization%2520problem.%2520Finally%252C%2520we%2520show%2520various%2520computational%250Atechniques%2520to%2520achieve%2520tractability.%2520We%2520report%2520extensive%2520computational%2520results%250Aon%2520shortest-path%2520instances%2520with%2520uncertain%2520cost%2520vectors.%2520Our%2520results%2520indicate%250Athat%2520our%2520approach%2520can%2520improve%2520training%2520performance%2520over%2520the%2520approach%2520of%250AElmachtoub%2520and%2520Grigas%2520%25282022%2529%252C%2520a%2520state-of-the-art%2520method%2520for%2520decision-focused%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17640v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision-focused%20predictions%20via%20pessimistic%20bilevel%20optimization%3A%20a%0A%20%20computational%20study&entry.906535625=V%C3%ADctor%20Bucarey%20and%20Sophia%20Calder%C3%B3n%20and%20Gonzalo%20Mu%C3%B1oz%20and%20Frederic%20Semet&entry.1292438233=%20%20Dealing%20with%20uncertainty%20in%20optimization%20parameters%20is%20an%20important%20and%0Alongstanding%20challenge.%20Typically%2C%20uncertain%20parameters%20are%20predicted%0Aaccurately%2C%20and%20then%20a%20deterministic%20optimization%20problem%20is%20solved.%20However%2C%0Athe%20decisions%20produced%20by%20this%20so-called%20%5Cemph%7Bpredict-then-optimize%7D%20procedure%0Acan%20be%20highly%20sensitive%20to%20uncertain%20parameters.%20In%20this%20work%2C%20we%20contribute%20to%0Arecent%20efforts%20in%20producing%20%5Cemph%7Bdecision-focused%7D%20predictions%2C%20i.e.%2C%20to%20build%0Apredictive%20models%20that%20are%20constructed%20with%20the%20goal%20of%20minimizing%20a%0A%5Cemph%7Bregret%7D%20measure%20on%20the%20decisions%20taken%20with%20them.%20We%20begin%20by%20formulating%0Athe%20exact%20expected%20regret%20minimization%20as%20a%20pessimistic%20bilevel%20optimization%0Amodel.%20Then%2C%20we%20establish%20NP-completeness%20of%20this%20problem%2C%20even%20in%20a%20heavily%0Arestricted%20case.%20Using%20duality%20arguments%2C%20we%20reformulate%20it%20as%20a%20non-convex%0Aquadratic%20optimization%20problem.%20Finally%2C%20we%20show%20various%20computational%0Atechniques%20to%20achieve%20tractability.%20We%20report%20extensive%20computational%20results%0Aon%20shortest-path%20instances%20with%20uncertain%20cost%20vectors.%20Our%20results%20indicate%0Athat%20our%20approach%20can%20improve%20training%20performance%20over%20the%20approach%20of%0AElmachtoub%20and%20Grigas%20%282022%29%2C%20a%20state-of-the-art%20method%20for%20decision-focused%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17640v4&entry.124074799=Read"},
{"title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse", "author": "Ryan Liu and Jiayi Geng and Addison J. Wu and Ilia Sucholutsky and Tania Lombrozo and Thomas L. Griffiths", "abstract": "  Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.\n", "link": "http://arxiv.org/abs/2410.21333v2", "date": "2024-11-05", "relevancy": 1.9651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse&body=Title%3A%20Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse%0AAuthor%3A%20Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Addison%20J.%20Wu%20and%20Ilia%20Sucholutsky%20and%20Tania%20Lombrozo%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20become%20a%20widely%20used%20strategy%20for%0Aworking%20with%20large%20language%20and%20multimodal%20models.%20While%20CoT%20has%20been%20shown%20to%0Aimprove%20performance%20across%20many%20tasks%2C%20determining%20the%20settings%20in%20which%20it%20is%0Aeffective%20remains%20an%20ongoing%20effort.%20In%20particular%2C%20it%20is%20still%20an%20open%0Aquestion%20in%20what%20settings%20CoT%20systematically%20reduces%20model%20performance.%20In%20this%0Apaper%2C%20we%20seek%20to%20identify%20the%20characteristics%20of%20tasks%20where%20CoT%20reduces%0Aperformance%20by%20drawing%20inspiration%20from%20cognitive%20psychology%2C%20looking%20at%20cases%0Awhere%20%28i%29%20verbal%20thinking%20or%20deliberation%20hurts%20performance%20in%20humans%2C%20and%20%28ii%29%0Athe%20constraints%20governing%20human%20performance%20generalize%20to%20language%20models.%0AThree%20such%20cases%20are%20implicit%20statistical%20learning%2C%20visual%20recognition%2C%20and%0Aclassifying%20with%20patterns%20containing%20exceptions.%20In%20extensive%20experiments%0Aacross%20all%20three%20settings%2C%20we%20find%20that%20a%20diverse%20collection%20of%0Astate-of-the-art%20models%20exhibit%20significant%20drop-offs%20in%20performance%20%28e.g.%2C%20up%0Ato%2036.3%25%20absolute%20accuracy%20for%20OpenAI%20o1-preview%20compared%20to%20GPT-4o%29%20when%20using%0Ainference-time%20reasoning%20compared%20to%20zero-shot%20counterparts.%20We%20also%20identify%0Athree%20tasks%20that%20satisfy%20condition%20%28i%29%20but%20not%20%28ii%29%2C%20and%20find%20that%20while%20verbal%0Athinking%20reduces%20human%20performance%20in%20these%20tasks%2C%20CoT%20retains%20or%20increases%0Amodel%20performance.%20Overall%2C%20our%20results%20show%20that%20while%20there%20is%20not%20an%20exact%0Aparallel%20between%20the%20cognitive%20processes%20of%20models%20and%20those%20of%20humans%2C%0Aconsidering%20cases%20where%20thinking%20has%20negative%20consequences%20for%20human%0Aperformance%20can%20help%20us%20identify%20settings%20where%20it%20negatively%20impacts%20models.%0ABy%20connecting%20the%20literature%20on%20human%20deliberation%20with%20evaluations%20of%20CoT%2C%20we%0Aoffer%20a%20new%20tool%20that%20can%20be%20used%20in%20understanding%20the%20impact%20of%20prompt%20choices%0Aand%20inference-time%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520Your%2520Step%2520%2528by%2520Step%2529%253A%2520Chain-of-Thought%2520can%2520Reduce%2520Performance%2520on%250A%2520%2520Tasks%2520where%2520Thinking%2520Makes%2520Humans%2520Worse%26entry.906535625%3DRyan%2520Liu%2520and%2520Jiayi%2520Geng%2520and%2520Addison%2520J.%2520Wu%2520and%2520Ilia%2520Sucholutsky%2520and%2520Tania%2520Lombrozo%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Chain-of-thought%2520%2528CoT%2529%2520prompting%2520has%2520become%2520a%2520widely%2520used%2520strategy%2520for%250Aworking%2520with%2520large%2520language%2520and%2520multimodal%2520models.%2520While%2520CoT%2520has%2520been%2520shown%2520to%250Aimprove%2520performance%2520across%2520many%2520tasks%252C%2520determining%2520the%2520settings%2520in%2520which%2520it%2520is%250Aeffective%2520remains%2520an%2520ongoing%2520effort.%2520In%2520particular%252C%2520it%2520is%2520still%2520an%2520open%250Aquestion%2520in%2520what%2520settings%2520CoT%2520systematically%2520reduces%2520model%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520seek%2520to%2520identify%2520the%2520characteristics%2520of%2520tasks%2520where%2520CoT%2520reduces%250Aperformance%2520by%2520drawing%2520inspiration%2520from%2520cognitive%2520psychology%252C%2520looking%2520at%2520cases%250Awhere%2520%2528i%2529%2520verbal%2520thinking%2520or%2520deliberation%2520hurts%2520performance%2520in%2520humans%252C%2520and%2520%2528ii%2529%250Athe%2520constraints%2520governing%2520human%2520performance%2520generalize%2520to%2520language%2520models.%250AThree%2520such%2520cases%2520are%2520implicit%2520statistical%2520learning%252C%2520visual%2520recognition%252C%2520and%250Aclassifying%2520with%2520patterns%2520containing%2520exceptions.%2520In%2520extensive%2520experiments%250Aacross%2520all%2520three%2520settings%252C%2520we%2520find%2520that%2520a%2520diverse%2520collection%2520of%250Astate-of-the-art%2520models%2520exhibit%2520significant%2520drop-offs%2520in%2520performance%2520%2528e.g.%252C%2520up%250Ato%252036.3%2525%2520absolute%2520accuracy%2520for%2520OpenAI%2520o1-preview%2520compared%2520to%2520GPT-4o%2529%2520when%2520using%250Ainference-time%2520reasoning%2520compared%2520to%2520zero-shot%2520counterparts.%2520We%2520also%2520identify%250Athree%2520tasks%2520that%2520satisfy%2520condition%2520%2528i%2529%2520but%2520not%2520%2528ii%2529%252C%2520and%2520find%2520that%2520while%2520verbal%250Athinking%2520reduces%2520human%2520performance%2520in%2520these%2520tasks%252C%2520CoT%2520retains%2520or%2520increases%250Amodel%2520performance.%2520Overall%252C%2520our%2520results%2520show%2520that%2520while%2520there%2520is%2520not%2520an%2520exact%250Aparallel%2520between%2520the%2520cognitive%2520processes%2520of%2520models%2520and%2520those%2520of%2520humans%252C%250Aconsidering%2520cases%2520where%2520thinking%2520has%2520negative%2520consequences%2520for%2520human%250Aperformance%2520can%2520help%2520us%2520identify%2520settings%2520where%2520it%2520negatively%2520impacts%2520models.%250ABy%2520connecting%2520the%2520literature%2520on%2520human%2520deliberation%2520with%2520evaluations%2520of%2520CoT%252C%2520we%250Aoffer%2520a%2520new%2520tool%2520that%2520can%2520be%2520used%2520in%2520understanding%2520the%2520impact%2520of%2520prompt%2520choices%250Aand%2520inference-time%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse&entry.906535625=Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Addison%20J.%20Wu%20and%20Ilia%20Sucholutsky%20and%20Tania%20Lombrozo%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20become%20a%20widely%20used%20strategy%20for%0Aworking%20with%20large%20language%20and%20multimodal%20models.%20While%20CoT%20has%20been%20shown%20to%0Aimprove%20performance%20across%20many%20tasks%2C%20determining%20the%20settings%20in%20which%20it%20is%0Aeffective%20remains%20an%20ongoing%20effort.%20In%20particular%2C%20it%20is%20still%20an%20open%0Aquestion%20in%20what%20settings%20CoT%20systematically%20reduces%20model%20performance.%20In%20this%0Apaper%2C%20we%20seek%20to%20identify%20the%20characteristics%20of%20tasks%20where%20CoT%20reduces%0Aperformance%20by%20drawing%20inspiration%20from%20cognitive%20psychology%2C%20looking%20at%20cases%0Awhere%20%28i%29%20verbal%20thinking%20or%20deliberation%20hurts%20performance%20in%20humans%2C%20and%20%28ii%29%0Athe%20constraints%20governing%20human%20performance%20generalize%20to%20language%20models.%0AThree%20such%20cases%20are%20implicit%20statistical%20learning%2C%20visual%20recognition%2C%20and%0Aclassifying%20with%20patterns%20containing%20exceptions.%20In%20extensive%20experiments%0Aacross%20all%20three%20settings%2C%20we%20find%20that%20a%20diverse%20collection%20of%0Astate-of-the-art%20models%20exhibit%20significant%20drop-offs%20in%20performance%20%28e.g.%2C%20up%0Ato%2036.3%25%20absolute%20accuracy%20for%20OpenAI%20o1-preview%20compared%20to%20GPT-4o%29%20when%20using%0Ainference-time%20reasoning%20compared%20to%20zero-shot%20counterparts.%20We%20also%20identify%0Athree%20tasks%20that%20satisfy%20condition%20%28i%29%20but%20not%20%28ii%29%2C%20and%20find%20that%20while%20verbal%0Athinking%20reduces%20human%20performance%20in%20these%20tasks%2C%20CoT%20retains%20or%20increases%0Amodel%20performance.%20Overall%2C%20our%20results%20show%20that%20while%20there%20is%20not%20an%20exact%0Aparallel%20between%20the%20cognitive%20processes%20of%20models%20and%20those%20of%20humans%2C%0Aconsidering%20cases%20where%20thinking%20has%20negative%20consequences%20for%20human%0Aperformance%20can%20help%20us%20identify%20settings%20where%20it%20negatively%20impacts%20models.%0ABy%20connecting%20the%20literature%20on%20human%20deliberation%20with%20evaluations%20of%20CoT%2C%20we%0Aoffer%20a%20new%20tool%20that%20can%20be%20used%20in%20understanding%20the%20impact%20of%20prompt%20choices%0Aand%20inference-time%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21333v2&entry.124074799=Read"},
{"title": "Blind Estimation of Sub-band Acoustic Parameters from Ambisonics\n  Recordings using Spectro-Spatial Covariance Features", "author": "Hanyu Meng and Jeroen Breebaart and Jeremy Stoddard and Vidhyasaharan Sethu and Eliathamby Ambikairajah", "abstract": "  Estimating frequency-varying acoustic parameters is essential for enhancing\nimmersive perception in realistic spatial audio creation. In this paper, we\npropose a unified framework that blindly estimates reverberation time (T60),\ndirect-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands\nusing first-order Ambisonics (FOA) speech recordings as inputs. The proposed\nframework utilizes a novel feature named Spectro-Spatial Covariance Vector\n(SSCV), efficiently representing temporal, spectral as well as spatial\ninformation of the FOA signal. Our models significantly outperform existing\nsingle-channel methods with only spectral information, reducing estimation\nerrors by more than half for all three acoustic parameters. Additionally, we\nintroduce FOA-Conv3D, a novel back-end network for effectively utilising the\nSSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the\nconvolutional neural network (CNN) and recurrent convolutional neural network\n(CRNN) backends, achieving lower estimation errors and accounting for a higher\nproportion of variance (PoV) for all 3 acoustic parameters.\n", "link": "http://arxiv.org/abs/2411.03172v1", "date": "2024-11-05", "relevancy": 1.9616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blind%20Estimation%20of%20Sub-band%20Acoustic%20Parameters%20from%20Ambisonics%0A%20%20Recordings%20using%20Spectro-Spatial%20Covariance%20Features&body=Title%3A%20Blind%20Estimation%20of%20Sub-band%20Acoustic%20Parameters%20from%20Ambisonics%0A%20%20Recordings%20using%20Spectro-Spatial%20Covariance%20Features%0AAuthor%3A%20Hanyu%20Meng%20and%20Jeroen%20Breebaart%20and%20Jeremy%20Stoddard%20and%20Vidhyasaharan%20Sethu%20and%20Eliathamby%20Ambikairajah%0AAbstract%3A%20%20%20Estimating%20frequency-varying%20acoustic%20parameters%20is%20essential%20for%20enhancing%0Aimmersive%20perception%20in%20realistic%20spatial%20audio%20creation.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20framework%20that%20blindly%20estimates%20reverberation%20time%20%28T60%29%2C%0Adirect-to-reverberant%20ratio%20%28DRR%29%2C%20and%20clarity%20%28C50%29%20across%2010%20frequency%20bands%0Ausing%20first-order%20Ambisonics%20%28FOA%29%20speech%20recordings%20as%20inputs.%20The%20proposed%0Aframework%20utilizes%20a%20novel%20feature%20named%20Spectro-Spatial%20Covariance%20Vector%0A%28SSCV%29%2C%20efficiently%20representing%20temporal%2C%20spectral%20as%20well%20as%20spatial%0Ainformation%20of%20the%20FOA%20signal.%20Our%20models%20significantly%20outperform%20existing%0Asingle-channel%20methods%20with%20only%20spectral%20information%2C%20reducing%20estimation%0Aerrors%20by%20more%20than%20half%20for%20all%20three%20acoustic%20parameters.%20Additionally%2C%20we%0Aintroduce%20FOA-Conv3D%2C%20a%20novel%20back-end%20network%20for%20effectively%20utilising%20the%0ASSCV%20feature%20with%20a%203D%20convolutional%20encoder.%20FOA-Conv3D%20outperforms%20the%0Aconvolutional%20neural%20network%20%28CNN%29%20and%20recurrent%20convolutional%20neural%20network%0A%28CRNN%29%20backends%2C%20achieving%20lower%20estimation%20errors%20and%20accounting%20for%20a%20higher%0Aproportion%20of%20variance%20%28PoV%29%20for%20all%203%20acoustic%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlind%2520Estimation%2520of%2520Sub-band%2520Acoustic%2520Parameters%2520from%2520Ambisonics%250A%2520%2520Recordings%2520using%2520Spectro-Spatial%2520Covariance%2520Features%26entry.906535625%3DHanyu%2520Meng%2520and%2520Jeroen%2520Breebaart%2520and%2520Jeremy%2520Stoddard%2520and%2520Vidhyasaharan%2520Sethu%2520and%2520Eliathamby%2520Ambikairajah%26entry.1292438233%3D%2520%2520Estimating%2520frequency-varying%2520acoustic%2520parameters%2520is%2520essential%2520for%2520enhancing%250Aimmersive%2520perception%2520in%2520realistic%2520spatial%2520audio%2520creation.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520unified%2520framework%2520that%2520blindly%2520estimates%2520reverberation%2520time%2520%2528T60%2529%252C%250Adirect-to-reverberant%2520ratio%2520%2528DRR%2529%252C%2520and%2520clarity%2520%2528C50%2529%2520across%252010%2520frequency%2520bands%250Ausing%2520first-order%2520Ambisonics%2520%2528FOA%2529%2520speech%2520recordings%2520as%2520inputs.%2520The%2520proposed%250Aframework%2520utilizes%2520a%2520novel%2520feature%2520named%2520Spectro-Spatial%2520Covariance%2520Vector%250A%2528SSCV%2529%252C%2520efficiently%2520representing%2520temporal%252C%2520spectral%2520as%2520well%2520as%2520spatial%250Ainformation%2520of%2520the%2520FOA%2520signal.%2520Our%2520models%2520significantly%2520outperform%2520existing%250Asingle-channel%2520methods%2520with%2520only%2520spectral%2520information%252C%2520reducing%2520estimation%250Aerrors%2520by%2520more%2520than%2520half%2520for%2520all%2520three%2520acoustic%2520parameters.%2520Additionally%252C%2520we%250Aintroduce%2520FOA-Conv3D%252C%2520a%2520novel%2520back-end%2520network%2520for%2520effectively%2520utilising%2520the%250ASSCV%2520feature%2520with%2520a%25203D%2520convolutional%2520encoder.%2520FOA-Conv3D%2520outperforms%2520the%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520and%2520recurrent%2520convolutional%2520neural%2520network%250A%2528CRNN%2529%2520backends%252C%2520achieving%2520lower%2520estimation%2520errors%2520and%2520accounting%2520for%2520a%2520higher%250Aproportion%2520of%2520variance%2520%2528PoV%2529%2520for%2520all%25203%2520acoustic%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Estimation%20of%20Sub-band%20Acoustic%20Parameters%20from%20Ambisonics%0A%20%20Recordings%20using%20Spectro-Spatial%20Covariance%20Features&entry.906535625=Hanyu%20Meng%20and%20Jeroen%20Breebaart%20and%20Jeremy%20Stoddard%20and%20Vidhyasaharan%20Sethu%20and%20Eliathamby%20Ambikairajah&entry.1292438233=%20%20Estimating%20frequency-varying%20acoustic%20parameters%20is%20essential%20for%20enhancing%0Aimmersive%20perception%20in%20realistic%20spatial%20audio%20creation.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20framework%20that%20blindly%20estimates%20reverberation%20time%20%28T60%29%2C%0Adirect-to-reverberant%20ratio%20%28DRR%29%2C%20and%20clarity%20%28C50%29%20across%2010%20frequency%20bands%0Ausing%20first-order%20Ambisonics%20%28FOA%29%20speech%20recordings%20as%20inputs.%20The%20proposed%0Aframework%20utilizes%20a%20novel%20feature%20named%20Spectro-Spatial%20Covariance%20Vector%0A%28SSCV%29%2C%20efficiently%20representing%20temporal%2C%20spectral%20as%20well%20as%20spatial%0Ainformation%20of%20the%20FOA%20signal.%20Our%20models%20significantly%20outperform%20existing%0Asingle-channel%20methods%20with%20only%20spectral%20information%2C%20reducing%20estimation%0Aerrors%20by%20more%20than%20half%20for%20all%20three%20acoustic%20parameters.%20Additionally%2C%20we%0Aintroduce%20FOA-Conv3D%2C%20a%20novel%20back-end%20network%20for%20effectively%20utilising%20the%0ASSCV%20feature%20with%20a%203D%20convolutional%20encoder.%20FOA-Conv3D%20outperforms%20the%0Aconvolutional%20neural%20network%20%28CNN%29%20and%20recurrent%20convolutional%20neural%20network%0A%28CRNN%29%20backends%2C%20achieving%20lower%20estimation%20errors%20and%20accounting%20for%20a%20higher%0Aproportion%20of%20variance%20%28PoV%29%20for%20all%203%20acoustic%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03172v1&entry.124074799=Read"},
{"title": "Poseidon: Efficient Foundation Models for PDEs", "author": "Maximilian Herde and Bogdan Raoni\u0107 and Tobias Rohner and Roger K\u00e4ppeli and Roberto Molinaro and Emmanuel de B\u00e9zenac and Siddhartha Mishra", "abstract": "  We introduce Poseidon, a foundation model for learning the solution operators\nof PDEs. It is based on a multiscale operator transformer, with\ntime-conditioned layer norms that enable continuous-in-time evaluations. A\nnovel training strategy leveraging the semi-group property of time-dependent\nPDEs to allow for significant scaling-up of the training data is also proposed.\nPoseidon is pretrained on a diverse, large scale dataset for the governing\nequations of fluid dynamics. It is then evaluated on a suite of 15 challenging\ndownstream tasks that include a wide variety of PDE types and operators. We\nshow that Poseidon exhibits excellent performance across the board by\noutperforming baselines significantly, both in terms of sample efficiency and\naccuracy. Poseidon also generalizes very well to new physics that is not seen\nduring pretraining. Moreover, Poseidon scales with respect to model and data\nsize, both for pretraining and for downstream tasks. Taken together, our\nresults showcase the surprising ability of Poseidon to learn effective\nrepresentations from a very small set of PDEs during pretraining in order to\ngeneralize well to unseen and unrelated PDEs downstream, demonstrating its\npotential as an effective, general purpose PDE foundation model. Finally, the\nPoseidon model as well as underlying pretraining and downstream datasets are\nopen sourced, with code being available at\nhttps://github.com/camlab-ethz/poseidon and pretrained models and datasets at\nhttps://huggingface.co/camlab-ethz.\n", "link": "http://arxiv.org/abs/2405.19101v2", "date": "2024-11-05", "relevancy": 1.9452, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5093}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poseidon%3A%20Efficient%20Foundation%20Models%20for%20PDEs&body=Title%3A%20Poseidon%3A%20Efficient%20Foundation%20Models%20for%20PDEs%0AAuthor%3A%20Maximilian%20Herde%20and%20Bogdan%20Raoni%C4%87%20and%20Tobias%20Rohner%20and%20Roger%20K%C3%A4ppeli%20and%20Roberto%20Molinaro%20and%20Emmanuel%20de%20B%C3%A9zenac%20and%20Siddhartha%20Mishra%0AAbstract%3A%20%20%20We%20introduce%20Poseidon%2C%20a%20foundation%20model%20for%20learning%20the%20solution%20operators%0Aof%20PDEs.%20It%20is%20based%20on%20a%20multiscale%20operator%20transformer%2C%20with%0Atime-conditioned%20layer%20norms%20that%20enable%20continuous-in-time%20evaluations.%20A%0Anovel%20training%20strategy%20leveraging%20the%20semi-group%20property%20of%20time-dependent%0APDEs%20to%20allow%20for%20significant%20scaling-up%20of%20the%20training%20data%20is%20also%20proposed.%0APoseidon%20is%20pretrained%20on%20a%20diverse%2C%20large%20scale%20dataset%20for%20the%20governing%0Aequations%20of%20fluid%20dynamics.%20It%20is%20then%20evaluated%20on%20a%20suite%20of%2015%20challenging%0Adownstream%20tasks%20that%20include%20a%20wide%20variety%20of%20PDE%20types%20and%20operators.%20We%0Ashow%20that%20Poseidon%20exhibits%20excellent%20performance%20across%20the%20board%20by%0Aoutperforming%20baselines%20significantly%2C%20both%20in%20terms%20of%20sample%20efficiency%20and%0Aaccuracy.%20Poseidon%20also%20generalizes%20very%20well%20to%20new%20physics%20that%20is%20not%20seen%0Aduring%20pretraining.%20Moreover%2C%20Poseidon%20scales%20with%20respect%20to%20model%20and%20data%0Asize%2C%20both%20for%20pretraining%20and%20for%20downstream%20tasks.%20Taken%20together%2C%20our%0Aresults%20showcase%20the%20surprising%20ability%20of%20Poseidon%20to%20learn%20effective%0Arepresentations%20from%20a%20very%20small%20set%20of%20PDEs%20during%20pretraining%20in%20order%20to%0Ageneralize%20well%20to%20unseen%20and%20unrelated%20PDEs%20downstream%2C%20demonstrating%20its%0Apotential%20as%20an%20effective%2C%20general%20purpose%20PDE%20foundation%20model.%20Finally%2C%20the%0APoseidon%20model%20as%20well%20as%20underlying%20pretraining%20and%20downstream%20datasets%20are%0Aopen%20sourced%2C%20with%20code%20being%20available%20at%0Ahttps%3A//github.com/camlab-ethz/poseidon%20and%20pretrained%20models%20and%20datasets%20at%0Ahttps%3A//huggingface.co/camlab-ethz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseidon%253A%2520Efficient%2520Foundation%2520Models%2520for%2520PDEs%26entry.906535625%3DMaximilian%2520Herde%2520and%2520Bogdan%2520Raoni%25C4%2587%2520and%2520Tobias%2520Rohner%2520and%2520Roger%2520K%25C3%25A4ppeli%2520and%2520Roberto%2520Molinaro%2520and%2520Emmanuel%2520de%2520B%25C3%25A9zenac%2520and%2520Siddhartha%2520Mishra%26entry.1292438233%3D%2520%2520We%2520introduce%2520Poseidon%252C%2520a%2520foundation%2520model%2520for%2520learning%2520the%2520solution%2520operators%250Aof%2520PDEs.%2520It%2520is%2520based%2520on%2520a%2520multiscale%2520operator%2520transformer%252C%2520with%250Atime-conditioned%2520layer%2520norms%2520that%2520enable%2520continuous-in-time%2520evaluations.%2520A%250Anovel%2520training%2520strategy%2520leveraging%2520the%2520semi-group%2520property%2520of%2520time-dependent%250APDEs%2520to%2520allow%2520for%2520significant%2520scaling-up%2520of%2520the%2520training%2520data%2520is%2520also%2520proposed.%250APoseidon%2520is%2520pretrained%2520on%2520a%2520diverse%252C%2520large%2520scale%2520dataset%2520for%2520the%2520governing%250Aequations%2520of%2520fluid%2520dynamics.%2520It%2520is%2520then%2520evaluated%2520on%2520a%2520suite%2520of%252015%2520challenging%250Adownstream%2520tasks%2520that%2520include%2520a%2520wide%2520variety%2520of%2520PDE%2520types%2520and%2520operators.%2520We%250Ashow%2520that%2520Poseidon%2520exhibits%2520excellent%2520performance%2520across%2520the%2520board%2520by%250Aoutperforming%2520baselines%2520significantly%252C%2520both%2520in%2520terms%2520of%2520sample%2520efficiency%2520and%250Aaccuracy.%2520Poseidon%2520also%2520generalizes%2520very%2520well%2520to%2520new%2520physics%2520that%2520is%2520not%2520seen%250Aduring%2520pretraining.%2520Moreover%252C%2520Poseidon%2520scales%2520with%2520respect%2520to%2520model%2520and%2520data%250Asize%252C%2520both%2520for%2520pretraining%2520and%2520for%2520downstream%2520tasks.%2520Taken%2520together%252C%2520our%250Aresults%2520showcase%2520the%2520surprising%2520ability%2520of%2520Poseidon%2520to%2520learn%2520effective%250Arepresentations%2520from%2520a%2520very%2520small%2520set%2520of%2520PDEs%2520during%2520pretraining%2520in%2520order%2520to%250Ageneralize%2520well%2520to%2520unseen%2520and%2520unrelated%2520PDEs%2520downstream%252C%2520demonstrating%2520its%250Apotential%2520as%2520an%2520effective%252C%2520general%2520purpose%2520PDE%2520foundation%2520model.%2520Finally%252C%2520the%250APoseidon%2520model%2520as%2520well%2520as%2520underlying%2520pretraining%2520and%2520downstream%2520datasets%2520are%250Aopen%2520sourced%252C%2520with%2520code%2520being%2520available%2520at%250Ahttps%253A//github.com/camlab-ethz/poseidon%2520and%2520pretrained%2520models%2520and%2520datasets%2520at%250Ahttps%253A//huggingface.co/camlab-ethz.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poseidon%3A%20Efficient%20Foundation%20Models%20for%20PDEs&entry.906535625=Maximilian%20Herde%20and%20Bogdan%20Raoni%C4%87%20and%20Tobias%20Rohner%20and%20Roger%20K%C3%A4ppeli%20and%20Roberto%20Molinaro%20and%20Emmanuel%20de%20B%C3%A9zenac%20and%20Siddhartha%20Mishra&entry.1292438233=%20%20We%20introduce%20Poseidon%2C%20a%20foundation%20model%20for%20learning%20the%20solution%20operators%0Aof%20PDEs.%20It%20is%20based%20on%20a%20multiscale%20operator%20transformer%2C%20with%0Atime-conditioned%20layer%20norms%20that%20enable%20continuous-in-time%20evaluations.%20A%0Anovel%20training%20strategy%20leveraging%20the%20semi-group%20property%20of%20time-dependent%0APDEs%20to%20allow%20for%20significant%20scaling-up%20of%20the%20training%20data%20is%20also%20proposed.%0APoseidon%20is%20pretrained%20on%20a%20diverse%2C%20large%20scale%20dataset%20for%20the%20governing%0Aequations%20of%20fluid%20dynamics.%20It%20is%20then%20evaluated%20on%20a%20suite%20of%2015%20challenging%0Adownstream%20tasks%20that%20include%20a%20wide%20variety%20of%20PDE%20types%20and%20operators.%20We%0Ashow%20that%20Poseidon%20exhibits%20excellent%20performance%20across%20the%20board%20by%0Aoutperforming%20baselines%20significantly%2C%20both%20in%20terms%20of%20sample%20efficiency%20and%0Aaccuracy.%20Poseidon%20also%20generalizes%20very%20well%20to%20new%20physics%20that%20is%20not%20seen%0Aduring%20pretraining.%20Moreover%2C%20Poseidon%20scales%20with%20respect%20to%20model%20and%20data%0Asize%2C%20both%20for%20pretraining%20and%20for%20downstream%20tasks.%20Taken%20together%2C%20our%0Aresults%20showcase%20the%20surprising%20ability%20of%20Poseidon%20to%20learn%20effective%0Arepresentations%20from%20a%20very%20small%20set%20of%20PDEs%20during%20pretraining%20in%20order%20to%0Ageneralize%20well%20to%20unseen%20and%20unrelated%20PDEs%20downstream%2C%20demonstrating%20its%0Apotential%20as%20an%20effective%2C%20general%20purpose%20PDE%20foundation%20model.%20Finally%2C%20the%0APoseidon%20model%20as%20well%20as%20underlying%20pretraining%20and%20downstream%20datasets%20are%0Aopen%20sourced%2C%20with%20code%20being%20available%20at%0Ahttps%3A//github.com/camlab-ethz/poseidon%20and%20pretrained%20models%20and%20datasets%20at%0Ahttps%3A//huggingface.co/camlab-ethz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19101v2&entry.124074799=Read"},
{"title": "Towards Deep Active Learning in Avian Bioacoustics", "author": "Lukas Rauch and Denis Huseljic and Moritz Wirth and Jens Decke and Bernhard Sick and Christoph Scholz", "abstract": "  Passive acoustic monitoring (PAM) in avian bioacoustics enables\ncost-effective and extensive data collection with minimal disruption to natural\nhabitats. Despite advancements in computational avian bioacoustics, deep\nlearning models continue to encounter challenges in adapting to diverse\nenvironments in practical PAM scenarios. This is primarily due to the scarcity\nof annotations, which requires labor-intensive efforts from human experts.\nActive learning (AL) reduces annotation cost and speed ups adaption to diverse\nscenarios by querying the most informative instances for labeling. This paper\noutlines a deep AL approach, introduces key challenges, and conducts a\nsmall-scale pilot study.\n", "link": "http://arxiv.org/abs/2406.18621v2", "date": "2024-11-05", "relevancy": 1.9443, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Deep%20Active%20Learning%20in%20Avian%20Bioacoustics&body=Title%3A%20Towards%20Deep%20Active%20Learning%20in%20Avian%20Bioacoustics%0AAuthor%3A%20Lukas%20Rauch%20and%20Denis%20Huseljic%20and%20Moritz%20Wirth%20and%20Jens%20Decke%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Passive%20acoustic%20monitoring%20%28PAM%29%20in%20avian%20bioacoustics%20enables%0Acost-effective%20and%20extensive%20data%20collection%20with%20minimal%20disruption%20to%20natural%0Ahabitats.%20Despite%20advancements%20in%20computational%20avian%20bioacoustics%2C%20deep%0Alearning%20models%20continue%20to%20encounter%20challenges%20in%20adapting%20to%20diverse%0Aenvironments%20in%20practical%20PAM%20scenarios.%20This%20is%20primarily%20due%20to%20the%20scarcity%0Aof%20annotations%2C%20which%20requires%20labor-intensive%20efforts%20from%20human%20experts.%0AActive%20learning%20%28AL%29%20reduces%20annotation%20cost%20and%20speed%20ups%20adaption%20to%20diverse%0Ascenarios%20by%20querying%20the%20most%20informative%20instances%20for%20labeling.%20This%20paper%0Aoutlines%20a%20deep%20AL%20approach%2C%20introduces%20key%20challenges%2C%20and%20conducts%20a%0Asmall-scale%20pilot%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Deep%2520Active%2520Learning%2520in%2520Avian%2520Bioacoustics%26entry.906535625%3DLukas%2520Rauch%2520and%2520Denis%2520Huseljic%2520and%2520Moritz%2520Wirth%2520and%2520Jens%2520Decke%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Passive%2520acoustic%2520monitoring%2520%2528PAM%2529%2520in%2520avian%2520bioacoustics%2520enables%250Acost-effective%2520and%2520extensive%2520data%2520collection%2520with%2520minimal%2520disruption%2520to%2520natural%250Ahabitats.%2520Despite%2520advancements%2520in%2520computational%2520avian%2520bioacoustics%252C%2520deep%250Alearning%2520models%2520continue%2520to%2520encounter%2520challenges%2520in%2520adapting%2520to%2520diverse%250Aenvironments%2520in%2520practical%2520PAM%2520scenarios.%2520This%2520is%2520primarily%2520due%2520to%2520the%2520scarcity%250Aof%2520annotations%252C%2520which%2520requires%2520labor-intensive%2520efforts%2520from%2520human%2520experts.%250AActive%2520learning%2520%2528AL%2529%2520reduces%2520annotation%2520cost%2520and%2520speed%2520ups%2520adaption%2520to%2520diverse%250Ascenarios%2520by%2520querying%2520the%2520most%2520informative%2520instances%2520for%2520labeling.%2520This%2520paper%250Aoutlines%2520a%2520deep%2520AL%2520approach%252C%2520introduces%2520key%2520challenges%252C%2520and%2520conducts%2520a%250Asmall-scale%2520pilot%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Deep%20Active%20Learning%20in%20Avian%20Bioacoustics&entry.906535625=Lukas%20Rauch%20and%20Denis%20Huseljic%20and%20Moritz%20Wirth%20and%20Jens%20Decke%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Passive%20acoustic%20monitoring%20%28PAM%29%20in%20avian%20bioacoustics%20enables%0Acost-effective%20and%20extensive%20data%20collection%20with%20minimal%20disruption%20to%20natural%0Ahabitats.%20Despite%20advancements%20in%20computational%20avian%20bioacoustics%2C%20deep%0Alearning%20models%20continue%20to%20encounter%20challenges%20in%20adapting%20to%20diverse%0Aenvironments%20in%20practical%20PAM%20scenarios.%20This%20is%20primarily%20due%20to%20the%20scarcity%0Aof%20annotations%2C%20which%20requires%20labor-intensive%20efforts%20from%20human%20experts.%0AActive%20learning%20%28AL%29%20reduces%20annotation%20cost%20and%20speed%20ups%20adaption%20to%20diverse%0Ascenarios%20by%20querying%20the%20most%20informative%20instances%20for%20labeling.%20This%20paper%0Aoutlines%20a%20deep%20AL%20approach%2C%20introduces%20key%20challenges%2C%20and%20conducts%20a%0Asmall-scale%20pilot%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18621v2&entry.124074799=Read"},
{"title": "Arrhythmia Classification Using Graph Neural Networks Based on\n  Correlation Matrix", "author": "Seungwoo Han", "abstract": "  With the advancements in graph neural network, there has been increasing\ninterest in applying this network to ECG signal analysis. In this study, we\ngenerated an adjacency matrix using correlation matrix of extracted features\nand applied a graph neural network to classify arrhythmias. The proposed model\nwas compared with existing approaches from the literature. The results\ndemonstrated that precision and recall for all arrhythmia classes exceeded 50%,\nsuggesting that this method can be considered an approach for arrhythmia\nclassification.\n", "link": "http://arxiv.org/abs/2410.10758v2", "date": "2024-11-05", "relevancy": 1.9372, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3971}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.391}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arrhythmia%20Classification%20Using%20Graph%20Neural%20Networks%20Based%20on%0A%20%20Correlation%20Matrix&body=Title%3A%20Arrhythmia%20Classification%20Using%20Graph%20Neural%20Networks%20Based%20on%0A%20%20Correlation%20Matrix%0AAuthor%3A%20Seungwoo%20Han%0AAbstract%3A%20%20%20With%20the%20advancements%20in%20graph%20neural%20network%2C%20there%20has%20been%20increasing%0Ainterest%20in%20applying%20this%20network%20to%20ECG%20signal%20analysis.%20In%20this%20study%2C%20we%0Agenerated%20an%20adjacency%20matrix%20using%20correlation%20matrix%20of%20extracted%20features%0Aand%20applied%20a%20graph%20neural%20network%20to%20classify%20arrhythmias.%20The%20proposed%20model%0Awas%20compared%20with%20existing%20approaches%20from%20the%20literature.%20The%20results%0Ademonstrated%20that%20precision%20and%20recall%20for%20all%20arrhythmia%20classes%20exceeded%2050%25%2C%0Asuggesting%20that%20this%20method%20can%20be%20considered%20an%20approach%20for%20arrhythmia%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArrhythmia%2520Classification%2520Using%2520Graph%2520Neural%2520Networks%2520Based%2520on%250A%2520%2520Correlation%2520Matrix%26entry.906535625%3DSeungwoo%2520Han%26entry.1292438233%3D%2520%2520With%2520the%2520advancements%2520in%2520graph%2520neural%2520network%252C%2520there%2520has%2520been%2520increasing%250Ainterest%2520in%2520applying%2520this%2520network%2520to%2520ECG%2520signal%2520analysis.%2520In%2520this%2520study%252C%2520we%250Agenerated%2520an%2520adjacency%2520matrix%2520using%2520correlation%2520matrix%2520of%2520extracted%2520features%250Aand%2520applied%2520a%2520graph%2520neural%2520network%2520to%2520classify%2520arrhythmias.%2520The%2520proposed%2520model%250Awas%2520compared%2520with%2520existing%2520approaches%2520from%2520the%2520literature.%2520The%2520results%250Ademonstrated%2520that%2520precision%2520and%2520recall%2520for%2520all%2520arrhythmia%2520classes%2520exceeded%252050%2525%252C%250Asuggesting%2520that%2520this%2520method%2520can%2520be%2520considered%2520an%2520approach%2520for%2520arrhythmia%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arrhythmia%20Classification%20Using%20Graph%20Neural%20Networks%20Based%20on%0A%20%20Correlation%20Matrix&entry.906535625=Seungwoo%20Han&entry.1292438233=%20%20With%20the%20advancements%20in%20graph%20neural%20network%2C%20there%20has%20been%20increasing%0Ainterest%20in%20applying%20this%20network%20to%20ECG%20signal%20analysis.%20In%20this%20study%2C%20we%0Agenerated%20an%20adjacency%20matrix%20using%20correlation%20matrix%20of%20extracted%20features%0Aand%20applied%20a%20graph%20neural%20network%20to%20classify%20arrhythmias.%20The%20proposed%20model%0Awas%20compared%20with%20existing%20approaches%20from%20the%20literature.%20The%20results%0Ademonstrated%20that%20precision%20and%20recall%20for%20all%20arrhythmia%20classes%20exceeded%2050%25%2C%0Asuggesting%20that%20this%20method%20can%20be%20considered%20an%20approach%20for%20arrhythmia%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10758v2&entry.124074799=Read"},
{"title": "AI-Driven approach for sustainable extraction of earth's subsurface\n  renewable energy while minimizing seismic activity", "author": "Diego Gutierrez-Oribio and Alexandros Stathas and Ioannis Stefanou", "abstract": "  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold\nconsiderable promise for meeting the energy sector's large-scale requirements\nand reducing CO$_2$ emissions. However, the injection of fluids into the\nEarth's crust, essential for these activities, can induce or trigger\nearthquakes. In this paper, we highlight a new approach based on Reinforcement\nLearning for the control of human-induced seismicity in the highly complex\nenvironment of an underground reservoir. This complex system poses significant\nchallenges in the control design due to parameter uncertainties and unmodeled\ndynamics. We show that the reinforcement learning algorithm can interact\nefficiently with a robust controller, by choosing the controller parameters in\nreal-time, reducing human-induced seismicity and allowing the consideration of\nfurther production objectives, \\textit{e.g.}, minimal control power.\nSimulations are presented for a simplified underground reservoir under various\nenergy demand scenarios, demonstrating the reliability and effectiveness of the\nproposed control-reinforcement learning approach.\n", "link": "http://arxiv.org/abs/2408.03664v2", "date": "2024-11-05", "relevancy": 1.9251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity&body=Title%3A%20AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity%0AAuthor%3A%20Diego%20Gutierrez-Oribio%20and%20Alexandros%20Stathas%20and%20Ioannis%20Stefanou%0AAbstract%3A%20%20%20Deep%20Geothermal%20Energy%2C%20Carbon%20Capture%20and%20Storage%2C%20and%20Hydrogen%20Storage%20hold%0Aconsiderable%20promise%20for%20meeting%20the%20energy%20sector%27s%20large-scale%20requirements%0Aand%20reducing%20CO%24_2%24%20emissions.%20However%2C%20the%20injection%20of%20fluids%20into%20the%0AEarth%27s%20crust%2C%20essential%20for%20these%20activities%2C%20can%20induce%20or%20trigger%0Aearthquakes.%20In%20this%20paper%2C%20we%20highlight%20a%20new%20approach%20based%20on%20Reinforcement%0ALearning%20for%20the%20control%20of%20human-induced%20seismicity%20in%20the%20highly%20complex%0Aenvironment%20of%20an%20underground%20reservoir.%20This%20complex%20system%20poses%20significant%0Achallenges%20in%20the%20control%20design%20due%20to%20parameter%20uncertainties%20and%20unmodeled%0Adynamics.%20We%20show%20that%20the%20reinforcement%20learning%20algorithm%20can%20interact%0Aefficiently%20with%20a%20robust%20controller%2C%20by%20choosing%20the%20controller%20parameters%20in%0Areal-time%2C%20reducing%20human-induced%20seismicity%20and%20allowing%20the%20consideration%20of%0Afurther%20production%20objectives%2C%20%5Ctextit%7Be.g.%7D%2C%20minimal%20control%20power.%0ASimulations%20are%20presented%20for%20a%20simplified%20underground%20reservoir%20under%20various%0Aenergy%20demand%20scenarios%2C%20demonstrating%20the%20reliability%20and%20effectiveness%20of%20the%0Aproposed%20control-reinforcement%20learning%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Driven%2520approach%2520for%2520sustainable%2520extraction%2520of%2520earth%2527s%2520subsurface%250A%2520%2520renewable%2520energy%2520while%2520minimizing%2520seismic%2520activity%26entry.906535625%3DDiego%2520Gutierrez-Oribio%2520and%2520Alexandros%2520Stathas%2520and%2520Ioannis%2520Stefanou%26entry.1292438233%3D%2520%2520Deep%2520Geothermal%2520Energy%252C%2520Carbon%2520Capture%2520and%2520Storage%252C%2520and%2520Hydrogen%2520Storage%2520hold%250Aconsiderable%2520promise%2520for%2520meeting%2520the%2520energy%2520sector%2527s%2520large-scale%2520requirements%250Aand%2520reducing%2520CO%2524_2%2524%2520emissions.%2520However%252C%2520the%2520injection%2520of%2520fluids%2520into%2520the%250AEarth%2527s%2520crust%252C%2520essential%2520for%2520these%2520activities%252C%2520can%2520induce%2520or%2520trigger%250Aearthquakes.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520a%2520new%2520approach%2520based%2520on%2520Reinforcement%250ALearning%2520for%2520the%2520control%2520of%2520human-induced%2520seismicity%2520in%2520the%2520highly%2520complex%250Aenvironment%2520of%2520an%2520underground%2520reservoir.%2520This%2520complex%2520system%2520poses%2520significant%250Achallenges%2520in%2520the%2520control%2520design%2520due%2520to%2520parameter%2520uncertainties%2520and%2520unmodeled%250Adynamics.%2520We%2520show%2520that%2520the%2520reinforcement%2520learning%2520algorithm%2520can%2520interact%250Aefficiently%2520with%2520a%2520robust%2520controller%252C%2520by%2520choosing%2520the%2520controller%2520parameters%2520in%250Areal-time%252C%2520reducing%2520human-induced%2520seismicity%2520and%2520allowing%2520the%2520consideration%2520of%250Afurther%2520production%2520objectives%252C%2520%255Ctextit%257Be.g.%257D%252C%2520minimal%2520control%2520power.%250ASimulations%2520are%2520presented%2520for%2520a%2520simplified%2520underground%2520reservoir%2520under%2520various%250Aenergy%2520demand%2520scenarios%252C%2520demonstrating%2520the%2520reliability%2520and%2520effectiveness%2520of%2520the%250Aproposed%2520control-reinforcement%2520learning%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity&entry.906535625=Diego%20Gutierrez-Oribio%20and%20Alexandros%20Stathas%20and%20Ioannis%20Stefanou&entry.1292438233=%20%20Deep%20Geothermal%20Energy%2C%20Carbon%20Capture%20and%20Storage%2C%20and%20Hydrogen%20Storage%20hold%0Aconsiderable%20promise%20for%20meeting%20the%20energy%20sector%27s%20large-scale%20requirements%0Aand%20reducing%20CO%24_2%24%20emissions.%20However%2C%20the%20injection%20of%20fluids%20into%20the%0AEarth%27s%20crust%2C%20essential%20for%20these%20activities%2C%20can%20induce%20or%20trigger%0Aearthquakes.%20In%20this%20paper%2C%20we%20highlight%20a%20new%20approach%20based%20on%20Reinforcement%0ALearning%20for%20the%20control%20of%20human-induced%20seismicity%20in%20the%20highly%20complex%0Aenvironment%20of%20an%20underground%20reservoir.%20This%20complex%20system%20poses%20significant%0Achallenges%20in%20the%20control%20design%20due%20to%20parameter%20uncertainties%20and%20unmodeled%0Adynamics.%20We%20show%20that%20the%20reinforcement%20learning%20algorithm%20can%20interact%0Aefficiently%20with%20a%20robust%20controller%2C%20by%20choosing%20the%20controller%20parameters%20in%0Areal-time%2C%20reducing%20human-induced%20seismicity%20and%20allowing%20the%20consideration%20of%0Afurther%20production%20objectives%2C%20%5Ctextit%7Be.g.%7D%2C%20minimal%20control%20power.%0ASimulations%20are%20presented%20for%20a%20simplified%20underground%20reservoir%20under%20various%0Aenergy%20demand%20scenarios%2C%20demonstrating%20the%20reliability%20and%20effectiveness%20of%20the%0Aproposed%20control-reinforcement%20learning%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03664v2&entry.124074799=Read"},
{"title": "Correlating Variational Autoencoders Natively For Multi-View Imputation", "author": "Ella S. C. Orme and Marina Evangelou and Ulrich Paquet", "abstract": "  Multi-view data from the same source often exhibit correlation. This is\nmirrored in correlation between the latent spaces of separate variational\nautoencoders (VAEs) trained on each data-view. A multi-view VAE approach is\nproposed that incorporates a joint prior with a non-zero correlation structure\nbetween the latent spaces of the VAEs. By enforcing such correlation structure,\nmore strongly correlated latent spaces are uncovered. Using conditional\ndistributions to move between these latent spaces, missing views can be imputed\nand used for downstream analysis. Learning this correlation structure involves\nmaintaining validity of the prior distribution, as well as a successful\nparameterization that allows end-to-end learning.\n", "link": "http://arxiv.org/abs/2411.03097v1", "date": "2024-11-05", "relevancy": 1.9191, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4882}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correlating%20Variational%20Autoencoders%20Natively%20For%20Multi-View%20Imputation&body=Title%3A%20Correlating%20Variational%20Autoencoders%20Natively%20For%20Multi-View%20Imputation%0AAuthor%3A%20Ella%20S.%20C.%20Orme%20and%20Marina%20Evangelou%20and%20Ulrich%20Paquet%0AAbstract%3A%20%20%20Multi-view%20data%20from%20the%20same%20source%20often%20exhibit%20correlation.%20This%20is%0Amirrored%20in%20correlation%20between%20the%20latent%20spaces%20of%20separate%20variational%0Aautoencoders%20%28VAEs%29%20trained%20on%20each%20data-view.%20A%20multi-view%20VAE%20approach%20is%0Aproposed%20that%20incorporates%20a%20joint%20prior%20with%20a%20non-zero%20correlation%20structure%0Abetween%20the%20latent%20spaces%20of%20the%20VAEs.%20By%20enforcing%20such%20correlation%20structure%2C%0Amore%20strongly%20correlated%20latent%20spaces%20are%20uncovered.%20Using%20conditional%0Adistributions%20to%20move%20between%20these%20latent%20spaces%2C%20missing%20views%20can%20be%20imputed%0Aand%20used%20for%20downstream%20analysis.%20Learning%20this%20correlation%20structure%20involves%0Amaintaining%20validity%20of%20the%20prior%20distribution%2C%20as%20well%20as%20a%20successful%0Aparameterization%20that%20allows%20end-to-end%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrelating%2520Variational%2520Autoencoders%2520Natively%2520For%2520Multi-View%2520Imputation%26entry.906535625%3DElla%2520S.%2520C.%2520Orme%2520and%2520Marina%2520Evangelou%2520and%2520Ulrich%2520Paquet%26entry.1292438233%3D%2520%2520Multi-view%2520data%2520from%2520the%2520same%2520source%2520often%2520exhibit%2520correlation.%2520This%2520is%250Amirrored%2520in%2520correlation%2520between%2520the%2520latent%2520spaces%2520of%2520separate%2520variational%250Aautoencoders%2520%2528VAEs%2529%2520trained%2520on%2520each%2520data-view.%2520A%2520multi-view%2520VAE%2520approach%2520is%250Aproposed%2520that%2520incorporates%2520a%2520joint%2520prior%2520with%2520a%2520non-zero%2520correlation%2520structure%250Abetween%2520the%2520latent%2520spaces%2520of%2520the%2520VAEs.%2520By%2520enforcing%2520such%2520correlation%2520structure%252C%250Amore%2520strongly%2520correlated%2520latent%2520spaces%2520are%2520uncovered.%2520Using%2520conditional%250Adistributions%2520to%2520move%2520between%2520these%2520latent%2520spaces%252C%2520missing%2520views%2520can%2520be%2520imputed%250Aand%2520used%2520for%2520downstream%2520analysis.%2520Learning%2520this%2520correlation%2520structure%2520involves%250Amaintaining%2520validity%2520of%2520the%2520prior%2520distribution%252C%2520as%2520well%2520as%2520a%2520successful%250Aparameterization%2520that%2520allows%2520end-to-end%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correlating%20Variational%20Autoencoders%20Natively%20For%20Multi-View%20Imputation&entry.906535625=Ella%20S.%20C.%20Orme%20and%20Marina%20Evangelou%20and%20Ulrich%20Paquet&entry.1292438233=%20%20Multi-view%20data%20from%20the%20same%20source%20often%20exhibit%20correlation.%20This%20is%0Amirrored%20in%20correlation%20between%20the%20latent%20spaces%20of%20separate%20variational%0Aautoencoders%20%28VAEs%29%20trained%20on%20each%20data-view.%20A%20multi-view%20VAE%20approach%20is%0Aproposed%20that%20incorporates%20a%20joint%20prior%20with%20a%20non-zero%20correlation%20structure%0Abetween%20the%20latent%20spaces%20of%20the%20VAEs.%20By%20enforcing%20such%20correlation%20structure%2C%0Amore%20strongly%20correlated%20latent%20spaces%20are%20uncovered.%20Using%20conditional%0Adistributions%20to%20move%20between%20these%20latent%20spaces%2C%20missing%20views%20can%20be%20imputed%0Aand%20used%20for%20downstream%20analysis.%20Learning%20this%20correlation%20structure%20involves%0Amaintaining%20validity%20of%20the%20prior%20distribution%2C%20as%20well%20as%20a%20successful%0Aparameterization%20that%20allows%20end-to-end%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03097v1&entry.124074799=Read"},
{"title": "Proxy-informed Bayesian transfer learning with unknown sources", "author": "Sabina J. Sloman and Julien Martinelli and Samuel Kaski", "abstract": "  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Bayesian transfer learning is a principled\nparadigm for specifying this knowledge, and refining it on the basis of data\nfrom the source (training) and target (prediction) tasks. We address the\nchallenging transfer learning setting where the learner (i) cannot fine-tune in\nthe target task, and (ii) does not know which source data points correspond to\nthe same task (i.e., the data sources are unknown). We propose a proxy-informed\nrobust method for probabilistic transfer learning (PROMPT), which provides a\nposterior predictive estimate tailored to the structure of the target task,\nwithout requiring the learner have access to any outcome information from the\ntarget task. Instead, PROMPT relies on the availability of proxy information.\nPROMPT uses the same proxy information for two purposes: (i) estimation of\neffects specific to the target task, and (ii) construction of a robust\nreweighting of the source data for estimation of effects that transfer between\ntasks. We provide theoretical results on the effect of this reweighting on the\nrisk of negative transfer, and demonstrate application of PROMPT in two\nsynthetic settings.\n", "link": "http://arxiv.org/abs/2411.03263v1", "date": "2024-11-05", "relevancy": 1.9171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4647}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources&body=Title%3A%20Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources%0AAuthor%3A%20Sabina%20J.%20Sloman%20and%20Julien%20Martinelli%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Generalization%20outside%20the%20scope%20of%20one%27s%20training%20data%20requires%20leveraging%0Aprior%20knowledge%20about%20the%20effects%20that%20transfer%2C%20and%20the%20effects%20that%20don%27t%2C%0Abetween%20different%20data%20sources.%20Bayesian%20transfer%20learning%20is%20a%20principled%0Aparadigm%20for%20specifying%20this%20knowledge%2C%20and%20refining%20it%20on%20the%20basis%20of%20data%0Afrom%20the%20source%20%28training%29%20and%20target%20%28prediction%29%20tasks.%20We%20address%20the%0Achallenging%20transfer%20learning%20setting%20where%20the%20learner%20%28i%29%20cannot%20fine-tune%20in%0Athe%20target%20task%2C%20and%20%28ii%29%20does%20not%20know%20which%20source%20data%20points%20correspond%20to%0Athe%20same%20task%20%28i.e.%2C%20the%20data%20sources%20are%20unknown%29.%20We%20propose%20a%20proxy-informed%0Arobust%20method%20for%20probabilistic%20transfer%20learning%20%28PROMPT%29%2C%20which%20provides%20a%0Aposterior%20predictive%20estimate%20tailored%20to%20the%20structure%20of%20the%20target%20task%2C%0Awithout%20requiring%20the%20learner%20have%20access%20to%20any%20outcome%20information%20from%20the%0Atarget%20task.%20Instead%2C%20PROMPT%20relies%20on%20the%20availability%20of%20proxy%20information.%0APROMPT%20uses%20the%20same%20proxy%20information%20for%20two%20purposes%3A%20%28i%29%20estimation%20of%0Aeffects%20specific%20to%20the%20target%20task%2C%20and%20%28ii%29%20construction%20of%20a%20robust%0Areweighting%20of%20the%20source%20data%20for%20estimation%20of%20effects%20that%20transfer%20between%0Atasks.%20We%20provide%20theoretical%20results%20on%20the%20effect%20of%20this%20reweighting%20on%20the%0Arisk%20of%20negative%20transfer%2C%20and%20demonstrate%20application%20of%20PROMPT%20in%20two%0Asynthetic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxy-informed%2520Bayesian%2520transfer%2520learning%2520with%2520unknown%2520sources%26entry.906535625%3DSabina%2520J.%2520Sloman%2520and%2520Julien%2520Martinelli%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Generalization%2520outside%2520the%2520scope%2520of%2520one%2527s%2520training%2520data%2520requires%2520leveraging%250Aprior%2520knowledge%2520about%2520the%2520effects%2520that%2520transfer%252C%2520and%2520the%2520effects%2520that%2520don%2527t%252C%250Abetween%2520different%2520data%2520sources.%2520Bayesian%2520transfer%2520learning%2520is%2520a%2520principled%250Aparadigm%2520for%2520specifying%2520this%2520knowledge%252C%2520and%2520refining%2520it%2520on%2520the%2520basis%2520of%2520data%250Afrom%2520the%2520source%2520%2528training%2529%2520and%2520target%2520%2528prediction%2529%2520tasks.%2520We%2520address%2520the%250Achallenging%2520transfer%2520learning%2520setting%2520where%2520the%2520learner%2520%2528i%2529%2520cannot%2520fine-tune%2520in%250Athe%2520target%2520task%252C%2520and%2520%2528ii%2529%2520does%2520not%2520know%2520which%2520source%2520data%2520points%2520correspond%2520to%250Athe%2520same%2520task%2520%2528i.e.%252C%2520the%2520data%2520sources%2520are%2520unknown%2529.%2520We%2520propose%2520a%2520proxy-informed%250Arobust%2520method%2520for%2520probabilistic%2520transfer%2520learning%2520%2528PROMPT%2529%252C%2520which%2520provides%2520a%250Aposterior%2520predictive%2520estimate%2520tailored%2520to%2520the%2520structure%2520of%2520the%2520target%2520task%252C%250Awithout%2520requiring%2520the%2520learner%2520have%2520access%2520to%2520any%2520outcome%2520information%2520from%2520the%250Atarget%2520task.%2520Instead%252C%2520PROMPT%2520relies%2520on%2520the%2520availability%2520of%2520proxy%2520information.%250APROMPT%2520uses%2520the%2520same%2520proxy%2520information%2520for%2520two%2520purposes%253A%2520%2528i%2529%2520estimation%2520of%250Aeffects%2520specific%2520to%2520the%2520target%2520task%252C%2520and%2520%2528ii%2529%2520construction%2520of%2520a%2520robust%250Areweighting%2520of%2520the%2520source%2520data%2520for%2520estimation%2520of%2520effects%2520that%2520transfer%2520between%250Atasks.%2520We%2520provide%2520theoretical%2520results%2520on%2520the%2520effect%2520of%2520this%2520reweighting%2520on%2520the%250Arisk%2520of%2520negative%2520transfer%252C%2520and%2520demonstrate%2520application%2520of%2520PROMPT%2520in%2520two%250Asynthetic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources&entry.906535625=Sabina%20J.%20Sloman%20and%20Julien%20Martinelli%20and%20Samuel%20Kaski&entry.1292438233=%20%20Generalization%20outside%20the%20scope%20of%20one%27s%20training%20data%20requires%20leveraging%0Aprior%20knowledge%20about%20the%20effects%20that%20transfer%2C%20and%20the%20effects%20that%20don%27t%2C%0Abetween%20different%20data%20sources.%20Bayesian%20transfer%20learning%20is%20a%20principled%0Aparadigm%20for%20specifying%20this%20knowledge%2C%20and%20refining%20it%20on%20the%20basis%20of%20data%0Afrom%20the%20source%20%28training%29%20and%20target%20%28prediction%29%20tasks.%20We%20address%20the%0Achallenging%20transfer%20learning%20setting%20where%20the%20learner%20%28i%29%20cannot%20fine-tune%20in%0Athe%20target%20task%2C%20and%20%28ii%29%20does%20not%20know%20which%20source%20data%20points%20correspond%20to%0Athe%20same%20task%20%28i.e.%2C%20the%20data%20sources%20are%20unknown%29.%20We%20propose%20a%20proxy-informed%0Arobust%20method%20for%20probabilistic%20transfer%20learning%20%28PROMPT%29%2C%20which%20provides%20a%0Aposterior%20predictive%20estimate%20tailored%20to%20the%20structure%20of%20the%20target%20task%2C%0Awithout%20requiring%20the%20learner%20have%20access%20to%20any%20outcome%20information%20from%20the%0Atarget%20task.%20Instead%2C%20PROMPT%20relies%20on%20the%20availability%20of%20proxy%20information.%0APROMPT%20uses%20the%20same%20proxy%20information%20for%20two%20purposes%3A%20%28i%29%20estimation%20of%0Aeffects%20specific%20to%20the%20target%20task%2C%20and%20%28ii%29%20construction%20of%20a%20robust%0Areweighting%20of%20the%20source%20data%20for%20estimation%20of%20effects%20that%20transfer%20between%0Atasks.%20We%20provide%20theoretical%20results%20on%20the%20effect%20of%20this%20reweighting%20on%20the%0Arisk%20of%20negative%20transfer%2C%20and%20demonstrate%20application%20of%20PROMPT%20in%20two%0Asynthetic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03263v1&entry.124074799=Read"},
{"title": "The Impact of Generative Artificial Intelligence on Ideation and the\n  performance of Innovation Teams (Preprint)", "author": "Michael Gindert and Marvin Lutz M\u00fcller", "abstract": "  This study investigates the impact of Generative Artificial Intelligence\n(GenAI) on the dynam-ics and performance of innovation teams during the idea\ngeneration phase of the innovation process. Utilizing a custom AI-augmented\nideation tool, the study applies the Knowledge Spill-over Theory of\nEntrepreneurship to understand the effects of AI on knowledge spillover,\ngen-eration and application. Through a framed field experiment with\nparticipants divided into exper-imental and control groups, findings indicate\nthat AI-augmented teams generated higher quali-ty ideas in less time. GenAI\napplication led to improved efficiency, knowledge exchange, in-creased\nsatisfaction and engagement as well as enhanced idea diversity. These results\nhigh-light the transformative role of the field of AI within the innovation\nmanagement domain and shows that GenAI has a positive impact on important\nelements of the Knowledge Spillover Theory of Entrepeneurship, emphasizing its\npotential impact on innovation, entrepreneurship, and economic growth. Future\nresearch should further explore the dynamic interaction be-tween GenAI and\ncreative processes.\n", "link": "http://arxiv.org/abs/2410.18357v3", "date": "2024-11-05", "relevancy": 1.9035, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5368}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4611}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Generative%20Artificial%20Intelligence%20on%20Ideation%20and%20the%0A%20%20performance%20of%20Innovation%20Teams%20%28Preprint%29&body=Title%3A%20The%20Impact%20of%20Generative%20Artificial%20Intelligence%20on%20Ideation%20and%20the%0A%20%20performance%20of%20Innovation%20Teams%20%28Preprint%29%0AAuthor%3A%20Michael%20Gindert%20and%20Marvin%20Lutz%20M%C3%BCller%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20impact%20of%20Generative%20Artificial%20Intelligence%0A%28GenAI%29%20on%20the%20dynam-ics%20and%20performance%20of%20innovation%20teams%20during%20the%20idea%0Ageneration%20phase%20of%20the%20innovation%20process.%20Utilizing%20a%20custom%20AI-augmented%0Aideation%20tool%2C%20the%20study%20applies%20the%20Knowledge%20Spill-over%20Theory%20of%0AEntrepreneurship%20to%20understand%20the%20effects%20of%20AI%20on%20knowledge%20spillover%2C%0Agen-eration%20and%20application.%20Through%20a%20framed%20field%20experiment%20with%0Aparticipants%20divided%20into%20exper-imental%20and%20control%20groups%2C%20findings%20indicate%0Athat%20AI-augmented%20teams%20generated%20higher%20quali-ty%20ideas%20in%20less%20time.%20GenAI%0Aapplication%20led%20to%20improved%20efficiency%2C%20knowledge%20exchange%2C%20in-creased%0Asatisfaction%20and%20engagement%20as%20well%20as%20enhanced%20idea%20diversity.%20These%20results%0Ahigh-light%20the%20transformative%20role%20of%20the%20field%20of%20AI%20within%20the%20innovation%0Amanagement%20domain%20and%20shows%20that%20GenAI%20has%20a%20positive%20impact%20on%20important%0Aelements%20of%20the%20Knowledge%20Spillover%20Theory%20of%20Entrepeneurship%2C%20emphasizing%20its%0Apotential%20impact%20on%20innovation%2C%20entrepreneurship%2C%20and%20economic%20growth.%20Future%0Aresearch%20should%20further%20explore%20the%20dynamic%20interaction%20be-tween%20GenAI%20and%0Acreative%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18357v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Generative%2520Artificial%2520Intelligence%2520on%2520Ideation%2520and%2520the%250A%2520%2520performance%2520of%2520Innovation%2520Teams%2520%2528Preprint%2529%26entry.906535625%3DMichael%2520Gindert%2520and%2520Marvin%2520Lutz%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520impact%2520of%2520Generative%2520Artificial%2520Intelligence%250A%2528GenAI%2529%2520on%2520the%2520dynam-ics%2520and%2520performance%2520of%2520innovation%2520teams%2520during%2520the%2520idea%250Ageneration%2520phase%2520of%2520the%2520innovation%2520process.%2520Utilizing%2520a%2520custom%2520AI-augmented%250Aideation%2520tool%252C%2520the%2520study%2520applies%2520the%2520Knowledge%2520Spill-over%2520Theory%2520of%250AEntrepreneurship%2520to%2520understand%2520the%2520effects%2520of%2520AI%2520on%2520knowledge%2520spillover%252C%250Agen-eration%2520and%2520application.%2520Through%2520a%2520framed%2520field%2520experiment%2520with%250Aparticipants%2520divided%2520into%2520exper-imental%2520and%2520control%2520groups%252C%2520findings%2520indicate%250Athat%2520AI-augmented%2520teams%2520generated%2520higher%2520quali-ty%2520ideas%2520in%2520less%2520time.%2520GenAI%250Aapplication%2520led%2520to%2520improved%2520efficiency%252C%2520knowledge%2520exchange%252C%2520in-creased%250Asatisfaction%2520and%2520engagement%2520as%2520well%2520as%2520enhanced%2520idea%2520diversity.%2520These%2520results%250Ahigh-light%2520the%2520transformative%2520role%2520of%2520the%2520field%2520of%2520AI%2520within%2520the%2520innovation%250Amanagement%2520domain%2520and%2520shows%2520that%2520GenAI%2520has%2520a%2520positive%2520impact%2520on%2520important%250Aelements%2520of%2520the%2520Knowledge%2520Spillover%2520Theory%2520of%2520Entrepeneurship%252C%2520emphasizing%2520its%250Apotential%2520impact%2520on%2520innovation%252C%2520entrepreneurship%252C%2520and%2520economic%2520growth.%2520Future%250Aresearch%2520should%2520further%2520explore%2520the%2520dynamic%2520interaction%2520be-tween%2520GenAI%2520and%250Acreative%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18357v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Generative%20Artificial%20Intelligence%20on%20Ideation%20and%20the%0A%20%20performance%20of%20Innovation%20Teams%20%28Preprint%29&entry.906535625=Michael%20Gindert%20and%20Marvin%20Lutz%20M%C3%BCller&entry.1292438233=%20%20This%20study%20investigates%20the%20impact%20of%20Generative%20Artificial%20Intelligence%0A%28GenAI%29%20on%20the%20dynam-ics%20and%20performance%20of%20innovation%20teams%20during%20the%20idea%0Ageneration%20phase%20of%20the%20innovation%20process.%20Utilizing%20a%20custom%20AI-augmented%0Aideation%20tool%2C%20the%20study%20applies%20the%20Knowledge%20Spill-over%20Theory%20of%0AEntrepreneurship%20to%20understand%20the%20effects%20of%20AI%20on%20knowledge%20spillover%2C%0Agen-eration%20and%20application.%20Through%20a%20framed%20field%20experiment%20with%0Aparticipants%20divided%20into%20exper-imental%20and%20control%20groups%2C%20findings%20indicate%0Athat%20AI-augmented%20teams%20generated%20higher%20quali-ty%20ideas%20in%20less%20time.%20GenAI%0Aapplication%20led%20to%20improved%20efficiency%2C%20knowledge%20exchange%2C%20in-creased%0Asatisfaction%20and%20engagement%20as%20well%20as%20enhanced%20idea%20diversity.%20These%20results%0Ahigh-light%20the%20transformative%20role%20of%20the%20field%20of%20AI%20within%20the%20innovation%0Amanagement%20domain%20and%20shows%20that%20GenAI%20has%20a%20positive%20impact%20on%20important%0Aelements%20of%20the%20Knowledge%20Spillover%20Theory%20of%20Entrepeneurship%2C%20emphasizing%20its%0Apotential%20impact%20on%20innovation%2C%20entrepreneurship%2C%20and%20economic%20growth.%20Future%0Aresearch%20should%20further%20explore%20the%20dynamic%20interaction%20be-tween%20GenAI%20and%0Acreative%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18357v3&entry.124074799=Read"},
{"title": "Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods", "author": "Remi Laumont and Yiqiu Dong and Martin Skovgaard Andersen", "abstract": "  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n", "link": "http://arxiv.org/abs/2406.16658v3", "date": "2024-11-05", "relevancy": 1.8908, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.473}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20Strategies%20in%20Bayesian%20Inversion%3A%20A%20Study%20of%20RTO%20and%20Langevin%0A%20%20Methods&body=Title%3A%20Sampling%20Strategies%20in%20Bayesian%20Inversion%3A%20A%20Study%20of%20RTO%20and%20Langevin%0A%20%20Methods%0AAuthor%3A%20Remi%20Laumont%20and%20Yiqiu%20Dong%20and%20Martin%20Skovgaard%20Andersen%0AAbstract%3A%20%20%20This%20paper%20studies%20two%20classes%20of%20sampling%20methods%20for%20the%20solution%20of%0Ainverse%20problems%2C%20namely%20Randomize-Then-Optimize%20%28RTO%29%2C%20which%20is%20rooted%20in%0Asensitivity%20analysis%2C%20and%20Langevin%20methods%2C%20which%20are%20rooted%20in%20the%20Bayesian%0Aframework.%20The%20two%20classes%20of%20methods%20correspond%20to%20different%20assumptions%20and%0Ayield%20samples%20from%20different%20target%20distributions.%20We%20highlight%20the%20main%0Aconceptual%20and%20theoretical%20differences%20between%20the%20two%20approaches%20and%20compare%0Athem%20from%20a%20practical%20point%20of%20view%20by%20tackling%20two%20classical%20inverse%20problems%0Ain%20imaging%3A%20deblurring%20and%20inpainting.%20We%20show%20that%20the%20choice%20of%20the%20sampling%0Amethod%20has%20a%20significant%20impact%20on%20the%20quality%20of%20the%20reconstruction%20and%20that%0Athe%20RTO%20method%20is%20more%20robust%20to%20the%20choice%20of%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16658v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520Strategies%2520in%2520Bayesian%2520Inversion%253A%2520A%2520Study%2520of%2520RTO%2520and%2520Langevin%250A%2520%2520Methods%26entry.906535625%3DRemi%2520Laumont%2520and%2520Yiqiu%2520Dong%2520and%2520Martin%2520Skovgaard%2520Andersen%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520two%2520classes%2520of%2520sampling%2520methods%2520for%2520the%2520solution%2520of%250Ainverse%2520problems%252C%2520namely%2520Randomize-Then-Optimize%2520%2528RTO%2529%252C%2520which%2520is%2520rooted%2520in%250Asensitivity%2520analysis%252C%2520and%2520Langevin%2520methods%252C%2520which%2520are%2520rooted%2520in%2520the%2520Bayesian%250Aframework.%2520The%2520two%2520classes%2520of%2520methods%2520correspond%2520to%2520different%2520assumptions%2520and%250Ayield%2520samples%2520from%2520different%2520target%2520distributions.%2520We%2520highlight%2520the%2520main%250Aconceptual%2520and%2520theoretical%2520differences%2520between%2520the%2520two%2520approaches%2520and%2520compare%250Athem%2520from%2520a%2520practical%2520point%2520of%2520view%2520by%2520tackling%2520two%2520classical%2520inverse%2520problems%250Ain%2520imaging%253A%2520deblurring%2520and%2520inpainting.%2520We%2520show%2520that%2520the%2520choice%2520of%2520the%2520sampling%250Amethod%2520has%2520a%2520significant%2520impact%2520on%2520the%2520quality%2520of%2520the%2520reconstruction%2520and%2520that%250Athe%2520RTO%2520method%2520is%2520more%2520robust%2520to%2520the%2520choice%2520of%2520the%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16658v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20Strategies%20in%20Bayesian%20Inversion%3A%20A%20Study%20of%20RTO%20and%20Langevin%0A%20%20Methods&entry.906535625=Remi%20Laumont%20and%20Yiqiu%20Dong%20and%20Martin%20Skovgaard%20Andersen&entry.1292438233=%20%20This%20paper%20studies%20two%20classes%20of%20sampling%20methods%20for%20the%20solution%20of%0Ainverse%20problems%2C%20namely%20Randomize-Then-Optimize%20%28RTO%29%2C%20which%20is%20rooted%20in%0Asensitivity%20analysis%2C%20and%20Langevin%20methods%2C%20which%20are%20rooted%20in%20the%20Bayesian%0Aframework.%20The%20two%20classes%20of%20methods%20correspond%20to%20different%20assumptions%20and%0Ayield%20samples%20from%20different%20target%20distributions.%20We%20highlight%20the%20main%0Aconceptual%20and%20theoretical%20differences%20between%20the%20two%20approaches%20and%20compare%0Athem%20from%20a%20practical%20point%20of%20view%20by%20tackling%20two%20classical%20inverse%20problems%0Ain%20imaging%3A%20deblurring%20and%20inpainting.%20We%20show%20that%20the%20choice%20of%20the%20sampling%0Amethod%20has%20a%20significant%20impact%20on%20the%20quality%20of%20the%20reconstruction%20and%20that%0Athe%20RTO%20method%20is%20more%20robust%20to%20the%20choice%20of%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16658v3&entry.124074799=Read"},
{"title": "Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs", "author": "Long-Fei Li and Peng Zhao and Zhi-Hua Zhou", "abstract": "  We study episodic linear mixture MDPs with the unknown transition and\nadversarial rewards under full-information feedback, employing dynamic regret\nas the performance measure. We start with in-depth analyses of the strengths\nand limitations of the two most popular methods: occupancy-measure-based and\npolicy-based methods. We observe that while the occupancy-measure-based method\nis effective in addressing non-stationary environments, it encounters\ndifficulties with the unknown transition. In contrast, the policy-based method\ncan deal with the unknown transition effectively but faces challenges in\nhandling non-stationary environments. Building on this, we propose a novel\nalgorithm that combines the benefits of both methods. Specifically, it employs\n(i) an occupancy-measure-based global optimization with a two-layer structure\nto handle non-stationary environments; and (ii) a policy-based variance-aware\nvalue-targeted regression to tackle the unknown transition. We bridge these two\nparts by a novel conversion. Our algorithm enjoys an $\\widetilde{\\mathcal{O}}(d\n\\sqrt{H^3 K} + \\sqrt{HK(H + \\bar{P}_K)})$ dynamic regret, where $d$ is the\nfeature dimension, $H$ is the episode length, $K$ is the number of episodes,\n$\\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up\nto logarithmic factors by establishing a matching lower bound. To the best of\nour knowledge, this is the first work that achieves near-optimal dynamic regret\nfor adversarial linear mixture MDPs with the unknown transition without prior\nknowledge of the non-stationarity measure.\n", "link": "http://arxiv.org/abs/2411.03107v1", "date": "2024-11-05", "relevancy": 1.8702, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4665}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Optimal%20Dynamic%20Regret%20for%20Adversarial%20Linear%20Mixture%20MDPs&body=Title%3A%20Near-Optimal%20Dynamic%20Regret%20for%20Adversarial%20Linear%20Mixture%20MDPs%0AAuthor%3A%20Long-Fei%20Li%20and%20Peng%20Zhao%20and%20Zhi-Hua%20Zhou%0AAbstract%3A%20%20%20We%20study%20episodic%20linear%20mixture%20MDPs%20with%20the%20unknown%20transition%20and%0Aadversarial%20rewards%20under%20full-information%20feedback%2C%20employing%20dynamic%20regret%0Aas%20the%20performance%20measure.%20We%20start%20with%20in-depth%20analyses%20of%20the%20strengths%0Aand%20limitations%20of%20the%20two%20most%20popular%20methods%3A%20occupancy-measure-based%20and%0Apolicy-based%20methods.%20We%20observe%20that%20while%20the%20occupancy-measure-based%20method%0Ais%20effective%20in%20addressing%20non-stationary%20environments%2C%20it%20encounters%0Adifficulties%20with%20the%20unknown%20transition.%20In%20contrast%2C%20the%20policy-based%20method%0Acan%20deal%20with%20the%20unknown%20transition%20effectively%20but%20faces%20challenges%20in%0Ahandling%20non-stationary%20environments.%20Building%20on%20this%2C%20we%20propose%20a%20novel%0Aalgorithm%20that%20combines%20the%20benefits%20of%20both%20methods.%20Specifically%2C%20it%20employs%0A%28i%29%20an%20occupancy-measure-based%20global%20optimization%20with%20a%20two-layer%20structure%0Ato%20handle%20non-stationary%20environments%3B%20and%20%28ii%29%20a%20policy-based%20variance-aware%0Avalue-targeted%20regression%20to%20tackle%20the%20unknown%20transition.%20We%20bridge%20these%20two%0Aparts%20by%20a%20novel%20conversion.%20Our%20algorithm%20enjoys%20an%20%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28d%0A%5Csqrt%7BH%5E3%20K%7D%20%2B%20%5Csqrt%7BHK%28H%20%2B%20%5Cbar%7BP%7D_K%29%7D%29%24%20dynamic%20regret%2C%20where%20%24d%24%20is%20the%0Afeature%20dimension%2C%20%24H%24%20is%20the%20episode%20length%2C%20%24K%24%20is%20the%20number%20of%20episodes%2C%0A%24%5Cbar%7BP%7D_K%24%20is%20the%20non-stationarity%20measure.%20We%20show%20it%20is%20minimax%20optimal%20up%0Ato%20logarithmic%20factors%20by%20establishing%20a%20matching%20lower%20bound.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20work%20that%20achieves%20near-optimal%20dynamic%20regret%0Afor%20adversarial%20linear%20mixture%20MDPs%20with%20the%20unknown%20transition%20without%20prior%0Aknowledge%20of%20the%20non-stationarity%20measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Optimal%2520Dynamic%2520Regret%2520for%2520Adversarial%2520Linear%2520Mixture%2520MDPs%26entry.906535625%3DLong-Fei%2520Li%2520and%2520Peng%2520Zhao%2520and%2520Zhi-Hua%2520Zhou%26entry.1292438233%3D%2520%2520We%2520study%2520episodic%2520linear%2520mixture%2520MDPs%2520with%2520the%2520unknown%2520transition%2520and%250Aadversarial%2520rewards%2520under%2520full-information%2520feedback%252C%2520employing%2520dynamic%2520regret%250Aas%2520the%2520performance%2520measure.%2520We%2520start%2520with%2520in-depth%2520analyses%2520of%2520the%2520strengths%250Aand%2520limitations%2520of%2520the%2520two%2520most%2520popular%2520methods%253A%2520occupancy-measure-based%2520and%250Apolicy-based%2520methods.%2520We%2520observe%2520that%2520while%2520the%2520occupancy-measure-based%2520method%250Ais%2520effective%2520in%2520addressing%2520non-stationary%2520environments%252C%2520it%2520encounters%250Adifficulties%2520with%2520the%2520unknown%2520transition.%2520In%2520contrast%252C%2520the%2520policy-based%2520method%250Acan%2520deal%2520with%2520the%2520unknown%2520transition%2520effectively%2520but%2520faces%2520challenges%2520in%250Ahandling%2520non-stationary%2520environments.%2520Building%2520on%2520this%252C%2520we%2520propose%2520a%2520novel%250Aalgorithm%2520that%2520combines%2520the%2520benefits%2520of%2520both%2520methods.%2520Specifically%252C%2520it%2520employs%250A%2528i%2529%2520an%2520occupancy-measure-based%2520global%2520optimization%2520with%2520a%2520two-layer%2520structure%250Ato%2520handle%2520non-stationary%2520environments%253B%2520and%2520%2528ii%2529%2520a%2520policy-based%2520variance-aware%250Avalue-targeted%2520regression%2520to%2520tackle%2520the%2520unknown%2520transition.%2520We%2520bridge%2520these%2520two%250Aparts%2520by%2520a%2520novel%2520conversion.%2520Our%2520algorithm%2520enjoys%2520an%2520%2524%255Cwidetilde%257B%255Cmathcal%257BO%257D%257D%2528d%250A%255Csqrt%257BH%255E3%2520K%257D%2520%252B%2520%255Csqrt%257BHK%2528H%2520%252B%2520%255Cbar%257BP%257D_K%2529%257D%2529%2524%2520dynamic%2520regret%252C%2520where%2520%2524d%2524%2520is%2520the%250Afeature%2520dimension%252C%2520%2524H%2524%2520is%2520the%2520episode%2520length%252C%2520%2524K%2524%2520is%2520the%2520number%2520of%2520episodes%252C%250A%2524%255Cbar%257BP%257D_K%2524%2520is%2520the%2520non-stationarity%2520measure.%2520We%2520show%2520it%2520is%2520minimax%2520optimal%2520up%250Ato%2520logarithmic%2520factors%2520by%2520establishing%2520a%2520matching%2520lower%2520bound.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%2520achieves%2520near-optimal%2520dynamic%2520regret%250Afor%2520adversarial%2520linear%2520mixture%2520MDPs%2520with%2520the%2520unknown%2520transition%2520without%2520prior%250Aknowledge%2520of%2520the%2520non-stationarity%2520measure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Optimal%20Dynamic%20Regret%20for%20Adversarial%20Linear%20Mixture%20MDPs&entry.906535625=Long-Fei%20Li%20and%20Peng%20Zhao%20and%20Zhi-Hua%20Zhou&entry.1292438233=%20%20We%20study%20episodic%20linear%20mixture%20MDPs%20with%20the%20unknown%20transition%20and%0Aadversarial%20rewards%20under%20full-information%20feedback%2C%20employing%20dynamic%20regret%0Aas%20the%20performance%20measure.%20We%20start%20with%20in-depth%20analyses%20of%20the%20strengths%0Aand%20limitations%20of%20the%20two%20most%20popular%20methods%3A%20occupancy-measure-based%20and%0Apolicy-based%20methods.%20We%20observe%20that%20while%20the%20occupancy-measure-based%20method%0Ais%20effective%20in%20addressing%20non-stationary%20environments%2C%20it%20encounters%0Adifficulties%20with%20the%20unknown%20transition.%20In%20contrast%2C%20the%20policy-based%20method%0Acan%20deal%20with%20the%20unknown%20transition%20effectively%20but%20faces%20challenges%20in%0Ahandling%20non-stationary%20environments.%20Building%20on%20this%2C%20we%20propose%20a%20novel%0Aalgorithm%20that%20combines%20the%20benefits%20of%20both%20methods.%20Specifically%2C%20it%20employs%0A%28i%29%20an%20occupancy-measure-based%20global%20optimization%20with%20a%20two-layer%20structure%0Ato%20handle%20non-stationary%20environments%3B%20and%20%28ii%29%20a%20policy-based%20variance-aware%0Avalue-targeted%20regression%20to%20tackle%20the%20unknown%20transition.%20We%20bridge%20these%20two%0Aparts%20by%20a%20novel%20conversion.%20Our%20algorithm%20enjoys%20an%20%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28d%0A%5Csqrt%7BH%5E3%20K%7D%20%2B%20%5Csqrt%7BHK%28H%20%2B%20%5Cbar%7BP%7D_K%29%7D%29%24%20dynamic%20regret%2C%20where%20%24d%24%20is%20the%0Afeature%20dimension%2C%20%24H%24%20is%20the%20episode%20length%2C%20%24K%24%20is%20the%20number%20of%20episodes%2C%0A%24%5Cbar%7BP%7D_K%24%20is%20the%20non-stationarity%20measure.%20We%20show%20it%20is%20minimax%20optimal%20up%0Ato%20logarithmic%20factors%20by%20establishing%20a%20matching%20lower%20bound.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20work%20that%20achieves%20near-optimal%20dynamic%20regret%0Afor%20adversarial%20linear%20mixture%20MDPs%20with%20the%20unknown%20transition%20without%20prior%0Aknowledge%20of%20the%20non-stationarity%20measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03107v1&entry.124074799=Read"},
{"title": "Optimal Matrix Sketching over Sliding Windows", "author": "Hanyan Yin and Dongxie Wen and Jiajun Li and Zhewei Wei and Xiao Zhang and Zengfeng Huang and Feifei Li", "abstract": "  Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in\n\\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a\nsmaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell\n\\ll N$, has garnered increasing attention in fields such as large-scale data\nanalytics and machine learning. A well-known deterministic matrix sketching\nmethod is the Frequent Directions algorithm, which achieves the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance\nerror guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} -\n\\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$.\nThe matrix sketching problem becomes particularly interesting in the context of\nsliding windows, where the goal is to approximate the matrix\n$\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time\nunits. However, despite recent efforts, whether achieving the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is\npossible has remained an open question.\n  In this paper, we introduce the DS-FD algorithm, which achieves the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over\nrow-normalized, sequence-based sliding windows. We also present matching upper\nand lower space bounds for time-based and unnormalized sliding windows,\ndemonstrating the generality and optimality of \\dsfd across various sliding\nwindow models. This conclusively answers the open question regarding the\noptimal space bound for matrix sketching over sliding windows. Furthermore, we\nconduct extensive experiments with both synthetic and real-world datasets,\nvalidating our theoretical claims and thus confirming the correctness and\neffectiveness of our algorithm, both theoretically and empirically.\n", "link": "http://arxiv.org/abs/2405.07792v2", "date": "2024-11-05", "relevancy": 1.8631, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4779}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4663}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows&body=Title%3A%20Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows%0AAuthor%3A%20Hanyan%20Yin%20and%20Dongxie%20Wen%20and%20Jiajun%20Li%20and%20Zhewei%20Wei%20and%20Xiao%20Zhang%20and%20Zengfeng%20Huang%20and%20Feifei%20Li%0AAbstract%3A%20%20%20Matrix%20sketching%2C%20aimed%20at%20approximating%20a%20matrix%20%24%5Cboldsymbol%7BA%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20d%7D%24%20consisting%20of%20vector%20streams%20of%20length%20%24N%24%20with%20a%0Asmaller%20sketching%20matrix%20%24%5Cboldsymbol%7BB%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cell%5Ctimes%20d%7D%2C%20%5Cell%0A%5Cll%20N%24%2C%20has%20garnered%20increasing%20attention%20in%20fields%20such%20as%20large-scale%20data%0Aanalytics%20and%20machine%20learning.%20A%20well-known%20deterministic%20matrix%20sketching%0Amethod%20is%20the%20Frequent%20Directions%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20and%20provides%20a%20covariance%0Aerror%20guarantee%20of%20%24%5Cvarepsilon%20%3D%20%5ClVert%20%5Cboldsymbol%7BA%7D%5E%5Ctop%20%5Cboldsymbol%7BA%7D%20-%0A%5Cboldsymbol%7BB%7D%5E%5Ctop%20%5Cboldsymbol%7BB%7D%20%5CrVert_2/%5ClVert%20%5Cboldsymbol%7BA%7D%20%5CrVert_F%5E2%24.%0AThe%20matrix%20sketching%20problem%20becomes%20particularly%20interesting%20in%20the%20context%20of%0Asliding%20windows%2C%20where%20the%20goal%20is%20to%20approximate%20the%20matrix%0A%24%5Cboldsymbol%7BA%7D_W%24%2C%20formed%20by%20input%20vectors%20over%20the%20most%20recent%20%24N%24%20time%0Aunits.%20However%2C%20despite%20recent%20efforts%2C%20whether%20achieving%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20on%20sliding%20windows%20is%0Apossible%20has%20remained%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20introduce%20the%20DS-FD%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20for%20matrix%20sketching%20over%0Arow-normalized%2C%20sequence-based%20sliding%20windows.%20We%20also%20present%20matching%20upper%0Aand%20lower%20space%20bounds%20for%20time-based%20and%20unnormalized%20sliding%20windows%2C%0Ademonstrating%20the%20generality%20and%20optimality%20of%20%5Cdsfd%20across%20various%20sliding%0Awindow%20models.%20This%20conclusively%20answers%20the%20open%20question%20regarding%20the%0Aoptimal%20space%20bound%20for%20matrix%20sketching%20over%20sliding%20windows.%20Furthermore%2C%20we%0Aconduct%20extensive%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%2C%0Avalidating%20our%20theoretical%20claims%20and%20thus%20confirming%20the%20correctness%20and%0Aeffectiveness%20of%20our%20algorithm%2C%20both%20theoretically%20and%20empirically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Matrix%2520Sketching%2520over%2520Sliding%2520Windows%26entry.906535625%3DHanyan%2520Yin%2520and%2520Dongxie%2520Wen%2520and%2520Jiajun%2520Li%2520and%2520Zhewei%2520Wei%2520and%2520Xiao%2520Zhang%2520and%2520Zengfeng%2520Huang%2520and%2520Feifei%2520Li%26entry.1292438233%3D%2520%2520Matrix%2520sketching%252C%2520aimed%2520at%2520approximating%2520a%2520matrix%2520%2524%255Cboldsymbol%257BA%257D%2520%255Cin%250A%255Cmathbb%257BR%257D%255E%257BN%255Ctimes%2520d%257D%2524%2520consisting%2520of%2520vector%2520streams%2520of%2520length%2520%2524N%2524%2520with%2520a%250Asmaller%2520sketching%2520matrix%2520%2524%255Cboldsymbol%257BB%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257B%255Cell%255Ctimes%2520d%257D%252C%2520%255Cell%250A%255Cll%2520N%2524%252C%2520has%2520garnered%2520increasing%2520attention%2520in%2520fields%2520such%2520as%2520large-scale%2520data%250Aanalytics%2520and%2520machine%2520learning.%2520A%2520well-known%2520deterministic%2520matrix%2520sketching%250Amethod%2520is%2520the%2520Frequent%2520Directions%2520algorithm%252C%2520which%2520achieves%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520and%2520provides%2520a%2520covariance%250Aerror%2520guarantee%2520of%2520%2524%255Cvarepsilon%2520%253D%2520%255ClVert%2520%255Cboldsymbol%257BA%257D%255E%255Ctop%2520%255Cboldsymbol%257BA%257D%2520-%250A%255Cboldsymbol%257BB%257D%255E%255Ctop%2520%255Cboldsymbol%257BB%257D%2520%255CrVert_2/%255ClVert%2520%255Cboldsymbol%257BA%257D%2520%255CrVert_F%255E2%2524.%250AThe%2520matrix%2520sketching%2520problem%2520becomes%2520particularly%2520interesting%2520in%2520the%2520context%2520of%250Asliding%2520windows%252C%2520where%2520the%2520goal%2520is%2520to%2520approximate%2520the%2520matrix%250A%2524%255Cboldsymbol%257BA%257D_W%2524%252C%2520formed%2520by%2520input%2520vectors%2520over%2520the%2520most%2520recent%2520%2524N%2524%2520time%250Aunits.%2520However%252C%2520despite%2520recent%2520efforts%252C%2520whether%2520achieving%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520on%2520sliding%2520windows%2520is%250Apossible%2520has%2520remained%2520an%2520open%2520question.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520DS-FD%2520algorithm%252C%2520which%2520achieves%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520for%2520matrix%2520sketching%2520over%250Arow-normalized%252C%2520sequence-based%2520sliding%2520windows.%2520We%2520also%2520present%2520matching%2520upper%250Aand%2520lower%2520space%2520bounds%2520for%2520time-based%2520and%2520unnormalized%2520sliding%2520windows%252C%250Ademonstrating%2520the%2520generality%2520and%2520optimality%2520of%2520%255Cdsfd%2520across%2520various%2520sliding%250Awindow%2520models.%2520This%2520conclusively%2520answers%2520the%2520open%2520question%2520regarding%2520the%250Aoptimal%2520space%2520bound%2520for%2520matrix%2520sketching%2520over%2520sliding%2520windows.%2520Furthermore%252C%2520we%250Aconduct%2520extensive%2520experiments%2520with%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%250Avalidating%2520our%2520theoretical%2520claims%2520and%2520thus%2520confirming%2520the%2520correctness%2520and%250Aeffectiveness%2520of%2520our%2520algorithm%252C%2520both%2520theoretically%2520and%2520empirically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows&entry.906535625=Hanyan%20Yin%20and%20Dongxie%20Wen%20and%20Jiajun%20Li%20and%20Zhewei%20Wei%20and%20Xiao%20Zhang%20and%20Zengfeng%20Huang%20and%20Feifei%20Li&entry.1292438233=%20%20Matrix%20sketching%2C%20aimed%20at%20approximating%20a%20matrix%20%24%5Cboldsymbol%7BA%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20d%7D%24%20consisting%20of%20vector%20streams%20of%20length%20%24N%24%20with%20a%0Asmaller%20sketching%20matrix%20%24%5Cboldsymbol%7BB%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cell%5Ctimes%20d%7D%2C%20%5Cell%0A%5Cll%20N%24%2C%20has%20garnered%20increasing%20attention%20in%20fields%20such%20as%20large-scale%20data%0Aanalytics%20and%20machine%20learning.%20A%20well-known%20deterministic%20matrix%20sketching%0Amethod%20is%20the%20Frequent%20Directions%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20and%20provides%20a%20covariance%0Aerror%20guarantee%20of%20%24%5Cvarepsilon%20%3D%20%5ClVert%20%5Cboldsymbol%7BA%7D%5E%5Ctop%20%5Cboldsymbol%7BA%7D%20-%0A%5Cboldsymbol%7BB%7D%5E%5Ctop%20%5Cboldsymbol%7BB%7D%20%5CrVert_2/%5ClVert%20%5Cboldsymbol%7BA%7D%20%5CrVert_F%5E2%24.%0AThe%20matrix%20sketching%20problem%20becomes%20particularly%20interesting%20in%20the%20context%20of%0Asliding%20windows%2C%20where%20the%20goal%20is%20to%20approximate%20the%20matrix%0A%24%5Cboldsymbol%7BA%7D_W%24%2C%20formed%20by%20input%20vectors%20over%20the%20most%20recent%20%24N%24%20time%0Aunits.%20However%2C%20despite%20recent%20efforts%2C%20whether%20achieving%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20on%20sliding%20windows%20is%0Apossible%20has%20remained%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20introduce%20the%20DS-FD%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20for%20matrix%20sketching%20over%0Arow-normalized%2C%20sequence-based%20sliding%20windows.%20We%20also%20present%20matching%20upper%0Aand%20lower%20space%20bounds%20for%20time-based%20and%20unnormalized%20sliding%20windows%2C%0Ademonstrating%20the%20generality%20and%20optimality%20of%20%5Cdsfd%20across%20various%20sliding%0Awindow%20models.%20This%20conclusively%20answers%20the%20open%20question%20regarding%20the%0Aoptimal%20space%20bound%20for%20matrix%20sketching%20over%20sliding%20windows.%20Furthermore%2C%20we%0Aconduct%20extensive%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%2C%0Avalidating%20our%20theoretical%20claims%20and%20thus%20confirming%20the%20correctness%20and%0Aeffectiveness%20of%20our%20algorithm%2C%20both%20theoretically%20and%20empirically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07792v2&entry.124074799=Read"},
{"title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation", "author": "Tianyu Zheng and Shuyue Guo and Xingwei Qu and Jiawei Guo and Xinrun Du and Qi Jia and Chenghua Lin and Wenhao Huang and Jie Fu and Ge Zhang", "abstract": "  In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun\n", "link": "http://arxiv.org/abs/2401.06477v4", "date": "2024-11-05", "relevancy": 1.8602, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation&body=Title%3A%20Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation%0AAuthor%3A%20Tianyu%20Zheng%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Jiawei%20Guo%20and%20Xinrun%20Du%20and%20Qi%20Jia%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Kun%2C%20a%20novel%20approach%20for%20creating%20high-quality%0Ainstruction-tuning%20datasets%20for%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%0Amanual%20annotations.%20Adapting%20a%20self-training%20algorithm%20based%20on%20instruction%0Aback-translation%20and%20answer%20polishment%2C%20Kun%20leverages%20unlabelled%20data%20from%0Adiverse%20sources%20such%20as%20Wudao%2C%20Wanjuan%2C%20and%20SkyPile%20to%20generate%20a%20substantial%0Adataset%20of%20over%20a%20million%20Chinese%20instructional%20data%20points.%20This%20approach%0Asignificantly%20deviates%20from%20traditional%20methods%20by%20using%20a%20self-curation%0Aprocess%20to%20refine%20and%20select%20the%20most%20effective%20instruction-output%20pairs.%20Our%0Aexperiments%20with%20the%206B-parameter%20Yi%20model%20across%20various%20benchmarks%0Ademonstrate%20Kun%27s%20robustness%20and%20scalability.%20Our%20method%27s%20core%20contributions%0Alie%20in%20its%20algorithmic%20advancement%2C%20which%20enhances%20data%20retention%20and%20clarity%2C%0Aand%20its%20innovative%20data%20generation%20approach%20that%20substantially%20reduces%20the%0Areliance%20on%20costly%20and%20time-consuming%20manual%20annotations.%20This%20methodology%0Apresents%20a%20scalable%20and%20efficient%20solution%20for%20improving%20the%0Ainstruction-following%20capabilities%20of%20LLMs%2C%20with%20significant%20implications%20for%0Atheir%20application%20across%20diverse%20fields.%20The%20code%20and%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/Zheng0428/COIG-Kun%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06477v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKun%253A%2520Answer%2520Polishment%2520for%2520Chinese%2520Self-Alignment%2520with%2520Instruction%250A%2520%2520Back-Translation%26entry.906535625%3DTianyu%2520Zheng%2520and%2520Shuyue%2520Guo%2520and%2520Xingwei%2520Qu%2520and%2520Jiawei%2520Guo%2520and%2520Xinrun%2520Du%2520and%2520Qi%2520Jia%2520and%2520Chenghua%2520Lin%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%2520and%2520Ge%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Kun%252C%2520a%2520novel%2520approach%2520for%2520creating%2520high-quality%250Ainstruction-tuning%2520datasets%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520without%2520relying%2520on%250Amanual%2520annotations.%2520Adapting%2520a%2520self-training%2520algorithm%2520based%2520on%2520instruction%250Aback-translation%2520and%2520answer%2520polishment%252C%2520Kun%2520leverages%2520unlabelled%2520data%2520from%250Adiverse%2520sources%2520such%2520as%2520Wudao%252C%2520Wanjuan%252C%2520and%2520SkyPile%2520to%2520generate%2520a%2520substantial%250Adataset%2520of%2520over%2520a%2520million%2520Chinese%2520instructional%2520data%2520points.%2520This%2520approach%250Asignificantly%2520deviates%2520from%2520traditional%2520methods%2520by%2520using%2520a%2520self-curation%250Aprocess%2520to%2520refine%2520and%2520select%2520the%2520most%2520effective%2520instruction-output%2520pairs.%2520Our%250Aexperiments%2520with%2520the%25206B-parameter%2520Yi%2520model%2520across%2520various%2520benchmarks%250Ademonstrate%2520Kun%2527s%2520robustness%2520and%2520scalability.%2520Our%2520method%2527s%2520core%2520contributions%250Alie%2520in%2520its%2520algorithmic%2520advancement%252C%2520which%2520enhances%2520data%2520retention%2520and%2520clarity%252C%250Aand%2520its%2520innovative%2520data%2520generation%2520approach%2520that%2520substantially%2520reduces%2520the%250Areliance%2520on%2520costly%2520and%2520time-consuming%2520manual%2520annotations.%2520This%2520methodology%250Apresents%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520improving%2520the%250Ainstruction-following%2520capabilities%2520of%2520LLMs%252C%2520with%2520significant%2520implications%2520for%250Atheir%2520application%2520across%2520diverse%2520fields.%2520The%2520code%2520and%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Zheng0428/COIG-Kun%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06477v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation&entry.906535625=Tianyu%20Zheng%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Jiawei%20Guo%20and%20Xinrun%20Du%20and%20Qi%20Jia%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Kun%2C%20a%20novel%20approach%20for%20creating%20high-quality%0Ainstruction-tuning%20datasets%20for%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%0Amanual%20annotations.%20Adapting%20a%20self-training%20algorithm%20based%20on%20instruction%0Aback-translation%20and%20answer%20polishment%2C%20Kun%20leverages%20unlabelled%20data%20from%0Adiverse%20sources%20such%20as%20Wudao%2C%20Wanjuan%2C%20and%20SkyPile%20to%20generate%20a%20substantial%0Adataset%20of%20over%20a%20million%20Chinese%20instructional%20data%20points.%20This%20approach%0Asignificantly%20deviates%20from%20traditional%20methods%20by%20using%20a%20self-curation%0Aprocess%20to%20refine%20and%20select%20the%20most%20effective%20instruction-output%20pairs.%20Our%0Aexperiments%20with%20the%206B-parameter%20Yi%20model%20across%20various%20benchmarks%0Ademonstrate%20Kun%27s%20robustness%20and%20scalability.%20Our%20method%27s%20core%20contributions%0Alie%20in%20its%20algorithmic%20advancement%2C%20which%20enhances%20data%20retention%20and%20clarity%2C%0Aand%20its%20innovative%20data%20generation%20approach%20that%20substantially%20reduces%20the%0Areliance%20on%20costly%20and%20time-consuming%20manual%20annotations.%20This%20methodology%0Apresents%20a%20scalable%20and%20efficient%20solution%20for%20improving%20the%0Ainstruction-following%20capabilities%20of%20LLMs%2C%20with%20significant%20implications%20for%0Atheir%20application%20across%20diverse%20fields.%20The%20code%20and%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/Zheng0428/COIG-Kun%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06477v4&entry.124074799=Read"},
{"title": "Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses", "author": "Weiwei Kong and M\u00f3nica Ribero", "abstract": "  Differentially-private stochastic gradient descent (DP-SGD) is a family of\niterative machine learning training algorithms that privatize gradients to\ngenerate a sequence of differentially-private (DP) model parameters. It is also\nthe standard tool used to train DP models in practice, even though most users\nare only interested in protecting the privacy of the final model. Tight DP\naccounting for the last iterate would minimize the amount of noise required\nwhile maintaining the same privacy guarantee and potentially increasing model\nutility. However, last-iterate accounting is challenging, and existing works\nrequire strong assumptions not satisfied by most implementations. These include\nassuming (i) the global sensitivity constant is known - to avoid gradient\nclipping; (ii) the loss function is Lipschitz or convex; and (iii) input\nbatches are sampled randomly.\n  In this work, we forego any unrealistic assumptions and provide privacy\nbounds for the most commonly used variant of DP-SGD, in which data is traversed\ncyclically, gradients are clipped, and only the last model is released. More\nspecifically, we establish new Renyi differential privacy (RDP) upper bounds\nfor the last iterate under realistic assumptions of small stepsize and\nLipschitz smoothness of the loss function. Our general bounds also recover the\nspecial-case convex bounds when the weak-convexity parameter of the objective\nfunction approaches zero and no clipping is performed. The approach itself\nleverages optimal transport techniques for last iterate bounds, which is a\nnontrivial task when the data is traversed cyclically and the loss function is\nnonconvex.\n", "link": "http://arxiv.org/abs/2407.05237v2", "date": "2024-11-05", "relevancy": 1.8589, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4744}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20of%20the%20last%20iterate%20in%20cyclically-sampled%20DP-SGD%20on%20nonconvex%0A%20%20composite%20losses&body=Title%3A%20Privacy%20of%20the%20last%20iterate%20in%20cyclically-sampled%20DP-SGD%20on%20nonconvex%0A%20%20composite%20losses%0AAuthor%3A%20Weiwei%20Kong%20and%20M%C3%B3nica%20Ribero%0AAbstract%3A%20%20%20Differentially-private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20is%20a%20family%20of%0Aiterative%20machine%20learning%20training%20algorithms%20that%20privatize%20gradients%20to%0Agenerate%20a%20sequence%20of%20differentially-private%20%28DP%29%20model%20parameters.%20It%20is%20also%0Athe%20standard%20tool%20used%20to%20train%20DP%20models%20in%20practice%2C%20even%20though%20most%20users%0Aare%20only%20interested%20in%20protecting%20the%20privacy%20of%20the%20final%20model.%20Tight%20DP%0Aaccounting%20for%20the%20last%20iterate%20would%20minimize%20the%20amount%20of%20noise%20required%0Awhile%20maintaining%20the%20same%20privacy%20guarantee%20and%20potentially%20increasing%20model%0Autility.%20However%2C%20last-iterate%20accounting%20is%20challenging%2C%20and%20existing%20works%0Arequire%20strong%20assumptions%20not%20satisfied%20by%20most%20implementations.%20These%20include%0Aassuming%20%28i%29%20the%20global%20sensitivity%20constant%20is%20known%20-%20to%20avoid%20gradient%0Aclipping%3B%20%28ii%29%20the%20loss%20function%20is%20Lipschitz%20or%20convex%3B%20and%20%28iii%29%20input%0Abatches%20are%20sampled%20randomly.%0A%20%20In%20this%20work%2C%20we%20forego%20any%20unrealistic%20assumptions%20and%20provide%20privacy%0Abounds%20for%20the%20most%20commonly%20used%20variant%20of%20DP-SGD%2C%20in%20which%20data%20is%20traversed%0Acyclically%2C%20gradients%20are%20clipped%2C%20and%20only%20the%20last%20model%20is%20released.%20More%0Aspecifically%2C%20we%20establish%20new%20Renyi%20differential%20privacy%20%28RDP%29%20upper%20bounds%0Afor%20the%20last%20iterate%20under%20realistic%20assumptions%20of%20small%20stepsize%20and%0ALipschitz%20smoothness%20of%20the%20loss%20function.%20Our%20general%20bounds%20also%20recover%20the%0Aspecial-case%20convex%20bounds%20when%20the%20weak-convexity%20parameter%20of%20the%20objective%0Afunction%20approaches%20zero%20and%20no%20clipping%20is%20performed.%20The%20approach%20itself%0Aleverages%20optimal%20transport%20techniques%20for%20last%20iterate%20bounds%2C%20which%20is%20a%0Anontrivial%20task%20when%20the%20data%20is%20traversed%20cyclically%20and%20the%20loss%20function%20is%0Anonconvex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520of%2520the%2520last%2520iterate%2520in%2520cyclically-sampled%2520DP-SGD%2520on%2520nonconvex%250A%2520%2520composite%2520losses%26entry.906535625%3DWeiwei%2520Kong%2520and%2520M%25C3%25B3nica%2520Ribero%26entry.1292438233%3D%2520%2520Differentially-private%2520stochastic%2520gradient%2520descent%2520%2528DP-SGD%2529%2520is%2520a%2520family%2520of%250Aiterative%2520machine%2520learning%2520training%2520algorithms%2520that%2520privatize%2520gradients%2520to%250Agenerate%2520a%2520sequence%2520of%2520differentially-private%2520%2528DP%2529%2520model%2520parameters.%2520It%2520is%2520also%250Athe%2520standard%2520tool%2520used%2520to%2520train%2520DP%2520models%2520in%2520practice%252C%2520even%2520though%2520most%2520users%250Aare%2520only%2520interested%2520in%2520protecting%2520the%2520privacy%2520of%2520the%2520final%2520model.%2520Tight%2520DP%250Aaccounting%2520for%2520the%2520last%2520iterate%2520would%2520minimize%2520the%2520amount%2520of%2520noise%2520required%250Awhile%2520maintaining%2520the%2520same%2520privacy%2520guarantee%2520and%2520potentially%2520increasing%2520model%250Autility.%2520However%252C%2520last-iterate%2520accounting%2520is%2520challenging%252C%2520and%2520existing%2520works%250Arequire%2520strong%2520assumptions%2520not%2520satisfied%2520by%2520most%2520implementations.%2520These%2520include%250Aassuming%2520%2528i%2529%2520the%2520global%2520sensitivity%2520constant%2520is%2520known%2520-%2520to%2520avoid%2520gradient%250Aclipping%253B%2520%2528ii%2529%2520the%2520loss%2520function%2520is%2520Lipschitz%2520or%2520convex%253B%2520and%2520%2528iii%2529%2520input%250Abatches%2520are%2520sampled%2520randomly.%250A%2520%2520In%2520this%2520work%252C%2520we%2520forego%2520any%2520unrealistic%2520assumptions%2520and%2520provide%2520privacy%250Abounds%2520for%2520the%2520most%2520commonly%2520used%2520variant%2520of%2520DP-SGD%252C%2520in%2520which%2520data%2520is%2520traversed%250Acyclically%252C%2520gradients%2520are%2520clipped%252C%2520and%2520only%2520the%2520last%2520model%2520is%2520released.%2520More%250Aspecifically%252C%2520we%2520establish%2520new%2520Renyi%2520differential%2520privacy%2520%2528RDP%2529%2520upper%2520bounds%250Afor%2520the%2520last%2520iterate%2520under%2520realistic%2520assumptions%2520of%2520small%2520stepsize%2520and%250ALipschitz%2520smoothness%2520of%2520the%2520loss%2520function.%2520Our%2520general%2520bounds%2520also%2520recover%2520the%250Aspecial-case%2520convex%2520bounds%2520when%2520the%2520weak-convexity%2520parameter%2520of%2520the%2520objective%250Afunction%2520approaches%2520zero%2520and%2520no%2520clipping%2520is%2520performed.%2520The%2520approach%2520itself%250Aleverages%2520optimal%2520transport%2520techniques%2520for%2520last%2520iterate%2520bounds%252C%2520which%2520is%2520a%250Anontrivial%2520task%2520when%2520the%2520data%2520is%2520traversed%2520cyclically%2520and%2520the%2520loss%2520function%2520is%250Anonconvex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20of%20the%20last%20iterate%20in%20cyclically-sampled%20DP-SGD%20on%20nonconvex%0A%20%20composite%20losses&entry.906535625=Weiwei%20Kong%20and%20M%C3%B3nica%20Ribero&entry.1292438233=%20%20Differentially-private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20is%20a%20family%20of%0Aiterative%20machine%20learning%20training%20algorithms%20that%20privatize%20gradients%20to%0Agenerate%20a%20sequence%20of%20differentially-private%20%28DP%29%20model%20parameters.%20It%20is%20also%0Athe%20standard%20tool%20used%20to%20train%20DP%20models%20in%20practice%2C%20even%20though%20most%20users%0Aare%20only%20interested%20in%20protecting%20the%20privacy%20of%20the%20final%20model.%20Tight%20DP%0Aaccounting%20for%20the%20last%20iterate%20would%20minimize%20the%20amount%20of%20noise%20required%0Awhile%20maintaining%20the%20same%20privacy%20guarantee%20and%20potentially%20increasing%20model%0Autility.%20However%2C%20last-iterate%20accounting%20is%20challenging%2C%20and%20existing%20works%0Arequire%20strong%20assumptions%20not%20satisfied%20by%20most%20implementations.%20These%20include%0Aassuming%20%28i%29%20the%20global%20sensitivity%20constant%20is%20known%20-%20to%20avoid%20gradient%0Aclipping%3B%20%28ii%29%20the%20loss%20function%20is%20Lipschitz%20or%20convex%3B%20and%20%28iii%29%20input%0Abatches%20are%20sampled%20randomly.%0A%20%20In%20this%20work%2C%20we%20forego%20any%20unrealistic%20assumptions%20and%20provide%20privacy%0Abounds%20for%20the%20most%20commonly%20used%20variant%20of%20DP-SGD%2C%20in%20which%20data%20is%20traversed%0Acyclically%2C%20gradients%20are%20clipped%2C%20and%20only%20the%20last%20model%20is%20released.%20More%0Aspecifically%2C%20we%20establish%20new%20Renyi%20differential%20privacy%20%28RDP%29%20upper%20bounds%0Afor%20the%20last%20iterate%20under%20realistic%20assumptions%20of%20small%20stepsize%20and%0ALipschitz%20smoothness%20of%20the%20loss%20function.%20Our%20general%20bounds%20also%20recover%20the%0Aspecial-case%20convex%20bounds%20when%20the%20weak-convexity%20parameter%20of%20the%20objective%0Afunction%20approaches%20zero%20and%20no%20clipping%20is%20performed.%20The%20approach%20itself%0Aleverages%20optimal%20transport%20techniques%20for%20last%20iterate%20bounds%2C%20which%20is%20a%0Anontrivial%20task%20when%20the%20data%20is%20traversed%20cyclically%20and%20the%20loss%20function%20is%0Anonconvex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05237v2&entry.124074799=Read"},
{"title": "Speech Separation with Pretrained Frontend to Minimize Domain Mismatch", "author": "Wupeng Wang and Zexu Pan and Xinke Li and Shuai Wang and Haizhou Li", "abstract": "  Speech separation seeks to separate individual speech signals from a speech\nmixture. Typically, most separation models are trained on synthetic data due to\nthe unavailability of target reference in real-world cocktail party scenarios.\nAs a result, there exists a domain gap between real and synthetic data when\ndeploying speech separation models in real-world applications. In this paper,\nwe propose a self-supervised domain-invariant pretrained (DIP) frontend that is\nexposed to mixture data without the need for target reference speech. The DIP\nfrontend utilizes a Siamese network with two innovative pretext tasks, mixture\npredictive coding (MPC) and mixture invariant coding (MIC), to capture shared\ncontextual cues between real and synthetic unlabeled mixtures. Subsequently, we\nfreeze the DIP frontend as a feature extractor when training the downstream\nspeech separation models on synthetic data. By pretraining the DIP frontend\nwith the contextual cues, we expect that the speech separation skills learned\nfrom synthetic data can be effectively transferred to real data. To benefit\nfrom the DIP frontend, we introduce a novel separation pipeline to align the\nfeature resolution of the separation models. We evaluate the speech separation\nquality on standard benchmarks and real-world datasets. The results confirm the\nsuperiority of our DIP frontend over existing speech separation models. This\nstudy underscores the potential of large-scale pretraining to enhance the\nquality and intelligibility of speech separation in real-world applications.\n", "link": "http://arxiv.org/abs/2411.03085v1", "date": "2024-11-05", "relevancy": 1.8556, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4743}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech%20Separation%20with%20Pretrained%20Frontend%20to%20Minimize%20Domain%20Mismatch&body=Title%3A%20Speech%20Separation%20with%20Pretrained%20Frontend%20to%20Minimize%20Domain%20Mismatch%0AAuthor%3A%20Wupeng%20Wang%20and%20Zexu%20Pan%20and%20Xinke%20Li%20and%20Shuai%20Wang%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Speech%20separation%20seeks%20to%20separate%20individual%20speech%20signals%20from%20a%20speech%0Amixture.%20Typically%2C%20most%20separation%20models%20are%20trained%20on%20synthetic%20data%20due%20to%0Athe%20unavailability%20of%20target%20reference%20in%20real-world%20cocktail%20party%20scenarios.%0AAs%20a%20result%2C%20there%20exists%20a%20domain%20gap%20between%20real%20and%20synthetic%20data%20when%0Adeploying%20speech%20separation%20models%20in%20real-world%20applications.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20domain-invariant%20pretrained%20%28DIP%29%20frontend%20that%20is%0Aexposed%20to%20mixture%20data%20without%20the%20need%20for%20target%20reference%20speech.%20The%20DIP%0Afrontend%20utilizes%20a%20Siamese%20network%20with%20two%20innovative%20pretext%20tasks%2C%20mixture%0Apredictive%20coding%20%28MPC%29%20and%20mixture%20invariant%20coding%20%28MIC%29%2C%20to%20capture%20shared%0Acontextual%20cues%20between%20real%20and%20synthetic%20unlabeled%20mixtures.%20Subsequently%2C%20we%0Afreeze%20the%20DIP%20frontend%20as%20a%20feature%20extractor%20when%20training%20the%20downstream%0Aspeech%20separation%20models%20on%20synthetic%20data.%20By%20pretraining%20the%20DIP%20frontend%0Awith%20the%20contextual%20cues%2C%20we%20expect%20that%20the%20speech%20separation%20skills%20learned%0Afrom%20synthetic%20data%20can%20be%20effectively%20transferred%20to%20real%20data.%20To%20benefit%0Afrom%20the%20DIP%20frontend%2C%20we%20introduce%20a%20novel%20separation%20pipeline%20to%20align%20the%0Afeature%20resolution%20of%20the%20separation%20models.%20We%20evaluate%20the%20speech%20separation%0Aquality%20on%20standard%20benchmarks%20and%20real-world%20datasets.%20The%20results%20confirm%20the%0Asuperiority%20of%20our%20DIP%20frontend%20over%20existing%20speech%20separation%20models.%20This%0Astudy%20underscores%20the%20potential%20of%20large-scale%20pretraining%20to%20enhance%20the%0Aquality%20and%20intelligibility%20of%20speech%20separation%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech%2520Separation%2520with%2520Pretrained%2520Frontend%2520to%2520Minimize%2520Domain%2520Mismatch%26entry.906535625%3DWupeng%2520Wang%2520and%2520Zexu%2520Pan%2520and%2520Xinke%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Speech%2520separation%2520seeks%2520to%2520separate%2520individual%2520speech%2520signals%2520from%2520a%2520speech%250Amixture.%2520Typically%252C%2520most%2520separation%2520models%2520are%2520trained%2520on%2520synthetic%2520data%2520due%2520to%250Athe%2520unavailability%2520of%2520target%2520reference%2520in%2520real-world%2520cocktail%2520party%2520scenarios.%250AAs%2520a%2520result%252C%2520there%2520exists%2520a%2520domain%2520gap%2520between%2520real%2520and%2520synthetic%2520data%2520when%250Adeploying%2520speech%2520separation%2520models%2520in%2520real-world%2520applications.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520self-supervised%2520domain-invariant%2520pretrained%2520%2528DIP%2529%2520frontend%2520that%2520is%250Aexposed%2520to%2520mixture%2520data%2520without%2520the%2520need%2520for%2520target%2520reference%2520speech.%2520The%2520DIP%250Afrontend%2520utilizes%2520a%2520Siamese%2520network%2520with%2520two%2520innovative%2520pretext%2520tasks%252C%2520mixture%250Apredictive%2520coding%2520%2528MPC%2529%2520and%2520mixture%2520invariant%2520coding%2520%2528MIC%2529%252C%2520to%2520capture%2520shared%250Acontextual%2520cues%2520between%2520real%2520and%2520synthetic%2520unlabeled%2520mixtures.%2520Subsequently%252C%2520we%250Afreeze%2520the%2520DIP%2520frontend%2520as%2520a%2520feature%2520extractor%2520when%2520training%2520the%2520downstream%250Aspeech%2520separation%2520models%2520on%2520synthetic%2520data.%2520By%2520pretraining%2520the%2520DIP%2520frontend%250Awith%2520the%2520contextual%2520cues%252C%2520we%2520expect%2520that%2520the%2520speech%2520separation%2520skills%2520learned%250Afrom%2520synthetic%2520data%2520can%2520be%2520effectively%2520transferred%2520to%2520real%2520data.%2520To%2520benefit%250Afrom%2520the%2520DIP%2520frontend%252C%2520we%2520introduce%2520a%2520novel%2520separation%2520pipeline%2520to%2520align%2520the%250Afeature%2520resolution%2520of%2520the%2520separation%2520models.%2520We%2520evaluate%2520the%2520speech%2520separation%250Aquality%2520on%2520standard%2520benchmarks%2520and%2520real-world%2520datasets.%2520The%2520results%2520confirm%2520the%250Asuperiority%2520of%2520our%2520DIP%2520frontend%2520over%2520existing%2520speech%2520separation%2520models.%2520This%250Astudy%2520underscores%2520the%2520potential%2520of%2520large-scale%2520pretraining%2520to%2520enhance%2520the%250Aquality%2520and%2520intelligibility%2520of%2520speech%2520separation%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech%20Separation%20with%20Pretrained%20Frontend%20to%20Minimize%20Domain%20Mismatch&entry.906535625=Wupeng%20Wang%20and%20Zexu%20Pan%20and%20Xinke%20Li%20and%20Shuai%20Wang%20and%20Haizhou%20Li&entry.1292438233=%20%20Speech%20separation%20seeks%20to%20separate%20individual%20speech%20signals%20from%20a%20speech%0Amixture.%20Typically%2C%20most%20separation%20models%20are%20trained%20on%20synthetic%20data%20due%20to%0Athe%20unavailability%20of%20target%20reference%20in%20real-world%20cocktail%20party%20scenarios.%0AAs%20a%20result%2C%20there%20exists%20a%20domain%20gap%20between%20real%20and%20synthetic%20data%20when%0Adeploying%20speech%20separation%20models%20in%20real-world%20applications.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20domain-invariant%20pretrained%20%28DIP%29%20frontend%20that%20is%0Aexposed%20to%20mixture%20data%20without%20the%20need%20for%20target%20reference%20speech.%20The%20DIP%0Afrontend%20utilizes%20a%20Siamese%20network%20with%20two%20innovative%20pretext%20tasks%2C%20mixture%0Apredictive%20coding%20%28MPC%29%20and%20mixture%20invariant%20coding%20%28MIC%29%2C%20to%20capture%20shared%0Acontextual%20cues%20between%20real%20and%20synthetic%20unlabeled%20mixtures.%20Subsequently%2C%20we%0Afreeze%20the%20DIP%20frontend%20as%20a%20feature%20extractor%20when%20training%20the%20downstream%0Aspeech%20separation%20models%20on%20synthetic%20data.%20By%20pretraining%20the%20DIP%20frontend%0Awith%20the%20contextual%20cues%2C%20we%20expect%20that%20the%20speech%20separation%20skills%20learned%0Afrom%20synthetic%20data%20can%20be%20effectively%20transferred%20to%20real%20data.%20To%20benefit%0Afrom%20the%20DIP%20frontend%2C%20we%20introduce%20a%20novel%20separation%20pipeline%20to%20align%20the%0Afeature%20resolution%20of%20the%20separation%20models.%20We%20evaluate%20the%20speech%20separation%0Aquality%20on%20standard%20benchmarks%20and%20real-world%20datasets.%20The%20results%20confirm%20the%0Asuperiority%20of%20our%20DIP%20frontend%20over%20existing%20speech%20separation%20models.%20This%0Astudy%20underscores%20the%20potential%20of%20large-scale%20pretraining%20to%20enhance%20the%0Aquality%20and%20intelligibility%20of%20speech%20separation%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03085v1&entry.124074799=Read"},
{"title": "Spontaneous Emergence of Agent Individuality through Social Interactions\n  in LLM-Based Communities", "author": "Ryosuke Takata and Atsushi Masumori and Takashi Ikegami", "abstract": "  We study the emergence of agency from scratch by using Large Language Model\n(LLM)-based agents. In previous studies of LLM-based agents, each agent's\ncharacteristics, including personality and memory, have traditionally been\npredefined. We focused on how individuality, such as behavior, personality, and\nmemory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation,\nexchanging context-based messages in natural language. By analyzing this\nmulti-agent simulation, we report valuable new insights into how social norms,\ncooperation, and personality traits can emerge spontaneously. This paper\ndemonstrates that autonomously interacting LLM-powered agents generate\nhallucinations and hashtags to sustain communication, which, in turn, increases\nthe diversity of words within their interactions. Each agent's emotions shift\nthrough communication, and as they form communities, the personalities of the\nagents emerge and evolve accordingly. This computational modeling approach and\nits findings will provide a new method for analyzing collective artificial\nintelligence.\n", "link": "http://arxiv.org/abs/2411.03252v1", "date": "2024-11-05", "relevancy": 1.8463, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4859}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4692}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spontaneous%20Emergence%20of%20Agent%20Individuality%20through%20Social%20Interactions%0A%20%20in%20LLM-Based%20Communities&body=Title%3A%20Spontaneous%20Emergence%20of%20Agent%20Individuality%20through%20Social%20Interactions%0A%20%20in%20LLM-Based%20Communities%0AAuthor%3A%20Ryosuke%20Takata%20and%20Atsushi%20Masumori%20and%20Takashi%20Ikegami%0AAbstract%3A%20%20%20We%20study%20the%20emergence%20of%20agency%20from%20scratch%20by%20using%20Large%20Language%20Model%0A%28LLM%29-based%20agents.%20In%20previous%20studies%20of%20LLM-based%20agents%2C%20each%20agent%27s%0Acharacteristics%2C%20including%20personality%20and%20memory%2C%20have%20traditionally%20been%0Apredefined.%20We%20focused%20on%20how%20individuality%2C%20such%20as%20behavior%2C%20personality%2C%20and%0Amemory%2C%20can%20be%20differentiated%20from%20an%20undifferentiated%20state.%20The%20present%20LLM%0Aagents%20engage%20in%20cooperative%20communication%20within%20a%20group%20simulation%2C%0Aexchanging%20context-based%20messages%20in%20natural%20language.%20By%20analyzing%20this%0Amulti-agent%20simulation%2C%20we%20report%20valuable%20new%20insights%20into%20how%20social%20norms%2C%0Acooperation%2C%20and%20personality%20traits%20can%20emerge%20spontaneously.%20This%20paper%0Ademonstrates%20that%20autonomously%20interacting%20LLM-powered%20agents%20generate%0Ahallucinations%20and%20hashtags%20to%20sustain%20communication%2C%20which%2C%20in%20turn%2C%20increases%0Athe%20diversity%20of%20words%20within%20their%20interactions.%20Each%20agent%27s%20emotions%20shift%0Athrough%20communication%2C%20and%20as%20they%20form%20communities%2C%20the%20personalities%20of%20the%0Aagents%20emerge%20and%20evolve%20accordingly.%20This%20computational%20modeling%20approach%20and%0Aits%20findings%20will%20provide%20a%20new%20method%20for%20analyzing%20collective%20artificial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpontaneous%2520Emergence%2520of%2520Agent%2520Individuality%2520through%2520Social%2520Interactions%250A%2520%2520in%2520LLM-Based%2520Communities%26entry.906535625%3DRyosuke%2520Takata%2520and%2520Atsushi%2520Masumori%2520and%2520Takashi%2520Ikegami%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520emergence%2520of%2520agency%2520from%2520scratch%2520by%2520using%2520Large%2520Language%2520Model%250A%2528LLM%2529-based%2520agents.%2520In%2520previous%2520studies%2520of%2520LLM-based%2520agents%252C%2520each%2520agent%2527s%250Acharacteristics%252C%2520including%2520personality%2520and%2520memory%252C%2520have%2520traditionally%2520been%250Apredefined.%2520We%2520focused%2520on%2520how%2520individuality%252C%2520such%2520as%2520behavior%252C%2520personality%252C%2520and%250Amemory%252C%2520can%2520be%2520differentiated%2520from%2520an%2520undifferentiated%2520state.%2520The%2520present%2520LLM%250Aagents%2520engage%2520in%2520cooperative%2520communication%2520within%2520a%2520group%2520simulation%252C%250Aexchanging%2520context-based%2520messages%2520in%2520natural%2520language.%2520By%2520analyzing%2520this%250Amulti-agent%2520simulation%252C%2520we%2520report%2520valuable%2520new%2520insights%2520into%2520how%2520social%2520norms%252C%250Acooperation%252C%2520and%2520personality%2520traits%2520can%2520emerge%2520spontaneously.%2520This%2520paper%250Ademonstrates%2520that%2520autonomously%2520interacting%2520LLM-powered%2520agents%2520generate%250Ahallucinations%2520and%2520hashtags%2520to%2520sustain%2520communication%252C%2520which%252C%2520in%2520turn%252C%2520increases%250Athe%2520diversity%2520of%2520words%2520within%2520their%2520interactions.%2520Each%2520agent%2527s%2520emotions%2520shift%250Athrough%2520communication%252C%2520and%2520as%2520they%2520form%2520communities%252C%2520the%2520personalities%2520of%2520the%250Aagents%2520emerge%2520and%2520evolve%2520accordingly.%2520This%2520computational%2520modeling%2520approach%2520and%250Aits%2520findings%2520will%2520provide%2520a%2520new%2520method%2520for%2520analyzing%2520collective%2520artificial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spontaneous%20Emergence%20of%20Agent%20Individuality%20through%20Social%20Interactions%0A%20%20in%20LLM-Based%20Communities&entry.906535625=Ryosuke%20Takata%20and%20Atsushi%20Masumori%20and%20Takashi%20Ikegami&entry.1292438233=%20%20We%20study%20the%20emergence%20of%20agency%20from%20scratch%20by%20using%20Large%20Language%20Model%0A%28LLM%29-based%20agents.%20In%20previous%20studies%20of%20LLM-based%20agents%2C%20each%20agent%27s%0Acharacteristics%2C%20including%20personality%20and%20memory%2C%20have%20traditionally%20been%0Apredefined.%20We%20focused%20on%20how%20individuality%2C%20such%20as%20behavior%2C%20personality%2C%20and%0Amemory%2C%20can%20be%20differentiated%20from%20an%20undifferentiated%20state.%20The%20present%20LLM%0Aagents%20engage%20in%20cooperative%20communication%20within%20a%20group%20simulation%2C%0Aexchanging%20context-based%20messages%20in%20natural%20language.%20By%20analyzing%20this%0Amulti-agent%20simulation%2C%20we%20report%20valuable%20new%20insights%20into%20how%20social%20norms%2C%0Acooperation%2C%20and%20personality%20traits%20can%20emerge%20spontaneously.%20This%20paper%0Ademonstrates%20that%20autonomously%20interacting%20LLM-powered%20agents%20generate%0Ahallucinations%20and%20hashtags%20to%20sustain%20communication%2C%20which%2C%20in%20turn%2C%20increases%0Athe%20diversity%20of%20words%20within%20their%20interactions.%20Each%20agent%27s%20emotions%20shift%0Athrough%20communication%2C%20and%20as%20they%20form%20communities%2C%20the%20personalities%20of%20the%0Aagents%20emerge%20and%20evolve%20accordingly.%20This%20computational%20modeling%20approach%20and%0Aits%20findings%20will%20provide%20a%20new%20method%20for%20analyzing%20collective%20artificial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03252v1&entry.124074799=Read"},
{"title": "Cognitive Planning for Object Goal Navigation using Generative AI Models", "author": "Arjun P S and Andrew Melnik and Gora Chand Nandi", "abstract": "  Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings.\n", "link": "http://arxiv.org/abs/2404.00318v2", "date": "2024-11-05", "relevancy": 1.8397, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6711}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Planning%20for%20Object%20Goal%20Navigation%20using%20Generative%20AI%20Models&body=Title%3A%20Cognitive%20Planning%20for%20Object%20Goal%20Navigation%20using%20Generative%20AI%20Models%0AAuthor%3A%20Arjun%20P%20S%20and%20Andrew%20Melnik%20and%20Gora%20Chand%20Nandi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Generative%20AI%2C%20particularly%20in%20Large%20Language%20Models%0A%28LLMs%29%20and%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20offer%20new%20possibilities%20for%0Aintegrating%20cognitive%20planning%20into%20robotic%20systems.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20framework%20for%20solving%20the%20object%20goal%20navigation%20problem%20that%20generates%0Aefficient%20exploration%20strategies.%20Our%20approach%20enables%20a%20robot%20to%20navigate%0Aunfamiliar%20environments%20by%20leveraging%20LLMs%20and%20LVLMs%20to%20understand%20the%20semantic%0Astructure%20of%20the%20scene.%20To%20address%20the%20challenge%20of%20representing%20complex%0Aenvironments%20without%20overwhelming%20the%20system%2C%20we%20propose%20a%203D%20modular%20scene%0Arepresentation%2C%20enriched%20with%20semantic%20descriptions.%20This%20representation%20is%0Adynamically%20pruned%20using%20an%20LLM-based%20mechanism%2C%20which%20filters%20irrelevant%0Ainformation%20and%20focuses%20on%20task-specific%20data.%20By%20combining%20these%20elements%2C%20our%0Asystem%20generates%20high-level%20sub-goals%20that%20guide%20the%20exploration%20of%20the%20robot%0Atoward%20the%20target%20object.%20We%20validate%20our%20approach%20in%20simulated%20environments%2C%0Ademonstrating%20its%20ability%20to%20enhance%20object%20search%20efficiency%20while%20maintaining%0Ascalability%20in%20complex%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Planning%2520for%2520Object%2520Goal%2520Navigation%2520using%2520Generative%2520AI%2520Models%26entry.906535625%3DArjun%2520P%2520S%2520and%2520Andrew%2520Melnik%2520and%2520Gora%2520Chand%2520Nandi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Generative%2520AI%252C%2520particularly%2520in%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520offer%2520new%2520possibilities%2520for%250Aintegrating%2520cognitive%2520planning%2520into%2520robotic%2520systems.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520framework%2520for%2520solving%2520the%2520object%2520goal%2520navigation%2520problem%2520that%2520generates%250Aefficient%2520exploration%2520strategies.%2520Our%2520approach%2520enables%2520a%2520robot%2520to%2520navigate%250Aunfamiliar%2520environments%2520by%2520leveraging%2520LLMs%2520and%2520LVLMs%2520to%2520understand%2520the%2520semantic%250Astructure%2520of%2520the%2520scene.%2520To%2520address%2520the%2520challenge%2520of%2520representing%2520complex%250Aenvironments%2520without%2520overwhelming%2520the%2520system%252C%2520we%2520propose%2520a%25203D%2520modular%2520scene%250Arepresentation%252C%2520enriched%2520with%2520semantic%2520descriptions.%2520This%2520representation%2520is%250Adynamically%2520pruned%2520using%2520an%2520LLM-based%2520mechanism%252C%2520which%2520filters%2520irrelevant%250Ainformation%2520and%2520focuses%2520on%2520task-specific%2520data.%2520By%2520combining%2520these%2520elements%252C%2520our%250Asystem%2520generates%2520high-level%2520sub-goals%2520that%2520guide%2520the%2520exploration%2520of%2520the%2520robot%250Atoward%2520the%2520target%2520object.%2520We%2520validate%2520our%2520approach%2520in%2520simulated%2520environments%252C%250Ademonstrating%2520its%2520ability%2520to%2520enhance%2520object%2520search%2520efficiency%2520while%2520maintaining%250Ascalability%2520in%2520complex%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Planning%20for%20Object%20Goal%20Navigation%20using%20Generative%20AI%20Models&entry.906535625=Arjun%20P%20S%20and%20Andrew%20Melnik%20and%20Gora%20Chand%20Nandi&entry.1292438233=%20%20Recent%20advancements%20in%20Generative%20AI%2C%20particularly%20in%20Large%20Language%20Models%0A%28LLMs%29%20and%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20offer%20new%20possibilities%20for%0Aintegrating%20cognitive%20planning%20into%20robotic%20systems.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20framework%20for%20solving%20the%20object%20goal%20navigation%20problem%20that%20generates%0Aefficient%20exploration%20strategies.%20Our%20approach%20enables%20a%20robot%20to%20navigate%0Aunfamiliar%20environments%20by%20leveraging%20LLMs%20and%20LVLMs%20to%20understand%20the%20semantic%0Astructure%20of%20the%20scene.%20To%20address%20the%20challenge%20of%20representing%20complex%0Aenvironments%20without%20overwhelming%20the%20system%2C%20we%20propose%20a%203D%20modular%20scene%0Arepresentation%2C%20enriched%20with%20semantic%20descriptions.%20This%20representation%20is%0Adynamically%20pruned%20using%20an%20LLM-based%20mechanism%2C%20which%20filters%20irrelevant%0Ainformation%20and%20focuses%20on%20task-specific%20data.%20By%20combining%20these%20elements%2C%20our%0Asystem%20generates%20high-level%20sub-goals%20that%20guide%20the%20exploration%20of%20the%20robot%0Atoward%20the%20target%20object.%20We%20validate%20our%20approach%20in%20simulated%20environments%2C%0Ademonstrating%20its%20ability%20to%20enhance%20object%20search%20efficiency%20while%20maintaining%0Ascalability%20in%20complex%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00318v2&entry.124074799=Read"},
{"title": "Blending Ensemble for Classification with Genetic-algorithm generated\n  Alpha factors and Sentiments (GAS)", "author": "Quechen Yang", "abstract": "  With the increasing maturity and expansion of the cryptocurrency market,\nunderstanding and predicting its price fluctuations has become an important\nissue in the field of financial engineering. This article introduces an\ninnovative Genetic Algorithm-generated Alpha Sentiment (GAS) blending ensemble\nmodel specifically designed to predict Bitcoin market trends. The model\nintegrates advanced ensemble learning methods, feature selection algorithms,\nand in-depth sentiment analysis to effectively capture the complexity and\nvariability of daily Bitcoin trading data. The GAS framework combines 34 Alpha\nfactors with 8 news economic sentiment factors to provide deep insights into\nBitcoin price fluctuations by accurately analyzing market sentiment and\ntechnical indicators. The core of this study is using a stacked model\n(including LightGBM, XGBoost, and Random Forest Classifier) for trend\nprediction which demonstrates excellent performance in traditional buy-and-hold\nstrategies. In addition, this article also explores the effectiveness of using\ngenetic algorithms to automate alpha factor construction as well as enhancing\npredictive models through sentiment analysis. Experimental results show that\nthe GAS model performs competitively in daily Bitcoin trend prediction\nespecially when analyzing highly volatile financial assets with rich data.\n", "link": "http://arxiv.org/abs/2411.03035v1", "date": "2024-11-05", "relevancy": 1.8357, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4627}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4589}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blending%20Ensemble%20for%20Classification%20with%20Genetic-algorithm%20generated%0A%20%20Alpha%20factors%20and%20Sentiments%20%28GAS%29&body=Title%3A%20Blending%20Ensemble%20for%20Classification%20with%20Genetic-algorithm%20generated%0A%20%20Alpha%20factors%20and%20Sentiments%20%28GAS%29%0AAuthor%3A%20Quechen%20Yang%0AAbstract%3A%20%20%20With%20the%20increasing%20maturity%20and%20expansion%20of%20the%20cryptocurrency%20market%2C%0Aunderstanding%20and%20predicting%20its%20price%20fluctuations%20has%20become%20an%20important%0Aissue%20in%20the%20field%20of%20financial%20engineering.%20This%20article%20introduces%20an%0Ainnovative%20Genetic%20Algorithm-generated%20Alpha%20Sentiment%20%28GAS%29%20blending%20ensemble%0Amodel%20specifically%20designed%20to%20predict%20Bitcoin%20market%20trends.%20The%20model%0Aintegrates%20advanced%20ensemble%20learning%20methods%2C%20feature%20selection%20algorithms%2C%0Aand%20in-depth%20sentiment%20analysis%20to%20effectively%20capture%20the%20complexity%20and%0Avariability%20of%20daily%20Bitcoin%20trading%20data.%20The%20GAS%20framework%20combines%2034%20Alpha%0Afactors%20with%208%20news%20economic%20sentiment%20factors%20to%20provide%20deep%20insights%20into%0ABitcoin%20price%20fluctuations%20by%20accurately%20analyzing%20market%20sentiment%20and%0Atechnical%20indicators.%20The%20core%20of%20this%20study%20is%20using%20a%20stacked%20model%0A%28including%20LightGBM%2C%20XGBoost%2C%20and%20Random%20Forest%20Classifier%29%20for%20trend%0Aprediction%20which%20demonstrates%20excellent%20performance%20in%20traditional%20buy-and-hold%0Astrategies.%20In%20addition%2C%20this%20article%20also%20explores%20the%20effectiveness%20of%20using%0Agenetic%20algorithms%20to%20automate%20alpha%20factor%20construction%20as%20well%20as%20enhancing%0Apredictive%20models%20through%20sentiment%20analysis.%20Experimental%20results%20show%20that%0Athe%20GAS%20model%20performs%20competitively%20in%20daily%20Bitcoin%20trend%20prediction%0Aespecially%20when%20analyzing%20highly%20volatile%20financial%20assets%20with%20rich%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlending%2520Ensemble%2520for%2520Classification%2520with%2520Genetic-algorithm%2520generated%250A%2520%2520Alpha%2520factors%2520and%2520Sentiments%2520%2528GAS%2529%26entry.906535625%3DQuechen%2520Yang%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520maturity%2520and%2520expansion%2520of%2520the%2520cryptocurrency%2520market%252C%250Aunderstanding%2520and%2520predicting%2520its%2520price%2520fluctuations%2520has%2520become%2520an%2520important%250Aissue%2520in%2520the%2520field%2520of%2520financial%2520engineering.%2520This%2520article%2520introduces%2520an%250Ainnovative%2520Genetic%2520Algorithm-generated%2520Alpha%2520Sentiment%2520%2528GAS%2529%2520blending%2520ensemble%250Amodel%2520specifically%2520designed%2520to%2520predict%2520Bitcoin%2520market%2520trends.%2520The%2520model%250Aintegrates%2520advanced%2520ensemble%2520learning%2520methods%252C%2520feature%2520selection%2520algorithms%252C%250Aand%2520in-depth%2520sentiment%2520analysis%2520to%2520effectively%2520capture%2520the%2520complexity%2520and%250Avariability%2520of%2520daily%2520Bitcoin%2520trading%2520data.%2520The%2520GAS%2520framework%2520combines%252034%2520Alpha%250Afactors%2520with%25208%2520news%2520economic%2520sentiment%2520factors%2520to%2520provide%2520deep%2520insights%2520into%250ABitcoin%2520price%2520fluctuations%2520by%2520accurately%2520analyzing%2520market%2520sentiment%2520and%250Atechnical%2520indicators.%2520The%2520core%2520of%2520this%2520study%2520is%2520using%2520a%2520stacked%2520model%250A%2528including%2520LightGBM%252C%2520XGBoost%252C%2520and%2520Random%2520Forest%2520Classifier%2529%2520for%2520trend%250Aprediction%2520which%2520demonstrates%2520excellent%2520performance%2520in%2520traditional%2520buy-and-hold%250Astrategies.%2520In%2520addition%252C%2520this%2520article%2520also%2520explores%2520the%2520effectiveness%2520of%2520using%250Agenetic%2520algorithms%2520to%2520automate%2520alpha%2520factor%2520construction%2520as%2520well%2520as%2520enhancing%250Apredictive%2520models%2520through%2520sentiment%2520analysis.%2520Experimental%2520results%2520show%2520that%250Athe%2520GAS%2520model%2520performs%2520competitively%2520in%2520daily%2520Bitcoin%2520trend%2520prediction%250Aespecially%2520when%2520analyzing%2520highly%2520volatile%2520financial%2520assets%2520with%2520rich%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blending%20Ensemble%20for%20Classification%20with%20Genetic-algorithm%20generated%0A%20%20Alpha%20factors%20and%20Sentiments%20%28GAS%29&entry.906535625=Quechen%20Yang&entry.1292438233=%20%20With%20the%20increasing%20maturity%20and%20expansion%20of%20the%20cryptocurrency%20market%2C%0Aunderstanding%20and%20predicting%20its%20price%20fluctuations%20has%20become%20an%20important%0Aissue%20in%20the%20field%20of%20financial%20engineering.%20This%20article%20introduces%20an%0Ainnovative%20Genetic%20Algorithm-generated%20Alpha%20Sentiment%20%28GAS%29%20blending%20ensemble%0Amodel%20specifically%20designed%20to%20predict%20Bitcoin%20market%20trends.%20The%20model%0Aintegrates%20advanced%20ensemble%20learning%20methods%2C%20feature%20selection%20algorithms%2C%0Aand%20in-depth%20sentiment%20analysis%20to%20effectively%20capture%20the%20complexity%20and%0Avariability%20of%20daily%20Bitcoin%20trading%20data.%20The%20GAS%20framework%20combines%2034%20Alpha%0Afactors%20with%208%20news%20economic%20sentiment%20factors%20to%20provide%20deep%20insights%20into%0ABitcoin%20price%20fluctuations%20by%20accurately%20analyzing%20market%20sentiment%20and%0Atechnical%20indicators.%20The%20core%20of%20this%20study%20is%20using%20a%20stacked%20model%0A%28including%20LightGBM%2C%20XGBoost%2C%20and%20Random%20Forest%20Classifier%29%20for%20trend%0Aprediction%20which%20demonstrates%20excellent%20performance%20in%20traditional%20buy-and-hold%0Astrategies.%20In%20addition%2C%20this%20article%20also%20explores%20the%20effectiveness%20of%20using%0Agenetic%20algorithms%20to%20automate%20alpha%20factor%20construction%20as%20well%20as%20enhancing%0Apredictive%20models%20through%20sentiment%20analysis.%20Experimental%20results%20show%20that%0Athe%20GAS%20model%20performs%20competitively%20in%20daily%20Bitcoin%20trend%20prediction%0Aespecially%20when%20analyzing%20highly%20volatile%20financial%20assets%20with%20rich%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03035v1&entry.124074799=Read"},
{"title": "Adaptive Genetic Selection based Pinning Control with Asymmetric\n  Coupling for Multi-Network Heterogeneous Vehicular Systems", "author": "Weian Guo and Ruizhi Sha and Li Li and Lun Zhang and Dongyang Li", "abstract": "  To alleviate computational load on RSUs and cloud platforms, reduce\ncommunication bandwidth requirements, and provide a more stable vehicular\nnetwork service, this paper proposes an optimized pinning control approach for\nheterogeneous multi-network vehicular ad-hoc networks (VANETs). In such\nnetworks, vehicles participate in multiple task-specific networks with\nasymmetric coupling and dynamic topologies. We first establish a rigorous\ntheoretical foundation by proving the stability of pinning control strategies\nunder both single and multi-network conditions, deriving sufficient stability\nconditions using Lyapunov theory and linear matrix inequalities (LMIs).\nBuilding on this theoretical groundwork, we propose an adaptive genetic\nalgorithm tailored to select optimal pinning nodes, effectively balancing LMI\nconstraints while prioritizing overlapping nodes to enhance control efficiency.\nExtensive simulations across various network scales demonstrate that our\napproach achieves rapid consensus with a reduced number of control nodes,\nparticularly when leveraging network overlaps. This work provides a\ncomprehensive solution for efficient control node selection in complex\nvehicular networks, offering practical implications for deploying large-scale\nintelligent transportation systems.\n", "link": "http://arxiv.org/abs/2411.03027v1", "date": "2024-11-05", "relevancy": 1.8342, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4578}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Genetic%20Selection%20based%20Pinning%20Control%20with%20Asymmetric%0A%20%20Coupling%20for%20Multi-Network%20Heterogeneous%20Vehicular%20Systems&body=Title%3A%20Adaptive%20Genetic%20Selection%20based%20Pinning%20Control%20with%20Asymmetric%0A%20%20Coupling%20for%20Multi-Network%20Heterogeneous%20Vehicular%20Systems%0AAuthor%3A%20Weian%20Guo%20and%20Ruizhi%20Sha%20and%20Li%20Li%20and%20Lun%20Zhang%20and%20Dongyang%20Li%0AAbstract%3A%20%20%20To%20alleviate%20computational%20load%20on%20RSUs%20and%20cloud%20platforms%2C%20reduce%0Acommunication%20bandwidth%20requirements%2C%20and%20provide%20a%20more%20stable%20vehicular%0Anetwork%20service%2C%20this%20paper%20proposes%20an%20optimized%20pinning%20control%20approach%20for%0Aheterogeneous%20multi-network%20vehicular%20ad-hoc%20networks%20%28VANETs%29.%20In%20such%0Anetworks%2C%20vehicles%20participate%20in%20multiple%20task-specific%20networks%20with%0Aasymmetric%20coupling%20and%20dynamic%20topologies.%20We%20first%20establish%20a%20rigorous%0Atheoretical%20foundation%20by%20proving%20the%20stability%20of%20pinning%20control%20strategies%0Aunder%20both%20single%20and%20multi-network%20conditions%2C%20deriving%20sufficient%20stability%0Aconditions%20using%20Lyapunov%20theory%20and%20linear%20matrix%20inequalities%20%28LMIs%29.%0ABuilding%20on%20this%20theoretical%20groundwork%2C%20we%20propose%20an%20adaptive%20genetic%0Aalgorithm%20tailored%20to%20select%20optimal%20pinning%20nodes%2C%20effectively%20balancing%20LMI%0Aconstraints%20while%20prioritizing%20overlapping%20nodes%20to%20enhance%20control%20efficiency.%0AExtensive%20simulations%20across%20various%20network%20scales%20demonstrate%20that%20our%0Aapproach%20achieves%20rapid%20consensus%20with%20a%20reduced%20number%20of%20control%20nodes%2C%0Aparticularly%20when%20leveraging%20network%20overlaps.%20This%20work%20provides%20a%0Acomprehensive%20solution%20for%20efficient%20control%20node%20selection%20in%20complex%0Avehicular%20networks%2C%20offering%20practical%20implications%20for%20deploying%20large-scale%0Aintelligent%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Genetic%2520Selection%2520based%2520Pinning%2520Control%2520with%2520Asymmetric%250A%2520%2520Coupling%2520for%2520Multi-Network%2520Heterogeneous%2520Vehicular%2520Systems%26entry.906535625%3DWeian%2520Guo%2520and%2520Ruizhi%2520Sha%2520and%2520Li%2520Li%2520and%2520Lun%2520Zhang%2520and%2520Dongyang%2520Li%26entry.1292438233%3D%2520%2520To%2520alleviate%2520computational%2520load%2520on%2520RSUs%2520and%2520cloud%2520platforms%252C%2520reduce%250Acommunication%2520bandwidth%2520requirements%252C%2520and%2520provide%2520a%2520more%2520stable%2520vehicular%250Anetwork%2520service%252C%2520this%2520paper%2520proposes%2520an%2520optimized%2520pinning%2520control%2520approach%2520for%250Aheterogeneous%2520multi-network%2520vehicular%2520ad-hoc%2520networks%2520%2528VANETs%2529.%2520In%2520such%250Anetworks%252C%2520vehicles%2520participate%2520in%2520multiple%2520task-specific%2520networks%2520with%250Aasymmetric%2520coupling%2520and%2520dynamic%2520topologies.%2520We%2520first%2520establish%2520a%2520rigorous%250Atheoretical%2520foundation%2520by%2520proving%2520the%2520stability%2520of%2520pinning%2520control%2520strategies%250Aunder%2520both%2520single%2520and%2520multi-network%2520conditions%252C%2520deriving%2520sufficient%2520stability%250Aconditions%2520using%2520Lyapunov%2520theory%2520and%2520linear%2520matrix%2520inequalities%2520%2528LMIs%2529.%250ABuilding%2520on%2520this%2520theoretical%2520groundwork%252C%2520we%2520propose%2520an%2520adaptive%2520genetic%250Aalgorithm%2520tailored%2520to%2520select%2520optimal%2520pinning%2520nodes%252C%2520effectively%2520balancing%2520LMI%250Aconstraints%2520while%2520prioritizing%2520overlapping%2520nodes%2520to%2520enhance%2520control%2520efficiency.%250AExtensive%2520simulations%2520across%2520various%2520network%2520scales%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520rapid%2520consensus%2520with%2520a%2520reduced%2520number%2520of%2520control%2520nodes%252C%250Aparticularly%2520when%2520leveraging%2520network%2520overlaps.%2520This%2520work%2520provides%2520a%250Acomprehensive%2520solution%2520for%2520efficient%2520control%2520node%2520selection%2520in%2520complex%250Avehicular%2520networks%252C%2520offering%2520practical%2520implications%2520for%2520deploying%2520large-scale%250Aintelligent%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Genetic%20Selection%20based%20Pinning%20Control%20with%20Asymmetric%0A%20%20Coupling%20for%20Multi-Network%20Heterogeneous%20Vehicular%20Systems&entry.906535625=Weian%20Guo%20and%20Ruizhi%20Sha%20and%20Li%20Li%20and%20Lun%20Zhang%20and%20Dongyang%20Li&entry.1292438233=%20%20To%20alleviate%20computational%20load%20on%20RSUs%20and%20cloud%20platforms%2C%20reduce%0Acommunication%20bandwidth%20requirements%2C%20and%20provide%20a%20more%20stable%20vehicular%0Anetwork%20service%2C%20this%20paper%20proposes%20an%20optimized%20pinning%20control%20approach%20for%0Aheterogeneous%20multi-network%20vehicular%20ad-hoc%20networks%20%28VANETs%29.%20In%20such%0Anetworks%2C%20vehicles%20participate%20in%20multiple%20task-specific%20networks%20with%0Aasymmetric%20coupling%20and%20dynamic%20topologies.%20We%20first%20establish%20a%20rigorous%0Atheoretical%20foundation%20by%20proving%20the%20stability%20of%20pinning%20control%20strategies%0Aunder%20both%20single%20and%20multi-network%20conditions%2C%20deriving%20sufficient%20stability%0Aconditions%20using%20Lyapunov%20theory%20and%20linear%20matrix%20inequalities%20%28LMIs%29.%0ABuilding%20on%20this%20theoretical%20groundwork%2C%20we%20propose%20an%20adaptive%20genetic%0Aalgorithm%20tailored%20to%20select%20optimal%20pinning%20nodes%2C%20effectively%20balancing%20LMI%0Aconstraints%20while%20prioritizing%20overlapping%20nodes%20to%20enhance%20control%20efficiency.%0AExtensive%20simulations%20across%20various%20network%20scales%20demonstrate%20that%20our%0Aapproach%20achieves%20rapid%20consensus%20with%20a%20reduced%20number%20of%20control%20nodes%2C%0Aparticularly%20when%20leveraging%20network%20overlaps.%20This%20work%20provides%20a%0Acomprehensive%20solution%20for%20efficient%20control%20node%20selection%20in%20complex%0Avehicular%20networks%2C%20offering%20practical%20implications%20for%20deploying%20large-scale%0Aintelligent%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03027v1&entry.124074799=Read"},
{"title": "Insights into Lunar Mineralogy: An Unsupervised Approach for Clustering\n  of the Moon Mineral Mapper (M3) spectral data", "author": "Freja Thoresen and Igor Drozdovskiy and Aidan Cowley and Magdelena Laban and Sebastien Besse and Sylvain Blunier", "abstract": "  This paper presents a novel method for mapping spectral features of the Moon\nusing machine learning-based clustering of hyperspectral data from the Moon\nMineral Mapper (M3) imaging spectrometer. The method uses a convolutional\nvariational autoencoder to reduce the dimensionality of the spectral data and\nextract features of the spectra. Then, a k-means algorithm is applied to\ncluster the latent variables into five distinct groups, corresponding to\ndominant spectral features, which are related to the mineral composition of the\nMoon's surface. The resulting global spectral cluster map shows the\ndistribution of the five clusters on the Moon, which consist of a mixture of,\namong others, plagioclase, pyroxene, olivine, and Fe-bearing minerals across\nthe Moon's surface. The clusters are compared to the mineral maps from the\nKaguya mission, which showed that the locations of the clusters overlap with\nthe locations of high wt% of minerals such as plagioclase, clinopyroxene, and\nolivine. The paper demonstrates the usefulness of unbiased unsupervised\nlearning for lunar mineral exploration and provides a comprehensive analysis of\nlunar mineralogy.\n", "link": "http://arxiv.org/abs/2411.03186v1", "date": "2024-11-05", "relevancy": 1.8264, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20into%20Lunar%20Mineralogy%3A%20An%20Unsupervised%20Approach%20for%20Clustering%0A%20%20of%20the%20Moon%20Mineral%20Mapper%20%28M3%29%20spectral%20data&body=Title%3A%20Insights%20into%20Lunar%20Mineralogy%3A%20An%20Unsupervised%20Approach%20for%20Clustering%0A%20%20of%20the%20Moon%20Mineral%20Mapper%20%28M3%29%20spectral%20data%0AAuthor%3A%20Freja%20Thoresen%20and%20Igor%20Drozdovskiy%20and%20Aidan%20Cowley%20and%20Magdelena%20Laban%20and%20Sebastien%20Besse%20and%20Sylvain%20Blunier%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20method%20for%20mapping%20spectral%20features%20of%20the%20Moon%0Ausing%20machine%20learning-based%20clustering%20of%20hyperspectral%20data%20from%20the%20Moon%0AMineral%20Mapper%20%28M3%29%20imaging%20spectrometer.%20The%20method%20uses%20a%20convolutional%0Avariational%20autoencoder%20to%20reduce%20the%20dimensionality%20of%20the%20spectral%20data%20and%0Aextract%20features%20of%20the%20spectra.%20Then%2C%20a%20k-means%20algorithm%20is%20applied%20to%0Acluster%20the%20latent%20variables%20into%20five%20distinct%20groups%2C%20corresponding%20to%0Adominant%20spectral%20features%2C%20which%20are%20related%20to%20the%20mineral%20composition%20of%20the%0AMoon%27s%20surface.%20The%20resulting%20global%20spectral%20cluster%20map%20shows%20the%0Adistribution%20of%20the%20five%20clusters%20on%20the%20Moon%2C%20which%20consist%20of%20a%20mixture%20of%2C%0Aamong%20others%2C%20plagioclase%2C%20pyroxene%2C%20olivine%2C%20and%20Fe-bearing%20minerals%20across%0Athe%20Moon%27s%20surface.%20The%20clusters%20are%20compared%20to%20the%20mineral%20maps%20from%20the%0AKaguya%20mission%2C%20which%20showed%20that%20the%20locations%20of%20the%20clusters%20overlap%20with%0Athe%20locations%20of%20high%20wt%25%20of%20minerals%20such%20as%20plagioclase%2C%20clinopyroxene%2C%20and%0Aolivine.%20The%20paper%20demonstrates%20the%20usefulness%20of%20unbiased%20unsupervised%0Alearning%20for%20lunar%20mineral%20exploration%20and%20provides%20a%20comprehensive%20analysis%20of%0Alunar%20mineralogy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520into%2520Lunar%2520Mineralogy%253A%2520An%2520Unsupervised%2520Approach%2520for%2520Clustering%250A%2520%2520of%2520the%2520Moon%2520Mineral%2520Mapper%2520%2528M3%2529%2520spectral%2520data%26entry.906535625%3DFreja%2520Thoresen%2520and%2520Igor%2520Drozdovskiy%2520and%2520Aidan%2520Cowley%2520and%2520Magdelena%2520Laban%2520and%2520Sebastien%2520Besse%2520and%2520Sylvain%2520Blunier%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520for%2520mapping%2520spectral%2520features%2520of%2520the%2520Moon%250Ausing%2520machine%2520learning-based%2520clustering%2520of%2520hyperspectral%2520data%2520from%2520the%2520Moon%250AMineral%2520Mapper%2520%2528M3%2529%2520imaging%2520spectrometer.%2520The%2520method%2520uses%2520a%2520convolutional%250Avariational%2520autoencoder%2520to%2520reduce%2520the%2520dimensionality%2520of%2520the%2520spectral%2520data%2520and%250Aextract%2520features%2520of%2520the%2520spectra.%2520Then%252C%2520a%2520k-means%2520algorithm%2520is%2520applied%2520to%250Acluster%2520the%2520latent%2520variables%2520into%2520five%2520distinct%2520groups%252C%2520corresponding%2520to%250Adominant%2520spectral%2520features%252C%2520which%2520are%2520related%2520to%2520the%2520mineral%2520composition%2520of%2520the%250AMoon%2527s%2520surface.%2520The%2520resulting%2520global%2520spectral%2520cluster%2520map%2520shows%2520the%250Adistribution%2520of%2520the%2520five%2520clusters%2520on%2520the%2520Moon%252C%2520which%2520consist%2520of%2520a%2520mixture%2520of%252C%250Aamong%2520others%252C%2520plagioclase%252C%2520pyroxene%252C%2520olivine%252C%2520and%2520Fe-bearing%2520minerals%2520across%250Athe%2520Moon%2527s%2520surface.%2520The%2520clusters%2520are%2520compared%2520to%2520the%2520mineral%2520maps%2520from%2520the%250AKaguya%2520mission%252C%2520which%2520showed%2520that%2520the%2520locations%2520of%2520the%2520clusters%2520overlap%2520with%250Athe%2520locations%2520of%2520high%2520wt%2525%2520of%2520minerals%2520such%2520as%2520plagioclase%252C%2520clinopyroxene%252C%2520and%250Aolivine.%2520The%2520paper%2520demonstrates%2520the%2520usefulness%2520of%2520unbiased%2520unsupervised%250Alearning%2520for%2520lunar%2520mineral%2520exploration%2520and%2520provides%2520a%2520comprehensive%2520analysis%2520of%250Alunar%2520mineralogy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20into%20Lunar%20Mineralogy%3A%20An%20Unsupervised%20Approach%20for%20Clustering%0A%20%20of%20the%20Moon%20Mineral%20Mapper%20%28M3%29%20spectral%20data&entry.906535625=Freja%20Thoresen%20and%20Igor%20Drozdovskiy%20and%20Aidan%20Cowley%20and%20Magdelena%20Laban%20and%20Sebastien%20Besse%20and%20Sylvain%20Blunier&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20method%20for%20mapping%20spectral%20features%20of%20the%20Moon%0Ausing%20machine%20learning-based%20clustering%20of%20hyperspectral%20data%20from%20the%20Moon%0AMineral%20Mapper%20%28M3%29%20imaging%20spectrometer.%20The%20method%20uses%20a%20convolutional%0Avariational%20autoencoder%20to%20reduce%20the%20dimensionality%20of%20the%20spectral%20data%20and%0Aextract%20features%20of%20the%20spectra.%20Then%2C%20a%20k-means%20algorithm%20is%20applied%20to%0Acluster%20the%20latent%20variables%20into%20five%20distinct%20groups%2C%20corresponding%20to%0Adominant%20spectral%20features%2C%20which%20are%20related%20to%20the%20mineral%20composition%20of%20the%0AMoon%27s%20surface.%20The%20resulting%20global%20spectral%20cluster%20map%20shows%20the%0Adistribution%20of%20the%20five%20clusters%20on%20the%20Moon%2C%20which%20consist%20of%20a%20mixture%20of%2C%0Aamong%20others%2C%20plagioclase%2C%20pyroxene%2C%20olivine%2C%20and%20Fe-bearing%20minerals%20across%0Athe%20Moon%27s%20surface.%20The%20clusters%20are%20compared%20to%20the%20mineral%20maps%20from%20the%0AKaguya%20mission%2C%20which%20showed%20that%20the%20locations%20of%20the%20clusters%20overlap%20with%0Athe%20locations%20of%20high%20wt%25%20of%20minerals%20such%20as%20plagioclase%2C%20clinopyroxene%2C%20and%0Aolivine.%20The%20paper%20demonstrates%20the%20usefulness%20of%20unbiased%20unsupervised%0Alearning%20for%20lunar%20mineral%20exploration%20and%20provides%20a%20comprehensive%20analysis%20of%0Alunar%20mineralogy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03186v1&entry.124074799=Read"},
{"title": "Statistical Properties of Deep Neural Networks with Dependent Data", "author": "Chad Brown", "abstract": "  This paper establishes statistical properties of deep neural network (DNN)\nestimators under dependent data. Two general results for nonparametric sieve\nestimators directly applicable to DNN estimators are given. The first\nestablishes rates for convergence in probability under nonstationary data. The\nsecond provides non-asymptotic probability bounds on $\\mathcal{L}^{2}$-errors\nunder stationary $\\beta$-mixing data. I apply these results to DNN estimators\nin both regression and classification contexts imposing only a standard\nH\\\"older smoothness assumption. The DNN architectures considered are common in\napplications, featuring fully connected feedforward networks with any\ncontinuous piecewise linear activation function, unbounded weights, and a width\nand depth that grows with sample size. The framework provided also offers\npotential for research into other DNN architectures and time-series\napplications.\n", "link": "http://arxiv.org/abs/2410.11113v2", "date": "2024-11-05", "relevancy": 1.8207, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5021}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Properties%20of%20Deep%20Neural%20Networks%20with%20Dependent%20Data&body=Title%3A%20Statistical%20Properties%20of%20Deep%20Neural%20Networks%20with%20Dependent%20Data%0AAuthor%3A%20Chad%20Brown%0AAbstract%3A%20%20%20This%20paper%20establishes%20statistical%20properties%20of%20deep%20neural%20network%20%28DNN%29%0Aestimators%20under%20dependent%20data.%20Two%20general%20results%20for%20nonparametric%20sieve%0Aestimators%20directly%20applicable%20to%20DNN%20estimators%20are%20given.%20The%20first%0Aestablishes%20rates%20for%20convergence%20in%20probability%20under%20nonstationary%20data.%20The%0Asecond%20provides%20non-asymptotic%20probability%20bounds%20on%20%24%5Cmathcal%7BL%7D%5E%7B2%7D%24-errors%0Aunder%20stationary%20%24%5Cbeta%24-mixing%20data.%20I%20apply%20these%20results%20to%20DNN%20estimators%0Ain%20both%20regression%20and%20classification%20contexts%20imposing%20only%20a%20standard%0AH%5C%22older%20smoothness%20assumption.%20The%20DNN%20architectures%20considered%20are%20common%20in%0Aapplications%2C%20featuring%20fully%20connected%20feedforward%20networks%20with%20any%0Acontinuous%20piecewise%20linear%20activation%20function%2C%20unbounded%20weights%2C%20and%20a%20width%0Aand%20depth%20that%20grows%20with%20sample%20size.%20The%20framework%20provided%20also%20offers%0Apotential%20for%20research%20into%20other%20DNN%20architectures%20and%20time-series%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Properties%2520of%2520Deep%2520Neural%2520Networks%2520with%2520Dependent%2520Data%26entry.906535625%3DChad%2520Brown%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520statistical%2520properties%2520of%2520deep%2520neural%2520network%2520%2528DNN%2529%250Aestimators%2520under%2520dependent%2520data.%2520Two%2520general%2520results%2520for%2520nonparametric%2520sieve%250Aestimators%2520directly%2520applicable%2520to%2520DNN%2520estimators%2520are%2520given.%2520The%2520first%250Aestablishes%2520rates%2520for%2520convergence%2520in%2520probability%2520under%2520nonstationary%2520data.%2520The%250Asecond%2520provides%2520non-asymptotic%2520probability%2520bounds%2520on%2520%2524%255Cmathcal%257BL%257D%255E%257B2%257D%2524-errors%250Aunder%2520stationary%2520%2524%255Cbeta%2524-mixing%2520data.%2520I%2520apply%2520these%2520results%2520to%2520DNN%2520estimators%250Ain%2520both%2520regression%2520and%2520classification%2520contexts%2520imposing%2520only%2520a%2520standard%250AH%255C%2522older%2520smoothness%2520assumption.%2520The%2520DNN%2520architectures%2520considered%2520are%2520common%2520in%250Aapplications%252C%2520featuring%2520fully%2520connected%2520feedforward%2520networks%2520with%2520any%250Acontinuous%2520piecewise%2520linear%2520activation%2520function%252C%2520unbounded%2520weights%252C%2520and%2520a%2520width%250Aand%2520depth%2520that%2520grows%2520with%2520sample%2520size.%2520The%2520framework%2520provided%2520also%2520offers%250Apotential%2520for%2520research%2520into%2520other%2520DNN%2520architectures%2520and%2520time-series%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Properties%20of%20Deep%20Neural%20Networks%20with%20Dependent%20Data&entry.906535625=Chad%20Brown&entry.1292438233=%20%20This%20paper%20establishes%20statistical%20properties%20of%20deep%20neural%20network%20%28DNN%29%0Aestimators%20under%20dependent%20data.%20Two%20general%20results%20for%20nonparametric%20sieve%0Aestimators%20directly%20applicable%20to%20DNN%20estimators%20are%20given.%20The%20first%0Aestablishes%20rates%20for%20convergence%20in%20probability%20under%20nonstationary%20data.%20The%0Asecond%20provides%20non-asymptotic%20probability%20bounds%20on%20%24%5Cmathcal%7BL%7D%5E%7B2%7D%24-errors%0Aunder%20stationary%20%24%5Cbeta%24-mixing%20data.%20I%20apply%20these%20results%20to%20DNN%20estimators%0Ain%20both%20regression%20and%20classification%20contexts%20imposing%20only%20a%20standard%0AH%5C%22older%20smoothness%20assumption.%20The%20DNN%20architectures%20considered%20are%20common%20in%0Aapplications%2C%20featuring%20fully%20connected%20feedforward%20networks%20with%20any%0Acontinuous%20piecewise%20linear%20activation%20function%2C%20unbounded%20weights%2C%20and%20a%20width%0Aand%20depth%20that%20grows%20with%20sample%20size.%20The%20framework%20provided%20also%20offers%0Apotential%20for%20research%20into%20other%20DNN%20architectures%20and%20time-series%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11113v2&entry.124074799=Read"},
{"title": "GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep\n  Learning", "author": "Tonghan Wang and Yanchen Jiang and David C. Parkes", "abstract": "  Automated mechanism design (AMD) uses computational methods for mechanism\ndesign. Differentiable economics is a form of AMD that uses deep learning to\nlearn mechanism designs and has enabled strong progress in AMD in recent years.\nNevertheless, a major open problem has been to learn multi-bidder, general, and\nfully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork\n(GemNet), which significantly extends the menu-based approach of the\nsingle-bidder RochetNet (D\\\"utting et al., 2024) to the multi-bidder setting.\nThe challenge in achieving SP is to learn bidder-independent menus that are\nfeasible, so that the optimal menu choices for each bidder do not over-allocate\nitems when taken together (we call this menu compatibility). GemNet penalizes\nthe failure of menu compatibility during training, and transforms learned menus\nafter training through price changes, by considering a set of discretized\nbidder values and reasoning about Lipschitz smoothness to guarantee menu\ncompatibility on the entire value space. This approach is general, leaving\ntrained menus that already satisfy menu compatibility undisturbed and reducing\nto RochetNet for a single bidder. Mixed-integer linear programs are used for\nmenu transforms, and through a number of optimizations enabled by deep\nlearning, including adaptive grids and methods to skip menu elements, we scale\nto large auction design problems. GemNet learns auctions with better revenue\nthan affine maximization methods, achieves exact SP whereas previous general\nmulti-bidder methods are approximately SP, and offers greatly enhanced\ninterpretability.\n", "link": "http://arxiv.org/abs/2406.07428v3", "date": "2024-11-05", "relevancy": 1.8201, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GemNet%3A%20Menu-Based%2C%20Strategy-Proof%20Multi-Bidder%20Auctions%20Through%20Deep%0A%20%20Learning&body=Title%3A%20GemNet%3A%20Menu-Based%2C%20Strategy-Proof%20Multi-Bidder%20Auctions%20Through%20Deep%0A%20%20Learning%0AAuthor%3A%20Tonghan%20Wang%20and%20Yanchen%20Jiang%20and%20David%20C.%20Parkes%0AAbstract%3A%20%20%20Automated%20mechanism%20design%20%28AMD%29%20uses%20computational%20methods%20for%20mechanism%0Adesign.%20Differentiable%20economics%20is%20a%20form%20of%20AMD%20that%20uses%20deep%20learning%20to%0Alearn%20mechanism%20designs%20and%20has%20enabled%20strong%20progress%20in%20AMD%20in%20recent%20years.%0ANevertheless%2C%20a%20major%20open%20problem%20has%20been%20to%20learn%20multi-bidder%2C%20general%2C%20and%0Afully%20strategy-proof%20%28SP%29%20auctions.%20We%20introduce%20GEneral%20Menu-based%20NETwork%0A%28GemNet%29%2C%20which%20significantly%20extends%20the%20menu-based%20approach%20of%20the%0Asingle-bidder%20RochetNet%20%28D%5C%22utting%20et%20al.%2C%202024%29%20to%20the%20multi-bidder%20setting.%0AThe%20challenge%20in%20achieving%20SP%20is%20to%20learn%20bidder-independent%20menus%20that%20are%0Afeasible%2C%20so%20that%20the%20optimal%20menu%20choices%20for%20each%20bidder%20do%20not%20over-allocate%0Aitems%20when%20taken%20together%20%28we%20call%20this%20menu%20compatibility%29.%20GemNet%20penalizes%0Athe%20failure%20of%20menu%20compatibility%20during%20training%2C%20and%20transforms%20learned%20menus%0Aafter%20training%20through%20price%20changes%2C%20by%20considering%20a%20set%20of%20discretized%0Abidder%20values%20and%20reasoning%20about%20Lipschitz%20smoothness%20to%20guarantee%20menu%0Acompatibility%20on%20the%20entire%20value%20space.%20This%20approach%20is%20general%2C%20leaving%0Atrained%20menus%20that%20already%20satisfy%20menu%20compatibility%20undisturbed%20and%20reducing%0Ato%20RochetNet%20for%20a%20single%20bidder.%20Mixed-integer%20linear%20programs%20are%20used%20for%0Amenu%20transforms%2C%20and%20through%20a%20number%20of%20optimizations%20enabled%20by%20deep%0Alearning%2C%20including%20adaptive%20grids%20and%20methods%20to%20skip%20menu%20elements%2C%20we%20scale%0Ato%20large%20auction%20design%20problems.%20GemNet%20learns%20auctions%20with%20better%20revenue%0Athan%20affine%20maximization%20methods%2C%20achieves%20exact%20SP%20whereas%20previous%20general%0Amulti-bidder%20methods%20are%20approximately%20SP%2C%20and%20offers%20greatly%20enhanced%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07428v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemNet%253A%2520Menu-Based%252C%2520Strategy-Proof%2520Multi-Bidder%2520Auctions%2520Through%2520Deep%250A%2520%2520Learning%26entry.906535625%3DTonghan%2520Wang%2520and%2520Yanchen%2520Jiang%2520and%2520David%2520C.%2520Parkes%26entry.1292438233%3D%2520%2520Automated%2520mechanism%2520design%2520%2528AMD%2529%2520uses%2520computational%2520methods%2520for%2520mechanism%250Adesign.%2520Differentiable%2520economics%2520is%2520a%2520form%2520of%2520AMD%2520that%2520uses%2520deep%2520learning%2520to%250Alearn%2520mechanism%2520designs%2520and%2520has%2520enabled%2520strong%2520progress%2520in%2520AMD%2520in%2520recent%2520years.%250ANevertheless%252C%2520a%2520major%2520open%2520problem%2520has%2520been%2520to%2520learn%2520multi-bidder%252C%2520general%252C%2520and%250Afully%2520strategy-proof%2520%2528SP%2529%2520auctions.%2520We%2520introduce%2520GEneral%2520Menu-based%2520NETwork%250A%2528GemNet%2529%252C%2520which%2520significantly%2520extends%2520the%2520menu-based%2520approach%2520of%2520the%250Asingle-bidder%2520RochetNet%2520%2528D%255C%2522utting%2520et%2520al.%252C%25202024%2529%2520to%2520the%2520multi-bidder%2520setting.%250AThe%2520challenge%2520in%2520achieving%2520SP%2520is%2520to%2520learn%2520bidder-independent%2520menus%2520that%2520are%250Afeasible%252C%2520so%2520that%2520the%2520optimal%2520menu%2520choices%2520for%2520each%2520bidder%2520do%2520not%2520over-allocate%250Aitems%2520when%2520taken%2520together%2520%2528we%2520call%2520this%2520menu%2520compatibility%2529.%2520GemNet%2520penalizes%250Athe%2520failure%2520of%2520menu%2520compatibility%2520during%2520training%252C%2520and%2520transforms%2520learned%2520menus%250Aafter%2520training%2520through%2520price%2520changes%252C%2520by%2520considering%2520a%2520set%2520of%2520discretized%250Abidder%2520values%2520and%2520reasoning%2520about%2520Lipschitz%2520smoothness%2520to%2520guarantee%2520menu%250Acompatibility%2520on%2520the%2520entire%2520value%2520space.%2520This%2520approach%2520is%2520general%252C%2520leaving%250Atrained%2520menus%2520that%2520already%2520satisfy%2520menu%2520compatibility%2520undisturbed%2520and%2520reducing%250Ato%2520RochetNet%2520for%2520a%2520single%2520bidder.%2520Mixed-integer%2520linear%2520programs%2520are%2520used%2520for%250Amenu%2520transforms%252C%2520and%2520through%2520a%2520number%2520of%2520optimizations%2520enabled%2520by%2520deep%250Alearning%252C%2520including%2520adaptive%2520grids%2520and%2520methods%2520to%2520skip%2520menu%2520elements%252C%2520we%2520scale%250Ato%2520large%2520auction%2520design%2520problems.%2520GemNet%2520learns%2520auctions%2520with%2520better%2520revenue%250Athan%2520affine%2520maximization%2520methods%252C%2520achieves%2520exact%2520SP%2520whereas%2520previous%2520general%250Amulti-bidder%2520methods%2520are%2520approximately%2520SP%252C%2520and%2520offers%2520greatly%2520enhanced%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07428v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GemNet%3A%20Menu-Based%2C%20Strategy-Proof%20Multi-Bidder%20Auctions%20Through%20Deep%0A%20%20Learning&entry.906535625=Tonghan%20Wang%20and%20Yanchen%20Jiang%20and%20David%20C.%20Parkes&entry.1292438233=%20%20Automated%20mechanism%20design%20%28AMD%29%20uses%20computational%20methods%20for%20mechanism%0Adesign.%20Differentiable%20economics%20is%20a%20form%20of%20AMD%20that%20uses%20deep%20learning%20to%0Alearn%20mechanism%20designs%20and%20has%20enabled%20strong%20progress%20in%20AMD%20in%20recent%20years.%0ANevertheless%2C%20a%20major%20open%20problem%20has%20been%20to%20learn%20multi-bidder%2C%20general%2C%20and%0Afully%20strategy-proof%20%28SP%29%20auctions.%20We%20introduce%20GEneral%20Menu-based%20NETwork%0A%28GemNet%29%2C%20which%20significantly%20extends%20the%20menu-based%20approach%20of%20the%0Asingle-bidder%20RochetNet%20%28D%5C%22utting%20et%20al.%2C%202024%29%20to%20the%20multi-bidder%20setting.%0AThe%20challenge%20in%20achieving%20SP%20is%20to%20learn%20bidder-independent%20menus%20that%20are%0Afeasible%2C%20so%20that%20the%20optimal%20menu%20choices%20for%20each%20bidder%20do%20not%20over-allocate%0Aitems%20when%20taken%20together%20%28we%20call%20this%20menu%20compatibility%29.%20GemNet%20penalizes%0Athe%20failure%20of%20menu%20compatibility%20during%20training%2C%20and%20transforms%20learned%20menus%0Aafter%20training%20through%20price%20changes%2C%20by%20considering%20a%20set%20of%20discretized%0Abidder%20values%20and%20reasoning%20about%20Lipschitz%20smoothness%20to%20guarantee%20menu%0Acompatibility%20on%20the%20entire%20value%20space.%20This%20approach%20is%20general%2C%20leaving%0Atrained%20menus%20that%20already%20satisfy%20menu%20compatibility%20undisturbed%20and%20reducing%0Ato%20RochetNet%20for%20a%20single%20bidder.%20Mixed-integer%20linear%20programs%20are%20used%20for%0Amenu%20transforms%2C%20and%20through%20a%20number%20of%20optimizations%20enabled%20by%20deep%0Alearning%2C%20including%20adaptive%20grids%20and%20methods%20to%20skip%20menu%20elements%2C%20we%20scale%0Ato%20large%20auction%20design%20problems.%20GemNet%20learns%20auctions%20with%20better%20revenue%0Athan%20affine%20maximization%20methods%2C%20achieves%20exact%20SP%20whereas%20previous%20general%0Amulti-bidder%20methods%20are%20approximately%20SP%2C%20and%20offers%20greatly%20enhanced%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07428v3&entry.124074799=Read"},
{"title": "ACE: All-round Creator and Editor Following Instructions via Diffusion\n  Transformer", "author": "Zhen Han and Zeyinzi Jiang and Yulin Pan and Jingfeng Zhang and Chaojie Mao and Chenwei Xie and Yu Liu and Jingren Zhou", "abstract": "  Diffusion models have emerged as a powerful generative technology and have\nbeen found to be applicable in various scenarios. Most existing foundational\ndiffusion models are primarily designed for text-guided visual generation and\ndo not support multi-modal conditions, which are essential for many visual\nediting tasks. This limitation prevents these foundational diffusion models\nfrom serving as a unified model in the field of visual generation, like GPT-4\nin the natural language processing field. In this work, we propose ACE, an\nAll-round Creator and Editor, which achieves comparable performance compared to\nthose expert models in a wide range of visual generation tasks. To achieve this\ngoal, we first introduce a unified condition format termed Long-context\nCondition Unit (LCU), and propose a novel Transformer-based diffusion model\nthat uses LCU as input, aiming for joint training across various generation and\nediting tasks. Furthermore, we propose an efficient data collection approach to\naddress the issue of the absence of available training data. It involves\nacquiring pairwise images with synthesis-based or clustering-based pipelines\nand supplying these pairs with accurate textual instructions by leveraging a\nfine-tuned multi-modal large language model. To comprehensively evaluate the\nperformance of our model, we establish a benchmark of manually annotated pairs\ndata across a variety of visual generation tasks. The extensive experimental\nresults demonstrate the superiority of our model in visual generation fields.\nThanks to the all-in-one capabilities of our model, we can easily build a\nmulti-modal chat system that responds to any interactive request for image\ncreation using a single model to serve as the backend, avoiding the cumbersome\npipeline typically employed in visual agents. Code and models will be available\non the project page: https://ali-vilab.github.io/ace-page/.\n", "link": "http://arxiv.org/abs/2410.00086v2", "date": "2024-11-05", "relevancy": 1.8143, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6107}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.599}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE%3A%20All-round%20Creator%20and%20Editor%20Following%20Instructions%20via%20Diffusion%0A%20%20Transformer&body=Title%3A%20ACE%3A%20All-round%20Creator%20and%20Editor%20Following%20Instructions%20via%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Zhen%20Han%20and%20Zeyinzi%20Jiang%20and%20Yulin%20Pan%20and%20Jingfeng%20Zhang%20and%20Chaojie%20Mao%20and%20Chenwei%20Xie%20and%20Yu%20Liu%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20a%20powerful%20generative%20technology%20and%20have%0Abeen%20found%20to%20be%20applicable%20in%20various%20scenarios.%20Most%20existing%20foundational%0Adiffusion%20models%20are%20primarily%20designed%20for%20text-guided%20visual%20generation%20and%0Ado%20not%20support%20multi-modal%20conditions%2C%20which%20are%20essential%20for%20many%20visual%0Aediting%20tasks.%20This%20limitation%20prevents%20these%20foundational%20diffusion%20models%0Afrom%20serving%20as%20a%20unified%20model%20in%20the%20field%20of%20visual%20generation%2C%20like%20GPT-4%0Ain%20the%20natural%20language%20processing%20field.%20In%20this%20work%2C%20we%20propose%20ACE%2C%20an%0AAll-round%20Creator%20and%20Editor%2C%20which%20achieves%20comparable%20performance%20compared%20to%0Athose%20expert%20models%20in%20a%20wide%20range%20of%20visual%20generation%20tasks.%20To%20achieve%20this%0Agoal%2C%20we%20first%20introduce%20a%20unified%20condition%20format%20termed%20Long-context%0ACondition%20Unit%20%28LCU%29%2C%20and%20propose%20a%20novel%20Transformer-based%20diffusion%20model%0Athat%20uses%20LCU%20as%20input%2C%20aiming%20for%20joint%20training%20across%20various%20generation%20and%0Aediting%20tasks.%20Furthermore%2C%20we%20propose%20an%20efficient%20data%20collection%20approach%20to%0Aaddress%20the%20issue%20of%20the%20absence%20of%20available%20training%20data.%20It%20involves%0Aacquiring%20pairwise%20images%20with%20synthesis-based%20or%20clustering-based%20pipelines%0Aand%20supplying%20these%20pairs%20with%20accurate%20textual%20instructions%20by%20leveraging%20a%0Afine-tuned%20multi-modal%20large%20language%20model.%20To%20comprehensively%20evaluate%20the%0Aperformance%20of%20our%20model%2C%20we%20establish%20a%20benchmark%20of%20manually%20annotated%20pairs%0Adata%20across%20a%20variety%20of%20visual%20generation%20tasks.%20The%20extensive%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20model%20in%20visual%20generation%20fields.%0AThanks%20to%20the%20all-in-one%20capabilities%20of%20our%20model%2C%20we%20can%20easily%20build%20a%0Amulti-modal%20chat%20system%20that%20responds%20to%20any%20interactive%20request%20for%20image%0Acreation%20using%20a%20single%20model%20to%20serve%20as%20the%20backend%2C%20avoiding%20the%20cumbersome%0Apipeline%20typically%20employed%20in%20visual%20agents.%20Code%20and%20models%20will%20be%20available%0Aon%20the%20project%20page%3A%20https%3A//ali-vilab.github.io/ace-page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE%253A%2520All-round%2520Creator%2520and%2520Editor%2520Following%2520Instructions%2520via%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DZhen%2520Han%2520and%2520Zeyinzi%2520Jiang%2520and%2520Yulin%2520Pan%2520and%2520Jingfeng%2520Zhang%2520and%2520Chaojie%2520Mao%2520and%2520Chenwei%2520Xie%2520and%2520Yu%2520Liu%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520generative%2520technology%2520and%2520have%250Abeen%2520found%2520to%2520be%2520applicable%2520in%2520various%2520scenarios.%2520Most%2520existing%2520foundational%250Adiffusion%2520models%2520are%2520primarily%2520designed%2520for%2520text-guided%2520visual%2520generation%2520and%250Ado%2520not%2520support%2520multi-modal%2520conditions%252C%2520which%2520are%2520essential%2520for%2520many%2520visual%250Aediting%2520tasks.%2520This%2520limitation%2520prevents%2520these%2520foundational%2520diffusion%2520models%250Afrom%2520serving%2520as%2520a%2520unified%2520model%2520in%2520the%2520field%2520of%2520visual%2520generation%252C%2520like%2520GPT-4%250Ain%2520the%2520natural%2520language%2520processing%2520field.%2520In%2520this%2520work%252C%2520we%2520propose%2520ACE%252C%2520an%250AAll-round%2520Creator%2520and%2520Editor%252C%2520which%2520achieves%2520comparable%2520performance%2520compared%2520to%250Athose%2520expert%2520models%2520in%2520a%2520wide%2520range%2520of%2520visual%2520generation%2520tasks.%2520To%2520achieve%2520this%250Agoal%252C%2520we%2520first%2520introduce%2520a%2520unified%2520condition%2520format%2520termed%2520Long-context%250ACondition%2520Unit%2520%2528LCU%2529%252C%2520and%2520propose%2520a%2520novel%2520Transformer-based%2520diffusion%2520model%250Athat%2520uses%2520LCU%2520as%2520input%252C%2520aiming%2520for%2520joint%2520training%2520across%2520various%2520generation%2520and%250Aediting%2520tasks.%2520Furthermore%252C%2520we%2520propose%2520an%2520efficient%2520data%2520collection%2520approach%2520to%250Aaddress%2520the%2520issue%2520of%2520the%2520absence%2520of%2520available%2520training%2520data.%2520It%2520involves%250Aacquiring%2520pairwise%2520images%2520with%2520synthesis-based%2520or%2520clustering-based%2520pipelines%250Aand%2520supplying%2520these%2520pairs%2520with%2520accurate%2520textual%2520instructions%2520by%2520leveraging%2520a%250Afine-tuned%2520multi-modal%2520large%2520language%2520model.%2520To%2520comprehensively%2520evaluate%2520the%250Aperformance%2520of%2520our%2520model%252C%2520we%2520establish%2520a%2520benchmark%2520of%2520manually%2520annotated%2520pairs%250Adata%2520across%2520a%2520variety%2520of%2520visual%2520generation%2520tasks.%2520The%2520extensive%2520experimental%250Aresults%2520demonstrate%2520the%2520superiority%2520of%2520our%2520model%2520in%2520visual%2520generation%2520fields.%250AThanks%2520to%2520the%2520all-in-one%2520capabilities%2520of%2520our%2520model%252C%2520we%2520can%2520easily%2520build%2520a%250Amulti-modal%2520chat%2520system%2520that%2520responds%2520to%2520any%2520interactive%2520request%2520for%2520image%250Acreation%2520using%2520a%2520single%2520model%2520to%2520serve%2520as%2520the%2520backend%252C%2520avoiding%2520the%2520cumbersome%250Apipeline%2520typically%2520employed%2520in%2520visual%2520agents.%2520Code%2520and%2520models%2520will%2520be%2520available%250Aon%2520the%2520project%2520page%253A%2520https%253A//ali-vilab.github.io/ace-page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%3A%20All-round%20Creator%20and%20Editor%20Following%20Instructions%20via%20Diffusion%0A%20%20Transformer&entry.906535625=Zhen%20Han%20and%20Zeyinzi%20Jiang%20and%20Yulin%20Pan%20and%20Jingfeng%20Zhang%20and%20Chaojie%20Mao%20and%20Chenwei%20Xie%20and%20Yu%20Liu%20and%20Jingren%20Zhou&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20a%20powerful%20generative%20technology%20and%20have%0Abeen%20found%20to%20be%20applicable%20in%20various%20scenarios.%20Most%20existing%20foundational%0Adiffusion%20models%20are%20primarily%20designed%20for%20text-guided%20visual%20generation%20and%0Ado%20not%20support%20multi-modal%20conditions%2C%20which%20are%20essential%20for%20many%20visual%0Aediting%20tasks.%20This%20limitation%20prevents%20these%20foundational%20diffusion%20models%0Afrom%20serving%20as%20a%20unified%20model%20in%20the%20field%20of%20visual%20generation%2C%20like%20GPT-4%0Ain%20the%20natural%20language%20processing%20field.%20In%20this%20work%2C%20we%20propose%20ACE%2C%20an%0AAll-round%20Creator%20and%20Editor%2C%20which%20achieves%20comparable%20performance%20compared%20to%0Athose%20expert%20models%20in%20a%20wide%20range%20of%20visual%20generation%20tasks.%20To%20achieve%20this%0Agoal%2C%20we%20first%20introduce%20a%20unified%20condition%20format%20termed%20Long-context%0ACondition%20Unit%20%28LCU%29%2C%20and%20propose%20a%20novel%20Transformer-based%20diffusion%20model%0Athat%20uses%20LCU%20as%20input%2C%20aiming%20for%20joint%20training%20across%20various%20generation%20and%0Aediting%20tasks.%20Furthermore%2C%20we%20propose%20an%20efficient%20data%20collection%20approach%20to%0Aaddress%20the%20issue%20of%20the%20absence%20of%20available%20training%20data.%20It%20involves%0Aacquiring%20pairwise%20images%20with%20synthesis-based%20or%20clustering-based%20pipelines%0Aand%20supplying%20these%20pairs%20with%20accurate%20textual%20instructions%20by%20leveraging%20a%0Afine-tuned%20multi-modal%20large%20language%20model.%20To%20comprehensively%20evaluate%20the%0Aperformance%20of%20our%20model%2C%20we%20establish%20a%20benchmark%20of%20manually%20annotated%20pairs%0Adata%20across%20a%20variety%20of%20visual%20generation%20tasks.%20The%20extensive%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20model%20in%20visual%20generation%20fields.%0AThanks%20to%20the%20all-in-one%20capabilities%20of%20our%20model%2C%20we%20can%20easily%20build%20a%0Amulti-modal%20chat%20system%20that%20responds%20to%20any%20interactive%20request%20for%20image%0Acreation%20using%20a%20single%20model%20to%20serve%20as%20the%20backend%2C%20avoiding%20the%20cumbersome%0Apipeline%20typically%20employed%20in%20visual%20agents.%20Code%20and%20models%20will%20be%20available%0Aon%20the%20project%20page%3A%20https%3A//ali-vilab.github.io/ace-page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00086v2&entry.124074799=Read"},
{"title": "Generative AI Enables EEG Super-Resolution via Spatio-Temporal Adaptive\n  Diffusion Learning", "author": "Tong Zhou and Shuqiang Wang", "abstract": "  Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG)\ndevices, are widely used in fields such as neuroscience. HD EEG devices improve\nthe spatial resolution of EEG by placing more electrodes on the scalp, which\nmeet the requirements of clinical diagnostic applications such as epilepsy\nfocus localization. However, this technique faces challenges, such as high\nacquisition costs and limited usage scenarios. In this paper, spatio-temporal\nadaptive diffusion models (STAD) are proposed to pioneer the use of diffusion\nmodels for achieving spatial SR reconstruction from low-resolution (LR, 64\nchannels or fewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically,\na spatio-temporal condition module is designed to extract the spatio-temporal\nfeatures of LR EEG, which then used as conditional inputs to direct the reverse\ndenoising process. Additionally, a multi-scale Transformer denoising module is\nconstructed to leverage multi-scale convolution blocks and\ncross-attention-based diffusion Transformer blocks for conditional guidance to\ngenerate subject-adaptive SR EEG. Experimental results demonstrate that the\nSTAD significantly enhances the spatial resolution of LR EEG and quantitatively\noutperforms existing methods. Furthermore, STAD demonstrate their value by\napplying synthetic SR EEG to classification and source localization tasks,\nindicating their potential to Substantially boost the spatial resolution of\nEEG.\n", "link": "http://arxiv.org/abs/2407.03089v4", "date": "2024-11-05", "relevancy": 1.8063, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6152}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6039}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Enables%20EEG%20Super-Resolution%20via%20Spatio-Temporal%20Adaptive%0A%20%20Diffusion%20Learning&body=Title%3A%20Generative%20AI%20Enables%20EEG%20Super-Resolution%20via%20Spatio-Temporal%20Adaptive%0A%20%20Diffusion%20Learning%0AAuthor%3A%20Tong%20Zhou%20and%20Shuqiang%20Wang%0AAbstract%3A%20%20%20Electroencephalogram%20%28EEG%29%20technology%2C%20particularly%20high-density%20EEG%20%28HD%20EEG%29%0Adevices%2C%20are%20widely%20used%20in%20fields%20such%20as%20neuroscience.%20HD%20EEG%20devices%20improve%0Athe%20spatial%20resolution%20of%20EEG%20by%20placing%20more%20electrodes%20on%20the%20scalp%2C%20which%0Ameet%20the%20requirements%20of%20clinical%20diagnostic%20applications%20such%20as%20epilepsy%0Afocus%20localization.%20However%2C%20this%20technique%20faces%20challenges%2C%20such%20as%20high%0Aacquisition%20costs%20and%20limited%20usage%20scenarios.%20In%20this%20paper%2C%20spatio-temporal%0Aadaptive%20diffusion%20models%20%28STAD%29%20are%20proposed%20to%20pioneer%20the%20use%20of%20diffusion%0Amodels%20for%20achieving%20spatial%20SR%20reconstruction%20from%20low-resolution%20%28LR%2C%2064%0Achannels%20or%20fewer%29%20EEG%20to%20high-resolution%20%28HR%2C%20256%20channels%29%20EEG.%20Specifically%2C%0Aa%20spatio-temporal%20condition%20module%20is%20designed%20to%20extract%20the%20spatio-temporal%0Afeatures%20of%20LR%20EEG%2C%20which%20then%20used%20as%20conditional%20inputs%20to%20direct%20the%20reverse%0Adenoising%20process.%20Additionally%2C%20a%20multi-scale%20Transformer%20denoising%20module%20is%0Aconstructed%20to%20leverage%20multi-scale%20convolution%20blocks%20and%0Across-attention-based%20diffusion%20Transformer%20blocks%20for%20conditional%20guidance%20to%0Agenerate%20subject-adaptive%20SR%20EEG.%20Experimental%20results%20demonstrate%20that%20the%0ASTAD%20significantly%20enhances%20the%20spatial%20resolution%20of%20LR%20EEG%20and%20quantitatively%0Aoutperforms%20existing%20methods.%20Furthermore%2C%20STAD%20demonstrate%20their%20value%20by%0Aapplying%20synthetic%20SR%20EEG%20to%20classification%20and%20source%20localization%20tasks%2C%0Aindicating%20their%20potential%20to%20Substantially%20boost%20the%20spatial%20resolution%20of%0AEEG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03089v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Enables%2520EEG%2520Super-Resolution%2520via%2520Spatio-Temporal%2520Adaptive%250A%2520%2520Diffusion%2520Learning%26entry.906535625%3DTong%2520Zhou%2520and%2520Shuqiang%2520Wang%26entry.1292438233%3D%2520%2520Electroencephalogram%2520%2528EEG%2529%2520technology%252C%2520particularly%2520high-density%2520EEG%2520%2528HD%2520EEG%2529%250Adevices%252C%2520are%2520widely%2520used%2520in%2520fields%2520such%2520as%2520neuroscience.%2520HD%2520EEG%2520devices%2520improve%250Athe%2520spatial%2520resolution%2520of%2520EEG%2520by%2520placing%2520more%2520electrodes%2520on%2520the%2520scalp%252C%2520which%250Ameet%2520the%2520requirements%2520of%2520clinical%2520diagnostic%2520applications%2520such%2520as%2520epilepsy%250Afocus%2520localization.%2520However%252C%2520this%2520technique%2520faces%2520challenges%252C%2520such%2520as%2520high%250Aacquisition%2520costs%2520and%2520limited%2520usage%2520scenarios.%2520In%2520this%2520paper%252C%2520spatio-temporal%250Aadaptive%2520diffusion%2520models%2520%2528STAD%2529%2520are%2520proposed%2520to%2520pioneer%2520the%2520use%2520of%2520diffusion%250Amodels%2520for%2520achieving%2520spatial%2520SR%2520reconstruction%2520from%2520low-resolution%2520%2528LR%252C%252064%250Achannels%2520or%2520fewer%2529%2520EEG%2520to%2520high-resolution%2520%2528HR%252C%2520256%2520channels%2529%2520EEG.%2520Specifically%252C%250Aa%2520spatio-temporal%2520condition%2520module%2520is%2520designed%2520to%2520extract%2520the%2520spatio-temporal%250Afeatures%2520of%2520LR%2520EEG%252C%2520which%2520then%2520used%2520as%2520conditional%2520inputs%2520to%2520direct%2520the%2520reverse%250Adenoising%2520process.%2520Additionally%252C%2520a%2520multi-scale%2520Transformer%2520denoising%2520module%2520is%250Aconstructed%2520to%2520leverage%2520multi-scale%2520convolution%2520blocks%2520and%250Across-attention-based%2520diffusion%2520Transformer%2520blocks%2520for%2520conditional%2520guidance%2520to%250Agenerate%2520subject-adaptive%2520SR%2520EEG.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250ASTAD%2520significantly%2520enhances%2520the%2520spatial%2520resolution%2520of%2520LR%2520EEG%2520and%2520quantitatively%250Aoutperforms%2520existing%2520methods.%2520Furthermore%252C%2520STAD%2520demonstrate%2520their%2520value%2520by%250Aapplying%2520synthetic%2520SR%2520EEG%2520to%2520classification%2520and%2520source%2520localization%2520tasks%252C%250Aindicating%2520their%2520potential%2520to%2520Substantially%2520boost%2520the%2520spatial%2520resolution%2520of%250AEEG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03089v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Enables%20EEG%20Super-Resolution%20via%20Spatio-Temporal%20Adaptive%0A%20%20Diffusion%20Learning&entry.906535625=Tong%20Zhou%20and%20Shuqiang%20Wang&entry.1292438233=%20%20Electroencephalogram%20%28EEG%29%20technology%2C%20particularly%20high-density%20EEG%20%28HD%20EEG%29%0Adevices%2C%20are%20widely%20used%20in%20fields%20such%20as%20neuroscience.%20HD%20EEG%20devices%20improve%0Athe%20spatial%20resolution%20of%20EEG%20by%20placing%20more%20electrodes%20on%20the%20scalp%2C%20which%0Ameet%20the%20requirements%20of%20clinical%20diagnostic%20applications%20such%20as%20epilepsy%0Afocus%20localization.%20However%2C%20this%20technique%20faces%20challenges%2C%20such%20as%20high%0Aacquisition%20costs%20and%20limited%20usage%20scenarios.%20In%20this%20paper%2C%20spatio-temporal%0Aadaptive%20diffusion%20models%20%28STAD%29%20are%20proposed%20to%20pioneer%20the%20use%20of%20diffusion%0Amodels%20for%20achieving%20spatial%20SR%20reconstruction%20from%20low-resolution%20%28LR%2C%2064%0Achannels%20or%20fewer%29%20EEG%20to%20high-resolution%20%28HR%2C%20256%20channels%29%20EEG.%20Specifically%2C%0Aa%20spatio-temporal%20condition%20module%20is%20designed%20to%20extract%20the%20spatio-temporal%0Afeatures%20of%20LR%20EEG%2C%20which%20then%20used%20as%20conditional%20inputs%20to%20direct%20the%20reverse%0Adenoising%20process.%20Additionally%2C%20a%20multi-scale%20Transformer%20denoising%20module%20is%0Aconstructed%20to%20leverage%20multi-scale%20convolution%20blocks%20and%0Across-attention-based%20diffusion%20Transformer%20blocks%20for%20conditional%20guidance%20to%0Agenerate%20subject-adaptive%20SR%20EEG.%20Experimental%20results%20demonstrate%20that%20the%0ASTAD%20significantly%20enhances%20the%20spatial%20resolution%20of%20LR%20EEG%20and%20quantitatively%0Aoutperforms%20existing%20methods.%20Furthermore%2C%20STAD%20demonstrate%20their%20value%20by%0Aapplying%20synthetic%20SR%20EEG%20to%20classification%20and%20source%20localization%20tasks%2C%0Aindicating%20their%20potential%20to%20Substantially%20boost%20the%20spatial%20resolution%20of%0AEEG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03089v4&entry.124074799=Read"},
{"title": "User Centric Semantic Communications", "author": "Xunze Liu and Yifei Sun and Zhaorui Wang and Lizhao You and Haoyuan Pan and Fangxin Wang and Shuguang Cui", "abstract": "  Current studies on semantic communications mainly focus on efficiently\nextracting semantic information to reduce bandwidth usage between a transmitter\nand a user. Although significant process has been made in the semantic\ncommunications, a fundamental design problem is that the semantic information\nis extracted based on certain criteria at the transmitter side along, without\nconsidering the user's actual requirements. As a result, critical information\nthat is of primary concern to the user may be lost. In such cases, the semantic\ntransmission becomes meaningless to the user, as all received information is\nirrelevant to the user's interests. To solve this problem, this paper presents\na user centric semantic communication system, where the user sends its request\nfor the desired semantic information to the transmitter at the start of each\ntransmission. Then, the transmitter extracts the required semantic information\naccordingly. A key challenge is how the transmitter can understand the user's\nrequests for semantic information and extract the required semantic information\nin a reasonable and robust manner. We solve this challenge by designing a\nwell-structured framework and leveraging off-the-shelf products, such as GPT-4,\nalong with several specialized tools for detection and estimation. Evaluation\nresults demonstrate the feasibility and effectiveness of the proposed user\ncentric semantic communication system.\n", "link": "http://arxiv.org/abs/2411.03127v1", "date": "2024-11-05", "relevancy": 1.8059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User%20Centric%20Semantic%20Communications&body=Title%3A%20User%20Centric%20Semantic%20Communications%0AAuthor%3A%20Xunze%20Liu%20and%20Yifei%20Sun%20and%20Zhaorui%20Wang%20and%20Lizhao%20You%20and%20Haoyuan%20Pan%20and%20Fangxin%20Wang%20and%20Shuguang%20Cui%0AAbstract%3A%20%20%20Current%20studies%20on%20semantic%20communications%20mainly%20focus%20on%20efficiently%0Aextracting%20semantic%20information%20to%20reduce%20bandwidth%20usage%20between%20a%20transmitter%0Aand%20a%20user.%20Although%20significant%20process%20has%20been%20made%20in%20the%20semantic%0Acommunications%2C%20a%20fundamental%20design%20problem%20is%20that%20the%20semantic%20information%0Ais%20extracted%20based%20on%20certain%20criteria%20at%20the%20transmitter%20side%20along%2C%20without%0Aconsidering%20the%20user%27s%20actual%20requirements.%20As%20a%20result%2C%20critical%20information%0Athat%20is%20of%20primary%20concern%20to%20the%20user%20may%20be%20lost.%20In%20such%20cases%2C%20the%20semantic%0Atransmission%20becomes%20meaningless%20to%20the%20user%2C%20as%20all%20received%20information%20is%0Airrelevant%20to%20the%20user%27s%20interests.%20To%20solve%20this%20problem%2C%20this%20paper%20presents%0Aa%20user%20centric%20semantic%20communication%20system%2C%20where%20the%20user%20sends%20its%20request%0Afor%20the%20desired%20semantic%20information%20to%20the%20transmitter%20at%20the%20start%20of%20each%0Atransmission.%20Then%2C%20the%20transmitter%20extracts%20the%20required%20semantic%20information%0Aaccordingly.%20A%20key%20challenge%20is%20how%20the%20transmitter%20can%20understand%20the%20user%27s%0Arequests%20for%20semantic%20information%20and%20extract%20the%20required%20semantic%20information%0Ain%20a%20reasonable%20and%20robust%20manner.%20We%20solve%20this%20challenge%20by%20designing%20a%0Awell-structured%20framework%20and%20leveraging%20off-the-shelf%20products%2C%20such%20as%20GPT-4%2C%0Aalong%20with%20several%20specialized%20tools%20for%20detection%20and%20estimation.%20Evaluation%0Aresults%20demonstrate%20the%20feasibility%20and%20effectiveness%20of%20the%20proposed%20user%0Acentric%20semantic%20communication%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser%2520Centric%2520Semantic%2520Communications%26entry.906535625%3DXunze%2520Liu%2520and%2520Yifei%2520Sun%2520and%2520Zhaorui%2520Wang%2520and%2520Lizhao%2520You%2520and%2520Haoyuan%2520Pan%2520and%2520Fangxin%2520Wang%2520and%2520Shuguang%2520Cui%26entry.1292438233%3D%2520%2520Current%2520studies%2520on%2520semantic%2520communications%2520mainly%2520focus%2520on%2520efficiently%250Aextracting%2520semantic%2520information%2520to%2520reduce%2520bandwidth%2520usage%2520between%2520a%2520transmitter%250Aand%2520a%2520user.%2520Although%2520significant%2520process%2520has%2520been%2520made%2520in%2520the%2520semantic%250Acommunications%252C%2520a%2520fundamental%2520design%2520problem%2520is%2520that%2520the%2520semantic%2520information%250Ais%2520extracted%2520based%2520on%2520certain%2520criteria%2520at%2520the%2520transmitter%2520side%2520along%252C%2520without%250Aconsidering%2520the%2520user%2527s%2520actual%2520requirements.%2520As%2520a%2520result%252C%2520critical%2520information%250Athat%2520is%2520of%2520primary%2520concern%2520to%2520the%2520user%2520may%2520be%2520lost.%2520In%2520such%2520cases%252C%2520the%2520semantic%250Atransmission%2520becomes%2520meaningless%2520to%2520the%2520user%252C%2520as%2520all%2520received%2520information%2520is%250Airrelevant%2520to%2520the%2520user%2527s%2520interests.%2520To%2520solve%2520this%2520problem%252C%2520this%2520paper%2520presents%250Aa%2520user%2520centric%2520semantic%2520communication%2520system%252C%2520where%2520the%2520user%2520sends%2520its%2520request%250Afor%2520the%2520desired%2520semantic%2520information%2520to%2520the%2520transmitter%2520at%2520the%2520start%2520of%2520each%250Atransmission.%2520Then%252C%2520the%2520transmitter%2520extracts%2520the%2520required%2520semantic%2520information%250Aaccordingly.%2520A%2520key%2520challenge%2520is%2520how%2520the%2520transmitter%2520can%2520understand%2520the%2520user%2527s%250Arequests%2520for%2520semantic%2520information%2520and%2520extract%2520the%2520required%2520semantic%2520information%250Ain%2520a%2520reasonable%2520and%2520robust%2520manner.%2520We%2520solve%2520this%2520challenge%2520by%2520designing%2520a%250Awell-structured%2520framework%2520and%2520leveraging%2520off-the-shelf%2520products%252C%2520such%2520as%2520GPT-4%252C%250Aalong%2520with%2520several%2520specialized%2520tools%2520for%2520detection%2520and%2520estimation.%2520Evaluation%250Aresults%2520demonstrate%2520the%2520feasibility%2520and%2520effectiveness%2520of%2520the%2520proposed%2520user%250Acentric%2520semantic%2520communication%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User%20Centric%20Semantic%20Communications&entry.906535625=Xunze%20Liu%20and%20Yifei%20Sun%20and%20Zhaorui%20Wang%20and%20Lizhao%20You%20and%20Haoyuan%20Pan%20and%20Fangxin%20Wang%20and%20Shuguang%20Cui&entry.1292438233=%20%20Current%20studies%20on%20semantic%20communications%20mainly%20focus%20on%20efficiently%0Aextracting%20semantic%20information%20to%20reduce%20bandwidth%20usage%20between%20a%20transmitter%0Aand%20a%20user.%20Although%20significant%20process%20has%20been%20made%20in%20the%20semantic%0Acommunications%2C%20a%20fundamental%20design%20problem%20is%20that%20the%20semantic%20information%0Ais%20extracted%20based%20on%20certain%20criteria%20at%20the%20transmitter%20side%20along%2C%20without%0Aconsidering%20the%20user%27s%20actual%20requirements.%20As%20a%20result%2C%20critical%20information%0Athat%20is%20of%20primary%20concern%20to%20the%20user%20may%20be%20lost.%20In%20such%20cases%2C%20the%20semantic%0Atransmission%20becomes%20meaningless%20to%20the%20user%2C%20as%20all%20received%20information%20is%0Airrelevant%20to%20the%20user%27s%20interests.%20To%20solve%20this%20problem%2C%20this%20paper%20presents%0Aa%20user%20centric%20semantic%20communication%20system%2C%20where%20the%20user%20sends%20its%20request%0Afor%20the%20desired%20semantic%20information%20to%20the%20transmitter%20at%20the%20start%20of%20each%0Atransmission.%20Then%2C%20the%20transmitter%20extracts%20the%20required%20semantic%20information%0Aaccordingly.%20A%20key%20challenge%20is%20how%20the%20transmitter%20can%20understand%20the%20user%27s%0Arequests%20for%20semantic%20information%20and%20extract%20the%20required%20semantic%20information%0Ain%20a%20reasonable%20and%20robust%20manner.%20We%20solve%20this%20challenge%20by%20designing%20a%0Awell-structured%20framework%20and%20leveraging%20off-the-shelf%20products%2C%20such%20as%20GPT-4%2C%0Aalong%20with%20several%20specialized%20tools%20for%20detection%20and%20estimation.%20Evaluation%0Aresults%20demonstrate%20the%20feasibility%20and%20effectiveness%20of%20the%20proposed%20user%0Acentric%20semantic%20communication%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03127v1&entry.124074799=Read"},
{"title": "DiT4Edit: Diffusion Transformer for Image Editing", "author": "Kunyu Feng and Yue Ma and Bingyuan Wang and Chenyang Qi and Haozhe Chen and Qifeng Chen and Zeyu Wang", "abstract": "  Despite recent advances in UNet-based image editing, methods for shape-aware\nobject editing in high-resolution images are still lacking. Compared to UNet,\nDiffusion Transformers (DiT) demonstrate superior capabilities to effectively\ncapture the long-range dependencies among patches, leading to higher-quality\nimage generation. In this paper, we propose DiT4Edit, the first Diffusion\nTransformer-based image editing framework. Specifically, DiT4Edit uses the\nDPM-Solver inversion algorithm to obtain the inverted latents, reducing the\nnumber of steps compared to the DDIM inversion algorithm commonly used in\nUNet-based frameworks. Additionally, we design unified attention control and\npatches merging, tailored for transformer computation streams. This integration\nallows our framework to generate higher-quality edited images faster. Our\ndesign leverages the advantages of DiT, enabling it to surpass UNet structures\nin image editing, especially in high-resolution and arbitrary-size images.\nExtensive experiments demonstrate the strong performance of DiT4Edit across\nvarious editing scenarios, highlighting the potential of Diffusion Transformers\nin supporting image editing.\n", "link": "http://arxiv.org/abs/2411.03286v1", "date": "2024-11-05", "relevancy": 1.8003, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6346}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6238}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiT4Edit%3A%20Diffusion%20Transformer%20for%20Image%20Editing&body=Title%3A%20DiT4Edit%3A%20Diffusion%20Transformer%20for%20Image%20Editing%0AAuthor%3A%20Kunyu%20Feng%20and%20Yue%20Ma%20and%20Bingyuan%20Wang%20and%20Chenyang%20Qi%20and%20Haozhe%20Chen%20and%20Qifeng%20Chen%20and%20Zeyu%20Wang%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20UNet-based%20image%20editing%2C%20methods%20for%20shape-aware%0Aobject%20editing%20in%20high-resolution%20images%20are%20still%20lacking.%20Compared%20to%20UNet%2C%0ADiffusion%20Transformers%20%28DiT%29%20demonstrate%20superior%20capabilities%20to%20effectively%0Acapture%20the%20long-range%20dependencies%20among%20patches%2C%20leading%20to%20higher-quality%0Aimage%20generation.%20In%20this%20paper%2C%20we%20propose%20DiT4Edit%2C%20the%20first%20Diffusion%0ATransformer-based%20image%20editing%20framework.%20Specifically%2C%20DiT4Edit%20uses%20the%0ADPM-Solver%20inversion%20algorithm%20to%20obtain%20the%20inverted%20latents%2C%20reducing%20the%0Anumber%20of%20steps%20compared%20to%20the%20DDIM%20inversion%20algorithm%20commonly%20used%20in%0AUNet-based%20frameworks.%20Additionally%2C%20we%20design%20unified%20attention%20control%20and%0Apatches%20merging%2C%20tailored%20for%20transformer%20computation%20streams.%20This%20integration%0Aallows%20our%20framework%20to%20generate%20higher-quality%20edited%20images%20faster.%20Our%0Adesign%20leverages%20the%20advantages%20of%20DiT%2C%20enabling%20it%20to%20surpass%20UNet%20structures%0Ain%20image%20editing%2C%20especially%20in%20high-resolution%20and%20arbitrary-size%20images.%0AExtensive%20experiments%20demonstrate%20the%20strong%20performance%20of%20DiT4Edit%20across%0Avarious%20editing%20scenarios%2C%20highlighting%20the%20potential%20of%20Diffusion%20Transformers%0Ain%20supporting%20image%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiT4Edit%253A%2520Diffusion%2520Transformer%2520for%2520Image%2520Editing%26entry.906535625%3DKunyu%2520Feng%2520and%2520Yue%2520Ma%2520and%2520Bingyuan%2520Wang%2520and%2520Chenyang%2520Qi%2520and%2520Haozhe%2520Chen%2520and%2520Qifeng%2520Chen%2520and%2520Zeyu%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520UNet-based%2520image%2520editing%252C%2520methods%2520for%2520shape-aware%250Aobject%2520editing%2520in%2520high-resolution%2520images%2520are%2520still%2520lacking.%2520Compared%2520to%2520UNet%252C%250ADiffusion%2520Transformers%2520%2528DiT%2529%2520demonstrate%2520superior%2520capabilities%2520to%2520effectively%250Acapture%2520the%2520long-range%2520dependencies%2520among%2520patches%252C%2520leading%2520to%2520higher-quality%250Aimage%2520generation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DiT4Edit%252C%2520the%2520first%2520Diffusion%250ATransformer-based%2520image%2520editing%2520framework.%2520Specifically%252C%2520DiT4Edit%2520uses%2520the%250ADPM-Solver%2520inversion%2520algorithm%2520to%2520obtain%2520the%2520inverted%2520latents%252C%2520reducing%2520the%250Anumber%2520of%2520steps%2520compared%2520to%2520the%2520DDIM%2520inversion%2520algorithm%2520commonly%2520used%2520in%250AUNet-based%2520frameworks.%2520Additionally%252C%2520we%2520design%2520unified%2520attention%2520control%2520and%250Apatches%2520merging%252C%2520tailored%2520for%2520transformer%2520computation%2520streams.%2520This%2520integration%250Aallows%2520our%2520framework%2520to%2520generate%2520higher-quality%2520edited%2520images%2520faster.%2520Our%250Adesign%2520leverages%2520the%2520advantages%2520of%2520DiT%252C%2520enabling%2520it%2520to%2520surpass%2520UNet%2520structures%250Ain%2520image%2520editing%252C%2520especially%2520in%2520high-resolution%2520and%2520arbitrary-size%2520images.%250AExtensive%2520experiments%2520demonstrate%2520the%2520strong%2520performance%2520of%2520DiT4Edit%2520across%250Avarious%2520editing%2520scenarios%252C%2520highlighting%2520the%2520potential%2520of%2520Diffusion%2520Transformers%250Ain%2520supporting%2520image%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiT4Edit%3A%20Diffusion%20Transformer%20for%20Image%20Editing&entry.906535625=Kunyu%20Feng%20and%20Yue%20Ma%20and%20Bingyuan%20Wang%20and%20Chenyang%20Qi%20and%20Haozhe%20Chen%20and%20Qifeng%20Chen%20and%20Zeyu%20Wang&entry.1292438233=%20%20Despite%20recent%20advances%20in%20UNet-based%20image%20editing%2C%20methods%20for%20shape-aware%0Aobject%20editing%20in%20high-resolution%20images%20are%20still%20lacking.%20Compared%20to%20UNet%2C%0ADiffusion%20Transformers%20%28DiT%29%20demonstrate%20superior%20capabilities%20to%20effectively%0Acapture%20the%20long-range%20dependencies%20among%20patches%2C%20leading%20to%20higher-quality%0Aimage%20generation.%20In%20this%20paper%2C%20we%20propose%20DiT4Edit%2C%20the%20first%20Diffusion%0ATransformer-based%20image%20editing%20framework.%20Specifically%2C%20DiT4Edit%20uses%20the%0ADPM-Solver%20inversion%20algorithm%20to%20obtain%20the%20inverted%20latents%2C%20reducing%20the%0Anumber%20of%20steps%20compared%20to%20the%20DDIM%20inversion%20algorithm%20commonly%20used%20in%0AUNet-based%20frameworks.%20Additionally%2C%20we%20design%20unified%20attention%20control%20and%0Apatches%20merging%2C%20tailored%20for%20transformer%20computation%20streams.%20This%20integration%0Aallows%20our%20framework%20to%20generate%20higher-quality%20edited%20images%20faster.%20Our%0Adesign%20leverages%20the%20advantages%20of%20DiT%2C%20enabling%20it%20to%20surpass%20UNet%20structures%0Ain%20image%20editing%2C%20especially%20in%20high-resolution%20and%20arbitrary-size%20images.%0AExtensive%20experiments%20demonstrate%20the%20strong%20performance%20of%20DiT4Edit%20across%0Avarious%20editing%20scenarios%2C%20highlighting%20the%20potential%20of%20Diffusion%20Transformers%0Ain%20supporting%20image%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03286v1&entry.124074799=Read"},
{"title": "Dimension-free deterministic equivalents and scaling laws for random\n  feature regression", "author": "Leonardo Defilippis and Bruno Loureiro and Theodor Misiakiewicz", "abstract": "  In this work we investigate the generalization performance of random feature\nridge regression (RFRR). Our main contribution is a general deterministic\nequivalent for the test error of RFRR. Specifically, under a certain\nconcentration property, we show that the test error is well approximated by a\nclosed-form expression that only depends on the feature map eigenvalues.\nNotably, our approximation guarantee is non-asymptotic, multiplicative, and\nindependent of the feature map dimension -- allowing for infinite-dimensional\nfeatures. We expect this deterministic equivalent to hold broadly beyond our\ntheoretical analysis, and we empirically validate its predictions on various\nreal and synthetic datasets. As an application, we derive sharp excess error\nrates under standard power-law assumptions of the spectrum and target decay. In\nparticular, we provide a tight result for the smallest number of features\nachieving optimal minimax error rate.\n", "link": "http://arxiv.org/abs/2405.15699v3", "date": "2024-11-05", "relevancy": 1.7877, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4407}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension-free%20deterministic%20equivalents%20and%20scaling%20laws%20for%20random%0A%20%20feature%20regression&body=Title%3A%20Dimension-free%20deterministic%20equivalents%20and%20scaling%20laws%20for%20random%0A%20%20feature%20regression%0AAuthor%3A%20Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Theodor%20Misiakiewicz%0AAbstract%3A%20%20%20In%20this%20work%20we%20investigate%20the%20generalization%20performance%20of%20random%20feature%0Aridge%20regression%20%28RFRR%29.%20Our%20main%20contribution%20is%20a%20general%20deterministic%0Aequivalent%20for%20the%20test%20error%20of%20RFRR.%20Specifically%2C%20under%20a%20certain%0Aconcentration%20property%2C%20we%20show%20that%20the%20test%20error%20is%20well%20approximated%20by%20a%0Aclosed-form%20expression%20that%20only%20depends%20on%20the%20feature%20map%20eigenvalues.%0ANotably%2C%20our%20approximation%20guarantee%20is%20non-asymptotic%2C%20multiplicative%2C%20and%0Aindependent%20of%20the%20feature%20map%20dimension%20--%20allowing%20for%20infinite-dimensional%0Afeatures.%20We%20expect%20this%20deterministic%20equivalent%20to%20hold%20broadly%20beyond%20our%0Atheoretical%20analysis%2C%20and%20we%20empirically%20validate%20its%20predictions%20on%20various%0Areal%20and%20synthetic%20datasets.%20As%20an%20application%2C%20we%20derive%20sharp%20excess%20error%0Arates%20under%20standard%20power-law%20assumptions%20of%20the%20spectrum%20and%20target%20decay.%20In%0Aparticular%2C%20we%20provide%20a%20tight%20result%20for%20the%20smallest%20number%20of%20features%0Aachieving%20optimal%20minimax%20error%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15699v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension-free%2520deterministic%2520equivalents%2520and%2520scaling%2520laws%2520for%2520random%250A%2520%2520feature%2520regression%26entry.906535625%3DLeonardo%2520Defilippis%2520and%2520Bruno%2520Loureiro%2520and%2520Theodor%2520Misiakiewicz%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520investigate%2520the%2520generalization%2520performance%2520of%2520random%2520feature%250Aridge%2520regression%2520%2528RFRR%2529.%2520Our%2520main%2520contribution%2520is%2520a%2520general%2520deterministic%250Aequivalent%2520for%2520the%2520test%2520error%2520of%2520RFRR.%2520Specifically%252C%2520under%2520a%2520certain%250Aconcentration%2520property%252C%2520we%2520show%2520that%2520the%2520test%2520error%2520is%2520well%2520approximated%2520by%2520a%250Aclosed-form%2520expression%2520that%2520only%2520depends%2520on%2520the%2520feature%2520map%2520eigenvalues.%250ANotably%252C%2520our%2520approximation%2520guarantee%2520is%2520non-asymptotic%252C%2520multiplicative%252C%2520and%250Aindependent%2520of%2520the%2520feature%2520map%2520dimension%2520--%2520allowing%2520for%2520infinite-dimensional%250Afeatures.%2520We%2520expect%2520this%2520deterministic%2520equivalent%2520to%2520hold%2520broadly%2520beyond%2520our%250Atheoretical%2520analysis%252C%2520and%2520we%2520empirically%2520validate%2520its%2520predictions%2520on%2520various%250Areal%2520and%2520synthetic%2520datasets.%2520As%2520an%2520application%252C%2520we%2520derive%2520sharp%2520excess%2520error%250Arates%2520under%2520standard%2520power-law%2520assumptions%2520of%2520the%2520spectrum%2520and%2520target%2520decay.%2520In%250Aparticular%252C%2520we%2520provide%2520a%2520tight%2520result%2520for%2520the%2520smallest%2520number%2520of%2520features%250Aachieving%2520optimal%2520minimax%2520error%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15699v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension-free%20deterministic%20equivalents%20and%20scaling%20laws%20for%20random%0A%20%20feature%20regression&entry.906535625=Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Theodor%20Misiakiewicz&entry.1292438233=%20%20In%20this%20work%20we%20investigate%20the%20generalization%20performance%20of%20random%20feature%0Aridge%20regression%20%28RFRR%29.%20Our%20main%20contribution%20is%20a%20general%20deterministic%0Aequivalent%20for%20the%20test%20error%20of%20RFRR.%20Specifically%2C%20under%20a%20certain%0Aconcentration%20property%2C%20we%20show%20that%20the%20test%20error%20is%20well%20approximated%20by%20a%0Aclosed-form%20expression%20that%20only%20depends%20on%20the%20feature%20map%20eigenvalues.%0ANotably%2C%20our%20approximation%20guarantee%20is%20non-asymptotic%2C%20multiplicative%2C%20and%0Aindependent%20of%20the%20feature%20map%20dimension%20--%20allowing%20for%20infinite-dimensional%0Afeatures.%20We%20expect%20this%20deterministic%20equivalent%20to%20hold%20broadly%20beyond%20our%0Atheoretical%20analysis%2C%20and%20we%20empirically%20validate%20its%20predictions%20on%20various%0Areal%20and%20synthetic%20datasets.%20As%20an%20application%2C%20we%20derive%20sharp%20excess%20error%0Arates%20under%20standard%20power-law%20assumptions%20of%20the%20spectrum%20and%20target%20decay.%20In%0Aparticular%2C%20we%20provide%20a%20tight%20result%20for%20the%20smallest%20number%20of%20features%0Aachieving%20optimal%20minimax%20error%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15699v3&entry.124074799=Read"},
{"title": "Blind Image Restoration via Fast Diffusion Inversion", "author": "Hamadi Chihaoui and Abdelhak Lemkhenter and Paolo Favaro", "abstract": "  Image Restoration (IR) methods based on a pre-trained diffusion model have\ndemonstrated state-of-the-art performance. However, they have two fundamental\nlimitations: 1) they often assume that the degradation operator is completely\nknown and 2) they alter the diffusion sampling process, which may result in\nrestored images that do not lie onto the data manifold. To address these\nissues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD)\na blind IR method that jointly optimizes for the degradation model parameters\nand the restored image. To ensure that the restored images lie onto the data\nmanifold, we propose a novel sampling technique on a pre-trained diffusion\nmodel. A key idea in our method is not to modify the reverse sampling, i.e, not\nto alter all the intermediate latents, once an initial noise is sampled. This\nis ultimately equivalent to casting the IR task as an optimization problem in\nthe space of the input noise. Moreover, to mitigate the computational cost\nassociated with inverting a fully unrolled diffusion model, we leverage the\ninherent capability of these models to skip ahead in the forward diffusion\nprocess using large time steps. We experimentally validate BIRD on several\nimage restoration tasks and show that it achieves state of the art performance\non all of them. Our code is available at\nhttps://github.com/hamadichihaoui/BIRD.\n", "link": "http://arxiv.org/abs/2405.19572v2", "date": "2024-11-05", "relevancy": 1.7738, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6096}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.594}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blind%20Image%20Restoration%20via%20Fast%20Diffusion%20Inversion&body=Title%3A%20Blind%20Image%20Restoration%20via%20Fast%20Diffusion%20Inversion%0AAuthor%3A%20Hamadi%20Chihaoui%20and%20Abdelhak%20Lemkhenter%20and%20Paolo%20Favaro%0AAbstract%3A%20%20%20Image%20Restoration%20%28IR%29%20methods%20based%20on%20a%20pre-trained%20diffusion%20model%20have%0Ademonstrated%20state-of-the-art%20performance.%20However%2C%20they%20have%20two%20fundamental%0Alimitations%3A%201%29%20they%20often%20assume%20that%20the%20degradation%20operator%20is%20completely%0Aknown%20and%202%29%20they%20alter%20the%20diffusion%20sampling%20process%2C%20which%20may%20result%20in%0Arestored%20images%20that%20do%20not%20lie%20onto%20the%20data%20manifold.%20To%20address%20these%0Aissues%2C%20we%20propose%20Blind%20Image%20Restoration%20via%20fast%20Diffusion%20inversion%20%28BIRD%29%0Aa%20blind%20IR%20method%20that%20jointly%20optimizes%20for%20the%20degradation%20model%20parameters%0Aand%20the%20restored%20image.%20To%20ensure%20that%20the%20restored%20images%20lie%20onto%20the%20data%0Amanifold%2C%20we%20propose%20a%20novel%20sampling%20technique%20on%20a%20pre-trained%20diffusion%0Amodel.%20A%20key%20idea%20in%20our%20method%20is%20not%20to%20modify%20the%20reverse%20sampling%2C%20i.e%2C%20not%0Ato%20alter%20all%20the%20intermediate%20latents%2C%20once%20an%20initial%20noise%20is%20sampled.%20This%0Ais%20ultimately%20equivalent%20to%20casting%20the%20IR%20task%20as%20an%20optimization%20problem%20in%0Athe%20space%20of%20the%20input%20noise.%20Moreover%2C%20to%20mitigate%20the%20computational%20cost%0Aassociated%20with%20inverting%20a%20fully%20unrolled%20diffusion%20model%2C%20we%20leverage%20the%0Ainherent%20capability%20of%20these%20models%20to%20skip%20ahead%20in%20the%20forward%20diffusion%0Aprocess%20using%20large%20time%20steps.%20We%20experimentally%20validate%20BIRD%20on%20several%0Aimage%20restoration%20tasks%20and%20show%20that%20it%20achieves%20state%20of%20the%20art%20performance%0Aon%20all%20of%20them.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/hamadichihaoui/BIRD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlind%2520Image%2520Restoration%2520via%2520Fast%2520Diffusion%2520Inversion%26entry.906535625%3DHamadi%2520Chihaoui%2520and%2520Abdelhak%2520Lemkhenter%2520and%2520Paolo%2520Favaro%26entry.1292438233%3D%2520%2520Image%2520Restoration%2520%2528IR%2529%2520methods%2520based%2520on%2520a%2520pre-trained%2520diffusion%2520model%2520have%250Ademonstrated%2520state-of-the-art%2520performance.%2520However%252C%2520they%2520have%2520two%2520fundamental%250Alimitations%253A%25201%2529%2520they%2520often%2520assume%2520that%2520the%2520degradation%2520operator%2520is%2520completely%250Aknown%2520and%25202%2529%2520they%2520alter%2520the%2520diffusion%2520sampling%2520process%252C%2520which%2520may%2520result%2520in%250Arestored%2520images%2520that%2520do%2520not%2520lie%2520onto%2520the%2520data%2520manifold.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Blind%2520Image%2520Restoration%2520via%2520fast%2520Diffusion%2520inversion%2520%2528BIRD%2529%250Aa%2520blind%2520IR%2520method%2520that%2520jointly%2520optimizes%2520for%2520the%2520degradation%2520model%2520parameters%250Aand%2520the%2520restored%2520image.%2520To%2520ensure%2520that%2520the%2520restored%2520images%2520lie%2520onto%2520the%2520data%250Amanifold%252C%2520we%2520propose%2520a%2520novel%2520sampling%2520technique%2520on%2520a%2520pre-trained%2520diffusion%250Amodel.%2520A%2520key%2520idea%2520in%2520our%2520method%2520is%2520not%2520to%2520modify%2520the%2520reverse%2520sampling%252C%2520i.e%252C%2520not%250Ato%2520alter%2520all%2520the%2520intermediate%2520latents%252C%2520once%2520an%2520initial%2520noise%2520is%2520sampled.%2520This%250Ais%2520ultimately%2520equivalent%2520to%2520casting%2520the%2520IR%2520task%2520as%2520an%2520optimization%2520problem%2520in%250Athe%2520space%2520of%2520the%2520input%2520noise.%2520Moreover%252C%2520to%2520mitigate%2520the%2520computational%2520cost%250Aassociated%2520with%2520inverting%2520a%2520fully%2520unrolled%2520diffusion%2520model%252C%2520we%2520leverage%2520the%250Ainherent%2520capability%2520of%2520these%2520models%2520to%2520skip%2520ahead%2520in%2520the%2520forward%2520diffusion%250Aprocess%2520using%2520large%2520time%2520steps.%2520We%2520experimentally%2520validate%2520BIRD%2520on%2520several%250Aimage%2520restoration%2520tasks%2520and%2520show%2520that%2520it%2520achieves%2520state%2520of%2520the%2520art%2520performance%250Aon%2520all%2520of%2520them.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/hamadichihaoui/BIRD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Image%20Restoration%20via%20Fast%20Diffusion%20Inversion&entry.906535625=Hamadi%20Chihaoui%20and%20Abdelhak%20Lemkhenter%20and%20Paolo%20Favaro&entry.1292438233=%20%20Image%20Restoration%20%28IR%29%20methods%20based%20on%20a%20pre-trained%20diffusion%20model%20have%0Ademonstrated%20state-of-the-art%20performance.%20However%2C%20they%20have%20two%20fundamental%0Alimitations%3A%201%29%20they%20often%20assume%20that%20the%20degradation%20operator%20is%20completely%0Aknown%20and%202%29%20they%20alter%20the%20diffusion%20sampling%20process%2C%20which%20may%20result%20in%0Arestored%20images%20that%20do%20not%20lie%20onto%20the%20data%20manifold.%20To%20address%20these%0Aissues%2C%20we%20propose%20Blind%20Image%20Restoration%20via%20fast%20Diffusion%20inversion%20%28BIRD%29%0Aa%20blind%20IR%20method%20that%20jointly%20optimizes%20for%20the%20degradation%20model%20parameters%0Aand%20the%20restored%20image.%20To%20ensure%20that%20the%20restored%20images%20lie%20onto%20the%20data%0Amanifold%2C%20we%20propose%20a%20novel%20sampling%20technique%20on%20a%20pre-trained%20diffusion%0Amodel.%20A%20key%20idea%20in%20our%20method%20is%20not%20to%20modify%20the%20reverse%20sampling%2C%20i.e%2C%20not%0Ato%20alter%20all%20the%20intermediate%20latents%2C%20once%20an%20initial%20noise%20is%20sampled.%20This%0Ais%20ultimately%20equivalent%20to%20casting%20the%20IR%20task%20as%20an%20optimization%20problem%20in%0Athe%20space%20of%20the%20input%20noise.%20Moreover%2C%20to%20mitigate%20the%20computational%20cost%0Aassociated%20with%20inverting%20a%20fully%20unrolled%20diffusion%20model%2C%20we%20leverage%20the%0Ainherent%20capability%20of%20these%20models%20to%20skip%20ahead%20in%20the%20forward%20diffusion%0Aprocess%20using%20large%20time%20steps.%20We%20experimentally%20validate%20BIRD%20on%20several%0Aimage%20restoration%20tasks%20and%20show%20that%20it%20achieves%20state%20of%20the%20art%20performance%0Aon%20all%20of%20them.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/hamadichihaoui/BIRD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19572v2&entry.124074799=Read"},
{"title": "UNet: A Generic and Reliable Multi-UAV Communication and Networking\n  Architecture for Heterogeneous Applications", "author": "Sanku Kumar Roy and Mohamed Samshad and Ketan Rajawat", "abstract": "  The rapid growth of UAV applications necessitates a robust communication and\nnetworking architecture capable of addressing the diverse requirements of\nvarious applications concurrently, rather than relying on application-specific\nsolutions. This paper proposes a generic and reliable multi-UAV communication\nand networking architecture designed to support the varying demands of\nheterogeneous applications, including short-range and long-range communication,\nstar and mesh topologies, different data rates, and multiple wireless\nstandards. Our architecture accommodates both adhoc and infrastructure\nnetworks, ensuring seamless connectivity throughout the network. Additionally,\nwe present the design of a multi-protocol UAV gateway that enables\ninteroperability among various communication protocols. Furthermore, we\nintroduce a data processing and service layer framework with a graphical user\ninterface of a ground control station that facilitates remote control and\nmonitoring from any location at any time. We practically implemented the\nproposed architecture and evaluated its performance using different metrics,\ndemonstrating its effectiveness.\n", "link": "http://arxiv.org/abs/2411.03048v1", "date": "2024-11-05", "relevancy": 1.7629, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4156}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNet%3A%20A%20Generic%20and%20Reliable%20Multi-UAV%20Communication%20and%20Networking%0A%20%20Architecture%20for%20Heterogeneous%20Applications&body=Title%3A%20UNet%3A%20A%20Generic%20and%20Reliable%20Multi-UAV%20Communication%20and%20Networking%0A%20%20Architecture%20for%20Heterogeneous%20Applications%0AAuthor%3A%20Sanku%20Kumar%20Roy%20and%20Mohamed%20Samshad%20and%20Ketan%20Rajawat%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20UAV%20applications%20necessitates%20a%20robust%20communication%20and%0Anetworking%20architecture%20capable%20of%20addressing%20the%20diverse%20requirements%20of%0Avarious%20applications%20concurrently%2C%20rather%20than%20relying%20on%20application-specific%0Asolutions.%20This%20paper%20proposes%20a%20generic%20and%20reliable%20multi-UAV%20communication%0Aand%20networking%20architecture%20designed%20to%20support%20the%20varying%20demands%20of%0Aheterogeneous%20applications%2C%20including%20short-range%20and%20long-range%20communication%2C%0Astar%20and%20mesh%20topologies%2C%20different%20data%20rates%2C%20and%20multiple%20wireless%0Astandards.%20Our%20architecture%20accommodates%20both%20adhoc%20and%20infrastructure%0Anetworks%2C%20ensuring%20seamless%20connectivity%20throughout%20the%20network.%20Additionally%2C%0Awe%20present%20the%20design%20of%20a%20multi-protocol%20UAV%20gateway%20that%20enables%0Ainteroperability%20among%20various%20communication%20protocols.%20Furthermore%2C%20we%0Aintroduce%20a%20data%20processing%20and%20service%20layer%20framework%20with%20a%20graphical%20user%0Ainterface%20of%20a%20ground%20control%20station%20that%20facilitates%20remote%20control%20and%0Amonitoring%20from%20any%20location%20at%20any%20time.%20We%20practically%20implemented%20the%0Aproposed%20architecture%20and%20evaluated%20its%20performance%20using%20different%20metrics%2C%0Ademonstrating%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNet%253A%2520A%2520Generic%2520and%2520Reliable%2520Multi-UAV%2520Communication%2520and%2520Networking%250A%2520%2520Architecture%2520for%2520Heterogeneous%2520Applications%26entry.906535625%3DSanku%2520Kumar%2520Roy%2520and%2520Mohamed%2520Samshad%2520and%2520Ketan%2520Rajawat%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520UAV%2520applications%2520necessitates%2520a%2520robust%2520communication%2520and%250Anetworking%2520architecture%2520capable%2520of%2520addressing%2520the%2520diverse%2520requirements%2520of%250Avarious%2520applications%2520concurrently%252C%2520rather%2520than%2520relying%2520on%2520application-specific%250Asolutions.%2520This%2520paper%2520proposes%2520a%2520generic%2520and%2520reliable%2520multi-UAV%2520communication%250Aand%2520networking%2520architecture%2520designed%2520to%2520support%2520the%2520varying%2520demands%2520of%250Aheterogeneous%2520applications%252C%2520including%2520short-range%2520and%2520long-range%2520communication%252C%250Astar%2520and%2520mesh%2520topologies%252C%2520different%2520data%2520rates%252C%2520and%2520multiple%2520wireless%250Astandards.%2520Our%2520architecture%2520accommodates%2520both%2520adhoc%2520and%2520infrastructure%250Anetworks%252C%2520ensuring%2520seamless%2520connectivity%2520throughout%2520the%2520network.%2520Additionally%252C%250Awe%2520present%2520the%2520design%2520of%2520a%2520multi-protocol%2520UAV%2520gateway%2520that%2520enables%250Ainteroperability%2520among%2520various%2520communication%2520protocols.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520data%2520processing%2520and%2520service%2520layer%2520framework%2520with%2520a%2520graphical%2520user%250Ainterface%2520of%2520a%2520ground%2520control%2520station%2520that%2520facilitates%2520remote%2520control%2520and%250Amonitoring%2520from%2520any%2520location%2520at%2520any%2520time.%2520We%2520practically%2520implemented%2520the%250Aproposed%2520architecture%2520and%2520evaluated%2520its%2520performance%2520using%2520different%2520metrics%252C%250Ademonstrating%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNet%3A%20A%20Generic%20and%20Reliable%20Multi-UAV%20Communication%20and%20Networking%0A%20%20Architecture%20for%20Heterogeneous%20Applications&entry.906535625=Sanku%20Kumar%20Roy%20and%20Mohamed%20Samshad%20and%20Ketan%20Rajawat&entry.1292438233=%20%20The%20rapid%20growth%20of%20UAV%20applications%20necessitates%20a%20robust%20communication%20and%0Anetworking%20architecture%20capable%20of%20addressing%20the%20diverse%20requirements%20of%0Avarious%20applications%20concurrently%2C%20rather%20than%20relying%20on%20application-specific%0Asolutions.%20This%20paper%20proposes%20a%20generic%20and%20reliable%20multi-UAV%20communication%0Aand%20networking%20architecture%20designed%20to%20support%20the%20varying%20demands%20of%0Aheterogeneous%20applications%2C%20including%20short-range%20and%20long-range%20communication%2C%0Astar%20and%20mesh%20topologies%2C%20different%20data%20rates%2C%20and%20multiple%20wireless%0Astandards.%20Our%20architecture%20accommodates%20both%20adhoc%20and%20infrastructure%0Anetworks%2C%20ensuring%20seamless%20connectivity%20throughout%20the%20network.%20Additionally%2C%0Awe%20present%20the%20design%20of%20a%20multi-protocol%20UAV%20gateway%20that%20enables%0Ainteroperability%20among%20various%20communication%20protocols.%20Furthermore%2C%20we%0Aintroduce%20a%20data%20processing%20and%20service%20layer%20framework%20with%20a%20graphical%20user%0Ainterface%20of%20a%20ground%20control%20station%20that%20facilitates%20remote%20control%20and%0Amonitoring%20from%20any%20location%20at%20any%20time.%20We%20practically%20implemented%20the%0Aproposed%20architecture%20and%20evaluated%20its%20performance%20using%20different%20metrics%2C%0Ademonstrating%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03048v1&entry.124074799=Read"},
{"title": "Practical hybrid PQC-QKD protocols with enhanced security and\n  performance", "author": "Pei Zeng and Debayan Bandyopadhyay and Jos\u00e9 A. M\u00e9ndez M\u00e9ndez and Nolan Bitner and Alexander Kolar and Michael T. Solomon and Ziyu Ye and Filip Rozp\u0229dek and Tian Zhong and F. Joseph Heremans and David D. Awschalom and Liang Jiang and Junyu Liu", "abstract": "  Quantum resistance is vital for emerging cryptographic systems as quantum\ntechnologies continue to advance towards large-scale, fault-tolerant quantum\ncomputers. Resistance may be offered by quantum key distribution (QKD), which\nprovides information-theoretic security using quantum states of photons, but\nmay be limited by transmission loss at long distances. An alternative approach\nuses classical means and is conjectured to be resistant to quantum attacks,\nso-called post-quantum cryptography (PQC), but it is yet to be rigorously\nproven, and its current implementations are computationally expensive. To\novercome the security and performance challenges present in each, here we\ndevelop hybrid protocols by which QKD and PQC inter-operate within a joint\nquantum-classical network. In particular, we consider different hybrid designs\nthat may offer enhanced speed and/or security over the individual performance\nof either approach. Furthermore, we present a method for analyzing the security\nof hybrid protocols in key distribution networks. Our hybrid approach paves the\nway for joint quantum-classical communication networks, which leverage the\nadvantages of both QKD and PQC and can be tailored to the requirements of\nvarious practical networks.\n", "link": "http://arxiv.org/abs/2411.01086v2", "date": "2024-11-05", "relevancy": 1.1259, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3739}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20hybrid%20PQC-QKD%20protocols%20with%20enhanced%20security%20and%0A%20%20performance&body=Title%3A%20Practical%20hybrid%20PQC-QKD%20protocols%20with%20enhanced%20security%20and%0A%20%20performance%0AAuthor%3A%20Pei%20Zeng%20and%20Debayan%20Bandyopadhyay%20and%20Jos%C3%A9%20A.%20M%C3%A9ndez%20M%C3%A9ndez%20and%20Nolan%20Bitner%20and%20Alexander%20Kolar%20and%20Michael%20T.%20Solomon%20and%20Ziyu%20Ye%20and%20Filip%20Rozp%C8%A9dek%20and%20Tian%20Zhong%20and%20F.%20Joseph%20Heremans%20and%20David%20D.%20Awschalom%20and%20Liang%20Jiang%20and%20Junyu%20Liu%0AAbstract%3A%20%20%20Quantum%20resistance%20is%20vital%20for%20emerging%20cryptographic%20systems%20as%20quantum%0Atechnologies%20continue%20to%20advance%20towards%20large-scale%2C%20fault-tolerant%20quantum%0Acomputers.%20Resistance%20may%20be%20offered%20by%20quantum%20key%20distribution%20%28QKD%29%2C%20which%0Aprovides%20information-theoretic%20security%20using%20quantum%20states%20of%20photons%2C%20but%0Amay%20be%20limited%20by%20transmission%20loss%20at%20long%20distances.%20An%20alternative%20approach%0Auses%20classical%20means%20and%20is%20conjectured%20to%20be%20resistant%20to%20quantum%20attacks%2C%0Aso-called%20post-quantum%20cryptography%20%28PQC%29%2C%20but%20it%20is%20yet%20to%20be%20rigorously%0Aproven%2C%20and%20its%20current%20implementations%20are%20computationally%20expensive.%20To%0Aovercome%20the%20security%20and%20performance%20challenges%20present%20in%20each%2C%20here%20we%0Adevelop%20hybrid%20protocols%20by%20which%20QKD%20and%20PQC%20inter-operate%20within%20a%20joint%0Aquantum-classical%20network.%20In%20particular%2C%20we%20consider%20different%20hybrid%20designs%0Athat%20may%20offer%20enhanced%20speed%20and/or%20security%20over%20the%20individual%20performance%0Aof%20either%20approach.%20Furthermore%2C%20we%20present%20a%20method%20for%20analyzing%20the%20security%0Aof%20hybrid%20protocols%20in%20key%20distribution%20networks.%20Our%20hybrid%20approach%20paves%20the%0Away%20for%20joint%20quantum-classical%20communication%20networks%2C%20which%20leverage%20the%0Aadvantages%20of%20both%20QKD%20and%20PQC%20and%20can%20be%20tailored%20to%20the%20requirements%20of%0Avarious%20practical%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520hybrid%2520PQC-QKD%2520protocols%2520with%2520enhanced%2520security%2520and%250A%2520%2520performance%26entry.906535625%3DPei%2520Zeng%2520and%2520Debayan%2520Bandyopadhyay%2520and%2520Jos%25C3%25A9%2520A.%2520M%25C3%25A9ndez%2520M%25C3%25A9ndez%2520and%2520Nolan%2520Bitner%2520and%2520Alexander%2520Kolar%2520and%2520Michael%2520T.%2520Solomon%2520and%2520Ziyu%2520Ye%2520and%2520Filip%2520Rozp%25C8%25A9dek%2520and%2520Tian%2520Zhong%2520and%2520F.%2520Joseph%2520Heremans%2520and%2520David%2520D.%2520Awschalom%2520and%2520Liang%2520Jiang%2520and%2520Junyu%2520Liu%26entry.1292438233%3D%2520%2520Quantum%2520resistance%2520is%2520vital%2520for%2520emerging%2520cryptographic%2520systems%2520as%2520quantum%250Atechnologies%2520continue%2520to%2520advance%2520towards%2520large-scale%252C%2520fault-tolerant%2520quantum%250Acomputers.%2520Resistance%2520may%2520be%2520offered%2520by%2520quantum%2520key%2520distribution%2520%2528QKD%2529%252C%2520which%250Aprovides%2520information-theoretic%2520security%2520using%2520quantum%2520states%2520of%2520photons%252C%2520but%250Amay%2520be%2520limited%2520by%2520transmission%2520loss%2520at%2520long%2520distances.%2520An%2520alternative%2520approach%250Auses%2520classical%2520means%2520and%2520is%2520conjectured%2520to%2520be%2520resistant%2520to%2520quantum%2520attacks%252C%250Aso-called%2520post-quantum%2520cryptography%2520%2528PQC%2529%252C%2520but%2520it%2520is%2520yet%2520to%2520be%2520rigorously%250Aproven%252C%2520and%2520its%2520current%2520implementations%2520are%2520computationally%2520expensive.%2520To%250Aovercome%2520the%2520security%2520and%2520performance%2520challenges%2520present%2520in%2520each%252C%2520here%2520we%250Adevelop%2520hybrid%2520protocols%2520by%2520which%2520QKD%2520and%2520PQC%2520inter-operate%2520within%2520a%2520joint%250Aquantum-classical%2520network.%2520In%2520particular%252C%2520we%2520consider%2520different%2520hybrid%2520designs%250Athat%2520may%2520offer%2520enhanced%2520speed%2520and/or%2520security%2520over%2520the%2520individual%2520performance%250Aof%2520either%2520approach.%2520Furthermore%252C%2520we%2520present%2520a%2520method%2520for%2520analyzing%2520the%2520security%250Aof%2520hybrid%2520protocols%2520in%2520key%2520distribution%2520networks.%2520Our%2520hybrid%2520approach%2520paves%2520the%250Away%2520for%2520joint%2520quantum-classical%2520communication%2520networks%252C%2520which%2520leverage%2520the%250Aadvantages%2520of%2520both%2520QKD%2520and%2520PQC%2520and%2520can%2520be%2520tailored%2520to%2520the%2520requirements%2520of%250Avarious%2520practical%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20hybrid%20PQC-QKD%20protocols%20with%20enhanced%20security%20and%0A%20%20performance&entry.906535625=Pei%20Zeng%20and%20Debayan%20Bandyopadhyay%20and%20Jos%C3%A9%20A.%20M%C3%A9ndez%20M%C3%A9ndez%20and%20Nolan%20Bitner%20and%20Alexander%20Kolar%20and%20Michael%20T.%20Solomon%20and%20Ziyu%20Ye%20and%20Filip%20Rozp%C8%A9dek%20and%20Tian%20Zhong%20and%20F.%20Joseph%20Heremans%20and%20David%20D.%20Awschalom%20and%20Liang%20Jiang%20and%20Junyu%20Liu&entry.1292438233=%20%20Quantum%20resistance%20is%20vital%20for%20emerging%20cryptographic%20systems%20as%20quantum%0Atechnologies%20continue%20to%20advance%20towards%20large-scale%2C%20fault-tolerant%20quantum%0Acomputers.%20Resistance%20may%20be%20offered%20by%20quantum%20key%20distribution%20%28QKD%29%2C%20which%0Aprovides%20information-theoretic%20security%20using%20quantum%20states%20of%20photons%2C%20but%0Amay%20be%20limited%20by%20transmission%20loss%20at%20long%20distances.%20An%20alternative%20approach%0Auses%20classical%20means%20and%20is%20conjectured%20to%20be%20resistant%20to%20quantum%20attacks%2C%0Aso-called%20post-quantum%20cryptography%20%28PQC%29%2C%20but%20it%20is%20yet%20to%20be%20rigorously%0Aproven%2C%20and%20its%20current%20implementations%20are%20computationally%20expensive.%20To%0Aovercome%20the%20security%20and%20performance%20challenges%20present%20in%20each%2C%20here%20we%0Adevelop%20hybrid%20protocols%20by%20which%20QKD%20and%20PQC%20inter-operate%20within%20a%20joint%0Aquantum-classical%20network.%20In%20particular%2C%20we%20consider%20different%20hybrid%20designs%0Athat%20may%20offer%20enhanced%20speed%20and/or%20security%20over%20the%20individual%20performance%0Aof%20either%20approach.%20Furthermore%2C%20we%20present%20a%20method%20for%20analyzing%20the%20security%0Aof%20hybrid%20protocols%20in%20key%20distribution%20networks.%20Our%20hybrid%20approach%20paves%20the%0Away%20for%20joint%20quantum-classical%20communication%20networks%2C%20which%20leverage%20the%0Aadvantages%20of%20both%20QKD%20and%20PQC%20and%20can%20be%20tailored%20to%20the%20requirements%20of%0Avarious%20practical%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01086v2&entry.124074799=Read"},
{"title": "Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models", "author": "Anjith George and Sebastien Marcel", "abstract": "  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and the\nadvancement in neural network architectures. However, these large-scale\ndatasets are often collected without explicit consent, raising ethical and\nprivacy concerns. To address this, there have been proposals to use synthetic\ndatasets for training face recognition models. Yet, such models still rely on\nreal data to train the generative models and generally exhibit inferior\nperformance compared to those trained on real datasets. One of these datasets,\nDigiFace, uses a graphics pipeline to generate different identities and\ndifferent intra-class variations without using real data in training the\nmodels. However, the performance of this approach is poor on face recognition\nbenchmarks, possibly due to the lack of realism in the images generated from\nthe graphics pipeline. In this work, we introduce a novel framework for realism\ntransfer aimed at enhancing the realism of synthetically generated face images.\nOur method leverages the large-scale face foundation model, and we adapt the\npipeline for realism enhancement. By integrating the controllable aspects of\nthe graphics pipeline with our realism enhancement technique, we generate a\nlarge amount of realistic variations-combining the advantages of both\napproaches. Our empirical evaluations demonstrate that models trained using our\nenhanced dataset significantly improve the performance of face recognition\nsystems over the baseline. The source code and datasets will be made available\npublicly.\n", "link": "http://arxiv.org/abs/2411.02188v2", "date": "2024-11-05", "relevancy": 1.7179, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.573}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models&body=Title%3A%20Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20The%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%20the%0Apast%20few%20years%2C%20thanks%20to%20the%20large%20amount%20of%20data%20collected%20and%20the%0Aadvancement%20in%20neural%20network%20architectures.%20However%2C%20these%20large-scale%0Adatasets%20are%20often%20collected%20without%20explicit%20consent%2C%20raising%20ethical%20and%0Aprivacy%20concerns.%20To%20address%20this%2C%20there%20have%20been%20proposals%20to%20use%20synthetic%0Adatasets%20for%20training%20face%20recognition%20models.%20Yet%2C%20such%20models%20still%20rely%20on%0Areal%20data%20to%20train%20the%20generative%20models%20and%20generally%20exhibit%20inferior%0Aperformance%20compared%20to%20those%20trained%20on%20real%20datasets.%20One%20of%20these%20datasets%2C%0ADigiFace%2C%20uses%20a%20graphics%20pipeline%20to%20generate%20different%20identities%20and%0Adifferent%20intra-class%20variations%20without%20using%20real%20data%20in%20training%20the%0Amodels.%20However%2C%20the%20performance%20of%20this%20approach%20is%20poor%20on%20face%20recognition%0Abenchmarks%2C%20possibly%20due%20to%20the%20lack%20of%20realism%20in%20the%20images%20generated%20from%0Athe%20graphics%20pipeline.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20for%20realism%0Atransfer%20aimed%20at%20enhancing%20the%20realism%20of%20synthetically%20generated%20face%20images.%0AOur%20method%20leverages%20the%20large-scale%20face%20foundation%20model%2C%20and%20we%20adapt%20the%0Apipeline%20for%20realism%20enhancement.%20By%20integrating%20the%20controllable%20aspects%20of%0Athe%20graphics%20pipeline%20with%20our%20realism%20enhancement%20technique%2C%20we%20generate%20a%0Alarge%20amount%20of%20realistic%20variations-combining%20the%20advantages%20of%20both%0Aapproaches.%20Our%20empirical%20evaluations%20demonstrate%20that%20models%20trained%20using%20our%0Aenhanced%20dataset%20significantly%20improve%20the%20performance%20of%20face%20recognition%0Asystems%20over%20the%20baseline.%20The%20source%20code%20and%20datasets%20will%20be%20made%20available%0Apublicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigi2Real%253A%2520Bridging%2520the%2520Realism%2520Gap%2520in%2520Synthetic%2520Data%2520Face%2520Recognition%250A%2520%2520via%2520Foundation%2520Models%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520The%2520accuracy%2520of%2520face%2520recognition%2520systems%2520has%2520improved%2520significantly%2520in%2520the%250Apast%2520few%2520years%252C%2520thanks%2520to%2520the%2520large%2520amount%2520of%2520data%2520collected%2520and%2520the%250Aadvancement%2520in%2520neural%2520network%2520architectures.%2520However%252C%2520these%2520large-scale%250Adatasets%2520are%2520often%2520collected%2520without%2520explicit%2520consent%252C%2520raising%2520ethical%2520and%250Aprivacy%2520concerns.%2520To%2520address%2520this%252C%2520there%2520have%2520been%2520proposals%2520to%2520use%2520synthetic%250Adatasets%2520for%2520training%2520face%2520recognition%2520models.%2520Yet%252C%2520such%2520models%2520still%2520rely%2520on%250Areal%2520data%2520to%2520train%2520the%2520generative%2520models%2520and%2520generally%2520exhibit%2520inferior%250Aperformance%2520compared%2520to%2520those%2520trained%2520on%2520real%2520datasets.%2520One%2520of%2520these%2520datasets%252C%250ADigiFace%252C%2520uses%2520a%2520graphics%2520pipeline%2520to%2520generate%2520different%2520identities%2520and%250Adifferent%2520intra-class%2520variations%2520without%2520using%2520real%2520data%2520in%2520training%2520the%250Amodels.%2520However%252C%2520the%2520performance%2520of%2520this%2520approach%2520is%2520poor%2520on%2520face%2520recognition%250Abenchmarks%252C%2520possibly%2520due%2520to%2520the%2520lack%2520of%2520realism%2520in%2520the%2520images%2520generated%2520from%250Athe%2520graphics%2520pipeline.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520realism%250Atransfer%2520aimed%2520at%2520enhancing%2520the%2520realism%2520of%2520synthetically%2520generated%2520face%2520images.%250AOur%2520method%2520leverages%2520the%2520large-scale%2520face%2520foundation%2520model%252C%2520and%2520we%2520adapt%2520the%250Apipeline%2520for%2520realism%2520enhancement.%2520By%2520integrating%2520the%2520controllable%2520aspects%2520of%250Athe%2520graphics%2520pipeline%2520with%2520our%2520realism%2520enhancement%2520technique%252C%2520we%2520generate%2520a%250Alarge%2520amount%2520of%2520realistic%2520variations-combining%2520the%2520advantages%2520of%2520both%250Aapproaches.%2520Our%2520empirical%2520evaluations%2520demonstrate%2520that%2520models%2520trained%2520using%2520our%250Aenhanced%2520dataset%2520significantly%2520improve%2520the%2520performance%2520of%2520face%2520recognition%250Asystems%2520over%2520the%2520baseline.%2520The%2520source%2520code%2520and%2520datasets%2520will%2520be%2520made%2520available%250Apublicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20The%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%20the%0Apast%20few%20years%2C%20thanks%20to%20the%20large%20amount%20of%20data%20collected%20and%20the%0Aadvancement%20in%20neural%20network%20architectures.%20However%2C%20these%20large-scale%0Adatasets%20are%20often%20collected%20without%20explicit%20consent%2C%20raising%20ethical%20and%0Aprivacy%20concerns.%20To%20address%20this%2C%20there%20have%20been%20proposals%20to%20use%20synthetic%0Adatasets%20for%20training%20face%20recognition%20models.%20Yet%2C%20such%20models%20still%20rely%20on%0Areal%20data%20to%20train%20the%20generative%20models%20and%20generally%20exhibit%20inferior%0Aperformance%20compared%20to%20those%20trained%20on%20real%20datasets.%20One%20of%20these%20datasets%2C%0ADigiFace%2C%20uses%20a%20graphics%20pipeline%20to%20generate%20different%20identities%20and%0Adifferent%20intra-class%20variations%20without%20using%20real%20data%20in%20training%20the%0Amodels.%20However%2C%20the%20performance%20of%20this%20approach%20is%20poor%20on%20face%20recognition%0Abenchmarks%2C%20possibly%20due%20to%20the%20lack%20of%20realism%20in%20the%20images%20generated%20from%0Athe%20graphics%20pipeline.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20for%20realism%0Atransfer%20aimed%20at%20enhancing%20the%20realism%20of%20synthetically%20generated%20face%20images.%0AOur%20method%20leverages%20the%20large-scale%20face%20foundation%20model%2C%20and%20we%20adapt%20the%0Apipeline%20for%20realism%20enhancement.%20By%20integrating%20the%20controllable%20aspects%20of%0Athe%20graphics%20pipeline%20with%20our%20realism%20enhancement%20technique%2C%20we%20generate%20a%0Alarge%20amount%20of%20realistic%20variations-combining%20the%20advantages%20of%20both%0Aapproaches.%20Our%20empirical%20evaluations%20demonstrate%20that%20models%20trained%20using%20our%0Aenhanced%20dataset%20significantly%20improve%20the%20performance%20of%20face%20recognition%0Asystems%20over%20the%20baseline.%20The%20source%20code%20and%20datasets%20will%20be%20made%20available%0Apublicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02188v2&entry.124074799=Read"},
{"title": "Adversarial Markov Games: On Adaptive Decision-Based Attacks and\n  Defenses", "author": "Ilias Tsingenopoulos and Vera Rimmer and Davy Preuveneers and Fabio Pierazzi and Lorenzo Cavallaro and Wouter Joosen", "abstract": "  Despite considerable efforts on making them robust, real-world ML-based\nsystems remain vulnerable to decision based attacks, as definitive proofs of\ntheir operational robustness have so far proven intractable. The canonical\napproach in robustness evaluation calls for adaptive attacks, that is with\ncomplete knowledge of the defense and tailored to bypass it. In this study, we\nintroduce a more expansive notion of being adaptive and show how attacks but\nalso defenses can benefit by it and by learning from each other through\ninteraction. We propose and evaluate a framework for adaptively optimizing\nblack-box attacks and defenses against each other through the competitive game\nthey form. To reliably measure robustness, it is important to evaluate against\nrealistic and worst-case attacks. We thus augment both attacks and the evasive\narsenal at their disposal through adaptive control, and observe that the same\ncan be done for defenses, before we evaluate them first apart and then jointly\nunder a multi-agent perspective. We demonstrate that active defenses, which\ncontrol how the system responds, are a necessary complement to model hardening\nwhen facing decision-based attacks; then how these defenses can be circumvented\nby adaptive attacks, only to finally elicit active and adaptive defenses. We\nvalidate our observations through a wide theoretical and empirical\ninvestigation to confirm that AI-enabled adversaries pose a considerable threat\nto black-box ML-based systems, rekindling the proverbial arms race where\ndefenses have to be AI-enabled too. Succinctly, we address the challenges posed\nby adaptive adversaries and develop adaptive defenses, thereby laying out\neffective strategies in ensuring the robustness of ML-based systems deployed in\nthe real-world.\n", "link": "http://arxiv.org/abs/2312.13435v2", "date": "2024-11-05", "relevancy": 1.3367, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4554}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Markov%20Games%3A%20On%20Adaptive%20Decision-Based%20Attacks%20and%0A%20%20Defenses&body=Title%3A%20Adversarial%20Markov%20Games%3A%20On%20Adaptive%20Decision-Based%20Attacks%20and%0A%20%20Defenses%0AAuthor%3A%20Ilias%20Tsingenopoulos%20and%20Vera%20Rimmer%20and%20Davy%20Preuveneers%20and%20Fabio%20Pierazzi%20and%20Lorenzo%20Cavallaro%20and%20Wouter%20Joosen%0AAbstract%3A%20%20%20Despite%20considerable%20efforts%20on%20making%20them%20robust%2C%20real-world%20ML-based%0Asystems%20remain%20vulnerable%20to%20decision%20based%20attacks%2C%20as%20definitive%20proofs%20of%0Atheir%20operational%20robustness%20have%20so%20far%20proven%20intractable.%20The%20canonical%0Aapproach%20in%20robustness%20evaluation%20calls%20for%20adaptive%20attacks%2C%20that%20is%20with%0Acomplete%20knowledge%20of%20the%20defense%20and%20tailored%20to%20bypass%20it.%20In%20this%20study%2C%20we%0Aintroduce%20a%20more%20expansive%20notion%20of%20being%20adaptive%20and%20show%20how%20attacks%20but%0Aalso%20defenses%20can%20benefit%20by%20it%20and%20by%20learning%20from%20each%20other%20through%0Ainteraction.%20We%20propose%20and%20evaluate%20a%20framework%20for%20adaptively%20optimizing%0Ablack-box%20attacks%20and%20defenses%20against%20each%20other%20through%20the%20competitive%20game%0Athey%20form.%20To%20reliably%20measure%20robustness%2C%20it%20is%20important%20to%20evaluate%20against%0Arealistic%20and%20worst-case%20attacks.%20We%20thus%20augment%20both%20attacks%20and%20the%20evasive%0Aarsenal%20at%20their%20disposal%20through%20adaptive%20control%2C%20and%20observe%20that%20the%20same%0Acan%20be%20done%20for%20defenses%2C%20before%20we%20evaluate%20them%20first%20apart%20and%20then%20jointly%0Aunder%20a%20multi-agent%20perspective.%20We%20demonstrate%20that%20active%20defenses%2C%20which%0Acontrol%20how%20the%20system%20responds%2C%20are%20a%20necessary%20complement%20to%20model%20hardening%0Awhen%20facing%20decision-based%20attacks%3B%20then%20how%20these%20defenses%20can%20be%20circumvented%0Aby%20adaptive%20attacks%2C%20only%20to%20finally%20elicit%20active%20and%20adaptive%20defenses.%20We%0Avalidate%20our%20observations%20through%20a%20wide%20theoretical%20and%20empirical%0Ainvestigation%20to%20confirm%20that%20AI-enabled%20adversaries%20pose%20a%20considerable%20threat%0Ato%20black-box%20ML-based%20systems%2C%20rekindling%20the%20proverbial%20arms%20race%20where%0Adefenses%20have%20to%20be%20AI-enabled%20too.%20Succinctly%2C%20we%20address%20the%20challenges%20posed%0Aby%20adaptive%20adversaries%20and%20develop%20adaptive%20defenses%2C%20thereby%20laying%20out%0Aeffective%20strategies%20in%20ensuring%20the%20robustness%20of%20ML-based%20systems%20deployed%20in%0Athe%20real-world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Markov%2520Games%253A%2520On%2520Adaptive%2520Decision-Based%2520Attacks%2520and%250A%2520%2520Defenses%26entry.906535625%3DIlias%2520Tsingenopoulos%2520and%2520Vera%2520Rimmer%2520and%2520Davy%2520Preuveneers%2520and%2520Fabio%2520Pierazzi%2520and%2520Lorenzo%2520Cavallaro%2520and%2520Wouter%2520Joosen%26entry.1292438233%3D%2520%2520Despite%2520considerable%2520efforts%2520on%2520making%2520them%2520robust%252C%2520real-world%2520ML-based%250Asystems%2520remain%2520vulnerable%2520to%2520decision%2520based%2520attacks%252C%2520as%2520definitive%2520proofs%2520of%250Atheir%2520operational%2520robustness%2520have%2520so%2520far%2520proven%2520intractable.%2520The%2520canonical%250Aapproach%2520in%2520robustness%2520evaluation%2520calls%2520for%2520adaptive%2520attacks%252C%2520that%2520is%2520with%250Acomplete%2520knowledge%2520of%2520the%2520defense%2520and%2520tailored%2520to%2520bypass%2520it.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520more%2520expansive%2520notion%2520of%2520being%2520adaptive%2520and%2520show%2520how%2520attacks%2520but%250Aalso%2520defenses%2520can%2520benefit%2520by%2520it%2520and%2520by%2520learning%2520from%2520each%2520other%2520through%250Ainteraction.%2520We%2520propose%2520and%2520evaluate%2520a%2520framework%2520for%2520adaptively%2520optimizing%250Ablack-box%2520attacks%2520and%2520defenses%2520against%2520each%2520other%2520through%2520the%2520competitive%2520game%250Athey%2520form.%2520To%2520reliably%2520measure%2520robustness%252C%2520it%2520is%2520important%2520to%2520evaluate%2520against%250Arealistic%2520and%2520worst-case%2520attacks.%2520We%2520thus%2520augment%2520both%2520attacks%2520and%2520the%2520evasive%250Aarsenal%2520at%2520their%2520disposal%2520through%2520adaptive%2520control%252C%2520and%2520observe%2520that%2520the%2520same%250Acan%2520be%2520done%2520for%2520defenses%252C%2520before%2520we%2520evaluate%2520them%2520first%2520apart%2520and%2520then%2520jointly%250Aunder%2520a%2520multi-agent%2520perspective.%2520We%2520demonstrate%2520that%2520active%2520defenses%252C%2520which%250Acontrol%2520how%2520the%2520system%2520responds%252C%2520are%2520a%2520necessary%2520complement%2520to%2520model%2520hardening%250Awhen%2520facing%2520decision-based%2520attacks%253B%2520then%2520how%2520these%2520defenses%2520can%2520be%2520circumvented%250Aby%2520adaptive%2520attacks%252C%2520only%2520to%2520finally%2520elicit%2520active%2520and%2520adaptive%2520defenses.%2520We%250Avalidate%2520our%2520observations%2520through%2520a%2520wide%2520theoretical%2520and%2520empirical%250Ainvestigation%2520to%2520confirm%2520that%2520AI-enabled%2520adversaries%2520pose%2520a%2520considerable%2520threat%250Ato%2520black-box%2520ML-based%2520systems%252C%2520rekindling%2520the%2520proverbial%2520arms%2520race%2520where%250Adefenses%2520have%2520to%2520be%2520AI-enabled%2520too.%2520Succinctly%252C%2520we%2520address%2520the%2520challenges%2520posed%250Aby%2520adaptive%2520adversaries%2520and%2520develop%2520adaptive%2520defenses%252C%2520thereby%2520laying%2520out%250Aeffective%2520strategies%2520in%2520ensuring%2520the%2520robustness%2520of%2520ML-based%2520systems%2520deployed%2520in%250Athe%2520real-world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Markov%20Games%3A%20On%20Adaptive%20Decision-Based%20Attacks%20and%0A%20%20Defenses&entry.906535625=Ilias%20Tsingenopoulos%20and%20Vera%20Rimmer%20and%20Davy%20Preuveneers%20and%20Fabio%20Pierazzi%20and%20Lorenzo%20Cavallaro%20and%20Wouter%20Joosen&entry.1292438233=%20%20Despite%20considerable%20efforts%20on%20making%20them%20robust%2C%20real-world%20ML-based%0Asystems%20remain%20vulnerable%20to%20decision%20based%20attacks%2C%20as%20definitive%20proofs%20of%0Atheir%20operational%20robustness%20have%20so%20far%20proven%20intractable.%20The%20canonical%0Aapproach%20in%20robustness%20evaluation%20calls%20for%20adaptive%20attacks%2C%20that%20is%20with%0Acomplete%20knowledge%20of%20the%20defense%20and%20tailored%20to%20bypass%20it.%20In%20this%20study%2C%20we%0Aintroduce%20a%20more%20expansive%20notion%20of%20being%20adaptive%20and%20show%20how%20attacks%20but%0Aalso%20defenses%20can%20benefit%20by%20it%20and%20by%20learning%20from%20each%20other%20through%0Ainteraction.%20We%20propose%20and%20evaluate%20a%20framework%20for%20adaptively%20optimizing%0Ablack-box%20attacks%20and%20defenses%20against%20each%20other%20through%20the%20competitive%20game%0Athey%20form.%20To%20reliably%20measure%20robustness%2C%20it%20is%20important%20to%20evaluate%20against%0Arealistic%20and%20worst-case%20attacks.%20We%20thus%20augment%20both%20attacks%20and%20the%20evasive%0Aarsenal%20at%20their%20disposal%20through%20adaptive%20control%2C%20and%20observe%20that%20the%20same%0Acan%20be%20done%20for%20defenses%2C%20before%20we%20evaluate%20them%20first%20apart%20and%20then%20jointly%0Aunder%20a%20multi-agent%20perspective.%20We%20demonstrate%20that%20active%20defenses%2C%20which%0Acontrol%20how%20the%20system%20responds%2C%20are%20a%20necessary%20complement%20to%20model%20hardening%0Awhen%20facing%20decision-based%20attacks%3B%20then%20how%20these%20defenses%20can%20be%20circumvented%0Aby%20adaptive%20attacks%2C%20only%20to%20finally%20elicit%20active%20and%20adaptive%20defenses.%20We%0Avalidate%20our%20observations%20through%20a%20wide%20theoretical%20and%20empirical%0Ainvestigation%20to%20confirm%20that%20AI-enabled%20adversaries%20pose%20a%20considerable%20threat%0Ato%20black-box%20ML-based%20systems%2C%20rekindling%20the%20proverbial%20arms%20race%20where%0Adefenses%20have%20to%20be%20AI-enabled%20too.%20Succinctly%2C%20we%20address%20the%20challenges%20posed%0Aby%20adaptive%20adversaries%20and%20develop%20adaptive%20defenses%2C%20thereby%20laying%20out%0Aeffective%20strategies%20in%20ensuring%20the%20robustness%20of%20ML-based%20systems%20deployed%20in%0Athe%20real-world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13435v2&entry.124074799=Read"},
{"title": "Fast Empirical Scenarios", "author": "Michael Multerer and Paul Schneider and Rohan Sen", "abstract": "  We seek to extract a small number of representative scenarios from large\npanel data that are consistent with sample moments. Among two novel algorithms,\nthe first identifies scenarios that have not been observed before, and comes\nwith a scenario-based representation of covariance matrices. The second\nproposal selects important data points from states of the world that have\nalready realized, and are consistent with higher-order sample moment\ninformation. Both algorithms are efficient to compute and lend themselves to\nconsistent scenario-based modeling and multi-dimensional numerical integration\nthat can be used for interpretable decision-making under uncertainty. Extensive\nnumerical benchmarking studies and an application in portfolio optimization\nfavor the proposed algorithms.\n", "link": "http://arxiv.org/abs/2307.03927v3", "date": "2024-11-05", "relevancy": 1.3114, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4648}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4304}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Empirical%20Scenarios&body=Title%3A%20Fast%20Empirical%20Scenarios%0AAuthor%3A%20Michael%20Multerer%20and%20Paul%20Schneider%20and%20Rohan%20Sen%0AAbstract%3A%20%20%20We%20seek%20to%20extract%20a%20small%20number%20of%20representative%20scenarios%20from%20large%0Apanel%20data%20that%20are%20consistent%20with%20sample%20moments.%20Among%20two%20novel%20algorithms%2C%0Athe%20first%20identifies%20scenarios%20that%20have%20not%20been%20observed%20before%2C%20and%20comes%0Awith%20a%20scenario-based%20representation%20of%20covariance%20matrices.%20The%20second%0Aproposal%20selects%20important%20data%20points%20from%20states%20of%20the%20world%20that%20have%0Aalready%20realized%2C%20and%20are%20consistent%20with%20higher-order%20sample%20moment%0Ainformation.%20Both%20algorithms%20are%20efficient%20to%20compute%20and%20lend%20themselves%20to%0Aconsistent%20scenario-based%20modeling%20and%20multi-dimensional%20numerical%20integration%0Athat%20can%20be%20used%20for%20interpretable%20decision-making%20under%20uncertainty.%20Extensive%0Anumerical%20benchmarking%20studies%20and%20an%20application%20in%20portfolio%20optimization%0Afavor%20the%20proposed%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Empirical%2520Scenarios%26entry.906535625%3DMichael%2520Multerer%2520and%2520Paul%2520Schneider%2520and%2520Rohan%2520Sen%26entry.1292438233%3D%2520%2520We%2520seek%2520to%2520extract%2520a%2520small%2520number%2520of%2520representative%2520scenarios%2520from%2520large%250Apanel%2520data%2520that%2520are%2520consistent%2520with%2520sample%2520moments.%2520Among%2520two%2520novel%2520algorithms%252C%250Athe%2520first%2520identifies%2520scenarios%2520that%2520have%2520not%2520been%2520observed%2520before%252C%2520and%2520comes%250Awith%2520a%2520scenario-based%2520representation%2520of%2520covariance%2520matrices.%2520The%2520second%250Aproposal%2520selects%2520important%2520data%2520points%2520from%2520states%2520of%2520the%2520world%2520that%2520have%250Aalready%2520realized%252C%2520and%2520are%2520consistent%2520with%2520higher-order%2520sample%2520moment%250Ainformation.%2520Both%2520algorithms%2520are%2520efficient%2520to%2520compute%2520and%2520lend%2520themselves%2520to%250Aconsistent%2520scenario-based%2520modeling%2520and%2520multi-dimensional%2520numerical%2520integration%250Athat%2520can%2520be%2520used%2520for%2520interpretable%2520decision-making%2520under%2520uncertainty.%2520Extensive%250Anumerical%2520benchmarking%2520studies%2520and%2520an%2520application%2520in%2520portfolio%2520optimization%250Afavor%2520the%2520proposed%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.03927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Empirical%20Scenarios&entry.906535625=Michael%20Multerer%20and%20Paul%20Schneider%20and%20Rohan%20Sen&entry.1292438233=%20%20We%20seek%20to%20extract%20a%20small%20number%20of%20representative%20scenarios%20from%20large%0Apanel%20data%20that%20are%20consistent%20with%20sample%20moments.%20Among%20two%20novel%20algorithms%2C%0Athe%20first%20identifies%20scenarios%20that%20have%20not%20been%20observed%20before%2C%20and%20comes%0Awith%20a%20scenario-based%20representation%20of%20covariance%20matrices.%20The%20second%0Aproposal%20selects%20important%20data%20points%20from%20states%20of%20the%20world%20that%20have%0Aalready%20realized%2C%20and%20are%20consistent%20with%20higher-order%20sample%20moment%0Ainformation.%20Both%20algorithms%20are%20efficient%20to%20compute%20and%20lend%20themselves%20to%0Aconsistent%20scenario-based%20modeling%20and%20multi-dimensional%20numerical%20integration%0Athat%20can%20be%20used%20for%20interpretable%20decision-making%20under%20uncertainty.%20Extensive%0Anumerical%20benchmarking%20studies%20and%20an%20application%20in%20portfolio%20optimization%0Afavor%20the%20proposed%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03927v3&entry.124074799=Read"},
{"title": "SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents", "author": "Dawei Li and Zhen Tan and Peijia Qian and Yifan Li and Kumar Satvik Chaudhary and Lijie Hu and Jiayi Shen", "abstract": "  While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.\n", "link": "http://arxiv.org/abs/2411.03284v1", "date": "2024-11-05", "relevancy": 1.5797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5292}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMoA%3A%20Improving%20Multi-agent%20Large%20Language%20Models%20with%20Sparse%0A%20%20Mixture-of-Agents&body=Title%3A%20SMoA%3A%20Improving%20Multi-agent%20Large%20Language%20Models%20with%20Sparse%0A%20%20Mixture-of-Agents%0AAuthor%3A%20Dawei%20Li%20and%20Zhen%20Tan%20and%20Peijia%20Qian%20and%20Yifan%20Li%20and%20Kumar%20Satvik%20Chaudhary%20and%20Lijie%20Hu%20and%20Jiayi%20Shen%0AAbstract%3A%20%20%20While%20multi-agent%20systems%20have%20been%20shown%20to%20significantly%20enhance%20the%0Aperformance%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20tasks%20and%0Aapplications%2C%20the%20dense%20interaction%20between%20scaling%20agents%20potentially%20hampers%0Atheir%20efficiency%20and%20diversity.%20To%20address%20these%20challenges%2C%20we%20draw%0Ainspiration%20from%20the%20sparse%20mixture-of-agents%20%28SMoE%29%20and%20propose%20a%20sparse%0Amixture-of-agents%20%28SMoA%29%20framework%20to%20improve%20the%20efficiency%20and%20diversity%20of%0Amulti-agent%20LLMs.%20Unlike%20completely%20connected%20structures%2C%20SMoA%20introduces%20novel%0AResponse%20Selection%20and%20Early%20Stopping%20mechanisms%20to%20sparsify%20information%20flows%0Aamong%20individual%20LLM%20agents%2C%20striking%20a%20balance%20between%20performance%20and%0Aefficiency.%20Additionally%2C%20inspired%20by%20the%20expert%20diversity%20principle%20in%20SMoE%0Aframeworks%20for%20workload%20balance%20between%20experts%2C%20we%20assign%20distinct%20role%0Adescriptions%20to%20each%20LLM%20agent%2C%20fostering%20diverse%20and%20divergent%20thinking.%0AExtensive%20experiments%20on%20reasoning%2C%20alignment%2C%20and%20fairness%20benchmarks%0Ademonstrate%20that%20SMoA%20achieves%20performance%20comparable%20to%20traditional%0Amixture-of-agents%20approaches%20but%20with%20significantly%20lower%20computational%20costs.%0AFurther%20analysis%20reveals%20that%20SMoA%20is%20more%20stable%2C%20has%20a%20greater%20capacity%20to%0Ascale%2C%20and%20offers%20considerable%20potential%20through%20hyper-parameter%20optimization.%0ACode%20and%20data%20will%20be%20available%20at%3A%20https%3A//github.com/David-Li0406/SMoA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMoA%253A%2520Improving%2520Multi-agent%2520Large%2520Language%2520Models%2520with%2520Sparse%250A%2520%2520Mixture-of-Agents%26entry.906535625%3DDawei%2520Li%2520and%2520Zhen%2520Tan%2520and%2520Peijia%2520Qian%2520and%2520Yifan%2520Li%2520and%2520Kumar%2520Satvik%2520Chaudhary%2520and%2520Lijie%2520Hu%2520and%2520Jiayi%2520Shen%26entry.1292438233%3D%2520%2520While%2520multi-agent%2520systems%2520have%2520been%2520shown%2520to%2520significantly%2520enhance%2520the%250Aperformance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%2520tasks%2520and%250Aapplications%252C%2520the%2520dense%2520interaction%2520between%2520scaling%2520agents%2520potentially%2520hampers%250Atheir%2520efficiency%2520and%2520diversity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520draw%250Ainspiration%2520from%2520the%2520sparse%2520mixture-of-agents%2520%2528SMoE%2529%2520and%2520propose%2520a%2520sparse%250Amixture-of-agents%2520%2528SMoA%2529%2520framework%2520to%2520improve%2520the%2520efficiency%2520and%2520diversity%2520of%250Amulti-agent%2520LLMs.%2520Unlike%2520completely%2520connected%2520structures%252C%2520SMoA%2520introduces%2520novel%250AResponse%2520Selection%2520and%2520Early%2520Stopping%2520mechanisms%2520to%2520sparsify%2520information%2520flows%250Aamong%2520individual%2520LLM%2520agents%252C%2520striking%2520a%2520balance%2520between%2520performance%2520and%250Aefficiency.%2520Additionally%252C%2520inspired%2520by%2520the%2520expert%2520diversity%2520principle%2520in%2520SMoE%250Aframeworks%2520for%2520workload%2520balance%2520between%2520experts%252C%2520we%2520assign%2520distinct%2520role%250Adescriptions%2520to%2520each%2520LLM%2520agent%252C%2520fostering%2520diverse%2520and%2520divergent%2520thinking.%250AExtensive%2520experiments%2520on%2520reasoning%252C%2520alignment%252C%2520and%2520fairness%2520benchmarks%250Ademonstrate%2520that%2520SMoA%2520achieves%2520performance%2520comparable%2520to%2520traditional%250Amixture-of-agents%2520approaches%2520but%2520with%2520significantly%2520lower%2520computational%2520costs.%250AFurther%2520analysis%2520reveals%2520that%2520SMoA%2520is%2520more%2520stable%252C%2520has%2520a%2520greater%2520capacity%2520to%250Ascale%252C%2520and%2520offers%2520considerable%2520potential%2520through%2520hyper-parameter%2520optimization.%250ACode%2520and%2520data%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/David-Li0406/SMoA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMoA%3A%20Improving%20Multi-agent%20Large%20Language%20Models%20with%20Sparse%0A%20%20Mixture-of-Agents&entry.906535625=Dawei%20Li%20and%20Zhen%20Tan%20and%20Peijia%20Qian%20and%20Yifan%20Li%20and%20Kumar%20Satvik%20Chaudhary%20and%20Lijie%20Hu%20and%20Jiayi%20Shen&entry.1292438233=%20%20While%20multi-agent%20systems%20have%20been%20shown%20to%20significantly%20enhance%20the%0Aperformance%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20tasks%20and%0Aapplications%2C%20the%20dense%20interaction%20between%20scaling%20agents%20potentially%20hampers%0Atheir%20efficiency%20and%20diversity.%20To%20address%20these%20challenges%2C%20we%20draw%0Ainspiration%20from%20the%20sparse%20mixture-of-agents%20%28SMoE%29%20and%20propose%20a%20sparse%0Amixture-of-agents%20%28SMoA%29%20framework%20to%20improve%20the%20efficiency%20and%20diversity%20of%0Amulti-agent%20LLMs.%20Unlike%20completely%20connected%20structures%2C%20SMoA%20introduces%20novel%0AResponse%20Selection%20and%20Early%20Stopping%20mechanisms%20to%20sparsify%20information%20flows%0Aamong%20individual%20LLM%20agents%2C%20striking%20a%20balance%20between%20performance%20and%0Aefficiency.%20Additionally%2C%20inspired%20by%20the%20expert%20diversity%20principle%20in%20SMoE%0Aframeworks%20for%20workload%20balance%20between%20experts%2C%20we%20assign%20distinct%20role%0Adescriptions%20to%20each%20LLM%20agent%2C%20fostering%20diverse%20and%20divergent%20thinking.%0AExtensive%20experiments%20on%20reasoning%2C%20alignment%2C%20and%20fairness%20benchmarks%0Ademonstrate%20that%20SMoA%20achieves%20performance%20comparable%20to%20traditional%0Amixture-of-agents%20approaches%20but%20with%20significantly%20lower%20computational%20costs.%0AFurther%20analysis%20reveals%20that%20SMoA%20is%20more%20stable%2C%20has%20a%20greater%20capacity%20to%0Ascale%2C%20and%20offers%20considerable%20potential%20through%20hyper-parameter%20optimization.%0ACode%20and%20data%20will%20be%20available%20at%3A%20https%3A//github.com/David-Li0406/SMoA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03284v1&entry.124074799=Read"},
{"title": "Online Data Collection for Efficient Semiparametric Inference", "author": "Shantanu Gupta and Zachary C. Lipton and David Childers", "abstract": "  While many works have studied statistical data fusion, they typically assume\nthat the various datasets are given in advance. However, in practice,\nestimation requires difficult data collection decisions like determining the\navailable data sources, their costs, and how many samples to collect from each\nsource. Moreover, this process is often sequential because the data collected\nat a given time can improve collection decisions in the future. In our setup,\ngiven access to multiple data sources and budget constraints, the agent must\nsequentially decide which data source to query to efficiently estimate a target\nparameter. We formalize this task using Online Moment Selection, a\nsemiparametric framework that applies to any parameter identified by a set of\nmoment conditions. Interestingly, the optimal budget allocation depends on the\n(unknown) true parameters. We present two online data collection policies,\nExplore-then-Commit and Explore-then-Greedy, that use the parameter estimates\nat a given time to optimally allocate the remaining budget in the future steps.\nWe prove that both policies achieve zero regret (assessed by asymptotic MSE)\nrelative to an oracle policy. We empirically validate our methods on both\nsynthetic and real-world causal effect estimation tasks, demonstrating that the\nonline data collection policies outperform their fixed counterparts.\n", "link": "http://arxiv.org/abs/2411.03195v1", "date": "2024-11-05", "relevancy": 1.3391, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4303}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Data%20Collection%20for%20Efficient%20Semiparametric%20Inference&body=Title%3A%20Online%20Data%20Collection%20for%20Efficient%20Semiparametric%20Inference%0AAuthor%3A%20Shantanu%20Gupta%20and%20Zachary%20C.%20Lipton%20and%20David%20Childers%0AAbstract%3A%20%20%20While%20many%20works%20have%20studied%20statistical%20data%20fusion%2C%20they%20typically%20assume%0Athat%20the%20various%20datasets%20are%20given%20in%20advance.%20However%2C%20in%20practice%2C%0Aestimation%20requires%20difficult%20data%20collection%20decisions%20like%20determining%20the%0Aavailable%20data%20sources%2C%20their%20costs%2C%20and%20how%20many%20samples%20to%20collect%20from%20each%0Asource.%20Moreover%2C%20this%20process%20is%20often%20sequential%20because%20the%20data%20collected%0Aat%20a%20given%20time%20can%20improve%20collection%20decisions%20in%20the%20future.%20In%20our%20setup%2C%0Agiven%20access%20to%20multiple%20data%20sources%20and%20budget%20constraints%2C%20the%20agent%20must%0Asequentially%20decide%20which%20data%20source%20to%20query%20to%20efficiently%20estimate%20a%20target%0Aparameter.%20We%20formalize%20this%20task%20using%20Online%20Moment%20Selection%2C%20a%0Asemiparametric%20framework%20that%20applies%20to%20any%20parameter%20identified%20by%20a%20set%20of%0Amoment%20conditions.%20Interestingly%2C%20the%20optimal%20budget%20allocation%20depends%20on%20the%0A%28unknown%29%20true%20parameters.%20We%20present%20two%20online%20data%20collection%20policies%2C%0AExplore-then-Commit%20and%20Explore-then-Greedy%2C%20that%20use%20the%20parameter%20estimates%0Aat%20a%20given%20time%20to%20optimally%20allocate%20the%20remaining%20budget%20in%20the%20future%20steps.%0AWe%20prove%20that%20both%20policies%20achieve%20zero%20regret%20%28assessed%20by%20asymptotic%20MSE%29%0Arelative%20to%20an%20oracle%20policy.%20We%20empirically%20validate%20our%20methods%20on%20both%0Asynthetic%20and%20real-world%20causal%20effect%20estimation%20tasks%2C%20demonstrating%20that%20the%0Aonline%20data%20collection%20policies%20outperform%20their%20fixed%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Data%2520Collection%2520for%2520Efficient%2520Semiparametric%2520Inference%26entry.906535625%3DShantanu%2520Gupta%2520and%2520Zachary%2520C.%2520Lipton%2520and%2520David%2520Childers%26entry.1292438233%3D%2520%2520While%2520many%2520works%2520have%2520studied%2520statistical%2520data%2520fusion%252C%2520they%2520typically%2520assume%250Athat%2520the%2520various%2520datasets%2520are%2520given%2520in%2520advance.%2520However%252C%2520in%2520practice%252C%250Aestimation%2520requires%2520difficult%2520data%2520collection%2520decisions%2520like%2520determining%2520the%250Aavailable%2520data%2520sources%252C%2520their%2520costs%252C%2520and%2520how%2520many%2520samples%2520to%2520collect%2520from%2520each%250Asource.%2520Moreover%252C%2520this%2520process%2520is%2520often%2520sequential%2520because%2520the%2520data%2520collected%250Aat%2520a%2520given%2520time%2520can%2520improve%2520collection%2520decisions%2520in%2520the%2520future.%2520In%2520our%2520setup%252C%250Agiven%2520access%2520to%2520multiple%2520data%2520sources%2520and%2520budget%2520constraints%252C%2520the%2520agent%2520must%250Asequentially%2520decide%2520which%2520data%2520source%2520to%2520query%2520to%2520efficiently%2520estimate%2520a%2520target%250Aparameter.%2520We%2520formalize%2520this%2520task%2520using%2520Online%2520Moment%2520Selection%252C%2520a%250Asemiparametric%2520framework%2520that%2520applies%2520to%2520any%2520parameter%2520identified%2520by%2520a%2520set%2520of%250Amoment%2520conditions.%2520Interestingly%252C%2520the%2520optimal%2520budget%2520allocation%2520depends%2520on%2520the%250A%2528unknown%2529%2520true%2520parameters.%2520We%2520present%2520two%2520online%2520data%2520collection%2520policies%252C%250AExplore-then-Commit%2520and%2520Explore-then-Greedy%252C%2520that%2520use%2520the%2520parameter%2520estimates%250Aat%2520a%2520given%2520time%2520to%2520optimally%2520allocate%2520the%2520remaining%2520budget%2520in%2520the%2520future%2520steps.%250AWe%2520prove%2520that%2520both%2520policies%2520achieve%2520zero%2520regret%2520%2528assessed%2520by%2520asymptotic%2520MSE%2529%250Arelative%2520to%2520an%2520oracle%2520policy.%2520We%2520empirically%2520validate%2520our%2520methods%2520on%2520both%250Asynthetic%2520and%2520real-world%2520causal%2520effect%2520estimation%2520tasks%252C%2520demonstrating%2520that%2520the%250Aonline%2520data%2520collection%2520policies%2520outperform%2520their%2520fixed%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Data%20Collection%20for%20Efficient%20Semiparametric%20Inference&entry.906535625=Shantanu%20Gupta%20and%20Zachary%20C.%20Lipton%20and%20David%20Childers&entry.1292438233=%20%20While%20many%20works%20have%20studied%20statistical%20data%20fusion%2C%20they%20typically%20assume%0Athat%20the%20various%20datasets%20are%20given%20in%20advance.%20However%2C%20in%20practice%2C%0Aestimation%20requires%20difficult%20data%20collection%20decisions%20like%20determining%20the%0Aavailable%20data%20sources%2C%20their%20costs%2C%20and%20how%20many%20samples%20to%20collect%20from%20each%0Asource.%20Moreover%2C%20this%20process%20is%20often%20sequential%20because%20the%20data%20collected%0Aat%20a%20given%20time%20can%20improve%20collection%20decisions%20in%20the%20future.%20In%20our%20setup%2C%0Agiven%20access%20to%20multiple%20data%20sources%20and%20budget%20constraints%2C%20the%20agent%20must%0Asequentially%20decide%20which%20data%20source%20to%20query%20to%20efficiently%20estimate%20a%20target%0Aparameter.%20We%20formalize%20this%20task%20using%20Online%20Moment%20Selection%2C%20a%0Asemiparametric%20framework%20that%20applies%20to%20any%20parameter%20identified%20by%20a%20set%20of%0Amoment%20conditions.%20Interestingly%2C%20the%20optimal%20budget%20allocation%20depends%20on%20the%0A%28unknown%29%20true%20parameters.%20We%20present%20two%20online%20data%20collection%20policies%2C%0AExplore-then-Commit%20and%20Explore-then-Greedy%2C%20that%20use%20the%20parameter%20estimates%0Aat%20a%20given%20time%20to%20optimally%20allocate%20the%20remaining%20budget%20in%20the%20future%20steps.%0AWe%20prove%20that%20both%20policies%20achieve%20zero%20regret%20%28assessed%20by%20asymptotic%20MSE%29%0Arelative%20to%20an%20oracle%20policy.%20We%20empirically%20validate%20our%20methods%20on%20both%0Asynthetic%20and%20real-world%20causal%20effect%20estimation%20tasks%2C%20demonstrating%20that%20the%0Aonline%20data%20collection%20policies%20outperform%20their%20fixed%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03195v1&entry.124074799=Read"},
{"title": "Data-Driven Socio-Economic Deprivation Prediction via Dimensionality\n  Reduction: The Power of Diffusion Maps", "author": "June Moh Goo", "abstract": "  This research proposes a model to predict the location of the most deprived\nareas in a city using data from the census. Census data is very\nhigh-dimensional and needs to be simplified. We use the diffusion map algorithm\nto reduce dimensionality and find patterns. Features are defined by\neigenvectors of the Laplacian matrix that defines the diffusion map. The\neigenvectors corresponding to the smallest eigenvalues indicate specific\ncharacteristics of the population. Previous work has found qualitatively that\nthe second most important dimension for describing the census data in Bristol,\nUK is linked to deprivation. In this research, we analyse how good this\ndimension is as a model for predicting deprivation by comparing it with the\nrecognised measures. The Pearson correlation coefficient was found to be\ngreater than 0.7. The top 10 per cent of deprived areas in the UK, which are\nalso located in Bristol, are extracted to test the accuracy of the model. There\nare 52 of the most deprived areas, and 38 areas are correctly identified by\ncomparing them to the model. The influence of scores of IMD domains that do not\ncorrelate with the models and Eigenvector 2 entries of non-deprived Output\nAreas cause the model to fail the prediction of 14 deprived areas. The model\ndemonstrates strong performance in predicting future deprivation in the project\nareas, which is expected to assist in government resource allocation and\nfunding greatly. The codes can be accessed here:\nhttps://github.com/junegoo94/diffusion_maps\n", "link": "http://arxiv.org/abs/2312.09830v2", "date": "2024-11-05", "relevancy": 1.4496, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4932}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Socio-Economic%20Deprivation%20Prediction%20via%20Dimensionality%0A%20%20Reduction%3A%20The%20Power%20of%20Diffusion%20Maps&body=Title%3A%20Data-Driven%20Socio-Economic%20Deprivation%20Prediction%20via%20Dimensionality%0A%20%20Reduction%3A%20The%20Power%20of%20Diffusion%20Maps%0AAuthor%3A%20June%20Moh%20Goo%0AAbstract%3A%20%20%20This%20research%20proposes%20a%20model%20to%20predict%20the%20location%20of%20the%20most%20deprived%0Aareas%20in%20a%20city%20using%20data%20from%20the%20census.%20Census%20data%20is%20very%0Ahigh-dimensional%20and%20needs%20to%20be%20simplified.%20We%20use%20the%20diffusion%20map%20algorithm%0Ato%20reduce%20dimensionality%20and%20find%20patterns.%20Features%20are%20defined%20by%0Aeigenvectors%20of%20the%20Laplacian%20matrix%20that%20defines%20the%20diffusion%20map.%20The%0Aeigenvectors%20corresponding%20to%20the%20smallest%20eigenvalues%20indicate%20specific%0Acharacteristics%20of%20the%20population.%20Previous%20work%20has%20found%20qualitatively%20that%0Athe%20second%20most%20important%20dimension%20for%20describing%20the%20census%20data%20in%20Bristol%2C%0AUK%20is%20linked%20to%20deprivation.%20In%20this%20research%2C%20we%20analyse%20how%20good%20this%0Adimension%20is%20as%20a%20model%20for%20predicting%20deprivation%20by%20comparing%20it%20with%20the%0Arecognised%20measures.%20The%20Pearson%20correlation%20coefficient%20was%20found%20to%20be%0Agreater%20than%200.7.%20The%20top%2010%20per%20cent%20of%20deprived%20areas%20in%20the%20UK%2C%20which%20are%0Aalso%20located%20in%20Bristol%2C%20are%20extracted%20to%20test%20the%20accuracy%20of%20the%20model.%20There%0Aare%2052%20of%20the%20most%20deprived%20areas%2C%20and%2038%20areas%20are%20correctly%20identified%20by%0Acomparing%20them%20to%20the%20model.%20The%20influence%20of%20scores%20of%20IMD%20domains%20that%20do%20not%0Acorrelate%20with%20the%20models%20and%20Eigenvector%202%20entries%20of%20non-deprived%20Output%0AAreas%20cause%20the%20model%20to%20fail%20the%20prediction%20of%2014%20deprived%20areas.%20The%20model%0Ademonstrates%20strong%20performance%20in%20predicting%20future%20deprivation%20in%20the%20project%0Aareas%2C%20which%20is%20expected%20to%20assist%20in%20government%20resource%20allocation%20and%0Afunding%20greatly.%20The%20codes%20can%20be%20accessed%20here%3A%0Ahttps%3A//github.com/junegoo94/diffusion_maps%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Socio-Economic%2520Deprivation%2520Prediction%2520via%2520Dimensionality%250A%2520%2520Reduction%253A%2520The%2520Power%2520of%2520Diffusion%2520Maps%26entry.906535625%3DJune%2520Moh%2520Goo%26entry.1292438233%3D%2520%2520This%2520research%2520proposes%2520a%2520model%2520to%2520predict%2520the%2520location%2520of%2520the%2520most%2520deprived%250Aareas%2520in%2520a%2520city%2520using%2520data%2520from%2520the%2520census.%2520Census%2520data%2520is%2520very%250Ahigh-dimensional%2520and%2520needs%2520to%2520be%2520simplified.%2520We%2520use%2520the%2520diffusion%2520map%2520algorithm%250Ato%2520reduce%2520dimensionality%2520and%2520find%2520patterns.%2520Features%2520are%2520defined%2520by%250Aeigenvectors%2520of%2520the%2520Laplacian%2520matrix%2520that%2520defines%2520the%2520diffusion%2520map.%2520The%250Aeigenvectors%2520corresponding%2520to%2520the%2520smallest%2520eigenvalues%2520indicate%2520specific%250Acharacteristics%2520of%2520the%2520population.%2520Previous%2520work%2520has%2520found%2520qualitatively%2520that%250Athe%2520second%2520most%2520important%2520dimension%2520for%2520describing%2520the%2520census%2520data%2520in%2520Bristol%252C%250AUK%2520is%2520linked%2520to%2520deprivation.%2520In%2520this%2520research%252C%2520we%2520analyse%2520how%2520good%2520this%250Adimension%2520is%2520as%2520a%2520model%2520for%2520predicting%2520deprivation%2520by%2520comparing%2520it%2520with%2520the%250Arecognised%2520measures.%2520The%2520Pearson%2520correlation%2520coefficient%2520was%2520found%2520to%2520be%250Agreater%2520than%25200.7.%2520The%2520top%252010%2520per%2520cent%2520of%2520deprived%2520areas%2520in%2520the%2520UK%252C%2520which%2520are%250Aalso%2520located%2520in%2520Bristol%252C%2520are%2520extracted%2520to%2520test%2520the%2520accuracy%2520of%2520the%2520model.%2520There%250Aare%252052%2520of%2520the%2520most%2520deprived%2520areas%252C%2520and%252038%2520areas%2520are%2520correctly%2520identified%2520by%250Acomparing%2520them%2520to%2520the%2520model.%2520The%2520influence%2520of%2520scores%2520of%2520IMD%2520domains%2520that%2520do%2520not%250Acorrelate%2520with%2520the%2520models%2520and%2520Eigenvector%25202%2520entries%2520of%2520non-deprived%2520Output%250AAreas%2520cause%2520the%2520model%2520to%2520fail%2520the%2520prediction%2520of%252014%2520deprived%2520areas.%2520The%2520model%250Ademonstrates%2520strong%2520performance%2520in%2520predicting%2520future%2520deprivation%2520in%2520the%2520project%250Aareas%252C%2520which%2520is%2520expected%2520to%2520assist%2520in%2520government%2520resource%2520allocation%2520and%250Afunding%2520greatly.%2520The%2520codes%2520can%2520be%2520accessed%2520here%253A%250Ahttps%253A//github.com/junegoo94/diffusion_maps%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Socio-Economic%20Deprivation%20Prediction%20via%20Dimensionality%0A%20%20Reduction%3A%20The%20Power%20of%20Diffusion%20Maps&entry.906535625=June%20Moh%20Goo&entry.1292438233=%20%20This%20research%20proposes%20a%20model%20to%20predict%20the%20location%20of%20the%20most%20deprived%0Aareas%20in%20a%20city%20using%20data%20from%20the%20census.%20Census%20data%20is%20very%0Ahigh-dimensional%20and%20needs%20to%20be%20simplified.%20We%20use%20the%20diffusion%20map%20algorithm%0Ato%20reduce%20dimensionality%20and%20find%20patterns.%20Features%20are%20defined%20by%0Aeigenvectors%20of%20the%20Laplacian%20matrix%20that%20defines%20the%20diffusion%20map.%20The%0Aeigenvectors%20corresponding%20to%20the%20smallest%20eigenvalues%20indicate%20specific%0Acharacteristics%20of%20the%20population.%20Previous%20work%20has%20found%20qualitatively%20that%0Athe%20second%20most%20important%20dimension%20for%20describing%20the%20census%20data%20in%20Bristol%2C%0AUK%20is%20linked%20to%20deprivation.%20In%20this%20research%2C%20we%20analyse%20how%20good%20this%0Adimension%20is%20as%20a%20model%20for%20predicting%20deprivation%20by%20comparing%20it%20with%20the%0Arecognised%20measures.%20The%20Pearson%20correlation%20coefficient%20was%20found%20to%20be%0Agreater%20than%200.7.%20The%20top%2010%20per%20cent%20of%20deprived%20areas%20in%20the%20UK%2C%20which%20are%0Aalso%20located%20in%20Bristol%2C%20are%20extracted%20to%20test%20the%20accuracy%20of%20the%20model.%20There%0Aare%2052%20of%20the%20most%20deprived%20areas%2C%20and%2038%20areas%20are%20correctly%20identified%20by%0Acomparing%20them%20to%20the%20model.%20The%20influence%20of%20scores%20of%20IMD%20domains%20that%20do%20not%0Acorrelate%20with%20the%20models%20and%20Eigenvector%202%20entries%20of%20non-deprived%20Output%0AAreas%20cause%20the%20model%20to%20fail%20the%20prediction%20of%2014%20deprived%20areas.%20The%20model%0Ademonstrates%20strong%20performance%20in%20predicting%20future%20deprivation%20in%20the%20project%0Aareas%2C%20which%20is%20expected%20to%20assist%20in%20government%20resource%20allocation%20and%0Afunding%20greatly.%20The%20codes%20can%20be%20accessed%20here%3A%0Ahttps%3A//github.com/junegoo94/diffusion_maps%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09830v2&entry.124074799=Read"},
{"title": "What Makes an Educational Robot Game Fun? Framework Analysis of\n  Children's Design Ideas", "author": "Elaheh Sanoubari and John Edison Mu\u00f1oz and Ali Yamini and Neil Randall and Kerstni Dautenhahn", "abstract": "  Fun acts as a catalyst for learning by enhancing motivation, active\nengagement and knowledge retention. As social robots gain traction as\neducational tools, understanding how their unique affordances can be leveraged\nto cultivate fun becomes crucial. This research investigates the concept of fun\nin educational games involving social robots to support the design of REMind:a\nrobot-mediated role-play game aimed at encouraging bystander intervention\nagainst peer bullying among children. To incorporate fun elements into design\nof REMind, we conducted a user-centered Research through Design (RtD) study\nwith focus groups of children to gain a deeper understanding of their\nperceptions of fun. We analyzed children's ideas by using Framework Analysis\nand leveraging LeBlanc's Taxonomy of Game Pleasures and identified 28 elements\nof fun that can be incorporated into robot-mediated games. We present our\nobservations, discuss their impact on REMind's design, and offer\nrecommendations for designing fun educational games using social robots.\n", "link": "http://arxiv.org/abs/2411.03213v1", "date": "2024-11-05", "relevancy": 1.2858, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4214}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20an%20Educational%20Robot%20Game%20Fun%3F%20Framework%20Analysis%20of%0A%20%20Children%27s%20Design%20Ideas&body=Title%3A%20What%20Makes%20an%20Educational%20Robot%20Game%20Fun%3F%20Framework%20Analysis%20of%0A%20%20Children%27s%20Design%20Ideas%0AAuthor%3A%20Elaheh%20Sanoubari%20and%20John%20Edison%20Mu%C3%B1oz%20and%20Ali%20Yamini%20and%20Neil%20Randall%20and%20Kerstni%20Dautenhahn%0AAbstract%3A%20%20%20Fun%20acts%20as%20a%20catalyst%20for%20learning%20by%20enhancing%20motivation%2C%20active%0Aengagement%20and%20knowledge%20retention.%20As%20social%20robots%20gain%20traction%20as%0Aeducational%20tools%2C%20understanding%20how%20their%20unique%20affordances%20can%20be%20leveraged%0Ato%20cultivate%20fun%20becomes%20crucial.%20This%20research%20investigates%20the%20concept%20of%20fun%0Ain%20educational%20games%20involving%20social%20robots%20to%20support%20the%20design%20of%20REMind%3Aa%0Arobot-mediated%20role-play%20game%20aimed%20at%20encouraging%20bystander%20intervention%0Aagainst%20peer%20bullying%20among%20children.%20To%20incorporate%20fun%20elements%20into%20design%0Aof%20REMind%2C%20we%20conducted%20a%20user-centered%20Research%20through%20Design%20%28RtD%29%20study%0Awith%20focus%20groups%20of%20children%20to%20gain%20a%20deeper%20understanding%20of%20their%0Aperceptions%20of%20fun.%20We%20analyzed%20children%27s%20ideas%20by%20using%20Framework%20Analysis%0Aand%20leveraging%20LeBlanc%27s%20Taxonomy%20of%20Game%20Pleasures%20and%20identified%2028%20elements%0Aof%20fun%20that%20can%20be%20incorporated%20into%20robot-mediated%20games.%20We%20present%20our%0Aobservations%2C%20discuss%20their%20impact%20on%20REMind%27s%20design%2C%20and%20offer%0Arecommendations%20for%20designing%20fun%20educational%20games%20using%20social%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520an%2520Educational%2520Robot%2520Game%2520Fun%253F%2520Framework%2520Analysis%2520of%250A%2520%2520Children%2527s%2520Design%2520Ideas%26entry.906535625%3DElaheh%2520Sanoubari%2520and%2520John%2520Edison%2520Mu%25C3%25B1oz%2520and%2520Ali%2520Yamini%2520and%2520Neil%2520Randall%2520and%2520Kerstni%2520Dautenhahn%26entry.1292438233%3D%2520%2520Fun%2520acts%2520as%2520a%2520catalyst%2520for%2520learning%2520by%2520enhancing%2520motivation%252C%2520active%250Aengagement%2520and%2520knowledge%2520retention.%2520As%2520social%2520robots%2520gain%2520traction%2520as%250Aeducational%2520tools%252C%2520understanding%2520how%2520their%2520unique%2520affordances%2520can%2520be%2520leveraged%250Ato%2520cultivate%2520fun%2520becomes%2520crucial.%2520This%2520research%2520investigates%2520the%2520concept%2520of%2520fun%250Ain%2520educational%2520games%2520involving%2520social%2520robots%2520to%2520support%2520the%2520design%2520of%2520REMind%253Aa%250Arobot-mediated%2520role-play%2520game%2520aimed%2520at%2520encouraging%2520bystander%2520intervention%250Aagainst%2520peer%2520bullying%2520among%2520children.%2520To%2520incorporate%2520fun%2520elements%2520into%2520design%250Aof%2520REMind%252C%2520we%2520conducted%2520a%2520user-centered%2520Research%2520through%2520Design%2520%2528RtD%2529%2520study%250Awith%2520focus%2520groups%2520of%2520children%2520to%2520gain%2520a%2520deeper%2520understanding%2520of%2520their%250Aperceptions%2520of%2520fun.%2520We%2520analyzed%2520children%2527s%2520ideas%2520by%2520using%2520Framework%2520Analysis%250Aand%2520leveraging%2520LeBlanc%2527s%2520Taxonomy%2520of%2520Game%2520Pleasures%2520and%2520identified%252028%2520elements%250Aof%2520fun%2520that%2520can%2520be%2520incorporated%2520into%2520robot-mediated%2520games.%2520We%2520present%2520our%250Aobservations%252C%2520discuss%2520their%2520impact%2520on%2520REMind%2527s%2520design%252C%2520and%2520offer%250Arecommendations%2520for%2520designing%2520fun%2520educational%2520games%2520using%2520social%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20an%20Educational%20Robot%20Game%20Fun%3F%20Framework%20Analysis%20of%0A%20%20Children%27s%20Design%20Ideas&entry.906535625=Elaheh%20Sanoubari%20and%20John%20Edison%20Mu%C3%B1oz%20and%20Ali%20Yamini%20and%20Neil%20Randall%20and%20Kerstni%20Dautenhahn&entry.1292438233=%20%20Fun%20acts%20as%20a%20catalyst%20for%20learning%20by%20enhancing%20motivation%2C%20active%0Aengagement%20and%20knowledge%20retention.%20As%20social%20robots%20gain%20traction%20as%0Aeducational%20tools%2C%20understanding%20how%20their%20unique%20affordances%20can%20be%20leveraged%0Ato%20cultivate%20fun%20becomes%20crucial.%20This%20research%20investigates%20the%20concept%20of%20fun%0Ain%20educational%20games%20involving%20social%20robots%20to%20support%20the%20design%20of%20REMind%3Aa%0Arobot-mediated%20role-play%20game%20aimed%20at%20encouraging%20bystander%20intervention%0Aagainst%20peer%20bullying%20among%20children.%20To%20incorporate%20fun%20elements%20into%20design%0Aof%20REMind%2C%20we%20conducted%20a%20user-centered%20Research%20through%20Design%20%28RtD%29%20study%0Awith%20focus%20groups%20of%20children%20to%20gain%20a%20deeper%20understanding%20of%20their%0Aperceptions%20of%20fun.%20We%20analyzed%20children%27s%20ideas%20by%20using%20Framework%20Analysis%0Aand%20leveraging%20LeBlanc%27s%20Taxonomy%20of%20Game%20Pleasures%20and%20identified%2028%20elements%0Aof%20fun%20that%20can%20be%20incorporated%20into%20robot-mediated%20games.%20We%20present%20our%0Aobservations%2C%20discuss%20their%20impact%20on%20REMind%27s%20design%2C%20and%20offer%0Arecommendations%20for%20designing%20fun%20educational%20games%20using%20social%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03213v1&entry.124074799=Read"},
{"title": "On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep\n  Support Vector Data Description", "author": "George Stamatelis and Panagiotis Gavriilidis and Aymen Fakhreddine and George C. Alexandropoulos", "abstract": "  In this paper, we study the problem of promptly detecting the presence of\nnon-cooperative activity from one or more Reconfigurable Intelligent Surfaces\n(RISs) with unknown characteristics lying in the vicinity of a Multiple-Input\nMultiple-Output (MIMO) communication system using Orthogonal Frequency-Division\nMultiplexing (OFDM) transmissions. We first present a novel wideband channel\nmodel incorporating RISs as well as non-reconfigurable stationary surfaces,\nwhich captures both the effect of the RIS actuation time on the channel in the\nfrequency domain as well as the difference between changing phase\nconfigurations during or among transmissions. Considering that RISs may operate\nunder the coordination of a third-party system, and thus, may negatively impact\nthe communication of the intended MIMO OFDM system, we present a novel RIS\nactivity detection framework that is unaware of the distribution of the phase\nconfiguration of any of the non-cooperative RISs. In particular, capitalizing\non the knowledge of the data distribution at the multi-antenna receiver, we\ndesign a novel online change point detection statistic that combines a deep\nsupport vector data description model with the scan $B$-test. The presented\nnumerical investigations demonstrate the improved detection accuracy as well as\ndecreased computational complexity of the proposed RIS detection approach over\nexisting change point detection schemes.\n", "link": "http://arxiv.org/abs/2411.03237v1", "date": "2024-11-05", "relevancy": 1.375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Detection%20of%20Non-Cooperative%20RISs%3A%20Scan%20B-Testing%20via%20Deep%0A%20%20Support%20Vector%20Data%20Description&body=Title%3A%20On%20the%20Detection%20of%20Non-Cooperative%20RISs%3A%20Scan%20B-Testing%20via%20Deep%0A%20%20Support%20Vector%20Data%20Description%0AAuthor%3A%20George%20Stamatelis%20and%20Panagiotis%20Gavriilidis%20and%20Aymen%20Fakhreddine%20and%20George%20C.%20Alexandropoulos%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20promptly%20detecting%20the%20presence%20of%0Anon-cooperative%20activity%20from%20one%20or%20more%20Reconfigurable%20Intelligent%20Surfaces%0A%28RISs%29%20with%20unknown%20characteristics%20lying%20in%20the%20vicinity%20of%20a%20Multiple-Input%0AMultiple-Output%20%28MIMO%29%20communication%20system%20using%20Orthogonal%20Frequency-Division%0AMultiplexing%20%28OFDM%29%20transmissions.%20We%20first%20present%20a%20novel%20wideband%20channel%0Amodel%20incorporating%20RISs%20as%20well%20as%20non-reconfigurable%20stationary%20surfaces%2C%0Awhich%20captures%20both%20the%20effect%20of%20the%20RIS%20actuation%20time%20on%20the%20channel%20in%20the%0Afrequency%20domain%20as%20well%20as%20the%20difference%20between%20changing%20phase%0Aconfigurations%20during%20or%20among%20transmissions.%20Considering%20that%20RISs%20may%20operate%0Aunder%20the%20coordination%20of%20a%20third-party%20system%2C%20and%20thus%2C%20may%20negatively%20impact%0Athe%20communication%20of%20the%20intended%20MIMO%20OFDM%20system%2C%20we%20present%20a%20novel%20RIS%0Aactivity%20detection%20framework%20that%20is%20unaware%20of%20the%20distribution%20of%20the%20phase%0Aconfiguration%20of%20any%20of%20the%20non-cooperative%20RISs.%20In%20particular%2C%20capitalizing%0Aon%20the%20knowledge%20of%20the%20data%20distribution%20at%20the%20multi-antenna%20receiver%2C%20we%0Adesign%20a%20novel%20online%20change%20point%20detection%20statistic%20that%20combines%20a%20deep%0Asupport%20vector%20data%20description%20model%20with%20the%20scan%20%24B%24-test.%20The%20presented%0Anumerical%20investigations%20demonstrate%20the%20improved%20detection%20accuracy%20as%20well%20as%0Adecreased%20computational%20complexity%20of%20the%20proposed%20RIS%20detection%20approach%20over%0Aexisting%20change%20point%20detection%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Detection%2520of%2520Non-Cooperative%2520RISs%253A%2520Scan%2520B-Testing%2520via%2520Deep%250A%2520%2520Support%2520Vector%2520Data%2520Description%26entry.906535625%3DGeorge%2520Stamatelis%2520and%2520Panagiotis%2520Gavriilidis%2520and%2520Aymen%2520Fakhreddine%2520and%2520George%2520C.%2520Alexandropoulos%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520problem%2520of%2520promptly%2520detecting%2520the%2520presence%2520of%250Anon-cooperative%2520activity%2520from%2520one%2520or%2520more%2520Reconfigurable%2520Intelligent%2520Surfaces%250A%2528RISs%2529%2520with%2520unknown%2520characteristics%2520lying%2520in%2520the%2520vicinity%2520of%2520a%2520Multiple-Input%250AMultiple-Output%2520%2528MIMO%2529%2520communication%2520system%2520using%2520Orthogonal%2520Frequency-Division%250AMultiplexing%2520%2528OFDM%2529%2520transmissions.%2520We%2520first%2520present%2520a%2520novel%2520wideband%2520channel%250Amodel%2520incorporating%2520RISs%2520as%2520well%2520as%2520non-reconfigurable%2520stationary%2520surfaces%252C%250Awhich%2520captures%2520both%2520the%2520effect%2520of%2520the%2520RIS%2520actuation%2520time%2520on%2520the%2520channel%2520in%2520the%250Afrequency%2520domain%2520as%2520well%2520as%2520the%2520difference%2520between%2520changing%2520phase%250Aconfigurations%2520during%2520or%2520among%2520transmissions.%2520Considering%2520that%2520RISs%2520may%2520operate%250Aunder%2520the%2520coordination%2520of%2520a%2520third-party%2520system%252C%2520and%2520thus%252C%2520may%2520negatively%2520impact%250Athe%2520communication%2520of%2520the%2520intended%2520MIMO%2520OFDM%2520system%252C%2520we%2520present%2520a%2520novel%2520RIS%250Aactivity%2520detection%2520framework%2520that%2520is%2520unaware%2520of%2520the%2520distribution%2520of%2520the%2520phase%250Aconfiguration%2520of%2520any%2520of%2520the%2520non-cooperative%2520RISs.%2520In%2520particular%252C%2520capitalizing%250Aon%2520the%2520knowledge%2520of%2520the%2520data%2520distribution%2520at%2520the%2520multi-antenna%2520receiver%252C%2520we%250Adesign%2520a%2520novel%2520online%2520change%2520point%2520detection%2520statistic%2520that%2520combines%2520a%2520deep%250Asupport%2520vector%2520data%2520description%2520model%2520with%2520the%2520scan%2520%2524B%2524-test.%2520The%2520presented%250Anumerical%2520investigations%2520demonstrate%2520the%2520improved%2520detection%2520accuracy%2520as%2520well%2520as%250Adecreased%2520computational%2520complexity%2520of%2520the%2520proposed%2520RIS%2520detection%2520approach%2520over%250Aexisting%2520change%2520point%2520detection%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Detection%20of%20Non-Cooperative%20RISs%3A%20Scan%20B-Testing%20via%20Deep%0A%20%20Support%20Vector%20Data%20Description&entry.906535625=George%20Stamatelis%20and%20Panagiotis%20Gavriilidis%20and%20Aymen%20Fakhreddine%20and%20George%20C.%20Alexandropoulos&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20promptly%20detecting%20the%20presence%20of%0Anon-cooperative%20activity%20from%20one%20or%20more%20Reconfigurable%20Intelligent%20Surfaces%0A%28RISs%29%20with%20unknown%20characteristics%20lying%20in%20the%20vicinity%20of%20a%20Multiple-Input%0AMultiple-Output%20%28MIMO%29%20communication%20system%20using%20Orthogonal%20Frequency-Division%0AMultiplexing%20%28OFDM%29%20transmissions.%20We%20first%20present%20a%20novel%20wideband%20channel%0Amodel%20incorporating%20RISs%20as%20well%20as%20non-reconfigurable%20stationary%20surfaces%2C%0Awhich%20captures%20both%20the%20effect%20of%20the%20RIS%20actuation%20time%20on%20the%20channel%20in%20the%0Afrequency%20domain%20as%20well%20as%20the%20difference%20between%20changing%20phase%0Aconfigurations%20during%20or%20among%20transmissions.%20Considering%20that%20RISs%20may%20operate%0Aunder%20the%20coordination%20of%20a%20third-party%20system%2C%20and%20thus%2C%20may%20negatively%20impact%0Athe%20communication%20of%20the%20intended%20MIMO%20OFDM%20system%2C%20we%20present%20a%20novel%20RIS%0Aactivity%20detection%20framework%20that%20is%20unaware%20of%20the%20distribution%20of%20the%20phase%0Aconfiguration%20of%20any%20of%20the%20non-cooperative%20RISs.%20In%20particular%2C%20capitalizing%0Aon%20the%20knowledge%20of%20the%20data%20distribution%20at%20the%20multi-antenna%20receiver%2C%20we%0Adesign%20a%20novel%20online%20change%20point%20detection%20statistic%20that%20combines%20a%20deep%0Asupport%20vector%20data%20description%20model%20with%20the%20scan%20%24B%24-test.%20The%20presented%0Anumerical%20investigations%20demonstrate%20the%20improved%20detection%20accuracy%20as%20well%20as%0Adecreased%20computational%20complexity%20of%20the%20proposed%20RIS%20detection%20approach%20over%0Aexisting%20change%20point%20detection%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03237v1&entry.124074799=Read"},
{"title": "MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based\n  Automatic Disease Detection", "author": "Yiqun Liu and Ke Zhang and Yin Zhu", "abstract": "  Ground reaction force (GRF) is the force exerted by the ground on a body in\ncontact with it. GRF-based automatic disease detection (ADD) has become an\nemerging medical diagnosis method, which aims to learn and identify disease\npatterns corresponding to different gait pressures based on deep learning\nmethods. Although existing ADD methods can save doctors time in making\ndiagnoses, training deep models still struggles with the cost caused by the\nlabeling engineering for a large number of gait diagnostic data for subjects.\nOn the other hand, the accuracy of the deep model under the unified benchmark\nGRF dataset and the generalization ability on scalable gait datasets need to be\nfurther improved. To address these issues, we propose MA2, a GRF-based\nself-supervised and motion augmenting auto-encoder, which models the ADD task\nas an encoder-decoder paradigm. In the encoder, we introduce an embedding block\nincluding the 3-layer 1D convolution for extracting the token and a mask\ngenerator to randomly mask out the sequence of tokens to maximize the model's\npotential to capture high-level, discriminative, intrinsic representations.\nwhereafter, the decoder utilizes this information to reconstruct the pixel\nsequence of the origin input and calculate the reconstruction loss to optimize\nthe network. Moreover, the backbone of an auto-encoder is multi-head\nself-attention that can consider the global information of the token from the\ninput, not just the local neighborhood. This allows the model to capture\ngeneralized contextual information. Extensive experiments demonstrate MA2 has\nSOTA performance of 90.91% accuracy on 1% limited pathological GRF samples with\nlabels, and good generalization ability of 78.57% accuracy on scalable\nParkinson disease dataset.\n", "link": "http://arxiv.org/abs/2411.03129v1", "date": "2024-11-05", "relevancy": 1.5432, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5115}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MA%5E2%3A%20A%20Self-Supervised%20and%20Motion%20Augmenting%20Autoencoder%20for%20Gait-Based%0A%20%20Automatic%20Disease%20Detection&body=Title%3A%20MA%5E2%3A%20A%20Self-Supervised%20and%20Motion%20Augmenting%20Autoencoder%20for%20Gait-Based%0A%20%20Automatic%20Disease%20Detection%0AAuthor%3A%20Yiqun%20Liu%20and%20Ke%20Zhang%20and%20Yin%20Zhu%0AAbstract%3A%20%20%20Ground%20reaction%20force%20%28GRF%29%20is%20the%20force%20exerted%20by%20the%20ground%20on%20a%20body%20in%0Acontact%20with%20it.%20GRF-based%20automatic%20disease%20detection%20%28ADD%29%20has%20become%20an%0Aemerging%20medical%20diagnosis%20method%2C%20which%20aims%20to%20learn%20and%20identify%20disease%0Apatterns%20corresponding%20to%20different%20gait%20pressures%20based%20on%20deep%20learning%0Amethods.%20Although%20existing%20ADD%20methods%20can%20save%20doctors%20time%20in%20making%0Adiagnoses%2C%20training%20deep%20models%20still%20struggles%20with%20the%20cost%20caused%20by%20the%0Alabeling%20engineering%20for%20a%20large%20number%20of%20gait%20diagnostic%20data%20for%20subjects.%0AOn%20the%20other%20hand%2C%20the%20accuracy%20of%20the%20deep%20model%20under%20the%20unified%20benchmark%0AGRF%20dataset%20and%20the%20generalization%20ability%20on%20scalable%20gait%20datasets%20need%20to%20be%0Afurther%20improved.%20To%20address%20these%20issues%2C%20we%20propose%20MA2%2C%20a%20GRF-based%0Aself-supervised%20and%20motion%20augmenting%20auto-encoder%2C%20which%20models%20the%20ADD%20task%0Aas%20an%20encoder-decoder%20paradigm.%20In%20the%20encoder%2C%20we%20introduce%20an%20embedding%20block%0Aincluding%20the%203-layer%201D%20convolution%20for%20extracting%20the%20token%20and%20a%20mask%0Agenerator%20to%20randomly%20mask%20out%20the%20sequence%20of%20tokens%20to%20maximize%20the%20model%27s%0Apotential%20to%20capture%20high-level%2C%20discriminative%2C%20intrinsic%20representations.%0Awhereafter%2C%20the%20decoder%20utilizes%20this%20information%20to%20reconstruct%20the%20pixel%0Asequence%20of%20the%20origin%20input%20and%20calculate%20the%20reconstruction%20loss%20to%20optimize%0Athe%20network.%20Moreover%2C%20the%20backbone%20of%20an%20auto-encoder%20is%20multi-head%0Aself-attention%20that%20can%20consider%20the%20global%20information%20of%20the%20token%20from%20the%0Ainput%2C%20not%20just%20the%20local%20neighborhood.%20This%20allows%20the%20model%20to%20capture%0Ageneralized%20contextual%20information.%20Extensive%20experiments%20demonstrate%20MA2%20has%0ASOTA%20performance%20of%2090.91%25%20accuracy%20on%201%25%20limited%20pathological%20GRF%20samples%20with%0Alabels%2C%20and%20good%20generalization%20ability%20of%2078.57%25%20accuracy%20on%20scalable%0AParkinson%20disease%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMA%255E2%253A%2520A%2520Self-Supervised%2520and%2520Motion%2520Augmenting%2520Autoencoder%2520for%2520Gait-Based%250A%2520%2520Automatic%2520Disease%2520Detection%26entry.906535625%3DYiqun%2520Liu%2520and%2520Ke%2520Zhang%2520and%2520Yin%2520Zhu%26entry.1292438233%3D%2520%2520Ground%2520reaction%2520force%2520%2528GRF%2529%2520is%2520the%2520force%2520exerted%2520by%2520the%2520ground%2520on%2520a%2520body%2520in%250Acontact%2520with%2520it.%2520GRF-based%2520automatic%2520disease%2520detection%2520%2528ADD%2529%2520has%2520become%2520an%250Aemerging%2520medical%2520diagnosis%2520method%252C%2520which%2520aims%2520to%2520learn%2520and%2520identify%2520disease%250Apatterns%2520corresponding%2520to%2520different%2520gait%2520pressures%2520based%2520on%2520deep%2520learning%250Amethods.%2520Although%2520existing%2520ADD%2520methods%2520can%2520save%2520doctors%2520time%2520in%2520making%250Adiagnoses%252C%2520training%2520deep%2520models%2520still%2520struggles%2520with%2520the%2520cost%2520caused%2520by%2520the%250Alabeling%2520engineering%2520for%2520a%2520large%2520number%2520of%2520gait%2520diagnostic%2520data%2520for%2520subjects.%250AOn%2520the%2520other%2520hand%252C%2520the%2520accuracy%2520of%2520the%2520deep%2520model%2520under%2520the%2520unified%2520benchmark%250AGRF%2520dataset%2520and%2520the%2520generalization%2520ability%2520on%2520scalable%2520gait%2520datasets%2520need%2520to%2520be%250Afurther%2520improved.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520MA2%252C%2520a%2520GRF-based%250Aself-supervised%2520and%2520motion%2520augmenting%2520auto-encoder%252C%2520which%2520models%2520the%2520ADD%2520task%250Aas%2520an%2520encoder-decoder%2520paradigm.%2520In%2520the%2520encoder%252C%2520we%2520introduce%2520an%2520embedding%2520block%250Aincluding%2520the%25203-layer%25201D%2520convolution%2520for%2520extracting%2520the%2520token%2520and%2520a%2520mask%250Agenerator%2520to%2520randomly%2520mask%2520out%2520the%2520sequence%2520of%2520tokens%2520to%2520maximize%2520the%2520model%2527s%250Apotential%2520to%2520capture%2520high-level%252C%2520discriminative%252C%2520intrinsic%2520representations.%250Awhereafter%252C%2520the%2520decoder%2520utilizes%2520this%2520information%2520to%2520reconstruct%2520the%2520pixel%250Asequence%2520of%2520the%2520origin%2520input%2520and%2520calculate%2520the%2520reconstruction%2520loss%2520to%2520optimize%250Athe%2520network.%2520Moreover%252C%2520the%2520backbone%2520of%2520an%2520auto-encoder%2520is%2520multi-head%250Aself-attention%2520that%2520can%2520consider%2520the%2520global%2520information%2520of%2520the%2520token%2520from%2520the%250Ainput%252C%2520not%2520just%2520the%2520local%2520neighborhood.%2520This%2520allows%2520the%2520model%2520to%2520capture%250Ageneralized%2520contextual%2520information.%2520Extensive%2520experiments%2520demonstrate%2520MA2%2520has%250ASOTA%2520performance%2520of%252090.91%2525%2520accuracy%2520on%25201%2525%2520limited%2520pathological%2520GRF%2520samples%2520with%250Alabels%252C%2520and%2520good%2520generalization%2520ability%2520of%252078.57%2525%2520accuracy%2520on%2520scalable%250AParkinson%2520disease%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MA%5E2%3A%20A%20Self-Supervised%20and%20Motion%20Augmenting%20Autoencoder%20for%20Gait-Based%0A%20%20Automatic%20Disease%20Detection&entry.906535625=Yiqun%20Liu%20and%20Ke%20Zhang%20and%20Yin%20Zhu&entry.1292438233=%20%20Ground%20reaction%20force%20%28GRF%29%20is%20the%20force%20exerted%20by%20the%20ground%20on%20a%20body%20in%0Acontact%20with%20it.%20GRF-based%20automatic%20disease%20detection%20%28ADD%29%20has%20become%20an%0Aemerging%20medical%20diagnosis%20method%2C%20which%20aims%20to%20learn%20and%20identify%20disease%0Apatterns%20corresponding%20to%20different%20gait%20pressures%20based%20on%20deep%20learning%0Amethods.%20Although%20existing%20ADD%20methods%20can%20save%20doctors%20time%20in%20making%0Adiagnoses%2C%20training%20deep%20models%20still%20struggles%20with%20the%20cost%20caused%20by%20the%0Alabeling%20engineering%20for%20a%20large%20number%20of%20gait%20diagnostic%20data%20for%20subjects.%0AOn%20the%20other%20hand%2C%20the%20accuracy%20of%20the%20deep%20model%20under%20the%20unified%20benchmark%0AGRF%20dataset%20and%20the%20generalization%20ability%20on%20scalable%20gait%20datasets%20need%20to%20be%0Afurther%20improved.%20To%20address%20these%20issues%2C%20we%20propose%20MA2%2C%20a%20GRF-based%0Aself-supervised%20and%20motion%20augmenting%20auto-encoder%2C%20which%20models%20the%20ADD%20task%0Aas%20an%20encoder-decoder%20paradigm.%20In%20the%20encoder%2C%20we%20introduce%20an%20embedding%20block%0Aincluding%20the%203-layer%201D%20convolution%20for%20extracting%20the%20token%20and%20a%20mask%0Agenerator%20to%20randomly%20mask%20out%20the%20sequence%20of%20tokens%20to%20maximize%20the%20model%27s%0Apotential%20to%20capture%20high-level%2C%20discriminative%2C%20intrinsic%20representations.%0Awhereafter%2C%20the%20decoder%20utilizes%20this%20information%20to%20reconstruct%20the%20pixel%0Asequence%20of%20the%20origin%20input%20and%20calculate%20the%20reconstruction%20loss%20to%20optimize%0Athe%20network.%20Moreover%2C%20the%20backbone%20of%20an%20auto-encoder%20is%20multi-head%0Aself-attention%20that%20can%20consider%20the%20global%20information%20of%20the%20token%20from%20the%0Ainput%2C%20not%20just%20the%20local%20neighborhood.%20This%20allows%20the%20model%20to%20capture%0Ageneralized%20contextual%20information.%20Extensive%20experiments%20demonstrate%20MA2%20has%0ASOTA%20performance%20of%2090.91%25%20accuracy%20on%201%25%20limited%20pathological%20GRF%20samples%20with%0Alabels%2C%20and%20good%20generalization%20ability%20of%2078.57%25%20accuracy%20on%20scalable%0AParkinson%20disease%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03129v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


