<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "author": "Ze Ma and Daquan Zhou and Chun-Hsiao Yeh and Xue-She Wang and Xiuyu Li and Huanrui Yang and Zhen Dong and Kurt Keutzer and Jiashi Feng", "abstract": "  Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.\n", "link": "http://arxiv.org/abs/2402.09368v2", "date": "2024-03-20", "relevancy": 3.1896, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.9878}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.719}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6384}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Magic-Me%3A%20Identity-Specific%20Video%20Customized%20Diffusion&body=Title%3A%20Magic-Me%3A%20Identity-Specific%20Video%20Customized%20Diffusion%0AAuthor%3A%20Ze%20Ma%20and%20Daquan%20Zhou%20and%20Chun-Hsiao%20Yeh%20and%20Xue-She%20Wang%20and%20Xiuyu%20Li%20and%20Huanrui%20Yang%20and%20Zhen%20Dong%20and%20Kurt%20Keutzer%20and%20Jiashi%20Feng%0AAbstract%3A%20%20%20Creating%20content%20with%20specified%20identities%20%28ID%29%20has%20attracted%20significant%0Ainterest%20in%20the%20field%20of%20generative%20models.%20In%20the%20field%20of%20text-to-image%0Ageneration%20%28T2I%29%2C%20subject-driven%20creation%20has%20achieved%20great%20progress%20with%20the%0Aidentity%20controlled%20via%20reference%20images.%20However%2C%20its%20extension%20to%20video%0Ageneration%20is%20not%20well%20explored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20subject%20identity%20controllable%20video%20generation%20framework%2C%20termed%0AVideo%20Custom%20Diffusion%20%28VCD%29.%20With%20a%20specified%20identity%20defined%20by%20a%20few%0Aimages%2C%20VCD%20reinforces%20the%20identity%20characteristics%20and%20injects%20frame-wise%0Acorrelation%20at%20the%20initialization%20stage%20for%20stable%20video%20outputs.%20To%20achieve%0Athis%2C%20we%20propose%20three%20novel%20components%20that%20are%20essential%20for%20high-quality%0Aidentity%20preservation%20and%20stable%20video%20generation%3A%201%29%20a%20noise%20initialization%0Amethod%20with%203D%20Gaussian%20Noise%20Prior%20for%20better%20inter-frame%20stability%3B%202%29%20an%20ID%0Amodule%20based%20on%20extended%20Textual%20Inversion%20trained%20with%20the%20cropped%20identity%20to%0Adisentangle%20the%20ID%20information%20from%20the%20background%203%29%20Face%20VCD%20and%20Tiled%20VCD%0Amodules%20to%20reinforce%20faces%20and%20upscale%20the%20video%20to%20higher%20resolution%20while%0Apreserving%20the%20identity%27s%20features.%20We%20conducted%20extensive%20experiments%20to%0Averify%20that%20VCD%20is%20able%20to%20generate%20stable%20videos%20with%20better%20ID%20over%20the%0Abaselines.%20Besides%2C%20with%20the%20transferability%20of%20the%20encoded%20identity%20in%20the%20ID%0Amodule%2C%20VCD%20is%20also%20working%20well%20with%20personalized%20text-to-image%20models%0Aavailable%20publicly.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Zhen-Dong/Magic-Me.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09368v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magic-Me%3A%20Identity-Specific%20Video%20Customized%20Diffusion&entry.906535625=Ze%20Ma%20and%20Daquan%20Zhou%20and%20Chun-Hsiao%20Yeh%20and%20Xue-She%20Wang%20and%20Xiuyu%20Li%20and%20Huanrui%20Yang%20and%20Zhen%20Dong%20and%20Kurt%20Keutzer%20and%20Jiashi%20Feng&entry.1292438233=%20%20Creating%20content%20with%20specified%20identities%20%28ID%29%20has%20attracted%20significant%0Ainterest%20in%20the%20field%20of%20generative%20models.%20In%20the%20field%20of%20text-to-image%0Ageneration%20%28T2I%29%2C%20subject-driven%20creation%20has%20achieved%20great%20progress%20with%20the%0Aidentity%20controlled%20via%20reference%20images.%20However%2C%20its%20extension%20to%20video%0Ageneration%20is%20not%20well%20explored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20subject%20identity%20controllable%20video%20generation%20framework%2C%20termed%0AVideo%20Custom%20Diffusion%20%28VCD%29.%20With%20a%20specified%20identity%20defined%20by%20a%20few%0Aimages%2C%20VCD%20reinforces%20the%20identity%20characteristics%20and%20injects%20frame-wise%0Acorrelation%20at%20the%20initialization%20stage%20for%20stable%20video%20outputs.%20To%20achieve%0Athis%2C%20we%20propose%20three%20novel%20components%20that%20are%20essential%20for%20high-quality%0Aidentity%20preservation%20and%20stable%20video%20generation%3A%201%29%20a%20noise%20initialization%0Amethod%20with%203D%20Gaussian%20Noise%20Prior%20for%20better%20inter-frame%20stability%3B%202%29%20an%20ID%0Amodule%20based%20on%20extended%20Textual%20Inversion%20trained%20with%20the%20cropped%20identity%20to%0Adisentangle%20the%20ID%20information%20from%20the%20background%203%29%20Face%20VCD%20and%20Tiled%20VCD%0Amodules%20to%20reinforce%20faces%20and%20upscale%20the%20video%20to%20higher%20resolution%20while%0Apreserving%20the%20identity%27s%20features.%20We%20conducted%20extensive%20experiments%20to%0Averify%20that%20VCD%20is%20able%20to%20generate%20stable%20videos%20with%20better%20ID%20over%20the%0Abaselines.%20Besides%2C%20with%20the%20transferability%20of%20the%20encoded%20identity%20in%20the%20ID%0Amodule%2C%20VCD%20is%20also%20working%20well%20with%20personalized%20text-to-image%20models%0Aavailable%20publicly.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Zhen-Dong/Magic-Me.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09368v2&entry.124074799=Read"},
{"title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image", "author": "Bowen Zhang and Tianyu Yang and Yu Li and Lei Zhang and Xi Zhao", "abstract": "  3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.\n", "link": "http://arxiv.org/abs/2403.13524v1", "date": "2024-03-20", "relevancy": 2.9067, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5856}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5667}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Compress3D%3A%20a%20Compressed%20Latent%20Space%20for%203D%20Generation%20from%20a%20Single%0A%20%20Image&body=Title%3A%20Compress3D%3A%20a%20Compressed%20Latent%20Space%20for%203D%20Generation%20from%20a%20Single%0A%20%20Image%0AAuthor%3A%20Bowen%20Zhang%20and%20Tianyu%20Yang%20and%20Yu%20Li%20and%20Lei%20Zhang%20and%20Xi%20Zhao%0AAbstract%3A%20%20%203D%20generation%20has%20witnessed%20significant%20advancements%2C%20yet%20efficiently%0Aproducing%20high-quality%203D%20assets%20from%20a%20single%20image%20remains%20challenging.%20In%0Athis%20paper%2C%20we%20present%20a%20triplane%20autoencoder%2C%20which%20encodes%203D%20models%20into%20a%0Acompact%20triplane%20latent%20space%20to%20effectively%20compress%20both%20the%203D%20geometry%20and%0Atexture%20information.%20Within%20the%20autoencoder%20framework%2C%20we%20introduce%20a%203D-aware%0Across-attention%20mechanism%2C%20which%20utilizes%20low-resolution%20latent%20representations%0Ato%20query%20features%20from%20a%20high-resolution%203D%20feature%20volume%2C%20thereby%20enhancing%0Athe%20representation%20capacity%20of%20the%20latent%20space.%20Subsequently%2C%20we%20train%20a%0Adiffusion%20model%20on%20this%20refined%20latent%20space.%20In%20contrast%20to%20solely%20relying%20on%0Aimage%20embedding%20for%203D%20generation%2C%20our%20proposed%20method%20advocates%20for%20the%0Asimultaneous%20utilization%20of%20both%20image%20embedding%20and%20shape%20embedding%20as%0Aconditions.%20Specifically%2C%20the%20shape%20embedding%20is%20estimated%20via%20a%20diffusion%0Aprior%20model%20conditioned%20on%20the%20image%20embedding.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%0Aalgorithms%2C%20achieving%20superior%20performance%20while%20requiring%20less%20training%20data%0Aand%20time.%20Our%20approach%20enables%20the%20generation%20of%20high-quality%203D%20assets%20in%0Amerely%207%20seconds%20on%20a%20single%20A100%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13524v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compress3D%3A%20a%20Compressed%20Latent%20Space%20for%203D%20Generation%20from%20a%20Single%0A%20%20Image&entry.906535625=Bowen%20Zhang%20and%20Tianyu%20Yang%20and%20Yu%20Li%20and%20Lei%20Zhang%20and%20Xi%20Zhao&entry.1292438233=%20%203D%20generation%20has%20witnessed%20significant%20advancements%2C%20yet%20efficiently%0Aproducing%20high-quality%203D%20assets%20from%20a%20single%20image%20remains%20challenging.%20In%0Athis%20paper%2C%20we%20present%20a%20triplane%20autoencoder%2C%20which%20encodes%203D%20models%20into%20a%0Acompact%20triplane%20latent%20space%20to%20effectively%20compress%20both%20the%203D%20geometry%20and%0Atexture%20information.%20Within%20the%20autoencoder%20framework%2C%20we%20introduce%20a%203D-aware%0Across-attention%20mechanism%2C%20which%20utilizes%20low-resolution%20latent%20representations%0Ato%20query%20features%20from%20a%20high-resolution%203D%20feature%20volume%2C%20thereby%20enhancing%0Athe%20representation%20capacity%20of%20the%20latent%20space.%20Subsequently%2C%20we%20train%20a%0Adiffusion%20model%20on%20this%20refined%20latent%20space.%20In%20contrast%20to%20solely%20relying%20on%0Aimage%20embedding%20for%203D%20generation%2C%20our%20proposed%20method%20advocates%20for%20the%0Asimultaneous%20utilization%20of%20both%20image%20embedding%20and%20shape%20embedding%20as%0Aconditions.%20Specifically%2C%20the%20shape%20embedding%20is%20estimated%20via%20a%20diffusion%0Aprior%20model%20conditioned%20on%20the%20image%20embedding.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%0Aalgorithms%2C%20achieving%20superior%20performance%20while%20requiring%20less%20training%20data%0Aand%20time.%20Our%20approach%20enables%20the%20generation%20of%20high-quality%203D%20assets%20in%0Amerely%207%20seconds%20on%20a%20single%20A100%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13524v1&entry.124074799=Read"},
{"title": "ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer", "author": "Hiroki Azuma and Yusuke Matsui and Atsuto Maki", "abstract": "  Deep learning models achieve high accuracy in segmentation tasks among\nothers, yet domain shift often degrades the models' performance, which can be\ncritical in real-world scenarios where no target images are available. This\npaper proposes a zero-shot domain adaptation method based on diffusion models,\ncalled ZoDi, which is two-fold by the design: zero-shot image transfer and\nmodel adaptation. First, we utilize an off-the-shelf diffusion model to\nsynthesize target-like images by transferring the domain of source images to\nthe target domain. In this we specifically try to maintain the layout and\ncontent by utilising layout-to-image diffusion models with stochastic\ninversion. Secondly, we train the model using both source images and\nsynthesized images with the original segmentation maps while maximizing the\nfeature similarity of images from the two domains to learn domain-robust\nrepresentations. Through experiments we show benefits of ZoDi in the task of\nimage segmentation over state-of-the-art methods. It is also more applicable\nthan existing CLIP-based methods because it assumes no specific backbone or\nmodels, and it enables to estimate the model's performance without target\nimages by inspecting generated images. Our implementation will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2403.13652v1", "date": "2024-03-20", "relevancy": 2.7984, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5767}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5514}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ZoDi%3A%20Zero-Shot%20Domain%20Adaptation%20with%20Diffusion-Based%20Image%20Transfer&body=Title%3A%20ZoDi%3A%20Zero-Shot%20Domain%20Adaptation%20with%20Diffusion-Based%20Image%20Transfer%0AAuthor%3A%20Hiroki%20Azuma%20and%20Yusuke%20Matsui%20and%20Atsuto%20Maki%0AAbstract%3A%20%20%20Deep%20learning%20models%20achieve%20high%20accuracy%20in%20segmentation%20tasks%20among%0Aothers%2C%20yet%20domain%20shift%20often%20degrades%20the%20models%27%20performance%2C%20which%20can%20be%0Acritical%20in%20real-world%20scenarios%20where%20no%20target%20images%20are%20available.%20This%0Apaper%20proposes%20a%20zero-shot%20domain%20adaptation%20method%20based%20on%20diffusion%20models%2C%0Acalled%20ZoDi%2C%20which%20is%20two-fold%20by%20the%20design%3A%20zero-shot%20image%20transfer%20and%0Amodel%20adaptation.%20First%2C%20we%20utilize%20an%20off-the-shelf%20diffusion%20model%20to%0Asynthesize%20target-like%20images%20by%20transferring%20the%20domain%20of%20source%20images%20to%0Athe%20target%20domain.%20In%20this%20we%20specifically%20try%20to%20maintain%20the%20layout%20and%0Acontent%20by%20utilising%20layout-to-image%20diffusion%20models%20with%20stochastic%0Ainversion.%20Secondly%2C%20we%20train%20the%20model%20using%20both%20source%20images%20and%0Asynthesized%20images%20with%20the%20original%20segmentation%20maps%20while%20maximizing%20the%0Afeature%20similarity%20of%20images%20from%20the%20two%20domains%20to%20learn%20domain-robust%0Arepresentations.%20Through%20experiments%20we%20show%20benefits%20of%20ZoDi%20in%20the%20task%20of%0Aimage%20segmentation%20over%20state-of-the-art%20methods.%20It%20is%20also%20more%20applicable%0Athan%20existing%20CLIP-based%20methods%20because%20it%20assumes%20no%20specific%20backbone%20or%0Amodels%2C%20and%20it%20enables%20to%20estimate%20the%20model%27s%20performance%20without%20target%0Aimages%20by%20inspecting%20generated%20images.%20Our%20implementation%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZoDi%3A%20Zero-Shot%20Domain%20Adaptation%20with%20Diffusion-Based%20Image%20Transfer&entry.906535625=Hiroki%20Azuma%20and%20Yusuke%20Matsui%20and%20Atsuto%20Maki&entry.1292438233=%20%20Deep%20learning%20models%20achieve%20high%20accuracy%20in%20segmentation%20tasks%20among%0Aothers%2C%20yet%20domain%20shift%20often%20degrades%20the%20models%27%20performance%2C%20which%20can%20be%0Acritical%20in%20real-world%20scenarios%20where%20no%20target%20images%20are%20available.%20This%0Apaper%20proposes%20a%20zero-shot%20domain%20adaptation%20method%20based%20on%20diffusion%20models%2C%0Acalled%20ZoDi%2C%20which%20is%20two-fold%20by%20the%20design%3A%20zero-shot%20image%20transfer%20and%0Amodel%20adaptation.%20First%2C%20we%20utilize%20an%20off-the-shelf%20diffusion%20model%20to%0Asynthesize%20target-like%20images%20by%20transferring%20the%20domain%20of%20source%20images%20to%0Athe%20target%20domain.%20In%20this%20we%20specifically%20try%20to%20maintain%20the%20layout%20and%0Acontent%20by%20utilising%20layout-to-image%20diffusion%20models%20with%20stochastic%0Ainversion.%20Secondly%2C%20we%20train%20the%20model%20using%20both%20source%20images%20and%0Asynthesized%20images%20with%20the%20original%20segmentation%20maps%20while%20maximizing%20the%0Afeature%20similarity%20of%20images%20from%20the%20two%20domains%20to%20learn%20domain-robust%0Arepresentations.%20Through%20experiments%20we%20show%20benefits%20of%20ZoDi%20in%20the%20task%20of%0Aimage%20segmentation%20over%20state-of-the-art%20methods.%20It%20is%20also%20more%20applicable%0Athan%20existing%20CLIP-based%20methods%20because%20it%20assumes%20no%20specific%20backbone%20or%0Amodels%2C%20and%20it%20enables%20to%20estimate%20the%20model%27s%20performance%20without%20target%0Aimages%20by%20inspecting%20generated%20images.%20Our%20implementation%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13652v1&entry.124074799=Read"},
{"title": "Lossless Point Cloud Geometry and Attribute Compression Using a Learned\n  Conditional Probability Model", "author": "Dat Thanh Nguyen and Andre Kaup", "abstract": "  In recent years, we have witnessed the presence of point cloud data in many\naspects of our life, from immersive media, autonomous driving to healthcare,\nalthough at the cost of a tremendous amount of data. In this paper, we present\nan efficient lossless point cloud compression method that uses sparse\ntensor-based deep neural networks to learn point cloud geometry and color\nprobability distributions. Our method represents a point cloud with both\noccupancy feature and three attribute features at different bit depths in a\nunified sparse representation. This allows us to efficiently exploit\nfeature-wise and point-wise dependencies within point clouds using a sparse\ntensor-based neural network and thus build an accurate auto-regressive context\nmodel for an arithmetic coder. To the best of our knowledge, this is the first\nlearning-based lossless point cloud geometry and attribute compression\napproach. Compared with the-state-of-the-art lossless point cloud compression\nmethod from Moving Picture Experts Group (MPEG), our method achieves 22.6%\nreduction in total bitrate on a diverse set of test point clouds while having\n49.0% and 18.3% rate reduction on geometry and color attribute component,\nrespectively.\n", "link": "http://arxiv.org/abs/2303.06519v2", "date": "2024-03-20", "relevancy": 2.7353, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5812}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5417}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5183}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lossless%20Point%20Cloud%20Geometry%20and%20Attribute%20Compression%20Using%20a%20Learned%0A%20%20Conditional%20Probability%20Model&body=Title%3A%20Lossless%20Point%20Cloud%20Geometry%20and%20Attribute%20Compression%20Using%20a%20Learned%0A%20%20Conditional%20Probability%20Model%0AAuthor%3A%20Dat%20Thanh%20Nguyen%20and%20Andre%20Kaup%0AAbstract%3A%20%20%20In%20recent%20years%2C%20we%20have%20witnessed%20the%20presence%20of%20point%20cloud%20data%20in%20many%0Aaspects%20of%20our%20life%2C%20from%20immersive%20media%2C%20autonomous%20driving%20to%20healthcare%2C%0Aalthough%20at%20the%20cost%20of%20a%20tremendous%20amount%20of%20data.%20In%20this%20paper%2C%20we%20present%0Aan%20efficient%20lossless%20point%20cloud%20compression%20method%20that%20uses%20sparse%0Atensor-based%20deep%20neural%20networks%20to%20learn%20point%20cloud%20geometry%20and%20color%0Aprobability%20distributions.%20Our%20method%20represents%20a%20point%20cloud%20with%20both%0Aoccupancy%20feature%20and%20three%20attribute%20features%20at%20different%20bit%20depths%20in%20a%0Aunified%20sparse%20representation.%20This%20allows%20us%20to%20efficiently%20exploit%0Afeature-wise%20and%20point-wise%20dependencies%20within%20point%20clouds%20using%20a%20sparse%0Atensor-based%20neural%20network%20and%20thus%20build%20an%20accurate%20auto-regressive%20context%0Amodel%20for%20an%20arithmetic%20coder.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Alearning-based%20lossless%20point%20cloud%20geometry%20and%20attribute%20compression%0Aapproach.%20Compared%20with%20the-state-of-the-art%20lossless%20point%20cloud%20compression%0Amethod%20from%20Moving%20Picture%20Experts%20Group%20%28MPEG%29%2C%20our%20method%20achieves%2022.6%25%0Areduction%20in%20total%20bitrate%20on%20a%20diverse%20set%20of%20test%20point%20clouds%20while%20having%0A49.0%25%20and%2018.3%25%20rate%20reduction%20on%20geometry%20and%20color%20attribute%20component%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06519v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossless%20Point%20Cloud%20Geometry%20and%20Attribute%20Compression%20Using%20a%20Learned%0A%20%20Conditional%20Probability%20Model&entry.906535625=Dat%20Thanh%20Nguyen%20and%20Andre%20Kaup&entry.1292438233=%20%20In%20recent%20years%2C%20we%20have%20witnessed%20the%20presence%20of%20point%20cloud%20data%20in%20many%0Aaspects%20of%20our%20life%2C%20from%20immersive%20media%2C%20autonomous%20driving%20to%20healthcare%2C%0Aalthough%20at%20the%20cost%20of%20a%20tremendous%20amount%20of%20data.%20In%20this%20paper%2C%20we%20present%0Aan%20efficient%20lossless%20point%20cloud%20compression%20method%20that%20uses%20sparse%0Atensor-based%20deep%20neural%20networks%20to%20learn%20point%20cloud%20geometry%20and%20color%0Aprobability%20distributions.%20Our%20method%20represents%20a%20point%20cloud%20with%20both%0Aoccupancy%20feature%20and%20three%20attribute%20features%20at%20different%20bit%20depths%20in%20a%0Aunified%20sparse%20representation.%20This%20allows%20us%20to%20efficiently%20exploit%0Afeature-wise%20and%20point-wise%20dependencies%20within%20point%20clouds%20using%20a%20sparse%0Atensor-based%20neural%20network%20and%20thus%20build%20an%20accurate%20auto-regressive%20context%0Amodel%20for%20an%20arithmetic%20coder.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Alearning-based%20lossless%20point%20cloud%20geometry%20and%20attribute%20compression%0Aapproach.%20Compared%20with%20the-state-of-the-art%20lossless%20point%20cloud%20compression%0Amethod%20from%20Moving%20Picture%20Experts%20Group%20%28MPEG%29%2C%20our%20method%20achieves%2022.6%25%0Areduction%20in%20total%20bitrate%20on%20a%20diverse%20set%20of%20test%20point%20clouds%20while%20having%0A49.0%25%20and%2018.3%25%20rate%20reduction%20on%20geometry%20and%20color%20attribute%20component%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06519v2&entry.124074799=Read"},
{"title": "Auto-Vocabulary Semantic Segmentation", "author": "Osman \u00dclger and Maksymilian Kulicki and Yuki Asano and Martin R. Oswald", "abstract": "  Open-ended image understanding tasks gained significant attention from the\nresearch community, particularly with the emergence of Vision-Language Models.\nOpen-Vocabulary Segmentation (OVS) methods are capable of performing semantic\nsegmentation without relying on a fixed vocabulary, and in some cases, they\noperate without the need for training or fine-tuning. However, OVS methods\ntypically require users to specify the vocabulary based on the task or dataset\nat hand. In this paper, we introduce \\textit{Auto-Vocabulary Semantic\nSegmentation (AVS)}, advancing open-ended image understanding by eliminating\nthe necessity to predefine object categories for segmentation. Our approach,\n\\ours, presents a framework that autonomously identifies relevant class names\nusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.\nGiven that open-ended object category predictions cannot be directly compared\nwith a fixed ground truth, we develop a Large Language Model-based\nAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automatically\ngenerated class names and their corresponding segments. Our method sets new\nbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapes\nfor AVS and showcases competitive performance to OVS methods that require\nspecified class names.\n", "link": "http://arxiv.org/abs/2312.04539v2", "date": "2024-03-20", "relevancy": 2.6863, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5166}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Auto-Vocabulary%20Semantic%20Segmentation&body=Title%3A%20Auto-Vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Osman%20%C3%9Clger%20and%20Maksymilian%20Kulicki%20and%20Yuki%20Asano%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Open-ended%20image%20understanding%20tasks%20gained%20significant%20attention%20from%20the%0Aresearch%20community%2C%20particularly%20with%20the%20emergence%20of%20Vision-Language%20Models.%0AOpen-Vocabulary%20Segmentation%20%28OVS%29%20methods%20are%20capable%20of%20performing%20semantic%0Asegmentation%20without%20relying%20on%20a%20fixed%20vocabulary%2C%20and%20in%20some%20cases%2C%20they%0Aoperate%20without%20the%20need%20for%20training%20or%20fine-tuning.%20However%2C%20OVS%20methods%0Atypically%20require%20users%20to%20specify%20the%20vocabulary%20based%20on%20the%20task%20or%20dataset%0Aat%20hand.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextit%7BAuto-Vocabulary%20Semantic%0ASegmentation%20%28AVS%29%7D%2C%20advancing%20open-ended%20image%20understanding%20by%20eliminating%0Athe%20necessity%20to%20predefine%20object%20categories%20for%20segmentation.%20Our%20approach%2C%0A%5Cours%2C%20presents%20a%20framework%20that%20autonomously%20identifies%20relevant%20class%20names%0Ausing%20enhanced%20BLIP%20embeddings%2C%20which%20are%20utilized%20for%20segmentation%20afterwards.%0AGiven%20that%20open-ended%20object%20category%20predictions%20cannot%20be%20directly%20compared%0Awith%20a%20fixed%20ground%20truth%2C%20we%20develop%20a%20Large%20Language%20Model-based%0AAuto-Vocabulary%20Evaluator%20%28LAVE%29%20to%20efficiently%20evaluate%20the%20automatically%0Agenerated%20class%20names%20and%20their%20corresponding%20segments.%20Our%20method%20sets%20new%0Abenchmarks%20on%20datasets%20such%20as%20PASCAL%20VOC%20and%20Context%2C%20ADE20K%2C%20and%20Cityscapes%0Afor%20AVS%20and%20showcases%20competitive%20performance%20to%20OVS%20methods%20that%20require%0Aspecified%20class%20names.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04539v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Vocabulary%20Semantic%20Segmentation&entry.906535625=Osman%20%C3%9Clger%20and%20Maksymilian%20Kulicki%20and%20Yuki%20Asano%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Open-ended%20image%20understanding%20tasks%20gained%20significant%20attention%20from%20the%0Aresearch%20community%2C%20particularly%20with%20the%20emergence%20of%20Vision-Language%20Models.%0AOpen-Vocabulary%20Segmentation%20%28OVS%29%20methods%20are%20capable%20of%20performing%20semantic%0Asegmentation%20without%20relying%20on%20a%20fixed%20vocabulary%2C%20and%20in%20some%20cases%2C%20they%0Aoperate%20without%20the%20need%20for%20training%20or%20fine-tuning.%20However%2C%20OVS%20methods%0Atypically%20require%20users%20to%20specify%20the%20vocabulary%20based%20on%20the%20task%20or%20dataset%0Aat%20hand.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextit%7BAuto-Vocabulary%20Semantic%0ASegmentation%20%28AVS%29%7D%2C%20advancing%20open-ended%20image%20understanding%20by%20eliminating%0Athe%20necessity%20to%20predefine%20object%20categories%20for%20segmentation.%20Our%20approach%2C%0A%5Cours%2C%20presents%20a%20framework%20that%20autonomously%20identifies%20relevant%20class%20names%0Ausing%20enhanced%20BLIP%20embeddings%2C%20which%20are%20utilized%20for%20segmentation%20afterwards.%0AGiven%20that%20open-ended%20object%20category%20predictions%20cannot%20be%20directly%20compared%0Awith%20a%20fixed%20ground%20truth%2C%20we%20develop%20a%20Large%20Language%20Model-based%0AAuto-Vocabulary%20Evaluator%20%28LAVE%29%20to%20efficiently%20evaluate%20the%20automatically%0Agenerated%20class%20names%20and%20their%20corresponding%20segments.%20Our%20method%20sets%20new%0Abenchmarks%20on%20datasets%20such%20as%20PASCAL%20VOC%20and%20Context%2C%20ADE20K%2C%20and%20Cityscapes%0Afor%20AVS%20and%20showcases%20competitive%20performance%20to%20OVS%20methods%20that%20require%0Aspecified%20class%20names.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04539v2&entry.124074799=Read"},
{"title": "Towards Principled Representation Learning from Videos for Reinforcement\n  Learning", "author": "Dipendra Misra and Akanksha Saran and Tengyang Xie and Alex Lamb and John Langford", "abstract": "  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n", "link": "http://arxiv.org/abs/2403.13765v1", "date": "2024-03-20", "relevancy": 2.6854, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5348}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5304}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Principled%20Representation%20Learning%20from%20Videos%20for%20Reinforcement%0A%20%20Learning&body=Title%3A%20Towards%20Principled%20Representation%20Learning%20from%20Videos%20for%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Dipendra%20Misra%20and%20Akanksha%20Saran%20and%20Tengyang%20Xie%20and%20Alex%20Lamb%20and%20John%20Langford%0AAbstract%3A%20%20%20We%20study%20pre-training%20representations%20for%20decision-making%20using%20video%20data%2C%0Awhich%20is%20abundantly%20available%20for%20tasks%20such%20as%20game%20agents%20and%20software%0Atesting.%20Even%20though%20significant%20empirical%20advances%20have%20been%20made%20on%20this%0Aproblem%2C%20a%20theoretical%20understanding%20remains%20absent.%20We%20initiate%20the%0Atheoretical%20investigation%20into%20principled%20approaches%20for%20representation%0Alearning%20and%20focus%20on%20learning%20the%20latent%20state%20representations%20of%20the%0Aunderlying%20MDP%20using%20video%20data.%20We%20study%20two%20types%20of%20settings%3A%20one%20where%0Athere%20is%20iid%20noise%20in%20the%20observation%2C%20and%20a%20more%20challenging%20setting%20where%0Athere%20is%20also%20the%20presence%20of%20exogenous%20noise%2C%20which%20is%20non-iid%20noise%20that%20is%0Atemporally%20correlated%2C%20such%20as%20the%20motion%20of%20people%20or%20cars%20in%20the%20background.%0AWe%20study%20three%20commonly%20used%20approaches%3A%20autoencoding%2C%20temporal%20contrastive%0Alearning%2C%20and%20forward%20modeling.%20We%20prove%20upper%20bounds%20for%20temporal%20contrastive%0Alearning%20and%20forward%20modeling%20in%20the%20presence%20of%20only%20iid%20noise.%20We%20show%20that%0Athese%20approaches%20can%20learn%20the%20latent%20state%20and%20use%20it%20to%20do%20efficient%0Adownstream%20RL%20with%20polynomial%20sample%20complexity.%20When%20exogenous%20noise%20is%20also%0Apresent%2C%20we%20establish%20a%20lower%20bound%20result%20showing%20that%20the%20sample%20complexity%0Aof%20learning%20from%20video%20data%20can%20be%20exponentially%20worse%20than%20learning%20from%0Aaction-labeled%20trajectory%20data.%20This%20partially%20explains%20why%20reinforcement%0Alearning%20with%20video%20pre-training%20is%20hard.%20We%20evaluate%20these%20representational%0Alearning%20methods%20in%20two%20visual%20domains%2C%20yielding%20results%20that%20are%20consistent%0Awith%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13765v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Principled%20Representation%20Learning%20from%20Videos%20for%20Reinforcement%0A%20%20Learning&entry.906535625=Dipendra%20Misra%20and%20Akanksha%20Saran%20and%20Tengyang%20Xie%20and%20Alex%20Lamb%20and%20John%20Langford&entry.1292438233=%20%20We%20study%20pre-training%20representations%20for%20decision-making%20using%20video%20data%2C%0Awhich%20is%20abundantly%20available%20for%20tasks%20such%20as%20game%20agents%20and%20software%0Atesting.%20Even%20though%20significant%20empirical%20advances%20have%20been%20made%20on%20this%0Aproblem%2C%20a%20theoretical%20understanding%20remains%20absent.%20We%20initiate%20the%0Atheoretical%20investigation%20into%20principled%20approaches%20for%20representation%0Alearning%20and%20focus%20on%20learning%20the%20latent%20state%20representations%20of%20the%0Aunderlying%20MDP%20using%20video%20data.%20We%20study%20two%20types%20of%20settings%3A%20one%20where%0Athere%20is%20iid%20noise%20in%20the%20observation%2C%20and%20a%20more%20challenging%20setting%20where%0Athere%20is%20also%20the%20presence%20of%20exogenous%20noise%2C%20which%20is%20non-iid%20noise%20that%20is%0Atemporally%20correlated%2C%20such%20as%20the%20motion%20of%20people%20or%20cars%20in%20the%20background.%0AWe%20study%20three%20commonly%20used%20approaches%3A%20autoencoding%2C%20temporal%20contrastive%0Alearning%2C%20and%20forward%20modeling.%20We%20prove%20upper%20bounds%20for%20temporal%20contrastive%0Alearning%20and%20forward%20modeling%20in%20the%20presence%20of%20only%20iid%20noise.%20We%20show%20that%0Athese%20approaches%20can%20learn%20the%20latent%20state%20and%20use%20it%20to%20do%20efficient%0Adownstream%20RL%20with%20polynomial%20sample%20complexity.%20When%20exogenous%20noise%20is%20also%0Apresent%2C%20we%20establish%20a%20lower%20bound%20result%20showing%20that%20the%20sample%20complexity%0Aof%20learning%20from%20video%20data%20can%20be%20exponentially%20worse%20than%20learning%20from%0Aaction-labeled%20trajectory%20data.%20This%20partially%20explains%20why%20reinforcement%0Alearning%20with%20video%20pre-training%20is%20hard.%20We%20evaluate%20these%20representational%0Alearning%20methods%20in%20two%20visual%20domains%2C%20yielding%20results%20that%20are%20consistent%0Awith%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13765v1&entry.124074799=Read"},
{"title": "SPTNet: An Efficient Alternative Framework for Generalized Category\n  Discovery with Spatial Prompt Tuning", "author": "Hongjun Wang and Sagar Vaze and Kai Han", "abstract": "  Generalized Category Discovery (GCD) aims to classify unlabelled images from\nboth `seen' and `unseen' classes by transferring knowledge from a set of\nlabelled `seen' class images. A key theme in existing GCD approaches is\nadapting large-scale pre-trained models for the GCD task. An alternate\nperspective, however, is to adapt the data representation itself for better\nalignment with the pre-trained model. As such, in this paper, we introduce a\ntwo-stage adaptation approach termed SPTNet, which iteratively optimizes model\nparameters (i.e., model-finetuning) and data parameters (i.e., prompt\nlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)\nwhich considers the spatial property of image data, enabling the method to\nbetter focus on object parts, which can transfer between seen and unseen\nclasses. We thoroughly evaluate our SPTNet on standard benchmarks and\ndemonstrate that our method outperforms existing GCD methods. Notably, we find\nour method achieves an average accuracy of 61.4% on the SSB, surpassing prior\nstate-of-the-art methods by approximately 10%. The improvement is particularly\nremarkable as our method yields extra parameters amounting to only 0.117% of\nthose in the backbone architecture. Project page:\nhttps://visual-ai.github.io/sptnet.\n", "link": "http://arxiv.org/abs/2403.13684v1", "date": "2024-03-20", "relevancy": 2.683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5359}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5318}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPTNet%3A%20An%20Efficient%20Alternative%20Framework%20for%20Generalized%20Category%0A%20%20Discovery%20with%20Spatial%20Prompt%20Tuning&body=Title%3A%20SPTNet%3A%20An%20Efficient%20Alternative%20Framework%20for%20Generalized%20Category%0A%20%20Discovery%20with%20Spatial%20Prompt%20Tuning%0AAuthor%3A%20Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20classify%20unlabelled%20images%20from%0Aboth%20%60seen%27%20and%20%60unseen%27%20classes%20by%20transferring%20knowledge%20from%20a%20set%20of%0Alabelled%20%60seen%27%20class%20images.%20A%20key%20theme%20in%20existing%20GCD%20approaches%20is%0Aadapting%20large-scale%20pre-trained%20models%20for%20the%20GCD%20task.%20An%20alternate%0Aperspective%2C%20however%2C%20is%20to%20adapt%20the%20data%20representation%20itself%20for%20better%0Aalignment%20with%20the%20pre-trained%20model.%20As%20such%2C%20in%20this%20paper%2C%20we%20introduce%20a%0Atwo-stage%20adaptation%20approach%20termed%20SPTNet%2C%20which%20iteratively%20optimizes%20model%0Aparameters%20%28i.e.%2C%20model-finetuning%29%20and%20data%20parameters%20%28i.e.%2C%20prompt%0Alearning%29.%20Furthermore%2C%20we%20propose%20a%20novel%20spatial%20prompt%20tuning%20method%20%28SPT%29%0Awhich%20considers%20the%20spatial%20property%20of%20image%20data%2C%20enabling%20the%20method%20to%0Abetter%20focus%20on%20object%20parts%2C%20which%20can%20transfer%20between%20seen%20and%20unseen%0Aclasses.%20We%20thoroughly%20evaluate%20our%20SPTNet%20on%20standard%20benchmarks%20and%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20GCD%20methods.%20Notably%2C%20we%20find%0Aour%20method%20achieves%20an%20average%20accuracy%20of%2061.4%25%20on%20the%20SSB%2C%20surpassing%20prior%0Astate-of-the-art%20methods%20by%20approximately%2010%25.%20The%20improvement%20is%20particularly%0Aremarkable%20as%20our%20method%20yields%20extra%20parameters%20amounting%20to%20only%200.117%25%20of%0Athose%20in%20the%20backbone%20architecture.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/sptnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13684v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPTNet%3A%20An%20Efficient%20Alternative%20Framework%20for%20Generalized%20Category%0A%20%20Discovery%20with%20Spatial%20Prompt%20Tuning&entry.906535625=Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20classify%20unlabelled%20images%20from%0Aboth%20%60seen%27%20and%20%60unseen%27%20classes%20by%20transferring%20knowledge%20from%20a%20set%20of%0Alabelled%20%60seen%27%20class%20images.%20A%20key%20theme%20in%20existing%20GCD%20approaches%20is%0Aadapting%20large-scale%20pre-trained%20models%20for%20the%20GCD%20task.%20An%20alternate%0Aperspective%2C%20however%2C%20is%20to%20adapt%20the%20data%20representation%20itself%20for%20better%0Aalignment%20with%20the%20pre-trained%20model.%20As%20such%2C%20in%20this%20paper%2C%20we%20introduce%20a%0Atwo-stage%20adaptation%20approach%20termed%20SPTNet%2C%20which%20iteratively%20optimizes%20model%0Aparameters%20%28i.e.%2C%20model-finetuning%29%20and%20data%20parameters%20%28i.e.%2C%20prompt%0Alearning%29.%20Furthermore%2C%20we%20propose%20a%20novel%20spatial%20prompt%20tuning%20method%20%28SPT%29%0Awhich%20considers%20the%20spatial%20property%20of%20image%20data%2C%20enabling%20the%20method%20to%0Abetter%20focus%20on%20object%20parts%2C%20which%20can%20transfer%20between%20seen%20and%20unseen%0Aclasses.%20We%20thoroughly%20evaluate%20our%20SPTNet%20on%20standard%20benchmarks%20and%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20GCD%20methods.%20Notably%2C%20we%20find%0Aour%20method%20achieves%20an%20average%20accuracy%20of%2061.4%25%20on%20the%20SSB%2C%20surpassing%20prior%0Astate-of-the-art%20methods%20by%20approximately%2010%25.%20The%20improvement%20is%20particularly%0Aremarkable%20as%20our%20method%20yields%20extra%20parameters%20amounting%20to%20only%200.117%25%20of%0Athose%20in%20the%20backbone%20architecture.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/sptnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13684v1&entry.124074799=Read"},
{"title": "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via\n  Comparing and Matching", "author": "Yuan Sun and Xuan Wang and Yunfan Zhang and Jie Zhang and Caigui Jiang and Yu Guo and Fei Wang", "abstract": "  We present a method named iComMa to address the 6D camera pose estimation\nproblem in computer vision. Conventional pose estimation methods typically rely\non the target's CAD model or necessitate specific network training tailored to\nparticular object classes. Some existing methods have achieved promising\nresults in mesh-free object and scene pose estimation by inverting the Neural\nRadiance Fields (NeRF). However, they still struggle with adverse\ninitializations such as large rotations and translations. To address this\nissue, we propose an efficient method for accurate camera pose estimation by\ninverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based\ndifferentiable framework optimizes camera pose by minimizing the residual\nbetween the query image and the rendered image, requiring no training. An\nend-to-end matching module is designed to enhance the model's robustness\nagainst adverse initializations, while minimizing pixel-level comparing loss\naids in precise pose estimation. Experimental results on synthetic and complex\nreal-world data demonstrate the effectiveness of the proposed approach in\nchallenging conditions and the accuracy of camera pose estimation.\n", "link": "http://arxiv.org/abs/2312.09031v2", "date": "2024-03-20", "relevancy": 2.6395, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20iComMa%3A%20Inverting%203D%20Gaussian%20Splatting%20for%20Camera%20Pose%20Estimation%20via%0A%20%20Comparing%20and%20Matching&body=Title%3A%20iComMa%3A%20Inverting%203D%20Gaussian%20Splatting%20for%20Camera%20Pose%20Estimation%20via%0A%20%20Comparing%20and%20Matching%0AAuthor%3A%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Yunfan%20Zhang%20and%20Jie%20Zhang%20and%20Caigui%20Jiang%20and%20Yu%20Guo%20and%20Fei%20Wang%0AAbstract%3A%20%20%20We%20present%20a%20method%20named%20iComMa%20to%20address%20the%206D%20camera%20pose%20estimation%0Aproblem%20in%20computer%20vision.%20Conventional%20pose%20estimation%20methods%20typically%20rely%0Aon%20the%20target%27s%20CAD%20model%20or%20necessitate%20specific%20network%20training%20tailored%20to%0Aparticular%20object%20classes.%20Some%20existing%20methods%20have%20achieved%20promising%0Aresults%20in%20mesh-free%20object%20and%20scene%20pose%20estimation%20by%20inverting%20the%20Neural%0ARadiance%20Fields%20%28NeRF%29.%20However%2C%20they%20still%20struggle%20with%20adverse%0Ainitializations%20such%20as%20large%20rotations%20and%20translations.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20efficient%20method%20for%20accurate%20camera%20pose%20estimation%20by%0Ainverting%203D%20Gaussian%20Splatting%20%283DGS%29.%20Specifically%2C%20a%20gradient-based%0Adifferentiable%20framework%20optimizes%20camera%20pose%20by%20minimizing%20the%20residual%0Abetween%20the%20query%20image%20and%20the%20rendered%20image%2C%20requiring%20no%20training.%20An%0Aend-to-end%20matching%20module%20is%20designed%20to%20enhance%20the%20model%27s%20robustness%0Aagainst%20adverse%20initializations%2C%20while%20minimizing%20pixel-level%20comparing%20loss%0Aaids%20in%20precise%20pose%20estimation.%20Experimental%20results%20on%20synthetic%20and%20complex%0Areal-world%20data%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%0Achallenging%20conditions%20and%20the%20accuracy%20of%20camera%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09031v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iComMa%3A%20Inverting%203D%20Gaussian%20Splatting%20for%20Camera%20Pose%20Estimation%20via%0A%20%20Comparing%20and%20Matching&entry.906535625=Yuan%20Sun%20and%20Xuan%20Wang%20and%20Yunfan%20Zhang%20and%20Jie%20Zhang%20and%20Caigui%20Jiang%20and%20Yu%20Guo%20and%20Fei%20Wang&entry.1292438233=%20%20We%20present%20a%20method%20named%20iComMa%20to%20address%20the%206D%20camera%20pose%20estimation%0Aproblem%20in%20computer%20vision.%20Conventional%20pose%20estimation%20methods%20typically%20rely%0Aon%20the%20target%27s%20CAD%20model%20or%20necessitate%20specific%20network%20training%20tailored%20to%0Aparticular%20object%20classes.%20Some%20existing%20methods%20have%20achieved%20promising%0Aresults%20in%20mesh-free%20object%20and%20scene%20pose%20estimation%20by%20inverting%20the%20Neural%0ARadiance%20Fields%20%28NeRF%29.%20However%2C%20they%20still%20struggle%20with%20adverse%0Ainitializations%20such%20as%20large%20rotations%20and%20translations.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20efficient%20method%20for%20accurate%20camera%20pose%20estimation%20by%0Ainverting%203D%20Gaussian%20Splatting%20%283DGS%29.%20Specifically%2C%20a%20gradient-based%0Adifferentiable%20framework%20optimizes%20camera%20pose%20by%20minimizing%20the%20residual%0Abetween%20the%20query%20image%20and%20the%20rendered%20image%2C%20requiring%20no%20training.%20An%0Aend-to-end%20matching%20module%20is%20designed%20to%20enhance%20the%20model%27s%20robustness%0Aagainst%20adverse%20initializations%2C%20while%20minimizing%20pixel-level%20comparing%20loss%0Aaids%20in%20precise%20pose%20estimation.%20Experimental%20results%20on%20synthetic%20and%20complex%0Areal-world%20data%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%0Achallenging%20conditions%20and%20the%20accuracy%20of%20camera%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09031v2&entry.124074799=Read"},
{"title": "Insight Into the Collocation of Multi-Source Satellite Imagery for\n  Multi-Scale Vessel Detection", "author": "Tran-Vu La and Minh-Tan Pham and Marco Chini", "abstract": "  Ship detection from satellite imagery using Deep Learning (DL) is an\nindispensable solution for maritime surveillance. However, applying DL models\ntrained on one dataset to others having differences in spatial resolution and\nradiometric features requires many adjustments. To overcome this issue, this\npaper focused on the DL models trained on datasets that consist of different\noptical images and a combination of radar and optical data. When dealing with a\nlimited number of training images, the performance of DL models via this\napproach was satisfactory. They could improve 5-20% of average precision,\ndepending on the optical images tested. Likewise, DL models trained on the\ncombined optical and radar dataset could be applied to both optical and radar\nimages. Our experiments showed that the models trained on an optical dataset\ncould be used for radar images, while those trained on a radar dataset offered\nvery poor scores when applied to optical images.\n", "link": "http://arxiv.org/abs/2403.13698v1", "date": "2024-03-20", "relevancy": 2.62, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5154}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection&body=Title%3A%20Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection%0AAuthor%3A%20Tran-Vu%20La%20and%20Minh-Tan%20Pham%20and%20Marco%20Chini%0AAbstract%3A%20%20%20Ship%20detection%20from%20satellite%20imagery%20using%20Deep%20Learning%20%28DL%29%20is%20an%0Aindispensable%20solution%20for%20maritime%20surveillance.%20However%2C%20applying%20DL%20models%0Atrained%20on%20one%20dataset%20to%20others%20having%20differences%20in%20spatial%20resolution%20and%0Aradiometric%20features%20requires%20many%20adjustments.%20To%20overcome%20this%20issue%2C%20this%0Apaper%20focused%20on%20the%20DL%20models%20trained%20on%20datasets%20that%20consist%20of%20different%0Aoptical%20images%20and%20a%20combination%20of%20radar%20and%20optical%20data.%20When%20dealing%20with%20a%0Alimited%20number%20of%20training%20images%2C%20the%20performance%20of%20DL%20models%20via%20this%0Aapproach%20was%20satisfactory.%20They%20could%20improve%205-20%25%20of%20average%20precision%2C%0Adepending%20on%20the%20optical%20images%20tested.%20Likewise%2C%20DL%20models%20trained%20on%20the%0Acombined%20optical%20and%20radar%20dataset%20could%20be%20applied%20to%20both%20optical%20and%20radar%0Aimages.%20Our%20experiments%20showed%20that%20the%20models%20trained%20on%20an%20optical%20dataset%0Acould%20be%20used%20for%20radar%20images%2C%20while%20those%20trained%20on%20a%20radar%20dataset%20offered%0Avery%20poor%20scores%20when%20applied%20to%20optical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13698v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection&entry.906535625=Tran-Vu%20La%20and%20Minh-Tan%20Pham%20and%20Marco%20Chini&entry.1292438233=%20%20Ship%20detection%20from%20satellite%20imagery%20using%20Deep%20Learning%20%28DL%29%20is%20an%0Aindispensable%20solution%20for%20maritime%20surveillance.%20However%2C%20applying%20DL%20models%0Atrained%20on%20one%20dataset%20to%20others%20having%20differences%20in%20spatial%20resolution%20and%0Aradiometric%20features%20requires%20many%20adjustments.%20To%20overcome%20this%20issue%2C%20this%0Apaper%20focused%20on%20the%20DL%20models%20trained%20on%20datasets%20that%20consist%20of%20different%0Aoptical%20images%20and%20a%20combination%20of%20radar%20and%20optical%20data.%20When%20dealing%20with%20a%0Alimited%20number%20of%20training%20images%2C%20the%20performance%20of%20DL%20models%20via%20this%0Aapproach%20was%20satisfactory.%20They%20could%20improve%205-20%25%20of%20average%20precision%2C%0Adepending%20on%20the%20optical%20images%20tested.%20Likewise%2C%20DL%20models%20trained%20on%20the%0Acombined%20optical%20and%20radar%20dataset%20could%20be%20applied%20to%20both%20optical%20and%20radar%0Aimages.%20Our%20experiments%20showed%20that%20the%20models%20trained%20on%20an%20optical%20dataset%0Acould%20be%20used%20for%20radar%20images%2C%20while%20those%20trained%20on%20a%20radar%20dataset%20offered%0Avery%20poor%20scores%20when%20applied%20to%20optical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13698v1&entry.124074799=Read"},
{"title": "AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for\n  High-dimensional Regression", "author": "Zelin He and Ying Sun and Jingyuan Liu and Runze Li", "abstract": "  We consider the transfer learning problem in the high dimensional setting,\nwhere the feature dimension is larger than the sample size. To learn\ntransferable information, which may vary across features or the source samples,\nwe propose an adaptive transfer learning method that can detect and aggregate\nthe feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable\nstructures. We achieve this by employing a novel fused-penalty, coupled with\nweights that can adapt according to the transferable structure. To choose the\nweight, we propose a theoretically informed, data-driven procedure, enabling\nF-AdaTrans to selectively fuse the transferable signals with the target while\nfiltering out non-transferable signals, and S-AdaTrans to obtain the optimal\ncombination of information transferred from each source sample. The\nnon-asymptotic rates are established, which recover existing near-minimax\noptimal rates in special cases. The effectiveness of the proposed method is\nvalidated using both synthetic and real data.\n", "link": "http://arxiv.org/abs/2403.13565v1", "date": "2024-03-20", "relevancy": 2.5982, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5148}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AdaTrans%3A%20Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Regression&body=Title%3A%20AdaTrans%3A%20Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Regression%0AAuthor%3A%20Zelin%20He%20and%20Ying%20Sun%20and%20Jingyuan%20Liu%20and%20Runze%20Li%0AAbstract%3A%20%20%20We%20consider%20the%20transfer%20learning%20problem%20in%20the%20high%20dimensional%20setting%2C%0Awhere%20the%20feature%20dimension%20is%20larger%20than%20the%20sample%20size.%20To%20learn%0Atransferable%20information%2C%20which%20may%20vary%20across%20features%20or%20the%20source%20samples%2C%0Awe%20propose%20an%20adaptive%20transfer%20learning%20method%20that%20can%20detect%20and%20aggregate%0Athe%20feature-wise%20%28F-AdaTrans%29%20or%20sample-wise%20%28S-AdaTrans%29%20transferable%0Astructures.%20We%20achieve%20this%20by%20employing%20a%20novel%20fused-penalty%2C%20coupled%20with%0Aweights%20that%20can%20adapt%20according%20to%20the%20transferable%20structure.%20To%20choose%20the%0Aweight%2C%20we%20propose%20a%20theoretically%20informed%2C%20data-driven%20procedure%2C%20enabling%0AF-AdaTrans%20to%20selectively%20fuse%20the%20transferable%20signals%20with%20the%20target%20while%0Afiltering%20out%20non-transferable%20signals%2C%20and%20S-AdaTrans%20to%20obtain%20the%20optimal%0Acombination%20of%20information%20transferred%20from%20each%20source%20sample.%20The%0Anon-asymptotic%20rates%20are%20established%2C%20which%20recover%20existing%20near-minimax%0Aoptimal%20rates%20in%20special%20cases.%20The%20effectiveness%20of%20the%20proposed%20method%20is%0Avalidated%20using%20both%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13565v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaTrans%3A%20Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Regression&entry.906535625=Zelin%20He%20and%20Ying%20Sun%20and%20Jingyuan%20Liu%20and%20Runze%20Li&entry.1292438233=%20%20We%20consider%20the%20transfer%20learning%20problem%20in%20the%20high%20dimensional%20setting%2C%0Awhere%20the%20feature%20dimension%20is%20larger%20than%20the%20sample%20size.%20To%20learn%0Atransferable%20information%2C%20which%20may%20vary%20across%20features%20or%20the%20source%20samples%2C%0Awe%20propose%20an%20adaptive%20transfer%20learning%20method%20that%20can%20detect%20and%20aggregate%0Athe%20feature-wise%20%28F-AdaTrans%29%20or%20sample-wise%20%28S-AdaTrans%29%20transferable%0Astructures.%20We%20achieve%20this%20by%20employing%20a%20novel%20fused-penalty%2C%20coupled%20with%0Aweights%20that%20can%20adapt%20according%20to%20the%20transferable%20structure.%20To%20choose%20the%0Aweight%2C%20we%20propose%20a%20theoretically%20informed%2C%20data-driven%20procedure%2C%20enabling%0AF-AdaTrans%20to%20selectively%20fuse%20the%20transferable%20signals%20with%20the%20target%20while%0Afiltering%20out%20non-transferable%20signals%2C%20and%20S-AdaTrans%20to%20obtain%20the%20optimal%0Acombination%20of%20information%20transferred%20from%20each%20source%20sample.%20The%0Anon-asymptotic%20rates%20are%20established%2C%20which%20recover%20existing%20near-minimax%0Aoptimal%20rates%20in%20special%20cases.%20The%20effectiveness%20of%20the%20proposed%20method%20is%0Avalidated%20using%20both%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13565v1&entry.124074799=Read"},
{"title": "Leveraging High-Resolution Features for Improved Deep Hashing-based\n  Image Retrieval", "author": "Aymene Berriche and Mehdi Adjal Zakaria and Riyadh Baghdadi", "abstract": "  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n", "link": "http://arxiv.org/abs/2403.13747v1", "date": "2024-03-20", "relevancy": 2.5752, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20High-Resolution%20Features%20for%20Improved%20Deep%20Hashing-based%0A%20%20Image%20Retrieval&body=Title%3A%20Leveraging%20High-Resolution%20Features%20for%20Improved%20Deep%20Hashing-based%0A%20%20Image%20Retrieval%0AAuthor%3A%20Aymene%20Berriche%20and%20Mehdi%20Adjal%20Zakaria%20and%20Riyadh%20Baghdadi%0AAbstract%3A%20%20%20Deep%20hashing%20techniques%20have%20emerged%20as%20the%20predominant%20approach%20for%0Aefficient%20image%20retrieval.%20Traditionally%2C%20these%20methods%20utilize%20pre-trained%0Aconvolutional%20neural%20networks%20%28CNNs%29%20such%20as%20AlexNet%20and%20VGG-16%20as%20feature%0Aextractors.%20However%2C%20the%20increasing%20complexity%20of%20datasets%20poses%20challenges%20for%0Athese%20backbone%20architectures%20in%20capturing%20meaningful%20features%20essential%20for%0Aeffective%20image%20retrieval.%20In%20this%20study%2C%20we%20explore%20the%20efficacy%20of%20employing%0Ahigh-resolution%20features%20learned%20through%20state-of-the-art%20techniques%20for%20image%0Aretrieval%20tasks.%20Specifically%2C%20we%20propose%20a%20novel%20methodology%20that%20utilizes%0AHigh-Resolution%20Networks%20%28HRNets%29%20as%20the%20backbone%20for%20the%20deep%20hashing%20task%2C%0Atermed%20High-Resolution%20Hashing%20Network%20%28HHNet%29.%20Our%20approach%20demonstrates%0Asuperior%20performance%20compared%20to%20existing%20methods%20across%20all%20tested%20benchmark%0Adatasets%2C%20including%20CIFAR-10%2C%20NUS-WIDE%2C%20MS%20COCO%2C%20and%20ImageNet.%20This%20performance%0Aimprovement%20is%20more%20pronounced%20for%20complex%20datasets%2C%20which%20highlights%20the%20need%0Ato%20learn%20high-resolution%20features%20for%20intricate%20image%20retrieval%20tasks.%0AFurthermore%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20different%20HRNet%0Aconfigurations%20and%20provide%20insights%20into%20the%20optimal%20architecture%20for%20the%20deep%0Ahashing%20task%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13747v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20High-Resolution%20Features%20for%20Improved%20Deep%20Hashing-based%0A%20%20Image%20Retrieval&entry.906535625=Aymene%20Berriche%20and%20Mehdi%20Adjal%20Zakaria%20and%20Riyadh%20Baghdadi&entry.1292438233=%20%20Deep%20hashing%20techniques%20have%20emerged%20as%20the%20predominant%20approach%20for%0Aefficient%20image%20retrieval.%20Traditionally%2C%20these%20methods%20utilize%20pre-trained%0Aconvolutional%20neural%20networks%20%28CNNs%29%20such%20as%20AlexNet%20and%20VGG-16%20as%20feature%0Aextractors.%20However%2C%20the%20increasing%20complexity%20of%20datasets%20poses%20challenges%20for%0Athese%20backbone%20architectures%20in%20capturing%20meaningful%20features%20essential%20for%0Aeffective%20image%20retrieval.%20In%20this%20study%2C%20we%20explore%20the%20efficacy%20of%20employing%0Ahigh-resolution%20features%20learned%20through%20state-of-the-art%20techniques%20for%20image%0Aretrieval%20tasks.%20Specifically%2C%20we%20propose%20a%20novel%20methodology%20that%20utilizes%0AHigh-Resolution%20Networks%20%28HRNets%29%20as%20the%20backbone%20for%20the%20deep%20hashing%20task%2C%0Atermed%20High-Resolution%20Hashing%20Network%20%28HHNet%29.%20Our%20approach%20demonstrates%0Asuperior%20performance%20compared%20to%20existing%20methods%20across%20all%20tested%20benchmark%0Adatasets%2C%20including%20CIFAR-10%2C%20NUS-WIDE%2C%20MS%20COCO%2C%20and%20ImageNet.%20This%20performance%0Aimprovement%20is%20more%20pronounced%20for%20complex%20datasets%2C%20which%20highlights%20the%20need%0Ato%20learn%20high-resolution%20features%20for%20intricate%20image%20retrieval%20tasks.%0AFurthermore%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20different%20HRNet%0Aconfigurations%20and%20provide%20insights%20into%20the%20optimal%20architecture%20for%20the%20deep%0Ahashing%20task%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13747v1&entry.124074799=Read"},
{"title": "IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models", "author": "Siying Cui and Jiankang Deng and Jia Guo and Xiang An and Yongle Zhao and Xinyu Wei and Ziyong Feng", "abstract": "  Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images.\n", "link": "http://arxiv.org/abs/2403.13535v1", "date": "2024-03-20", "relevancy": 2.5633, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6811}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IDAdapter%3A%20Learning%20Mixed%20Features%20for%20Tuning-Free%20Personalization%20of%0A%20%20Text-to-Image%20Models&body=Title%3A%20IDAdapter%3A%20Learning%20Mixed%20Features%20for%20Tuning-Free%20Personalization%20of%0A%20%20Text-to-Image%20Models%0AAuthor%3A%20Siying%20Cui%20and%20Jiankang%20Deng%20and%20Jia%20Guo%20and%20Xiang%20An%20and%20Yongle%20Zhao%20and%20Xinyu%20Wei%20and%20Ziyong%20Feng%0AAbstract%3A%20%20%20Leveraging%20Stable%20Diffusion%20for%20the%20generation%20of%20personalized%20portraits%20has%0Aemerged%20as%20a%20powerful%20and%20noteworthy%20tool%2C%20enabling%20users%20to%20create%0Ahigh-fidelity%2C%20custom%20character%20avatars%20based%20on%20their%20specific%20prompts.%0AHowever%2C%20existing%20personalization%20methods%20face%20challenges%2C%20including%20test-time%0Afine-tuning%2C%20the%20requirement%20of%20multiple%20input%20images%2C%20low%20preservation%20of%0Aidentity%2C%20and%20limited%20diversity%20in%20generated%20outcomes.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20IDAdapter%2C%20a%20tuning-free%20approach%20that%20enhances%20the%0Adiversity%20and%20identity%20preservation%20in%20personalized%20image%20generation%20from%20a%0Asingle%20face%20image.%20IDAdapter%20integrates%20a%20personalized%20concept%20into%20the%0Ageneration%20process%20through%20a%20combination%20of%20textual%20and%20visual%20injections%20and%20a%0Aface%20identity%20loss.%20During%20the%20training%20phase%2C%20we%20incorporate%20mixed%20features%0Afrom%20multiple%20reference%20images%20of%20a%20specific%20identity%20to%20enrich%0Aidentity-related%20content%20details%2C%20guiding%20the%20model%20to%20generate%20images%20with%0Amore%20diverse%20styles%2C%20expressions%2C%20and%20angles%20compared%20to%20previous%20works.%0AExtensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%0Aboth%20diversity%20and%20identity%20fidelity%20in%20generated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13535v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDAdapter%3A%20Learning%20Mixed%20Features%20for%20Tuning-Free%20Personalization%20of%0A%20%20Text-to-Image%20Models&entry.906535625=Siying%20Cui%20and%20Jiankang%20Deng%20and%20Jia%20Guo%20and%20Xiang%20An%20and%20Yongle%20Zhao%20and%20Xinyu%20Wei%20and%20Ziyong%20Feng&entry.1292438233=%20%20Leveraging%20Stable%20Diffusion%20for%20the%20generation%20of%20personalized%20portraits%20has%0Aemerged%20as%20a%20powerful%20and%20noteworthy%20tool%2C%20enabling%20users%20to%20create%0Ahigh-fidelity%2C%20custom%20character%20avatars%20based%20on%20their%20specific%20prompts.%0AHowever%2C%20existing%20personalization%20methods%20face%20challenges%2C%20including%20test-time%0Afine-tuning%2C%20the%20requirement%20of%20multiple%20input%20images%2C%20low%20preservation%20of%0Aidentity%2C%20and%20limited%20diversity%20in%20generated%20outcomes.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20IDAdapter%2C%20a%20tuning-free%20approach%20that%20enhances%20the%0Adiversity%20and%20identity%20preservation%20in%20personalized%20image%20generation%20from%20a%0Asingle%20face%20image.%20IDAdapter%20integrates%20a%20personalized%20concept%20into%20the%0Ageneration%20process%20through%20a%20combination%20of%20textual%20and%20visual%20injections%20and%20a%0Aface%20identity%20loss.%20During%20the%20training%20phase%2C%20we%20incorporate%20mixed%20features%0Afrom%20multiple%20reference%20images%20of%20a%20specific%20identity%20to%20enrich%0Aidentity-related%20content%20details%2C%20guiding%20the%20model%20to%20generate%20images%20with%0Amore%20diverse%20styles%2C%20expressions%2C%20and%20angles%20compared%20to%20previous%20works.%0AExtensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%0Aboth%20diversity%20and%20identity%20fidelity%20in%20generated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13535v1&entry.124074799=Read"},
{"title": "TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion\n  Models", "author": "Pengxiang Li and Kai Chen and Zhili Liu and Ruiyuan Gao and Lanqing Hong and Guo Zhou and Hua Yao and Dit-Yan Yeung and Huchuan Lu and Xu Jia", "abstract": "  Despite remarkable achievements in video synthesis, achieving granular\ncontrol over complex dynamics, such as nuanced movement among multiple\ninteracting objects, still presents a significant hurdle for dynamic world\nmodeling, compounded by the necessity to manage appearance and disappearance,\ndrastic scale changes, and ensure consistency for instances across frames.\nThese challenges hinder the development of video generation that can faithfully\nmimic real-world complexity, limiting utility for applications requiring\nhigh-level realism and controllability, including advanced scene simulation and\ntraining of perception systems. To address that, we propose TrackDiffusion, a\nnovel video generation framework affording fine-grained trajectory-conditioned\nmotion control via diffusion models, which facilitates the precise manipulation\nof the object trajectories and interactions, overcoming the prevalent\nlimitation of scale and continuity disruptions. A pivotal component of\nTrackDiffusion is the instance enhancer, which explicitly ensures inter-frame\nconsistency of multiple objects, a critical factor overlooked in the current\nliterature. Moreover, we demonstrate that generated video sequences by our\nTrackDiffusion can be used as training data for visual perception models. To\nthe best of our knowledge, this is the first work to apply video diffusion\nmodels with tracklet conditions and demonstrate that generated frames can be\nbeneficial for improving the performance of object trackers.\n", "link": "http://arxiv.org/abs/2312.00651v2", "date": "2024-03-20", "relevancy": 2.557, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6885}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6452}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6136}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TrackDiffusion%3A%20Tracklet-Conditioned%20Video%20Generation%20via%20Diffusion%0A%20%20Models&body=Title%3A%20TrackDiffusion%3A%20Tracklet-Conditioned%20Video%20Generation%20via%20Diffusion%0A%20%20Models%0AAuthor%3A%20Pengxiang%20Li%20and%20Kai%20Chen%20and%20Zhili%20Liu%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Guo%20Zhou%20and%20Hua%20Yao%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Despite%20remarkable%20achievements%20in%20video%20synthesis%2C%20achieving%20granular%0Acontrol%20over%20complex%20dynamics%2C%20such%20as%20nuanced%20movement%20among%20multiple%0Ainteracting%20objects%2C%20still%20presents%20a%20significant%20hurdle%20for%20dynamic%20world%0Amodeling%2C%20compounded%20by%20the%20necessity%20to%20manage%20appearance%20and%20disappearance%2C%0Adrastic%20scale%20changes%2C%20and%20ensure%20consistency%20for%20instances%20across%20frames.%0AThese%20challenges%20hinder%20the%20development%20of%20video%20generation%20that%20can%20faithfully%0Amimic%20real-world%20complexity%2C%20limiting%20utility%20for%20applications%20requiring%0Ahigh-level%20realism%20and%20controllability%2C%20including%20advanced%20scene%20simulation%20and%0Atraining%20of%20perception%20systems.%20To%20address%20that%2C%20we%20propose%20TrackDiffusion%2C%20a%0Anovel%20video%20generation%20framework%20affording%20fine-grained%20trajectory-conditioned%0Amotion%20control%20via%20diffusion%20models%2C%20which%20facilitates%20the%20precise%20manipulation%0Aof%20the%20object%20trajectories%20and%20interactions%2C%20overcoming%20the%20prevalent%0Alimitation%20of%20scale%20and%20continuity%20disruptions.%20A%20pivotal%20component%20of%0ATrackDiffusion%20is%20the%20instance%20enhancer%2C%20which%20explicitly%20ensures%20inter-frame%0Aconsistency%20of%20multiple%20objects%2C%20a%20critical%20factor%20overlooked%20in%20the%20current%0Aliterature.%20Moreover%2C%20we%20demonstrate%20that%20generated%20video%20sequences%20by%20our%0ATrackDiffusion%20can%20be%20used%20as%20training%20data%20for%20visual%20perception%20models.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20video%20diffusion%0Amodels%20with%20tracklet%20conditions%20and%20demonstrate%20that%20generated%20frames%20can%20be%0Abeneficial%20for%20improving%20the%20performance%20of%20object%20trackers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00651v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackDiffusion%3A%20Tracklet-Conditioned%20Video%20Generation%20via%20Diffusion%0A%20%20Models&entry.906535625=Pengxiang%20Li%20and%20Kai%20Chen%20and%20Zhili%20Liu%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Guo%20Zhou%20and%20Hua%20Yao%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia&entry.1292438233=%20%20Despite%20remarkable%20achievements%20in%20video%20synthesis%2C%20achieving%20granular%0Acontrol%20over%20complex%20dynamics%2C%20such%20as%20nuanced%20movement%20among%20multiple%0Ainteracting%20objects%2C%20still%20presents%20a%20significant%20hurdle%20for%20dynamic%20world%0Amodeling%2C%20compounded%20by%20the%20necessity%20to%20manage%20appearance%20and%20disappearance%2C%0Adrastic%20scale%20changes%2C%20and%20ensure%20consistency%20for%20instances%20across%20frames.%0AThese%20challenges%20hinder%20the%20development%20of%20video%20generation%20that%20can%20faithfully%0Amimic%20real-world%20complexity%2C%20limiting%20utility%20for%20applications%20requiring%0Ahigh-level%20realism%20and%20controllability%2C%20including%20advanced%20scene%20simulation%20and%0Atraining%20of%20perception%20systems.%20To%20address%20that%2C%20we%20propose%20TrackDiffusion%2C%20a%0Anovel%20video%20generation%20framework%20affording%20fine-grained%20trajectory-conditioned%0Amotion%20control%20via%20diffusion%20models%2C%20which%20facilitates%20the%20precise%20manipulation%0Aof%20the%20object%20trajectories%20and%20interactions%2C%20overcoming%20the%20prevalent%0Alimitation%20of%20scale%20and%20continuity%20disruptions.%20A%20pivotal%20component%20of%0ATrackDiffusion%20is%20the%20instance%20enhancer%2C%20which%20explicitly%20ensures%20inter-frame%0Aconsistency%20of%20multiple%20objects%2C%20a%20critical%20factor%20overlooked%20in%20the%20current%0Aliterature.%20Moreover%2C%20we%20demonstrate%20that%20generated%20video%20sequences%20by%20our%0ATrackDiffusion%20can%20be%20used%20as%20training%20data%20for%20visual%20perception%20models.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20video%20diffusion%0Amodels%20with%20tracklet%20conditions%20and%20demonstrate%20that%20generated%20frames%20can%20be%0Abeneficial%20for%20improving%20the%20performance%20of%20object%20trackers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00651v2&entry.124074799=Read"},
{"title": "Analyzing and Improving the Training Dynamics of Diffusion Models", "author": "Tero Karras and Miika Aittala and Jaakko Lehtinen and Janne Hellsten and Timo Aila and Samuli Laine", "abstract": "  Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.\n", "link": "http://arxiv.org/abs/2312.02696v2", "date": "2024-03-20", "relevancy": 2.4834, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6568}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6447}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5826}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analyzing%20and%20Improving%20the%20Training%20Dynamics%20of%20Diffusion%20Models&body=Title%3A%20Analyzing%20and%20Improving%20the%20Training%20Dynamics%20of%20Diffusion%20Models%0AAuthor%3A%20Tero%20Karras%20and%20Miika%20Aittala%20and%20Jaakko%20Lehtinen%20and%20Janne%20Hellsten%20and%20Timo%20Aila%20and%20Samuli%20Laine%0AAbstract%3A%20%20%20Diffusion%20models%20currently%20dominate%20the%20field%20of%20data-driven%20image%20synthesis%0Awith%20their%20unparalleled%20scaling%20to%20large%20datasets.%20In%20this%20paper%2C%20we%20identify%0Aand%20rectify%20several%20causes%20for%20uneven%20and%20ineffective%20training%20in%20the%20popular%0AADM%20diffusion%20model%20architecture%2C%20without%20altering%20its%20high-level%20structure.%0AObserving%20uncontrolled%20magnitude%20changes%20and%20imbalances%20in%20both%20the%20network%0Aactivations%20and%20weights%20over%20the%20course%20of%20training%2C%20we%20redesign%20the%20network%0Alayers%20to%20preserve%20activation%2C%20weight%2C%20and%20update%20magnitudes%20on%20expectation.%20We%0Afind%20that%20systematic%20application%20of%20this%20philosophy%20eliminates%20the%20observed%0Adrifts%20and%20imbalances%2C%20resulting%20in%20considerably%20better%20networks%20at%20equal%0Acomputational%20complexity.%20Our%20modifications%20improve%20the%20previous%20record%20FID%20of%0A2.41%20in%20ImageNet-512%20synthesis%20to%201.81%2C%20achieved%20using%20fast%20deterministic%0Asampling.%0A%20%20As%20an%20independent%20contribution%2C%20we%20present%20a%20method%20for%20setting%20the%0Aexponential%20moving%20average%20%28EMA%29%20parameters%20post-hoc%2C%20i.e.%2C%20after%20completing%0Athe%20training%20run.%20This%20allows%20precise%20tuning%20of%20EMA%20length%20without%20the%20cost%20of%0Aperforming%20several%20training%20runs%2C%20and%20reveals%20its%20surprising%20interactions%20with%0Anetwork%20architecture%2C%20training%20time%2C%20and%20guidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02696v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20and%20Improving%20the%20Training%20Dynamics%20of%20Diffusion%20Models&entry.906535625=Tero%20Karras%20and%20Miika%20Aittala%20and%20Jaakko%20Lehtinen%20and%20Janne%20Hellsten%20and%20Timo%20Aila%20and%20Samuli%20Laine&entry.1292438233=%20%20Diffusion%20models%20currently%20dominate%20the%20field%20of%20data-driven%20image%20synthesis%0Awith%20their%20unparalleled%20scaling%20to%20large%20datasets.%20In%20this%20paper%2C%20we%20identify%0Aand%20rectify%20several%20causes%20for%20uneven%20and%20ineffective%20training%20in%20the%20popular%0AADM%20diffusion%20model%20architecture%2C%20without%20altering%20its%20high-level%20structure.%0AObserving%20uncontrolled%20magnitude%20changes%20and%20imbalances%20in%20both%20the%20network%0Aactivations%20and%20weights%20over%20the%20course%20of%20training%2C%20we%20redesign%20the%20network%0Alayers%20to%20preserve%20activation%2C%20weight%2C%20and%20update%20magnitudes%20on%20expectation.%20We%0Afind%20that%20systematic%20application%20of%20this%20philosophy%20eliminates%20the%20observed%0Adrifts%20and%20imbalances%2C%20resulting%20in%20considerably%20better%20networks%20at%20equal%0Acomputational%20complexity.%20Our%20modifications%20improve%20the%20previous%20record%20FID%20of%0A2.41%20in%20ImageNet-512%20synthesis%20to%201.81%2C%20achieved%20using%20fast%20deterministic%0Asampling.%0A%20%20As%20an%20independent%20contribution%2C%20we%20present%20a%20method%20for%20setting%20the%0Aexponential%20moving%20average%20%28EMA%29%20parameters%20post-hoc%2C%20i.e.%2C%20after%20completing%0Athe%20training%20run.%20This%20allows%20precise%20tuning%20of%20EMA%20length%20without%20the%20cost%20of%0Aperforming%20several%20training%20runs%2C%20and%20reveals%20its%20surprising%20interactions%20with%0Anetwork%20architecture%2C%20training%20time%2C%20and%20guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02696v2&entry.124074799=Read"},
{"title": "Data Augmentation for Time-Series Classification: An Extensive Empirical\n  Study and Comprehensive Survey", "author": "Zijun Gao and Lingbo Li", "abstract": "  Data Augmentation (DA) has emerged as an indispensable strategy in Time\nSeries Classification (TSC), primarily due to its capacity to amplify training\nsamples, thereby bolstering model robustness, diversifying datasets, and\ncurtailing overfitting. However, the current landscape of DA in TSC is plagued\nwith fragmented literature reviews, nebulous methodological taxonomies,\ninadequate evaluative measures, and a dearth of accessible, user-oriented\ntools. In light of these challenges, this study embarks on an exhaustive\ndissection of DA methodologies within the TSC realm. Our initial approach\ninvolved an extensive literature review spanning a decade, revealing that\ncontemporary surveys scarcely capture the breadth of advancements in DA for\nTSC, prompting us to meticulously analyze over 100 scholarly articles to\ndistill more than 60 unique DA techniques. This rigorous analysis precipitated\nthe formulation of a novel taxonomy, purpose-built for the intricacies of DA in\nTSC, categorizing techniques into five principal echelons:\nTransformation-Based, Pattern-Based, Generative, Decomposition-Based, and\nAutomated Data Augmentation. Our taxonomy promises to serve as a robust\nnavigational aid for scholars, offering clarity and direction in method\nselection. Addressing the conspicuous absence of holistic evaluations for\nprevalent DA techniques, we executed an all-encompassing empirical assessment,\nwherein upwards of 15 DA strategies were subjected to scrutiny across 8 UCR\ntime-series datasets, employing ResNet and a multi-faceted evaluation paradigm\nencompassing Accuracy, Method Ranking, and Residual Analysis, yielding a\nbenchmark accuracy of 88.94 +- 11.83%. Our investigation underscored the\ninconsistent efficacies of DA techniques, with...\n", "link": "http://arxiv.org/abs/2310.10060v3", "date": "2024-03-20", "relevancy": 2.4752, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4774}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Time-Series%20Classification%3A%20An%20Extensive%20Empirical%0A%20%20Study%20and%20Comprehensive%20Survey&body=Title%3A%20Data%20Augmentation%20for%20Time-Series%20Classification%3A%20An%20Extensive%20Empirical%0A%20%20Study%20and%20Comprehensive%20Survey%0AAuthor%3A%20Zijun%20Gao%20and%20Lingbo%20Li%0AAbstract%3A%20%20%20Data%20Augmentation%20%28DA%29%20has%20emerged%20as%20an%20indispensable%20strategy%20in%20Time%0ASeries%20Classification%20%28TSC%29%2C%20primarily%20due%20to%20its%20capacity%20to%20amplify%20training%0Asamples%2C%20thereby%20bolstering%20model%20robustness%2C%20diversifying%20datasets%2C%20and%0Acurtailing%20overfitting.%20However%2C%20the%20current%20landscape%20of%20DA%20in%20TSC%20is%20plagued%0Awith%20fragmented%20literature%20reviews%2C%20nebulous%20methodological%20taxonomies%2C%0Ainadequate%20evaluative%20measures%2C%20and%20a%20dearth%20of%20accessible%2C%20user-oriented%0Atools.%20In%20light%20of%20these%20challenges%2C%20this%20study%20embarks%20on%20an%20exhaustive%0Adissection%20of%20DA%20methodologies%20within%20the%20TSC%20realm.%20Our%20initial%20approach%0Ainvolved%20an%20extensive%20literature%20review%20spanning%20a%20decade%2C%20revealing%20that%0Acontemporary%20surveys%20scarcely%20capture%20the%20breadth%20of%20advancements%20in%20DA%20for%0ATSC%2C%20prompting%20us%20to%20meticulously%20analyze%20over%20100%20scholarly%20articles%20to%0Adistill%20more%20than%2060%20unique%20DA%20techniques.%20This%20rigorous%20analysis%20precipitated%0Athe%20formulation%20of%20a%20novel%20taxonomy%2C%20purpose-built%20for%20the%20intricacies%20of%20DA%20in%0ATSC%2C%20categorizing%20techniques%20into%20five%20principal%20echelons%3A%0ATransformation-Based%2C%20Pattern-Based%2C%20Generative%2C%20Decomposition-Based%2C%20and%0AAutomated%20Data%20Augmentation.%20Our%20taxonomy%20promises%20to%20serve%20as%20a%20robust%0Anavigational%20aid%20for%20scholars%2C%20offering%20clarity%20and%20direction%20in%20method%0Aselection.%20Addressing%20the%20conspicuous%20absence%20of%20holistic%20evaluations%20for%0Aprevalent%20DA%20techniques%2C%20we%20executed%20an%20all-encompassing%20empirical%20assessment%2C%0Awherein%20upwards%20of%2015%20DA%20strategies%20were%20subjected%20to%20scrutiny%20across%208%20UCR%0Atime-series%20datasets%2C%20employing%20ResNet%20and%20a%20multi-faceted%20evaluation%20paradigm%0Aencompassing%20Accuracy%2C%20Method%20Ranking%2C%20and%20Residual%20Analysis%2C%20yielding%20a%0Abenchmark%20accuracy%20of%2088.94%20%2B-%2011.83%25.%20Our%20investigation%20underscored%20the%0Ainconsistent%20efficacies%20of%20DA%20techniques%2C%20with...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10060v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Time-Series%20Classification%3A%20An%20Extensive%20Empirical%0A%20%20Study%20and%20Comprehensive%20Survey&entry.906535625=Zijun%20Gao%20and%20Lingbo%20Li&entry.1292438233=%20%20Data%20Augmentation%20%28DA%29%20has%20emerged%20as%20an%20indispensable%20strategy%20in%20Time%0ASeries%20Classification%20%28TSC%29%2C%20primarily%20due%20to%20its%20capacity%20to%20amplify%20training%0Asamples%2C%20thereby%20bolstering%20model%20robustness%2C%20diversifying%20datasets%2C%20and%0Acurtailing%20overfitting.%20However%2C%20the%20current%20landscape%20of%20DA%20in%20TSC%20is%20plagued%0Awith%20fragmented%20literature%20reviews%2C%20nebulous%20methodological%20taxonomies%2C%0Ainadequate%20evaluative%20measures%2C%20and%20a%20dearth%20of%20accessible%2C%20user-oriented%0Atools.%20In%20light%20of%20these%20challenges%2C%20this%20study%20embarks%20on%20an%20exhaustive%0Adissection%20of%20DA%20methodologies%20within%20the%20TSC%20realm.%20Our%20initial%20approach%0Ainvolved%20an%20extensive%20literature%20review%20spanning%20a%20decade%2C%20revealing%20that%0Acontemporary%20surveys%20scarcely%20capture%20the%20breadth%20of%20advancements%20in%20DA%20for%0ATSC%2C%20prompting%20us%20to%20meticulously%20analyze%20over%20100%20scholarly%20articles%20to%0Adistill%20more%20than%2060%20unique%20DA%20techniques.%20This%20rigorous%20analysis%20precipitated%0Athe%20formulation%20of%20a%20novel%20taxonomy%2C%20purpose-built%20for%20the%20intricacies%20of%20DA%20in%0ATSC%2C%20categorizing%20techniques%20into%20five%20principal%20echelons%3A%0ATransformation-Based%2C%20Pattern-Based%2C%20Generative%2C%20Decomposition-Based%2C%20and%0AAutomated%20Data%20Augmentation.%20Our%20taxonomy%20promises%20to%20serve%20as%20a%20robust%0Anavigational%20aid%20for%20scholars%2C%20offering%20clarity%20and%20direction%20in%20method%0Aselection.%20Addressing%20the%20conspicuous%20absence%20of%20holistic%20evaluations%20for%0Aprevalent%20DA%20techniques%2C%20we%20executed%20an%20all-encompassing%20empirical%20assessment%2C%0Awherein%20upwards%20of%2015%20DA%20strategies%20were%20subjected%20to%20scrutiny%20across%208%20UCR%0Atime-series%20datasets%2C%20employing%20ResNet%20and%20a%20multi-faceted%20evaluation%20paradigm%0Aencompassing%20Accuracy%2C%20Method%20Ranking%2C%20and%20Residual%20Analysis%2C%20yielding%20a%0Abenchmark%20accuracy%20of%2088.94%20%2B-%2011.83%25.%20Our%20investigation%20underscored%20the%0Ainconsistent%20efficacies%20of%20DA%20techniques%2C%20with...%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10060v3&entry.124074799=Read"},
{"title": "Mobile Robot Localization: a Modular, Odometry-Improving Approach", "author": "Luca Mozzarelli and Luca Cattaneo and Matteo Corno and Sergio Matteo Savaresi", "abstract": "  Despite the number of works published in recent years, vehicle localization\nremains an open, challenging problem. While map-based localization and SLAM\nalgorithms are getting better and better, they remain a single point of failure\nin typical localization pipelines. This paper proposes a modular localization\narchitecture that fuses sensor measurements with the outputs of off-the-shelf\nlocalization algorithms. The fusion filter estimates model uncertainties to\nimprove odometry in case absolute pose measurements are lost entirely. The\narchitecture is validated experimentally on a real robot navigating\nautonomously proving a reduction of the position error of more than 90% with\nrespect to the odometrical estimate without uncertainty estimation in a\ntwo-minute navigation period without position measurements.\n", "link": "http://arxiv.org/abs/2403.13452v1", "date": "2024-03-20", "relevancy": 2.4739, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.653}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6098}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5874}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mobile%20Robot%20Localization%3A%20a%20Modular%2C%20Odometry-Improving%20Approach&body=Title%3A%20Mobile%20Robot%20Localization%3A%20a%20Modular%2C%20Odometry-Improving%20Approach%0AAuthor%3A%20Luca%20Mozzarelli%20and%20Luca%20Cattaneo%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi%0AAbstract%3A%20%20%20Despite%20the%20number%20of%20works%20published%20in%20recent%20years%2C%20vehicle%20localization%0Aremains%20an%20open%2C%20challenging%20problem.%20While%20map-based%20localization%20and%20SLAM%0Aalgorithms%20are%20getting%20better%20and%20better%2C%20they%20remain%20a%20single%20point%20of%20failure%0Ain%20typical%20localization%20pipelines.%20This%20paper%20proposes%20a%20modular%20localization%0Aarchitecture%20that%20fuses%20sensor%20measurements%20with%20the%20outputs%20of%20off-the-shelf%0Alocalization%20algorithms.%20The%20fusion%20filter%20estimates%20model%20uncertainties%20to%0Aimprove%20odometry%20in%20case%20absolute%20pose%20measurements%20are%20lost%20entirely.%20The%0Aarchitecture%20is%20validated%20experimentally%20on%20a%20real%20robot%20navigating%0Aautonomously%20proving%20a%20reduction%20of%20the%20position%20error%20of%20more%20than%2090%25%20with%0Arespect%20to%20the%20odometrical%20estimate%20without%20uncertainty%20estimation%20in%20a%0Atwo-minute%20navigation%20period%20without%20position%20measurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13452v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobile%20Robot%20Localization%3A%20a%20Modular%2C%20Odometry-Improving%20Approach&entry.906535625=Luca%20Mozzarelli%20and%20Luca%20Cattaneo%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi&entry.1292438233=%20%20Despite%20the%20number%20of%20works%20published%20in%20recent%20years%2C%20vehicle%20localization%0Aremains%20an%20open%2C%20challenging%20problem.%20While%20map-based%20localization%20and%20SLAM%0Aalgorithms%20are%20getting%20better%20and%20better%2C%20they%20remain%20a%20single%20point%20of%20failure%0Ain%20typical%20localization%20pipelines.%20This%20paper%20proposes%20a%20modular%20localization%0Aarchitecture%20that%20fuses%20sensor%20measurements%20with%20the%20outputs%20of%20off-the-shelf%0Alocalization%20algorithms.%20The%20fusion%20filter%20estimates%20model%20uncertainties%20to%0Aimprove%20odometry%20in%20case%20absolute%20pose%20measurements%20are%20lost%20entirely.%20The%0Aarchitecture%20is%20validated%20experimentally%20on%20a%20real%20robot%20navigating%0Aautonomously%20proving%20a%20reduction%20of%20the%20position%20error%20of%20more%20than%2090%25%20with%0Arespect%20to%20the%20odometrical%20estimate%20without%20uncertainty%20estimation%20in%20a%0Atwo-minute%20navigation%20period%20without%20position%20measurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13452v1&entry.124074799=Read"},
{"title": "Deepfake Detection without Deepfakes: Generalization via Synthetic\n  Frequency Patterns Injection", "author": "Davide Alessandro Coccomini and Roberto Caldelli and Claudio Gennaro and Giuseppe Fiameni and Giuseppe Amato and Fabrizio Falchi", "abstract": "  Deepfake detectors are typically trained on large sets of pristine and\ngenerated images, resulting in limited generalization capacity; they excel at\nidentifying deepfakes created through methods encountered during training but\nstruggle with those generated by unknown techniques. This paper introduces a\nlearning approach aimed at significantly enhancing the generalization\ncapabilities of deepfake detectors. Our method takes inspiration from the\nunique \"fingerprints\" that image generation processes consistently introduce\ninto the frequency domain. These fingerprints manifest as structured and\ndistinctly recognizable frequency patterns. We propose to train detectors using\nonly pristine images injecting in part of them crafted frequency patterns,\nsimulating the effects of various deepfake generation techniques without being\nspecific to any. These synthetic patterns are based on generic shapes, grids,\nor auras. We evaluated our approach using diverse architectures across 25\ndifferent generation methods. The models trained with our approach were able to\nperform state-of-the-art deepfake detection, demonstrating also superior\ngeneralization capabilities in comparison with previous methods. Indeed, they\nare untied to any specific generation technique and can effectively identify\ndeepfakes regardless of how they were made.\n", "link": "http://arxiv.org/abs/2403.13479v1", "date": "2024-03-20", "relevancy": 2.4636, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5005}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4933}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4844}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Detection%20without%20Deepfakes%3A%20Generalization%20via%20Synthetic%0A%20%20Frequency%20Patterns%20Injection&body=Title%3A%20Deepfake%20Detection%20without%20Deepfakes%3A%20Generalization%20via%20Synthetic%0A%20%20Frequency%20Patterns%20Injection%0AAuthor%3A%20Davide%20Alessandro%20Coccomini%20and%20Roberto%20Caldelli%20and%20Claudio%20Gennaro%20and%20Giuseppe%20Fiameni%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Deepfake%20detectors%20are%20typically%20trained%20on%20large%20sets%20of%20pristine%20and%0Agenerated%20images%2C%20resulting%20in%20limited%20generalization%20capacity%3B%20they%20excel%20at%0Aidentifying%20deepfakes%20created%20through%20methods%20encountered%20during%20training%20but%0Astruggle%20with%20those%20generated%20by%20unknown%20techniques.%20This%20paper%20introduces%20a%0Alearning%20approach%20aimed%20at%20significantly%20enhancing%20the%20generalization%0Acapabilities%20of%20deepfake%20detectors.%20Our%20method%20takes%20inspiration%20from%20the%0Aunique%20%22fingerprints%22%20that%20image%20generation%20processes%20consistently%20introduce%0Ainto%20the%20frequency%20domain.%20These%20fingerprints%20manifest%20as%20structured%20and%0Adistinctly%20recognizable%20frequency%20patterns.%20We%20propose%20to%20train%20detectors%20using%0Aonly%20pristine%20images%20injecting%20in%20part%20of%20them%20crafted%20frequency%20patterns%2C%0Asimulating%20the%20effects%20of%20various%20deepfake%20generation%20techniques%20without%20being%0Aspecific%20to%20any.%20These%20synthetic%20patterns%20are%20based%20on%20generic%20shapes%2C%20grids%2C%0Aor%20auras.%20We%20evaluated%20our%20approach%20using%20diverse%20architectures%20across%2025%0Adifferent%20generation%20methods.%20The%20models%20trained%20with%20our%20approach%20were%20able%20to%0Aperform%20state-of-the-art%20deepfake%20detection%2C%20demonstrating%20also%20superior%0Ageneralization%20capabilities%20in%20comparison%20with%20previous%20methods.%20Indeed%2C%20they%0Aare%20untied%20to%20any%20specific%20generation%20technique%20and%20can%20effectively%20identify%0Adeepfakes%20regardless%20of%20how%20they%20were%20made.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13479v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Detection%20without%20Deepfakes%3A%20Generalization%20via%20Synthetic%0A%20%20Frequency%20Patterns%20Injection&entry.906535625=Davide%20Alessandro%20Coccomini%20and%20Roberto%20Caldelli%20and%20Claudio%20Gennaro%20and%20Giuseppe%20Fiameni%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Deepfake%20detectors%20are%20typically%20trained%20on%20large%20sets%20of%20pristine%20and%0Agenerated%20images%2C%20resulting%20in%20limited%20generalization%20capacity%3B%20they%20excel%20at%0Aidentifying%20deepfakes%20created%20through%20methods%20encountered%20during%20training%20but%0Astruggle%20with%20those%20generated%20by%20unknown%20techniques.%20This%20paper%20introduces%20a%0Alearning%20approach%20aimed%20at%20significantly%20enhancing%20the%20generalization%0Acapabilities%20of%20deepfake%20detectors.%20Our%20method%20takes%20inspiration%20from%20the%0Aunique%20%22fingerprints%22%20that%20image%20generation%20processes%20consistently%20introduce%0Ainto%20the%20frequency%20domain.%20These%20fingerprints%20manifest%20as%20structured%20and%0Adistinctly%20recognizable%20frequency%20patterns.%20We%20propose%20to%20train%20detectors%20using%0Aonly%20pristine%20images%20injecting%20in%20part%20of%20them%20crafted%20frequency%20patterns%2C%0Asimulating%20the%20effects%20of%20various%20deepfake%20generation%20techniques%20without%20being%0Aspecific%20to%20any.%20These%20synthetic%20patterns%20are%20based%20on%20generic%20shapes%2C%20grids%2C%0Aor%20auras.%20We%20evaluated%20our%20approach%20using%20diverse%20architectures%20across%2025%0Adifferent%20generation%20methods.%20The%20models%20trained%20with%20our%20approach%20were%20able%20to%0Aperform%20state-of-the-art%20deepfake%20detection%2C%20demonstrating%20also%20superior%0Ageneralization%20capabilities%20in%20comparison%20with%20previous%20methods.%20Indeed%2C%20they%0Aare%20untied%20to%20any%20specific%20generation%20technique%20and%20can%20effectively%20identify%0Adeepfakes%20regardless%20of%20how%20they%20were%20made.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13479v1&entry.124074799=Read"},
{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "author": "Kshitij Sirohi and Daniel B\u00fcscher and Wolfram Burgard", "abstract": "  The availability of a robust map-based localization system is essential for\nthe operation of many autonomously navigating vehicles. Since uncertainty is an\ninevitable part of perception, it is beneficial for the robustness of the robot\nto consider it in typical downstream tasks of navigation stacks. In particular\nlocalization and mapping methods, which in modern systems often employ\nconvolutional neural networks (CNNs) for perception tasks, require proper\nuncertainty estimates. In this work, we present uncertainty-aware Panoptic\nLocalization and Mapping (uPLAM), which employs pixel-wise uncertainty\nestimates for panoptic CNNs as a bridge to fuse modern perception with\nclassical probabilistic localization and mapping approaches. Beyond the\nperception, we introduce an uncertainty-based map aggregation technique to\ncreate accurate panoptic maps, containing surface semantics and landmark\ninstances. Moreover, we provide cell-wise map uncertainties, and present a\nparticle filter-based localization method that employs perception\nuncertainties. Extensive evaluations show that our proposed incorporation of\nuncertainties leads to more accurate maps with reliable uncertainty estimates\nand improved localization accuracy. Additionally, we present the Freiburg\nPanoptic Driving dataset for evaluating panoptic mapping and localization\nmethods. We make our code and dataset available at:\n\\url{http://uplam.cs.uni-freiburg.de}\n", "link": "http://arxiv.org/abs/2402.05840v2", "date": "2024-03-20", "relevancy": 2.4596, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.9796}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6104}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20uPLAM%3A%20Robust%20Panoptic%20Localization%20and%20Mapping%20Leveraging%20Perception%0A%20%20Uncertainties&body=Title%3A%20uPLAM%3A%20Robust%20Panoptic%20Localization%20and%20Mapping%20Leveraging%20Perception%0A%20%20Uncertainties%0AAuthor%3A%20Kshitij%20Sirohi%20and%20Daniel%20B%C3%BCscher%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20The%20availability%20of%20a%20robust%20map-based%20localization%20system%20is%20essential%20for%0Athe%20operation%20of%20many%20autonomously%20navigating%20vehicles.%20Since%20uncertainty%20is%20an%0Ainevitable%20part%20of%20perception%2C%20it%20is%20beneficial%20for%20the%20robustness%20of%20the%20robot%0Ato%20consider%20it%20in%20typical%20downstream%20tasks%20of%20navigation%20stacks.%20In%20particular%0Alocalization%20and%20mapping%20methods%2C%20which%20in%20modern%20systems%20often%20employ%0Aconvolutional%20neural%20networks%20%28CNNs%29%20for%20perception%20tasks%2C%20require%20proper%0Auncertainty%20estimates.%20In%20this%20work%2C%20we%20present%20uncertainty-aware%20Panoptic%0ALocalization%20and%20Mapping%20%28uPLAM%29%2C%20which%20employs%20pixel-wise%20uncertainty%0Aestimates%20for%20panoptic%20CNNs%20as%20a%20bridge%20to%20fuse%20modern%20perception%20with%0Aclassical%20probabilistic%20localization%20and%20mapping%20approaches.%20Beyond%20the%0Aperception%2C%20we%20introduce%20an%20uncertainty-based%20map%20aggregation%20technique%20to%0Acreate%20accurate%20panoptic%20maps%2C%20containing%20surface%20semantics%20and%20landmark%0Ainstances.%20Moreover%2C%20we%20provide%20cell-wise%20map%20uncertainties%2C%20and%20present%20a%0Aparticle%20filter-based%20localization%20method%20that%20employs%20perception%0Auncertainties.%20Extensive%20evaluations%20show%20that%20our%20proposed%20incorporation%20of%0Auncertainties%20leads%20to%20more%20accurate%20maps%20with%20reliable%20uncertainty%20estimates%0Aand%20improved%20localization%20accuracy.%20Additionally%2C%20we%20present%20the%20Freiburg%0APanoptic%20Driving%20dataset%20for%20evaluating%20panoptic%20mapping%20and%20localization%0Amethods.%20We%20make%20our%20code%20and%20dataset%20available%20at%3A%0A%5Curl%7Bhttp%3A//uplam.cs.uni-freiburg.de%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05840v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=uPLAM%3A%20Robust%20Panoptic%20Localization%20and%20Mapping%20Leveraging%20Perception%0A%20%20Uncertainties&entry.906535625=Kshitij%20Sirohi%20and%20Daniel%20B%C3%BCscher%20and%20Wolfram%20Burgard&entry.1292438233=%20%20The%20availability%20of%20a%20robust%20map-based%20localization%20system%20is%20essential%20for%0Athe%20operation%20of%20many%20autonomously%20navigating%20vehicles.%20Since%20uncertainty%20is%20an%0Ainevitable%20part%20of%20perception%2C%20it%20is%20beneficial%20for%20the%20robustness%20of%20the%20robot%0Ato%20consider%20it%20in%20typical%20downstream%20tasks%20of%20navigation%20stacks.%20In%20particular%0Alocalization%20and%20mapping%20methods%2C%20which%20in%20modern%20systems%20often%20employ%0Aconvolutional%20neural%20networks%20%28CNNs%29%20for%20perception%20tasks%2C%20require%20proper%0Auncertainty%20estimates.%20In%20this%20work%2C%20we%20present%20uncertainty-aware%20Panoptic%0ALocalization%20and%20Mapping%20%28uPLAM%29%2C%20which%20employs%20pixel-wise%20uncertainty%0Aestimates%20for%20panoptic%20CNNs%20as%20a%20bridge%20to%20fuse%20modern%20perception%20with%0Aclassical%20probabilistic%20localization%20and%20mapping%20approaches.%20Beyond%20the%0Aperception%2C%20we%20introduce%20an%20uncertainty-based%20map%20aggregation%20technique%20to%0Acreate%20accurate%20panoptic%20maps%2C%20containing%20surface%20semantics%20and%20landmark%0Ainstances.%20Moreover%2C%20we%20provide%20cell-wise%20map%20uncertainties%2C%20and%20present%20a%0Aparticle%20filter-based%20localization%20method%20that%20employs%20perception%0Auncertainties.%20Extensive%20evaluations%20show%20that%20our%20proposed%20incorporation%20of%0Auncertainties%20leads%20to%20more%20accurate%20maps%20with%20reliable%20uncertainty%20estimates%0Aand%20improved%20localization%20accuracy.%20Additionally%2C%20we%20present%20the%20Freiburg%0APanoptic%20Driving%20dataset%20for%20evaluating%20panoptic%20mapping%20and%20localization%0Amethods.%20We%20make%20our%20code%20and%20dataset%20available%20at%3A%0A%5Curl%7Bhttp%3A//uplam.cs.uni-freiburg.de%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05840v2&entry.124074799=Read"},
{"title": "Describe-and-Dissect: Interpreting Neurons in Vision Networks with\n  Language Models", "author": "Nicholas Bai and Rahul A. Iyer and Tuomas Oikarinen and Tsui-Wei Weng", "abstract": "  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2 times as likely to be selected as the best\nexplanation for a neuron than the best baseline.\n", "link": "http://arxiv.org/abs/2403.13771v1", "date": "2024-03-20", "relevancy": 2.445, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Describe-and-Dissect%3A%20Interpreting%20Neurons%20in%20Vision%20Networks%20with%0A%20%20Language%20Models&body=Title%3A%20Describe-and-Dissect%3A%20Interpreting%20Neurons%20in%20Vision%20Networks%20with%0A%20%20Language%20Models%0AAuthor%3A%20Nicholas%20Bai%20and%20Rahul%20A.%20Iyer%20and%20Tuomas%20Oikarinen%20and%20Tsui-Wei%20Weng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Describe-and-Dissect%20%28DnD%29%2C%20a%20novel%20method%20to%0Adescribe%20the%20roles%20of%20hidden%20neurons%20in%20vision%20networks.%20DnD%20utilizes%20recent%0Aadvancements%20in%20multimodal%20deep%20learning%20to%20produce%20complex%20natural%20language%0Adescriptions%2C%20without%20the%20need%20for%20labeled%20training%20data%20or%20a%20predefined%20set%20of%0Aconcepts%20to%20choose%20from.%20Additionally%2C%20DnD%20is%20training-free%2C%20meaning%20we%20don%27t%0Atrain%20any%20new%20models%20and%20can%20easily%20leverage%20more%20capable%20general%20purpose%0Amodels%20in%20the%20future.%20We%20have%20conducted%20extensive%20qualitative%20and%20quantitative%0Aanalysis%20to%20show%20that%20DnD%20outperforms%20prior%20work%20by%20providing%20higher%20quality%0Aneuron%20descriptions.%20Specifically%2C%20our%20method%20on%20average%20provides%20the%20highest%0Aquality%20labels%20and%20is%20more%20than%202%20times%20as%20likely%20to%20be%20selected%20as%20the%20best%0Aexplanation%20for%20a%20neuron%20than%20the%20best%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13771v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Describe-and-Dissect%3A%20Interpreting%20Neurons%20in%20Vision%20Networks%20with%0A%20%20Language%20Models&entry.906535625=Nicholas%20Bai%20and%20Rahul%20A.%20Iyer%20and%20Tuomas%20Oikarinen%20and%20Tsui-Wei%20Weng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Describe-and-Dissect%20%28DnD%29%2C%20a%20novel%20method%20to%0Adescribe%20the%20roles%20of%20hidden%20neurons%20in%20vision%20networks.%20DnD%20utilizes%20recent%0Aadvancements%20in%20multimodal%20deep%20learning%20to%20produce%20complex%20natural%20language%0Adescriptions%2C%20without%20the%20need%20for%20labeled%20training%20data%20or%20a%20predefined%20set%20of%0Aconcepts%20to%20choose%20from.%20Additionally%2C%20DnD%20is%20training-free%2C%20meaning%20we%20don%27t%0Atrain%20any%20new%20models%20and%20can%20easily%20leverage%20more%20capable%20general%20purpose%0Amodels%20in%20the%20future.%20We%20have%20conducted%20extensive%20qualitative%20and%20quantitative%0Aanalysis%20to%20show%20that%20DnD%20outperforms%20prior%20work%20by%20providing%20higher%20quality%0Aneuron%20descriptions.%20Specifically%2C%20our%20method%20on%20average%20provides%20the%20highest%0Aquality%20labels%20and%20is%20more%20than%202%20times%20as%20likely%20to%20be%20selected%20as%20the%20best%0Aexplanation%20for%20a%20neuron%20than%20the%20best%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13771v1&entry.124074799=Read"},
{"title": "LNPT: Label-free Network Pruning and Training", "author": "Jinying Xiao and Ping Li and Zhe Tang and Jie Nie", "abstract": "  Pruning before training enables the deployment of neural networks on smart\ndevices. By retaining weights conducive to generalization, pruned networks can\nbe accommodated on resource-constrained smart devices. It is commonly held that\nthe distance on weight norms between the initialized and the fully-trained\nnetworks correlates with generalization performance. However, as we have\nuncovered, inconsistency between this metric and generalization during training\nprocesses, which poses an obstacle to determine the pruned structures on smart\ndevices in advance. In this paper, we introduce the concept of the learning\ngap, emphasizing its accurate correlation with generalization. Experiments show\nthat the learning gap, in the form of feature maps from the penultimate layer\nof networks, aligns with variations of generalization performance. We propose a\nnovel learning framework, LNPT, which enables mature networks on the cloud to\nprovide online guidance for network pruning and learning on smart devices with\nunlabeled data. Our results demonstrate the superiority of this approach over\nsupervised training.\n", "link": "http://arxiv.org/abs/2403.12690v2", "date": "2024-03-20", "relevancy": 2.4353, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training&body=Title%3A%20LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training%0AAuthor%3A%20Jinying%20Xiao%20and%20Ping%20Li%20and%20Zhe%20Tang%20and%20Jie%20Nie%0AAbstract%3A%20%20%20Pruning%20before%20training%20enables%20the%20deployment%20of%20neural%20networks%20on%20smart%0Adevices.%20By%20retaining%20weights%20conducive%20to%20generalization%2C%20pruned%20networks%20can%0Abe%20accommodated%20on%20resource-constrained%20smart%20devices.%20It%20is%20commonly%20held%20that%0Athe%20distance%20on%20weight%20norms%20between%20the%20initialized%20and%20the%20fully-trained%0Anetworks%20correlates%20with%20generalization%20performance.%20However%2C%20as%20we%20have%0Auncovered%2C%20inconsistency%20between%20this%20metric%20and%20generalization%20during%20training%0Aprocesses%2C%20which%20poses%20an%20obstacle%20to%20determine%20the%20pruned%20structures%20on%20smart%0Adevices%20in%20advance.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20learning%0Agap%2C%20emphasizing%20its%20accurate%20correlation%20with%20generalization.%20Experiments%20show%0Athat%20the%20learning%20gap%2C%20in%20the%20form%20of%20feature%20maps%20from%20the%20penultimate%20layer%0Aof%20networks%2C%20aligns%20with%20variations%20of%20generalization%20performance.%20We%20propose%20a%0Anovel%20learning%20framework%2C%20LNPT%2C%20which%20enables%20mature%20networks%20on%20the%20cloud%20to%0Aprovide%20online%20guidance%20for%20network%20pruning%20and%20learning%20on%20smart%20devices%20with%0Aunlabeled%20data.%20Our%20results%20demonstrate%20the%20superiority%20of%20this%20approach%20over%0Asupervised%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12690v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training&entry.906535625=Jinying%20Xiao%20and%20Ping%20Li%20and%20Zhe%20Tang%20and%20Jie%20Nie&entry.1292438233=%20%20Pruning%20before%20training%20enables%20the%20deployment%20of%20neural%20networks%20on%20smart%0Adevices.%20By%20retaining%20weights%20conducive%20to%20generalization%2C%20pruned%20networks%20can%0Abe%20accommodated%20on%20resource-constrained%20smart%20devices.%20It%20is%20commonly%20held%20that%0Athe%20distance%20on%20weight%20norms%20between%20the%20initialized%20and%20the%20fully-trained%0Anetworks%20correlates%20with%20generalization%20performance.%20However%2C%20as%20we%20have%0Auncovered%2C%20inconsistency%20between%20this%20metric%20and%20generalization%20during%20training%0Aprocesses%2C%20which%20poses%20an%20obstacle%20to%20determine%20the%20pruned%20structures%20on%20smart%0Adevices%20in%20advance.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20learning%0Agap%2C%20emphasizing%20its%20accurate%20correlation%20with%20generalization.%20Experiments%20show%0Athat%20the%20learning%20gap%2C%20in%20the%20form%20of%20feature%20maps%20from%20the%20penultimate%20layer%0Aof%20networks%2C%20aligns%20with%20variations%20of%20generalization%20performance.%20We%20propose%20a%0Anovel%20learning%20framework%2C%20LNPT%2C%20which%20enables%20mature%20networks%20on%20the%20cloud%20to%0Aprovide%20online%20guidance%20for%20network%20pruning%20and%20learning%20on%20smart%20devices%20with%0Aunlabeled%20data.%20Our%20results%20demonstrate%20the%20superiority%20of%20this%20approach%20over%0Asupervised%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12690v2&entry.124074799=Read"},
{"title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis", "author": "Yumeng Li and William Beluch and Margret Keuper and Dan Zhang and Anna Khoreva", "abstract": "  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n", "link": "http://arxiv.org/abs/2403.13501v1", "date": "2024-03-20", "relevancy": 2.41, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6354}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6181}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5737}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VSTAR%3A%20Generative%20Temporal%20Nursing%20for%20Longer%20Dynamic%20Video%20Synthesis&body=Title%3A%20VSTAR%3A%20Generative%20Temporal%20Nursing%20for%20Longer%20Dynamic%20Video%20Synthesis%0AAuthor%3A%20Yumeng%20Li%20and%20William%20Beluch%20and%20Margret%20Keuper%20and%20Dan%20Zhang%20and%20Anna%20Khoreva%0AAbstract%3A%20%20%20Despite%20tremendous%20progress%20in%20the%20field%20of%20text-to-video%20%28T2V%29%20synthesis%2C%0Aopen-sourced%20T2V%20diffusion%20models%20struggle%20to%20generate%20longer%20videos%20with%0Adynamically%20varying%20and%20evolving%20content.%20They%20tend%20to%20synthesize%20quasi-static%0Avideos%2C%20ignoring%20the%20necessary%20visual%20change-over-time%20implied%20in%20the%20text%0Aprompt.%20At%20the%20same%20time%2C%20scaling%20these%20models%20to%20enable%20longer%2C%20more%20dynamic%0Avideo%20synthesis%20often%20remains%20computationally%20intractable.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20the%20concept%20of%20Generative%20Temporal%20Nursing%20%28GTN%29%2C%20where%0Awe%20aim%20to%20alter%20the%20generative%20process%20on%20the%20fly%20during%20inference%20to%20improve%0Acontrol%20over%20the%20temporal%20dynamics%20and%20enable%20generation%20of%20longer%20videos.%20We%0Apropose%20a%20method%20for%20GTN%2C%20dubbed%20VSTAR%2C%20which%20consists%20of%20two%20key%20ingredients%3A%0A1%29%20Video%20Synopsis%20Prompting%20%28VSP%29%20-%20automatic%20generation%20of%20a%20video%20synopsis%0Abased%20on%20the%20original%20single%20prompt%20leveraging%20LLMs%2C%20which%20gives%20accurate%0Atextual%20guidance%20to%20different%20visual%20states%20of%20longer%20videos%2C%20and%202%29%20Temporal%0AAttention%20Regularization%20%28TAR%29%20-%20a%20regularization%20technique%20to%20refine%20the%0Atemporal%20attention%20units%20of%20the%20pre-trained%20T2V%20diffusion%20models%2C%20which%20enables%0Acontrol%20over%20the%20video%20dynamics.%20We%20experimentally%20showcase%20the%20superiority%20of%0Athe%20proposed%20approach%20in%20generating%20longer%2C%20visually%20appealing%20videos%20over%0Aexisting%20open-sourced%20T2V%20models.%20We%20additionally%20analyze%20the%20temporal%0Aattention%20maps%20realized%20with%20and%20without%20VSTAR%2C%20demonstrating%20the%20importance%20of%0Aapplying%20our%20method%20to%20mitigate%20neglect%20of%20the%20desired%20visual%20change%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13501v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VSTAR%3A%20Generative%20Temporal%20Nursing%20for%20Longer%20Dynamic%20Video%20Synthesis&entry.906535625=Yumeng%20Li%20and%20William%20Beluch%20and%20Margret%20Keuper%20and%20Dan%20Zhang%20and%20Anna%20Khoreva&entry.1292438233=%20%20Despite%20tremendous%20progress%20in%20the%20field%20of%20text-to-video%20%28T2V%29%20synthesis%2C%0Aopen-sourced%20T2V%20diffusion%20models%20struggle%20to%20generate%20longer%20videos%20with%0Adynamically%20varying%20and%20evolving%20content.%20They%20tend%20to%20synthesize%20quasi-static%0Avideos%2C%20ignoring%20the%20necessary%20visual%20change-over-time%20implied%20in%20the%20text%0Aprompt.%20At%20the%20same%20time%2C%20scaling%20these%20models%20to%20enable%20longer%2C%20more%20dynamic%0Avideo%20synthesis%20often%20remains%20computationally%20intractable.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20the%20concept%20of%20Generative%20Temporal%20Nursing%20%28GTN%29%2C%20where%0Awe%20aim%20to%20alter%20the%20generative%20process%20on%20the%20fly%20during%20inference%20to%20improve%0Acontrol%20over%20the%20temporal%20dynamics%20and%20enable%20generation%20of%20longer%20videos.%20We%0Apropose%20a%20method%20for%20GTN%2C%20dubbed%20VSTAR%2C%20which%20consists%20of%20two%20key%20ingredients%3A%0A1%29%20Video%20Synopsis%20Prompting%20%28VSP%29%20-%20automatic%20generation%20of%20a%20video%20synopsis%0Abased%20on%20the%20original%20single%20prompt%20leveraging%20LLMs%2C%20which%20gives%20accurate%0Atextual%20guidance%20to%20different%20visual%20states%20of%20longer%20videos%2C%20and%202%29%20Temporal%0AAttention%20Regularization%20%28TAR%29%20-%20a%20regularization%20technique%20to%20refine%20the%0Atemporal%20attention%20units%20of%20the%20pre-trained%20T2V%20diffusion%20models%2C%20which%20enables%0Acontrol%20over%20the%20video%20dynamics.%20We%20experimentally%20showcase%20the%20superiority%20of%0Athe%20proposed%20approach%20in%20generating%20longer%2C%20visually%20appealing%20videos%20over%0Aexisting%20open-sourced%20T2V%20models.%20We%20additionally%20analyze%20the%20temporal%0Aattention%20maps%20realized%20with%20and%20without%20VSTAR%2C%20demonstrating%20the%20importance%20of%0Aapplying%20our%20method%20to%20mitigate%20neglect%20of%20the%20desired%20visual%20change%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13501v1&entry.124074799=Read"},
{"title": "Fostc3net:A Lightweight YOLOv5 Based On the Network Structure\n  Optimization", "author": "Danqing Ma and Shaojie Li and Bo Dang and Hengyi Zang and Xinqi Dong", "abstract": "  Transmission line detection technology is crucial for automatic monitoring\nand ensuring the safety of electrical facilities. The YOLOv5 series is\ncurrently one of the most advanced and widely used methods for object\ndetection. However, it faces inherent challenges, such as high computational\nload on devices and insufficient detection accuracy. To address these concerns,\nthis paper presents an enhanced lightweight YOLOv5 technique customized for\nmobile devices, specifically intended for identifying objects associated with\ntransmission lines. The C3Ghost module is integrated into the convolutional\nnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in the\nfeature channel fusion process and improve feature expression performance. In\naddition, a FasterNet module is introduced to replace the c3 module in the\nYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only\na portion of the input channels, improving feature extraction efficiency and\nreducing computational overhead. To address the imbalance between simple and\nchallenging samples in the dataset and the diversity of aspect ratios of\nbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate\nthe performance of the proposed approach, Experiments are conducted on a custom\ndataset of transmission line poles. The results show that the proposed model\nachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a\n26% decrease in model parameters compared to the existing YOLOv5.In the\nablation experiment, it was also discovered that while the Fastnet module and\nthe CSghost module improved the precision of the original YOLOv5 baseline\nmodel, they caused a decrease in the mAP@.5-.95 metric. However, the\nimprovement of the wIoUv3 loss function significantly mitigated the decline of\nthe mAP@.5-.95 metric.\n", "link": "http://arxiv.org/abs/2403.13703v1", "date": "2024-03-20", "relevancy": 2.4082, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4888}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4842}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4719}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fostc3net%3AA%20Lightweight%20YOLOv5%20Based%20On%20the%20Network%20Structure%0A%20%20Optimization&body=Title%3A%20Fostc3net%3AA%20Lightweight%20YOLOv5%20Based%20On%20the%20Network%20Structure%0A%20%20Optimization%0AAuthor%3A%20Danqing%20Ma%20and%20Shaojie%20Li%20and%20Bo%20Dang%20and%20Hengyi%20Zang%20and%20Xinqi%20Dong%0AAbstract%3A%20%20%20Transmission%20line%20detection%20technology%20is%20crucial%20for%20automatic%20monitoring%0Aand%20ensuring%20the%20safety%20of%20electrical%20facilities.%20The%20YOLOv5%20series%20is%0Acurrently%20one%20of%20the%20most%20advanced%20and%20widely%20used%20methods%20for%20object%0Adetection.%20However%2C%20it%20faces%20inherent%20challenges%2C%20such%20as%20high%20computational%0Aload%20on%20devices%20and%20insufficient%20detection%20accuracy.%20To%20address%20these%20concerns%2C%0Athis%20paper%20presents%20an%20enhanced%20lightweight%20YOLOv5%20technique%20customized%20for%0Amobile%20devices%2C%20specifically%20intended%20for%20identifying%20objects%20associated%20with%0Atransmission%20lines.%20The%20C3Ghost%20module%20is%20integrated%20into%20the%20convolutional%0Anetwork%20of%20YOLOv5%20to%20reduce%20floating%20point%20operations%20per%20second%20%28FLOPs%29%20in%20the%0Afeature%20channel%20fusion%20process%20and%20improve%20feature%20expression%20performance.%20In%0Aaddition%2C%20a%20FasterNet%20module%20is%20introduced%20to%20replace%20the%20c3%20module%20in%20the%0AYOLOv5%20Backbone.%20The%20FasterNet%20module%20uses%20Partial%20Convolutions%20to%20process%20only%0Aa%20portion%20of%20the%20input%20channels%2C%20improving%20feature%20extraction%20efficiency%20and%0Areducing%20computational%20overhead.%20To%20address%20the%20imbalance%20between%20simple%20and%0Achallenging%20samples%20in%20the%20dataset%20and%20the%20diversity%20of%20aspect%20ratios%20of%0Abounding%20boxes%2C%20the%20wIoU%20v3%20LOSS%20is%20adopted%20as%20the%20loss%20function.%20To%20validate%0Athe%20performance%20of%20the%20proposed%20approach%2C%20Experiments%20are%20conducted%20on%20a%20custom%0Adataset%20of%20transmission%20line%20poles.%20The%20results%20show%20that%20the%20proposed%20model%0Aachieves%20a%201%25%20increase%20in%20detection%20accuracy%2C%20a%2013%25%20reduction%20in%20FLOPs%2C%20and%20a%0A26%25%20decrease%20in%20model%20parameters%20compared%20to%20the%20existing%20YOLOv5.In%20the%0Aablation%20experiment%2C%20it%20was%20also%20discovered%20that%20while%20the%20Fastnet%20module%20and%0Athe%20CSghost%20module%20improved%20the%20precision%20of%20the%20original%20YOLOv5%20baseline%0Amodel%2C%20they%20caused%20a%20decrease%20in%20the%20mAP%40.5-.95%20metric.%20However%2C%20the%0Aimprovement%20of%20the%20wIoUv3%20loss%20function%20significantly%20mitigated%20the%20decline%20of%0Athe%20mAP%40.5-.95%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13703v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fostc3net%3AA%20Lightweight%20YOLOv5%20Based%20On%20the%20Network%20Structure%0A%20%20Optimization&entry.906535625=Danqing%20Ma%20and%20Shaojie%20Li%20and%20Bo%20Dang%20and%20Hengyi%20Zang%20and%20Xinqi%20Dong&entry.1292438233=%20%20Transmission%20line%20detection%20technology%20is%20crucial%20for%20automatic%20monitoring%0Aand%20ensuring%20the%20safety%20of%20electrical%20facilities.%20The%20YOLOv5%20series%20is%0Acurrently%20one%20of%20the%20most%20advanced%20and%20widely%20used%20methods%20for%20object%0Adetection.%20However%2C%20it%20faces%20inherent%20challenges%2C%20such%20as%20high%20computational%0Aload%20on%20devices%20and%20insufficient%20detection%20accuracy.%20To%20address%20these%20concerns%2C%0Athis%20paper%20presents%20an%20enhanced%20lightweight%20YOLOv5%20technique%20customized%20for%0Amobile%20devices%2C%20specifically%20intended%20for%20identifying%20objects%20associated%20with%0Atransmission%20lines.%20The%20C3Ghost%20module%20is%20integrated%20into%20the%20convolutional%0Anetwork%20of%20YOLOv5%20to%20reduce%20floating%20point%20operations%20per%20second%20%28FLOPs%29%20in%20the%0Afeature%20channel%20fusion%20process%20and%20improve%20feature%20expression%20performance.%20In%0Aaddition%2C%20a%20FasterNet%20module%20is%20introduced%20to%20replace%20the%20c3%20module%20in%20the%0AYOLOv5%20Backbone.%20The%20FasterNet%20module%20uses%20Partial%20Convolutions%20to%20process%20only%0Aa%20portion%20of%20the%20input%20channels%2C%20improving%20feature%20extraction%20efficiency%20and%0Areducing%20computational%20overhead.%20To%20address%20the%20imbalance%20between%20simple%20and%0Achallenging%20samples%20in%20the%20dataset%20and%20the%20diversity%20of%20aspect%20ratios%20of%0Abounding%20boxes%2C%20the%20wIoU%20v3%20LOSS%20is%20adopted%20as%20the%20loss%20function.%20To%20validate%0Athe%20performance%20of%20the%20proposed%20approach%2C%20Experiments%20are%20conducted%20on%20a%20custom%0Adataset%20of%20transmission%20line%20poles.%20The%20results%20show%20that%20the%20proposed%20model%0Aachieves%20a%201%25%20increase%20in%20detection%20accuracy%2C%20a%2013%25%20reduction%20in%20FLOPs%2C%20and%20a%0A26%25%20decrease%20in%20model%20parameters%20compared%20to%20the%20existing%20YOLOv5.In%20the%0Aablation%20experiment%2C%20it%20was%20also%20discovered%20that%20while%20the%20Fastnet%20module%20and%0Athe%20CSghost%20module%20improved%20the%20precision%20of%20the%20original%20YOLOv5%20baseline%0Amodel%2C%20they%20caused%20a%20decrease%20in%20the%20mAP%40.5-.95%20metric.%20However%2C%20the%0Aimprovement%20of%20the%20wIoUv3%20loss%20function%20significantly%20mitigated%20the%20decline%20of%0Athe%20mAP%40.5-.95%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13703v1&entry.124074799=Read"},
{"title": "Sparse Implementation of Versatile Graph-Informed Layers", "author": "Francesco Della Santa", "abstract": "  Graph Neural Networks (GNNs) have emerged as effective tools for learning\ntasks on graph-structured data. Recently, Graph-Informed (GI) layers were\nintroduced to address regression tasks on graph nodes, extending their\napplicability beyond classic GNNs. However, existing implementations of GI\nlayers lack efficiency due to dense memory allocation. This paper presents a\nsparse implementation of GI layers, leveraging the sparsity of adjacency\nmatrices to reduce memory usage significantly. Additionally, a versatile\ngeneral form of GI layers is introduced, enabling their application to subsets\nof graph nodes. The proposed sparse implementation improves the concrete\ncomputational efficiency and scalability of the GI layers, permitting to build\ndeeper Graph-Informed Neural Networks (GINNs) and facilitating their\nscalability to larger graphs.\n", "link": "http://arxiv.org/abs/2403.13781v1", "date": "2024-03-20", "relevancy": 2.4053, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5213}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4696}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Implementation%20of%20Versatile%20Graph-Informed%20Layers&body=Title%3A%20Sparse%20Implementation%20of%20Versatile%20Graph-Informed%20Layers%0AAuthor%3A%20Francesco%20Della%20Santa%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20effective%20tools%20for%20learning%0Atasks%20on%20graph-structured%20data.%20Recently%2C%20Graph-Informed%20%28GI%29%20layers%20were%0Aintroduced%20to%20address%20regression%20tasks%20on%20graph%20nodes%2C%20extending%20their%0Aapplicability%20beyond%20classic%20GNNs.%20However%2C%20existing%20implementations%20of%20GI%0Alayers%20lack%20efficiency%20due%20to%20dense%20memory%20allocation.%20This%20paper%20presents%20a%0Asparse%20implementation%20of%20GI%20layers%2C%20leveraging%20the%20sparsity%20of%20adjacency%0Amatrices%20to%20reduce%20memory%20usage%20significantly.%20Additionally%2C%20a%20versatile%0Ageneral%20form%20of%20GI%20layers%20is%20introduced%2C%20enabling%20their%20application%20to%20subsets%0Aof%20graph%20nodes.%20The%20proposed%20sparse%20implementation%20improves%20the%20concrete%0Acomputational%20efficiency%20and%20scalability%20of%20the%20GI%20layers%2C%20permitting%20to%20build%0Adeeper%20Graph-Informed%20Neural%20Networks%20%28GINNs%29%20and%20facilitating%20their%0Ascalability%20to%20larger%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13781v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Implementation%20of%20Versatile%20Graph-Informed%20Layers&entry.906535625=Francesco%20Della%20Santa&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20effective%20tools%20for%20learning%0Atasks%20on%20graph-structured%20data.%20Recently%2C%20Graph-Informed%20%28GI%29%20layers%20were%0Aintroduced%20to%20address%20regression%20tasks%20on%20graph%20nodes%2C%20extending%20their%0Aapplicability%20beyond%20classic%20GNNs.%20However%2C%20existing%20implementations%20of%20GI%0Alayers%20lack%20efficiency%20due%20to%20dense%20memory%20allocation.%20This%20paper%20presents%20a%0Asparse%20implementation%20of%20GI%20layers%2C%20leveraging%20the%20sparsity%20of%20adjacency%0Amatrices%20to%20reduce%20memory%20usage%20significantly.%20Additionally%2C%20a%20versatile%0Ageneral%20form%20of%20GI%20layers%20is%20introduced%2C%20enabling%20their%20application%20to%20subsets%0Aof%20graph%20nodes.%20The%20proposed%20sparse%20implementation%20improves%20the%20concrete%0Acomputational%20efficiency%20and%20scalability%20of%20the%20GI%20layers%2C%20permitting%20to%20build%0Adeeper%20Graph-Informed%20Neural%20Networks%20%28GINNs%29%20and%20facilitating%20their%0Ascalability%20to%20larger%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13781v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning: A Convex Optimization Approach", "author": "Ather Gattami", "abstract": "  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n", "link": "http://arxiv.org/abs/2402.19212v3", "date": "2024-03-20", "relevancy": 2.4035, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.505}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4844}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&body=Title%3A%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach%0AAuthor%3A%20Ather%20Gattami%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19212v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&entry.906535625=Ather%20Gattami&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19212v3&entry.124074799=Read"},
{"title": "DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with\n  Multiple Sensors for Large-Scale Localization and Mapping", "author": "Yuxuan Zhou and Xingxing Li and Shengyu Li and Xuanbin Wang and Shaoquan Feng and Yuxuan Tan", "abstract": "  Visual simultaneous localization and mapping (VSLAM) has broad applications,\nwith state-of-the-art methods leveraging deep neural networks for better\nrobustness and applicability. However, there is a lack of research in fusing\nthese learning-based methods with multi-sensor information, which could be\nindispensable to push related applications to large-scale and complex\nscenarios. In this paper, we tightly integrate the trainable deep dense bundle\nadjustment (DBA) with multi-sensor information through a factor graph. In the\nframework, recurrent optical flow and DBA are performed among sequential\nimages. The Hessian information derived from DBA is fed into a generic factor\ngraph for multi-sensor fusion, which employs a sliding window and supports\nprobabilistic marginalization. A pipeline for visual-inertial integration is\nfirstly developed, which provides the minimum ability of metric-scale\nlocalization and mapping. Furthermore, other sensors (e.g., global navigation\nsatellite system) are integrated for driftless and geo-referencing\nfunctionality. Extensive tests are conducted on both public datasets and\nself-collected datasets. The results validate the superior localization\nperformance of our approach, which enables real-time dense mapping in\nlarge-scale environments. The code has been made open-source\n(https://github.com/GREAT-WHU/DBA-Fusion).\n", "link": "http://arxiv.org/abs/2403.13714v1", "date": "2024-03-20", "relevancy": 2.4021, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6062}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DBA-Fusion%3A%20Tightly%20Integrating%20Deep%20Dense%20Visual%20Bundle%20Adjustment%20with%0A%20%20Multiple%20Sensors%20for%20Large-Scale%20Localization%20and%20Mapping&body=Title%3A%20DBA-Fusion%3A%20Tightly%20Integrating%20Deep%20Dense%20Visual%20Bundle%20Adjustment%20with%0A%20%20Multiple%20Sensors%20for%20Large-Scale%20Localization%20and%20Mapping%0AAuthor%3A%20Yuxuan%20Zhou%20and%20Xingxing%20Li%20and%20Shengyu%20Li%20and%20Xuanbin%20Wang%20and%20Shaoquan%20Feng%20and%20Yuxuan%20Tan%0AAbstract%3A%20%20%20Visual%20simultaneous%20localization%20and%20mapping%20%28VSLAM%29%20has%20broad%20applications%2C%0Awith%20state-of-the-art%20methods%20leveraging%20deep%20neural%20networks%20for%20better%0Arobustness%20and%20applicability.%20However%2C%20there%20is%20a%20lack%20of%20research%20in%20fusing%0Athese%20learning-based%20methods%20with%20multi-sensor%20information%2C%20which%20could%20be%0Aindispensable%20to%20push%20related%20applications%20to%20large-scale%20and%20complex%0Ascenarios.%20In%20this%20paper%2C%20we%20tightly%20integrate%20the%20trainable%20deep%20dense%20bundle%0Aadjustment%20%28DBA%29%20with%20multi-sensor%20information%20through%20a%20factor%20graph.%20In%20the%0Aframework%2C%20recurrent%20optical%20flow%20and%20DBA%20are%20performed%20among%20sequential%0Aimages.%20The%20Hessian%20information%20derived%20from%20DBA%20is%20fed%20into%20a%20generic%20factor%0Agraph%20for%20multi-sensor%20fusion%2C%20which%20employs%20a%20sliding%20window%20and%20supports%0Aprobabilistic%20marginalization.%20A%20pipeline%20for%20visual-inertial%20integration%20is%0Afirstly%20developed%2C%20which%20provides%20the%20minimum%20ability%20of%20metric-scale%0Alocalization%20and%20mapping.%20Furthermore%2C%20other%20sensors%20%28e.g.%2C%20global%20navigation%0Asatellite%20system%29%20are%20integrated%20for%20driftless%20and%20geo-referencing%0Afunctionality.%20Extensive%20tests%20are%20conducted%20on%20both%20public%20datasets%20and%0Aself-collected%20datasets.%20The%20results%20validate%20the%20superior%20localization%0Aperformance%20of%20our%20approach%2C%20which%20enables%20real-time%20dense%20mapping%20in%0Alarge-scale%20environments.%20The%20code%20has%20been%20made%20open-source%0A%28https%3A//github.com/GREAT-WHU/DBA-Fusion%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13714v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBA-Fusion%3A%20Tightly%20Integrating%20Deep%20Dense%20Visual%20Bundle%20Adjustment%20with%0A%20%20Multiple%20Sensors%20for%20Large-Scale%20Localization%20and%20Mapping&entry.906535625=Yuxuan%20Zhou%20and%20Xingxing%20Li%20and%20Shengyu%20Li%20and%20Xuanbin%20Wang%20and%20Shaoquan%20Feng%20and%20Yuxuan%20Tan&entry.1292438233=%20%20Visual%20simultaneous%20localization%20and%20mapping%20%28VSLAM%29%20has%20broad%20applications%2C%0Awith%20state-of-the-art%20methods%20leveraging%20deep%20neural%20networks%20for%20better%0Arobustness%20and%20applicability.%20However%2C%20there%20is%20a%20lack%20of%20research%20in%20fusing%0Athese%20learning-based%20methods%20with%20multi-sensor%20information%2C%20which%20could%20be%0Aindispensable%20to%20push%20related%20applications%20to%20large-scale%20and%20complex%0Ascenarios.%20In%20this%20paper%2C%20we%20tightly%20integrate%20the%20trainable%20deep%20dense%20bundle%0Aadjustment%20%28DBA%29%20with%20multi-sensor%20information%20through%20a%20factor%20graph.%20In%20the%0Aframework%2C%20recurrent%20optical%20flow%20and%20DBA%20are%20performed%20among%20sequential%0Aimages.%20The%20Hessian%20information%20derived%20from%20DBA%20is%20fed%20into%20a%20generic%20factor%0Agraph%20for%20multi-sensor%20fusion%2C%20which%20employs%20a%20sliding%20window%20and%20supports%0Aprobabilistic%20marginalization.%20A%20pipeline%20for%20visual-inertial%20integration%20is%0Afirstly%20developed%2C%20which%20provides%20the%20minimum%20ability%20of%20metric-scale%0Alocalization%20and%20mapping.%20Furthermore%2C%20other%20sensors%20%28e.g.%2C%20global%20navigation%0Asatellite%20system%29%20are%20integrated%20for%20driftless%20and%20geo-referencing%0Afunctionality.%20Extensive%20tests%20are%20conducted%20on%20both%20public%20datasets%20and%0Aself-collected%20datasets.%20The%20results%20validate%20the%20superior%20localization%0Aperformance%20of%20our%20approach%2C%20which%20enables%20real-time%20dense%20mapping%20in%0Alarge-scale%20environments.%20The%20code%20has%20been%20made%20open-source%0A%28https%3A//github.com/GREAT-WHU/DBA-Fusion%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13714v1&entry.124074799=Read"},
{"title": "TimeRewind: Rewinding Time with Image-and-Events Video Diffusion", "author": "Jingxi Chen and Brandon Y. Feng and Haoming Cai and Mingyang Xie and Christopher Metzler and Cornelia Fermuller and Yiannis Aloimonos", "abstract": "  This paper addresses the novel challenge of ``rewinding'' time from a single\ncaptured image to recover the fleeting moments missed just before the shutter\nbutton is pressed. This problem poses a significant challenge in computer\nvision and computational photography, as it requires predicting plausible\npre-capture motion from a single static frame, an inherently ill-posed task due\nto the high degree of freedom in potential pixel movements. We overcome this\nchallenge by leveraging the emerging technology of neuromorphic event cameras,\nwhich capture motion information with high temporal resolution, and integrating\nthis data with advanced image-to-video diffusion models. Our proposed framework\nintroduces an event motion adaptor conditioned on event camera data, guiding\nthe diffusion model to generate videos that are visually coherent and\nphysically grounded in the captured events. Through extensive experimentation,\nwe demonstrate the capability of our approach to synthesize high-quality videos\nthat effectively ``rewind'' time, showcasing the potential of combining event\ncamera technology with generative models. Our work opens new avenues for\nresearch at the intersection of computer vision, computational photography, and\ngenerative modeling, offering a forward-thinking solution to capturing missed\nmoments and enhancing future consumer cameras and smartphones. Please see the\nproject page at https://timerewind.github.io/ for video results and code\nrelease.\n", "link": "http://arxiv.org/abs/2403.13800v1", "date": "2024-03-20", "relevancy": 2.3894, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6203}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5987}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5869}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TimeRewind%3A%20Rewinding%20Time%20with%20Image-and-Events%20Video%20Diffusion&body=Title%3A%20TimeRewind%3A%20Rewinding%20Time%20with%20Image-and-Events%20Video%20Diffusion%0AAuthor%3A%20Jingxi%20Chen%20and%20Brandon%20Y.%20Feng%20and%20Haoming%20Cai%20and%20Mingyang%20Xie%20and%20Christopher%20Metzler%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20novel%20challenge%20of%20%60%60rewinding%27%27%20time%20from%20a%20single%0Acaptured%20image%20to%20recover%20the%20fleeting%20moments%20missed%20just%20before%20the%20shutter%0Abutton%20is%20pressed.%20This%20problem%20poses%20a%20significant%20challenge%20in%20computer%0Avision%20and%20computational%20photography%2C%20as%20it%20requires%20predicting%20plausible%0Apre-capture%20motion%20from%20a%20single%20static%20frame%2C%20an%20inherently%20ill-posed%20task%20due%0Ato%20the%20high%20degree%20of%20freedom%20in%20potential%20pixel%20movements.%20We%20overcome%20this%0Achallenge%20by%20leveraging%20the%20emerging%20technology%20of%20neuromorphic%20event%20cameras%2C%0Awhich%20capture%20motion%20information%20with%20high%20temporal%20resolution%2C%20and%20integrating%0Athis%20data%20with%20advanced%20image-to-video%20diffusion%20models.%20Our%20proposed%20framework%0Aintroduces%20an%20event%20motion%20adaptor%20conditioned%20on%20event%20camera%20data%2C%20guiding%0Athe%20diffusion%20model%20to%20generate%20videos%20that%20are%20visually%20coherent%20and%0Aphysically%20grounded%20in%20the%20captured%20events.%20Through%20extensive%20experimentation%2C%0Awe%20demonstrate%20the%20capability%20of%20our%20approach%20to%20synthesize%20high-quality%20videos%0Athat%20effectively%20%60%60rewind%27%27%20time%2C%20showcasing%20the%20potential%20of%20combining%20event%0Acamera%20technology%20with%20generative%20models.%20Our%20work%20opens%20new%20avenues%20for%0Aresearch%20at%20the%20intersection%20of%20computer%20vision%2C%20computational%20photography%2C%20and%0Agenerative%20modeling%2C%20offering%20a%20forward-thinking%20solution%20to%20capturing%20missed%0Amoments%20and%20enhancing%20future%20consumer%20cameras%20and%20smartphones.%20Please%20see%20the%0Aproject%20page%20at%20https%3A//timerewind.github.io/%20for%20video%20results%20and%20code%0Arelease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13800v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeRewind%3A%20Rewinding%20Time%20with%20Image-and-Events%20Video%20Diffusion&entry.906535625=Jingxi%20Chen%20and%20Brandon%20Y.%20Feng%20and%20Haoming%20Cai%20and%20Mingyang%20Xie%20and%20Christopher%20Metzler%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20This%20paper%20addresses%20the%20novel%20challenge%20of%20%60%60rewinding%27%27%20time%20from%20a%20single%0Acaptured%20image%20to%20recover%20the%20fleeting%20moments%20missed%20just%20before%20the%20shutter%0Abutton%20is%20pressed.%20This%20problem%20poses%20a%20significant%20challenge%20in%20computer%0Avision%20and%20computational%20photography%2C%20as%20it%20requires%20predicting%20plausible%0Apre-capture%20motion%20from%20a%20single%20static%20frame%2C%20an%20inherently%20ill-posed%20task%20due%0Ato%20the%20high%20degree%20of%20freedom%20in%20potential%20pixel%20movements.%20We%20overcome%20this%0Achallenge%20by%20leveraging%20the%20emerging%20technology%20of%20neuromorphic%20event%20cameras%2C%0Awhich%20capture%20motion%20information%20with%20high%20temporal%20resolution%2C%20and%20integrating%0Athis%20data%20with%20advanced%20image-to-video%20diffusion%20models.%20Our%20proposed%20framework%0Aintroduces%20an%20event%20motion%20adaptor%20conditioned%20on%20event%20camera%20data%2C%20guiding%0Athe%20diffusion%20model%20to%20generate%20videos%20that%20are%20visually%20coherent%20and%0Aphysically%20grounded%20in%20the%20captured%20events.%20Through%20extensive%20experimentation%2C%0Awe%20demonstrate%20the%20capability%20of%20our%20approach%20to%20synthesize%20high-quality%20videos%0Athat%20effectively%20%60%60rewind%27%27%20time%2C%20showcasing%20the%20potential%20of%20combining%20event%0Acamera%20technology%20with%20generative%20models.%20Our%20work%20opens%20new%20avenues%20for%0Aresearch%20at%20the%20intersection%20of%20computer%20vision%2C%20computational%20photography%2C%20and%0Agenerative%20modeling%2C%20offering%20a%20forward-thinking%20solution%20to%20capturing%20missed%0Amoments%20and%20enhancing%20future%20consumer%20cameras%20and%20smartphones.%20Please%20see%20the%0Aproject%20page%20at%20https%3A//timerewind.github.io/%20for%20video%20results%20and%20code%0Arelease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13800v1&entry.124074799=Read"},
{"title": "DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced\n  Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs", "author": "Haoyu Wang and Basel Halak and Jianjie Ren and Ahmad Atamli", "abstract": "  This study introduces a refined Flooding Injection Rate-adjustable\nDenial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly\npresents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame\nFusion (2F) for DoS detection and localization. Two Convolutional Neural\nNetworks models for classification and segmentation were developed to detect\nand localize DoS respectively. It achieves detection and localization\naccuracies of 95.8\\% and 91.7\\%, and precision rates of 98.5\\% and 99.3\\% in a\n16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\\%\nwhen scaling from 8x8 to 16x16 NoCs, and it requires 42.4\\% less hardware\ncompared to state-of-the-arts. This advancement demonstrates DL2Fence's\neffectiveness in balancing outstanding detection performance in large-scale\nNoCs with extremely low hardware overhead.\n", "link": "http://arxiv.org/abs/2403.13563v1", "date": "2024-03-20", "relevancy": 2.3805, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4877}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4722}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4684}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DL2Fence%3A%20Integrating%20Deep%20Learning%20and%20Frame%20Fusion%20for%20Enhanced%0A%20%20Detection%20and%20Localization%20of%20Refined%20Denial-of-Service%20in%20Large-Scale%20NoCs&body=Title%3A%20DL2Fence%3A%20Integrating%20Deep%20Learning%20and%20Frame%20Fusion%20for%20Enhanced%0A%20%20Detection%20and%20Localization%20of%20Refined%20Denial-of-Service%20in%20Large-Scale%20NoCs%0AAuthor%3A%20Haoyu%20Wang%20and%20Basel%20Halak%20and%20Jianjie%20Ren%20and%20Ahmad%20Atamli%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20refined%20Flooding%20Injection%20Rate-adjustable%0ADenial-of-Service%20%28DoS%29%20model%20for%20Network-on-Chips%20%28NoCs%29%20and%20more%20importantly%0Apresents%20DL2Fence%2C%20a%20novel%20framework%20utilizing%20Deep%20Learning%20%28DL%29%20and%20Frame%0AFusion%20%282F%29%20for%20DoS%20detection%20and%20localization.%20Two%20Convolutional%20Neural%0ANetworks%20models%20for%20classification%20and%20segmentation%20were%20developed%20to%20detect%0Aand%20localize%20DoS%20respectively.%20It%20achieves%20detection%20and%20localization%0Aaccuracies%20of%2095.8%5C%25%20and%2091.7%5C%25%2C%20and%20precision%20rates%20of%2098.5%5C%25%20and%2099.3%5C%25%20in%20a%0A16x16%20mesh%20NoC.%20The%20framework%27s%20hardware%20overhead%20notably%20decreases%20by%2076.3%5C%25%0Awhen%20scaling%20from%208x8%20to%2016x16%20NoCs%2C%20and%20it%20requires%2042.4%5C%25%20less%20hardware%0Acompared%20to%20state-of-the-arts.%20This%20advancement%20demonstrates%20DL2Fence%27s%0Aeffectiveness%20in%20balancing%20outstanding%20detection%20performance%20in%20large-scale%0ANoCs%20with%20extremely%20low%20hardware%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13563v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DL2Fence%3A%20Integrating%20Deep%20Learning%20and%20Frame%20Fusion%20for%20Enhanced%0A%20%20Detection%20and%20Localization%20of%20Refined%20Denial-of-Service%20in%20Large-Scale%20NoCs&entry.906535625=Haoyu%20Wang%20and%20Basel%20Halak%20and%20Jianjie%20Ren%20and%20Ahmad%20Atamli&entry.1292438233=%20%20This%20study%20introduces%20a%20refined%20Flooding%20Injection%20Rate-adjustable%0ADenial-of-Service%20%28DoS%29%20model%20for%20Network-on-Chips%20%28NoCs%29%20and%20more%20importantly%0Apresents%20DL2Fence%2C%20a%20novel%20framework%20utilizing%20Deep%20Learning%20%28DL%29%20and%20Frame%0AFusion%20%282F%29%20for%20DoS%20detection%20and%20localization.%20Two%20Convolutional%20Neural%0ANetworks%20models%20for%20classification%20and%20segmentation%20were%20developed%20to%20detect%0Aand%20localize%20DoS%20respectively.%20It%20achieves%20detection%20and%20localization%0Aaccuracies%20of%2095.8%5C%25%20and%2091.7%5C%25%2C%20and%20precision%20rates%20of%2098.5%5C%25%20and%2099.3%5C%25%20in%20a%0A16x16%20mesh%20NoC.%20The%20framework%27s%20hardware%20overhead%20notably%20decreases%20by%2076.3%5C%25%0Awhen%20scaling%20from%208x8%20to%2016x16%20NoCs%2C%20and%20it%20requires%2042.4%5C%25%20less%20hardware%0Acompared%20to%20state-of-the-arts.%20This%20advancement%20demonstrates%20DL2Fence%27s%0Aeffectiveness%20in%20balancing%20outstanding%20detection%20performance%20in%20large-scale%0ANoCs%20with%20extremely%20low%20hardware%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13563v1&entry.124074799=Read"},
{"title": "DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing\n  High-Quality Implicit Neural Representations", "author": "Dogyun Park and Sihyeon Kim and Sojin Lee and Hyunwoo J. Kim", "abstract": "  Recent studies have introduced a new class of generative models for\nsynthesizing implicit neural representations (INRs) that capture arbitrary\ncontinuous signals in various domains. These models opened the door for\ndomain-agnostic generative models, but they often fail to achieve high-quality\ngeneration. We observed that the existing methods generate the weights of\nneural networks to parameterize INRs and evaluate the network with fixed\npositional embeddings (PEs). Arguably, this architecture limits the expressive\npower of generative models and results in low-quality INR generation. To\naddress this limitation, we propose Domain-agnostic Latent Diffusion Model for\nINRs (DDMI) that generates adaptive positional embeddings instead of neural\nnetworks' weights. Specifically, we develop a Discrete-to-continuous space\nVariational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and\nthe continuous signal functions in the shared latent space. Additionally, we\nintroduce a novel conditioning mechanism for evaluating INRs with the\nhierarchically decomposed PEs to further enhance expressive power. Extensive\nexperiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance\nFields, and videos, with seven benchmark datasets, demonstrate the versatility\nof DDMI and its superior performance compared to the existing INR generative\nmodels.\n", "link": "http://arxiv.org/abs/2401.12517v2", "date": "2024-03-20", "relevancy": 2.3755, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6047}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5952}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DDMI%3A%20Domain-Agnostic%20Latent%20Diffusion%20Models%20for%20Synthesizing%0A%20%20High-Quality%20Implicit%20Neural%20Representations&body=Title%3A%20DDMI%3A%20Domain-Agnostic%20Latent%20Diffusion%20Models%20for%20Synthesizing%0A%20%20High-Quality%20Implicit%20Neural%20Representations%0AAuthor%3A%20Dogyun%20Park%20and%20Sihyeon%20Kim%20and%20Sojin%20Lee%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Recent%20studies%20have%20introduced%20a%20new%20class%20of%20generative%20models%20for%0Asynthesizing%20implicit%20neural%20representations%20%28INRs%29%20that%20capture%20arbitrary%0Acontinuous%20signals%20in%20various%20domains.%20These%20models%20opened%20the%20door%20for%0Adomain-agnostic%20generative%20models%2C%20but%20they%20often%20fail%20to%20achieve%20high-quality%0Ageneration.%20We%20observed%20that%20the%20existing%20methods%20generate%20the%20weights%20of%0Aneural%20networks%20to%20parameterize%20INRs%20and%20evaluate%20the%20network%20with%20fixed%0Apositional%20embeddings%20%28PEs%29.%20Arguably%2C%20this%20architecture%20limits%20the%20expressive%0Apower%20of%20generative%20models%20and%20results%20in%20low-quality%20INR%20generation.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20Domain-agnostic%20Latent%20Diffusion%20Model%20for%0AINRs%20%28DDMI%29%20that%20generates%20adaptive%20positional%20embeddings%20instead%20of%20neural%0Anetworks%27%20weights.%20Specifically%2C%20we%20develop%20a%20Discrete-to-continuous%20space%0AVariational%20AutoEncoder%20%28D2C-VAE%29%2C%20which%20seamlessly%20connects%20discrete%20data%20and%0Athe%20continuous%20signal%20functions%20in%20the%20shared%20latent%20space.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20conditioning%20mechanism%20for%20evaluating%20INRs%20with%20the%0Ahierarchically%20decomposed%20PEs%20to%20further%20enhance%20expressive%20power.%20Extensive%0Aexperiments%20across%20four%20modalities%2C%20e.g.%2C%202D%20images%2C%203D%20shapes%2C%20Neural%20Radiance%0AFields%2C%20and%20videos%2C%20with%20seven%20benchmark%20datasets%2C%20demonstrate%20the%20versatility%0Aof%20DDMI%20and%20its%20superior%20performance%20compared%20to%20the%20existing%20INR%20generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12517v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDMI%3A%20Domain-Agnostic%20Latent%20Diffusion%20Models%20for%20Synthesizing%0A%20%20High-Quality%20Implicit%20Neural%20Representations&entry.906535625=Dogyun%20Park%20and%20Sihyeon%20Kim%20and%20Sojin%20Lee%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Recent%20studies%20have%20introduced%20a%20new%20class%20of%20generative%20models%20for%0Asynthesizing%20implicit%20neural%20representations%20%28INRs%29%20that%20capture%20arbitrary%0Acontinuous%20signals%20in%20various%20domains.%20These%20models%20opened%20the%20door%20for%0Adomain-agnostic%20generative%20models%2C%20but%20they%20often%20fail%20to%20achieve%20high-quality%0Ageneration.%20We%20observed%20that%20the%20existing%20methods%20generate%20the%20weights%20of%0Aneural%20networks%20to%20parameterize%20INRs%20and%20evaluate%20the%20network%20with%20fixed%0Apositional%20embeddings%20%28PEs%29.%20Arguably%2C%20this%20architecture%20limits%20the%20expressive%0Apower%20of%20generative%20models%20and%20results%20in%20low-quality%20INR%20generation.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20Domain-agnostic%20Latent%20Diffusion%20Model%20for%0AINRs%20%28DDMI%29%20that%20generates%20adaptive%20positional%20embeddings%20instead%20of%20neural%0Anetworks%27%20weights.%20Specifically%2C%20we%20develop%20a%20Discrete-to-continuous%20space%0AVariational%20AutoEncoder%20%28D2C-VAE%29%2C%20which%20seamlessly%20connects%20discrete%20data%20and%0Athe%20continuous%20signal%20functions%20in%20the%20shared%20latent%20space.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20conditioning%20mechanism%20for%20evaluating%20INRs%20with%20the%0Ahierarchically%20decomposed%20PEs%20to%20further%20enhance%20expressive%20power.%20Extensive%0Aexperiments%20across%20four%20modalities%2C%20e.g.%2C%202D%20images%2C%203D%20shapes%2C%20Neural%20Radiance%0AFields%2C%20and%20videos%2C%20with%20seven%20benchmark%20datasets%2C%20demonstrate%20the%20versatility%0Aof%20DDMI%20and%20its%20superior%20performance%20compared%20to%20the%20existing%20INR%20generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12517v2&entry.124074799=Read"},
{"title": "Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific\n  Adaptation", "author": "Fu-Yun Wang and Xiaoshi Wu and Zhaoyang Huang and Xiaoyu Shi and Dazhong Shen and Guanglu Song and Yu Liu and Hongsheng Li", "abstract": "  Video outpainting is a challenging task, aiming at generating video content\noutside the viewport of the input video while maintaining inter-frame and\nintra-frame consistency. Existing methods fall short in either generation\nquality or flexibility. We introduce MOTIA Mastering Video Outpainting Through\nInput-Specific Adaptation, a diffusion-based pipeline that leverages both the\nintrinsic data-specific patterns of the source video and the image/video\ngenerative prior for effective outpainting. MOTIA comprises two main phases:\ninput-specific adaptation and pattern-aware outpainting. The input-specific\nadaptation phase involves conducting efficient and effective pseudo outpainting\nlearning on the single-shot source video. This process encourages the model to\nidentify and learn patterns within the source video, as well as bridging the\ngap between standard generative processes and outpainting. The subsequent\nphase, pattern-aware outpainting, is dedicated to the generalization of these\nlearned patterns to generate outpainting outcomes. Additional strategies\nincluding spatial-aware insertion and noise travel are proposed to better\nleverage the diffusion model's generative prior and the acquired video patterns\nfrom source videos. Extensive evaluations underscore MOTIA's superiority,\noutperforming existing state-of-the-art methods in widely recognized\nbenchmarks. Notably, these advancements are achieved without necessitating\nextensive, task-specific tuning.\n", "link": "http://arxiv.org/abs/2403.13745v1", "date": "2024-03-20", "relevancy": 2.3707, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6283}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5711}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Be-Your-Outpainter%3A%20Mastering%20Video%20Outpainting%20through%20Input-Specific%0A%20%20Adaptation&body=Title%3A%20Be-Your-Outpainter%3A%20Mastering%20Video%20Outpainting%20through%20Input-Specific%0A%20%20Adaptation%0AAuthor%3A%20Fu-Yun%20Wang%20and%20Xiaoshi%20Wu%20and%20Zhaoyang%20Huang%20and%20Xiaoyu%20Shi%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Yu%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Video%20outpainting%20is%20a%20challenging%20task%2C%20aiming%20at%20generating%20video%20content%0Aoutside%20the%20viewport%20of%20the%20input%20video%20while%20maintaining%20inter-frame%20and%0Aintra-frame%20consistency.%20Existing%20methods%20fall%20short%20in%20either%20generation%0Aquality%20or%20flexibility.%20We%20introduce%20MOTIA%20Mastering%20Video%20Outpainting%20Through%0AInput-Specific%20Adaptation%2C%20a%20diffusion-based%20pipeline%20that%20leverages%20both%20the%0Aintrinsic%20data-specific%20patterns%20of%20the%20source%20video%20and%20the%20image/video%0Agenerative%20prior%20for%20effective%20outpainting.%20MOTIA%20comprises%20two%20main%20phases%3A%0Ainput-specific%20adaptation%20and%20pattern-aware%20outpainting.%20The%20input-specific%0Aadaptation%20phase%20involves%20conducting%20efficient%20and%20effective%20pseudo%20outpainting%0Alearning%20on%20the%20single-shot%20source%20video.%20This%20process%20encourages%20the%20model%20to%0Aidentify%20and%20learn%20patterns%20within%20the%20source%20video%2C%20as%20well%20as%20bridging%20the%0Agap%20between%20standard%20generative%20processes%20and%20outpainting.%20The%20subsequent%0Aphase%2C%20pattern-aware%20outpainting%2C%20is%20dedicated%20to%20the%20generalization%20of%20these%0Alearned%20patterns%20to%20generate%20outpainting%20outcomes.%20Additional%20strategies%0Aincluding%20spatial-aware%20insertion%20and%20noise%20travel%20are%20proposed%20to%20better%0Aleverage%20the%20diffusion%20model%27s%20generative%20prior%20and%20the%20acquired%20video%20patterns%0Afrom%20source%20videos.%20Extensive%20evaluations%20underscore%20MOTIA%27s%20superiority%2C%0Aoutperforming%20existing%20state-of-the-art%20methods%20in%20widely%20recognized%0Abenchmarks.%20Notably%2C%20these%20advancements%20are%20achieved%20without%20necessitating%0Aextensive%2C%20task-specific%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13745v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be-Your-Outpainter%3A%20Mastering%20Video%20Outpainting%20through%20Input-Specific%0A%20%20Adaptation&entry.906535625=Fu-Yun%20Wang%20and%20Xiaoshi%20Wu%20and%20Zhaoyang%20Huang%20and%20Xiaoyu%20Shi%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Yu%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20Video%20outpainting%20is%20a%20challenging%20task%2C%20aiming%20at%20generating%20video%20content%0Aoutside%20the%20viewport%20of%20the%20input%20video%20while%20maintaining%20inter-frame%20and%0Aintra-frame%20consistency.%20Existing%20methods%20fall%20short%20in%20either%20generation%0Aquality%20or%20flexibility.%20We%20introduce%20MOTIA%20Mastering%20Video%20Outpainting%20Through%0AInput-Specific%20Adaptation%2C%20a%20diffusion-based%20pipeline%20that%20leverages%20both%20the%0Aintrinsic%20data-specific%20patterns%20of%20the%20source%20video%20and%20the%20image/video%0Agenerative%20prior%20for%20effective%20outpainting.%20MOTIA%20comprises%20two%20main%20phases%3A%0Ainput-specific%20adaptation%20and%20pattern-aware%20outpainting.%20The%20input-specific%0Aadaptation%20phase%20involves%20conducting%20efficient%20and%20effective%20pseudo%20outpainting%0Alearning%20on%20the%20single-shot%20source%20video.%20This%20process%20encourages%20the%20model%20to%0Aidentify%20and%20learn%20patterns%20within%20the%20source%20video%2C%20as%20well%20as%20bridging%20the%0Agap%20between%20standard%20generative%20processes%20and%20outpainting.%20The%20subsequent%0Aphase%2C%20pattern-aware%20outpainting%2C%20is%20dedicated%20to%20the%20generalization%20of%20these%0Alearned%20patterns%20to%20generate%20outpainting%20outcomes.%20Additional%20strategies%0Aincluding%20spatial-aware%20insertion%20and%20noise%20travel%20are%20proposed%20to%20better%0Aleverage%20the%20diffusion%20model%27s%20generative%20prior%20and%20the%20acquired%20video%20patterns%0Afrom%20source%20videos.%20Extensive%20evaluations%20underscore%20MOTIA%27s%20superiority%2C%0Aoutperforming%20existing%20state-of-the-art%20methods%20in%20widely%20recognized%0Abenchmarks.%20Notably%2C%20these%20advancements%20are%20achieved%20without%20necessitating%0Aextensive%2C%20task-specific%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13745v1&entry.124074799=Read"},
{"title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments", "author": "Djamahl Etchegaray and Zi Huang and Tatsuya Harada and Yadan Luo", "abstract": "  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available in the supplementary material.\n", "link": "http://arxiv.org/abs/2403.13556v1", "date": "2024-03-20", "relevancy": 2.3663, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5924}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5906}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments&body=Title%3A%20Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments%0AAuthor%3A%20Djamahl%20Etchegaray%20and%20Zi%20Huang%20and%20Tatsuya%20Harada%20and%20Yadan%20Luo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20limitations%20of%20current%20LiDAR-based%203D%20object%0Adetection%20systems%2C%20which%20are%20hindered%20by%20a%20restricted%20class%20vocabulary%20and%20the%0Ahigh%20costs%20associated%20with%20annotating%20new%20object%20classes.%20Our%20exploration%20of%0Aopen-vocabulary%20%28OV%29%20learning%20in%20urban%20environments%20aims%20to%20capture%20novel%0Ainstances%20using%20pre-trained%20vision-language%20models%20%28VLMs%29%20with%20multi-sensor%0Adata.%20We%20design%20and%20benchmark%20a%20set%20of%20four%20potential%20solutions%20as%20baselines%2C%0Acategorizing%20them%20into%20either%20top-down%20or%20bottom-up%20approaches%20based%20on%20their%0Ainput%20data%20strategies.%20While%20effective%2C%20these%20methods%20exhibit%20certain%0Alimitations%2C%20such%20as%20missing%20novel%20objects%20in%203D%20box%20estimation%20or%20applying%0Arigorous%20priors%2C%20leading%20to%20biases%20towards%20objects%20near%20the%20camera%20or%20of%0Arectangular%20geometries.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20a%20universal%0A%5Ctextsc%7BFind%20n%27%20Propagate%7D%20approach%20for%203D%20OV%20tasks%2C%20aimed%20at%20maximizing%20the%0Arecall%20of%20novel%20objects%20and%20propagating%20this%20detection%20capability%20to%20more%0Adistant%20areas%20thereby%20progressively%20capturing%20more.%20In%20particular%2C%20we%20utilize%20a%0Agreedy%20box%20seeker%20to%20search%20against%203D%20novel%20boxes%20of%20varying%20orientations%20and%0Adepth%20in%20each%20generated%20frustum%20and%20ensure%20the%20reliability%20of%20newly%20identified%0Aboxes%20by%20cross%20alignment%20and%20density%20ranker.%20Additionally%2C%20the%20inherent%20bias%0Atowards%20camera-proximal%20objects%20is%20alleviated%20by%20the%20proposed%20remote%20simulator%2C%0Awhich%20randomly%20diversifies%20pseudo-labeled%20novel%20instances%20in%20the%20self-training%0Aprocess%2C%20combined%20with%20the%20fusion%20of%20base%20samples%20in%20the%20memory%20bank.%20Extensive%0Aexperiments%20demonstrate%20a%2053%25%20improvement%20in%20novel%20recall%20across%20diverse%20OV%0Asettings%2C%20VLMs%2C%20and%203D%20detectors.%20Notably%2C%20we%20achieve%20up%20to%20a%203.97-fold%0Aincrease%20in%20Average%20Precision%20%28AP%29%20for%20novel%20object%20classes.%20The%20source%20code%20is%0Amade%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13556v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments&entry.906535625=Djamahl%20Etchegaray%20and%20Zi%20Huang%20and%20Tatsuya%20Harada%20and%20Yadan%20Luo&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20limitations%20of%20current%20LiDAR-based%203D%20object%0Adetection%20systems%2C%20which%20are%20hindered%20by%20a%20restricted%20class%20vocabulary%20and%20the%0Ahigh%20costs%20associated%20with%20annotating%20new%20object%20classes.%20Our%20exploration%20of%0Aopen-vocabulary%20%28OV%29%20learning%20in%20urban%20environments%20aims%20to%20capture%20novel%0Ainstances%20using%20pre-trained%20vision-language%20models%20%28VLMs%29%20with%20multi-sensor%0Adata.%20We%20design%20and%20benchmark%20a%20set%20of%20four%20potential%20solutions%20as%20baselines%2C%0Acategorizing%20them%20into%20either%20top-down%20or%20bottom-up%20approaches%20based%20on%20their%0Ainput%20data%20strategies.%20While%20effective%2C%20these%20methods%20exhibit%20certain%0Alimitations%2C%20such%20as%20missing%20novel%20objects%20in%203D%20box%20estimation%20or%20applying%0Arigorous%20priors%2C%20leading%20to%20biases%20towards%20objects%20near%20the%20camera%20or%20of%0Arectangular%20geometries.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20a%20universal%0A%5Ctextsc%7BFind%20n%27%20Propagate%7D%20approach%20for%203D%20OV%20tasks%2C%20aimed%20at%20maximizing%20the%0Arecall%20of%20novel%20objects%20and%20propagating%20this%20detection%20capability%20to%20more%0Adistant%20areas%20thereby%20progressively%20capturing%20more.%20In%20particular%2C%20we%20utilize%20a%0Agreedy%20box%20seeker%20to%20search%20against%203D%20novel%20boxes%20of%20varying%20orientations%20and%0Adepth%20in%20each%20generated%20frustum%20and%20ensure%20the%20reliability%20of%20newly%20identified%0Aboxes%20by%20cross%20alignment%20and%20density%20ranker.%20Additionally%2C%20the%20inherent%20bias%0Atowards%20camera-proximal%20objects%20is%20alleviated%20by%20the%20proposed%20remote%20simulator%2C%0Awhich%20randomly%20diversifies%20pseudo-labeled%20novel%20instances%20in%20the%20self-training%0Aprocess%2C%20combined%20with%20the%20fusion%20of%20base%20samples%20in%20the%20memory%20bank.%20Extensive%0Aexperiments%20demonstrate%20a%2053%25%20improvement%20in%20novel%20recall%20across%20diverse%20OV%0Asettings%2C%20VLMs%2C%20and%203D%20detectors.%20Notably%2C%20we%20achieve%20up%20to%20a%203.97-fold%0Aincrease%20in%20Average%20Precision%20%28AP%29%20for%20novel%20object%20classes.%20The%20source%20code%20is%0Amade%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13556v1&entry.124074799=Read"},
{"title": "Have You Poisoned My Data? Defending Neural Networks against Data\n  Poisoning", "author": "Fabio De Gaspari and Dorjan Hitaj and Luigi V. Mancini", "abstract": "  The unprecedented availability of training data fueled the rapid development\nof powerful neural networks in recent years. However, the need for such large\namounts of data leads to potential threats such as poisoning attacks:\nadversarial manipulations of the training data aimed at compromising the\nlearned model to achieve a given adversarial goal.\n  This paper investigates defenses against clean-label poisoning attacks and\nproposes a novel approach to detect and filter poisoned datapoints in the\ntransfer learning setting. We define a new characteristic vector representation\nof datapoints and show that it effectively captures the intrinsic properties of\nthe data distribution. Through experimental analysis, we demonstrate that\neffective poisons can be successfully differentiated from clean points in the\ncharacteristic vector space. We thoroughly evaluate our proposed approach and\ncompare it to existing state-of-the-art defenses using multiple architectures,\ndatasets, and poison budgets. Our evaluation shows that our proposal\noutperforms existing approaches in defense rate and final trained model\nperformance across all experimental settings.\n", "link": "http://arxiv.org/abs/2403.13523v1", "date": "2024-03-20", "relevancy": 2.3538, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4699}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Have%20You%20Poisoned%20My%20Data%3F%20Defending%20Neural%20Networks%20against%20Data%0A%20%20Poisoning&body=Title%3A%20Have%20You%20Poisoned%20My%20Data%3F%20Defending%20Neural%20Networks%20against%20Data%0A%20%20Poisoning%0AAuthor%3A%20Fabio%20De%20Gaspari%20and%20Dorjan%20Hitaj%20and%20Luigi%20V.%20Mancini%0AAbstract%3A%20%20%20The%20unprecedented%20availability%20of%20training%20data%20fueled%20the%20rapid%20development%0Aof%20powerful%20neural%20networks%20in%20recent%20years.%20However%2C%20the%20need%20for%20such%20large%0Aamounts%20of%20data%20leads%20to%20potential%20threats%20such%20as%20poisoning%20attacks%3A%0Aadversarial%20manipulations%20of%20the%20training%20data%20aimed%20at%20compromising%20the%0Alearned%20model%20to%20achieve%20a%20given%20adversarial%20goal.%0A%20%20This%20paper%20investigates%20defenses%20against%20clean-label%20poisoning%20attacks%20and%0Aproposes%20a%20novel%20approach%20to%20detect%20and%20filter%20poisoned%20datapoints%20in%20the%0Atransfer%20learning%20setting.%20We%20define%20a%20new%20characteristic%20vector%20representation%0Aof%20datapoints%20and%20show%20that%20it%20effectively%20captures%20the%20intrinsic%20properties%20of%0Athe%20data%20distribution.%20Through%20experimental%20analysis%2C%20we%20demonstrate%20that%0Aeffective%20poisons%20can%20be%20successfully%20differentiated%20from%20clean%20points%20in%20the%0Acharacteristic%20vector%20space.%20We%20thoroughly%20evaluate%20our%20proposed%20approach%20and%0Acompare%20it%20to%20existing%20state-of-the-art%20defenses%20using%20multiple%20architectures%2C%0Adatasets%2C%20and%20poison%20budgets.%20Our%20evaluation%20shows%20that%20our%20proposal%0Aoutperforms%20existing%20approaches%20in%20defense%20rate%20and%20final%20trained%20model%0Aperformance%20across%20all%20experimental%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13523v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Have%20You%20Poisoned%20My%20Data%3F%20Defending%20Neural%20Networks%20against%20Data%0A%20%20Poisoning&entry.906535625=Fabio%20De%20Gaspari%20and%20Dorjan%20Hitaj%20and%20Luigi%20V.%20Mancini&entry.1292438233=%20%20The%20unprecedented%20availability%20of%20training%20data%20fueled%20the%20rapid%20development%0Aof%20powerful%20neural%20networks%20in%20recent%20years.%20However%2C%20the%20need%20for%20such%20large%0Aamounts%20of%20data%20leads%20to%20potential%20threats%20such%20as%20poisoning%20attacks%3A%0Aadversarial%20manipulations%20of%20the%20training%20data%20aimed%20at%20compromising%20the%0Alearned%20model%20to%20achieve%20a%20given%20adversarial%20goal.%0A%20%20This%20paper%20investigates%20defenses%20against%20clean-label%20poisoning%20attacks%20and%0Aproposes%20a%20novel%20approach%20to%20detect%20and%20filter%20poisoned%20datapoints%20in%20the%0Atransfer%20learning%20setting.%20We%20define%20a%20new%20characteristic%20vector%20representation%0Aof%20datapoints%20and%20show%20that%20it%20effectively%20captures%20the%20intrinsic%20properties%20of%0Athe%20data%20distribution.%20Through%20experimental%20analysis%2C%20we%20demonstrate%20that%0Aeffective%20poisons%20can%20be%20successfully%20differentiated%20from%20clean%20points%20in%20the%0Acharacteristic%20vector%20space.%20We%20thoroughly%20evaluate%20our%20proposed%20approach%20and%0Acompare%20it%20to%20existing%20state-of-the-art%20defenses%20using%20multiple%20architectures%2C%0Adatasets%2C%20and%20poison%20budgets.%20Our%20evaluation%20shows%20that%20our%20proposal%0Aoutperforms%20existing%20approaches%20in%20defense%20rate%20and%20final%20trained%20model%0Aperformance%20across%20all%20experimental%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13523v1&entry.124074799=Read"},
{"title": "Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face\n  Deepfake Detection", "author": "Yuting Xu and Jian Liang and Lijun Sheng and Xiao-Yu Zhang", "abstract": "  The deepfake threats to society and cybersecurity have provoked significant\npublic apprehension, driving intensified efforts within the realm of deepfake\nvideo detection. Current video-level methods are mostly based on {3D CNNs}\nresulting in high computational demands, although have achieved good\nperformance. This paper introduces an elegantly simple yet effective strategy\nnamed Thumbnail Layout (TALL), which transforms a video clip into a pre-defined\nlayout to realize the preservation of spatial and temporal dependencies. This\ntransformation process involves sequentially masking frames at the same\npositions within each frame. These frames are then resized into sub-frames and\nreorganized into the predetermined layout, forming thumbnails. TALL is\nmodel-agnostic and has remarkable simplicity, necessitating only minimal code\nmodifications. Furthermore, we introduce a graph reasoning block (GRB) and\nsemantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB\nenhances interactions between different semantic regions to capture\nsemantic-level inconsistency clues. The semantic consistency loss imposes\nconsistency constraints on semantic features to improve model generalization\nability. Extensive experiments on intra-dataset, cross-dataset,\ndiffusion-generated image detection, and deepfake generation method recognition\nshow that TALL++ achieves results surpassing or comparable to the\nstate-of-the-art methods, demonstrating the effectiveness of our approaches for\nvarious deepfake detection problems. The code is available at\nhttps://github.com/rainy-xu/TALL4Deepfake.\n", "link": "http://arxiv.org/abs/2403.10261v2", "date": "2024-03-20", "relevancy": 2.3388, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.601}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5715}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Spatiotemporal%20Inconsistency%20via%20Thumbnail%20Layout%20for%20Face%0A%20%20Deepfake%20Detection&body=Title%3A%20Learning%20Spatiotemporal%20Inconsistency%20via%20Thumbnail%20Layout%20for%20Face%0A%20%20Deepfake%20Detection%0AAuthor%3A%20Yuting%20Xu%20and%20Jian%20Liang%20and%20Lijun%20Sheng%20and%20Xiao-Yu%20Zhang%0AAbstract%3A%20%20%20The%20deepfake%20threats%20to%20society%20and%20cybersecurity%20have%20provoked%20significant%0Apublic%20apprehension%2C%20driving%20intensified%20efforts%20within%20the%20realm%20of%20deepfake%0Avideo%20detection.%20Current%20video-level%20methods%20are%20mostly%20based%20on%20%7B3D%20CNNs%7D%0Aresulting%20in%20high%20computational%20demands%2C%20although%20have%20achieved%20good%0Aperformance.%20This%20paper%20introduces%20an%20elegantly%20simple%20yet%20effective%20strategy%0Anamed%20Thumbnail%20Layout%20%28TALL%29%2C%20which%20transforms%20a%20video%20clip%20into%20a%20pre-defined%0Alayout%20to%20realize%20the%20preservation%20of%20spatial%20and%20temporal%20dependencies.%20This%0Atransformation%20process%20involves%20sequentially%20masking%20frames%20at%20the%20same%0Apositions%20within%20each%20frame.%20These%20frames%20are%20then%20resized%20into%20sub-frames%20and%0Areorganized%20into%20the%20predetermined%20layout%2C%20forming%20thumbnails.%20TALL%20is%0Amodel-agnostic%20and%20has%20remarkable%20simplicity%2C%20necessitating%20only%20minimal%20code%0Amodifications.%20Furthermore%2C%20we%20introduce%20a%20graph%20reasoning%20block%20%28GRB%29%20and%0Asemantic%20consistency%20%28SC%29%20loss%20to%20strengthen%20TALL%2C%20culminating%20in%20TALL%2B%2B.%20GRB%0Aenhances%20interactions%20between%20different%20semantic%20regions%20to%20capture%0Asemantic-level%20inconsistency%20clues.%20The%20semantic%20consistency%20loss%20imposes%0Aconsistency%20constraints%20on%20semantic%20features%20to%20improve%20model%20generalization%0Aability.%20Extensive%20experiments%20on%20intra-dataset%2C%20cross-dataset%2C%0Adiffusion-generated%20image%20detection%2C%20and%20deepfake%20generation%20method%20recognition%0Ashow%20that%20TALL%2B%2B%20achieves%20results%20surpassing%20or%20comparable%20to%20the%0Astate-of-the-art%20methods%2C%20demonstrating%20the%20effectiveness%20of%20our%20approaches%20for%0Avarious%20deepfake%20detection%20problems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/rainy-xu/TALL4Deepfake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10261v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Spatiotemporal%20Inconsistency%20via%20Thumbnail%20Layout%20for%20Face%0A%20%20Deepfake%20Detection&entry.906535625=Yuting%20Xu%20and%20Jian%20Liang%20and%20Lijun%20Sheng%20and%20Xiao-Yu%20Zhang&entry.1292438233=%20%20The%20deepfake%20threats%20to%20society%20and%20cybersecurity%20have%20provoked%20significant%0Apublic%20apprehension%2C%20driving%20intensified%20efforts%20within%20the%20realm%20of%20deepfake%0Avideo%20detection.%20Current%20video-level%20methods%20are%20mostly%20based%20on%20%7B3D%20CNNs%7D%0Aresulting%20in%20high%20computational%20demands%2C%20although%20have%20achieved%20good%0Aperformance.%20This%20paper%20introduces%20an%20elegantly%20simple%20yet%20effective%20strategy%0Anamed%20Thumbnail%20Layout%20%28TALL%29%2C%20which%20transforms%20a%20video%20clip%20into%20a%20pre-defined%0Alayout%20to%20realize%20the%20preservation%20of%20spatial%20and%20temporal%20dependencies.%20This%0Atransformation%20process%20involves%20sequentially%20masking%20frames%20at%20the%20same%0Apositions%20within%20each%20frame.%20These%20frames%20are%20then%20resized%20into%20sub-frames%20and%0Areorganized%20into%20the%20predetermined%20layout%2C%20forming%20thumbnails.%20TALL%20is%0Amodel-agnostic%20and%20has%20remarkable%20simplicity%2C%20necessitating%20only%20minimal%20code%0Amodifications.%20Furthermore%2C%20we%20introduce%20a%20graph%20reasoning%20block%20%28GRB%29%20and%0Asemantic%20consistency%20%28SC%29%20loss%20to%20strengthen%20TALL%2C%20culminating%20in%20TALL%2B%2B.%20GRB%0Aenhances%20interactions%20between%20different%20semantic%20regions%20to%20capture%0Asemantic-level%20inconsistency%20clues.%20The%20semantic%20consistency%20loss%20imposes%0Aconsistency%20constraints%20on%20semantic%20features%20to%20improve%20model%20generalization%0Aability.%20Extensive%20experiments%20on%20intra-dataset%2C%20cross-dataset%2C%0Adiffusion-generated%20image%20detection%2C%20and%20deepfake%20generation%20method%20recognition%0Ashow%20that%20TALL%2B%2B%20achieves%20results%20surpassing%20or%20comparable%20to%20the%0Astate-of-the-art%20methods%2C%20demonstrating%20the%20effectiveness%20of%20our%20approaches%20for%0Avarious%20deepfake%20detection%20problems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/rainy-xu/TALL4Deepfake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10261v2&entry.124074799=Read"},
{"title": "ReGround: Improving Textual and Spatial Grounding at No Cost", "author": "Yuseung Lee and Minhyuk Sung", "abstract": "  When an image generation process is guided by both a text prompt and spatial\ncues, such as a set of bounding boxes, do these elements work in harmony, or\ndoes one dominate the other? Our analysis of a pretrained image diffusion model\nthat integrates gated self-attention into the U-Net reveals that spatial\ngrounding often outweighs textual grounding due to the sequential flow from\ngated self-attention to cross-attention. We demonstrate that such bias can be\nsignificantly mitigated without sacrificing accuracy in either grounding by\nsimply rewiring the network architecture, changing from sequential to parallel\nfor gated self-attention and cross-attention. This surprisingly simple yet\neffective solution does not require any fine-tuning of the network but\nsignificantly reduces the trade-off between the two groundings. Our experiments\ndemonstrate significant improvements from the original GLIGEN to the rewired\nversion in the trade-off between textual grounding and spatial grounding.\n", "link": "http://arxiv.org/abs/2403.13589v1", "date": "2024-03-20", "relevancy": 2.3349, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6201}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5921}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5608}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ReGround%3A%20Improving%20Textual%20and%20Spatial%20Grounding%20at%20No%20Cost&body=Title%3A%20ReGround%3A%20Improving%20Textual%20and%20Spatial%20Grounding%20at%20No%20Cost%0AAuthor%3A%20Yuseung%20Lee%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20When%20an%20image%20generation%20process%20is%20guided%20by%20both%20a%20text%20prompt%20and%20spatial%0Acues%2C%20such%20as%20a%20set%20of%20bounding%20boxes%2C%20do%20these%20elements%20work%20in%20harmony%2C%20or%0Adoes%20one%20dominate%20the%20other%3F%20Our%20analysis%20of%20a%20pretrained%20image%20diffusion%20model%0Athat%20integrates%20gated%20self-attention%20into%20the%20U-Net%20reveals%20that%20spatial%0Agrounding%20often%20outweighs%20textual%20grounding%20due%20to%20the%20sequential%20flow%20from%0Agated%20self-attention%20to%20cross-attention.%20We%20demonstrate%20that%20such%20bias%20can%20be%0Asignificantly%20mitigated%20without%20sacrificing%20accuracy%20in%20either%20grounding%20by%0Asimply%20rewiring%20the%20network%20architecture%2C%20changing%20from%20sequential%20to%20parallel%0Afor%20gated%20self-attention%20and%20cross-attention.%20This%20surprisingly%20simple%20yet%0Aeffective%20solution%20does%20not%20require%20any%20fine-tuning%20of%20the%20network%20but%0Asignificantly%20reduces%20the%20trade-off%20between%20the%20two%20groundings.%20Our%20experiments%0Ademonstrate%20significant%20improvements%20from%20the%20original%20GLIGEN%20to%20the%20rewired%0Aversion%20in%20the%20trade-off%20between%20textual%20grounding%20and%20spatial%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReGround%3A%20Improving%20Textual%20and%20Spatial%20Grounding%20at%20No%20Cost&entry.906535625=Yuseung%20Lee%20and%20Minhyuk%20Sung&entry.1292438233=%20%20When%20an%20image%20generation%20process%20is%20guided%20by%20both%20a%20text%20prompt%20and%20spatial%0Acues%2C%20such%20as%20a%20set%20of%20bounding%20boxes%2C%20do%20these%20elements%20work%20in%20harmony%2C%20or%0Adoes%20one%20dominate%20the%20other%3F%20Our%20analysis%20of%20a%20pretrained%20image%20diffusion%20model%0Athat%20integrates%20gated%20self-attention%20into%20the%20U-Net%20reveals%20that%20spatial%0Agrounding%20often%20outweighs%20textual%20grounding%20due%20to%20the%20sequential%20flow%20from%0Agated%20self-attention%20to%20cross-attention.%20We%20demonstrate%20that%20such%20bias%20can%20be%0Asignificantly%20mitigated%20without%20sacrificing%20accuracy%20in%20either%20grounding%20by%0Asimply%20rewiring%20the%20network%20architecture%2C%20changing%20from%20sequential%20to%20parallel%0Afor%20gated%20self-attention%20and%20cross-attention.%20This%20surprisingly%20simple%20yet%0Aeffective%20solution%20does%20not%20require%20any%20fine-tuning%20of%20the%20network%20but%0Asignificantly%20reduces%20the%20trade-off%20between%20the%20two%20groundings.%20Our%20experiments%0Ademonstrate%20significant%20improvements%20from%20the%20original%20GLIGEN%20to%20the%20rewired%0Aversion%20in%20the%20trade-off%20between%20textual%20grounding%20and%20spatial%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13589v1&entry.124074799=Read"},
{"title": "On Pretraining Data Diversity for Self-Supervised Learning", "author": "Hasan Abed Al Kader Hammoud and Tuhin Das and Fabio Pizzati and Philip Torr and Adel Bibi and Bernard Ghanem", "abstract": "  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n", "link": "http://arxiv.org/abs/2403.13808v1", "date": "2024-03-20", "relevancy": 2.3261, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4688}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4628}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Pretraining%20Data%20Diversity%20for%20Self-Supervised%20Learning&body=Title%3A%20On%20Pretraining%20Data%20Diversity%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Tuhin%20Das%20and%20Fabio%20Pizzati%20and%20Philip%20Torr%20and%20Adel%20Bibi%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20We%20explore%20the%20impact%20of%20training%20with%20more%20diverse%20datasets%2C%20characterized%0Aby%20the%20number%20of%20unique%20samples%2C%20on%20the%20performance%20of%20self-supervised%20learning%0A%28SSL%29%20under%20a%20fixed%20computational%20budget.%20Our%20findings%20consistently%20demonstrate%0Athat%20increasing%20pretraining%20data%20diversity%20enhances%20SSL%20performance%2C%20albeit%0Aonly%20when%20the%20distribution%20distance%20to%20the%20downstream%20data%20is%20minimal.%20Notably%2C%0Aeven%20with%20an%20exceptionally%20large%20pretraining%20data%20diversity%20achieved%20through%0Amethods%20like%20web%20crawling%20or%20diffusion-generated%20data%2C%20among%20other%20ways%2C%20the%0Adistribution%20shift%20remains%20a%20challenge.%20Our%20experiments%20are%20comprehensive%20with%0Aseven%20SSL%20methods%20using%20large-scale%20datasets%20such%20as%20ImageNet%20and%20YFCC100M%0Aamounting%20to%20over%20200%20GPU%20days.%20Code%20and%20trained%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/hammoudhasan/DiversitySSL%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13808v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Pretraining%20Data%20Diversity%20for%20Self-Supervised%20Learning&entry.906535625=Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Tuhin%20Das%20and%20Fabio%20Pizzati%20and%20Philip%20Torr%20and%20Adel%20Bibi%20and%20Bernard%20Ghanem&entry.1292438233=%20%20We%20explore%20the%20impact%20of%20training%20with%20more%20diverse%20datasets%2C%20characterized%0Aby%20the%20number%20of%20unique%20samples%2C%20on%20the%20performance%20of%20self-supervised%20learning%0A%28SSL%29%20under%20a%20fixed%20computational%20budget.%20Our%20findings%20consistently%20demonstrate%0Athat%20increasing%20pretraining%20data%20diversity%20enhances%20SSL%20performance%2C%20albeit%0Aonly%20when%20the%20distribution%20distance%20to%20the%20downstream%20data%20is%20minimal.%20Notably%2C%0Aeven%20with%20an%20exceptionally%20large%20pretraining%20data%20diversity%20achieved%20through%0Amethods%20like%20web%20crawling%20or%20diffusion-generated%20data%2C%20among%20other%20ways%2C%20the%0Adistribution%20shift%20remains%20a%20challenge.%20Our%20experiments%20are%20comprehensive%20with%0Aseven%20SSL%20methods%20using%20large-scale%20datasets%20such%20as%20ImageNet%20and%20YFCC100M%0Aamounting%20to%20over%20200%20GPU%20days.%20Code%20and%20trained%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/hammoudhasan/DiversitySSL%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13808v1&entry.124074799=Read"},
{"title": "AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and\n  GPT-2 in Wild Audiovisual Contexts", "author": "Jun Yu and Zerui Zhang and Zhihong Wei and Gongpeng Zhao and Zhongpeng Cai and Yongqi Wang and Guochen Xie and Jichao Zhu and Wangyuan Zhu", "abstract": "  Leveraging the synergy of both audio data and visual data is essential for\nunderstanding human emotions and behaviors, especially in in-the-wild setting.\nTraditional methods for integrating such multimodal information often stumble,\nleading to less-than-ideal outcomes in the task of facial action unit\ndetection. To overcome these shortcomings, we propose a novel approach\nutilizing audio-visual multimodal data. This method enhances audio feature\nextraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel\nspectrogram features alongside a pre-trained VGGish network. Moreover, this\npaper adaptively captures fusion features across modalities by modeling the\ntemporal relationships, and ultilizes a pre-trained GPT-2 model for\nsophisticated context-aware fusion of multimodal information. Our method\nnotably improves the accuracy of AU detection by understanding the temporal and\ncontextual nuances of the data, showcasing significant advancements in the\ncomprehension of intricate scenarios. These findings underscore the potential\nof integrating temporal dynamics and contextual interpretation, paving the way\nfor future research endeavors.\n", "link": "http://arxiv.org/abs/2403.13678v1", "date": "2024-03-20", "relevancy": 2.303, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AUD-TGN%3A%20Advancing%20Action%20Unit%20Detection%20with%20Temporal%20Convolution%20and%0A%20%20GPT-2%20in%20Wild%20Audiovisual%20Contexts&body=Title%3A%20AUD-TGN%3A%20Advancing%20Action%20Unit%20Detection%20with%20Temporal%20Convolution%20and%0A%20%20GPT-2%20in%20Wild%20Audiovisual%20Contexts%0AAuthor%3A%20Jun%20Yu%20and%20Zerui%20Zhang%20and%20Zhihong%20Wei%20and%20Gongpeng%20Zhao%20and%20Zhongpeng%20Cai%20and%20Yongqi%20Wang%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu%0AAbstract%3A%20%20%20Leveraging%20the%20synergy%20of%20both%20audio%20data%20and%20visual%20data%20is%20essential%20for%0Aunderstanding%20human%20emotions%20and%20behaviors%2C%20especially%20in%20in-the-wild%20setting.%0ATraditional%20methods%20for%20integrating%20such%20multimodal%20information%20often%20stumble%2C%0Aleading%20to%20less-than-ideal%20outcomes%20in%20the%20task%20of%20facial%20action%20unit%0Adetection.%20To%20overcome%20these%20shortcomings%2C%20we%20propose%20a%20novel%20approach%0Autilizing%20audio-visual%20multimodal%20data.%20This%20method%20enhances%20audio%20feature%0Aextraction%20by%20leveraging%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%20Log-Mel%0Aspectrogram%20features%20alongside%20a%20pre-trained%20VGGish%20network.%20Moreover%2C%20this%0Apaper%20adaptively%20captures%20fusion%20features%20across%20modalities%20by%20modeling%20the%0Atemporal%20relationships%2C%20and%20ultilizes%20a%20pre-trained%20GPT-2%20model%20for%0Asophisticated%20context-aware%20fusion%20of%20multimodal%20information.%20Our%20method%0Anotably%20improves%20the%20accuracy%20of%20AU%20detection%20by%20understanding%20the%20temporal%20and%0Acontextual%20nuances%20of%20the%20data%2C%20showcasing%20significant%20advancements%20in%20the%0Acomprehension%20of%20intricate%20scenarios.%20These%20findings%20underscore%20the%20potential%0Aof%20integrating%20temporal%20dynamics%20and%20contextual%20interpretation%2C%20paving%20the%20way%0Afor%20future%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13678v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUD-TGN%3A%20Advancing%20Action%20Unit%20Detection%20with%20Temporal%20Convolution%20and%0A%20%20GPT-2%20in%20Wild%20Audiovisual%20Contexts&entry.906535625=Jun%20Yu%20and%20Zerui%20Zhang%20and%20Zhihong%20Wei%20and%20Gongpeng%20Zhao%20and%20Zhongpeng%20Cai%20and%20Yongqi%20Wang%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu&entry.1292438233=%20%20Leveraging%20the%20synergy%20of%20both%20audio%20data%20and%20visual%20data%20is%20essential%20for%0Aunderstanding%20human%20emotions%20and%20behaviors%2C%20especially%20in%20in-the-wild%20setting.%0ATraditional%20methods%20for%20integrating%20such%20multimodal%20information%20often%20stumble%2C%0Aleading%20to%20less-than-ideal%20outcomes%20in%20the%20task%20of%20facial%20action%20unit%0Adetection.%20To%20overcome%20these%20shortcomings%2C%20we%20propose%20a%20novel%20approach%0Autilizing%20audio-visual%20multimodal%20data.%20This%20method%20enhances%20audio%20feature%0Aextraction%20by%20leveraging%20Mel%20Frequency%20Cepstral%20Coefficients%20%28MFCC%29%20and%20Log-Mel%0Aspectrogram%20features%20alongside%20a%20pre-trained%20VGGish%20network.%20Moreover%2C%20this%0Apaper%20adaptively%20captures%20fusion%20features%20across%20modalities%20by%20modeling%20the%0Atemporal%20relationships%2C%20and%20ultilizes%20a%20pre-trained%20GPT-2%20model%20for%0Asophisticated%20context-aware%20fusion%20of%20multimodal%20information.%20Our%20method%0Anotably%20improves%20the%20accuracy%20of%20AU%20detection%20by%20understanding%20the%20temporal%20and%0Acontextual%20nuances%20of%20the%20data%2C%20showcasing%20significant%20advancements%20in%20the%0Acomprehension%20of%20intricate%20scenarios.%20These%20findings%20underscore%20the%20potential%0Aof%20integrating%20temporal%20dynamics%20and%20contextual%20interpretation%2C%20paving%20the%20way%0Afor%20future%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13678v1&entry.124074799=Read"},
{"title": "T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh\n  Generation from a Single Image", "author": "Shijie Zhang and Boyan Jiang and Keke He and Junwei Zhu and Ying Tai and Chengjie Wang and Yinda Zhang and Yanwei Fu", "abstract": "  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a\nsingle color image through coarse-to-fine mesh deformation. Although P2M is\ncapable of generating plausible global shapes, its Graph Convolution Network\n(GCN) often produces overly smooth results, causing the loss of fine-grained\ngeometry details. Moreover, P2M generates non-credible features for occluded\nregions and struggles with the domain gap from synthetic data to real-world\nimages, which is a common challenge for single-view 3D reconstruction methods.\nTo address these challenges, we propose a novel Transformer-boosted\narchitecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of\nP2M. Specifically, we use a global Transformer to control the holistic shape\nand a local Transformer to progressively refine the local geometry details with\ngraph-based point upsampling. To enhance real-world reconstruction, we present\nthe simple yet effective Linear Scale Search (LSS), which serves as prompt\ntuning during the input preprocessing. Our experiments on ShapeNet demonstrate\nstate-of-the-art performance, while results on real-world data show the\ngeneralization capability.\n", "link": "http://arxiv.org/abs/2403.13663v1", "date": "2024-03-20", "relevancy": 2.3029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6103}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-Pixel2Mesh%3A%20Combining%20Global%20and%20Local%20Transformer%20for%203D%20Mesh%0A%20%20Generation%20from%20a%20Single%20Image&body=Title%3A%20T-Pixel2Mesh%3A%20Combining%20Global%20and%20Local%20Transformer%20for%203D%20Mesh%0A%20%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Shijie%20Zhang%20and%20Boyan%20Jiang%20and%20Keke%20He%20and%20Junwei%20Zhu%20and%20Ying%20Tai%20and%20Chengjie%20Wang%20and%20Yinda%20Zhang%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Pixel2Mesh%20%28P2M%29%20is%20a%20classical%20approach%20for%20reconstructing%203D%20shapes%20from%20a%0Asingle%20color%20image%20through%20coarse-to-fine%20mesh%20deformation.%20Although%20P2M%20is%0Acapable%20of%20generating%20plausible%20global%20shapes%2C%20its%20Graph%20Convolution%20Network%0A%28GCN%29%20often%20produces%20overly%20smooth%20results%2C%20causing%20the%20loss%20of%20fine-grained%0Ageometry%20details.%20Moreover%2C%20P2M%20generates%20non-credible%20features%20for%20occluded%0Aregions%20and%20struggles%20with%20the%20domain%20gap%20from%20synthetic%20data%20to%20real-world%0Aimages%2C%20which%20is%20a%20common%20challenge%20for%20single-view%203D%20reconstruction%20methods.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Transformer-boosted%0Aarchitecture%2C%20named%20T-Pixel2Mesh%2C%20inspired%20by%20the%20coarse-to-fine%20approach%20of%0AP2M.%20Specifically%2C%20we%20use%20a%20global%20Transformer%20to%20control%20the%20holistic%20shape%0Aand%20a%20local%20Transformer%20to%20progressively%20refine%20the%20local%20geometry%20details%20with%0Agraph-based%20point%20upsampling.%20To%20enhance%20real-world%20reconstruction%2C%20we%20present%0Athe%20simple%20yet%20effective%20Linear%20Scale%20Search%20%28LSS%29%2C%20which%20serves%20as%20prompt%0Atuning%20during%20the%20input%20preprocessing.%20Our%20experiments%20on%20ShapeNet%20demonstrate%0Astate-of-the-art%20performance%2C%20while%20results%20on%20real-world%20data%20show%20the%0Ageneralization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13663v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Pixel2Mesh%3A%20Combining%20Global%20and%20Local%20Transformer%20for%203D%20Mesh%0A%20%20Generation%20from%20a%20Single%20Image&entry.906535625=Shijie%20Zhang%20and%20Boyan%20Jiang%20and%20Keke%20He%20and%20Junwei%20Zhu%20and%20Ying%20Tai%20and%20Chengjie%20Wang%20and%20Yinda%20Zhang%20and%20Yanwei%20Fu&entry.1292438233=%20%20Pixel2Mesh%20%28P2M%29%20is%20a%20classical%20approach%20for%20reconstructing%203D%20shapes%20from%20a%0Asingle%20color%20image%20through%20coarse-to-fine%20mesh%20deformation.%20Although%20P2M%20is%0Acapable%20of%20generating%20plausible%20global%20shapes%2C%20its%20Graph%20Convolution%20Network%0A%28GCN%29%20often%20produces%20overly%20smooth%20results%2C%20causing%20the%20loss%20of%20fine-grained%0Ageometry%20details.%20Moreover%2C%20P2M%20generates%20non-credible%20features%20for%20occluded%0Aregions%20and%20struggles%20with%20the%20domain%20gap%20from%20synthetic%20data%20to%20real-world%0Aimages%2C%20which%20is%20a%20common%20challenge%20for%20single-view%203D%20reconstruction%20methods.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Transformer-boosted%0Aarchitecture%2C%20named%20T-Pixel2Mesh%2C%20inspired%20by%20the%20coarse-to-fine%20approach%20of%0AP2M.%20Specifically%2C%20we%20use%20a%20global%20Transformer%20to%20control%20the%20holistic%20shape%0Aand%20a%20local%20Transformer%20to%20progressively%20refine%20the%20local%20geometry%20details%20with%0Agraph-based%20point%20upsampling.%20To%20enhance%20real-world%20reconstruction%2C%20we%20present%0Athe%20simple%20yet%20effective%20Linear%20Scale%20Search%20%28LSS%29%2C%20which%20serves%20as%20prompt%0Atuning%20during%20the%20input%20preprocessing.%20Our%20experiments%20on%20ShapeNet%20demonstrate%0Astate-of-the-art%20performance%2C%20while%20results%20on%20real-world%20data%20show%20the%0Ageneralization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13663v1&entry.124074799=Read"},
{"title": "HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text\n  Recognition", "author": "Yuyi Zhang and Yuanzhi Zhu and Dezhi Peng and Peirong Zhang and Zhenhua Yang and Zhibo Yang and Cong Yao and Lianwen Jin", "abstract": "  Text recognition, especially for complex scripts like Chinese, faces unique\nchallenges due to its intricate character structures and vast vocabulary.\nTraditional one-hot encoding methods struggle with the representation of\nhierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and\non-device deployment due to their computational intensity. To address these\nchallenges, we propose HierCode, a novel and lightweight codebook that exploits\nthe innate hierarchical nature of Chinese characters. HierCode employs a\nmulti-hot encoding strategy, leveraging hierarchical binary tree encoding and\nprototype learning to create distinctive, informative representations for each\ncharacter. This approach not only facilitates zero-shot recognition of OOV\ncharacters by utilizing shared radicals and structures but also excels in\nline-level recognition tasks by computing similarity with visual features, a\nnotable advantage over existing methods. Extensive experiments across diverse\nbenchmarks, including handwritten, scene, document, web, and ancient text, have\nshowcased HierCode's superiority for both conventional and zero-shot Chinese\ncharacter or text recognition, exhibiting state-of-the-art performance with\nsignificantly fewer parameters and fast inference speed.\n", "link": "http://arxiv.org/abs/2403.13761v1", "date": "2024-03-20", "relevancy": 2.3012, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4715}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4663}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.443}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HierCode%3A%20A%20Lightweight%20Hierarchical%20Codebook%20for%20Zero-shot%20Chinese%20Text%0A%20%20Recognition&body=Title%3A%20HierCode%3A%20A%20Lightweight%20Hierarchical%20Codebook%20for%20Zero-shot%20Chinese%20Text%0A%20%20Recognition%0AAuthor%3A%20Yuyi%20Zhang%20and%20Yuanzhi%20Zhu%20and%20Dezhi%20Peng%20and%20Peirong%20Zhang%20and%20Zhenhua%20Yang%20and%20Zhibo%20Yang%20and%20Cong%20Yao%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Text%20recognition%2C%20especially%20for%20complex%20scripts%20like%20Chinese%2C%20faces%20unique%0Achallenges%20due%20to%20its%20intricate%20character%20structures%20and%20vast%20vocabulary.%0ATraditional%20one-hot%20encoding%20methods%20struggle%20with%20the%20representation%20of%0Ahierarchical%20radicals%2C%20recognition%20of%20Out-Of-Vocabulary%20%28OOV%29%20characters%2C%20and%0Aon-device%20deployment%20due%20to%20their%20computational%20intensity.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HierCode%2C%20a%20novel%20and%20lightweight%20codebook%20that%20exploits%0Athe%20innate%20hierarchical%20nature%20of%20Chinese%20characters.%20HierCode%20employs%20a%0Amulti-hot%20encoding%20strategy%2C%20leveraging%20hierarchical%20binary%20tree%20encoding%20and%0Aprototype%20learning%20to%20create%20distinctive%2C%20informative%20representations%20for%20each%0Acharacter.%20This%20approach%20not%20only%20facilitates%20zero-shot%20recognition%20of%20OOV%0Acharacters%20by%20utilizing%20shared%20radicals%20and%20structures%20but%20also%20excels%20in%0Aline-level%20recognition%20tasks%20by%20computing%20similarity%20with%20visual%20features%2C%20a%0Anotable%20advantage%20over%20existing%20methods.%20Extensive%20experiments%20across%20diverse%0Abenchmarks%2C%20including%20handwritten%2C%20scene%2C%20document%2C%20web%2C%20and%20ancient%20text%2C%20have%0Ashowcased%20HierCode%27s%20superiority%20for%20both%20conventional%20and%20zero-shot%20Chinese%0Acharacter%20or%20text%20recognition%2C%20exhibiting%20state-of-the-art%20performance%20with%0Asignificantly%20fewer%20parameters%20and%20fast%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13761v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierCode%3A%20A%20Lightweight%20Hierarchical%20Codebook%20for%20Zero-shot%20Chinese%20Text%0A%20%20Recognition&entry.906535625=Yuyi%20Zhang%20and%20Yuanzhi%20Zhu%20and%20Dezhi%20Peng%20and%20Peirong%20Zhang%20and%20Zhenhua%20Yang%20and%20Zhibo%20Yang%20and%20Cong%20Yao%20and%20Lianwen%20Jin&entry.1292438233=%20%20Text%20recognition%2C%20especially%20for%20complex%20scripts%20like%20Chinese%2C%20faces%20unique%0Achallenges%20due%20to%20its%20intricate%20character%20structures%20and%20vast%20vocabulary.%0ATraditional%20one-hot%20encoding%20methods%20struggle%20with%20the%20representation%20of%0Ahierarchical%20radicals%2C%20recognition%20of%20Out-Of-Vocabulary%20%28OOV%29%20characters%2C%20and%0Aon-device%20deployment%20due%20to%20their%20computational%20intensity.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HierCode%2C%20a%20novel%20and%20lightweight%20codebook%20that%20exploits%0Athe%20innate%20hierarchical%20nature%20of%20Chinese%20characters.%20HierCode%20employs%20a%0Amulti-hot%20encoding%20strategy%2C%20leveraging%20hierarchical%20binary%20tree%20encoding%20and%0Aprototype%20learning%20to%20create%20distinctive%2C%20informative%20representations%20for%20each%0Acharacter.%20This%20approach%20not%20only%20facilitates%20zero-shot%20recognition%20of%20OOV%0Acharacters%20by%20utilizing%20shared%20radicals%20and%20structures%20but%20also%20excels%20in%0Aline-level%20recognition%20tasks%20by%20computing%20similarity%20with%20visual%20features%2C%20a%0Anotable%20advantage%20over%20existing%20methods.%20Extensive%20experiments%20across%20diverse%0Abenchmarks%2C%20including%20handwritten%2C%20scene%2C%20document%2C%20web%2C%20and%20ancient%20text%2C%20have%0Ashowcased%20HierCode%27s%20superiority%20for%20both%20conventional%20and%20zero-shot%20Chinese%0Acharacter%20or%20text%20recognition%2C%20exhibiting%20state-of-the-art%20performance%20with%0Asignificantly%20fewer%20parameters%20and%20fast%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13761v1&entry.124074799=Read"},
{"title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition", "author": "Ziyu Liu and Zeyi Sun and Yuhang Zang and Wei Li and Pan Zhang and Xiaoyi Dong and Yuanjun Xiong and Dahua Lin and Jiaqi Wang", "abstract": "  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n", "link": "http://arxiv.org/abs/2403.13805v1", "date": "2024-03-20", "relevancy": 2.298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6078}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RAR%3A%20Retrieving%20And%20Ranking%20Augmented%20MLLMs%20for%20Visual%20Recognition&body=Title%3A%20RAR%3A%20Retrieving%20And%20Ranking%20Augmented%20MLLMs%20for%20Visual%20Recognition%0AAuthor%3A%20Ziyu%20Liu%20and%20Zeyi%20Sun%20and%20Yuhang%20Zang%20and%20Wei%20Li%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuanjun%20Xiong%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20uses%20contrastive%20learning%20from%0Anoise%20image-text%20pairs%20to%20excel%20at%20recognizing%20a%20wide%20array%20of%20candidates%2C%20yet%0Aits%20focus%20on%20broad%20associations%20hinders%20the%20precision%20in%20distinguishing%20subtle%0Adifferences%20among%20fine-grained%20items.%20Conversely%2C%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20excel%20at%20classifying%20fine-grained%20categories%2C%20thanks%20to%20their%0Asubstantial%20knowledge%20from%20pre-training%20on%20web-level%20corpora.%20However%2C%20the%0Aperformance%20of%20MLLMs%20declines%20with%20an%20increase%20in%20category%20numbers%2C%20primarily%0Adue%20to%20growing%20complexity%20and%20constraints%20of%20limited%20context%20window%20size.%20To%0Asynergize%20the%20strengths%20of%20both%20approaches%20and%20enhance%20the%20few-shot/zero-shot%0Arecognition%20abilities%20for%20datasets%20characterized%20by%20extensive%20and%20fine-grained%0Avocabularies%2C%20this%20paper%20introduces%20RAR%2C%20a%20Retrieving%20And%20Ranking%20augmented%0Amethod%20for%20MLLMs.%20We%20initially%20establish%20a%20multi-modal%20retriever%20based%20on%20CLIP%0Ato%20create%20and%20store%20explicit%20memory%20for%20different%20categories%20beyond%20the%0Aimmediate%20context%20window.%20During%20inference%2C%20RAR%20retrieves%20the%20top-k%20similar%0Aresults%20from%20the%20memory%20and%20uses%20MLLMs%20to%20rank%20and%20make%20the%20final%20predictions.%0AOur%20proposed%20approach%20not%20only%20addresses%20the%20inherent%20limitations%20in%0Afine-grained%20recognition%20but%20also%20preserves%20the%20model%27s%20comprehensive%20knowledge%0Abase%2C%20significantly%20boosting%20accuracy%20across%20a%20range%20of%20vision-language%0Arecognition%20tasks.%20Notably%2C%20our%20approach%20demonstrates%20a%20significant%20improvement%0Ain%20performance%20on%205%20fine-grained%20visual%20recognition%20benchmarks%2C%2011%20few-shot%0Aimage%20recognition%20datasets%2C%20and%20the%202%20object%20detection%20datasets%20under%20the%0Azero-shot%20recognition%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13805v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAR%3A%20Retrieving%20And%20Ranking%20Augmented%20MLLMs%20for%20Visual%20Recognition&entry.906535625=Ziyu%20Liu%20and%20Zeyi%20Sun%20and%20Yuhang%20Zang%20and%20Wei%20Li%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuanjun%20Xiong%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20CLIP%20%28Contrastive%20Language-Image%20Pre-training%29%20uses%20contrastive%20learning%20from%0Anoise%20image-text%20pairs%20to%20excel%20at%20recognizing%20a%20wide%20array%20of%20candidates%2C%20yet%0Aits%20focus%20on%20broad%20associations%20hinders%20the%20precision%20in%20distinguishing%20subtle%0Adifferences%20among%20fine-grained%20items.%20Conversely%2C%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20excel%20at%20classifying%20fine-grained%20categories%2C%20thanks%20to%20their%0Asubstantial%20knowledge%20from%20pre-training%20on%20web-level%20corpora.%20However%2C%20the%0Aperformance%20of%20MLLMs%20declines%20with%20an%20increase%20in%20category%20numbers%2C%20primarily%0Adue%20to%20growing%20complexity%20and%20constraints%20of%20limited%20context%20window%20size.%20To%0Asynergize%20the%20strengths%20of%20both%20approaches%20and%20enhance%20the%20few-shot/zero-shot%0Arecognition%20abilities%20for%20datasets%20characterized%20by%20extensive%20and%20fine-grained%0Avocabularies%2C%20this%20paper%20introduces%20RAR%2C%20a%20Retrieving%20And%20Ranking%20augmented%0Amethod%20for%20MLLMs.%20We%20initially%20establish%20a%20multi-modal%20retriever%20based%20on%20CLIP%0Ato%20create%20and%20store%20explicit%20memory%20for%20different%20categories%20beyond%20the%0Aimmediate%20context%20window.%20During%20inference%2C%20RAR%20retrieves%20the%20top-k%20similar%0Aresults%20from%20the%20memory%20and%20uses%20MLLMs%20to%20rank%20and%20make%20the%20final%20predictions.%0AOur%20proposed%20approach%20not%20only%20addresses%20the%20inherent%20limitations%20in%0Afine-grained%20recognition%20but%20also%20preserves%20the%20model%27s%20comprehensive%20knowledge%0Abase%2C%20significantly%20boosting%20accuracy%20across%20a%20range%20of%20vision-language%0Arecognition%20tasks.%20Notably%2C%20our%20approach%20demonstrates%20a%20significant%20improvement%0Ain%20performance%20on%205%20fine-grained%20visual%20recognition%20benchmarks%2C%2011%20few-shot%0Aimage%20recognition%20datasets%2C%20and%20the%202%20object%20detection%20datasets%20under%20the%0Azero-shot%20recognition%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13805v1&entry.124074799=Read"},
{"title": "Enhanced Face Authentication With Separate Loss Functions", "author": "Anh-Kiet Duong and Hoang-Lan Nguyen and Toan-Thinh Truong", "abstract": "  The overall objective of the main project is to propose and develop a system\nof facial authentication in unlocking phones or applications in phones using\nfacial recognition. The system will include four separate architectures: face\ndetection, face recognition, face spoofing, and classification of closed eyes.\nIn which, we consider the problem of face recognition to be the most important,\ndetermining the true identity of the person standing in front of the screen\nwith absolute accuracy is what facial recognition systems need to achieve.\nAlong with the development of the face recognition problem, the problem of the\nanti-fake face is also gradually becoming popular and equally important. Our\ngoal is to propose and develop two loss functions: LMCot and Double Loss. Then\napply them to the face authentication process.\n", "link": "http://arxiv.org/abs/2302.11427v2", "date": "2024-03-20", "relevancy": 2.2797, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4685}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.453}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4464}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Face%20Authentication%20With%20Separate%20Loss%20Functions&body=Title%3A%20Enhanced%20Face%20Authentication%20With%20Separate%20Loss%20Functions%0AAuthor%3A%20Anh-Kiet%20Duong%20and%20Hoang-Lan%20Nguyen%20and%20Toan-Thinh%20Truong%0AAbstract%3A%20%20%20The%20overall%20objective%20of%20the%20main%20project%20is%20to%20propose%20and%20develop%20a%20system%0Aof%20facial%20authentication%20in%20unlocking%20phones%20or%20applications%20in%20phones%20using%0Afacial%20recognition.%20The%20system%20will%20include%20four%20separate%20architectures%3A%20face%0Adetection%2C%20face%20recognition%2C%20face%20spoofing%2C%20and%20classification%20of%20closed%20eyes.%0AIn%20which%2C%20we%20consider%20the%20problem%20of%20face%20recognition%20to%20be%20the%20most%20important%2C%0Adetermining%20the%20true%20identity%20of%20the%20person%20standing%20in%20front%20of%20the%20screen%0Awith%20absolute%20accuracy%20is%20what%20facial%20recognition%20systems%20need%20to%20achieve.%0AAlong%20with%20the%20development%20of%20the%20face%20recognition%20problem%2C%20the%20problem%20of%20the%0Aanti-fake%20face%20is%20also%20gradually%20becoming%20popular%20and%20equally%20important.%20Our%0Agoal%20is%20to%20propose%20and%20develop%20two%20loss%20functions%3A%20LMCot%20and%20Double%20Loss.%20Then%0Aapply%20them%20to%20the%20face%20authentication%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.11427v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Face%20Authentication%20With%20Separate%20Loss%20Functions&entry.906535625=Anh-Kiet%20Duong%20and%20Hoang-Lan%20Nguyen%20and%20Toan-Thinh%20Truong&entry.1292438233=%20%20The%20overall%20objective%20of%20the%20main%20project%20is%20to%20propose%20and%20develop%20a%20system%0Aof%20facial%20authentication%20in%20unlocking%20phones%20or%20applications%20in%20phones%20using%0Afacial%20recognition.%20The%20system%20will%20include%20four%20separate%20architectures%3A%20face%0Adetection%2C%20face%20recognition%2C%20face%20spoofing%2C%20and%20classification%20of%20closed%20eyes.%0AIn%20which%2C%20we%20consider%20the%20problem%20of%20face%20recognition%20to%20be%20the%20most%20important%2C%0Adetermining%20the%20true%20identity%20of%20the%20person%20standing%20in%20front%20of%20the%20screen%0Awith%20absolute%20accuracy%20is%20what%20facial%20recognition%20systems%20need%20to%20achieve.%0AAlong%20with%20the%20development%20of%20the%20face%20recognition%20problem%2C%20the%20problem%20of%20the%0Aanti-fake%20face%20is%20also%20gradually%20becoming%20popular%20and%20equally%20important.%20Our%0Agoal%20is%20to%20propose%20and%20develop%20two%20loss%20functions%3A%20LMCot%20and%20Double%20Loss.%20Then%0Aapply%20them%20to%20the%20face%20authentication%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11427v2&entry.124074799=Read"},
{"title": "NetInfoF Framework: Measuring and Exploiting Network Usable Information", "author": "Meng-Chieh Lee and Haiyang Yu and Jian Zhang and Vassilis N. Ioannidis and Xiang Song and Soji Adeshina and Da Zheng and Christos Faloutsos", "abstract": "  Given a node-attributed graph, and a graph task (link prediction or node\nclassification), can we tell if a graph neural network (GNN) will perform well?\nMore specifically, do the graph structure and the node features carry enough\nusable information for the task? Our goals are (1) to develop a fast tool to\nmeasure how much information is in the graph structure and in the node\nfeatures, and (2) to exploit the information to solve the task, if there is\nenough. We propose NetInfoF, a framework including NetInfoF_Probe and\nNetInfoF_Act, for the measurement and the exploitation of network usable\ninformation (NUI), respectively. Given a graph data, NetInfoF_Probe measures\nNUI without any model training, and NetInfoF_Act solves link prediction and\nnode classification, while two modules share the same backbone. In summary,\nNetInfoF has following notable advantages: (a) General, handling both link\nprediction and node classification; (b) Principled, with theoretical guarantee\nand closed-form solution; (c) Effective, thanks to the proposed adjustment to\nnode similarity; (d) Scalable, scaling linearly with the input size. In our\ncarefully designed synthetic datasets, NetInfoF correctly identifies the ground\ntruth of NUI and is the only method being robust to all graph scenarios.\nApplied on real-world datasets, NetInfoF wins in 11 out of 12 times on link\nprediction compared to general GNN baselines.\n", "link": "http://arxiv.org/abs/2402.07999v3", "date": "2024-03-20", "relevancy": 2.27, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4439}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4397}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NetInfoF%20Framework%3A%20Measuring%20and%20Exploiting%20Network%20Usable%20Information&body=Title%3A%20NetInfoF%20Framework%3A%20Measuring%20and%20Exploiting%20Network%20Usable%20Information%0AAuthor%3A%20Meng-Chieh%20Lee%20and%20Haiyang%20Yu%20and%20Jian%20Zhang%20and%20Vassilis%20N.%20Ioannidis%20and%20Xiang%20Song%20and%20Soji%20Adeshina%20and%20Da%20Zheng%20and%20Christos%20Faloutsos%0AAbstract%3A%20%20%20Given%20a%20node-attributed%20graph%2C%20and%20a%20graph%20task%20%28link%20prediction%20or%20node%0Aclassification%29%2C%20can%20we%20tell%20if%20a%20graph%20neural%20network%20%28GNN%29%20will%20perform%20well%3F%0AMore%20specifically%2C%20do%20the%20graph%20structure%20and%20the%20node%20features%20carry%20enough%0Ausable%20information%20for%20the%20task%3F%20Our%20goals%20are%20%281%29%20to%20develop%20a%20fast%20tool%20to%0Ameasure%20how%20much%20information%20is%20in%20the%20graph%20structure%20and%20in%20the%20node%0Afeatures%2C%20and%20%282%29%20to%20exploit%20the%20information%20to%20solve%20the%20task%2C%20if%20there%20is%0Aenough.%20We%20propose%20NetInfoF%2C%20a%20framework%20including%20NetInfoF_Probe%20and%0ANetInfoF_Act%2C%20for%20the%20measurement%20and%20the%20exploitation%20of%20network%20usable%0Ainformation%20%28NUI%29%2C%20respectively.%20Given%20a%20graph%20data%2C%20NetInfoF_Probe%20measures%0ANUI%20without%20any%20model%20training%2C%20and%20NetInfoF_Act%20solves%20link%20prediction%20and%0Anode%20classification%2C%20while%20two%20modules%20share%20the%20same%20backbone.%20In%20summary%2C%0ANetInfoF%20has%20following%20notable%20advantages%3A%20%28a%29%20General%2C%20handling%20both%20link%0Aprediction%20and%20node%20classification%3B%20%28b%29%20Principled%2C%20with%20theoretical%20guarantee%0Aand%20closed-form%20solution%3B%20%28c%29%20Effective%2C%20thanks%20to%20the%20proposed%20adjustment%20to%0Anode%20similarity%3B%20%28d%29%20Scalable%2C%20scaling%20linearly%20with%20the%20input%20size.%20In%20our%0Acarefully%20designed%20synthetic%20datasets%2C%20NetInfoF%20correctly%20identifies%20the%20ground%0Atruth%20of%20NUI%20and%20is%20the%20only%20method%20being%20robust%20to%20all%20graph%20scenarios.%0AApplied%20on%20real-world%20datasets%2C%20NetInfoF%20wins%20in%2011%20out%20of%2012%20times%20on%20link%0Aprediction%20compared%20to%20general%20GNN%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07999v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NetInfoF%20Framework%3A%20Measuring%20and%20Exploiting%20Network%20Usable%20Information&entry.906535625=Meng-Chieh%20Lee%20and%20Haiyang%20Yu%20and%20Jian%20Zhang%20and%20Vassilis%20N.%20Ioannidis%20and%20Xiang%20Song%20and%20Soji%20Adeshina%20and%20Da%20Zheng%20and%20Christos%20Faloutsos&entry.1292438233=%20%20Given%20a%20node-attributed%20graph%2C%20and%20a%20graph%20task%20%28link%20prediction%20or%20node%0Aclassification%29%2C%20can%20we%20tell%20if%20a%20graph%20neural%20network%20%28GNN%29%20will%20perform%20well%3F%0AMore%20specifically%2C%20do%20the%20graph%20structure%20and%20the%20node%20features%20carry%20enough%0Ausable%20information%20for%20the%20task%3F%20Our%20goals%20are%20%281%29%20to%20develop%20a%20fast%20tool%20to%0Ameasure%20how%20much%20information%20is%20in%20the%20graph%20structure%20and%20in%20the%20node%0Afeatures%2C%20and%20%282%29%20to%20exploit%20the%20information%20to%20solve%20the%20task%2C%20if%20there%20is%0Aenough.%20We%20propose%20NetInfoF%2C%20a%20framework%20including%20NetInfoF_Probe%20and%0ANetInfoF_Act%2C%20for%20the%20measurement%20and%20the%20exploitation%20of%20network%20usable%0Ainformation%20%28NUI%29%2C%20respectively.%20Given%20a%20graph%20data%2C%20NetInfoF_Probe%20measures%0ANUI%20without%20any%20model%20training%2C%20and%20NetInfoF_Act%20solves%20link%20prediction%20and%0Anode%20classification%2C%20while%20two%20modules%20share%20the%20same%20backbone.%20In%20summary%2C%0ANetInfoF%20has%20following%20notable%20advantages%3A%20%28a%29%20General%2C%20handling%20both%20link%0Aprediction%20and%20node%20classification%3B%20%28b%29%20Principled%2C%20with%20theoretical%20guarantee%0Aand%20closed-form%20solution%3B%20%28c%29%20Effective%2C%20thanks%20to%20the%20proposed%20adjustment%20to%0Anode%20similarity%3B%20%28d%29%20Scalable%2C%20scaling%20linearly%20with%20the%20input%20size.%20In%20our%0Acarefully%20designed%20synthetic%20datasets%2C%20NetInfoF%20correctly%20identifies%20the%20ground%0Atruth%20of%20NUI%20and%20is%20the%20only%20method%20being%20robust%20to%20all%20graph%20scenarios.%0AApplied%20on%20real-world%20datasets%2C%20NetInfoF%20wins%20in%2011%20out%20of%2012%20times%20on%20link%0Aprediction%20compared%20to%20general%20GNN%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07999v3&entry.124074799=Read"},
{"title": "DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses", "author": "Chen Zhao and Tong Zhang and Zheng Dang and Mathieu Salzmann", "abstract": "  Determining the relative pose of an object between two images is pivotal to\nthe success of generalizable object pose estimation. Existing approaches\ntypically approximate the continuous pose representation with a large number of\ndiscrete pose hypotheses, which incurs a computationally expensive process of\nscoring each hypothesis at test time. By contrast, we present a Deep Voxel\nMatching Network (DVMNet) that eliminates the need for pose hypotheses and\ncomputes the relative object pose in a single pass. To this end, we map the two\ninput RGB images, reference and query, to their respective voxelized 3D\nrepresentations. We then pass the resulting voxels through a pose estimation\nmodule, where the voxels are aligned and the pose is computed in an end-to-end\nfashion by solving a least-squares problem. To enhance robustness, we introduce\na weighted closest voxel algorithm capable of mitigating the impact of noisy\nvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse\ndatasets, demonstrating that our method delivers more accurate relative pose\nestimates for novel objects at a lower computational cost compared to\nstate-of-the-art methods. Our code is released at:\nhttps://github.com/sailor-z/DVMNet/.\n", "link": "http://arxiv.org/abs/2403.13683v1", "date": "2024-03-20", "relevancy": 2.265, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5369}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DVMNet%3A%20Computing%20Relative%20Pose%20for%20Unseen%20Objects%20Beyond%20Hypotheses&body=Title%3A%20DVMNet%3A%20Computing%20Relative%20Pose%20for%20Unseen%20Objects%20Beyond%20Hypotheses%0AAuthor%3A%20Chen%20Zhao%20and%20Tong%20Zhang%20and%20Zheng%20Dang%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%20Determining%20the%20relative%20pose%20of%20an%20object%20between%20two%20images%20is%20pivotal%20to%0Athe%20success%20of%20generalizable%20object%20pose%20estimation.%20Existing%20approaches%0Atypically%20approximate%20the%20continuous%20pose%20representation%20with%20a%20large%20number%20of%0Adiscrete%20pose%20hypotheses%2C%20which%20incurs%20a%20computationally%20expensive%20process%20of%0Ascoring%20each%20hypothesis%20at%20test%20time.%20By%20contrast%2C%20we%20present%20a%20Deep%20Voxel%0AMatching%20Network%20%28DVMNet%29%20that%20eliminates%20the%20need%20for%20pose%20hypotheses%20and%0Acomputes%20the%20relative%20object%20pose%20in%20a%20single%20pass.%20To%20this%20end%2C%20we%20map%20the%20two%0Ainput%20RGB%20images%2C%20reference%20and%20query%2C%20to%20their%20respective%20voxelized%203D%0Arepresentations.%20We%20then%20pass%20the%20resulting%20voxels%20through%20a%20pose%20estimation%0Amodule%2C%20where%20the%20voxels%20are%20aligned%20and%20the%20pose%20is%20computed%20in%20an%20end-to-end%0Afashion%20by%20solving%20a%20least-squares%20problem.%20To%20enhance%20robustness%2C%20we%20introduce%0Aa%20weighted%20closest%20voxel%20algorithm%20capable%20of%20mitigating%20the%20impact%20of%20noisy%0Avoxels.%20We%20conduct%20extensive%20experiments%20on%20the%20CO3D%2C%20LINEMOD%2C%20and%20Objaverse%0Adatasets%2C%20demonstrating%20that%20our%20method%20delivers%20more%20accurate%20relative%20pose%0Aestimates%20for%20novel%20objects%20at%20a%20lower%20computational%20cost%20compared%20to%0Astate-of-the-art%20methods.%20Our%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/sailor-z/DVMNet/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13683v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVMNet%3A%20Computing%20Relative%20Pose%20for%20Unseen%20Objects%20Beyond%20Hypotheses&entry.906535625=Chen%20Zhao%20and%20Tong%20Zhang%20and%20Zheng%20Dang%20and%20Mathieu%20Salzmann&entry.1292438233=%20%20Determining%20the%20relative%20pose%20of%20an%20object%20between%20two%20images%20is%20pivotal%20to%0Athe%20success%20of%20generalizable%20object%20pose%20estimation.%20Existing%20approaches%0Atypically%20approximate%20the%20continuous%20pose%20representation%20with%20a%20large%20number%20of%0Adiscrete%20pose%20hypotheses%2C%20which%20incurs%20a%20computationally%20expensive%20process%20of%0Ascoring%20each%20hypothesis%20at%20test%20time.%20By%20contrast%2C%20we%20present%20a%20Deep%20Voxel%0AMatching%20Network%20%28DVMNet%29%20that%20eliminates%20the%20need%20for%20pose%20hypotheses%20and%0Acomputes%20the%20relative%20object%20pose%20in%20a%20single%20pass.%20To%20this%20end%2C%20we%20map%20the%20two%0Ainput%20RGB%20images%2C%20reference%20and%20query%2C%20to%20their%20respective%20voxelized%203D%0Arepresentations.%20We%20then%20pass%20the%20resulting%20voxels%20through%20a%20pose%20estimation%0Amodule%2C%20where%20the%20voxels%20are%20aligned%20and%20the%20pose%20is%20computed%20in%20an%20end-to-end%0Afashion%20by%20solving%20a%20least-squares%20problem.%20To%20enhance%20robustness%2C%20we%20introduce%0Aa%20weighted%20closest%20voxel%20algorithm%20capable%20of%20mitigating%20the%20impact%20of%20noisy%0Avoxels.%20We%20conduct%20extensive%20experiments%20on%20the%20CO3D%2C%20LINEMOD%2C%20and%20Objaverse%0Adatasets%2C%20demonstrating%20that%20our%20method%20delivers%20more%20accurate%20relative%20pose%0Aestimates%20for%20novel%20objects%20at%20a%20lower%20computational%20cost%20compared%20to%0Astate-of-the-art%20methods.%20Our%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/sailor-z/DVMNet/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13683v1&entry.124074799=Read"},
{"title": "DepthFM: Fast Monocular Depth Estimation with Flow Matching", "author": "Ming Gui and Johannes S. Fischer and Ulrich Prestel and Pingchuan Ma and Dmytro Kotovenko and Olga Grebenkova and Stefan Andreas Baumann and Vincent Tao Hu and Bj\u00f6rn Ommer", "abstract": "  Monocular depth estimation is crucial for numerous downstream vision tasks\nand applications. Current discriminative approaches to this problem are limited\ndue to blurry artifacts, while state-of-the-art generative methods suffer from\nslow sampling due to their SDE nature. Rather than starting from noise, we seek\na direct mapping from input image to depth map. We observe that this can be\neffectively framed using flow matching, since its straight trajectories through\nsolution space offer efficiency and high quality. Our study demonstrates that a\npre-trained image diffusion model can serve as an adequate prior for a flow\nmatching depth model, allowing efficient training on only synthetic data to\ngeneralize to real images. We find that an auxiliary surface normals loss\nfurther improves the depth estimates. Due to the generative nature of our\napproach, our model reliably predicts the confidence of its depth estimates. On\nstandard benchmarks of complex natural scenes, our lightweight approach\nexhibits state-of-the-art performance at favorable low computational cost\ndespite only being trained on little synthetic data.\n", "link": "http://arxiv.org/abs/2403.13788v1", "date": "2024-03-20", "relevancy": 2.2619, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5615}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching&body=Title%3A%20DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching%0AAuthor%3A%20Ming%20Gui%20and%20Johannes%20S.%20Fischer%20and%20Ulrich%20Prestel%20and%20Pingchuan%20Ma%20and%20Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Stefan%20Andreas%20Baumann%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20is%20crucial%20for%20numerous%20downstream%20vision%20tasks%0Aand%20applications.%20Current%20discriminative%20approaches%20to%20this%20problem%20are%20limited%0Adue%20to%20blurry%20artifacts%2C%20while%20state-of-the-art%20generative%20methods%20suffer%20from%0Aslow%20sampling%20due%20to%20their%20SDE%20nature.%20Rather%20than%20starting%20from%20noise%2C%20we%20seek%0Aa%20direct%20mapping%20from%20input%20image%20to%20depth%20map.%20We%20observe%20that%20this%20can%20be%0Aeffectively%20framed%20using%20flow%20matching%2C%20since%20its%20straight%20trajectories%20through%0Asolution%20space%20offer%20efficiency%20and%20high%20quality.%20Our%20study%20demonstrates%20that%20a%0Apre-trained%20image%20diffusion%20model%20can%20serve%20as%20an%20adequate%20prior%20for%20a%20flow%0Amatching%20depth%20model%2C%20allowing%20efficient%20training%20on%20only%20synthetic%20data%20to%0Ageneralize%20to%20real%20images.%20We%20find%20that%20an%20auxiliary%20surface%20normals%20loss%0Afurther%20improves%20the%20depth%20estimates.%20Due%20to%20the%20generative%20nature%20of%20our%0Aapproach%2C%20our%20model%20reliably%20predicts%20the%20confidence%20of%20its%20depth%20estimates.%20On%0Astandard%20benchmarks%20of%20complex%20natural%20scenes%2C%20our%20lightweight%20approach%0Aexhibits%20state-of-the-art%20performance%20at%20favorable%20low%20computational%20cost%0Adespite%20only%20being%20trained%20on%20little%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13788v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching&entry.906535625=Ming%20Gui%20and%20Johannes%20S.%20Fischer%20and%20Ulrich%20Prestel%20and%20Pingchuan%20Ma%20and%20Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Stefan%20Andreas%20Baumann%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Monocular%20depth%20estimation%20is%20crucial%20for%20numerous%20downstream%20vision%20tasks%0Aand%20applications.%20Current%20discriminative%20approaches%20to%20this%20problem%20are%20limited%0Adue%20to%20blurry%20artifacts%2C%20while%20state-of-the-art%20generative%20methods%20suffer%20from%0Aslow%20sampling%20due%20to%20their%20SDE%20nature.%20Rather%20than%20starting%20from%20noise%2C%20we%20seek%0Aa%20direct%20mapping%20from%20input%20image%20to%20depth%20map.%20We%20observe%20that%20this%20can%20be%0Aeffectively%20framed%20using%20flow%20matching%2C%20since%20its%20straight%20trajectories%20through%0Asolution%20space%20offer%20efficiency%20and%20high%20quality.%20Our%20study%20demonstrates%20that%20a%0Apre-trained%20image%20diffusion%20model%20can%20serve%20as%20an%20adequate%20prior%20for%20a%20flow%0Amatching%20depth%20model%2C%20allowing%20efficient%20training%20on%20only%20synthetic%20data%20to%0Ageneralize%20to%20real%20images.%20We%20find%20that%20an%20auxiliary%20surface%20normals%20loss%0Afurther%20improves%20the%20depth%20estimates.%20Due%20to%20the%20generative%20nature%20of%20our%0Aapproach%2C%20our%20model%20reliably%20predicts%20the%20confidence%20of%20its%20depth%20estimates.%20On%0Astandard%20benchmarks%20of%20complex%20natural%20scenes%2C%20our%20lightweight%20approach%0Aexhibits%20state-of-the-art%20performance%20at%20favorable%20low%20computational%20cost%0Adespite%20only%20being%20trained%20on%20little%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13788v1&entry.124074799=Read"},
{"title": "Towards Architecture-Agnostic Untrained Network Priors for Image\n  Reconstruction with Frequency Regularization", "author": "Yilin Liu and Yunkui Pang and Jiang Li and Yong Chen and Pew-Thian Yap", "abstract": "  Untrained networks inspired by deep image prior have shown promising\ncapabilities in recovering a high-quality image from noisy or partial\nmeasurements, without requiring training data. Their success has been widely\nattributed to the spectral bias acting as an implicit regularization induced by\nsuitable network architectures. However, applications of such network-based\npriors often entail superfluous architectural decisions, overfitting risks, and\nslow optimization, all of which hinder their practicality. In this work, we\npropose efficient, architecture-agnostic methods for a more direct frequency\ncontrol over the network priors: 1) constraining the bandwidth of the\nwhite-noise input, 2) controlling the bandwidth of the interpolation-based\nupsamplers, and 3) regularizing the Lipschitz constants of the layers. We show\nthat even with just one extra line of code, the overfitting issues in\nunderperforming architectures can be alleviated such that their performance\ngaps with the high-performing counterparts can be largely closed despite their\ndistinct configurations, mitigating the need for architecture tuning. This then\nmakes it possible to employ a more compact model to achieve similar or superior\nperformance to larger models with greater efficiency. Our regularized network\npriors compare favorably with current supervised and self-supervised methods on\nMRI reconstruction and image inpainting tasks, serving as a stronger zero-shot\nbaseline reconstructor. Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2312.09988v2", "date": "2024-03-20", "relevancy": 2.2546, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5866}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5456}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Architecture-Agnostic%20Untrained%20Network%20Priors%20for%20Image%0A%20%20Reconstruction%20with%20Frequency%20Regularization&body=Title%3A%20Towards%20Architecture-Agnostic%20Untrained%20Network%20Priors%20for%20Image%0A%20%20Reconstruction%20with%20Frequency%20Regularization%0AAuthor%3A%20Yilin%20Liu%20and%20Yunkui%20Pang%20and%20Jiang%20Li%20and%20Yong%20Chen%20and%20Pew-Thian%20Yap%0AAbstract%3A%20%20%20Untrained%20networks%20inspired%20by%20deep%20image%20prior%20have%20shown%20promising%0Acapabilities%20in%20recovering%20a%20high-quality%20image%20from%20noisy%20or%20partial%0Ameasurements%2C%20without%20requiring%20training%20data.%20Their%20success%20has%20been%20widely%0Aattributed%20to%20the%20spectral%20bias%20acting%20as%20an%20implicit%20regularization%20induced%20by%0Asuitable%20network%20architectures.%20However%2C%20applications%20of%20such%20network-based%0Apriors%20often%20entail%20superfluous%20architectural%20decisions%2C%20overfitting%20risks%2C%20and%0Aslow%20optimization%2C%20all%20of%20which%20hinder%20their%20practicality.%20In%20this%20work%2C%20we%0Apropose%20efficient%2C%20architecture-agnostic%20methods%20for%20a%20more%20direct%20frequency%0Acontrol%20over%20the%20network%20priors%3A%201%29%20constraining%20the%20bandwidth%20of%20the%0Awhite-noise%20input%2C%202%29%20controlling%20the%20bandwidth%20of%20the%20interpolation-based%0Aupsamplers%2C%20and%203%29%20regularizing%20the%20Lipschitz%20constants%20of%20the%20layers.%20We%20show%0Athat%20even%20with%20just%20one%20extra%20line%20of%20code%2C%20the%20overfitting%20issues%20in%0Aunderperforming%20architectures%20can%20be%20alleviated%20such%20that%20their%20performance%0Agaps%20with%20the%20high-performing%20counterparts%20can%20be%20largely%20closed%20despite%20their%0Adistinct%20configurations%2C%20mitigating%20the%20need%20for%20architecture%20tuning.%20This%20then%0Amakes%20it%20possible%20to%20employ%20a%20more%20compact%20model%20to%20achieve%20similar%20or%20superior%0Aperformance%20to%20larger%20models%20with%20greater%20efficiency.%20Our%20regularized%20network%0Apriors%20compare%20favorably%20with%20current%20supervised%20and%20self-supervised%20methods%20on%0AMRI%20reconstruction%20and%20image%20inpainting%20tasks%2C%20serving%20as%20a%20stronger%20zero-shot%0Abaseline%20reconstructor.%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09988v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Architecture-Agnostic%20Untrained%20Network%20Priors%20for%20Image%0A%20%20Reconstruction%20with%20Frequency%20Regularization&entry.906535625=Yilin%20Liu%20and%20Yunkui%20Pang%20and%20Jiang%20Li%20and%20Yong%20Chen%20and%20Pew-Thian%20Yap&entry.1292438233=%20%20Untrained%20networks%20inspired%20by%20deep%20image%20prior%20have%20shown%20promising%0Acapabilities%20in%20recovering%20a%20high-quality%20image%20from%20noisy%20or%20partial%0Ameasurements%2C%20without%20requiring%20training%20data.%20Their%20success%20has%20been%20widely%0Aattributed%20to%20the%20spectral%20bias%20acting%20as%20an%20implicit%20regularization%20induced%20by%0Asuitable%20network%20architectures.%20However%2C%20applications%20of%20such%20network-based%0Apriors%20often%20entail%20superfluous%20architectural%20decisions%2C%20overfitting%20risks%2C%20and%0Aslow%20optimization%2C%20all%20of%20which%20hinder%20their%20practicality.%20In%20this%20work%2C%20we%0Apropose%20efficient%2C%20architecture-agnostic%20methods%20for%20a%20more%20direct%20frequency%0Acontrol%20over%20the%20network%20priors%3A%201%29%20constraining%20the%20bandwidth%20of%20the%0Awhite-noise%20input%2C%202%29%20controlling%20the%20bandwidth%20of%20the%20interpolation-based%0Aupsamplers%2C%20and%203%29%20regularizing%20the%20Lipschitz%20constants%20of%20the%20layers.%20We%20show%0Athat%20even%20with%20just%20one%20extra%20line%20of%20code%2C%20the%20overfitting%20issues%20in%0Aunderperforming%20architectures%20can%20be%20alleviated%20such%20that%20their%20performance%0Agaps%20with%20the%20high-performing%20counterparts%20can%20be%20largely%20closed%20despite%20their%0Adistinct%20configurations%2C%20mitigating%20the%20need%20for%20architecture%20tuning.%20This%20then%0Amakes%20it%20possible%20to%20employ%20a%20more%20compact%20model%20to%20achieve%20similar%20or%20superior%0Aperformance%20to%20larger%20models%20with%20greater%20efficiency.%20Our%20regularized%20network%0Apriors%20compare%20favorably%20with%20current%20supervised%20and%20self-supervised%20methods%20on%0AMRI%20reconstruction%20and%20image%20inpainting%20tasks%2C%20serving%20as%20a%20stronger%20zero-shot%0Abaseline%20reconstructor.%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09988v2&entry.124074799=Read"},
{"title": "Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer", "author": "Yu Deng and Duomin Wang and Baoyuan Wang", "abstract": "  In this paper, we propose a novel learning approach for feed-forward one-shot\n4D head avatar synthesis. Different from existing methods that often learn from\nreconstructing monocular videos guided by 3DMM, we employ pseudo multi-view\nvideos to learn a 4D head synthesizer in a data-driven manner, avoiding\nreliance on inaccurate 3DMM reconstruction that could be detrimental to the\nsynthesis performance. The key idea is to first learn a 3D head synthesizer\nusing synthetic multi-view images to convert monocular real videos into\nmulti-view ones, and then utilize the pseudo multi-view videos to learn a 4D\nhead synthesizer via cross-view self-reenactment. By leveraging a simple vision\ntransformer backbone with motion-aware cross-attentions, our method exhibits\nsuperior performance compared to previous methods in terms of reconstruction\nfidelity, geometry consistency, and motion control accuracy. We hope our method\noffers novel insights into integrating 3D priors with 2D supervisions for\nimproved 4D head avatar creation.\n", "link": "http://arxiv.org/abs/2403.13570v1", "date": "2024-03-20", "relevancy": 2.2484, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5519}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Portrait4D-v2%3A%20Pseudo%20Multi-View%20Data%20Creates%20Better%204D%20Head%20Synthesizer&body=Title%3A%20Portrait4D-v2%3A%20Pseudo%20Multi-View%20Data%20Creates%20Better%204D%20Head%20Synthesizer%0AAuthor%3A%20Yu%20Deng%20and%20Duomin%20Wang%20and%20Baoyuan%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20learning%20approach%20for%20feed-forward%20one-shot%0A4D%20head%20avatar%20synthesis.%20Different%20from%20existing%20methods%20that%20often%20learn%20from%0Areconstructing%20monocular%20videos%20guided%20by%203DMM%2C%20we%20employ%20pseudo%20multi-view%0Avideos%20to%20learn%20a%204D%20head%20synthesizer%20in%20a%20data-driven%20manner%2C%20avoiding%0Areliance%20on%20inaccurate%203DMM%20reconstruction%20that%20could%20be%20detrimental%20to%20the%0Asynthesis%20performance.%20The%20key%20idea%20is%20to%20first%20learn%20a%203D%20head%20synthesizer%0Ausing%20synthetic%20multi-view%20images%20to%20convert%20monocular%20real%20videos%20into%0Amulti-view%20ones%2C%20and%20then%20utilize%20the%20pseudo%20multi-view%20videos%20to%20learn%20a%204D%0Ahead%20synthesizer%20via%20cross-view%20self-reenactment.%20By%20leveraging%20a%20simple%20vision%0Atransformer%20backbone%20with%20motion-aware%20cross-attentions%2C%20our%20method%20exhibits%0Asuperior%20performance%20compared%20to%20previous%20methods%20in%20terms%20of%20reconstruction%0Afidelity%2C%20geometry%20consistency%2C%20and%20motion%20control%20accuracy.%20We%20hope%20our%20method%0Aoffers%20novel%20insights%20into%20integrating%203D%20priors%20with%202D%20supervisions%20for%0Aimproved%204D%20head%20avatar%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13570v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Portrait4D-v2%3A%20Pseudo%20Multi-View%20Data%20Creates%20Better%204D%20Head%20Synthesizer&entry.906535625=Yu%20Deng%20and%20Duomin%20Wang%20and%20Baoyuan%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20learning%20approach%20for%20feed-forward%20one-shot%0A4D%20head%20avatar%20synthesis.%20Different%20from%20existing%20methods%20that%20often%20learn%20from%0Areconstructing%20monocular%20videos%20guided%20by%203DMM%2C%20we%20employ%20pseudo%20multi-view%0Avideos%20to%20learn%20a%204D%20head%20synthesizer%20in%20a%20data-driven%20manner%2C%20avoiding%0Areliance%20on%20inaccurate%203DMM%20reconstruction%20that%20could%20be%20detrimental%20to%20the%0Asynthesis%20performance.%20The%20key%20idea%20is%20to%20first%20learn%20a%203D%20head%20synthesizer%0Ausing%20synthetic%20multi-view%20images%20to%20convert%20monocular%20real%20videos%20into%0Amulti-view%20ones%2C%20and%20then%20utilize%20the%20pseudo%20multi-view%20videos%20to%20learn%20a%204D%0Ahead%20synthesizer%20via%20cross-view%20self-reenactment.%20By%20leveraging%20a%20simple%20vision%0Atransformer%20backbone%20with%20motion-aware%20cross-attentions%2C%20our%20method%20exhibits%0Asuperior%20performance%20compared%20to%20previous%20methods%20in%20terms%20of%20reconstruction%0Afidelity%2C%20geometry%20consistency%2C%20and%20motion%20control%20accuracy.%20We%20hope%20our%20method%0Aoffers%20novel%20insights%20into%20integrating%203D%20priors%20with%202D%20supervisions%20for%0Aimproved%204D%20head%20avatar%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13570v1&entry.124074799=Read"},
{"title": "Meta-Point Learning and Refining for Category-Agnostic Pose Estimation", "author": "Junjie Chen and Jiebin Yan and Yuming Fang and Li Niu", "abstract": "  Category-agnostic pose estimation (CAPE) aims to predict keypoints for\narbitrary classes given a few support images annotated with keypoints. Existing\nmethods only rely on the features extracted at support keypoints to predict or\nrefine the keypoints on query image, but a few support feature vectors are\nlocal and inadequate for CAPE. Considering that human can quickly perceive\npotential keypoints of arbitrary objects, we propose a novel framework for CAPE\nbased on such potential keypoints (named as meta-points). Specifically, we\nmaintain learnable embeddings to capture inherent information of various\nkeypoints, which interact with image feature maps to produce meta-points\nwithout any support. The produced meta-points could serve as meaningful\npotential keypoints for CAPE. Due to the inevitable gap between inherency and\nannotation, we finally utilize the identities and details offered by support\nkeypoints to assign and refine meta-points to desired keypoints in query image.\nIn addition, we propose a progressive deformable point decoder and a slacked\nregression loss for better prediction and supervision. Our novel framework not\nonly reveals the inherency of keypoints but also outperforms existing methods\nof CAPE. Comprehensive experiments and in-depth studies on large-scale MP-100\ndataset demonstrate the effectiveness of our framework.\n", "link": "http://arxiv.org/abs/2403.13647v1", "date": "2024-03-20", "relevancy": 2.2401, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5964}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Meta-Point%20Learning%20and%20Refining%20for%20Category-Agnostic%20Pose%20Estimation&body=Title%3A%20Meta-Point%20Learning%20and%20Refining%20for%20Category-Agnostic%20Pose%20Estimation%0AAuthor%3A%20Junjie%20Chen%20and%20Jiebin%20Yan%20and%20Yuming%20Fang%20and%20Li%20Niu%0AAbstract%3A%20%20%20Category-agnostic%20pose%20estimation%20%28CAPE%29%20aims%20to%20predict%20keypoints%20for%0Aarbitrary%20classes%20given%20a%20few%20support%20images%20annotated%20with%20keypoints.%20Existing%0Amethods%20only%20rely%20on%20the%20features%20extracted%20at%20support%20keypoints%20to%20predict%20or%0Arefine%20the%20keypoints%20on%20query%20image%2C%20but%20a%20few%20support%20feature%20vectors%20are%0Alocal%20and%20inadequate%20for%20CAPE.%20Considering%20that%20human%20can%20quickly%20perceive%0Apotential%20keypoints%20of%20arbitrary%20objects%2C%20we%20propose%20a%20novel%20framework%20for%20CAPE%0Abased%20on%20such%20potential%20keypoints%20%28named%20as%20meta-points%29.%20Specifically%2C%20we%0Amaintain%20learnable%20embeddings%20to%20capture%20inherent%20information%20of%20various%0Akeypoints%2C%20which%20interact%20with%20image%20feature%20maps%20to%20produce%20meta-points%0Awithout%20any%20support.%20The%20produced%20meta-points%20could%20serve%20as%20meaningful%0Apotential%20keypoints%20for%20CAPE.%20Due%20to%20the%20inevitable%20gap%20between%20inherency%20and%0Aannotation%2C%20we%20finally%20utilize%20the%20identities%20and%20details%20offered%20by%20support%0Akeypoints%20to%20assign%20and%20refine%20meta-points%20to%20desired%20keypoints%20in%20query%20image.%0AIn%20addition%2C%20we%20propose%20a%20progressive%20deformable%20point%20decoder%20and%20a%20slacked%0Aregression%20loss%20for%20better%20prediction%20and%20supervision.%20Our%20novel%20framework%20not%0Aonly%20reveals%20the%20inherency%20of%20keypoints%20but%20also%20outperforms%20existing%20methods%0Aof%20CAPE.%20Comprehensive%20experiments%20and%20in-depth%20studies%20on%20large-scale%20MP-100%0Adataset%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13647v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Point%20Learning%20and%20Refining%20for%20Category-Agnostic%20Pose%20Estimation&entry.906535625=Junjie%20Chen%20and%20Jiebin%20Yan%20and%20Yuming%20Fang%20and%20Li%20Niu&entry.1292438233=%20%20Category-agnostic%20pose%20estimation%20%28CAPE%29%20aims%20to%20predict%20keypoints%20for%0Aarbitrary%20classes%20given%20a%20few%20support%20images%20annotated%20with%20keypoints.%20Existing%0Amethods%20only%20rely%20on%20the%20features%20extracted%20at%20support%20keypoints%20to%20predict%20or%0Arefine%20the%20keypoints%20on%20query%20image%2C%20but%20a%20few%20support%20feature%20vectors%20are%0Alocal%20and%20inadequate%20for%20CAPE.%20Considering%20that%20human%20can%20quickly%20perceive%0Apotential%20keypoints%20of%20arbitrary%20objects%2C%20we%20propose%20a%20novel%20framework%20for%20CAPE%0Abased%20on%20such%20potential%20keypoints%20%28named%20as%20meta-points%29.%20Specifically%2C%20we%0Amaintain%20learnable%20embeddings%20to%20capture%20inherent%20information%20of%20various%0Akeypoints%2C%20which%20interact%20with%20image%20feature%20maps%20to%20produce%20meta-points%0Awithout%20any%20support.%20The%20produced%20meta-points%20could%20serve%20as%20meaningful%0Apotential%20keypoints%20for%20CAPE.%20Due%20to%20the%20inevitable%20gap%20between%20inherency%20and%0Aannotation%2C%20we%20finally%20utilize%20the%20identities%20and%20details%20offered%20by%20support%0Akeypoints%20to%20assign%20and%20refine%20meta-points%20to%20desired%20keypoints%20in%20query%20image.%0AIn%20addition%2C%20we%20propose%20a%20progressive%20deformable%20point%20decoder%20and%20a%20slacked%0Aregression%20loss%20for%20better%20prediction%20and%20supervision.%20Our%20novel%20framework%20not%0Aonly%20reveals%20the%20inherency%20of%20keypoints%20but%20also%20outperforms%20existing%20methods%0Aof%20CAPE.%20Comprehensive%20experiments%20and%20in-depth%20studies%20on%20large-scale%20MP-100%0Adataset%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13647v1&entry.124074799=Read"},
{"title": "Learning from Models and Data for Visual Grounding", "author": "Ruozhen He and Paola Cascante-Bonilla and Ziyan Yang and Alexander C. Berg and Vicente Ordonez", "abstract": "  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n", "link": "http://arxiv.org/abs/2403.13804v1", "date": "2024-03-20", "relevancy": 2.2399, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Models%20and%20Data%20for%20Visual%20Grounding&body=Title%3A%20Learning%20from%20Models%20and%20Data%20for%20Visual%20Grounding%0AAuthor%3A%20Ruozhen%20He%20and%20Paola%20Cascante-Bonilla%20and%20Ziyan%20Yang%20and%20Alexander%20C.%20Berg%20and%20Vicente%20Ordonez%0AAbstract%3A%20%20%20We%20introduce%20SynGround%2C%20a%20novel%20framework%20that%20combines%20data-driven%20learning%0Aand%20knowledge%20transfer%20from%20various%20large-scale%20pretrained%20models%20to%20enhance%0Athe%20visual%20grounding%20capabilities%20of%20a%20pretrained%20vision-and-language%20model.%0AThe%20knowledge%20transfer%20from%20the%20models%20initiates%20the%20generation%20of%20image%0Adescriptions%20through%20an%20image%20description%20generator.%20These%20descriptions%20serve%0Adual%20purposes%3A%20they%20act%20as%20prompts%20for%20synthesizing%20images%20through%20a%0Atext-to-image%20generator%2C%20and%20as%20queries%20for%20synthesizing%20text%2C%20from%20which%0Aphrases%20are%20extracted%20using%20a%20large%20language%20model.%20Finally%2C%20we%20leverage%20an%0Aopen-vocabulary%20object%20detector%20to%20generate%20synthetic%20bounding%20boxes%20for%20the%0Asynthetic%20images%20and%20texts.%20We%20finetune%20a%20pretrained%20vision-and-language%20model%0Aon%20this%20dataset%20by%20optimizing%20a%20mask-attention%20consistency%20objective%20that%0Aaligns%20region%20annotations%20with%20gradient-based%20model%20explanations.%20The%20resulting%0Amodel%20improves%20the%20grounding%20capabilities%20of%20an%20off-the-shelf%0Avision-and-language%20model.%20Particularly%2C%20SynGround%20improves%20the%20pointing%20game%0Aaccuracy%20of%20ALBEF%20on%20the%20Flickr30k%20dataset%20from%2079.38%25%20to%2087.26%25%2C%20and%20on%0ARefCOCO%2B%20Test%20A%20from%2069.35%25%20to%2079.06%25%20and%20on%20RefCOCO%2B%20Test%20B%20from%2053.77%25%20to%0A63.67%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13804v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Models%20and%20Data%20for%20Visual%20Grounding&entry.906535625=Ruozhen%20He%20and%20Paola%20Cascante-Bonilla%20and%20Ziyan%20Yang%20and%20Alexander%20C.%20Berg%20and%20Vicente%20Ordonez&entry.1292438233=%20%20We%20introduce%20SynGround%2C%20a%20novel%20framework%20that%20combines%20data-driven%20learning%0Aand%20knowledge%20transfer%20from%20various%20large-scale%20pretrained%20models%20to%20enhance%0Athe%20visual%20grounding%20capabilities%20of%20a%20pretrained%20vision-and-language%20model.%0AThe%20knowledge%20transfer%20from%20the%20models%20initiates%20the%20generation%20of%20image%0Adescriptions%20through%20an%20image%20description%20generator.%20These%20descriptions%20serve%0Adual%20purposes%3A%20they%20act%20as%20prompts%20for%20synthesizing%20images%20through%20a%0Atext-to-image%20generator%2C%20and%20as%20queries%20for%20synthesizing%20text%2C%20from%20which%0Aphrases%20are%20extracted%20using%20a%20large%20language%20model.%20Finally%2C%20we%20leverage%20an%0Aopen-vocabulary%20object%20detector%20to%20generate%20synthetic%20bounding%20boxes%20for%20the%0Asynthetic%20images%20and%20texts.%20We%20finetune%20a%20pretrained%20vision-and-language%20model%0Aon%20this%20dataset%20by%20optimizing%20a%20mask-attention%20consistency%20objective%20that%0Aaligns%20region%20annotations%20with%20gradient-based%20model%20explanations.%20The%20resulting%0Amodel%20improves%20the%20grounding%20capabilities%20of%20an%20off-the-shelf%0Avision-and-language%20model.%20Particularly%2C%20SynGround%20improves%20the%20pointing%20game%0Aaccuracy%20of%20ALBEF%20on%20the%20Flickr30k%20dataset%20from%2079.38%25%20to%2087.26%25%2C%20and%20on%0ARefCOCO%2B%20Test%20A%20from%2069.35%25%20to%2079.06%25%20and%20on%20RefCOCO%2B%20Test%20B%20from%2053.77%25%20to%0A63.67%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13804v1&entry.124074799=Read"},
{"title": "Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and\n  Real-time Rendering", "author": "Yurui Chen and Chun Gu and Junzhe Jiang and Xiatian Zhu and Li Zhang", "abstract": "  Modeling dynamic, large-scale urban scenes is challenging due to their highly\nintricate geometric structures and unconstrained dynamics in both space and\ntime. Prior methods often employ high-level architectural priors, separating\nstatic and dynamic elements, resulting in suboptimal capture of their\nsynergistic interactions. To address this challenge, we present a unified\nrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds upon\nthe efficient 3D Gaussian splatting technique, originally designed for static\nscene representation, by introducing periodic vibration-based temporal\ndynamics. This innovation enables PVG to elegantly and uniformly represent the\ncharacteristics of various objects and elements in dynamic urban scenes. To\nenhance temporally coherent and large scene representation learning with sparse\ntraining data, we introduce a novel temporal smoothing mechanism and a\nposition-aware adaptive control strategy respectively. Extensive experiments on\nWaymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses\nstate-of-the-art alternatives in both reconstruction and novel view synthesis\nfor both dynamic and static scenes. Notably, PVG achieves this without relying\non manually labeled object bounding boxes or expensive optical flow estimation.\nMoreover, PVG exhibits 900-fold acceleration in rendering over the best\nalternative.\n", "link": "http://arxiv.org/abs/2311.18561v2", "date": "2024-03-20", "relevancy": 2.2335, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5563}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Periodic%20Vibration%20Gaussian%3A%20Dynamic%20Urban%20Scene%20Reconstruction%20and%0A%20%20Real-time%20Rendering&body=Title%3A%20Periodic%20Vibration%20Gaussian%3A%20Dynamic%20Urban%20Scene%20Reconstruction%20and%0A%20%20Real-time%20Rendering%0AAuthor%3A%20Yurui%20Chen%20and%20Chun%20Gu%20and%20Junzhe%20Jiang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Modeling%20dynamic%2C%20large-scale%20urban%20scenes%20is%20challenging%20due%20to%20their%20highly%0Aintricate%20geometric%20structures%20and%20unconstrained%20dynamics%20in%20both%20space%20and%0Atime.%20Prior%20methods%20often%20employ%20high-level%20architectural%20priors%2C%20separating%0Astatic%20and%20dynamic%20elements%2C%20resulting%20in%20suboptimal%20capture%20of%20their%0Asynergistic%20interactions.%20To%20address%20this%20challenge%2C%20we%20present%20a%20unified%0Arepresentation%20model%2C%20called%20Periodic%20Vibration%20Gaussian%20%28PVG%29.%20PVG%20builds%20upon%0Athe%20efficient%203D%20Gaussian%20splatting%20technique%2C%20originally%20designed%20for%20static%0Ascene%20representation%2C%20by%20introducing%20periodic%20vibration-based%20temporal%0Adynamics.%20This%20innovation%20enables%20PVG%20to%20elegantly%20and%20uniformly%20represent%20the%0Acharacteristics%20of%20various%20objects%20and%20elements%20in%20dynamic%20urban%20scenes.%20To%0Aenhance%20temporally%20coherent%20and%20large%20scene%20representation%20learning%20with%20sparse%0Atraining%20data%2C%20we%20introduce%20a%20novel%20temporal%20smoothing%20mechanism%20and%20a%0Aposition-aware%20adaptive%20control%20strategy%20respectively.%20Extensive%20experiments%20on%0AWaymo%20Open%20Dataset%20and%20KITTI%20benchmarks%20demonstrate%20that%20PVG%20surpasses%0Astate-of-the-art%20alternatives%20in%20both%20reconstruction%20and%20novel%20view%20synthesis%0Afor%20both%20dynamic%20and%20static%20scenes.%20Notably%2C%20PVG%20achieves%20this%20without%20relying%0Aon%20manually%20labeled%20object%20bounding%20boxes%20or%20expensive%20optical%20flow%20estimation.%0AMoreover%2C%20PVG%20exhibits%20900-fold%20acceleration%20in%20rendering%20over%20the%20best%0Aalternative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18561v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Periodic%20Vibration%20Gaussian%3A%20Dynamic%20Urban%20Scene%20Reconstruction%20and%0A%20%20Real-time%20Rendering&entry.906535625=Yurui%20Chen%20and%20Chun%20Gu%20and%20Junzhe%20Jiang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang&entry.1292438233=%20%20Modeling%20dynamic%2C%20large-scale%20urban%20scenes%20is%20challenging%20due%20to%20their%20highly%0Aintricate%20geometric%20structures%20and%20unconstrained%20dynamics%20in%20both%20space%20and%0Atime.%20Prior%20methods%20often%20employ%20high-level%20architectural%20priors%2C%20separating%0Astatic%20and%20dynamic%20elements%2C%20resulting%20in%20suboptimal%20capture%20of%20their%0Asynergistic%20interactions.%20To%20address%20this%20challenge%2C%20we%20present%20a%20unified%0Arepresentation%20model%2C%20called%20Periodic%20Vibration%20Gaussian%20%28PVG%29.%20PVG%20builds%20upon%0Athe%20efficient%203D%20Gaussian%20splatting%20technique%2C%20originally%20designed%20for%20static%0Ascene%20representation%2C%20by%20introducing%20periodic%20vibration-based%20temporal%0Adynamics.%20This%20innovation%20enables%20PVG%20to%20elegantly%20and%20uniformly%20represent%20the%0Acharacteristics%20of%20various%20objects%20and%20elements%20in%20dynamic%20urban%20scenes.%20To%0Aenhance%20temporally%20coherent%20and%20large%20scene%20representation%20learning%20with%20sparse%0Atraining%20data%2C%20we%20introduce%20a%20novel%20temporal%20smoothing%20mechanism%20and%20a%0Aposition-aware%20adaptive%20control%20strategy%20respectively.%20Extensive%20experiments%20on%0AWaymo%20Open%20Dataset%20and%20KITTI%20benchmarks%20demonstrate%20that%20PVG%20surpasses%0Astate-of-the-art%20alternatives%20in%20both%20reconstruction%20and%20novel%20view%20synthesis%0Afor%20both%20dynamic%20and%20static%20scenes.%20Notably%2C%20PVG%20achieves%20this%20without%20relying%0Aon%20manually%20labeled%20object%20bounding%20boxes%20or%20expensive%20optical%20flow%20estimation.%0AMoreover%2C%20PVG%20exhibits%20900-fold%20acceleration%20in%20rendering%20over%20the%20best%0Aalternative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18561v2&entry.124074799=Read"},
{"title": "LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning", "author": "Shu Wang and Muzhi Han and Ziyuan Jiao and Zeyu Zhang and Ying Nian Wu and Song-Chun Zhu and Hangxin Liu", "abstract": "  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n", "link": "http://arxiv.org/abs/2403.11552v2", "date": "2024-03-20", "relevancy": 2.226, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.578}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5404}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLM3%3ALarge%20Language%20Model-based%20Task%20and%20Motion%20Planning%20with%20Motion%0A%20%20Failure%20Reasoning&body=Title%3A%20LLM3%3ALarge%20Language%20Model-based%20Task%20and%20Motion%20Planning%20with%20Motion%0A%20%20Failure%20Reasoning%0AAuthor%3A%20Shu%20Wang%20and%20Muzhi%20Han%20and%20Ziyuan%20Jiao%20and%20Zeyu%20Zhang%20and%20Ying%20Nian%20Wu%20and%20Song-Chun%20Zhu%20and%20Hangxin%20Liu%0AAbstract%3A%20%20%20Conventional%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approaches%20rely%20on%20manually%0Acrafted%20interfaces%20connecting%20symbolic%20task%20planning%20with%20continuous%20motion%0Ageneration.%20These%20domain-specific%20and%20labor-intensive%20modules%20are%20limited%20in%0Aaddressing%20emerging%20tasks%20in%20real-world%20settings.%20Here%2C%20we%20present%20LLM%5E3%2C%20a%0Anovel%20Large%20Language%20Model%20%28LLM%29-based%20TAMP%20framework%20featuring%20a%0Adomain-independent%20interface.%20Specifically%2C%20we%20leverage%20the%20powerful%20reasoning%0Aand%20planning%20capabilities%20of%20pre-trained%20LLMs%20to%20propose%20symbolic%20action%0Asequences%20and%20select%20continuous%20action%20parameters%20for%20motion%20planning.%0ACrucially%2C%20LLM%5E3%20incorporates%20motion%20planning%20feedback%20through%20prompting%2C%0Aallowing%20the%20LLM%20to%20iteratively%20refine%20its%20proposals%20by%20reasoning%20about%20motion%0Afailure.%20Consequently%2C%20LLM%5E3%20interfaces%20between%20task%20planning%20and%20motion%0Aplanning%2C%20alleviating%20the%20intricate%20design%20process%20of%20handling%20domain-specific%0Amessages%20between%20them.%20Through%20a%20series%20of%20simulations%20in%20a%20box-packing%20domain%2C%0Awe%20quantitatively%20demonstrate%20the%20effectiveness%20of%20LLM%5E3%20in%20solving%20TAMP%0Aproblems%20and%20the%20efficiency%20in%20selecting%20action%20parameters.%20Ablation%20studies%0Aunderscore%20the%20significant%20contribution%20of%20motion%20failure%20reasoning%20to%20the%0Asuccess%20of%20LLM%5E3.%20Furthermore%2C%20we%20conduct%20qualitative%20experiments%20on%20a%20physical%0Amanipulator%2C%20demonstrating%20the%20practical%20applicability%20of%20our%20approach%20in%0Areal-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11552v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM3%3ALarge%20Language%20Model-based%20Task%20and%20Motion%20Planning%20with%20Motion%0A%20%20Failure%20Reasoning&entry.906535625=Shu%20Wang%20and%20Muzhi%20Han%20and%20Ziyuan%20Jiao%20and%20Zeyu%20Zhang%20and%20Ying%20Nian%20Wu%20and%20Song-Chun%20Zhu%20and%20Hangxin%20Liu&entry.1292438233=%20%20Conventional%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approaches%20rely%20on%20manually%0Acrafted%20interfaces%20connecting%20symbolic%20task%20planning%20with%20continuous%20motion%0Ageneration.%20These%20domain-specific%20and%20labor-intensive%20modules%20are%20limited%20in%0Aaddressing%20emerging%20tasks%20in%20real-world%20settings.%20Here%2C%20we%20present%20LLM%5E3%2C%20a%0Anovel%20Large%20Language%20Model%20%28LLM%29-based%20TAMP%20framework%20featuring%20a%0Adomain-independent%20interface.%20Specifically%2C%20we%20leverage%20the%20powerful%20reasoning%0Aand%20planning%20capabilities%20of%20pre-trained%20LLMs%20to%20propose%20symbolic%20action%0Asequences%20and%20select%20continuous%20action%20parameters%20for%20motion%20planning.%0ACrucially%2C%20LLM%5E3%20incorporates%20motion%20planning%20feedback%20through%20prompting%2C%0Aallowing%20the%20LLM%20to%20iteratively%20refine%20its%20proposals%20by%20reasoning%20about%20motion%0Afailure.%20Consequently%2C%20LLM%5E3%20interfaces%20between%20task%20planning%20and%20motion%0Aplanning%2C%20alleviating%20the%20intricate%20design%20process%20of%20handling%20domain-specific%0Amessages%20between%20them.%20Through%20a%20series%20of%20simulations%20in%20a%20box-packing%20domain%2C%0Awe%20quantitatively%20demonstrate%20the%20effectiveness%20of%20LLM%5E3%20in%20solving%20TAMP%0Aproblems%20and%20the%20efficiency%20in%20selecting%20action%20parameters.%20Ablation%20studies%0Aunderscore%20the%20significant%20contribution%20of%20motion%20failure%20reasoning%20to%20the%0Asuccess%20of%20LLM%5E3.%20Furthermore%2C%20we%20conduct%20qualitative%20experiments%20on%20a%20physical%0Amanipulator%2C%20demonstrating%20the%20practical%20applicability%20of%20our%20approach%20in%0Areal-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11552v2&entry.124074799=Read"},
{"title": "Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into\n  Vision Transformers", "author": "Yuyang Shu and Michael E. Bain", "abstract": "  Humans see low and high spatial frequency components at the same time, and\ncombine the information from both to form a visual scene. Drawing on this\nneuroscientific inspiration, we propose an altered Vision Transformer\narchitecture where patches from scaled down versions of the input image are\nadded to the input of the first Transformer Encoder layer. We name this model\nRetina Vision Transformer (RetinaViT) due to its inspiration from the human\nvisual system. Our experiments show that when trained on the ImageNet-1K\ndataset with a moderate configuration, RetinaViT achieves a 3.3% performance\nimprovement over the original ViT. We hypothesize that this improvement can be\nattributed to the inclusion of low spatial frequency components in the input,\nwhich improves the ability to capture structural features, and to select and\nforward important features to deeper layers. RetinaViT thereby opens doors to\nfurther investigations into vertical pathways and attention patterns.\n", "link": "http://arxiv.org/abs/2403.13677v1", "date": "2024-03-20", "relevancy": 2.2215, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.612}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers&body=Title%3A%20Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers%0AAuthor%3A%20Yuyang%20Shu%20and%20Michael%20E.%20Bain%0AAbstract%3A%20%20%20Humans%20see%20low%20and%20high%20spatial%20frequency%20components%20at%20the%20same%20time%2C%20and%0Acombine%20the%20information%20from%20both%20to%20form%20a%20visual%20scene.%20Drawing%20on%20this%0Aneuroscientific%20inspiration%2C%20we%20propose%20an%20altered%20Vision%20Transformer%0Aarchitecture%20where%20patches%20from%20scaled%20down%20versions%20of%20the%20input%20image%20are%0Aadded%20to%20the%20input%20of%20the%20first%20Transformer%20Encoder%20layer.%20We%20name%20this%20model%0ARetina%20Vision%20Transformer%20%28RetinaViT%29%20due%20to%20its%20inspiration%20from%20the%20human%0Avisual%20system.%20Our%20experiments%20show%20that%20when%20trained%20on%20the%20ImageNet-1K%0Adataset%20with%20a%20moderate%20configuration%2C%20RetinaViT%20achieves%20a%203.3%25%20performance%0Aimprovement%20over%20the%20original%20ViT.%20We%20hypothesize%20that%20this%20improvement%20can%20be%0Aattributed%20to%20the%20inclusion%20of%20low%20spatial%20frequency%20components%20in%20the%20input%2C%0Awhich%20improves%20the%20ability%20to%20capture%20structural%20features%2C%20and%20to%20select%20and%0Aforward%20important%20features%20to%20deeper%20layers.%20RetinaViT%20thereby%20opens%20doors%20to%0Afurther%20investigations%20into%20vertical%20pathways%20and%20attention%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13677v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retina%20Vision%20Transformer%20%28RetinaViT%29%3A%20Introducing%20Scaled%20Patches%20into%0A%20%20Vision%20Transformers&entry.906535625=Yuyang%20Shu%20and%20Michael%20E.%20Bain&entry.1292438233=%20%20Humans%20see%20low%20and%20high%20spatial%20frequency%20components%20at%20the%20same%20time%2C%20and%0Acombine%20the%20information%20from%20both%20to%20form%20a%20visual%20scene.%20Drawing%20on%20this%0Aneuroscientific%20inspiration%2C%20we%20propose%20an%20altered%20Vision%20Transformer%0Aarchitecture%20where%20patches%20from%20scaled%20down%20versions%20of%20the%20input%20image%20are%0Aadded%20to%20the%20input%20of%20the%20first%20Transformer%20Encoder%20layer.%20We%20name%20this%20model%0ARetina%20Vision%20Transformer%20%28RetinaViT%29%20due%20to%20its%20inspiration%20from%20the%20human%0Avisual%20system.%20Our%20experiments%20show%20that%20when%20trained%20on%20the%20ImageNet-1K%0Adataset%20with%20a%20moderate%20configuration%2C%20RetinaViT%20achieves%20a%203.3%25%20performance%0Aimprovement%20over%20the%20original%20ViT.%20We%20hypothesize%20that%20this%20improvement%20can%20be%0Aattributed%20to%20the%20inclusion%20of%20low%20spatial%20frequency%20components%20in%20the%20input%2C%0Awhich%20improves%20the%20ability%20to%20capture%20structural%20features%2C%20and%20to%20select%20and%0Aforward%20important%20features%20to%20deeper%20layers.%20RetinaViT%20thereby%20opens%20doors%20to%0Afurther%20investigations%20into%20vertical%20pathways%20and%20attention%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13677v1&entry.124074799=Read"},
{"title": "Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion", "author": "Lucas Nunes and Rodrigo Marcuzzi and Benedikt Mersch and Jens Behley and Cyrill Stachniss", "abstract": "  Computer vision techniques play a central role in the perception stack of\nautonomous vehicles. Such methods are employed to perceive the vehicle\nsurroundings given sensor data. 3D LiDAR sensors are commonly used to collect\nsparse 3D point clouds from the scene. However, compared to human perception,\nsuch systems struggle to deduce the unseen parts of the scene given those\nsparse point clouds. In this matter, the scene completion task aims at\npredicting the gaps in the LiDAR measurements to achieve a more complete scene\nrepresentation. Given the promising results of recent diffusion models as\ngenerative models for images, we propose extending them to achieve scene\ncompletion from a single 3D LiDAR scan. Previous works used diffusion models\nover range images extracted from LiDAR data, directly applying image-based\ndiffusion methods. Distinctly, we propose to directly operate on the points,\nreformulating the noising and denoising diffusion process such that it can\nefficiently work at scene scale. Together with our approach, we propose a\nregularization loss to stabilize the noise predicted during the denoising\nprocess. Our experimental evaluation shows that our method can complete the\nscene given a single LiDAR scan as input, producing a scene with more details\ncompared to state-of-the-art scene completion methods. We believe that our\nproposed diffusion process formulation can support further research in\ndiffusion models applied to scene-scale point cloud data.\n", "link": "http://arxiv.org/abs/2403.13470v1", "date": "2024-03-20", "relevancy": 2.1983, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5455}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5354}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scaling%20Diffusion%20Models%20to%20Real-World%203D%20LiDAR%20Scene%20Completion&body=Title%3A%20Scaling%20Diffusion%20Models%20to%20Real-World%203D%20LiDAR%20Scene%20Completion%0AAuthor%3A%20Lucas%20Nunes%20and%20Rodrigo%20Marcuzzi%20and%20Benedikt%20Mersch%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Computer%20vision%20techniques%20play%20a%20central%20role%20in%20the%20perception%20stack%20of%0Aautonomous%20vehicles.%20Such%20methods%20are%20employed%20to%20perceive%20the%20vehicle%0Asurroundings%20given%20sensor%20data.%203D%20LiDAR%20sensors%20are%20commonly%20used%20to%20collect%0Asparse%203D%20point%20clouds%20from%20the%20scene.%20However%2C%20compared%20to%20human%20perception%2C%0Asuch%20systems%20struggle%20to%20deduce%20the%20unseen%20parts%20of%20the%20scene%20given%20those%0Asparse%20point%20clouds.%20In%20this%20matter%2C%20the%20scene%20completion%20task%20aims%20at%0Apredicting%20the%20gaps%20in%20the%20LiDAR%20measurements%20to%20achieve%20a%20more%20complete%20scene%0Arepresentation.%20Given%20the%20promising%20results%20of%20recent%20diffusion%20models%20as%0Agenerative%20models%20for%20images%2C%20we%20propose%20extending%20them%20to%20achieve%20scene%0Acompletion%20from%20a%20single%203D%20LiDAR%20scan.%20Previous%20works%20used%20diffusion%20models%0Aover%20range%20images%20extracted%20from%20LiDAR%20data%2C%20directly%20applying%20image-based%0Adiffusion%20methods.%20Distinctly%2C%20we%20propose%20to%20directly%20operate%20on%20the%20points%2C%0Areformulating%20the%20noising%20and%20denoising%20diffusion%20process%20such%20that%20it%20can%0Aefficiently%20work%20at%20scene%20scale.%20Together%20with%20our%20approach%2C%20we%20propose%20a%0Aregularization%20loss%20to%20stabilize%20the%20noise%20predicted%20during%20the%20denoising%0Aprocess.%20Our%20experimental%20evaluation%20shows%20that%20our%20method%20can%20complete%20the%0Ascene%20given%20a%20single%20LiDAR%20scan%20as%20input%2C%20producing%20a%20scene%20with%20more%20details%0Acompared%20to%20state-of-the-art%20scene%20completion%20methods.%20We%20believe%20that%20our%0Aproposed%20diffusion%20process%20formulation%20can%20support%20further%20research%20in%0Adiffusion%20models%20applied%20to%20scene-scale%20point%20cloud%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13470v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Diffusion%20Models%20to%20Real-World%203D%20LiDAR%20Scene%20Completion&entry.906535625=Lucas%20Nunes%20and%20Rodrigo%20Marcuzzi%20and%20Benedikt%20Mersch%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Computer%20vision%20techniques%20play%20a%20central%20role%20in%20the%20perception%20stack%20of%0Aautonomous%20vehicles.%20Such%20methods%20are%20employed%20to%20perceive%20the%20vehicle%0Asurroundings%20given%20sensor%20data.%203D%20LiDAR%20sensors%20are%20commonly%20used%20to%20collect%0Asparse%203D%20point%20clouds%20from%20the%20scene.%20However%2C%20compared%20to%20human%20perception%2C%0Asuch%20systems%20struggle%20to%20deduce%20the%20unseen%20parts%20of%20the%20scene%20given%20those%0Asparse%20point%20clouds.%20In%20this%20matter%2C%20the%20scene%20completion%20task%20aims%20at%0Apredicting%20the%20gaps%20in%20the%20LiDAR%20measurements%20to%20achieve%20a%20more%20complete%20scene%0Arepresentation.%20Given%20the%20promising%20results%20of%20recent%20diffusion%20models%20as%0Agenerative%20models%20for%20images%2C%20we%20propose%20extending%20them%20to%20achieve%20scene%0Acompletion%20from%20a%20single%203D%20LiDAR%20scan.%20Previous%20works%20used%20diffusion%20models%0Aover%20range%20images%20extracted%20from%20LiDAR%20data%2C%20directly%20applying%20image-based%0Adiffusion%20methods.%20Distinctly%2C%20we%20propose%20to%20directly%20operate%20on%20the%20points%2C%0Areformulating%20the%20noising%20and%20denoising%20diffusion%20process%20such%20that%20it%20can%0Aefficiently%20work%20at%20scene%20scale.%20Together%20with%20our%20approach%2C%20we%20propose%20a%0Aregularization%20loss%20to%20stabilize%20the%20noise%20predicted%20during%20the%20denoising%0Aprocess.%20Our%20experimental%20evaluation%20shows%20that%20our%20method%20can%20complete%20the%0Ascene%20given%20a%20single%20LiDAR%20scan%20as%20input%2C%20producing%20a%20scene%20with%20more%20details%0Acompared%20to%20state-of-the-art%20scene%20completion%20methods.%20We%20believe%20that%20our%0Aproposed%20diffusion%20process%20formulation%20can%20support%20further%20research%20in%0Adiffusion%20models%20applied%20to%20scene-scale%20point%20cloud%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13470v1&entry.124074799=Read"},
{"title": "IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting", "author": "Hang Wang and Zhi-Qi Cheng and Youtian Du and Lei Zhang", "abstract": "  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n", "link": "http://arxiv.org/abs/2403.11959v2", "date": "2024-03-20", "relevancy": 2.188, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5505}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5348}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IVAC-P2L%3A%20Leveraging%20Irregular%20Repetition%20Priors%20for%20Improving%20Video%0A%20%20Action%20Counting&body=Title%3A%20IVAC-P2L%3A%20Leveraging%20Irregular%20Repetition%20Priors%20for%20Improving%20Video%0A%20%20Action%20Counting%0AAuthor%3A%20Hang%20Wang%20and%20Zhi-Qi%20Cheng%20and%20Youtian%20Du%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Video%20Action%20Counting%20%28VAC%29%20is%20crucial%20in%20analyzing%20sports%2C%20fitness%2C%20and%0Aeveryday%20activities%20by%20quantifying%20repetitive%20actions%20in%20videos.%20However%2C%0Atraditional%20VAC%20methods%20have%20overlooked%20the%20complexity%20of%20action%20repetitions%2C%0Asuch%20as%20interruptions%20and%20the%20variability%20in%20cycle%20duration.%20Our%20research%0Aaddresses%20the%20shortfall%20by%20introducing%20a%20novel%20approach%20to%20VAC%2C%20called%0AIrregular%20Video%20Action%20Counting%20%28IVAC%29.%20IVAC%20prioritizes%20modeling%20irregular%0Arepetition%20patterns%20in%20videos%2C%20which%20we%20define%20through%20two%20primary%20aspects%3A%0AInter-cycle%20Consistency%20and%20Cycle-interval%20Inconsistency.%20Inter-cycle%0AConsistency%20ensures%20homogeneity%20in%20the%20spatial-temporal%20representations%20of%0Acycle%20segments%2C%20signifying%20action%20uniformity%20within%20cycles.%20Cycle-interval%0Ainconsistency%20highlights%20the%20importance%20of%20distinguishing%20between%20cycle%0Asegments%20and%20intervals%20based%20on%20their%20inherent%20content%20differences.%20To%0Aencapsulate%20these%20principles%2C%20we%20propose%20a%20new%20methodology%20that%20includes%0Aconsistency%20and%20inconsistency%20modules%2C%20supported%20by%20a%20unique%20pull-push%20loss%0A%28P2L%29%20mechanism.%20The%20IVAC-P2L%20model%20applies%20a%20pull%20loss%20to%20promote%20coherence%0Aamong%20cycle%20segment%20features%20and%20a%20push%20loss%20to%20clearly%20distinguish%20features%20of%0Acycle%20segments%20from%20interval%20segments.%20Empirical%20evaluations%20conducted%20on%20the%0ARepCount%20dataset%20demonstrate%20that%20the%20IVAC-P2L%20model%20sets%20a%20new%20benchmark%20in%0AVAC%20task%20performance.%20Furthermore%2C%20the%20model%20demonstrates%20exceptional%0Aadaptability%20and%20generalization%20across%20various%20video%20contents%2C%20outperforming%0Aexisting%20models%20on%20two%20additional%20datasets%2C%20UCFRep%20and%20Countix%2C%20without%20the%0Aneed%20for%20dataset-specific%20optimization.%20These%20results%20confirm%20the%20efficacy%20of%0Aour%20approach%20in%20addressing%20irregular%20repetitions%20in%20videos%20and%20pave%20the%20way%20for%0Afurther%20advancements%20in%20video%20analysis%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11959v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IVAC-P2L%3A%20Leveraging%20Irregular%20Repetition%20Priors%20for%20Improving%20Video%0A%20%20Action%20Counting&entry.906535625=Hang%20Wang%20and%20Zhi-Qi%20Cheng%20and%20Youtian%20Du%20and%20Lei%20Zhang&entry.1292438233=%20%20Video%20Action%20Counting%20%28VAC%29%20is%20crucial%20in%20analyzing%20sports%2C%20fitness%2C%20and%0Aeveryday%20activities%20by%20quantifying%20repetitive%20actions%20in%20videos.%20However%2C%0Atraditional%20VAC%20methods%20have%20overlooked%20the%20complexity%20of%20action%20repetitions%2C%0Asuch%20as%20interruptions%20and%20the%20variability%20in%20cycle%20duration.%20Our%20research%0Aaddresses%20the%20shortfall%20by%20introducing%20a%20novel%20approach%20to%20VAC%2C%20called%0AIrregular%20Video%20Action%20Counting%20%28IVAC%29.%20IVAC%20prioritizes%20modeling%20irregular%0Arepetition%20patterns%20in%20videos%2C%20which%20we%20define%20through%20two%20primary%20aspects%3A%0AInter-cycle%20Consistency%20and%20Cycle-interval%20Inconsistency.%20Inter-cycle%0AConsistency%20ensures%20homogeneity%20in%20the%20spatial-temporal%20representations%20of%0Acycle%20segments%2C%20signifying%20action%20uniformity%20within%20cycles.%20Cycle-interval%0Ainconsistency%20highlights%20the%20importance%20of%20distinguishing%20between%20cycle%0Asegments%20and%20intervals%20based%20on%20their%20inherent%20content%20differences.%20To%0Aencapsulate%20these%20principles%2C%20we%20propose%20a%20new%20methodology%20that%20includes%0Aconsistency%20and%20inconsistency%20modules%2C%20supported%20by%20a%20unique%20pull-push%20loss%0A%28P2L%29%20mechanism.%20The%20IVAC-P2L%20model%20applies%20a%20pull%20loss%20to%20promote%20coherence%0Aamong%20cycle%20segment%20features%20and%20a%20push%20loss%20to%20clearly%20distinguish%20features%20of%0Acycle%20segments%20from%20interval%20segments.%20Empirical%20evaluations%20conducted%20on%20the%0ARepCount%20dataset%20demonstrate%20that%20the%20IVAC-P2L%20model%20sets%20a%20new%20benchmark%20in%0AVAC%20task%20performance.%20Furthermore%2C%20the%20model%20demonstrates%20exceptional%0Aadaptability%20and%20generalization%20across%20various%20video%20contents%2C%20outperforming%0Aexisting%20models%20on%20two%20additional%20datasets%2C%20UCFRep%20and%20Countix%2C%20without%20the%0Aneed%20for%20dataset-specific%20optimization.%20These%20results%20confirm%20the%20efficacy%20of%0Aour%20approach%20in%20addressing%20irregular%20repetitions%20in%20videos%20and%20pave%20the%20way%20for%0Afurther%20advancements%20in%20video%20analysis%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11959v2&entry.124074799=Read"},
{"title": "Step-Calibrated Diffusion for Biomedical Optical Image Restoration", "author": "Yiwei Lyu and Sung Jik Cha and Cheng Jiang and Asadur Chowdury and Xinhai Hou and Edward Harake and Akhil Kondepudi and Christian Freudiger and Honglak Lee and Todd C. Hollon", "abstract": "  High-quality, high-resolution medical imaging is essential for clinical care.\nRaman-based biomedical optical imaging uses non-ionizing infrared radiation to\nevaluate human tissues in real time and is used for early cancer detection,\nbrain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,\noptical imaging is vulnerable to image degradation due to laser scattering and\nabsorption, which can result in diagnostic errors and misguided treatment.\nRestoration of optical images is a challenging computer vision task because the\nsources of image degradation are multi-factorial, stochastic, and\ntissue-dependent, preventing a straightforward method to obtain paired\nlow-quality/high-quality data. Here, we present Restorative Step-Calibrated\nDiffusion (RSCD), an unpaired image restoration method that views the image\nrestoration problem as completing the finishing steps of a diffusion-based\nimage generation task. RSCD uses a step calibrator model to dynamically\ndetermine the severity of image degradation and the number of steps required to\ncomplete the reverse diffusion process for image restoration. RSCD outperforms\nother widely used unpaired image restoration methods on both image quality and\nperceptual evaluation metrics for restoring optical images. Medical imaging\nexperts consistently prefer images restored using RSCD in blinded comparison\nexperiments and report minimal to no hallucinations. Finally, we show that RSCD\nimproves performance on downstream clinical imaging tasks, including automated\nbrain tumor diagnosis and deep tissue imaging. Our code is available at\nhttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.\n", "link": "http://arxiv.org/abs/2403.13680v1", "date": "2024-03-20", "relevancy": 2.1862, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5597}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5415}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Step-Calibrated%20Diffusion%20for%20Biomedical%20Optical%20Image%20Restoration&body=Title%3A%20Step-Calibrated%20Diffusion%20for%20Biomedical%20Optical%20Image%20Restoration%0AAuthor%3A%20Yiwei%20Lyu%20and%20Sung%20Jik%20Cha%20and%20Cheng%20Jiang%20and%20Asadur%20Chowdury%20and%20Xinhai%20Hou%20and%20Edward%20Harake%20and%20Akhil%20Kondepudi%20and%20Christian%20Freudiger%20and%20Honglak%20Lee%20and%20Todd%20C.%20Hollon%0AAbstract%3A%20%20%20High-quality%2C%20high-resolution%20medical%20imaging%20is%20essential%20for%20clinical%20care.%0ARaman-based%20biomedical%20optical%20imaging%20uses%20non-ionizing%20infrared%20radiation%20to%0Aevaluate%20human%20tissues%20in%20real%20time%20and%20is%20used%20for%20early%20cancer%20detection%2C%0Abrain%20tumor%20diagnosis%2C%20and%20intraoperative%20tissue%20analysis.%20Unfortunately%2C%0Aoptical%20imaging%20is%20vulnerable%20to%20image%20degradation%20due%20to%20laser%20scattering%20and%0Aabsorption%2C%20which%20can%20result%20in%20diagnostic%20errors%20and%20misguided%20treatment.%0ARestoration%20of%20optical%20images%20is%20a%20challenging%20computer%20vision%20task%20because%20the%0Asources%20of%20image%20degradation%20are%20multi-factorial%2C%20stochastic%2C%20and%0Atissue-dependent%2C%20preventing%20a%20straightforward%20method%20to%20obtain%20paired%0Alow-quality/high-quality%20data.%20Here%2C%20we%20present%20Restorative%20Step-Calibrated%0ADiffusion%20%28RSCD%29%2C%20an%20unpaired%20image%20restoration%20method%20that%20views%20the%20image%0Arestoration%20problem%20as%20completing%20the%20finishing%20steps%20of%20a%20diffusion-based%0Aimage%20generation%20task.%20RSCD%20uses%20a%20step%20calibrator%20model%20to%20dynamically%0Adetermine%20the%20severity%20of%20image%20degradation%20and%20the%20number%20of%20steps%20required%20to%0Acomplete%20the%20reverse%20diffusion%20process%20for%20image%20restoration.%20RSCD%20outperforms%0Aother%20widely%20used%20unpaired%20image%20restoration%20methods%20on%20both%20image%20quality%20and%0Aperceptual%20evaluation%20metrics%20for%20restoring%20optical%20images.%20Medical%20imaging%0Aexperts%20consistently%20prefer%20images%20restored%20using%20RSCD%20in%20blinded%20comparison%0Aexperiments%20and%20report%20minimal%20to%20no%20hallucinations.%20Finally%2C%20we%20show%20that%20RSCD%0Aimproves%20performance%20on%20downstream%20clinical%20imaging%20tasks%2C%20including%20automated%0Abrain%20tumor%20diagnosis%20and%20deep%20tissue%20imaging.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/MLNeurosurg/restorative_step-calibrated_diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13680v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-Calibrated%20Diffusion%20for%20Biomedical%20Optical%20Image%20Restoration&entry.906535625=Yiwei%20Lyu%20and%20Sung%20Jik%20Cha%20and%20Cheng%20Jiang%20and%20Asadur%20Chowdury%20and%20Xinhai%20Hou%20and%20Edward%20Harake%20and%20Akhil%20Kondepudi%20and%20Christian%20Freudiger%20and%20Honglak%20Lee%20and%20Todd%20C.%20Hollon&entry.1292438233=%20%20High-quality%2C%20high-resolution%20medical%20imaging%20is%20essential%20for%20clinical%20care.%0ARaman-based%20biomedical%20optical%20imaging%20uses%20non-ionizing%20infrared%20radiation%20to%0Aevaluate%20human%20tissues%20in%20real%20time%20and%20is%20used%20for%20early%20cancer%20detection%2C%0Abrain%20tumor%20diagnosis%2C%20and%20intraoperative%20tissue%20analysis.%20Unfortunately%2C%0Aoptical%20imaging%20is%20vulnerable%20to%20image%20degradation%20due%20to%20laser%20scattering%20and%0Aabsorption%2C%20which%20can%20result%20in%20diagnostic%20errors%20and%20misguided%20treatment.%0ARestoration%20of%20optical%20images%20is%20a%20challenging%20computer%20vision%20task%20because%20the%0Asources%20of%20image%20degradation%20are%20multi-factorial%2C%20stochastic%2C%20and%0Atissue-dependent%2C%20preventing%20a%20straightforward%20method%20to%20obtain%20paired%0Alow-quality/high-quality%20data.%20Here%2C%20we%20present%20Restorative%20Step-Calibrated%0ADiffusion%20%28RSCD%29%2C%20an%20unpaired%20image%20restoration%20method%20that%20views%20the%20image%0Arestoration%20problem%20as%20completing%20the%20finishing%20steps%20of%20a%20diffusion-based%0Aimage%20generation%20task.%20RSCD%20uses%20a%20step%20calibrator%20model%20to%20dynamically%0Adetermine%20the%20severity%20of%20image%20degradation%20and%20the%20number%20of%20steps%20required%20to%0Acomplete%20the%20reverse%20diffusion%20process%20for%20image%20restoration.%20RSCD%20outperforms%0Aother%20widely%20used%20unpaired%20image%20restoration%20methods%20on%20both%20image%20quality%20and%0Aperceptual%20evaluation%20metrics%20for%20restoring%20optical%20images.%20Medical%20imaging%0Aexperts%20consistently%20prefer%20images%20restored%20using%20RSCD%20in%20blinded%20comparison%0Aexperiments%20and%20report%20minimal%20to%20no%20hallucinations.%20Finally%2C%20we%20show%20that%20RSCD%0Aimproves%20performance%20on%20downstream%20clinical%20imaging%20tasks%2C%20including%20automated%0Abrain%20tumor%20diagnosis%20and%20deep%20tissue%20imaging.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/MLNeurosurg/restorative_step-calibrated_diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13680v1&entry.124074799=Read"},
{"title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning", "author": "Haozhe Zhao and Zefan Cai and Shuzheng Si and Xiaojian Ma and Kaikai An and Liang Chen and Zixuan Liu and Sheng Wang and Wenjuan Han and Baobao Chang", "abstract": "  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n", "link": "http://arxiv.org/abs/2309.07915v3", "date": "2024-03-20", "relevancy": 2.1851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5142}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5097}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MMICL%3A%20Empowering%20Vision-language%20Model%20with%20Multi-Modal%20In-Context%0A%20%20Learning&body=Title%3A%20MMICL%3A%20Empowering%20Vision-language%20Model%20with%20Multi-Modal%20In-Context%0A%20%20Learning%0AAuthor%3A%20Haozhe%20Zhao%20and%20Zefan%20Cai%20and%20Shuzheng%20Si%20and%20Xiaojian%20Ma%20and%20Kaikai%20An%20and%20Liang%20Chen%20and%20Zixuan%20Liu%20and%20Sheng%20Wang%20and%20Wenjuan%20Han%20and%20Baobao%20Chang%0AAbstract%3A%20%20%20Since%20the%20resurgence%20of%20deep%20learning%2C%20vision-language%20models%20%28VLMs%29%20enhanced%0Aby%20large%20language%20models%20%28LLMs%29%20have%20grown%20exponentially%20in%20popularity.%0AHowever%2C%20while%20LLMs%20can%20utilize%20extensive%20background%20knowledge%20and%20task%0Ainformation%20with%20in-context%20learning%2C%20most%20VLMs%20still%20struggle%20with%0Aunderstanding%20complex%20multi-modal%20prompts%20with%20multiple%20images%2C%20making%20VLMs%0Aless%20effective%20in%20downstream%20vision-language%20tasks.%20In%20this%20paper%2C%20we%20address%0Athe%20limitation%20above%20by%201%29%20introducing%20vision-language%20Model%20with%20Multi-Modal%0AIn-Context%20Learning%28MMICL%29%2C%20a%20new%20approach%20to%20allow%20the%20VLM%20to%20deal%20with%0Amulti-modal%20inputs%20efficiently%3B%202%29%20proposing%20a%20novel%20context%20scheme%20to%20augment%0Athe%20in-context%20learning%20ability%20of%20the%20VLM%3B%203%29%20constructing%20the%20Multi-modal%0AIn-Context%20Learning%20%28MIC%29%20dataset%2C%20designed%20to%20enhance%20the%20VLM%27s%20ability%20to%0Aunderstand%20complex%20multi-modal%20prompts.%20Our%20experiments%20confirm%20that%20MMICL%0Aachieves%20new%20state-of-the-art%20zero-shot%20performance%20on%20a%20wide%20range%20of%20general%0Avision-language%20tasks%2C%20especially%20for%20complex%20benchmarks%2C%20including%20MME%20and%0AMMBench.%20Our%20analysis%20demonstrates%20that%20MMICL%20effectively%20tackles%20the%20challenge%0Aof%20complex%20multi-modal%20prompt%20understanding%20and%20emerges%20the%20impressive%20ICL%0Aability.%20Furthermore%2C%20we%20observe%20that%20MMICL%20successfully%20alleviates%20language%0Abias%20in%20VLMs%2C%20a%20common%20issue%20for%20VLMs%20that%20often%20leads%20to%20hallucination%20when%0Afaced%20with%20extensive%20textual%20context.%20Our%20code%2C%20dataset%2C%20dataset%20tool%2C%20and%0Amodel%20are%20available%20at%20https%3A//github.com/PKUnlp-icler/MIC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07915v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMICL%3A%20Empowering%20Vision-language%20Model%20with%20Multi-Modal%20In-Context%0A%20%20Learning&entry.906535625=Haozhe%20Zhao%20and%20Zefan%20Cai%20and%20Shuzheng%20Si%20and%20Xiaojian%20Ma%20and%20Kaikai%20An%20and%20Liang%20Chen%20and%20Zixuan%20Liu%20and%20Sheng%20Wang%20and%20Wenjuan%20Han%20and%20Baobao%20Chang&entry.1292438233=%20%20Since%20the%20resurgence%20of%20deep%20learning%2C%20vision-language%20models%20%28VLMs%29%20enhanced%0Aby%20large%20language%20models%20%28LLMs%29%20have%20grown%20exponentially%20in%20popularity.%0AHowever%2C%20while%20LLMs%20can%20utilize%20extensive%20background%20knowledge%20and%20task%0Ainformation%20with%20in-context%20learning%2C%20most%20VLMs%20still%20struggle%20with%0Aunderstanding%20complex%20multi-modal%20prompts%20with%20multiple%20images%2C%20making%20VLMs%0Aless%20effective%20in%20downstream%20vision-language%20tasks.%20In%20this%20paper%2C%20we%20address%0Athe%20limitation%20above%20by%201%29%20introducing%20vision-language%20Model%20with%20Multi-Modal%0AIn-Context%20Learning%28MMICL%29%2C%20a%20new%20approach%20to%20allow%20the%20VLM%20to%20deal%20with%0Amulti-modal%20inputs%20efficiently%3B%202%29%20proposing%20a%20novel%20context%20scheme%20to%20augment%0Athe%20in-context%20learning%20ability%20of%20the%20VLM%3B%203%29%20constructing%20the%20Multi-modal%0AIn-Context%20Learning%20%28MIC%29%20dataset%2C%20designed%20to%20enhance%20the%20VLM%27s%20ability%20to%0Aunderstand%20complex%20multi-modal%20prompts.%20Our%20experiments%20confirm%20that%20MMICL%0Aachieves%20new%20state-of-the-art%20zero-shot%20performance%20on%20a%20wide%20range%20of%20general%0Avision-language%20tasks%2C%20especially%20for%20complex%20benchmarks%2C%20including%20MME%20and%0AMMBench.%20Our%20analysis%20demonstrates%20that%20MMICL%20effectively%20tackles%20the%20challenge%0Aof%20complex%20multi-modal%20prompt%20understanding%20and%20emerges%20the%20impressive%20ICL%0Aability.%20Furthermore%2C%20we%20observe%20that%20MMICL%20successfully%20alleviates%20language%0Abias%20in%20VLMs%2C%20a%20common%20issue%20for%20VLMs%20that%20often%20leads%20to%20hallucination%20when%0Afaced%20with%20extensive%20textual%20context.%20Our%20code%2C%20dataset%2C%20dataset%20tool%2C%20and%0Amodel%20are%20available%20at%20https%3A//github.com/PKUnlp-icler/MIC%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07915v3&entry.124074799=Read"},
{"title": "FACT: Fast and Active Coordinate Initialization for Vision-based Drone\n  Swarms", "author": "Yuan Li and Anke Zhao and Yingjian Wang and Ziyi Xu and Xin Zhou and Jinni Zhou and Chao Xu and Fei Gao", "abstract": "  Swarm robots have sparked remarkable developments across a range of fields.\nWhile it is necessary for various applications in swarm robots, a fast and\nrobust coordinate initialization in vision-based drone swarms remains elusive.\nTo this end, our paper proposes a complete system to recover a swarm's initial\nrelative pose on platforms with size, weight, and power (SWaP) constraints. To\novercome limited coverage of field-of-view (FoV), the drones rotate in place to\nobtain observations. To tackle the anonymous measurements, we formulate a\nnon-convex rotation estimation problem and transform it into a semi-definite\nprogramming (SDP) problem, which can steadily obtain global optimal values.\nThen we utilize the Hungarian algorithm to recover relative translation and\ncorrespondences between observations and drone identities. To safely acquire\ncomplete observations, we actively search for positions and generate feasible\ntrajectories to avoid collisions. To validate the practicability of our system,\nwe conduct experiments on a vision-based drone swarm with only stereo cameras\nand inertial measurement units (IMUs) as sensors. The results demonstrate that\nthe system can robustly get accurate relative poses in real time with limited\nonboard computation resources. The source code is released.\n", "link": "http://arxiv.org/abs/2403.13455v1", "date": "2024-03-20", "relevancy": 2.1773, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FACT%3A%20Fast%20and%20Active%20Coordinate%20Initialization%20for%20Vision-based%20Drone%0A%20%20Swarms&body=Title%3A%20FACT%3A%20Fast%20and%20Active%20Coordinate%20Initialization%20for%20Vision-based%20Drone%0A%20%20Swarms%0AAuthor%3A%20Yuan%20Li%20and%20Anke%20Zhao%20and%20Yingjian%20Wang%20and%20Ziyi%20Xu%20and%20Xin%20Zhou%20and%20Jinni%20Zhou%20and%20Chao%20Xu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Swarm%20robots%20have%20sparked%20remarkable%20developments%20across%20a%20range%20of%20fields.%0AWhile%20it%20is%20necessary%20for%20various%20applications%20in%20swarm%20robots%2C%20a%20fast%20and%0Arobust%20coordinate%20initialization%20in%20vision-based%20drone%20swarms%20remains%20elusive.%0ATo%20this%20end%2C%20our%20paper%20proposes%20a%20complete%20system%20to%20recover%20a%20swarm%27s%20initial%0Arelative%20pose%20on%20platforms%20with%20size%2C%20weight%2C%20and%20power%20%28SWaP%29%20constraints.%20To%0Aovercome%20limited%20coverage%20of%20field-of-view%20%28FoV%29%2C%20the%20drones%20rotate%20in%20place%20to%0Aobtain%20observations.%20To%20tackle%20the%20anonymous%20measurements%2C%20we%20formulate%20a%0Anon-convex%20rotation%20estimation%20problem%20and%20transform%20it%20into%20a%20semi-definite%0Aprogramming%20%28SDP%29%20problem%2C%20which%20can%20steadily%20obtain%20global%20optimal%20values.%0AThen%20we%20utilize%20the%20Hungarian%20algorithm%20to%20recover%20relative%20translation%20and%0Acorrespondences%20between%20observations%20and%20drone%20identities.%20To%20safely%20acquire%0Acomplete%20observations%2C%20we%20actively%20search%20for%20positions%20and%20generate%20feasible%0Atrajectories%20to%20avoid%20collisions.%20To%20validate%20the%20practicability%20of%20our%20system%2C%0Awe%20conduct%20experiments%20on%20a%20vision-based%20drone%20swarm%20with%20only%20stereo%20cameras%0Aand%20inertial%20measurement%20units%20%28IMUs%29%20as%20sensors.%20The%20results%20demonstrate%20that%0Athe%20system%20can%20robustly%20get%20accurate%20relative%20poses%20in%20real%20time%20with%20limited%0Aonboard%20computation%20resources.%20The%20source%20code%20is%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13455v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FACT%3A%20Fast%20and%20Active%20Coordinate%20Initialization%20for%20Vision-based%20Drone%0A%20%20Swarms&entry.906535625=Yuan%20Li%20and%20Anke%20Zhao%20and%20Yingjian%20Wang%20and%20Ziyi%20Xu%20and%20Xin%20Zhou%20and%20Jinni%20Zhou%20and%20Chao%20Xu%20and%20Fei%20Gao&entry.1292438233=%20%20Swarm%20robots%20have%20sparked%20remarkable%20developments%20across%20a%20range%20of%20fields.%0AWhile%20it%20is%20necessary%20for%20various%20applications%20in%20swarm%20robots%2C%20a%20fast%20and%0Arobust%20coordinate%20initialization%20in%20vision-based%20drone%20swarms%20remains%20elusive.%0ATo%20this%20end%2C%20our%20paper%20proposes%20a%20complete%20system%20to%20recover%20a%20swarm%27s%20initial%0Arelative%20pose%20on%20platforms%20with%20size%2C%20weight%2C%20and%20power%20%28SWaP%29%20constraints.%20To%0Aovercome%20limited%20coverage%20of%20field-of-view%20%28FoV%29%2C%20the%20drones%20rotate%20in%20place%20to%0Aobtain%20observations.%20To%20tackle%20the%20anonymous%20measurements%2C%20we%20formulate%20a%0Anon-convex%20rotation%20estimation%20problem%20and%20transform%20it%20into%20a%20semi-definite%0Aprogramming%20%28SDP%29%20problem%2C%20which%20can%20steadily%20obtain%20global%20optimal%20values.%0AThen%20we%20utilize%20the%20Hungarian%20algorithm%20to%20recover%20relative%20translation%20and%0Acorrespondences%20between%20observations%20and%20drone%20identities.%20To%20safely%20acquire%0Acomplete%20observations%2C%20we%20actively%20search%20for%20positions%20and%20generate%20feasible%0Atrajectories%20to%20avoid%20collisions.%20To%20validate%20the%20practicability%20of%20our%20system%2C%0Awe%20conduct%20experiments%20on%20a%20vision-based%20drone%20swarm%20with%20only%20stereo%20cameras%0Aand%20inertial%20measurement%20units%20%28IMUs%29%20as%20sensors.%20The%20results%20demonstrate%20that%0Athe%20system%20can%20robustly%20get%20accurate%20relative%20poses%20in%20real%20time%20with%20limited%0Aonboard%20computation%20resources.%20The%20source%20code%20is%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13455v1&entry.124074799=Read"},
{"title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints", "author": "Francesco Corti and Balz Maag and Joachim Schauer and Ulrich Pferschy and Olga Saukh", "abstract": "  Deep models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models, not capable to\nadapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks\n(REDS) to tackle model adaptation to variable resources. In contrast to the\nstate-of-the-art, REDS use structured sparsity constructively by exploiting\npermutation invariance of neurons, which allows for hardware-specific\noptimizations. Specifically, REDS achieve computational efficiency by (1)\nskipping sequential computational blocks identified by a novel iterative\nknapsack optimizer, and (2) leveraging simple math to re-arrange the order of\noperations in REDS computational graph to take advantage of the data cache.\nREDS support conventional deep networks frequently deployed on the edge and\nprovide computational benefits even for small and simple networks. We evaluate\nREDS on seven benchmark architectures trained on the Visual Wake Words, Google\nSpeech Commands, Fashion-MNIST and CIFAR10 datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence for REDS outstanding performance in terms of\nsubmodels' test set accuracy, and demonstrate an adaptation time in response to\ndynamic resource constraints of under 40$\\mu$s, utilizing a 2-layer\nfully-connected network on Arduino Nano 33 BLE.\n", "link": "http://arxiv.org/abs/2311.13349v2", "date": "2024-03-20", "relevancy": 2.168, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5759}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5014}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints&body=Title%3A%20REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints%0AAuthor%3A%20Francesco%20Corti%20and%20Balz%20Maag%20and%20Joachim%20Schauer%20and%20Ulrich%20Pferschy%20and%20Olga%20Saukh%0AAbstract%3A%20%20%20Deep%20models%20deployed%20on%20edge%20devices%20frequently%20encounter%20resource%0Avariability%2C%20which%20arises%20from%20fluctuating%20energy%20levels%2C%20timing%20constraints%2C%0Aor%20prioritization%20of%20other%20critical%20tasks%20within%20the%20system.%20State-of-the-art%0Amachine%20learning%20pipelines%20generate%20resource-agnostic%20models%2C%20not%20capable%20to%0Aadapt%20at%20runtime.%20In%20this%20work%20we%20introduce%20Resource-Efficient%20Deep%20Subnetworks%0A%28REDS%29%20to%20tackle%20model%20adaptation%20to%20variable%20resources.%20In%20contrast%20to%20the%0Astate-of-the-art%2C%20REDS%20use%20structured%20sparsity%20constructively%20by%20exploiting%0Apermutation%20invariance%20of%20neurons%2C%20which%20allows%20for%20hardware-specific%0Aoptimizations.%20Specifically%2C%20REDS%20achieve%20computational%20efficiency%20by%20%281%29%0Askipping%20sequential%20computational%20blocks%20identified%20by%20a%20novel%20iterative%0Aknapsack%20optimizer%2C%20and%20%282%29%20leveraging%20simple%20math%20to%20re-arrange%20the%20order%20of%0Aoperations%20in%20REDS%20computational%20graph%20to%20take%20advantage%20of%20the%20data%20cache.%0AREDS%20support%20conventional%20deep%20networks%20frequently%20deployed%20on%20the%20edge%20and%0Aprovide%20computational%20benefits%20even%20for%20small%20and%20simple%20networks.%20We%20evaluate%0AREDS%20on%20seven%20benchmark%20architectures%20trained%20on%20the%20Visual%20Wake%20Words%2C%20Google%0ASpeech%20Commands%2C%20Fashion-MNIST%20and%20CIFAR10%20datasets%2C%20and%20test%20on%20four%0Aoff-the-shelf%20mobile%20and%20embedded%20hardware%20platforms.%20We%20provide%20a%20theoretical%0Aresult%20and%20empirical%20evidence%20for%20REDS%20outstanding%20performance%20in%20terms%20of%0Asubmodels%27%20test%20set%20accuracy%2C%20and%20demonstrate%20an%20adaptation%20time%20in%20response%20to%0Adynamic%20resource%20constraints%20of%20under%2040%24%5Cmu%24s%2C%20utilizing%20a%202-layer%0Afully-connected%20network%20on%20Arduino%20Nano%2033%20BLE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13349v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints&entry.906535625=Francesco%20Corti%20and%20Balz%20Maag%20and%20Joachim%20Schauer%20and%20Ulrich%20Pferschy%20and%20Olga%20Saukh&entry.1292438233=%20%20Deep%20models%20deployed%20on%20edge%20devices%20frequently%20encounter%20resource%0Avariability%2C%20which%20arises%20from%20fluctuating%20energy%20levels%2C%20timing%20constraints%2C%0Aor%20prioritization%20of%20other%20critical%20tasks%20within%20the%20system.%20State-of-the-art%0Amachine%20learning%20pipelines%20generate%20resource-agnostic%20models%2C%20not%20capable%20to%0Aadapt%20at%20runtime.%20In%20this%20work%20we%20introduce%20Resource-Efficient%20Deep%20Subnetworks%0A%28REDS%29%20to%20tackle%20model%20adaptation%20to%20variable%20resources.%20In%20contrast%20to%20the%0Astate-of-the-art%2C%20REDS%20use%20structured%20sparsity%20constructively%20by%20exploiting%0Apermutation%20invariance%20of%20neurons%2C%20which%20allows%20for%20hardware-specific%0Aoptimizations.%20Specifically%2C%20REDS%20achieve%20computational%20efficiency%20by%20%281%29%0Askipping%20sequential%20computational%20blocks%20identified%20by%20a%20novel%20iterative%0Aknapsack%20optimizer%2C%20and%20%282%29%20leveraging%20simple%20math%20to%20re-arrange%20the%20order%20of%0Aoperations%20in%20REDS%20computational%20graph%20to%20take%20advantage%20of%20the%20data%20cache.%0AREDS%20support%20conventional%20deep%20networks%20frequently%20deployed%20on%20the%20edge%20and%0Aprovide%20computational%20benefits%20even%20for%20small%20and%20simple%20networks.%20We%20evaluate%0AREDS%20on%20seven%20benchmark%20architectures%20trained%20on%20the%20Visual%20Wake%20Words%2C%20Google%0ASpeech%20Commands%2C%20Fashion-MNIST%20and%20CIFAR10%20datasets%2C%20and%20test%20on%20four%0Aoff-the-shelf%20mobile%20and%20embedded%20hardware%20platforms.%20We%20provide%20a%20theoretical%0Aresult%20and%20empirical%20evidence%20for%20REDS%20outstanding%20performance%20in%20terms%20of%0Asubmodels%27%20test%20set%20accuracy%2C%20and%20demonstrate%20an%20adaptation%20time%20in%20response%20to%0Adynamic%20resource%20constraints%20of%20under%2040%24%5Cmu%24s%2C%20utilizing%20a%202-layer%0Afully-connected%20network%20on%20Arduino%20Nano%2033%20BLE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13349v2&entry.124074799=Read"},
{"title": "Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and\n  Fidelity for All-in-One Image Restoration", "author": "Yuang Ai and Huaibo Huang and Xiaoqiang Zhou and Jiexiang Wang and Ran He", "abstract": "  Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks underscore the superiority of MPerceiver in terms of\nadaptiveness, generalizability and fidelity.\n", "link": "http://arxiv.org/abs/2312.02918v2", "date": "2024-03-20", "relevancy": 2.162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Prompt%20Perceiver%3A%20Empower%20Adaptiveness%2C%20Generalizability%20and%0A%20%20Fidelity%20for%20All-in-One%20Image%20Restoration&body=Title%3A%20Multimodal%20Prompt%20Perceiver%3A%20Empower%20Adaptiveness%2C%20Generalizability%20and%0A%20%20Fidelity%20for%20All-in-One%20Image%20Restoration%0AAuthor%3A%20Yuang%20Ai%20and%20Huaibo%20Huang%20and%20Xiaoqiang%20Zhou%20and%20Jiexiang%20Wang%20and%20Ran%20He%0AAbstract%3A%20%20%20Despite%20substantial%20progress%2C%20all-in-one%20image%20restoration%20%28IR%29%20grapples%20with%0Apersistent%20challenges%20in%20handling%20intricate%20real-world%20degradations.%20This%20paper%0Aintroduces%20MPerceiver%3A%20a%20novel%20multimodal%20prompt%20learning%20approach%20that%0Aharnesses%20Stable%20Diffusion%20%28SD%29%20priors%20to%20enhance%20adaptiveness%2C%0Ageneralizability%20and%20fidelity%20for%20all-in-one%20image%20restoration.%20Specifically%2C%0Awe%20develop%20a%20dual-branch%20module%20to%20master%20two%20types%20of%20SD%20prompts%3A%20textual%20for%0Aholistic%20representation%20and%20visual%20for%20multiscale%20detail%20representation.%20Both%0Aprompts%20are%20dynamically%20adjusted%20by%20degradation%20predictions%20from%20the%20CLIP%20image%0Aencoder%2C%20enabling%20adaptive%20responses%20to%20diverse%20unknown%20degradations.%20Moreover%2C%0Aa%20plug-in%20detail%20refinement%20module%20improves%20restoration%20fidelity%20via%20direct%0Aencoder-to-decoder%20information%20transformation.%20To%20assess%20our%20method%2C%20MPerceiver%0Ais%20trained%20on%209%20tasks%20for%20all-in-one%20IR%20and%20outperforms%20state-of-the-art%0Atask-specific%20methods%20across%20most%20tasks.%20Post%20multitask%20pre-training%2C%0AMPerceiver%20attains%20a%20generalized%20representation%20in%20low-level%20vision%2C%20exhibiting%0Aremarkable%20zero-shot%20and%20few-shot%20capabilities%20in%20unseen%20tasks.%20Extensive%0Aexperiments%20on%2016%20IR%20tasks%20underscore%20the%20superiority%20of%20MPerceiver%20in%20terms%20of%0Aadaptiveness%2C%20generalizability%20and%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02918v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Prompt%20Perceiver%3A%20Empower%20Adaptiveness%2C%20Generalizability%20and%0A%20%20Fidelity%20for%20All-in-One%20Image%20Restoration&entry.906535625=Yuang%20Ai%20and%20Huaibo%20Huang%20and%20Xiaoqiang%20Zhou%20and%20Jiexiang%20Wang%20and%20Ran%20He&entry.1292438233=%20%20Despite%20substantial%20progress%2C%20all-in-one%20image%20restoration%20%28IR%29%20grapples%20with%0Apersistent%20challenges%20in%20handling%20intricate%20real-world%20degradations.%20This%20paper%0Aintroduces%20MPerceiver%3A%20a%20novel%20multimodal%20prompt%20learning%20approach%20that%0Aharnesses%20Stable%20Diffusion%20%28SD%29%20priors%20to%20enhance%20adaptiveness%2C%0Ageneralizability%20and%20fidelity%20for%20all-in-one%20image%20restoration.%20Specifically%2C%0Awe%20develop%20a%20dual-branch%20module%20to%20master%20two%20types%20of%20SD%20prompts%3A%20textual%20for%0Aholistic%20representation%20and%20visual%20for%20multiscale%20detail%20representation.%20Both%0Aprompts%20are%20dynamically%20adjusted%20by%20degradation%20predictions%20from%20the%20CLIP%20image%0Aencoder%2C%20enabling%20adaptive%20responses%20to%20diverse%20unknown%20degradations.%20Moreover%2C%0Aa%20plug-in%20detail%20refinement%20module%20improves%20restoration%20fidelity%20via%20direct%0Aencoder-to-decoder%20information%20transformation.%20To%20assess%20our%20method%2C%20MPerceiver%0Ais%20trained%20on%209%20tasks%20for%20all-in-one%20IR%20and%20outperforms%20state-of-the-art%0Atask-specific%20methods%20across%20most%20tasks.%20Post%20multitask%20pre-training%2C%0AMPerceiver%20attains%20a%20generalized%20representation%20in%20low-level%20vision%2C%20exhibiting%0Aremarkable%20zero-shot%20and%20few-shot%20capabilities%20in%20unseen%20tasks.%20Extensive%0Aexperiments%20on%2016%20IR%20tasks%20underscore%20the%20superiority%20of%20MPerceiver%20in%20terms%20of%0Aadaptiveness%2C%20generalizability%20and%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02918v2&entry.124074799=Read"},
{"title": "Improved Baselines for Data-efficient Perceptual Augmentation of LLMs", "author": "Th\u00e9ophane Vallaeys and Mustafa Shukor and Matthieu Cord and Jakob Verbeek", "abstract": "  The abilities of large language models (LLMs) have recently progressed to\nunprecedented levels, paving the way to novel applications in a wide variety of\nareas. In computer vision, LLMs can be used to prime vision-language tasks such\nimage captioning and visual question answering when coupled with pre-trained\nvision backbones. While different approaches have been explored to interface\nLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,\nthey are often explored for different tasks, different datasets, and using\ndifferent perceptual backbones and language models, hindering direct comparison\nof the interfacing mechanisms. To remedy this lack of comparability between\nmethods, we present an extensive experimental evaluation of different\ninterfacing mechanisms, across multiple tasks (including image, video, and\naudio captioning as well as visual question answering), datasets and backbones,\npaying special attention to low-data settings. We find improved performance\nusing existing mechanisms over state-of-the-art results, and identify a new\ninterfacing mechanism that yields (near) optimal results across different\ntasks, while obtaining a 4x reduction in training time.\n", "link": "http://arxiv.org/abs/2403.13499v1", "date": "2024-03-20", "relevancy": 2.1585, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5254}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improved%20Baselines%20for%20Data-efficient%20Perceptual%20Augmentation%20of%20LLMs&body=Title%3A%20Improved%20Baselines%20for%20Data-efficient%20Perceptual%20Augmentation%20of%20LLMs%0AAuthor%3A%20Th%C3%A9ophane%20Vallaeys%20and%20Mustafa%20Shukor%20and%20Matthieu%20Cord%20and%20Jakob%20Verbeek%0AAbstract%3A%20%20%20The%20abilities%20of%20large%20language%20models%20%28LLMs%29%20have%20recently%20progressed%20to%0Aunprecedented%20levels%2C%20paving%20the%20way%20to%20novel%20applications%20in%20a%20wide%20variety%20of%0Aareas.%20In%20computer%20vision%2C%20LLMs%20can%20be%20used%20to%20prime%20vision-language%20tasks%20such%0Aimage%20captioning%20and%20visual%20question%20answering%20when%20coupled%20with%20pre-trained%0Avision%20backbones.%20While%20different%20approaches%20have%20been%20explored%20to%20interface%0ALLMs%20with%20%60%60perceptual%20backbones%27%27%20that%20process%2C%20e.g.%2C%20visual%20or%20audio%20data%2C%0Athey%20are%20often%20explored%20for%20different%20tasks%2C%20different%20datasets%2C%20and%20using%0Adifferent%20perceptual%20backbones%20and%20language%20models%2C%20hindering%20direct%20comparison%0Aof%20the%20interfacing%20mechanisms.%20To%20remedy%20this%20lack%20of%20comparability%20between%0Amethods%2C%20we%20present%20an%20extensive%20experimental%20evaluation%20of%20different%0Ainterfacing%20mechanisms%2C%20across%20multiple%20tasks%20%28including%20image%2C%20video%2C%20and%0Aaudio%20captioning%20as%20well%20as%20visual%20question%20answering%29%2C%20datasets%20and%20backbones%2C%0Apaying%20special%20attention%20to%20low-data%20settings.%20We%20find%20improved%20performance%0Ausing%20existing%20mechanisms%20over%20state-of-the-art%20results%2C%20and%20identify%20a%20new%0Ainterfacing%20mechanism%20that%20yields%20%28near%29%20optimal%20results%20across%20different%0Atasks%2C%20while%20obtaining%20a%204x%20reduction%20in%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Baselines%20for%20Data-efficient%20Perceptual%20Augmentation%20of%20LLMs&entry.906535625=Th%C3%A9ophane%20Vallaeys%20and%20Mustafa%20Shukor%20and%20Matthieu%20Cord%20and%20Jakob%20Verbeek&entry.1292438233=%20%20The%20abilities%20of%20large%20language%20models%20%28LLMs%29%20have%20recently%20progressed%20to%0Aunprecedented%20levels%2C%20paving%20the%20way%20to%20novel%20applications%20in%20a%20wide%20variety%20of%0Aareas.%20In%20computer%20vision%2C%20LLMs%20can%20be%20used%20to%20prime%20vision-language%20tasks%20such%0Aimage%20captioning%20and%20visual%20question%20answering%20when%20coupled%20with%20pre-trained%0Avision%20backbones.%20While%20different%20approaches%20have%20been%20explored%20to%20interface%0ALLMs%20with%20%60%60perceptual%20backbones%27%27%20that%20process%2C%20e.g.%2C%20visual%20or%20audio%20data%2C%0Athey%20are%20often%20explored%20for%20different%20tasks%2C%20different%20datasets%2C%20and%20using%0Adifferent%20perceptual%20backbones%20and%20language%20models%2C%20hindering%20direct%20comparison%0Aof%20the%20interfacing%20mechanisms.%20To%20remedy%20this%20lack%20of%20comparability%20between%0Amethods%2C%20we%20present%20an%20extensive%20experimental%20evaluation%20of%20different%0Ainterfacing%20mechanisms%2C%20across%20multiple%20tasks%20%28including%20image%2C%20video%2C%20and%0Aaudio%20captioning%20as%20well%20as%20visual%20question%20answering%29%2C%20datasets%20and%20backbones%2C%0Apaying%20special%20attention%20to%20low-data%20settings.%20We%20find%20improved%20performance%0Ausing%20existing%20mechanisms%20over%20state-of-the-art%20results%2C%20and%20identify%20a%20new%0Ainterfacing%20mechanism%20that%20yields%20%28near%29%20optimal%20results%20across%20different%0Atasks%2C%20while%20obtaining%20a%204x%20reduction%20in%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13499v1&entry.124074799=Read"},
{"title": "Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation", "author": "Jun Yu and Gongpeng Zhao and Yongqi Wang and Zhihong Wei and Yang Zheng and Zerui Zhang and Zhongpeng Cai and Guochen Xie and Jichao Zhu and Wangyuan Zhu", "abstract": "  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n", "link": "http://arxiv.org/abs/2403.12425v2", "date": "2024-03-20", "relevancy": 2.1519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.543}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Fusion%20Method%20with%20Spatiotemporal%20Sequences%20and%20Relationship%0A%20%20Learning%20for%20Valence-Arousal%20Estimation&body=Title%3A%20Multimodal%20Fusion%20Method%20with%20Spatiotemporal%20Sequences%20and%20Relationship%0A%20%20Learning%20for%20Valence-Arousal%20Estimation%0AAuthor%3A%20Jun%20Yu%20and%20Gongpeng%20Zhao%20and%20Yongqi%20Wang%20and%20Zhihong%20Wei%20and%20Yang%20Zheng%20and%20Zerui%20Zhang%20and%20Zhongpeng%20Cai%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20approach%20for%20the%20VA%20%28Valence-Arousal%29%20estimation%20task%0Ain%20the%20ABAW6%20competition.%20We%20devised%20a%20comprehensive%20model%20by%20preprocessing%0Avideo%20frames%20and%20audio%20segments%20to%20extract%20visual%20and%20audio%20features.%20Through%0Athe%20utilization%20of%20Temporal%20Convolutional%20Network%20%28TCN%29%20modules%2C%20we%20effectively%0Acaptured%20the%20temporal%20and%20spatial%20correlations%20between%20these%20features.%0ASubsequently%2C%20we%20employed%20a%20Transformer%20encoder%20structure%20to%20learn%20long-range%0Adependencies%2C%20thereby%20enhancing%20the%20model%27s%20performance%20and%20generalization%0Aability.%20Our%20method%20leverages%20a%20multimodal%20data%20fusion%20approach%2C%20integrating%0Apre-trained%20audio%20and%20video%20backbones%20for%20feature%20extraction%2C%20followed%20by%0ATCN-based%20spatiotemporal%20encoding%20and%20Transformer-based%20temporal%20information%0Acapture.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20competitive%20performance%20in%20VA%20estimation%20on%20the%20AffWild2%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12425v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Fusion%20Method%20with%20Spatiotemporal%20Sequences%20and%20Relationship%0A%20%20Learning%20for%20Valence-Arousal%20Estimation&entry.906535625=Jun%20Yu%20and%20Gongpeng%20Zhao%20and%20Yongqi%20Wang%20and%20Zhihong%20Wei%20and%20Yang%20Zheng%20and%20Zerui%20Zhang%20and%20Zhongpeng%20Cai%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu&entry.1292438233=%20%20This%20paper%20presents%20our%20approach%20for%20the%20VA%20%28Valence-Arousal%29%20estimation%20task%0Ain%20the%20ABAW6%20competition.%20We%20devised%20a%20comprehensive%20model%20by%20preprocessing%0Avideo%20frames%20and%20audio%20segments%20to%20extract%20visual%20and%20audio%20features.%20Through%0Athe%20utilization%20of%20Temporal%20Convolutional%20Network%20%28TCN%29%20modules%2C%20we%20effectively%0Acaptured%20the%20temporal%20and%20spatial%20correlations%20between%20these%20features.%0ASubsequently%2C%20we%20employed%20a%20Transformer%20encoder%20structure%20to%20learn%20long-range%0Adependencies%2C%20thereby%20enhancing%20the%20model%27s%20performance%20and%20generalization%0Aability.%20Our%20method%20leverages%20a%20multimodal%20data%20fusion%20approach%2C%20integrating%0Apre-trained%20audio%20and%20video%20backbones%20for%20feature%20extraction%2C%20followed%20by%0ATCN-based%20spatiotemporal%20encoding%20and%20Transformer-based%20temporal%20information%0Acapture.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20competitive%20performance%20in%20VA%20estimation%20on%20the%20AffWild2%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12425v2&entry.124074799=Read"},
{"title": "FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based\n  LLMs", "author": "Jinmin Li and Kuofeng Gao and Yang Bai and Jingyun Zhang and Shu-tao Xia and Yisen Wang", "abstract": "  Despite the remarkable performance of video-based large language models\n(LLMs), their adversarial threat remains unexplored. To fill this gap, we\npropose the first adversarial attack tailored for video-based LLMs by crafting\nflow-based multi-modal adversarial perturbations on a small fraction of frames\nwithin a video, dubbed FMM-Attack. Extensive experiments show that our attack\ncan effectively induce video-based LLMs to generate incorrect answers when\nvideos are added with imperceptible adversarial perturbations. Intriguingly,\nour FMM-Attack can also induce garbling in the model output, prompting\nvideo-based LLMs to hallucinate. Overall, our observations inspire a further\nunderstanding of multi-modal robustness and safety-related feature alignment\nacross different modalities, which is of great importance for various large\nmulti-modal models. Our code is available at\nhttps://github.com/THU-Kingmin/FMM-Attack.\n", "link": "http://arxiv.org/abs/2403.13507v1", "date": "2024-03-20", "relevancy": 2.1422, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5334}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5258}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FMM-Attack%3A%20A%20Flow-based%20Multi-modal%20Adversarial%20Attack%20on%20Video-based%0A%20%20LLMs&body=Title%3A%20FMM-Attack%3A%20A%20Flow-based%20Multi-modal%20Adversarial%20Attack%20on%20Video-based%0A%20%20LLMs%0AAuthor%3A%20Jinmin%20Li%20and%20Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jingyun%20Zhang%20and%20Shu-tao%20Xia%20and%20Yisen%20Wang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20performance%20of%20video-based%20large%20language%20models%0A%28LLMs%29%2C%20their%20adversarial%20threat%20remains%20unexplored.%20To%20fill%20this%20gap%2C%20we%0Apropose%20the%20first%20adversarial%20attack%20tailored%20for%20video-based%20LLMs%20by%20crafting%0Aflow-based%20multi-modal%20adversarial%20perturbations%20on%20a%20small%20fraction%20of%20frames%0Awithin%20a%20video%2C%20dubbed%20FMM-Attack.%20Extensive%20experiments%20show%20that%20our%20attack%0Acan%20effectively%20induce%20video-based%20LLMs%20to%20generate%20incorrect%20answers%20when%0Avideos%20are%20added%20with%20imperceptible%20adversarial%20perturbations.%20Intriguingly%2C%0Aour%20FMM-Attack%20can%20also%20induce%20garbling%20in%20the%20model%20output%2C%20prompting%0Avideo-based%20LLMs%20to%20hallucinate.%20Overall%2C%20our%20observations%20inspire%20a%20further%0Aunderstanding%20of%20multi-modal%20robustness%20and%20safety-related%20feature%20alignment%0Aacross%20different%20modalities%2C%20which%20is%20of%20great%20importance%20for%20various%20large%0Amulti-modal%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/THU-Kingmin/FMM-Attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13507v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMM-Attack%3A%20A%20Flow-based%20Multi-modal%20Adversarial%20Attack%20on%20Video-based%0A%20%20LLMs&entry.906535625=Jinmin%20Li%20and%20Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jingyun%20Zhang%20and%20Shu-tao%20Xia%20and%20Yisen%20Wang&entry.1292438233=%20%20Despite%20the%20remarkable%20performance%20of%20video-based%20large%20language%20models%0A%28LLMs%29%2C%20their%20adversarial%20threat%20remains%20unexplored.%20To%20fill%20this%20gap%2C%20we%0Apropose%20the%20first%20adversarial%20attack%20tailored%20for%20video-based%20LLMs%20by%20crafting%0Aflow-based%20multi-modal%20adversarial%20perturbations%20on%20a%20small%20fraction%20of%20frames%0Awithin%20a%20video%2C%20dubbed%20FMM-Attack.%20Extensive%20experiments%20show%20that%20our%20attack%0Acan%20effectively%20induce%20video-based%20LLMs%20to%20generate%20incorrect%20answers%20when%0Avideos%20are%20added%20with%20imperceptible%20adversarial%20perturbations.%20Intriguingly%2C%0Aour%20FMM-Attack%20can%20also%20induce%20garbling%20in%20the%20model%20output%2C%20prompting%0Avideo-based%20LLMs%20to%20hallucinate.%20Overall%2C%20our%20observations%20inspire%20a%20further%0Aunderstanding%20of%20multi-modal%20robustness%20and%20safety-related%20feature%20alignment%0Aacross%20different%20modalities%2C%20which%20is%20of%20great%20importance%20for%20various%20large%0Amulti-modal%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/THU-Kingmin/FMM-Attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13507v1&entry.124074799=Read"},
{"title": "Certified Human Trajectory Prediction", "author": "Mohammadhossein Bahari and Saeed Saadatnejad and Amirhossein Asgari Farsangi and Seyed-Mohsen Moosavi-Dezfooli and Alexandre Alahi", "abstract": "  Trajectory prediction plays an essential role in autonomous vehicles. While\nnumerous strategies have been developed to enhance the robustness of trajectory\nprediction models, these methods are predominantly heuristic and do not offer\nguaranteed robustness against adversarial attacks and noisy observations. In\nthis work, we propose a certification approach tailored for the task of\ntrajectory prediction. To this end, we address the inherent challenges\nassociated with trajectory prediction, including unbounded outputs, and\nmutli-modality, resulting in a model that provides guaranteed robustness.\nFurthermore, we integrate a denoiser into our method to further improve the\nperformance. Through comprehensive evaluations, we demonstrate the\neffectiveness of the proposed technique across various baselines and using\nstandard trajectory prediction datasets. The code will be made available\nonline: https://s-attack.github.io/\n", "link": "http://arxiv.org/abs/2403.13778v1", "date": "2024-03-20", "relevancy": 2.1419, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5303}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5249}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Certified%20Human%20Trajectory%20Prediction&body=Title%3A%20Certified%20Human%20Trajectory%20Prediction%0AAuthor%3A%20Mohammadhossein%20Bahari%20and%20Saeed%20Saadatnejad%20and%20Amirhossein%20Asgari%20Farsangi%20and%20Seyed-Mohsen%20Moosavi-Dezfooli%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Trajectory%20prediction%20plays%20an%20essential%20role%20in%20autonomous%20vehicles.%20While%0Anumerous%20strategies%20have%20been%20developed%20to%20enhance%20the%20robustness%20of%20trajectory%0Aprediction%20models%2C%20these%20methods%20are%20predominantly%20heuristic%20and%20do%20not%20offer%0Aguaranteed%20robustness%20against%20adversarial%20attacks%20and%20noisy%20observations.%20In%0Athis%20work%2C%20we%20propose%20a%20certification%20approach%20tailored%20for%20the%20task%20of%0Atrajectory%20prediction.%20To%20this%20end%2C%20we%20address%20the%20inherent%20challenges%0Aassociated%20with%20trajectory%20prediction%2C%20including%20unbounded%20outputs%2C%20and%0Amutli-modality%2C%20resulting%20in%20a%20model%20that%20provides%20guaranteed%20robustness.%0AFurthermore%2C%20we%20integrate%20a%20denoiser%20into%20our%20method%20to%20further%20improve%20the%0Aperformance.%20Through%20comprehensive%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20technique%20across%20various%20baselines%20and%20using%0Astandard%20trajectory%20prediction%20datasets.%20The%20code%20will%20be%20made%20available%0Aonline%3A%20https%3A//s-attack.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13778v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Human%20Trajectory%20Prediction&entry.906535625=Mohammadhossein%20Bahari%20and%20Saeed%20Saadatnejad%20and%20Amirhossein%20Asgari%20Farsangi%20and%20Seyed-Mohsen%20Moosavi-Dezfooli%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Trajectory%20prediction%20plays%20an%20essential%20role%20in%20autonomous%20vehicles.%20While%0Anumerous%20strategies%20have%20been%20developed%20to%20enhance%20the%20robustness%20of%20trajectory%0Aprediction%20models%2C%20these%20methods%20are%20predominantly%20heuristic%20and%20do%20not%20offer%0Aguaranteed%20robustness%20against%20adversarial%20attacks%20and%20noisy%20observations.%20In%0Athis%20work%2C%20we%20propose%20a%20certification%20approach%20tailored%20for%20the%20task%20of%0Atrajectory%20prediction.%20To%20this%20end%2C%20we%20address%20the%20inherent%20challenges%0Aassociated%20with%20trajectory%20prediction%2C%20including%20unbounded%20outputs%2C%20and%0Amutli-modality%2C%20resulting%20in%20a%20model%20that%20provides%20guaranteed%20robustness.%0AFurthermore%2C%20we%20integrate%20a%20denoiser%20into%20our%20method%20to%20further%20improve%20the%0Aperformance.%20Through%20comprehensive%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20technique%20across%20various%20baselines%20and%20using%0Astandard%20trajectory%20prediction%20datasets.%20The%20code%20will%20be%20made%20available%0Aonline%3A%20https%3A//s-attack.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13778v1&entry.124074799=Read"},
{"title": "VL-Mamba: Exploring State Space Models for Multimodal Learning", "author": "Yanyuan Qiao and Zheng Yu and Longteng Guo and Sihan Chen and Zijia Zhao and Mingzhen Sun and Qi Wu and Jing Liu", "abstract": "  Multimodal large language models (MLLMs) have attracted widespread interest\nand have rich applications. However, the inherent attention mechanism in its\nTransformer structure requires quadratic complexity and results in expensive\ncomputational overhead. Therefore, in this work, we propose VL-Mamba, a\nmultimodal large language model based on state space models, which have been\nshown to have great potential for long-sequence modeling with fast inference\nand linear scaling in sequence length. Specifically, we first replace the\ntransformer-based backbone language model such as LLama or Vicuna with the\npre-trained Mamba language model. Then, we empirically explore how to\neffectively apply the 2D vision selective scan mechanism for multimodal\nlearning and the combinations of different vision encoders and variants of\npretrained Mamba language models. The extensive experiments on diverse\nmultimodal benchmarks with competitive performance show the effectiveness of\nour proposed VL-Mamba and demonstrate the great potential of applying state\nspace models for multimodal learning tasks.\n", "link": "http://arxiv.org/abs/2403.13600v1", "date": "2024-03-20", "relevancy": 2.1418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VL-Mamba%3A%20Exploring%20State%20Space%20Models%20for%20Multimodal%20Learning&body=Title%3A%20VL-Mamba%3A%20Exploring%20State%20Space%20Models%20for%20Multimodal%20Learning%0AAuthor%3A%20Yanyuan%20Qiao%20and%20Zheng%20Yu%20and%20Longteng%20Guo%20and%20Sihan%20Chen%20and%20Zijia%20Zhao%20and%20Mingzhen%20Sun%20and%20Qi%20Wu%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20widespread%20interest%0Aand%20have%20rich%20applications.%20However%2C%20the%20inherent%20attention%20mechanism%20in%20its%0ATransformer%20structure%20requires%20quadratic%20complexity%20and%20results%20in%20expensive%0Acomputational%20overhead.%20Therefore%2C%20in%20this%20work%2C%20we%20propose%20VL-Mamba%2C%20a%0Amultimodal%20large%20language%20model%20based%20on%20state%20space%20models%2C%20which%20have%20been%0Ashown%20to%20have%20great%20potential%20for%20long-sequence%20modeling%20with%20fast%20inference%0Aand%20linear%20scaling%20in%20sequence%20length.%20Specifically%2C%20we%20first%20replace%20the%0Atransformer-based%20backbone%20language%20model%20such%20as%20LLama%20or%20Vicuna%20with%20the%0Apre-trained%20Mamba%20language%20model.%20Then%2C%20we%20empirically%20explore%20how%20to%0Aeffectively%20apply%20the%202D%20vision%20selective%20scan%20mechanism%20for%20multimodal%0Alearning%20and%20the%20combinations%20of%20different%20vision%20encoders%20and%20variants%20of%0Apretrained%20Mamba%20language%20models.%20The%20extensive%20experiments%20on%20diverse%0Amultimodal%20benchmarks%20with%20competitive%20performance%20show%20the%20effectiveness%20of%0Aour%20proposed%20VL-Mamba%20and%20demonstrate%20the%20great%20potential%20of%20applying%20state%0Aspace%20models%20for%20multimodal%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-Mamba%3A%20Exploring%20State%20Space%20Models%20for%20Multimodal%20Learning&entry.906535625=Yanyuan%20Qiao%20and%20Zheng%20Yu%20and%20Longteng%20Guo%20and%20Sihan%20Chen%20and%20Zijia%20Zhao%20and%20Mingzhen%20Sun%20and%20Qi%20Wu%20and%20Jing%20Liu&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20attracted%20widespread%20interest%0Aand%20have%20rich%20applications.%20However%2C%20the%20inherent%20attention%20mechanism%20in%20its%0ATransformer%20structure%20requires%20quadratic%20complexity%20and%20results%20in%20expensive%0Acomputational%20overhead.%20Therefore%2C%20in%20this%20work%2C%20we%20propose%20VL-Mamba%2C%20a%0Amultimodal%20large%20language%20model%20based%20on%20state%20space%20models%2C%20which%20have%20been%0Ashown%20to%20have%20great%20potential%20for%20long-sequence%20modeling%20with%20fast%20inference%0Aand%20linear%20scaling%20in%20sequence%20length.%20Specifically%2C%20we%20first%20replace%20the%0Atransformer-based%20backbone%20language%20model%20such%20as%20LLama%20or%20Vicuna%20with%20the%0Apre-trained%20Mamba%20language%20model.%20Then%2C%20we%20empirically%20explore%20how%20to%0Aeffectively%20apply%20the%202D%20vision%20selective%20scan%20mechanism%20for%20multimodal%0Alearning%20and%20the%20combinations%20of%20different%20vision%20encoders%20and%20variants%20of%0Apretrained%20Mamba%20language%20models.%20The%20extensive%20experiments%20on%20diverse%0Amultimodal%20benchmarks%20with%20competitive%20performance%20show%20the%20effectiveness%20of%0Aour%20proposed%20VL-Mamba%20and%20demonstrate%20the%20great%20potential%20of%20applying%20state%0Aspace%20models%20for%20multimodal%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13600v1&entry.124074799=Read"},
{"title": "Reward-Driven Automated Curriculum Learning for Interaction-Aware\n  Self-Driving at Unsignalized Intersections", "author": "Zengqi Peng and Xiao Zhou and Lei Zheng and Yubin Wang and Jun Ma", "abstract": "  In this work, we present a reward-driven automated curriculum reinforcement\nlearning approach for interaction-aware self-driving at unsignalized\nintersections, taking into account the uncertainties associated with\nsurrounding vehicles (SVs). These uncertainties encompass the uncertainty of\nSVs' driving intention and also the quantity of SVs. To deal with this problem,\nthe curriculum set is specifically designed to accommodate a progressively\nincreasing number of SVs. By implementing an automated curriculum selection\nmechanism, the importance weights are rationally allocated across various\ncurricula, thereby facilitating improved sample efficiency and training\noutcomes. Furthermore, the reward function is meticulously designed to guide\nthe agent towards effective policy exploration. Thus the proposed framework\ncould proactively address the above uncertainties at unsignalized intersections\nby employing the automated curriculum learning technique that progressively\nincreases task difficulty, and this ensures safe self-driving through effective\ninteraction with SVs. Comparative experiments are conducted in $Highway\\_Env$,\nand the results indicate that our approach achieves the highest task success\nrate, attains strong robustness to initialization parameters of the curriculum\nselection module, and exhibits superior adaptability to diverse situational\nconfigurations at unsignalized intersections. Furthermore, the effectiveness of\nthe proposed method is validated using the high-fidelity CARLA simulator.\n", "link": "http://arxiv.org/abs/2403.13674v1", "date": "2024-03-20", "relevancy": 2.13, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5352}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5327}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5312}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections&body=Title%3A%20Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections%0AAuthor%3A%20Zengqi%20Peng%20and%20Xiao%20Zhou%20and%20Lei%20Zheng%20and%20Yubin%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20reward-driven%20automated%20curriculum%20reinforcement%0Alearning%20approach%20for%20interaction-aware%20self-driving%20at%20unsignalized%0Aintersections%2C%20taking%20into%20account%20the%20uncertainties%20associated%20with%0Asurrounding%20vehicles%20%28SVs%29.%20These%20uncertainties%20encompass%20the%20uncertainty%20of%0ASVs%27%20driving%20intention%20and%20also%20the%20quantity%20of%20SVs.%20To%20deal%20with%20this%20problem%2C%0Athe%20curriculum%20set%20is%20specifically%20designed%20to%20accommodate%20a%20progressively%0Aincreasing%20number%20of%20SVs.%20By%20implementing%20an%20automated%20curriculum%20selection%0Amechanism%2C%20the%20importance%20weights%20are%20rationally%20allocated%20across%20various%0Acurricula%2C%20thereby%20facilitating%20improved%20sample%20efficiency%20and%20training%0Aoutcomes.%20Furthermore%2C%20the%20reward%20function%20is%20meticulously%20designed%20to%20guide%0Athe%20agent%20towards%20effective%20policy%20exploration.%20Thus%20the%20proposed%20framework%0Acould%20proactively%20address%20the%20above%20uncertainties%20at%20unsignalized%20intersections%0Aby%20employing%20the%20automated%20curriculum%20learning%20technique%20that%20progressively%0Aincreases%20task%20difficulty%2C%20and%20this%20ensures%20safe%20self-driving%20through%20effective%0Ainteraction%20with%20SVs.%20Comparative%20experiments%20are%20conducted%20in%20%24Highway%5C_Env%24%2C%0Aand%20the%20results%20indicate%20that%20our%20approach%20achieves%20the%20highest%20task%20success%0Arate%2C%20attains%20strong%20robustness%20to%20initialization%20parameters%20of%20the%20curriculum%0Aselection%20module%2C%20and%20exhibits%20superior%20adaptability%20to%20diverse%20situational%0Aconfigurations%20at%20unsignalized%20intersections.%20Furthermore%2C%20the%20effectiveness%20of%0Athe%20proposed%20method%20is%20validated%20using%20the%20high-fidelity%20CARLA%20simulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Driven%20Automated%20Curriculum%20Learning%20for%20Interaction-Aware%0A%20%20Self-Driving%20at%20Unsignalized%20Intersections&entry.906535625=Zengqi%20Peng%20and%20Xiao%20Zhou%20and%20Lei%20Zheng%20and%20Yubin%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20reward-driven%20automated%20curriculum%20reinforcement%0Alearning%20approach%20for%20interaction-aware%20self-driving%20at%20unsignalized%0Aintersections%2C%20taking%20into%20account%20the%20uncertainties%20associated%20with%0Asurrounding%20vehicles%20%28SVs%29.%20These%20uncertainties%20encompass%20the%20uncertainty%20of%0ASVs%27%20driving%20intention%20and%20also%20the%20quantity%20of%20SVs.%20To%20deal%20with%20this%20problem%2C%0Athe%20curriculum%20set%20is%20specifically%20designed%20to%20accommodate%20a%20progressively%0Aincreasing%20number%20of%20SVs.%20By%20implementing%20an%20automated%20curriculum%20selection%0Amechanism%2C%20the%20importance%20weights%20are%20rationally%20allocated%20across%20various%0Acurricula%2C%20thereby%20facilitating%20improved%20sample%20efficiency%20and%20training%0Aoutcomes.%20Furthermore%2C%20the%20reward%20function%20is%20meticulously%20designed%20to%20guide%0Athe%20agent%20towards%20effective%20policy%20exploration.%20Thus%20the%20proposed%20framework%0Acould%20proactively%20address%20the%20above%20uncertainties%20at%20unsignalized%20intersections%0Aby%20employing%20the%20automated%20curriculum%20learning%20technique%20that%20progressively%0Aincreases%20task%20difficulty%2C%20and%20this%20ensures%20safe%20self-driving%20through%20effective%0Ainteraction%20with%20SVs.%20Comparative%20experiments%20are%20conducted%20in%20%24Highway%5C_Env%24%2C%0Aand%20the%20results%20indicate%20that%20our%20approach%20achieves%20the%20highest%20task%20success%0Arate%2C%20attains%20strong%20robustness%20to%20initialization%20parameters%20of%20the%20curriculum%0Aselection%20module%2C%20and%20exhibits%20superior%20adaptability%20to%20diverse%20situational%0Aconfigurations%20at%20unsignalized%20intersections.%20Furthermore%2C%20the%20effectiveness%20of%0Athe%20proposed%20method%20is%20validated%20using%20the%20high-fidelity%20CARLA%20simulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13674v1&entry.124074799=Read"},
{"title": "Weakly supervised segmentation of intracranial aneurysms using a novel\n  3D focal modulation UNet", "author": "Amirhossein Rasoulian and Arash Harirpoush and Soorena Salari and Yiming Xiao", "abstract": "  Accurate identification and quantification of unruptured intracranial\naneurysms (UIAs) is crucial for the risk assessment and treatment of this\ncerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonance\nangiography (MRA) is suboptimal and time-consuming. In addition, one major\nissue in medical image segmentation is the need for large well-annotated data,\nwhich can be expensive to obtain. Techniques that mitigate this requirement,\nsuch as weakly supervised learning with coarse labels are highly desirable. In\nthe paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detect\nan aneurysm and offer an initial, coarse segmentation of it from time-of-flight\nMRA image patches, which is further refined with a dense conditional random\nfield (CRF) post-processing layer to produce a final segmentation map. We\ntrained and evaluated our model on a public dataset, and in terms of UIA\ndetection, our model showed a low false-positive rate of 0.21 and a high\nsensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dice\nscore of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating its\nstrong performance. We evaluated our algorithms against the state-of-the-art 3D\nResidual-UNet and Swin-UNETR, and illustrated the superior performance of our\nproposed FocalSegNet, highlighting the advantages of employing focal modulation\nfor this task.\n", "link": "http://arxiv.org/abs/2308.03001v2", "date": "2024-03-20", "relevancy": 2.1232, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5285}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Weakly%20supervised%20segmentation%20of%20intracranial%20aneurysms%20using%20a%20novel%0A%20%203D%20focal%20modulation%20UNet&body=Title%3A%20Weakly%20supervised%20segmentation%20of%20intracranial%20aneurysms%20using%20a%20novel%0A%20%203D%20focal%20modulation%20UNet%0AAuthor%3A%20Amirhossein%20Rasoulian%20and%20Arash%20Harirpoush%20and%20Soorena%20Salari%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Accurate%20identification%20and%20quantification%20of%20unruptured%20intracranial%0Aaneurysms%20%28UIAs%29%20is%20crucial%20for%20the%20risk%20assessment%20and%20treatment%20of%20this%0Acerebrovascular%20disorder.%20Current%202D%20manual%20assessment%20on%203D%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20suboptimal%20and%20time-consuming.%20In%20addition%2C%20one%20major%0Aissue%20in%20medical%20image%20segmentation%20is%20the%20need%20for%20large%20well-annotated%20data%2C%0Awhich%20can%20be%20expensive%20to%20obtain.%20Techniques%20that%20mitigate%20this%20requirement%2C%0Asuch%20as%20weakly%20supervised%20learning%20with%20coarse%20labels%20are%20highly%20desirable.%20In%0Athe%20paper%2C%20we%20propose%20FocalSegNet%2C%20a%20novel%203D%20focal%20modulation%20UNet%2C%20to%20detect%0Aan%20aneurysm%20and%20offer%20an%20initial%2C%20coarse%20segmentation%20of%20it%20from%20time-of-flight%0AMRA%20image%20patches%2C%20which%20is%20further%20refined%20with%20a%20dense%20conditional%20random%0Afield%20%28CRF%29%20post-processing%20layer%20to%20produce%20a%20final%20segmentation%20map.%20We%0Atrained%20and%20evaluated%20our%20model%20on%20a%20public%20dataset%2C%20and%20in%20terms%20of%20UIA%0Adetection%2C%20our%20model%20showed%20a%20low%20false-positive%20rate%20of%200.21%20and%20a%20high%0Asensitivity%20of%200.80.%20For%20voxel-wise%20aneurysm%20segmentation%2C%20we%20achieved%20a%20Dice%0Ascore%20of%200.68%20and%20a%2095%25%20Hausdorff%20distance%20of%20~0.95%20mm%2C%20demonstrating%20its%0Astrong%20performance.%20We%20evaluated%20our%20algorithms%20against%20the%20state-of-the-art%203D%0AResidual-UNet%20and%20Swin-UNETR%2C%20and%20illustrated%20the%20superior%20performance%20of%20our%0Aproposed%20FocalSegNet%2C%20highlighting%20the%20advantages%20of%20employing%20focal%20modulation%0Afor%20this%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03001v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20supervised%20segmentation%20of%20intracranial%20aneurysms%20using%20a%20novel%0A%20%203D%20focal%20modulation%20UNet&entry.906535625=Amirhossein%20Rasoulian%20and%20Arash%20Harirpoush%20and%20Soorena%20Salari%20and%20Yiming%20Xiao&entry.1292438233=%20%20Accurate%20identification%20and%20quantification%20of%20unruptured%20intracranial%0Aaneurysms%20%28UIAs%29%20is%20crucial%20for%20the%20risk%20assessment%20and%20treatment%20of%20this%0Acerebrovascular%20disorder.%20Current%202D%20manual%20assessment%20on%203D%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20suboptimal%20and%20time-consuming.%20In%20addition%2C%20one%20major%0Aissue%20in%20medical%20image%20segmentation%20is%20the%20need%20for%20large%20well-annotated%20data%2C%0Awhich%20can%20be%20expensive%20to%20obtain.%20Techniques%20that%20mitigate%20this%20requirement%2C%0Asuch%20as%20weakly%20supervised%20learning%20with%20coarse%20labels%20are%20highly%20desirable.%20In%0Athe%20paper%2C%20we%20propose%20FocalSegNet%2C%20a%20novel%203D%20focal%20modulation%20UNet%2C%20to%20detect%0Aan%20aneurysm%20and%20offer%20an%20initial%2C%20coarse%20segmentation%20of%20it%20from%20time-of-flight%0AMRA%20image%20patches%2C%20which%20is%20further%20refined%20with%20a%20dense%20conditional%20random%0Afield%20%28CRF%29%20post-processing%20layer%20to%20produce%20a%20final%20segmentation%20map.%20We%0Atrained%20and%20evaluated%20our%20model%20on%20a%20public%20dataset%2C%20and%20in%20terms%20of%20UIA%0Adetection%2C%20our%20model%20showed%20a%20low%20false-positive%20rate%20of%200.21%20and%20a%20high%0Asensitivity%20of%200.80.%20For%20voxel-wise%20aneurysm%20segmentation%2C%20we%20achieved%20a%20Dice%0Ascore%20of%200.68%20and%20a%2095%25%20Hausdorff%20distance%20of%20~0.95%20mm%2C%20demonstrating%20its%0Astrong%20performance.%20We%20evaluated%20our%20algorithms%20against%20the%20state-of-the-art%203D%0AResidual-UNet%20and%20Swin-UNETR%2C%20and%20illustrated%20the%20superior%20performance%20of%20our%0Aproposed%20FocalSegNet%2C%20highlighting%20the%20advantages%20of%20employing%20focal%20modulation%0Afor%20this%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03001v2&entry.124074799=Read"},
{"title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection", "author": "Chao Yi and De-Chuan Zhan and Han-Jia Ye", "abstract": "  Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.\n", "link": "http://arxiv.org/abs/2403.13797v1", "date": "2024-03-20", "relevancy": 2.1069, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4809}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4801}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridge%20the%20Modality%20and%20Capacity%20Gaps%20in%20Vision-Language%20Model%20Selection&body=Title%3A%20Bridge%20the%20Modality%20and%20Capacity%20Gaps%20in%20Vision-Language%20Model%20Selection%0AAuthor%3A%20Chao%20Yi%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20excel%20in%20zero-shot%20image%20classification%20by%0Apairing%20images%20with%20textual%20category%20names.%20The%20expanding%20variety%20of%0APre-Trained%20VLMs%20enhances%20the%20likelihood%20of%20identifying%20a%20suitable%20VLM%20for%0Aspecific%20tasks.%20Thus%2C%20a%20promising%20zero-shot%20image%20classification%20strategy%20is%0Aselecting%20the%20most%20appropriate%20Pre-Trained%20VLM%20from%20the%20VLM%20Zoo%2C%20relying%20solely%0Aon%20the%20text%20data%20of%20the%20target%20dataset%20without%20access%20to%20the%20dataset%27s%20images.%0AIn%20this%20paper%2C%20we%20analyze%20two%20inherent%20challenges%20in%20assessing%20the%20ability%20of%20a%0AVLM%20in%20this%20Language-Only%20VLM%20selection%3A%20the%20%22Modality%20Gap%22%20--%20the%20disparity%20in%0AVLM%27s%20embeddings%20across%20two%20different%20modalities%2C%20making%20text%20a%20less%20reliable%0Asubstitute%20for%20images%3B%20and%20the%20%22Capability%20Gap%22%20--%20the%20discrepancy%20between%20the%0AVLM%27s%20overall%20ranking%20and%20its%20ranking%20for%20target%20dataset%2C%20hindering%20direct%0Aprediction%20of%20a%20model%27s%20dataset-specific%20performance%20from%20its%20general%0Aperformance.%20We%20propose%20VLM%20Selection%20With%20gAp%20Bridging%20%28SWAB%29%20to%20mitigate%20the%0Anegative%20impact%20of%20these%20two%20gaps.%20SWAB%20first%20adopts%20optimal%20transport%20to%0Acapture%20the%20relevance%20between%20open-source%20datasets%20and%20target%20dataset%20with%20a%0Atransportation%20matrix.%20It%20then%20uses%20this%20matrix%20to%20transfer%20useful%20statistics%0Aof%20VLMs%20from%20open-source%20datasets%20to%20the%20target%20dataset%20for%20bridging%20those%20two%0Agaps%20and%20enhancing%20the%20VLM%27s%20capacity%20estimation%20for%20VLM%20selection.%20Experiments%0Aacross%20various%20VLMs%20and%20image%20classification%20datasets%20validate%20SWAB%27s%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13797v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridge%20the%20Modality%20and%20Capacity%20Gaps%20in%20Vision-Language%20Model%20Selection&entry.906535625=Chao%20Yi%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20excel%20in%20zero-shot%20image%20classification%20by%0Apairing%20images%20with%20textual%20category%20names.%20The%20expanding%20variety%20of%0APre-Trained%20VLMs%20enhances%20the%20likelihood%20of%20identifying%20a%20suitable%20VLM%20for%0Aspecific%20tasks.%20Thus%2C%20a%20promising%20zero-shot%20image%20classification%20strategy%20is%0Aselecting%20the%20most%20appropriate%20Pre-Trained%20VLM%20from%20the%20VLM%20Zoo%2C%20relying%20solely%0Aon%20the%20text%20data%20of%20the%20target%20dataset%20without%20access%20to%20the%20dataset%27s%20images.%0AIn%20this%20paper%2C%20we%20analyze%20two%20inherent%20challenges%20in%20assessing%20the%20ability%20of%20a%0AVLM%20in%20this%20Language-Only%20VLM%20selection%3A%20the%20%22Modality%20Gap%22%20--%20the%20disparity%20in%0AVLM%27s%20embeddings%20across%20two%20different%20modalities%2C%20making%20text%20a%20less%20reliable%0Asubstitute%20for%20images%3B%20and%20the%20%22Capability%20Gap%22%20--%20the%20discrepancy%20between%20the%0AVLM%27s%20overall%20ranking%20and%20its%20ranking%20for%20target%20dataset%2C%20hindering%20direct%0Aprediction%20of%20a%20model%27s%20dataset-specific%20performance%20from%20its%20general%0Aperformance.%20We%20propose%20VLM%20Selection%20With%20gAp%20Bridging%20%28SWAB%29%20to%20mitigate%20the%0Anegative%20impact%20of%20these%20two%20gaps.%20SWAB%20first%20adopts%20optimal%20transport%20to%0Acapture%20the%20relevance%20between%20open-source%20datasets%20and%20target%20dataset%20with%20a%0Atransportation%20matrix.%20It%20then%20uses%20this%20matrix%20to%20transfer%20useful%20statistics%0Aof%20VLMs%20from%20open-source%20datasets%20to%20the%20target%20dataset%20for%20bridging%20those%20two%0Agaps%20and%20enhancing%20the%20VLM%27s%20capacity%20estimation%20for%20VLM%20selection.%20Experiments%0Aacross%20various%20VLMs%20and%20image%20classification%20datasets%20validate%20SWAB%27s%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13797v1&entry.124074799=Read"},
{"title": "Intention-Aware Planner for Robust and Safe Aerial Tracking", "author": "Qiuyu Ren and Huan Yu and Jiajun Dai and Zhi Zheng and Jun Meng and Li Xu and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Autonomous target tracking with quadrotors has wide applications in many\nscenarios, such as cinematographic follow-up shooting or suspect chasing.\nTarget motion prediction is necessary when designing the tracking planner.\nHowever, the widely used constant velocity or constant rotation assumption can\nnot fully capture the dynamics of the target. The tracker may fail when the\ntarget happens to move aggressively, such as sudden turn or deceleration. In\nthis paper, we propose an intention-aware planner by additionally considering\nthe intention of the target to enhance safety and robustness in aerial tracking\napplications. Firstly, a designated intention prediction method is proposed,\nwhich combines a user-defined potential assessment function and a state\nobservation function. A reachable region is generated to specifically evaluate\nthe turning intentions. Then we design an intention-driven hybrid A* method to\npredict the future possible positions for the target. Finally, an\nintention-aware optimization approach is designed to generate a\nspatial-temporal optimal trajectory, allowing the tracker to perceive\nunexpected situations from the target. Benchmark comparisons and real-world\nexperiments are conducted to validate the performance of our method.\n", "link": "http://arxiv.org/abs/2309.08854v2", "date": "2024-03-20", "relevancy": 2.1065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&body=Title%3A%20Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking%0AAuthor%3A%20Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08854v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intention-Aware%20Planner%20for%20Robust%20and%20Safe%20Aerial%20Tracking&entry.906535625=Qiuyu%20Ren%20and%20Huan%20Yu%20and%20Jiajun%20Dai%20and%20Zhi%20Zheng%20and%20Jun%20Meng%20and%20Li%20Xu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Autonomous%20target%20tracking%20with%20quadrotors%20has%20wide%20applications%20in%20many%0Ascenarios%2C%20such%20as%20cinematographic%20follow-up%20shooting%20or%20suspect%20chasing.%0ATarget%20motion%20prediction%20is%20necessary%20when%20designing%20the%20tracking%20planner.%0AHowever%2C%20the%20widely%20used%20constant%20velocity%20or%20constant%20rotation%20assumption%20can%0Anot%20fully%20capture%20the%20dynamics%20of%20the%20target.%20The%20tracker%20may%20fail%20when%20the%0Atarget%20happens%20to%20move%20aggressively%2C%20such%20as%20sudden%20turn%20or%20deceleration.%20In%0Athis%20paper%2C%20we%20propose%20an%20intention-aware%20planner%20by%20additionally%20considering%0Athe%20intention%20of%20the%20target%20to%20enhance%20safety%20and%20robustness%20in%20aerial%20tracking%0Aapplications.%20Firstly%2C%20a%20designated%20intention%20prediction%20method%20is%20proposed%2C%0Awhich%20combines%20a%20user-defined%20potential%20assessment%20function%20and%20a%20state%0Aobservation%20function.%20A%20reachable%20region%20is%20generated%20to%20specifically%20evaluate%0Athe%20turning%20intentions.%20Then%20we%20design%20an%20intention-driven%20hybrid%20A%2A%20method%20to%0Apredict%20the%20future%20possible%20positions%20for%20the%20target.%20Finally%2C%20an%0Aintention-aware%20optimization%20approach%20is%20designed%20to%20generate%20a%0Aspatial-temporal%20optimal%20trajectory%2C%20allowing%20the%20tracker%20to%20perceive%0Aunexpected%20situations%20from%20the%20target.%20Benchmark%20comparisons%20and%20real-world%0Aexperiments%20are%20conducted%20to%20validate%20the%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08854v2&entry.124074799=Read"},
{"title": "Loss Regularizing Robotic Terrain Classification", "author": "Shakti Deo Kumar and Sudhanshu Tripathi and Krishna Ujjwal and Sarvada Sakshi Jha and Suddhasil De", "abstract": "  Locomotion mechanics of legged robots are suitable when pacing through\ndifficult terrains. Recognising terrains for such robots are important to fully\nyoke the versatility of their movements. Consequently, robotic terrain\nclassification becomes significant to classify terrains in real time with high\naccuracy. The conventional classifiers suffer from overfitting problem, low\naccuracy problem, high variance problem, and not suitable for live dataset. On\nthe other hand, classifying a growing dataset is difficult for convolution\nbased terrain classification. Supervised recurrent models are also not\npractical for this classification. Further, the existing recurrent\narchitectures are still evolving to improve accuracy of terrain classification\nbased on live variable-length sensory data collected from legged robots. This\npaper proposes a new semi-supervised method for terrain classification of\nlegged robots, avoiding preprocessing of long variable-length dataset. The\nproposed method has a stacked Long Short-Term Memory architecture, including a\nnew loss regularization. The proposed method solves the existing problems and\nimproves accuracy. Comparison with the existing architectures show the\nimprovements.\n", "link": "http://arxiv.org/abs/2403.13695v1", "date": "2024-03-20", "relevancy": 2.0927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5276}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5019}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Loss%20Regularizing%20Robotic%20Terrain%20Classification&body=Title%3A%20Loss%20Regularizing%20Robotic%20Terrain%20Classification%0AAuthor%3A%20Shakti%20Deo%20Kumar%20and%20Sudhanshu%20Tripathi%20and%20Krishna%20Ujjwal%20and%20Sarvada%20Sakshi%20Jha%20and%20Suddhasil%20De%0AAbstract%3A%20%20%20Locomotion%20mechanics%20of%20legged%20robots%20are%20suitable%20when%20pacing%20through%0Adifficult%20terrains.%20Recognising%20terrains%20for%20such%20robots%20are%20important%20to%20fully%0Ayoke%20the%20versatility%20of%20their%20movements.%20Consequently%2C%20robotic%20terrain%0Aclassification%20becomes%20significant%20to%20classify%20terrains%20in%20real%20time%20with%20high%0Aaccuracy.%20The%20conventional%20classifiers%20suffer%20from%20overfitting%20problem%2C%20low%0Aaccuracy%20problem%2C%20high%20variance%20problem%2C%20and%20not%20suitable%20for%20live%20dataset.%20On%0Athe%20other%20hand%2C%20classifying%20a%20growing%20dataset%20is%20difficult%20for%20convolution%0Abased%20terrain%20classification.%20Supervised%20recurrent%20models%20are%20also%20not%0Apractical%20for%20this%20classification.%20Further%2C%20the%20existing%20recurrent%0Aarchitectures%20are%20still%20evolving%20to%20improve%20accuracy%20of%20terrain%20classification%0Abased%20on%20live%20variable-length%20sensory%20data%20collected%20from%20legged%20robots.%20This%0Apaper%20proposes%20a%20new%20semi-supervised%20method%20for%20terrain%20classification%20of%0Alegged%20robots%2C%20avoiding%20preprocessing%20of%20long%20variable-length%20dataset.%20The%0Aproposed%20method%20has%20a%20stacked%20Long%20Short-Term%20Memory%20architecture%2C%20including%20a%0Anew%20loss%20regularization.%20The%20proposed%20method%20solves%20the%20existing%20problems%20and%0Aimproves%20accuracy.%20Comparison%20with%20the%20existing%20architectures%20show%20the%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loss%20Regularizing%20Robotic%20Terrain%20Classification&entry.906535625=Shakti%20Deo%20Kumar%20and%20Sudhanshu%20Tripathi%20and%20Krishna%20Ujjwal%20and%20Sarvada%20Sakshi%20Jha%20and%20Suddhasil%20De&entry.1292438233=%20%20Locomotion%20mechanics%20of%20legged%20robots%20are%20suitable%20when%20pacing%20through%0Adifficult%20terrains.%20Recognising%20terrains%20for%20such%20robots%20are%20important%20to%20fully%0Ayoke%20the%20versatility%20of%20their%20movements.%20Consequently%2C%20robotic%20terrain%0Aclassification%20becomes%20significant%20to%20classify%20terrains%20in%20real%20time%20with%20high%0Aaccuracy.%20The%20conventional%20classifiers%20suffer%20from%20overfitting%20problem%2C%20low%0Aaccuracy%20problem%2C%20high%20variance%20problem%2C%20and%20not%20suitable%20for%20live%20dataset.%20On%0Athe%20other%20hand%2C%20classifying%20a%20growing%20dataset%20is%20difficult%20for%20convolution%0Abased%20terrain%20classification.%20Supervised%20recurrent%20models%20are%20also%20not%0Apractical%20for%20this%20classification.%20Further%2C%20the%20existing%20recurrent%0Aarchitectures%20are%20still%20evolving%20to%20improve%20accuracy%20of%20terrain%20classification%0Abased%20on%20live%20variable-length%20sensory%20data%20collected%20from%20legged%20robots.%20This%0Apaper%20proposes%20a%20new%20semi-supervised%20method%20for%20terrain%20classification%20of%0Alegged%20robots%2C%20avoiding%20preprocessing%20of%20long%20variable-length%20dataset.%20The%0Aproposed%20method%20has%20a%20stacked%20Long%20Short-Term%20Memory%20architecture%2C%20including%20a%0Anew%20loss%20regularization.%20The%20proposed%20method%20solves%20the%20existing%20problems%20and%0Aimproves%20accuracy.%20Comparison%20with%20the%20existing%20architectures%20show%20the%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13695v1&entry.124074799=Read"},
{"title": "When Cars meet Drones: Hyperbolic Federated Learning for Source-Free\n  Domain Adaptation in Adverse Weather", "author": "Giulia Rizzoli and Matteo Caligiuri and Donald Shenaj and Francesco Barbato and Pietro Zanuttigh", "abstract": "  In Federated Learning (FL), multiple clients collaboratively train a global\nmodel without sharing private data. In semantic segmentation, the Federated\nsource Free Domain Adaptation (FFreeDA) setting is of particular interest,\nwhere clients undergo unsupervised training after supervised pretraining at the\nserver side. While few recent works address FL for autonomous vehicles,\nintrinsic real-world challenges such as the presence of adverse weather\nconditions and the existence of different autonomous agents are still\nunexplored. To bridge this gap, we address both problems and introduce a new\nfederated semantic segmentation setting where both car and drone clients\nco-exist and collaborate. Specifically, we propose a novel approach for this\nsetting which exploits a batch-norm weather-aware strategy to dynamically adapt\nthe model to the different weather conditions, while hyperbolic space\nprototypes are used to align the heterogeneous client representations. Finally,\nwe introduce FLYAWARE, the first semantic segmentation dataset with adverse\nweather data for aerial vehicles.\n", "link": "http://arxiv.org/abs/2403.13762v1", "date": "2024-03-20", "relevancy": 2.083, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather&body=Title%3A%20When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather%0AAuthor%3A%20Giulia%20Rizzoli%20and%20Matteo%20Caligiuri%20and%20Donald%20Shenaj%20and%20Francesco%20Barbato%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20In%20Federated%20Learning%20%28FL%29%2C%20multiple%20clients%20collaboratively%20train%20a%20global%0Amodel%20without%20sharing%20private%20data.%20In%20semantic%20segmentation%2C%20the%20Federated%0Asource%20Free%20Domain%20Adaptation%20%28FFreeDA%29%20setting%20is%20of%20particular%20interest%2C%0Awhere%20clients%20undergo%20unsupervised%20training%20after%20supervised%20pretraining%20at%20the%0Aserver%20side.%20While%20few%20recent%20works%20address%20FL%20for%20autonomous%20vehicles%2C%0Aintrinsic%20real-world%20challenges%20such%20as%20the%20presence%20of%20adverse%20weather%0Aconditions%20and%20the%20existence%20of%20different%20autonomous%20agents%20are%20still%0Aunexplored.%20To%20bridge%20this%20gap%2C%20we%20address%20both%20problems%20and%20introduce%20a%20new%0Afederated%20semantic%20segmentation%20setting%20where%20both%20car%20and%20drone%20clients%0Aco-exist%20and%20collaborate.%20Specifically%2C%20we%20propose%20a%20novel%20approach%20for%20this%0Asetting%20which%20exploits%20a%20batch-norm%20weather-aware%20strategy%20to%20dynamically%20adapt%0Athe%20model%20to%20the%20different%20weather%20conditions%2C%20while%20hyperbolic%20space%0Aprototypes%20are%20used%20to%20align%20the%20heterogeneous%20client%20representations.%20Finally%2C%0Awe%20introduce%20FLYAWARE%2C%20the%20first%20semantic%20segmentation%20dataset%20with%20adverse%0Aweather%20data%20for%20aerial%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13762v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather&entry.906535625=Giulia%20Rizzoli%20and%20Matteo%20Caligiuri%20and%20Donald%20Shenaj%20and%20Francesco%20Barbato%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20In%20Federated%20Learning%20%28FL%29%2C%20multiple%20clients%20collaboratively%20train%20a%20global%0Amodel%20without%20sharing%20private%20data.%20In%20semantic%20segmentation%2C%20the%20Federated%0Asource%20Free%20Domain%20Adaptation%20%28FFreeDA%29%20setting%20is%20of%20particular%20interest%2C%0Awhere%20clients%20undergo%20unsupervised%20training%20after%20supervised%20pretraining%20at%20the%0Aserver%20side.%20While%20few%20recent%20works%20address%20FL%20for%20autonomous%20vehicles%2C%0Aintrinsic%20real-world%20challenges%20such%20as%20the%20presence%20of%20adverse%20weather%0Aconditions%20and%20the%20existence%20of%20different%20autonomous%20agents%20are%20still%0Aunexplored.%20To%20bridge%20this%20gap%2C%20we%20address%20both%20problems%20and%20introduce%20a%20new%0Afederated%20semantic%20segmentation%20setting%20where%20both%20car%20and%20drone%20clients%0Aco-exist%20and%20collaborate.%20Specifically%2C%20we%20propose%20a%20novel%20approach%20for%20this%0Asetting%20which%20exploits%20a%20batch-norm%20weather-aware%20strategy%20to%20dynamically%20adapt%0Athe%20model%20to%20the%20different%20weather%20conditions%2C%20while%20hyperbolic%20space%0Aprototypes%20are%20used%20to%20align%20the%20heterogeneous%20client%20representations.%20Finally%2C%0Awe%20introduce%20FLYAWARE%2C%20the%20first%20semantic%20segmentation%20dataset%20with%20adverse%0Aweather%20data%20for%20aerial%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13762v1&entry.124074799=Read"},
{"title": "REAL: Representation Enhanced Analytic Learning for Exemplar-free\n  Class-incremental Learning", "author": "Run He and Huiping Zhuang and Di Fang and Yizhu Chen and Kai Tong and Cen Chen", "abstract": "  Exemplar-free class-incremental learning (EFCIL) aims to mitigate\ncatastrophic forgetting in class-incremental learning without available\nhistorical data. Compared with its counterpart (replay-based CIL) that stores\nhistorical samples, the EFCIL suffers more from forgetting issues under the\nexemplar-free constraint. In this paper, inspired by the recently developed\nanalytic learning (AL) based CIL, we propose a representation enhanced analytic\nlearning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining\n(DS-BPT) and a representation enhancing distillation (RED) process to enhance\nthe representation of the extractor. The DS-BPT pretrains model in streams of\nboth supervised learning and self-supervised contrastive learning (SSCL) for\nbase knowledge extraction. The RED process distills the supervised knowledge to\nthe SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that\nconverts the CIL to a recursive least-square problem. Our method addresses the\nissue of insufficient discriminability in representations of unseen data caused\nby a frozen backbone in the existing AL-based CIL. Empirical results on various\ndatasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that\nour REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or\neven more superior performance compared with the replay-based methods.\n", "link": "http://arxiv.org/abs/2403.13522v1", "date": "2024-03-20", "relevancy": 2.0785, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5378}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5026}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%0A%20%20Class-incremental%20Learning&body=Title%3A%20REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%0A%20%20Class-incremental%20Learning%0AAuthor%3A%20Run%20He%20and%20Huiping%20Zhuang%20and%20Di%20Fang%20and%20Yizhu%20Chen%20and%20Kai%20Tong%20and%20Cen%20Chen%0AAbstract%3A%20%20%20Exemplar-free%20class-incremental%20learning%20%28EFCIL%29%20aims%20to%20mitigate%0Acatastrophic%20forgetting%20in%20class-incremental%20learning%20without%20available%0Ahistorical%20data.%20Compared%20with%20its%20counterpart%20%28replay-based%20CIL%29%20that%20stores%0Ahistorical%20samples%2C%20the%20EFCIL%20suffers%20more%20from%20forgetting%20issues%20under%20the%0Aexemplar-free%20constraint.%20In%20this%20paper%2C%20inspired%20by%20the%20recently%20developed%0Aanalytic%20learning%20%28AL%29%20based%20CIL%2C%20we%20propose%20a%20representation%20enhanced%20analytic%0Alearning%20%28REAL%29%20for%20EFCIL.%20The%20REAL%20constructs%20a%20dual-stream%20base%20pretraining%0A%28DS-BPT%29%20and%20a%20representation%20enhancing%20distillation%20%28RED%29%20process%20to%20enhance%0Athe%20representation%20of%20the%20extractor.%20The%20DS-BPT%20pretrains%20model%20in%20streams%20of%0Aboth%20supervised%20learning%20and%20self-supervised%20contrastive%20learning%20%28SSCL%29%20for%0Abase%20knowledge%20extraction.%20The%20RED%20process%20distills%20the%20supervised%20knowledge%20to%0Athe%20SSCL%20pretrained%20backbone%20and%20facilitates%20a%20subsequent%20AL-basd%20CIL%20that%0Aconverts%20the%20CIL%20to%20a%20recursive%20least-square%20problem.%20Our%20method%20addresses%20the%0Aissue%20of%20insufficient%20discriminability%20in%20representations%20of%20unseen%20data%20caused%0Aby%20a%20frozen%20backbone%20in%20the%20existing%20AL-based%20CIL.%20Empirical%20results%20on%20various%0Adatasets%20including%20CIFAR-100%2C%20ImageNet-100%20and%20ImageNet-1k%2C%20demonstrate%20that%0Aour%20REAL%20outperforms%20the%20state-of-the-arts%20in%20EFCIL%2C%20and%20achieves%20comparable%20or%0Aeven%20more%20superior%20performance%20compared%20with%20the%20replay-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13522v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%0A%20%20Class-incremental%20Learning&entry.906535625=Run%20He%20and%20Huiping%20Zhuang%20and%20Di%20Fang%20and%20Yizhu%20Chen%20and%20Kai%20Tong%20and%20Cen%20Chen&entry.1292438233=%20%20Exemplar-free%20class-incremental%20learning%20%28EFCIL%29%20aims%20to%20mitigate%0Acatastrophic%20forgetting%20in%20class-incremental%20learning%20without%20available%0Ahistorical%20data.%20Compared%20with%20its%20counterpart%20%28replay-based%20CIL%29%20that%20stores%0Ahistorical%20samples%2C%20the%20EFCIL%20suffers%20more%20from%20forgetting%20issues%20under%20the%0Aexemplar-free%20constraint.%20In%20this%20paper%2C%20inspired%20by%20the%20recently%20developed%0Aanalytic%20learning%20%28AL%29%20based%20CIL%2C%20we%20propose%20a%20representation%20enhanced%20analytic%0Alearning%20%28REAL%29%20for%20EFCIL.%20The%20REAL%20constructs%20a%20dual-stream%20base%20pretraining%0A%28DS-BPT%29%20and%20a%20representation%20enhancing%20distillation%20%28RED%29%20process%20to%20enhance%0Athe%20representation%20of%20the%20extractor.%20The%20DS-BPT%20pretrains%20model%20in%20streams%20of%0Aboth%20supervised%20learning%20and%20self-supervised%20contrastive%20learning%20%28SSCL%29%20for%0Abase%20knowledge%20extraction.%20The%20RED%20process%20distills%20the%20supervised%20knowledge%20to%0Athe%20SSCL%20pretrained%20backbone%20and%20facilitates%20a%20subsequent%20AL-basd%20CIL%20that%0Aconverts%20the%20CIL%20to%20a%20recursive%20least-square%20problem.%20Our%20method%20addresses%20the%0Aissue%20of%20insufficient%20discriminability%20in%20representations%20of%20unseen%20data%20caused%0Aby%20a%20frozen%20backbone%20in%20the%20existing%20AL-based%20CIL.%20Empirical%20results%20on%20various%0Adatasets%20including%20CIFAR-100%2C%20ImageNet-100%20and%20ImageNet-1k%2C%20demonstrate%20that%0Aour%20REAL%20outperforms%20the%20state-of-the-arts%20in%20EFCIL%2C%20and%20achieves%20comparable%20or%0Aeven%20more%20superior%20performance%20compared%20with%20the%20replay-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13522v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks", "author": "Miltiadis Kofinas and Boris Knyazev and Yan Zhang and Yunlu Chen and Gertjan J. Burghouts and Efstratios Gavves and Cees G. M. Snoek and David W. Zhang", "abstract": "  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n", "link": "http://arxiv.org/abs/2403.12143v2", "date": "2024-03-20", "relevancy": 2.0777, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks&body=Title%3A%20Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks%0AAuthor%3A%20Miltiadis%20Kofinas%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%20and%20Yunlu%20Chen%20and%20Gertjan%20J.%20Burghouts%20and%20Efstratios%20Gavves%20and%20Cees%20G.%20M.%20Snoek%20and%20David%20W.%20Zhang%0AAbstract%3A%20%20%20Neural%20networks%20that%20process%20the%20parameters%20of%20other%20neural%20networks%20find%0Aapplications%20in%20domains%20as%20diverse%20as%20classifying%20implicit%20neural%0Arepresentations%2C%20generating%20neural%20network%20weights%2C%20and%20predicting%0Ageneralization%20errors.%20However%2C%20existing%20approaches%20either%20overlook%20the%0Ainherent%20permutation%20symmetry%20in%20the%20neural%20network%20or%20rely%20on%20intricate%0Aweight-sharing%20patterns%20to%20achieve%20equivariance%2C%20while%20ignoring%20the%20impact%20of%0Athe%20network%20architecture%20itself.%20In%20this%20work%2C%20we%20propose%20to%20represent%20neural%0Anetworks%20as%20computational%20graphs%20of%20parameters%2C%20which%20allows%20us%20to%20harness%0Apowerful%20graph%20neural%20networks%20and%20transformers%20that%20preserve%20permutation%0Asymmetry.%20Consequently%2C%20our%20approach%20enables%20a%20single%20model%20to%20encode%20neural%0Acomputational%20graphs%20with%20diverse%20architectures.%20We%20showcase%20the%20effectiveness%0Aof%20our%20method%20on%20a%20wide%20range%20of%20tasks%2C%20including%20classification%20and%20editing%20of%0Aimplicit%20neural%20representations%2C%20predicting%20generalization%20performance%2C%20and%0Alearning%20to%20optimize%2C%20while%20consistently%20outperforming%20state-of-the-art%0Amethods.%20The%20source%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/mkofinas/neural-graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12143v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks&entry.906535625=Miltiadis%20Kofinas%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%20and%20Yunlu%20Chen%20and%20Gertjan%20J.%20Burghouts%20and%20Efstratios%20Gavves%20and%20Cees%20G.%20M.%20Snoek%20and%20David%20W.%20Zhang&entry.1292438233=%20%20Neural%20networks%20that%20process%20the%20parameters%20of%20other%20neural%20networks%20find%0Aapplications%20in%20domains%20as%20diverse%20as%20classifying%20implicit%20neural%0Arepresentations%2C%20generating%20neural%20network%20weights%2C%20and%20predicting%0Ageneralization%20errors.%20However%2C%20existing%20approaches%20either%20overlook%20the%0Ainherent%20permutation%20symmetry%20in%20the%20neural%20network%20or%20rely%20on%20intricate%0Aweight-sharing%20patterns%20to%20achieve%20equivariance%2C%20while%20ignoring%20the%20impact%20of%0Athe%20network%20architecture%20itself.%20In%20this%20work%2C%20we%20propose%20to%20represent%20neural%0Anetworks%20as%20computational%20graphs%20of%20parameters%2C%20which%20allows%20us%20to%20harness%0Apowerful%20graph%20neural%20networks%20and%20transformers%20that%20preserve%20permutation%0Asymmetry.%20Consequently%2C%20our%20approach%20enables%20a%20single%20model%20to%20encode%20neural%0Acomputational%20graphs%20with%20diverse%20architectures.%20We%20showcase%20the%20effectiveness%0Aof%20our%20method%20on%20a%20wide%20range%20of%20tasks%2C%20including%20classification%20and%20editing%20of%0Aimplicit%20neural%20representations%2C%20predicting%20generalization%20performance%2C%20and%0Alearning%20to%20optimize%2C%20while%20consistently%20outperforming%20state-of-the-art%0Amethods.%20The%20source%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/mkofinas/neural-graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12143v2&entry.124074799=Read"},
{"title": "H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation", "author": "Renkai Wu and Yinghao Liu and Pengchen Liang and Qing Chang", "abstract": "  In the field of medical image segmentation, variant models based on\nConvolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base\nmodules have been very widely developed and applied. However, CNNs are often\nlimited in their ability to deal with long sequences of information, while the\nlow sensitivity of ViTs to local feature information and the problem of\nsecondary computational complexity limit their development. Recently, the\nemergence of state-space models (SSMs), especially 2D-selective-scan (SS2D),\nhas had an impact on the longtime dominance of traditional CNNs and ViTs as the\nfoundational modules of visual neural networks. In this paper, we extend the\nadaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for\nmedical image segmentation. Among them, the proposed High-order\n2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant\ninformation during SS2D operations through higher-order interactions. In\naddition, the proposed Local-SS2D module improves the learning ability of local\nfeatures of SS2D at each order of interaction. We conducted comparison and\nablation experiments on three publicly available medical image datasets\n(ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the\nstrong competitiveness of H-vmunet in medical image segmentation tasks. The\ncode is available from https://github.com/wurenkai/H-vmunet .\n", "link": "http://arxiv.org/abs/2403.13642v1", "date": "2024-03-20", "relevancy": 2.0756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5153}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20H-vmunet%3A%20High-order%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation&body=Title%3A%20H-vmunet%3A%20High-order%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Renkai%20Wu%20and%20Yinghao%20Liu%20and%20Pengchen%20Liang%20and%20Qing%20Chang%0AAbstract%3A%20%20%20In%20the%20field%20of%20medical%20image%20segmentation%2C%20variant%20models%20based%20on%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Visual%20Transformers%20%28ViTs%29%20as%20the%20base%0Amodules%20have%20been%20very%20widely%20developed%20and%20applied.%20However%2C%20CNNs%20are%20often%0Alimited%20in%20their%20ability%20to%20deal%20with%20long%20sequences%20of%20information%2C%20while%20the%0Alow%20sensitivity%20of%20ViTs%20to%20local%20feature%20information%20and%20the%20problem%20of%0Asecondary%20computational%20complexity%20limit%20their%20development.%20Recently%2C%20the%0Aemergence%20of%20state-space%20models%20%28SSMs%29%2C%20especially%202D-selective-scan%20%28SS2D%29%2C%0Ahas%20had%20an%20impact%20on%20the%20longtime%20dominance%20of%20traditional%20CNNs%20and%20ViTs%20as%20the%0Afoundational%20modules%20of%20visual%20neural%20networks.%20In%20this%20paper%2C%20we%20extend%20the%0Aadaptability%20of%20SS2D%20by%20proposing%20a%20High-order%20Vision%20Mamba%20UNet%20%28H-vmunet%29%20for%0Amedical%20image%20segmentation.%20Among%20them%2C%20the%20proposed%20High-order%0A2D-selective-scan%20%28H-SS2D%29%20progressively%20reduces%20the%20introduction%20of%20redundant%0Ainformation%20during%20SS2D%20operations%20through%20higher-order%20interactions.%20In%0Aaddition%2C%20the%20proposed%20Local-SS2D%20module%20improves%20the%20learning%20ability%20of%20local%0Afeatures%20of%20SS2D%20at%20each%20order%20of%20interaction.%20We%20conducted%20comparison%20and%0Aablation%20experiments%20on%20three%20publicly%20available%20medical%20image%20datasets%0A%28ISIC2017%2C%20Spleen%2C%20and%20CVC-ClinicDB%29%2C%20and%20the%20results%20all%20demonstrate%20the%0Astrong%20competitiveness%20of%20H-vmunet%20in%20medical%20image%20segmentation%20tasks.%20The%0Acode%20is%20available%20from%20https%3A//github.com/wurenkai/H-vmunet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13642v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-vmunet%3A%20High-order%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation&entry.906535625=Renkai%20Wu%20and%20Yinghao%20Liu%20and%20Pengchen%20Liang%20and%20Qing%20Chang&entry.1292438233=%20%20In%20the%20field%20of%20medical%20image%20segmentation%2C%20variant%20models%20based%20on%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Visual%20Transformers%20%28ViTs%29%20as%20the%20base%0Amodules%20have%20been%20very%20widely%20developed%20and%20applied.%20However%2C%20CNNs%20are%20often%0Alimited%20in%20their%20ability%20to%20deal%20with%20long%20sequences%20of%20information%2C%20while%20the%0Alow%20sensitivity%20of%20ViTs%20to%20local%20feature%20information%20and%20the%20problem%20of%0Asecondary%20computational%20complexity%20limit%20their%20development.%20Recently%2C%20the%0Aemergence%20of%20state-space%20models%20%28SSMs%29%2C%20especially%202D-selective-scan%20%28SS2D%29%2C%0Ahas%20had%20an%20impact%20on%20the%20longtime%20dominance%20of%20traditional%20CNNs%20and%20ViTs%20as%20the%0Afoundational%20modules%20of%20visual%20neural%20networks.%20In%20this%20paper%2C%20we%20extend%20the%0Aadaptability%20of%20SS2D%20by%20proposing%20a%20High-order%20Vision%20Mamba%20UNet%20%28H-vmunet%29%20for%0Amedical%20image%20segmentation.%20Among%20them%2C%20the%20proposed%20High-order%0A2D-selective-scan%20%28H-SS2D%29%20progressively%20reduces%20the%20introduction%20of%20redundant%0Ainformation%20during%20SS2D%20operations%20through%20higher-order%20interactions.%20In%0Aaddition%2C%20the%20proposed%20Local-SS2D%20module%20improves%20the%20learning%20ability%20of%20local%0Afeatures%20of%20SS2D%20at%20each%20order%20of%20interaction.%20We%20conducted%20comparison%20and%0Aablation%20experiments%20on%20three%20publicly%20available%20medical%20image%20datasets%0A%28ISIC2017%2C%20Spleen%2C%20and%20CVC-ClinicDB%29%2C%20and%20the%20results%20all%20demonstrate%20the%0Astrong%20competitiveness%20of%20H-vmunet%20in%20medical%20image%20segmentation%20tasks.%20The%0Acode%20is%20available%20from%20https%3A//github.com/wurenkai/H-vmunet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13642v1&entry.124074799=Read"},
{"title": "DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance", "author": "Zixuan Wang and Jia Jia and Shikun Sun and Haozhe Wu and Rong Han and Zhenyu Li and Di Tang and Jiaqing Zhou and Jiebo Luo", "abstract": "  Choreographers determine what the dances look like, while cameramen determine\nthe final presentation of dances. Recently, various methods and datasets have\nshowcased the feasibility of dance synthesis. However, camera movement\nsynthesis with music and dance remains an unsolved challenging problem due to\nthe scarcity of paired data. Thus, we present DCM, a new multi-modal 3D\ndataset, which for the first time combines camera movement with dance motion\nand music audio. This dataset encompasses 108 dance sequences (3.2 hours) of\npaired dance-camera-music data from the anime community, covering 4 music\ngenres. With this dataset, we uncover that dance camera movement is\nmultifaceted and human-centric, and possesses multiple influencing factors,\nmaking dance camera synthesis a more challenging task compared to camera or\ndance synthesis alone. To overcome these difficulties, we propose\nDanceCamera3D, a transformer-based diffusion model that incorporates a novel\nbody attention loss and a condition separation strategy. For evaluation, we\ndevise new metrics measuring camera movement quality, diversity, and dancer\nfidelity. Utilizing these metrics, we conduct extensive experiments on our DCM\ndataset, providing both quantitative and qualitative evidence showcasing the\neffectiveness of our DanceCamera3D model. Code and video demos are available at\nhttps://github.com/Carmenw1203/DanceCamera3D-Official.\n", "link": "http://arxiv.org/abs/2403.13667v1", "date": "2024-03-20", "relevancy": 2.0684, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5218}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5019}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DanceCamera3D%3A%203D%20Camera%20Movement%20Synthesis%20with%20Music%20and%20Dance&body=Title%3A%20DanceCamera3D%3A%203D%20Camera%20Movement%20Synthesis%20with%20Music%20and%20Dance%0AAuthor%3A%20Zixuan%20Wang%20and%20Jia%20Jia%20and%20Shikun%20Sun%20and%20Haozhe%20Wu%20and%20Rong%20Han%20and%20Zhenyu%20Li%20and%20Di%20Tang%20and%20Jiaqing%20Zhou%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Choreographers%20determine%20what%20the%20dances%20look%20like%2C%20while%20cameramen%20determine%0Athe%20final%20presentation%20of%20dances.%20Recently%2C%20various%20methods%20and%20datasets%20have%0Ashowcased%20the%20feasibility%20of%20dance%20synthesis.%20However%2C%20camera%20movement%0Asynthesis%20with%20music%20and%20dance%20remains%20an%20unsolved%20challenging%20problem%20due%20to%0Athe%20scarcity%20of%20paired%20data.%20Thus%2C%20we%20present%20DCM%2C%20a%20new%20multi-modal%203D%0Adataset%2C%20which%20for%20the%20first%20time%20combines%20camera%20movement%20with%20dance%20motion%0Aand%20music%20audio.%20This%20dataset%20encompasses%20108%20dance%20sequences%20%283.2%20hours%29%20of%0Apaired%20dance-camera-music%20data%20from%20the%20anime%20community%2C%20covering%204%20music%0Agenres.%20With%20this%20dataset%2C%20we%20uncover%20that%20dance%20camera%20movement%20is%0Amultifaceted%20and%20human-centric%2C%20and%20possesses%20multiple%20influencing%20factors%2C%0Amaking%20dance%20camera%20synthesis%20a%20more%20challenging%20task%20compared%20to%20camera%20or%0Adance%20synthesis%20alone.%20To%20overcome%20these%20difficulties%2C%20we%20propose%0ADanceCamera3D%2C%20a%20transformer-based%20diffusion%20model%20that%20incorporates%20a%20novel%0Abody%20attention%20loss%20and%20a%20condition%20separation%20strategy.%20For%20evaluation%2C%20we%0Adevise%20new%20metrics%20measuring%20camera%20movement%20quality%2C%20diversity%2C%20and%20dancer%0Afidelity.%20Utilizing%20these%20metrics%2C%20we%20conduct%20extensive%20experiments%20on%20our%20DCM%0Adataset%2C%20providing%20both%20quantitative%20and%20qualitative%20evidence%20showcasing%20the%0Aeffectiveness%20of%20our%20DanceCamera3D%20model.%20Code%20and%20video%20demos%20are%20available%20at%0Ahttps%3A//github.com/Carmenw1203/DanceCamera3D-Official.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceCamera3D%3A%203D%20Camera%20Movement%20Synthesis%20with%20Music%20and%20Dance&entry.906535625=Zixuan%20Wang%20and%20Jia%20Jia%20and%20Shikun%20Sun%20and%20Haozhe%20Wu%20and%20Rong%20Han%20and%20Zhenyu%20Li%20and%20Di%20Tang%20and%20Jiaqing%20Zhou%20and%20Jiebo%20Luo&entry.1292438233=%20%20Choreographers%20determine%20what%20the%20dances%20look%20like%2C%20while%20cameramen%20determine%0Athe%20final%20presentation%20of%20dances.%20Recently%2C%20various%20methods%20and%20datasets%20have%0Ashowcased%20the%20feasibility%20of%20dance%20synthesis.%20However%2C%20camera%20movement%0Asynthesis%20with%20music%20and%20dance%20remains%20an%20unsolved%20challenging%20problem%20due%20to%0Athe%20scarcity%20of%20paired%20data.%20Thus%2C%20we%20present%20DCM%2C%20a%20new%20multi-modal%203D%0Adataset%2C%20which%20for%20the%20first%20time%20combines%20camera%20movement%20with%20dance%20motion%0Aand%20music%20audio.%20This%20dataset%20encompasses%20108%20dance%20sequences%20%283.2%20hours%29%20of%0Apaired%20dance-camera-music%20data%20from%20the%20anime%20community%2C%20covering%204%20music%0Agenres.%20With%20this%20dataset%2C%20we%20uncover%20that%20dance%20camera%20movement%20is%0Amultifaceted%20and%20human-centric%2C%20and%20possesses%20multiple%20influencing%20factors%2C%0Amaking%20dance%20camera%20synthesis%20a%20more%20challenging%20task%20compared%20to%20camera%20or%0Adance%20synthesis%20alone.%20To%20overcome%20these%20difficulties%2C%20we%20propose%0ADanceCamera3D%2C%20a%20transformer-based%20diffusion%20model%20that%20incorporates%20a%20novel%0Abody%20attention%20loss%20and%20a%20condition%20separation%20strategy.%20For%20evaluation%2C%20we%0Adevise%20new%20metrics%20measuring%20camera%20movement%20quality%2C%20diversity%2C%20and%20dancer%0Afidelity.%20Utilizing%20these%20metrics%2C%20we%20conduct%20extensive%20experiments%20on%20our%20DCM%0Adataset%2C%20providing%20both%20quantitative%20and%20qualitative%20evidence%20showcasing%20the%0Aeffectiveness%20of%20our%20DanceCamera3D%20model.%20Code%20and%20video%20demos%20are%20available%20at%0Ahttps%3A//github.com/Carmenw1203/DanceCamera3D-Official.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13667v1&entry.124074799=Read"},
{"title": "Simple Semantic-Aided Few-Shot Learning", "author": "Hai Zhang and Junzhe Xu and Shanlin Jiang and Zhenan He", "abstract": "  Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nfew-shot learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on six benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks. Code is available at\nhttps://github.com/zhangdoudou123/SemFew.\n", "link": "http://arxiv.org/abs/2311.18649v2", "date": "2024-03-20", "relevancy": 2.062, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20Semantic-Aided%20Few-Shot%20Learning&body=Title%3A%20Simple%20Semantic-Aided%20Few-Shot%20Learning%0AAuthor%3A%20Hai%20Zhang%20and%20Junzhe%20Xu%20and%20Shanlin%20Jiang%20and%20Zhenan%20He%0AAbstract%3A%20%20%20Learning%20from%20a%20limited%20amount%20of%20data%2C%20namely%20Few-Shot%20Learning%2C%20stands%20out%0Aas%20a%20challenging%20computer%20vision%20task.%20Several%20works%20exploit%20semantics%20and%0Adesign%20complicated%20semantic%20fusion%20mechanisms%20to%20compensate%20for%20rare%0Arepresentative%20features%20within%20restricted%20data.%20However%2C%20relying%20on%20naive%0Asemantics%20such%20as%20class%20names%20introduces%20biases%20due%20to%20their%20brevity%2C%20while%0Aacquiring%20extensive%20semantics%20from%20external%20knowledge%20takes%20a%20huge%20time%20and%0Aeffort.%20This%20limitation%20severely%20constrains%20the%20potential%20of%20semantics%20in%0Afew-shot%20learning.%20In%20this%20paper%2C%20we%20design%20an%20automatic%20way%20called%20Semantic%0AEvolution%20to%20generate%20high-quality%20semantics.%20The%20incorporation%20of%20high-quality%0Asemantics%20alleviates%20the%20need%20for%20complex%20network%20structures%20and%20learning%0Aalgorithms%20used%20in%20previous%20works.%20Hence%2C%20we%20employ%20a%20simple%20two-layer%20network%0Atermed%20Semantic%20Alignment%20Network%20to%20transform%20semantics%20and%20visual%20features%0Ainto%20robust%20class%20prototypes%20with%20rich%20discriminative%20features%20for%20few-shot%0Aclassification.%20The%20experimental%20results%20show%20our%20framework%20outperforms%20all%0Aprevious%20methods%20on%20six%20benchmarks%2C%20demonstrating%20a%20simple%20network%20with%0Ahigh-quality%20semantics%20can%20beat%20intricate%20multi-modal%20modules%20on%20few-shot%0Aclassification%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangdoudou123/SemFew.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18649v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Semantic-Aided%20Few-Shot%20Learning&entry.906535625=Hai%20Zhang%20and%20Junzhe%20Xu%20and%20Shanlin%20Jiang%20and%20Zhenan%20He&entry.1292438233=%20%20Learning%20from%20a%20limited%20amount%20of%20data%2C%20namely%20Few-Shot%20Learning%2C%20stands%20out%0Aas%20a%20challenging%20computer%20vision%20task.%20Several%20works%20exploit%20semantics%20and%0Adesign%20complicated%20semantic%20fusion%20mechanisms%20to%20compensate%20for%20rare%0Arepresentative%20features%20within%20restricted%20data.%20However%2C%20relying%20on%20naive%0Asemantics%20such%20as%20class%20names%20introduces%20biases%20due%20to%20their%20brevity%2C%20while%0Aacquiring%20extensive%20semantics%20from%20external%20knowledge%20takes%20a%20huge%20time%20and%0Aeffort.%20This%20limitation%20severely%20constrains%20the%20potential%20of%20semantics%20in%0Afew-shot%20learning.%20In%20this%20paper%2C%20we%20design%20an%20automatic%20way%20called%20Semantic%0AEvolution%20to%20generate%20high-quality%20semantics.%20The%20incorporation%20of%20high-quality%0Asemantics%20alleviates%20the%20need%20for%20complex%20network%20structures%20and%20learning%0Aalgorithms%20used%20in%20previous%20works.%20Hence%2C%20we%20employ%20a%20simple%20two-layer%20network%0Atermed%20Semantic%20Alignment%20Network%20to%20transform%20semantics%20and%20visual%20features%0Ainto%20robust%20class%20prototypes%20with%20rich%20discriminative%20features%20for%20few-shot%0Aclassification.%20The%20experimental%20results%20show%20our%20framework%20outperforms%20all%0Aprevious%20methods%20on%20six%20benchmarks%2C%20demonstrating%20a%20simple%20network%20with%0Ahigh-quality%20semantics%20can%20beat%20intricate%20multi-modal%20modules%20on%20few-shot%0Aclassification%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangdoudou123/SemFew.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18649v2&entry.124074799=Read"},
{"title": "Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning", "author": "Nathanael L. Baisa", "abstract": "  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To simplify the age prediction,\nwe create age groups for the age estimation. We make extensive evaluations and\ncomparisons of both convolution-based and transformer-based deep learning\narchitectures on a publicly available 11k hands dataset. Our experimental\nanalysis shows that it is possible to efficiently estimate not only identity\nbut also other attributes such as gender and age of suspects jointly from hand\nimages for criminal investigations, which is crucial in assisting international\npolice forces in the court to identify and convict abusers.\n", "link": "http://arxiv.org/abs/2303.15263v4", "date": "2024-03-20", "relevancy": 2.0619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Joint%20Person%20Identity%2C%20Gender%20and%20Age%20Estimation%20from%20Hand%20Images%20using%0A%20%20Deep%20Multi-Task%20Representation%20Learning&body=Title%3A%20Joint%20Person%20Identity%2C%20Gender%20and%20Age%20Estimation%20from%20Hand%20Images%20using%0A%20%20Deep%20Multi-Task%20Representation%20Learning%0AAuthor%3A%20Nathanael%20L.%20Baisa%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20multi-task%20representation%20learning%20framework%20to%0Ajointly%20estimate%20the%20identity%2C%20gender%20and%20age%20of%20individuals%20from%20their%20hand%0Aimages%20for%20the%20purpose%20of%20criminal%20investigations%20since%20the%20hand%20images%20are%0Aoften%20the%20only%20available%20information%20in%20cases%20of%20serious%20crime%20such%20as%20sexual%0Aabuse.%20We%20investigate%20different%20up-to-date%20deep%20learning%20architectures%20and%0Acompare%20their%20performance%20for%20joint%20estimation%20of%20identity%2C%20gender%20and%20age%20from%0Ahand%20images%20of%20perpetrators%20of%20serious%20crime.%20To%20simplify%20the%20age%20prediction%2C%0Awe%20create%20age%20groups%20for%20the%20age%20estimation.%20We%20make%20extensive%20evaluations%20and%0Acomparisons%20of%20both%20convolution-based%20and%20transformer-based%20deep%20learning%0Aarchitectures%20on%20a%20publicly%20available%2011k%20hands%20dataset.%20Our%20experimental%0Aanalysis%20shows%20that%20it%20is%20possible%20to%20efficiently%20estimate%20not%20only%20identity%0Abut%20also%20other%20attributes%20such%20as%20gender%20and%20age%20of%20suspects%20jointly%20from%20hand%0Aimages%20for%20criminal%20investigations%2C%20which%20is%20crucial%20in%20assisting%20international%0Apolice%20forces%20in%20the%20court%20to%20identify%20and%20convict%20abusers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15263v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Person%20Identity%2C%20Gender%20and%20Age%20Estimation%20from%20Hand%20Images%20using%0A%20%20Deep%20Multi-Task%20Representation%20Learning&entry.906535625=Nathanael%20L.%20Baisa&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20multi-task%20representation%20learning%20framework%20to%0Ajointly%20estimate%20the%20identity%2C%20gender%20and%20age%20of%20individuals%20from%20their%20hand%0Aimages%20for%20the%20purpose%20of%20criminal%20investigations%20since%20the%20hand%20images%20are%0Aoften%20the%20only%20available%20information%20in%20cases%20of%20serious%20crime%20such%20as%20sexual%0Aabuse.%20We%20investigate%20different%20up-to-date%20deep%20learning%20architectures%20and%0Acompare%20their%20performance%20for%20joint%20estimation%20of%20identity%2C%20gender%20and%20age%20from%0Ahand%20images%20of%20perpetrators%20of%20serious%20crime.%20To%20simplify%20the%20age%20prediction%2C%0Awe%20create%20age%20groups%20for%20the%20age%20estimation.%20We%20make%20extensive%20evaluations%20and%0Acomparisons%20of%20both%20convolution-based%20and%20transformer-based%20deep%20learning%0Aarchitectures%20on%20a%20publicly%20available%2011k%20hands%20dataset.%20Our%20experimental%0Aanalysis%20shows%20that%20it%20is%20possible%20to%20efficiently%20estimate%20not%20only%20identity%0Abut%20also%20other%20attributes%20such%20as%20gender%20and%20age%20of%20suspects%20jointly%20from%20hand%0Aimages%20for%20criminal%20investigations%2C%20which%20is%20crucial%20in%20assisting%20international%0Apolice%20forces%20in%20the%20court%20to%20identify%20and%20convict%20abusers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15263v4&entry.124074799=Read"},
{"title": "LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction\n  By Enhancing Laminar Characteristics in Human Flow", "author": "Yufei Zhu and Han Fan and Andrey Rudenko and Martin Magnusson and Erik Schaffernicht and Achim J. Lilienthal", "abstract": "  Long-term human motion prediction (LHMP) is essential for safely operating\nautonomous robots and vehicles in populated environments. It is fundamental for\nvarious applications, including motion planning, tracking, human-robot\ninteraction and safety monitoring. However, accurate prediction of human\ntrajectories is challenging due to complex factors, including, for example,\nsocial norms and environmental conditions. The influence of such factors can be\ncaptured through Maps of Dynamics (MoDs), which encode spatial motion patterns\nlearned from (possibly scattered and partial) past observations of motion in\nthe environment and which can be used for data-efficient, interpretable motion\nprediction (MoD-LHMP). To address the limitations of prior work, especially\nregarding accuracy and sensitivity to anomalies in long-term prediction, we\npropose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach\nis inspired by data-driven airflow modelling, which estimates laminar and\nturbulent flow components and uses predominantly the laminar components to make\nflow predictions. Based on the hypothesis that human trajectory patterns also\nmanifest laminar flow (that represents predictable motion) and turbulent flow\ncomponents (that reflect more unpredictable and arbitrary motion), LaCE-LHMP\nextracts the laminar patterns in human dynamics and uses them for human motion\nprediction. We demonstrate the superior prediction performance of LaCE-LHMP\nthrough benchmark comparisons with state-of-the-art LHMP methods, offering an\nunconventional perspective and a more intuitive understanding of human movement\npatterns.\n", "link": "http://arxiv.org/abs/2403.13640v1", "date": "2024-03-20", "relevancy": 2.0603, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5467}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaCE-LHMP%3A%20Airflow%20Modelling-Inspired%20Long-Term%20Human%20Motion%20Prediction%0A%20%20By%20Enhancing%20Laminar%20Characteristics%20in%20Human%20Flow&body=Title%3A%20LaCE-LHMP%3A%20Airflow%20Modelling-Inspired%20Long-Term%20Human%20Motion%20Prediction%0A%20%20By%20Enhancing%20Laminar%20Characteristics%20in%20Human%20Flow%0AAuthor%3A%20Yufei%20Zhu%20and%20Han%20Fan%20and%20Andrey%20Rudenko%20and%20Martin%20Magnusson%20and%20Erik%20Schaffernicht%20and%20Achim%20J.%20Lilienthal%0AAbstract%3A%20%20%20Long-term%20human%20motion%20prediction%20%28LHMP%29%20is%20essential%20for%20safely%20operating%0Aautonomous%20robots%20and%20vehicles%20in%20populated%20environments.%20It%20is%20fundamental%20for%0Avarious%20applications%2C%20including%20motion%20planning%2C%20tracking%2C%20human-robot%0Ainteraction%20and%20safety%20monitoring.%20However%2C%20accurate%20prediction%20of%20human%0Atrajectories%20is%20challenging%20due%20to%20complex%20factors%2C%20including%2C%20for%20example%2C%0Asocial%20norms%20and%20environmental%20conditions.%20The%20influence%20of%20such%20factors%20can%20be%0Acaptured%20through%20Maps%20of%20Dynamics%20%28MoDs%29%2C%20which%20encode%20spatial%20motion%20patterns%0Alearned%20from%20%28possibly%20scattered%20and%20partial%29%20past%20observations%20of%20motion%20in%0Athe%20environment%20and%20which%20can%20be%20used%20for%20data-efficient%2C%20interpretable%20motion%0Aprediction%20%28MoD-LHMP%29.%20To%20address%20the%20limitations%20of%20prior%20work%2C%20especially%0Aregarding%20accuracy%20and%20sensitivity%20to%20anomalies%20in%20long-term%20prediction%2C%20we%0Apropose%20the%20Laminar%20Component%20Enhanced%20LHMP%20approach%20%28LaCE-LHMP%29.%20Our%20approach%0Ais%20inspired%20by%20data-driven%20airflow%20modelling%2C%20which%20estimates%20laminar%20and%0Aturbulent%20flow%20components%20and%20uses%20predominantly%20the%20laminar%20components%20to%20make%0Aflow%20predictions.%20Based%20on%20the%20hypothesis%20that%20human%20trajectory%20patterns%20also%0Amanifest%20laminar%20flow%20%28that%20represents%20predictable%20motion%29%20and%20turbulent%20flow%0Acomponents%20%28that%20reflect%20more%20unpredictable%20and%20arbitrary%20motion%29%2C%20LaCE-LHMP%0Aextracts%20the%20laminar%20patterns%20in%20human%20dynamics%20and%20uses%20them%20for%20human%20motion%0Aprediction.%20We%20demonstrate%20the%20superior%20prediction%20performance%20of%20LaCE-LHMP%0Athrough%20benchmark%20comparisons%20with%20state-of-the-art%20LHMP%20methods%2C%20offering%20an%0Aunconventional%20perspective%20and%20a%20more%20intuitive%20understanding%20of%20human%20movement%0Apatterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaCE-LHMP%3A%20Airflow%20Modelling-Inspired%20Long-Term%20Human%20Motion%20Prediction%0A%20%20By%20Enhancing%20Laminar%20Characteristics%20in%20Human%20Flow&entry.906535625=Yufei%20Zhu%20and%20Han%20Fan%20and%20Andrey%20Rudenko%20and%20Martin%20Magnusson%20and%20Erik%20Schaffernicht%20and%20Achim%20J.%20Lilienthal&entry.1292438233=%20%20Long-term%20human%20motion%20prediction%20%28LHMP%29%20is%20essential%20for%20safely%20operating%0Aautonomous%20robots%20and%20vehicles%20in%20populated%20environments.%20It%20is%20fundamental%20for%0Avarious%20applications%2C%20including%20motion%20planning%2C%20tracking%2C%20human-robot%0Ainteraction%20and%20safety%20monitoring.%20However%2C%20accurate%20prediction%20of%20human%0Atrajectories%20is%20challenging%20due%20to%20complex%20factors%2C%20including%2C%20for%20example%2C%0Asocial%20norms%20and%20environmental%20conditions.%20The%20influence%20of%20such%20factors%20can%20be%0Acaptured%20through%20Maps%20of%20Dynamics%20%28MoDs%29%2C%20which%20encode%20spatial%20motion%20patterns%0Alearned%20from%20%28possibly%20scattered%20and%20partial%29%20past%20observations%20of%20motion%20in%0Athe%20environment%20and%20which%20can%20be%20used%20for%20data-efficient%2C%20interpretable%20motion%0Aprediction%20%28MoD-LHMP%29.%20To%20address%20the%20limitations%20of%20prior%20work%2C%20especially%0Aregarding%20accuracy%20and%20sensitivity%20to%20anomalies%20in%20long-term%20prediction%2C%20we%0Apropose%20the%20Laminar%20Component%20Enhanced%20LHMP%20approach%20%28LaCE-LHMP%29.%20Our%20approach%0Ais%20inspired%20by%20data-driven%20airflow%20modelling%2C%20which%20estimates%20laminar%20and%0Aturbulent%20flow%20components%20and%20uses%20predominantly%20the%20laminar%20components%20to%20make%0Aflow%20predictions.%20Based%20on%20the%20hypothesis%20that%20human%20trajectory%20patterns%20also%0Amanifest%20laminar%20flow%20%28that%20represents%20predictable%20motion%29%20and%20turbulent%20flow%0Acomponents%20%28that%20reflect%20more%20unpredictable%20and%20arbitrary%20motion%29%2C%20LaCE-LHMP%0Aextracts%20the%20laminar%20patterns%20in%20human%20dynamics%20and%20uses%20them%20for%20human%20motion%0Aprediction.%20We%20demonstrate%20the%20superior%20prediction%20performance%20of%20LaCE-LHMP%0Athrough%20benchmark%20comparisons%20with%20state-of-the-art%20LHMP%20methods%2C%20offering%20an%0Aunconventional%20perspective%20and%20a%20more%20intuitive%20understanding%20of%20human%20movement%0Apatterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13640v1&entry.124074799=Read"},
{"title": "M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via\n  Multiplier Induced Loss Landscape Scheduling", "author": "Xudong Sun and Nutan Chen and Alexej Gossmann and Yu Xing and Carla Feistner and Emilio Dorigatt and Felix Drost and Daniele Scarcella and Lisa Beer and Carsten Marr", "abstract": "  When a neural network parameterized loss function consists of many terms, the\ncombinatorial choice of weight multipliers during the optimization process\nforms a challenging problem. To address this, we proposed a probabilistic\ngraphical model (PGM) for the joint model parameter and multiplier evolution\nprocess, with a hypervolume based likelihood that promotes multi-objective\ndescent of each loss term. The corresponding parameter and multiplier\nestimation as a sequential decision process is then cast into an optimal\ncontrol problem, where the multi-objective descent goal is dispatched\nhierarchically into a series of constraint optimization sub-problems. The\nsub-problem constraint automatically adapts itself according to Pareto\ndominance and serves as the setpoint for the low level multiplier controller to\nschedule loss landscapes via output feedback of each loss term. Our method is\nmultiplier-free and operates at the timescale of epochs, thus saves tremendous\ncomputational resources compared to full training cycle multiplier tuning. We\napplied it to domain invariant variational auto-encoding with 6 loss terms on\nthe PACS domain generalization task, and observed robust performance across a\nrange of controller hyperparameters, as well as different multiplier initial\nconditions, outperforming other multiplier scheduling methods. We offered\nmodular implementation of our method, admitting custom definition of many loss\nterms for applying our multi-objective hierarchical output feedback training\nscheme to other deep learning fields.\n", "link": "http://arxiv.org/abs/2403.13728v1", "date": "2024-03-20", "relevancy": 2.0575, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5306}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5277}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4946}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling&body=Title%3A%20M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling%0AAuthor%3A%20Xudong%20Sun%20and%20Nutan%20Chen%20and%20Alexej%20Gossmann%20and%20Yu%20Xing%20and%20Carla%20Feistner%20and%20Emilio%20Dorigatt%20and%20Felix%20Drost%20and%20Daniele%20Scarcella%20and%20Lisa%20Beer%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20When%20a%20neural%20network%20parameterized%20loss%20function%20consists%20of%20many%20terms%2C%20the%0Acombinatorial%20choice%20of%20weight%20multipliers%20during%20the%20optimization%20process%0Aforms%20a%20challenging%20problem.%20To%20address%20this%2C%20we%20proposed%20a%20probabilistic%0Agraphical%20model%20%28PGM%29%20for%20the%20joint%20model%20parameter%20and%20multiplier%20evolution%0Aprocess%2C%20with%20a%20hypervolume%20based%20likelihood%20that%20promotes%20multi-objective%0Adescent%20of%20each%20loss%20term.%20The%20corresponding%20parameter%20and%20multiplier%0Aestimation%20as%20a%20sequential%20decision%20process%20is%20then%20cast%20into%20an%20optimal%0Acontrol%20problem%2C%20where%20the%20multi-objective%20descent%20goal%20is%20dispatched%0Ahierarchically%20into%20a%20series%20of%20constraint%20optimization%20sub-problems.%20The%0Asub-problem%20constraint%20automatically%20adapts%20itself%20according%20to%20Pareto%0Adominance%20and%20serves%20as%20the%20setpoint%20for%20the%20low%20level%20multiplier%20controller%20to%0Aschedule%20loss%20landscapes%20via%20output%20feedback%20of%20each%20loss%20term.%20Our%20method%20is%0Amultiplier-free%20and%20operates%20at%20the%20timescale%20of%20epochs%2C%20thus%20saves%20tremendous%0Acomputational%20resources%20compared%20to%20full%20training%20cycle%20multiplier%20tuning.%20We%0Aapplied%20it%20to%20domain%20invariant%20variational%20auto-encoding%20with%206%20loss%20terms%20on%0Athe%20PACS%20domain%20generalization%20task%2C%20and%20observed%20robust%20performance%20across%20a%0Arange%20of%20controller%20hyperparameters%2C%20as%20well%20as%20different%20multiplier%20initial%0Aconditions%2C%20outperforming%20other%20multiplier%20scheduling%20methods.%20We%20offered%0Amodular%20implementation%20of%20our%20method%2C%20admitting%20custom%20definition%20of%20many%20loss%0Aterms%20for%20applying%20our%20multi-objective%20hierarchical%20output%20feedback%20training%0Ascheme%20to%20other%20deep%20learning%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13728v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling&entry.906535625=Xudong%20Sun%20and%20Nutan%20Chen%20and%20Alexej%20Gossmann%20and%20Yu%20Xing%20and%20Carla%20Feistner%20and%20Emilio%20Dorigatt%20and%20Felix%20Drost%20and%20Daniele%20Scarcella%20and%20Lisa%20Beer%20and%20Carsten%20Marr&entry.1292438233=%20%20When%20a%20neural%20network%20parameterized%20loss%20function%20consists%20of%20many%20terms%2C%20the%0Acombinatorial%20choice%20of%20weight%20multipliers%20during%20the%20optimization%20process%0Aforms%20a%20challenging%20problem.%20To%20address%20this%2C%20we%20proposed%20a%20probabilistic%0Agraphical%20model%20%28PGM%29%20for%20the%20joint%20model%20parameter%20and%20multiplier%20evolution%0Aprocess%2C%20with%20a%20hypervolume%20based%20likelihood%20that%20promotes%20multi-objective%0Adescent%20of%20each%20loss%20term.%20The%20corresponding%20parameter%20and%20multiplier%0Aestimation%20as%20a%20sequential%20decision%20process%20is%20then%20cast%20into%20an%20optimal%0Acontrol%20problem%2C%20where%20the%20multi-objective%20descent%20goal%20is%20dispatched%0Ahierarchically%20into%20a%20series%20of%20constraint%20optimization%20sub-problems.%20The%0Asub-problem%20constraint%20automatically%20adapts%20itself%20according%20to%20Pareto%0Adominance%20and%20serves%20as%20the%20setpoint%20for%20the%20low%20level%20multiplier%20controller%20to%0Aschedule%20loss%20landscapes%20via%20output%20feedback%20of%20each%20loss%20term.%20Our%20method%20is%0Amultiplier-free%20and%20operates%20at%20the%20timescale%20of%20epochs%2C%20thus%20saves%20tremendous%0Acomputational%20resources%20compared%20to%20full%20training%20cycle%20multiplier%20tuning.%20We%0Aapplied%20it%20to%20domain%20invariant%20variational%20auto-encoding%20with%206%20loss%20terms%20on%0Athe%20PACS%20domain%20generalization%20task%2C%20and%20observed%20robust%20performance%20across%20a%0Arange%20of%20controller%20hyperparameters%2C%20as%20well%20as%20different%20multiplier%20initial%0Aconditions%2C%20outperforming%20other%20multiplier%20scheduling%20methods.%20We%20offered%0Amodular%20implementation%20of%20our%20method%2C%20admitting%20custom%20definition%20of%20many%20loss%0Aterms%20for%20applying%20our%20multi-objective%20hierarchical%20output%20feedback%20training%0Ascheme%20to%20other%20deep%20learning%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13728v1&entry.124074799=Read"},
{"title": "AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes", "author": "Rao Fu and Zehao Wen and Zichen Liu and Srinath Sridhar", "abstract": "  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n", "link": "http://arxiv.org/abs/2312.06644v2", "date": "2024-03-20", "relevancy": 2.0568, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5439}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnyHome%3A%20Open-Vocabulary%20Generation%20of%20Structured%20and%20Textured%203D%20Homes&body=Title%3A%20AnyHome%3A%20Open-Vocabulary%20Generation%20of%20Structured%20and%20Textured%203D%20Homes%0AAuthor%3A%20Rao%20Fu%20and%20Zehao%20Wen%20and%20Zichen%20Liu%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Inspired%20by%20cognitive%20theories%2C%20we%20introduce%20AnyHome%2C%20a%20framework%20that%0Atranslates%20any%20text%20into%20well-structured%20and%20textured%20indoor%20scenes%20at%20a%0Ahouse-scale.%20By%20prompting%20Large%20Language%20Models%20%28LLMs%29%20with%20designed%20templates%2C%0Aour%20approach%20converts%20provided%20textual%20narratives%20into%20amodal%20structured%0Arepresentations.%20These%20representations%20guarantee%20consistent%20and%20realistic%0Aspatial%20layouts%20by%20directing%20the%20synthesis%20of%20a%20geometry%20mesh%20within%20defined%0Aconstraints.%20A%20Score%20Distillation%20Sampling%20process%20is%20then%20employed%20to%20refine%0Athe%20geometry%2C%20followed%20by%20an%20egocentric%20inpainting%20process%20that%20adds%20lifelike%0Atextures%20to%20it.%20AnyHome%20stands%20out%20with%20its%20editability%2C%20customizability%2C%0Adiversity%2C%20and%20realism.%20The%20structured%20representations%20for%20scenes%20allow%20for%0Aextensive%20editing%20at%20varying%20levels%20of%20granularity.%20Capable%20of%20interpreting%0Atexts%20ranging%20from%20simple%20labels%20to%20detailed%20narratives%2C%20AnyHome%20generates%0Adetailed%20geometries%20and%20textures%20that%20outperform%20existing%20methods%20in%20both%0Aquantitative%20and%20qualitative%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06644v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyHome%3A%20Open-Vocabulary%20Generation%20of%20Structured%20and%20Textured%203D%20Homes&entry.906535625=Rao%20Fu%20and%20Zehao%20Wen%20and%20Zichen%20Liu%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Inspired%20by%20cognitive%20theories%2C%20we%20introduce%20AnyHome%2C%20a%20framework%20that%0Atranslates%20any%20text%20into%20well-structured%20and%20textured%20indoor%20scenes%20at%20a%0Ahouse-scale.%20By%20prompting%20Large%20Language%20Models%20%28LLMs%29%20with%20designed%20templates%2C%0Aour%20approach%20converts%20provided%20textual%20narratives%20into%20amodal%20structured%0Arepresentations.%20These%20representations%20guarantee%20consistent%20and%20realistic%0Aspatial%20layouts%20by%20directing%20the%20synthesis%20of%20a%20geometry%20mesh%20within%20defined%0Aconstraints.%20A%20Score%20Distillation%20Sampling%20process%20is%20then%20employed%20to%20refine%0Athe%20geometry%2C%20followed%20by%20an%20egocentric%20inpainting%20process%20that%20adds%20lifelike%0Atextures%20to%20it.%20AnyHome%20stands%20out%20with%20its%20editability%2C%20customizability%2C%0Adiversity%2C%20and%20realism.%20The%20structured%20representations%20for%20scenes%20allow%20for%0Aextensive%20editing%20at%20varying%20levels%20of%20granularity.%20Capable%20of%20interpreting%0Atexts%20ranging%20from%20simple%20labels%20to%20detailed%20narratives%2C%20AnyHome%20generates%0Adetailed%20geometries%20and%20textures%20that%20outperform%20existing%20methods%20in%20both%0Aquantitative%20and%20qualitative%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06644v2&entry.124074799=Read"},
{"title": "Poly Kernel Inception Network for Remote Sensing Detection", "author": "Xinhao Cai and Qiuxia Lai and Yuwei Wang and Wenguan Wang and Zeren Sun and Yazhou Yao", "abstract": "  Object detection in remote sensing images (RSIs) often suffers from several\nincreasing challenges, including the large variation in object scales and the\ndiverse-ranging context. Prior methods tried to address these challenges by\nexpanding the spatial receptive field of the backbone, either through\nlarge-kernel convolution or dilated convolution. However, the former typically\nintroduces considerable background noise, while the latter risks generating\noverly sparse feature representations. In this paper, we introduce the Poly\nKernel Inception Network (PKINet) to handle the above challenges. PKINet\nemploys multi-scale convolution kernels without dilation to extract object\nfeatures of varying scales and capture local context. In addition, a Context\nAnchor Attention (CAA) module is introduced in parallel to capture long-range\ncontextual information. These two components work jointly to advance the\nperformance of PKINet on four challenging remote sensing detection benchmarks,\nnamely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.\n", "link": "http://arxiv.org/abs/2403.06258v2", "date": "2024-03-20", "relevancy": 2.0322, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5135}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5096}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Poly%20Kernel%20Inception%20Network%20for%20Remote%20Sensing%20Detection&body=Title%3A%20Poly%20Kernel%20Inception%20Network%20for%20Remote%20Sensing%20Detection%0AAuthor%3A%20Xinhao%20Cai%20and%20Qiuxia%20Lai%20and%20Yuwei%20Wang%20and%20Wenguan%20Wang%20and%20Zeren%20Sun%20and%20Yazhou%20Yao%0AAbstract%3A%20%20%20Object%20detection%20in%20remote%20sensing%20images%20%28RSIs%29%20often%20suffers%20from%20several%0Aincreasing%20challenges%2C%20including%20the%20large%20variation%20in%20object%20scales%20and%20the%0Adiverse-ranging%20context.%20Prior%20methods%20tried%20to%20address%20these%20challenges%20by%0Aexpanding%20the%20spatial%20receptive%20field%20of%20the%20backbone%2C%20either%20through%0Alarge-kernel%20convolution%20or%20dilated%20convolution.%20However%2C%20the%20former%20typically%0Aintroduces%20considerable%20background%20noise%2C%20while%20the%20latter%20risks%20generating%0Aoverly%20sparse%20feature%20representations.%20In%20this%20paper%2C%20we%20introduce%20the%20Poly%0AKernel%20Inception%20Network%20%28PKINet%29%20to%20handle%20the%20above%20challenges.%20PKINet%0Aemploys%20multi-scale%20convolution%20kernels%20without%20dilation%20to%20extract%20object%0Afeatures%20of%20varying%20scales%20and%20capture%20local%20context.%20In%20addition%2C%20a%20Context%0AAnchor%20Attention%20%28CAA%29%20module%20is%20introduced%20in%20parallel%20to%20capture%20long-range%0Acontextual%20information.%20These%20two%20components%20work%20jointly%20to%20advance%20the%0Aperformance%20of%20PKINet%20on%20four%20challenging%20remote%20sensing%20detection%20benchmarks%2C%0Anamely%20DOTA-v1.0%2C%20DOTA-v1.5%2C%20HRSC2016%2C%20and%20DIOR-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06258v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poly%20Kernel%20Inception%20Network%20for%20Remote%20Sensing%20Detection&entry.906535625=Xinhao%20Cai%20and%20Qiuxia%20Lai%20and%20Yuwei%20Wang%20and%20Wenguan%20Wang%20and%20Zeren%20Sun%20and%20Yazhou%20Yao&entry.1292438233=%20%20Object%20detection%20in%20remote%20sensing%20images%20%28RSIs%29%20often%20suffers%20from%20several%0Aincreasing%20challenges%2C%20including%20the%20large%20variation%20in%20object%20scales%20and%20the%0Adiverse-ranging%20context.%20Prior%20methods%20tried%20to%20address%20these%20challenges%20by%0Aexpanding%20the%20spatial%20receptive%20field%20of%20the%20backbone%2C%20either%20through%0Alarge-kernel%20convolution%20or%20dilated%20convolution.%20However%2C%20the%20former%20typically%0Aintroduces%20considerable%20background%20noise%2C%20while%20the%20latter%20risks%20generating%0Aoverly%20sparse%20feature%20representations.%20In%20this%20paper%2C%20we%20introduce%20the%20Poly%0AKernel%20Inception%20Network%20%28PKINet%29%20to%20handle%20the%20above%20challenges.%20PKINet%0Aemploys%20multi-scale%20convolution%20kernels%20without%20dilation%20to%20extract%20object%0Afeatures%20of%20varying%20scales%20and%20capture%20local%20context.%20In%20addition%2C%20a%20Context%0AAnchor%20Attention%20%28CAA%29%20module%20is%20introduced%20in%20parallel%20to%20capture%20long-range%0Acontextual%20information.%20These%20two%20components%20work%20jointly%20to%20advance%20the%0Aperformance%20of%20PKINet%20on%20four%20challenging%20remote%20sensing%20detection%20benchmarks%2C%0Anamely%20DOTA-v1.0%2C%20DOTA-v1.5%2C%20HRSC2016%2C%20and%20DIOR-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06258v2&entry.124074799=Read"},
{"title": "Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction", "author": "Florian Strohm and Mihai B\u00e2ce and Andreas Bulling", "abstract": "  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n", "link": "http://arxiv.org/abs/2403.13653v1", "date": "2024-03-20", "relevancy": 2.0197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20User%20Embeddings%20from%20Human%20Gaze%20for%20Personalised%20Saliency%0A%20%20Prediction&body=Title%3A%20Learning%20User%20Embeddings%20from%20Human%20Gaze%20for%20Personalised%20Saliency%0A%20%20Prediction%0AAuthor%3A%20Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20Reusable%20embeddings%20of%20user%20behaviour%20have%20shown%20significant%20performance%0Aimprovements%20for%20the%20personalised%20saliency%20prediction%20task.%20However%2C%20prior%0Aworks%20require%20explicit%20user%20characteristics%20and%20preferences%20as%20input%2C%20which%20are%0Aoften%20difficult%20to%20obtain.%20We%20present%20a%20novel%20method%20to%20extract%20user%20embeddings%0Afrom%20pairs%20of%20natural%20images%20and%20corresponding%20saliency%20maps%20generated%20from%20a%0Asmall%20amount%20of%20user-specific%20eye%20tracking%20data.%20At%20the%20core%20of%20our%20method%20is%20a%0ASiamese%20convolutional%20neural%20encoder%20that%20learns%20the%20user%20embeddings%20by%0Acontrasting%20the%20image%20and%20personal%20saliency%20map%20pairs%20of%20different%20users.%0AEvaluations%20on%20two%20public%20saliency%20datasets%20show%20that%20the%20generated%20embeddings%0Ahave%20high%20discriminative%20power%2C%20are%20effective%20at%20refining%20universal%20saliency%0Amaps%20to%20the%20individual%20users%2C%20and%20generalise%20well%20across%20users%20and%20images.%0AFinally%2C%20based%20on%20our%20model%27s%20ability%20to%20encode%20individual%20user%0Acharacteristics%2C%20our%20work%20points%20towards%20other%20applications%20that%20can%20benefit%0Afrom%20reusable%20embeddings%20of%20gaze%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13653v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20User%20Embeddings%20from%20Human%20Gaze%20for%20Personalised%20Saliency%0A%20%20Prediction&entry.906535625=Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling&entry.1292438233=%20%20Reusable%20embeddings%20of%20user%20behaviour%20have%20shown%20significant%20performance%0Aimprovements%20for%20the%20personalised%20saliency%20prediction%20task.%20However%2C%20prior%0Aworks%20require%20explicit%20user%20characteristics%20and%20preferences%20as%20input%2C%20which%20are%0Aoften%20difficult%20to%20obtain.%20We%20present%20a%20novel%20method%20to%20extract%20user%20embeddings%0Afrom%20pairs%20of%20natural%20images%20and%20corresponding%20saliency%20maps%20generated%20from%20a%0Asmall%20amount%20of%20user-specific%20eye%20tracking%20data.%20At%20the%20core%20of%20our%20method%20is%20a%0ASiamese%20convolutional%20neural%20encoder%20that%20learns%20the%20user%20embeddings%20by%0Acontrasting%20the%20image%20and%20personal%20saliency%20map%20pairs%20of%20different%20users.%0AEvaluations%20on%20two%20public%20saliency%20datasets%20show%20that%20the%20generated%20embeddings%0Ahave%20high%20discriminative%20power%2C%20are%20effective%20at%20refining%20universal%20saliency%0Amaps%20to%20the%20individual%20users%2C%20and%20generalise%20well%20across%20users%20and%20images.%0AFinally%2C%20based%20on%20our%20model%27s%20ability%20to%20encode%20individual%20user%0Acharacteristics%2C%20our%20work%20points%20towards%20other%20applications%20that%20can%20benefit%0Afrom%20reusable%20embeddings%20of%20gaze%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13653v1&entry.124074799=Read"},
{"title": "Real-Fake: Effective Training Data Synthesis Through Distribution\n  Matching", "author": "Jianhao Yuan and Jie Zhang and Shuyang Sun and Philip Torr and Bo Zhao", "abstract": "  Synthetic training data has gained prominence in numerous learning tasks and\nscenarios, offering advantages such as dataset augmentation, generalization\nevaluation, and privacy preservation. Despite these benefits, the efficiency of\nsynthetic data generated by current methodologies remains inferior when\ntraining advanced deep models exclusively, limiting its practical utility. To\naddress this challenge, we analyze the principles underlying training data\nsynthesis for supervised learning and elucidate a principled theoretical\nframework from the distribution-matching perspective that explicates the\nmechanisms governing synthesis efficacy. Through extensive experiments, we\ndemonstrate the effectiveness of our synthetic data across diverse image\nclassification tasks, both as a replacement for and augmentation to real\ndatasets, while also benefits such as out-of-distribution generalization,\nprivacy preservation, and scalability. Specifically, we achieve 70.9% top1\nclassification accuracy on ImageNet1K when training solely with synthetic data\nequivalent to 1 X the original real data size, which increases to 76.0% when\nscaling up to 10 X synthetic data.\n", "link": "http://arxiv.org/abs/2310.10402v2", "date": "2024-03-20", "relevancy": 2.0152, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5215}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-Fake%3A%20Effective%20Training%20Data%20Synthesis%20Through%20Distribution%0A%20%20Matching&body=Title%3A%20Real-Fake%3A%20Effective%20Training%20Data%20Synthesis%20Through%20Distribution%0A%20%20Matching%0AAuthor%3A%20Jianhao%20Yuan%20and%20Jie%20Zhang%20and%20Shuyang%20Sun%20and%20Philip%20Torr%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20Synthetic%20training%20data%20has%20gained%20prominence%20in%20numerous%20learning%20tasks%20and%0Ascenarios%2C%20offering%20advantages%20such%20as%20dataset%20augmentation%2C%20generalization%0Aevaluation%2C%20and%20privacy%20preservation.%20Despite%20these%20benefits%2C%20the%20efficiency%20of%0Asynthetic%20data%20generated%20by%20current%20methodologies%20remains%20inferior%20when%0Atraining%20advanced%20deep%20models%20exclusively%2C%20limiting%20its%20practical%20utility.%20To%0Aaddress%20this%20challenge%2C%20we%20analyze%20the%20principles%20underlying%20training%20data%0Asynthesis%20for%20supervised%20learning%20and%20elucidate%20a%20principled%20theoretical%0Aframework%20from%20the%20distribution-matching%20perspective%20that%20explicates%20the%0Amechanisms%20governing%20synthesis%20efficacy.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20synthetic%20data%20across%20diverse%20image%0Aclassification%20tasks%2C%20both%20as%20a%20replacement%20for%20and%20augmentation%20to%20real%0Adatasets%2C%20while%20also%20benefits%20such%20as%20out-of-distribution%20generalization%2C%0Aprivacy%20preservation%2C%20and%20scalability.%20Specifically%2C%20we%20achieve%2070.9%25%20top1%0Aclassification%20accuracy%20on%20ImageNet1K%20when%20training%20solely%20with%20synthetic%20data%0Aequivalent%20to%201%20X%20the%20original%20real%20data%20size%2C%20which%20increases%20to%2076.0%25%20when%0Ascaling%20up%20to%2010%20X%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10402v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Fake%3A%20Effective%20Training%20Data%20Synthesis%20Through%20Distribution%0A%20%20Matching&entry.906535625=Jianhao%20Yuan%20and%20Jie%20Zhang%20and%20Shuyang%20Sun%20and%20Philip%20Torr%20and%20Bo%20Zhao&entry.1292438233=%20%20Synthetic%20training%20data%20has%20gained%20prominence%20in%20numerous%20learning%20tasks%20and%0Ascenarios%2C%20offering%20advantages%20such%20as%20dataset%20augmentation%2C%20generalization%0Aevaluation%2C%20and%20privacy%20preservation.%20Despite%20these%20benefits%2C%20the%20efficiency%20of%0Asynthetic%20data%20generated%20by%20current%20methodologies%20remains%20inferior%20when%0Atraining%20advanced%20deep%20models%20exclusively%2C%20limiting%20its%20practical%20utility.%20To%0Aaddress%20this%20challenge%2C%20we%20analyze%20the%20principles%20underlying%20training%20data%0Asynthesis%20for%20supervised%20learning%20and%20elucidate%20a%20principled%20theoretical%0Aframework%20from%20the%20distribution-matching%20perspective%20that%20explicates%20the%0Amechanisms%20governing%20synthesis%20efficacy.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20our%20synthetic%20data%20across%20diverse%20image%0Aclassification%20tasks%2C%20both%20as%20a%20replacement%20for%20and%20augmentation%20to%20real%0Adatasets%2C%20while%20also%20benefits%20such%20as%20out-of-distribution%20generalization%2C%0Aprivacy%20preservation%2C%20and%20scalability.%20Specifically%2C%20we%20achieve%2070.9%25%20top1%0Aclassification%20accuracy%20on%20ImageNet1K%20when%20training%20solely%20with%20synthetic%20data%0Aequivalent%20to%201%20X%20the%20original%20real%20data%20size%2C%20which%20increases%20to%2076.0%25%20when%0Ascaling%20up%20to%2010%20X%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10402v2&entry.124074799=Read"},
{"title": "High-confidence pseudo-labels for domain adaptation in COVID-19\n  detection", "author": "Robert Turnbull and Simon Mutch", "abstract": "  This paper outlines our submission for the 4th COV19D competition as part of\nthe `Domain adaptation, Explainability, Fairness in AI for Medical Image\nAnalysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition\nConference (CVPR). The competition consists of two challenges. The first is to\ntrain a classifier to detect the presence of COVID-19 from over one thousand CT\nscans from the COV19-CT-DB database. The second challenge is to perform domain\nadaptation by taking the dataset from Challenge 1 and adding a small number of\nscans (some annotated and other not) for a different distribution. We\npreprocessed the CT scans to segment the lungs, and output volumes with the\nlungs individually and together. We then trained 3D ResNet and Swin Transformer\nmodels on these inputs. We annotated the unlabeled CT scans using an ensemble\nof these models and chose the high-confidence predictions as pseudo-labels for\nfine-tuning. This resulted in a best cross-validation mean F1 score of 93.39\\%\nfor Challenge 1 and a mean F1 score of 92.15 for Challenge 2.\n", "link": "http://arxiv.org/abs/2403.13509v1", "date": "2024-03-20", "relevancy": 2.0144, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5678}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20High-confidence%20pseudo-labels%20for%20domain%20adaptation%20in%20COVID-19%0A%20%20detection&body=Title%3A%20High-confidence%20pseudo-labels%20for%20domain%20adaptation%20in%20COVID-19%0A%20%20detection%0AAuthor%3A%20Robert%20Turnbull%20and%20Simon%20Mutch%0AAbstract%3A%20%20%20This%20paper%20outlines%20our%20submission%20for%20the%204th%20COV19D%20competition%20as%20part%20of%0Athe%20%60Domain%20adaptation%2C%20Explainability%2C%20Fairness%20in%20AI%20for%20Medical%20Image%0AAnalysis%27%20%28DEF-AI-MIA%29%20workshop%20at%20the%20Computer%20Vision%20and%20Pattern%20Recognition%0AConference%20%28CVPR%29.%20The%20competition%20consists%20of%20two%20challenges.%20The%20first%20is%20to%0Atrain%20a%20classifier%20to%20detect%20the%20presence%20of%20COVID-19%20from%20over%20one%20thousand%20CT%0Ascans%20from%20the%20COV19-CT-DB%20database.%20The%20second%20challenge%20is%20to%20perform%20domain%0Aadaptation%20by%20taking%20the%20dataset%20from%20Challenge%201%20and%20adding%20a%20small%20number%20of%0Ascans%20%28some%20annotated%20and%20other%20not%29%20for%20a%20different%20distribution.%20We%0Apreprocessed%20the%20CT%20scans%20to%20segment%20the%20lungs%2C%20and%20output%20volumes%20with%20the%0Alungs%20individually%20and%20together.%20We%20then%20trained%203D%20ResNet%20and%20Swin%20Transformer%0Amodels%20on%20these%20inputs.%20We%20annotated%20the%20unlabeled%20CT%20scans%20using%20an%20ensemble%0Aof%20these%20models%20and%20chose%20the%20high-confidence%20predictions%20as%20pseudo-labels%20for%0Afine-tuning.%20This%20resulted%20in%20a%20best%20cross-validation%20mean%20F1%20score%20of%2093.39%5C%25%0Afor%20Challenge%201%20and%20a%20mean%20F1%20score%20of%2092.15%20for%20Challenge%202.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13509v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-confidence%20pseudo-labels%20for%20domain%20adaptation%20in%20COVID-19%0A%20%20detection&entry.906535625=Robert%20Turnbull%20and%20Simon%20Mutch&entry.1292438233=%20%20This%20paper%20outlines%20our%20submission%20for%20the%204th%20COV19D%20competition%20as%20part%20of%0Athe%20%60Domain%20adaptation%2C%20Explainability%2C%20Fairness%20in%20AI%20for%20Medical%20Image%0AAnalysis%27%20%28DEF-AI-MIA%29%20workshop%20at%20the%20Computer%20Vision%20and%20Pattern%20Recognition%0AConference%20%28CVPR%29.%20The%20competition%20consists%20of%20two%20challenges.%20The%20first%20is%20to%0Atrain%20a%20classifier%20to%20detect%20the%20presence%20of%20COVID-19%20from%20over%20one%20thousand%20CT%0Ascans%20from%20the%20COV19-CT-DB%20database.%20The%20second%20challenge%20is%20to%20perform%20domain%0Aadaptation%20by%20taking%20the%20dataset%20from%20Challenge%201%20and%20adding%20a%20small%20number%20of%0Ascans%20%28some%20annotated%20and%20other%20not%29%20for%20a%20different%20distribution.%20We%0Apreprocessed%20the%20CT%20scans%20to%20segment%20the%20lungs%2C%20and%20output%20volumes%20with%20the%0Alungs%20individually%20and%20together.%20We%20then%20trained%203D%20ResNet%20and%20Swin%20Transformer%0Amodels%20on%20these%20inputs.%20We%20annotated%20the%20unlabeled%20CT%20scans%20using%20an%20ensemble%0Aof%20these%20models%20and%20chose%20the%20high-confidence%20predictions%20as%20pseudo-labels%20for%0Afine-tuning.%20This%20resulted%20in%20a%20best%20cross-validation%20mean%20F1%20score%20of%2093.39%5C%25%0Afor%20Challenge%201%20and%20a%20mean%20F1%20score%20of%2092.15%20for%20Challenge%202.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13509v1&entry.124074799=Read"},
{"title": "In Search of Truth: An Interrogation Approach to Hallucination Detection", "author": "Yakir Yehuda and Itzik Malkiel and Oren Barkan and Jonathan Weill and Royi Ronen and Noam Koenigstein", "abstract": "  Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.\n", "link": "http://arxiv.org/abs/2403.02889v2", "date": "2024-03-20", "relevancy": 2.0115, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5346}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5078}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20In%20Search%20of%20Truth%3A%20An%20Interrogation%20Approach%20to%20Hallucination%20Detection&body=Title%3A%20In%20Search%20of%20Truth%3A%20An%20Interrogation%20Approach%20to%20Hallucination%20Detection%0AAuthor%3A%20Yakir%20Yehuda%20and%20Itzik%20Malkiel%20and%20Oren%20Barkan%20and%20Jonathan%20Weill%20and%20Royi%20Ronen%20and%20Noam%20Koenigstein%0AAbstract%3A%20%20%20Despite%20the%20many%20advances%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20their%0Aunprecedented%20rapid%20evolution%2C%20their%20impact%20and%20integration%20into%20every%20facet%20of%0Aour%20daily%20lives%20is%20limited%20due%20to%20various%20reasons.%20One%20critical%20factor%0Ahindering%20their%20widespread%20adoption%20is%20the%20occurrence%20of%20hallucinations%2C%20where%0ALLMs%20invent%20answers%20that%20sound%20realistic%2C%20yet%20drift%20away%20from%20factual%20truth.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20method%20for%20detecting%20hallucinations%20in%20large%0Alanguage%20models%2C%20which%20tackles%20a%20critical%20issue%20in%20the%20adoption%20of%20these%20models%0Ain%20various%20real-world%20scenarios.%20Through%20extensive%20evaluations%20across%20multiple%0Adatasets%20and%20LLMs%2C%20including%20Llama-2%2C%20we%20study%20the%20hallucination%20levels%20of%0Avarious%20recent%20LLMs%20and%20demonstrate%20the%20effectiveness%20of%20our%20method%20to%0Aautomatically%20detect%20them.%20Notably%2C%20we%20observe%20up%20to%2062%25%20hallucinations%20for%0ALlama-2%20in%20a%20specific%20experiment%2C%20where%20our%20method%20achieves%20a%20Balanced%20Accuracy%0A%28B-ACC%29%20of%2087%25%2C%20all%20without%20relying%20on%20external%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02889v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Search%20of%20Truth%3A%20An%20Interrogation%20Approach%20to%20Hallucination%20Detection&entry.906535625=Yakir%20Yehuda%20and%20Itzik%20Malkiel%20and%20Oren%20Barkan%20and%20Jonathan%20Weill%20and%20Royi%20Ronen%20and%20Noam%20Koenigstein&entry.1292438233=%20%20Despite%20the%20many%20advances%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20their%0Aunprecedented%20rapid%20evolution%2C%20their%20impact%20and%20integration%20into%20every%20facet%20of%0Aour%20daily%20lives%20is%20limited%20due%20to%20various%20reasons.%20One%20critical%20factor%0Ahindering%20their%20widespread%20adoption%20is%20the%20occurrence%20of%20hallucinations%2C%20where%0ALLMs%20invent%20answers%20that%20sound%20realistic%2C%20yet%20drift%20away%20from%20factual%20truth.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20method%20for%20detecting%20hallucinations%20in%20large%0Alanguage%20models%2C%20which%20tackles%20a%20critical%20issue%20in%20the%20adoption%20of%20these%20models%0Ain%20various%20real-world%20scenarios.%20Through%20extensive%20evaluations%20across%20multiple%0Adatasets%20and%20LLMs%2C%20including%20Llama-2%2C%20we%20study%20the%20hallucination%20levels%20of%0Avarious%20recent%20LLMs%20and%20demonstrate%20the%20effectiveness%20of%20our%20method%20to%0Aautomatically%20detect%20them.%20Notably%2C%20we%20observe%20up%20to%2062%25%20hallucinations%20for%0ALlama-2%20in%20a%20specific%20experiment%2C%20where%20our%20method%20achieves%20a%20Balanced%20Accuracy%0A%28B-ACC%29%20of%2087%25%2C%20all%20without%20relying%20on%20external%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02889v2&entry.124074799=Read"},
{"title": "MoST: Motion Style Transformer between Diverse Action Contents", "author": "Boeun Kim and Jungho Kim and Hyung Jin Chang and Jin Young Choi", "abstract": "  While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with `part-attentive style modulator across body\nparts' and `Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.\n", "link": "http://arxiv.org/abs/2403.06225v2", "date": "2024-03-20", "relevancy": 1.9979, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.566}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoST%3A%20Motion%20Style%20Transformer%20between%20Diverse%20Action%20Contents&body=Title%3A%20MoST%3A%20Motion%20Style%20Transformer%20between%20Diverse%20Action%20Contents%0AAuthor%3A%20Boeun%20Kim%20and%20Jungho%20Kim%20and%20Hyung%20Jin%20Chang%20and%20Jin%20Young%20Choi%0AAbstract%3A%20%20%20While%20existing%20motion%20style%20transfer%20methods%20are%20effective%20between%20two%0Amotions%20with%20identical%20content%2C%20their%20performance%20significantly%20diminishes%20when%0Atransferring%20style%20between%20motions%20with%20different%20contents.%20This%20challenge%20lies%0Ain%20the%20lack%20of%20clear%20separation%20between%20content%20and%20style%20of%20a%20motion.%20To%0Atackle%20this%20challenge%2C%20we%20propose%20a%20novel%20motion%20style%20transformer%20that%0Aeffectively%20disentangles%20style%20from%20content%20and%20generates%20a%20plausible%20motion%0Awith%20transferred%20style%20from%20a%20source%20motion.%20Our%20distinctive%20approach%20to%0Aachieving%20the%20goal%20of%20disentanglement%20is%20twofold%3A%20%281%29%20a%20new%20architecture%20for%0Amotion%20style%20transformer%20with%20%60part-attentive%20style%20modulator%20across%20body%0Aparts%27%20and%20%60Siamese%20encoders%20that%20encode%20style%20and%20content%20features%0Aseparately%27%3B%20%282%29%20style%20disentanglement%20loss.%20Our%20method%20outperforms%20existing%0Amethods%20and%20demonstrates%20exceptionally%20high%20quality%2C%20particularly%20in%20motion%0Apairs%20with%20different%20contents%2C%20without%20the%20need%20for%20heuristic%20post-processing.%0ACodes%20are%20available%20at%20https%3A//github.com/Boeun-Kim/MoST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06225v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoST%3A%20Motion%20Style%20Transformer%20between%20Diverse%20Action%20Contents&entry.906535625=Boeun%20Kim%20and%20Jungho%20Kim%20and%20Hyung%20Jin%20Chang%20and%20Jin%20Young%20Choi&entry.1292438233=%20%20While%20existing%20motion%20style%20transfer%20methods%20are%20effective%20between%20two%0Amotions%20with%20identical%20content%2C%20their%20performance%20significantly%20diminishes%20when%0Atransferring%20style%20between%20motions%20with%20different%20contents.%20This%20challenge%20lies%0Ain%20the%20lack%20of%20clear%20separation%20between%20content%20and%20style%20of%20a%20motion.%20To%0Atackle%20this%20challenge%2C%20we%20propose%20a%20novel%20motion%20style%20transformer%20that%0Aeffectively%20disentangles%20style%20from%20content%20and%20generates%20a%20plausible%20motion%0Awith%20transferred%20style%20from%20a%20source%20motion.%20Our%20distinctive%20approach%20to%0Aachieving%20the%20goal%20of%20disentanglement%20is%20twofold%3A%20%281%29%20a%20new%20architecture%20for%0Amotion%20style%20transformer%20with%20%60part-attentive%20style%20modulator%20across%20body%0Aparts%27%20and%20%60Siamese%20encoders%20that%20encode%20style%20and%20content%20features%0Aseparately%27%3B%20%282%29%20style%20disentanglement%20loss.%20Our%20method%20outperforms%20existing%0Amethods%20and%20demonstrates%20exceptionally%20high%20quality%2C%20particularly%20in%20motion%0Apairs%20with%20different%20contents%2C%20without%20the%20need%20for%20heuristic%20post-processing.%0ACodes%20are%20available%20at%20https%3A//github.com/Boeun-Kim/MoST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06225v2&entry.124074799=Read"},
{"title": "Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS\n  to Exclude Abnormal Input", "author": "Zhihao Cao", "abstract": "  Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for\nmonitoring brain activity. To better understand the brain, researchers often\nuse deep learning to address the classification challenges of fNIRS data. Our\nstudy shows that while current networks in fNIRS are highly accurate for\npredictions within their training distribution, they falter at identifying and\nexcluding abnormal data which is out-of-distribution, affecting their\nreliability. We propose integrating metric learning and supervised methods into\nfNIRS research to improve networks capability in identifying and excluding\nout-of-distribution outliers. This method is simple yet effective. In our\nexperiments, it significantly enhances the performance of various networks in\nfNIRS, particularly transformer-based one, which shows the great improvement in\nreliability. We will make our experiment data available on GitHub.\n", "link": "http://arxiv.org/abs/2402.18112v2", "date": "2024-03-20", "relevancy": 1.9925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5076}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4956}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4809}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20But%20Effective%3A%20Rethinking%20the%20Ability%20of%20Deep%20Learning%20in%20fNIRS%0A%20%20to%20Exclude%20Abnormal%20Input&body=Title%3A%20Simple%20But%20Effective%3A%20Rethinking%20the%20Ability%20of%20Deep%20Learning%20in%20fNIRS%0A%20%20to%20Exclude%20Abnormal%20Input%0AAuthor%3A%20Zhihao%20Cao%0AAbstract%3A%20%20%20Functional%20near-infrared%20spectroscopy%20%28fNIRS%29%20is%20a%20non-invasive%20technique%20for%0Amonitoring%20brain%20activity.%20To%20better%20understand%20the%20brain%2C%20researchers%20often%0Ause%20deep%20learning%20to%20address%20the%20classification%20challenges%20of%20fNIRS%20data.%20Our%0Astudy%20shows%20that%20while%20current%20networks%20in%20fNIRS%20are%20highly%20accurate%20for%0Apredictions%20within%20their%20training%20distribution%2C%20they%20falter%20at%20identifying%20and%0Aexcluding%20abnormal%20data%20which%20is%20out-of-distribution%2C%20affecting%20their%0Areliability.%20We%20propose%20integrating%20metric%20learning%20and%20supervised%20methods%20into%0AfNIRS%20research%20to%20improve%20networks%20capability%20in%20identifying%20and%20excluding%0Aout-of-distribution%20outliers.%20This%20method%20is%20simple%20yet%20effective.%20In%20our%0Aexperiments%2C%20it%20significantly%20enhances%20the%20performance%20of%20various%20networks%20in%0AfNIRS%2C%20particularly%20transformer-based%20one%2C%20which%20shows%20the%20great%20improvement%20in%0Areliability.%20We%20will%20make%20our%20experiment%20data%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18112v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20But%20Effective%3A%20Rethinking%20the%20Ability%20of%20Deep%20Learning%20in%20fNIRS%0A%20%20to%20Exclude%20Abnormal%20Input&entry.906535625=Zhihao%20Cao&entry.1292438233=%20%20Functional%20near-infrared%20spectroscopy%20%28fNIRS%29%20is%20a%20non-invasive%20technique%20for%0Amonitoring%20brain%20activity.%20To%20better%20understand%20the%20brain%2C%20researchers%20often%0Ause%20deep%20learning%20to%20address%20the%20classification%20challenges%20of%20fNIRS%20data.%20Our%0Astudy%20shows%20that%20while%20current%20networks%20in%20fNIRS%20are%20highly%20accurate%20for%0Apredictions%20within%20their%20training%20distribution%2C%20they%20falter%20at%20identifying%20and%0Aexcluding%20abnormal%20data%20which%20is%20out-of-distribution%2C%20affecting%20their%0Areliability.%20We%20propose%20integrating%20metric%20learning%20and%20supervised%20methods%20into%0AfNIRS%20research%20to%20improve%20networks%20capability%20in%20identifying%20and%20excluding%0Aout-of-distribution%20outliers.%20This%20method%20is%20simple%20yet%20effective.%20In%20our%0Aexperiments%2C%20it%20significantly%20enhances%20the%20performance%20of%20various%20networks%20in%0AfNIRS%2C%20particularly%20transformer-based%20one%2C%20which%20shows%20the%20great%20improvement%20in%0Areliability.%20We%20will%20make%20our%20experiment%20data%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18112v2&entry.124074799=Read"},
{"title": "ProMamba: Prompt-Mamba for polyp segmentation", "author": "Jianhao Xie and Ruofan Liao and Ziang Zhang and Sida Yi and Yuesheng Zhu and Guibo Luo", "abstract": "  Detecting polyps through colonoscopy is an important task in medical image\nsegmentation, which provides significant assistance and reference value for\nclinical surgery. However, accurate segmentation of polyps is a challenging\ntask due to two main reasons. Firstly, polyps exhibit various shapes and\ncolors. Secondly, the boundaries between polyps and their normal surroundings\nare often unclear. Additionally, significant differences between different\ndatasets lead to limited generalization capabilities of existing methods. To\naddress these issues, we propose a segmentation model based on Prompt-Mamba,\nwhich incorporates the latest Vision-Mamba and prompt technologies. Compared to\nprevious models trained on the same dataset, our model not only maintains high\nsegmentation accuracy on the validation part of the same dataset but also\ndemonstrates superior accuracy on unseen datasets, exhibiting excellent\ngeneralization capabilities. Notably, we are the first to apply the\nVision-Mamba architecture to polyp segmentation and the first to utilize prompt\ntechnology in a polyp segmentation model. Our model efficiently accomplishes\nsegmentation tasks, surpassing previous state-of-the-art methods by an average\nof 5% across six datasets. Furthermore, we have developed multiple versions of\nour model with scaled parameter counts, achieving better performance than\nprevious models even with fewer parameters. Our code and trained weights will\nbe released soon.\n", "link": "http://arxiv.org/abs/2403.13660v1", "date": "2024-03-20", "relevancy": 1.9675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4771}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ProMamba%3A%20Prompt-Mamba%20for%20polyp%20segmentation&body=Title%3A%20ProMamba%3A%20Prompt-Mamba%20for%20polyp%20segmentation%0AAuthor%3A%20Jianhao%20Xie%20and%20Ruofan%20Liao%20and%20Ziang%20Zhang%20and%20Sida%20Yi%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo%0AAbstract%3A%20%20%20Detecting%20polyps%20through%20colonoscopy%20is%20an%20important%20task%20in%20medical%20image%0Asegmentation%2C%20which%20provides%20significant%20assistance%20and%20reference%20value%20for%0Aclinical%20surgery.%20However%2C%20accurate%20segmentation%20of%20polyps%20is%20a%20challenging%0Atask%20due%20to%20two%20main%20reasons.%20Firstly%2C%20polyps%20exhibit%20various%20shapes%20and%0Acolors.%20Secondly%2C%20the%20boundaries%20between%20polyps%20and%20their%20normal%20surroundings%0Aare%20often%20unclear.%20Additionally%2C%20significant%20differences%20between%20different%0Adatasets%20lead%20to%20limited%20generalization%20capabilities%20of%20existing%20methods.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20segmentation%20model%20based%20on%20Prompt-Mamba%2C%0Awhich%20incorporates%20the%20latest%20Vision-Mamba%20and%20prompt%20technologies.%20Compared%20to%0Aprevious%20models%20trained%20on%20the%20same%20dataset%2C%20our%20model%20not%20only%20maintains%20high%0Asegmentation%20accuracy%20on%20the%20validation%20part%20of%20the%20same%20dataset%20but%20also%0Ademonstrates%20superior%20accuracy%20on%20unseen%20datasets%2C%20exhibiting%20excellent%0Ageneralization%20capabilities.%20Notably%2C%20we%20are%20the%20first%20to%20apply%20the%0AVision-Mamba%20architecture%20to%20polyp%20segmentation%20and%20the%20first%20to%20utilize%20prompt%0Atechnology%20in%20a%20polyp%20segmentation%20model.%20Our%20model%20efficiently%20accomplishes%0Asegmentation%20tasks%2C%20surpassing%20previous%20state-of-the-art%20methods%20by%20an%20average%0Aof%205%25%20across%20six%20datasets.%20Furthermore%2C%20we%20have%20developed%20multiple%20versions%20of%0Aour%20model%20with%20scaled%20parameter%20counts%2C%20achieving%20better%20performance%20than%0Aprevious%20models%20even%20with%20fewer%20parameters.%20Our%20code%20and%20trained%20weights%20will%0Abe%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13660v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProMamba%3A%20Prompt-Mamba%20for%20polyp%20segmentation&entry.906535625=Jianhao%20Xie%20and%20Ruofan%20Liao%20and%20Ziang%20Zhang%20and%20Sida%20Yi%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo&entry.1292438233=%20%20Detecting%20polyps%20through%20colonoscopy%20is%20an%20important%20task%20in%20medical%20image%0Asegmentation%2C%20which%20provides%20significant%20assistance%20and%20reference%20value%20for%0Aclinical%20surgery.%20However%2C%20accurate%20segmentation%20of%20polyps%20is%20a%20challenging%0Atask%20due%20to%20two%20main%20reasons.%20Firstly%2C%20polyps%20exhibit%20various%20shapes%20and%0Acolors.%20Secondly%2C%20the%20boundaries%20between%20polyps%20and%20their%20normal%20surroundings%0Aare%20often%20unclear.%20Additionally%2C%20significant%20differences%20between%20different%0Adatasets%20lead%20to%20limited%20generalization%20capabilities%20of%20existing%20methods.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20segmentation%20model%20based%20on%20Prompt-Mamba%2C%0Awhich%20incorporates%20the%20latest%20Vision-Mamba%20and%20prompt%20technologies.%20Compared%20to%0Aprevious%20models%20trained%20on%20the%20same%20dataset%2C%20our%20model%20not%20only%20maintains%20high%0Asegmentation%20accuracy%20on%20the%20validation%20part%20of%20the%20same%20dataset%20but%20also%0Ademonstrates%20superior%20accuracy%20on%20unseen%20datasets%2C%20exhibiting%20excellent%0Ageneralization%20capabilities.%20Notably%2C%20we%20are%20the%20first%20to%20apply%20the%0AVision-Mamba%20architecture%20to%20polyp%20segmentation%20and%20the%20first%20to%20utilize%20prompt%0Atechnology%20in%20a%20polyp%20segmentation%20model.%20Our%20model%20efficiently%20accomplishes%0Asegmentation%20tasks%2C%20surpassing%20previous%20state-of-the-art%20methods%20by%20an%20average%0Aof%205%25%20across%20six%20datasets.%20Furthermore%2C%20we%20have%20developed%20multiple%20versions%20of%0Aour%20model%20with%20scaled%20parameter%20counts%2C%20achieving%20better%20performance%20than%0Aprevious%20models%20even%20with%20fewer%20parameters.%20Our%20code%20and%20trained%20weights%20will%0Abe%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13660v1&entry.124074799=Read"},
{"title": "Riemannian Multinomial Logistics Regression for SPD Neural Networks", "author": "Ziheng Chen and Yue Song and Gaowen Liu and Ramana Rao Kompella and Xiaojun Wu and Nicu Sebe", "abstract": "  Deep neural networks for learning Symmetric Positive Definite (SPD) matrices\nare gaining increasing attention in machine learning. Despite the significant\nprogress, most existing SPD networks use traditional Euclidean classifiers on\nan approximated space rather than intrinsic classifiers that accurately capture\nthe geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs),\nwe propose Riemannian Multinomial Logistics Regression (RMLR) for the\nclassification layers in SPD networks. We introduce a unified framework for\nbuilding Riemannian classifiers under the metrics pulled back from the\nEuclidean space, and showcase our framework under the parameterized\nLog-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, our\nframework offers a novel intrinsic explanation for the most popular LogEig\nclassifier in existing SPD networks. The effectiveness of our method is\ndemonstrated in three applications: radar recognition, human action\nrecognition, and electroencephalography (EEG) classification. The code is\navailable at https://github.com/GitZH-Chen/SPDMLR.git.\n", "link": "http://arxiv.org/abs/2305.11288v2", "date": "2024-03-20", "relevancy": 1.9643, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Multinomial%20Logistics%20Regression%20for%20SPD%20Neural%20Networks&body=Title%3A%20Riemannian%20Multinomial%20Logistics%20Regression%20for%20SPD%20Neural%20Networks%0AAuthor%3A%20Ziheng%20Chen%20and%20Yue%20Song%20and%20Gaowen%20Liu%20and%20Ramana%20Rao%20Kompella%20and%20Xiaojun%20Wu%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Deep%20neural%20networks%20for%20learning%20Symmetric%20Positive%20Definite%20%28SPD%29%20matrices%0Aare%20gaining%20increasing%20attention%20in%20machine%20learning.%20Despite%20the%20significant%0Aprogress%2C%20most%20existing%20SPD%20networks%20use%20traditional%20Euclidean%20classifiers%20on%0Aan%20approximated%20space%20rather%20than%20intrinsic%20classifiers%20that%20accurately%20capture%0Athe%20geometry%20of%20SPD%20manifolds.%20Inspired%20by%20Hyperbolic%20Neural%20Networks%20%28HNNs%29%2C%0Awe%20propose%20Riemannian%20Multinomial%20Logistics%20Regression%20%28RMLR%29%20for%20the%0Aclassification%20layers%20in%20SPD%20networks.%20We%20introduce%20a%20unified%20framework%20for%0Abuilding%20Riemannian%20classifiers%20under%20the%20metrics%20pulled%20back%20from%20the%0AEuclidean%20space%2C%20and%20showcase%20our%20framework%20under%20the%20parameterized%0ALog-Euclidean%20Metric%20%28LEM%29%20and%20Log-Cholesky%20Metric%20%28LCM%29.%20Besides%2C%20our%0Aframework%20offers%20a%20novel%20intrinsic%20explanation%20for%20the%20most%20popular%20LogEig%0Aclassifier%20in%20existing%20SPD%20networks.%20The%20effectiveness%20of%20our%20method%20is%0Ademonstrated%20in%20three%20applications%3A%20radar%20recognition%2C%20human%20action%0Arecognition%2C%20and%20electroencephalography%20%28EEG%29%20classification.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/GitZH-Chen/SPDMLR.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.11288v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Multinomial%20Logistics%20Regression%20for%20SPD%20Neural%20Networks&entry.906535625=Ziheng%20Chen%20and%20Yue%20Song%20and%20Gaowen%20Liu%20and%20Ramana%20Rao%20Kompella%20and%20Xiaojun%20Wu%20and%20Nicu%20Sebe&entry.1292438233=%20%20Deep%20neural%20networks%20for%20learning%20Symmetric%20Positive%20Definite%20%28SPD%29%20matrices%0Aare%20gaining%20increasing%20attention%20in%20machine%20learning.%20Despite%20the%20significant%0Aprogress%2C%20most%20existing%20SPD%20networks%20use%20traditional%20Euclidean%20classifiers%20on%0Aan%20approximated%20space%20rather%20than%20intrinsic%20classifiers%20that%20accurately%20capture%0Athe%20geometry%20of%20SPD%20manifolds.%20Inspired%20by%20Hyperbolic%20Neural%20Networks%20%28HNNs%29%2C%0Awe%20propose%20Riemannian%20Multinomial%20Logistics%20Regression%20%28RMLR%29%20for%20the%0Aclassification%20layers%20in%20SPD%20networks.%20We%20introduce%20a%20unified%20framework%20for%0Abuilding%20Riemannian%20classifiers%20under%20the%20metrics%20pulled%20back%20from%20the%0AEuclidean%20space%2C%20and%20showcase%20our%20framework%20under%20the%20parameterized%0ALog-Euclidean%20Metric%20%28LEM%29%20and%20Log-Cholesky%20Metric%20%28LCM%29.%20Besides%2C%20our%0Aframework%20offers%20a%20novel%20intrinsic%20explanation%20for%20the%20most%20popular%20LogEig%0Aclassifier%20in%20existing%20SPD%20networks.%20The%20effectiveness%20of%20our%20method%20is%0Ademonstrated%20in%20three%20applications%3A%20radar%20recognition%2C%20human%20action%0Arecognition%2C%20and%20electroencephalography%20%28EEG%29%20classification.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/GitZH-Chen/SPDMLR.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11288v2&entry.124074799=Read"},
{"title": "Bounce: Reliable High-Dimensional Bayesian Optimization for\n  Combinatorial and Mixed Spaces", "author": "Leonard Papenmeier and Luigi Nardi and Matthias Poloczek", "abstract": "  Impactful applications such as materials discovery, hardware design, neural\narchitecture search, or portfolio optimization require optimizing\nhigh-dimensional black-box functions with mixed and combinatorial input spaces.\nWhile Bayesian optimization has recently made significant progress in solving\nsuch problems, an in-depth analysis reveals that the current state-of-the-art\nmethods are not reliable. Their performances degrade substantially when the\nunknown optima of the function do not have a certain structure. To fill the\nneed for a reliable algorithm for combinatorial and mixed spaces, this paper\nproposes Bounce that relies on a novel map of various variable types into\nnested embeddings of increasing dimensionality. Comprehensive experiments show\nthat Bounce reliably achieves and often even improves upon state-of-the-art\nperformance on a variety of high-dimensional problems.\n", "link": "http://arxiv.org/abs/2307.00618v2", "date": "2024-03-20", "relevancy": 1.9627, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4696}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bounce%3A%20Reliable%20High-Dimensional%20Bayesian%20Optimization%20for%0A%20%20Combinatorial%20and%20Mixed%20Spaces&body=Title%3A%20Bounce%3A%20Reliable%20High-Dimensional%20Bayesian%20Optimization%20for%0A%20%20Combinatorial%20and%20Mixed%20Spaces%0AAuthor%3A%20Leonard%20Papenmeier%20and%20Luigi%20Nardi%20and%20Matthias%20Poloczek%0AAbstract%3A%20%20%20Impactful%20applications%20such%20as%20materials%20discovery%2C%20hardware%20design%2C%20neural%0Aarchitecture%20search%2C%20or%20portfolio%20optimization%20require%20optimizing%0Ahigh-dimensional%20black-box%20functions%20with%20mixed%20and%20combinatorial%20input%20spaces.%0AWhile%20Bayesian%20optimization%20has%20recently%20made%20significant%20progress%20in%20solving%0Asuch%20problems%2C%20an%20in-depth%20analysis%20reveals%20that%20the%20current%20state-of-the-art%0Amethods%20are%20not%20reliable.%20Their%20performances%20degrade%20substantially%20when%20the%0Aunknown%20optima%20of%20the%20function%20do%20not%20have%20a%20certain%20structure.%20To%20fill%20the%0Aneed%20for%20a%20reliable%20algorithm%20for%20combinatorial%20and%20mixed%20spaces%2C%20this%20paper%0Aproposes%20Bounce%20that%20relies%20on%20a%20novel%20map%20of%20various%20variable%20types%20into%0Anested%20embeddings%20of%20increasing%20dimensionality.%20Comprehensive%20experiments%20show%0Athat%20Bounce%20reliably%20achieves%20and%20often%20even%20improves%20upon%20state-of-the-art%0Aperformance%20on%20a%20variety%20of%20high-dimensional%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00618v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounce%3A%20Reliable%20High-Dimensional%20Bayesian%20Optimization%20for%0A%20%20Combinatorial%20and%20Mixed%20Spaces&entry.906535625=Leonard%20Papenmeier%20and%20Luigi%20Nardi%20and%20Matthias%20Poloczek&entry.1292438233=%20%20Impactful%20applications%20such%20as%20materials%20discovery%2C%20hardware%20design%2C%20neural%0Aarchitecture%20search%2C%20or%20portfolio%20optimization%20require%20optimizing%0Ahigh-dimensional%20black-box%20functions%20with%20mixed%20and%20combinatorial%20input%20spaces.%0AWhile%20Bayesian%20optimization%20has%20recently%20made%20significant%20progress%20in%20solving%0Asuch%20problems%2C%20an%20in-depth%20analysis%20reveals%20that%20the%20current%20state-of-the-art%0Amethods%20are%20not%20reliable.%20Their%20performances%20degrade%20substantially%20when%20the%0Aunknown%20optima%20of%20the%20function%20do%20not%20have%20a%20certain%20structure.%20To%20fill%20the%0Aneed%20for%20a%20reliable%20algorithm%20for%20combinatorial%20and%20mixed%20spaces%2C%20this%20paper%0Aproposes%20Bounce%20that%20relies%20on%20a%20novel%20map%20of%20various%20variable%20types%20into%0Anested%20embeddings%20of%20increasing%20dimensionality.%20Comprehensive%20experiments%20show%0Athat%20Bounce%20reliably%20achieves%20and%20often%20even%20improves%20upon%20state-of-the-art%0Aperformance%20on%20a%20variety%20of%20high-dimensional%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00618v2&entry.124074799=Read"},
{"title": "Impact of Synthetic Images on Morphing Attack Detection Using a Siamese\n  Network", "author": "Juan Tapia and Christoph Busch", "abstract": "  This paper evaluated the impact of synthetic images on Morphing Attack\nDetection (MAD) using a Siamese network with a semi-hard-loss function. Intra\nand cross-dataset evaluations were performed to measure synthetic image\ngeneralisation capabilities using a cross-dataset for evaluation. Three\ndifferent pre-trained networks were used as feature extractors from traditional\nMobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained\non EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in\ncomparison with SOTA. Conversely, worse performances were reached when the\nsystem was trained only with synthetic images. A mixed approach (synthetic +\ndigital) database may help to improve MAD and reduce the error rate. This fact\nshows that we still need to keep going with our efforts to include synthetic\nimages in the training process.\n", "link": "http://arxiv.org/abs/2403.09380v2", "date": "2024-03-20", "relevancy": 1.9624, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Synthetic%20Images%20on%20Morphing%20Attack%20Detection%20Using%20a%20Siamese%0A%20%20Network&body=Title%3A%20Impact%20of%20Synthetic%20Images%20on%20Morphing%20Attack%20Detection%20Using%20a%20Siamese%0A%20%20Network%0AAuthor%3A%20Juan%20Tapia%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20This%20paper%20evaluated%20the%20impact%20of%20synthetic%20images%20on%20Morphing%20Attack%0ADetection%20%28MAD%29%20using%20a%20Siamese%20network%20with%20a%20semi-hard-loss%20function.%20Intra%0Aand%20cross-dataset%20evaluations%20were%20performed%20to%20measure%20synthetic%20image%0Ageneralisation%20capabilities%20using%20a%20cross-dataset%20for%20evaluation.%20Three%0Adifferent%20pre-trained%20networks%20were%20used%20as%20feature%20extractors%20from%20traditional%0AMobileNetV2%2C%20MobileNetV3%20and%20EfficientNetB0.%20Our%20results%20show%20that%20MAD%20trained%0Aon%20EfficientNetB0%20from%20FERET%2C%20FRGCv2%2C%20and%20FRLL%20can%20reach%20a%20lower%20error%20rate%20in%0Acomparison%20with%20SOTA.%20Conversely%2C%20worse%20performances%20were%20reached%20when%20the%0Asystem%20was%20trained%20only%20with%20synthetic%20images.%20A%20mixed%20approach%20%28synthetic%20%2B%0Adigital%29%20database%20may%20help%20to%20improve%20MAD%20and%20reduce%20the%20error%20rate.%20This%20fact%0Ashows%20that%20we%20still%20need%20to%20keep%20going%20with%20our%20efforts%20to%20include%20synthetic%0Aimages%20in%20the%20training%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09380v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Synthetic%20Images%20on%20Morphing%20Attack%20Detection%20Using%20a%20Siamese%0A%20%20Network&entry.906535625=Juan%20Tapia%20and%20Christoph%20Busch&entry.1292438233=%20%20This%20paper%20evaluated%20the%20impact%20of%20synthetic%20images%20on%20Morphing%20Attack%0ADetection%20%28MAD%29%20using%20a%20Siamese%20network%20with%20a%20semi-hard-loss%20function.%20Intra%0Aand%20cross-dataset%20evaluations%20were%20performed%20to%20measure%20synthetic%20image%0Ageneralisation%20capabilities%20using%20a%20cross-dataset%20for%20evaluation.%20Three%0Adifferent%20pre-trained%20networks%20were%20used%20as%20feature%20extractors%20from%20traditional%0AMobileNetV2%2C%20MobileNetV3%20and%20EfficientNetB0.%20Our%20results%20show%20that%20MAD%20trained%0Aon%20EfficientNetB0%20from%20FERET%2C%20FRGCv2%2C%20and%20FRLL%20can%20reach%20a%20lower%20error%20rate%20in%0Acomparison%20with%20SOTA.%20Conversely%2C%20worse%20performances%20were%20reached%20when%20the%0Asystem%20was%20trained%20only%20with%20synthetic%20images.%20A%20mixed%20approach%20%28synthetic%20%2B%0Adigital%29%20database%20may%20help%20to%20improve%20MAD%20and%20reduce%20the%20error%20rate.%20This%20fact%0Ashows%20that%20we%20still%20need%20to%20keep%20going%20with%20our%20efforts%20to%20include%20synthetic%0Aimages%20in%20the%20training%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09380v2&entry.124074799=Read"},
{"title": "Leveraging feature communication in federated learning for remote\n  sensing image classification", "author": "Anh-Kiet Duong and Ho\u00e0ng-\u00c2n L\u00ea and Minh-Tan Pham", "abstract": "  In the realm of Federated Learning (FL) applied to remote sensing image\nclassification, this study introduces and assesses several innovative\ncommunication strategies. Our exploration includes feature-centric\ncommunication, pseudo-weight amalgamation, and a combined method utilizing both\nweights and features. Experiments conducted on two public scene classification\ndatasets unveil the effectiveness of these strategies, showcasing accelerated\nconvergence, heightened privacy, and reduced network information exchange. This\nresearch provides valuable insights into the implications of feature-centric\ncommunication in FL, offering potential applications tailored for remote\nsensing scenarios.\n", "link": "http://arxiv.org/abs/2403.13575v1", "date": "2024-03-20", "relevancy": 1.9528, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5088}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4651}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification&body=Title%3A%20Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification%0AAuthor%3A%20Anh-Kiet%20Duong%20and%20Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Minh-Tan%20Pham%0AAbstract%3A%20%20%20In%20the%20realm%20of%20Federated%20Learning%20%28FL%29%20applied%20to%20remote%20sensing%20image%0Aclassification%2C%20this%20study%20introduces%20and%20assesses%20several%20innovative%0Acommunication%20strategies.%20Our%20exploration%20includes%20feature-centric%0Acommunication%2C%20pseudo-weight%20amalgamation%2C%20and%20a%20combined%20method%20utilizing%20both%0Aweights%20and%20features.%20Experiments%20conducted%20on%20two%20public%20scene%20classification%0Adatasets%20unveil%20the%20effectiveness%20of%20these%20strategies%2C%20showcasing%20accelerated%0Aconvergence%2C%20heightened%20privacy%2C%20and%20reduced%20network%20information%20exchange.%20This%0Aresearch%20provides%20valuable%20insights%20into%20the%20implications%20of%20feature-centric%0Acommunication%20in%20FL%2C%20offering%20potential%20applications%20tailored%20for%20remote%0Asensing%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13575v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification&entry.906535625=Anh-Kiet%20Duong%20and%20Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Minh-Tan%20Pham&entry.1292438233=%20%20In%20the%20realm%20of%20Federated%20Learning%20%28FL%29%20applied%20to%20remote%20sensing%20image%0Aclassification%2C%20this%20study%20introduces%20and%20assesses%20several%20innovative%0Acommunication%20strategies.%20Our%20exploration%20includes%20feature-centric%0Acommunication%2C%20pseudo-weight%20amalgamation%2C%20and%20a%20combined%20method%20utilizing%20both%0Aweights%20and%20features.%20Experiments%20conducted%20on%20two%20public%20scene%20classification%0Adatasets%20unveil%20the%20effectiveness%20of%20these%20strategies%2C%20showcasing%20accelerated%0Aconvergence%2C%20heightened%20privacy%2C%20and%20reduced%20network%20information%20exchange.%20This%0Aresearch%20provides%20valuable%20insights%20into%20the%20implications%20of%20feature-centric%0Acommunication%20in%20FL%2C%20offering%20potential%20applications%20tailored%20for%20remote%0Asensing%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13575v1&entry.124074799=Read"},
{"title": "Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for\n  Counselor Reflection Generation", "author": "Do June Min and Veronica Perez-Rosas and Kenneth Resnicow and Rada Mihalcea", "abstract": "  In this paper, we study the problem of multi-reward reinforcement learning to\njointly optimize for multiple text qualities for natural language generation.\nWe focus on the task of counselor reflection generation, where we optimize the\ngenerators to simultaneously improve the fluency, coherence, and reflection\nquality of generated counselor responses. We introduce two novel bandit\nmethods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining\nrewards into a single value and optimizing them simultaneously. Specifically,\nwe employ non-contextual and contextual multi-arm bandits to dynamically adjust\nmultiple reward weights during training. Through automatic and manual\nevaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,\noutperform existing naive and bandit baselines, showcasing their potential for\nenhancing language models.\n", "link": "http://arxiv.org/abs/2403.13578v1", "date": "2024-03-20", "relevancy": 1.9527, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4987}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4636}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Reward%20Adjustment%20in%20Multi-Reward%20Reinforcement%20Learning%20for%0A%20%20Counselor%20Reflection%20Generation&body=Title%3A%20Dynamic%20Reward%20Adjustment%20in%20Multi-Reward%20Reinforcement%20Learning%20for%0A%20%20Counselor%20Reflection%20Generation%0AAuthor%3A%20Do%20June%20Min%20and%20Veronica%20Perez-Rosas%20and%20Kenneth%20Resnicow%20and%20Rada%20Mihalcea%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20multi-reward%20reinforcement%20learning%20to%0Ajointly%20optimize%20for%20multiple%20text%20qualities%20for%20natural%20language%20generation.%0AWe%20focus%20on%20the%20task%20of%20counselor%20reflection%20generation%2C%20where%20we%20optimize%20the%0Agenerators%20to%20simultaneously%20improve%20the%20fluency%2C%20coherence%2C%20and%20reflection%0Aquality%20of%20generated%20counselor%20responses.%20We%20introduce%20two%20novel%20bandit%0Amethods%2C%20DynaOpt%20and%20C-DynaOpt%2C%20which%20rely%20on%20the%20broad%20strategy%20of%20combining%0Arewards%20into%20a%20single%20value%20and%20optimizing%20them%20simultaneously.%20Specifically%2C%0Awe%20employ%20non-contextual%20and%20contextual%20multi-arm%20bandits%20to%20dynamically%20adjust%0Amultiple%20reward%20weights%20during%20training.%20Through%20automatic%20and%20manual%0Aevaluations%2C%20we%20show%20that%20our%20proposed%20techniques%2C%20DynaOpt%20and%20C-DynaOpt%2C%0Aoutperform%20existing%20naive%20and%20bandit%20baselines%2C%20showcasing%20their%20potential%20for%0Aenhancing%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13578v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Reward%20Adjustment%20in%20Multi-Reward%20Reinforcement%20Learning%20for%0A%20%20Counselor%20Reflection%20Generation&entry.906535625=Do%20June%20Min%20and%20Veronica%20Perez-Rosas%20and%20Kenneth%20Resnicow%20and%20Rada%20Mihalcea&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20multi-reward%20reinforcement%20learning%20to%0Ajointly%20optimize%20for%20multiple%20text%20qualities%20for%20natural%20language%20generation.%0AWe%20focus%20on%20the%20task%20of%20counselor%20reflection%20generation%2C%20where%20we%20optimize%20the%0Agenerators%20to%20simultaneously%20improve%20the%20fluency%2C%20coherence%2C%20and%20reflection%0Aquality%20of%20generated%20counselor%20responses.%20We%20introduce%20two%20novel%20bandit%0Amethods%2C%20DynaOpt%20and%20C-DynaOpt%2C%20which%20rely%20on%20the%20broad%20strategy%20of%20combining%0Arewards%20into%20a%20single%20value%20and%20optimizing%20them%20simultaneously.%20Specifically%2C%0Awe%20employ%20non-contextual%20and%20contextual%20multi-arm%20bandits%20to%20dynamically%20adjust%0Amultiple%20reward%20weights%20during%20training.%20Through%20automatic%20and%20manual%0Aevaluations%2C%20we%20show%20that%20our%20proposed%20techniques%2C%20DynaOpt%20and%20C-DynaOpt%2C%0Aoutperform%20existing%20naive%20and%20bandit%20baselines%2C%20showcasing%20their%20potential%20for%0Aenhancing%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13578v1&entry.124074799=Read"},
{"title": "Diversity-aware Channel Pruning for StyleGAN Compression", "author": "Jiwoo Chung and Sangeek Hyun and Sang-Heon Shim and Jae-Pil Heo", "abstract": "  StyleGAN has shown remarkable performance in unconditional image generation.\nHowever, its high computational cost poses a significant challenge for\npractical applications. Although recent efforts have been made to compress\nStyleGAN while preserving its performance, existing compressed models still lag\nbehind the original model, particularly in terms of sample diversity. To\novercome this, we propose a novel channel pruning method that leverages varying\nsensitivities of channels to latent vectors, which is a key factor in sample\ndiversity. Specifically, by assessing channel importance based on their\nsensitivities to latent vector perturbations, our method enhances the diversity\nof samples in the compressed model. Since our method solely focuses on the\nchannel pruning stage, it has complementary benefits with prior training\nschemes without additional training cost. Extensive experiments demonstrate\nthat our method significantly enhances sample diversity across various\ndatasets. Moreover, in terms of FID scores, our method not only surpasses\nstate-of-the-art by a large margin but also achieves comparable scores with\nonly half training iterations.\n", "link": "http://arxiv.org/abs/2403.13548v1", "date": "2024-03-20", "relevancy": 1.9436, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4923}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4615}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diversity-aware%20Channel%20Pruning%20for%20StyleGAN%20Compression&body=Title%3A%20Diversity-aware%20Channel%20Pruning%20for%20StyleGAN%20Compression%0AAuthor%3A%20Jiwoo%20Chung%20and%20Sangeek%20Hyun%20and%20Sang-Heon%20Shim%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20StyleGAN%20has%20shown%20remarkable%20performance%20in%20unconditional%20image%20generation.%0AHowever%2C%20its%20high%20computational%20cost%20poses%20a%20significant%20challenge%20for%0Apractical%20applications.%20Although%20recent%20efforts%20have%20been%20made%20to%20compress%0AStyleGAN%20while%20preserving%20its%20performance%2C%20existing%20compressed%20models%20still%20lag%0Abehind%20the%20original%20model%2C%20particularly%20in%20terms%20of%20sample%20diversity.%20To%0Aovercome%20this%2C%20we%20propose%20a%20novel%20channel%20pruning%20method%20that%20leverages%20varying%0Asensitivities%20of%20channels%20to%20latent%20vectors%2C%20which%20is%20a%20key%20factor%20in%20sample%0Adiversity.%20Specifically%2C%20by%20assessing%20channel%20importance%20based%20on%20their%0Asensitivities%20to%20latent%20vector%20perturbations%2C%20our%20method%20enhances%20the%20diversity%0Aof%20samples%20in%20the%20compressed%20model.%20Since%20our%20method%20solely%20focuses%20on%20the%0Achannel%20pruning%20stage%2C%20it%20has%20complementary%20benefits%20with%20prior%20training%0Aschemes%20without%20additional%20training%20cost.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20sample%20diversity%20across%20various%0Adatasets.%20Moreover%2C%20in%20terms%20of%20FID%20scores%2C%20our%20method%20not%20only%20surpasses%0Astate-of-the-art%20by%20a%20large%20margin%20but%20also%20achieves%20comparable%20scores%20with%0Aonly%20half%20training%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13548v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity-aware%20Channel%20Pruning%20for%20StyleGAN%20Compression&entry.906535625=Jiwoo%20Chung%20and%20Sangeek%20Hyun%20and%20Sang-Heon%20Shim%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20StyleGAN%20has%20shown%20remarkable%20performance%20in%20unconditional%20image%20generation.%0AHowever%2C%20its%20high%20computational%20cost%20poses%20a%20significant%20challenge%20for%0Apractical%20applications.%20Although%20recent%20efforts%20have%20been%20made%20to%20compress%0AStyleGAN%20while%20preserving%20its%20performance%2C%20existing%20compressed%20models%20still%20lag%0Abehind%20the%20original%20model%2C%20particularly%20in%20terms%20of%20sample%20diversity.%20To%0Aovercome%20this%2C%20we%20propose%20a%20novel%20channel%20pruning%20method%20that%20leverages%20varying%0Asensitivities%20of%20channels%20to%20latent%20vectors%2C%20which%20is%20a%20key%20factor%20in%20sample%0Adiversity.%20Specifically%2C%20by%20assessing%20channel%20importance%20based%20on%20their%0Asensitivities%20to%20latent%20vector%20perturbations%2C%20our%20method%20enhances%20the%20diversity%0Aof%20samples%20in%20the%20compressed%20model.%20Since%20our%20method%20solely%20focuses%20on%20the%0Achannel%20pruning%20stage%2C%20it%20has%20complementary%20benefits%20with%20prior%20training%0Aschemes%20without%20additional%20training%20cost.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20sample%20diversity%20across%20various%0Adatasets.%20Moreover%2C%20in%20terms%20of%20FID%20scores%2C%20our%20method%20not%20only%20surpasses%0Astate-of-the-art%20by%20a%20large%20margin%20but%20also%20achieves%20comparable%20scores%20with%0Aonly%20half%20training%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13548v1&entry.124074799=Read"},
{"title": "MotorEase: Automated Detection of Motor Impairment Accessibility Issues\n  in Mobile App UIs", "author": "Arun Krishnavajjala and SM Hasan Mansur and Justin Jose and Kevin Moran", "abstract": "  Recent research has begun to examine the potential of automatically finding\nand fixing accessibility issues that manifest in software. However, while\nrecent work makes important progress, it has generally been skewed toward\nidentifying issues that affect users with certain disabilities, such as those\nwith visual or hearing impairments. However, there are other groups of users\nwith different types of disabilities that also need software tooling support to\nimprove their experience. As such, this paper aims to automatically identify\naccessibility issues that affect users with motor-impairments.\n  To move toward this goal, this paper introduces a novel approach, called\nMotorEase, capable of identifying accessibility issues in mobile app UIs that\nimpact motor-impaired users. Motor-impaired users often have limited ability to\ninteract with touch-based devices, and instead may make use of a switch or\nother assistive mechanism -- hence UIs must be designed to support both limited\ntouch gestures and the use of assistive devices. MotorEase adapts computer\nvision and text processing techniques to enable a semantic understanding of app\nUI screens, enabling the detection of violations related to four popular,\npreviously unexplored UI design guidelines that support motor-impaired users,\nincluding: (i) visual touch target size, (ii) expanding sections, (iii)\npersisting elements, and (iv) adjacent icon visual distance. We evaluate\nMotorEase on a newly derived benchmark, called MotorCheck, that contains 555\nmanually annotated examples of violations to the above accessibility\nguidelines, across 1599 screens collected from 70 applications via a mobile app\ntesting tool. Our experiments illustrate that MotorEase is able to identify\nviolations with an average accuracy of ~90%, and a false positive rate of less\nthan 9%, outperforming baseline techniques.\n", "link": "http://arxiv.org/abs/2403.13690v1", "date": "2024-03-20", "relevancy": 1.5668, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.486}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4404}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MotorEase%3A%20Automated%20Detection%20of%20Motor%20Impairment%20Accessibility%20Issues%0A%20%20in%20Mobile%20App%20UIs&body=Title%3A%20MotorEase%3A%20Automated%20Detection%20of%20Motor%20Impairment%20Accessibility%20Issues%0A%20%20in%20Mobile%20App%20UIs%0AAuthor%3A%20Arun%20Krishnavajjala%20and%20SM%20Hasan%20Mansur%20and%20Justin%20Jose%20and%20Kevin%20Moran%0AAbstract%3A%20%20%20Recent%20research%20has%20begun%20to%20examine%20the%20potential%20of%20automatically%20finding%0Aand%20fixing%20accessibility%20issues%20that%20manifest%20in%20software.%20However%2C%20while%0Arecent%20work%20makes%20important%20progress%2C%20it%20has%20generally%20been%20skewed%20toward%0Aidentifying%20issues%20that%20affect%20users%20with%20certain%20disabilities%2C%20such%20as%20those%0Awith%20visual%20or%20hearing%20impairments.%20However%2C%20there%20are%20other%20groups%20of%20users%0Awith%20different%20types%20of%20disabilities%20that%20also%20need%20software%20tooling%20support%20to%0Aimprove%20their%20experience.%20As%20such%2C%20this%20paper%20aims%20to%20automatically%20identify%0Aaccessibility%20issues%20that%20affect%20users%20with%20motor-impairments.%0A%20%20To%20move%20toward%20this%20goal%2C%20this%20paper%20introduces%20a%20novel%20approach%2C%20called%0AMotorEase%2C%20capable%20of%20identifying%20accessibility%20issues%20in%20mobile%20app%20UIs%20that%0Aimpact%20motor-impaired%20users.%20Motor-impaired%20users%20often%20have%20limited%20ability%20to%0Ainteract%20with%20touch-based%20devices%2C%20and%20instead%20may%20make%20use%20of%20a%20switch%20or%0Aother%20assistive%20mechanism%20--%20hence%20UIs%20must%20be%20designed%20to%20support%20both%20limited%0Atouch%20gestures%20and%20the%20use%20of%20assistive%20devices.%20MotorEase%20adapts%20computer%0Avision%20and%20text%20processing%20techniques%20to%20enable%20a%20semantic%20understanding%20of%20app%0AUI%20screens%2C%20enabling%20the%20detection%20of%20violations%20related%20to%20four%20popular%2C%0Apreviously%20unexplored%20UI%20design%20guidelines%20that%20support%20motor-impaired%20users%2C%0Aincluding%3A%20%28i%29%20visual%20touch%20target%20size%2C%20%28ii%29%20expanding%20sections%2C%20%28iii%29%0Apersisting%20elements%2C%20and%20%28iv%29%20adjacent%20icon%20visual%20distance.%20We%20evaluate%0AMotorEase%20on%20a%20newly%20derived%20benchmark%2C%20called%20MotorCheck%2C%20that%20contains%20555%0Amanually%20annotated%20examples%20of%20violations%20to%20the%20above%20accessibility%0Aguidelines%2C%20across%201599%20screens%20collected%20from%2070%20applications%20via%20a%20mobile%20app%0Atesting%20tool.%20Our%20experiments%20illustrate%20that%20MotorEase%20is%20able%20to%20identify%0Aviolations%20with%20an%20average%20accuracy%20of%20~90%25%2C%20and%20a%20false%20positive%20rate%20of%20less%0Athan%209%25%2C%20outperforming%20baseline%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13690v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotorEase%3A%20Automated%20Detection%20of%20Motor%20Impairment%20Accessibility%20Issues%0A%20%20in%20Mobile%20App%20UIs&entry.906535625=Arun%20Krishnavajjala%20and%20SM%20Hasan%20Mansur%20and%20Justin%20Jose%20and%20Kevin%20Moran&entry.1292438233=%20%20Recent%20research%20has%20begun%20to%20examine%20the%20potential%20of%20automatically%20finding%0Aand%20fixing%20accessibility%20issues%20that%20manifest%20in%20software.%20However%2C%20while%0Arecent%20work%20makes%20important%20progress%2C%20it%20has%20generally%20been%20skewed%20toward%0Aidentifying%20issues%20that%20affect%20users%20with%20certain%20disabilities%2C%20such%20as%20those%0Awith%20visual%20or%20hearing%20impairments.%20However%2C%20there%20are%20other%20groups%20of%20users%0Awith%20different%20types%20of%20disabilities%20that%20also%20need%20software%20tooling%20support%20to%0Aimprove%20their%20experience.%20As%20such%2C%20this%20paper%20aims%20to%20automatically%20identify%0Aaccessibility%20issues%20that%20affect%20users%20with%20motor-impairments.%0A%20%20To%20move%20toward%20this%20goal%2C%20this%20paper%20introduces%20a%20novel%20approach%2C%20called%0AMotorEase%2C%20capable%20of%20identifying%20accessibility%20issues%20in%20mobile%20app%20UIs%20that%0Aimpact%20motor-impaired%20users.%20Motor-impaired%20users%20often%20have%20limited%20ability%20to%0Ainteract%20with%20touch-based%20devices%2C%20and%20instead%20may%20make%20use%20of%20a%20switch%20or%0Aother%20assistive%20mechanism%20--%20hence%20UIs%20must%20be%20designed%20to%20support%20both%20limited%0Atouch%20gestures%20and%20the%20use%20of%20assistive%20devices.%20MotorEase%20adapts%20computer%0Avision%20and%20text%20processing%20techniques%20to%20enable%20a%20semantic%20understanding%20of%20app%0AUI%20screens%2C%20enabling%20the%20detection%20of%20violations%20related%20to%20four%20popular%2C%0Apreviously%20unexplored%20UI%20design%20guidelines%20that%20support%20motor-impaired%20users%2C%0Aincluding%3A%20%28i%29%20visual%20touch%20target%20size%2C%20%28ii%29%20expanding%20sections%2C%20%28iii%29%0Apersisting%20elements%2C%20and%20%28iv%29%20adjacent%20icon%20visual%20distance.%20We%20evaluate%0AMotorEase%20on%20a%20newly%20derived%20benchmark%2C%20called%20MotorCheck%2C%20that%20contains%20555%0Amanually%20annotated%20examples%20of%20violations%20to%20the%20above%20accessibility%0Aguidelines%2C%20across%201599%20screens%20collected%20from%2070%20applications%20via%20a%20mobile%20app%0Atesting%20tool.%20Our%20experiments%20illustrate%20that%20MotorEase%20is%20able%20to%20identify%0Aviolations%20with%20an%20average%20accuracy%20of%20~90%25%2C%20and%20a%20false%20positive%20rate%20of%20less%0Athan%209%25%2C%20outperforming%20baseline%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13690v1&entry.124074799=Read"},
{"title": "From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive\n  Processes", "author": "Julian Kaduk and M\u00fcge Cavdan and Knut Drewing and Heiko Hamann", "abstract": "  In robotics, understanding human interaction with autonomous systems is\ncrucial for enhancing collaborative technologies. We focus on human-swarm\ninteraction (HSI), exploring how differently sized groups of active robots\naffect operators' cognitive and perceptual reactions over different durations.\nWe analyze the impact of different numbers of active robots within a 15-robot\nswarm on operators' time perception, emotional state, flow experience, and task\ndifficulty perception. Our findings indicate that managing multiple active\nrobots when compared to one active robot significantly alters time perception\nand flow experience, leading to a faster passage of time and increased flow.\nMore active robots and extended durations cause increased emotional arousal and\nperceived task difficulty, highlighting the interaction between robot the\nnumber of active robots and human cognitive processes. These insights inform\nthe creation of intuitive human-swarm interfaces and aid in developing swarm\nrobotic systems aligned with human cognitive structures, enhancing human-robot\ncollaboration.\n", "link": "http://arxiv.org/abs/2403.13541v1", "date": "2024-03-20", "relevancy": 1.4418, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5062}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4423}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20One%20to%20Many%3A%20How%20Active%20Robot%20Swarm%20Sizes%20Influence%20Human%20Cognitive%0A%20%20Processes&body=Title%3A%20From%20One%20to%20Many%3A%20How%20Active%20Robot%20Swarm%20Sizes%20Influence%20Human%20Cognitive%0A%20%20Processes%0AAuthor%3A%20Julian%20Kaduk%20and%20M%C3%BCge%20Cavdan%20and%20Knut%20Drewing%20and%20Heiko%20Hamann%0AAbstract%3A%20%20%20In%20robotics%2C%20understanding%20human%20interaction%20with%20autonomous%20systems%20is%0Acrucial%20for%20enhancing%20collaborative%20technologies.%20We%20focus%20on%20human-swarm%0Ainteraction%20%28HSI%29%2C%20exploring%20how%20differently%20sized%20groups%20of%20active%20robots%0Aaffect%20operators%27%20cognitive%20and%20perceptual%20reactions%20over%20different%20durations.%0AWe%20analyze%20the%20impact%20of%20different%20numbers%20of%20active%20robots%20within%20a%2015-robot%0Aswarm%20on%20operators%27%20time%20perception%2C%20emotional%20state%2C%20flow%20experience%2C%20and%20task%0Adifficulty%20perception.%20Our%20findings%20indicate%20that%20managing%20multiple%20active%0Arobots%20when%20compared%20to%20one%20active%20robot%20significantly%20alters%20time%20perception%0Aand%20flow%20experience%2C%20leading%20to%20a%20faster%20passage%20of%20time%20and%20increased%20flow.%0AMore%20active%20robots%20and%20extended%20durations%20cause%20increased%20emotional%20arousal%20and%0Aperceived%20task%20difficulty%2C%20highlighting%20the%20interaction%20between%20robot%20the%0Anumber%20of%20active%20robots%20and%20human%20cognitive%20processes.%20These%20insights%20inform%0Athe%20creation%20of%20intuitive%20human-swarm%20interfaces%20and%20aid%20in%20developing%20swarm%0Arobotic%20systems%20aligned%20with%20human%20cognitive%20structures%2C%20enhancing%20human-robot%0Acollaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13541v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20One%20to%20Many%3A%20How%20Active%20Robot%20Swarm%20Sizes%20Influence%20Human%20Cognitive%0A%20%20Processes&entry.906535625=Julian%20Kaduk%20and%20M%C3%BCge%20Cavdan%20and%20Knut%20Drewing%20and%20Heiko%20Hamann&entry.1292438233=%20%20In%20robotics%2C%20understanding%20human%20interaction%20with%20autonomous%20systems%20is%0Acrucial%20for%20enhancing%20collaborative%20technologies.%20We%20focus%20on%20human-swarm%0Ainteraction%20%28HSI%29%2C%20exploring%20how%20differently%20sized%20groups%20of%20active%20robots%0Aaffect%20operators%27%20cognitive%20and%20perceptual%20reactions%20over%20different%20durations.%0AWe%20analyze%20the%20impact%20of%20different%20numbers%20of%20active%20robots%20within%20a%2015-robot%0Aswarm%20on%20operators%27%20time%20perception%2C%20emotional%20state%2C%20flow%20experience%2C%20and%20task%0Adifficulty%20perception.%20Our%20findings%20indicate%20that%20managing%20multiple%20active%0Arobots%20when%20compared%20to%20one%20active%20robot%20significantly%20alters%20time%20perception%0Aand%20flow%20experience%2C%20leading%20to%20a%20faster%20passage%20of%20time%20and%20increased%20flow.%0AMore%20active%20robots%20and%20extended%20durations%20cause%20increased%20emotional%20arousal%20and%0Aperceived%20task%20difficulty%2C%20highlighting%20the%20interaction%20between%20robot%20the%0Anumber%20of%20active%20robots%20and%20human%20cognitive%20processes.%20These%20insights%20inform%0Athe%20creation%20of%20intuitive%20human-swarm%20interfaces%20and%20aid%20in%20developing%20swarm%0Arobotic%20systems%20aligned%20with%20human%20cognitive%20structures%2C%20enhancing%20human-robot%0Acollaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13541v1&entry.124074799=Read"},
{"title": "Large Language Models meet Network Slicing Management and Orchestration", "author": "Abdulhalim Dandoush and Viswanath Kumarskandpriya and Mueen Uddin and Usman Khalil", "abstract": "  Network slicing, a cornerstone technology for future networks, enables the\ncreation of customized virtual networks on a shared physical infrastructure.\nThis fosters innovation and agility by providing dedicated resources tailored\nto specific applications. However, current orchestration and management\napproaches face limitations in handling the complexity of new service demands\nwithin multi-administrative domain environments. This paper proposes a future\nvision for network slicing powered by Large Language Models (LLMs) and\nmulti-agent systems, offering a framework that can be integrated with existing\nManagement and Orchestration (MANO) frameworks. This framework leverages LLMs\nto translate user intent into technical requirements, map network functions to\ninfrastructure, and manage the entire slice lifecycle, while multi-agent\nsystems facilitate collaboration across different administrative domains. We\nalso discuss the challenges associated with implementing this framework and\npotential solutions to mitigate them.\n", "link": "http://arxiv.org/abs/2403.13721v1", "date": "2024-03-20", "relevancy": 1.2264, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4336}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4014}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20meet%20Network%20Slicing%20Management%20and%20Orchestration&body=Title%3A%20Large%20Language%20Models%20meet%20Network%20Slicing%20Management%20and%20Orchestration%0AAuthor%3A%20Abdulhalim%20Dandoush%20and%20Viswanath%20Kumarskandpriya%20and%20Mueen%20Uddin%20and%20Usman%20Khalil%0AAbstract%3A%20%20%20Network%20slicing%2C%20a%20cornerstone%20technology%20for%20future%20networks%2C%20enables%20the%0Acreation%20of%20customized%20virtual%20networks%20on%20a%20shared%20physical%20infrastructure.%0AThis%20fosters%20innovation%20and%20agility%20by%20providing%20dedicated%20resources%20tailored%0Ato%20specific%20applications.%20However%2C%20current%20orchestration%20and%20management%0Aapproaches%20face%20limitations%20in%20handling%20the%20complexity%20of%20new%20service%20demands%0Awithin%20multi-administrative%20domain%20environments.%20This%20paper%20proposes%20a%20future%0Avision%20for%20network%20slicing%20powered%20by%20Large%20Language%20Models%20%28LLMs%29%20and%0Amulti-agent%20systems%2C%20offering%20a%20framework%20that%20can%20be%20integrated%20with%20existing%0AManagement%20and%20Orchestration%20%28MANO%29%20frameworks.%20This%20framework%20leverages%20LLMs%0Ato%20translate%20user%20intent%20into%20technical%20requirements%2C%20map%20network%20functions%20to%0Ainfrastructure%2C%20and%20manage%20the%20entire%20slice%20lifecycle%2C%20while%20multi-agent%0Asystems%20facilitate%20collaboration%20across%20different%20administrative%20domains.%20We%0Aalso%20discuss%20the%20challenges%20associated%20with%20implementing%20this%20framework%20and%0Apotential%20solutions%20to%20mitigate%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13721v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20meet%20Network%20Slicing%20Management%20and%20Orchestration&entry.906535625=Abdulhalim%20Dandoush%20and%20Viswanath%20Kumarskandpriya%20and%20Mueen%20Uddin%20and%20Usman%20Khalil&entry.1292438233=%20%20Network%20slicing%2C%20a%20cornerstone%20technology%20for%20future%20networks%2C%20enables%20the%0Acreation%20of%20customized%20virtual%20networks%20on%20a%20shared%20physical%20infrastructure.%0AThis%20fosters%20innovation%20and%20agility%20by%20providing%20dedicated%20resources%20tailored%0Ato%20specific%20applications.%20However%2C%20current%20orchestration%20and%20management%0Aapproaches%20face%20limitations%20in%20handling%20the%20complexity%20of%20new%20service%20demands%0Awithin%20multi-administrative%20domain%20environments.%20This%20paper%20proposes%20a%20future%0Avision%20for%20network%20slicing%20powered%20by%20Large%20Language%20Models%20%28LLMs%29%20and%0Amulti-agent%20systems%2C%20offering%20a%20framework%20that%20can%20be%20integrated%20with%20existing%0AManagement%20and%20Orchestration%20%28MANO%29%20frameworks.%20This%20framework%20leverages%20LLMs%0Ato%20translate%20user%20intent%20into%20technical%20requirements%2C%20map%20network%20functions%20to%0Ainfrastructure%2C%20and%20manage%20the%20entire%20slice%20lifecycle%2C%20while%20multi-agent%0Asystems%20facilitate%20collaboration%20across%20different%20administrative%20domains.%20We%0Aalso%20discuss%20the%20challenges%20associated%20with%20implementing%20this%20framework%20and%0Apotential%20solutions%20to%20mitigate%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13721v1&entry.124074799=Read"},
{"title": "OSCaR: Object State Captioning and State Change Representation", "author": "Nguyen Nguyen and Jing Bi and Ali Vosoughi and Yapeng Tian and Pooyan Fazli and Chenliang Xu", "abstract": "  The capability of intelligent models to extrapolate and comprehend changes in\nobject states is a crucial yet demanding aspect of AI research, particularly\nthrough the lens of human interaction in real-world settings. This task\ninvolves describing complex visual environments, identifying active objects,\nand interpreting their changes as conveyed through language. Traditional\nmethods, which isolate object captioning and state change detection, offer a\nlimited view of dynamic environments. Moreover, relying on a small set of\nsymbolic words to represent changes has restricted the expressiveness of the\nlanguage. To address these challenges, in this paper, we introduce the Object\nState Captioning and State Change Representation (OSCaR) dataset and benchmark.\nOSCaR consists of 14,084 annotated video segments with nearly 1,000 unique\nobjects from various egocentric video collections. It sets a new testbed for\nevaluating multimodal large language models (MLLMs). Our experiments\ndemonstrate that while MLLMs show some skill, they lack a full understanding of\nobject state changes. The benchmark includes a fine-tuned model that, despite\ninitial capabilities, requires significant improvements in accuracy and\ngeneralization ability for effective understanding of these changes. Our code\nand dataset are available at https://github.com/nguyennm1024/OSCaR.\n", "link": "http://arxiv.org/abs/2402.17128v3", "date": "2024-03-20", "relevancy": 1.6025, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OSCaR%3A%20Object%20State%20Captioning%20and%20State%20Change%20Representation&body=Title%3A%20OSCaR%3A%20Object%20State%20Captioning%20and%20State%20Change%20Representation%0AAuthor%3A%20Nguyen%20Nguyen%20and%20Jing%20Bi%20and%20Ali%20Vosoughi%20and%20Yapeng%20Tian%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20The%20capability%20of%20intelligent%20models%20to%20extrapolate%20and%20comprehend%20changes%20in%0Aobject%20states%20is%20a%20crucial%20yet%20demanding%20aspect%20of%20AI%20research%2C%20particularly%0Athrough%20the%20lens%20of%20human%20interaction%20in%20real-world%20settings.%20This%20task%0Ainvolves%20describing%20complex%20visual%20environments%2C%20identifying%20active%20objects%2C%0Aand%20interpreting%20their%20changes%20as%20conveyed%20through%20language.%20Traditional%0Amethods%2C%20which%20isolate%20object%20captioning%20and%20state%20change%20detection%2C%20offer%20a%0Alimited%20view%20of%20dynamic%20environments.%20Moreover%2C%20relying%20on%20a%20small%20set%20of%0Asymbolic%20words%20to%20represent%20changes%20has%20restricted%20the%20expressiveness%20of%20the%0Alanguage.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20introduce%20the%20Object%0AState%20Captioning%20and%20State%20Change%20Representation%20%28OSCaR%29%20dataset%20and%20benchmark.%0AOSCaR%20consists%20of%2014%2C084%20annotated%20video%20segments%20with%20nearly%201%2C000%20unique%0Aobjects%20from%20various%20egocentric%20video%20collections.%20It%20sets%20a%20new%20testbed%20for%0Aevaluating%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20experiments%0Ademonstrate%20that%20while%20MLLMs%20show%20some%20skill%2C%20they%20lack%20a%20full%20understanding%20of%0Aobject%20state%20changes.%20The%20benchmark%20includes%20a%20fine-tuned%20model%20that%2C%20despite%0Ainitial%20capabilities%2C%20requires%20significant%20improvements%20in%20accuracy%20and%0Ageneralization%20ability%20for%20effective%20understanding%20of%20these%20changes.%20Our%20code%0Aand%20dataset%20are%20available%20at%20https%3A//github.com/nguyennm1024/OSCaR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17128v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSCaR%3A%20Object%20State%20Captioning%20and%20State%20Change%20Representation&entry.906535625=Nguyen%20Nguyen%20and%20Jing%20Bi%20and%20Ali%20Vosoughi%20and%20Yapeng%20Tian%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu&entry.1292438233=%20%20The%20capability%20of%20intelligent%20models%20to%20extrapolate%20and%20comprehend%20changes%20in%0Aobject%20states%20is%20a%20crucial%20yet%20demanding%20aspect%20of%20AI%20research%2C%20particularly%0Athrough%20the%20lens%20of%20human%20interaction%20in%20real-world%20settings.%20This%20task%0Ainvolves%20describing%20complex%20visual%20environments%2C%20identifying%20active%20objects%2C%0Aand%20interpreting%20their%20changes%20as%20conveyed%20through%20language.%20Traditional%0Amethods%2C%20which%20isolate%20object%20captioning%20and%20state%20change%20detection%2C%20offer%20a%0Alimited%20view%20of%20dynamic%20environments.%20Moreover%2C%20relying%20on%20a%20small%20set%20of%0Asymbolic%20words%20to%20represent%20changes%20has%20restricted%20the%20expressiveness%20of%20the%0Alanguage.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20introduce%20the%20Object%0AState%20Captioning%20and%20State%20Change%20Representation%20%28OSCaR%29%20dataset%20and%20benchmark.%0AOSCaR%20consists%20of%2014%2C084%20annotated%20video%20segments%20with%20nearly%201%2C000%20unique%0Aobjects%20from%20various%20egocentric%20video%20collections.%20It%20sets%20a%20new%20testbed%20for%0Aevaluating%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20experiments%0Ademonstrate%20that%20while%20MLLMs%20show%20some%20skill%2C%20they%20lack%20a%20full%20understanding%20of%0Aobject%20state%20changes.%20The%20benchmark%20includes%20a%20fine-tuned%20model%20that%2C%20despite%0Ainitial%20capabilities%2C%20requires%20significant%20improvements%20in%20accuracy%20and%0Ageneralization%20ability%20for%20effective%20understanding%20of%20these%20changes.%20Our%20code%0Aand%20dataset%20are%20available%20at%20https%3A//github.com/nguyennm1024/OSCaR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17128v3&entry.124074799=Read"},
{"title": "AutoMix: Automatically Mixing Language Models", "author": "Aman Madaan and Pranjal Aggarwal and Ankit Anand and Srividya Pranavi Potharaju and Swaroop Mishra and Pei Zhou and Aditya Gupta and Dheeraj Rajagopal and Karthik Kappaganthu and Yiming Yang and Shyam Upadhyay and  Mausam and Manaal Faruqui", "abstract": "  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present AutoMix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to AutoMix is a\nfew-shot self-verification mechanism, which estimates the reliability of its\nown outputs without requiring training. Given that verifications can be noisy,\nwe employ a meta-verifier in AutoMix to refine the accuracy of these\nassessments. Our experiments using LLAMA2-13B and GPT-4, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 86%.\nOur code and data are available at https://github.com/automix-llm/automix.\n", "link": "http://arxiv.org/abs/2310.12963v3", "date": "2024-03-20", "relevancy": 1.9232, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4723}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AutoMix%3A%20Automatically%20Mixing%20Language%20Models&body=Title%3A%20AutoMix%3A%20Automatically%20Mixing%20Language%20Models%0AAuthor%3A%20Aman%20Madaan%20and%20Pranjal%20Aggarwal%20and%20Ankit%20Anand%20and%20Srividya%20Pranavi%20Potharaju%20and%20Swaroop%20Mishra%20and%20Pei%20Zhou%20and%20Aditya%20Gupta%20and%20Dheeraj%20Rajagopal%20and%20Karthik%20Kappaganthu%20and%20Yiming%20Yang%20and%20Shyam%20Upadhyay%20and%20%20Mausam%20and%20Manaal%20Faruqui%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20available%20from%20cloud%20API%20providers%20in%0Avarious%20sizes%20and%20configurations.%20While%20this%20diversity%20offers%20a%20broad%20spectrum%0Aof%20choices%2C%20effectively%20leveraging%20the%20options%20to%20optimize%20computational%20cost%0Aand%20performance%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20AutoMix%2C%20an%0Aapproach%20that%20strategically%20routes%20queries%20to%20larger%20LMs%2C%20based%20on%20the%0Aapproximate%20correctness%20of%20outputs%20from%20a%20smaller%20LM.%20Central%20to%20AutoMix%20is%20a%0Afew-shot%20self-verification%20mechanism%2C%20which%20estimates%20the%20reliability%20of%20its%0Aown%20outputs%20without%20requiring%20training.%20Given%20that%20verifications%20can%20be%20noisy%2C%0Awe%20employ%20a%20meta-verifier%20in%20AutoMix%20to%20refine%20the%20accuracy%20of%20these%0Aassessments.%20Our%20experiments%20using%20LLAMA2-13B%20and%20GPT-4%2C%20on%20five%0Acontext-grounded%20reasoning%20datasets%20demonstrate%20that%20AutoMix%20surpasses%0Aestablished%20baselines%2C%20improving%20the%20incremental%20benefit%20per%20cost%20by%20up%20to%2086%25.%0AOur%20code%20and%20data%20are%20available%20at%20https%3A//github.com/automix-llm/automix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12963v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoMix%3A%20Automatically%20Mixing%20Language%20Models&entry.906535625=Aman%20Madaan%20and%20Pranjal%20Aggarwal%20and%20Ankit%20Anand%20and%20Srividya%20Pranavi%20Potharaju%20and%20Swaroop%20Mishra%20and%20Pei%20Zhou%20and%20Aditya%20Gupta%20and%20Dheeraj%20Rajagopal%20and%20Karthik%20Kappaganthu%20and%20Yiming%20Yang%20and%20Shyam%20Upadhyay%20and%20%20Mausam%20and%20Manaal%20Faruqui&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20available%20from%20cloud%20API%20providers%20in%0Avarious%20sizes%20and%20configurations.%20While%20this%20diversity%20offers%20a%20broad%20spectrum%0Aof%20choices%2C%20effectively%20leveraging%20the%20options%20to%20optimize%20computational%20cost%0Aand%20performance%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20AutoMix%2C%20an%0Aapproach%20that%20strategically%20routes%20queries%20to%20larger%20LMs%2C%20based%20on%20the%0Aapproximate%20correctness%20of%20outputs%20from%20a%20smaller%20LM.%20Central%20to%20AutoMix%20is%20a%0Afew-shot%20self-verification%20mechanism%2C%20which%20estimates%20the%20reliability%20of%20its%0Aown%20outputs%20without%20requiring%20training.%20Given%20that%20verifications%20can%20be%20noisy%2C%0Awe%20employ%20a%20meta-verifier%20in%20AutoMix%20to%20refine%20the%20accuracy%20of%20these%0Aassessments.%20Our%20experiments%20using%20LLAMA2-13B%20and%20GPT-4%2C%20on%20five%0Acontext-grounded%20reasoning%20datasets%20demonstrate%20that%20AutoMix%20surpasses%0Aestablished%20baselines%2C%20improving%20the%20incremental%20benefit%20per%20cost%20by%20up%20to%2086%25.%0AOur%20code%20and%20data%20are%20available%20at%20https%3A//github.com/automix-llm/automix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12963v3&entry.124074799=Read"},
{"title": "Surfer: Progressive Reasoning with World Models for Robotic Manipulation", "author": "Pengzhen Ren and Kaidong Zhang and Hetao Zheng and Zixuan Li and Yuhang Wen and Fengda Zhu and Mas Ma and Xiaodan Liang", "abstract": "  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n", "link": "http://arxiv.org/abs/2306.11335v4", "date": "2024-03-20", "relevancy": 1.6445, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6326}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5979}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Surfer%3A%20Progressive%20Reasoning%20with%20World%20Models%20for%20Robotic%20Manipulation&body=Title%3A%20Surfer%3A%20Progressive%20Reasoning%20with%20World%20Models%20for%20Robotic%20Manipulation%0AAuthor%3A%20Pengzhen%20Ren%20and%20Kaidong%20Zhang%20and%20Hetao%20Zheng%20and%20Zixuan%20Li%20and%20Yuhang%20Wen%20and%20Fengda%20Zhu%20and%20Mas%20Ma%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Considering%20how%20to%20make%20the%20model%20accurately%20understand%20and%20follow%20natural%0Alanguage%20instructions%20and%20perform%20actions%20consistent%20with%20world%20knowledge%20is%20a%0Akey%20challenge%20in%20robot%20manipulation.%20This%20mainly%20includes%20human%20fuzzy%0Ainstruction%20reasoning%20and%20the%20following%20of%20physical%20knowledge.%20Therefore%2C%20the%0Aembodied%20intelligence%20agent%20must%20have%20the%20ability%20to%20model%20world%20knowledge%20from%0Atraining%20data.%20However%2C%20most%20existing%20vision%20and%20language%20robot%20manipulation%0Amethods%20mainly%20operate%20in%20less%20realistic%20simulator%20and%20language%20settings%20and%0Alack%20explicit%20modeling%20of%20world%20knowledge.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%0Anovel%20and%20simple%20robot%20manipulation%20framework%2C%20called%20Surfer.%20It%20is%20based%20on%0Athe%20world%20model%2C%20treats%20robot%20manipulation%20as%20a%20state%20transfer%20of%20the%20visual%0Ascene%2C%20and%20decouples%20it%20into%20two%20parts%3A%20action%20and%20scene.%20Then%2C%20the%0Ageneralization%20ability%20of%20the%20model%20on%20new%20instructions%20and%20new%20scenes%20is%0Aenhanced%20by%20explicit%20modeling%20of%20the%20action%20and%20scene%20prediction%20in%20multi-modal%0Ainformation.%20In%20addition%20to%20the%20framework%2C%20we%20also%20built%20a%20robot%20manipulation%0Asimulator%20that%20supports%20full%20physics%20execution%20based%20on%20the%20MuJoCo%20physics%0Aengine.%20It%20can%20automatically%20generate%20demonstration%20training%20data%20and%20test%0Adata%2C%20effectively%20reducing%20labor%20costs.%20To%20conduct%20a%20comprehensive%20and%0Asystematic%20evaluation%20of%20the%20robot%20manipulation%20model%20in%20terms%20of%20language%0Aunderstanding%20and%20physical%20execution%2C%20we%20also%20created%20a%20robotic%20manipulation%0Abenchmark%20with%20progressive%20reasoning%20tasks%2C%20called%20SeaWave.%20It%20contains%204%0Alevels%20of%20progressive%20reasoning%20tasks%20and%20can%20provide%20a%20standardized%20testing%0Aplatform%20for%20embedded%20AI%20agents%20in%20multi-modal%20environments.%20On%20average%2C%20Surfer%0Aachieved%20a%20success%20rate%20of%2054.74%25%20on%20the%20defined%20four%20levels%20of%20manipulation%0Atasks%2C%20exceeding%20the%20best%20baseline%20performance%20of%2047.64%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11335v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surfer%3A%20Progressive%20Reasoning%20with%20World%20Models%20for%20Robotic%20Manipulation&entry.906535625=Pengzhen%20Ren%20and%20Kaidong%20Zhang%20and%20Hetao%20Zheng%20and%20Zixuan%20Li%20and%20Yuhang%20Wen%20and%20Fengda%20Zhu%20and%20Mas%20Ma%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Considering%20how%20to%20make%20the%20model%20accurately%20understand%20and%20follow%20natural%0Alanguage%20instructions%20and%20perform%20actions%20consistent%20with%20world%20knowledge%20is%20a%0Akey%20challenge%20in%20robot%20manipulation.%20This%20mainly%20includes%20human%20fuzzy%0Ainstruction%20reasoning%20and%20the%20following%20of%20physical%20knowledge.%20Therefore%2C%20the%0Aembodied%20intelligence%20agent%20must%20have%20the%20ability%20to%20model%20world%20knowledge%20from%0Atraining%20data.%20However%2C%20most%20existing%20vision%20and%20language%20robot%20manipulation%0Amethods%20mainly%20operate%20in%20less%20realistic%20simulator%20and%20language%20settings%20and%0Alack%20explicit%20modeling%20of%20world%20knowledge.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%0Anovel%20and%20simple%20robot%20manipulation%20framework%2C%20called%20Surfer.%20It%20is%20based%20on%0Athe%20world%20model%2C%20treats%20robot%20manipulation%20as%20a%20state%20transfer%20of%20the%20visual%0Ascene%2C%20and%20decouples%20it%20into%20two%20parts%3A%20action%20and%20scene.%20Then%2C%20the%0Ageneralization%20ability%20of%20the%20model%20on%20new%20instructions%20and%20new%20scenes%20is%0Aenhanced%20by%20explicit%20modeling%20of%20the%20action%20and%20scene%20prediction%20in%20multi-modal%0Ainformation.%20In%20addition%20to%20the%20framework%2C%20we%20also%20built%20a%20robot%20manipulation%0Asimulator%20that%20supports%20full%20physics%20execution%20based%20on%20the%20MuJoCo%20physics%0Aengine.%20It%20can%20automatically%20generate%20demonstration%20training%20data%20and%20test%0Adata%2C%20effectively%20reducing%20labor%20costs.%20To%20conduct%20a%20comprehensive%20and%0Asystematic%20evaluation%20of%20the%20robot%20manipulation%20model%20in%20terms%20of%20language%0Aunderstanding%20and%20physical%20execution%2C%20we%20also%20created%20a%20robotic%20manipulation%0Abenchmark%20with%20progressive%20reasoning%20tasks%2C%20called%20SeaWave.%20It%20contains%204%0Alevels%20of%20progressive%20reasoning%20tasks%20and%20can%20provide%20a%20standardized%20testing%0Aplatform%20for%20embedded%20AI%20agents%20in%20multi-modal%20environments.%20On%20average%2C%20Surfer%0Aachieved%20a%20success%20rate%20of%2054.74%25%20on%20the%20defined%20four%20levels%20of%20manipulation%0Atasks%2C%20exceeding%20the%20best%20baseline%20performance%20of%2047.64%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11335v4&entry.124074799=Read"},
{"title": "LogPr\u00e9cis: Unleashing Language Models for Automated Shell Log Analysis", "author": "Matteo Boffa and Rodolfo Vieira Valentim and Luca Vassio and Danilo Giordano and Idilio Drago and Marco Mellia and Zied Ben Houidi", "abstract": "  The collection of security-related logs holds the key to understanding attack\nbehaviors and diagnosing vulnerabilities. Still, their analysis remains a\ndaunting challenge. Recently, Language Models (LMs) have demonstrated unmatched\npotential in understanding natural and programming languages. The question\narises whether and how LMs could be also useful for security experts since\ntheir logs contain intrinsically confused and obfuscated information. In this\npaper, we systematically study how to benefit from the state-of-the-art in LM\nto automatically analyze text-like Unix shell attack logs. We present a\nthorough design methodology that leads to LogPr\\'ecis. It receives as input raw\nshell sessions and automatically identifies and assigns the attacker tactic to\neach portion of the session, i.e., unveiling the sequence of the attacker's\ngoals. We demonstrate LogPr\\'ecis capability to support the analysis of two\nlarge datasets containing about 400,000 unique Unix shell attacks. LogPr\\'ecis\nreduces them into about 3,000 fingerprints, each grouping sessions with the\nsame sequence of tactics. The abstraction it provides lets the analyst better\nunderstand attacks, identify fingerprints, detect novelty, link similar\nattacks, and track families and mutations. Overall, LogPr\\'ecis, released as\nopen source, paves the way for better and more responsive defense against\ncyberattacks.\n", "link": "http://arxiv.org/abs/2307.08309v2", "date": "2024-03-20", "relevancy": 1.6631, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4156}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4119}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LogPr%C3%A9cis%3A%20Unleashing%20Language%20Models%20for%20Automated%20Shell%20Log%20Analysis&body=Title%3A%20LogPr%C3%A9cis%3A%20Unleashing%20Language%20Models%20for%20Automated%20Shell%20Log%20Analysis%0AAuthor%3A%20Matteo%20Boffa%20and%20Rodolfo%20Vieira%20Valentim%20and%20Luca%20Vassio%20and%20Danilo%20Giordano%20and%20Idilio%20Drago%20and%20Marco%20Mellia%20and%20Zied%20Ben%20Houidi%0AAbstract%3A%20%20%20The%20collection%20of%20security-related%20logs%20holds%20the%20key%20to%20understanding%20attack%0Abehaviors%20and%20diagnosing%20vulnerabilities.%20Still%2C%20their%20analysis%20remains%20a%0Adaunting%20challenge.%20Recently%2C%20Language%20Models%20%28LMs%29%20have%20demonstrated%20unmatched%0Apotential%20in%20understanding%20natural%20and%20programming%20languages.%20The%20question%0Aarises%20whether%20and%20how%20LMs%20could%20be%20also%20useful%20for%20security%20experts%20since%0Atheir%20logs%20contain%20intrinsically%20confused%20and%20obfuscated%20information.%20In%20this%0Apaper%2C%20we%20systematically%20study%20how%20to%20benefit%20from%20the%20state-of-the-art%20in%20LM%0Ato%20automatically%20analyze%20text-like%20Unix%20shell%20attack%20logs.%20We%20present%20a%0Athorough%20design%20methodology%20that%20leads%20to%20LogPr%5C%27ecis.%20It%20receives%20as%20input%20raw%0Ashell%20sessions%20and%20automatically%20identifies%20and%20assigns%20the%20attacker%20tactic%20to%0Aeach%20portion%20of%20the%20session%2C%20i.e.%2C%20unveiling%20the%20sequence%20of%20the%20attacker%27s%0Agoals.%20We%20demonstrate%20LogPr%5C%27ecis%20capability%20to%20support%20the%20analysis%20of%20two%0Alarge%20datasets%20containing%20about%20400%2C000%20unique%20Unix%20shell%20attacks.%20LogPr%5C%27ecis%0Areduces%20them%20into%20about%203%2C000%20fingerprints%2C%20each%20grouping%20sessions%20with%20the%0Asame%20sequence%20of%20tactics.%20The%20abstraction%20it%20provides%20lets%20the%20analyst%20better%0Aunderstand%20attacks%2C%20identify%20fingerprints%2C%20detect%20novelty%2C%20link%20similar%0Aattacks%2C%20and%20track%20families%20and%20mutations.%20Overall%2C%20LogPr%5C%27ecis%2C%20released%20as%0Aopen%20source%2C%20paves%20the%20way%20for%20better%20and%20more%20responsive%20defense%20against%0Acyberattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08309v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogPr%C3%A9cis%3A%20Unleashing%20Language%20Models%20for%20Automated%20Shell%20Log%20Analysis&entry.906535625=Matteo%20Boffa%20and%20Rodolfo%20Vieira%20Valentim%20and%20Luca%20Vassio%20and%20Danilo%20Giordano%20and%20Idilio%20Drago%20and%20Marco%20Mellia%20and%20Zied%20Ben%20Houidi&entry.1292438233=%20%20The%20collection%20of%20security-related%20logs%20holds%20the%20key%20to%20understanding%20attack%0Abehaviors%20and%20diagnosing%20vulnerabilities.%20Still%2C%20their%20analysis%20remains%20a%0Adaunting%20challenge.%20Recently%2C%20Language%20Models%20%28LMs%29%20have%20demonstrated%20unmatched%0Apotential%20in%20understanding%20natural%20and%20programming%20languages.%20The%20question%0Aarises%20whether%20and%20how%20LMs%20could%20be%20also%20useful%20for%20security%20experts%20since%0Atheir%20logs%20contain%20intrinsically%20confused%20and%20obfuscated%20information.%20In%20this%0Apaper%2C%20we%20systematically%20study%20how%20to%20benefit%20from%20the%20state-of-the-art%20in%20LM%0Ato%20automatically%20analyze%20text-like%20Unix%20shell%20attack%20logs.%20We%20present%20a%0Athorough%20design%20methodology%20that%20leads%20to%20LogPr%5C%27ecis.%20It%20receives%20as%20input%20raw%0Ashell%20sessions%20and%20automatically%20identifies%20and%20assigns%20the%20attacker%20tactic%20to%0Aeach%20portion%20of%20the%20session%2C%20i.e.%2C%20unveiling%20the%20sequence%20of%20the%20attacker%27s%0Agoals.%20We%20demonstrate%20LogPr%5C%27ecis%20capability%20to%20support%20the%20analysis%20of%20two%0Alarge%20datasets%20containing%20about%20400%2C000%20unique%20Unix%20shell%20attacks.%20LogPr%5C%27ecis%0Areduces%20them%20into%20about%203%2C000%20fingerprints%2C%20each%20grouping%20sessions%20with%20the%0Asame%20sequence%20of%20tactics.%20The%20abstraction%20it%20provides%20lets%20the%20analyst%20better%0Aunderstand%20attacks%2C%20identify%20fingerprints%2C%20detect%20novelty%2C%20link%20similar%0Aattacks%2C%20and%20track%20families%20and%20mutations.%20Overall%2C%20LogPr%5C%27ecis%2C%20released%20as%0Aopen%20source%2C%20paves%20the%20way%20for%20better%20and%20more%20responsive%20defense%20against%0Acyberattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08309v2&entry.124074799=Read"},
{"title": "Editing Massive Concepts in Text-to-Image Diffusion Models", "author": "Tianwei Xiong and Yue Wu and Enze Xie and Yue Wu and Zhenguo Li and Xihui Liu", "abstract": "  Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.\n", "link": "http://arxiv.org/abs/2403.13807v1", "date": "2024-03-20", "relevancy": 1.764, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5862}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Editing%20Massive%20Concepts%20in%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Editing%20Massive%20Concepts%20in%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Tianwei%20Xiong%20and%20Yue%20Wu%20and%20Enze%20Xie%20and%20Yue%20Wu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20suffer%20from%20the%20risk%20of%20generating%20outdated%2C%0Acopyrighted%2C%20incorrect%2C%20and%20biased%20content.%20While%20previous%20methods%20have%0Amitigated%20the%20issues%20on%20a%20small%20scale%2C%20it%20is%20essential%20to%20handle%20them%0Asimultaneously%20in%20larger-scale%20real-world%20scenarios.%20We%20propose%20a%20two-stage%0Amethod%2C%20Editing%20Massive%20Concepts%20In%20Diffusion%20Models%20%28EMCID%29.%20The%20first%20stage%0Aperforms%20memory%20optimization%20for%20each%20individual%20concept%20with%20dual%0Aself-distillation%20from%20text%20alignment%20loss%20and%20diffusion%20noise%20prediction%20loss.%0AThe%20second%20stage%20conducts%20massive%20concept%20editing%20with%20multi-layer%2C%20closed%20form%0Amodel%20editing.%20We%20further%20propose%20a%20comprehensive%20benchmark%2C%20named%20ImageNet%0AConcept%20Editing%20Benchmark%20%28ICEB%29%2C%20for%20evaluating%20massive%20concept%20editing%20for%0AT2I%20models%20with%20two%20subtasks%2C%20free-form%20prompts%2C%20massive%20concept%20categories%2C%0Aand%20extensive%20evaluation%20metrics.%20Extensive%20experiments%20conducted%20on%20our%0Aproposed%20benchmark%20and%20previous%20benchmarks%20demonstrate%20the%20superior%20scalability%0Aof%20EMCID%20for%20editing%20up%20to%201%2C000%20concepts%2C%20providing%20a%20practical%20approach%20for%0Afast%20adjustment%20and%20re-deployment%20of%20T2I%20diffusion%20models%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13807v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editing%20Massive%20Concepts%20in%20Text-to-Image%20Diffusion%20Models&entry.906535625=Tianwei%20Xiong%20and%20Yue%20Wu%20and%20Enze%20Xie%20and%20Yue%20Wu%20and%20Zhenguo%20Li%20and%20Xihui%20Liu&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20suffer%20from%20the%20risk%20of%20generating%20outdated%2C%0Acopyrighted%2C%20incorrect%2C%20and%20biased%20content.%20While%20previous%20methods%20have%0Amitigated%20the%20issues%20on%20a%20small%20scale%2C%20it%20is%20essential%20to%20handle%20them%0Asimultaneously%20in%20larger-scale%20real-world%20scenarios.%20We%20propose%20a%20two-stage%0Amethod%2C%20Editing%20Massive%20Concepts%20In%20Diffusion%20Models%20%28EMCID%29.%20The%20first%20stage%0Aperforms%20memory%20optimization%20for%20each%20individual%20concept%20with%20dual%0Aself-distillation%20from%20text%20alignment%20loss%20and%20diffusion%20noise%20prediction%20loss.%0AThe%20second%20stage%20conducts%20massive%20concept%20editing%20with%20multi-layer%2C%20closed%20form%0Amodel%20editing.%20We%20further%20propose%20a%20comprehensive%20benchmark%2C%20named%20ImageNet%0AConcept%20Editing%20Benchmark%20%28ICEB%29%2C%20for%20evaluating%20massive%20concept%20editing%20for%0AT2I%20models%20with%20two%20subtasks%2C%20free-form%20prompts%2C%20massive%20concept%20categories%2C%0Aand%20extensive%20evaluation%20metrics.%20Extensive%20experiments%20conducted%20on%20our%0Aproposed%20benchmark%20and%20previous%20benchmarks%20demonstrate%20the%20superior%20scalability%0Aof%20EMCID%20for%20editing%20up%20to%201%2C000%20concepts%2C%20providing%20a%20practical%20approach%20for%0Afast%20adjustment%20and%20re-deployment%20of%20T2I%20diffusion%20models%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13807v1&entry.124074799=Read"},
{"title": "DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with\n  Non-linear Prediction", "author": "Weiyi Lv and Yuhang Huang and Ning Zhang and Ruei-Sung Lin and Mei Han and Dan Zeng", "abstract": "  In Multiple Object Tracking, objects often exhibit non-linear motion of\nacceleration and deceleration, with irregular direction changes.\nTacking-by-detection (TBD) trackers with Kalman Filter motion prediction work\nwell in pedestrian-dominant scenarios but fall short in complex situations when\nmultiple objects perform non-linear and diverse motion simultaneously. To\ntackle the complex non-linear motion, we propose a real-time diffusion-based\nMOT approach named DiffMOT. Specifically, for the motion predictor component,\nwe propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It\nmodels the entire distribution of various motion presented by the data as a\nwhole. It also predicts an individual object's motion conditioning on an\nindividual's historical motion information. Furthermore, it optimizes the\ndiffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT\nis real-time at 22.7FPS, and also outperforms the state-of-the-art on\nDanceTrack and SportsMOT datasets with $62.3\\%$ and $76.2\\%$ in HOTA metrics,\nrespectively. To the best of our knowledge, DiffMOT is the first to introduce a\ndiffusion probabilistic model into the MOT to tackle non-linear motion\nprediction.\n", "link": "http://arxiv.org/abs/2403.02075v2", "date": "2024-03-20", "relevancy": 1.6074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5902}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffMOT%3A%20A%20Real-time%20Diffusion-based%20Multiple%20Object%20Tracker%20with%0A%20%20Non-linear%20Prediction&body=Title%3A%20DiffMOT%3A%20A%20Real-time%20Diffusion-based%20Multiple%20Object%20Tracker%20with%0A%20%20Non-linear%20Prediction%0AAuthor%3A%20Weiyi%20Lv%20and%20Yuhang%20Huang%20and%20Ning%20Zhang%20and%20Ruei-Sung%20Lin%20and%20Mei%20Han%20and%20Dan%20Zeng%0AAbstract%3A%20%20%20In%20Multiple%20Object%20Tracking%2C%20objects%20often%20exhibit%20non-linear%20motion%20of%0Aacceleration%20and%20deceleration%2C%20with%20irregular%20direction%20changes.%0ATacking-by-detection%20%28TBD%29%20trackers%20with%20Kalman%20Filter%20motion%20prediction%20work%0Awell%20in%20pedestrian-dominant%20scenarios%20but%20fall%20short%20in%20complex%20situations%20when%0Amultiple%20objects%20perform%20non-linear%20and%20diverse%20motion%20simultaneously.%20To%0Atackle%20the%20complex%20non-linear%20motion%2C%20we%20propose%20a%20real-time%20diffusion-based%0AMOT%20approach%20named%20DiffMOT.%20Specifically%2C%20for%20the%20motion%20predictor%20component%2C%0Awe%20propose%20a%20novel%20Decoupled%20Diffusion-based%20Motion%20Predictor%20%28D%24%5E2%24MP%29.%20It%0Amodels%20the%20entire%20distribution%20of%20various%20motion%20presented%20by%20the%20data%20as%20a%0Awhole.%20It%20also%20predicts%20an%20individual%20object%27s%20motion%20conditioning%20on%20an%0Aindividual%27s%20historical%20motion%20information.%20Furthermore%2C%20it%20optimizes%20the%0Adiffusion%20process%20with%20much%20fewer%20sampling%20steps.%20As%20a%20MOT%20tracker%2C%20the%20DiffMOT%0Ais%20real-time%20at%2022.7FPS%2C%20and%20also%20outperforms%20the%20state-of-the-art%20on%0ADanceTrack%20and%20SportsMOT%20datasets%20with%20%2462.3%5C%25%24%20and%20%2476.2%5C%25%24%20in%20HOTA%20metrics%2C%0Arespectively.%20To%20the%20best%20of%20our%20knowledge%2C%20DiffMOT%20is%20the%20first%20to%20introduce%20a%0Adiffusion%20probabilistic%20model%20into%20the%20MOT%20to%20tackle%20non-linear%20motion%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02075v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffMOT%3A%20A%20Real-time%20Diffusion-based%20Multiple%20Object%20Tracker%20with%0A%20%20Non-linear%20Prediction&entry.906535625=Weiyi%20Lv%20and%20Yuhang%20Huang%20and%20Ning%20Zhang%20and%20Ruei-Sung%20Lin%20and%20Mei%20Han%20and%20Dan%20Zeng&entry.1292438233=%20%20In%20Multiple%20Object%20Tracking%2C%20objects%20often%20exhibit%20non-linear%20motion%20of%0Aacceleration%20and%20deceleration%2C%20with%20irregular%20direction%20changes.%0ATacking-by-detection%20%28TBD%29%20trackers%20with%20Kalman%20Filter%20motion%20prediction%20work%0Awell%20in%20pedestrian-dominant%20scenarios%20but%20fall%20short%20in%20complex%20situations%20when%0Amultiple%20objects%20perform%20non-linear%20and%20diverse%20motion%20simultaneously.%20To%0Atackle%20the%20complex%20non-linear%20motion%2C%20we%20propose%20a%20real-time%20diffusion-based%0AMOT%20approach%20named%20DiffMOT.%20Specifically%2C%20for%20the%20motion%20predictor%20component%2C%0Awe%20propose%20a%20novel%20Decoupled%20Diffusion-based%20Motion%20Predictor%20%28D%24%5E2%24MP%29.%20It%0Amodels%20the%20entire%20distribution%20of%20various%20motion%20presented%20by%20the%20data%20as%20a%0Awhole.%20It%20also%20predicts%20an%20individual%20object%27s%20motion%20conditioning%20on%20an%0Aindividual%27s%20historical%20motion%20information.%20Furthermore%2C%20it%20optimizes%20the%0Adiffusion%20process%20with%20much%20fewer%20sampling%20steps.%20As%20a%20MOT%20tracker%2C%20the%20DiffMOT%0Ais%20real-time%20at%2022.7FPS%2C%20and%20also%20outperforms%20the%20state-of-the-art%20on%0ADanceTrack%20and%20SportsMOT%20datasets%20with%20%2462.3%5C%25%24%20and%20%2476.2%5C%25%24%20in%20HOTA%20metrics%2C%0Arespectively.%20To%20the%20best%20of%20our%20knowledge%2C%20DiffMOT%20is%20the%20first%20to%20introduce%20a%0Adiffusion%20probabilistic%20model%20into%20the%20MOT%20to%20tackle%20non-linear%20motion%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02075v2&entry.124074799=Read"},
{"title": "Probabilistic Forecasting with Stochastic Interpolants and F\u00f6llmer\n  Processes", "author": "Yifan Chen and Mark Goldstein and Mengjian Hua and Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden", "abstract": "  We propose a framework for probabilistic forecasting of dynamical systems\nbased on generative modeling. Given observations of the system state over time,\nwe formulate the forecasting problem as sampling from the conditional\ndistribution of the future system state given its current state. To this end,\nwe leverage the framework of stochastic interpolants, which facilitates the\nconstruction of a generative model between an arbitrary base distribution and\nthe target. We design a fictitious, non-physical stochastic dynamics that takes\nas initial condition the current system state and produces as output a sample\nfrom the target conditional distribution in finite time and without bias. This\nprocess therefore maps a point mass centered at the current state onto a\nprobabilistic ensemble of forecasts. We prove that the drift coefficient\nentering the stochastic differential equation (SDE) achieving this task is\nnon-singular, and that it can be learned efficiently by square loss regression\nover the time-series data. We show that the drift and the diffusion\ncoefficients of this SDE can be adjusted after training, and that a specific\nchoice that minimizes the impact of the estimation error gives a F\\\"ollmer\nprocess. We highlight the utility of our approach on several complex,\nhigh-dimensional forecasting problems, including stochastically forced\nNavier-Stokes and video prediction on the KTH and CLEVRER datasets.\n", "link": "http://arxiv.org/abs/2403.13724v1", "date": "2024-03-20", "relevancy": 1.4278, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4743}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4668}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Forecasting%20with%20Stochastic%20Interpolants%20and%20F%C3%B6llmer%0A%20%20Processes&body=Title%3A%20Probabilistic%20Forecasting%20with%20Stochastic%20Interpolants%20and%20F%C3%B6llmer%0A%20%20Processes%0AAuthor%3A%20Yifan%20Chen%20and%20Mark%20Goldstein%20and%20Mengjian%20Hua%20and%20Michael%20S.%20Albergo%20and%20Nicholas%20M.%20Boffi%20and%20Eric%20Vanden-Eijnden%0AAbstract%3A%20%20%20We%20propose%20a%20framework%20for%20probabilistic%20forecasting%20of%20dynamical%20systems%0Abased%20on%20generative%20modeling.%20Given%20observations%20of%20the%20system%20state%20over%20time%2C%0Awe%20formulate%20the%20forecasting%20problem%20as%20sampling%20from%20the%20conditional%0Adistribution%20of%20the%20future%20system%20state%20given%20its%20current%20state.%20To%20this%20end%2C%0Awe%20leverage%20the%20framework%20of%20stochastic%20interpolants%2C%20which%20facilitates%20the%0Aconstruction%20of%20a%20generative%20model%20between%20an%20arbitrary%20base%20distribution%20and%0Athe%20target.%20We%20design%20a%20fictitious%2C%20non-physical%20stochastic%20dynamics%20that%20takes%0Aas%20initial%20condition%20the%20current%20system%20state%20and%20produces%20as%20output%20a%20sample%0Afrom%20the%20target%20conditional%20distribution%20in%20finite%20time%20and%20without%20bias.%20This%0Aprocess%20therefore%20maps%20a%20point%20mass%20centered%20at%20the%20current%20state%20onto%20a%0Aprobabilistic%20ensemble%20of%20forecasts.%20We%20prove%20that%20the%20drift%20coefficient%0Aentering%20the%20stochastic%20differential%20equation%20%28SDE%29%20achieving%20this%20task%20is%0Anon-singular%2C%20and%20that%20it%20can%20be%20learned%20efficiently%20by%20square%20loss%20regression%0Aover%20the%20time-series%20data.%20We%20show%20that%20the%20drift%20and%20the%20diffusion%0Acoefficients%20of%20this%20SDE%20can%20be%20adjusted%20after%20training%2C%20and%20that%20a%20specific%0Achoice%20that%20minimizes%20the%20impact%20of%20the%20estimation%20error%20gives%20a%20F%5C%22ollmer%0Aprocess.%20We%20highlight%20the%20utility%20of%20our%20approach%20on%20several%20complex%2C%0Ahigh-dimensional%20forecasting%20problems%2C%20including%20stochastically%20forced%0ANavier-Stokes%20and%20video%20prediction%20on%20the%20KTH%20and%20CLEVRER%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13724v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Forecasting%20with%20Stochastic%20Interpolants%20and%20F%C3%B6llmer%0A%20%20Processes&entry.906535625=Yifan%20Chen%20and%20Mark%20Goldstein%20and%20Mengjian%20Hua%20and%20Michael%20S.%20Albergo%20and%20Nicholas%20M.%20Boffi%20and%20Eric%20Vanden-Eijnden&entry.1292438233=%20%20We%20propose%20a%20framework%20for%20probabilistic%20forecasting%20of%20dynamical%20systems%0Abased%20on%20generative%20modeling.%20Given%20observations%20of%20the%20system%20state%20over%20time%2C%0Awe%20formulate%20the%20forecasting%20problem%20as%20sampling%20from%20the%20conditional%0Adistribution%20of%20the%20future%20system%20state%20given%20its%20current%20state.%20To%20this%20end%2C%0Awe%20leverage%20the%20framework%20of%20stochastic%20interpolants%2C%20which%20facilitates%20the%0Aconstruction%20of%20a%20generative%20model%20between%20an%20arbitrary%20base%20distribution%20and%0Athe%20target.%20We%20design%20a%20fictitious%2C%20non-physical%20stochastic%20dynamics%20that%20takes%0Aas%20initial%20condition%20the%20current%20system%20state%20and%20produces%20as%20output%20a%20sample%0Afrom%20the%20target%20conditional%20distribution%20in%20finite%20time%20and%20without%20bias.%20This%0Aprocess%20therefore%20maps%20a%20point%20mass%20centered%20at%20the%20current%20state%20onto%20a%0Aprobabilistic%20ensemble%20of%20forecasts.%20We%20prove%20that%20the%20drift%20coefficient%0Aentering%20the%20stochastic%20differential%20equation%20%28SDE%29%20achieving%20this%20task%20is%0Anon-singular%2C%20and%20that%20it%20can%20be%20learned%20efficiently%20by%20square%20loss%20regression%0Aover%20the%20time-series%20data.%20We%20show%20that%20the%20drift%20and%20the%20diffusion%0Acoefficients%20of%20this%20SDE%20can%20be%20adjusted%20after%20training%2C%20and%20that%20a%20specific%0Achoice%20that%20minimizes%20the%20impact%20of%20the%20estimation%20error%20gives%20a%20F%5C%22ollmer%0Aprocess.%20We%20highlight%20the%20utility%20of%20our%20approach%20on%20several%20complex%2C%0Ahigh-dimensional%20forecasting%20problems%2C%20including%20stochastically%20forced%0ANavier-Stokes%20and%20video%20prediction%20on%20the%20KTH%20and%20CLEVRER%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13724v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


