<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250921.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Sparse Multiview Open-Vocabulary 3D Detection", "author": "Olivier Moliner and Viktor Larsson and Kalle \u00c5str\u00f6m", "abstract": "  The ability to interpret and comprehend a 3D scene is essential for many\nvision and robotics systems. In numerous applications, this involves 3D object\ndetection, i.e.~identifying the location and dimensions of objects belonging to\na specific category, typically represented as bounding boxes. This has\ntraditionally been solved by training to detect a fixed set of categories,\nwhich limits its use. In this work, we investigate open-vocabulary 3D object\ndetection in the challenging yet practical sparse-view setting, where only a\nlimited number of posed RGB images are available as input. Our approach is\ntraining-free, relying on pre-trained, off-the-shelf 2D foundation models\ninstead of employing computationally expensive 3D feature fusion or requiring\n3D-specific learning. By lifting 2D detections and directly optimizing 3D\nproposals for featuremetric consistency across views, we fully leverage the\nextensive training data available in 2D compared to 3D. Through standard\nbenchmarks, we demonstrate that this simple pipeline establishes a powerful\nbaseline, performing competitively with state-of-the-art techniques in densely\nsampled scenarios while significantly outperforming them in the sparse-view\nsetting.\n", "link": "http://arxiv.org/abs/2509.15924v1", "date": "2025-09-19", "relevancy": 3.2826, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Multiview%20Open-Vocabulary%203D%20Detection&body=Title%3A%20Sparse%20Multiview%20Open-Vocabulary%203D%20Detection%0AAuthor%3A%20Olivier%20Moliner%20and%20Viktor%20Larsson%20and%20Kalle%20%C3%85str%C3%B6m%0AAbstract%3A%20%20%20The%20ability%20to%20interpret%20and%20comprehend%20a%203D%20scene%20is%20essential%20for%20many%0Avision%20and%20robotics%20systems.%20In%20numerous%20applications%2C%20this%20involves%203D%20object%0Adetection%2C%20i.e.~identifying%20the%20location%20and%20dimensions%20of%20objects%20belonging%20to%0Aa%20specific%20category%2C%20typically%20represented%20as%20bounding%20boxes.%20This%20has%0Atraditionally%20been%20solved%20by%20training%20to%20detect%20a%20fixed%20set%20of%20categories%2C%0Awhich%20limits%20its%20use.%20In%20this%20work%2C%20we%20investigate%20open-vocabulary%203D%20object%0Adetection%20in%20the%20challenging%20yet%20practical%20sparse-view%20setting%2C%20where%20only%20a%0Alimited%20number%20of%20posed%20RGB%20images%20are%20available%20as%20input.%20Our%20approach%20is%0Atraining-free%2C%20relying%20on%20pre-trained%2C%20off-the-shelf%202D%20foundation%20models%0Ainstead%20of%20employing%20computationally%20expensive%203D%20feature%20fusion%20or%20requiring%0A3D-specific%20learning.%20By%20lifting%202D%20detections%20and%20directly%20optimizing%203D%0Aproposals%20for%20featuremetric%20consistency%20across%20views%2C%20we%20fully%20leverage%20the%0Aextensive%20training%20data%20available%20in%202D%20compared%20to%203D.%20Through%20standard%0Abenchmarks%2C%20we%20demonstrate%20that%20this%20simple%20pipeline%20establishes%20a%20powerful%0Abaseline%2C%20performing%20competitively%20with%20state-of-the-art%20techniques%20in%20densely%0Asampled%20scenarios%20while%20significantly%20outperforming%20them%20in%20the%20sparse-view%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Multiview%2520Open-Vocabulary%25203D%2520Detection%26entry.906535625%3DOlivier%2520Moliner%2520and%2520Viktor%2520Larsson%2520and%2520Kalle%2520%25C3%2585str%25C3%25B6m%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520interpret%2520and%2520comprehend%2520a%25203D%2520scene%2520is%2520essential%2520for%2520many%250Avision%2520and%2520robotics%2520systems.%2520In%2520numerous%2520applications%252C%2520this%2520involves%25203D%2520object%250Adetection%252C%2520i.e.~identifying%2520the%2520location%2520and%2520dimensions%2520of%2520objects%2520belonging%2520to%250Aa%2520specific%2520category%252C%2520typically%2520represented%2520as%2520bounding%2520boxes.%2520This%2520has%250Atraditionally%2520been%2520solved%2520by%2520training%2520to%2520detect%2520a%2520fixed%2520set%2520of%2520categories%252C%250Awhich%2520limits%2520its%2520use.%2520In%2520this%2520work%252C%2520we%2520investigate%2520open-vocabulary%25203D%2520object%250Adetection%2520in%2520the%2520challenging%2520yet%2520practical%2520sparse-view%2520setting%252C%2520where%2520only%2520a%250Alimited%2520number%2520of%2520posed%2520RGB%2520images%2520are%2520available%2520as%2520input.%2520Our%2520approach%2520is%250Atraining-free%252C%2520relying%2520on%2520pre-trained%252C%2520off-the-shelf%25202D%2520foundation%2520models%250Ainstead%2520of%2520employing%2520computationally%2520expensive%25203D%2520feature%2520fusion%2520or%2520requiring%250A3D-specific%2520learning.%2520By%2520lifting%25202D%2520detections%2520and%2520directly%2520optimizing%25203D%250Aproposals%2520for%2520featuremetric%2520consistency%2520across%2520views%252C%2520we%2520fully%2520leverage%2520the%250Aextensive%2520training%2520data%2520available%2520in%25202D%2520compared%2520to%25203D.%2520Through%2520standard%250Abenchmarks%252C%2520we%2520demonstrate%2520that%2520this%2520simple%2520pipeline%2520establishes%2520a%2520powerful%250Abaseline%252C%2520performing%2520competitively%2520with%2520state-of-the-art%2520techniques%2520in%2520densely%250Asampled%2520scenarios%2520while%2520significantly%2520outperforming%2520them%2520in%2520the%2520sparse-view%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Multiview%20Open-Vocabulary%203D%20Detection&entry.906535625=Olivier%20Moliner%20and%20Viktor%20Larsson%20and%20Kalle%20%C3%85str%C3%B6m&entry.1292438233=%20%20The%20ability%20to%20interpret%20and%20comprehend%20a%203D%20scene%20is%20essential%20for%20many%0Avision%20and%20robotics%20systems.%20In%20numerous%20applications%2C%20this%20involves%203D%20object%0Adetection%2C%20i.e.~identifying%20the%20location%20and%20dimensions%20of%20objects%20belonging%20to%0Aa%20specific%20category%2C%20typically%20represented%20as%20bounding%20boxes.%20This%20has%0Atraditionally%20been%20solved%20by%20training%20to%20detect%20a%20fixed%20set%20of%20categories%2C%0Awhich%20limits%20its%20use.%20In%20this%20work%2C%20we%20investigate%20open-vocabulary%203D%20object%0Adetection%20in%20the%20challenging%20yet%20practical%20sparse-view%20setting%2C%20where%20only%20a%0Alimited%20number%20of%20posed%20RGB%20images%20are%20available%20as%20input.%20Our%20approach%20is%0Atraining-free%2C%20relying%20on%20pre-trained%2C%20off-the-shelf%202D%20foundation%20models%0Ainstead%20of%20employing%20computationally%20expensive%203D%20feature%20fusion%20or%20requiring%0A3D-specific%20learning.%20By%20lifting%202D%20detections%20and%20directly%20optimizing%203D%0Aproposals%20for%20featuremetric%20consistency%20across%20views%2C%20we%20fully%20leverage%20the%0Aextensive%20training%20data%20available%20in%202D%20compared%20to%203D.%20Through%20standard%0Abenchmarks%2C%20we%20demonstrate%20that%20this%20simple%20pipeline%20establishes%20a%20powerful%0Abaseline%2C%20performing%20competitively%20with%20state-of-the-art%20techniques%20in%20densely%0Asampled%20scenarios%20while%20significantly%20outperforming%20them%20in%20the%20sparse-view%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15924v1&entry.124074799=Read"},
{"title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval", "author": "Liwei Liao and Xufeng Li and Xiaoyun Zheng and Boning Liu and Feng Gao and Ronggang Wang", "abstract": "  3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text\nprompts, which is essential for applications such as robotics. However,\nexisting 3DVG methods encounter two main challenges: first, they struggle to\nhandle the implicit representation of spatial textures in 3D Gaussian Splatting\n(3DGS), making per-scene training indispensable; second, they typically require\nlarges amounts of labeled data for effective training. To this end, we propose\n\\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel\nzero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D\nretrieval task that leverages object-level view retrieval to collect grounding\nclues from multiple views, which not only avoids the costly process of 3D\nannotation, but also eliminates the need for per-scene training. Extensive\nexperiments demonstrate that our method achieves state-of-the-art visual\ngrounding performance while avoiding per-scene training, providing a solid\nfoundation for zero-shot 3DVG research. Video demos can be found in\nhttps://github.com/leviome/GVR_demos.\n", "link": "http://arxiv.org/abs/2509.15871v1", "date": "2025-09-19", "relevancy": 3.1618, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6435}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.631}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Visual%20Grounding%20in%203D%20Gaussians%20via%20View%20Retrieval&body=Title%3A%20Zero-Shot%20Visual%20Grounding%20in%203D%20Gaussians%20via%20View%20Retrieval%0AAuthor%3A%20Liwei%20Liao%20and%20Xufeng%20Li%20and%20Xiaoyun%20Zheng%20and%20Boning%20Liu%20and%20Feng%20Gao%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%203D%20scenes%20based%20on%20text%0Aprompts%2C%20which%20is%20essential%20for%20applications%20such%20as%20robotics.%20However%2C%0Aexisting%203DVG%20methods%20encounter%20two%20main%20challenges%3A%20first%2C%20they%20struggle%20to%0Ahandle%20the%20implicit%20representation%20of%20spatial%20textures%20in%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20making%20per-scene%20training%20indispensable%3B%20second%2C%20they%20typically%20require%0Alarges%20amounts%20of%20labeled%20data%20for%20effective%20training.%20To%20this%20end%2C%20we%20propose%0A%5Cunderline%7BG%7Drounding%20via%20%5Cunderline%7BV%7Diew%20%5Cunderline%7BR%7Detrieval%20%28GVR%29%2C%20a%20novel%0Azero-shot%20visual%20grounding%20framework%20for%203DGS%20to%20transform%203DVG%20as%20a%202D%0Aretrieval%20task%20that%20leverages%20object-level%20view%20retrieval%20to%20collect%20grounding%0Aclues%20from%20multiple%20views%2C%20which%20not%20only%20avoids%20the%20costly%20process%20of%203D%0Aannotation%2C%20but%20also%20eliminates%20the%20need%20for%20per-scene%20training.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20visual%0Agrounding%20performance%20while%20avoiding%20per-scene%20training%2C%20providing%20a%20solid%0Afoundation%20for%20zero-shot%203DVG%20research.%20Video%20demos%20can%20be%20found%20in%0Ahttps%3A//github.com/leviome/GVR_demos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Visual%2520Grounding%2520in%25203D%2520Gaussians%2520via%2520View%2520Retrieval%26entry.906535625%3DLiwei%2520Liao%2520and%2520Xufeng%2520Li%2520and%2520Xiaoyun%2520Zheng%2520and%2520Boning%2520Liu%2520and%2520Feng%2520Gao%2520and%2520Ronggang%2520Wang%26entry.1292438233%3D%2520%25203D%2520Visual%2520Grounding%2520%25283DVG%2529%2520aims%2520to%2520locate%2520objects%2520in%25203D%2520scenes%2520based%2520on%2520text%250Aprompts%252C%2520which%2520is%2520essential%2520for%2520applications%2520such%2520as%2520robotics.%2520However%252C%250Aexisting%25203DVG%2520methods%2520encounter%2520two%2520main%2520challenges%253A%2520first%252C%2520they%2520struggle%2520to%250Ahandle%2520the%2520implicit%2520representation%2520of%2520spatial%2520textures%2520in%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%252C%2520making%2520per-scene%2520training%2520indispensable%253B%2520second%252C%2520they%2520typically%2520require%250Alarges%2520amounts%2520of%2520labeled%2520data%2520for%2520effective%2520training.%2520To%2520this%2520end%252C%2520we%2520propose%250A%255Cunderline%257BG%257Drounding%2520via%2520%255Cunderline%257BV%257Diew%2520%255Cunderline%257BR%257Detrieval%2520%2528GVR%2529%252C%2520a%2520novel%250Azero-shot%2520visual%2520grounding%2520framework%2520for%25203DGS%2520to%2520transform%25203DVG%2520as%2520a%25202D%250Aretrieval%2520task%2520that%2520leverages%2520object-level%2520view%2520retrieval%2520to%2520collect%2520grounding%250Aclues%2520from%2520multiple%2520views%252C%2520which%2520not%2520only%2520avoids%2520the%2520costly%2520process%2520of%25203D%250Aannotation%252C%2520but%2520also%2520eliminates%2520the%2520need%2520for%2520per-scene%2520training.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520visual%250Agrounding%2520performance%2520while%2520avoiding%2520per-scene%2520training%252C%2520providing%2520a%2520solid%250Afoundation%2520for%2520zero-shot%25203DVG%2520research.%2520Video%2520demos%2520can%2520be%2520found%2520in%250Ahttps%253A//github.com/leviome/GVR_demos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Visual%20Grounding%20in%203D%20Gaussians%20via%20View%20Retrieval&entry.906535625=Liwei%20Liao%20and%20Xufeng%20Li%20and%20Xiaoyun%20Zheng%20and%20Boning%20Liu%20and%20Feng%20Gao%20and%20Ronggang%20Wang&entry.1292438233=%20%203D%20Visual%20Grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%203D%20scenes%20based%20on%20text%0Aprompts%2C%20which%20is%20essential%20for%20applications%20such%20as%20robotics.%20However%2C%0Aexisting%203DVG%20methods%20encounter%20two%20main%20challenges%3A%20first%2C%20they%20struggle%20to%0Ahandle%20the%20implicit%20representation%20of%20spatial%20textures%20in%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20making%20per-scene%20training%20indispensable%3B%20second%2C%20they%20typically%20require%0Alarges%20amounts%20of%20labeled%20data%20for%20effective%20training.%20To%20this%20end%2C%20we%20propose%0A%5Cunderline%7BG%7Drounding%20via%20%5Cunderline%7BV%7Diew%20%5Cunderline%7BR%7Detrieval%20%28GVR%29%2C%20a%20novel%0Azero-shot%20visual%20grounding%20framework%20for%203DGS%20to%20transform%203DVG%20as%20a%202D%0Aretrieval%20task%20that%20leverages%20object-level%20view%20retrieval%20to%20collect%20grounding%0Aclues%20from%20multiple%20views%2C%20which%20not%20only%20avoids%20the%20costly%20process%20of%203D%0Aannotation%2C%20but%20also%20eliminates%20the%20need%20for%20per-scene%20training.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20visual%0Agrounding%20performance%20while%20avoiding%20per-scene%20training%2C%20providing%20a%20solid%0Afoundation%20for%20zero-shot%203DVG%20research.%20Video%20demos%20can%20be%20found%20in%0Ahttps%3A//github.com/leviome/GVR_demos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15871v1&entry.124074799=Read"},
{"title": "LLMs Can Compensate for Deficiencies in Visual Representations", "author": "Sho Takishita and Jay Gala and Abdelrahman Mohamed and Kentaro Inui and Yova Kementchedjhieva", "abstract": "  Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder.\n", "link": "http://arxiv.org/abs/2506.05439v2", "date": "2025-09-19", "relevancy": 3.1535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Can%20Compensate%20for%20Deficiencies%20in%20Visual%20Representations&body=Title%3A%20LLMs%20Can%20Compensate%20for%20Deficiencies%20in%20Visual%20Representations%0AAuthor%3A%20Sho%20Takishita%20and%20Jay%20Gala%20and%20Abdelrahman%20Mohamed%20and%20Kentaro%20Inui%20and%20Yova%20Kementchedjhieva%0AAbstract%3A%20%20%20Many%20vision-language%20models%20%28VLMs%29%20that%20prove%20very%20effective%20at%20a%20range%20of%0Amultimodal%20task%2C%20build%20on%20CLIP-based%20vision%20encoders%2C%20which%20are%20known%20to%20have%0Avarious%20limitations.%20We%20investigate%20the%20hypothesis%20that%20the%20strong%20language%0Abackbone%20in%20VLMs%20compensates%20for%20possibly%20weak%20visual%20features%20by%0Acontextualizing%20or%20enriching%20them.%20Using%20three%20CLIP-based%20VLMs%2C%20we%20perform%0Acontrolled%20self-attention%20ablations%20on%20a%20carefully%20designed%20probing%20task.%20Our%0Afindings%20show%20that%20despite%20known%20limitations%2C%20CLIP%20visual%20representations%20offer%0Aready-to-read%20semantic%20information%20to%20the%20language%20decoder.%20However%2C%20in%0Ascenarios%20of%20reduced%20contextualization%20in%20the%20visual%20representations%2C%20the%0Alanguage%20decoder%20can%20largely%20compensate%20for%20the%20deficiency%20and%20recover%0Aperformance.%20This%20suggests%20a%20dynamic%20division%20of%20labor%20in%20VLMs%20and%20motivates%0Afuture%20architectures%20that%20offload%20more%20visual%20processing%20to%20the%20language%0Adecoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Can%2520Compensate%2520for%2520Deficiencies%2520in%2520Visual%2520Representations%26entry.906535625%3DSho%2520Takishita%2520and%2520Jay%2520Gala%2520and%2520Abdelrahman%2520Mohamed%2520and%2520Kentaro%2520Inui%2520and%2520Yova%2520Kementchedjhieva%26entry.1292438233%3D%2520%2520Many%2520vision-language%2520models%2520%2528VLMs%2529%2520that%2520prove%2520very%2520effective%2520at%2520a%2520range%2520of%250Amultimodal%2520task%252C%2520build%2520on%2520CLIP-based%2520vision%2520encoders%252C%2520which%2520are%2520known%2520to%2520have%250Avarious%2520limitations.%2520We%2520investigate%2520the%2520hypothesis%2520that%2520the%2520strong%2520language%250Abackbone%2520in%2520VLMs%2520compensates%2520for%2520possibly%2520weak%2520visual%2520features%2520by%250Acontextualizing%2520or%2520enriching%2520them.%2520Using%2520three%2520CLIP-based%2520VLMs%252C%2520we%2520perform%250Acontrolled%2520self-attention%2520ablations%2520on%2520a%2520carefully%2520designed%2520probing%2520task.%2520Our%250Afindings%2520show%2520that%2520despite%2520known%2520limitations%252C%2520CLIP%2520visual%2520representations%2520offer%250Aready-to-read%2520semantic%2520information%2520to%2520the%2520language%2520decoder.%2520However%252C%2520in%250Ascenarios%2520of%2520reduced%2520contextualization%2520in%2520the%2520visual%2520representations%252C%2520the%250Alanguage%2520decoder%2520can%2520largely%2520compensate%2520for%2520the%2520deficiency%2520and%2520recover%250Aperformance.%2520This%2520suggests%2520a%2520dynamic%2520division%2520of%2520labor%2520in%2520VLMs%2520and%2520motivates%250Afuture%2520architectures%2520that%2520offload%2520more%2520visual%2520processing%2520to%2520the%2520language%250Adecoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Can%20Compensate%20for%20Deficiencies%20in%20Visual%20Representations&entry.906535625=Sho%20Takishita%20and%20Jay%20Gala%20and%20Abdelrahman%20Mohamed%20and%20Kentaro%20Inui%20and%20Yova%20Kementchedjhieva&entry.1292438233=%20%20Many%20vision-language%20models%20%28VLMs%29%20that%20prove%20very%20effective%20at%20a%20range%20of%0Amultimodal%20task%2C%20build%20on%20CLIP-based%20vision%20encoders%2C%20which%20are%20known%20to%20have%0Avarious%20limitations.%20We%20investigate%20the%20hypothesis%20that%20the%20strong%20language%0Abackbone%20in%20VLMs%20compensates%20for%20possibly%20weak%20visual%20features%20by%0Acontextualizing%20or%20enriching%20them.%20Using%20three%20CLIP-based%20VLMs%2C%20we%20perform%0Acontrolled%20self-attention%20ablations%20on%20a%20carefully%20designed%20probing%20task.%20Our%0Afindings%20show%20that%20despite%20known%20limitations%2C%20CLIP%20visual%20representations%20offer%0Aready-to-read%20semantic%20information%20to%20the%20language%20decoder.%20However%2C%20in%0Ascenarios%20of%20reduced%20contextualization%20in%20the%20visual%20representations%2C%20the%0Alanguage%20decoder%20can%20largely%20compensate%20for%20the%20deficiency%20and%20recover%0Aperformance.%20This%20suggests%20a%20dynamic%20division%20of%20labor%20in%20VLMs%20and%20motivates%0Afuture%20architectures%20that%20offload%20more%20visual%20processing%20to%20the%20language%0Adecoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05439v2&entry.124074799=Read"},
{"title": "Self-Supervised Cross-Modal Learning for Image-to-Point Cloud\n  Registration", "author": "Xingmei Wang and Xiaoyu Hu and Chengkai Huang and Ziyan Zeng and Guohao Nie and Quan Z. Sheng and Lina Yao", "abstract": "  Bridging 2D and 3D sensor modalities is critical for robust perception in\nautonomous systems. However, image-to-point cloud (I2P) registration remains\nchallenging due to the semantic-geometric gap between texture-rich but\ndepth-ambiguous images and sparse yet metrically precise point clouds, as well\nas the tendency of existing methods to converge to local optima. To overcome\nthese limitations, we introduce CrossI2P, a self-supervised framework that\nunifies cross-modal learning and two-stage registration in a single end-to-end\npipeline. First, we learn a geometric-semantic fused embedding space via\ndual-path contrastive learning, enabling annotation-free, bidirectional\nalignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine\nregistration paradigm: a global stage establishes superpoint-superpixel\ncorrespondences through joint intra-modal context and cross-modal interaction\nmodeling, followed by a geometry-constrained point-level refinement for precise\nregistration. Third, we employ a dynamic training mechanism with gradient\nnormalization to balance losses for feature alignment, correspondence\nrefinement, and pose estimation. Extensive experiments demonstrate that\nCrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry\nbenchmark and by 37.9% on nuScenes, significantly improving both accuracy and\nrobustness.\n", "link": "http://arxiv.org/abs/2509.15882v1", "date": "2025-09-19", "relevancy": 3.1452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.654}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Cross-Modal%20Learning%20for%20Image-to-Point%20Cloud%0A%20%20Registration&body=Title%3A%20Self-Supervised%20Cross-Modal%20Learning%20for%20Image-to-Point%20Cloud%0A%20%20Registration%0AAuthor%3A%20Xingmei%20Wang%20and%20Xiaoyu%20Hu%20and%20Chengkai%20Huang%20and%20Ziyan%20Zeng%20and%20Guohao%20Nie%20and%20Quan%20Z.%20Sheng%20and%20Lina%20Yao%0AAbstract%3A%20%20%20Bridging%202D%20and%203D%20sensor%20modalities%20is%20critical%20for%20robust%20perception%20in%0Aautonomous%20systems.%20However%2C%20image-to-point%20cloud%20%28I2P%29%20registration%20remains%0Achallenging%20due%20to%20the%20semantic-geometric%20gap%20between%20texture-rich%20but%0Adepth-ambiguous%20images%20and%20sparse%20yet%20metrically%20precise%20point%20clouds%2C%20as%20well%0Aas%20the%20tendency%20of%20existing%20methods%20to%20converge%20to%20local%20optima.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20CrossI2P%2C%20a%20self-supervised%20framework%20that%0Aunifies%20cross-modal%20learning%20and%20two-stage%20registration%20in%20a%20single%20end-to-end%0Apipeline.%20First%2C%20we%20learn%20a%20geometric-semantic%20fused%20embedding%20space%20via%0Adual-path%20contrastive%20learning%2C%20enabling%20annotation-free%2C%20bidirectional%0Aalignment%20of%202D%20textures%20and%203D%20structures.%20Second%2C%20we%20adopt%20a%20coarse-to-fine%0Aregistration%20paradigm%3A%20a%20global%20stage%20establishes%20superpoint-superpixel%0Acorrespondences%20through%20joint%20intra-modal%20context%20and%20cross-modal%20interaction%0Amodeling%2C%20followed%20by%20a%20geometry-constrained%20point-level%20refinement%20for%20precise%0Aregistration.%20Third%2C%20we%20employ%20a%20dynamic%20training%20mechanism%20with%20gradient%0Anormalization%20to%20balance%20losses%20for%20feature%20alignment%2C%20correspondence%0Arefinement%2C%20and%20pose%20estimation.%20Extensive%20experiments%20demonstrate%20that%0ACrossI2P%20outperforms%20state-of-the-art%20methods%20by%2023.7%25%20on%20the%20KITTI%20Odometry%0Abenchmark%20and%20by%2037.9%25%20on%20nuScenes%2C%20significantly%20improving%20both%20accuracy%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Cross-Modal%2520Learning%2520for%2520Image-to-Point%2520Cloud%250A%2520%2520Registration%26entry.906535625%3DXingmei%2520Wang%2520and%2520Xiaoyu%2520Hu%2520and%2520Chengkai%2520Huang%2520and%2520Ziyan%2520Zeng%2520and%2520Guohao%2520Nie%2520and%2520Quan%2520Z.%2520Sheng%2520and%2520Lina%2520Yao%26entry.1292438233%3D%2520%2520Bridging%25202D%2520and%25203D%2520sensor%2520modalities%2520is%2520critical%2520for%2520robust%2520perception%2520in%250Aautonomous%2520systems.%2520However%252C%2520image-to-point%2520cloud%2520%2528I2P%2529%2520registration%2520remains%250Achallenging%2520due%2520to%2520the%2520semantic-geometric%2520gap%2520between%2520texture-rich%2520but%250Adepth-ambiguous%2520images%2520and%2520sparse%2520yet%2520metrically%2520precise%2520point%2520clouds%252C%2520as%2520well%250Aas%2520the%2520tendency%2520of%2520existing%2520methods%2520to%2520converge%2520to%2520local%2520optima.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520CrossI2P%252C%2520a%2520self-supervised%2520framework%2520that%250Aunifies%2520cross-modal%2520learning%2520and%2520two-stage%2520registration%2520in%2520a%2520single%2520end-to-end%250Apipeline.%2520First%252C%2520we%2520learn%2520a%2520geometric-semantic%2520fused%2520embedding%2520space%2520via%250Adual-path%2520contrastive%2520learning%252C%2520enabling%2520annotation-free%252C%2520bidirectional%250Aalignment%2520of%25202D%2520textures%2520and%25203D%2520structures.%2520Second%252C%2520we%2520adopt%2520a%2520coarse-to-fine%250Aregistration%2520paradigm%253A%2520a%2520global%2520stage%2520establishes%2520superpoint-superpixel%250Acorrespondences%2520through%2520joint%2520intra-modal%2520context%2520and%2520cross-modal%2520interaction%250Amodeling%252C%2520followed%2520by%2520a%2520geometry-constrained%2520point-level%2520refinement%2520for%2520precise%250Aregistration.%2520Third%252C%2520we%2520employ%2520a%2520dynamic%2520training%2520mechanism%2520with%2520gradient%250Anormalization%2520to%2520balance%2520losses%2520for%2520feature%2520alignment%252C%2520correspondence%250Arefinement%252C%2520and%2520pose%2520estimation.%2520Extensive%2520experiments%2520demonstrate%2520that%250ACrossI2P%2520outperforms%2520state-of-the-art%2520methods%2520by%252023.7%2525%2520on%2520the%2520KITTI%2520Odometry%250Abenchmark%2520and%2520by%252037.9%2525%2520on%2520nuScenes%252C%2520significantly%2520improving%2520both%2520accuracy%2520and%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Cross-Modal%20Learning%20for%20Image-to-Point%20Cloud%0A%20%20Registration&entry.906535625=Xingmei%20Wang%20and%20Xiaoyu%20Hu%20and%20Chengkai%20Huang%20and%20Ziyan%20Zeng%20and%20Guohao%20Nie%20and%20Quan%20Z.%20Sheng%20and%20Lina%20Yao&entry.1292438233=%20%20Bridging%202D%20and%203D%20sensor%20modalities%20is%20critical%20for%20robust%20perception%20in%0Aautonomous%20systems.%20However%2C%20image-to-point%20cloud%20%28I2P%29%20registration%20remains%0Achallenging%20due%20to%20the%20semantic-geometric%20gap%20between%20texture-rich%20but%0Adepth-ambiguous%20images%20and%20sparse%20yet%20metrically%20precise%20point%20clouds%2C%20as%20well%0Aas%20the%20tendency%20of%20existing%20methods%20to%20converge%20to%20local%20optima.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20CrossI2P%2C%20a%20self-supervised%20framework%20that%0Aunifies%20cross-modal%20learning%20and%20two-stage%20registration%20in%20a%20single%20end-to-end%0Apipeline.%20First%2C%20we%20learn%20a%20geometric-semantic%20fused%20embedding%20space%20via%0Adual-path%20contrastive%20learning%2C%20enabling%20annotation-free%2C%20bidirectional%0Aalignment%20of%202D%20textures%20and%203D%20structures.%20Second%2C%20we%20adopt%20a%20coarse-to-fine%0Aregistration%20paradigm%3A%20a%20global%20stage%20establishes%20superpoint-superpixel%0Acorrespondences%20through%20joint%20intra-modal%20context%20and%20cross-modal%20interaction%0Amodeling%2C%20followed%20by%20a%20geometry-constrained%20point-level%20refinement%20for%20precise%0Aregistration.%20Third%2C%20we%20employ%20a%20dynamic%20training%20mechanism%20with%20gradient%0Anormalization%20to%20balance%20losses%20for%20feature%20alignment%2C%20correspondence%0Arefinement%2C%20and%20pose%20estimation.%20Extensive%20experiments%20demonstrate%20that%0ACrossI2P%20outperforms%20state-of-the-art%20methods%20by%2023.7%25%20on%20the%20KITTI%20Odometry%0Abenchmark%20and%20by%2037.9%25%20on%20nuScenes%2C%20significantly%20improving%20both%20accuracy%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15882v1&entry.124074799=Read"},
{"title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented\n  LiDAR segmentation", "author": "Paul Julius K\u00fchn and Duc Anh Nguyen and Arjan Kuijper and Holger Graf and Dieter Fellner and Saptarshi Neil Sinha", "abstract": "  Point cloud segmentation is central to autonomous driving and 3D scene\nunderstanding. While voxel- and point-based methods dominate recent research\ndue to their compatibility with deep architectures and ability to capture\nfine-grained geometry, they often incur high computational cost, irregular\nmemory access, and limited real-time efficiency. In contrast, range-view\nmethods, though relatively underexplored - can leverage mature 2D semantic\nsegmentation techniques for fast and accurate predictions. Motivated by the\nrapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot\nrecognition, and multimodal tasks, we investigate whether SAM2, the current\nstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone for\nLiDAR point cloud segmentation in the range view. We present , to our\nknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,\ncoupling efficient 2D feature extraction with standard\nprojection/back-projection to operate on point clouds. To optimize SAM2 for\nrange-view representations, we implement several architectural modifications to\nthe encoder: (1) a novel module that emphasizes horizontal spatial dependencies\ninherent in LiDAR range images, (2) a customized configuration of tailored to\nthe geometric properties of spherical projections, and (3) an adapted mechanism\nin the encoder backbone specifically designed to capture the unique spatial\npatterns and discontinuities present in range-view pseudo-images. Our approach\nachieves competitive performance on SemanticKITTI while benefiting from the\nspeed, scalability, and deployment simplicity of 2D-centric pipelines. This\nwork highlights the viability of VFMs as general-purpose backbones for 3D\nperception and opens a path toward unified, foundation-model-driven LiDAR\nsegmentation. Results lets us conclude that range-view segmentation methods\nusing VFMs leads to promising results.\n", "link": "http://arxiv.org/abs/2509.15886v1", "date": "2025-09-19", "relevancy": 3.0769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RangeSAM%3A%20Leveraging%20Visual%20Foundation%20Models%20for%20Range-View%20repesented%0A%20%20LiDAR%20segmentation&body=Title%3A%20RangeSAM%3A%20Leveraging%20Visual%20Foundation%20Models%20for%20Range-View%20repesented%0A%20%20LiDAR%20segmentation%0AAuthor%3A%20Paul%20Julius%20K%C3%BChn%20and%20Duc%20Anh%20Nguyen%20and%20Arjan%20Kuijper%20and%20Holger%20Graf%20and%20Dieter%20Fellner%20and%20Saptarshi%20Neil%20Sinha%0AAbstract%3A%20%20%20Point%20cloud%20segmentation%20is%20central%20to%20autonomous%20driving%20and%203D%20scene%0Aunderstanding.%20While%20voxel-%20and%20point-based%20methods%20dominate%20recent%20research%0Adue%20to%20their%20compatibility%20with%20deep%20architectures%20and%20ability%20to%20capture%0Afine-grained%20geometry%2C%20they%20often%20incur%20high%20computational%20cost%2C%20irregular%0Amemory%20access%2C%20and%20limited%20real-time%20efficiency.%20In%20contrast%2C%20range-view%0Amethods%2C%20though%20relatively%20underexplored%20-%20can%20leverage%20mature%202D%20semantic%0Asegmentation%20techniques%20for%20fast%20and%20accurate%20predictions.%20Motivated%20by%20the%0Arapid%20progress%20in%20Visual%20Foundation%20Models%20%28VFMs%29%20for%20captioning%2C%20zero-shot%0Arecognition%2C%20and%20multimodal%20tasks%2C%20we%20investigate%20whether%20SAM2%2C%20the%20current%0Astate-of-the-art%20VFM%20for%20segmentation%20tasks%2C%20can%20serve%20as%20a%20strong%20backbone%20for%0ALiDAR%20point%20cloud%20segmentation%20in%20the%20range%20view.%20We%20present%20%2C%20to%20our%0Aknowledge%2C%20the%20first%20range-view%20framework%20that%20adapts%20SAM2%20to%203D%20segmentation%2C%0Acoupling%20efficient%202D%20feature%20extraction%20with%20standard%0Aprojection/back-projection%20to%20operate%20on%20point%20clouds.%20To%20optimize%20SAM2%20for%0Arange-view%20representations%2C%20we%20implement%20several%20architectural%20modifications%20to%0Athe%20encoder%3A%20%281%29%20a%20novel%20module%20that%20emphasizes%20horizontal%20spatial%20dependencies%0Ainherent%20in%20LiDAR%20range%20images%2C%20%282%29%20a%20customized%20configuration%20of%20tailored%20to%0Athe%20geometric%20properties%20of%20spherical%20projections%2C%20and%20%283%29%20an%20adapted%20mechanism%0Ain%20the%20encoder%20backbone%20specifically%20designed%20to%20capture%20the%20unique%20spatial%0Apatterns%20and%20discontinuities%20present%20in%20range-view%20pseudo-images.%20Our%20approach%0Aachieves%20competitive%20performance%20on%20SemanticKITTI%20while%20benefiting%20from%20the%0Aspeed%2C%20scalability%2C%20and%20deployment%20simplicity%20of%202D-centric%20pipelines.%20This%0Awork%20highlights%20the%20viability%20of%20VFMs%20as%20general-purpose%20backbones%20for%203D%0Aperception%20and%20opens%20a%20path%20toward%20unified%2C%20foundation-model-driven%20LiDAR%0Asegmentation.%20Results%20lets%20us%20conclude%20that%20range-view%20segmentation%20methods%0Ausing%20VFMs%20leads%20to%20promising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRangeSAM%253A%2520Leveraging%2520Visual%2520Foundation%2520Models%2520for%2520Range-View%2520repesented%250A%2520%2520LiDAR%2520segmentation%26entry.906535625%3DPaul%2520Julius%2520K%25C3%25BChn%2520and%2520Duc%2520Anh%2520Nguyen%2520and%2520Arjan%2520Kuijper%2520and%2520Holger%2520Graf%2520and%2520Dieter%2520Fellner%2520and%2520Saptarshi%2520Neil%2520Sinha%26entry.1292438233%3D%2520%2520Point%2520cloud%2520segmentation%2520is%2520central%2520to%2520autonomous%2520driving%2520and%25203D%2520scene%250Aunderstanding.%2520While%2520voxel-%2520and%2520point-based%2520methods%2520dominate%2520recent%2520research%250Adue%2520to%2520their%2520compatibility%2520with%2520deep%2520architectures%2520and%2520ability%2520to%2520capture%250Afine-grained%2520geometry%252C%2520they%2520often%2520incur%2520high%2520computational%2520cost%252C%2520irregular%250Amemory%2520access%252C%2520and%2520limited%2520real-time%2520efficiency.%2520In%2520contrast%252C%2520range-view%250Amethods%252C%2520though%2520relatively%2520underexplored%2520-%2520can%2520leverage%2520mature%25202D%2520semantic%250Asegmentation%2520techniques%2520for%2520fast%2520and%2520accurate%2520predictions.%2520Motivated%2520by%2520the%250Arapid%2520progress%2520in%2520Visual%2520Foundation%2520Models%2520%2528VFMs%2529%2520for%2520captioning%252C%2520zero-shot%250Arecognition%252C%2520and%2520multimodal%2520tasks%252C%2520we%2520investigate%2520whether%2520SAM2%252C%2520the%2520current%250Astate-of-the-art%2520VFM%2520for%2520segmentation%2520tasks%252C%2520can%2520serve%2520as%2520a%2520strong%2520backbone%2520for%250ALiDAR%2520point%2520cloud%2520segmentation%2520in%2520the%2520range%2520view.%2520We%2520present%2520%252C%2520to%2520our%250Aknowledge%252C%2520the%2520first%2520range-view%2520framework%2520that%2520adapts%2520SAM2%2520to%25203D%2520segmentation%252C%250Acoupling%2520efficient%25202D%2520feature%2520extraction%2520with%2520standard%250Aprojection/back-projection%2520to%2520operate%2520on%2520point%2520clouds.%2520To%2520optimize%2520SAM2%2520for%250Arange-view%2520representations%252C%2520we%2520implement%2520several%2520architectural%2520modifications%2520to%250Athe%2520encoder%253A%2520%25281%2529%2520a%2520novel%2520module%2520that%2520emphasizes%2520horizontal%2520spatial%2520dependencies%250Ainherent%2520in%2520LiDAR%2520range%2520images%252C%2520%25282%2529%2520a%2520customized%2520configuration%2520of%2520tailored%2520to%250Athe%2520geometric%2520properties%2520of%2520spherical%2520projections%252C%2520and%2520%25283%2529%2520an%2520adapted%2520mechanism%250Ain%2520the%2520encoder%2520backbone%2520specifically%2520designed%2520to%2520capture%2520the%2520unique%2520spatial%250Apatterns%2520and%2520discontinuities%2520present%2520in%2520range-view%2520pseudo-images.%2520Our%2520approach%250Aachieves%2520competitive%2520performance%2520on%2520SemanticKITTI%2520while%2520benefiting%2520from%2520the%250Aspeed%252C%2520scalability%252C%2520and%2520deployment%2520simplicity%2520of%25202D-centric%2520pipelines.%2520This%250Awork%2520highlights%2520the%2520viability%2520of%2520VFMs%2520as%2520general-purpose%2520backbones%2520for%25203D%250Aperception%2520and%2520opens%2520a%2520path%2520toward%2520unified%252C%2520foundation-model-driven%2520LiDAR%250Asegmentation.%2520Results%2520lets%2520us%2520conclude%2520that%2520range-view%2520segmentation%2520methods%250Ausing%2520VFMs%2520leads%2520to%2520promising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RangeSAM%3A%20Leveraging%20Visual%20Foundation%20Models%20for%20Range-View%20repesented%0A%20%20LiDAR%20segmentation&entry.906535625=Paul%20Julius%20K%C3%BChn%20and%20Duc%20Anh%20Nguyen%20and%20Arjan%20Kuijper%20and%20Holger%20Graf%20and%20Dieter%20Fellner%20and%20Saptarshi%20Neil%20Sinha&entry.1292438233=%20%20Point%20cloud%20segmentation%20is%20central%20to%20autonomous%20driving%20and%203D%20scene%0Aunderstanding.%20While%20voxel-%20and%20point-based%20methods%20dominate%20recent%20research%0Adue%20to%20their%20compatibility%20with%20deep%20architectures%20and%20ability%20to%20capture%0Afine-grained%20geometry%2C%20they%20often%20incur%20high%20computational%20cost%2C%20irregular%0Amemory%20access%2C%20and%20limited%20real-time%20efficiency.%20In%20contrast%2C%20range-view%0Amethods%2C%20though%20relatively%20underexplored%20-%20can%20leverage%20mature%202D%20semantic%0Asegmentation%20techniques%20for%20fast%20and%20accurate%20predictions.%20Motivated%20by%20the%0Arapid%20progress%20in%20Visual%20Foundation%20Models%20%28VFMs%29%20for%20captioning%2C%20zero-shot%0Arecognition%2C%20and%20multimodal%20tasks%2C%20we%20investigate%20whether%20SAM2%2C%20the%20current%0Astate-of-the-art%20VFM%20for%20segmentation%20tasks%2C%20can%20serve%20as%20a%20strong%20backbone%20for%0ALiDAR%20point%20cloud%20segmentation%20in%20the%20range%20view.%20We%20present%20%2C%20to%20our%0Aknowledge%2C%20the%20first%20range-view%20framework%20that%20adapts%20SAM2%20to%203D%20segmentation%2C%0Acoupling%20efficient%202D%20feature%20extraction%20with%20standard%0Aprojection/back-projection%20to%20operate%20on%20point%20clouds.%20To%20optimize%20SAM2%20for%0Arange-view%20representations%2C%20we%20implement%20several%20architectural%20modifications%20to%0Athe%20encoder%3A%20%281%29%20a%20novel%20module%20that%20emphasizes%20horizontal%20spatial%20dependencies%0Ainherent%20in%20LiDAR%20range%20images%2C%20%282%29%20a%20customized%20configuration%20of%20tailored%20to%0Athe%20geometric%20properties%20of%20spherical%20projections%2C%20and%20%283%29%20an%20adapted%20mechanism%0Ain%20the%20encoder%20backbone%20specifically%20designed%20to%20capture%20the%20unique%20spatial%0Apatterns%20and%20discontinuities%20present%20in%20range-view%20pseudo-images.%20Our%20approach%0Aachieves%20competitive%20performance%20on%20SemanticKITTI%20while%20benefiting%20from%20the%0Aspeed%2C%20scalability%2C%20and%20deployment%20simplicity%20of%202D-centric%20pipelines.%20This%0Awork%20highlights%20the%20viability%20of%20VFMs%20as%20general-purpose%20backbones%20for%203D%0Aperception%20and%20opens%20a%20path%20toward%20unified%2C%20foundation-model-driven%20LiDAR%0Asegmentation.%20Results%20lets%20us%20conclude%20that%20range-view%20segmentation%20methods%0Ausing%20VFMs%20leads%20to%20promising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15886v1&entry.124074799=Read"},
{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language\n  Model", "author": "Pengteng Li and Pinhao Song and Wuyang Li and Weiyu Guo and Huizai Yao and Yijie Xu and Dugang Liu and Hui Xiong", "abstract": "  We introduce SEE&TREK, the first training-free prompting framework tailored\nto enhance the spatial understanding of Multimodal Large Language Models\n(MLLMS) under vision-only constraints. While prior efforts have incorporated\nmodalities like depth or point clouds to improve spatial reasoning, purely\nvisualspatial understanding remains underexplored. SEE&TREK addresses this gap\nby focusing on two core principles: increasing visual diversity and motion\nreconstruction. For visual diversity, we conduct Maximum Semantic Richness\nSampling, which employs an off-the-shell perception model to extract\nsemantically rich keyframes that capture scene structure. For motion\nreconstruction, we simulate visual trajectories and encode relative spatial\npositions into keyframes to preserve both spatial relations and temporal\ncoherence. Our method is training&GPU-free, requiring only a single forward\npass, and can be seamlessly integrated into existing MLLM'S. Extensive\nexperiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently\nboosts various MLLM S performance across diverse spatial reasoning tasks with\nthe most +3.5% improvement, offering a promising path toward stronger spatial\nintelligence.\n", "link": "http://arxiv.org/abs/2509.16087v1", "date": "2025-09-19", "relevancy": 3.0669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6205}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%26Trek%3A%20Training-Free%20Spatial%20Prompting%20for%20Multimodal%20Large%20Language%0A%20%20Model&body=Title%3A%20See%26Trek%3A%20Training-Free%20Spatial%20Prompting%20for%20Multimodal%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Pengteng%20Li%20and%20Pinhao%20Song%20and%20Wuyang%20Li%20and%20Weiyu%20Guo%20and%20Huizai%20Yao%20and%20Yijie%20Xu%20and%20Dugang%20Liu%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20We%20introduce%20SEE%26TREK%2C%20the%20first%20training-free%20prompting%20framework%20tailored%0Ato%20enhance%20the%20spatial%20understanding%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMS%29%20under%20vision-only%20constraints.%20While%20prior%20efforts%20have%20incorporated%0Amodalities%20like%20depth%20or%20point%20clouds%20to%20improve%20spatial%20reasoning%2C%20purely%0Avisualspatial%20understanding%20remains%20underexplored.%20SEE%26TREK%20addresses%20this%20gap%0Aby%20focusing%20on%20two%20core%20principles%3A%20increasing%20visual%20diversity%20and%20motion%0Areconstruction.%20For%20visual%20diversity%2C%20we%20conduct%20Maximum%20Semantic%20Richness%0ASampling%2C%20which%20employs%20an%20off-the-shell%20perception%20model%20to%20extract%0Asemantically%20rich%20keyframes%20that%20capture%20scene%20structure.%20For%20motion%0Areconstruction%2C%20we%20simulate%20visual%20trajectories%20and%20encode%20relative%20spatial%0Apositions%20into%20keyframes%20to%20preserve%20both%20spatial%20relations%20and%20temporal%0Acoherence.%20Our%20method%20is%20training%26GPU-free%2C%20requiring%20only%20a%20single%20forward%0Apass%2C%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20MLLM%27S.%20Extensive%0Aexperiments%20on%20the%20VSI-B%20ENCH%20and%20STI-B%20ENCH%20show%20that%20S%20EE%20%26T%20REK%20consistently%0Aboosts%20various%20MLLM%20S%20performance%20across%20diverse%20spatial%20reasoning%20tasks%20with%0Athe%20most%20%2B3.5%25%20improvement%2C%20offering%20a%20promising%20path%20toward%20stronger%20spatial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2526Trek%253A%2520Training-Free%2520Spatial%2520Prompting%2520for%2520Multimodal%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DPengteng%2520Li%2520and%2520Pinhao%2520Song%2520and%2520Wuyang%2520Li%2520and%2520Weiyu%2520Guo%2520and%2520Huizai%2520Yao%2520and%2520Yijie%2520Xu%2520and%2520Dugang%2520Liu%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520We%2520introduce%2520SEE%2526TREK%252C%2520the%2520first%2520training-free%2520prompting%2520framework%2520tailored%250Ato%2520enhance%2520the%2520spatial%2520understanding%2520of%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMS%2529%2520under%2520vision-only%2520constraints.%2520While%2520prior%2520efforts%2520have%2520incorporated%250Amodalities%2520like%2520depth%2520or%2520point%2520clouds%2520to%2520improve%2520spatial%2520reasoning%252C%2520purely%250Avisualspatial%2520understanding%2520remains%2520underexplored.%2520SEE%2526TREK%2520addresses%2520this%2520gap%250Aby%2520focusing%2520on%2520two%2520core%2520principles%253A%2520increasing%2520visual%2520diversity%2520and%2520motion%250Areconstruction.%2520For%2520visual%2520diversity%252C%2520we%2520conduct%2520Maximum%2520Semantic%2520Richness%250ASampling%252C%2520which%2520employs%2520an%2520off-the-shell%2520perception%2520model%2520to%2520extract%250Asemantically%2520rich%2520keyframes%2520that%2520capture%2520scene%2520structure.%2520For%2520motion%250Areconstruction%252C%2520we%2520simulate%2520visual%2520trajectories%2520and%2520encode%2520relative%2520spatial%250Apositions%2520into%2520keyframes%2520to%2520preserve%2520both%2520spatial%2520relations%2520and%2520temporal%250Acoherence.%2520Our%2520method%2520is%2520training%2526GPU-free%252C%2520requiring%2520only%2520a%2520single%2520forward%250Apass%252C%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520MLLM%2527S.%2520Extensive%250Aexperiments%2520on%2520the%2520VSI-B%2520ENCH%2520and%2520STI-B%2520ENCH%2520show%2520that%2520S%2520EE%2520%2526T%2520REK%2520consistently%250Aboosts%2520various%2520MLLM%2520S%2520performance%2520across%2520diverse%2520spatial%2520reasoning%2520tasks%2520with%250Athe%2520most%2520%252B3.5%2525%2520improvement%252C%2520offering%2520a%2520promising%2520path%2520toward%2520stronger%2520spatial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%26Trek%3A%20Training-Free%20Spatial%20Prompting%20for%20Multimodal%20Large%20Language%0A%20%20Model&entry.906535625=Pengteng%20Li%20and%20Pinhao%20Song%20and%20Wuyang%20Li%20and%20Weiyu%20Guo%20and%20Huizai%20Yao%20and%20Yijie%20Xu%20and%20Dugang%20Liu%20and%20Hui%20Xiong&entry.1292438233=%20%20We%20introduce%20SEE%26TREK%2C%20the%20first%20training-free%20prompting%20framework%20tailored%0Ato%20enhance%20the%20spatial%20understanding%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMS%29%20under%20vision-only%20constraints.%20While%20prior%20efforts%20have%20incorporated%0Amodalities%20like%20depth%20or%20point%20clouds%20to%20improve%20spatial%20reasoning%2C%20purely%0Avisualspatial%20understanding%20remains%20underexplored.%20SEE%26TREK%20addresses%20this%20gap%0Aby%20focusing%20on%20two%20core%20principles%3A%20increasing%20visual%20diversity%20and%20motion%0Areconstruction.%20For%20visual%20diversity%2C%20we%20conduct%20Maximum%20Semantic%20Richness%0ASampling%2C%20which%20employs%20an%20off-the-shell%20perception%20model%20to%20extract%0Asemantically%20rich%20keyframes%20that%20capture%20scene%20structure.%20For%20motion%0Areconstruction%2C%20we%20simulate%20visual%20trajectories%20and%20encode%20relative%20spatial%0Apositions%20into%20keyframes%20to%20preserve%20both%20spatial%20relations%20and%20temporal%0Acoherence.%20Our%20method%20is%20training%26GPU-free%2C%20requiring%20only%20a%20single%20forward%0Apass%2C%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20MLLM%27S.%20Extensive%0Aexperiments%20on%20the%20VSI-B%20ENCH%20and%20STI-B%20ENCH%20show%20that%20S%20EE%20%26T%20REK%20consistently%0Aboosts%20various%20MLLM%20S%20performance%20across%20diverse%20spatial%20reasoning%20tasks%20with%0Athe%20most%20%2B3.5%25%20improvement%2C%20offering%20a%20promising%20path%20toward%20stronger%20spatial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16087v1&entry.124074799=Read"},
{"title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels", "author": "Carter Sifferman and Yiquan Li and Yiming Li and Fangzhou Mu and Michael Gleicher and Mohit Gupta and Yin Li", "abstract": "  We aim to recover the geometry of 3D parametric scenes using very few depth\nmeasurements from low-cost, commercially available time-of-flight sensors.\nThese sensors offer very low spatial resolution (i.e., a single pixel), but\nimage a wide field-of-view per pixel and capture detailed time-of-flight data\nin the form of time-resolved photon counts. This time-of-flight data encodes\nrich scene information and thus enables recovery of simple scenes from sparse\nmeasurements. We investigate the feasibility of using a distributed set of few\nmeasurements (e.g., as few as 15 pixels) to recover the geometry of simple\nparametric scenes with a strong prior, such as estimating the 6D pose of a\nknown object. To achieve this, we design a method that utilizes both\nfeed-forward prediction to infer scene parameters, and differentiable rendering\nwithin an analysis-by-synthesis framework to refine the scene parameter\nestimate. We develop hardware prototypes and demonstrate that our method\neffectively recovers object pose given an untextured 3D model in both\nsimulations and controlled real-world captures, and show promising initial\nresults for other parametric scenes. We additionally conduct experiments to\nexplore the limits and capabilities of our imaging solution.\n", "link": "http://arxiv.org/abs/2509.16132v1", "date": "2025-09-19", "relevancy": 3.0212, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6082}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6082}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recovering%20Parametric%20Scenes%20from%20Very%20Few%20Time-of-Flight%20Pixels&body=Title%3A%20Recovering%20Parametric%20Scenes%20from%20Very%20Few%20Time-of-Flight%20Pixels%0AAuthor%3A%20Carter%20Sifferman%20and%20Yiquan%20Li%20and%20Yiming%20Li%20and%20Fangzhou%20Mu%20and%20Michael%20Gleicher%20and%20Mohit%20Gupta%20and%20Yin%20Li%0AAbstract%3A%20%20%20We%20aim%20to%20recover%20the%20geometry%20of%203D%20parametric%20scenes%20using%20very%20few%20depth%0Ameasurements%20from%20low-cost%2C%20commercially%20available%20time-of-flight%20sensors.%0AThese%20sensors%20offer%20very%20low%20spatial%20resolution%20%28i.e.%2C%20a%20single%20pixel%29%2C%20but%0Aimage%20a%20wide%20field-of-view%20per%20pixel%20and%20capture%20detailed%20time-of-flight%20data%0Ain%20the%20form%20of%20time-resolved%20photon%20counts.%20This%20time-of-flight%20data%20encodes%0Arich%20scene%20information%20and%20thus%20enables%20recovery%20of%20simple%20scenes%20from%20sparse%0Ameasurements.%20We%20investigate%20the%20feasibility%20of%20using%20a%20distributed%20set%20of%20few%0Ameasurements%20%28e.g.%2C%20as%20few%20as%2015%20pixels%29%20to%20recover%20the%20geometry%20of%20simple%0Aparametric%20scenes%20with%20a%20strong%20prior%2C%20such%20as%20estimating%20the%206D%20pose%20of%20a%0Aknown%20object.%20To%20achieve%20this%2C%20we%20design%20a%20method%20that%20utilizes%20both%0Afeed-forward%20prediction%20to%20infer%20scene%20parameters%2C%20and%20differentiable%20rendering%0Awithin%20an%20analysis-by-synthesis%20framework%20to%20refine%20the%20scene%20parameter%0Aestimate.%20We%20develop%20hardware%20prototypes%20and%20demonstrate%20that%20our%20method%0Aeffectively%20recovers%20object%20pose%20given%20an%20untextured%203D%20model%20in%20both%0Asimulations%20and%20controlled%20real-world%20captures%2C%20and%20show%20promising%20initial%0Aresults%20for%20other%20parametric%20scenes.%20We%20additionally%20conduct%20experiments%20to%0Aexplore%20the%20limits%20and%20capabilities%20of%20our%20imaging%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecovering%2520Parametric%2520Scenes%2520from%2520Very%2520Few%2520Time-of-Flight%2520Pixels%26entry.906535625%3DCarter%2520Sifferman%2520and%2520Yiquan%2520Li%2520and%2520Yiming%2520Li%2520and%2520Fangzhou%2520Mu%2520and%2520Michael%2520Gleicher%2520and%2520Mohit%2520Gupta%2520and%2520Yin%2520Li%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520recover%2520the%2520geometry%2520of%25203D%2520parametric%2520scenes%2520using%2520very%2520few%2520depth%250Ameasurements%2520from%2520low-cost%252C%2520commercially%2520available%2520time-of-flight%2520sensors.%250AThese%2520sensors%2520offer%2520very%2520low%2520spatial%2520resolution%2520%2528i.e.%252C%2520a%2520single%2520pixel%2529%252C%2520but%250Aimage%2520a%2520wide%2520field-of-view%2520per%2520pixel%2520and%2520capture%2520detailed%2520time-of-flight%2520data%250Ain%2520the%2520form%2520of%2520time-resolved%2520photon%2520counts.%2520This%2520time-of-flight%2520data%2520encodes%250Arich%2520scene%2520information%2520and%2520thus%2520enables%2520recovery%2520of%2520simple%2520scenes%2520from%2520sparse%250Ameasurements.%2520We%2520investigate%2520the%2520feasibility%2520of%2520using%2520a%2520distributed%2520set%2520of%2520few%250Ameasurements%2520%2528e.g.%252C%2520as%2520few%2520as%252015%2520pixels%2529%2520to%2520recover%2520the%2520geometry%2520of%2520simple%250Aparametric%2520scenes%2520with%2520a%2520strong%2520prior%252C%2520such%2520as%2520estimating%2520the%25206D%2520pose%2520of%2520a%250Aknown%2520object.%2520To%2520achieve%2520this%252C%2520we%2520design%2520a%2520method%2520that%2520utilizes%2520both%250Afeed-forward%2520prediction%2520to%2520infer%2520scene%2520parameters%252C%2520and%2520differentiable%2520rendering%250Awithin%2520an%2520analysis-by-synthesis%2520framework%2520to%2520refine%2520the%2520scene%2520parameter%250Aestimate.%2520We%2520develop%2520hardware%2520prototypes%2520and%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520recovers%2520object%2520pose%2520given%2520an%2520untextured%25203D%2520model%2520in%2520both%250Asimulations%2520and%2520controlled%2520real-world%2520captures%252C%2520and%2520show%2520promising%2520initial%250Aresults%2520for%2520other%2520parametric%2520scenes.%2520We%2520additionally%2520conduct%2520experiments%2520to%250Aexplore%2520the%2520limits%2520and%2520capabilities%2520of%2520our%2520imaging%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recovering%20Parametric%20Scenes%20from%20Very%20Few%20Time-of-Flight%20Pixels&entry.906535625=Carter%20Sifferman%20and%20Yiquan%20Li%20and%20Yiming%20Li%20and%20Fangzhou%20Mu%20and%20Michael%20Gleicher%20and%20Mohit%20Gupta%20and%20Yin%20Li&entry.1292438233=%20%20We%20aim%20to%20recover%20the%20geometry%20of%203D%20parametric%20scenes%20using%20very%20few%20depth%0Ameasurements%20from%20low-cost%2C%20commercially%20available%20time-of-flight%20sensors.%0AThese%20sensors%20offer%20very%20low%20spatial%20resolution%20%28i.e.%2C%20a%20single%20pixel%29%2C%20but%0Aimage%20a%20wide%20field-of-view%20per%20pixel%20and%20capture%20detailed%20time-of-flight%20data%0Ain%20the%20form%20of%20time-resolved%20photon%20counts.%20This%20time-of-flight%20data%20encodes%0Arich%20scene%20information%20and%20thus%20enables%20recovery%20of%20simple%20scenes%20from%20sparse%0Ameasurements.%20We%20investigate%20the%20feasibility%20of%20using%20a%20distributed%20set%20of%20few%0Ameasurements%20%28e.g.%2C%20as%20few%20as%2015%20pixels%29%20to%20recover%20the%20geometry%20of%20simple%0Aparametric%20scenes%20with%20a%20strong%20prior%2C%20such%20as%20estimating%20the%206D%20pose%20of%20a%0Aknown%20object.%20To%20achieve%20this%2C%20we%20design%20a%20method%20that%20utilizes%20both%0Afeed-forward%20prediction%20to%20infer%20scene%20parameters%2C%20and%20differentiable%20rendering%0Awithin%20an%20analysis-by-synthesis%20framework%20to%20refine%20the%20scene%20parameter%0Aestimate.%20We%20develop%20hardware%20prototypes%20and%20demonstrate%20that%20our%20method%0Aeffectively%20recovers%20object%20pose%20given%20an%20untextured%203D%20model%20in%20both%0Asimulations%20and%20controlled%20real-world%20captures%2C%20and%20show%20promising%20initial%0Aresults%20for%20other%20parametric%20scenes.%20We%20additionally%20conduct%20experiments%20to%0Aexplore%20the%20limits%20and%20capabilities%20of%20our%20imaging%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16132v1&entry.124074799=Read"},
{"title": "SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and\n  Object-Level 2D Features", "author": "Jinyuan Qu and Hongyang Li and Xingyu Chen and Shilong Liu and Yukai Shi and Tianhe Ren and Ruitao Jing and Lei Zhang", "abstract": "  In this paper, we present SegDINO3D, a novel Transformer encoder-decoder\nframework for 3D instance segmentation. As 3D training data is generally not as\nsufficient as 2D training images, SegDINO3D is designed to fully leverage 2D\nrepresentation from a pre-trained 2D detection model, including both\nimage-level and object-level features, for improving 3D representation.\nSegDINO3D takes both a point cloud and its associated 2D images as input. In\nthe encoder stage, it first enriches each 3D point by retrieving 2D image\nfeatures from its corresponding image views and then leverages a 3D encoder for\n3D context fusion. In the decoder stage, it formulates 3D object queries as 3D\nanchor boxes and performs cross-attention from 3D queries to 2D object queries\nobtained from 2D images using the 2D detection model. These 2D object queries\nserve as a compact object-level representation of 2D images, effectively\navoiding the challenge of keeping thousands of image feature maps in the memory\nwhile faithfully preserving the knowledge of the pre-trained 2D model. The\nintroducing of 3D box queries also enables the model to modulate\ncross-attention using the predicted boxes for more precise querying. SegDINO3D\nachieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D\ninstance segmentation benchmarks. Notably, on the challenging ScanNet200\ndataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP\non the validation and hidden test sets, respectively, demonstrating its\nsuperiority.\n", "link": "http://arxiv.org/abs/2509.16098v1", "date": "2025-09-19", "relevancy": 3.0195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%0A%20%20Object-Level%202D%20Features&body=Title%3A%20SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%0A%20%20Object-Level%202D%20Features%0AAuthor%3A%20Jinyuan%20Qu%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Shilong%20Liu%20and%20Yukai%20Shi%20and%20Tianhe%20Ren%20and%20Ruitao%20Jing%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20SegDINO3D%2C%20a%20novel%20Transformer%20encoder-decoder%0Aframework%20for%203D%20instance%20segmentation.%20As%203D%20training%20data%20is%20generally%20not%20as%0Asufficient%20as%202D%20training%20images%2C%20SegDINO3D%20is%20designed%20to%20fully%20leverage%202D%0Arepresentation%20from%20a%20pre-trained%202D%20detection%20model%2C%20including%20both%0Aimage-level%20and%20object-level%20features%2C%20for%20improving%203D%20representation.%0ASegDINO3D%20takes%20both%20a%20point%20cloud%20and%20its%20associated%202D%20images%20as%20input.%20In%0Athe%20encoder%20stage%2C%20it%20first%20enriches%20each%203D%20point%20by%20retrieving%202D%20image%0Afeatures%20from%20its%20corresponding%20image%20views%20and%20then%20leverages%20a%203D%20encoder%20for%0A3D%20context%20fusion.%20In%20the%20decoder%20stage%2C%20it%20formulates%203D%20object%20queries%20as%203D%0Aanchor%20boxes%20and%20performs%20cross-attention%20from%203D%20queries%20to%202D%20object%20queries%0Aobtained%20from%202D%20images%20using%20the%202D%20detection%20model.%20These%202D%20object%20queries%0Aserve%20as%20a%20compact%20object-level%20representation%20of%202D%20images%2C%20effectively%0Aavoiding%20the%20challenge%20of%20keeping%20thousands%20of%20image%20feature%20maps%20in%20the%20memory%0Awhile%20faithfully%20preserving%20the%20knowledge%20of%20the%20pre-trained%202D%20model.%20The%0Aintroducing%20of%203D%20box%20queries%20also%20enables%20the%20model%20to%20modulate%0Across-attention%20using%20the%20predicted%20boxes%20for%20more%20precise%20querying.%20SegDINO3D%0Aachieves%20the%20state-of-the-art%20performance%20on%20the%20ScanNetV2%20and%20ScanNet200%203D%0Ainstance%20segmentation%20benchmarks.%20Notably%2C%20on%20the%20challenging%20ScanNet200%0Adataset%2C%20SegDINO3D%20significantly%20outperforms%20prior%20methods%20by%20%2B8.7%20and%20%2B6.8%20mAP%0Aon%20the%20validation%20and%20hidden%20test%20sets%2C%20respectively%2C%20demonstrating%20its%0Asuperiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegDINO3D%253A%25203D%2520Instance%2520Segmentation%2520Empowered%2520by%2520Both%2520Image-Level%2520and%250A%2520%2520Object-Level%25202D%2520Features%26entry.906535625%3DJinyuan%2520Qu%2520and%2520Hongyang%2520Li%2520and%2520Xingyu%2520Chen%2520and%2520Shilong%2520Liu%2520and%2520Yukai%2520Shi%2520and%2520Tianhe%2520Ren%2520and%2520Ruitao%2520Jing%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520SegDINO3D%252C%2520a%2520novel%2520Transformer%2520encoder-decoder%250Aframework%2520for%25203D%2520instance%2520segmentation.%2520As%25203D%2520training%2520data%2520is%2520generally%2520not%2520as%250Asufficient%2520as%25202D%2520training%2520images%252C%2520SegDINO3D%2520is%2520designed%2520to%2520fully%2520leverage%25202D%250Arepresentation%2520from%2520a%2520pre-trained%25202D%2520detection%2520model%252C%2520including%2520both%250Aimage-level%2520and%2520object-level%2520features%252C%2520for%2520improving%25203D%2520representation.%250ASegDINO3D%2520takes%2520both%2520a%2520point%2520cloud%2520and%2520its%2520associated%25202D%2520images%2520as%2520input.%2520In%250Athe%2520encoder%2520stage%252C%2520it%2520first%2520enriches%2520each%25203D%2520point%2520by%2520retrieving%25202D%2520image%250Afeatures%2520from%2520its%2520corresponding%2520image%2520views%2520and%2520then%2520leverages%2520a%25203D%2520encoder%2520for%250A3D%2520context%2520fusion.%2520In%2520the%2520decoder%2520stage%252C%2520it%2520formulates%25203D%2520object%2520queries%2520as%25203D%250Aanchor%2520boxes%2520and%2520performs%2520cross-attention%2520from%25203D%2520queries%2520to%25202D%2520object%2520queries%250Aobtained%2520from%25202D%2520images%2520using%2520the%25202D%2520detection%2520model.%2520These%25202D%2520object%2520queries%250Aserve%2520as%2520a%2520compact%2520object-level%2520representation%2520of%25202D%2520images%252C%2520effectively%250Aavoiding%2520the%2520challenge%2520of%2520keeping%2520thousands%2520of%2520image%2520feature%2520maps%2520in%2520the%2520memory%250Awhile%2520faithfully%2520preserving%2520the%2520knowledge%2520of%2520the%2520pre-trained%25202D%2520model.%2520The%250Aintroducing%2520of%25203D%2520box%2520queries%2520also%2520enables%2520the%2520model%2520to%2520modulate%250Across-attention%2520using%2520the%2520predicted%2520boxes%2520for%2520more%2520precise%2520querying.%2520SegDINO3D%250Aachieves%2520the%2520state-of-the-art%2520performance%2520on%2520the%2520ScanNetV2%2520and%2520ScanNet200%25203D%250Ainstance%2520segmentation%2520benchmarks.%2520Notably%252C%2520on%2520the%2520challenging%2520ScanNet200%250Adataset%252C%2520SegDINO3D%2520significantly%2520outperforms%2520prior%2520methods%2520by%2520%252B8.7%2520and%2520%252B6.8%2520mAP%250Aon%2520the%2520validation%2520and%2520hidden%2520test%2520sets%252C%2520respectively%252C%2520demonstrating%2520its%250Asuperiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%0A%20%20Object-Level%202D%20Features&entry.906535625=Jinyuan%20Qu%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Shilong%20Liu%20and%20Yukai%20Shi%20and%20Tianhe%20Ren%20and%20Ruitao%20Jing%20and%20Lei%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20SegDINO3D%2C%20a%20novel%20Transformer%20encoder-decoder%0Aframework%20for%203D%20instance%20segmentation.%20As%203D%20training%20data%20is%20generally%20not%20as%0Asufficient%20as%202D%20training%20images%2C%20SegDINO3D%20is%20designed%20to%20fully%20leverage%202D%0Arepresentation%20from%20a%20pre-trained%202D%20detection%20model%2C%20including%20both%0Aimage-level%20and%20object-level%20features%2C%20for%20improving%203D%20representation.%0ASegDINO3D%20takes%20both%20a%20point%20cloud%20and%20its%20associated%202D%20images%20as%20input.%20In%0Athe%20encoder%20stage%2C%20it%20first%20enriches%20each%203D%20point%20by%20retrieving%202D%20image%0Afeatures%20from%20its%20corresponding%20image%20views%20and%20then%20leverages%20a%203D%20encoder%20for%0A3D%20context%20fusion.%20In%20the%20decoder%20stage%2C%20it%20formulates%203D%20object%20queries%20as%203D%0Aanchor%20boxes%20and%20performs%20cross-attention%20from%203D%20queries%20to%202D%20object%20queries%0Aobtained%20from%202D%20images%20using%20the%202D%20detection%20model.%20These%202D%20object%20queries%0Aserve%20as%20a%20compact%20object-level%20representation%20of%202D%20images%2C%20effectively%0Aavoiding%20the%20challenge%20of%20keeping%20thousands%20of%20image%20feature%20maps%20in%20the%20memory%0Awhile%20faithfully%20preserving%20the%20knowledge%20of%20the%20pre-trained%202D%20model.%20The%0Aintroducing%20of%203D%20box%20queries%20also%20enables%20the%20model%20to%20modulate%0Across-attention%20using%20the%20predicted%20boxes%20for%20more%20precise%20querying.%20SegDINO3D%0Aachieves%20the%20state-of-the-art%20performance%20on%20the%20ScanNetV2%20and%20ScanNet200%203D%0Ainstance%20segmentation%20benchmarks.%20Notably%2C%20on%20the%20challenging%20ScanNet200%0Adataset%2C%20SegDINO3D%20significantly%20outperforms%20prior%20methods%20by%20%2B8.7%20and%20%2B6.8%20mAP%0Aon%20the%20validation%20and%20hidden%20test%20sets%2C%20respectively%2C%20demonstrating%20its%0Asuperiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16098v1&entry.124074799=Read"},
{"title": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision", "author": "Xiwei Liu and Yulong Li and Yichen Li and Xinlin Zhuang and Haolin Yang and Huifa Li and Imran Razzak", "abstract": "  Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning.\n", "link": "http://arxiv.org/abs/2509.16011v1", "date": "2025-09-19", "relevancy": 3.0092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Visual%20Continual%20Learning%20with%20Multi-Prototype%0A%20%20Supervision&body=Title%3A%20Towards%20Robust%20Visual%20Continual%20Learning%20with%20Multi-Prototype%0A%20%20Supervision%0AAuthor%3A%20Xiwei%20Liu%20and%20Yulong%20Li%20and%20Yichen%20Li%20and%20Xinlin%20Zhuang%20and%20Haolin%20Yang%20and%20Huifa%20Li%20and%20Imran%20Razzak%0AAbstract%3A%20%20%20Language-guided%20supervision%2C%20which%20utilizes%20a%20frozen%20semantic%20target%20from%20a%0APretrained%20Language%20Model%20%28PLM%29%2C%20has%20emerged%20as%20a%20promising%20paradigm%20for%20visual%0AContinual%20Learning%20%28CL%29.%20However%2C%20relying%20on%20a%20single%20target%20introduces%20two%0Acritical%20limitations%3A%201%29%20semantic%20ambiguity%2C%20where%20a%20polysemous%20category%20name%0Aresults%20in%20conflicting%20visual%20representations%2C%20and%202%29%20intra-class%20visual%0Adiversity%2C%20where%20a%20single%20prototype%20fails%20to%20capture%20the%20rich%20variety%20of%20visual%0Aappearances%20within%20a%20class.%20To%20this%20end%2C%20we%20propose%20MuproCL%2C%20a%20novel%20framework%0Athat%20replaces%20the%20single%20target%20with%20multiple%2C%20context-aware%20prototypes.%0ASpecifically%2C%20we%20employ%20a%20lightweight%20LLM%20agent%20to%20perform%20category%0Adisambiguation%20and%20visual-modal%20expansion%20to%20generate%20a%20robust%20set%20of%20semantic%0Aprototypes.%20A%20LogSumExp%20aggregation%20mechanism%20allows%20the%20vision%20model%20to%0Aadaptively%20align%20with%20the%20most%20relevant%20prototype%20for%20a%20given%20image.%20Extensive%0Aexperiments%20across%20various%20CL%20baselines%20demonstrate%20that%20MuproCL%20consistently%0Aenhances%20performance%20and%20robustness%2C%20establishing%20a%20more%20effective%20path%20for%0Alanguage-guided%20continual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Visual%2520Continual%2520Learning%2520with%2520Multi-Prototype%250A%2520%2520Supervision%26entry.906535625%3DXiwei%2520Liu%2520and%2520Yulong%2520Li%2520and%2520Yichen%2520Li%2520and%2520Xinlin%2520Zhuang%2520and%2520Haolin%2520Yang%2520and%2520Huifa%2520Li%2520and%2520Imran%2520Razzak%26entry.1292438233%3D%2520%2520Language-guided%2520supervision%252C%2520which%2520utilizes%2520a%2520frozen%2520semantic%2520target%2520from%2520a%250APretrained%2520Language%2520Model%2520%2528PLM%2529%252C%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520visual%250AContinual%2520Learning%2520%2528CL%2529.%2520However%252C%2520relying%2520on%2520a%2520single%2520target%2520introduces%2520two%250Acritical%2520limitations%253A%25201%2529%2520semantic%2520ambiguity%252C%2520where%2520a%2520polysemous%2520category%2520name%250Aresults%2520in%2520conflicting%2520visual%2520representations%252C%2520and%25202%2529%2520intra-class%2520visual%250Adiversity%252C%2520where%2520a%2520single%2520prototype%2520fails%2520to%2520capture%2520the%2520rich%2520variety%2520of%2520visual%250Aappearances%2520within%2520a%2520class.%2520To%2520this%2520end%252C%2520we%2520propose%2520MuproCL%252C%2520a%2520novel%2520framework%250Athat%2520replaces%2520the%2520single%2520target%2520with%2520multiple%252C%2520context-aware%2520prototypes.%250ASpecifically%252C%2520we%2520employ%2520a%2520lightweight%2520LLM%2520agent%2520to%2520perform%2520category%250Adisambiguation%2520and%2520visual-modal%2520expansion%2520to%2520generate%2520a%2520robust%2520set%2520of%2520semantic%250Aprototypes.%2520A%2520LogSumExp%2520aggregation%2520mechanism%2520allows%2520the%2520vision%2520model%2520to%250Aadaptively%2520align%2520with%2520the%2520most%2520relevant%2520prototype%2520for%2520a%2520given%2520image.%2520Extensive%250Aexperiments%2520across%2520various%2520CL%2520baselines%2520demonstrate%2520that%2520MuproCL%2520consistently%250Aenhances%2520performance%2520and%2520robustness%252C%2520establishing%2520a%2520more%2520effective%2520path%2520for%250Alanguage-guided%2520continual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Visual%20Continual%20Learning%20with%20Multi-Prototype%0A%20%20Supervision&entry.906535625=Xiwei%20Liu%20and%20Yulong%20Li%20and%20Yichen%20Li%20and%20Xinlin%20Zhuang%20and%20Haolin%20Yang%20and%20Huifa%20Li%20and%20Imran%20Razzak&entry.1292438233=%20%20Language-guided%20supervision%2C%20which%20utilizes%20a%20frozen%20semantic%20target%20from%20a%0APretrained%20Language%20Model%20%28PLM%29%2C%20has%20emerged%20as%20a%20promising%20paradigm%20for%20visual%0AContinual%20Learning%20%28CL%29.%20However%2C%20relying%20on%20a%20single%20target%20introduces%20two%0Acritical%20limitations%3A%201%29%20semantic%20ambiguity%2C%20where%20a%20polysemous%20category%20name%0Aresults%20in%20conflicting%20visual%20representations%2C%20and%202%29%20intra-class%20visual%0Adiversity%2C%20where%20a%20single%20prototype%20fails%20to%20capture%20the%20rich%20variety%20of%20visual%0Aappearances%20within%20a%20class.%20To%20this%20end%2C%20we%20propose%20MuproCL%2C%20a%20novel%20framework%0Athat%20replaces%20the%20single%20target%20with%20multiple%2C%20context-aware%20prototypes.%0ASpecifically%2C%20we%20employ%20a%20lightweight%20LLM%20agent%20to%20perform%20category%0Adisambiguation%20and%20visual-modal%20expansion%20to%20generate%20a%20robust%20set%20of%20semantic%0Aprototypes.%20A%20LogSumExp%20aggregation%20mechanism%20allows%20the%20vision%20model%20to%0Aadaptively%20align%20with%20the%20most%20relevant%20prototype%20for%20a%20given%20image.%20Extensive%0Aexperiments%20across%20various%20CL%20baselines%20demonstrate%20that%20MuproCL%20consistently%0Aenhances%20performance%20and%20robustness%2C%20establishing%20a%20more%20effective%20path%20for%0Alanguage-guided%20continual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16011v1&entry.124074799=Read"},
{"title": "ENSAM: an efficient foundation model for interactive segmentation of 3D\n  medical images", "author": "Elias Stenhede and Agnar Martin Bj\u00f8rnstad and Arian Ranjbar", "abstract": "  We present ENSAM (Equivariant, Normalized, Segment Anything Model), a\nlightweight and promptable model for universal 3D medical image segmentation.\nENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder\nin a U-Net-style architecture, using latent cross-attention, relative\npositional encoding, normalized attention, and the Muon optimizer for training.\nENSAM is designed to achieve good performance under limited data and\ncomputational budgets, and is trained from scratch on under 5,000 volumes from\nmultiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB\nGPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D\nBiomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set\nwith multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of\n2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously\npublished baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),\nsurpassing its performance in final DSC but trailing behind in the other three\nmetrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall\nand best among the approaches not utilizing pretrained weights. Ablation\nstudies confirm that our use of relative positional encodings and the Muon\noptimizer each substantially speed up convergence and improve segmentation\nquality.\n", "link": "http://arxiv.org/abs/2509.15874v1", "date": "2025-09-19", "relevancy": 2.9721, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENSAM%3A%20an%20efficient%20foundation%20model%20for%20interactive%20segmentation%20of%203D%0A%20%20medical%20images&body=Title%3A%20ENSAM%3A%20an%20efficient%20foundation%20model%20for%20interactive%20segmentation%20of%203D%0A%20%20medical%20images%0AAuthor%3A%20Elias%20Stenhede%20and%20Agnar%20Martin%20Bj%C3%B8rnstad%20and%20Arian%20Ranjbar%0AAbstract%3A%20%20%20We%20present%20ENSAM%20%28Equivariant%2C%20Normalized%2C%20Segment%20Anything%20Model%29%2C%20a%0Alightweight%20and%20promptable%20model%20for%20universal%203D%20medical%20image%20segmentation.%0AENSAM%20combines%20a%20SegResNet-based%20encoder%20with%20a%20prompt%20encoder%20and%20mask%20decoder%0Ain%20a%20U-Net-style%20architecture%2C%20using%20latent%20cross-attention%2C%20relative%0Apositional%20encoding%2C%20normalized%20attention%2C%20and%20the%20Muon%20optimizer%20for%20training.%0AENSAM%20is%20designed%20to%20achieve%20good%20performance%20under%20limited%20data%20and%0Acomputational%20budgets%2C%20and%20is%20trained%20from%20scratch%20on%20under%205%2C000%20volumes%20from%0Amultiple%20modalities%20%28CT%2C%20MRI%2C%20PET%2C%20ultrasound%2C%20microscopy%29%20on%20a%20single%2032%20GB%0AGPU%20in%206%20hours.%20As%20part%20of%20the%20CVPR%202025%20Foundation%20Models%20for%20Interactive%203D%0ABiomedical%20Image%20Segmentation%20Challenge%2C%20ENSAM%20was%20evaluated%20on%20hidden%20test%20set%0Awith%20multimodal%203D%20medical%20images%2C%20obtaining%20a%20DSC%20AUC%20of%202.404%2C%20NSD%20AUC%20of%0A2.266%2C%20final%20DSC%20of%200.627%2C%20and%20final%20NSD%20of%200.597%2C%20outperforming%20two%20previously%0Apublished%20baseline%20models%20%28VISTA3D%2C%20SAM-Med3D%29%20and%20matching%20the%20third%20%28SegVol%29%2C%0Asurpassing%20its%20performance%20in%20final%20DSC%20but%20trailing%20behind%20in%20the%20other%20three%0Ametrics.%20In%20the%20coreset%20track%20of%20the%20challenge%2C%20ENSAM%20ranks%205th%20of%2010%20overall%0Aand%20best%20among%20the%20approaches%20not%20utilizing%20pretrained%20weights.%20Ablation%0Astudies%20confirm%20that%20our%20use%20of%20relative%20positional%20encodings%20and%20the%20Muon%0Aoptimizer%20each%20substantially%20speed%20up%20convergence%20and%20improve%20segmentation%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENSAM%253A%2520an%2520efficient%2520foundation%2520model%2520for%2520interactive%2520segmentation%2520of%25203D%250A%2520%2520medical%2520images%26entry.906535625%3DElias%2520Stenhede%2520and%2520Agnar%2520Martin%2520Bj%25C3%25B8rnstad%2520and%2520Arian%2520Ranjbar%26entry.1292438233%3D%2520%2520We%2520present%2520ENSAM%2520%2528Equivariant%252C%2520Normalized%252C%2520Segment%2520Anything%2520Model%2529%252C%2520a%250Alightweight%2520and%2520promptable%2520model%2520for%2520universal%25203D%2520medical%2520image%2520segmentation.%250AENSAM%2520combines%2520a%2520SegResNet-based%2520encoder%2520with%2520a%2520prompt%2520encoder%2520and%2520mask%2520decoder%250Ain%2520a%2520U-Net-style%2520architecture%252C%2520using%2520latent%2520cross-attention%252C%2520relative%250Apositional%2520encoding%252C%2520normalized%2520attention%252C%2520and%2520the%2520Muon%2520optimizer%2520for%2520training.%250AENSAM%2520is%2520designed%2520to%2520achieve%2520good%2520performance%2520under%2520limited%2520data%2520and%250Acomputational%2520budgets%252C%2520and%2520is%2520trained%2520from%2520scratch%2520on%2520under%25205%252C000%2520volumes%2520from%250Amultiple%2520modalities%2520%2528CT%252C%2520MRI%252C%2520PET%252C%2520ultrasound%252C%2520microscopy%2529%2520on%2520a%2520single%252032%2520GB%250AGPU%2520in%25206%2520hours.%2520As%2520part%2520of%2520the%2520CVPR%25202025%2520Foundation%2520Models%2520for%2520Interactive%25203D%250ABiomedical%2520Image%2520Segmentation%2520Challenge%252C%2520ENSAM%2520was%2520evaluated%2520on%2520hidden%2520test%2520set%250Awith%2520multimodal%25203D%2520medical%2520images%252C%2520obtaining%2520a%2520DSC%2520AUC%2520of%25202.404%252C%2520NSD%2520AUC%2520of%250A2.266%252C%2520final%2520DSC%2520of%25200.627%252C%2520and%2520final%2520NSD%2520of%25200.597%252C%2520outperforming%2520two%2520previously%250Apublished%2520baseline%2520models%2520%2528VISTA3D%252C%2520SAM-Med3D%2529%2520and%2520matching%2520the%2520third%2520%2528SegVol%2529%252C%250Asurpassing%2520its%2520performance%2520in%2520final%2520DSC%2520but%2520trailing%2520behind%2520in%2520the%2520other%2520three%250Ametrics.%2520In%2520the%2520coreset%2520track%2520of%2520the%2520challenge%252C%2520ENSAM%2520ranks%25205th%2520of%252010%2520overall%250Aand%2520best%2520among%2520the%2520approaches%2520not%2520utilizing%2520pretrained%2520weights.%2520Ablation%250Astudies%2520confirm%2520that%2520our%2520use%2520of%2520relative%2520positional%2520encodings%2520and%2520the%2520Muon%250Aoptimizer%2520each%2520substantially%2520speed%2520up%2520convergence%2520and%2520improve%2520segmentation%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENSAM%3A%20an%20efficient%20foundation%20model%20for%20interactive%20segmentation%20of%203D%0A%20%20medical%20images&entry.906535625=Elias%20Stenhede%20and%20Agnar%20Martin%20Bj%C3%B8rnstad%20and%20Arian%20Ranjbar&entry.1292438233=%20%20We%20present%20ENSAM%20%28Equivariant%2C%20Normalized%2C%20Segment%20Anything%20Model%29%2C%20a%0Alightweight%20and%20promptable%20model%20for%20universal%203D%20medical%20image%20segmentation.%0AENSAM%20combines%20a%20SegResNet-based%20encoder%20with%20a%20prompt%20encoder%20and%20mask%20decoder%0Ain%20a%20U-Net-style%20architecture%2C%20using%20latent%20cross-attention%2C%20relative%0Apositional%20encoding%2C%20normalized%20attention%2C%20and%20the%20Muon%20optimizer%20for%20training.%0AENSAM%20is%20designed%20to%20achieve%20good%20performance%20under%20limited%20data%20and%0Acomputational%20budgets%2C%20and%20is%20trained%20from%20scratch%20on%20under%205%2C000%20volumes%20from%0Amultiple%20modalities%20%28CT%2C%20MRI%2C%20PET%2C%20ultrasound%2C%20microscopy%29%20on%20a%20single%2032%20GB%0AGPU%20in%206%20hours.%20As%20part%20of%20the%20CVPR%202025%20Foundation%20Models%20for%20Interactive%203D%0ABiomedical%20Image%20Segmentation%20Challenge%2C%20ENSAM%20was%20evaluated%20on%20hidden%20test%20set%0Awith%20multimodal%203D%20medical%20images%2C%20obtaining%20a%20DSC%20AUC%20of%202.404%2C%20NSD%20AUC%20of%0A2.266%2C%20final%20DSC%20of%200.627%2C%20and%20final%20NSD%20of%200.597%2C%20outperforming%20two%20previously%0Apublished%20baseline%20models%20%28VISTA3D%2C%20SAM-Med3D%29%20and%20matching%20the%20third%20%28SegVol%29%2C%0Asurpassing%20its%20performance%20in%20final%20DSC%20but%20trailing%20behind%20in%20the%20other%20three%0Ametrics.%20In%20the%20coreset%20track%20of%20the%20challenge%2C%20ENSAM%20ranks%205th%20of%2010%20overall%0Aand%20best%20among%20the%20approaches%20not%20utilizing%20pretrained%20weights.%20Ablation%0Astudies%20confirm%20that%20our%20use%20of%20relative%20positional%20encodings%20and%20the%20Muon%0Aoptimizer%20each%20substantially%20speed%20up%20convergence%20and%20improve%20segmentation%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15874v1&entry.124074799=Read"},
{"title": "GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for\n  Image Representation and Generation", "author": "Zhengqiang Zhang and Rongyuan Wu and Lingchen Sun and Lei Zhang", "abstract": "  Effective and efficient tokenization plays an important role in image\nrepresentation and generation. Conventional methods, constrained by uniform\n2D/1D grid tokenization, are inflexible to represent regions with varying\nshapes and textures and at different locations, limiting their efficacy of\nfeature representation. In this work, we propose $\\textbf{GPSToken}$, a novel\n$\\textbf{G}$aussian $\\textbf{P}$arameterized $\\textbf{S}$patially-adaptive\n$\\textbf{Token}$ization framework, to achieve non-uniform image tokenization by\nleveraging parametric 2D Gaussians to dynamically model the shape, position,\nand textures of different image regions. We first employ an entropy-driven\nalgorithm to partition the image into texture-homogeneous regions of variable\nsizes. Then, we parameterize each region as a 2D Gaussian (mean for position,\ncovariance for shape) coupled with texture features. A specialized transformer\nis trained to optimize the Gaussian parameters, enabling continuous adaptation\nof position/shape and content-aware feature extraction. During decoding,\nGaussian parameterized tokens are reconstructed into 2D feature maps through a\ndifferentiable splatting-based renderer, bridging our adaptive tokenization\nwith standard decoders for end-to-end training. GPSToken disentangles spatial\nlayout (Gaussian parameters) from texture features to enable efficient\ntwo-stage generation: structural layout synthesis using lightweight networks,\nfollowed by structure-conditioned texture generation. Experiments demonstrate\nthe state-of-the-art performance of GPSToken, which achieves rFID and FID\nscores of 0.65 and 1.50 on image reconstruction and generation tasks using 128\ntokens, respectively. Codes and models of GPSToken can be found at\n$\\href{https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$.\n", "link": "http://arxiv.org/abs/2509.01109v2", "date": "2025-09-19", "relevancy": 2.9487, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6274}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5846}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPSToken%3A%20Gaussian%20Parameterized%20Spatially-adaptive%20Tokenization%20for%0A%20%20Image%20Representation%20and%20Generation&body=Title%3A%20GPSToken%3A%20Gaussian%20Parameterized%20Spatially-adaptive%20Tokenization%20for%0A%20%20Image%20Representation%20and%20Generation%0AAuthor%3A%20Zhengqiang%20Zhang%20and%20Rongyuan%20Wu%20and%20Lingchen%20Sun%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Effective%20and%20efficient%20tokenization%20plays%20an%20important%20role%20in%20image%0Arepresentation%20and%20generation.%20Conventional%20methods%2C%20constrained%20by%20uniform%0A2D/1D%20grid%20tokenization%2C%20are%20inflexible%20to%20represent%20regions%20with%20varying%0Ashapes%20and%20textures%20and%20at%20different%20locations%2C%20limiting%20their%20efficacy%20of%0Afeature%20representation.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextbf%7BGPSToken%7D%24%2C%20a%20novel%0A%24%5Ctextbf%7BG%7D%24aussian%20%24%5Ctextbf%7BP%7D%24arameterized%20%24%5Ctextbf%7BS%7D%24patially-adaptive%0A%24%5Ctextbf%7BToken%7D%24ization%20framework%2C%20to%20achieve%20non-uniform%20image%20tokenization%20by%0Aleveraging%20parametric%202D%20Gaussians%20to%20dynamically%20model%20the%20shape%2C%20position%2C%0Aand%20textures%20of%20different%20image%20regions.%20We%20first%20employ%20an%20entropy-driven%0Aalgorithm%20to%20partition%20the%20image%20into%20texture-homogeneous%20regions%20of%20variable%0Asizes.%20Then%2C%20we%20parameterize%20each%20region%20as%20a%202D%20Gaussian%20%28mean%20for%20position%2C%0Acovariance%20for%20shape%29%20coupled%20with%20texture%20features.%20A%20specialized%20transformer%0Ais%20trained%20to%20optimize%20the%20Gaussian%20parameters%2C%20enabling%20continuous%20adaptation%0Aof%20position/shape%20and%20content-aware%20feature%20extraction.%20During%20decoding%2C%0AGaussian%20parameterized%20tokens%20are%20reconstructed%20into%202D%20feature%20maps%20through%20a%0Adifferentiable%20splatting-based%20renderer%2C%20bridging%20our%20adaptive%20tokenization%0Awith%20standard%20decoders%20for%20end-to-end%20training.%20GPSToken%20disentangles%20spatial%0Alayout%20%28Gaussian%20parameters%29%20from%20texture%20features%20to%20enable%20efficient%0Atwo-stage%20generation%3A%20structural%20layout%20synthesis%20using%20lightweight%20networks%2C%0Afollowed%20by%20structure-conditioned%20texture%20generation.%20Experiments%20demonstrate%0Athe%20state-of-the-art%20performance%20of%20GPSToken%2C%20which%20achieves%20rFID%20and%20FID%0Ascores%20of%200.65%20and%201.50%20on%20image%20reconstruction%20and%20generation%20tasks%20using%20128%0Atokens%2C%20respectively.%20Codes%20and%20models%20of%20GPSToken%20can%20be%20found%20at%0A%24%5Chref%7Bhttps%3A//github.com/xtudbxk/GPSToken%7D%7Bhttps%3A//github.com/xtudbxk/GPSToken%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPSToken%253A%2520Gaussian%2520Parameterized%2520Spatially-adaptive%2520Tokenization%2520for%250A%2520%2520Image%2520Representation%2520and%2520Generation%26entry.906535625%3DZhengqiang%2520Zhang%2520and%2520Rongyuan%2520Wu%2520and%2520Lingchen%2520Sun%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Effective%2520and%2520efficient%2520tokenization%2520plays%2520an%2520important%2520role%2520in%2520image%250Arepresentation%2520and%2520generation.%2520Conventional%2520methods%252C%2520constrained%2520by%2520uniform%250A2D/1D%2520grid%2520tokenization%252C%2520are%2520inflexible%2520to%2520represent%2520regions%2520with%2520varying%250Ashapes%2520and%2520textures%2520and%2520at%2520different%2520locations%252C%2520limiting%2520their%2520efficacy%2520of%250Afeature%2520representation.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2524%255Ctextbf%257BGPSToken%257D%2524%252C%2520a%2520novel%250A%2524%255Ctextbf%257BG%257D%2524aussian%2520%2524%255Ctextbf%257BP%257D%2524arameterized%2520%2524%255Ctextbf%257BS%257D%2524patially-adaptive%250A%2524%255Ctextbf%257BToken%257D%2524ization%2520framework%252C%2520to%2520achieve%2520non-uniform%2520image%2520tokenization%2520by%250Aleveraging%2520parametric%25202D%2520Gaussians%2520to%2520dynamically%2520model%2520the%2520shape%252C%2520position%252C%250Aand%2520textures%2520of%2520different%2520image%2520regions.%2520We%2520first%2520employ%2520an%2520entropy-driven%250Aalgorithm%2520to%2520partition%2520the%2520image%2520into%2520texture-homogeneous%2520regions%2520of%2520variable%250Asizes.%2520Then%252C%2520we%2520parameterize%2520each%2520region%2520as%2520a%25202D%2520Gaussian%2520%2528mean%2520for%2520position%252C%250Acovariance%2520for%2520shape%2529%2520coupled%2520with%2520texture%2520features.%2520A%2520specialized%2520transformer%250Ais%2520trained%2520to%2520optimize%2520the%2520Gaussian%2520parameters%252C%2520enabling%2520continuous%2520adaptation%250Aof%2520position/shape%2520and%2520content-aware%2520feature%2520extraction.%2520During%2520decoding%252C%250AGaussian%2520parameterized%2520tokens%2520are%2520reconstructed%2520into%25202D%2520feature%2520maps%2520through%2520a%250Adifferentiable%2520splatting-based%2520renderer%252C%2520bridging%2520our%2520adaptive%2520tokenization%250Awith%2520standard%2520decoders%2520for%2520end-to-end%2520training.%2520GPSToken%2520disentangles%2520spatial%250Alayout%2520%2528Gaussian%2520parameters%2529%2520from%2520texture%2520features%2520to%2520enable%2520efficient%250Atwo-stage%2520generation%253A%2520structural%2520layout%2520synthesis%2520using%2520lightweight%2520networks%252C%250Afollowed%2520by%2520structure-conditioned%2520texture%2520generation.%2520Experiments%2520demonstrate%250Athe%2520state-of-the-art%2520performance%2520of%2520GPSToken%252C%2520which%2520achieves%2520rFID%2520and%2520FID%250Ascores%2520of%25200.65%2520and%25201.50%2520on%2520image%2520reconstruction%2520and%2520generation%2520tasks%2520using%2520128%250Atokens%252C%2520respectively.%2520Codes%2520and%2520models%2520of%2520GPSToken%2520can%2520be%2520found%2520at%250A%2524%255Chref%257Bhttps%253A//github.com/xtudbxk/GPSToken%257D%257Bhttps%253A//github.com/xtudbxk/GPSToken%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPSToken%3A%20Gaussian%20Parameterized%20Spatially-adaptive%20Tokenization%20for%0A%20%20Image%20Representation%20and%20Generation&entry.906535625=Zhengqiang%20Zhang%20and%20Rongyuan%20Wu%20and%20Lingchen%20Sun%20and%20Lei%20Zhang&entry.1292438233=%20%20Effective%20and%20efficient%20tokenization%20plays%20an%20important%20role%20in%20image%0Arepresentation%20and%20generation.%20Conventional%20methods%2C%20constrained%20by%20uniform%0A2D/1D%20grid%20tokenization%2C%20are%20inflexible%20to%20represent%20regions%20with%20varying%0Ashapes%20and%20textures%20and%20at%20different%20locations%2C%20limiting%20their%20efficacy%20of%0Afeature%20representation.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextbf%7BGPSToken%7D%24%2C%20a%20novel%0A%24%5Ctextbf%7BG%7D%24aussian%20%24%5Ctextbf%7BP%7D%24arameterized%20%24%5Ctextbf%7BS%7D%24patially-adaptive%0A%24%5Ctextbf%7BToken%7D%24ization%20framework%2C%20to%20achieve%20non-uniform%20image%20tokenization%20by%0Aleveraging%20parametric%202D%20Gaussians%20to%20dynamically%20model%20the%20shape%2C%20position%2C%0Aand%20textures%20of%20different%20image%20regions.%20We%20first%20employ%20an%20entropy-driven%0Aalgorithm%20to%20partition%20the%20image%20into%20texture-homogeneous%20regions%20of%20variable%0Asizes.%20Then%2C%20we%20parameterize%20each%20region%20as%20a%202D%20Gaussian%20%28mean%20for%20position%2C%0Acovariance%20for%20shape%29%20coupled%20with%20texture%20features.%20A%20specialized%20transformer%0Ais%20trained%20to%20optimize%20the%20Gaussian%20parameters%2C%20enabling%20continuous%20adaptation%0Aof%20position/shape%20and%20content-aware%20feature%20extraction.%20During%20decoding%2C%0AGaussian%20parameterized%20tokens%20are%20reconstructed%20into%202D%20feature%20maps%20through%20a%0Adifferentiable%20splatting-based%20renderer%2C%20bridging%20our%20adaptive%20tokenization%0Awith%20standard%20decoders%20for%20end-to-end%20training.%20GPSToken%20disentangles%20spatial%0Alayout%20%28Gaussian%20parameters%29%20from%20texture%20features%20to%20enable%20efficient%0Atwo-stage%20generation%3A%20structural%20layout%20synthesis%20using%20lightweight%20networks%2C%0Afollowed%20by%20structure-conditioned%20texture%20generation.%20Experiments%20demonstrate%0Athe%20state-of-the-art%20performance%20of%20GPSToken%2C%20which%20achieves%20rFID%20and%20FID%0Ascores%20of%200.65%20and%201.50%20on%20image%20reconstruction%20and%20generation%20tasks%20using%20128%0Atokens%2C%20respectively.%20Codes%20and%20models%20of%20GPSToken%20can%20be%20found%20at%0A%24%5Chref%7Bhttps%3A//github.com/xtudbxk/GPSToken%7D%7Bhttps%3A//github.com/xtudbxk/GPSToken%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01109v2&entry.124074799=Read"},
{"title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars", "author": "Weiyi Xiong and Bing Zhu and Tao Huang and Zewei Zheng", "abstract": "  4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving.\n", "link": "http://arxiv.org/abs/2509.16119v1", "date": "2025-09-19", "relevancy": 2.9482, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6444}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5639}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadarGaussianDet3D%3A%20An%20Efficient%20and%20Effective%20Gaussian-based%203D%0A%20%20Detector%20with%204D%20Automotive%20Radars&body=Title%3A%20RadarGaussianDet3D%3A%20An%20Efficient%20and%20Effective%20Gaussian-based%203D%0A%20%20Detector%20with%204D%20Automotive%20Radars%0AAuthor%3A%20Weiyi%20Xiong%20and%20Bing%20Zhu%20and%20Tao%20Huang%20and%20Zewei%20Zheng%0AAbstract%3A%20%20%204D%20automotive%20radars%20have%20gained%20increasing%20attention%20for%20autonomous%20driving%0Adue%20to%20their%20low%20cost%2C%20robustness%2C%20and%20inherent%20velocity%20measurement%0Acapability.%20However%2C%20existing%204D%20radar-based%203D%20detectors%20rely%20heavily%20on%0Apillar%20encoders%20for%20BEV%20feature%20extraction%2C%20where%20each%20point%20contributes%20to%0Aonly%20a%20single%20BEV%20grid%2C%20resulting%20in%20sparse%20feature%20maps%20and%20degraded%0Arepresentation%20quality.%20In%20addition%2C%20they%20also%20optimize%20bounding%20box%20attributes%0Aindependently%2C%20leading%20to%20sub-optimal%20detection%20accuracy.%20Moreover%2C%20their%0Ainference%20speed%2C%20while%20sufficient%20for%20high-end%20GPUs%2C%20may%20fail%20to%20meet%20the%0Areal-time%20requirement%20on%20vehicle-mounted%20embedded%20devices.%20To%20overcome%20these%0Alimitations%2C%20an%20efficient%20and%20effective%20Gaussian-based%203D%20detector%2C%20namely%0ARadarGaussianDet3D%20is%20introduced%2C%20leveraging%20Gaussian%20primitives%20and%0Adistributions%20as%20intermediate%20representations%20for%20radar%20points%20and%20bounding%0Aboxes.%20In%20RadarGaussianDet3D%2C%20a%20novel%20Point%20Gaussian%20Encoder%20%28PGE%29%20is%20designed%0Ato%20transform%20each%20point%20into%20a%20Gaussian%20primitive%20after%20feature%20aggregation%20and%0Aemploys%20the%203D%20Gaussian%20Splatting%20%283DGS%29%20technique%20for%20BEV%20rasterization%2C%0Ayielding%20denser%20feature%20maps.%20PGE%20exhibits%20exceptionally%20low%20latency%2C%20owing%20to%0Athe%20optimized%20algorithm%20for%20point%20feature%20aggregation%20and%20fast%20rendering%20of%0A3DGS.%20In%20addition%2C%20a%20new%20Box%20Gaussian%20Loss%20%28BGL%29%20is%20proposed%2C%20which%20converts%0Abounding%20boxes%20into%203D%20Gaussian%20distributions%20and%20measures%20their%20distance%20to%0Aenable%20more%20comprehensive%20and%20consistent%20optimization.%20Extensive%20experiments%20on%0ATJ4DRadSet%20and%20View-of-Delft%20demonstrate%20that%20RadarGaussianDet3D%20achieves%0Astate-of-the-art%20detection%20accuracy%20while%20delivering%20substantially%20faster%0Ainference%2C%20highlighting%20its%20potential%20for%20real-time%20deployment%20in%20autonomous%0Adriving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadarGaussianDet3D%253A%2520An%2520Efficient%2520and%2520Effective%2520Gaussian-based%25203D%250A%2520%2520Detector%2520with%25204D%2520Automotive%2520Radars%26entry.906535625%3DWeiyi%2520Xiong%2520and%2520Bing%2520Zhu%2520and%2520Tao%2520Huang%2520and%2520Zewei%2520Zheng%26entry.1292438233%3D%2520%25204D%2520automotive%2520radars%2520have%2520gained%2520increasing%2520attention%2520for%2520autonomous%2520driving%250Adue%2520to%2520their%2520low%2520cost%252C%2520robustness%252C%2520and%2520inherent%2520velocity%2520measurement%250Acapability.%2520However%252C%2520existing%25204D%2520radar-based%25203D%2520detectors%2520rely%2520heavily%2520on%250Apillar%2520encoders%2520for%2520BEV%2520feature%2520extraction%252C%2520where%2520each%2520point%2520contributes%2520to%250Aonly%2520a%2520single%2520BEV%2520grid%252C%2520resulting%2520in%2520sparse%2520feature%2520maps%2520and%2520degraded%250Arepresentation%2520quality.%2520In%2520addition%252C%2520they%2520also%2520optimize%2520bounding%2520box%2520attributes%250Aindependently%252C%2520leading%2520to%2520sub-optimal%2520detection%2520accuracy.%2520Moreover%252C%2520their%250Ainference%2520speed%252C%2520while%2520sufficient%2520for%2520high-end%2520GPUs%252C%2520may%2520fail%2520to%2520meet%2520the%250Areal-time%2520requirement%2520on%2520vehicle-mounted%2520embedded%2520devices.%2520To%2520overcome%2520these%250Alimitations%252C%2520an%2520efficient%2520and%2520effective%2520Gaussian-based%25203D%2520detector%252C%2520namely%250ARadarGaussianDet3D%2520is%2520introduced%252C%2520leveraging%2520Gaussian%2520primitives%2520and%250Adistributions%2520as%2520intermediate%2520representations%2520for%2520radar%2520points%2520and%2520bounding%250Aboxes.%2520In%2520RadarGaussianDet3D%252C%2520a%2520novel%2520Point%2520Gaussian%2520Encoder%2520%2528PGE%2529%2520is%2520designed%250Ato%2520transform%2520each%2520point%2520into%2520a%2520Gaussian%2520primitive%2520after%2520feature%2520aggregation%2520and%250Aemploys%2520the%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520technique%2520for%2520BEV%2520rasterization%252C%250Ayielding%2520denser%2520feature%2520maps.%2520PGE%2520exhibits%2520exceptionally%2520low%2520latency%252C%2520owing%2520to%250Athe%2520optimized%2520algorithm%2520for%2520point%2520feature%2520aggregation%2520and%2520fast%2520rendering%2520of%250A3DGS.%2520In%2520addition%252C%2520a%2520new%2520Box%2520Gaussian%2520Loss%2520%2528BGL%2529%2520is%2520proposed%252C%2520which%2520converts%250Abounding%2520boxes%2520into%25203D%2520Gaussian%2520distributions%2520and%2520measures%2520their%2520distance%2520to%250Aenable%2520more%2520comprehensive%2520and%2520consistent%2520optimization.%2520Extensive%2520experiments%2520on%250ATJ4DRadSet%2520and%2520View-of-Delft%2520demonstrate%2520that%2520RadarGaussianDet3D%2520achieves%250Astate-of-the-art%2520detection%2520accuracy%2520while%2520delivering%2520substantially%2520faster%250Ainference%252C%2520highlighting%2520its%2520potential%2520for%2520real-time%2520deployment%2520in%2520autonomous%250Adriving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadarGaussianDet3D%3A%20An%20Efficient%20and%20Effective%20Gaussian-based%203D%0A%20%20Detector%20with%204D%20Automotive%20Radars&entry.906535625=Weiyi%20Xiong%20and%20Bing%20Zhu%20and%20Tao%20Huang%20and%20Zewei%20Zheng&entry.1292438233=%20%204D%20automotive%20radars%20have%20gained%20increasing%20attention%20for%20autonomous%20driving%0Adue%20to%20their%20low%20cost%2C%20robustness%2C%20and%20inherent%20velocity%20measurement%0Acapability.%20However%2C%20existing%204D%20radar-based%203D%20detectors%20rely%20heavily%20on%0Apillar%20encoders%20for%20BEV%20feature%20extraction%2C%20where%20each%20point%20contributes%20to%0Aonly%20a%20single%20BEV%20grid%2C%20resulting%20in%20sparse%20feature%20maps%20and%20degraded%0Arepresentation%20quality.%20In%20addition%2C%20they%20also%20optimize%20bounding%20box%20attributes%0Aindependently%2C%20leading%20to%20sub-optimal%20detection%20accuracy.%20Moreover%2C%20their%0Ainference%20speed%2C%20while%20sufficient%20for%20high-end%20GPUs%2C%20may%20fail%20to%20meet%20the%0Areal-time%20requirement%20on%20vehicle-mounted%20embedded%20devices.%20To%20overcome%20these%0Alimitations%2C%20an%20efficient%20and%20effective%20Gaussian-based%203D%20detector%2C%20namely%0ARadarGaussianDet3D%20is%20introduced%2C%20leveraging%20Gaussian%20primitives%20and%0Adistributions%20as%20intermediate%20representations%20for%20radar%20points%20and%20bounding%0Aboxes.%20In%20RadarGaussianDet3D%2C%20a%20novel%20Point%20Gaussian%20Encoder%20%28PGE%29%20is%20designed%0Ato%20transform%20each%20point%20into%20a%20Gaussian%20primitive%20after%20feature%20aggregation%20and%0Aemploys%20the%203D%20Gaussian%20Splatting%20%283DGS%29%20technique%20for%20BEV%20rasterization%2C%0Ayielding%20denser%20feature%20maps.%20PGE%20exhibits%20exceptionally%20low%20latency%2C%20owing%20to%0Athe%20optimized%20algorithm%20for%20point%20feature%20aggregation%20and%20fast%20rendering%20of%0A3DGS.%20In%20addition%2C%20a%20new%20Box%20Gaussian%20Loss%20%28BGL%29%20is%20proposed%2C%20which%20converts%0Abounding%20boxes%20into%203D%20Gaussian%20distributions%20and%20measures%20their%20distance%20to%0Aenable%20more%20comprehensive%20and%20consistent%20optimization.%20Extensive%20experiments%20on%0ATJ4DRadSet%20and%20View-of-Delft%20demonstrate%20that%20RadarGaussianDet3D%20achieves%0Astate-of-the-art%20detection%20accuracy%20while%20delivering%20substantially%20faster%0Ainference%2C%20highlighting%20its%20potential%20for%20real-time%20deployment%20in%20autonomous%0Adriving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16119v1&entry.124074799=Read"},
{"title": "Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal", "author": "Chang Yu and Siyu Ma and Wenxin Du and Zeshun Zong and Han Xue and Wendi Chen and Cewu Lu and Yin Yang and Xuchen Han and Joseph Masterjohn and Alejandro Castro and Chenfanfu Jiang", "abstract": "  Turning garments right-side out is a challenging manipulation task: it is\nhighly dynamic, entails rapid contact changes, and is subject to severe visual\nocclusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that\neffectively solves this challenge by exploiting task structures. We decompose\nthe task into Drag/Fling to create and stabilize an access opening, followed by\nInsert&Pull to invert the garment. Each step uses a depth-inferred,\nkeypoint-parameterized bimanual primitive that sharply reduces the action space\nwhile preserving robustness. Efficient data generation is enabled by our\ncustom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator\nthat models thin-shell deformation and provides robust and efficient contact\nhandling for batched rollouts. Built on the simulator, our fully automated\npipeline scales data generation by randomizing garment geometry, material\nparameters, and viewpoints, producing depth, masks, and per-primitive keypoint\nlabels without any human annotations. With a single depth camera, policies\ntrained entirely in simulation deploy zero-shot on real hardware, achieving up\nto 81.3% success rate. By employing task decomposition and high fidelity\nsimulation, our framework enables tackling highly dynamic, severely occluded\ntasks without laborious human demonstrations.\n", "link": "http://arxiv.org/abs/2509.15953v1", "date": "2025-09-19", "relevancy": 2.93, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6047}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5944}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Right-Side-Out%3A%20Learning%20Zero-Shot%20Sim-to-Real%20Garment%20Reversal&body=Title%3A%20Right-Side-Out%3A%20Learning%20Zero-Shot%20Sim-to-Real%20Garment%20Reversal%0AAuthor%3A%20Chang%20Yu%20and%20Siyu%20Ma%20and%20Wenxin%20Du%20and%20Zeshun%20Zong%20and%20Han%20Xue%20and%20Wendi%20Chen%20and%20Cewu%20Lu%20and%20Yin%20Yang%20and%20Xuchen%20Han%20and%20Joseph%20Masterjohn%20and%20Alejandro%20Castro%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20%20%20Turning%20garments%20right-side%20out%20is%20a%20challenging%20manipulation%20task%3A%20it%20is%0Ahighly%20dynamic%2C%20entails%20rapid%20contact%20changes%2C%20and%20is%20subject%20to%20severe%20visual%0Aocclusion.%20We%20introduce%20Right-Side-Out%2C%20a%20zero-shot%20sim-to-real%20framework%20that%0Aeffectively%20solves%20this%20challenge%20by%20exploiting%20task%20structures.%20We%20decompose%0Athe%20task%20into%20Drag/Fling%20to%20create%20and%20stabilize%20an%20access%20opening%2C%20followed%20by%0AInsert%26Pull%20to%20invert%20the%20garment.%20Each%20step%20uses%20a%20depth-inferred%2C%0Akeypoint-parameterized%20bimanual%20primitive%20that%20sharply%20reduces%20the%20action%20space%0Awhile%20preserving%20robustness.%20Efficient%20data%20generation%20is%20enabled%20by%20our%0Acustom-built%2C%20high-fidelity%2C%20GPU-parallel%20Material%20Point%20Method%20%28MPM%29%20simulator%0Athat%20models%20thin-shell%20deformation%20and%20provides%20robust%20and%20efficient%20contact%0Ahandling%20for%20batched%20rollouts.%20Built%20on%20the%20simulator%2C%20our%20fully%20automated%0Apipeline%20scales%20data%20generation%20by%20randomizing%20garment%20geometry%2C%20material%0Aparameters%2C%20and%20viewpoints%2C%20producing%20depth%2C%20masks%2C%20and%20per-primitive%20keypoint%0Alabels%20without%20any%20human%20annotations.%20With%20a%20single%20depth%20camera%2C%20policies%0Atrained%20entirely%20in%20simulation%20deploy%20zero-shot%20on%20real%20hardware%2C%20achieving%20up%0Ato%2081.3%25%20success%20rate.%20By%20employing%20task%20decomposition%20and%20high%20fidelity%0Asimulation%2C%20our%20framework%20enables%20tackling%20highly%20dynamic%2C%20severely%20occluded%0Atasks%20without%20laborious%20human%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRight-Side-Out%253A%2520Learning%2520Zero-Shot%2520Sim-to-Real%2520Garment%2520Reversal%26entry.906535625%3DChang%2520Yu%2520and%2520Siyu%2520Ma%2520and%2520Wenxin%2520Du%2520and%2520Zeshun%2520Zong%2520and%2520Han%2520Xue%2520and%2520Wendi%2520Chen%2520and%2520Cewu%2520Lu%2520and%2520Yin%2520Yang%2520and%2520Xuchen%2520Han%2520and%2520Joseph%2520Masterjohn%2520and%2520Alejandro%2520Castro%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3D%2520%2520Turning%2520garments%2520right-side%2520out%2520is%2520a%2520challenging%2520manipulation%2520task%253A%2520it%2520is%250Ahighly%2520dynamic%252C%2520entails%2520rapid%2520contact%2520changes%252C%2520and%2520is%2520subject%2520to%2520severe%2520visual%250Aocclusion.%2520We%2520introduce%2520Right-Side-Out%252C%2520a%2520zero-shot%2520sim-to-real%2520framework%2520that%250Aeffectively%2520solves%2520this%2520challenge%2520by%2520exploiting%2520task%2520structures.%2520We%2520decompose%250Athe%2520task%2520into%2520Drag/Fling%2520to%2520create%2520and%2520stabilize%2520an%2520access%2520opening%252C%2520followed%2520by%250AInsert%2526Pull%2520to%2520invert%2520the%2520garment.%2520Each%2520step%2520uses%2520a%2520depth-inferred%252C%250Akeypoint-parameterized%2520bimanual%2520primitive%2520that%2520sharply%2520reduces%2520the%2520action%2520space%250Awhile%2520preserving%2520robustness.%2520Efficient%2520data%2520generation%2520is%2520enabled%2520by%2520our%250Acustom-built%252C%2520high-fidelity%252C%2520GPU-parallel%2520Material%2520Point%2520Method%2520%2528MPM%2529%2520simulator%250Athat%2520models%2520thin-shell%2520deformation%2520and%2520provides%2520robust%2520and%2520efficient%2520contact%250Ahandling%2520for%2520batched%2520rollouts.%2520Built%2520on%2520the%2520simulator%252C%2520our%2520fully%2520automated%250Apipeline%2520scales%2520data%2520generation%2520by%2520randomizing%2520garment%2520geometry%252C%2520material%250Aparameters%252C%2520and%2520viewpoints%252C%2520producing%2520depth%252C%2520masks%252C%2520and%2520per-primitive%2520keypoint%250Alabels%2520without%2520any%2520human%2520annotations.%2520With%2520a%2520single%2520depth%2520camera%252C%2520policies%250Atrained%2520entirely%2520in%2520simulation%2520deploy%2520zero-shot%2520on%2520real%2520hardware%252C%2520achieving%2520up%250Ato%252081.3%2525%2520success%2520rate.%2520By%2520employing%2520task%2520decomposition%2520and%2520high%2520fidelity%250Asimulation%252C%2520our%2520framework%2520enables%2520tackling%2520highly%2520dynamic%252C%2520severely%2520occluded%250Atasks%2520without%2520laborious%2520human%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Right-Side-Out%3A%20Learning%20Zero-Shot%20Sim-to-Real%20Garment%20Reversal&entry.906535625=Chang%20Yu%20and%20Siyu%20Ma%20and%20Wenxin%20Du%20and%20Zeshun%20Zong%20and%20Han%20Xue%20and%20Wendi%20Chen%20and%20Cewu%20Lu%20and%20Yin%20Yang%20and%20Xuchen%20Han%20and%20Joseph%20Masterjohn%20and%20Alejandro%20Castro%20and%20Chenfanfu%20Jiang&entry.1292438233=%20%20Turning%20garments%20right-side%20out%20is%20a%20challenging%20manipulation%20task%3A%20it%20is%0Ahighly%20dynamic%2C%20entails%20rapid%20contact%20changes%2C%20and%20is%20subject%20to%20severe%20visual%0Aocclusion.%20We%20introduce%20Right-Side-Out%2C%20a%20zero-shot%20sim-to-real%20framework%20that%0Aeffectively%20solves%20this%20challenge%20by%20exploiting%20task%20structures.%20We%20decompose%0Athe%20task%20into%20Drag/Fling%20to%20create%20and%20stabilize%20an%20access%20opening%2C%20followed%20by%0AInsert%26Pull%20to%20invert%20the%20garment.%20Each%20step%20uses%20a%20depth-inferred%2C%0Akeypoint-parameterized%20bimanual%20primitive%20that%20sharply%20reduces%20the%20action%20space%0Awhile%20preserving%20robustness.%20Efficient%20data%20generation%20is%20enabled%20by%20our%0Acustom-built%2C%20high-fidelity%2C%20GPU-parallel%20Material%20Point%20Method%20%28MPM%29%20simulator%0Athat%20models%20thin-shell%20deformation%20and%20provides%20robust%20and%20efficient%20contact%0Ahandling%20for%20batched%20rollouts.%20Built%20on%20the%20simulator%2C%20our%20fully%20automated%0Apipeline%20scales%20data%20generation%20by%20randomizing%20garment%20geometry%2C%20material%0Aparameters%2C%20and%20viewpoints%2C%20producing%20depth%2C%20masks%2C%20and%20per-primitive%20keypoint%0Alabels%20without%20any%20human%20annotations.%20With%20a%20single%20depth%20camera%2C%20policies%0Atrained%20entirely%20in%20simulation%20deploy%20zero-shot%20on%20real%20hardware%2C%20achieving%20up%0Ato%2081.3%25%20success%20rate.%20By%20employing%20task%20decomposition%20and%20high%20fidelity%0Asimulation%2C%20our%20framework%20enables%20tackling%20highly%20dynamic%2C%20severely%20occluded%0Atasks%20without%20laborious%20human%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15953v1&entry.124074799=Read"},
{"title": "The Missing Piece: A Case for Pre-Training in 3D Medical Object\n  Detection", "author": "Katharina Eckstein and Constantin Ulrich and Michael Baumgartner and Jessica K\u00e4chele and Dimitrios Bounias and Tassilo Wald and Ralf Floca and Klaus H. Maier-Hein", "abstract": "  Large-scale pre-training holds the promise to advance 3D medical object\ndetection, a crucial component of accurate computer-aided diagnosis. Yet, it\nremains underexplored compared to segmentation, where pre-training has already\ndemonstrated significant benefits. Existing pre-training approaches for 3D\nobject detection rely on 2D medical data or natural image pre-training, failing\nto fully leverage 3D volumetric information. In this work, we present the first\nsystematic study of how existing pre-training methods can be integrated into\nstate-of-the-art detection architectures, covering both CNNs and Transformers.\nOur results show that pre-training consistently improves detection performance\nacross various tasks and datasets. Notably, reconstruction-based\nself-supervised pre-training outperforms supervised pre-training, while\ncontrastive pre-training provides no clear benefit for 3D medical object\ndetection. Our code is publicly available at:\nhttps://github.com/MIC-DKFZ/nnDetection-finetuning.\n", "link": "http://arxiv.org/abs/2509.15947v1", "date": "2025-09-19", "relevancy": 2.924, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.579}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Missing%20Piece%3A%20A%20Case%20for%20Pre-Training%20in%203D%20Medical%20Object%0A%20%20Detection&body=Title%3A%20The%20Missing%20Piece%3A%20A%20Case%20for%20Pre-Training%20in%203D%20Medical%20Object%0A%20%20Detection%0AAuthor%3A%20Katharina%20Eckstein%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Jessica%20K%C3%A4chele%20and%20Dimitrios%20Bounias%20and%20Tassilo%20Wald%20and%20Ralf%20Floca%20and%20Klaus%20H.%20Maier-Hein%0AAbstract%3A%20%20%20Large-scale%20pre-training%20holds%20the%20promise%20to%20advance%203D%20medical%20object%0Adetection%2C%20a%20crucial%20component%20of%20accurate%20computer-aided%20diagnosis.%20Yet%2C%20it%0Aremains%20underexplored%20compared%20to%20segmentation%2C%20where%20pre-training%20has%20already%0Ademonstrated%20significant%20benefits.%20Existing%20pre-training%20approaches%20for%203D%0Aobject%20detection%20rely%20on%202D%20medical%20data%20or%20natural%20image%20pre-training%2C%20failing%0Ato%20fully%20leverage%203D%20volumetric%20information.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20study%20of%20how%20existing%20pre-training%20methods%20can%20be%20integrated%20into%0Astate-of-the-art%20detection%20architectures%2C%20covering%20both%20CNNs%20and%20Transformers.%0AOur%20results%20show%20that%20pre-training%20consistently%20improves%20detection%20performance%0Aacross%20various%20tasks%20and%20datasets.%20Notably%2C%20reconstruction-based%0Aself-supervised%20pre-training%20outperforms%20supervised%20pre-training%2C%20while%0Acontrastive%20pre-training%20provides%20no%20clear%20benefit%20for%203D%20medical%20object%0Adetection.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/MIC-DKFZ/nnDetection-finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Missing%2520Piece%253A%2520A%2520Case%2520for%2520Pre-Training%2520in%25203D%2520Medical%2520Object%250A%2520%2520Detection%26entry.906535625%3DKatharina%2520Eckstein%2520and%2520Constantin%2520Ulrich%2520and%2520Michael%2520Baumgartner%2520and%2520Jessica%2520K%25C3%25A4chele%2520and%2520Dimitrios%2520Bounias%2520and%2520Tassilo%2520Wald%2520and%2520Ralf%2520Floca%2520and%2520Klaus%2520H.%2520Maier-Hein%26entry.1292438233%3D%2520%2520Large-scale%2520pre-training%2520holds%2520the%2520promise%2520to%2520advance%25203D%2520medical%2520object%250Adetection%252C%2520a%2520crucial%2520component%2520of%2520accurate%2520computer-aided%2520diagnosis.%2520Yet%252C%2520it%250Aremains%2520underexplored%2520compared%2520to%2520segmentation%252C%2520where%2520pre-training%2520has%2520already%250Ademonstrated%2520significant%2520benefits.%2520Existing%2520pre-training%2520approaches%2520for%25203D%250Aobject%2520detection%2520rely%2520on%25202D%2520medical%2520data%2520or%2520natural%2520image%2520pre-training%252C%2520failing%250Ato%2520fully%2520leverage%25203D%2520volumetric%2520information.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%250Asystematic%2520study%2520of%2520how%2520existing%2520pre-training%2520methods%2520can%2520be%2520integrated%2520into%250Astate-of-the-art%2520detection%2520architectures%252C%2520covering%2520both%2520CNNs%2520and%2520Transformers.%250AOur%2520results%2520show%2520that%2520pre-training%2520consistently%2520improves%2520detection%2520performance%250Aacross%2520various%2520tasks%2520and%2520datasets.%2520Notably%252C%2520reconstruction-based%250Aself-supervised%2520pre-training%2520outperforms%2520supervised%2520pre-training%252C%2520while%250Acontrastive%2520pre-training%2520provides%2520no%2520clear%2520benefit%2520for%25203D%2520medical%2520object%250Adetection.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/MIC-DKFZ/nnDetection-finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Missing%20Piece%3A%20A%20Case%20for%20Pre-Training%20in%203D%20Medical%20Object%0A%20%20Detection&entry.906535625=Katharina%20Eckstein%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Jessica%20K%C3%A4chele%20and%20Dimitrios%20Bounias%20and%20Tassilo%20Wald%20and%20Ralf%20Floca%20and%20Klaus%20H.%20Maier-Hein&entry.1292438233=%20%20Large-scale%20pre-training%20holds%20the%20promise%20to%20advance%203D%20medical%20object%0Adetection%2C%20a%20crucial%20component%20of%20accurate%20computer-aided%20diagnosis.%20Yet%2C%20it%0Aremains%20underexplored%20compared%20to%20segmentation%2C%20where%20pre-training%20has%20already%0Ademonstrated%20significant%20benefits.%20Existing%20pre-training%20approaches%20for%203D%0Aobject%20detection%20rely%20on%202D%20medical%20data%20or%20natural%20image%20pre-training%2C%20failing%0Ato%20fully%20leverage%203D%20volumetric%20information.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20study%20of%20how%20existing%20pre-training%20methods%20can%20be%20integrated%20into%0Astate-of-the-art%20detection%20architectures%2C%20covering%20both%20CNNs%20and%20Transformers.%0AOur%20results%20show%20that%20pre-training%20consistently%20improves%20detection%20performance%0Aacross%20various%20tasks%20and%20datasets.%20Notably%2C%20reconstruction-based%0Aself-supervised%20pre-training%20outperforms%20supervised%20pre-training%2C%20while%0Acontrastive%20pre-training%20provides%20no%20clear%20benefit%20for%203D%20medical%20object%0Adetection.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/MIC-DKFZ/nnDetection-finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15947v1&entry.124074799=Read"},
{"title": "Robust Vision-Language Models via Tensor Decomposition: A Defense\n  Against Adversarial Attacks", "author": "Het Patel and Muzammil Allie and Qian Zhang and Jia Chen and Evangelos E. Papalexakis", "abstract": "  Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.\n", "link": "http://arxiv.org/abs/2509.16163v1", "date": "2025-09-19", "relevancy": 2.8723, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Vision-Language%20Models%20via%20Tensor%20Decomposition%3A%20A%20Defense%0A%20%20Against%20Adversarial%20Attacks&body=Title%3A%20Robust%20Vision-Language%20Models%20via%20Tensor%20Decomposition%3A%20A%20Defense%0A%20%20Against%20Adversarial%20Attacks%0AAuthor%3A%20Het%20Patel%20and%20Muzammil%20Allie%20and%20Qian%20Zhang%20and%20Jia%20Chen%20and%20Evangelos%20E.%20Papalexakis%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20excel%20in%20multimodal%20understanding%20but%20are%20prone%0Ato%20adversarial%20attacks.%20Existing%20defenses%20often%20demand%20costly%20retraining%20or%0Asignificant%20architecture%20changes.%20We%20introduce%20a%20lightweight%20defense%20using%0Atensor%20decomposition%20suitable%20for%20any%20pre-trained%20VLM%2C%20requiring%20no%20retraining.%0ABy%20decomposing%20and%20reconstructing%20vision%20encoder%20representations%2C%20it%20filters%0Aadversarial%20noise%20while%20preserving%20meaning.%20Experiments%20with%20CLIP%20on%20COCO%20and%0AFlickr30K%20show%20improved%20robustness.%20On%20Flickr30K%2C%20it%20restores%2012.3%5C%25%0Aperformance%20lost%20to%20attacks%2C%20raising%20Recall%401%20accuracy%20from%207.5%5C%25%20to%2019.8%5C%25.%20On%0ACOCO%2C%20it%20recovers%208.1%5C%25%20performance%2C%20improving%20accuracy%20from%203.8%5C%25%20to%2011.9%5C%25.%0AAnalysis%20shows%20Tensor%20Train%20decomposition%20with%20low%20rank%20%288-32%29%20and%20low%20residual%0Astrength%20%28%24%5Calpha%3D0.1-0.2%24%29%20is%20optimal.%20This%20method%20is%20a%20practical%2C%0Aplug-and-play%20solution%20with%20minimal%20overhead%20for%20existing%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Vision-Language%2520Models%2520via%2520Tensor%2520Decomposition%253A%2520A%2520Defense%250A%2520%2520Against%2520Adversarial%2520Attacks%26entry.906535625%3DHet%2520Patel%2520and%2520Muzammil%2520Allie%2520and%2520Qian%2520Zhang%2520and%2520Jia%2520Chen%2520and%2520Evangelos%2520E.%2520Papalexakis%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520excel%2520in%2520multimodal%2520understanding%2520but%2520are%2520prone%250Ato%2520adversarial%2520attacks.%2520Existing%2520defenses%2520often%2520demand%2520costly%2520retraining%2520or%250Asignificant%2520architecture%2520changes.%2520We%2520introduce%2520a%2520lightweight%2520defense%2520using%250Atensor%2520decomposition%2520suitable%2520for%2520any%2520pre-trained%2520VLM%252C%2520requiring%2520no%2520retraining.%250ABy%2520decomposing%2520and%2520reconstructing%2520vision%2520encoder%2520representations%252C%2520it%2520filters%250Aadversarial%2520noise%2520while%2520preserving%2520meaning.%2520Experiments%2520with%2520CLIP%2520on%2520COCO%2520and%250AFlickr30K%2520show%2520improved%2520robustness.%2520On%2520Flickr30K%252C%2520it%2520restores%252012.3%255C%2525%250Aperformance%2520lost%2520to%2520attacks%252C%2520raising%2520Recall%25401%2520accuracy%2520from%25207.5%255C%2525%2520to%252019.8%255C%2525.%2520On%250ACOCO%252C%2520it%2520recovers%25208.1%255C%2525%2520performance%252C%2520improving%2520accuracy%2520from%25203.8%255C%2525%2520to%252011.9%255C%2525.%250AAnalysis%2520shows%2520Tensor%2520Train%2520decomposition%2520with%2520low%2520rank%2520%25288-32%2529%2520and%2520low%2520residual%250Astrength%2520%2528%2524%255Calpha%253D0.1-0.2%2524%2529%2520is%2520optimal.%2520This%2520method%2520is%2520a%2520practical%252C%250Aplug-and-play%2520solution%2520with%2520minimal%2520overhead%2520for%2520existing%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Vision-Language%20Models%20via%20Tensor%20Decomposition%3A%20A%20Defense%0A%20%20Against%20Adversarial%20Attacks&entry.906535625=Het%20Patel%20and%20Muzammil%20Allie%20and%20Qian%20Zhang%20and%20Jia%20Chen%20and%20Evangelos%20E.%20Papalexakis&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20excel%20in%20multimodal%20understanding%20but%20are%20prone%0Ato%20adversarial%20attacks.%20Existing%20defenses%20often%20demand%20costly%20retraining%20or%0Asignificant%20architecture%20changes.%20We%20introduce%20a%20lightweight%20defense%20using%0Atensor%20decomposition%20suitable%20for%20any%20pre-trained%20VLM%2C%20requiring%20no%20retraining.%0ABy%20decomposing%20and%20reconstructing%20vision%20encoder%20representations%2C%20it%20filters%0Aadversarial%20noise%20while%20preserving%20meaning.%20Experiments%20with%20CLIP%20on%20COCO%20and%0AFlickr30K%20show%20improved%20robustness.%20On%20Flickr30K%2C%20it%20restores%2012.3%5C%25%0Aperformance%20lost%20to%20attacks%2C%20raising%20Recall%401%20accuracy%20from%207.5%5C%25%20to%2019.8%5C%25.%20On%0ACOCO%2C%20it%20recovers%208.1%5C%25%20performance%2C%20improving%20accuracy%20from%203.8%5C%25%20to%2011.9%5C%25.%0AAnalysis%20shows%20Tensor%20Train%20decomposition%20with%20low%20rank%20%288-32%29%20and%20low%20residual%0Astrength%20%28%24%5Calpha%3D0.1-0.2%24%29%20is%20optimal.%20This%20method%20is%20a%20practical%2C%0Aplug-and-play%20solution%20with%20minimal%20overhead%20for%20existing%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16163v1&entry.124074799=Read"},
{"title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer", "author": "Yanghao Li and Rui Qian and Bowen Pan and Haotian Zhang and Haoshuo Huang and Bowen Zhang and Jialing Tong and Haoxuan You and Xianzhi Du and Zhe Gan and Hyunjik Kim and Chao Jia and Zhenbang Wang and Yinfei Yang and Mingfei Gao and Zi-Yi Dou and Wenze Hu and Chang Gao and Dongxu Li and Philipp Dufter and Zirui Wang and Guoli Yin and Zhengdong Zhang and Chen Chen and Yang Zhao and Ruoming Pang and Zhifeng Chen", "abstract": "  Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.\n", "link": "http://arxiv.org/abs/2509.16197v1", "date": "2025-09-19", "relevancy": 2.863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MANZANO%3A%20A%20Simple%20and%20Scalable%20Unified%20Multimodal%20Model%20with%20a%20Hybrid%0A%20%20Vision%20Tokenizer&body=Title%3A%20MANZANO%3A%20A%20Simple%20and%20Scalable%20Unified%20Multimodal%20Model%20with%20a%20Hybrid%0A%20%20Vision%20Tokenizer%0AAuthor%3A%20Yanghao%20Li%20and%20Rui%20Qian%20and%20Bowen%20Pan%20and%20Haotian%20Zhang%20and%20Haoshuo%20Huang%20and%20Bowen%20Zhang%20and%20Jialing%20Tong%20and%20Haoxuan%20You%20and%20Xianzhi%20Du%20and%20Zhe%20Gan%20and%20Hyunjik%20Kim%20and%20Chao%20Jia%20and%20Zhenbang%20Wang%20and%20Yinfei%20Yang%20and%20Mingfei%20Gao%20and%20Zi-Yi%20Dou%20and%20Wenze%20Hu%20and%20Chang%20Gao%20and%20Dongxu%20Li%20and%20Philipp%20Dufter%20and%20Zirui%20Wang%20and%20Guoli%20Yin%20and%20Zhengdong%20Zhang%20and%20Chen%20Chen%20and%20Yang%20Zhao%20and%20Ruoming%20Pang%20and%20Zhifeng%20Chen%0AAbstract%3A%20%20%20Unified%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20both%20understand%20and%0Agenerate%20visual%20content%20hold%20immense%20potential.%20However%2C%20existing%20open-source%0Amodels%20often%20suffer%20from%20a%20performance%20trade-off%20between%20these%20capabilities.%20We%0Apresent%20Manzano%2C%20a%20simple%20and%20scalable%20unified%20framework%20that%20substantially%0Areduces%20this%20tension%20by%20coupling%20a%20hybrid%20image%20tokenizer%20with%20a%20well-curated%0Atraining%20recipe.%20A%20single%20shared%20vision%20encoder%20feeds%20two%20lightweight%20adapters%0Athat%20produce%20continuous%20embeddings%20for%20image-to-text%20understanding%20and%20discrete%0Atokens%20for%20text-to-image%20generation%20within%20a%20common%20semantic%20space.%20A%20unified%0Aautoregressive%20LLM%20predicts%20high-level%20semantics%20in%20the%20form%20of%20text%20and%20image%0Atokens%2C%20with%20an%20auxiliary%20diffusion%20decoder%20subsequently%20translating%20the%20image%0Atokens%20into%20pixels.%20The%20architecture%2C%20together%20with%20a%20unified%20training%20recipe%0Aover%20understanding%20and%20generation%20data%2C%20enables%20scalable%20joint%20learning%20of%20both%0Acapabilities.%20Manzano%20achieves%20state-of-the-art%20results%20among%20unified%20models%2C%0Aand%20is%20competitive%20with%20specialist%20models%2C%20particularly%20on%20text-rich%0Aevaluation.%20Our%20studies%20show%20minimal%20task%20conflicts%20and%20consistent%20gains%20from%0Ascaling%20model%20size%2C%20validating%20our%20design%20choice%20of%20a%20hybrid%20tokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMANZANO%253A%2520A%2520Simple%2520and%2520Scalable%2520Unified%2520Multimodal%2520Model%2520with%2520a%2520Hybrid%250A%2520%2520Vision%2520Tokenizer%26entry.906535625%3DYanghao%2520Li%2520and%2520Rui%2520Qian%2520and%2520Bowen%2520Pan%2520and%2520Haotian%2520Zhang%2520and%2520Haoshuo%2520Huang%2520and%2520Bowen%2520Zhang%2520and%2520Jialing%2520Tong%2520and%2520Haoxuan%2520You%2520and%2520Xianzhi%2520Du%2520and%2520Zhe%2520Gan%2520and%2520Hyunjik%2520Kim%2520and%2520Chao%2520Jia%2520and%2520Zhenbang%2520Wang%2520and%2520Yinfei%2520Yang%2520and%2520Mingfei%2520Gao%2520and%2520Zi-Yi%2520Dou%2520and%2520Wenze%2520Hu%2520and%2520Chang%2520Gao%2520and%2520Dongxu%2520Li%2520and%2520Philipp%2520Dufter%2520and%2520Zirui%2520Wang%2520and%2520Guoli%2520Yin%2520and%2520Zhengdong%2520Zhang%2520and%2520Chen%2520Chen%2520and%2520Yang%2520Zhao%2520and%2520Ruoming%2520Pang%2520and%2520Zhifeng%2520Chen%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520can%2520both%2520understand%2520and%250Agenerate%2520visual%2520content%2520hold%2520immense%2520potential.%2520However%252C%2520existing%2520open-source%250Amodels%2520often%2520suffer%2520from%2520a%2520performance%2520trade-off%2520between%2520these%2520capabilities.%2520We%250Apresent%2520Manzano%252C%2520a%2520simple%2520and%2520scalable%2520unified%2520framework%2520that%2520substantially%250Areduces%2520this%2520tension%2520by%2520coupling%2520a%2520hybrid%2520image%2520tokenizer%2520with%2520a%2520well-curated%250Atraining%2520recipe.%2520A%2520single%2520shared%2520vision%2520encoder%2520feeds%2520two%2520lightweight%2520adapters%250Athat%2520produce%2520continuous%2520embeddings%2520for%2520image-to-text%2520understanding%2520and%2520discrete%250Atokens%2520for%2520text-to-image%2520generation%2520within%2520a%2520common%2520semantic%2520space.%2520A%2520unified%250Aautoregressive%2520LLM%2520predicts%2520high-level%2520semantics%2520in%2520the%2520form%2520of%2520text%2520and%2520image%250Atokens%252C%2520with%2520an%2520auxiliary%2520diffusion%2520decoder%2520subsequently%2520translating%2520the%2520image%250Atokens%2520into%2520pixels.%2520The%2520architecture%252C%2520together%2520with%2520a%2520unified%2520training%2520recipe%250Aover%2520understanding%2520and%2520generation%2520data%252C%2520enables%2520scalable%2520joint%2520learning%2520of%2520both%250Acapabilities.%2520Manzano%2520achieves%2520state-of-the-art%2520results%2520among%2520unified%2520models%252C%250Aand%2520is%2520competitive%2520with%2520specialist%2520models%252C%2520particularly%2520on%2520text-rich%250Aevaluation.%2520Our%2520studies%2520show%2520minimal%2520task%2520conflicts%2520and%2520consistent%2520gains%2520from%250Ascaling%2520model%2520size%252C%2520validating%2520our%2520design%2520choice%2520of%2520a%2520hybrid%2520tokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MANZANO%3A%20A%20Simple%20and%20Scalable%20Unified%20Multimodal%20Model%20with%20a%20Hybrid%0A%20%20Vision%20Tokenizer&entry.906535625=Yanghao%20Li%20and%20Rui%20Qian%20and%20Bowen%20Pan%20and%20Haotian%20Zhang%20and%20Haoshuo%20Huang%20and%20Bowen%20Zhang%20and%20Jialing%20Tong%20and%20Haoxuan%20You%20and%20Xianzhi%20Du%20and%20Zhe%20Gan%20and%20Hyunjik%20Kim%20and%20Chao%20Jia%20and%20Zhenbang%20Wang%20and%20Yinfei%20Yang%20and%20Mingfei%20Gao%20and%20Zi-Yi%20Dou%20and%20Wenze%20Hu%20and%20Chang%20Gao%20and%20Dongxu%20Li%20and%20Philipp%20Dufter%20and%20Zirui%20Wang%20and%20Guoli%20Yin%20and%20Zhengdong%20Zhang%20and%20Chen%20Chen%20and%20Yang%20Zhao%20and%20Ruoming%20Pang%20and%20Zhifeng%20Chen&entry.1292438233=%20%20Unified%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20both%20understand%20and%0Agenerate%20visual%20content%20hold%20immense%20potential.%20However%2C%20existing%20open-source%0Amodels%20often%20suffer%20from%20a%20performance%20trade-off%20between%20these%20capabilities.%20We%0Apresent%20Manzano%2C%20a%20simple%20and%20scalable%20unified%20framework%20that%20substantially%0Areduces%20this%20tension%20by%20coupling%20a%20hybrid%20image%20tokenizer%20with%20a%20well-curated%0Atraining%20recipe.%20A%20single%20shared%20vision%20encoder%20feeds%20two%20lightweight%20adapters%0Athat%20produce%20continuous%20embeddings%20for%20image-to-text%20understanding%20and%20discrete%0Atokens%20for%20text-to-image%20generation%20within%20a%20common%20semantic%20space.%20A%20unified%0Aautoregressive%20LLM%20predicts%20high-level%20semantics%20in%20the%20form%20of%20text%20and%20image%0Atokens%2C%20with%20an%20auxiliary%20diffusion%20decoder%20subsequently%20translating%20the%20image%0Atokens%20into%20pixels.%20The%20architecture%2C%20together%20with%20a%20unified%20training%20recipe%0Aover%20understanding%20and%20generation%20data%2C%20enables%20scalable%20joint%20learning%20of%20both%0Acapabilities.%20Manzano%20achieves%20state-of-the-art%20results%20among%20unified%20models%2C%0Aand%20is%20competitive%20with%20specialist%20models%2C%20particularly%20on%20text-rich%0Aevaluation.%20Our%20studies%20show%20minimal%20task%20conflicts%20and%20consistent%20gains%20from%0Ascaling%20model%20size%2C%20validating%20our%20design%20choice%20of%20a%20hybrid%20tokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16197v1&entry.124074799=Read"},
{"title": "Diffusion-Based Depth Inpainting for Transparent and Reflective Objects", "author": "Tianyu Sun and Dingchang Hu and Yixiang Dai and Guijin Wang", "abstract": "  Transparent and reflective objects, which are common in our everyday lives,\npresent a significant challenge to 3D imaging techniques due to their unique\nvisual and optical properties. Faced with these types of objects, RGB-D cameras\nfail to capture the real depth value with their accurate spatial information.\nTo address this issue, we propose DITR, a diffusion-based Depth Inpainting\nframework specifically designed for Transparent and Reflective objects. This\nnetwork consists of two stages, including a Region Proposal stage and a Depth\nInpainting stage. DITR dynamically analyzes the optical and geometric depth\nloss and inpaints them automatically. Furthermore, comprehensive experimental\nresults demonstrate that DITR is highly effective in depth inpainting tasks of\ntransparent and reflective objects with robust adaptability.\n", "link": "http://arxiv.org/abs/2410.08567v3", "date": "2025-09-19", "relevancy": 2.8548, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5808}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.566}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Depth%20Inpainting%20for%20Transparent%20and%20Reflective%20Objects&body=Title%3A%20Diffusion-Based%20Depth%20Inpainting%20for%20Transparent%20and%20Reflective%20Objects%0AAuthor%3A%20Tianyu%20Sun%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20Transparent%20and%20reflective%20objects%2C%20which%20are%20common%20in%20our%20everyday%20lives%2C%0Apresent%20a%20significant%20challenge%20to%203D%20imaging%20techniques%20due%20to%20their%20unique%0Avisual%20and%20optical%20properties.%20Faced%20with%20these%20types%20of%20objects%2C%20RGB-D%20cameras%0Afail%20to%20capture%20the%20real%20depth%20value%20with%20their%20accurate%20spatial%20information.%0ATo%20address%20this%20issue%2C%20we%20propose%20DITR%2C%20a%20diffusion-based%20Depth%20Inpainting%0Aframework%20specifically%20designed%20for%20Transparent%20and%20Reflective%20objects.%20This%0Anetwork%20consists%20of%20two%20stages%2C%20including%20a%20Region%20Proposal%20stage%20and%20a%20Depth%0AInpainting%20stage.%20DITR%20dynamically%20analyzes%20the%20optical%20and%20geometric%20depth%0Aloss%20and%20inpaints%20them%20automatically.%20Furthermore%2C%20comprehensive%20experimental%0Aresults%20demonstrate%20that%20DITR%20is%20highly%20effective%20in%20depth%20inpainting%20tasks%20of%0Atransparent%20and%20reflective%20objects%20with%20robust%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08567v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520Depth%2520Inpainting%2520for%2520Transparent%2520and%2520Reflective%2520Objects%26entry.906535625%3DTianyu%2520Sun%2520and%2520Dingchang%2520Hu%2520and%2520Yixiang%2520Dai%2520and%2520Guijin%2520Wang%26entry.1292438233%3D%2520%2520Transparent%2520and%2520reflective%2520objects%252C%2520which%2520are%2520common%2520in%2520our%2520everyday%2520lives%252C%250Apresent%2520a%2520significant%2520challenge%2520to%25203D%2520imaging%2520techniques%2520due%2520to%2520their%2520unique%250Avisual%2520and%2520optical%2520properties.%2520Faced%2520with%2520these%2520types%2520of%2520objects%252C%2520RGB-D%2520cameras%250Afail%2520to%2520capture%2520the%2520real%2520depth%2520value%2520with%2520their%2520accurate%2520spatial%2520information.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520DITR%252C%2520a%2520diffusion-based%2520Depth%2520Inpainting%250Aframework%2520specifically%2520designed%2520for%2520Transparent%2520and%2520Reflective%2520objects.%2520This%250Anetwork%2520consists%2520of%2520two%2520stages%252C%2520including%2520a%2520Region%2520Proposal%2520stage%2520and%2520a%2520Depth%250AInpainting%2520stage.%2520DITR%2520dynamically%2520analyzes%2520the%2520optical%2520and%2520geometric%2520depth%250Aloss%2520and%2520inpaints%2520them%2520automatically.%2520Furthermore%252C%2520comprehensive%2520experimental%250Aresults%2520demonstrate%2520that%2520DITR%2520is%2520highly%2520effective%2520in%2520depth%2520inpainting%2520tasks%2520of%250Atransparent%2520and%2520reflective%2520objects%2520with%2520robust%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08567v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Depth%20Inpainting%20for%20Transparent%20and%20Reflective%20Objects&entry.906535625=Tianyu%20Sun%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang&entry.1292438233=%20%20Transparent%20and%20reflective%20objects%2C%20which%20are%20common%20in%20our%20everyday%20lives%2C%0Apresent%20a%20significant%20challenge%20to%203D%20imaging%20techniques%20due%20to%20their%20unique%0Avisual%20and%20optical%20properties.%20Faced%20with%20these%20types%20of%20objects%2C%20RGB-D%20cameras%0Afail%20to%20capture%20the%20real%20depth%20value%20with%20their%20accurate%20spatial%20information.%0ATo%20address%20this%20issue%2C%20we%20propose%20DITR%2C%20a%20diffusion-based%20Depth%20Inpainting%0Aframework%20specifically%20designed%20for%20Transparent%20and%20Reflective%20objects.%20This%0Anetwork%20consists%20of%20two%20stages%2C%20including%20a%20Region%20Proposal%20stage%20and%20a%20Depth%0AInpainting%20stage.%20DITR%20dynamically%20analyzes%20the%20optical%20and%20geometric%20depth%0Aloss%20and%20inpaints%20them%20automatically.%20Furthermore%2C%20comprehensive%20experimental%0Aresults%20demonstrate%20that%20DITR%20is%20highly%20effective%20in%20depth%20inpainting%20tasks%20of%0Atransparent%20and%20reflective%20objects%20with%20robust%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08567v3&entry.124074799=Read"},
{"title": "LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land\n  Cover Classification from Satellite Imagery and Sparse In-situ Labels", "author": "Johannes Leonhardt and Juergen Gall and Ribana Roscher", "abstract": "  Large-scale land cover maps generated using deep learning play a critical\nrole across a wide range of Earth science applications. Open in-situ datasets\nfrom principled land cover surveys offer a scalable alternative to manual\nannotation for training such models. However, their sparse spatial coverage\noften leads to fragmented and noisy predictions when used with existing deep\nlearning-based land cover mapping approaches. A promising direction to address\nthis issue is object-based classification, which assigns labels to semantically\ncoherent image regions rather than individual pixels, thereby imposing a\nminimum mapping unit. Despite this potential, object-based methods remain\nunderexplored in deep learning-based land cover mapping pipelines, especially\nin the context of medium-resolution imagery and sparse supervision. To address\nthis gap, we propose LC-SLab, the first deep learning framework for\nsystematically exploring object-based deep learning methods for large-scale\nland cover classification under sparse supervision. LC-SLab supports both\ninput-level aggregation via graph neural networks, and output-level aggregation\nby postprocessing results from established semantic segmentation models.\nAdditionally, we incorporate features from a large pre-trained network to\nimprove performance on small datasets. We evaluate the framework on annual\nSentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff\nbetween accuracy and fragmentation, as well as sensitivity to dataset size. Our\nresults show that object-based methods can match or exceed the accuracy of\ncommon pixel-wise models while producing substantially more coherent maps.\nInput-level aggregation proves more robust on smaller datasets, whereas\noutput-level aggregation performs best with more data. Several configurations\nof LC-SLab also outperform existing land cover products, highlighting the\nframework's practical utility.\n", "link": "http://arxiv.org/abs/2509.15868v1", "date": "2025-09-19", "relevancy": 2.8426, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LC-SLab%20--%20An%20Object-based%20Deep%20Learning%20Framework%20for%20Large-scale%20Land%0A%20%20Cover%20Classification%20from%20Satellite%20Imagery%20and%20Sparse%20In-situ%20Labels&body=Title%3A%20LC-SLab%20--%20An%20Object-based%20Deep%20Learning%20Framework%20for%20Large-scale%20Land%0A%20%20Cover%20Classification%20from%20Satellite%20Imagery%20and%20Sparse%20In-situ%20Labels%0AAuthor%3A%20Johannes%20Leonhardt%20and%20Juergen%20Gall%20and%20Ribana%20Roscher%0AAbstract%3A%20%20%20Large-scale%20land%20cover%20maps%20generated%20using%20deep%20learning%20play%20a%20critical%0Arole%20across%20a%20wide%20range%20of%20Earth%20science%20applications.%20Open%20in-situ%20datasets%0Afrom%20principled%20land%20cover%20surveys%20offer%20a%20scalable%20alternative%20to%20manual%0Aannotation%20for%20training%20such%20models.%20However%2C%20their%20sparse%20spatial%20coverage%0Aoften%20leads%20to%20fragmented%20and%20noisy%20predictions%20when%20used%20with%20existing%20deep%0Alearning-based%20land%20cover%20mapping%20approaches.%20A%20promising%20direction%20to%20address%0Athis%20issue%20is%20object-based%20classification%2C%20which%20assigns%20labels%20to%20semantically%0Acoherent%20image%20regions%20rather%20than%20individual%20pixels%2C%20thereby%20imposing%20a%0Aminimum%20mapping%20unit.%20Despite%20this%20potential%2C%20object-based%20methods%20remain%0Aunderexplored%20in%20deep%20learning-based%20land%20cover%20mapping%20pipelines%2C%20especially%0Ain%20the%20context%20of%20medium-resolution%20imagery%20and%20sparse%20supervision.%20To%20address%0Athis%20gap%2C%20we%20propose%20LC-SLab%2C%20the%20first%20deep%20learning%20framework%20for%0Asystematically%20exploring%20object-based%20deep%20learning%20methods%20for%20large-scale%0Aland%20cover%20classification%20under%20sparse%20supervision.%20LC-SLab%20supports%20both%0Ainput-level%20aggregation%20via%20graph%20neural%20networks%2C%20and%20output-level%20aggregation%0Aby%20postprocessing%20results%20from%20established%20semantic%20segmentation%20models.%0AAdditionally%2C%20we%20incorporate%20features%20from%20a%20large%20pre-trained%20network%20to%0Aimprove%20performance%20on%20small%20datasets.%20We%20evaluate%20the%20framework%20on%20annual%0ASentinel-2%20composites%20with%20sparse%20LUCAS%20labels%2C%20focusing%20on%20the%20tradeoff%0Abetween%20accuracy%20and%20fragmentation%2C%20as%20well%20as%20sensitivity%20to%20dataset%20size.%20Our%0Aresults%20show%20that%20object-based%20methods%20can%20match%20or%20exceed%20the%20accuracy%20of%0Acommon%20pixel-wise%20models%20while%20producing%20substantially%20more%20coherent%20maps.%0AInput-level%20aggregation%20proves%20more%20robust%20on%20smaller%20datasets%2C%20whereas%0Aoutput-level%20aggregation%20performs%20best%20with%20more%20data.%20Several%20configurations%0Aof%20LC-SLab%20also%20outperform%20existing%20land%20cover%20products%2C%20highlighting%20the%0Aframework%27s%20practical%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLC-SLab%2520--%2520An%2520Object-based%2520Deep%2520Learning%2520Framework%2520for%2520Large-scale%2520Land%250A%2520%2520Cover%2520Classification%2520from%2520Satellite%2520Imagery%2520and%2520Sparse%2520In-situ%2520Labels%26entry.906535625%3DJohannes%2520Leonhardt%2520and%2520Juergen%2520Gall%2520and%2520Ribana%2520Roscher%26entry.1292438233%3D%2520%2520Large-scale%2520land%2520cover%2520maps%2520generated%2520using%2520deep%2520learning%2520play%2520a%2520critical%250Arole%2520across%2520a%2520wide%2520range%2520of%2520Earth%2520science%2520applications.%2520Open%2520in-situ%2520datasets%250Afrom%2520principled%2520land%2520cover%2520surveys%2520offer%2520a%2520scalable%2520alternative%2520to%2520manual%250Aannotation%2520for%2520training%2520such%2520models.%2520However%252C%2520their%2520sparse%2520spatial%2520coverage%250Aoften%2520leads%2520to%2520fragmented%2520and%2520noisy%2520predictions%2520when%2520used%2520with%2520existing%2520deep%250Alearning-based%2520land%2520cover%2520mapping%2520approaches.%2520A%2520promising%2520direction%2520to%2520address%250Athis%2520issue%2520is%2520object-based%2520classification%252C%2520which%2520assigns%2520labels%2520to%2520semantically%250Acoherent%2520image%2520regions%2520rather%2520than%2520individual%2520pixels%252C%2520thereby%2520imposing%2520a%250Aminimum%2520mapping%2520unit.%2520Despite%2520this%2520potential%252C%2520object-based%2520methods%2520remain%250Aunderexplored%2520in%2520deep%2520learning-based%2520land%2520cover%2520mapping%2520pipelines%252C%2520especially%250Ain%2520the%2520context%2520of%2520medium-resolution%2520imagery%2520and%2520sparse%2520supervision.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520LC-SLab%252C%2520the%2520first%2520deep%2520learning%2520framework%2520for%250Asystematically%2520exploring%2520object-based%2520deep%2520learning%2520methods%2520for%2520large-scale%250Aland%2520cover%2520classification%2520under%2520sparse%2520supervision.%2520LC-SLab%2520supports%2520both%250Ainput-level%2520aggregation%2520via%2520graph%2520neural%2520networks%252C%2520and%2520output-level%2520aggregation%250Aby%2520postprocessing%2520results%2520from%2520established%2520semantic%2520segmentation%2520models.%250AAdditionally%252C%2520we%2520incorporate%2520features%2520from%2520a%2520large%2520pre-trained%2520network%2520to%250Aimprove%2520performance%2520on%2520small%2520datasets.%2520We%2520evaluate%2520the%2520framework%2520on%2520annual%250ASentinel-2%2520composites%2520with%2520sparse%2520LUCAS%2520labels%252C%2520focusing%2520on%2520the%2520tradeoff%250Abetween%2520accuracy%2520and%2520fragmentation%252C%2520as%2520well%2520as%2520sensitivity%2520to%2520dataset%2520size.%2520Our%250Aresults%2520show%2520that%2520object-based%2520methods%2520can%2520match%2520or%2520exceed%2520the%2520accuracy%2520of%250Acommon%2520pixel-wise%2520models%2520while%2520producing%2520substantially%2520more%2520coherent%2520maps.%250AInput-level%2520aggregation%2520proves%2520more%2520robust%2520on%2520smaller%2520datasets%252C%2520whereas%250Aoutput-level%2520aggregation%2520performs%2520best%2520with%2520more%2520data.%2520Several%2520configurations%250Aof%2520LC-SLab%2520also%2520outperform%2520existing%2520land%2520cover%2520products%252C%2520highlighting%2520the%250Aframework%2527s%2520practical%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LC-SLab%20--%20An%20Object-based%20Deep%20Learning%20Framework%20for%20Large-scale%20Land%0A%20%20Cover%20Classification%20from%20Satellite%20Imagery%20and%20Sparse%20In-situ%20Labels&entry.906535625=Johannes%20Leonhardt%20and%20Juergen%20Gall%20and%20Ribana%20Roscher&entry.1292438233=%20%20Large-scale%20land%20cover%20maps%20generated%20using%20deep%20learning%20play%20a%20critical%0Arole%20across%20a%20wide%20range%20of%20Earth%20science%20applications.%20Open%20in-situ%20datasets%0Afrom%20principled%20land%20cover%20surveys%20offer%20a%20scalable%20alternative%20to%20manual%0Aannotation%20for%20training%20such%20models.%20However%2C%20their%20sparse%20spatial%20coverage%0Aoften%20leads%20to%20fragmented%20and%20noisy%20predictions%20when%20used%20with%20existing%20deep%0Alearning-based%20land%20cover%20mapping%20approaches.%20A%20promising%20direction%20to%20address%0Athis%20issue%20is%20object-based%20classification%2C%20which%20assigns%20labels%20to%20semantically%0Acoherent%20image%20regions%20rather%20than%20individual%20pixels%2C%20thereby%20imposing%20a%0Aminimum%20mapping%20unit.%20Despite%20this%20potential%2C%20object-based%20methods%20remain%0Aunderexplored%20in%20deep%20learning-based%20land%20cover%20mapping%20pipelines%2C%20especially%0Ain%20the%20context%20of%20medium-resolution%20imagery%20and%20sparse%20supervision.%20To%20address%0Athis%20gap%2C%20we%20propose%20LC-SLab%2C%20the%20first%20deep%20learning%20framework%20for%0Asystematically%20exploring%20object-based%20deep%20learning%20methods%20for%20large-scale%0Aland%20cover%20classification%20under%20sparse%20supervision.%20LC-SLab%20supports%20both%0Ainput-level%20aggregation%20via%20graph%20neural%20networks%2C%20and%20output-level%20aggregation%0Aby%20postprocessing%20results%20from%20established%20semantic%20segmentation%20models.%0AAdditionally%2C%20we%20incorporate%20features%20from%20a%20large%20pre-trained%20network%20to%0Aimprove%20performance%20on%20small%20datasets.%20We%20evaluate%20the%20framework%20on%20annual%0ASentinel-2%20composites%20with%20sparse%20LUCAS%20labels%2C%20focusing%20on%20the%20tradeoff%0Abetween%20accuracy%20and%20fragmentation%2C%20as%20well%20as%20sensitivity%20to%20dataset%20size.%20Our%0Aresults%20show%20that%20object-based%20methods%20can%20match%20or%20exceed%20the%20accuracy%20of%0Acommon%20pixel-wise%20models%20while%20producing%20substantially%20more%20coherent%20maps.%0AInput-level%20aggregation%20proves%20more%20robust%20on%20smaller%20datasets%2C%20whereas%0Aoutput-level%20aggregation%20performs%20best%20with%20more%20data.%20Several%20configurations%0Aof%20LC-SLab%20also%20outperform%20existing%20land%20cover%20products%2C%20highlighting%20the%0Aframework%27s%20practical%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15868v1&entry.124074799=Read"},
{"title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs", "author": "Jinghao Zhang and Sihang Jiang and Shiwei Guo and Shisong Chen and Yanghua Xiao and Hongwei Feng and Jiaqing Liang and Minggui HE and Shimin Tao and Hongxia Ma", "abstract": "  As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture\n", "link": "http://arxiv.org/abs/2509.16188v1", "date": "2025-09-19", "relevancy": 2.7999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CultureScope%3A%20A%20Dimensional%20Lens%20for%20Probing%20Cultural%20Understanding%20in%0A%20%20LLMs&body=Title%3A%20CultureScope%3A%20A%20Dimensional%20Lens%20for%20Probing%20Cultural%20Understanding%20in%0A%20%20LLMs%0AAuthor%3A%20Jinghao%20Zhang%20and%20Sihang%20Jiang%20and%20Shiwei%20Guo%20and%20Shisong%20Chen%20and%20Yanghua%20Xiao%20and%20Hongwei%20Feng%20and%20Jiaqing%20Liang%20and%20Minggui%20HE%20and%20Shimin%20Tao%20and%20Hongxia%20Ma%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20diverse%20cultural%0Aenvironments%2C%20evaluating%20their%20cultural%20understanding%20capability%20has%20become%0Aessential%20for%20ensuring%20trustworthy%20and%20culturally%20aligned%20applications.%0AHowever%2C%20most%20existing%20benchmarks%20lack%20comprehensiveness%20and%20are%20challenging%20to%0Ascale%20and%20adapt%20across%20different%20cultural%20contexts%2C%20because%20their%20frameworks%0Aoften%20lack%20guidance%20from%20well-established%20cultural%20theories%20and%20tend%20to%20rely%20on%0Aexpert-driven%20manual%20annotations.%20To%20address%20these%20issues%2C%20we%20propose%0ACultureScope%2C%20the%20most%20comprehensive%20evaluation%20framework%20to%20date%20for%20assessing%0Acultural%20understanding%20in%20LLMs.%20Inspired%20by%20the%20cultural%20iceberg%20theory%2C%20we%0Adesign%20a%20novel%20dimensional%20schema%20for%20cultural%20knowledge%20classification%2C%0Acomprising%203%20layers%20and%20140%20dimensions%2C%20which%20guides%20the%20automated%20construction%0Aof%20culture-specific%20knowledge%20bases%20and%20corresponding%20evaluation%20datasets%20for%0Aany%20given%20languages%20and%20cultures.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20can%20effectively%20evaluate%20cultural%20understanding.%20They%20also%20reveal%20that%0Aexisting%20large%20language%20models%20lack%20comprehensive%20cultural%20competence%2C%20and%0Amerely%20incorporating%20multilingual%20data%20does%20not%20necessarily%20enhance%20cultural%0Aunderstanding.%20All%20code%20and%20data%20files%20are%20available%20at%0Ahttps%3A//github.com/HoganZinger/Culture%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCultureScope%253A%2520A%2520Dimensional%2520Lens%2520for%2520Probing%2520Cultural%2520Understanding%2520in%250A%2520%2520LLMs%26entry.906535625%3DJinghao%2520Zhang%2520and%2520Sihang%2520Jiang%2520and%2520Shiwei%2520Guo%2520and%2520Shisong%2520Chen%2520and%2520Yanghua%2520Xiao%2520and%2520Hongwei%2520Feng%2520and%2520Jiaqing%2520Liang%2520and%2520Minggui%2520HE%2520and%2520Shimin%2520Tao%2520and%2520Hongxia%2520Ma%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520diverse%2520cultural%250Aenvironments%252C%2520evaluating%2520their%2520cultural%2520understanding%2520capability%2520has%2520become%250Aessential%2520for%2520ensuring%2520trustworthy%2520and%2520culturally%2520aligned%2520applications.%250AHowever%252C%2520most%2520existing%2520benchmarks%2520lack%2520comprehensiveness%2520and%2520are%2520challenging%2520to%250Ascale%2520and%2520adapt%2520across%2520different%2520cultural%2520contexts%252C%2520because%2520their%2520frameworks%250Aoften%2520lack%2520guidance%2520from%2520well-established%2520cultural%2520theories%2520and%2520tend%2520to%2520rely%2520on%250Aexpert-driven%2520manual%2520annotations.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ACultureScope%252C%2520the%2520most%2520comprehensive%2520evaluation%2520framework%2520to%2520date%2520for%2520assessing%250Acultural%2520understanding%2520in%2520LLMs.%2520Inspired%2520by%2520the%2520cultural%2520iceberg%2520theory%252C%2520we%250Adesign%2520a%2520novel%2520dimensional%2520schema%2520for%2520cultural%2520knowledge%2520classification%252C%250Acomprising%25203%2520layers%2520and%2520140%2520dimensions%252C%2520which%2520guides%2520the%2520automated%2520construction%250Aof%2520culture-specific%2520knowledge%2520bases%2520and%2520corresponding%2520evaluation%2520datasets%2520for%250Aany%2520given%2520languages%2520and%2520cultures.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520can%2520effectively%2520evaluate%2520cultural%2520understanding.%2520They%2520also%2520reveal%2520that%250Aexisting%2520large%2520language%2520models%2520lack%2520comprehensive%2520cultural%2520competence%252C%2520and%250Amerely%2520incorporating%2520multilingual%2520data%2520does%2520not%2520necessarily%2520enhance%2520cultural%250Aunderstanding.%2520All%2520code%2520and%2520data%2520files%2520are%2520available%2520at%250Ahttps%253A//github.com/HoganZinger/Culture%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CultureScope%3A%20A%20Dimensional%20Lens%20for%20Probing%20Cultural%20Understanding%20in%0A%20%20LLMs&entry.906535625=Jinghao%20Zhang%20and%20Sihang%20Jiang%20and%20Shiwei%20Guo%20and%20Shisong%20Chen%20and%20Yanghua%20Xiao%20and%20Hongwei%20Feng%20and%20Jiaqing%20Liang%20and%20Minggui%20HE%20and%20Shimin%20Tao%20and%20Hongxia%20Ma&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20diverse%20cultural%0Aenvironments%2C%20evaluating%20their%20cultural%20understanding%20capability%20has%20become%0Aessential%20for%20ensuring%20trustworthy%20and%20culturally%20aligned%20applications.%0AHowever%2C%20most%20existing%20benchmarks%20lack%20comprehensiveness%20and%20are%20challenging%20to%0Ascale%20and%20adapt%20across%20different%20cultural%20contexts%2C%20because%20their%20frameworks%0Aoften%20lack%20guidance%20from%20well-established%20cultural%20theories%20and%20tend%20to%20rely%20on%0Aexpert-driven%20manual%20annotations.%20To%20address%20these%20issues%2C%20we%20propose%0ACultureScope%2C%20the%20most%20comprehensive%20evaluation%20framework%20to%20date%20for%20assessing%0Acultural%20understanding%20in%20LLMs.%20Inspired%20by%20the%20cultural%20iceberg%20theory%2C%20we%0Adesign%20a%20novel%20dimensional%20schema%20for%20cultural%20knowledge%20classification%2C%0Acomprising%203%20layers%20and%20140%20dimensions%2C%20which%20guides%20the%20automated%20construction%0Aof%20culture-specific%20knowledge%20bases%20and%20corresponding%20evaluation%20datasets%20for%0Aany%20given%20languages%20and%20cultures.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20can%20effectively%20evaluate%20cultural%20understanding.%20They%20also%20reveal%20that%0Aexisting%20large%20language%20models%20lack%20comprehensive%20cultural%20competence%2C%20and%0Amerely%20incorporating%20multilingual%20data%20does%20not%20necessarily%20enhance%20cultural%0Aunderstanding.%20All%20code%20and%20data%20files%20are%20available%20at%0Ahttps%3A//github.com/HoganZinger/Culture%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16188v1&entry.124074799=Read"},
{"title": "GLip: A Global-Local Integrated Progressive Framework for Robust Visual\n  Speech Recognition", "author": "Tianyue Wang and Shuang Yang and Shiguang Shan and Xilin Chen", "abstract": "  Visual speech recognition (VSR), also known as lip reading, is the task of\nrecognizing speech from silent video. Despite significant advancements in VSR\nover recent decades, most existing methods pay limited attention to real-world\nvisual challenges such as illumination variations, occlusions, blurring, and\npose changes. To address these challenges, we propose GLip, a Global-Local\nIntegrated Progressive framework designed for robust VSR. GLip is built upon\ntwo key insights: (i) learning an initial \\textit{coarse} alignment between\nvisual features across varying conditions and corresponding speech content\nfacilitates the subsequent learning of \\textit{precise} visual-to-speech\nmappings in challenging environments; (ii) under adverse conditions, certain\nlocal regions (e.g., non-occluded areas) often exhibit more discriminative cues\nfor lip reading than global features. To this end, GLip introduces a dual-path\nfeature extraction architecture that integrates both global and local features\nwithin a two-stage progressive learning framework. In the first stage, the\nmodel learns to align both global and local visual features with corresponding\nacoustic speech units using easily accessible audio-visual data, establishing a\ncoarse yet semantically robust foundation. In the second stage, we introduce a\nContextual Enhancement Module (CEM) to dynamically integrate local features\nwith relevant global context across both spatial and temporal dimensions,\nrefining the coarse representations into precise visual-speech mappings. Our\nframework uniquely exploits discriminative local regions through a progressive\nlearning strategy, demonstrating enhanced robustness against various visual\nchallenges and consistently outperforming existing methods on the LRS2 and LRS3\nbenchmarks. We further validate its effectiveness on a newly introduced\nchallenging Mandarin dataset.\n", "link": "http://arxiv.org/abs/2509.16031v1", "date": "2025-09-19", "relevancy": 2.7969, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition&body=Title%3A%20GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition%0AAuthor%3A%20Tianyue%20Wang%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20also%20known%20as%20lip%20reading%2C%20is%20the%20task%20of%0Arecognizing%20speech%20from%20silent%20video.%20Despite%20significant%20advancements%20in%20VSR%0Aover%20recent%20decades%2C%20most%20existing%20methods%20pay%20limited%20attention%20to%20real-world%0Avisual%20challenges%20such%20as%20illumination%20variations%2C%20occlusions%2C%20blurring%2C%20and%0Apose%20changes.%20To%20address%20these%20challenges%2C%20we%20propose%20GLip%2C%20a%20Global-Local%0AIntegrated%20Progressive%20framework%20designed%20for%20robust%20VSR.%20GLip%20is%20built%20upon%0Atwo%20key%20insights%3A%20%28i%29%20learning%20an%20initial%20%5Ctextit%7Bcoarse%7D%20alignment%20between%0Avisual%20features%20across%20varying%20conditions%20and%20corresponding%20speech%20content%0Afacilitates%20the%20subsequent%20learning%20of%20%5Ctextit%7Bprecise%7D%20visual-to-speech%0Amappings%20in%20challenging%20environments%3B%20%28ii%29%20under%20adverse%20conditions%2C%20certain%0Alocal%20regions%20%28e.g.%2C%20non-occluded%20areas%29%20often%20exhibit%20more%20discriminative%20cues%0Afor%20lip%20reading%20than%20global%20features.%20To%20this%20end%2C%20GLip%20introduces%20a%20dual-path%0Afeature%20extraction%20architecture%20that%20integrates%20both%20global%20and%20local%20features%0Awithin%20a%20two-stage%20progressive%20learning%20framework.%20In%20the%20first%20stage%2C%20the%0Amodel%20learns%20to%20align%20both%20global%20and%20local%20visual%20features%20with%20corresponding%0Aacoustic%20speech%20units%20using%20easily%20accessible%20audio-visual%20data%2C%20establishing%20a%0Acoarse%20yet%20semantically%20robust%20foundation.%20In%20the%20second%20stage%2C%20we%20introduce%20a%0AContextual%20Enhancement%20Module%20%28CEM%29%20to%20dynamically%20integrate%20local%20features%0Awith%20relevant%20global%20context%20across%20both%20spatial%20and%20temporal%20dimensions%2C%0Arefining%20the%20coarse%20representations%20into%20precise%20visual-speech%20mappings.%20Our%0Aframework%20uniquely%20exploits%20discriminative%20local%20regions%20through%20a%20progressive%0Alearning%20strategy%2C%20demonstrating%20enhanced%20robustness%20against%20various%20visual%0Achallenges%20and%20consistently%20outperforming%20existing%20methods%20on%20the%20LRS2%20and%20LRS3%0Abenchmarks.%20We%20further%20validate%20its%20effectiveness%20on%20a%20newly%20introduced%0Achallenging%20Mandarin%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLip%253A%2520A%2520Global-Local%2520Integrated%2520Progressive%2520Framework%2520for%2520Robust%2520Visual%250A%2520%2520Speech%2520Recognition%26entry.906535625%3DTianyue%2520Wang%2520and%2520Shuang%2520Yang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520speech%2520recognition%2520%2528VSR%2529%252C%2520also%2520known%2520as%2520lip%2520reading%252C%2520is%2520the%2520task%2520of%250Arecognizing%2520speech%2520from%2520silent%2520video.%2520Despite%2520significant%2520advancements%2520in%2520VSR%250Aover%2520recent%2520decades%252C%2520most%2520existing%2520methods%2520pay%2520limited%2520attention%2520to%2520real-world%250Avisual%2520challenges%2520such%2520as%2520illumination%2520variations%252C%2520occlusions%252C%2520blurring%252C%2520and%250Apose%2520changes.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GLip%252C%2520a%2520Global-Local%250AIntegrated%2520Progressive%2520framework%2520designed%2520for%2520robust%2520VSR.%2520GLip%2520is%2520built%2520upon%250Atwo%2520key%2520insights%253A%2520%2528i%2529%2520learning%2520an%2520initial%2520%255Ctextit%257Bcoarse%257D%2520alignment%2520between%250Avisual%2520features%2520across%2520varying%2520conditions%2520and%2520corresponding%2520speech%2520content%250Afacilitates%2520the%2520subsequent%2520learning%2520of%2520%255Ctextit%257Bprecise%257D%2520visual-to-speech%250Amappings%2520in%2520challenging%2520environments%253B%2520%2528ii%2529%2520under%2520adverse%2520conditions%252C%2520certain%250Alocal%2520regions%2520%2528e.g.%252C%2520non-occluded%2520areas%2529%2520often%2520exhibit%2520more%2520discriminative%2520cues%250Afor%2520lip%2520reading%2520than%2520global%2520features.%2520To%2520this%2520end%252C%2520GLip%2520introduces%2520a%2520dual-path%250Afeature%2520extraction%2520architecture%2520that%2520integrates%2520both%2520global%2520and%2520local%2520features%250Awithin%2520a%2520two-stage%2520progressive%2520learning%2520framework.%2520In%2520the%2520first%2520stage%252C%2520the%250Amodel%2520learns%2520to%2520align%2520both%2520global%2520and%2520local%2520visual%2520features%2520with%2520corresponding%250Aacoustic%2520speech%2520units%2520using%2520easily%2520accessible%2520audio-visual%2520data%252C%2520establishing%2520a%250Acoarse%2520yet%2520semantically%2520robust%2520foundation.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%250AContextual%2520Enhancement%2520Module%2520%2528CEM%2529%2520to%2520dynamically%2520integrate%2520local%2520features%250Awith%2520relevant%2520global%2520context%2520across%2520both%2520spatial%2520and%2520temporal%2520dimensions%252C%250Arefining%2520the%2520coarse%2520representations%2520into%2520precise%2520visual-speech%2520mappings.%2520Our%250Aframework%2520uniquely%2520exploits%2520discriminative%2520local%2520regions%2520through%2520a%2520progressive%250Alearning%2520strategy%252C%2520demonstrating%2520enhanced%2520robustness%2520against%2520various%2520visual%250Achallenges%2520and%2520consistently%2520outperforming%2520existing%2520methods%2520on%2520the%2520LRS2%2520and%2520LRS3%250Abenchmarks.%2520We%2520further%2520validate%2520its%2520effectiveness%2520on%2520a%2520newly%2520introduced%250Achallenging%2520Mandarin%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition&entry.906535625=Tianyue%20Wang%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20also%20known%20as%20lip%20reading%2C%20is%20the%20task%20of%0Arecognizing%20speech%20from%20silent%20video.%20Despite%20significant%20advancements%20in%20VSR%0Aover%20recent%20decades%2C%20most%20existing%20methods%20pay%20limited%20attention%20to%20real-world%0Avisual%20challenges%20such%20as%20illumination%20variations%2C%20occlusions%2C%20blurring%2C%20and%0Apose%20changes.%20To%20address%20these%20challenges%2C%20we%20propose%20GLip%2C%20a%20Global-Local%0AIntegrated%20Progressive%20framework%20designed%20for%20robust%20VSR.%20GLip%20is%20built%20upon%0Atwo%20key%20insights%3A%20%28i%29%20learning%20an%20initial%20%5Ctextit%7Bcoarse%7D%20alignment%20between%0Avisual%20features%20across%20varying%20conditions%20and%20corresponding%20speech%20content%0Afacilitates%20the%20subsequent%20learning%20of%20%5Ctextit%7Bprecise%7D%20visual-to-speech%0Amappings%20in%20challenging%20environments%3B%20%28ii%29%20under%20adverse%20conditions%2C%20certain%0Alocal%20regions%20%28e.g.%2C%20non-occluded%20areas%29%20often%20exhibit%20more%20discriminative%20cues%0Afor%20lip%20reading%20than%20global%20features.%20To%20this%20end%2C%20GLip%20introduces%20a%20dual-path%0Afeature%20extraction%20architecture%20that%20integrates%20both%20global%20and%20local%20features%0Awithin%20a%20two-stage%20progressive%20learning%20framework.%20In%20the%20first%20stage%2C%20the%0Amodel%20learns%20to%20align%20both%20global%20and%20local%20visual%20features%20with%20corresponding%0Aacoustic%20speech%20units%20using%20easily%20accessible%20audio-visual%20data%2C%20establishing%20a%0Acoarse%20yet%20semantically%20robust%20foundation.%20In%20the%20second%20stage%2C%20we%20introduce%20a%0AContextual%20Enhancement%20Module%20%28CEM%29%20to%20dynamically%20integrate%20local%20features%0Awith%20relevant%20global%20context%20across%20both%20spatial%20and%20temporal%20dimensions%2C%0Arefining%20the%20coarse%20representations%20into%20precise%20visual-speech%20mappings.%20Our%0Aframework%20uniquely%20exploits%20discriminative%20local%20regions%20through%20a%20progressive%0Alearning%20strategy%2C%20demonstrating%20enhanced%20robustness%20against%20various%20visual%0Achallenges%20and%20consistently%20outperforming%20existing%20methods%20on%20the%20LRS2%20and%20LRS3%0Abenchmarks.%20We%20further%20validate%20its%20effectiveness%20on%20a%20newly%20introduced%0Achallenging%20Mandarin%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16031v1&entry.124074799=Read"},
{"title": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation", "author": "Zhen Han and Mattias Teye and Derek Yadgaroff and Judith B\u00fctepage", "abstract": "  The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.\n", "link": "http://arxiv.org/abs/2507.18352v2", "date": "2025-09-19", "relevancy": 2.7945, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5855}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation&body=Title%3A%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation%0AAuthor%3A%20Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage%0AAbstract%3A%20%20%20The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%0Aspeech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%0Ahigh-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%0Arecent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%0Avariations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%0Ato%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%0Aresulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%0Aonly%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%0Aon-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%0Adevelopment.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%0Adistillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%0Ahigh-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%0Ato%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%0Aconvolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%0Acontext%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%0Areduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%0Ato%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%0Afor%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%0Adigital%20characters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520is%2520not%2520small%2520enough%253A%2520High-quality%252C%2520low-resource%2520facial%2520animation%250A%2520%2520models%2520through%2520hybrid%2520knowledge%2520distillation%26entry.906535625%3DZhen%2520Han%2520and%2520Mattias%2520Teye%2520and%2520Derek%2520Yadgaroff%2520and%2520Judith%2520B%25C3%25BCtepage%26entry.1292438233%3D%2520%2520The%2520training%2520of%2520high-quality%252C%2520robust%2520machine%2520learning%2520models%2520for%250Aspeech-driven%25203D%2520facial%2520animation%2520requires%2520a%2520large%252C%2520diverse%2520dataset%2520of%250Ahigh-quality%2520audio-animation%2520pairs.%2520To%2520overcome%2520the%2520lack%2520of%2520such%2520a%2520dataset%252C%250Arecent%2520work%2520has%2520introduced%2520large%2520pre-trained%2520speech%2520encoders%2520that%2520are%2520robust%2520to%250Avariations%2520in%2520the%2520input%2520audio%2520and%252C%2520therefore%252C%2520enable%2520the%2520facial%2520animation%2520model%250Ato%2520generalize%2520across%2520speakers%252C%2520audio%2520quality%252C%2520and%2520languages.%2520However%252C%2520the%250Aresulting%2520facial%2520animation%2520models%2520are%2520prohibitively%2520large%2520and%2520lend%2520themselves%250Aonly%2520to%2520offline%2520inference%2520on%2520a%2520dedicated%2520machine.%2520In%2520this%2520work%252C%2520we%2520explore%250Aon-device%252C%2520real-time%2520facial%2520animation%2520models%2520in%2520the%2520context%2520of%2520game%250Adevelopment.%2520We%2520overcome%2520the%2520lack%2520of%2520large%2520datasets%2520by%2520using%2520hybrid%2520knowledge%250Adistillation%2520with%2520pseudo-labeling.%2520Given%2520a%2520large%2520audio%2520dataset%252C%2520we%2520employ%2520a%250Ahigh-performing%2520teacher%2520model%2520to%2520train%2520very%2520small%2520student%2520models.%2520In%2520contrast%250Ato%2520the%2520pre-trained%2520speech%2520encoders%252C%2520our%2520student%2520models%2520only%2520consist%2520of%250Aconvolutional%2520and%2520fully-connected%2520layers%252C%2520removing%2520the%2520need%2520for%2520attention%250Acontext%2520or%2520recurrent%2520updates.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%2520we%2520can%250Areduce%2520the%2520memory%2520footprint%2520to%2520up%2520to%25203.4%2520MB%2520and%2520required%2520future%2520audio%2520context%250Ato%2520up%2520to%252081%2520ms%2520while%2520maintaining%2520high-quality%2520animations.%2520This%2520paves%2520the%2520way%250Afor%2520on-device%2520inference%252C%2520an%2520important%2520step%2520towards%2520realistic%252C%2520model-driven%250Adigital%2520characters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation&entry.906535625=Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage&entry.1292438233=%20%20The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%0Aspeech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%0Ahigh-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%0Arecent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%0Avariations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%0Ato%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%0Aresulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%0Aonly%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%0Aon-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%0Adevelopment.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%0Adistillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%0Ahigh-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%0Ato%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%0Aconvolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%0Acontext%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%0Areduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%0Ato%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%0Afor%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%0Adigital%20characters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18352v2&entry.124074799=Read"},
{"title": "Global Regulation and Excitation via Attention Tuning for Stereo\n  Matching", "author": "Jiahao Li and Xinhong Chen and Zhengmin Jiang and Qian Zhou and Yung-Hui Li and Jianping Wang", "abstract": "  Stereo matching achieves significant progress with iterative algorithms like\nRAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed\nregions with occlusions, textureless, or repetitive patterns, due to a lack of\nglobal context and geometric information for effective iterative refinement. To\nenable the existing iterative approaches to incorporate global context, we\npropose the Global Regulation and Excitation via Attention Tuning (GREAT)\nframework which encompasses three attention modules. Specifically, Spatial\nAttention (SA) captures the global context within the spatial dimension,\nMatching Attention (MA) extracts global context along epipolar lines, and\nVolume Attention (VA) works in conjunction with SA and MA to construct a more\nrobust cost-volume excited by global context and geometric details. To verify\nthe universality and effectiveness of this framework, we integrate it into\nseveral representative iterative stereo-matching methods and validate it\nthrough extensive experiments, collectively denoted as GREAT-Stereo. This\nframework demonstrates superior performance in challenging ill-posed regions.\nApplied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first\non the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves\nsecond on the Middlebury benchmark. Code is available at\nhttps://github.com/JarvisLee0423/GREAT-Stereo.\n", "link": "http://arxiv.org/abs/2509.15891v1", "date": "2025-09-19", "relevancy": 2.7901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Regulation%20and%20Excitation%20via%20Attention%20Tuning%20for%20Stereo%0A%20%20Matching&body=Title%3A%20Global%20Regulation%20and%20Excitation%20via%20Attention%20Tuning%20for%20Stereo%0A%20%20Matching%0AAuthor%3A%20Jiahao%20Li%20and%20Xinhong%20Chen%20and%20Zhengmin%20Jiang%20and%20Qian%20Zhou%20and%20Yung-Hui%20Li%20and%20Jianping%20Wang%0AAbstract%3A%20%20%20Stereo%20matching%20achieves%20significant%20progress%20with%20iterative%20algorithms%20like%0ARAFT-Stereo%20and%20IGEV-Stereo.%20However%2C%20these%20methods%20struggle%20in%20ill-posed%0Aregions%20with%20occlusions%2C%20textureless%2C%20or%20repetitive%20patterns%2C%20due%20to%20a%20lack%20of%0Aglobal%20context%20and%20geometric%20information%20for%20effective%20iterative%20refinement.%20To%0Aenable%20the%20existing%20iterative%20approaches%20to%20incorporate%20global%20context%2C%20we%0Apropose%20the%20Global%20Regulation%20and%20Excitation%20via%20Attention%20Tuning%20%28GREAT%29%0Aframework%20which%20encompasses%20three%20attention%20modules.%20Specifically%2C%20Spatial%0AAttention%20%28SA%29%20captures%20the%20global%20context%20within%20the%20spatial%20dimension%2C%0AMatching%20Attention%20%28MA%29%20extracts%20global%20context%20along%20epipolar%20lines%2C%20and%0AVolume%20Attention%20%28VA%29%20works%20in%20conjunction%20with%20SA%20and%20MA%20to%20construct%20a%20more%0Arobust%20cost-volume%20excited%20by%20global%20context%20and%20geometric%20details.%20To%20verify%0Athe%20universality%20and%20effectiveness%20of%20this%20framework%2C%20we%20integrate%20it%20into%0Aseveral%20representative%20iterative%20stereo-matching%20methods%20and%20validate%20it%0Athrough%20extensive%20experiments%2C%20collectively%20denoted%20as%20GREAT-Stereo.%20This%0Aframework%20demonstrates%20superior%20performance%20in%20challenging%20ill-posed%20regions.%0AApplied%20to%20IGEV-Stereo%2C%20among%20all%20published%20methods%2C%20our%20GREAT-IGEV%20ranks%20first%0Aon%20the%20Scene%20Flow%20test%20set%2C%20KITTI%202015%2C%20and%20ETH3D%20leaderboards%2C%20and%20achieves%0Asecond%20on%20the%20Middlebury%20benchmark.%20Code%20is%20available%20at%0Ahttps%3A//github.com/JarvisLee0423/GREAT-Stereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Regulation%2520and%2520Excitation%2520via%2520Attention%2520Tuning%2520for%2520Stereo%250A%2520%2520Matching%26entry.906535625%3DJiahao%2520Li%2520and%2520Xinhong%2520Chen%2520and%2520Zhengmin%2520Jiang%2520and%2520Qian%2520Zhou%2520and%2520Yung-Hui%2520Li%2520and%2520Jianping%2520Wang%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520achieves%2520significant%2520progress%2520with%2520iterative%2520algorithms%2520like%250ARAFT-Stereo%2520and%2520IGEV-Stereo.%2520However%252C%2520these%2520methods%2520struggle%2520in%2520ill-posed%250Aregions%2520with%2520occlusions%252C%2520textureless%252C%2520or%2520repetitive%2520patterns%252C%2520due%2520to%2520a%2520lack%2520of%250Aglobal%2520context%2520and%2520geometric%2520information%2520for%2520effective%2520iterative%2520refinement.%2520To%250Aenable%2520the%2520existing%2520iterative%2520approaches%2520to%2520incorporate%2520global%2520context%252C%2520we%250Apropose%2520the%2520Global%2520Regulation%2520and%2520Excitation%2520via%2520Attention%2520Tuning%2520%2528GREAT%2529%250Aframework%2520which%2520encompasses%2520three%2520attention%2520modules.%2520Specifically%252C%2520Spatial%250AAttention%2520%2528SA%2529%2520captures%2520the%2520global%2520context%2520within%2520the%2520spatial%2520dimension%252C%250AMatching%2520Attention%2520%2528MA%2529%2520extracts%2520global%2520context%2520along%2520epipolar%2520lines%252C%2520and%250AVolume%2520Attention%2520%2528VA%2529%2520works%2520in%2520conjunction%2520with%2520SA%2520and%2520MA%2520to%2520construct%2520a%2520more%250Arobust%2520cost-volume%2520excited%2520by%2520global%2520context%2520and%2520geometric%2520details.%2520To%2520verify%250Athe%2520universality%2520and%2520effectiveness%2520of%2520this%2520framework%252C%2520we%2520integrate%2520it%2520into%250Aseveral%2520representative%2520iterative%2520stereo-matching%2520methods%2520and%2520validate%2520it%250Athrough%2520extensive%2520experiments%252C%2520collectively%2520denoted%2520as%2520GREAT-Stereo.%2520This%250Aframework%2520demonstrates%2520superior%2520performance%2520in%2520challenging%2520ill-posed%2520regions.%250AApplied%2520to%2520IGEV-Stereo%252C%2520among%2520all%2520published%2520methods%252C%2520our%2520GREAT-IGEV%2520ranks%2520first%250Aon%2520the%2520Scene%2520Flow%2520test%2520set%252C%2520KITTI%25202015%252C%2520and%2520ETH3D%2520leaderboards%252C%2520and%2520achieves%250Asecond%2520on%2520the%2520Middlebury%2520benchmark.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/JarvisLee0423/GREAT-Stereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Regulation%20and%20Excitation%20via%20Attention%20Tuning%20for%20Stereo%0A%20%20Matching&entry.906535625=Jiahao%20Li%20and%20Xinhong%20Chen%20and%20Zhengmin%20Jiang%20and%20Qian%20Zhou%20and%20Yung-Hui%20Li%20and%20Jianping%20Wang&entry.1292438233=%20%20Stereo%20matching%20achieves%20significant%20progress%20with%20iterative%20algorithms%20like%0ARAFT-Stereo%20and%20IGEV-Stereo.%20However%2C%20these%20methods%20struggle%20in%20ill-posed%0Aregions%20with%20occlusions%2C%20textureless%2C%20or%20repetitive%20patterns%2C%20due%20to%20a%20lack%20of%0Aglobal%20context%20and%20geometric%20information%20for%20effective%20iterative%20refinement.%20To%0Aenable%20the%20existing%20iterative%20approaches%20to%20incorporate%20global%20context%2C%20we%0Apropose%20the%20Global%20Regulation%20and%20Excitation%20via%20Attention%20Tuning%20%28GREAT%29%0Aframework%20which%20encompasses%20three%20attention%20modules.%20Specifically%2C%20Spatial%0AAttention%20%28SA%29%20captures%20the%20global%20context%20within%20the%20spatial%20dimension%2C%0AMatching%20Attention%20%28MA%29%20extracts%20global%20context%20along%20epipolar%20lines%2C%20and%0AVolume%20Attention%20%28VA%29%20works%20in%20conjunction%20with%20SA%20and%20MA%20to%20construct%20a%20more%0Arobust%20cost-volume%20excited%20by%20global%20context%20and%20geometric%20details.%20To%20verify%0Athe%20universality%20and%20effectiveness%20of%20this%20framework%2C%20we%20integrate%20it%20into%0Aseveral%20representative%20iterative%20stereo-matching%20methods%20and%20validate%20it%0Athrough%20extensive%20experiments%2C%20collectively%20denoted%20as%20GREAT-Stereo.%20This%0Aframework%20demonstrates%20superior%20performance%20in%20challenging%20ill-posed%20regions.%0AApplied%20to%20IGEV-Stereo%2C%20among%20all%20published%20methods%2C%20our%20GREAT-IGEV%20ranks%20first%0Aon%20the%20Scene%20Flow%20test%20set%2C%20KITTI%202015%2C%20and%20ETH3D%20leaderboards%2C%20and%20achieves%0Asecond%20on%20the%20Middlebury%20benchmark.%20Code%20is%20available%20at%0Ahttps%3A//github.com/JarvisLee0423/GREAT-Stereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15891v1&entry.124074799=Read"},
{"title": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds", "author": "Remo Sasso and Michelangelo Conserva and Dominik Jeurissen and Paulo Rauber", "abstract": "  While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.\n", "link": "http://arxiv.org/abs/2509.15915v1", "date": "2025-09-19", "relevancy": 2.7738, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20as%20World%20Models%3A%20A%20Foundational%20Study%20in%20Text-Based%0A%20%20GridWorlds&body=Title%3A%20Foundation%20Models%20as%20World%20Models%3A%20A%20Foundational%20Study%20in%20Text-Based%0A%20%20GridWorlds%0AAuthor%3A%20Remo%20Sasso%20and%20Michelangelo%20Conserva%20and%20Dominik%20Jeurissen%20and%20Paulo%20Rauber%0AAbstract%3A%20%20%20While%20reinforcement%20learning%20from%20scratch%20has%20shown%20impressive%20results%20in%0Asolving%20sequential%20decision-making%20tasks%20with%20efficient%20simulators%2C%20real-world%0Aapplications%20with%20expensive%20interactions%20require%20more%20sample-efficient%20agents.%0AFoundation%20models%20%28FMs%29%20are%20natural%20candidates%20to%20improve%20sample%20efficiency%20as%0Athey%20possess%20broad%20knowledge%20and%20reasoning%20capabilities%2C%20but%20it%20is%20yet%20unclear%0Ahow%20to%20effectively%20integrate%20them%20into%20the%20reinforcement%20learning%20framework.%20In%0Athis%20paper%2C%20we%20anticipate%20and%2C%20most%20importantly%2C%20evaluate%20two%20promising%0Astrategies.%20First%2C%20we%20consider%20the%20use%20of%20foundation%20world%20models%20%28FWMs%29%20that%0Aexploit%20the%20prior%20knowledge%20of%20FMs%20to%20enable%20training%20and%20evaluating%20agents%0Awith%20simulated%20interactions.%20Second%2C%20we%20consider%20the%20use%20of%20foundation%20agents%0A%28FAs%29%20that%20exploit%20the%20reasoning%20capabilities%20of%20FMs%20for%20decision-making.%20We%0Aevaluate%20both%20approaches%20empirically%20in%20a%20family%20of%20grid-world%20environments%0Athat%20are%20suitable%20for%20the%20current%20generation%20of%20large%20language%20models%20%28LLMs%29.%0AOur%20results%20suggest%20that%20improvements%20in%20LLMs%20already%20translate%20into%20better%0AFWMs%20and%20FAs%3B%20that%20FAs%20based%20on%20current%20LLMs%20can%20already%20provide%20excellent%0Apolicies%20for%20sufficiently%20simple%20environments%3B%20and%20that%20the%20coupling%20of%20FWMs%0Aand%20reinforcement%20learning%20agents%20is%20highly%20promising%20for%20more%20complex%20settings%0Awith%20partial%20observability%20and%20stochastic%20elements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520as%2520World%2520Models%253A%2520A%2520Foundational%2520Study%2520in%2520Text-Based%250A%2520%2520GridWorlds%26entry.906535625%3DRemo%2520Sasso%2520and%2520Michelangelo%2520Conserva%2520and%2520Dominik%2520Jeurissen%2520and%2520Paulo%2520Rauber%26entry.1292438233%3D%2520%2520While%2520reinforcement%2520learning%2520from%2520scratch%2520has%2520shown%2520impressive%2520results%2520in%250Asolving%2520sequential%2520decision-making%2520tasks%2520with%2520efficient%2520simulators%252C%2520real-world%250Aapplications%2520with%2520expensive%2520interactions%2520require%2520more%2520sample-efficient%2520agents.%250AFoundation%2520models%2520%2528FMs%2529%2520are%2520natural%2520candidates%2520to%2520improve%2520sample%2520efficiency%2520as%250Athey%2520possess%2520broad%2520knowledge%2520and%2520reasoning%2520capabilities%252C%2520but%2520it%2520is%2520yet%2520unclear%250Ahow%2520to%2520effectively%2520integrate%2520them%2520into%2520the%2520reinforcement%2520learning%2520framework.%2520In%250Athis%2520paper%252C%2520we%2520anticipate%2520and%252C%2520most%2520importantly%252C%2520evaluate%2520two%2520promising%250Astrategies.%2520First%252C%2520we%2520consider%2520the%2520use%2520of%2520foundation%2520world%2520models%2520%2528FWMs%2529%2520that%250Aexploit%2520the%2520prior%2520knowledge%2520of%2520FMs%2520to%2520enable%2520training%2520and%2520evaluating%2520agents%250Awith%2520simulated%2520interactions.%2520Second%252C%2520we%2520consider%2520the%2520use%2520of%2520foundation%2520agents%250A%2528FAs%2529%2520that%2520exploit%2520the%2520reasoning%2520capabilities%2520of%2520FMs%2520for%2520decision-making.%2520We%250Aevaluate%2520both%2520approaches%2520empirically%2520in%2520a%2520family%2520of%2520grid-world%2520environments%250Athat%2520are%2520suitable%2520for%2520the%2520current%2520generation%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%250AOur%2520results%2520suggest%2520that%2520improvements%2520in%2520LLMs%2520already%2520translate%2520into%2520better%250AFWMs%2520and%2520FAs%253B%2520that%2520FAs%2520based%2520on%2520current%2520LLMs%2520can%2520already%2520provide%2520excellent%250Apolicies%2520for%2520sufficiently%2520simple%2520environments%253B%2520and%2520that%2520the%2520coupling%2520of%2520FWMs%250Aand%2520reinforcement%2520learning%2520agents%2520is%2520highly%2520promising%2520for%2520more%2520complex%2520settings%250Awith%2520partial%2520observability%2520and%2520stochastic%2520elements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20as%20World%20Models%3A%20A%20Foundational%20Study%20in%20Text-Based%0A%20%20GridWorlds&entry.906535625=Remo%20Sasso%20and%20Michelangelo%20Conserva%20and%20Dominik%20Jeurissen%20and%20Paulo%20Rauber&entry.1292438233=%20%20While%20reinforcement%20learning%20from%20scratch%20has%20shown%20impressive%20results%20in%0Asolving%20sequential%20decision-making%20tasks%20with%20efficient%20simulators%2C%20real-world%0Aapplications%20with%20expensive%20interactions%20require%20more%20sample-efficient%20agents.%0AFoundation%20models%20%28FMs%29%20are%20natural%20candidates%20to%20improve%20sample%20efficiency%20as%0Athey%20possess%20broad%20knowledge%20and%20reasoning%20capabilities%2C%20but%20it%20is%20yet%20unclear%0Ahow%20to%20effectively%20integrate%20them%20into%20the%20reinforcement%20learning%20framework.%20In%0Athis%20paper%2C%20we%20anticipate%20and%2C%20most%20importantly%2C%20evaluate%20two%20promising%0Astrategies.%20First%2C%20we%20consider%20the%20use%20of%20foundation%20world%20models%20%28FWMs%29%20that%0Aexploit%20the%20prior%20knowledge%20of%20FMs%20to%20enable%20training%20and%20evaluating%20agents%0Awith%20simulated%20interactions.%20Second%2C%20we%20consider%20the%20use%20of%20foundation%20agents%0A%28FAs%29%20that%20exploit%20the%20reasoning%20capabilities%20of%20FMs%20for%20decision-making.%20We%0Aevaluate%20both%20approaches%20empirically%20in%20a%20family%20of%20grid-world%20environments%0Athat%20are%20suitable%20for%20the%20current%20generation%20of%20large%20language%20models%20%28LLMs%29.%0AOur%20results%20suggest%20that%20improvements%20in%20LLMs%20already%20translate%20into%20better%0AFWMs%20and%20FAs%3B%20that%20FAs%20based%20on%20current%20LLMs%20can%20already%20provide%20excellent%0Apolicies%20for%20sufficiently%20simple%20environments%3B%20and%20that%20the%20coupling%20of%20FWMs%0Aand%20reinforcement%20learning%20agents%20is%20highly%20promising%20for%20more%20complex%20settings%0Awith%20partial%20observability%20and%20stochastic%20elements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15915v1&entry.124074799=Read"},
{"title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning", "author": "Mingyuan Wu and Jize Jiang and Haozhen Zheng and Meitang Li and Zhaoheng Li and Beitong Tian and Bo Chen and Yongjoo Park and Minjia Zhang and Chengxiang Zhai and Klara Nahrstedt", "abstract": "  Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts\n", "link": "http://arxiv.org/abs/2502.20587v2", "date": "2025-09-19", "relevancy": 2.7118, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cache-of-Thought%3A%20Master-Apprentice%20Framework%20for%20Cost-Effective%20Vision%0A%20%20Language%20Model%20Reasoning&body=Title%3A%20Cache-of-Thought%3A%20Master-Apprentice%20Framework%20for%20Cost-Effective%20Vision%0A%20%20Language%20Model%20Reasoning%0AAuthor%3A%20Mingyuan%20Wu%20and%20Jize%20Jiang%20and%20Haozhen%20Zheng%20and%20Meitang%20Li%20and%20Zhaoheng%20Li%20and%20Beitong%20Tian%20and%20Bo%20Chen%20and%20Yongjoo%20Park%20and%20Minjia%20Zhang%20and%20Chengxiang%20Zhai%20and%20Klara%20Nahrstedt%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20in%20a%20wide%0Arange%20of%20vision%20applications%20of%20increasing%20complexity%20and%20scales%2C%20yet%20choosing%0Athe%20right%20VLM%20model%20size%20involves%20a%20trade-off%20between%20response%20quality%20and%0Acost.%20While%20smaller%20VLMs%20are%20cheaper%20to%20run%2C%20they%20typically%20produce%20responses%0Aonly%20marginally%20better%20than%20random%20guessing%20on%20benchmarks%20such%20as%20MMMU.%0A%20%20In%20this%20paper%2C%20we%20propose%20Cache%20of%20Thought%20%28CoT%29%2C%20a%20master%20apprentice%0Aframework%20for%20collaborative%20inference%20between%20large%20and%20small%20VLMs.%20CoT%20manages%0Ahigh%20quality%20query%20results%20from%20large%20VLMs%20%28master%29%20in%20a%20cache%2C%20which%20are%20then%0Aselected%20via%20a%20novel%20multi%20modal%20retrieval%20and%20in-context%20learning%20to%20aid%20the%0Aperformance%20of%20small%20VLMs%20%28apprentice%29.%20We%20extensively%20evaluate%20CoT%20on%20various%0Awidely%20recognized%20and%20challenging%20general%20reasoning%20benchmarks%2C%20and%20show%20that%0ACoT%20increases%20overall%20reasoning%20performance%20by%20up%20to%207.7%25%20under%20the%20same%0Abudget%2C%20and%20specifically%20boosts%20the%20performance%20of%20apprentice%20VLMs%20by%20up%20to%0A36.6%25.%20Our%20code%20is%20available%20at%20https%3A//github.com/UIUC-MONET/Cache-of-Thoughts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCache-of-Thought%253A%2520Master-Apprentice%2520Framework%2520for%2520Cost-Effective%2520Vision%250A%2520%2520Language%2520Model%2520Reasoning%26entry.906535625%3DMingyuan%2520Wu%2520and%2520Jize%2520Jiang%2520and%2520Haozhen%2520Zheng%2520and%2520Meitang%2520Li%2520and%2520Zhaoheng%2520Li%2520and%2520Beitong%2520Tian%2520and%2520Bo%2520Chen%2520and%2520Yongjoo%2520Park%2520and%2520Minjia%2520Zhang%2520and%2520Chengxiang%2520Zhai%2520and%2520Klara%2520Nahrstedt%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520a%2520wide%250Arange%2520of%2520vision%2520applications%2520of%2520increasing%2520complexity%2520and%2520scales%252C%2520yet%2520choosing%250Athe%2520right%2520VLM%2520model%2520size%2520involves%2520a%2520trade-off%2520between%2520response%2520quality%2520and%250Acost.%2520While%2520smaller%2520VLMs%2520are%2520cheaper%2520to%2520run%252C%2520they%2520typically%2520produce%2520responses%250Aonly%2520marginally%2520better%2520than%2520random%2520guessing%2520on%2520benchmarks%2520such%2520as%2520MMMU.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Cache%2520of%2520Thought%2520%2528CoT%2529%252C%2520a%2520master%2520apprentice%250Aframework%2520for%2520collaborative%2520inference%2520between%2520large%2520and%2520small%2520VLMs.%2520CoT%2520manages%250Ahigh%2520quality%2520query%2520results%2520from%2520large%2520VLMs%2520%2528master%2529%2520in%2520a%2520cache%252C%2520which%2520are%2520then%250Aselected%2520via%2520a%2520novel%2520multi%2520modal%2520retrieval%2520and%2520in-context%2520learning%2520to%2520aid%2520the%250Aperformance%2520of%2520small%2520VLMs%2520%2528apprentice%2529.%2520We%2520extensively%2520evaluate%2520CoT%2520on%2520various%250Awidely%2520recognized%2520and%2520challenging%2520general%2520reasoning%2520benchmarks%252C%2520and%2520show%2520that%250ACoT%2520increases%2520overall%2520reasoning%2520performance%2520by%2520up%2520to%25207.7%2525%2520under%2520the%2520same%250Abudget%252C%2520and%2520specifically%2520boosts%2520the%2520performance%2520of%2520apprentice%2520VLMs%2520by%2520up%2520to%250A36.6%2525.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/UIUC-MONET/Cache-of-Thoughts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cache-of-Thought%3A%20Master-Apprentice%20Framework%20for%20Cost-Effective%20Vision%0A%20%20Language%20Model%20Reasoning&entry.906535625=Mingyuan%20Wu%20and%20Jize%20Jiang%20and%20Haozhen%20Zheng%20and%20Meitang%20Li%20and%20Zhaoheng%20Li%20and%20Beitong%20Tian%20and%20Bo%20Chen%20and%20Yongjoo%20Park%20and%20Minjia%20Zhang%20and%20Chengxiang%20Zhai%20and%20Klara%20Nahrstedt&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20in%20a%20wide%0Arange%20of%20vision%20applications%20of%20increasing%20complexity%20and%20scales%2C%20yet%20choosing%0Athe%20right%20VLM%20model%20size%20involves%20a%20trade-off%20between%20response%20quality%20and%0Acost.%20While%20smaller%20VLMs%20are%20cheaper%20to%20run%2C%20they%20typically%20produce%20responses%0Aonly%20marginally%20better%20than%20random%20guessing%20on%20benchmarks%20such%20as%20MMMU.%0A%20%20In%20this%20paper%2C%20we%20propose%20Cache%20of%20Thought%20%28CoT%29%2C%20a%20master%20apprentice%0Aframework%20for%20collaborative%20inference%20between%20large%20and%20small%20VLMs.%20CoT%20manages%0Ahigh%20quality%20query%20results%20from%20large%20VLMs%20%28master%29%20in%20a%20cache%2C%20which%20are%20then%0Aselected%20via%20a%20novel%20multi%20modal%20retrieval%20and%20in-context%20learning%20to%20aid%20the%0Aperformance%20of%20small%20VLMs%20%28apprentice%29.%20We%20extensively%20evaluate%20CoT%20on%20various%0Awidely%20recognized%20and%20challenging%20general%20reasoning%20benchmarks%2C%20and%20show%20that%0ACoT%20increases%20overall%20reasoning%20performance%20by%20up%20to%207.7%25%20under%20the%20same%0Abudget%2C%20and%20specifically%20boosts%20the%20performance%20of%20apprentice%20VLMs%20by%20up%20to%0A36.6%25.%20Our%20code%20is%20available%20at%20https%3A//github.com/UIUC-MONET/Cache-of-Thoughts%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20587v2&entry.124074799=Read"},
{"title": "Graph-based Point Cloud Surface Reconstruction using B-Splines", "author": "Stuti Pathak and Rhys G. Evans and Gunther Steenackers and Rudi Penne", "abstract": "  Generating continuous surfaces from discrete point cloud data is a\nfundamental task in several 3D vision applications. Real-world point clouds are\ninherently noisy due to various technical and environmental factors. Existing\ndata-driven surface reconstruction algorithms rely heavily on ground truth\nnormals or compute approximate normals as an intermediate step. This dependency\nmakes them extremely unreliable for noisy point cloud datasets, even if the\navailability of ground truth training data is ensured, which is not always the\ncase. B-spline reconstruction techniques provide compact surface\nrepresentations of point clouds and are especially known for their smoothening\nproperties. However, the complexity of the surfaces approximated using\nB-splines is directly influenced by the number and location of the spline\ncontrol points. Existing spline-based modeling methods predict the locations of\na fixed number of control points for a given point cloud, which makes it very\ndifficult to match the complexity of its underlying surface. In this work, we\ndevelop a Dictionary-Guided Graph Convolutional Network-based surface\nreconstruction strategy where we simultaneously predict both the location and\nthe number of control points for noisy point cloud data to generate smooth\nsurfaces without the use of any point normals. We compare our reconstruction\nmethod with several well-known as well as recent baselines by employing\nwidely-used evaluation metrics, and demonstrate that our method outperforms all\nof them both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2509.16050v1", "date": "2025-09-19", "relevancy": 2.708, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5541}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5385}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%20Point%20Cloud%20Surface%20Reconstruction%20using%20B-Splines&body=Title%3A%20Graph-based%20Point%20Cloud%20Surface%20Reconstruction%20using%20B-Splines%0AAuthor%3A%20Stuti%20Pathak%20and%20Rhys%20G.%20Evans%20and%20Gunther%20Steenackers%20and%20Rudi%20Penne%0AAbstract%3A%20%20%20Generating%20continuous%20surfaces%20from%20discrete%20point%20cloud%20data%20is%20a%0Afundamental%20task%20in%20several%203D%20vision%20applications.%20Real-world%20point%20clouds%20are%0Ainherently%20noisy%20due%20to%20various%20technical%20and%20environmental%20factors.%20Existing%0Adata-driven%20surface%20reconstruction%20algorithms%20rely%20heavily%20on%20ground%20truth%0Anormals%20or%20compute%20approximate%20normals%20as%20an%20intermediate%20step.%20This%20dependency%0Amakes%20them%20extremely%20unreliable%20for%20noisy%20point%20cloud%20datasets%2C%20even%20if%20the%0Aavailability%20of%20ground%20truth%20training%20data%20is%20ensured%2C%20which%20is%20not%20always%20the%0Acase.%20B-spline%20reconstruction%20techniques%20provide%20compact%20surface%0Arepresentations%20of%20point%20clouds%20and%20are%20especially%20known%20for%20their%20smoothening%0Aproperties.%20However%2C%20the%20complexity%20of%20the%20surfaces%20approximated%20using%0AB-splines%20is%20directly%20influenced%20by%20the%20number%20and%20location%20of%20the%20spline%0Acontrol%20points.%20Existing%20spline-based%20modeling%20methods%20predict%20the%20locations%20of%0Aa%20fixed%20number%20of%20control%20points%20for%20a%20given%20point%20cloud%2C%20which%20makes%20it%20very%0Adifficult%20to%20match%20the%20complexity%20of%20its%20underlying%20surface.%20In%20this%20work%2C%20we%0Adevelop%20a%20Dictionary-Guided%20Graph%20Convolutional%20Network-based%20surface%0Areconstruction%20strategy%20where%20we%20simultaneously%20predict%20both%20the%20location%20and%0Athe%20number%20of%20control%20points%20for%20noisy%20point%20cloud%20data%20to%20generate%20smooth%0Asurfaces%20without%20the%20use%20of%20any%20point%20normals.%20We%20compare%20our%20reconstruction%0Amethod%20with%20several%20well-known%20as%20well%20as%20recent%20baselines%20by%20employing%0Awidely-used%20evaluation%20metrics%2C%20and%20demonstrate%20that%20our%20method%20outperforms%20all%0Aof%20them%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%2520Point%2520Cloud%2520Surface%2520Reconstruction%2520using%2520B-Splines%26entry.906535625%3DStuti%2520Pathak%2520and%2520Rhys%2520G.%2520Evans%2520and%2520Gunther%2520Steenackers%2520and%2520Rudi%2520Penne%26entry.1292438233%3D%2520%2520Generating%2520continuous%2520surfaces%2520from%2520discrete%2520point%2520cloud%2520data%2520is%2520a%250Afundamental%2520task%2520in%2520several%25203D%2520vision%2520applications.%2520Real-world%2520point%2520clouds%2520are%250Ainherently%2520noisy%2520due%2520to%2520various%2520technical%2520and%2520environmental%2520factors.%2520Existing%250Adata-driven%2520surface%2520reconstruction%2520algorithms%2520rely%2520heavily%2520on%2520ground%2520truth%250Anormals%2520or%2520compute%2520approximate%2520normals%2520as%2520an%2520intermediate%2520step.%2520This%2520dependency%250Amakes%2520them%2520extremely%2520unreliable%2520for%2520noisy%2520point%2520cloud%2520datasets%252C%2520even%2520if%2520the%250Aavailability%2520of%2520ground%2520truth%2520training%2520data%2520is%2520ensured%252C%2520which%2520is%2520not%2520always%2520the%250Acase.%2520B-spline%2520reconstruction%2520techniques%2520provide%2520compact%2520surface%250Arepresentations%2520of%2520point%2520clouds%2520and%2520are%2520especially%2520known%2520for%2520their%2520smoothening%250Aproperties.%2520However%252C%2520the%2520complexity%2520of%2520the%2520surfaces%2520approximated%2520using%250AB-splines%2520is%2520directly%2520influenced%2520by%2520the%2520number%2520and%2520location%2520of%2520the%2520spline%250Acontrol%2520points.%2520Existing%2520spline-based%2520modeling%2520methods%2520predict%2520the%2520locations%2520of%250Aa%2520fixed%2520number%2520of%2520control%2520points%2520for%2520a%2520given%2520point%2520cloud%252C%2520which%2520makes%2520it%2520very%250Adifficult%2520to%2520match%2520the%2520complexity%2520of%2520its%2520underlying%2520surface.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520a%2520Dictionary-Guided%2520Graph%2520Convolutional%2520Network-based%2520surface%250Areconstruction%2520strategy%2520where%2520we%2520simultaneously%2520predict%2520both%2520the%2520location%2520and%250Athe%2520number%2520of%2520control%2520points%2520for%2520noisy%2520point%2520cloud%2520data%2520to%2520generate%2520smooth%250Asurfaces%2520without%2520the%2520use%2520of%2520any%2520point%2520normals.%2520We%2520compare%2520our%2520reconstruction%250Amethod%2520with%2520several%2520well-known%2520as%2520well%2520as%2520recent%2520baselines%2520by%2520employing%250Awidely-used%2520evaluation%2520metrics%252C%2520and%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520all%250Aof%2520them%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%20Point%20Cloud%20Surface%20Reconstruction%20using%20B-Splines&entry.906535625=Stuti%20Pathak%20and%20Rhys%20G.%20Evans%20and%20Gunther%20Steenackers%20and%20Rudi%20Penne&entry.1292438233=%20%20Generating%20continuous%20surfaces%20from%20discrete%20point%20cloud%20data%20is%20a%0Afundamental%20task%20in%20several%203D%20vision%20applications.%20Real-world%20point%20clouds%20are%0Ainherently%20noisy%20due%20to%20various%20technical%20and%20environmental%20factors.%20Existing%0Adata-driven%20surface%20reconstruction%20algorithms%20rely%20heavily%20on%20ground%20truth%0Anormals%20or%20compute%20approximate%20normals%20as%20an%20intermediate%20step.%20This%20dependency%0Amakes%20them%20extremely%20unreliable%20for%20noisy%20point%20cloud%20datasets%2C%20even%20if%20the%0Aavailability%20of%20ground%20truth%20training%20data%20is%20ensured%2C%20which%20is%20not%20always%20the%0Acase.%20B-spline%20reconstruction%20techniques%20provide%20compact%20surface%0Arepresentations%20of%20point%20clouds%20and%20are%20especially%20known%20for%20their%20smoothening%0Aproperties.%20However%2C%20the%20complexity%20of%20the%20surfaces%20approximated%20using%0AB-splines%20is%20directly%20influenced%20by%20the%20number%20and%20location%20of%20the%20spline%0Acontrol%20points.%20Existing%20spline-based%20modeling%20methods%20predict%20the%20locations%20of%0Aa%20fixed%20number%20of%20control%20points%20for%20a%20given%20point%20cloud%2C%20which%20makes%20it%20very%0Adifficult%20to%20match%20the%20complexity%20of%20its%20underlying%20surface.%20In%20this%20work%2C%20we%0Adevelop%20a%20Dictionary-Guided%20Graph%20Convolutional%20Network-based%20surface%0Areconstruction%20strategy%20where%20we%20simultaneously%20predict%20both%20the%20location%20and%0Athe%20number%20of%20control%20points%20for%20noisy%20point%20cloud%20data%20to%20generate%20smooth%0Asurfaces%20without%20the%20use%20of%20any%20point%20normals.%20We%20compare%20our%20reconstruction%0Amethod%20with%20several%20well-known%20as%20well%20as%20recent%20baselines%20by%20employing%0Awidely-used%20evaluation%20metrics%2C%20and%20demonstrate%20that%20our%20method%20outperforms%20all%0Aof%20them%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16050v1&entry.124074799=Read"},
{"title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess\n  Vision-Language Descriptions", "author": "Sajjad Abdoli and Rudi Cilibrasi and Rima Al-Shikh", "abstract": "  As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.\n", "link": "http://arxiv.org/abs/2509.10707v2", "date": "2025-09-19", "relevancy": 2.6856, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5494}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20AI%20Evaluation%20Patterns%3A%20How%20Different%20GPT%20Models%20Assess%0A%20%20Vision-Language%20Descriptions&body=Title%3A%20Understanding%20AI%20Evaluation%20Patterns%3A%20How%20Different%20GPT%20Models%20Assess%0A%20%20Vision-Language%20Descriptions%0AAuthor%3A%20Sajjad%20Abdoli%20and%20Rudi%20Cilibrasi%20and%20Rima%20Al-Shikh%0AAbstract%3A%20%20%20As%20AI%20systems%20increasingly%20evaluate%20other%20AI%20outputs%2C%20understanding%20their%0Aassessment%20behavior%20becomes%20crucial%20for%20preventing%20cascading%20biases.%20This%20study%0Aanalyzes%20vision-language%20descriptions%20generated%20by%20NVIDIA%27s%20Describe%20Anything%0AModel%20and%20evaluated%20by%20three%20GPT%20variants%20%28GPT-4o%2C%20GPT-4o-mini%2C%20GPT-5%29%20to%0Auncover%20distinct%20%22evaluation%20personalities%22%20the%20underlying%20assessment%0Astrategies%20and%20biases%20each%20model%20demonstrates.%20GPT-4o-mini%20exhibits%20systematic%0Aconsistency%20with%20minimal%20variance%2C%20GPT-4o%20excels%20at%20error%20detection%2C%20while%0AGPT-5%20shows%20extreme%20conservatism%20with%20high%20variability.%20Controlled%20experiments%0Ausing%20Gemini%202.5%20Pro%20as%20an%20independent%20question%20generator%20validate%20that%20these%0Apersonalities%20are%20inherent%20model%20properties%20rather%20than%20artifacts.%20Cross-family%0Aanalysis%20through%20semantic%20similarity%20of%20generated%20questions%20reveals%20significant%0Adivergence%3A%20GPT%20models%20cluster%20together%20with%20high%20similarity%20while%20Gemini%0Aexhibits%20markedly%20different%20evaluation%20strategies.%20All%20GPT%20models%20demonstrate%20a%0Aconsistent%202%3A1%20bias%20favoring%20negative%20assessment%20over%20positive%20confirmation%2C%0Athough%20this%20pattern%20appears%20family-specific%20rather%20than%20universal%20across%20AI%0Aarchitectures.%20These%20findings%20suggest%20that%20evaluation%20competence%20does%20not%20scale%0Awith%20general%20capability%20and%20that%20robust%20AI%20assessment%20requires%20diverse%0Aarchitectural%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520AI%2520Evaluation%2520Patterns%253A%2520How%2520Different%2520GPT%2520Models%2520Assess%250A%2520%2520Vision-Language%2520Descriptions%26entry.906535625%3DSajjad%2520Abdoli%2520and%2520Rudi%2520Cilibrasi%2520and%2520Rima%2520Al-Shikh%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520increasingly%2520evaluate%2520other%2520AI%2520outputs%252C%2520understanding%2520their%250Aassessment%2520behavior%2520becomes%2520crucial%2520for%2520preventing%2520cascading%2520biases.%2520This%2520study%250Aanalyzes%2520vision-language%2520descriptions%2520generated%2520by%2520NVIDIA%2527s%2520Describe%2520Anything%250AModel%2520and%2520evaluated%2520by%2520three%2520GPT%2520variants%2520%2528GPT-4o%252C%2520GPT-4o-mini%252C%2520GPT-5%2529%2520to%250Auncover%2520distinct%2520%2522evaluation%2520personalities%2522%2520the%2520underlying%2520assessment%250Astrategies%2520and%2520biases%2520each%2520model%2520demonstrates.%2520GPT-4o-mini%2520exhibits%2520systematic%250Aconsistency%2520with%2520minimal%2520variance%252C%2520GPT-4o%2520excels%2520at%2520error%2520detection%252C%2520while%250AGPT-5%2520shows%2520extreme%2520conservatism%2520with%2520high%2520variability.%2520Controlled%2520experiments%250Ausing%2520Gemini%25202.5%2520Pro%2520as%2520an%2520independent%2520question%2520generator%2520validate%2520that%2520these%250Apersonalities%2520are%2520inherent%2520model%2520properties%2520rather%2520than%2520artifacts.%2520Cross-family%250Aanalysis%2520through%2520semantic%2520similarity%2520of%2520generated%2520questions%2520reveals%2520significant%250Adivergence%253A%2520GPT%2520models%2520cluster%2520together%2520with%2520high%2520similarity%2520while%2520Gemini%250Aexhibits%2520markedly%2520different%2520evaluation%2520strategies.%2520All%2520GPT%2520models%2520demonstrate%2520a%250Aconsistent%25202%253A1%2520bias%2520favoring%2520negative%2520assessment%2520over%2520positive%2520confirmation%252C%250Athough%2520this%2520pattern%2520appears%2520family-specific%2520rather%2520than%2520universal%2520across%2520AI%250Aarchitectures.%2520These%2520findings%2520suggest%2520that%2520evaluation%2520competence%2520does%2520not%2520scale%250Awith%2520general%2520capability%2520and%2520that%2520robust%2520AI%2520assessment%2520requires%2520diverse%250Aarchitectural%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20AI%20Evaluation%20Patterns%3A%20How%20Different%20GPT%20Models%20Assess%0A%20%20Vision-Language%20Descriptions&entry.906535625=Sajjad%20Abdoli%20and%20Rudi%20Cilibrasi%20and%20Rima%20Al-Shikh&entry.1292438233=%20%20As%20AI%20systems%20increasingly%20evaluate%20other%20AI%20outputs%2C%20understanding%20their%0Aassessment%20behavior%20becomes%20crucial%20for%20preventing%20cascading%20biases.%20This%20study%0Aanalyzes%20vision-language%20descriptions%20generated%20by%20NVIDIA%27s%20Describe%20Anything%0AModel%20and%20evaluated%20by%20three%20GPT%20variants%20%28GPT-4o%2C%20GPT-4o-mini%2C%20GPT-5%29%20to%0Auncover%20distinct%20%22evaluation%20personalities%22%20the%20underlying%20assessment%0Astrategies%20and%20biases%20each%20model%20demonstrates.%20GPT-4o-mini%20exhibits%20systematic%0Aconsistency%20with%20minimal%20variance%2C%20GPT-4o%20excels%20at%20error%20detection%2C%20while%0AGPT-5%20shows%20extreme%20conservatism%20with%20high%20variability.%20Controlled%20experiments%0Ausing%20Gemini%202.5%20Pro%20as%20an%20independent%20question%20generator%20validate%20that%20these%0Apersonalities%20are%20inherent%20model%20properties%20rather%20than%20artifacts.%20Cross-family%0Aanalysis%20through%20semantic%20similarity%20of%20generated%20questions%20reveals%20significant%0Adivergence%3A%20GPT%20models%20cluster%20together%20with%20high%20similarity%20while%20Gemini%0Aexhibits%20markedly%20different%20evaluation%20strategies.%20All%20GPT%20models%20demonstrate%20a%0Aconsistent%202%3A1%20bias%20favoring%20negative%20assessment%20over%20positive%20confirmation%2C%0Athough%20this%20pattern%20appears%20family-specific%20rather%20than%20universal%20across%20AI%0Aarchitectures.%20These%20findings%20suggest%20that%20evaluation%20competence%20does%20not%20scale%0Awith%20general%20capability%20and%20that%20robust%20AI%20assessment%20requires%20diverse%0Aarchitectural%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10707v2&entry.124074799=Read"},
{"title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via\n  Grayscale-Aware Attention", "author": "Zhanjiang Yang and Lijun Sun and Jiawei Dong and Xiaoxin An and Yang Liu and Meng Li", "abstract": "  Reconstructing hyperspectral images (HSIs) from RGB inputs provides a\ncost-effective alternative to hyperspectral cameras, but reconstructing\nhigh-dimensional spectra from three channels is inherently ill-posed. Existing\nmethods typically directly regress RGB-to-HSI mappings using large attention\nnetworks, which are computationally expensive and handle ill-posedness only\nimplicitly. We propose MCGA, a Mixture-of-Codebooks with Grayscale-aware\nAttention framework that explicitly addresses these challenges using spectral\npriors and photometric consistency. MCGA first learns transferable spectral\npriors via a mixture-of-codebooks (MoC) from heterogeneous HSI datasets, then\naligns RGB features with these priors through grayscale-aware photometric\nattention (GANet). Efficiency and robustness are further improved via top-K\nattention design and test-time adaptation (TTA). Experiments on benchmarks and\nreal-world data demonstrate the state-of-the-art accuracy, strong cross-dataset\ngeneralization, and 4-5x faster inference. Codes will be available once\nacceptance at https://github.com/Fibonaccirabbit/MCGA.\n", "link": "http://arxiv.org/abs/2507.09885v2", "date": "2025-09-19", "relevancy": 2.6793, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5392}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCGA%3A%20Mixture%20of%20Codebooks%20Hyperspectral%20Reconstruction%20via%0A%20%20Grayscale-Aware%20Attention&body=Title%3A%20MCGA%3A%20Mixture%20of%20Codebooks%20Hyperspectral%20Reconstruction%20via%0A%20%20Grayscale-Aware%20Attention%0AAuthor%3A%20Zhanjiang%20Yang%20and%20Lijun%20Sun%20and%20Jiawei%20Dong%20and%20Xiaoxin%20An%20and%20Yang%20Liu%20and%20Meng%20Li%0AAbstract%3A%20%20%20Reconstructing%20hyperspectral%20images%20%28HSIs%29%20from%20RGB%20inputs%20provides%20a%0Acost-effective%20alternative%20to%20hyperspectral%20cameras%2C%20but%20reconstructing%0Ahigh-dimensional%20spectra%20from%20three%20channels%20is%20inherently%20ill-posed.%20Existing%0Amethods%20typically%20directly%20regress%20RGB-to-HSI%20mappings%20using%20large%20attention%0Anetworks%2C%20which%20are%20computationally%20expensive%20and%20handle%20ill-posedness%20only%0Aimplicitly.%20We%20propose%20MCGA%2C%20a%20Mixture-of-Codebooks%20with%20Grayscale-aware%0AAttention%20framework%20that%20explicitly%20addresses%20these%20challenges%20using%20spectral%0Apriors%20and%20photometric%20consistency.%20MCGA%20first%20learns%20transferable%20spectral%0Apriors%20via%20a%20mixture-of-codebooks%20%28MoC%29%20from%20heterogeneous%20HSI%20datasets%2C%20then%0Aaligns%20RGB%20features%20with%20these%20priors%20through%20grayscale-aware%20photometric%0Aattention%20%28GANet%29.%20Efficiency%20and%20robustness%20are%20further%20improved%20via%20top-K%0Aattention%20design%20and%20test-time%20adaptation%20%28TTA%29.%20Experiments%20on%20benchmarks%20and%0Areal-world%20data%20demonstrate%20the%20state-of-the-art%20accuracy%2C%20strong%20cross-dataset%0Ageneralization%2C%20and%204-5x%20faster%20inference.%20Codes%20will%20be%20available%20once%0Aacceptance%20at%20https%3A//github.com/Fibonaccirabbit/MCGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCGA%253A%2520Mixture%2520of%2520Codebooks%2520Hyperspectral%2520Reconstruction%2520via%250A%2520%2520Grayscale-Aware%2520Attention%26entry.906535625%3DZhanjiang%2520Yang%2520and%2520Lijun%2520Sun%2520and%2520Jiawei%2520Dong%2520and%2520Xiaoxin%2520An%2520and%2520Yang%2520Liu%2520and%2520Meng%2520Li%26entry.1292438233%3D%2520%2520Reconstructing%2520hyperspectral%2520images%2520%2528HSIs%2529%2520from%2520RGB%2520inputs%2520provides%2520a%250Acost-effective%2520alternative%2520to%2520hyperspectral%2520cameras%252C%2520but%2520reconstructing%250Ahigh-dimensional%2520spectra%2520from%2520three%2520channels%2520is%2520inherently%2520ill-posed.%2520Existing%250Amethods%2520typically%2520directly%2520regress%2520RGB-to-HSI%2520mappings%2520using%2520large%2520attention%250Anetworks%252C%2520which%2520are%2520computationally%2520expensive%2520and%2520handle%2520ill-posedness%2520only%250Aimplicitly.%2520We%2520propose%2520MCGA%252C%2520a%2520Mixture-of-Codebooks%2520with%2520Grayscale-aware%250AAttention%2520framework%2520that%2520explicitly%2520addresses%2520these%2520challenges%2520using%2520spectral%250Apriors%2520and%2520photometric%2520consistency.%2520MCGA%2520first%2520learns%2520transferable%2520spectral%250Apriors%2520via%2520a%2520mixture-of-codebooks%2520%2528MoC%2529%2520from%2520heterogeneous%2520HSI%2520datasets%252C%2520then%250Aaligns%2520RGB%2520features%2520with%2520these%2520priors%2520through%2520grayscale-aware%2520photometric%250Aattention%2520%2528GANet%2529.%2520Efficiency%2520and%2520robustness%2520are%2520further%2520improved%2520via%2520top-K%250Aattention%2520design%2520and%2520test-time%2520adaptation%2520%2528TTA%2529.%2520Experiments%2520on%2520benchmarks%2520and%250Areal-world%2520data%2520demonstrate%2520the%2520state-of-the-art%2520accuracy%252C%2520strong%2520cross-dataset%250Ageneralization%252C%2520and%25204-5x%2520faster%2520inference.%2520Codes%2520will%2520be%2520available%2520once%250Aacceptance%2520at%2520https%253A//github.com/Fibonaccirabbit/MCGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCGA%3A%20Mixture%20of%20Codebooks%20Hyperspectral%20Reconstruction%20via%0A%20%20Grayscale-Aware%20Attention&entry.906535625=Zhanjiang%20Yang%20and%20Lijun%20Sun%20and%20Jiawei%20Dong%20and%20Xiaoxin%20An%20and%20Yang%20Liu%20and%20Meng%20Li&entry.1292438233=%20%20Reconstructing%20hyperspectral%20images%20%28HSIs%29%20from%20RGB%20inputs%20provides%20a%0Acost-effective%20alternative%20to%20hyperspectral%20cameras%2C%20but%20reconstructing%0Ahigh-dimensional%20spectra%20from%20three%20channels%20is%20inherently%20ill-posed.%20Existing%0Amethods%20typically%20directly%20regress%20RGB-to-HSI%20mappings%20using%20large%20attention%0Anetworks%2C%20which%20are%20computationally%20expensive%20and%20handle%20ill-posedness%20only%0Aimplicitly.%20We%20propose%20MCGA%2C%20a%20Mixture-of-Codebooks%20with%20Grayscale-aware%0AAttention%20framework%20that%20explicitly%20addresses%20these%20challenges%20using%20spectral%0Apriors%20and%20photometric%20consistency.%20MCGA%20first%20learns%20transferable%20spectral%0Apriors%20via%20a%20mixture-of-codebooks%20%28MoC%29%20from%20heterogeneous%20HSI%20datasets%2C%20then%0Aaligns%20RGB%20features%20with%20these%20priors%20through%20grayscale-aware%20photometric%0Aattention%20%28GANet%29.%20Efficiency%20and%20robustness%20are%20further%20improved%20via%20top-K%0Aattention%20design%20and%20test-time%20adaptation%20%28TTA%29.%20Experiments%20on%20benchmarks%20and%0Areal-world%20data%20demonstrate%20the%20state-of-the-art%20accuracy%2C%20strong%20cross-dataset%0Ageneralization%2C%20and%204-5x%20faster%20inference.%20Codes%20will%20be%20available%20once%0Aacceptance%20at%20https%3A//github.com/Fibonaccirabbit/MCGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09885v2&entry.124074799=Read"},
{"title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for\n  Long-Tail Scenarios via Collect-and-Refine", "author": "Shiyu Fang and Yiming Cui and Haoyang Liang and Chen Lv and Peng Hang and Jian Sun", "abstract": "  Autonomous Driving (AD) systems have made notable progress, but their\nperformance in long-tail, safety-critical scenarios remains limited. These rare\ncases contribute a disproportionate number of accidents. Vision-Language Action\n(VLA) models have strong reasoning abilities and offer a potential solution,\nbut their effectiveness is limited by the lack of high-quality data and\ninefficient learning in such conditions. To address these challenges, we\npropose CoReVLA, a continual learning end-to-end autonomous driving framework\nthat improves the performance in long-tail scenarios through a dual-stage\nprocess of data Collection and behavior Refinement. First, the model is jointly\nfine-tuned on a mixture of open-source driving QA datasets, allowing it to\nacquire a foundational understanding of driving scenarios. Next, CoReVLA is\ndeployed within the Cave Automatic Virtual Environment (CAVE) simulation\nplatform, where driver takeover data is collected from real-time interactions.\nEach takeover indicates a long-tail scenario that CoReVLA fails to handle\nreliably. Finally, the model is refined via Direct Preference Optimization\n(DPO), allowing it to learn directly from human preferences and thereby avoid\nreward hacking caused by manually designed rewards. Extensive open-loop and\nclosed-loop experiments demonstrate that the proposed CoReVLA model can\naccurately perceive driving scenarios and make appropriate decisions. On the\nBench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a\nSuccess Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and\n15% SR under long-tail, safety-critical scenarios. Furthermore, case studies\ndemonstrate the model's ability to continually improve its performance in\nsimilar failure-prone scenarios by leveraging past takeover experiences. All\ncodea and preprocessed datasets are available at:\nhttps://github.com/FanGShiYuu/CoReVLA\n", "link": "http://arxiv.org/abs/2509.15968v1", "date": "2025-09-19", "relevancy": 2.6784, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoReVLA%3A%20A%20Dual-Stage%20End-to-End%20Autonomous%20Driving%20Framework%20for%0A%20%20Long-Tail%20Scenarios%20via%20Collect-and-Refine&body=Title%3A%20CoReVLA%3A%20A%20Dual-Stage%20End-to-End%20Autonomous%20Driving%20Framework%20for%0A%20%20Long-Tail%20Scenarios%20via%20Collect-and-Refine%0AAuthor%3A%20Shiyu%20Fang%20and%20Yiming%20Cui%20and%20Haoyang%20Liang%20and%20Chen%20Lv%20and%20Peng%20Hang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20Autonomous%20Driving%20%28AD%29%20systems%20have%20made%20notable%20progress%2C%20but%20their%0Aperformance%20in%20long-tail%2C%20safety-critical%20scenarios%20remains%20limited.%20These%20rare%0Acases%20contribute%20a%20disproportionate%20number%20of%20accidents.%20Vision-Language%20Action%0A%28VLA%29%20models%20have%20strong%20reasoning%20abilities%20and%20offer%20a%20potential%20solution%2C%0Abut%20their%20effectiveness%20is%20limited%20by%20the%20lack%20of%20high-quality%20data%20and%0Ainefficient%20learning%20in%20such%20conditions.%20To%20address%20these%20challenges%2C%20we%0Apropose%20CoReVLA%2C%20a%20continual%20learning%20end-to-end%20autonomous%20driving%20framework%0Athat%20improves%20the%20performance%20in%20long-tail%20scenarios%20through%20a%20dual-stage%0Aprocess%20of%20data%20Collection%20and%20behavior%20Refinement.%20First%2C%20the%20model%20is%20jointly%0Afine-tuned%20on%20a%20mixture%20of%20open-source%20driving%20QA%20datasets%2C%20allowing%20it%20to%0Aacquire%20a%20foundational%20understanding%20of%20driving%20scenarios.%20Next%2C%20CoReVLA%20is%0Adeployed%20within%20the%20Cave%20Automatic%20Virtual%20Environment%20%28CAVE%29%20simulation%0Aplatform%2C%20where%20driver%20takeover%20data%20is%20collected%20from%20real-time%20interactions.%0AEach%20takeover%20indicates%20a%20long-tail%20scenario%20that%20CoReVLA%20fails%20to%20handle%0Areliably.%20Finally%2C%20the%20model%20is%20refined%20via%20Direct%20Preference%20Optimization%0A%28DPO%29%2C%20allowing%20it%20to%20learn%20directly%20from%20human%20preferences%20and%20thereby%20avoid%0Areward%20hacking%20caused%20by%20manually%20designed%20rewards.%20Extensive%20open-loop%20and%0Aclosed-loop%20experiments%20demonstrate%20that%20the%20proposed%20CoReVLA%20model%20can%0Aaccurately%20perceive%20driving%20scenarios%20and%20make%20appropriate%20decisions.%20On%20the%0ABench2Drive%20benchmark%2C%20CoReVLA%20achieves%20a%20Driving%20Score%20%28DS%29%20of%2072.18%20and%20a%0ASuccess%20Rate%20%28SR%29%20of%2050%25%2C%20outperforming%20state-of-the-art%20methods%20by%207.96%20DS%20and%0A15%25%20SR%20under%20long-tail%2C%20safety-critical%20scenarios.%20Furthermore%2C%20case%20studies%0Ademonstrate%20the%20model%27s%20ability%20to%20continually%20improve%20its%20performance%20in%0Asimilar%20failure-prone%20scenarios%20by%20leveraging%20past%20takeover%20experiences.%20All%0Acodea%20and%20preprocessed%20datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/FanGShiYuu/CoReVLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoReVLA%253A%2520A%2520Dual-Stage%2520End-to-End%2520Autonomous%2520Driving%2520Framework%2520for%250A%2520%2520Long-Tail%2520Scenarios%2520via%2520Collect-and-Refine%26entry.906535625%3DShiyu%2520Fang%2520and%2520Yiming%2520Cui%2520and%2520Haoyang%2520Liang%2520and%2520Chen%2520Lv%2520and%2520Peng%2520Hang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520Autonomous%2520Driving%2520%2528AD%2529%2520systems%2520have%2520made%2520notable%2520progress%252C%2520but%2520their%250Aperformance%2520in%2520long-tail%252C%2520safety-critical%2520scenarios%2520remains%2520limited.%2520These%2520rare%250Acases%2520contribute%2520a%2520disproportionate%2520number%2520of%2520accidents.%2520Vision-Language%2520Action%250A%2528VLA%2529%2520models%2520have%2520strong%2520reasoning%2520abilities%2520and%2520offer%2520a%2520potential%2520solution%252C%250Abut%2520their%2520effectiveness%2520is%2520limited%2520by%2520the%2520lack%2520of%2520high-quality%2520data%2520and%250Ainefficient%2520learning%2520in%2520such%2520conditions.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520CoReVLA%252C%2520a%2520continual%2520learning%2520end-to-end%2520autonomous%2520driving%2520framework%250Athat%2520improves%2520the%2520performance%2520in%2520long-tail%2520scenarios%2520through%2520a%2520dual-stage%250Aprocess%2520of%2520data%2520Collection%2520and%2520behavior%2520Refinement.%2520First%252C%2520the%2520model%2520is%2520jointly%250Afine-tuned%2520on%2520a%2520mixture%2520of%2520open-source%2520driving%2520QA%2520datasets%252C%2520allowing%2520it%2520to%250Aacquire%2520a%2520foundational%2520understanding%2520of%2520driving%2520scenarios.%2520Next%252C%2520CoReVLA%2520is%250Adeployed%2520within%2520the%2520Cave%2520Automatic%2520Virtual%2520Environment%2520%2528CAVE%2529%2520simulation%250Aplatform%252C%2520where%2520driver%2520takeover%2520data%2520is%2520collected%2520from%2520real-time%2520interactions.%250AEach%2520takeover%2520indicates%2520a%2520long-tail%2520scenario%2520that%2520CoReVLA%2520fails%2520to%2520handle%250Areliably.%2520Finally%252C%2520the%2520model%2520is%2520refined%2520via%2520Direct%2520Preference%2520Optimization%250A%2528DPO%2529%252C%2520allowing%2520it%2520to%2520learn%2520directly%2520from%2520human%2520preferences%2520and%2520thereby%2520avoid%250Areward%2520hacking%2520caused%2520by%2520manually%2520designed%2520rewards.%2520Extensive%2520open-loop%2520and%250Aclosed-loop%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520CoReVLA%2520model%2520can%250Aaccurately%2520perceive%2520driving%2520scenarios%2520and%2520make%2520appropriate%2520decisions.%2520On%2520the%250ABench2Drive%2520benchmark%252C%2520CoReVLA%2520achieves%2520a%2520Driving%2520Score%2520%2528DS%2529%2520of%252072.18%2520and%2520a%250ASuccess%2520Rate%2520%2528SR%2529%2520of%252050%2525%252C%2520outperforming%2520state-of-the-art%2520methods%2520by%25207.96%2520DS%2520and%250A15%2525%2520SR%2520under%2520long-tail%252C%2520safety-critical%2520scenarios.%2520Furthermore%252C%2520case%2520studies%250Ademonstrate%2520the%2520model%2527s%2520ability%2520to%2520continually%2520improve%2520its%2520performance%2520in%250Asimilar%2520failure-prone%2520scenarios%2520by%2520leveraging%2520past%2520takeover%2520experiences.%2520All%250Acodea%2520and%2520preprocessed%2520datasets%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/FanGShiYuu/CoReVLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoReVLA%3A%20A%20Dual-Stage%20End-to-End%20Autonomous%20Driving%20Framework%20for%0A%20%20Long-Tail%20Scenarios%20via%20Collect-and-Refine&entry.906535625=Shiyu%20Fang%20and%20Yiming%20Cui%20and%20Haoyang%20Liang%20and%20Chen%20Lv%20and%20Peng%20Hang%20and%20Jian%20Sun&entry.1292438233=%20%20Autonomous%20Driving%20%28AD%29%20systems%20have%20made%20notable%20progress%2C%20but%20their%0Aperformance%20in%20long-tail%2C%20safety-critical%20scenarios%20remains%20limited.%20These%20rare%0Acases%20contribute%20a%20disproportionate%20number%20of%20accidents.%20Vision-Language%20Action%0A%28VLA%29%20models%20have%20strong%20reasoning%20abilities%20and%20offer%20a%20potential%20solution%2C%0Abut%20their%20effectiveness%20is%20limited%20by%20the%20lack%20of%20high-quality%20data%20and%0Ainefficient%20learning%20in%20such%20conditions.%20To%20address%20these%20challenges%2C%20we%0Apropose%20CoReVLA%2C%20a%20continual%20learning%20end-to-end%20autonomous%20driving%20framework%0Athat%20improves%20the%20performance%20in%20long-tail%20scenarios%20through%20a%20dual-stage%0Aprocess%20of%20data%20Collection%20and%20behavior%20Refinement.%20First%2C%20the%20model%20is%20jointly%0Afine-tuned%20on%20a%20mixture%20of%20open-source%20driving%20QA%20datasets%2C%20allowing%20it%20to%0Aacquire%20a%20foundational%20understanding%20of%20driving%20scenarios.%20Next%2C%20CoReVLA%20is%0Adeployed%20within%20the%20Cave%20Automatic%20Virtual%20Environment%20%28CAVE%29%20simulation%0Aplatform%2C%20where%20driver%20takeover%20data%20is%20collected%20from%20real-time%20interactions.%0AEach%20takeover%20indicates%20a%20long-tail%20scenario%20that%20CoReVLA%20fails%20to%20handle%0Areliably.%20Finally%2C%20the%20model%20is%20refined%20via%20Direct%20Preference%20Optimization%0A%28DPO%29%2C%20allowing%20it%20to%20learn%20directly%20from%20human%20preferences%20and%20thereby%20avoid%0Areward%20hacking%20caused%20by%20manually%20designed%20rewards.%20Extensive%20open-loop%20and%0Aclosed-loop%20experiments%20demonstrate%20that%20the%20proposed%20CoReVLA%20model%20can%0Aaccurately%20perceive%20driving%20scenarios%20and%20make%20appropriate%20decisions.%20On%20the%0ABench2Drive%20benchmark%2C%20CoReVLA%20achieves%20a%20Driving%20Score%20%28DS%29%20of%2072.18%20and%20a%0ASuccess%20Rate%20%28SR%29%20of%2050%25%2C%20outperforming%20state-of-the-art%20methods%20by%207.96%20DS%20and%0A15%25%20SR%20under%20long-tail%2C%20safety-critical%20scenarios.%20Furthermore%2C%20case%20studies%0Ademonstrate%20the%20model%27s%20ability%20to%20continually%20improve%20its%20performance%20in%0Asimilar%20failure-prone%20scenarios%20by%20leveraging%20past%20takeover%20experiences.%20All%0Acodea%20and%20preprocessed%20datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/FanGShiYuu/CoReVLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15968v1&entry.124074799=Read"},
{"title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech", "author": "Sang Hoon Woo and Sehun Lee and Kang-wook Kim and Gunhee Kim", "abstract": "  Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT\n", "link": "http://arxiv.org/abs/2509.16028v1", "date": "2025-09-19", "relevancy": 2.6611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%2C%20Verbalize%2C%20then%20Speak%3A%20Bridging%20Complex%20Thoughts%20and%0A%20%20Comprehensible%20Speech&body=Title%3A%20Think%2C%20Verbalize%2C%20then%20Speak%3A%20Bridging%20Complex%20Thoughts%20and%0A%20%20Comprehensible%20Speech%0AAuthor%3A%20Sang%20Hoon%20Woo%20and%20Sehun%20Lee%20and%20Kang-wook%20Kim%20and%20Gunhee%20Kim%0AAbstract%3A%20%20%20Spoken%20dialogue%20systems%20increasingly%20employ%20large%20language%20models%20%28LLMs%29%20to%0Aleverage%20their%20advanced%20reasoning%20capabilities.%20However%2C%20direct%20application%20of%0ALLMs%20in%20spoken%20communication%20often%20yield%20suboptimal%20results%20due%20to%20mismatches%0Abetween%20optimal%20textual%20and%20verbal%20delivery.%20While%20existing%20approaches%20adapt%0ALLMs%20to%20produce%20speech-friendly%20outputs%2C%20their%20impact%20on%20reasoning%20performance%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20propose%20Think-Verbalize-Speak%2C%20a%0Aframework%20that%20decouples%20reasoning%20from%20spoken%20delivery%20to%20preserve%20the%20full%0Areasoning%20capacity%20of%20LLMs.%20Central%20to%20our%20method%20is%20verbalizing%2C%20an%0Aintermediate%20step%20that%20translates%20thoughts%20into%20natural%2C%20speech-ready%20text.%20We%0Aalso%20introduce%20ReVerT%2C%20a%20latency-efficient%20verbalizer%20based%20on%20incremental%20and%0Aasynchronous%20summarization.%20Experiments%20across%20multiple%20benchmarks%20show%20that%0Aour%20method%20enhances%20speech%20naturalness%20and%20conciseness%20with%20minimal%20impact%20on%0Areasoning.%20The%20project%20page%20with%20the%20dataset%20and%20the%20source%20code%20is%20available%0Aat%20https%3A//yhytoto12.github.io/TVS-ReVerT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%252C%2520Verbalize%252C%2520then%2520Speak%253A%2520Bridging%2520Complex%2520Thoughts%2520and%250A%2520%2520Comprehensible%2520Speech%26entry.906535625%3DSang%2520Hoon%2520Woo%2520and%2520Sehun%2520Lee%2520and%2520Kang-wook%2520Kim%2520and%2520Gunhee%2520Kim%26entry.1292438233%3D%2520%2520Spoken%2520dialogue%2520systems%2520increasingly%2520employ%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Aleverage%2520their%2520advanced%2520reasoning%2520capabilities.%2520However%252C%2520direct%2520application%2520of%250ALLMs%2520in%2520spoken%2520communication%2520often%2520yield%2520suboptimal%2520results%2520due%2520to%2520mismatches%250Abetween%2520optimal%2520textual%2520and%2520verbal%2520delivery.%2520While%2520existing%2520approaches%2520adapt%250ALLMs%2520to%2520produce%2520speech-friendly%2520outputs%252C%2520their%2520impact%2520on%2520reasoning%2520performance%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520Think-Verbalize-Speak%252C%2520a%250Aframework%2520that%2520decouples%2520reasoning%2520from%2520spoken%2520delivery%2520to%2520preserve%2520the%2520full%250Areasoning%2520capacity%2520of%2520LLMs.%2520Central%2520to%2520our%2520method%2520is%2520verbalizing%252C%2520an%250Aintermediate%2520step%2520that%2520translates%2520thoughts%2520into%2520natural%252C%2520speech-ready%2520text.%2520We%250Aalso%2520introduce%2520ReVerT%252C%2520a%2520latency-efficient%2520verbalizer%2520based%2520on%2520incremental%2520and%250Aasynchronous%2520summarization.%2520Experiments%2520across%2520multiple%2520benchmarks%2520show%2520that%250Aour%2520method%2520enhances%2520speech%2520naturalness%2520and%2520conciseness%2520with%2520minimal%2520impact%2520on%250Areasoning.%2520The%2520project%2520page%2520with%2520the%2520dataset%2520and%2520the%2520source%2520code%2520is%2520available%250Aat%2520https%253A//yhytoto12.github.io/TVS-ReVerT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%2C%20Verbalize%2C%20then%20Speak%3A%20Bridging%20Complex%20Thoughts%20and%0A%20%20Comprehensible%20Speech&entry.906535625=Sang%20Hoon%20Woo%20and%20Sehun%20Lee%20and%20Kang-wook%20Kim%20and%20Gunhee%20Kim&entry.1292438233=%20%20Spoken%20dialogue%20systems%20increasingly%20employ%20large%20language%20models%20%28LLMs%29%20to%0Aleverage%20their%20advanced%20reasoning%20capabilities.%20However%2C%20direct%20application%20of%0ALLMs%20in%20spoken%20communication%20often%20yield%20suboptimal%20results%20due%20to%20mismatches%0Abetween%20optimal%20textual%20and%20verbal%20delivery.%20While%20existing%20approaches%20adapt%0ALLMs%20to%20produce%20speech-friendly%20outputs%2C%20their%20impact%20on%20reasoning%20performance%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20propose%20Think-Verbalize-Speak%2C%20a%0Aframework%20that%20decouples%20reasoning%20from%20spoken%20delivery%20to%20preserve%20the%20full%0Areasoning%20capacity%20of%20LLMs.%20Central%20to%20our%20method%20is%20verbalizing%2C%20an%0Aintermediate%20step%20that%20translates%20thoughts%20into%20natural%2C%20speech-ready%20text.%20We%0Aalso%20introduce%20ReVerT%2C%20a%20latency-efficient%20verbalizer%20based%20on%20incremental%20and%0Aasynchronous%20summarization.%20Experiments%20across%20multiple%20benchmarks%20show%20that%0Aour%20method%20enhances%20speech%20naturalness%20and%20conciseness%20with%20minimal%20impact%20on%0Areasoning.%20The%20project%20page%20with%20the%20dataset%20and%20the%20source%20code%20is%20available%0Aat%20https%3A//yhytoto12.github.io/TVS-ReVerT%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16028v1&entry.124074799=Read"},
{"title": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation", "author": "Ahmed Elhady and Eneko Agirre and Mikel Artetxe", "abstract": "  Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.\n", "link": "http://arxiv.org/abs/2506.00288v3", "date": "2025-09-19", "relevancy": 2.653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation&body=Title%3A%20Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation%0AAuthor%3A%20Ahmed%20Elhady%20and%20Eneko%20Agirre%20and%20Mikel%20Artetxe%0AAbstract%3A%20%20%20Continued%20pretraining%20%28CPT%29%20is%20a%20popular%20approach%20to%20adapt%20existing%20large%0Alanguage%20models%20%28LLMs%29%20to%20new%20languages.%20When%20doing%20so%2C%20it%20is%20common%20practice%0Ato%20include%20a%20portion%20of%20English%20data%20in%20the%20mixture%2C%20but%20its%20role%20has%20not%20been%0Acarefully%20studied%20to%20date.%20In%20this%20work%2C%20we%20show%20that%20including%20English%20does%0Anot%20impact%20validation%20perplexity%2C%20yet%20it%20is%20critical%20for%20the%20emergence%20of%0Adownstream%20capabilities%20in%20the%20target%20language.%20We%20introduce%20a%0Alanguage-agnostic%20benchmark%20for%20in-context%20learning%20%28ICL%29%2C%20which%20reveals%0Acatastrophic%20forgetting%20early%20on%20CPT%20when%20English%20is%20not%20included.%20This%20in%20turn%0Adamages%20the%20ability%20of%20the%20model%20to%20generalize%20to%20downstream%20prompts%20in%20the%0Atarget%20language%20as%20measured%20by%20perplexity%2C%20even%20if%20it%20does%20not%20manifest%20in%0Aterms%20of%20accuracy%20until%20later%20in%20training%2C%20and%20can%20be%20tied%20to%20a%20big%20shift%20in%0Athe%20model%20parameters.%20Based%20on%20these%20insights%2C%20we%20introduce%20curriculum%20learning%0Aand%20exponential%20moving%20average%20%28EMA%29%20of%20weights%20as%20effective%20alternatives%20to%0Amitigate%20the%20need%20for%20English.%20All%20in%20all%2C%20our%20work%20sheds%20light%20into%20the%0Adynamics%20by%20which%20emergent%20abilities%20arise%20when%20doing%20CPT%20for%20language%0Aadaptation%2C%20and%20can%20serve%20as%20a%20foundation%20to%20design%20more%20effective%20methods%20in%0Athe%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00288v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Abilities%2520of%2520Large%2520Language%2520Models%2520under%2520Continued%2520Pretraining%250A%2520%2520for%2520Language%2520Adaptation%26entry.906535625%3DAhmed%2520Elhady%2520and%2520Eneko%2520Agirre%2520and%2520Mikel%2520Artetxe%26entry.1292438233%3D%2520%2520Continued%2520pretraining%2520%2528CPT%2529%2520is%2520a%2520popular%2520approach%2520to%2520adapt%2520existing%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520new%2520languages.%2520When%2520doing%2520so%252C%2520it%2520is%2520common%2520practice%250Ato%2520include%2520a%2520portion%2520of%2520English%2520data%2520in%2520the%2520mixture%252C%2520but%2520its%2520role%2520has%2520not%2520been%250Acarefully%2520studied%2520to%2520date.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520including%2520English%2520does%250Anot%2520impact%2520validation%2520perplexity%252C%2520yet%2520it%2520is%2520critical%2520for%2520the%2520emergence%2520of%250Adownstream%2520capabilities%2520in%2520the%2520target%2520language.%2520We%2520introduce%2520a%250Alanguage-agnostic%2520benchmark%2520for%2520in-context%2520learning%2520%2528ICL%2529%252C%2520which%2520reveals%250Acatastrophic%2520forgetting%2520early%2520on%2520CPT%2520when%2520English%2520is%2520not%2520included.%2520This%2520in%2520turn%250Adamages%2520the%2520ability%2520of%2520the%2520model%2520to%2520generalize%2520to%2520downstream%2520prompts%2520in%2520the%250Atarget%2520language%2520as%2520measured%2520by%2520perplexity%252C%2520even%2520if%2520it%2520does%2520not%2520manifest%2520in%250Aterms%2520of%2520accuracy%2520until%2520later%2520in%2520training%252C%2520and%2520can%2520be%2520tied%2520to%2520a%2520big%2520shift%2520in%250Athe%2520model%2520parameters.%2520Based%2520on%2520these%2520insights%252C%2520we%2520introduce%2520curriculum%2520learning%250Aand%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520of%2520weights%2520as%2520effective%2520alternatives%2520to%250Amitigate%2520the%2520need%2520for%2520English.%2520All%2520in%2520all%252C%2520our%2520work%2520sheds%2520light%2520into%2520the%250Adynamics%2520by%2520which%2520emergent%2520abilities%2520arise%2520when%2520doing%2520CPT%2520for%2520language%250Aadaptation%252C%2520and%2520can%2520serve%2520as%2520a%2520foundation%2520to%2520design%2520more%2520effective%2520methods%2520in%250Athe%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00288v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation&entry.906535625=Ahmed%20Elhady%20and%20Eneko%20Agirre%20and%20Mikel%20Artetxe&entry.1292438233=%20%20Continued%20pretraining%20%28CPT%29%20is%20a%20popular%20approach%20to%20adapt%20existing%20large%0Alanguage%20models%20%28LLMs%29%20to%20new%20languages.%20When%20doing%20so%2C%20it%20is%20common%20practice%0Ato%20include%20a%20portion%20of%20English%20data%20in%20the%20mixture%2C%20but%20its%20role%20has%20not%20been%0Acarefully%20studied%20to%20date.%20In%20this%20work%2C%20we%20show%20that%20including%20English%20does%0Anot%20impact%20validation%20perplexity%2C%20yet%20it%20is%20critical%20for%20the%20emergence%20of%0Adownstream%20capabilities%20in%20the%20target%20language.%20We%20introduce%20a%0Alanguage-agnostic%20benchmark%20for%20in-context%20learning%20%28ICL%29%2C%20which%20reveals%0Acatastrophic%20forgetting%20early%20on%20CPT%20when%20English%20is%20not%20included.%20This%20in%20turn%0Adamages%20the%20ability%20of%20the%20model%20to%20generalize%20to%20downstream%20prompts%20in%20the%0Atarget%20language%20as%20measured%20by%20perplexity%2C%20even%20if%20it%20does%20not%20manifest%20in%0Aterms%20of%20accuracy%20until%20later%20in%20training%2C%20and%20can%20be%20tied%20to%20a%20big%20shift%20in%0Athe%20model%20parameters.%20Based%20on%20these%20insights%2C%20we%20introduce%20curriculum%20learning%0Aand%20exponential%20moving%20average%20%28EMA%29%20of%20weights%20as%20effective%20alternatives%20to%0Amitigate%20the%20need%20for%20English.%20All%20in%20all%2C%20our%20work%20sheds%20light%20into%20the%0Adynamics%20by%20which%20emergent%20abilities%20arise%20when%20doing%20CPT%20for%20language%0Aadaptation%2C%20and%20can%20serve%20as%20a%20foundation%20to%20design%20more%20effective%20methods%20in%0Athe%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00288v3&entry.124074799=Read"},
{"title": "RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented\n  Image Captioning", "author": "Xiaosheng Long and Hanyu Wang and Zhentao Song and Kun Luo and Hongde Liu", "abstract": "  Recent retrieval-augmented image captioning methods incorporate external\nknowledge to compensate for the limitations in comprehending complex scenes.\nHowever, current approaches face challenges in relation modeling: (1) the\nrepresentation of semantic prompts is too coarse-grained to capture\nfine-grained relationships; (2) these methods lack explicit modeling of image\nobjects and their semantic relationships. To address these limitations, we\npropose RACap, a relation-aware retrieval-augmented model for image captioning,\nwhich not only mines structured relation semantics from retrieval captions, but\nalso identifies heterogeneous objects from the image. RACap effectively\nretrieves structured relation features that contain heterogeneous visual\ninformation to enhance the semantic consistency and relational expressiveness.\nExperimental results show that RACap, with only 10.8M trainable parameters,\nachieves superior performance compared to previous lightweight captioning\nmodels.\n", "link": "http://arxiv.org/abs/2509.15883v1", "date": "2025-09-19", "relevancy": 2.6311, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RACap%3A%20Relation-Aware%20Prompting%20for%20Lightweight%20Retrieval-Augmented%0A%20%20Image%20Captioning&body=Title%3A%20RACap%3A%20Relation-Aware%20Prompting%20for%20Lightweight%20Retrieval-Augmented%0A%20%20Image%20Captioning%0AAuthor%3A%20Xiaosheng%20Long%20and%20Hanyu%20Wang%20and%20Zhentao%20Song%20and%20Kun%20Luo%20and%20Hongde%20Liu%0AAbstract%3A%20%20%20Recent%20retrieval-augmented%20image%20captioning%20methods%20incorporate%20external%0Aknowledge%20to%20compensate%20for%20the%20limitations%20in%20comprehending%20complex%20scenes.%0AHowever%2C%20current%20approaches%20face%20challenges%20in%20relation%20modeling%3A%20%281%29%20the%0Arepresentation%20of%20semantic%20prompts%20is%20too%20coarse-grained%20to%20capture%0Afine-grained%20relationships%3B%20%282%29%20these%20methods%20lack%20explicit%20modeling%20of%20image%0Aobjects%20and%20their%20semantic%20relationships.%20To%20address%20these%20limitations%2C%20we%0Apropose%20RACap%2C%20a%20relation-aware%20retrieval-augmented%20model%20for%20image%20captioning%2C%0Awhich%20not%20only%20mines%20structured%20relation%20semantics%20from%20retrieval%20captions%2C%20but%0Aalso%20identifies%20heterogeneous%20objects%20from%20the%20image.%20RACap%20effectively%0Aretrieves%20structured%20relation%20features%20that%20contain%20heterogeneous%20visual%0Ainformation%20to%20enhance%20the%20semantic%20consistency%20and%20relational%20expressiveness.%0AExperimental%20results%20show%20that%20RACap%2C%20with%20only%2010.8M%20trainable%20parameters%2C%0Aachieves%20superior%20performance%20compared%20to%20previous%20lightweight%20captioning%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRACap%253A%2520Relation-Aware%2520Prompting%2520for%2520Lightweight%2520Retrieval-Augmented%250A%2520%2520Image%2520Captioning%26entry.906535625%3DXiaosheng%2520Long%2520and%2520Hanyu%2520Wang%2520and%2520Zhentao%2520Song%2520and%2520Kun%2520Luo%2520and%2520Hongde%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520retrieval-augmented%2520image%2520captioning%2520methods%2520incorporate%2520external%250Aknowledge%2520to%2520compensate%2520for%2520the%2520limitations%2520in%2520comprehending%2520complex%2520scenes.%250AHowever%252C%2520current%2520approaches%2520face%2520challenges%2520in%2520relation%2520modeling%253A%2520%25281%2529%2520the%250Arepresentation%2520of%2520semantic%2520prompts%2520is%2520too%2520coarse-grained%2520to%2520capture%250Afine-grained%2520relationships%253B%2520%25282%2529%2520these%2520methods%2520lack%2520explicit%2520modeling%2520of%2520image%250Aobjects%2520and%2520their%2520semantic%2520relationships.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520RACap%252C%2520a%2520relation-aware%2520retrieval-augmented%2520model%2520for%2520image%2520captioning%252C%250Awhich%2520not%2520only%2520mines%2520structured%2520relation%2520semantics%2520from%2520retrieval%2520captions%252C%2520but%250Aalso%2520identifies%2520heterogeneous%2520objects%2520from%2520the%2520image.%2520RACap%2520effectively%250Aretrieves%2520structured%2520relation%2520features%2520that%2520contain%2520heterogeneous%2520visual%250Ainformation%2520to%2520enhance%2520the%2520semantic%2520consistency%2520and%2520relational%2520expressiveness.%250AExperimental%2520results%2520show%2520that%2520RACap%252C%2520with%2520only%252010.8M%2520trainable%2520parameters%252C%250Aachieves%2520superior%2520performance%2520compared%2520to%2520previous%2520lightweight%2520captioning%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACap%3A%20Relation-Aware%20Prompting%20for%20Lightweight%20Retrieval-Augmented%0A%20%20Image%20Captioning&entry.906535625=Xiaosheng%20Long%20and%20Hanyu%20Wang%20and%20Zhentao%20Song%20and%20Kun%20Luo%20and%20Hongde%20Liu&entry.1292438233=%20%20Recent%20retrieval-augmented%20image%20captioning%20methods%20incorporate%20external%0Aknowledge%20to%20compensate%20for%20the%20limitations%20in%20comprehending%20complex%20scenes.%0AHowever%2C%20current%20approaches%20face%20challenges%20in%20relation%20modeling%3A%20%281%29%20the%0Arepresentation%20of%20semantic%20prompts%20is%20too%20coarse-grained%20to%20capture%0Afine-grained%20relationships%3B%20%282%29%20these%20methods%20lack%20explicit%20modeling%20of%20image%0Aobjects%20and%20their%20semantic%20relationships.%20To%20address%20these%20limitations%2C%20we%0Apropose%20RACap%2C%20a%20relation-aware%20retrieval-augmented%20model%20for%20image%20captioning%2C%0Awhich%20not%20only%20mines%20structured%20relation%20semantics%20from%20retrieval%20captions%2C%20but%0Aalso%20identifies%20heterogeneous%20objects%20from%20the%20image.%20RACap%20effectively%0Aretrieves%20structured%20relation%20features%20that%20contain%20heterogeneous%20visual%0Ainformation%20to%20enhance%20the%20semantic%20consistency%20and%20relational%20expressiveness.%0AExperimental%20results%20show%20that%20RACap%2C%20with%20only%2010.8M%20trainable%20parameters%2C%0Aachieves%20superior%20performance%20compared%20to%20previous%20lightweight%20captioning%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15883v1&entry.124074799=Read"},
{"title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning", "author": "Mingsheng Cai and Jiuming Jiang and Wenhao Huang and Che Liu and Rossella Arcucci", "abstract": "  Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.\n", "link": "http://arxiv.org/abs/2502.19668v4", "date": "2025-09-19", "relevancy": 2.6078, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5408}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning&body=Title%3A%20SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning%0AAuthor%3A%20Mingsheng%20Cai%20and%20Jiuming%20Jiang%20and%20Wenhao%20Huang%20and%20Che%20Liu%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20death%20and%20disability%0Aworldwide.%20Electrocardiogram%20%28ECG%29%20is%20critical%20for%20diagnosing%20and%20monitoring%0Acardiac%20health%2C%20but%20obtaining%20large-scale%20annotated%20ECG%20datasets%20is%0Alabor-intensive%20and%20time-consuming.%20Recent%20ECG%20Self-Supervised%20Learning%20%28eSSL%29%0Amethods%20mitigate%20this%20by%20learning%20features%20without%20extensive%20labels%20but%20fail%20to%0Acapture%20fine-grained%20clinical%20semantics%20and%20require%20extensive%20task-specific%0Afine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BSuPreME%7D%24%2C%20a%0A%24%5Ctextbf%7BSu%7D%24pervised%20%24%5Ctextbf%7BPre%7D%24-training%20framework%20for%0A%24%5Ctextbf%7BM%7D%24ultimodal%20%24%5Ctextbf%7BE%7D%24CG%20representation%20learning.%20SuPreME%20is%0Apre-trained%20using%20structured%20diagnostic%20labels%20derived%20from%20ECG%20report%20entities%0Athrough%20a%20one-time%20offline%20extraction%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahelp%20denoise%2C%20standardize%20cardiac%20concepts%2C%20and%20improve%20clinical%20representation%0Alearning.%20By%20fusing%20ECG%20signals%20with%20textual%20cardiac%20queries%20instead%20of%20fixed%0Alabels%2C%20SuPreME%20enables%20zero-shot%20classification%20of%20unseen%20conditions%20without%0Afurther%20fine-tuning.%20We%20evaluate%20SuPreME%20on%20six%20downstream%20datasets%20covering%0A106%20cardiac%20conditions%2C%20achieving%20superior%20zero-shot%20AUC%20performance%20of%0A%2477.20%5C%25%24%2C%20surpassing%20state-of-the-art%20eSSLs%20by%20%244.98%5C%25%24.%20Results%20demonstrate%0ASuPreME%27s%20effectiveness%20in%20leveraging%20structured%2C%20clinically%20relevant%20knowledge%0Afor%20high-quality%20ECG%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19668v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuPreME%253A%2520A%2520Supervised%2520Pre-training%2520Framework%2520for%2520Multimodal%2520ECG%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMingsheng%2520Cai%2520and%2520Jiuming%2520Jiang%2520and%2520Wenhao%2520Huang%2520and%2520Che%2520Liu%2520and%2520Rossella%2520Arcucci%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520are%2520a%2520leading%2520cause%2520of%2520death%2520and%2520disability%250Aworldwide.%2520Electrocardiogram%2520%2528ECG%2529%2520is%2520critical%2520for%2520diagnosing%2520and%2520monitoring%250Acardiac%2520health%252C%2520but%2520obtaining%2520large-scale%2520annotated%2520ECG%2520datasets%2520is%250Alabor-intensive%2520and%2520time-consuming.%2520Recent%2520ECG%2520Self-Supervised%2520Learning%2520%2528eSSL%2529%250Amethods%2520mitigate%2520this%2520by%2520learning%2520features%2520without%2520extensive%2520labels%2520but%2520fail%2520to%250Acapture%2520fine-grained%2520clinical%2520semantics%2520and%2520require%2520extensive%2520task-specific%250Afine-tuning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%2524%255Ctextbf%257BSuPreME%257D%2524%252C%2520a%250A%2524%255Ctextbf%257BSu%257D%2524pervised%2520%2524%255Ctextbf%257BPre%257D%2524-training%2520framework%2520for%250A%2524%255Ctextbf%257BM%257D%2524ultimodal%2520%2524%255Ctextbf%257BE%257D%2524CG%2520representation%2520learning.%2520SuPreME%2520is%250Apre-trained%2520using%2520structured%2520diagnostic%2520labels%2520derived%2520from%2520ECG%2520report%2520entities%250Athrough%2520a%2520one-time%2520offline%2520extraction%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%250Ahelp%2520denoise%252C%2520standardize%2520cardiac%2520concepts%252C%2520and%2520improve%2520clinical%2520representation%250Alearning.%2520By%2520fusing%2520ECG%2520signals%2520with%2520textual%2520cardiac%2520queries%2520instead%2520of%2520fixed%250Alabels%252C%2520SuPreME%2520enables%2520zero-shot%2520classification%2520of%2520unseen%2520conditions%2520without%250Afurther%2520fine-tuning.%2520We%2520evaluate%2520SuPreME%2520on%2520six%2520downstream%2520datasets%2520covering%250A106%2520cardiac%2520conditions%252C%2520achieving%2520superior%2520zero-shot%2520AUC%2520performance%2520of%250A%252477.20%255C%2525%2524%252C%2520surpassing%2520state-of-the-art%2520eSSLs%2520by%2520%25244.98%255C%2525%2524.%2520Results%2520demonstrate%250ASuPreME%2527s%2520effectiveness%2520in%2520leveraging%2520structured%252C%2520clinically%2520relevant%2520knowledge%250Afor%2520high-quality%2520ECG%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19668v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning&entry.906535625=Mingsheng%20Cai%20and%20Jiuming%20Jiang%20and%20Wenhao%20Huang%20and%20Che%20Liu%20and%20Rossella%20Arcucci&entry.1292438233=%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20death%20and%20disability%0Aworldwide.%20Electrocardiogram%20%28ECG%29%20is%20critical%20for%20diagnosing%20and%20monitoring%0Acardiac%20health%2C%20but%20obtaining%20large-scale%20annotated%20ECG%20datasets%20is%0Alabor-intensive%20and%20time-consuming.%20Recent%20ECG%20Self-Supervised%20Learning%20%28eSSL%29%0Amethods%20mitigate%20this%20by%20learning%20features%20without%20extensive%20labels%20but%20fail%20to%0Acapture%20fine-grained%20clinical%20semantics%20and%20require%20extensive%20task-specific%0Afine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BSuPreME%7D%24%2C%20a%0A%24%5Ctextbf%7BSu%7D%24pervised%20%24%5Ctextbf%7BPre%7D%24-training%20framework%20for%0A%24%5Ctextbf%7BM%7D%24ultimodal%20%24%5Ctextbf%7BE%7D%24CG%20representation%20learning.%20SuPreME%20is%0Apre-trained%20using%20structured%20diagnostic%20labels%20derived%20from%20ECG%20report%20entities%0Athrough%20a%20one-time%20offline%20extraction%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahelp%20denoise%2C%20standardize%20cardiac%20concepts%2C%20and%20improve%20clinical%20representation%0Alearning.%20By%20fusing%20ECG%20signals%20with%20textual%20cardiac%20queries%20instead%20of%20fixed%0Alabels%2C%20SuPreME%20enables%20zero-shot%20classification%20of%20unseen%20conditions%20without%0Afurther%20fine-tuning.%20We%20evaluate%20SuPreME%20on%20six%20downstream%20datasets%20covering%0A106%20cardiac%20conditions%2C%20achieving%20superior%20zero-shot%20AUC%20performance%20of%0A%2477.20%5C%25%24%2C%20surpassing%20state-of-the-art%20eSSLs%20by%20%244.98%5C%25%24.%20Results%20demonstrate%0ASuPreME%27s%20effectiveness%20in%20leveraging%20structured%2C%20clinically%20relevant%20knowledge%0Afor%20high-quality%20ECG%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19668v4&entry.124074799=Read"},
{"title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens", "author": "Dmitry Akulov and Mohamed Sana and Antonio De Domenico and Tareq Si Salem and Nicola Piovesan and Fadhel Ayed", "abstract": "  Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.\n", "link": "http://arxiv.org/abs/2509.05165v2", "date": "2025-09-19", "relevancy": 2.6029, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens&body=Title%3A%20KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens%0AAuthor%3A%20Dmitry%20Akulov%20and%20Mohamed%20Sana%20and%20Antonio%20De%20Domenico%20and%20Tareq%20Si%20Salem%20and%20Nicola%20Piovesan%20and%20Fadhel%20Ayed%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20%28KV%29%20caches%20for%20efficient%0Aautoregressive%20decoding%3B%20however%2C%20cache%20size%20grows%20linearly%20with%20context%20length%0Aand%20model%20depth%2C%20becoming%20a%20major%20bottleneck%20in%20long-context%20inference.%20Prior%0AKV%20cache%20compression%20methods%20either%20enforce%20rigid%20heuristics%2C%20disrupt%20tensor%0Alayouts%20with%20per-attention-head%20variability%2C%20or%20require%20specialized%20compute%0Akernels.%0A%20%20We%20propose%20a%20simple%2C%20yet%20effective%2C%20KV%20cache%20compression%20framework%20based%20on%0Aattention-guided%2C%20layer-adaptive%20composite%20tokens.%20Our%20method%20aggregates%0Aattention%20scores%20to%20estimate%20token%20importance%2C%20selects%20head-specific%20tokens%0Aindependently%2C%20and%20aligns%20them%20into%20composite%20tokens%20that%20respect%20the%20uniform%0Acache%20structure%20required%20by%20existing%20inference%20engines.%20A%20global%20allocation%0Amechanism%20further%20adapts%20retention%20budgets%20across%20layers%2C%20assigning%20more%0Acapacity%20to%20layers%20with%20informative%20tokens.%20This%20approach%20achieves%20significant%0Amemory%20reduction%20while%20preserving%20accuracy%2C%20consistently%20outperforming%20prior%0Astructured%20and%20semi-structured%20methods.%20Crucially%2C%20our%20approach%20remains%20fully%0Acompatible%20with%20standard%20inference%20pipelines%2C%20offering%20a%20practical%20and%20scalable%0Asolution%20for%20efficient%20long-context%20LLM%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKVCompose%253A%2520Efficient%2520Structured%2520KV%2520Cache%2520Compression%2520with%2520Composite%250A%2520%2520Tokens%26entry.906535625%3DDmitry%2520Akulov%2520and%2520Mohamed%2520Sana%2520and%2520Antonio%2520De%2520Domenico%2520and%2520Tareq%2520Si%2520Salem%2520and%2520Nicola%2520Piovesan%2520and%2520Fadhel%2520Ayed%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520rely%2520on%2520key-value%2520%2528KV%2529%2520caches%2520for%2520efficient%250Aautoregressive%2520decoding%253B%2520however%252C%2520cache%2520size%2520grows%2520linearly%2520with%2520context%2520length%250Aand%2520model%2520depth%252C%2520becoming%2520a%2520major%2520bottleneck%2520in%2520long-context%2520inference.%2520Prior%250AKV%2520cache%2520compression%2520methods%2520either%2520enforce%2520rigid%2520heuristics%252C%2520disrupt%2520tensor%250Alayouts%2520with%2520per-attention-head%2520variability%252C%2520or%2520require%2520specialized%2520compute%250Akernels.%250A%2520%2520We%2520propose%2520a%2520simple%252C%2520yet%2520effective%252C%2520KV%2520cache%2520compression%2520framework%2520based%2520on%250Aattention-guided%252C%2520layer-adaptive%2520composite%2520tokens.%2520Our%2520method%2520aggregates%250Aattention%2520scores%2520to%2520estimate%2520token%2520importance%252C%2520selects%2520head-specific%2520tokens%250Aindependently%252C%2520and%2520aligns%2520them%2520into%2520composite%2520tokens%2520that%2520respect%2520the%2520uniform%250Acache%2520structure%2520required%2520by%2520existing%2520inference%2520engines.%2520A%2520global%2520allocation%250Amechanism%2520further%2520adapts%2520retention%2520budgets%2520across%2520layers%252C%2520assigning%2520more%250Acapacity%2520to%2520layers%2520with%2520informative%2520tokens.%2520This%2520approach%2520achieves%2520significant%250Amemory%2520reduction%2520while%2520preserving%2520accuracy%252C%2520consistently%2520outperforming%2520prior%250Astructured%2520and%2520semi-structured%2520methods.%2520Crucially%252C%2520our%2520approach%2520remains%2520fully%250Acompatible%2520with%2520standard%2520inference%2520pipelines%252C%2520offering%2520a%2520practical%2520and%2520scalable%250Asolution%2520for%2520efficient%2520long-context%2520LLM%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KVCompose%3A%20Efficient%20Structured%20KV%20Cache%20Compression%20with%20Composite%0A%20%20Tokens&entry.906535625=Dmitry%20Akulov%20and%20Mohamed%20Sana%20and%20Antonio%20De%20Domenico%20and%20Tareq%20Si%20Salem%20and%20Nicola%20Piovesan%20and%20Fadhel%20Ayed&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20%28KV%29%20caches%20for%20efficient%0Aautoregressive%20decoding%3B%20however%2C%20cache%20size%20grows%20linearly%20with%20context%20length%0Aand%20model%20depth%2C%20becoming%20a%20major%20bottleneck%20in%20long-context%20inference.%20Prior%0AKV%20cache%20compression%20methods%20either%20enforce%20rigid%20heuristics%2C%20disrupt%20tensor%0Alayouts%20with%20per-attention-head%20variability%2C%20or%20require%20specialized%20compute%0Akernels.%0A%20%20We%20propose%20a%20simple%2C%20yet%20effective%2C%20KV%20cache%20compression%20framework%20based%20on%0Aattention-guided%2C%20layer-adaptive%20composite%20tokens.%20Our%20method%20aggregates%0Aattention%20scores%20to%20estimate%20token%20importance%2C%20selects%20head-specific%20tokens%0Aindependently%2C%20and%20aligns%20them%20into%20composite%20tokens%20that%20respect%20the%20uniform%0Acache%20structure%20required%20by%20existing%20inference%20engines.%20A%20global%20allocation%0Amechanism%20further%20adapts%20retention%20budgets%20across%20layers%2C%20assigning%20more%0Acapacity%20to%20layers%20with%20informative%20tokens.%20This%20approach%20achieves%20significant%0Amemory%20reduction%20while%20preserving%20accuracy%2C%20consistently%20outperforming%20prior%0Astructured%20and%20semi-structured%20methods.%20Crucially%2C%20our%20approach%20remains%20fully%0Acompatible%20with%20standard%20inference%20pipelines%2C%20offering%20a%20practical%20and%20scalable%0Asolution%20for%20efficient%20long-context%20LLM%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05165v2&entry.124074799=Read"},
{"title": "Quantum Generative Adversarial Autoencoders: Learning latent\n  representations for quantum data generation", "author": "Naipunnya Raj and Rajiv Sangle and Avinash Singh and Krishna Kumar Sabapathy", "abstract": "  In this work, we introduce the Quantum Generative Adversarial Autoencoder\n(QGAA), a quantum model for generation of quantum data. The QGAA consists of\ntwo components: (a) Quantum Autoencoder (QAE) to compress quantum states, and\n(b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of\nthe trained QAE. This approach imparts the QAE with generative capabilities.\nThe utility of QGAA is demonstrated in two representative scenarios: (a)\ngeneration of pure entangled states, and (b) generation of parameterized\nmolecular ground states for H$_2$ and LiH. The average errors in the energies\nestimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in\nsimulations upto 6 qubits. These results illustrate the potential of QGAA for\nquantum state generation, quantum chemistry, and near-term quantum machine\nlearning applications.\n", "link": "http://arxiv.org/abs/2509.16186v1", "date": "2025-09-19", "relevancy": 2.6021, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5257}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.525}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Generative%20Adversarial%20Autoencoders%3A%20Learning%20latent%0A%20%20representations%20for%20quantum%20data%20generation&body=Title%3A%20Quantum%20Generative%20Adversarial%20Autoencoders%3A%20Learning%20latent%0A%20%20representations%20for%20quantum%20data%20generation%0AAuthor%3A%20Naipunnya%20Raj%20and%20Rajiv%20Sangle%20and%20Avinash%20Singh%20and%20Krishna%20Kumar%20Sabapathy%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20Quantum%20Generative%20Adversarial%20Autoencoder%0A%28QGAA%29%2C%20a%20quantum%20model%20for%20generation%20of%20quantum%20data.%20The%20QGAA%20consists%20of%0Atwo%20components%3A%20%28a%29%20Quantum%20Autoencoder%20%28QAE%29%20to%20compress%20quantum%20states%2C%20and%0A%28b%29%20Quantum%20Generative%20Adversarial%20Network%20%28QGAN%29%20to%20learn%20the%20latent%20space%20of%0Athe%20trained%20QAE.%20This%20approach%20imparts%20the%20QAE%20with%20generative%20capabilities.%0AThe%20utility%20of%20QGAA%20is%20demonstrated%20in%20two%20representative%20scenarios%3A%20%28a%29%0Ageneration%20of%20pure%20entangled%20states%2C%20and%20%28b%29%20generation%20of%20parameterized%0Amolecular%20ground%20states%20for%20H%24_2%24%20and%20LiH.%20The%20average%20errors%20in%20the%20energies%0Aestimated%20by%20the%20trained%20QGAA%20are%200.02%20Ha%20for%20H%24_2%24%20and%200.06%20Ha%20for%20LiH%20in%0Asimulations%20upto%206%20qubits.%20These%20results%20illustrate%20the%20potential%20of%20QGAA%20for%0Aquantum%20state%20generation%2C%20quantum%20chemistry%2C%20and%20near-term%20quantum%20machine%0Alearning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Generative%2520Adversarial%2520Autoencoders%253A%2520Learning%2520latent%250A%2520%2520representations%2520for%2520quantum%2520data%2520generation%26entry.906535625%3DNaipunnya%2520Raj%2520and%2520Rajiv%2520Sangle%2520and%2520Avinash%2520Singh%2520and%2520Krishna%2520Kumar%2520Sabapathy%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Quantum%2520Generative%2520Adversarial%2520Autoencoder%250A%2528QGAA%2529%252C%2520a%2520quantum%2520model%2520for%2520generation%2520of%2520quantum%2520data.%2520The%2520QGAA%2520consists%2520of%250Atwo%2520components%253A%2520%2528a%2529%2520Quantum%2520Autoencoder%2520%2528QAE%2529%2520to%2520compress%2520quantum%2520states%252C%2520and%250A%2528b%2529%2520Quantum%2520Generative%2520Adversarial%2520Network%2520%2528QGAN%2529%2520to%2520learn%2520the%2520latent%2520space%2520of%250Athe%2520trained%2520QAE.%2520This%2520approach%2520imparts%2520the%2520QAE%2520with%2520generative%2520capabilities.%250AThe%2520utility%2520of%2520QGAA%2520is%2520demonstrated%2520in%2520two%2520representative%2520scenarios%253A%2520%2528a%2529%250Ageneration%2520of%2520pure%2520entangled%2520states%252C%2520and%2520%2528b%2529%2520generation%2520of%2520parameterized%250Amolecular%2520ground%2520states%2520for%2520H%2524_2%2524%2520and%2520LiH.%2520The%2520average%2520errors%2520in%2520the%2520energies%250Aestimated%2520by%2520the%2520trained%2520QGAA%2520are%25200.02%2520Ha%2520for%2520H%2524_2%2524%2520and%25200.06%2520Ha%2520for%2520LiH%2520in%250Asimulations%2520upto%25206%2520qubits.%2520These%2520results%2520illustrate%2520the%2520potential%2520of%2520QGAA%2520for%250Aquantum%2520state%2520generation%252C%2520quantum%2520chemistry%252C%2520and%2520near-term%2520quantum%2520machine%250Alearning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Generative%20Adversarial%20Autoencoders%3A%20Learning%20latent%0A%20%20representations%20for%20quantum%20data%20generation&entry.906535625=Naipunnya%20Raj%20and%20Rajiv%20Sangle%20and%20Avinash%20Singh%20and%20Krishna%20Kumar%20Sabapathy&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20Quantum%20Generative%20Adversarial%20Autoencoder%0A%28QGAA%29%2C%20a%20quantum%20model%20for%20generation%20of%20quantum%20data.%20The%20QGAA%20consists%20of%0Atwo%20components%3A%20%28a%29%20Quantum%20Autoencoder%20%28QAE%29%20to%20compress%20quantum%20states%2C%20and%0A%28b%29%20Quantum%20Generative%20Adversarial%20Network%20%28QGAN%29%20to%20learn%20the%20latent%20space%20of%0Athe%20trained%20QAE.%20This%20approach%20imparts%20the%20QAE%20with%20generative%20capabilities.%0AThe%20utility%20of%20QGAA%20is%20demonstrated%20in%20two%20representative%20scenarios%3A%20%28a%29%0Ageneration%20of%20pure%20entangled%20states%2C%20and%20%28b%29%20generation%20of%20parameterized%0Amolecular%20ground%20states%20for%20H%24_2%24%20and%20LiH.%20The%20average%20errors%20in%20the%20energies%0Aestimated%20by%20the%20trained%20QGAA%20are%200.02%20Ha%20for%20H%24_2%24%20and%200.06%20Ha%20for%20LiH%20in%0Asimulations%20upto%206%20qubits.%20These%20results%20illustrate%20the%20potential%20of%20QGAA%20for%0Aquantum%20state%20generation%2C%20quantum%20chemistry%2C%20and%20near-term%20quantum%20machine%0Alearning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16186v1&entry.124074799=Read"},
{"title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "author": "Ailiang Lin and Zhuoyun Li and Kotaro Funakoshi and Manabu Okumura", "abstract": "  Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.\n", "link": "http://arxiv.org/abs/2507.23386v2", "date": "2025-09-19", "relevancy": 2.5912, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models&body=Title%3A%20Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models%0AAuthor%3A%20Ailiang%20Lin%20and%20Zhuoyun%20Li%20and%20Kotaro%20Funakoshi%20and%20Manabu%20Okumura%0AAbstract%3A%20%20%20Decoder-only%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20build%0Aembedding%20models%20that%20effectively%20encode%20the%20semantic%20information%20of%20natural%0Alanguage%20texts%20into%20dense%20vector%20representations%20for%20various%20embedding%20tasks.%0AHowever%2C%20many%20existing%20methods%20primarily%20focus%20on%20removing%20the%20causal%20attention%0Amask%20in%20LLMs%20to%20enable%20bidirectional%20attention%2C%20potentially%20undermining%20the%0Amodel%27s%20ability%20to%20extract%20semantic%20information%20acquired%20during%20pretraining.%0AAdditionally%2C%20leading%20unidirectional%20approaches%20often%20rely%20on%20extra%20input%20text%0Ato%20overcome%20the%20inherent%20limitations%20of%20causal%20attention%2C%20inevitably%20increasing%0Acomputational%20costs.%20In%20this%20work%2C%20we%20propose%20Causal2Vec%2C%20a%20general-purpose%0Aembedding%20model%20tailored%20to%20enhance%20the%20performance%20of%20decoder-only%20LLMs%0Awithout%20altering%20their%20original%20architectures%20or%20introducing%20significant%0Acomputational%20overhead.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20BERT-style%0Amodel%20to%20pre-encode%20the%20input%20text%20into%20a%20single%20Contextual%20token%2C%20which%20is%0Athen%20prepended%20to%20the%20LLM%27s%20input%20sequence%2C%20allowing%20each%20token%20to%20capture%0Acontextualized%20information%20even%20without%20attending%20to%20future%20tokens.%0AFurthermore%2C%20to%20mitigate%20the%20recency%20bias%20introduced%20by%20last-token%20pooling%20and%0Ahelp%20LLMs%20better%20leverage%20the%20semantic%20information%20encoded%20in%20the%20Contextual%0Atoken%2C%20we%20concatenate%20the%20last%20hidden%20states%20of%20Contextual%20and%20EOS%20tokens%20as%0Athe%20final%20text%20embedding.%20In%20practice%2C%20Causal2Vec%20achieves%20state-of-the-art%0Aperformance%20on%20the%20Massive%20Text%20Embeddings%20Benchmark%20%28MTEB%29%20among%20models%0Atrained%20solely%20on%20publicly%20available%20retrieval%20datasets%2C%20while%20reducing%20the%0Arequired%20sequence%20length%20by%20up%20to%2085%25%20and%20inference%20time%20by%20up%20to%2082%25%20compared%0Ato%20best-performing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal2Vec%253A%2520Improving%2520Decoder-only%2520LLMs%2520as%2520Versatile%2520Embedding%2520Models%26entry.906535625%3DAiliang%2520Lin%2520and%2520Zhuoyun%2520Li%2520and%2520Kotaro%2520Funakoshi%2520and%2520Manabu%2520Okumura%26entry.1292438233%3D%2520%2520Decoder-only%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520to%2520build%250Aembedding%2520models%2520that%2520effectively%2520encode%2520the%2520semantic%2520information%2520of%2520natural%250Alanguage%2520texts%2520into%2520dense%2520vector%2520representations%2520for%2520various%2520embedding%2520tasks.%250AHowever%252C%2520many%2520existing%2520methods%2520primarily%2520focus%2520on%2520removing%2520the%2520causal%2520attention%250Amask%2520in%2520LLMs%2520to%2520enable%2520bidirectional%2520attention%252C%2520potentially%2520undermining%2520the%250Amodel%2527s%2520ability%2520to%2520extract%2520semantic%2520information%2520acquired%2520during%2520pretraining.%250AAdditionally%252C%2520leading%2520unidirectional%2520approaches%2520often%2520rely%2520on%2520extra%2520input%2520text%250Ato%2520overcome%2520the%2520inherent%2520limitations%2520of%2520causal%2520attention%252C%2520inevitably%2520increasing%250Acomputational%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Causal2Vec%252C%2520a%2520general-purpose%250Aembedding%2520model%2520tailored%2520to%2520enhance%2520the%2520performance%2520of%2520decoder-only%2520LLMs%250Awithout%2520altering%2520their%2520original%2520architectures%2520or%2520introducing%2520significant%250Acomputational%2520overhead.%2520Specifically%252C%2520we%2520first%2520employ%2520a%2520lightweight%2520BERT-style%250Amodel%2520to%2520pre-encode%2520the%2520input%2520text%2520into%2520a%2520single%2520Contextual%2520token%252C%2520which%2520is%250Athen%2520prepended%2520to%2520the%2520LLM%2527s%2520input%2520sequence%252C%2520allowing%2520each%2520token%2520to%2520capture%250Acontextualized%2520information%2520even%2520without%2520attending%2520to%2520future%2520tokens.%250AFurthermore%252C%2520to%2520mitigate%2520the%2520recency%2520bias%2520introduced%2520by%2520last-token%2520pooling%2520and%250Ahelp%2520LLMs%2520better%2520leverage%2520the%2520semantic%2520information%2520encoded%2520in%2520the%2520Contextual%250Atoken%252C%2520we%2520concatenate%2520the%2520last%2520hidden%2520states%2520of%2520Contextual%2520and%2520EOS%2520tokens%2520as%250Athe%2520final%2520text%2520embedding.%2520In%2520practice%252C%2520Causal2Vec%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520Massive%2520Text%2520Embeddings%2520Benchmark%2520%2528MTEB%2529%2520among%2520models%250Atrained%2520solely%2520on%2520publicly%2520available%2520retrieval%2520datasets%252C%2520while%2520reducing%2520the%250Arequired%2520sequence%2520length%2520by%2520up%2520to%252085%2525%2520and%2520inference%2520time%2520by%2520up%2520to%252082%2525%2520compared%250Ato%2520best-performing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models&entry.906535625=Ailiang%20Lin%20and%20Zhuoyun%20Li%20and%20Kotaro%20Funakoshi%20and%20Manabu%20Okumura&entry.1292438233=%20%20Decoder-only%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20build%0Aembedding%20models%20that%20effectively%20encode%20the%20semantic%20information%20of%20natural%0Alanguage%20texts%20into%20dense%20vector%20representations%20for%20various%20embedding%20tasks.%0AHowever%2C%20many%20existing%20methods%20primarily%20focus%20on%20removing%20the%20causal%20attention%0Amask%20in%20LLMs%20to%20enable%20bidirectional%20attention%2C%20potentially%20undermining%20the%0Amodel%27s%20ability%20to%20extract%20semantic%20information%20acquired%20during%20pretraining.%0AAdditionally%2C%20leading%20unidirectional%20approaches%20often%20rely%20on%20extra%20input%20text%0Ato%20overcome%20the%20inherent%20limitations%20of%20causal%20attention%2C%20inevitably%20increasing%0Acomputational%20costs.%20In%20this%20work%2C%20we%20propose%20Causal2Vec%2C%20a%20general-purpose%0Aembedding%20model%20tailored%20to%20enhance%20the%20performance%20of%20decoder-only%20LLMs%0Awithout%20altering%20their%20original%20architectures%20or%20introducing%20significant%0Acomputational%20overhead.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20BERT-style%0Amodel%20to%20pre-encode%20the%20input%20text%20into%20a%20single%20Contextual%20token%2C%20which%20is%0Athen%20prepended%20to%20the%20LLM%27s%20input%20sequence%2C%20allowing%20each%20token%20to%20capture%0Acontextualized%20information%20even%20without%20attending%20to%20future%20tokens.%0AFurthermore%2C%20to%20mitigate%20the%20recency%20bias%20introduced%20by%20last-token%20pooling%20and%0Ahelp%20LLMs%20better%20leverage%20the%20semantic%20information%20encoded%20in%20the%20Contextual%0Atoken%2C%20we%20concatenate%20the%20last%20hidden%20states%20of%20Contextual%20and%20EOS%20tokens%20as%0Athe%20final%20text%20embedding.%20In%20practice%2C%20Causal2Vec%20achieves%20state-of-the-art%0Aperformance%20on%20the%20Massive%20Text%20Embeddings%20Benchmark%20%28MTEB%29%20among%20models%0Atrained%20solely%20on%20publicly%20available%20retrieval%20datasets%2C%20while%20reducing%20the%0Arequired%20sequence%20length%20by%20up%20to%2085%25%20and%20inference%20time%20by%20up%20to%2082%25%20compared%0Ato%20best-performing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23386v2&entry.124074799=Read"},
{"title": "Cross-Resolution SAR Target Detection Using Structural Hierarchy\n  Adaptation and Reliable Adjacency Alignment", "author": "Jiang Qin and Bin Zou and Haolin Li and Lamei Zhang", "abstract": "  In recent years, continuous improvements in SAR resolution have significantly\nbenefited applications such as urban monitoring and target detection. However,\nthe improvement in resolution leads to increased discrepancies in scattering\ncharacteristics, posing challenges to the generalization ability of target\ndetection models. While domain adaptation technology is a potential solution,\nthe inevitable discrepancies caused by resolution differences often lead to\nblind feature adaptation and unreliable semantic propagation, ultimately\ndegrading the domain adaptation performance. To address these challenges, this\npaper proposes a novel SAR target detection method (termed CR-Net), that\nincorporates structure priors and evidential learning theory into the detection\nmodel, enabling reliable domain adaptation for cross-resolution detection. To\nbe specific, CR-Net integrates Structure-induced Hierarchical Feature\nAdaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA\nmodule is introduced to establish structural correlations between targets and\nachieve structure-aware feature adaptation, thereby enhancing the\ninterpretability of the feature adaptation process. Afterwards, the RSAA module\nis proposed to enhance reliable semantic alignment, by leveraging the secure\nadjacency set to transfer valuable discriminative knowledge from the source\ndomain to the target domain. This further improves the discriminability of the\ndetection model in the target domain. Based on experimental results from\ndifferent-resolution datasets,the proposed CR-Net significantly enhances\ncross-resolution adaptation by preserving intra-domain structures and improving\ndiscriminability. It achieves state-of-the-art (SOTA) performance in\ncross-resolution SAR target detection.\n", "link": "http://arxiv.org/abs/2507.08290v2", "date": "2025-09-19", "relevancy": 2.5865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5098}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Resolution%20SAR%20Target%20Detection%20Using%20Structural%20Hierarchy%0A%20%20Adaptation%20and%20Reliable%20Adjacency%20Alignment&body=Title%3A%20Cross-Resolution%20SAR%20Target%20Detection%20Using%20Structural%20Hierarchy%0A%20%20Adaptation%20and%20Reliable%20Adjacency%20Alignment%0AAuthor%3A%20Jiang%20Qin%20and%20Bin%20Zou%20and%20Haolin%20Li%20and%20Lamei%20Zhang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20continuous%20improvements%20in%20SAR%20resolution%20have%20significantly%0Abenefited%20applications%20such%20as%20urban%20monitoring%20and%20target%20detection.%20However%2C%0Athe%20improvement%20in%20resolution%20leads%20to%20increased%20discrepancies%20in%20scattering%0Acharacteristics%2C%20posing%20challenges%20to%20the%20generalization%20ability%20of%20target%0Adetection%20models.%20While%20domain%20adaptation%20technology%20is%20a%20potential%20solution%2C%0Athe%20inevitable%20discrepancies%20caused%20by%20resolution%20differences%20often%20lead%20to%0Ablind%20feature%20adaptation%20and%20unreliable%20semantic%20propagation%2C%20ultimately%0Adegrading%20the%20domain%20adaptation%20performance.%20To%20address%20these%20challenges%2C%20this%0Apaper%20proposes%20a%20novel%20SAR%20target%20detection%20method%20%28termed%20CR-Net%29%2C%20that%0Aincorporates%20structure%20priors%20and%20evidential%20learning%20theory%20into%20the%20detection%0Amodel%2C%20enabling%20reliable%20domain%20adaptation%20for%20cross-resolution%20detection.%20To%0Abe%20specific%2C%20CR-Net%20integrates%20Structure-induced%20Hierarchical%20Feature%0AAdaptation%20%28SHFA%29%20and%20Reliable%20Structural%20Adjacency%20Alignment%20%28RSAA%29.%20SHFA%0Amodule%20is%20introduced%20to%20establish%20structural%20correlations%20between%20targets%20and%0Aachieve%20structure-aware%20feature%20adaptation%2C%20thereby%20enhancing%20the%0Ainterpretability%20of%20the%20feature%20adaptation%20process.%20Afterwards%2C%20the%20RSAA%20module%0Ais%20proposed%20to%20enhance%20reliable%20semantic%20alignment%2C%20by%20leveraging%20the%20secure%0Aadjacency%20set%20to%20transfer%20valuable%20discriminative%20knowledge%20from%20the%20source%0Adomain%20to%20the%20target%20domain.%20This%20further%20improves%20the%20discriminability%20of%20the%0Adetection%20model%20in%20the%20target%20domain.%20Based%20on%20experimental%20results%20from%0Adifferent-resolution%20datasets%2Cthe%20proposed%20CR-Net%20significantly%20enhances%0Across-resolution%20adaptation%20by%20preserving%20intra-domain%20structures%20and%20improving%0Adiscriminability.%20It%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20in%0Across-resolution%20SAR%20target%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08290v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Resolution%2520SAR%2520Target%2520Detection%2520Using%2520Structural%2520Hierarchy%250A%2520%2520Adaptation%2520and%2520Reliable%2520Adjacency%2520Alignment%26entry.906535625%3DJiang%2520Qin%2520and%2520Bin%2520Zou%2520and%2520Haolin%2520Li%2520and%2520Lamei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520continuous%2520improvements%2520in%2520SAR%2520resolution%2520have%2520significantly%250Abenefited%2520applications%2520such%2520as%2520urban%2520monitoring%2520and%2520target%2520detection.%2520However%252C%250Athe%2520improvement%2520in%2520resolution%2520leads%2520to%2520increased%2520discrepancies%2520in%2520scattering%250Acharacteristics%252C%2520posing%2520challenges%2520to%2520the%2520generalization%2520ability%2520of%2520target%250Adetection%2520models.%2520While%2520domain%2520adaptation%2520technology%2520is%2520a%2520potential%2520solution%252C%250Athe%2520inevitable%2520discrepancies%2520caused%2520by%2520resolution%2520differences%2520often%2520lead%2520to%250Ablind%2520feature%2520adaptation%2520and%2520unreliable%2520semantic%2520propagation%252C%2520ultimately%250Adegrading%2520the%2520domain%2520adaptation%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520SAR%2520target%2520detection%2520method%2520%2528termed%2520CR-Net%2529%252C%2520that%250Aincorporates%2520structure%2520priors%2520and%2520evidential%2520learning%2520theory%2520into%2520the%2520detection%250Amodel%252C%2520enabling%2520reliable%2520domain%2520adaptation%2520for%2520cross-resolution%2520detection.%2520To%250Abe%2520specific%252C%2520CR-Net%2520integrates%2520Structure-induced%2520Hierarchical%2520Feature%250AAdaptation%2520%2528SHFA%2529%2520and%2520Reliable%2520Structural%2520Adjacency%2520Alignment%2520%2528RSAA%2529.%2520SHFA%250Amodule%2520is%2520introduced%2520to%2520establish%2520structural%2520correlations%2520between%2520targets%2520and%250Aachieve%2520structure-aware%2520feature%2520adaptation%252C%2520thereby%2520enhancing%2520the%250Ainterpretability%2520of%2520the%2520feature%2520adaptation%2520process.%2520Afterwards%252C%2520the%2520RSAA%2520module%250Ais%2520proposed%2520to%2520enhance%2520reliable%2520semantic%2520alignment%252C%2520by%2520leveraging%2520the%2520secure%250Aadjacency%2520set%2520to%2520transfer%2520valuable%2520discriminative%2520knowledge%2520from%2520the%2520source%250Adomain%2520to%2520the%2520target%2520domain.%2520This%2520further%2520improves%2520the%2520discriminability%2520of%2520the%250Adetection%2520model%2520in%2520the%2520target%2520domain.%2520Based%2520on%2520experimental%2520results%2520from%250Adifferent-resolution%2520datasets%252Cthe%2520proposed%2520CR-Net%2520significantly%2520enhances%250Across-resolution%2520adaptation%2520by%2520preserving%2520intra-domain%2520structures%2520and%2520improving%250Adiscriminability.%2520It%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%250Across-resolution%2520SAR%2520target%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08290v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Resolution%20SAR%20Target%20Detection%20Using%20Structural%20Hierarchy%0A%20%20Adaptation%20and%20Reliable%20Adjacency%20Alignment&entry.906535625=Jiang%20Qin%20and%20Bin%20Zou%20and%20Haolin%20Li%20and%20Lamei%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20continuous%20improvements%20in%20SAR%20resolution%20have%20significantly%0Abenefited%20applications%20such%20as%20urban%20monitoring%20and%20target%20detection.%20However%2C%0Athe%20improvement%20in%20resolution%20leads%20to%20increased%20discrepancies%20in%20scattering%0Acharacteristics%2C%20posing%20challenges%20to%20the%20generalization%20ability%20of%20target%0Adetection%20models.%20While%20domain%20adaptation%20technology%20is%20a%20potential%20solution%2C%0Athe%20inevitable%20discrepancies%20caused%20by%20resolution%20differences%20often%20lead%20to%0Ablind%20feature%20adaptation%20and%20unreliable%20semantic%20propagation%2C%20ultimately%0Adegrading%20the%20domain%20adaptation%20performance.%20To%20address%20these%20challenges%2C%20this%0Apaper%20proposes%20a%20novel%20SAR%20target%20detection%20method%20%28termed%20CR-Net%29%2C%20that%0Aincorporates%20structure%20priors%20and%20evidential%20learning%20theory%20into%20the%20detection%0Amodel%2C%20enabling%20reliable%20domain%20adaptation%20for%20cross-resolution%20detection.%20To%0Abe%20specific%2C%20CR-Net%20integrates%20Structure-induced%20Hierarchical%20Feature%0AAdaptation%20%28SHFA%29%20and%20Reliable%20Structural%20Adjacency%20Alignment%20%28RSAA%29.%20SHFA%0Amodule%20is%20introduced%20to%20establish%20structural%20correlations%20between%20targets%20and%0Aachieve%20structure-aware%20feature%20adaptation%2C%20thereby%20enhancing%20the%0Ainterpretability%20of%20the%20feature%20adaptation%20process.%20Afterwards%2C%20the%20RSAA%20module%0Ais%20proposed%20to%20enhance%20reliable%20semantic%20alignment%2C%20by%20leveraging%20the%20secure%0Aadjacency%20set%20to%20transfer%20valuable%20discriminative%20knowledge%20from%20the%20source%0Adomain%20to%20the%20target%20domain.%20This%20further%20improves%20the%20discriminability%20of%20the%0Adetection%20model%20in%20the%20target%20domain.%20Based%20on%20experimental%20results%20from%0Adifferent-resolution%20datasets%2Cthe%20proposed%20CR-Net%20significantly%20enhances%0Across-resolution%20adaptation%20by%20preserving%20intra-domain%20structures%20and%20improving%0Adiscriminability.%20It%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20in%0Across-resolution%20SAR%20target%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08290v2&entry.124074799=Read"},
{"title": "TESSERA: Precomputed FAIR Global Pixel Embeddings for Earth\n  Representation and Analysis", "author": "Zhengpeng Feng and Clement Atzberger and Sadiq Jaffer and Jovana Knezevic and Silja Sormunen and Robin Young and Madeline C Lisaius and Markus Immitzer and Toby Jackson and James Ball and David A. Coomes and Anil Madhavapeddy and Andrew Blake and Srinivasan Keshav", "abstract": "  Petabytes of satellite Earth Observation (EO) data are freely available and\ncan address critical global challenges. However, EO data quality is poor due to\nclouds and variable lighting conditions. To address this, practitioners\ntypically use compositing, but this critically removes the temporal\nphenological signal. Moreover, supervised machine learning to map composited\npixels to task-specific classes requires accurately labelled data that are\nrarely available. We present TESSERA, a pixel-oriented foundation model for EO\ntime series that creates 128-dimensional latent embeddings requiring only a few\nlabels for task-specific training to achieve state-of-the-art performance\nacross diverse complex tasks. TESSERA uses two encoders that combine optical\ndata with synthetic aperture radar backscatter coefficients at 10m resolution,\ncreating embeddings fused with a multilayer perceptron to generate annual\nglobal embedding maps. TESSERA closely matches or outperforms state-of-the-art\ntask-specific models and other foundation models across five diverse downstream\ntasks. It is unprecedented in ease of use, scale, and accuracy: no other open\nfoundation model provides precomputed outputs with global, annual coverage at\n10m resolution.\n", "link": "http://arxiv.org/abs/2506.20380v5", "date": "2025-09-19", "relevancy": 2.5632, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TESSERA%3A%20Precomputed%20FAIR%20Global%20Pixel%20Embeddings%20for%20Earth%0A%20%20Representation%20and%20Analysis&body=Title%3A%20TESSERA%3A%20Precomputed%20FAIR%20Global%20Pixel%20Embeddings%20for%20Earth%0A%20%20Representation%20and%20Analysis%0AAuthor%3A%20Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20Toby%20Jackson%20and%20James%20Ball%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav%0AAbstract%3A%20%20%20Petabytes%20of%20satellite%20Earth%20Observation%20%28EO%29%20data%20are%20freely%20available%20and%0Acan%20address%20critical%20global%20challenges.%20However%2C%20EO%20data%20quality%20is%20poor%20due%20to%0Aclouds%20and%20variable%20lighting%20conditions.%20To%20address%20this%2C%20practitioners%0Atypically%20use%20compositing%2C%20but%20this%20critically%20removes%20the%20temporal%0Aphenological%20signal.%20Moreover%2C%20supervised%20machine%20learning%20to%20map%20composited%0Apixels%20to%20task-specific%20classes%20requires%20accurately%20labelled%20data%20that%20are%0Ararely%20available.%20We%20present%20TESSERA%2C%20a%20pixel-oriented%20foundation%20model%20for%20EO%0Atime%20series%20that%20creates%20128-dimensional%20latent%20embeddings%20requiring%20only%20a%20few%0Alabels%20for%20task-specific%20training%20to%20achieve%20state-of-the-art%20performance%0Aacross%20diverse%20complex%20tasks.%20TESSERA%20uses%20two%20encoders%20that%20combine%20optical%0Adata%20with%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010m%20resolution%2C%0Acreating%20embeddings%20fused%20with%20a%20multilayer%20perceptron%20to%20generate%20annual%0Aglobal%20embedding%20maps.%20TESSERA%20closely%20matches%20or%20outperforms%20state-of-the-art%0Atask-specific%20models%20and%20other%20foundation%20models%20across%20five%20diverse%20downstream%0Atasks.%20It%20is%20unprecedented%20in%20ease%20of%20use%2C%20scale%2C%20and%20accuracy%3A%20no%20other%20open%0Afoundation%20model%20provides%20precomputed%20outputs%20with%20global%2C%20annual%20coverage%20at%0A10m%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20380v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTESSERA%253A%2520Precomputed%2520FAIR%2520Global%2520Pixel%2520Embeddings%2520for%2520Earth%250A%2520%2520Representation%2520and%2520Analysis%26entry.906535625%3DZhengpeng%2520Feng%2520and%2520Clement%2520Atzberger%2520and%2520Sadiq%2520Jaffer%2520and%2520Jovana%2520Knezevic%2520and%2520Silja%2520Sormunen%2520and%2520Robin%2520Young%2520and%2520Madeline%2520C%2520Lisaius%2520and%2520Markus%2520Immitzer%2520and%2520Toby%2520Jackson%2520and%2520James%2520Ball%2520and%2520David%2520A.%2520Coomes%2520and%2520Anil%2520Madhavapeddy%2520and%2520Andrew%2520Blake%2520and%2520Srinivasan%2520Keshav%26entry.1292438233%3D%2520%2520Petabytes%2520of%2520satellite%2520Earth%2520Observation%2520%2528EO%2529%2520data%2520are%2520freely%2520available%2520and%250Acan%2520address%2520critical%2520global%2520challenges.%2520However%252C%2520EO%2520data%2520quality%2520is%2520poor%2520due%2520to%250Aclouds%2520and%2520variable%2520lighting%2520conditions.%2520To%2520address%2520this%252C%2520practitioners%250Atypically%2520use%2520compositing%252C%2520but%2520this%2520critically%2520removes%2520the%2520temporal%250Aphenological%2520signal.%2520Moreover%252C%2520supervised%2520machine%2520learning%2520to%2520map%2520composited%250Apixels%2520to%2520task-specific%2520classes%2520requires%2520accurately%2520labelled%2520data%2520that%2520are%250Ararely%2520available.%2520We%2520present%2520TESSERA%252C%2520a%2520pixel-oriented%2520foundation%2520model%2520for%2520EO%250Atime%2520series%2520that%2520creates%2520128-dimensional%2520latent%2520embeddings%2520requiring%2520only%2520a%2520few%250Alabels%2520for%2520task-specific%2520training%2520to%2520achieve%2520state-of-the-art%2520performance%250Aacross%2520diverse%2520complex%2520tasks.%2520TESSERA%2520uses%2520two%2520encoders%2520that%2520combine%2520optical%250Adata%2520with%2520synthetic%2520aperture%2520radar%2520backscatter%2520coefficients%2520at%252010m%2520resolution%252C%250Acreating%2520embeddings%2520fused%2520with%2520a%2520multilayer%2520perceptron%2520to%2520generate%2520annual%250Aglobal%2520embedding%2520maps.%2520TESSERA%2520closely%2520matches%2520or%2520outperforms%2520state-of-the-art%250Atask-specific%2520models%2520and%2520other%2520foundation%2520models%2520across%2520five%2520diverse%2520downstream%250Atasks.%2520It%2520is%2520unprecedented%2520in%2520ease%2520of%2520use%252C%2520scale%252C%2520and%2520accuracy%253A%2520no%2520other%2520open%250Afoundation%2520model%2520provides%2520precomputed%2520outputs%2520with%2520global%252C%2520annual%2520coverage%2520at%250A10m%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20380v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TESSERA%3A%20Precomputed%20FAIR%20Global%20Pixel%20Embeddings%20for%20Earth%0A%20%20Representation%20and%20Analysis&entry.906535625=Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20Toby%20Jackson%20and%20James%20Ball%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav&entry.1292438233=%20%20Petabytes%20of%20satellite%20Earth%20Observation%20%28EO%29%20data%20are%20freely%20available%20and%0Acan%20address%20critical%20global%20challenges.%20However%2C%20EO%20data%20quality%20is%20poor%20due%20to%0Aclouds%20and%20variable%20lighting%20conditions.%20To%20address%20this%2C%20practitioners%0Atypically%20use%20compositing%2C%20but%20this%20critically%20removes%20the%20temporal%0Aphenological%20signal.%20Moreover%2C%20supervised%20machine%20learning%20to%20map%20composited%0Apixels%20to%20task-specific%20classes%20requires%20accurately%20labelled%20data%20that%20are%0Ararely%20available.%20We%20present%20TESSERA%2C%20a%20pixel-oriented%20foundation%20model%20for%20EO%0Atime%20series%20that%20creates%20128-dimensional%20latent%20embeddings%20requiring%20only%20a%20few%0Alabels%20for%20task-specific%20training%20to%20achieve%20state-of-the-art%20performance%0Aacross%20diverse%20complex%20tasks.%20TESSERA%20uses%20two%20encoders%20that%20combine%20optical%0Adata%20with%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010m%20resolution%2C%0Acreating%20embeddings%20fused%20with%20a%20multilayer%20perceptron%20to%20generate%20annual%0Aglobal%20embedding%20maps.%20TESSERA%20closely%20matches%20or%20outperforms%20state-of-the-art%0Atask-specific%20models%20and%20other%20foundation%20models%20across%20five%20diverse%20downstream%0Atasks.%20It%20is%20unprecedented%20in%20ease%20of%20use%2C%20scale%2C%20and%20accuracy%3A%20no%20other%20open%0Afoundation%20model%20provides%20precomputed%20outputs%20with%20global%2C%20annual%20coverage%20at%0A10m%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20380v5&entry.124074799=Read"},
{"title": "Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised\n  Learning with Tensorial Imputation", "author": "Zhangqi Jiang and Tingjin Luo and Xu Yang and Xinyan Liang", "abstract": "  View missing remains a significant challenge in graph-based multi-view\nsemi-supervised learning, hindering their real-world applications. To address\nthis issue, traditional methods introduce a missing indicator matrix and focus\non mining partial structure among existing samples in each view for label\npropagation (LP). However, we argue that these disregarded missing samples\nsometimes induce discontinuous local structures, i.e., sub-clusters, breaking\nthe fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster\nProblem (SCP) would distort graph fusion and degrade classification\nperformance. To alleviate SCP, we propose a novel incomplete multi-view\nsemi-supervised learning method, termed AGF-TI. Firstly, we design an\nadversarial graph fusion scheme to learn a robust consensus graph against the\ndistorted local structure through a min-max framework. By stacking all\nsimilarity matrices into a tensor, we further recover the incomplete structure\nfrom the high-order consistency information based on the low-rank tensor\nlearning. Additionally, the anchor-based strategy is incorporated to reduce the\ncomputational complexity. An efficient alternative optimization algorithm\ncombining a reduced gradient descent method is developed to solve the\nformulated objective, with theoretical convergence. Extensive experimental\nresults on various datasets validate the superiority of our proposed AGF-TI as\ncompared to state-of-the-art methods. Code is available at\nhttps://github.com/ZhangqiJiang07/AGF_TI.\n", "link": "http://arxiv.org/abs/2509.15955v1", "date": "2025-09-19", "relevancy": 2.5614, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Graph%20Fusion%20for%20Incomplete%20Multi-view%20Semi-supervised%0A%20%20Learning%20with%20Tensorial%20Imputation&body=Title%3A%20Adversarial%20Graph%20Fusion%20for%20Incomplete%20Multi-view%20Semi-supervised%0A%20%20Learning%20with%20Tensorial%20Imputation%0AAuthor%3A%20Zhangqi%20Jiang%20and%20Tingjin%20Luo%20and%20Xu%20Yang%20and%20Xinyan%20Liang%0AAbstract%3A%20%20%20View%20missing%20remains%20a%20significant%20challenge%20in%20graph-based%20multi-view%0Asemi-supervised%20learning%2C%20hindering%20their%20real-world%20applications.%20To%20address%0Athis%20issue%2C%20traditional%20methods%20introduce%20a%20missing%20indicator%20matrix%20and%20focus%0Aon%20mining%20partial%20structure%20among%20existing%20samples%20in%20each%20view%20for%20label%0Apropagation%20%28LP%29.%20However%2C%20we%20argue%20that%20these%20disregarded%20missing%20samples%0Asometimes%20induce%20discontinuous%20local%20structures%2C%20i.e.%2C%20sub-clusters%2C%20breaking%0Athe%20fundamental%20smoothness%20assumption%20in%20LP.%20Consequently%2C%20such%20a%20Sub-Cluster%0AProblem%20%28SCP%29%20would%20distort%20graph%20fusion%20and%20degrade%20classification%0Aperformance.%20To%20alleviate%20SCP%2C%20we%20propose%20a%20novel%20incomplete%20multi-view%0Asemi-supervised%20learning%20method%2C%20termed%20AGF-TI.%20Firstly%2C%20we%20design%20an%0Aadversarial%20graph%20fusion%20scheme%20to%20learn%20a%20robust%20consensus%20graph%20against%20the%0Adistorted%20local%20structure%20through%20a%20min-max%20framework.%20By%20stacking%20all%0Asimilarity%20matrices%20into%20a%20tensor%2C%20we%20further%20recover%20the%20incomplete%20structure%0Afrom%20the%20high-order%20consistency%20information%20based%20on%20the%20low-rank%20tensor%0Alearning.%20Additionally%2C%20the%20anchor-based%20strategy%20is%20incorporated%20to%20reduce%20the%0Acomputational%20complexity.%20An%20efficient%20alternative%20optimization%20algorithm%0Acombining%20a%20reduced%20gradient%20descent%20method%20is%20developed%20to%20solve%20the%0Aformulated%20objective%2C%20with%20theoretical%20convergence.%20Extensive%20experimental%0Aresults%20on%20various%20datasets%20validate%20the%20superiority%20of%20our%20proposed%20AGF-TI%20as%0Acompared%20to%20state-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ZhangqiJiang07/AGF_TI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Graph%2520Fusion%2520for%2520Incomplete%2520Multi-view%2520Semi-supervised%250A%2520%2520Learning%2520with%2520Tensorial%2520Imputation%26entry.906535625%3DZhangqi%2520Jiang%2520and%2520Tingjin%2520Luo%2520and%2520Xu%2520Yang%2520and%2520Xinyan%2520Liang%26entry.1292438233%3D%2520%2520View%2520missing%2520remains%2520a%2520significant%2520challenge%2520in%2520graph-based%2520multi-view%250Asemi-supervised%2520learning%252C%2520hindering%2520their%2520real-world%2520applications.%2520To%2520address%250Athis%2520issue%252C%2520traditional%2520methods%2520introduce%2520a%2520missing%2520indicator%2520matrix%2520and%2520focus%250Aon%2520mining%2520partial%2520structure%2520among%2520existing%2520samples%2520in%2520each%2520view%2520for%2520label%250Apropagation%2520%2528LP%2529.%2520However%252C%2520we%2520argue%2520that%2520these%2520disregarded%2520missing%2520samples%250Asometimes%2520induce%2520discontinuous%2520local%2520structures%252C%2520i.e.%252C%2520sub-clusters%252C%2520breaking%250Athe%2520fundamental%2520smoothness%2520assumption%2520in%2520LP.%2520Consequently%252C%2520such%2520a%2520Sub-Cluster%250AProblem%2520%2528SCP%2529%2520would%2520distort%2520graph%2520fusion%2520and%2520degrade%2520classification%250Aperformance.%2520To%2520alleviate%2520SCP%252C%2520we%2520propose%2520a%2520novel%2520incomplete%2520multi-view%250Asemi-supervised%2520learning%2520method%252C%2520termed%2520AGF-TI.%2520Firstly%252C%2520we%2520design%2520an%250Aadversarial%2520graph%2520fusion%2520scheme%2520to%2520learn%2520a%2520robust%2520consensus%2520graph%2520against%2520the%250Adistorted%2520local%2520structure%2520through%2520a%2520min-max%2520framework.%2520By%2520stacking%2520all%250Asimilarity%2520matrices%2520into%2520a%2520tensor%252C%2520we%2520further%2520recover%2520the%2520incomplete%2520structure%250Afrom%2520the%2520high-order%2520consistency%2520information%2520based%2520on%2520the%2520low-rank%2520tensor%250Alearning.%2520Additionally%252C%2520the%2520anchor-based%2520strategy%2520is%2520incorporated%2520to%2520reduce%2520the%250Acomputational%2520complexity.%2520An%2520efficient%2520alternative%2520optimization%2520algorithm%250Acombining%2520a%2520reduced%2520gradient%2520descent%2520method%2520is%2520developed%2520to%2520solve%2520the%250Aformulated%2520objective%252C%2520with%2520theoretical%2520convergence.%2520Extensive%2520experimental%250Aresults%2520on%2520various%2520datasets%2520validate%2520the%2520superiority%2520of%2520our%2520proposed%2520AGF-TI%2520as%250Acompared%2520to%2520state-of-the-art%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ZhangqiJiang07/AGF_TI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Graph%20Fusion%20for%20Incomplete%20Multi-view%20Semi-supervised%0A%20%20Learning%20with%20Tensorial%20Imputation&entry.906535625=Zhangqi%20Jiang%20and%20Tingjin%20Luo%20and%20Xu%20Yang%20and%20Xinyan%20Liang&entry.1292438233=%20%20View%20missing%20remains%20a%20significant%20challenge%20in%20graph-based%20multi-view%0Asemi-supervised%20learning%2C%20hindering%20their%20real-world%20applications.%20To%20address%0Athis%20issue%2C%20traditional%20methods%20introduce%20a%20missing%20indicator%20matrix%20and%20focus%0Aon%20mining%20partial%20structure%20among%20existing%20samples%20in%20each%20view%20for%20label%0Apropagation%20%28LP%29.%20However%2C%20we%20argue%20that%20these%20disregarded%20missing%20samples%0Asometimes%20induce%20discontinuous%20local%20structures%2C%20i.e.%2C%20sub-clusters%2C%20breaking%0Athe%20fundamental%20smoothness%20assumption%20in%20LP.%20Consequently%2C%20such%20a%20Sub-Cluster%0AProblem%20%28SCP%29%20would%20distort%20graph%20fusion%20and%20degrade%20classification%0Aperformance.%20To%20alleviate%20SCP%2C%20we%20propose%20a%20novel%20incomplete%20multi-view%0Asemi-supervised%20learning%20method%2C%20termed%20AGF-TI.%20Firstly%2C%20we%20design%20an%0Aadversarial%20graph%20fusion%20scheme%20to%20learn%20a%20robust%20consensus%20graph%20against%20the%0Adistorted%20local%20structure%20through%20a%20min-max%20framework.%20By%20stacking%20all%0Asimilarity%20matrices%20into%20a%20tensor%2C%20we%20further%20recover%20the%20incomplete%20structure%0Afrom%20the%20high-order%20consistency%20information%20based%20on%20the%20low-rank%20tensor%0Alearning.%20Additionally%2C%20the%20anchor-based%20strategy%20is%20incorporated%20to%20reduce%20the%0Acomputational%20complexity.%20An%20efficient%20alternative%20optimization%20algorithm%0Acombining%20a%20reduced%20gradient%20descent%20method%20is%20developed%20to%20solve%20the%0Aformulated%20objective%2C%20with%20theoretical%20convergence.%20Extensive%20experimental%0Aresults%20on%20various%20datasets%20validate%20the%20superiority%20of%20our%20proposed%20AGF-TI%20as%0Acompared%20to%20state-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ZhangqiJiang07/AGF_TI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15955v1&entry.124074799=Read"},
{"title": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments", "author": "Jinwen Tang and Qiming Guo and Wenbo Sun and Yi Shang", "abstract": "  Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening.\n", "link": "http://arxiv.org/abs/2501.13951v3", "date": "2025-09-19", "relevancy": 2.5514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Layered%20Multi-Expert%20Framework%20for%20Long-Context%20Mental%20Health%0A%20%20Assessments&body=Title%3A%20A%20Layered%20Multi-Expert%20Framework%20for%20Long-Context%20Mental%20Health%0A%20%20Assessments%0AAuthor%3A%20Jinwen%20Tang%20and%20Qiming%20Guo%20and%20Wenbo%20Sun%20and%20Yi%20Shang%0AAbstract%3A%20%20%20Long-form%20mental%20health%20assessments%20pose%20unique%20challenges%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20which%20often%20exhibit%20hallucinations%20or%20inconsistent%20reasoning%0Awhen%20handling%20extended%2C%20domain-specific%20contexts.%20We%20introduce%20Stacked%0AMulti-Model%20Reasoning%20%28SMMR%29%2C%20a%20layered%20framework%20that%20leverages%20multiple%20LLMs%0Aand%20specialized%20smaller%20models%20as%20coequal%20%27experts%27.%20Early%20layers%20isolate%0Ashort%2C%20discrete%20subtasks%2C%20while%20later%20layers%20integrate%20and%20refine%20these%20partial%0Aoutputs%20through%20more%20advanced%20long-context%20models.%20We%20evaluate%20SMMR%20on%20the%0ADAIC-WOZ%20depression-screening%20dataset%20and%2048%20curated%20case%20studies%20with%0Apsychiatric%20diagnoses%2C%20demonstrating%20consistent%20improvements%20over%20single-model%0Abaselines%20in%20terms%20of%20accuracy%2C%20F1-score%2C%20and%20PHQ-8%20error%20reduction.%20By%0Aharnessing%20diverse%20%27second%20opinions%27%2C%20SMMR%20mitigates%20hallucinations%2C%20captures%0Asubtle%20clinical%20nuances%2C%20and%20enhances%20reliability%20in%20high-stakes%20mental%20health%0Aassessments.%20Our%20findings%20underscore%20the%20value%20of%20multi-expert%20frameworks%20for%0Amore%20trustworthy%20AI-driven%20screening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13951v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Layered%2520Multi-Expert%2520Framework%2520for%2520Long-Context%2520Mental%2520Health%250A%2520%2520Assessments%26entry.906535625%3DJinwen%2520Tang%2520and%2520Qiming%2520Guo%2520and%2520Wenbo%2520Sun%2520and%2520Yi%2520Shang%26entry.1292438233%3D%2520%2520Long-form%2520mental%2520health%2520assessments%2520pose%2520unique%2520challenges%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520which%2520often%2520exhibit%2520hallucinations%2520or%2520inconsistent%2520reasoning%250Awhen%2520handling%2520extended%252C%2520domain-specific%2520contexts.%2520We%2520introduce%2520Stacked%250AMulti-Model%2520Reasoning%2520%2528SMMR%2529%252C%2520a%2520layered%2520framework%2520that%2520leverages%2520multiple%2520LLMs%250Aand%2520specialized%2520smaller%2520models%2520as%2520coequal%2520%2527experts%2527.%2520Early%2520layers%2520isolate%250Ashort%252C%2520discrete%2520subtasks%252C%2520while%2520later%2520layers%2520integrate%2520and%2520refine%2520these%2520partial%250Aoutputs%2520through%2520more%2520advanced%2520long-context%2520models.%2520We%2520evaluate%2520SMMR%2520on%2520the%250ADAIC-WOZ%2520depression-screening%2520dataset%2520and%252048%2520curated%2520case%2520studies%2520with%250Apsychiatric%2520diagnoses%252C%2520demonstrating%2520consistent%2520improvements%2520over%2520single-model%250Abaselines%2520in%2520terms%2520of%2520accuracy%252C%2520F1-score%252C%2520and%2520PHQ-8%2520error%2520reduction.%2520By%250Aharnessing%2520diverse%2520%2527second%2520opinions%2527%252C%2520SMMR%2520mitigates%2520hallucinations%252C%2520captures%250Asubtle%2520clinical%2520nuances%252C%2520and%2520enhances%2520reliability%2520in%2520high-stakes%2520mental%2520health%250Aassessments.%2520Our%2520findings%2520underscore%2520the%2520value%2520of%2520multi-expert%2520frameworks%2520for%250Amore%2520trustworthy%2520AI-driven%2520screening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13951v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Layered%20Multi-Expert%20Framework%20for%20Long-Context%20Mental%20Health%0A%20%20Assessments&entry.906535625=Jinwen%20Tang%20and%20Qiming%20Guo%20and%20Wenbo%20Sun%20and%20Yi%20Shang&entry.1292438233=%20%20Long-form%20mental%20health%20assessments%20pose%20unique%20challenges%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20which%20often%20exhibit%20hallucinations%20or%20inconsistent%20reasoning%0Awhen%20handling%20extended%2C%20domain-specific%20contexts.%20We%20introduce%20Stacked%0AMulti-Model%20Reasoning%20%28SMMR%29%2C%20a%20layered%20framework%20that%20leverages%20multiple%20LLMs%0Aand%20specialized%20smaller%20models%20as%20coequal%20%27experts%27.%20Early%20layers%20isolate%0Ashort%2C%20discrete%20subtasks%2C%20while%20later%20layers%20integrate%20and%20refine%20these%20partial%0Aoutputs%20through%20more%20advanced%20long-context%20models.%20We%20evaluate%20SMMR%20on%20the%0ADAIC-WOZ%20depression-screening%20dataset%20and%2048%20curated%20case%20studies%20with%0Apsychiatric%20diagnoses%2C%20demonstrating%20consistent%20improvements%20over%20single-model%0Abaselines%20in%20terms%20of%20accuracy%2C%20F1-score%2C%20and%20PHQ-8%20error%20reduction.%20By%0Aharnessing%20diverse%20%27second%20opinions%27%2C%20SMMR%20mitigates%20hallucinations%2C%20captures%0Asubtle%20clinical%20nuances%2C%20and%20enhances%20reliability%20in%20high-stakes%20mental%20health%0Aassessments.%20Our%20findings%20underscore%20the%20value%20of%20multi-expert%20frameworks%20for%0Amore%20trustworthy%20AI-driven%20screening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13951v3&entry.124074799=Read"},
{"title": "MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes", "author": "Mohamed Ebbed and Zorah L\u00e4hner", "abstract": "  Dynamic scene reconstruction from multi-view videos remains a fundamental\nchallenge in computer vision. While recent neural surface reconstruction\nmethods have achieved remarkable results in static 3D reconstruction, extending\nthese approaches with comparable quality for dynamic scenes introduces\nsignificant computational and representational challenges. Existing dynamic\nmethods focus on novel-view synthesis, therefore, their extracted meshes tend\nto be noisy. Even approaches aiming for geometric fidelity often result in too\nsmooth meshes due to the ill-posedness of the problem. We present a novel\nframework for highly detailed dynamic reconstruction that extends the static 3D\nreconstruction method NeuralAngelo to work in dynamic settings. To that end, we\nstart with a high-quality template scene reconstruction from the initial frame\nusing NeuralAngelo, and then jointly optimize deformation fields that track the\ntemplate and refine it based on the temporal sequence. This flexible template\nallows updating the geometry to include changes that cannot be modeled with the\ndeformation field, for instance occluded parts or the changes in the topology.\nWe show superior reconstruction accuracy in comparison to previous\nstate-of-the-art methods on the ActorsHQ dataset.\n", "link": "http://arxiv.org/abs/2509.15892v1", "date": "2025-09-19", "relevancy": 2.5485, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6807}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6109}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoAngelo%3A%20Motion-Aware%20Neural%20Surface%20Reconstruction%20for%20Dynamic%20Scenes&body=Title%3A%20MoAngelo%3A%20Motion-Aware%20Neural%20Surface%20Reconstruction%20for%20Dynamic%20Scenes%0AAuthor%3A%20Mohamed%20Ebbed%20and%20Zorah%20L%C3%A4hner%0AAbstract%3A%20%20%20Dynamic%20scene%20reconstruction%20from%20multi-view%20videos%20remains%20a%20fundamental%0Achallenge%20in%20computer%20vision.%20While%20recent%20neural%20surface%20reconstruction%0Amethods%20have%20achieved%20remarkable%20results%20in%20static%203D%20reconstruction%2C%20extending%0Athese%20approaches%20with%20comparable%20quality%20for%20dynamic%20scenes%20introduces%0Asignificant%20computational%20and%20representational%20challenges.%20Existing%20dynamic%0Amethods%20focus%20on%20novel-view%20synthesis%2C%20therefore%2C%20their%20extracted%20meshes%20tend%0Ato%20be%20noisy.%20Even%20approaches%20aiming%20for%20geometric%20fidelity%20often%20result%20in%20too%0Asmooth%20meshes%20due%20to%20the%20ill-posedness%20of%20the%20problem.%20We%20present%20a%20novel%0Aframework%20for%20highly%20detailed%20dynamic%20reconstruction%20that%20extends%20the%20static%203D%0Areconstruction%20method%20NeuralAngelo%20to%20work%20in%20dynamic%20settings.%20To%20that%20end%2C%20we%0Astart%20with%20a%20high-quality%20template%20scene%20reconstruction%20from%20the%20initial%20frame%0Ausing%20NeuralAngelo%2C%20and%20then%20jointly%20optimize%20deformation%20fields%20that%20track%20the%0Atemplate%20and%20refine%20it%20based%20on%20the%20temporal%20sequence.%20This%20flexible%20template%0Aallows%20updating%20the%20geometry%20to%20include%20changes%20that%20cannot%20be%20modeled%20with%20the%0Adeformation%20field%2C%20for%20instance%20occluded%20parts%20or%20the%20changes%20in%20the%20topology.%0AWe%20show%20superior%20reconstruction%20accuracy%20in%20comparison%20to%20previous%0Astate-of-the-art%20methods%20on%20the%20ActorsHQ%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoAngelo%253A%2520Motion-Aware%2520Neural%2520Surface%2520Reconstruction%2520for%2520Dynamic%2520Scenes%26entry.906535625%3DMohamed%2520Ebbed%2520and%2520Zorah%2520L%25C3%25A4hner%26entry.1292438233%3D%2520%2520Dynamic%2520scene%2520reconstruction%2520from%2520multi-view%2520videos%2520remains%2520a%2520fundamental%250Achallenge%2520in%2520computer%2520vision.%2520While%2520recent%2520neural%2520surface%2520reconstruction%250Amethods%2520have%2520achieved%2520remarkable%2520results%2520in%2520static%25203D%2520reconstruction%252C%2520extending%250Athese%2520approaches%2520with%2520comparable%2520quality%2520for%2520dynamic%2520scenes%2520introduces%250Asignificant%2520computational%2520and%2520representational%2520challenges.%2520Existing%2520dynamic%250Amethods%2520focus%2520on%2520novel-view%2520synthesis%252C%2520therefore%252C%2520their%2520extracted%2520meshes%2520tend%250Ato%2520be%2520noisy.%2520Even%2520approaches%2520aiming%2520for%2520geometric%2520fidelity%2520often%2520result%2520in%2520too%250Asmooth%2520meshes%2520due%2520to%2520the%2520ill-posedness%2520of%2520the%2520problem.%2520We%2520present%2520a%2520novel%250Aframework%2520for%2520highly%2520detailed%2520dynamic%2520reconstruction%2520that%2520extends%2520the%2520static%25203D%250Areconstruction%2520method%2520NeuralAngelo%2520to%2520work%2520in%2520dynamic%2520settings.%2520To%2520that%2520end%252C%2520we%250Astart%2520with%2520a%2520high-quality%2520template%2520scene%2520reconstruction%2520from%2520the%2520initial%2520frame%250Ausing%2520NeuralAngelo%252C%2520and%2520then%2520jointly%2520optimize%2520deformation%2520fields%2520that%2520track%2520the%250Atemplate%2520and%2520refine%2520it%2520based%2520on%2520the%2520temporal%2520sequence.%2520This%2520flexible%2520template%250Aallows%2520updating%2520the%2520geometry%2520to%2520include%2520changes%2520that%2520cannot%2520be%2520modeled%2520with%2520the%250Adeformation%2520field%252C%2520for%2520instance%2520occluded%2520parts%2520or%2520the%2520changes%2520in%2520the%2520topology.%250AWe%2520show%2520superior%2520reconstruction%2520accuracy%2520in%2520comparison%2520to%2520previous%250Astate-of-the-art%2520methods%2520on%2520the%2520ActorsHQ%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoAngelo%3A%20Motion-Aware%20Neural%20Surface%20Reconstruction%20for%20Dynamic%20Scenes&entry.906535625=Mohamed%20Ebbed%20and%20Zorah%20L%C3%A4hner&entry.1292438233=%20%20Dynamic%20scene%20reconstruction%20from%20multi-view%20videos%20remains%20a%20fundamental%0Achallenge%20in%20computer%20vision.%20While%20recent%20neural%20surface%20reconstruction%0Amethods%20have%20achieved%20remarkable%20results%20in%20static%203D%20reconstruction%2C%20extending%0Athese%20approaches%20with%20comparable%20quality%20for%20dynamic%20scenes%20introduces%0Asignificant%20computational%20and%20representational%20challenges.%20Existing%20dynamic%0Amethods%20focus%20on%20novel-view%20synthesis%2C%20therefore%2C%20their%20extracted%20meshes%20tend%0Ato%20be%20noisy.%20Even%20approaches%20aiming%20for%20geometric%20fidelity%20often%20result%20in%20too%0Asmooth%20meshes%20due%20to%20the%20ill-posedness%20of%20the%20problem.%20We%20present%20a%20novel%0Aframework%20for%20highly%20detailed%20dynamic%20reconstruction%20that%20extends%20the%20static%203D%0Areconstruction%20method%20NeuralAngelo%20to%20work%20in%20dynamic%20settings.%20To%20that%20end%2C%20we%0Astart%20with%20a%20high-quality%20template%20scene%20reconstruction%20from%20the%20initial%20frame%0Ausing%20NeuralAngelo%2C%20and%20then%20jointly%20optimize%20deformation%20fields%20that%20track%20the%0Atemplate%20and%20refine%20it%20based%20on%20the%20temporal%20sequence.%20This%20flexible%20template%0Aallows%20updating%20the%20geometry%20to%20include%20changes%20that%20cannot%20be%20modeled%20with%20the%0Adeformation%20field%2C%20for%20instance%20occluded%20parts%20or%20the%20changes%20in%20the%20topology.%0AWe%20show%20superior%20reconstruction%20accuracy%20in%20comparison%20to%20previous%0Astate-of-the-art%20methods%20on%20the%20ActorsHQ%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15892v1&entry.124074799=Read"},
{"title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study", "author": "DongGeon Lee and Joonwon Jang and Jihae Jeong and Hwanjo Yu", "abstract": "  Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.\n", "link": "http://arxiv.org/abs/2505.15389v2", "date": "2025-09-19", "relevancy": 2.5402, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&body=Title%3A%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study%0AAuthor%3A%20DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu%0AAbstract%3A%20%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20are%20more%20vulnerable%20to%20meme-based%0Aharmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%20significantly%0Aincrease%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%20text-only%20inputs.%0AThough%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%20elevated%0Avulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%20ecologically%20valid%0Aevaluations%20and%20stronger%20safety%20mechanisms.%20MemeSafetyBench%20is%20publicly%0Aavailable%20at%20https%3A//github.com/oneonlee/Meme-Safety-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision-Language%2520Models%2520Safe%2520in%2520the%2520Wild%253F%2520A%2520Meme-Based%2520Benchmark%250A%2520%2520Study%26entry.906535625%3DDongGeon%2520Lee%2520and%2520Joonwon%2520Jang%2520and%2520Jihae%2520Jeong%2520and%2520Hwanjo%2520Yu%26entry.1292438233%3D%2520%2520Rapid%2520deployment%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520magnifies%2520safety%2520risks%252C%2520yet%250Amost%2520evaluations%2520rely%2520on%2520artificial%2520images.%2520This%2520study%2520asks%253A%2520How%2520safe%2520are%250Acurrent%2520VLMs%2520when%2520confronted%2520with%2520meme%2520images%2520that%2520ordinary%2520users%2520share%253F%2520To%250Ainvestigate%2520this%2520question%252C%2520we%2520introduce%2520MemeSafetyBench%252C%2520a%252050%252C430-instance%250Abenchmark%2520pairing%2520real%2520meme%2520images%2520with%2520both%2520harmful%2520and%2520benign%2520instructions.%250AUsing%2520a%2520comprehensive%2520safety%2520taxonomy%2520and%2520LLM-based%2520instruction%2520generation%252C%2520we%250Aassess%2520multiple%2520VLMs%2520across%2520single%2520and%2520multi-turn%2520interactions.%2520We%2520investigate%250Ahow%2520real-world%2520memes%2520influence%2520harmful%2520outputs%252C%2520the%2520mitigating%2520effects%2520of%250Aconversational%2520context%252C%2520and%2520the%2520relationship%2520between%2520model%2520scale%2520and%2520safety%250Ametrics.%2520Our%2520findings%2520demonstrate%2520that%2520VLMs%2520are%2520more%2520vulnerable%2520to%2520meme-based%250Aharmful%2520prompts%2520than%2520to%2520synthetic%2520or%2520typographic%2520images.%2520Memes%2520significantly%250Aincrease%2520harmful%2520responses%2520and%2520decrease%2520refusals%2520compared%2520to%2520text-only%2520inputs.%250AThough%2520multi-turn%2520interactions%2520provide%2520partial%2520mitigation%252C%2520elevated%250Avulnerability%2520persists.%2520These%2520results%2520highlight%2520the%2520need%2520for%2520ecologically%2520valid%250Aevaluations%2520and%2520stronger%2520safety%2520mechanisms.%2520MemeSafetyBench%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/oneonlee/Meme-Safety-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&entry.906535625=DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu&entry.1292438233=%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20are%20more%20vulnerable%20to%20meme-based%0Aharmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%20significantly%0Aincrease%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%20text-only%20inputs.%0AThough%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%20elevated%0Avulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%20ecologically%20valid%0Aevaluations%20and%20stronger%20safety%20mechanisms.%20MemeSafetyBench%20is%20publicly%0Aavailable%20at%20https%3A//github.com/oneonlee/Meme-Safety-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15389v2&entry.124074799=Read"},
{"title": "Foundational Design Principles and Patterns for Building Robust and\n  Adaptive GenAI-Native Systems", "author": "Frederik Vandeputte", "abstract": "  Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework.\n", "link": "http://arxiv.org/abs/2508.15411v2", "date": "2025-09-19", "relevancy": 2.4812, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5171}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.504}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems&body=Title%3A%20Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems%0AAuthor%3A%20Frederik%20Vandeputte%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20has%20emerged%20as%20a%20transformative%20technology%2C%0Ademonstrating%20remarkable%20capabilities%20across%20diverse%20application%20domains.%0AHowever%2C%20GenAI%20faces%20several%20major%20challenges%20in%20developing%20reliable%20and%0Aefficient%20GenAI-empowered%20systems%20due%20to%20its%20unpredictability%20and%20inefficiency.%0AThis%20paper%20advocates%20for%20a%20paradigm%20shift%3A%20future%20GenAI-native%20systems%20should%0Aintegrate%20GenAI%27s%20cognitive%20capabilities%20with%20traditional%20software%20engineering%0Aprinciples%20to%20create%20robust%2C%20adaptive%2C%20and%20efficient%20systems.%0A%20%20We%20introduce%20foundational%20GenAI-native%20design%20principles%20centered%20around%20five%0Akey%20pillars%20--%20reliability%2C%20excellence%2C%20evolvability%2C%20self-reliance%2C%20and%0Aassurance%20--%20and%20propose%20architectural%20patterns%20such%20as%20GenAI-native%20cells%2C%0Aorganic%20substrates%2C%20and%20programmable%20routers%20to%20guide%20the%20creation%20of%20resilient%0Aand%20self-evolving%20systems.%20Additionally%2C%20we%20outline%20the%20key%20ingredients%20of%20a%0AGenAI-native%20software%20stack%20and%20discuss%20the%20impact%20of%20these%20systems%20from%0Atechnical%2C%20user%20adoption%2C%20economic%2C%20and%20legal%20perspectives%2C%20underscoring%20the%0Aneed%20for%20further%20validation%20and%20experimentation.%20Our%20work%20aims%20to%20inspire%0Afuture%20research%20and%20encourage%20relevant%20communities%20to%20implement%20and%20refine%20this%0Aconceptual%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520Design%2520Principles%2520and%2520Patterns%2520for%2520Building%2520Robust%2520and%250A%2520%2520Adaptive%2520GenAI-Native%2520Systems%26entry.906535625%3DFrederik%2520Vandeputte%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520technology%252C%250Ademonstrating%2520remarkable%2520capabilities%2520across%2520diverse%2520application%2520domains.%250AHowever%252C%2520GenAI%2520faces%2520several%2520major%2520challenges%2520in%2520developing%2520reliable%2520and%250Aefficient%2520GenAI-empowered%2520systems%2520due%2520to%2520its%2520unpredictability%2520and%2520inefficiency.%250AThis%2520paper%2520advocates%2520for%2520a%2520paradigm%2520shift%253A%2520future%2520GenAI-native%2520systems%2520should%250Aintegrate%2520GenAI%2527s%2520cognitive%2520capabilities%2520with%2520traditional%2520software%2520engineering%250Aprinciples%2520to%2520create%2520robust%252C%2520adaptive%252C%2520and%2520efficient%2520systems.%250A%2520%2520We%2520introduce%2520foundational%2520GenAI-native%2520design%2520principles%2520centered%2520around%2520five%250Akey%2520pillars%2520--%2520reliability%252C%2520excellence%252C%2520evolvability%252C%2520self-reliance%252C%2520and%250Aassurance%2520--%2520and%2520propose%2520architectural%2520patterns%2520such%2520as%2520GenAI-native%2520cells%252C%250Aorganic%2520substrates%252C%2520and%2520programmable%2520routers%2520to%2520guide%2520the%2520creation%2520of%2520resilient%250Aand%2520self-evolving%2520systems.%2520Additionally%252C%2520we%2520outline%2520the%2520key%2520ingredients%2520of%2520a%250AGenAI-native%2520software%2520stack%2520and%2520discuss%2520the%2520impact%2520of%2520these%2520systems%2520from%250Atechnical%252C%2520user%2520adoption%252C%2520economic%252C%2520and%2520legal%2520perspectives%252C%2520underscoring%2520the%250Aneed%2520for%2520further%2520validation%2520and%2520experimentation.%2520Our%2520work%2520aims%2520to%2520inspire%250Afuture%2520research%2520and%2520encourage%2520relevant%2520communities%2520to%2520implement%2520and%2520refine%2520this%250Aconceptual%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems&entry.906535625=Frederik%20Vandeputte&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20has%20emerged%20as%20a%20transformative%20technology%2C%0Ademonstrating%20remarkable%20capabilities%20across%20diverse%20application%20domains.%0AHowever%2C%20GenAI%20faces%20several%20major%20challenges%20in%20developing%20reliable%20and%0Aefficient%20GenAI-empowered%20systems%20due%20to%20its%20unpredictability%20and%20inefficiency.%0AThis%20paper%20advocates%20for%20a%20paradigm%20shift%3A%20future%20GenAI-native%20systems%20should%0Aintegrate%20GenAI%27s%20cognitive%20capabilities%20with%20traditional%20software%20engineering%0Aprinciples%20to%20create%20robust%2C%20adaptive%2C%20and%20efficient%20systems.%0A%20%20We%20introduce%20foundational%20GenAI-native%20design%20principles%20centered%20around%20five%0Akey%20pillars%20--%20reliability%2C%20excellence%2C%20evolvability%2C%20self-reliance%2C%20and%0Aassurance%20--%20and%20propose%20architectural%20patterns%20such%20as%20GenAI-native%20cells%2C%0Aorganic%20substrates%2C%20and%20programmable%20routers%20to%20guide%20the%20creation%20of%20resilient%0Aand%20self-evolving%20systems.%20Additionally%2C%20we%20outline%20the%20key%20ingredients%20of%20a%0AGenAI-native%20software%20stack%20and%20discuss%20the%20impact%20of%20these%20systems%20from%0Atechnical%2C%20user%20adoption%2C%20economic%2C%20and%20legal%20perspectives%2C%20underscoring%20the%0Aneed%20for%20further%20validation%20and%20experimentation.%20Our%20work%20aims%20to%20inspire%0Afuture%20research%20and%20encourage%20relevant%20communities%20to%20implement%20and%20refine%20this%0Aconceptual%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15411v2&entry.124074799=Read"},
{"title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions", "author": "Frederic Kirstein and Sonu Kumar and Terry Ruas and Bela Gipp", "abstract": "  Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.\n", "link": "http://arxiv.org/abs/2509.15901v1", "date": "2025-09-19", "relevancy": 2.4788, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%0A%20%20Personalization%20via%20Questions&body=Title%3A%20Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%0A%20%20Personalization%20via%20Questions%0AAuthor%3A%20Frederic%20Kirstein%20and%20Sonu%20Kumar%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Meeting%20summarization%20with%20large%20language%20models%20%28LLMs%29%20remains%20error-prone%2C%0Aoften%20producing%20outputs%20with%20hallucinations%2C%20omissions%2C%20and%20irrelevancies.%20We%0Apresent%20FRAME%2C%20a%20modular%20pipeline%20that%20reframes%20summarization%20as%20a%20semantic%0Aenrichment%20task.%20FRAME%20extracts%20and%20scores%20salient%20facts%2C%20organizes%20them%0Athematically%2C%20and%20uses%20these%20to%20enrich%20an%20outline%20into%20an%20abstractive%20summary.%0ATo%20personalize%20summaries%2C%20we%20introduce%20SCOPE%2C%20a%20reason-out-loud%20protocol%20that%0Ahas%20the%20model%20build%20a%20reasoning%20trace%20by%20answering%20nine%20questions%20before%0Acontent%20selection.%20For%20evaluation%2C%20we%20propose%20P-MESA%2C%20a%20multi-dimensional%2C%0Areference-free%20evaluation%20framework%20to%20assess%20if%20a%20summary%20fits%20a%20target%0Areader.%20P-MESA%20reliably%20identifies%20error%20instances%2C%20achieving%20%3E%3D%2089%25%20balanced%0Aaccuracy%20against%20human%20annotations%20and%20strongly%20aligns%20with%20human%20severity%0Aratings%20%28r%20%3E%3D%200.70%29.%20On%20QMSum%20and%20FAME%2C%20FRAME%20reduces%20hallucination%20and%0Aomission%20by%202%20out%20of%205%20points%20%28measured%20with%20MESA%29%2C%20while%20SCOPE%20improves%0Aknowledge%20fit%20and%20goal%20alignment%20over%20prompt-only%20baselines.%20Our%20findings%0Aadvocate%20for%20rethinking%20summarization%20to%20improve%20control%2C%20faithfulness%2C%20and%0Apersonalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-FRAME%2520the%2520Meeting%2520Summarization%2520SCOPE%253A%2520Fact-Based%2520Summarization%2520and%250A%2520%2520Personalization%2520via%2520Questions%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Sonu%2520Kumar%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Meeting%2520summarization%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520error-prone%252C%250Aoften%2520producing%2520outputs%2520with%2520hallucinations%252C%2520omissions%252C%2520and%2520irrelevancies.%2520We%250Apresent%2520FRAME%252C%2520a%2520modular%2520pipeline%2520that%2520reframes%2520summarization%2520as%2520a%2520semantic%250Aenrichment%2520task.%2520FRAME%2520extracts%2520and%2520scores%2520salient%2520facts%252C%2520organizes%2520them%250Athematically%252C%2520and%2520uses%2520these%2520to%2520enrich%2520an%2520outline%2520into%2520an%2520abstractive%2520summary.%250ATo%2520personalize%2520summaries%252C%2520we%2520introduce%2520SCOPE%252C%2520a%2520reason-out-loud%2520protocol%2520that%250Ahas%2520the%2520model%2520build%2520a%2520reasoning%2520trace%2520by%2520answering%2520nine%2520questions%2520before%250Acontent%2520selection.%2520For%2520evaluation%252C%2520we%2520propose%2520P-MESA%252C%2520a%2520multi-dimensional%252C%250Areference-free%2520evaluation%2520framework%2520to%2520assess%2520if%2520a%2520summary%2520fits%2520a%2520target%250Areader.%2520P-MESA%2520reliably%2520identifies%2520error%2520instances%252C%2520achieving%2520%253E%253D%252089%2525%2520balanced%250Aaccuracy%2520against%2520human%2520annotations%2520and%2520strongly%2520aligns%2520with%2520human%2520severity%250Aratings%2520%2528r%2520%253E%253D%25200.70%2529.%2520On%2520QMSum%2520and%2520FAME%252C%2520FRAME%2520reduces%2520hallucination%2520and%250Aomission%2520by%25202%2520out%2520of%25205%2520points%2520%2528measured%2520with%2520MESA%2529%252C%2520while%2520SCOPE%2520improves%250Aknowledge%2520fit%2520and%2520goal%2520alignment%2520over%2520prompt-only%2520baselines.%2520Our%2520findings%250Aadvocate%2520for%2520rethinking%2520summarization%2520to%2520improve%2520control%252C%2520faithfulness%252C%2520and%250Apersonalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%0A%20%20Personalization%20via%20Questions&entry.906535625=Frederic%20Kirstein%20and%20Sonu%20Kumar%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=%20%20Meeting%20summarization%20with%20large%20language%20models%20%28LLMs%29%20remains%20error-prone%2C%0Aoften%20producing%20outputs%20with%20hallucinations%2C%20omissions%2C%20and%20irrelevancies.%20We%0Apresent%20FRAME%2C%20a%20modular%20pipeline%20that%20reframes%20summarization%20as%20a%20semantic%0Aenrichment%20task.%20FRAME%20extracts%20and%20scores%20salient%20facts%2C%20organizes%20them%0Athematically%2C%20and%20uses%20these%20to%20enrich%20an%20outline%20into%20an%20abstractive%20summary.%0ATo%20personalize%20summaries%2C%20we%20introduce%20SCOPE%2C%20a%20reason-out-loud%20protocol%20that%0Ahas%20the%20model%20build%20a%20reasoning%20trace%20by%20answering%20nine%20questions%20before%0Acontent%20selection.%20For%20evaluation%2C%20we%20propose%20P-MESA%2C%20a%20multi-dimensional%2C%0Areference-free%20evaluation%20framework%20to%20assess%20if%20a%20summary%20fits%20a%20target%0Areader.%20P-MESA%20reliably%20identifies%20error%20instances%2C%20achieving%20%3E%3D%2089%25%20balanced%0Aaccuracy%20against%20human%20annotations%20and%20strongly%20aligns%20with%20human%20severity%0Aratings%20%28r%20%3E%3D%200.70%29.%20On%20QMSum%20and%20FAME%2C%20FRAME%20reduces%20hallucination%20and%0Aomission%20by%202%20out%20of%205%20points%20%28measured%20with%20MESA%29%2C%20while%20SCOPE%20improves%0Aknowledge%20fit%20and%20goal%20alignment%20over%20prompt-only%20baselines.%20Our%20findings%0Aadvocate%20for%20rethinking%20summarization%20to%20improve%20control%2C%20faithfulness%2C%20and%0Apersonalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15901v1&entry.124074799=Read"},
{"title": "Personalized Federated Learning with Heat-Kernel Enhanced Tensorized\n  Multi-View Clustering", "author": "Kristina P. Sinaga", "abstract": "  We present a robust personalized federated learning framework that leverages\nheat-kernel enhanced tensorized multi-view fuzzy c-means clustering with\nadvanced tensor decomposition techniques. Our approach integrates heat-kernel\ncoefficients adapted from quantum field theory with Tucker decomposition and\ncanonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional\ndistance metrics and efficiently represent high-dimensional multi-view\nstructures. The framework employs matriculation and vectorization techniques to\nfacilitate the discovery of hidden structures and multilinear relationships via\nN-way generalized tensors. The proposed method introduces a dual-level\noptimization scheme: local heat-kernel enhanced fuzzy clustering with tensor\ndecomposition operating on order-N input tensors, and federated aggregation of\ntensor factors with privacy-preserving personalization mechanisms. The local\nstage employs tensorized kernel Euclidean distance transformations and Tucker\ndecomposition to discover client-specific patterns in multi-view tensor data,\nwhile the global aggregation process coordinates tensor factors (core tensors\nand factor matrices) across clients through differential privacy-preserving\nprotocols. This tensorized approach enables efficient handling of\nhigh-dimensional multi-view data with significant communication savings through\nlow-rank tensor approximations.\n", "link": "http://arxiv.org/abs/2509.16101v1", "date": "2025-09-19", "relevancy": 2.4744, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Federated%20Learning%20with%20Heat-Kernel%20Enhanced%20Tensorized%0A%20%20Multi-View%20Clustering&body=Title%3A%20Personalized%20Federated%20Learning%20with%20Heat-Kernel%20Enhanced%20Tensorized%0A%20%20Multi-View%20Clustering%0AAuthor%3A%20Kristina%20P.%20Sinaga%0AAbstract%3A%20%20%20We%20present%20a%20robust%20personalized%20federated%20learning%20framework%20that%20leverages%0Aheat-kernel%20enhanced%20tensorized%20multi-view%20fuzzy%20c-means%20clustering%20with%0Aadvanced%20tensor%20decomposition%20techniques.%20Our%20approach%20integrates%20heat-kernel%0Acoefficients%20adapted%20from%20quantum%20field%20theory%20with%20Tucker%20decomposition%20and%0Acanonical%20polyadic%20decomposition%20%28CANDECOMP/PARAFAC%29%20to%20transform%20conventional%0Adistance%20metrics%20and%20efficiently%20represent%20high-dimensional%20multi-view%0Astructures.%20The%20framework%20employs%20matriculation%20and%20vectorization%20techniques%20to%0Afacilitate%20the%20discovery%20of%20hidden%20structures%20and%20multilinear%20relationships%20via%0AN-way%20generalized%20tensors.%20The%20proposed%20method%20introduces%20a%20dual-level%0Aoptimization%20scheme%3A%20local%20heat-kernel%20enhanced%20fuzzy%20clustering%20with%20tensor%0Adecomposition%20operating%20on%20order-N%20input%20tensors%2C%20and%20federated%20aggregation%20of%0Atensor%20factors%20with%20privacy-preserving%20personalization%20mechanisms.%20The%20local%0Astage%20employs%20tensorized%20kernel%20Euclidean%20distance%20transformations%20and%20Tucker%0Adecomposition%20to%20discover%20client-specific%20patterns%20in%20multi-view%20tensor%20data%2C%0Awhile%20the%20global%20aggregation%20process%20coordinates%20tensor%20factors%20%28core%20tensors%0Aand%20factor%20matrices%29%20across%20clients%20through%20differential%20privacy-preserving%0Aprotocols.%20This%20tensorized%20approach%20enables%20efficient%20handling%20of%0Ahigh-dimensional%20multi-view%20data%20with%20significant%20communication%20savings%20through%0Alow-rank%20tensor%20approximations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Federated%2520Learning%2520with%2520Heat-Kernel%2520Enhanced%2520Tensorized%250A%2520%2520Multi-View%2520Clustering%26entry.906535625%3DKristina%2520P.%2520Sinaga%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520robust%2520personalized%2520federated%2520learning%2520framework%2520that%2520leverages%250Aheat-kernel%2520enhanced%2520tensorized%2520multi-view%2520fuzzy%2520c-means%2520clustering%2520with%250Aadvanced%2520tensor%2520decomposition%2520techniques.%2520Our%2520approach%2520integrates%2520heat-kernel%250Acoefficients%2520adapted%2520from%2520quantum%2520field%2520theory%2520with%2520Tucker%2520decomposition%2520and%250Acanonical%2520polyadic%2520decomposition%2520%2528CANDECOMP/PARAFAC%2529%2520to%2520transform%2520conventional%250Adistance%2520metrics%2520and%2520efficiently%2520represent%2520high-dimensional%2520multi-view%250Astructures.%2520The%2520framework%2520employs%2520matriculation%2520and%2520vectorization%2520techniques%2520to%250Afacilitate%2520the%2520discovery%2520of%2520hidden%2520structures%2520and%2520multilinear%2520relationships%2520via%250AN-way%2520generalized%2520tensors.%2520The%2520proposed%2520method%2520introduces%2520a%2520dual-level%250Aoptimization%2520scheme%253A%2520local%2520heat-kernel%2520enhanced%2520fuzzy%2520clustering%2520with%2520tensor%250Adecomposition%2520operating%2520on%2520order-N%2520input%2520tensors%252C%2520and%2520federated%2520aggregation%2520of%250Atensor%2520factors%2520with%2520privacy-preserving%2520personalization%2520mechanisms.%2520The%2520local%250Astage%2520employs%2520tensorized%2520kernel%2520Euclidean%2520distance%2520transformations%2520and%2520Tucker%250Adecomposition%2520to%2520discover%2520client-specific%2520patterns%2520in%2520multi-view%2520tensor%2520data%252C%250Awhile%2520the%2520global%2520aggregation%2520process%2520coordinates%2520tensor%2520factors%2520%2528core%2520tensors%250Aand%2520factor%2520matrices%2529%2520across%2520clients%2520through%2520differential%2520privacy-preserving%250Aprotocols.%2520This%2520tensorized%2520approach%2520enables%2520efficient%2520handling%2520of%250Ahigh-dimensional%2520multi-view%2520data%2520with%2520significant%2520communication%2520savings%2520through%250Alow-rank%2520tensor%2520approximations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Federated%20Learning%20with%20Heat-Kernel%20Enhanced%20Tensorized%0A%20%20Multi-View%20Clustering&entry.906535625=Kristina%20P.%20Sinaga&entry.1292438233=%20%20We%20present%20a%20robust%20personalized%20federated%20learning%20framework%20that%20leverages%0Aheat-kernel%20enhanced%20tensorized%20multi-view%20fuzzy%20c-means%20clustering%20with%0Aadvanced%20tensor%20decomposition%20techniques.%20Our%20approach%20integrates%20heat-kernel%0Acoefficients%20adapted%20from%20quantum%20field%20theory%20with%20Tucker%20decomposition%20and%0Acanonical%20polyadic%20decomposition%20%28CANDECOMP/PARAFAC%29%20to%20transform%20conventional%0Adistance%20metrics%20and%20efficiently%20represent%20high-dimensional%20multi-view%0Astructures.%20The%20framework%20employs%20matriculation%20and%20vectorization%20techniques%20to%0Afacilitate%20the%20discovery%20of%20hidden%20structures%20and%20multilinear%20relationships%20via%0AN-way%20generalized%20tensors.%20The%20proposed%20method%20introduces%20a%20dual-level%0Aoptimization%20scheme%3A%20local%20heat-kernel%20enhanced%20fuzzy%20clustering%20with%20tensor%0Adecomposition%20operating%20on%20order-N%20input%20tensors%2C%20and%20federated%20aggregation%20of%0Atensor%20factors%20with%20privacy-preserving%20personalization%20mechanisms.%20The%20local%0Astage%20employs%20tensorized%20kernel%20Euclidean%20distance%20transformations%20and%20Tucker%0Adecomposition%20to%20discover%20client-specific%20patterns%20in%20multi-view%20tensor%20data%2C%0Awhile%20the%20global%20aggregation%20process%20coordinates%20tensor%20factors%20%28core%20tensors%0Aand%20factor%20matrices%29%20across%20clients%20through%20differential%20privacy-preserving%0Aprotocols.%20This%20tensorized%20approach%20enables%20efficient%20handling%20of%0Ahigh-dimensional%20multi-view%20data%20with%20significant%20communication%20savings%20through%0Alow-rank%20tensor%20approximations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16101v1&entry.124074799=Read"},
{"title": "Two Is Better Than One: Aligned Representation Pairs for Anomaly\n  Detection", "author": "Alain Ryser and Thomas M. Sutter and Alexander Marx and Julia E. Vogt", "abstract": "  Anomaly detection focuses on identifying samples that deviate from the norm.\nDiscovering informative representations of normal samples is crucial to\ndetecting anomalies effectively. Recent self-supervised methods have\nsuccessfully learned such representations by employing prior knowledge about\nanomalies to create synthetic outliers during training. However, we often do\nnot know what to expect from unseen data in specialized real-world\napplications. In this work, we address this limitation with our new approach\nCon$_2$, which leverages prior knowledge about symmetries in normal samples to\nobserve the data in different contexts. Con$_2$ consists of two parts: Context\nContrasting clusters representations according to their context, while Content\nAlignment encourages the model to capture semantic information by aligning the\npositions of normal samples across clusters. The resulting representation space\nallows us to detect anomalies as outliers of the learned context clusters. We\ndemonstrate the benefit of this approach in extensive experiments on\nspecialized medical datasets, outperforming competitive baselines based on\nself-supervised learning and pretrained models and presenting competitive\nperformance on natural imaging benchmarks.\n", "link": "http://arxiv.org/abs/2405.18848v3", "date": "2025-09-19", "relevancy": 2.4676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4831}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Is%20Better%20Than%20One%3A%20Aligned%20Representation%20Pairs%20for%20Anomaly%0A%20%20Detection&body=Title%3A%20Two%20Is%20Better%20Than%20One%3A%20Aligned%20Representation%20Pairs%20for%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Alain%20Ryser%20and%20Thomas%20M.%20Sutter%20and%20Alexander%20Marx%20and%20Julia%20E.%20Vogt%0AAbstract%3A%20%20%20Anomaly%20detection%20focuses%20on%20identifying%20samples%20that%20deviate%20from%20the%20norm.%0ADiscovering%20informative%20representations%20of%20normal%20samples%20is%20crucial%20to%0Adetecting%20anomalies%20effectively.%20Recent%20self-supervised%20methods%20have%0Asuccessfully%20learned%20such%20representations%20by%20employing%20prior%20knowledge%20about%0Aanomalies%20to%20create%20synthetic%20outliers%20during%20training.%20However%2C%20we%20often%20do%0Anot%20know%20what%20to%20expect%20from%20unseen%20data%20in%20specialized%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20address%20this%20limitation%20with%20our%20new%20approach%0ACon%24_2%24%2C%20which%20leverages%20prior%20knowledge%20about%20symmetries%20in%20normal%20samples%20to%0Aobserve%20the%20data%20in%20different%20contexts.%20Con%24_2%24%20consists%20of%20two%20parts%3A%20Context%0AContrasting%20clusters%20representations%20according%20to%20their%20context%2C%20while%20Content%0AAlignment%20encourages%20the%20model%20to%20capture%20semantic%20information%20by%20aligning%20the%0Apositions%20of%20normal%20samples%20across%20clusters.%20The%20resulting%20representation%20space%0Aallows%20us%20to%20detect%20anomalies%20as%20outliers%20of%20the%20learned%20context%20clusters.%20We%0Ademonstrate%20the%20benefit%20of%20this%20approach%20in%20extensive%20experiments%20on%0Aspecialized%20medical%20datasets%2C%20outperforming%20competitive%20baselines%20based%20on%0Aself-supervised%20learning%20and%20pretrained%20models%20and%20presenting%20competitive%0Aperformance%20on%20natural%20imaging%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18848v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Is%2520Better%2520Than%2520One%253A%2520Aligned%2520Representation%2520Pairs%2520for%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DAlain%2520Ryser%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Alexander%2520Marx%2520and%2520Julia%2520E.%2520Vogt%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520focuses%2520on%2520identifying%2520samples%2520that%2520deviate%2520from%2520the%2520norm.%250ADiscovering%2520informative%2520representations%2520of%2520normal%2520samples%2520is%2520crucial%2520to%250Adetecting%2520anomalies%2520effectively.%2520Recent%2520self-supervised%2520methods%2520have%250Asuccessfully%2520learned%2520such%2520representations%2520by%2520employing%2520prior%2520knowledge%2520about%250Aanomalies%2520to%2520create%2520synthetic%2520outliers%2520during%2520training.%2520However%252C%2520we%2520often%2520do%250Anot%2520know%2520what%2520to%2520expect%2520from%2520unseen%2520data%2520in%2520specialized%2520real-world%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520limitation%2520with%2520our%2520new%2520approach%250ACon%2524_2%2524%252C%2520which%2520leverages%2520prior%2520knowledge%2520about%2520symmetries%2520in%2520normal%2520samples%2520to%250Aobserve%2520the%2520data%2520in%2520different%2520contexts.%2520Con%2524_2%2524%2520consists%2520of%2520two%2520parts%253A%2520Context%250AContrasting%2520clusters%2520representations%2520according%2520to%2520their%2520context%252C%2520while%2520Content%250AAlignment%2520encourages%2520the%2520model%2520to%2520capture%2520semantic%2520information%2520by%2520aligning%2520the%250Apositions%2520of%2520normal%2520samples%2520across%2520clusters.%2520The%2520resulting%2520representation%2520space%250Aallows%2520us%2520to%2520detect%2520anomalies%2520as%2520outliers%2520of%2520the%2520learned%2520context%2520clusters.%2520We%250Ademonstrate%2520the%2520benefit%2520of%2520this%2520approach%2520in%2520extensive%2520experiments%2520on%250Aspecialized%2520medical%2520datasets%252C%2520outperforming%2520competitive%2520baselines%2520based%2520on%250Aself-supervised%2520learning%2520and%2520pretrained%2520models%2520and%2520presenting%2520competitive%250Aperformance%2520on%2520natural%2520imaging%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18848v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Is%20Better%20Than%20One%3A%20Aligned%20Representation%20Pairs%20for%20Anomaly%0A%20%20Detection&entry.906535625=Alain%20Ryser%20and%20Thomas%20M.%20Sutter%20and%20Alexander%20Marx%20and%20Julia%20E.%20Vogt&entry.1292438233=%20%20Anomaly%20detection%20focuses%20on%20identifying%20samples%20that%20deviate%20from%20the%20norm.%0ADiscovering%20informative%20representations%20of%20normal%20samples%20is%20crucial%20to%0Adetecting%20anomalies%20effectively.%20Recent%20self-supervised%20methods%20have%0Asuccessfully%20learned%20such%20representations%20by%20employing%20prior%20knowledge%20about%0Aanomalies%20to%20create%20synthetic%20outliers%20during%20training.%20However%2C%20we%20often%20do%0Anot%20know%20what%20to%20expect%20from%20unseen%20data%20in%20specialized%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20address%20this%20limitation%20with%20our%20new%20approach%0ACon%24_2%24%2C%20which%20leverages%20prior%20knowledge%20about%20symmetries%20in%20normal%20samples%20to%0Aobserve%20the%20data%20in%20different%20contexts.%20Con%24_2%24%20consists%20of%20two%20parts%3A%20Context%0AContrasting%20clusters%20representations%20according%20to%20their%20context%2C%20while%20Content%0AAlignment%20encourages%20the%20model%20to%20capture%20semantic%20information%20by%20aligning%20the%0Apositions%20of%20normal%20samples%20across%20clusters.%20The%20resulting%20representation%20space%0Aallows%20us%20to%20detect%20anomalies%20as%20outliers%20of%20the%20learned%20context%20clusters.%20We%0Ademonstrate%20the%20benefit%20of%20this%20approach%20in%20extensive%20experiments%20on%0Aspecialized%20medical%20datasets%2C%20outperforming%20competitive%20baselines%20based%20on%0Aself-supervised%20learning%20and%20pretrained%20models%20and%20presenting%20competitive%0Aperformance%20on%20natural%20imaging%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18848v3&entry.124074799=Read"},
{"title": "Riemannian Batch Normalization: A Gyro Approach", "author": "Ziheng Chen and Xiao-Jun Wu and Bernhard Sch\u00f6lkopf and Nicu Sebe", "abstract": "  Normalization layers are crucial for deep learning, but their Euclidean\nformulations are inadequate for data on manifolds. On the other hand, many\nRiemannian manifolds in machine learning admit gyro-structures, enabling\nprincipled extensions of Euclidean neural networks to non-Euclidean domains.\nInspired by this, we introduce GyroBN, a principled Riemannian batch\nnormalization framework for gyrogroups. We establish two necessary conditions,\nnamely \\emph{pseudo-reduction} and \\emph{gyroisometric gyrations}, that\nguarantee GyroBN with theoretical control over sample statistics, and show that\nthese conditions hold for all known gyrogroups in machine learning. Our\nframework also incorporates several existing Riemannian normalization methods\nas special cases. We further instantiate GyroBN on seven representative\ngeometries, including the Grassmannian, five constant curvature spaces, and the\ncorrelation manifold, and derive novel gyro and Riemannian structures to enable\nthese instantiations. Experiments across these geometries demonstrate the\neffectiveness of GyroBN. The code is available at\nhttps://github.com/GitZH-Chen/GyroBN.git.\n", "link": "http://arxiv.org/abs/2509.07115v2", "date": "2025-09-19", "relevancy": 2.4577, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5083}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Batch%20Normalization%3A%20A%20Gyro%20Approach&body=Title%3A%20Riemannian%20Batch%20Normalization%3A%20A%20Gyro%20Approach%0AAuthor%3A%20Ziheng%20Chen%20and%20Xiao-Jun%20Wu%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Normalization%20layers%20are%20crucial%20for%20deep%20learning%2C%20but%20their%20Euclidean%0Aformulations%20are%20inadequate%20for%20data%20on%20manifolds.%20On%20the%20other%20hand%2C%20many%0ARiemannian%20manifolds%20in%20machine%20learning%20admit%20gyro-structures%2C%20enabling%0Aprincipled%20extensions%20of%20Euclidean%20neural%20networks%20to%20non-Euclidean%20domains.%0AInspired%20by%20this%2C%20we%20introduce%20GyroBN%2C%20a%20principled%20Riemannian%20batch%0Anormalization%20framework%20for%20gyrogroups.%20We%20establish%20two%20necessary%20conditions%2C%0Anamely%20%5Cemph%7Bpseudo-reduction%7D%20and%20%5Cemph%7Bgyroisometric%20gyrations%7D%2C%20that%0Aguarantee%20GyroBN%20with%20theoretical%20control%20over%20sample%20statistics%2C%20and%20show%20that%0Athese%20conditions%20hold%20for%20all%20known%20gyrogroups%20in%20machine%20learning.%20Our%0Aframework%20also%20incorporates%20several%20existing%20Riemannian%20normalization%20methods%0Aas%20special%20cases.%20We%20further%20instantiate%20GyroBN%20on%20seven%20representative%0Ageometries%2C%20including%20the%20Grassmannian%2C%20five%20constant%20curvature%20spaces%2C%20and%20the%0Acorrelation%20manifold%2C%20and%20derive%20novel%20gyro%20and%20Riemannian%20structures%20to%20enable%0Athese%20instantiations.%20Experiments%20across%20these%20geometries%20demonstrate%20the%0Aeffectiveness%20of%20GyroBN.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/GitZH-Chen/GyroBN.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Batch%2520Normalization%253A%2520A%2520Gyro%2520Approach%26entry.906535625%3DZiheng%2520Chen%2520and%2520Xiao-Jun%2520Wu%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Normalization%2520layers%2520are%2520crucial%2520for%2520deep%2520learning%252C%2520but%2520their%2520Euclidean%250Aformulations%2520are%2520inadequate%2520for%2520data%2520on%2520manifolds.%2520On%2520the%2520other%2520hand%252C%2520many%250ARiemannian%2520manifolds%2520in%2520machine%2520learning%2520admit%2520gyro-structures%252C%2520enabling%250Aprincipled%2520extensions%2520of%2520Euclidean%2520neural%2520networks%2520to%2520non-Euclidean%2520domains.%250AInspired%2520by%2520this%252C%2520we%2520introduce%2520GyroBN%252C%2520a%2520principled%2520Riemannian%2520batch%250Anormalization%2520framework%2520for%2520gyrogroups.%2520We%2520establish%2520two%2520necessary%2520conditions%252C%250Anamely%2520%255Cemph%257Bpseudo-reduction%257D%2520and%2520%255Cemph%257Bgyroisometric%2520gyrations%257D%252C%2520that%250Aguarantee%2520GyroBN%2520with%2520theoretical%2520control%2520over%2520sample%2520statistics%252C%2520and%2520show%2520that%250Athese%2520conditions%2520hold%2520for%2520all%2520known%2520gyrogroups%2520in%2520machine%2520learning.%2520Our%250Aframework%2520also%2520incorporates%2520several%2520existing%2520Riemannian%2520normalization%2520methods%250Aas%2520special%2520cases.%2520We%2520further%2520instantiate%2520GyroBN%2520on%2520seven%2520representative%250Ageometries%252C%2520including%2520the%2520Grassmannian%252C%2520five%2520constant%2520curvature%2520spaces%252C%2520and%2520the%250Acorrelation%2520manifold%252C%2520and%2520derive%2520novel%2520gyro%2520and%2520Riemannian%2520structures%2520to%2520enable%250Athese%2520instantiations.%2520Experiments%2520across%2520these%2520geometries%2520demonstrate%2520the%250Aeffectiveness%2520of%2520GyroBN.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/GitZH-Chen/GyroBN.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Batch%20Normalization%3A%20A%20Gyro%20Approach&entry.906535625=Ziheng%20Chen%20and%20Xiao-Jun%20Wu%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Nicu%20Sebe&entry.1292438233=%20%20Normalization%20layers%20are%20crucial%20for%20deep%20learning%2C%20but%20their%20Euclidean%0Aformulations%20are%20inadequate%20for%20data%20on%20manifolds.%20On%20the%20other%20hand%2C%20many%0ARiemannian%20manifolds%20in%20machine%20learning%20admit%20gyro-structures%2C%20enabling%0Aprincipled%20extensions%20of%20Euclidean%20neural%20networks%20to%20non-Euclidean%20domains.%0AInspired%20by%20this%2C%20we%20introduce%20GyroBN%2C%20a%20principled%20Riemannian%20batch%0Anormalization%20framework%20for%20gyrogroups.%20We%20establish%20two%20necessary%20conditions%2C%0Anamely%20%5Cemph%7Bpseudo-reduction%7D%20and%20%5Cemph%7Bgyroisometric%20gyrations%7D%2C%20that%0Aguarantee%20GyroBN%20with%20theoretical%20control%20over%20sample%20statistics%2C%20and%20show%20that%0Athese%20conditions%20hold%20for%20all%20known%20gyrogroups%20in%20machine%20learning.%20Our%0Aframework%20also%20incorporates%20several%20existing%20Riemannian%20normalization%20methods%0Aas%20special%20cases.%20We%20further%20instantiate%20GyroBN%20on%20seven%20representative%0Ageometries%2C%20including%20the%20Grassmannian%2C%20five%20constant%20curvature%20spaces%2C%20and%20the%0Acorrelation%20manifold%2C%20and%20derive%20novel%20gyro%20and%20Riemannian%20structures%20to%20enable%0Athese%20instantiations.%20Experiments%20across%20these%20geometries%20demonstrate%20the%0Aeffectiveness%20of%20GyroBN.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/GitZH-Chen/GyroBN.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07115v2&entry.124074799=Read"},
{"title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation", "author": "Yang Zhou and Shiyu Zhao and Yuxiao Chen and Zhenting Wang and Can Jin and Dimitris N. Metaxas", "abstract": "  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n", "link": "http://arxiv.org/abs/2503.13794v6", "date": "2025-09-19", "relevancy": 2.4391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&body=Title%3A%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation%0AAuthor%3A%20Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13794v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLED%253A%2520LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2520without%2520Human%2520Curated%250A%2520%2520Data%2520Generation%26entry.906535625%3DYang%2520Zhou%2520and%2520Shiyu%2520Zhao%2520and%2520Yuxiao%2520Chen%2520and%2520Zhenting%2520Wang%2520and%2520Can%2520Jin%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520Large%2520foundation%2520models%2520trained%2520on%2520large-scale%2520vision-language%2520data%2520can%2520boost%250AOpen-Vocabulary%2520Object%2520Detection%2520%2528OVD%2529%2520via%2520synthetic%2520training%2520data%252C%2520yet%2520the%250Ahand-crafted%2520pipelines%2520often%2520introduce%2520bias%2520and%2520overfit%2520to%2520specific%2520prompts.%2520We%250Asidestep%2520this%2520issue%2520by%2520directly%2520fusing%2520hidden%2520states%2520from%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520into%2520detectors-an%2520avenue%2520surprisingly%2520under-explored.%2520This%2520paper%250Apresents%2520a%2520systematic%2520method%2520to%2520enhance%2520visual%2520grounding%2520by%2520utilizing%2520decoder%250Alayers%2520of%2520the%2520LLM%2520of%2520an%2520MLLM.%2520We%2520introduce%2520a%2520zero-initialized%2520cross-attention%250Aadapter%2520to%2520enable%2520efficient%2520knowledge%2520fusion%2520from%2520LLMs%2520to%2520object%2520detectors%252C%2520a%250Anew%2520approach%2520called%2520LED%2520%2528LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2529.%2520We%250Afind%2520that%2520intermediate%2520LLM%2520layers%2520already%2520encode%2520rich%2520spatial%2520semantics%253B%250Aadapting%2520only%2520the%2520early%2520layers%2520yields%2520most%2520of%2520the%2520gain.%2520With%2520Swin-T%2520as%2520the%250Avision%2520encoder%252C%2520Qwen2-0.5B%2520%252B%2520LED%2520lifts%2520GroundingDINO%2520by%25203.82%2520%2525%2520on%2520OmniLabel%2520at%250Ajust%25208.7%2520%2525%2520extra%2520GFLOPs%252C%2520and%2520a%2520larger%2520vision%2520backbone%2520pushes%2520the%2520improvement%2520to%250A6.22%2520%2525.%2520Extensive%2520ablations%2520on%2520adapter%2520variants%252C%2520LLM%2520scales%2520and%2520fusion%2520depths%250Afurther%2520corroborate%2520our%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13794v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&entry.906535625=Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13794v6&entry.124074799=Read"},
{"title": "Permutation recovery of spikes in noisy high-dimensional tensor\n  estimation", "author": "G\u00e9rard Ben Arous and C\u00e9dric Gerbelot and Vanessa Piccolo", "abstract": "  We study the dynamics of gradient flow in high dimensions for the\nmulti-spiked tensor problem, where the goal is to estimate $r$ unknown signal\nvectors (spikes) from noisy Gaussian tensor observations. Specifically, we\nanalyze the maximum likelihood estimation procedure, which involves optimizing\na highly nonconvex random function. We determine the sample complexity required\nfor gradient flow to efficiently recover all spikes, without imposing any\nassumptions on the separation of the signal-to-noise ratios (SNRs). More\nprecisely, our results provide the sample complexity required to guarantee\nrecovery of the spikes up to a permutation. Our work builds on our companion\npaper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and\ndetermines the sample complexity and separation conditions for the SNRs\nnecessary for ensuring exact recovery of the spikes (where the recovered\npermutation matches the identity). During the recovery process, the\ncorrelations between the estimators and the hidden vectors increase in a\nsequential manner. The order in which these correlations become significant\ndepends on their initial values and the corresponding SNRs, which ultimately\ndetermines the permutation of the recovered spikes.\n", "link": "http://arxiv.org/abs/2412.14650v3", "date": "2025-09-19", "relevancy": 2.4359, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.49}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4869}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Permutation%20recovery%20of%20spikes%20in%20noisy%20high-dimensional%20tensor%0A%20%20estimation&body=Title%3A%20Permutation%20recovery%20of%20spikes%20in%20noisy%20high-dimensional%20tensor%0A%20%20estimation%0AAuthor%3A%20G%C3%A9rard%20Ben%20Arous%20and%20C%C3%A9dric%20Gerbelot%20and%20Vanessa%20Piccolo%0AAbstract%3A%20%20%20We%20study%20the%20dynamics%20of%20gradient%20flow%20in%20high%20dimensions%20for%20the%0Amulti-spiked%20tensor%20problem%2C%20where%20the%20goal%20is%20to%20estimate%20%24r%24%20unknown%20signal%0Avectors%20%28spikes%29%20from%20noisy%20Gaussian%20tensor%20observations.%20Specifically%2C%20we%0Aanalyze%20the%20maximum%20likelihood%20estimation%20procedure%2C%20which%20involves%20optimizing%0Aa%20highly%20nonconvex%20random%20function.%20We%20determine%20the%20sample%20complexity%20required%0Afor%20gradient%20flow%20to%20efficiently%20recover%20all%20spikes%2C%20without%20imposing%20any%0Aassumptions%20on%20the%20separation%20of%20the%20signal-to-noise%20ratios%20%28SNRs%29.%20More%0Aprecisely%2C%20our%20results%20provide%20the%20sample%20complexity%20required%20to%20guarantee%0Arecovery%20of%20the%20spikes%20up%20to%20a%20permutation.%20Our%20work%20builds%20on%20our%20companion%0Apaper%20%5BBen%20Arous%2C%20Gerbelot%2C%20Piccolo%202024%5D%2C%20which%20studies%20Langevin%20dynamics%20and%0Adetermines%20the%20sample%20complexity%20and%20separation%20conditions%20for%20the%20SNRs%0Anecessary%20for%20ensuring%20exact%20recovery%20of%20the%20spikes%20%28where%20the%20recovered%0Apermutation%20matches%20the%20identity%29.%20During%20the%20recovery%20process%2C%20the%0Acorrelations%20between%20the%20estimators%20and%20the%20hidden%20vectors%20increase%20in%20a%0Asequential%20manner.%20The%20order%20in%20which%20these%20correlations%20become%20significant%0Adepends%20on%20their%20initial%20values%20and%20the%20corresponding%20SNRs%2C%20which%20ultimately%0Adetermines%20the%20permutation%20of%20the%20recovered%20spikes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14650v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPermutation%2520recovery%2520of%2520spikes%2520in%2520noisy%2520high-dimensional%2520tensor%250A%2520%2520estimation%26entry.906535625%3DG%25C3%25A9rard%2520Ben%2520Arous%2520and%2520C%25C3%25A9dric%2520Gerbelot%2520and%2520Vanessa%2520Piccolo%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520dynamics%2520of%2520gradient%2520flow%2520in%2520high%2520dimensions%2520for%2520the%250Amulti-spiked%2520tensor%2520problem%252C%2520where%2520the%2520goal%2520is%2520to%2520estimate%2520%2524r%2524%2520unknown%2520signal%250Avectors%2520%2528spikes%2529%2520from%2520noisy%2520Gaussian%2520tensor%2520observations.%2520Specifically%252C%2520we%250Aanalyze%2520the%2520maximum%2520likelihood%2520estimation%2520procedure%252C%2520which%2520involves%2520optimizing%250Aa%2520highly%2520nonconvex%2520random%2520function.%2520We%2520determine%2520the%2520sample%2520complexity%2520required%250Afor%2520gradient%2520flow%2520to%2520efficiently%2520recover%2520all%2520spikes%252C%2520without%2520imposing%2520any%250Aassumptions%2520on%2520the%2520separation%2520of%2520the%2520signal-to-noise%2520ratios%2520%2528SNRs%2529.%2520More%250Aprecisely%252C%2520our%2520results%2520provide%2520the%2520sample%2520complexity%2520required%2520to%2520guarantee%250Arecovery%2520of%2520the%2520spikes%2520up%2520to%2520a%2520permutation.%2520Our%2520work%2520builds%2520on%2520our%2520companion%250Apaper%2520%255BBen%2520Arous%252C%2520Gerbelot%252C%2520Piccolo%25202024%255D%252C%2520which%2520studies%2520Langevin%2520dynamics%2520and%250Adetermines%2520the%2520sample%2520complexity%2520and%2520separation%2520conditions%2520for%2520the%2520SNRs%250Anecessary%2520for%2520ensuring%2520exact%2520recovery%2520of%2520the%2520spikes%2520%2528where%2520the%2520recovered%250Apermutation%2520matches%2520the%2520identity%2529.%2520During%2520the%2520recovery%2520process%252C%2520the%250Acorrelations%2520between%2520the%2520estimators%2520and%2520the%2520hidden%2520vectors%2520increase%2520in%2520a%250Asequential%2520manner.%2520The%2520order%2520in%2520which%2520these%2520correlations%2520become%2520significant%250Adepends%2520on%2520their%2520initial%2520values%2520and%2520the%2520corresponding%2520SNRs%252C%2520which%2520ultimately%250Adetermines%2520the%2520permutation%2520of%2520the%2520recovered%2520spikes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14650v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Permutation%20recovery%20of%20spikes%20in%20noisy%20high-dimensional%20tensor%0A%20%20estimation&entry.906535625=G%C3%A9rard%20Ben%20Arous%20and%20C%C3%A9dric%20Gerbelot%20and%20Vanessa%20Piccolo&entry.1292438233=%20%20We%20study%20the%20dynamics%20of%20gradient%20flow%20in%20high%20dimensions%20for%20the%0Amulti-spiked%20tensor%20problem%2C%20where%20the%20goal%20is%20to%20estimate%20%24r%24%20unknown%20signal%0Avectors%20%28spikes%29%20from%20noisy%20Gaussian%20tensor%20observations.%20Specifically%2C%20we%0Aanalyze%20the%20maximum%20likelihood%20estimation%20procedure%2C%20which%20involves%20optimizing%0Aa%20highly%20nonconvex%20random%20function.%20We%20determine%20the%20sample%20complexity%20required%0Afor%20gradient%20flow%20to%20efficiently%20recover%20all%20spikes%2C%20without%20imposing%20any%0Aassumptions%20on%20the%20separation%20of%20the%20signal-to-noise%20ratios%20%28SNRs%29.%20More%0Aprecisely%2C%20our%20results%20provide%20the%20sample%20complexity%20required%20to%20guarantee%0Arecovery%20of%20the%20spikes%20up%20to%20a%20permutation.%20Our%20work%20builds%20on%20our%20companion%0Apaper%20%5BBen%20Arous%2C%20Gerbelot%2C%20Piccolo%202024%5D%2C%20which%20studies%20Langevin%20dynamics%20and%0Adetermines%20the%20sample%20complexity%20and%20separation%20conditions%20for%20the%20SNRs%0Anecessary%20for%20ensuring%20exact%20recovery%20of%20the%20spikes%20%28where%20the%20recovered%0Apermutation%20matches%20the%20identity%29.%20During%20the%20recovery%20process%2C%20the%0Acorrelations%20between%20the%20estimators%20and%20the%20hidden%20vectors%20increase%20in%20a%0Asequential%20manner.%20The%20order%20in%20which%20these%20correlations%20become%20significant%0Adepends%20on%20their%20initial%20values%20and%20the%20corresponding%20SNRs%2C%20which%20ultimately%0Adetermines%20the%20permutation%20of%20the%20recovered%20spikes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14650v3&entry.124074799=Read"},
{"title": "Interpreting Learned Feedback Patterns in Large Language Models", "author": "Luke Marks and Amir Abdullah and Clement Neo and Rauno Arike and David Krueger and Philip Torr and Fazl Barez", "abstract": "  Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs.\n", "link": "http://arxiv.org/abs/2310.08164v6", "date": "2025-09-19", "relevancy": 2.424, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models&body=Title%3A%20Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models%0AAuthor%3A%20Luke%20Marks%20and%20Amir%20Abdullah%20and%20Clement%20Neo%20and%20Rauno%20Arike%20and%20David%20Krueger%20and%20Philip%20Torr%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20widely%20used%20to%20train%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20unclear%20whether%20LLMs%20accurately%0Alearn%20the%20underlying%20preferences%20in%20human%20feedback%20data.%20We%20coin%20the%20term%0A%5Ctextit%7BLearned%20Feedback%20Pattern%7D%20%28LFP%29%20for%20patterns%20in%20an%20LLM%27s%20activations%0Alearned%20during%20RLHF%20that%20improve%20its%20performance%20on%20the%20fine-tuning%20task.%20We%0Ahypothesize%20that%20LLMs%20with%20LFPs%20accurately%20aligned%20to%20the%20fine-tuning%20feedback%0Aexhibit%20consistent%20activation%20patterns%20for%20outputs%20that%20would%20have%20received%0Asimilar%20feedback%20during%20RLHF.%20To%20test%20this%2C%20we%20train%20probes%20to%20estimate%20the%0Afeedback%20signal%20implicit%20in%20the%20activations%20of%20a%20fine-tuned%20LLM.%20We%20then%0Acompare%20these%20estimates%20to%20the%20true%20feedback%2C%20measuring%20how%20accurate%20the%20LFPs%0Aare%20to%20the%20fine-tuning%20feedback.%20Our%20probes%20are%20trained%20on%20a%20condensed%2C%20sparse%0Aand%20interpretable%20representation%20of%20LLM%20activations%2C%20making%20it%20easier%20to%0Acorrelate%20features%20of%20the%20input%20with%20our%20probe%27s%20predictions.%20We%20validate%20our%0Aprobes%20by%20comparing%20the%20neural%20features%20they%20correlate%20with%20positive%20feedback%0Ainputs%20against%20the%20features%20GPT-4%20describes%20and%20classifies%20as%20related%20to%20LFPs.%0AUnderstanding%20LFPs%20can%20help%20minimize%20discrepancies%20between%20LLM%20behavior%20and%0Atraining%20objectives%2C%20which%20is%20essential%20for%20the%20safety%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08164v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Learned%2520Feedback%2520Patterns%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DLuke%2520Marks%2520and%2520Amir%2520Abdullah%2520and%2520Clement%2520Neo%2520and%2520Rauno%2520Arike%2520and%2520David%2520Krueger%2520and%2520Philip%2520Torr%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520is%2520widely%2520used%2520to%2520train%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520LLMs%2520accurately%250Alearn%2520the%2520underlying%2520preferences%2520in%2520human%2520feedback%2520data.%2520We%2520coin%2520the%2520term%250A%255Ctextit%257BLearned%2520Feedback%2520Pattern%257D%2520%2528LFP%2529%2520for%2520patterns%2520in%2520an%2520LLM%2527s%2520activations%250Alearned%2520during%2520RLHF%2520that%2520improve%2520its%2520performance%2520on%2520the%2520fine-tuning%2520task.%2520We%250Ahypothesize%2520that%2520LLMs%2520with%2520LFPs%2520accurately%2520aligned%2520to%2520the%2520fine-tuning%2520feedback%250Aexhibit%2520consistent%2520activation%2520patterns%2520for%2520outputs%2520that%2520would%2520have%2520received%250Asimilar%2520feedback%2520during%2520RLHF.%2520To%2520test%2520this%252C%2520we%2520train%2520probes%2520to%2520estimate%2520the%250Afeedback%2520signal%2520implicit%2520in%2520the%2520activations%2520of%2520a%2520fine-tuned%2520LLM.%2520We%2520then%250Acompare%2520these%2520estimates%2520to%2520the%2520true%2520feedback%252C%2520measuring%2520how%2520accurate%2520the%2520LFPs%250Aare%2520to%2520the%2520fine-tuning%2520feedback.%2520Our%2520probes%2520are%2520trained%2520on%2520a%2520condensed%252C%2520sparse%250Aand%2520interpretable%2520representation%2520of%2520LLM%2520activations%252C%2520making%2520it%2520easier%2520to%250Acorrelate%2520features%2520of%2520the%2520input%2520with%2520our%2520probe%2527s%2520predictions.%2520We%2520validate%2520our%250Aprobes%2520by%2520comparing%2520the%2520neural%2520features%2520they%2520correlate%2520with%2520positive%2520feedback%250Ainputs%2520against%2520the%2520features%2520GPT-4%2520describes%2520and%2520classifies%2520as%2520related%2520to%2520LFPs.%250AUnderstanding%2520LFPs%2520can%2520help%2520minimize%2520discrepancies%2520between%2520LLM%2520behavior%2520and%250Atraining%2520objectives%252C%2520which%2520is%2520essential%2520for%2520the%2520safety%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08164v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Learned%20Feedback%20Patterns%20in%20Large%20Language%20Models&entry.906535625=Luke%20Marks%20and%20Amir%20Abdullah%20and%20Clement%20Neo%20and%20Rauno%20Arike%20and%20David%20Krueger%20and%20Philip%20Torr%20and%20Fazl%20Barez&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20widely%20used%20to%20train%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20unclear%20whether%20LLMs%20accurately%0Alearn%20the%20underlying%20preferences%20in%20human%20feedback%20data.%20We%20coin%20the%20term%0A%5Ctextit%7BLearned%20Feedback%20Pattern%7D%20%28LFP%29%20for%20patterns%20in%20an%20LLM%27s%20activations%0Alearned%20during%20RLHF%20that%20improve%20its%20performance%20on%20the%20fine-tuning%20task.%20We%0Ahypothesize%20that%20LLMs%20with%20LFPs%20accurately%20aligned%20to%20the%20fine-tuning%20feedback%0Aexhibit%20consistent%20activation%20patterns%20for%20outputs%20that%20would%20have%20received%0Asimilar%20feedback%20during%20RLHF.%20To%20test%20this%2C%20we%20train%20probes%20to%20estimate%20the%0Afeedback%20signal%20implicit%20in%20the%20activations%20of%20a%20fine-tuned%20LLM.%20We%20then%0Acompare%20these%20estimates%20to%20the%20true%20feedback%2C%20measuring%20how%20accurate%20the%20LFPs%0Aare%20to%20the%20fine-tuning%20feedback.%20Our%20probes%20are%20trained%20on%20a%20condensed%2C%20sparse%0Aand%20interpretable%20representation%20of%20LLM%20activations%2C%20making%20it%20easier%20to%0Acorrelate%20features%20of%20the%20input%20with%20our%20probe%27s%20predictions.%20We%20validate%20our%0Aprobes%20by%20comparing%20the%20neural%20features%20they%20correlate%20with%20positive%20feedback%0Ainputs%20against%20the%20features%20GPT-4%20describes%20and%20classifies%20as%20related%20to%20LFPs.%0AUnderstanding%20LFPs%20can%20help%20minimize%20discrepancies%20between%20LLM%20behavior%20and%0Atraining%20objectives%2C%20which%20is%20essential%20for%20the%20safety%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08164v6&entry.124074799=Read"},
{"title": "Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models", "author": "Hao Dong and Moru Liu and Kaiyang Zhou and Eleni Chatzi and Juho Kannala and Cyrill Stachniss and Olga Fink", "abstract": "  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n", "link": "http://arxiv.org/abs/2501.18592v4", "date": "2025-09-19", "relevancy": 2.3892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6271}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&body=Title%3A%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models%0AAuthor%3A%20Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18592v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Multimodal%2520Adaptation%2520and%2520Generalization%253A%2520From%2520Traditional%250A%2520%2520Approaches%2520to%2520Foundation%2520Models%26entry.906535625%3DHao%2520Dong%2520and%2520Moru%2520Liu%2520and%2520Kaiyang%2520Zhou%2520and%2520Eleni%2520Chatzi%2520and%2520Juho%2520Kannala%2520and%2520Cyrill%2520Stachniss%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520achieving%2520domain%2520adaptation%2520and%2520generalization%2520poses%250Asignificant%2520challenges%252C%2520as%2520models%2520must%2520adapt%2520to%2520or%2520generalize%2520across%2520unknown%250Atarget%2520distributions.%2520Extending%2520these%2520capabilities%2520to%2520unseen%2520multimodal%250Adistributions%252C%2520i.e.%252C%2520multimodal%2520domain%2520adaptation%2520and%2520generalization%252C%2520is%2520even%250Amore%2520challenging%2520due%2520to%2520the%2520distinct%2520characteristics%2520of%2520different%2520modalities.%250ASignificant%2520progress%2520has%2520been%2520made%2520over%2520the%2520years%252C%2520with%2520applications%2520ranging%250Afrom%2520action%2520recognition%2520to%2520semantic%2520segmentation.%2520Besides%252C%2520the%2520recent%2520advent%2520of%250Alarge-scale%2520pre-trained%2520multimodal%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520has%250Ainspired%2520works%2520leveraging%2520these%2520models%2520to%2520enhance%2520adaptation%2520and%2520generalization%250Aperformances%2520or%2520adapting%2520them%2520to%2520downstream%2520tasks.%2520This%2520survey%2520provides%2520the%250Afirst%2520comprehensive%2520review%2520of%2520recent%2520advances%2520from%2520traditional%2520approaches%2520to%250Afoundation%2520models%252C%2520covering%253A%2520%25281%2529%2520Multimodal%2520domain%2520adaptation%253B%2520%25282%2529%2520Multimodal%250Atest-time%2520adaptation%253B%2520%25283%2529%2520Multimodal%2520domain%2520generalization%253B%2520%25284%2529%2520Domain%250Aadaptation%2520and%2520generalization%2520with%2520the%2520help%2520of%2520multimodal%2520foundation%2520models%253B%250Aand%2520%25285%2529%2520Adaptation%2520of%2520multimodal%2520foundation%2520models.%2520For%2520each%2520topic%252C%2520we%2520formally%250Adefine%2520the%2520problem%2520and%2520thoroughly%2520review%2520existing%2520methods.%2520Additionally%252C%2520we%250Aanalyze%2520relevant%2520datasets%2520and%2520applications%252C%2520highlighting%2520open%2520challenges%2520and%250Apotential%2520future%2520research%2520directions.%2520We%2520maintain%2520an%2520active%2520repository%2520that%250Acontains%2520up-to-date%2520literature%2520at%250Ahttps%253A//github.com/donghao51/Awesome-Multimodal-Adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18592v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&entry.906535625=Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18592v4&entry.124074799=Read"},
{"title": "UniTac2Pose: A Unified Approach Learned in Simulation for Category-level\n  Visuotactile In-hand Pose Estimation", "author": "Mingdong Wu and Long Yang and Jin Liu and Weiyao Huang and Lehong Wu and Zelin Chen and Daolin Ma and Hao Dong", "abstract": "  Accurate estimation of the in-hand pose of an object based on its CAD model\nis crucial in both industrial applications and everyday tasks, ranging from\npositioning workpieces and assembling components to seamlessly inserting\ndevices like USB connectors. While existing methods often rely on regression,\nfeature matching, or registration techniques, achieving high precision and\ngeneralizability to unseen CAD models remains a significant challenge. In this\npaper, we propose a novel three-stage framework for in-hand pose estimation.\nThe first stage involves sampling and pre-ranking pose candidates, followed by\niterative refinement of these candidates in the second stage. In the final\nstage, post-ranking is applied to identify the most likely pose candidates.\nThese stages are governed by a unified energy-based diffusion model, which is\ntrained solely on simulated data. This energy model simultaneously generates\ngradients to refine pose estimates and produces an energy scalar that\nquantifies the quality of the pose estimates. Additionally, borrowing the idea\nfrom the computer vision domain, we incorporate a render-compare architecture\nwithin the energy-based score network to significantly enhance sim-to-real\nperformance, as demonstrated by our ablation studies. We conduct comprehensive\nexperiments to show that our method outperforms conventional baselines based on\nregression, matching, and registration techniques, while also exhibiting strong\nintra-category generalization to previously unseen CAD models. Moreover, our\napproach integrates tactile object pose estimation, pose tracking, and\nuncertainty estimation into a unified framework, enabling robust performance\nacross a variety of real-world conditions.\n", "link": "http://arxiv.org/abs/2509.15934v1", "date": "2025-09-19", "relevancy": 2.3863, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTac2Pose%3A%20A%20Unified%20Approach%20Learned%20in%20Simulation%20for%20Category-level%0A%20%20Visuotactile%20In-hand%20Pose%20Estimation&body=Title%3A%20UniTac2Pose%3A%20A%20Unified%20Approach%20Learned%20in%20Simulation%20for%20Category-level%0A%20%20Visuotactile%20In-hand%20Pose%20Estimation%0AAuthor%3A%20Mingdong%20Wu%20and%20Long%20Yang%20and%20Jin%20Liu%20and%20Weiyao%20Huang%20and%20Lehong%20Wu%20and%20Zelin%20Chen%20and%20Daolin%20Ma%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Accurate%20estimation%20of%20the%20in-hand%20pose%20of%20an%20object%20based%20on%20its%20CAD%20model%0Ais%20crucial%20in%20both%20industrial%20applications%20and%20everyday%20tasks%2C%20ranging%20from%0Apositioning%20workpieces%20and%20assembling%20components%20to%20seamlessly%20inserting%0Adevices%20like%20USB%20connectors.%20While%20existing%20methods%20often%20rely%20on%20regression%2C%0Afeature%20matching%2C%20or%20registration%20techniques%2C%20achieving%20high%20precision%20and%0Ageneralizability%20to%20unseen%20CAD%20models%20remains%20a%20significant%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20three-stage%20framework%20for%20in-hand%20pose%20estimation.%0AThe%20first%20stage%20involves%20sampling%20and%20pre-ranking%20pose%20candidates%2C%20followed%20by%0Aiterative%20refinement%20of%20these%20candidates%20in%20the%20second%20stage.%20In%20the%20final%0Astage%2C%20post-ranking%20is%20applied%20to%20identify%20the%20most%20likely%20pose%20candidates.%0AThese%20stages%20are%20governed%20by%20a%20unified%20energy-based%20diffusion%20model%2C%20which%20is%0Atrained%20solely%20on%20simulated%20data.%20This%20energy%20model%20simultaneously%20generates%0Agradients%20to%20refine%20pose%20estimates%20and%20produces%20an%20energy%20scalar%20that%0Aquantifies%20the%20quality%20of%20the%20pose%20estimates.%20Additionally%2C%20borrowing%20the%20idea%0Afrom%20the%20computer%20vision%20domain%2C%20we%20incorporate%20a%20render-compare%20architecture%0Awithin%20the%20energy-based%20score%20network%20to%20significantly%20enhance%20sim-to-real%0Aperformance%2C%20as%20demonstrated%20by%20our%20ablation%20studies.%20We%20conduct%20comprehensive%0Aexperiments%20to%20show%20that%20our%20method%20outperforms%20conventional%20baselines%20based%20on%0Aregression%2C%20matching%2C%20and%20registration%20techniques%2C%20while%20also%20exhibiting%20strong%0Aintra-category%20generalization%20to%20previously%20unseen%20CAD%20models.%20Moreover%2C%20our%0Aapproach%20integrates%20tactile%20object%20pose%20estimation%2C%20pose%20tracking%2C%20and%0Auncertainty%20estimation%20into%20a%20unified%20framework%2C%20enabling%20robust%20performance%0Aacross%20a%20variety%20of%20real-world%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTac2Pose%253A%2520A%2520Unified%2520Approach%2520Learned%2520in%2520Simulation%2520for%2520Category-level%250A%2520%2520Visuotactile%2520In-hand%2520Pose%2520Estimation%26entry.906535625%3DMingdong%2520Wu%2520and%2520Long%2520Yang%2520and%2520Jin%2520Liu%2520and%2520Weiyao%2520Huang%2520and%2520Lehong%2520Wu%2520and%2520Zelin%2520Chen%2520and%2520Daolin%2520Ma%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520Accurate%2520estimation%2520of%2520the%2520in-hand%2520pose%2520of%2520an%2520object%2520based%2520on%2520its%2520CAD%2520model%250Ais%2520crucial%2520in%2520both%2520industrial%2520applications%2520and%2520everyday%2520tasks%252C%2520ranging%2520from%250Apositioning%2520workpieces%2520and%2520assembling%2520components%2520to%2520seamlessly%2520inserting%250Adevices%2520like%2520USB%2520connectors.%2520While%2520existing%2520methods%2520often%2520rely%2520on%2520regression%252C%250Afeature%2520matching%252C%2520or%2520registration%2520techniques%252C%2520achieving%2520high%2520precision%2520and%250Ageneralizability%2520to%2520unseen%2520CAD%2520models%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520three-stage%2520framework%2520for%2520in-hand%2520pose%2520estimation.%250AThe%2520first%2520stage%2520involves%2520sampling%2520and%2520pre-ranking%2520pose%2520candidates%252C%2520followed%2520by%250Aiterative%2520refinement%2520of%2520these%2520candidates%2520in%2520the%2520second%2520stage.%2520In%2520the%2520final%250Astage%252C%2520post-ranking%2520is%2520applied%2520to%2520identify%2520the%2520most%2520likely%2520pose%2520candidates.%250AThese%2520stages%2520are%2520governed%2520by%2520a%2520unified%2520energy-based%2520diffusion%2520model%252C%2520which%2520is%250Atrained%2520solely%2520on%2520simulated%2520data.%2520This%2520energy%2520model%2520simultaneously%2520generates%250Agradients%2520to%2520refine%2520pose%2520estimates%2520and%2520produces%2520an%2520energy%2520scalar%2520that%250Aquantifies%2520the%2520quality%2520of%2520the%2520pose%2520estimates.%2520Additionally%252C%2520borrowing%2520the%2520idea%250Afrom%2520the%2520computer%2520vision%2520domain%252C%2520we%2520incorporate%2520a%2520render-compare%2520architecture%250Awithin%2520the%2520energy-based%2520score%2520network%2520to%2520significantly%2520enhance%2520sim-to-real%250Aperformance%252C%2520as%2520demonstrated%2520by%2520our%2520ablation%2520studies.%2520We%2520conduct%2520comprehensive%250Aexperiments%2520to%2520show%2520that%2520our%2520method%2520outperforms%2520conventional%2520baselines%2520based%2520on%250Aregression%252C%2520matching%252C%2520and%2520registration%2520techniques%252C%2520while%2520also%2520exhibiting%2520strong%250Aintra-category%2520generalization%2520to%2520previously%2520unseen%2520CAD%2520models.%2520Moreover%252C%2520our%250Aapproach%2520integrates%2520tactile%2520object%2520pose%2520estimation%252C%2520pose%2520tracking%252C%2520and%250Auncertainty%2520estimation%2520into%2520a%2520unified%2520framework%252C%2520enabling%2520robust%2520performance%250Aacross%2520a%2520variety%2520of%2520real-world%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTac2Pose%3A%20A%20Unified%20Approach%20Learned%20in%20Simulation%20for%20Category-level%0A%20%20Visuotactile%20In-hand%20Pose%20Estimation&entry.906535625=Mingdong%20Wu%20and%20Long%20Yang%20and%20Jin%20Liu%20and%20Weiyao%20Huang%20and%20Lehong%20Wu%20and%20Zelin%20Chen%20and%20Daolin%20Ma%20and%20Hao%20Dong&entry.1292438233=%20%20Accurate%20estimation%20of%20the%20in-hand%20pose%20of%20an%20object%20based%20on%20its%20CAD%20model%0Ais%20crucial%20in%20both%20industrial%20applications%20and%20everyday%20tasks%2C%20ranging%20from%0Apositioning%20workpieces%20and%20assembling%20components%20to%20seamlessly%20inserting%0Adevices%20like%20USB%20connectors.%20While%20existing%20methods%20often%20rely%20on%20regression%2C%0Afeature%20matching%2C%20or%20registration%20techniques%2C%20achieving%20high%20precision%20and%0Ageneralizability%20to%20unseen%20CAD%20models%20remains%20a%20significant%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20three-stage%20framework%20for%20in-hand%20pose%20estimation.%0AThe%20first%20stage%20involves%20sampling%20and%20pre-ranking%20pose%20candidates%2C%20followed%20by%0Aiterative%20refinement%20of%20these%20candidates%20in%20the%20second%20stage.%20In%20the%20final%0Astage%2C%20post-ranking%20is%20applied%20to%20identify%20the%20most%20likely%20pose%20candidates.%0AThese%20stages%20are%20governed%20by%20a%20unified%20energy-based%20diffusion%20model%2C%20which%20is%0Atrained%20solely%20on%20simulated%20data.%20This%20energy%20model%20simultaneously%20generates%0Agradients%20to%20refine%20pose%20estimates%20and%20produces%20an%20energy%20scalar%20that%0Aquantifies%20the%20quality%20of%20the%20pose%20estimates.%20Additionally%2C%20borrowing%20the%20idea%0Afrom%20the%20computer%20vision%20domain%2C%20we%20incorporate%20a%20render-compare%20architecture%0Awithin%20the%20energy-based%20score%20network%20to%20significantly%20enhance%20sim-to-real%0Aperformance%2C%20as%20demonstrated%20by%20our%20ablation%20studies.%20We%20conduct%20comprehensive%0Aexperiments%20to%20show%20that%20our%20method%20outperforms%20conventional%20baselines%20based%20on%0Aregression%2C%20matching%2C%20and%20registration%20techniques%2C%20while%20also%20exhibiting%20strong%0Aintra-category%20generalization%20to%20previously%20unseen%20CAD%20models.%20Moreover%2C%20our%0Aapproach%20integrates%20tactile%20object%20pose%20estimation%2C%20pose%20tracking%2C%20and%0Auncertainty%20estimation%20into%20a%20unified%20framework%2C%20enabling%20robust%20performance%0Aacross%20a%20variety%20of%20real-world%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15934v1&entry.124074799=Read"},
{"title": "A noise-corrected Langevin algorithm and sampling by half-denoising", "author": "Aapo Hyv\u00e4rinen", "abstract": "  The Langevin algorithm is a classic method for sampling from a given pdf in a\nreal space. In its basic version, it only requires knowledge of the gradient of\nthe log-density, also called the score function. However, in deep learning, it\nis often easier to learn the so-called \"noisy-data score function\", i.e. the\ngradient of the log-density of noisy data, more precisely when Gaussian noise\nis added to the data. Such an estimate is biased and complicates the use of the\nLangevin method. Here, we propose a noise-corrected version of the Langevin\nalgorithm, where the bias due to noisy data is removed, at least regarding\nfirst-order terms. Unlike diffusion models, our algorithm only needs to know\nthe noisy score function for one single noise level. We further propose a\nsimple special case which has an interesting intuitive interpretation of\niteratively adding noise the data and then attempting to remove half of that\nnoise.\n", "link": "http://arxiv.org/abs/2410.05837v3", "date": "2025-09-19", "relevancy": 2.3861, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4984}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4803}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20noise-corrected%20Langevin%20algorithm%20and%20sampling%20by%20half-denoising&body=Title%3A%20A%20noise-corrected%20Langevin%20algorithm%20and%20sampling%20by%20half-denoising%0AAuthor%3A%20Aapo%20Hyv%C3%A4rinen%0AAbstract%3A%20%20%20The%20Langevin%20algorithm%20is%20a%20classic%20method%20for%20sampling%20from%20a%20given%20pdf%20in%20a%0Areal%20space.%20In%20its%20basic%20version%2C%20it%20only%20requires%20knowledge%20of%20the%20gradient%20of%0Athe%20log-density%2C%20also%20called%20the%20score%20function.%20However%2C%20in%20deep%20learning%2C%20it%0Ais%20often%20easier%20to%20learn%20the%20so-called%20%22noisy-data%20score%20function%22%2C%20i.e.%20the%0Agradient%20of%20the%20log-density%20of%20noisy%20data%2C%20more%20precisely%20when%20Gaussian%20noise%0Ais%20added%20to%20the%20data.%20Such%20an%20estimate%20is%20biased%20and%20complicates%20the%20use%20of%20the%0ALangevin%20method.%20Here%2C%20we%20propose%20a%20noise-corrected%20version%20of%20the%20Langevin%0Aalgorithm%2C%20where%20the%20bias%20due%20to%20noisy%20data%20is%20removed%2C%20at%20least%20regarding%0Afirst-order%20terms.%20Unlike%20diffusion%20models%2C%20our%20algorithm%20only%20needs%20to%20know%0Athe%20noisy%20score%20function%20for%20one%20single%20noise%20level.%20We%20further%20propose%20a%0Asimple%20special%20case%20which%20has%20an%20interesting%20intuitive%20interpretation%20of%0Aiteratively%20adding%20noise%20the%20data%20and%20then%20attempting%20to%20remove%20half%20of%20that%0Anoise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05837v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520noise-corrected%2520Langevin%2520algorithm%2520and%2520sampling%2520by%2520half-denoising%26entry.906535625%3DAapo%2520Hyv%25C3%25A4rinen%26entry.1292438233%3D%2520%2520The%2520Langevin%2520algorithm%2520is%2520a%2520classic%2520method%2520for%2520sampling%2520from%2520a%2520given%2520pdf%2520in%2520a%250Areal%2520space.%2520In%2520its%2520basic%2520version%252C%2520it%2520only%2520requires%2520knowledge%2520of%2520the%2520gradient%2520of%250Athe%2520log-density%252C%2520also%2520called%2520the%2520score%2520function.%2520However%252C%2520in%2520deep%2520learning%252C%2520it%250Ais%2520often%2520easier%2520to%2520learn%2520the%2520so-called%2520%2522noisy-data%2520score%2520function%2522%252C%2520i.e.%2520the%250Agradient%2520of%2520the%2520log-density%2520of%2520noisy%2520data%252C%2520more%2520precisely%2520when%2520Gaussian%2520noise%250Ais%2520added%2520to%2520the%2520data.%2520Such%2520an%2520estimate%2520is%2520biased%2520and%2520complicates%2520the%2520use%2520of%2520the%250ALangevin%2520method.%2520Here%252C%2520we%2520propose%2520a%2520noise-corrected%2520version%2520of%2520the%2520Langevin%250Aalgorithm%252C%2520where%2520the%2520bias%2520due%2520to%2520noisy%2520data%2520is%2520removed%252C%2520at%2520least%2520regarding%250Afirst-order%2520terms.%2520Unlike%2520diffusion%2520models%252C%2520our%2520algorithm%2520only%2520needs%2520to%2520know%250Athe%2520noisy%2520score%2520function%2520for%2520one%2520single%2520noise%2520level.%2520We%2520further%2520propose%2520a%250Asimple%2520special%2520case%2520which%2520has%2520an%2520interesting%2520intuitive%2520interpretation%2520of%250Aiteratively%2520adding%2520noise%2520the%2520data%2520and%2520then%2520attempting%2520to%2520remove%2520half%2520of%2520that%250Anoise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05837v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20noise-corrected%20Langevin%20algorithm%20and%20sampling%20by%20half-denoising&entry.906535625=Aapo%20Hyv%C3%A4rinen&entry.1292438233=%20%20The%20Langevin%20algorithm%20is%20a%20classic%20method%20for%20sampling%20from%20a%20given%20pdf%20in%20a%0Areal%20space.%20In%20its%20basic%20version%2C%20it%20only%20requires%20knowledge%20of%20the%20gradient%20of%0Athe%20log-density%2C%20also%20called%20the%20score%20function.%20However%2C%20in%20deep%20learning%2C%20it%0Ais%20often%20easier%20to%20learn%20the%20so-called%20%22noisy-data%20score%20function%22%2C%20i.e.%20the%0Agradient%20of%20the%20log-density%20of%20noisy%20data%2C%20more%20precisely%20when%20Gaussian%20noise%0Ais%20added%20to%20the%20data.%20Such%20an%20estimate%20is%20biased%20and%20complicates%20the%20use%20of%20the%0ALangevin%20method.%20Here%2C%20we%20propose%20a%20noise-corrected%20version%20of%20the%20Langevin%0Aalgorithm%2C%20where%20the%20bias%20due%20to%20noisy%20data%20is%20removed%2C%20at%20least%20regarding%0Afirst-order%20terms.%20Unlike%20diffusion%20models%2C%20our%20algorithm%20only%20needs%20to%20know%0Athe%20noisy%20score%20function%20for%20one%20single%20noise%20level.%20We%20further%20propose%20a%0Asimple%20special%20case%20which%20has%20an%20interesting%20intuitive%20interpretation%20of%0Aiteratively%20adding%20noise%20the%20data%20and%20then%20attempting%20to%20remove%20half%20of%20that%0Anoise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05837v3&entry.124074799=Read"},
{"title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework", "author": "Laura Kopf and Nils Feldhus and Kirill Bykov and Philine Lou Bommer and Anna Hedstr\u00f6m and Marina M. -C. H\u00f6hne and Oliver Eberle", "abstract": "  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nWithin the context of large language models (LLMs) for natural language\nprocessing (NLP), current automated neuron-level feature description methods\nface two key challenges: limited robustness and the assumption that each neuron\nencodes a single concept (monosemanticity), despite increasing evidence of\npolysemanticity. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework specifically\ndesigned to capture the complexity of features in LLMs. Unlike approaches that\nassign a single description per neuron, common in many automated\ninterpretability methods in NLP, PRISM produces more nuanced descriptions that\naccount for both monosemantic and polysemantic behavior. We apply PRISM to LLMs\nand, through extensive benchmarking against existing methods, demonstrate that\nour approach produces more accurate and faithful feature descriptions,\nimproving both overall description quality (via a description score) and the\nability to capture distinct concepts when polysemanticity is present (via a\npolysemanticity score).\n", "link": "http://arxiv.org/abs/2506.15538v3", "date": "2025-09-19", "relevancy": 2.385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6038}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework&body=Title%3A%20Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework%0AAuthor%3A%20Laura%20Kopf%20and%20Nils%20Feldhus%20and%20Kirill%20Bykov%20and%20Philine%20Lou%20Bommer%20and%20Anna%20Hedstr%C3%B6m%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Oliver%20Eberle%0AAbstract%3A%20%20%20Automated%20interpretability%20research%20aims%20to%20identify%20concepts%20encoded%20in%0Aneural%20network%20features%20to%20enhance%20human%20understanding%20of%20model%20behavior.%0AWithin%20the%20context%20of%20large%20language%20models%20%28LLMs%29%20for%20natural%20language%0Aprocessing%20%28NLP%29%2C%20current%20automated%20neuron-level%20feature%20description%20methods%0Aface%20two%20key%20challenges%3A%20limited%20robustness%20and%20the%20assumption%20that%20each%20neuron%0Aencodes%20a%20single%20concept%20%28monosemanticity%29%2C%20despite%20increasing%20evidence%20of%0Apolysemanticity.%20This%20assumption%20restricts%20the%20expressiveness%20of%20feature%0Adescriptions%20and%20limits%20their%20ability%20to%20capture%20the%20full%20range%20of%20behaviors%0Aencoded%20in%20model%20internals.%20To%20address%20this%2C%20we%20introduce%20Polysemantic%20FeatuRe%0AIdentification%20and%20Scoring%20Method%20%28PRISM%29%2C%20a%20novel%20framework%20specifically%0Adesigned%20to%20capture%20the%20complexity%20of%20features%20in%20LLMs.%20Unlike%20approaches%20that%0Aassign%20a%20single%20description%20per%20neuron%2C%20common%20in%20many%20automated%0Ainterpretability%20methods%20in%20NLP%2C%20PRISM%20produces%20more%20nuanced%20descriptions%20that%0Aaccount%20for%20both%20monosemantic%20and%20polysemantic%20behavior.%20We%20apply%20PRISM%20to%20LLMs%0Aand%2C%20through%20extensive%20benchmarking%20against%20existing%20methods%2C%20demonstrate%20that%0Aour%20approach%20produces%20more%20accurate%20and%20faithful%20feature%20descriptions%2C%0Aimproving%20both%20overall%20description%20quality%20%28via%20a%20description%20score%29%20and%20the%0Aability%20to%20capture%20distinct%20concepts%20when%20polysemanticity%20is%20present%20%28via%20a%0Apolysemanticity%20score%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15538v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Polysemanticity%2520with%2520PRISM%253A%2520A%2520Multi-Concept%2520Feature%250A%2520%2520Description%2520Framework%26entry.906535625%3DLaura%2520Kopf%2520and%2520Nils%2520Feldhus%2520and%2520Kirill%2520Bykov%2520and%2520Philine%2520Lou%2520Bommer%2520and%2520Anna%2520Hedstr%25C3%25B6m%2520and%2520Marina%2520M.%2520-C.%2520H%25C3%25B6hne%2520and%2520Oliver%2520Eberle%26entry.1292438233%3D%2520%2520Automated%2520interpretability%2520research%2520aims%2520to%2520identify%2520concepts%2520encoded%2520in%250Aneural%2520network%2520features%2520to%2520enhance%2520human%2520understanding%2520of%2520model%2520behavior.%250AWithin%2520the%2520context%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%252C%2520current%2520automated%2520neuron-level%2520feature%2520description%2520methods%250Aface%2520two%2520key%2520challenges%253A%2520limited%2520robustness%2520and%2520the%2520assumption%2520that%2520each%2520neuron%250Aencodes%2520a%2520single%2520concept%2520%2528monosemanticity%2529%252C%2520despite%2520increasing%2520evidence%2520of%250Apolysemanticity.%2520This%2520assumption%2520restricts%2520the%2520expressiveness%2520of%2520feature%250Adescriptions%2520and%2520limits%2520their%2520ability%2520to%2520capture%2520the%2520full%2520range%2520of%2520behaviors%250Aencoded%2520in%2520model%2520internals.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Polysemantic%2520FeatuRe%250AIdentification%2520and%2520Scoring%2520Method%2520%2528PRISM%2529%252C%2520a%2520novel%2520framework%2520specifically%250Adesigned%2520to%2520capture%2520the%2520complexity%2520of%2520features%2520in%2520LLMs.%2520Unlike%2520approaches%2520that%250Aassign%2520a%2520single%2520description%2520per%2520neuron%252C%2520common%2520in%2520many%2520automated%250Ainterpretability%2520methods%2520in%2520NLP%252C%2520PRISM%2520produces%2520more%2520nuanced%2520descriptions%2520that%250Aaccount%2520for%2520both%2520monosemantic%2520and%2520polysemantic%2520behavior.%2520We%2520apply%2520PRISM%2520to%2520LLMs%250Aand%252C%2520through%2520extensive%2520benchmarking%2520against%2520existing%2520methods%252C%2520demonstrate%2520that%250Aour%2520approach%2520produces%2520more%2520accurate%2520and%2520faithful%2520feature%2520descriptions%252C%250Aimproving%2520both%2520overall%2520description%2520quality%2520%2528via%2520a%2520description%2520score%2529%2520and%2520the%250Aability%2520to%2520capture%2520distinct%2520concepts%2520when%2520polysemanticity%2520is%2520present%2520%2528via%2520a%250Apolysemanticity%2520score%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15538v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework&entry.906535625=Laura%20Kopf%20and%20Nils%20Feldhus%20and%20Kirill%20Bykov%20and%20Philine%20Lou%20Bommer%20and%20Anna%20Hedstr%C3%B6m%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Oliver%20Eberle&entry.1292438233=%20%20Automated%20interpretability%20research%20aims%20to%20identify%20concepts%20encoded%20in%0Aneural%20network%20features%20to%20enhance%20human%20understanding%20of%20model%20behavior.%0AWithin%20the%20context%20of%20large%20language%20models%20%28LLMs%29%20for%20natural%20language%0Aprocessing%20%28NLP%29%2C%20current%20automated%20neuron-level%20feature%20description%20methods%0Aface%20two%20key%20challenges%3A%20limited%20robustness%20and%20the%20assumption%20that%20each%20neuron%0Aencodes%20a%20single%20concept%20%28monosemanticity%29%2C%20despite%20increasing%20evidence%20of%0Apolysemanticity.%20This%20assumption%20restricts%20the%20expressiveness%20of%20feature%0Adescriptions%20and%20limits%20their%20ability%20to%20capture%20the%20full%20range%20of%20behaviors%0Aencoded%20in%20model%20internals.%20To%20address%20this%2C%20we%20introduce%20Polysemantic%20FeatuRe%0AIdentification%20and%20Scoring%20Method%20%28PRISM%29%2C%20a%20novel%20framework%20specifically%0Adesigned%20to%20capture%20the%20complexity%20of%20features%20in%20LLMs.%20Unlike%20approaches%20that%0Aassign%20a%20single%20description%20per%20neuron%2C%20common%20in%20many%20automated%0Ainterpretability%20methods%20in%20NLP%2C%20PRISM%20produces%20more%20nuanced%20descriptions%20that%0Aaccount%20for%20both%20monosemantic%20and%20polysemantic%20behavior.%20We%20apply%20PRISM%20to%20LLMs%0Aand%2C%20through%20extensive%20benchmarking%20against%20existing%20methods%2C%20demonstrate%20that%0Aour%20approach%20produces%20more%20accurate%20and%20faithful%20feature%20descriptions%2C%0Aimproving%20both%20overall%20description%20quality%20%28via%20a%20description%20score%29%20and%20the%0Aability%20to%20capture%20distinct%20concepts%20when%20polysemanticity%20is%20present%20%28via%20a%0Apolysemanticity%20score%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15538v3&entry.124074799=Read"},
{"title": "A re-calibration method for object detection with multi-modal alignment\n  bias in autonomous driving", "author": "Zhihang Song and Dingyi Yao and Ruibo Ming and Lihui Peng and Danya Yao and Yi Zhang", "abstract": "  Multi-modal object detection in autonomous driving has achieved great\nbreakthroughs due to the usage of fusing complementary information from\ndifferent sensors. The calibration in fusion between sensors such as LiDAR and\ncamera was always supposed to be precise in previous work. However, in reality,\ncalibration matrices are fixed when the vehicles leave the factory, but\nmechanical vibration, road bumps, and data lags may cause calibration bias. As\nthere is relatively limited research on the impact of calibration on fusion\ndetection performance, multi-sensor detection methods with flexible calibration\ndependency have remained a key objective. In this paper, we systematically\nevaluate the sensitivity of the SOTA EPNet++ detection framework and prove that\neven slight bias on calibration can reduce the performance seriously. To\naddress this vulnerability, we propose a re-calibration model to re-calibrate\nthe misalignment in detection tasks. This model integrates LiDAR point cloud,\ncamera image, and initial calibration matrix as inputs, generating\nre-calibrated bias through semantic segmentation guidance and a tailored loss\nfunction design. The re-calibration model can operate with existing detection\nalgorithms, enhancing both robustness against calibration bias and overall\nobject detection performance. Our approach establishes a foundational\nmethodology for maintaining reliability in multi-modal perception systems under\nreal-world calibration uncertainties.\n", "link": "http://arxiv.org/abs/2405.16848v3", "date": "2025-09-19", "relevancy": 2.3645, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5866}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20re-calibration%20method%20for%20object%20detection%20with%20multi-modal%20alignment%0A%20%20bias%20in%20autonomous%20driving&body=Title%3A%20A%20re-calibration%20method%20for%20object%20detection%20with%20multi-modal%20alignment%0A%20%20bias%20in%20autonomous%20driving%0AAuthor%3A%20Zhihang%20Song%20and%20Dingyi%20Yao%20and%20Ruibo%20Ming%20and%20Lihui%20Peng%20and%20Danya%20Yao%20and%20Yi%20Zhang%0AAbstract%3A%20%20%20Multi-modal%20object%20detection%20in%20autonomous%20driving%20has%20achieved%20great%0Abreakthroughs%20due%20to%20the%20usage%20of%20fusing%20complementary%20information%20from%0Adifferent%20sensors.%20The%20calibration%20in%20fusion%20between%20sensors%20such%20as%20LiDAR%20and%0Acamera%20was%20always%20supposed%20to%20be%20precise%20in%20previous%20work.%20However%2C%20in%20reality%2C%0Acalibration%20matrices%20are%20fixed%20when%20the%20vehicles%20leave%20the%20factory%2C%20but%0Amechanical%20vibration%2C%20road%20bumps%2C%20and%20data%20lags%20may%20cause%20calibration%20bias.%20As%0Athere%20is%20relatively%20limited%20research%20on%20the%20impact%20of%20calibration%20on%20fusion%0Adetection%20performance%2C%20multi-sensor%20detection%20methods%20with%20flexible%20calibration%0Adependency%20have%20remained%20a%20key%20objective.%20In%20this%20paper%2C%20we%20systematically%0Aevaluate%20the%20sensitivity%20of%20the%20SOTA%20EPNet%2B%2B%20detection%20framework%20and%20prove%20that%0Aeven%20slight%20bias%20on%20calibration%20can%20reduce%20the%20performance%20seriously.%20To%0Aaddress%20this%20vulnerability%2C%20we%20propose%20a%20re-calibration%20model%20to%20re-calibrate%0Athe%20misalignment%20in%20detection%20tasks.%20This%20model%20integrates%20LiDAR%20point%20cloud%2C%0Acamera%20image%2C%20and%20initial%20calibration%20matrix%20as%20inputs%2C%20generating%0Are-calibrated%20bias%20through%20semantic%20segmentation%20guidance%20and%20a%20tailored%20loss%0Afunction%20design.%20The%20re-calibration%20model%20can%20operate%20with%20existing%20detection%0Aalgorithms%2C%20enhancing%20both%20robustness%20against%20calibration%20bias%20and%20overall%0Aobject%20detection%20performance.%20Our%20approach%20establishes%20a%20foundational%0Amethodology%20for%20maintaining%20reliability%20in%20multi-modal%20perception%20systems%20under%0Areal-world%20calibration%20uncertainties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16848v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520re-calibration%2520method%2520for%2520object%2520detection%2520with%2520multi-modal%2520alignment%250A%2520%2520bias%2520in%2520autonomous%2520driving%26entry.906535625%3DZhihang%2520Song%2520and%2520Dingyi%2520Yao%2520and%2520Ruibo%2520Ming%2520and%2520Lihui%2520Peng%2520and%2520Danya%2520Yao%2520and%2520Yi%2520Zhang%26entry.1292438233%3D%2520%2520Multi-modal%2520object%2520detection%2520in%2520autonomous%2520driving%2520has%2520achieved%2520great%250Abreakthroughs%2520due%2520to%2520the%2520usage%2520of%2520fusing%2520complementary%2520information%2520from%250Adifferent%2520sensors.%2520The%2520calibration%2520in%2520fusion%2520between%2520sensors%2520such%2520as%2520LiDAR%2520and%250Acamera%2520was%2520always%2520supposed%2520to%2520be%2520precise%2520in%2520previous%2520work.%2520However%252C%2520in%2520reality%252C%250Acalibration%2520matrices%2520are%2520fixed%2520when%2520the%2520vehicles%2520leave%2520the%2520factory%252C%2520but%250Amechanical%2520vibration%252C%2520road%2520bumps%252C%2520and%2520data%2520lags%2520may%2520cause%2520calibration%2520bias.%2520As%250Athere%2520is%2520relatively%2520limited%2520research%2520on%2520the%2520impact%2520of%2520calibration%2520on%2520fusion%250Adetection%2520performance%252C%2520multi-sensor%2520detection%2520methods%2520with%2520flexible%2520calibration%250Adependency%2520have%2520remained%2520a%2520key%2520objective.%2520In%2520this%2520paper%252C%2520we%2520systematically%250Aevaluate%2520the%2520sensitivity%2520of%2520the%2520SOTA%2520EPNet%252B%252B%2520detection%2520framework%2520and%2520prove%2520that%250Aeven%2520slight%2520bias%2520on%2520calibration%2520can%2520reduce%2520the%2520performance%2520seriously.%2520To%250Aaddress%2520this%2520vulnerability%252C%2520we%2520propose%2520a%2520re-calibration%2520model%2520to%2520re-calibrate%250Athe%2520misalignment%2520in%2520detection%2520tasks.%2520This%2520model%2520integrates%2520LiDAR%2520point%2520cloud%252C%250Acamera%2520image%252C%2520and%2520initial%2520calibration%2520matrix%2520as%2520inputs%252C%2520generating%250Are-calibrated%2520bias%2520through%2520semantic%2520segmentation%2520guidance%2520and%2520a%2520tailored%2520loss%250Afunction%2520design.%2520The%2520re-calibration%2520model%2520can%2520operate%2520with%2520existing%2520detection%250Aalgorithms%252C%2520enhancing%2520both%2520robustness%2520against%2520calibration%2520bias%2520and%2520overall%250Aobject%2520detection%2520performance.%2520Our%2520approach%2520establishes%2520a%2520foundational%250Amethodology%2520for%2520maintaining%2520reliability%2520in%2520multi-modal%2520perception%2520systems%2520under%250Areal-world%2520calibration%2520uncertainties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16848v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20re-calibration%20method%20for%20object%20detection%20with%20multi-modal%20alignment%0A%20%20bias%20in%20autonomous%20driving&entry.906535625=Zhihang%20Song%20and%20Dingyi%20Yao%20and%20Ruibo%20Ming%20and%20Lihui%20Peng%20and%20Danya%20Yao%20and%20Yi%20Zhang&entry.1292438233=%20%20Multi-modal%20object%20detection%20in%20autonomous%20driving%20has%20achieved%20great%0Abreakthroughs%20due%20to%20the%20usage%20of%20fusing%20complementary%20information%20from%0Adifferent%20sensors.%20The%20calibration%20in%20fusion%20between%20sensors%20such%20as%20LiDAR%20and%0Acamera%20was%20always%20supposed%20to%20be%20precise%20in%20previous%20work.%20However%2C%20in%20reality%2C%0Acalibration%20matrices%20are%20fixed%20when%20the%20vehicles%20leave%20the%20factory%2C%20but%0Amechanical%20vibration%2C%20road%20bumps%2C%20and%20data%20lags%20may%20cause%20calibration%20bias.%20As%0Athere%20is%20relatively%20limited%20research%20on%20the%20impact%20of%20calibration%20on%20fusion%0Adetection%20performance%2C%20multi-sensor%20detection%20methods%20with%20flexible%20calibration%0Adependency%20have%20remained%20a%20key%20objective.%20In%20this%20paper%2C%20we%20systematically%0Aevaluate%20the%20sensitivity%20of%20the%20SOTA%20EPNet%2B%2B%20detection%20framework%20and%20prove%20that%0Aeven%20slight%20bias%20on%20calibration%20can%20reduce%20the%20performance%20seriously.%20To%0Aaddress%20this%20vulnerability%2C%20we%20propose%20a%20re-calibration%20model%20to%20re-calibrate%0Athe%20misalignment%20in%20detection%20tasks.%20This%20model%20integrates%20LiDAR%20point%20cloud%2C%0Acamera%20image%2C%20and%20initial%20calibration%20matrix%20as%20inputs%2C%20generating%0Are-calibrated%20bias%20through%20semantic%20segmentation%20guidance%20and%20a%20tailored%20loss%0Afunction%20design.%20The%20re-calibration%20model%20can%20operate%20with%20existing%20detection%0Aalgorithms%2C%20enhancing%20both%20robustness%20against%20calibration%20bias%20and%20overall%0Aobject%20detection%20performance.%20Our%20approach%20establishes%20a%20foundational%0Amethodology%20for%20maintaining%20reliability%20in%20multi-modal%20perception%20systems%20under%0Areal-world%20calibration%20uncertainties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16848v3&entry.124074799=Read"},
{"title": "Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data", "author": "Nakul Sharma", "abstract": "  Imbalanced classification datasets pose significant challenges in machine\nlearning, often leading to biased models that perform poorly on\nunderrepresented classes. With the rise of foundation models, recent research\nhas focused on the full, partial, and parameter-efficient fine-tuning of these\nmodels to deal with long-tail classification. Despite the impressive\nperformance of these works on the benchmark datasets, they still fail to close\nthe gap with the networks trained using the balanced datasets and still require\nsubstantial computational resources, even for relatively smaller datasets.\nUnderscoring the importance of computational efficiency and simplicity, in this\nwork we propose a novel framework that leverages the rich semantic latent space\nof Vision Foundation Models to generate synthetic data and train a simple\nlinear classifier using a mixture of real and synthetic data for long-tail\nclassification. The computational efficiency gain arises from the number of\ntrainable parameters that are reduced to just the number of parameters in the\nlinear model. Our method sets a new state-of-the-art for the CIFAR-100-LT\nbenchmark and demonstrates strong performance on the Places-LT benchmark,\nhighlighting the effectiveness and adaptability of our simple and effective\napproach.\n", "link": "http://arxiv.org/abs/2509.15859v1", "date": "2025-09-19", "relevancy": 2.3629, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6316}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5674}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Long-Tail%20Learning%20in%20Latent%20Space%20by%20sampling%20Synthetic%20Data&body=Title%3A%20Efficient%20Long-Tail%20Learning%20in%20Latent%20Space%20by%20sampling%20Synthetic%20Data%0AAuthor%3A%20Nakul%20Sharma%0AAbstract%3A%20%20%20Imbalanced%20classification%20datasets%20pose%20significant%20challenges%20in%20machine%0Alearning%2C%20often%20leading%20to%20biased%20models%20that%20perform%20poorly%20on%0Aunderrepresented%20classes.%20With%20the%20rise%20of%20foundation%20models%2C%20recent%20research%0Ahas%20focused%20on%20the%20full%2C%20partial%2C%20and%20parameter-efficient%20fine-tuning%20of%20these%0Amodels%20to%20deal%20with%20long-tail%20classification.%20Despite%20the%20impressive%0Aperformance%20of%20these%20works%20on%20the%20benchmark%20datasets%2C%20they%20still%20fail%20to%20close%0Athe%20gap%20with%20the%20networks%20trained%20using%20the%20balanced%20datasets%20and%20still%20require%0Asubstantial%20computational%20resources%2C%20even%20for%20relatively%20smaller%20datasets.%0AUnderscoring%20the%20importance%20of%20computational%20efficiency%20and%20simplicity%2C%20in%20this%0Awork%20we%20propose%20a%20novel%20framework%20that%20leverages%20the%20rich%20semantic%20latent%20space%0Aof%20Vision%20Foundation%20Models%20to%20generate%20synthetic%20data%20and%20train%20a%20simple%0Alinear%20classifier%20using%20a%20mixture%20of%20real%20and%20synthetic%20data%20for%20long-tail%0Aclassification.%20The%20computational%20efficiency%20gain%20arises%20from%20the%20number%20of%0Atrainable%20parameters%20that%20are%20reduced%20to%20just%20the%20number%20of%20parameters%20in%20the%0Alinear%20model.%20Our%20method%20sets%20a%20new%20state-of-the-art%20for%20the%20CIFAR-100-LT%0Abenchmark%20and%20demonstrates%20strong%20performance%20on%20the%20Places-LT%20benchmark%2C%0Ahighlighting%20the%20effectiveness%20and%20adaptability%20of%20our%20simple%20and%20effective%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Long-Tail%2520Learning%2520in%2520Latent%2520Space%2520by%2520sampling%2520Synthetic%2520Data%26entry.906535625%3DNakul%2520Sharma%26entry.1292438233%3D%2520%2520Imbalanced%2520classification%2520datasets%2520pose%2520significant%2520challenges%2520in%2520machine%250Alearning%252C%2520often%2520leading%2520to%2520biased%2520models%2520that%2520perform%2520poorly%2520on%250Aunderrepresented%2520classes.%2520With%2520the%2520rise%2520of%2520foundation%2520models%252C%2520recent%2520research%250Ahas%2520focused%2520on%2520the%2520full%252C%2520partial%252C%2520and%2520parameter-efficient%2520fine-tuning%2520of%2520these%250Amodels%2520to%2520deal%2520with%2520long-tail%2520classification.%2520Despite%2520the%2520impressive%250Aperformance%2520of%2520these%2520works%2520on%2520the%2520benchmark%2520datasets%252C%2520they%2520still%2520fail%2520to%2520close%250Athe%2520gap%2520with%2520the%2520networks%2520trained%2520using%2520the%2520balanced%2520datasets%2520and%2520still%2520require%250Asubstantial%2520computational%2520resources%252C%2520even%2520for%2520relatively%2520smaller%2520datasets.%250AUnderscoring%2520the%2520importance%2520of%2520computational%2520efficiency%2520and%2520simplicity%252C%2520in%2520this%250Awork%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520leverages%2520the%2520rich%2520semantic%2520latent%2520space%250Aof%2520Vision%2520Foundation%2520Models%2520to%2520generate%2520synthetic%2520data%2520and%2520train%2520a%2520simple%250Alinear%2520classifier%2520using%2520a%2520mixture%2520of%2520real%2520and%2520synthetic%2520data%2520for%2520long-tail%250Aclassification.%2520The%2520computational%2520efficiency%2520gain%2520arises%2520from%2520the%2520number%2520of%250Atrainable%2520parameters%2520that%2520are%2520reduced%2520to%2520just%2520the%2520number%2520of%2520parameters%2520in%2520the%250Alinear%2520model.%2520Our%2520method%2520sets%2520a%2520new%2520state-of-the-art%2520for%2520the%2520CIFAR-100-LT%250Abenchmark%2520and%2520demonstrates%2520strong%2520performance%2520on%2520the%2520Places-LT%2520benchmark%252C%250Ahighlighting%2520the%2520effectiveness%2520and%2520adaptability%2520of%2520our%2520simple%2520and%2520effective%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Long-Tail%20Learning%20in%20Latent%20Space%20by%20sampling%20Synthetic%20Data&entry.906535625=Nakul%20Sharma&entry.1292438233=%20%20Imbalanced%20classification%20datasets%20pose%20significant%20challenges%20in%20machine%0Alearning%2C%20often%20leading%20to%20biased%20models%20that%20perform%20poorly%20on%0Aunderrepresented%20classes.%20With%20the%20rise%20of%20foundation%20models%2C%20recent%20research%0Ahas%20focused%20on%20the%20full%2C%20partial%2C%20and%20parameter-efficient%20fine-tuning%20of%20these%0Amodels%20to%20deal%20with%20long-tail%20classification.%20Despite%20the%20impressive%0Aperformance%20of%20these%20works%20on%20the%20benchmark%20datasets%2C%20they%20still%20fail%20to%20close%0Athe%20gap%20with%20the%20networks%20trained%20using%20the%20balanced%20datasets%20and%20still%20require%0Asubstantial%20computational%20resources%2C%20even%20for%20relatively%20smaller%20datasets.%0AUnderscoring%20the%20importance%20of%20computational%20efficiency%20and%20simplicity%2C%20in%20this%0Awork%20we%20propose%20a%20novel%20framework%20that%20leverages%20the%20rich%20semantic%20latent%20space%0Aof%20Vision%20Foundation%20Models%20to%20generate%20synthetic%20data%20and%20train%20a%20simple%0Alinear%20classifier%20using%20a%20mixture%20of%20real%20and%20synthetic%20data%20for%20long-tail%0Aclassification.%20The%20computational%20efficiency%20gain%20arises%20from%20the%20number%20of%0Atrainable%20parameters%20that%20are%20reduced%20to%20just%20the%20number%20of%20parameters%20in%20the%0Alinear%20model.%20Our%20method%20sets%20a%20new%20state-of-the-art%20for%20the%20CIFAR-100-LT%0Abenchmark%20and%20demonstrates%20strong%20performance%20on%20the%20Places-LT%20benchmark%2C%0Ahighlighting%20the%20effectiveness%20and%20adaptability%20of%20our%20simple%20and%20effective%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15859v1&entry.124074799=Read"},
{"title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "author": "Marc Lafon and Yannis Karmim and Julio Silva-Rodr\u00edguez and Paul Couairon and Cl\u00e9ment Rambour and Rapha\u00ebl Fournier-Sniehotta and Ismail Ben Ayed and Jose Dolz and Nicolas Thome", "abstract": "  Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.\n", "link": "http://arxiv.org/abs/2507.07620v4", "date": "2025-09-19", "relevancy": 2.3403, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&body=Title%3A%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction%0AAuthor%3A%20Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07620v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLU%253A%2520Learning%2520Vision-Language%2520Uncertainties%2520for%2520Failure%2520Prediction%26entry.906535625%3DMarc%2520Lafon%2520and%2520Yannis%2520Karmim%2520and%2520Julio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Paul%2520Couairon%2520and%2520Cl%25C3%25A9ment%2520Rambour%2520and%2520Rapha%25C3%25ABl%2520Fournier-Sniehotta%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Jose%2520Dolz%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Reliable%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520and%2520failure%2520prediction%2520remain%2520open%250Achallenges%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%2520introduce%2520ViLU%252C%2520a%2520new%250AVision-Language%2520Uncertainty%2520quantification%2520framework%2520that%2520contextualizes%250Auncertainty%2520estimates%2520by%2520leveraging%2520all%2520task-relevant%2520textual%2520representations.%250AViLU%2520constructs%2520an%2520uncertainty-aware%2520multi-modal%2520representation%2520by%2520integrating%250Athe%2520visual%2520embedding%252C%2520the%2520predicted%2520textual%2520embedding%252C%2520and%2520an%2520image-conditioned%250Atextual%2520representation%2520via%2520cross-attention.%2520Unlike%2520traditional%2520UQ%2520methods%2520based%250Aon%2520loss%2520prediction%252C%2520ViLU%2520trains%2520an%2520uncertainty%2520predictor%2520as%2520a%2520binary%2520classifier%250Ato%2520distinguish%2520correct%2520from%2520incorrect%2520predictions%2520using%2520a%2520weighted%2520binary%250Across-entropy%2520loss%252C%2520making%2520it%2520loss-agnostic.%2520In%2520particular%252C%2520our%2520proposed%250Aapproach%2520is%2520well-suited%2520for%2520post-hoc%2520settings%252C%2520where%2520only%2520vision%2520and%2520text%250Aembeddings%2520are%2520available%2520without%2520direct%2520access%2520to%2520the%2520model%2520itself.%2520Extensive%250Aexperiments%2520on%2520diverse%2520datasets%2520show%2520the%2520significant%2520gains%2520of%2520our%2520method%250Acompared%2520to%2520state-of-the-art%2520failure%2520prediction%2520methods.%2520We%2520apply%2520our%2520method%2520to%250Astandard%2520classification%2520datasets%252C%2520such%2520as%2520ImageNet-1k%252C%2520as%2520well%2520as%2520large-scale%250Aimage-caption%2520datasets%2520like%2520CC12M%2520and%2520LAION-400M.%2520Ablation%2520studies%2520highlight%250Athe%2520critical%2520role%2520of%2520our%2520architecture%2520and%2520training%2520in%2520achieving%2520effective%250Auncertainty%2520quantification.%2520Our%2520code%2520is%2520publicly%2520available%2520and%2520can%2520be%2520found%250Ahere%253A%2520https%253A//github.com/ykrmm/ViLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07620v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&entry.906535625=Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome&entry.1292438233=%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07620v4&entry.124074799=Read"},
{"title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning", "author": "Maksim Kolodiazhnyi and Denis Tarasov and Dmitrii Zhemchuzhnikov and Alexander Nikulin and Ilya Zisman and Anna Vorontsova and Anton Konushin and Vladislav Kurenkov and Danila Rukhovich", "abstract": "  Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.\n", "link": "http://arxiv.org/abs/2505.22914v2", "date": "2025-09-19", "relevancy": 2.2875, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.567}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Online%20Reinforcement%0A%20%20Learning&body=Title%3A%20cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Online%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Maksim%20Kolodiazhnyi%20and%20Denis%20Tarasov%20and%20Dmitrii%20Zhemchuzhnikov%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Anna%20Vorontsova%20and%20Anton%20Konushin%20and%20Vladislav%20Kurenkov%20and%20Danila%20Rukhovich%0AAbstract%3A%20%20%20Computer-Aided%20Design%20%28CAD%29%20plays%20a%20central%20role%20in%20engineering%20and%0Amanufacturing%2C%20making%20it%20possible%20to%20create%20precise%20and%20editable%203D%20models.%0AUsing%20a%20variety%20of%20sensor%20or%20user-provided%20data%20as%20inputs%20for%20CAD%0Areconstruction%20can%20democratize%20access%20to%20design%20applications.%20However%2C%20existing%0Amethods%20typically%20focus%20on%20a%20single%20input%20modality%2C%20such%20as%20point%20clouds%2C%0Aimages%2C%20or%20text%2C%20which%20limits%20their%20generalizability%20and%20robustness.%20Leveraging%0Arecent%20advances%20in%20vision-language%20models%20%28VLM%29%2C%20we%20propose%20a%20multi-modal%20CAD%0Areconstruction%20model%20that%20simultaneously%20processes%20all%20three%20input%20modalities.%0AInspired%20by%20large%20language%20model%20%28LLM%29%20training%20paradigms%2C%20we%20adopt%20a%20two-stage%0Apipeline%3A%20supervised%20fine-tuning%20%28SFT%29%20on%20large-scale%20procedurally%20generated%0Adata%2C%20followed%20by%20reinforcement%20learning%20%28RL%29%20fine-tuning%20using%20online%0Afeedback%2C%20obtained%20programatically.%20Furthermore%2C%20we%20are%20the%20first%20to%20explore%20RL%0Afine-tuning%20of%20LLMs%20for%20CAD%20tasks%20demonstrating%20that%20online%20RL%20algorithms%20such%0Aas%20Group%20Relative%20Preference%20Optimization%20%28GRPO%29%20outperform%20offline%0Aalternatives.%20In%20the%20DeepCAD%20benchmark%2C%20our%20SFT%20model%20outperforms%20existing%0Asingle-modal%20approaches%20in%20all%20three%20input%20modalities%20simultaneously.%20More%0Aimportantly%2C%20after%20RL%20fine-tuning%2C%20cadrille%20sets%20new%20state-of-the-art%20on%20three%0Achallenging%20datasets%2C%20including%20a%20real-world%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dcadrille%253A%2520Multi-modal%2520CAD%2520Reconstruction%2520with%2520Online%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMaksim%2520Kolodiazhnyi%2520and%2520Denis%2520Tarasov%2520and%2520Dmitrii%2520Zhemchuzhnikov%2520and%2520Alexander%2520Nikulin%2520and%2520Ilya%2520Zisman%2520and%2520Anna%2520Vorontsova%2520and%2520Anton%2520Konushin%2520and%2520Vladislav%2520Kurenkov%2520and%2520Danila%2520Rukhovich%26entry.1292438233%3D%2520%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520plays%2520a%2520central%2520role%2520in%2520engineering%2520and%250Amanufacturing%252C%2520making%2520it%2520possible%2520to%2520create%2520precise%2520and%2520editable%25203D%2520models.%250AUsing%2520a%2520variety%2520of%2520sensor%2520or%2520user-provided%2520data%2520as%2520inputs%2520for%2520CAD%250Areconstruction%2520can%2520democratize%2520access%2520to%2520design%2520applications.%2520However%252C%2520existing%250Amethods%2520typically%2520focus%2520on%2520a%2520single%2520input%2520modality%252C%2520such%2520as%2520point%2520clouds%252C%250Aimages%252C%2520or%2520text%252C%2520which%2520limits%2520their%2520generalizability%2520and%2520robustness.%2520Leveraging%250Arecent%2520advances%2520in%2520vision-language%2520models%2520%2528VLM%2529%252C%2520we%2520propose%2520a%2520multi-modal%2520CAD%250Areconstruction%2520model%2520that%2520simultaneously%2520processes%2520all%2520three%2520input%2520modalities.%250AInspired%2520by%2520large%2520language%2520model%2520%2528LLM%2529%2520training%2520paradigms%252C%2520we%2520adopt%2520a%2520two-stage%250Apipeline%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520large-scale%2520procedurally%2520generated%250Adata%252C%2520followed%2520by%2520reinforcement%2520learning%2520%2528RL%2529%2520fine-tuning%2520using%2520online%250Afeedback%252C%2520obtained%2520programatically.%2520Furthermore%252C%2520we%2520are%2520the%2520first%2520to%2520explore%2520RL%250Afine-tuning%2520of%2520LLMs%2520for%2520CAD%2520tasks%2520demonstrating%2520that%2520online%2520RL%2520algorithms%2520such%250Aas%2520Group%2520Relative%2520Preference%2520Optimization%2520%2528GRPO%2529%2520outperform%2520offline%250Aalternatives.%2520In%2520the%2520DeepCAD%2520benchmark%252C%2520our%2520SFT%2520model%2520outperforms%2520existing%250Asingle-modal%2520approaches%2520in%2520all%2520three%2520input%2520modalities%2520simultaneously.%2520More%250Aimportantly%252C%2520after%2520RL%2520fine-tuning%252C%2520cadrille%2520sets%2520new%2520state-of-the-art%2520on%2520three%250Achallenging%2520datasets%252C%2520including%2520a%2520real-world%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Online%20Reinforcement%0A%20%20Learning&entry.906535625=Maksim%20Kolodiazhnyi%20and%20Denis%20Tarasov%20and%20Dmitrii%20Zhemchuzhnikov%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Anna%20Vorontsova%20and%20Anton%20Konushin%20and%20Vladislav%20Kurenkov%20and%20Danila%20Rukhovich&entry.1292438233=%20%20Computer-Aided%20Design%20%28CAD%29%20plays%20a%20central%20role%20in%20engineering%20and%0Amanufacturing%2C%20making%20it%20possible%20to%20create%20precise%20and%20editable%203D%20models.%0AUsing%20a%20variety%20of%20sensor%20or%20user-provided%20data%20as%20inputs%20for%20CAD%0Areconstruction%20can%20democratize%20access%20to%20design%20applications.%20However%2C%20existing%0Amethods%20typically%20focus%20on%20a%20single%20input%20modality%2C%20such%20as%20point%20clouds%2C%0Aimages%2C%20or%20text%2C%20which%20limits%20their%20generalizability%20and%20robustness.%20Leveraging%0Arecent%20advances%20in%20vision-language%20models%20%28VLM%29%2C%20we%20propose%20a%20multi-modal%20CAD%0Areconstruction%20model%20that%20simultaneously%20processes%20all%20three%20input%20modalities.%0AInspired%20by%20large%20language%20model%20%28LLM%29%20training%20paradigms%2C%20we%20adopt%20a%20two-stage%0Apipeline%3A%20supervised%20fine-tuning%20%28SFT%29%20on%20large-scale%20procedurally%20generated%0Adata%2C%20followed%20by%20reinforcement%20learning%20%28RL%29%20fine-tuning%20using%20online%0Afeedback%2C%20obtained%20programatically.%20Furthermore%2C%20we%20are%20the%20first%20to%20explore%20RL%0Afine-tuning%20of%20LLMs%20for%20CAD%20tasks%20demonstrating%20that%20online%20RL%20algorithms%20such%0Aas%20Group%20Relative%20Preference%20Optimization%20%28GRPO%29%20outperform%20offline%0Aalternatives.%20In%20the%20DeepCAD%20benchmark%2C%20our%20SFT%20model%20outperforms%20existing%0Asingle-modal%20approaches%20in%20all%20three%20input%20modalities%20simultaneously.%20More%0Aimportantly%2C%20after%20RL%20fine-tuning%2C%20cadrille%20sets%20new%20state-of-the-art%20on%20three%0Achallenging%20datasets%2C%20including%20a%20real-world%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22914v2&entry.124074799=Read"},
{"title": "A Vision-Language-Action-Critic Model for Robotic Real-World\n  Reinforcement Learning", "author": "Shaopeng Zhai and Qi Zhang and Tianyi Zhang and Fuxian Huang and Haoran Zhang and Ming Zhou and Shengzhe Zhang and Litao Liu and Sixu Lin and Jiangmiao Pang", "abstract": "  Robotic real-world reinforcement learning (RL) with vision-language-action\n(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient\nexploration. We introduce VLAC, a general process reward model built upon\nInternVL and trained on large scale heterogeneous datasets. Given pairwise\nobservations and a language goal, it outputs dense progress delta and done\nsignal, eliminating task-specific reward engineering, and supports one-shot\nin-context transfer to unseen tasks and environments. VLAC is trained on\nvision-language datasets to strengthen perception, dialogic and reasoning\ncapabilities, together with robot and human trajectories data that ground\naction generation and progress estimation, and additionally strengthened to\nreject irrelevant prompts as well as detect regression or stagnation by\nconstructing large numbers of negative and semantically mismatched samples.\nWith prompt control, a single VLAC model alternately generating reward and\naction tokens, unifying critic and policy. Deployed inside an asynchronous\nreal-world RL loop, we layer a graded human-in-the-loop protocol (offline\ndemonstration replay, return and explore, human guided explore) that\naccelerates exploration and stabilizes early learning. Across four distinct\nreal-world manipulation tasks, VLAC lifts success rates from about 30\\% to\nabout 90\\% within 200 real-world interaction episodes; incorporating\nhuman-in-the-loop interventions yields a further 50% improvement in sample\nefficiency and achieves up to 100% final success.\n", "link": "http://arxiv.org/abs/2509.15937v1", "date": "2025-09-19", "relevancy": 2.2768, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Vision-Language-Action-Critic%20Model%20for%20Robotic%20Real-World%0A%20%20Reinforcement%20Learning&body=Title%3A%20A%20Vision-Language-Action-Critic%20Model%20for%20Robotic%20Real-World%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Shaopeng%20Zhai%20and%20Qi%20Zhang%20and%20Tianyi%20Zhang%20and%20Fuxian%20Huang%20and%20Haoran%20Zhang%20and%20Ming%20Zhou%20and%20Shengzhe%20Zhang%20and%20Litao%20Liu%20and%20Sixu%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Robotic%20real-world%20reinforcement%20learning%20%28RL%29%20with%20vision-language-action%0A%28VLA%29%20models%20is%20bottlenecked%20by%20sparse%2C%20handcrafted%20rewards%20and%20inefficient%0Aexploration.%20We%20introduce%20VLAC%2C%20a%20general%20process%20reward%20model%20built%20upon%0AInternVL%20and%20trained%20on%20large%20scale%20heterogeneous%20datasets.%20Given%20pairwise%0Aobservations%20and%20a%20language%20goal%2C%20it%20outputs%20dense%20progress%20delta%20and%20done%0Asignal%2C%20eliminating%20task-specific%20reward%20engineering%2C%20and%20supports%20one-shot%0Ain-context%20transfer%20to%20unseen%20tasks%20and%20environments.%20VLAC%20is%20trained%20on%0Avision-language%20datasets%20to%20strengthen%20perception%2C%20dialogic%20and%20reasoning%0Acapabilities%2C%20together%20with%20robot%20and%20human%20trajectories%20data%20that%20ground%0Aaction%20generation%20and%20progress%20estimation%2C%20and%20additionally%20strengthened%20to%0Areject%20irrelevant%20prompts%20as%20well%20as%20detect%20regression%20or%20stagnation%20by%0Aconstructing%20large%20numbers%20of%20negative%20and%20semantically%20mismatched%20samples.%0AWith%20prompt%20control%2C%20a%20single%20VLAC%20model%20alternately%20generating%20reward%20and%0Aaction%20tokens%2C%20unifying%20critic%20and%20policy.%20Deployed%20inside%20an%20asynchronous%0Areal-world%20RL%20loop%2C%20we%20layer%20a%20graded%20human-in-the-loop%20protocol%20%28offline%0Ademonstration%20replay%2C%20return%20and%20explore%2C%20human%20guided%20explore%29%20that%0Aaccelerates%20exploration%20and%20stabilizes%20early%20learning.%20Across%20four%20distinct%0Areal-world%20manipulation%20tasks%2C%20VLAC%20lifts%20success%20rates%20from%20about%2030%5C%25%20to%0Aabout%2090%5C%25%20within%20200%20real-world%20interaction%20episodes%3B%20incorporating%0Ahuman-in-the-loop%20interventions%20yields%20a%20further%2050%25%20improvement%20in%20sample%0Aefficiency%20and%20achieves%20up%20to%20100%25%20final%20success.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Vision-Language-Action-Critic%2520Model%2520for%2520Robotic%2520Real-World%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DShaopeng%2520Zhai%2520and%2520Qi%2520Zhang%2520and%2520Tianyi%2520Zhang%2520and%2520Fuxian%2520Huang%2520and%2520Haoran%2520Zhang%2520and%2520Ming%2520Zhou%2520and%2520Shengzhe%2520Zhang%2520and%2520Litao%2520Liu%2520and%2520Sixu%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Robotic%2520real-world%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520vision-language-action%250A%2528VLA%2529%2520models%2520is%2520bottlenecked%2520by%2520sparse%252C%2520handcrafted%2520rewards%2520and%2520inefficient%250Aexploration.%2520We%2520introduce%2520VLAC%252C%2520a%2520general%2520process%2520reward%2520model%2520built%2520upon%250AInternVL%2520and%2520trained%2520on%2520large%2520scale%2520heterogeneous%2520datasets.%2520Given%2520pairwise%250Aobservations%2520and%2520a%2520language%2520goal%252C%2520it%2520outputs%2520dense%2520progress%2520delta%2520and%2520done%250Asignal%252C%2520eliminating%2520task-specific%2520reward%2520engineering%252C%2520and%2520supports%2520one-shot%250Ain-context%2520transfer%2520to%2520unseen%2520tasks%2520and%2520environments.%2520VLAC%2520is%2520trained%2520on%250Avision-language%2520datasets%2520to%2520strengthen%2520perception%252C%2520dialogic%2520and%2520reasoning%250Acapabilities%252C%2520together%2520with%2520robot%2520and%2520human%2520trajectories%2520data%2520that%2520ground%250Aaction%2520generation%2520and%2520progress%2520estimation%252C%2520and%2520additionally%2520strengthened%2520to%250Areject%2520irrelevant%2520prompts%2520as%2520well%2520as%2520detect%2520regression%2520or%2520stagnation%2520by%250Aconstructing%2520large%2520numbers%2520of%2520negative%2520and%2520semantically%2520mismatched%2520samples.%250AWith%2520prompt%2520control%252C%2520a%2520single%2520VLAC%2520model%2520alternately%2520generating%2520reward%2520and%250Aaction%2520tokens%252C%2520unifying%2520critic%2520and%2520policy.%2520Deployed%2520inside%2520an%2520asynchronous%250Areal-world%2520RL%2520loop%252C%2520we%2520layer%2520a%2520graded%2520human-in-the-loop%2520protocol%2520%2528offline%250Ademonstration%2520replay%252C%2520return%2520and%2520explore%252C%2520human%2520guided%2520explore%2529%2520that%250Aaccelerates%2520exploration%2520and%2520stabilizes%2520early%2520learning.%2520Across%2520four%2520distinct%250Areal-world%2520manipulation%2520tasks%252C%2520VLAC%2520lifts%2520success%2520rates%2520from%2520about%252030%255C%2525%2520to%250Aabout%252090%255C%2525%2520within%2520200%2520real-world%2520interaction%2520episodes%253B%2520incorporating%250Ahuman-in-the-loop%2520interventions%2520yields%2520a%2520further%252050%2525%2520improvement%2520in%2520sample%250Aefficiency%2520and%2520achieves%2520up%2520to%2520100%2525%2520final%2520success.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Vision-Language-Action-Critic%20Model%20for%20Robotic%20Real-World%0A%20%20Reinforcement%20Learning&entry.906535625=Shaopeng%20Zhai%20and%20Qi%20Zhang%20and%20Tianyi%20Zhang%20and%20Fuxian%20Huang%20and%20Haoran%20Zhang%20and%20Ming%20Zhou%20and%20Shengzhe%20Zhang%20and%20Litao%20Liu%20and%20Sixu%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Robotic%20real-world%20reinforcement%20learning%20%28RL%29%20with%20vision-language-action%0A%28VLA%29%20models%20is%20bottlenecked%20by%20sparse%2C%20handcrafted%20rewards%20and%20inefficient%0Aexploration.%20We%20introduce%20VLAC%2C%20a%20general%20process%20reward%20model%20built%20upon%0AInternVL%20and%20trained%20on%20large%20scale%20heterogeneous%20datasets.%20Given%20pairwise%0Aobservations%20and%20a%20language%20goal%2C%20it%20outputs%20dense%20progress%20delta%20and%20done%0Asignal%2C%20eliminating%20task-specific%20reward%20engineering%2C%20and%20supports%20one-shot%0Ain-context%20transfer%20to%20unseen%20tasks%20and%20environments.%20VLAC%20is%20trained%20on%0Avision-language%20datasets%20to%20strengthen%20perception%2C%20dialogic%20and%20reasoning%0Acapabilities%2C%20together%20with%20robot%20and%20human%20trajectories%20data%20that%20ground%0Aaction%20generation%20and%20progress%20estimation%2C%20and%20additionally%20strengthened%20to%0Areject%20irrelevant%20prompts%20as%20well%20as%20detect%20regression%20or%20stagnation%20by%0Aconstructing%20large%20numbers%20of%20negative%20and%20semantically%20mismatched%20samples.%0AWith%20prompt%20control%2C%20a%20single%20VLAC%20model%20alternately%20generating%20reward%20and%0Aaction%20tokens%2C%20unifying%20critic%20and%20policy.%20Deployed%20inside%20an%20asynchronous%0Areal-world%20RL%20loop%2C%20we%20layer%20a%20graded%20human-in-the-loop%20protocol%20%28offline%0Ademonstration%20replay%2C%20return%20and%20explore%2C%20human%20guided%20explore%29%20that%0Aaccelerates%20exploration%20and%20stabilizes%20early%20learning.%20Across%20four%20distinct%0Areal-world%20manipulation%20tasks%2C%20VLAC%20lifts%20success%20rates%20from%20about%2030%5C%25%20to%0Aabout%2090%5C%25%20within%20200%20real-world%20interaction%20episodes%3B%20incorporating%0Ahuman-in-the-loop%20interventions%20yields%20a%20further%2050%25%20improvement%20in%20sample%0Aefficiency%20and%20achieves%20up%20to%20100%25%20final%20success.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15937v1&entry.124074799=Read"},
{"title": "Improving Robotic Manipulation with Efficient Geometry-Aware Vision\n  Encoder", "author": "An Dinh Vuong and Minh Nhat Vu and Ian Reid", "abstract": "  Existing RGB-based imitation learning approaches typically employ traditional\nvision encoders such as ResNet or ViT, which lack explicit 3D reasoning\ncapabilities. Recent geometry-grounded vision models, such as\nVGGT~\\cite{wang2025vggt}, provide robust spatial understanding and are\npromising candidates to address this limitation. This work investigates the\nintegration of geometry-aware visual representations into robotic manipulation.\nOur results suggest that incorporating the geometry-aware vision encoder into\nimitation learning frameworks, including ACT and DP, yields up to 6.5%\nimprovement over standard vision encoders in success rate across single- and\nbi-manual manipulation tasks in both simulation and real-world settings.\nDespite these benefits, most geometry-grounded models require high\ncomputational cost, limiting their deployment in practical robotic systems. To\naddress this challenge, we propose eVGGT, an efficient geometry-aware encoder\ndistilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than\nVGGT, while preserving strong 3D reasoning capabilities. Code and pretrained\nmodels will be released to facilitate further research in geometry-aware\nrobotics.\n", "link": "http://arxiv.org/abs/2509.15880v1", "date": "2025-09-19", "relevancy": 2.2636, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5813}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Robotic%20Manipulation%20with%20Efficient%20Geometry-Aware%20Vision%0A%20%20Encoder&body=Title%3A%20Improving%20Robotic%20Manipulation%20with%20Efficient%20Geometry-Aware%20Vision%0A%20%20Encoder%0AAuthor%3A%20An%20Dinh%20Vuong%20and%20Minh%20Nhat%20Vu%20and%20Ian%20Reid%0AAbstract%3A%20%20%20Existing%20RGB-based%20imitation%20learning%20approaches%20typically%20employ%20traditional%0Avision%20encoders%20such%20as%20ResNet%20or%20ViT%2C%20which%20lack%20explicit%203D%20reasoning%0Acapabilities.%20Recent%20geometry-grounded%20vision%20models%2C%20such%20as%0AVGGT~%5Ccite%7Bwang2025vggt%7D%2C%20provide%20robust%20spatial%20understanding%20and%20are%0Apromising%20candidates%20to%20address%20this%20limitation.%20This%20work%20investigates%20the%0Aintegration%20of%20geometry-aware%20visual%20representations%20into%20robotic%20manipulation.%0AOur%20results%20suggest%20that%20incorporating%20the%20geometry-aware%20vision%20encoder%20into%0Aimitation%20learning%20frameworks%2C%20including%20ACT%20and%20DP%2C%20yields%20up%20to%206.5%25%0Aimprovement%20over%20standard%20vision%20encoders%20in%20success%20rate%20across%20single-%20and%0Abi-manual%20manipulation%20tasks%20in%20both%20simulation%20and%20real-world%20settings.%0ADespite%20these%20benefits%2C%20most%20geometry-grounded%20models%20require%20high%0Acomputational%20cost%2C%20limiting%20their%20deployment%20in%20practical%20robotic%20systems.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20eVGGT%2C%20an%20efficient%20geometry-aware%20encoder%0Adistilled%20from%20VGGT.%20eVGGT%20is%20nearly%209%20times%20faster%20and%205%20times%20smaller%20than%0AVGGT%2C%20while%20preserving%20strong%203D%20reasoning%20capabilities.%20Code%20and%20pretrained%0Amodels%20will%20be%20released%20to%20facilitate%20further%20research%20in%20geometry-aware%0Arobotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Robotic%2520Manipulation%2520with%2520Efficient%2520Geometry-Aware%2520Vision%250A%2520%2520Encoder%26entry.906535625%3DAn%2520Dinh%2520Vuong%2520and%2520Minh%2520Nhat%2520Vu%2520and%2520Ian%2520Reid%26entry.1292438233%3D%2520%2520Existing%2520RGB-based%2520imitation%2520learning%2520approaches%2520typically%2520employ%2520traditional%250Avision%2520encoders%2520such%2520as%2520ResNet%2520or%2520ViT%252C%2520which%2520lack%2520explicit%25203D%2520reasoning%250Acapabilities.%2520Recent%2520geometry-grounded%2520vision%2520models%252C%2520such%2520as%250AVGGT~%255Ccite%257Bwang2025vggt%257D%252C%2520provide%2520robust%2520spatial%2520understanding%2520and%2520are%250Apromising%2520candidates%2520to%2520address%2520this%2520limitation.%2520This%2520work%2520investigates%2520the%250Aintegration%2520of%2520geometry-aware%2520visual%2520representations%2520into%2520robotic%2520manipulation.%250AOur%2520results%2520suggest%2520that%2520incorporating%2520the%2520geometry-aware%2520vision%2520encoder%2520into%250Aimitation%2520learning%2520frameworks%252C%2520including%2520ACT%2520and%2520DP%252C%2520yields%2520up%2520to%25206.5%2525%250Aimprovement%2520over%2520standard%2520vision%2520encoders%2520in%2520success%2520rate%2520across%2520single-%2520and%250Abi-manual%2520manipulation%2520tasks%2520in%2520both%2520simulation%2520and%2520real-world%2520settings.%250ADespite%2520these%2520benefits%252C%2520most%2520geometry-grounded%2520models%2520require%2520high%250Acomputational%2520cost%252C%2520limiting%2520their%2520deployment%2520in%2520practical%2520robotic%2520systems.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520eVGGT%252C%2520an%2520efficient%2520geometry-aware%2520encoder%250Adistilled%2520from%2520VGGT.%2520eVGGT%2520is%2520nearly%25209%2520times%2520faster%2520and%25205%2520times%2520smaller%2520than%250AVGGT%252C%2520while%2520preserving%2520strong%25203D%2520reasoning%2520capabilities.%2520Code%2520and%2520pretrained%250Amodels%2520will%2520be%2520released%2520to%2520facilitate%2520further%2520research%2520in%2520geometry-aware%250Arobotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Robotic%20Manipulation%20with%20Efficient%20Geometry-Aware%20Vision%0A%20%20Encoder&entry.906535625=An%20Dinh%20Vuong%20and%20Minh%20Nhat%20Vu%20and%20Ian%20Reid&entry.1292438233=%20%20Existing%20RGB-based%20imitation%20learning%20approaches%20typically%20employ%20traditional%0Avision%20encoders%20such%20as%20ResNet%20or%20ViT%2C%20which%20lack%20explicit%203D%20reasoning%0Acapabilities.%20Recent%20geometry-grounded%20vision%20models%2C%20such%20as%0AVGGT~%5Ccite%7Bwang2025vggt%7D%2C%20provide%20robust%20spatial%20understanding%20and%20are%0Apromising%20candidates%20to%20address%20this%20limitation.%20This%20work%20investigates%20the%0Aintegration%20of%20geometry-aware%20visual%20representations%20into%20robotic%20manipulation.%0AOur%20results%20suggest%20that%20incorporating%20the%20geometry-aware%20vision%20encoder%20into%0Aimitation%20learning%20frameworks%2C%20including%20ACT%20and%20DP%2C%20yields%20up%20to%206.5%25%0Aimprovement%20over%20standard%20vision%20encoders%20in%20success%20rate%20across%20single-%20and%0Abi-manual%20manipulation%20tasks%20in%20both%20simulation%20and%20real-world%20settings.%0ADespite%20these%20benefits%2C%20most%20geometry-grounded%20models%20require%20high%0Acomputational%20cost%2C%20limiting%20their%20deployment%20in%20practical%20robotic%20systems.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20eVGGT%2C%20an%20efficient%20geometry-aware%20encoder%0Adistilled%20from%20VGGT.%20eVGGT%20is%20nearly%209%20times%20faster%20and%205%20times%20smaller%20than%0AVGGT%2C%20while%20preserving%20strong%203D%20reasoning%20capabilities.%20Code%20and%20pretrained%0Amodels%20will%20be%20released%20to%20facilitate%20further%20research%20in%20geometry-aware%0Arobotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15880v1&entry.124074799=Read"},
{"title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models", "author": "Vatsal Malaviya and Agneet Chatterjee and Maitreya Patel and Yezhou Yang and Chitta Baral", "abstract": "  Text-to-Image (T2I) models have recently achieved remarkable success in\ngenerating images from textual descriptions. However, challenges still persist\nin accurately rendering complex scenes where actions and interactions form the\nprimary semantic focus. Our key observation in this work is that T2I models\nfrequently struggle to capture nuanced and often implicit attributes inherent\nin action depiction, leading to generating images that lack key contextual\ndetails. To enable systematic evaluation, we introduce AcT2I, a benchmark\ndesigned to evaluate the performance of T2I models in generating images from\naction-centric prompts. We experimentally validate that leading T2I models do\nnot fare well on AcT2I. We further hypothesize that this shortcoming arises\nfrom the incomplete representation of the inherent attributes and contextual\ndependencies in the training corpora of existing T2I models. We build upon this\nby developing a training-free, knowledge distillation technique utilizing Large\nLanguage Models to address this limitation. Specifically, we enhance prompts by\nincorporating dense information across three dimensions, observing that\ninjecting prompts with temporal details significantly improves image generation\naccuracy, with our best model achieving an increase of 72%. Our findings\nhighlight the limitations of current T2I methods in generating images that\nrequire complex reasoning and demonstrate that integrating linguistic knowledge\nin a systematic way can notably advance the generation of nuanced and\ncontextually accurate images.\n", "link": "http://arxiv.org/abs/2509.16141v1", "date": "2025-09-19", "relevancy": 2.2605, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.579}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AcT2I%3A%20Evaluating%20and%20Improving%20Action%20Depiction%20in%20Text-to-Image%20Models&body=Title%3A%20AcT2I%3A%20Evaluating%20and%20Improving%20Action%20Depiction%20in%20Text-to-Image%20Models%0AAuthor%3A%20Vatsal%20Malaviya%20and%20Agneet%20Chatterjee%20and%20Maitreya%20Patel%20and%20Yezhou%20Yang%20and%20Chitta%20Baral%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20models%20have%20recently%20achieved%20remarkable%20success%20in%0Agenerating%20images%20from%20textual%20descriptions.%20However%2C%20challenges%20still%20persist%0Ain%20accurately%20rendering%20complex%20scenes%20where%20actions%20and%20interactions%20form%20the%0Aprimary%20semantic%20focus.%20Our%20key%20observation%20in%20this%20work%20is%20that%20T2I%20models%0Afrequently%20struggle%20to%20capture%20nuanced%20and%20often%20implicit%20attributes%20inherent%0Ain%20action%20depiction%2C%20leading%20to%20generating%20images%20that%20lack%20key%20contextual%0Adetails.%20To%20enable%20systematic%20evaluation%2C%20we%20introduce%20AcT2I%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20performance%20of%20T2I%20models%20in%20generating%20images%20from%0Aaction-centric%20prompts.%20We%20experimentally%20validate%20that%20leading%20T2I%20models%20do%0Anot%20fare%20well%20on%20AcT2I.%20We%20further%20hypothesize%20that%20this%20shortcoming%20arises%0Afrom%20the%20incomplete%20representation%20of%20the%20inherent%20attributes%20and%20contextual%0Adependencies%20in%20the%20training%20corpora%20of%20existing%20T2I%20models.%20We%20build%20upon%20this%0Aby%20developing%20a%20training-free%2C%20knowledge%20distillation%20technique%20utilizing%20Large%0ALanguage%20Models%20to%20address%20this%20limitation.%20Specifically%2C%20we%20enhance%20prompts%20by%0Aincorporating%20dense%20information%20across%20three%20dimensions%2C%20observing%20that%0Ainjecting%20prompts%20with%20temporal%20details%20significantly%20improves%20image%20generation%0Aaccuracy%2C%20with%20our%20best%20model%20achieving%20an%20increase%20of%2072%25.%20Our%20findings%0Ahighlight%20the%20limitations%20of%20current%20T2I%20methods%20in%20generating%20images%20that%0Arequire%20complex%20reasoning%20and%20demonstrate%20that%20integrating%20linguistic%20knowledge%0Ain%20a%20systematic%20way%20can%20notably%20advance%20the%20generation%20of%20nuanced%20and%0Acontextually%20accurate%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAcT2I%253A%2520Evaluating%2520and%2520Improving%2520Action%2520Depiction%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DVatsal%2520Malaviya%2520and%2520Agneet%2520Chatterjee%2520and%2520Maitreya%2520Patel%2520and%2520Yezhou%2520Yang%2520and%2520Chitta%2520Baral%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520models%2520have%2520recently%2520achieved%2520remarkable%2520success%2520in%250Agenerating%2520images%2520from%2520textual%2520descriptions.%2520However%252C%2520challenges%2520still%2520persist%250Ain%2520accurately%2520rendering%2520complex%2520scenes%2520where%2520actions%2520and%2520interactions%2520form%2520the%250Aprimary%2520semantic%2520focus.%2520Our%2520key%2520observation%2520in%2520this%2520work%2520is%2520that%2520T2I%2520models%250Afrequently%2520struggle%2520to%2520capture%2520nuanced%2520and%2520often%2520implicit%2520attributes%2520inherent%250Ain%2520action%2520depiction%252C%2520leading%2520to%2520generating%2520images%2520that%2520lack%2520key%2520contextual%250Adetails.%2520To%2520enable%2520systematic%2520evaluation%252C%2520we%2520introduce%2520AcT2I%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520the%2520performance%2520of%2520T2I%2520models%2520in%2520generating%2520images%2520from%250Aaction-centric%2520prompts.%2520We%2520experimentally%2520validate%2520that%2520leading%2520T2I%2520models%2520do%250Anot%2520fare%2520well%2520on%2520AcT2I.%2520We%2520further%2520hypothesize%2520that%2520this%2520shortcoming%2520arises%250Afrom%2520the%2520incomplete%2520representation%2520of%2520the%2520inherent%2520attributes%2520and%2520contextual%250Adependencies%2520in%2520the%2520training%2520corpora%2520of%2520existing%2520T2I%2520models.%2520We%2520build%2520upon%2520this%250Aby%2520developing%2520a%2520training-free%252C%2520knowledge%2520distillation%2520technique%2520utilizing%2520Large%250ALanguage%2520Models%2520to%2520address%2520this%2520limitation.%2520Specifically%252C%2520we%2520enhance%2520prompts%2520by%250Aincorporating%2520dense%2520information%2520across%2520three%2520dimensions%252C%2520observing%2520that%250Ainjecting%2520prompts%2520with%2520temporal%2520details%2520significantly%2520improves%2520image%2520generation%250Aaccuracy%252C%2520with%2520our%2520best%2520model%2520achieving%2520an%2520increase%2520of%252072%2525.%2520Our%2520findings%250Ahighlight%2520the%2520limitations%2520of%2520current%2520T2I%2520methods%2520in%2520generating%2520images%2520that%250Arequire%2520complex%2520reasoning%2520and%2520demonstrate%2520that%2520integrating%2520linguistic%2520knowledge%250Ain%2520a%2520systematic%2520way%2520can%2520notably%2520advance%2520the%2520generation%2520of%2520nuanced%2520and%250Acontextually%2520accurate%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AcT2I%3A%20Evaluating%20and%20Improving%20Action%20Depiction%20in%20Text-to-Image%20Models&entry.906535625=Vatsal%20Malaviya%20and%20Agneet%20Chatterjee%20and%20Maitreya%20Patel%20and%20Yezhou%20Yang%20and%20Chitta%20Baral&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20models%20have%20recently%20achieved%20remarkable%20success%20in%0Agenerating%20images%20from%20textual%20descriptions.%20However%2C%20challenges%20still%20persist%0Ain%20accurately%20rendering%20complex%20scenes%20where%20actions%20and%20interactions%20form%20the%0Aprimary%20semantic%20focus.%20Our%20key%20observation%20in%20this%20work%20is%20that%20T2I%20models%0Afrequently%20struggle%20to%20capture%20nuanced%20and%20often%20implicit%20attributes%20inherent%0Ain%20action%20depiction%2C%20leading%20to%20generating%20images%20that%20lack%20key%20contextual%0Adetails.%20To%20enable%20systematic%20evaluation%2C%20we%20introduce%20AcT2I%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20performance%20of%20T2I%20models%20in%20generating%20images%20from%0Aaction-centric%20prompts.%20We%20experimentally%20validate%20that%20leading%20T2I%20models%20do%0Anot%20fare%20well%20on%20AcT2I.%20We%20further%20hypothesize%20that%20this%20shortcoming%20arises%0Afrom%20the%20incomplete%20representation%20of%20the%20inherent%20attributes%20and%20contextual%0Adependencies%20in%20the%20training%20corpora%20of%20existing%20T2I%20models.%20We%20build%20upon%20this%0Aby%20developing%20a%20training-free%2C%20knowledge%20distillation%20technique%20utilizing%20Large%0ALanguage%20Models%20to%20address%20this%20limitation.%20Specifically%2C%20we%20enhance%20prompts%20by%0Aincorporating%20dense%20information%20across%20three%20dimensions%2C%20observing%20that%0Ainjecting%20prompts%20with%20temporal%20details%20significantly%20improves%20image%20generation%0Aaccuracy%2C%20with%20our%20best%20model%20achieving%20an%20increase%20of%2072%25.%20Our%20findings%0Ahighlight%20the%20limitations%20of%20current%20T2I%20methods%20in%20generating%20images%20that%0Arequire%20complex%20reasoning%20and%20demonstrate%20that%20integrating%20linguistic%20knowledge%0Ain%20a%20systematic%20way%20can%20notably%20advance%20the%20generation%20of%20nuanced%20and%0Acontextually%20accurate%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16141v1&entry.124074799=Read"},
{"title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data", "author": "Bidyapati Pradhan and Surajit Dasgupta and Amit Kumar Saha and Omkar Anustoop and Sriram Puttagunta and Vipul Mittal and Gopal Sarda", "abstract": "  The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines.\n", "link": "http://arxiv.org/abs/2508.15432v2", "date": "2025-09-19", "relevancy": 2.2281, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5723}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5471}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data&body=Title%3A%20SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data%0AAuthor%3A%20Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda%0AAbstract%3A%20%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%0Athe%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Aalignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%0Ascalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%0Afor%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%0Aconfiguration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%0Aminimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%0Amechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%0Aautomatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%0Aconversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%0Aresulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%0Aand%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%0Aworkflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%0Aand%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%0Aoverhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyGra%253A%2520A%2520Unified%2520Graph-Based%2520Framework%2520for%2520Scalable%2520Generation%252C%2520Quality%250A%2520%2520Tagging%252C%2520and%2520Management%2520of%2520Synthetic%2520Data%26entry.906535625%3DBidyapati%2520Pradhan%2520and%2520Surajit%2520Dasgupta%2520and%2520Amit%2520Kumar%2520Saha%2520and%2520Omkar%2520Anustoop%2520and%2520Sriram%2520Puttagunta%2520and%2520Vipul%2520Mittal%2520and%2520Gopal%2520Sarda%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520critically%2520dependent%2520on%250Athe%2520availability%2520of%2520high-quality%2520datasets%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%250Aalignment%2520tasks%2520like%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520etc.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520comprehensive%2520synthetic%2520data%2520generation%2520framework%2520that%2520facilitates%250Ascalable%252C%2520configurable%252C%2520and%2520high-fidelity%2520generation%2520of%2520synthetic%2520data%2520tailored%250Afor%2520these%2520training%2520paradigms.%2520Our%2520approach%2520employs%2520a%2520modular%2520and%250Aconfiguration-based%2520pipeline%2520capable%2520of%2520modeling%2520complex%2520dialogue%2520flows%2520with%250Aminimal%2520manual%2520intervention.%2520This%2520framework%2520uses%2520a%2520dual-stage%2520quality%2520tagging%250Amechanism%252C%2520combining%2520heuristic%2520rules%2520and%2520LLM-based%2520evaluations%252C%2520to%250Aautomatically%2520filter%2520and%2520score%2520data%2520extracted%2520from%2520OASST-formatted%250Aconversations%252C%2520ensuring%2520the%2520curation%2520of%2520high-quality%2520dialogue%2520samples.%2520The%250Aresulting%2520datasets%2520are%2520structured%2520under%2520a%2520flexible%2520schema%2520supporting%2520both%2520SFT%250Aand%2520DPO%2520use%2520cases%252C%2520enabling%2520seamless%2520integration%2520into%2520diverse%2520training%250Aworkflows.%2520Together%252C%2520these%2520innovations%2520offer%2520a%2520robust%2520solution%2520for%2520generating%250Aand%2520managing%2520synthetic%2520conversational%2520data%2520at%2520scale%252C%2520significantly%2520reducing%2520the%250Aoverhead%2520of%2520data%2520preparation%2520in%2520LLM%2520training%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data&entry.906535625=Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda&entry.1292438233=%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%0Athe%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Aalignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%0Ascalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%0Afor%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%0Aconfiguration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%0Aminimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%0Amechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%0Aautomatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%0Aconversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%0Aresulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%0Aand%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%0Aworkflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%0Aand%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%0Aoverhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15432v2&entry.124074799=Read"},
{"title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation\n  Model for Multimodal Image Matching", "author": "Meng Yang and Fan Fan and Zizhuo Li and Songchu Deng and Yong Ma and Jiayi Ma", "abstract": "  Multimodal image matching seeks pixel-level correspondences between images of\ndifferent modalities, crucial for cross-modal perception, fusion and analysis.\nHowever, the significant appearance differences between modalities make this\ntask challenging. Due to the scarcity of high-quality annotated datasets,\nexisting deep learning methods that extract modality-common features for\nmatching perform poorly and lack adaptability to diverse scenarios. Vision\nFoundation Model (VFM), trained on large-scale data, yields generalizable and\nrobust feature representations adapted to data and tasks of various modalities,\nincluding multimodal matching. Thus, we propose DistillMatch, a multimodal\nimage matching method using knowledge distillation from VFM. DistillMatch\nemploys knowledge distillation to build a lightweight student model that\nextracts high-level semantic features from VFM (including DINOv2 and DINOv3) to\nassist matching across modalities. To retain modality-specific information, it\nextracts and injects modality category information into the other modality's\nfeatures, which enhances the model's understanding of cross-modal correlations.\nFurthermore, we design V2I-GAN to boost the model's generalization by\ntranslating visible to pseudo-infrared images for data augmentation.\nExperiments show that DistillMatch outperforms existing algorithms on public\ndatasets.\n", "link": "http://arxiv.org/abs/2509.16017v1", "date": "2025-09-19", "relevancy": 2.2263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistillMatch%3A%20Leveraging%20Knowledge%20Distillation%20from%20Vision%20Foundation%0A%20%20Model%20for%20Multimodal%20Image%20Matching&body=Title%3A%20DistillMatch%3A%20Leveraging%20Knowledge%20Distillation%20from%20Vision%20Foundation%0A%20%20Model%20for%20Multimodal%20Image%20Matching%0AAuthor%3A%20Meng%20Yang%20and%20Fan%20Fan%20and%20Zizhuo%20Li%20and%20Songchu%20Deng%20and%20Yong%20Ma%20and%20Jiayi%20Ma%0AAbstract%3A%20%20%20Multimodal%20image%20matching%20seeks%20pixel-level%20correspondences%20between%20images%20of%0Adifferent%20modalities%2C%20crucial%20for%20cross-modal%20perception%2C%20fusion%20and%20analysis.%0AHowever%2C%20the%20significant%20appearance%20differences%20between%20modalities%20make%20this%0Atask%20challenging.%20Due%20to%20the%20scarcity%20of%20high-quality%20annotated%20datasets%2C%0Aexisting%20deep%20learning%20methods%20that%20extract%20modality-common%20features%20for%0Amatching%20perform%20poorly%20and%20lack%20adaptability%20to%20diverse%20scenarios.%20Vision%0AFoundation%20Model%20%28VFM%29%2C%20trained%20on%20large-scale%20data%2C%20yields%20generalizable%20and%0Arobust%20feature%20representations%20adapted%20to%20data%20and%20tasks%20of%20various%20modalities%2C%0Aincluding%20multimodal%20matching.%20Thus%2C%20we%20propose%20DistillMatch%2C%20a%20multimodal%0Aimage%20matching%20method%20using%20knowledge%20distillation%20from%20VFM.%20DistillMatch%0Aemploys%20knowledge%20distillation%20to%20build%20a%20lightweight%20student%20model%20that%0Aextracts%20high-level%20semantic%20features%20from%20VFM%20%28including%20DINOv2%20and%20DINOv3%29%20to%0Aassist%20matching%20across%20modalities.%20To%20retain%20modality-specific%20information%2C%20it%0Aextracts%20and%20injects%20modality%20category%20information%20into%20the%20other%20modality%27s%0Afeatures%2C%20which%20enhances%20the%20model%27s%20understanding%20of%20cross-modal%20correlations.%0AFurthermore%2C%20we%20design%20V2I-GAN%20to%20boost%20the%20model%27s%20generalization%20by%0Atranslating%20visible%20to%20pseudo-infrared%20images%20for%20data%20augmentation.%0AExperiments%20show%20that%20DistillMatch%20outperforms%20existing%20algorithms%20on%20public%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistillMatch%253A%2520Leveraging%2520Knowledge%2520Distillation%2520from%2520Vision%2520Foundation%250A%2520%2520Model%2520for%2520Multimodal%2520Image%2520Matching%26entry.906535625%3DMeng%2520Yang%2520and%2520Fan%2520Fan%2520and%2520Zizhuo%2520Li%2520and%2520Songchu%2520Deng%2520and%2520Yong%2520Ma%2520and%2520Jiayi%2520Ma%26entry.1292438233%3D%2520%2520Multimodal%2520image%2520matching%2520seeks%2520pixel-level%2520correspondences%2520between%2520images%2520of%250Adifferent%2520modalities%252C%2520crucial%2520for%2520cross-modal%2520perception%252C%2520fusion%2520and%2520analysis.%250AHowever%252C%2520the%2520significant%2520appearance%2520differences%2520between%2520modalities%2520make%2520this%250Atask%2520challenging.%2520Due%2520to%2520the%2520scarcity%2520of%2520high-quality%2520annotated%2520datasets%252C%250Aexisting%2520deep%2520learning%2520methods%2520that%2520extract%2520modality-common%2520features%2520for%250Amatching%2520perform%2520poorly%2520and%2520lack%2520adaptability%2520to%2520diverse%2520scenarios.%2520Vision%250AFoundation%2520Model%2520%2528VFM%2529%252C%2520trained%2520on%2520large-scale%2520data%252C%2520yields%2520generalizable%2520and%250Arobust%2520feature%2520representations%2520adapted%2520to%2520data%2520and%2520tasks%2520of%2520various%2520modalities%252C%250Aincluding%2520multimodal%2520matching.%2520Thus%252C%2520we%2520propose%2520DistillMatch%252C%2520a%2520multimodal%250Aimage%2520matching%2520method%2520using%2520knowledge%2520distillation%2520from%2520VFM.%2520DistillMatch%250Aemploys%2520knowledge%2520distillation%2520to%2520build%2520a%2520lightweight%2520student%2520model%2520that%250Aextracts%2520high-level%2520semantic%2520features%2520from%2520VFM%2520%2528including%2520DINOv2%2520and%2520DINOv3%2529%2520to%250Aassist%2520matching%2520across%2520modalities.%2520To%2520retain%2520modality-specific%2520information%252C%2520it%250Aextracts%2520and%2520injects%2520modality%2520category%2520information%2520into%2520the%2520other%2520modality%2527s%250Afeatures%252C%2520which%2520enhances%2520the%2520model%2527s%2520understanding%2520of%2520cross-modal%2520correlations.%250AFurthermore%252C%2520we%2520design%2520V2I-GAN%2520to%2520boost%2520the%2520model%2527s%2520generalization%2520by%250Atranslating%2520visible%2520to%2520pseudo-infrared%2520images%2520for%2520data%2520augmentation.%250AExperiments%2520show%2520that%2520DistillMatch%2520outperforms%2520existing%2520algorithms%2520on%2520public%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistillMatch%3A%20Leveraging%20Knowledge%20Distillation%20from%20Vision%20Foundation%0A%20%20Model%20for%20Multimodal%20Image%20Matching&entry.906535625=Meng%20Yang%20and%20Fan%20Fan%20and%20Zizhuo%20Li%20and%20Songchu%20Deng%20and%20Yong%20Ma%20and%20Jiayi%20Ma&entry.1292438233=%20%20Multimodal%20image%20matching%20seeks%20pixel-level%20correspondences%20between%20images%20of%0Adifferent%20modalities%2C%20crucial%20for%20cross-modal%20perception%2C%20fusion%20and%20analysis.%0AHowever%2C%20the%20significant%20appearance%20differences%20between%20modalities%20make%20this%0Atask%20challenging.%20Due%20to%20the%20scarcity%20of%20high-quality%20annotated%20datasets%2C%0Aexisting%20deep%20learning%20methods%20that%20extract%20modality-common%20features%20for%0Amatching%20perform%20poorly%20and%20lack%20adaptability%20to%20diverse%20scenarios.%20Vision%0AFoundation%20Model%20%28VFM%29%2C%20trained%20on%20large-scale%20data%2C%20yields%20generalizable%20and%0Arobust%20feature%20representations%20adapted%20to%20data%20and%20tasks%20of%20various%20modalities%2C%0Aincluding%20multimodal%20matching.%20Thus%2C%20we%20propose%20DistillMatch%2C%20a%20multimodal%0Aimage%20matching%20method%20using%20knowledge%20distillation%20from%20VFM.%20DistillMatch%0Aemploys%20knowledge%20distillation%20to%20build%20a%20lightweight%20student%20model%20that%0Aextracts%20high-level%20semantic%20features%20from%20VFM%20%28including%20DINOv2%20and%20DINOv3%29%20to%0Aassist%20matching%20across%20modalities.%20To%20retain%20modality-specific%20information%2C%20it%0Aextracts%20and%20injects%20modality%20category%20information%20into%20the%20other%20modality%27s%0Afeatures%2C%20which%20enhances%20the%20model%27s%20understanding%20of%20cross-modal%20correlations.%0AFurthermore%2C%20we%20design%20V2I-GAN%20to%20boost%20the%20model%27s%20generalization%20by%0Atranslating%20visible%20to%20pseudo-infrared%20images%20for%20data%20augmentation.%0AExperiments%20show%20that%20DistillMatch%20outperforms%20existing%20algorithms%20on%20public%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16017v1&entry.124074799=Read"},
{"title": "High-Bandwidth Tactile-Reactive Control for Grasp Adjustment", "author": "Yonghyeon Lee and Tzu-Yuan Lin and Alexander Alexiev and Sangbae Kim", "abstract": "  Vision-only grasping systems are fundamentally constrained by calibration\nerrors, sensor noise, and grasp pose prediction inaccuracies, leading to\nunavoidable contact uncertainty in the final stage of grasping. High-bandwidth\ntactile feedback, when paired with a well-designed tactile-reactive controller,\ncan significantly improve robustness in the presence of perception errors. This\npaper contributes to controller design by proposing a purely tactile-feedback\ngrasp-adjustment algorithm. The proposed controller requires neither prior\nknowledge of the object's geometry nor an accurate grasp pose, and is capable\nof refining a grasp even when starting from a crude, imprecise initial\nconfiguration and uncertain contact points. Through simulation studies and\nreal-world experiments on a 15-DoF arm-hand system (featuring an 8-DoF hand)\nequipped with fingertip tactile sensors operating at 200 Hz, we demonstrate\nthat our tactile-reactive grasping framework effectively improves grasp\nstability.\n", "link": "http://arxiv.org/abs/2509.15876v1", "date": "2025-09-19", "relevancy": 2.2181, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5609}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Bandwidth%20Tactile-Reactive%20Control%20for%20Grasp%20Adjustment&body=Title%3A%20High-Bandwidth%20Tactile-Reactive%20Control%20for%20Grasp%20Adjustment%0AAuthor%3A%20Yonghyeon%20Lee%20and%20Tzu-Yuan%20Lin%20and%20Alexander%20Alexiev%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20Vision-only%20grasping%20systems%20are%20fundamentally%20constrained%20by%20calibration%0Aerrors%2C%20sensor%20noise%2C%20and%20grasp%20pose%20prediction%20inaccuracies%2C%20leading%20to%0Aunavoidable%20contact%20uncertainty%20in%20the%20final%20stage%20of%20grasping.%20High-bandwidth%0Atactile%20feedback%2C%20when%20paired%20with%20a%20well-designed%20tactile-reactive%20controller%2C%0Acan%20significantly%20improve%20robustness%20in%20the%20presence%20of%20perception%20errors.%20This%0Apaper%20contributes%20to%20controller%20design%20by%20proposing%20a%20purely%20tactile-feedback%0Agrasp-adjustment%20algorithm.%20The%20proposed%20controller%20requires%20neither%20prior%0Aknowledge%20of%20the%20object%27s%20geometry%20nor%20an%20accurate%20grasp%20pose%2C%20and%20is%20capable%0Aof%20refining%20a%20grasp%20even%20when%20starting%20from%20a%20crude%2C%20imprecise%20initial%0Aconfiguration%20and%20uncertain%20contact%20points.%20Through%20simulation%20studies%20and%0Areal-world%20experiments%20on%20a%2015-DoF%20arm-hand%20system%20%28featuring%20an%208-DoF%20hand%29%0Aequipped%20with%20fingertip%20tactile%20sensors%20operating%20at%20200%20Hz%2C%20we%20demonstrate%0Athat%20our%20tactile-reactive%20grasping%20framework%20effectively%20improves%20grasp%0Astability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Bandwidth%2520Tactile-Reactive%2520Control%2520for%2520Grasp%2520Adjustment%26entry.906535625%3DYonghyeon%2520Lee%2520and%2520Tzu-Yuan%2520Lin%2520and%2520Alexander%2520Alexiev%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520Vision-only%2520grasping%2520systems%2520are%2520fundamentally%2520constrained%2520by%2520calibration%250Aerrors%252C%2520sensor%2520noise%252C%2520and%2520grasp%2520pose%2520prediction%2520inaccuracies%252C%2520leading%2520to%250Aunavoidable%2520contact%2520uncertainty%2520in%2520the%2520final%2520stage%2520of%2520grasping.%2520High-bandwidth%250Atactile%2520feedback%252C%2520when%2520paired%2520with%2520a%2520well-designed%2520tactile-reactive%2520controller%252C%250Acan%2520significantly%2520improve%2520robustness%2520in%2520the%2520presence%2520of%2520perception%2520errors.%2520This%250Apaper%2520contributes%2520to%2520controller%2520design%2520by%2520proposing%2520a%2520purely%2520tactile-feedback%250Agrasp-adjustment%2520algorithm.%2520The%2520proposed%2520controller%2520requires%2520neither%2520prior%250Aknowledge%2520of%2520the%2520object%2527s%2520geometry%2520nor%2520an%2520accurate%2520grasp%2520pose%252C%2520and%2520is%2520capable%250Aof%2520refining%2520a%2520grasp%2520even%2520when%2520starting%2520from%2520a%2520crude%252C%2520imprecise%2520initial%250Aconfiguration%2520and%2520uncertain%2520contact%2520points.%2520Through%2520simulation%2520studies%2520and%250Areal-world%2520experiments%2520on%2520a%252015-DoF%2520arm-hand%2520system%2520%2528featuring%2520an%25208-DoF%2520hand%2529%250Aequipped%2520with%2520fingertip%2520tactile%2520sensors%2520operating%2520at%2520200%2520Hz%252C%2520we%2520demonstrate%250Athat%2520our%2520tactile-reactive%2520grasping%2520framework%2520effectively%2520improves%2520grasp%250Astability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Bandwidth%20Tactile-Reactive%20Control%20for%20Grasp%20Adjustment&entry.906535625=Yonghyeon%20Lee%20and%20Tzu-Yuan%20Lin%20and%20Alexander%20Alexiev%20and%20Sangbae%20Kim&entry.1292438233=%20%20Vision-only%20grasping%20systems%20are%20fundamentally%20constrained%20by%20calibration%0Aerrors%2C%20sensor%20noise%2C%20and%20grasp%20pose%20prediction%20inaccuracies%2C%20leading%20to%0Aunavoidable%20contact%20uncertainty%20in%20the%20final%20stage%20of%20grasping.%20High-bandwidth%0Atactile%20feedback%2C%20when%20paired%20with%20a%20well-designed%20tactile-reactive%20controller%2C%0Acan%20significantly%20improve%20robustness%20in%20the%20presence%20of%20perception%20errors.%20This%0Apaper%20contributes%20to%20controller%20design%20by%20proposing%20a%20purely%20tactile-feedback%0Agrasp-adjustment%20algorithm.%20The%20proposed%20controller%20requires%20neither%20prior%0Aknowledge%20of%20the%20object%27s%20geometry%20nor%20an%20accurate%20grasp%20pose%2C%20and%20is%20capable%0Aof%20refining%20a%20grasp%20even%20when%20starting%20from%20a%20crude%2C%20imprecise%20initial%0Aconfiguration%20and%20uncertain%20contact%20points.%20Through%20simulation%20studies%20and%0Areal-world%20experiments%20on%20a%2015-DoF%20arm-hand%20system%20%28featuring%20an%208-DoF%20hand%29%0Aequipped%20with%20fingertip%20tactile%20sensors%20operating%20at%20200%20Hz%2C%20we%20demonstrate%0Athat%20our%20tactile-reactive%20grasping%20framework%20effectively%20improves%20grasp%0Astability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15876v1&entry.124074799=Read"},
{"title": "Shedding Light on Depth: Explainability Assessment in Monocular Depth\n  Estimation", "author": "Lorenzo Cirillo and Claudio Schiavella and Lorenzo Papa and Paolo Russo and Irene Amerini", "abstract": "  Explainable artificial intelligence is increasingly employed to understand\nthe decision-making process of deep learning models and create trustworthiness\nin their adoption. However, the explainability of Monocular Depth Estimation\n(MDE) remains largely unexplored despite its wide deployment in real-world\napplications. In this work, we study how to analyze MDE networks to map the\ninput image to the predicted depth map. More in detail, we investigate\nwell-established feature attribution methods, Saliency Maps, Integrated\nGradients, and Attention Rollout on different computationally complex models\nfor MDE: METER, a lightweight network, and PixelFormer, a deep network. We\nassess the quality of the generated visual explanations by selectively\nperturbing the most relevant and irrelevant pixels, as identified by the\nexplainability methods, and analyzing the impact of these perturbations on the\nmodel's output. Moreover, since existing evaluation metrics can have some\nlimitations in measuring the validity of visual explanations for MDE, we\nadditionally introduce the Attribution Fidelity. This metric evaluates the\nreliability of the feature attribution by assessing their consistency with the\npredicted depth map. Experimental results demonstrate that Saliency Maps and\nIntegrated Gradients have good performance in highlighting the most important\ninput features for MDE lightweight and deep models, respectively. Furthermore,\nwe show that Attribution Fidelity effectively identifies whether an\nexplainability method fails to produce reliable visual maps, even in scenarios\nwhere conventional metrics might suggest satisfactory results.\n", "link": "http://arxiv.org/abs/2509.15980v1", "date": "2025-09-19", "relevancy": 2.2139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shedding%20Light%20on%20Depth%3A%20Explainability%20Assessment%20in%20Monocular%20Depth%0A%20%20Estimation&body=Title%3A%20Shedding%20Light%20on%20Depth%3A%20Explainability%20Assessment%20in%20Monocular%20Depth%0A%20%20Estimation%0AAuthor%3A%20Lorenzo%20Cirillo%20and%20Claudio%20Schiavella%20and%20Lorenzo%20Papa%20and%20Paolo%20Russo%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20Explainable%20artificial%20intelligence%20is%20increasingly%20employed%20to%20understand%0Athe%20decision-making%20process%20of%20deep%20learning%20models%20and%20create%20trustworthiness%0Ain%20their%20adoption.%20However%2C%20the%20explainability%20of%20Monocular%20Depth%20Estimation%0A%28MDE%29%20remains%20largely%20unexplored%20despite%20its%20wide%20deployment%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20study%20how%20to%20analyze%20MDE%20networks%20to%20map%20the%0Ainput%20image%20to%20the%20predicted%20depth%20map.%20More%20in%20detail%2C%20we%20investigate%0Awell-established%20feature%20attribution%20methods%2C%20Saliency%20Maps%2C%20Integrated%0AGradients%2C%20and%20Attention%20Rollout%20on%20different%20computationally%20complex%20models%0Afor%20MDE%3A%20METER%2C%20a%20lightweight%20network%2C%20and%20PixelFormer%2C%20a%20deep%20network.%20We%0Aassess%20the%20quality%20of%20the%20generated%20visual%20explanations%20by%20selectively%0Aperturbing%20the%20most%20relevant%20and%20irrelevant%20pixels%2C%20as%20identified%20by%20the%0Aexplainability%20methods%2C%20and%20analyzing%20the%20impact%20of%20these%20perturbations%20on%20the%0Amodel%27s%20output.%20Moreover%2C%20since%20existing%20evaluation%20metrics%20can%20have%20some%0Alimitations%20in%20measuring%20the%20validity%20of%20visual%20explanations%20for%20MDE%2C%20we%0Aadditionally%20introduce%20the%20Attribution%20Fidelity.%20This%20metric%20evaluates%20the%0Areliability%20of%20the%20feature%20attribution%20by%20assessing%20their%20consistency%20with%20the%0Apredicted%20depth%20map.%20Experimental%20results%20demonstrate%20that%20Saliency%20Maps%20and%0AIntegrated%20Gradients%20have%20good%20performance%20in%20highlighting%20the%20most%20important%0Ainput%20features%20for%20MDE%20lightweight%20and%20deep%20models%2C%20respectively.%20Furthermore%2C%0Awe%20show%20that%20Attribution%20Fidelity%20effectively%20identifies%20whether%20an%0Aexplainability%20method%20fails%20to%20produce%20reliable%20visual%20maps%2C%20even%20in%20scenarios%0Awhere%20conventional%20metrics%20might%20suggest%20satisfactory%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShedding%2520Light%2520on%2520Depth%253A%2520Explainability%2520Assessment%2520in%2520Monocular%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DLorenzo%2520Cirillo%2520and%2520Claudio%2520Schiavella%2520and%2520Lorenzo%2520Papa%2520and%2520Paolo%2520Russo%2520and%2520Irene%2520Amerini%26entry.1292438233%3D%2520%2520Explainable%2520artificial%2520intelligence%2520is%2520increasingly%2520employed%2520to%2520understand%250Athe%2520decision-making%2520process%2520of%2520deep%2520learning%2520models%2520and%2520create%2520trustworthiness%250Ain%2520their%2520adoption.%2520However%252C%2520the%2520explainability%2520of%2520Monocular%2520Depth%2520Estimation%250A%2528MDE%2529%2520remains%2520largely%2520unexplored%2520despite%2520its%2520wide%2520deployment%2520in%2520real-world%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520study%2520how%2520to%2520analyze%2520MDE%2520networks%2520to%2520map%2520the%250Ainput%2520image%2520to%2520the%2520predicted%2520depth%2520map.%2520More%2520in%2520detail%252C%2520we%2520investigate%250Awell-established%2520feature%2520attribution%2520methods%252C%2520Saliency%2520Maps%252C%2520Integrated%250AGradients%252C%2520and%2520Attention%2520Rollout%2520on%2520different%2520computationally%2520complex%2520models%250Afor%2520MDE%253A%2520METER%252C%2520a%2520lightweight%2520network%252C%2520and%2520PixelFormer%252C%2520a%2520deep%2520network.%2520We%250Aassess%2520the%2520quality%2520of%2520the%2520generated%2520visual%2520explanations%2520by%2520selectively%250Aperturbing%2520the%2520most%2520relevant%2520and%2520irrelevant%2520pixels%252C%2520as%2520identified%2520by%2520the%250Aexplainability%2520methods%252C%2520and%2520analyzing%2520the%2520impact%2520of%2520these%2520perturbations%2520on%2520the%250Amodel%2527s%2520output.%2520Moreover%252C%2520since%2520existing%2520evaluation%2520metrics%2520can%2520have%2520some%250Alimitations%2520in%2520measuring%2520the%2520validity%2520of%2520visual%2520explanations%2520for%2520MDE%252C%2520we%250Aadditionally%2520introduce%2520the%2520Attribution%2520Fidelity.%2520This%2520metric%2520evaluates%2520the%250Areliability%2520of%2520the%2520feature%2520attribution%2520by%2520assessing%2520their%2520consistency%2520with%2520the%250Apredicted%2520depth%2520map.%2520Experimental%2520results%2520demonstrate%2520that%2520Saliency%2520Maps%2520and%250AIntegrated%2520Gradients%2520have%2520good%2520performance%2520in%2520highlighting%2520the%2520most%2520important%250Ainput%2520features%2520for%2520MDE%2520lightweight%2520and%2520deep%2520models%252C%2520respectively.%2520Furthermore%252C%250Awe%2520show%2520that%2520Attribution%2520Fidelity%2520effectively%2520identifies%2520whether%2520an%250Aexplainability%2520method%2520fails%2520to%2520produce%2520reliable%2520visual%2520maps%252C%2520even%2520in%2520scenarios%250Awhere%2520conventional%2520metrics%2520might%2520suggest%2520satisfactory%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shedding%20Light%20on%20Depth%3A%20Explainability%20Assessment%20in%20Monocular%20Depth%0A%20%20Estimation&entry.906535625=Lorenzo%20Cirillo%20and%20Claudio%20Schiavella%20and%20Lorenzo%20Papa%20and%20Paolo%20Russo%20and%20Irene%20Amerini&entry.1292438233=%20%20Explainable%20artificial%20intelligence%20is%20increasingly%20employed%20to%20understand%0Athe%20decision-making%20process%20of%20deep%20learning%20models%20and%20create%20trustworthiness%0Ain%20their%20adoption.%20However%2C%20the%20explainability%20of%20Monocular%20Depth%20Estimation%0A%28MDE%29%20remains%20largely%20unexplored%20despite%20its%20wide%20deployment%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20study%20how%20to%20analyze%20MDE%20networks%20to%20map%20the%0Ainput%20image%20to%20the%20predicted%20depth%20map.%20More%20in%20detail%2C%20we%20investigate%0Awell-established%20feature%20attribution%20methods%2C%20Saliency%20Maps%2C%20Integrated%0AGradients%2C%20and%20Attention%20Rollout%20on%20different%20computationally%20complex%20models%0Afor%20MDE%3A%20METER%2C%20a%20lightweight%20network%2C%20and%20PixelFormer%2C%20a%20deep%20network.%20We%0Aassess%20the%20quality%20of%20the%20generated%20visual%20explanations%20by%20selectively%0Aperturbing%20the%20most%20relevant%20and%20irrelevant%20pixels%2C%20as%20identified%20by%20the%0Aexplainability%20methods%2C%20and%20analyzing%20the%20impact%20of%20these%20perturbations%20on%20the%0Amodel%27s%20output.%20Moreover%2C%20since%20existing%20evaluation%20metrics%20can%20have%20some%0Alimitations%20in%20measuring%20the%20validity%20of%20visual%20explanations%20for%20MDE%2C%20we%0Aadditionally%20introduce%20the%20Attribution%20Fidelity.%20This%20metric%20evaluates%20the%0Areliability%20of%20the%20feature%20attribution%20by%20assessing%20their%20consistency%20with%20the%0Apredicted%20depth%20map.%20Experimental%20results%20demonstrate%20that%20Saliency%20Maps%20and%0AIntegrated%20Gradients%20have%20good%20performance%20in%20highlighting%20the%20most%20important%0Ainput%20features%20for%20MDE%20lightweight%20and%20deep%20models%2C%20respectively.%20Furthermore%2C%0Awe%20show%20that%20Attribution%20Fidelity%20effectively%20identifies%20whether%20an%0Aexplainability%20method%20fails%20to%20produce%20reliable%20visual%20maps%2C%20even%20in%20scenarios%0Awhere%20conventional%20metrics%20might%20suggest%20satisfactory%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15980v1&entry.124074799=Read"},
{"title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "author": "Marc Lafon and Gustavo Adolfo Vargas Hakim and Cl\u00e9ment Rambour and Christian Desrosier and Nicolas Thome", "abstract": "  Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.\n", "link": "http://arxiv.org/abs/2507.14312v2", "date": "2025-09-19", "relevancy": 2.2104, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5549}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPTTA%3A%20Robust%20Contrastive%20Vision-Language%20Test-Time%20Adaptation&body=Title%3A%20CLIPTTA%3A%20Robust%20Contrastive%20Vision-Language%20Test-Time%20Adaptation%0AAuthor%3A%20Marc%20Lafon%20and%20Gustavo%20Adolfo%20Vargas%20Hakim%20and%20Cl%C3%A9ment%20Rambour%20and%20Christian%20Desrosier%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20exhibit%20strong%20zero-shot%20capabilities%0Abut%20often%20fail%20to%20generalize%20under%20distribution%20shifts.%20Test-time%20adaptation%0A%28TTA%29%20allows%20models%20to%20update%20at%20inference%20time%20without%20labeled%20data%2C%20typically%0Avia%20entropy%20minimization.%20However%2C%20this%20objective%20is%20fundamentally%20misaligned%0Awith%20the%20contrastive%20image-text%20training%20of%20VLMs%2C%20limiting%20adaptation%0Aperformance%20and%20introducing%20failure%20modes%20such%20as%20pseudo-label%20drift%20and%20class%0Acollapse.%20We%20propose%20CLIPTTA%2C%20a%20new%20gradient-based%20TTA%20method%20for%0Avision-language%20models%20that%20leverages%20a%20soft%20contrastive%20loss%20aligned%20with%0ACLIP%27s%20pre-training%20objective.%20We%20provide%20a%20theoretical%20analysis%20of%20CLIPTTA%27s%0Agradients%2C%20showing%20how%20its%20batch-aware%20design%20mitigates%20the%20risk%20of%20collapse.%0AWe%20further%20extend%20CLIPTTA%20to%20the%20open-set%20setting%2C%20where%20both%20in-distribution%0A%28ID%29%20and%20out-of-distribution%20%28OOD%29%20samples%20are%20encountered%2C%20using%20an%20Outlier%0AContrastive%20Exposure%20%28OCE%29%20loss%20to%20improve%20OOD%20detection.%20Evaluated%20on%2075%0Adatasets%20spanning%20diverse%20distribution%20shifts%2C%20CLIPTTA%20consistently%20outperforms%0Aentropy-based%20objectives%20and%20is%20highly%20competitive%20with%20state-of-the-art%20TTA%0Amethods%2C%20outperforming%20them%20on%20a%20large%20number%20of%20datasets%20and%20exhibiting%20more%0Astable%20performance%20across%20diverse%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPTTA%253A%2520Robust%2520Contrastive%2520Vision-Language%2520Test-Time%2520Adaptation%26entry.906535625%3DMarc%2520Lafon%2520and%2520Gustavo%2520Adolfo%2520Vargas%2520Hakim%2520and%2520Cl%25C3%25A9ment%2520Rambour%2520and%2520Christian%2520Desrosier%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520exhibit%2520strong%2520zero-shot%2520capabilities%250Abut%2520often%2520fail%2520to%2520generalize%2520under%2520distribution%2520shifts.%2520Test-time%2520adaptation%250A%2528TTA%2529%2520allows%2520models%2520to%2520update%2520at%2520inference%2520time%2520without%2520labeled%2520data%252C%2520typically%250Avia%2520entropy%2520minimization.%2520However%252C%2520this%2520objective%2520is%2520fundamentally%2520misaligned%250Awith%2520the%2520contrastive%2520image-text%2520training%2520of%2520VLMs%252C%2520limiting%2520adaptation%250Aperformance%2520and%2520introducing%2520failure%2520modes%2520such%2520as%2520pseudo-label%2520drift%2520and%2520class%250Acollapse.%2520We%2520propose%2520CLIPTTA%252C%2520a%2520new%2520gradient-based%2520TTA%2520method%2520for%250Avision-language%2520models%2520that%2520leverages%2520a%2520soft%2520contrastive%2520loss%2520aligned%2520with%250ACLIP%2527s%2520pre-training%2520objective.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520of%2520CLIPTTA%2527s%250Agradients%252C%2520showing%2520how%2520its%2520batch-aware%2520design%2520mitigates%2520the%2520risk%2520of%2520collapse.%250AWe%2520further%2520extend%2520CLIPTTA%2520to%2520the%2520open-set%2520setting%252C%2520where%2520both%2520in-distribution%250A%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520are%2520encountered%252C%2520using%2520an%2520Outlier%250AContrastive%2520Exposure%2520%2528OCE%2529%2520loss%2520to%2520improve%2520OOD%2520detection.%2520Evaluated%2520on%252075%250Adatasets%2520spanning%2520diverse%2520distribution%2520shifts%252C%2520CLIPTTA%2520consistently%2520outperforms%250Aentropy-based%2520objectives%2520and%2520is%2520highly%2520competitive%2520with%2520state-of-the-art%2520TTA%250Amethods%252C%2520outperforming%2520them%2520on%2520a%2520large%2520number%2520of%2520datasets%2520and%2520exhibiting%2520more%250Astable%2520performance%2520across%2520diverse%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPTTA%3A%20Robust%20Contrastive%20Vision-Language%20Test-Time%20Adaptation&entry.906535625=Marc%20Lafon%20and%20Gustavo%20Adolfo%20Vargas%20Hakim%20and%20Cl%C3%A9ment%20Rambour%20and%20Christian%20Desrosier%20and%20Nicolas%20Thome&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20like%20CLIP%20exhibit%20strong%20zero-shot%20capabilities%0Abut%20often%20fail%20to%20generalize%20under%20distribution%20shifts.%20Test-time%20adaptation%0A%28TTA%29%20allows%20models%20to%20update%20at%20inference%20time%20without%20labeled%20data%2C%20typically%0Avia%20entropy%20minimization.%20However%2C%20this%20objective%20is%20fundamentally%20misaligned%0Awith%20the%20contrastive%20image-text%20training%20of%20VLMs%2C%20limiting%20adaptation%0Aperformance%20and%20introducing%20failure%20modes%20such%20as%20pseudo-label%20drift%20and%20class%0Acollapse.%20We%20propose%20CLIPTTA%2C%20a%20new%20gradient-based%20TTA%20method%20for%0Avision-language%20models%20that%20leverages%20a%20soft%20contrastive%20loss%20aligned%20with%0ACLIP%27s%20pre-training%20objective.%20We%20provide%20a%20theoretical%20analysis%20of%20CLIPTTA%27s%0Agradients%2C%20showing%20how%20its%20batch-aware%20design%20mitigates%20the%20risk%20of%20collapse.%0AWe%20further%20extend%20CLIPTTA%20to%20the%20open-set%20setting%2C%20where%20both%20in-distribution%0A%28ID%29%20and%20out-of-distribution%20%28OOD%29%20samples%20are%20encountered%2C%20using%20an%20Outlier%0AContrastive%20Exposure%20%28OCE%29%20loss%20to%20improve%20OOD%20detection.%20Evaluated%20on%2075%0Adatasets%20spanning%20diverse%20distribution%20shifts%2C%20CLIPTTA%20consistently%20outperforms%0Aentropy-based%20objectives%20and%20is%20highly%20competitive%20with%20state-of-the-art%20TTA%0Amethods%2C%20outperforming%20them%20on%20a%20large%20number%20of%20datasets%20and%20exhibiting%20more%0Astable%20performance%20across%20diverse%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14312v2&entry.124074799=Read"},
{"title": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models", "author": "Clemence Grislain and Hamed Rahimi and Olivier Sigaud and Mohamed Chetouani", "abstract": "  Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/).\n", "link": "http://arxiv.org/abs/2509.16072v1", "date": "2025-09-19", "relevancy": 2.202, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models&body=Title%3A%20I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Clemence%20Grislain%20and%20Hamed%20Rahimi%20and%20Olivier%20Sigaud%20and%20Mohamed%20Chetouani%0AAbstract%3A%20%20%20Language-conditioned%20robotic%20manipulation%20in%20open-world%20settings%20requires%20not%0Aonly%20accurate%20task%20execution%20but%20also%20the%20ability%20to%20detect%20failures%20for%20robust%0Adeployment%20in%20real-world%20environments.%20Although%20recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20significantly%20improved%20the%20spatial%20reasoning%0Aand%20task-planning%20capabilities%20of%20robots%2C%20they%20remain%20limited%20in%20their%20ability%0Ato%20recognize%20their%20own%20failures.%20In%20particular%2C%20a%20critical%20yet%20underexplored%0Achallenge%20lies%20in%20detecting%20semantic%20misalignment%20errors%2C%20where%20the%20robot%0Aexecutes%20a%20task%20that%20is%20semantically%20meaningful%20but%20inconsistent%20with%20the%20given%0Ainstruction.%20To%20address%20this%2C%20we%20propose%20a%20method%20for%20building%20datasets%0Atargeting%20Semantic%20Misalignment%20Failures%20detection%2C%20from%20existing%0Alanguage-conditioned%20manipulation%20datasets.%20We%20also%20present%20I-FailSense%2C%20an%0Aopen-source%20VLM%20framework%20with%20grounded%20arbitration%20designed%20specifically%20for%0Afailure%20detection.%20Our%20approach%20relies%20on%20post-training%20a%20base%20VLM%2C%20followed%20by%0Atraining%20lightweight%20classification%20heads%2C%20called%20FS%20blocks%2C%20attached%20to%0Adifferent%20internal%20layers%20of%20the%20VLM%20and%20whose%20predictions%20are%20aggregated%20using%0Aan%20ensembling%20mechanism.%20Experiments%20show%20that%20I-FailSense%20outperforms%0Astate-of-the-art%20VLMs%2C%20both%20comparable%20in%20size%20and%20larger%2C%20in%20detecting%0Asemantic%20misalignment%20errors.%20Notably%2C%20despite%20being%20trained%20only%20on%20semantic%0Amisalignment%20detection%2C%20I-FailSense%20generalizes%20to%20broader%20robotic%20failure%0Acategories%20and%20effectively%20transfers%20to%20other%20simulation%20environments%20and%0Areal-world%20with%20zero-shot%20or%20minimal%20post-training.%20The%20datasets%20and%20models%20are%0Apublicly%20released%20on%20HuggingFace%20%28Webpage%3A%0Ahttps%3A//clemgris.github.io/I-FailSense/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-FailSense%253A%2520Towards%2520General%2520Robotic%2520Failure%2520Detection%2520with%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DClemence%2520Grislain%2520and%2520Hamed%2520Rahimi%2520and%2520Olivier%2520Sigaud%2520and%2520Mohamed%2520Chetouani%26entry.1292438233%3D%2520%2520Language-conditioned%2520robotic%2520manipulation%2520in%2520open-world%2520settings%2520requires%2520not%250Aonly%2520accurate%2520task%2520execution%2520but%2520also%2520the%2520ability%2520to%2520detect%2520failures%2520for%2520robust%250Adeployment%2520in%2520real-world%2520environments.%2520Although%2520recent%2520advances%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520significantly%2520improved%2520the%2520spatial%2520reasoning%250Aand%2520task-planning%2520capabilities%2520of%2520robots%252C%2520they%2520remain%2520limited%2520in%2520their%2520ability%250Ato%2520recognize%2520their%2520own%2520failures.%2520In%2520particular%252C%2520a%2520critical%2520yet%2520underexplored%250Achallenge%2520lies%2520in%2520detecting%2520semantic%2520misalignment%2520errors%252C%2520where%2520the%2520robot%250Aexecutes%2520a%2520task%2520that%2520is%2520semantically%2520meaningful%2520but%2520inconsistent%2520with%2520the%2520given%250Ainstruction.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520method%2520for%2520building%2520datasets%250Atargeting%2520Semantic%2520Misalignment%2520Failures%2520detection%252C%2520from%2520existing%250Alanguage-conditioned%2520manipulation%2520datasets.%2520We%2520also%2520present%2520I-FailSense%252C%2520an%250Aopen-source%2520VLM%2520framework%2520with%2520grounded%2520arbitration%2520designed%2520specifically%2520for%250Afailure%2520detection.%2520Our%2520approach%2520relies%2520on%2520post-training%2520a%2520base%2520VLM%252C%2520followed%2520by%250Atraining%2520lightweight%2520classification%2520heads%252C%2520called%2520FS%2520blocks%252C%2520attached%2520to%250Adifferent%2520internal%2520layers%2520of%2520the%2520VLM%2520and%2520whose%2520predictions%2520are%2520aggregated%2520using%250Aan%2520ensembling%2520mechanism.%2520Experiments%2520show%2520that%2520I-FailSense%2520outperforms%250Astate-of-the-art%2520VLMs%252C%2520both%2520comparable%2520in%2520size%2520and%2520larger%252C%2520in%2520detecting%250Asemantic%2520misalignment%2520errors.%2520Notably%252C%2520despite%2520being%2520trained%2520only%2520on%2520semantic%250Amisalignment%2520detection%252C%2520I-FailSense%2520generalizes%2520to%2520broader%2520robotic%2520failure%250Acategories%2520and%2520effectively%2520transfers%2520to%2520other%2520simulation%2520environments%2520and%250Areal-world%2520with%2520zero-shot%2520or%2520minimal%2520post-training.%2520The%2520datasets%2520and%2520models%2520are%250Apublicly%2520released%2520on%2520HuggingFace%2520%2528Webpage%253A%250Ahttps%253A//clemgris.github.io/I-FailSense/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-FailSense%3A%20Towards%20General%20Robotic%20Failure%20Detection%20with%0A%20%20Vision-Language%20Models&entry.906535625=Clemence%20Grislain%20and%20Hamed%20Rahimi%20and%20Olivier%20Sigaud%20and%20Mohamed%20Chetouani&entry.1292438233=%20%20Language-conditioned%20robotic%20manipulation%20in%20open-world%20settings%20requires%20not%0Aonly%20accurate%20task%20execution%20but%20also%20the%20ability%20to%20detect%20failures%20for%20robust%0Adeployment%20in%20real-world%20environments.%20Although%20recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20significantly%20improved%20the%20spatial%20reasoning%0Aand%20task-planning%20capabilities%20of%20robots%2C%20they%20remain%20limited%20in%20their%20ability%0Ato%20recognize%20their%20own%20failures.%20In%20particular%2C%20a%20critical%20yet%20underexplored%0Achallenge%20lies%20in%20detecting%20semantic%20misalignment%20errors%2C%20where%20the%20robot%0Aexecutes%20a%20task%20that%20is%20semantically%20meaningful%20but%20inconsistent%20with%20the%20given%0Ainstruction.%20To%20address%20this%2C%20we%20propose%20a%20method%20for%20building%20datasets%0Atargeting%20Semantic%20Misalignment%20Failures%20detection%2C%20from%20existing%0Alanguage-conditioned%20manipulation%20datasets.%20We%20also%20present%20I-FailSense%2C%20an%0Aopen-source%20VLM%20framework%20with%20grounded%20arbitration%20designed%20specifically%20for%0Afailure%20detection.%20Our%20approach%20relies%20on%20post-training%20a%20base%20VLM%2C%20followed%20by%0Atraining%20lightweight%20classification%20heads%2C%20called%20FS%20blocks%2C%20attached%20to%0Adifferent%20internal%20layers%20of%20the%20VLM%20and%20whose%20predictions%20are%20aggregated%20using%0Aan%20ensembling%20mechanism.%20Experiments%20show%20that%20I-FailSense%20outperforms%0Astate-of-the-art%20VLMs%2C%20both%20comparable%20in%20size%20and%20larger%2C%20in%20detecting%0Asemantic%20misalignment%20errors.%20Notably%2C%20despite%20being%20trained%20only%20on%20semantic%0Amisalignment%20detection%2C%20I-FailSense%20generalizes%20to%20broader%20robotic%20failure%0Acategories%20and%20effectively%20transfers%20to%20other%20simulation%20environments%20and%0Areal-world%20with%20zero-shot%20or%20minimal%20post-training.%20The%20datasets%20and%20models%20are%0Apublicly%20released%20on%20HuggingFace%20%28Webpage%3A%0Ahttps%3A//clemgris.github.io/I-FailSense/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16072v1&entry.124074799=Read"},
{"title": "Language-Instructed Reasoning for Group Activity Detection via\n  Multimodal Large Language Model", "author": "Jihua Peng and Qianxiong Xu and Yichen Liu and Chenxi Liu and Cheng Long and Rui Zhao and Ziyue Li", "abstract": "  Group activity detection (GAD) aims to simultaneously identify group members\nand categorize their collective activities within video sequences. Existing\ndeep learning-based methods develop specialized architectures (e.g.,\ntransformer networks) to model the dynamics of individual roles and semantic\ndependencies between individuals and groups. However, they rely solely on\nimplicit pattern recognition from visual features and struggle with contextual\nreasoning and explainability. In this work, we propose LIR-GAD, a novel\nframework of language-instructed reasoning for GAD via Multimodal Large\nLanguage Model (MLLM). Our approach expand the original vocabulary of MLLM by\nintroducing an activity-level <ACT> token and multiple cluster-specific <GROUP>\ntokens. We process video frames alongside two specially designed tokens and\nlanguage instructions, which are then integrated into the MLLM. The pretrained\ncommonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>\ntokens to effectively capture the semantic information of collective activities\nand learn distinct representational features of different groups, respectively.\nAlso, we introduce a multi-label classification loss to further enhance the\n<ACT> token's ability to learn discriminative semantic representations. Then,\nwe design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates\nMLLM's hidden embeddings corresponding to the designed tokens with visual\nfeatures, significantly enhancing the performance of GAD. Both quantitative and\nqualitative experiments demonstrate the superior performance of our proposed\nmethod in GAD taks.\n", "link": "http://arxiv.org/abs/2509.16054v1", "date": "2025-09-19", "relevancy": 2.2002, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Instructed%20Reasoning%20for%20Group%20Activity%20Detection%20via%0A%20%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Language-Instructed%20Reasoning%20for%20Group%20Activity%20Detection%20via%0A%20%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Jihua%20Peng%20and%20Qianxiong%20Xu%20and%20Yichen%20Liu%20and%20Chenxi%20Liu%20and%20Cheng%20Long%20and%20Rui%20Zhao%20and%20Ziyue%20Li%0AAbstract%3A%20%20%20Group%20activity%20detection%20%28GAD%29%20aims%20to%20simultaneously%20identify%20group%20members%0Aand%20categorize%20their%20collective%20activities%20within%20video%20sequences.%20Existing%0Adeep%20learning-based%20methods%20develop%20specialized%20architectures%20%28e.g.%2C%0Atransformer%20networks%29%20to%20model%20the%20dynamics%20of%20individual%20roles%20and%20semantic%0Adependencies%20between%20individuals%20and%20groups.%20However%2C%20they%20rely%20solely%20on%0Aimplicit%20pattern%20recognition%20from%20visual%20features%20and%20struggle%20with%20contextual%0Areasoning%20and%20explainability.%20In%20this%20work%2C%20we%20propose%20LIR-GAD%2C%20a%20novel%0Aframework%20of%20language-instructed%20reasoning%20for%20GAD%20via%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29.%20Our%20approach%20expand%20the%20original%20vocabulary%20of%20MLLM%20by%0Aintroducing%20an%20activity-level%20%3CACT%3E%20token%20and%20multiple%20cluster-specific%20%3CGROUP%3E%0Atokens.%20We%20process%20video%20frames%20alongside%20two%20specially%20designed%20tokens%20and%0Alanguage%20instructions%2C%20which%20are%20then%20integrated%20into%20the%20MLLM.%20The%20pretrained%0Acommonsense%20knowledge%20embedded%20in%20the%20MLLM%20enables%20the%20%3CACT%3E%20token%20and%20%3CGROUP%3E%0Atokens%20to%20effectively%20capture%20the%20semantic%20information%20of%20collective%20activities%0Aand%20learn%20distinct%20representational%20features%20of%20different%20groups%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20multi-label%20classification%20loss%20to%20further%20enhance%20the%0A%3CACT%3E%20token%27s%20ability%20to%20learn%20discriminative%20semantic%20representations.%20Then%2C%0Awe%20design%20a%20Multimodal%20Dual-Alignment%20Fusion%20%28MDAF%29%20module%20that%20integrates%0AMLLM%27s%20hidden%20embeddings%20corresponding%20to%20the%20designed%20tokens%20with%20visual%0Afeatures%2C%20significantly%20enhancing%20the%20performance%20of%20GAD.%20Both%20quantitative%20and%0Aqualitative%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%20proposed%0Amethod%20in%20GAD%20taks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Instructed%2520Reasoning%2520for%2520Group%2520Activity%2520Detection%2520via%250A%2520%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DJihua%2520Peng%2520and%2520Qianxiong%2520Xu%2520and%2520Yichen%2520Liu%2520and%2520Chenxi%2520Liu%2520and%2520Cheng%2520Long%2520and%2520Rui%2520Zhao%2520and%2520Ziyue%2520Li%26entry.1292438233%3D%2520%2520Group%2520activity%2520detection%2520%2528GAD%2529%2520aims%2520to%2520simultaneously%2520identify%2520group%2520members%250Aand%2520categorize%2520their%2520collective%2520activities%2520within%2520video%2520sequences.%2520Existing%250Adeep%2520learning-based%2520methods%2520develop%2520specialized%2520architectures%2520%2528e.g.%252C%250Atransformer%2520networks%2529%2520to%2520model%2520the%2520dynamics%2520of%2520individual%2520roles%2520and%2520semantic%250Adependencies%2520between%2520individuals%2520and%2520groups.%2520However%252C%2520they%2520rely%2520solely%2520on%250Aimplicit%2520pattern%2520recognition%2520from%2520visual%2520features%2520and%2520struggle%2520with%2520contextual%250Areasoning%2520and%2520explainability.%2520In%2520this%2520work%252C%2520we%2520propose%2520LIR-GAD%252C%2520a%2520novel%250Aframework%2520of%2520language-instructed%2520reasoning%2520for%2520GAD%2520via%2520Multimodal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529.%2520Our%2520approach%2520expand%2520the%2520original%2520vocabulary%2520of%2520MLLM%2520by%250Aintroducing%2520an%2520activity-level%2520%253CACT%253E%2520token%2520and%2520multiple%2520cluster-specific%2520%253CGROUP%253E%250Atokens.%2520We%2520process%2520video%2520frames%2520alongside%2520two%2520specially%2520designed%2520tokens%2520and%250Alanguage%2520instructions%252C%2520which%2520are%2520then%2520integrated%2520into%2520the%2520MLLM.%2520The%2520pretrained%250Acommonsense%2520knowledge%2520embedded%2520in%2520the%2520MLLM%2520enables%2520the%2520%253CACT%253E%2520token%2520and%2520%253CGROUP%253E%250Atokens%2520to%2520effectively%2520capture%2520the%2520semantic%2520information%2520of%2520collective%2520activities%250Aand%2520learn%2520distinct%2520representational%2520features%2520of%2520different%2520groups%252C%2520respectively.%250AAlso%252C%2520we%2520introduce%2520a%2520multi-label%2520classification%2520loss%2520to%2520further%2520enhance%2520the%250A%253CACT%253E%2520token%2527s%2520ability%2520to%2520learn%2520discriminative%2520semantic%2520representations.%2520Then%252C%250Awe%2520design%2520a%2520Multimodal%2520Dual-Alignment%2520Fusion%2520%2528MDAF%2529%2520module%2520that%2520integrates%250AMLLM%2527s%2520hidden%2520embeddings%2520corresponding%2520to%2520the%2520designed%2520tokens%2520with%2520visual%250Afeatures%252C%2520significantly%2520enhancing%2520the%2520performance%2520of%2520GAD.%2520Both%2520quantitative%2520and%250Aqualitative%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520proposed%250Amethod%2520in%2520GAD%2520taks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Instructed%20Reasoning%20for%20Group%20Activity%20Detection%20via%0A%20%20Multimodal%20Large%20Language%20Model&entry.906535625=Jihua%20Peng%20and%20Qianxiong%20Xu%20and%20Yichen%20Liu%20and%20Chenxi%20Liu%20and%20Cheng%20Long%20and%20Rui%20Zhao%20and%20Ziyue%20Li&entry.1292438233=%20%20Group%20activity%20detection%20%28GAD%29%20aims%20to%20simultaneously%20identify%20group%20members%0Aand%20categorize%20their%20collective%20activities%20within%20video%20sequences.%20Existing%0Adeep%20learning-based%20methods%20develop%20specialized%20architectures%20%28e.g.%2C%0Atransformer%20networks%29%20to%20model%20the%20dynamics%20of%20individual%20roles%20and%20semantic%0Adependencies%20between%20individuals%20and%20groups.%20However%2C%20they%20rely%20solely%20on%0Aimplicit%20pattern%20recognition%20from%20visual%20features%20and%20struggle%20with%20contextual%0Areasoning%20and%20explainability.%20In%20this%20work%2C%20we%20propose%20LIR-GAD%2C%20a%20novel%0Aframework%20of%20language-instructed%20reasoning%20for%20GAD%20via%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29.%20Our%20approach%20expand%20the%20original%20vocabulary%20of%20MLLM%20by%0Aintroducing%20an%20activity-level%20%3CACT%3E%20token%20and%20multiple%20cluster-specific%20%3CGROUP%3E%0Atokens.%20We%20process%20video%20frames%20alongside%20two%20specially%20designed%20tokens%20and%0Alanguage%20instructions%2C%20which%20are%20then%20integrated%20into%20the%20MLLM.%20The%20pretrained%0Acommonsense%20knowledge%20embedded%20in%20the%20MLLM%20enables%20the%20%3CACT%3E%20token%20and%20%3CGROUP%3E%0Atokens%20to%20effectively%20capture%20the%20semantic%20information%20of%20collective%20activities%0Aand%20learn%20distinct%20representational%20features%20of%20different%20groups%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20multi-label%20classification%20loss%20to%20further%20enhance%20the%0A%3CACT%3E%20token%27s%20ability%20to%20learn%20discriminative%20semantic%20representations.%20Then%2C%0Awe%20design%20a%20Multimodal%20Dual-Alignment%20Fusion%20%28MDAF%29%20module%20that%20integrates%0AMLLM%27s%20hidden%20embeddings%20corresponding%20to%20the%20designed%20tokens%20with%20visual%0Afeatures%2C%20significantly%20enhancing%20the%20performance%20of%20GAD.%20Both%20quantitative%20and%0Aqualitative%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%20proposed%0Amethod%20in%20GAD%20taks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16054v1&entry.124074799=Read"},
{"title": "Network-Based Detection of Autism Spectrum Disorder Using Sustainable\n  and Non-invasive Salivary Biomarkers", "author": "Janayna M. Fernandes and Robinson Sabino-Silva and Murillo G. Carneiro", "abstract": "  Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying\nearly diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,\nwe developed GANet, a genetic algorithm-based network optimization framework\nleveraging PageRank and Degree for importance-based feature characterization.\nGANet systematically optimizes network structure to extract meaningful patterns\nfrom high-dimensional spectral data. It achieved superior performance compared\nto linear discriminant analysis, support vector machines, and deep learning\nmodels, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74\nharmonic mean. These results demonstrate GANet's potential as a robust,\nbio-inspired, non-invasive tool for precise ASD detection and broader\nspectral-based health applications.\n", "link": "http://arxiv.org/abs/2509.16126v1", "date": "2025-09-19", "relevancy": 2.1984, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4359}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network-Based%20Detection%20of%20Autism%20Spectrum%20Disorder%20Using%20Sustainable%0A%20%20and%20Non-invasive%20Salivary%20Biomarkers&body=Title%3A%20Network-Based%20Detection%20of%20Autism%20Spectrum%20Disorder%20Using%20Sustainable%0A%20%20and%20Non-invasive%20Salivary%20Biomarkers%0AAuthor%3A%20Janayna%20M.%20Fernandes%20and%20Robinson%20Sabino-Silva%20and%20Murillo%20G.%20Carneiro%0AAbstract%3A%20%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20lacks%20reliable%20biological%20markers%2C%20delaying%0Aearly%20diagnosis.%20Using%20159%20salivary%20samples%20analyzed%20by%20ATR-FTIR%20spectroscopy%2C%0Awe%20developed%20GANet%2C%20a%20genetic%20algorithm-based%20network%20optimization%20framework%0Aleveraging%20PageRank%20and%20Degree%20for%20importance-based%20feature%20characterization.%0AGANet%20systematically%20optimizes%20network%20structure%20to%20extract%20meaningful%20patterns%0Afrom%20high-dimensional%20spectral%20data.%20It%20achieved%20superior%20performance%20compared%0Ato%20linear%20discriminant%20analysis%2C%20support%20vector%20machines%2C%20and%20deep%20learning%0Amodels%2C%20reaching%200.78%20accuracy%2C%200.61%20sensitivity%2C%200.90%20specificity%2C%20and%20a%200.74%0Aharmonic%20mean.%20These%20results%20demonstrate%20GANet%27s%20potential%20as%20a%20robust%2C%0Abio-inspired%2C%20non-invasive%20tool%20for%20precise%20ASD%20detection%20and%20broader%0Aspectral-based%20health%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork-Based%2520Detection%2520of%2520Autism%2520Spectrum%2520Disorder%2520Using%2520Sustainable%250A%2520%2520and%2520Non-invasive%2520Salivary%2520Biomarkers%26entry.906535625%3DJanayna%2520M.%2520Fernandes%2520and%2520Robinson%2520Sabino-Silva%2520and%2520Murillo%2520G.%2520Carneiro%26entry.1292438233%3D%2520%2520Autism%2520Spectrum%2520Disorder%2520%2528ASD%2529%2520lacks%2520reliable%2520biological%2520markers%252C%2520delaying%250Aearly%2520diagnosis.%2520Using%2520159%2520salivary%2520samples%2520analyzed%2520by%2520ATR-FTIR%2520spectroscopy%252C%250Awe%2520developed%2520GANet%252C%2520a%2520genetic%2520algorithm-based%2520network%2520optimization%2520framework%250Aleveraging%2520PageRank%2520and%2520Degree%2520for%2520importance-based%2520feature%2520characterization.%250AGANet%2520systematically%2520optimizes%2520network%2520structure%2520to%2520extract%2520meaningful%2520patterns%250Afrom%2520high-dimensional%2520spectral%2520data.%2520It%2520achieved%2520superior%2520performance%2520compared%250Ato%2520linear%2520discriminant%2520analysis%252C%2520support%2520vector%2520machines%252C%2520and%2520deep%2520learning%250Amodels%252C%2520reaching%25200.78%2520accuracy%252C%25200.61%2520sensitivity%252C%25200.90%2520specificity%252C%2520and%2520a%25200.74%250Aharmonic%2520mean.%2520These%2520results%2520demonstrate%2520GANet%2527s%2520potential%2520as%2520a%2520robust%252C%250Abio-inspired%252C%2520non-invasive%2520tool%2520for%2520precise%2520ASD%2520detection%2520and%2520broader%250Aspectral-based%2520health%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network-Based%20Detection%20of%20Autism%20Spectrum%20Disorder%20Using%20Sustainable%0A%20%20and%20Non-invasive%20Salivary%20Biomarkers&entry.906535625=Janayna%20M.%20Fernandes%20and%20Robinson%20Sabino-Silva%20and%20Murillo%20G.%20Carneiro&entry.1292438233=%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20lacks%20reliable%20biological%20markers%2C%20delaying%0Aearly%20diagnosis.%20Using%20159%20salivary%20samples%20analyzed%20by%20ATR-FTIR%20spectroscopy%2C%0Awe%20developed%20GANet%2C%20a%20genetic%20algorithm-based%20network%20optimization%20framework%0Aleveraging%20PageRank%20and%20Degree%20for%20importance-based%20feature%20characterization.%0AGANet%20systematically%20optimizes%20network%20structure%20to%20extract%20meaningful%20patterns%0Afrom%20high-dimensional%20spectral%20data.%20It%20achieved%20superior%20performance%20compared%0Ato%20linear%20discriminant%20analysis%2C%20support%20vector%20machines%2C%20and%20deep%20learning%0Amodels%2C%20reaching%200.78%20accuracy%2C%200.61%20sensitivity%2C%200.90%20specificity%2C%20and%20a%200.74%0Aharmonic%20mean.%20These%20results%20demonstrate%20GANet%27s%20potential%20as%20a%20robust%2C%0Abio-inspired%2C%20non-invasive%20tool%20for%20precise%20ASD%20detection%20and%20broader%0Aspectral-based%20health%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16126v1&entry.124074799=Read"},
{"title": "PAN: Pillars-Attention-Based Network for 3D Object Detection", "author": "Ruan Bispo and Dane Mitrev and Letizia Mariotti and Cl\u00e9ment Botty and Denver Humphrey and Anthony Scanlan and Ciar\u00e1n Eising", "abstract": "  Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category.\n", "link": "http://arxiv.org/abs/2509.15935v1", "date": "2025-09-19", "relevancy": 2.1777, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5496}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5437}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAN%3A%20Pillars-Attention-Based%20Network%20for%203D%20Object%20Detection&body=Title%3A%20PAN%3A%20Pillars-Attention-Based%20Network%20for%203D%20Object%20Detection%0AAuthor%3A%20Ruan%20Bispo%20and%20Dane%20Mitrev%20and%20Letizia%20Mariotti%20and%20Cl%C3%A9ment%20Botty%20and%20Denver%20Humphrey%20and%20Anthony%20Scanlan%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Camera-radar%20fusion%20offers%20a%20robust%20and%20low-cost%20alternative%20to%20Camera-lidar%0Afusion%20for%20the%203D%20object%20detection%20task%20in%20real-time%20under%20adverse%20weather%20and%0Alighting%20conditions.%20However%2C%20currently%2C%20in%20the%20literature%2C%20it%20is%20possible%20to%0Afind%20few%20works%20focusing%20on%20this%20modality%20and%2C%20most%20importantly%2C%20developing%20new%0Aarchitectures%20to%20explore%20the%20advantages%20of%20the%20radar%20point%20cloud%2C%20such%20as%0Aaccurate%20distance%20estimation%20and%20speed%20information.%20Therefore%2C%20this%20work%0Apresents%20a%20novel%20and%20efficient%203D%20object%20detection%20algorithm%20using%20cameras%20and%0Aradars%20in%20the%20bird%27s-eye-view%20%28BEV%29.%20Our%20algorithm%20exploits%20the%20advantages%20of%0Aradar%20before%20fusing%20the%20features%20into%20a%20detection%20head.%20A%20new%20backbone%20is%0Aintroduced%2C%20which%20maps%20the%20radar%20pillar%20features%20into%20an%20embedded%20dimension.%20A%0Aself-attention%20mechanism%20allows%20the%20backbone%20to%20model%20the%20dependencies%20between%0Athe%20radar%20points.%20We%20are%20using%20a%20simplified%20convolutional%20layer%20to%20replace%20the%0AFPN-based%20convolutional%20layers%20used%20in%20the%20PointPillars-based%20architectures%0Awith%20the%20main%20goal%20of%20reducing%20inference%20time.%20Our%20results%20show%20that%20with%20this%0Amodification%2C%20our%20approach%20achieves%20the%20new%20state-of-the-art%20in%20the%203D%20object%0Adetection%20problem%2C%20reaching%2058.2%20of%20the%20NDS%20metric%20for%20the%20use%20of%20ResNet-50%2C%0Awhile%20also%20setting%20a%20new%20benchmark%20for%20inference%20time%20on%20the%20nuScenes%20dataset%0Afor%20the%20same%20category.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAN%253A%2520Pillars-Attention-Based%2520Network%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DRuan%2520Bispo%2520and%2520Dane%2520Mitrev%2520and%2520Letizia%2520Mariotti%2520and%2520Cl%25C3%25A9ment%2520Botty%2520and%2520Denver%2520Humphrey%2520and%2520Anthony%2520Scanlan%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Camera-radar%2520fusion%2520offers%2520a%2520robust%2520and%2520low-cost%2520alternative%2520to%2520Camera-lidar%250Afusion%2520for%2520the%25203D%2520object%2520detection%2520task%2520in%2520real-time%2520under%2520adverse%2520weather%2520and%250Alighting%2520conditions.%2520However%252C%2520currently%252C%2520in%2520the%2520literature%252C%2520it%2520is%2520possible%2520to%250Afind%2520few%2520works%2520focusing%2520on%2520this%2520modality%2520and%252C%2520most%2520importantly%252C%2520developing%2520new%250Aarchitectures%2520to%2520explore%2520the%2520advantages%2520of%2520the%2520radar%2520point%2520cloud%252C%2520such%2520as%250Aaccurate%2520distance%2520estimation%2520and%2520speed%2520information.%2520Therefore%252C%2520this%2520work%250Apresents%2520a%2520novel%2520and%2520efficient%25203D%2520object%2520detection%2520algorithm%2520using%2520cameras%2520and%250Aradars%2520in%2520the%2520bird%2527s-eye-view%2520%2528BEV%2529.%2520Our%2520algorithm%2520exploits%2520the%2520advantages%2520of%250Aradar%2520before%2520fusing%2520the%2520features%2520into%2520a%2520detection%2520head.%2520A%2520new%2520backbone%2520is%250Aintroduced%252C%2520which%2520maps%2520the%2520radar%2520pillar%2520features%2520into%2520an%2520embedded%2520dimension.%2520A%250Aself-attention%2520mechanism%2520allows%2520the%2520backbone%2520to%2520model%2520the%2520dependencies%2520between%250Athe%2520radar%2520points.%2520We%2520are%2520using%2520a%2520simplified%2520convolutional%2520layer%2520to%2520replace%2520the%250AFPN-based%2520convolutional%2520layers%2520used%2520in%2520the%2520PointPillars-based%2520architectures%250Awith%2520the%2520main%2520goal%2520of%2520reducing%2520inference%2520time.%2520Our%2520results%2520show%2520that%2520with%2520this%250Amodification%252C%2520our%2520approach%2520achieves%2520the%2520new%2520state-of-the-art%2520in%2520the%25203D%2520object%250Adetection%2520problem%252C%2520reaching%252058.2%2520of%2520the%2520NDS%2520metric%2520for%2520the%2520use%2520of%2520ResNet-50%252C%250Awhile%2520also%2520setting%2520a%2520new%2520benchmark%2520for%2520inference%2520time%2520on%2520the%2520nuScenes%2520dataset%250Afor%2520the%2520same%2520category.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAN%3A%20Pillars-Attention-Based%20Network%20for%203D%20Object%20Detection&entry.906535625=Ruan%20Bispo%20and%20Dane%20Mitrev%20and%20Letizia%20Mariotti%20and%20Cl%C3%A9ment%20Botty%20and%20Denver%20Humphrey%20and%20Anthony%20Scanlan%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Camera-radar%20fusion%20offers%20a%20robust%20and%20low-cost%20alternative%20to%20Camera-lidar%0Afusion%20for%20the%203D%20object%20detection%20task%20in%20real-time%20under%20adverse%20weather%20and%0Alighting%20conditions.%20However%2C%20currently%2C%20in%20the%20literature%2C%20it%20is%20possible%20to%0Afind%20few%20works%20focusing%20on%20this%20modality%20and%2C%20most%20importantly%2C%20developing%20new%0Aarchitectures%20to%20explore%20the%20advantages%20of%20the%20radar%20point%20cloud%2C%20such%20as%0Aaccurate%20distance%20estimation%20and%20speed%20information.%20Therefore%2C%20this%20work%0Apresents%20a%20novel%20and%20efficient%203D%20object%20detection%20algorithm%20using%20cameras%20and%0Aradars%20in%20the%20bird%27s-eye-view%20%28BEV%29.%20Our%20algorithm%20exploits%20the%20advantages%20of%0Aradar%20before%20fusing%20the%20features%20into%20a%20detection%20head.%20A%20new%20backbone%20is%0Aintroduced%2C%20which%20maps%20the%20radar%20pillar%20features%20into%20an%20embedded%20dimension.%20A%0Aself-attention%20mechanism%20allows%20the%20backbone%20to%20model%20the%20dependencies%20between%0Athe%20radar%20points.%20We%20are%20using%20a%20simplified%20convolutional%20layer%20to%20replace%20the%0AFPN-based%20convolutional%20layers%20used%20in%20the%20PointPillars-based%20architectures%0Awith%20the%20main%20goal%20of%20reducing%20inference%20time.%20Our%20results%20show%20that%20with%20this%0Amodification%2C%20our%20approach%20achieves%20the%20new%20state-of-the-art%20in%20the%203D%20object%0Adetection%20problem%2C%20reaching%2058.2%20of%20the%20NDS%20metric%20for%20the%20use%20of%20ResNet-50%2C%0Awhile%20also%20setting%20a%20new%20benchmark%20for%20inference%20time%20on%20the%20nuScenes%20dataset%0Afor%20the%20same%20category.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15935v1&entry.124074799=Read"},
{"title": "Session-Level Spoken Language Assessment with a Multimodal Foundation\n  Model via Multi-Target Learning", "author": "Hong-Yun Lin and Jhen-Ke Lin and Chung-Chun Wang and Hao-Chien Lu and Berlin Chen", "abstract": "  Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.\n", "link": "http://arxiv.org/abs/2509.16025v1", "date": "2025-09-19", "relevancy": 2.1682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Session-Level%20Spoken%20Language%20Assessment%20with%20a%20Multimodal%20Foundation%0A%20%20Model%20via%20Multi-Target%20Learning&body=Title%3A%20Session-Level%20Spoken%20Language%20Assessment%20with%20a%20Multimodal%20Foundation%0A%20%20Model%20via%20Multi-Target%20Learning%0AAuthor%3A%20Hong-Yun%20Lin%20and%20Jhen-Ke%20Lin%20and%20Chung-Chun%20Wang%20and%20Hao-Chien%20Lu%20and%20Berlin%20Chen%0AAbstract%3A%20%20%20Spoken%20Language%20Assessment%20%28SLA%29%20estimates%20a%20learner%27s%20oral%20proficiency%20from%0Aspontaneous%20speech.%20The%20growing%20population%20of%20L2%20English%20speakers%20has%0Aintensified%20the%20demand%20for%20reliable%20SLA%2C%20a%20critical%20component%20of%20Computer%0AAssisted%20Language%20Learning%20%28CALL%29.%20Existing%20efforts%20often%20rely%20on%20cascaded%0Apipelines%2C%20which%20are%20prone%20to%20error%20propagation%2C%20or%20end-to-end%20models%20that%0Aoften%20operate%20on%20a%20short%20audio%20window%2C%20which%20might%20miss%20discourse-level%0Aevidence.%20This%20paper%20introduces%20a%20novel%20multimodal%20foundation%20model%20approach%0Athat%20performs%20session-level%20evaluation%20in%20a%20single%20pass.%20Our%20approach%20couples%0Amulti-target%20learning%20with%20a%20frozen%2C%20Whisper%20ASR%20model-based%20speech%20prior%20for%0Aacoustic-aware%20calibration%2C%20allowing%20for%20jointly%20learning%20holistic%20and%0Atrait-level%20objectives%20of%20SLA%20without%20resorting%20to%20handcrafted%20features.%20By%0Acoherently%20processing%20the%20entire%20response%20session%20of%20an%20L2%20speaker%2C%20the%20model%0Aexcels%20at%20predicting%20holistic%20oral%20proficiency.%20Experiments%20conducted%20on%20the%0ASpeak%20%26%20Improve%20benchmark%20demonstrate%20that%20our%20proposed%20approach%20outperforms%0Athe%20previous%20state-of-the-art%20cascaded%20system%20and%20exhibits%20robust%20cross-part%0Ageneralization%2C%20producing%20a%20compact%20deployable%20grader%20that%20is%20tailored%20for%20CALL%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSession-Level%2520Spoken%2520Language%2520Assessment%2520with%2520a%2520Multimodal%2520Foundation%250A%2520%2520Model%2520via%2520Multi-Target%2520Learning%26entry.906535625%3DHong-Yun%2520Lin%2520and%2520Jhen-Ke%2520Lin%2520and%2520Chung-Chun%2520Wang%2520and%2520Hao-Chien%2520Lu%2520and%2520Berlin%2520Chen%26entry.1292438233%3D%2520%2520Spoken%2520Language%2520Assessment%2520%2528SLA%2529%2520estimates%2520a%2520learner%2527s%2520oral%2520proficiency%2520from%250Aspontaneous%2520speech.%2520The%2520growing%2520population%2520of%2520L2%2520English%2520speakers%2520has%250Aintensified%2520the%2520demand%2520for%2520reliable%2520SLA%252C%2520a%2520critical%2520component%2520of%2520Computer%250AAssisted%2520Language%2520Learning%2520%2528CALL%2529.%2520Existing%2520efforts%2520often%2520rely%2520on%2520cascaded%250Apipelines%252C%2520which%2520are%2520prone%2520to%2520error%2520propagation%252C%2520or%2520end-to-end%2520models%2520that%250Aoften%2520operate%2520on%2520a%2520short%2520audio%2520window%252C%2520which%2520might%2520miss%2520discourse-level%250Aevidence.%2520This%2520paper%2520introduces%2520a%2520novel%2520multimodal%2520foundation%2520model%2520approach%250Athat%2520performs%2520session-level%2520evaluation%2520in%2520a%2520single%2520pass.%2520Our%2520approach%2520couples%250Amulti-target%2520learning%2520with%2520a%2520frozen%252C%2520Whisper%2520ASR%2520model-based%2520speech%2520prior%2520for%250Aacoustic-aware%2520calibration%252C%2520allowing%2520for%2520jointly%2520learning%2520holistic%2520and%250Atrait-level%2520objectives%2520of%2520SLA%2520without%2520resorting%2520to%2520handcrafted%2520features.%2520By%250Acoherently%2520processing%2520the%2520entire%2520response%2520session%2520of%2520an%2520L2%2520speaker%252C%2520the%2520model%250Aexcels%2520at%2520predicting%2520holistic%2520oral%2520proficiency.%2520Experiments%2520conducted%2520on%2520the%250ASpeak%2520%2526%2520Improve%2520benchmark%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520outperforms%250Athe%2520previous%2520state-of-the-art%2520cascaded%2520system%2520and%2520exhibits%2520robust%2520cross-part%250Ageneralization%252C%2520producing%2520a%2520compact%2520deployable%2520grader%2520that%2520is%2520tailored%2520for%2520CALL%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Session-Level%20Spoken%20Language%20Assessment%20with%20a%20Multimodal%20Foundation%0A%20%20Model%20via%20Multi-Target%20Learning&entry.906535625=Hong-Yun%20Lin%20and%20Jhen-Ke%20Lin%20and%20Chung-Chun%20Wang%20and%20Hao-Chien%20Lu%20and%20Berlin%20Chen&entry.1292438233=%20%20Spoken%20Language%20Assessment%20%28SLA%29%20estimates%20a%20learner%27s%20oral%20proficiency%20from%0Aspontaneous%20speech.%20The%20growing%20population%20of%20L2%20English%20speakers%20has%0Aintensified%20the%20demand%20for%20reliable%20SLA%2C%20a%20critical%20component%20of%20Computer%0AAssisted%20Language%20Learning%20%28CALL%29.%20Existing%20efforts%20often%20rely%20on%20cascaded%0Apipelines%2C%20which%20are%20prone%20to%20error%20propagation%2C%20or%20end-to-end%20models%20that%0Aoften%20operate%20on%20a%20short%20audio%20window%2C%20which%20might%20miss%20discourse-level%0Aevidence.%20This%20paper%20introduces%20a%20novel%20multimodal%20foundation%20model%20approach%0Athat%20performs%20session-level%20evaluation%20in%20a%20single%20pass.%20Our%20approach%20couples%0Amulti-target%20learning%20with%20a%20frozen%2C%20Whisper%20ASR%20model-based%20speech%20prior%20for%0Aacoustic-aware%20calibration%2C%20allowing%20for%20jointly%20learning%20holistic%20and%0Atrait-level%20objectives%20of%20SLA%20without%20resorting%20to%20handcrafted%20features.%20By%0Acoherently%20processing%20the%20entire%20response%20session%20of%20an%20L2%20speaker%2C%20the%20model%0Aexcels%20at%20predicting%20holistic%20oral%20proficiency.%20Experiments%20conducted%20on%20the%0ASpeak%20%26%20Improve%20benchmark%20demonstrate%20that%20our%20proposed%20approach%20outperforms%0Athe%20previous%20state-of-the-art%20cascaded%20system%20and%20exhibits%20robust%20cross-part%0Ageneralization%2C%20producing%20a%20compact%20deployable%20grader%20that%20is%20tailored%20for%20CALL%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16025v1&entry.124074799=Read"},
{"title": "Structured Information for Improving Spatial Relationships in\n  Text-to-Image Generation", "author": "Sander Schildermans and Chang Tian and Ying Jiao and Marie-Francine Moens", "abstract": "  Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing\nspatial relationships described in natural language prompts remains a major\nchallenge. Prior efforts have addressed this issue through prompt optimization,\nspatially grounded generation, and semantic refinement. This work introduces a\nlightweight approach that augments prompts with tuple-based structured\ninformation, using a fine-tuned language model for automatic conversion and\nseamless integration into T2I pipelines. Experimental results demonstrate\nsubstantial improvements in spatial accuracy, without compromising overall\nimage quality as measured by Inception Score. Furthermore, the automatically\ngenerated tuples exhibit quality comparable to human-crafted tuples. This\nstructured information provides a practical and portable solution to enhance\nspatial relationships in T2I generation, addressing a key limitation of current\nlarge-scale generative systems.\n", "link": "http://arxiv.org/abs/2509.15962v1", "date": "2025-09-19", "relevancy": 2.1561, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.54}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Information%20for%20Improving%20Spatial%20Relationships%20in%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Structured%20Information%20for%20Improving%20Spatial%20Relationships%20in%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Sander%20Schildermans%20and%20Chang%20Tian%20and%20Ying%20Jiao%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20has%20advanced%20rapidly%2C%20yet%20faithfully%20capturing%0Aspatial%20relationships%20described%20in%20natural%20language%20prompts%20remains%20a%20major%0Achallenge.%20Prior%20efforts%20have%20addressed%20this%20issue%20through%20prompt%20optimization%2C%0Aspatially%20grounded%20generation%2C%20and%20semantic%20refinement.%20This%20work%20introduces%20a%0Alightweight%20approach%20that%20augments%20prompts%20with%20tuple-based%20structured%0Ainformation%2C%20using%20a%20fine-tuned%20language%20model%20for%20automatic%20conversion%20and%0Aseamless%20integration%20into%20T2I%20pipelines.%20Experimental%20results%20demonstrate%0Asubstantial%20improvements%20in%20spatial%20accuracy%2C%20without%20compromising%20overall%0Aimage%20quality%20as%20measured%20by%20Inception%20Score.%20Furthermore%2C%20the%20automatically%0Agenerated%20tuples%20exhibit%20quality%20comparable%20to%20human-crafted%20tuples.%20This%0Astructured%20information%20provides%20a%20practical%20and%20portable%20solution%20to%20enhance%0Aspatial%20relationships%20in%20T2I%20generation%2C%20addressing%20a%20key%20limitation%20of%20current%0Alarge-scale%20generative%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Information%2520for%2520Improving%2520Spatial%2520Relationships%2520in%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DSander%2520Schildermans%2520and%2520Chang%2520Tian%2520and%2520Ying%2520Jiao%2520and%2520Marie-Francine%2520Moens%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520has%2520advanced%2520rapidly%252C%2520yet%2520faithfully%2520capturing%250Aspatial%2520relationships%2520described%2520in%2520natural%2520language%2520prompts%2520remains%2520a%2520major%250Achallenge.%2520Prior%2520efforts%2520have%2520addressed%2520this%2520issue%2520through%2520prompt%2520optimization%252C%250Aspatially%2520grounded%2520generation%252C%2520and%2520semantic%2520refinement.%2520This%2520work%2520introduces%2520a%250Alightweight%2520approach%2520that%2520augments%2520prompts%2520with%2520tuple-based%2520structured%250Ainformation%252C%2520using%2520a%2520fine-tuned%2520language%2520model%2520for%2520automatic%2520conversion%2520and%250Aseamless%2520integration%2520into%2520T2I%2520pipelines.%2520Experimental%2520results%2520demonstrate%250Asubstantial%2520improvements%2520in%2520spatial%2520accuracy%252C%2520without%2520compromising%2520overall%250Aimage%2520quality%2520as%2520measured%2520by%2520Inception%2520Score.%2520Furthermore%252C%2520the%2520automatically%250Agenerated%2520tuples%2520exhibit%2520quality%2520comparable%2520to%2520human-crafted%2520tuples.%2520This%250Astructured%2520information%2520provides%2520a%2520practical%2520and%2520portable%2520solution%2520to%2520enhance%250Aspatial%2520relationships%2520in%2520T2I%2520generation%252C%2520addressing%2520a%2520key%2520limitation%2520of%2520current%250Alarge-scale%2520generative%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Information%20for%20Improving%20Spatial%20Relationships%20in%0A%20%20Text-to-Image%20Generation&entry.906535625=Sander%20Schildermans%20and%20Chang%20Tian%20and%20Ying%20Jiao%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20has%20advanced%20rapidly%2C%20yet%20faithfully%20capturing%0Aspatial%20relationships%20described%20in%20natural%20language%20prompts%20remains%20a%20major%0Achallenge.%20Prior%20efforts%20have%20addressed%20this%20issue%20through%20prompt%20optimization%2C%0Aspatially%20grounded%20generation%2C%20and%20semantic%20refinement.%20This%20work%20introduces%20a%0Alightweight%20approach%20that%20augments%20prompts%20with%20tuple-based%20structured%0Ainformation%2C%20using%20a%20fine-tuned%20language%20model%20for%20automatic%20conversion%20and%0Aseamless%20integration%20into%20T2I%20pipelines.%20Experimental%20results%20demonstrate%0Asubstantial%20improvements%20in%20spatial%20accuracy%2C%20without%20compromising%20overall%0Aimage%20quality%20as%20measured%20by%20Inception%20Score.%20Furthermore%2C%20the%20automatically%0Agenerated%20tuples%20exhibit%20quality%20comparable%20to%20human-crafted%20tuples.%20This%0Astructured%20information%20provides%20a%20practical%20and%20portable%20solution%20to%20enhance%0Aspatial%20relationships%20in%20T2I%20generation%2C%20addressing%20a%20key%20limitation%20of%20current%0Alarge-scale%20generative%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15962v1&entry.124074799=Read"},
{"title": "DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware\n  Batch Size Adaptation", "author": "Yuen Chen and Yian Wang and Hari Sundaram", "abstract": "  The goal of this paper is to accelerate the training of machine learning\nmodels, a critical challenge since the training of large-scale deep neural\nmodels can be computationally expensive. Stochastic gradient descent (SGD) and\nits variants are widely used to train deep neural networks. In contrast to\ntraditional approaches that focus on tuning the learning rate, we propose a\nnovel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts\nthe batch size. Adapting the batch size is challenging: using large batch sizes\nis more efficient due to parallel computation, but small-batch training often\nconverges in fewer epochs and generalizes better. To address this challenge, we\nintroduce a data-driven adaptation based on gradient diversity, enabling\nDiveBatch to maintain the generalization performance of small-batch training\nwhile improving convergence speed and computational efficiency. Gradient\ndiversity has a strong theoretical justification: it emerges from the\nconvergence analysis of SGD. Evaluations of DiveBatch on synthetic and\nCiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges\nsignificantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a\nslight trade-off in performance.\n", "link": "http://arxiv.org/abs/2509.16173v1", "date": "2025-09-19", "relevancy": 2.1554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5482}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5421}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVEBATCH%3A%20Accelerating%20Model%20Training%20Through%20Gradient-Diversity%20Aware%0A%20%20Batch%20Size%20Adaptation&body=Title%3A%20DIVEBATCH%3A%20Accelerating%20Model%20Training%20Through%20Gradient-Diversity%20Aware%0A%20%20Batch%20Size%20Adaptation%0AAuthor%3A%20Yuen%20Chen%20and%20Yian%20Wang%20and%20Hari%20Sundaram%0AAbstract%3A%20%20%20The%20goal%20of%20this%20paper%20is%20to%20accelerate%20the%20training%20of%20machine%20learning%0Amodels%2C%20a%20critical%20challenge%20since%20the%20training%20of%20large-scale%20deep%20neural%0Amodels%20can%20be%20computationally%20expensive.%20Stochastic%20gradient%20descent%20%28SGD%29%20and%0Aits%20variants%20are%20widely%20used%20to%20train%20deep%20neural%20networks.%20In%20contrast%20to%0Atraditional%20approaches%20that%20focus%20on%20tuning%20the%20learning%20rate%2C%20we%20propose%20a%0Anovel%20adaptive%20batch%20size%20SGD%20algorithm%2C%20DiveBatch%2C%20that%20dynamically%20adjusts%0Athe%20batch%20size.%20Adapting%20the%20batch%20size%20is%20challenging%3A%20using%20large%20batch%20sizes%0Ais%20more%20efficient%20due%20to%20parallel%20computation%2C%20but%20small-batch%20training%20often%0Aconverges%20in%20fewer%20epochs%20and%20generalizes%20better.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20a%20data-driven%20adaptation%20based%20on%20gradient%20diversity%2C%20enabling%0ADiveBatch%20to%20maintain%20the%20generalization%20performance%20of%20small-batch%20training%0Awhile%20improving%20convergence%20speed%20and%20computational%20efficiency.%20Gradient%0Adiversity%20has%20a%20strong%20theoretical%20justification%3A%20it%20emerges%20from%20the%0Aconvergence%20analysis%20of%20SGD.%20Evaluations%20of%20DiveBatch%20on%20synthetic%20and%0ACiFar-10%2C%20CiFar-100%2C%20and%20Tiny-ImageNet%20demonstrate%20that%20DiveBatch%20converges%0Asignificantly%20faster%20than%20standard%20SGD%20and%20AdaBatch%20%281.06%20--%205.0x%29%2C%20with%20a%0Aslight%20trade-off%20in%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVEBATCH%253A%2520Accelerating%2520Model%2520Training%2520Through%2520Gradient-Diversity%2520Aware%250A%2520%2520Batch%2520Size%2520Adaptation%26entry.906535625%3DYuen%2520Chen%2520and%2520Yian%2520Wang%2520and%2520Hari%2520Sundaram%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520accelerate%2520the%2520training%2520of%2520machine%2520learning%250Amodels%252C%2520a%2520critical%2520challenge%2520since%2520the%2520training%2520of%2520large-scale%2520deep%2520neural%250Amodels%2520can%2520be%2520computationally%2520expensive.%2520Stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520and%250Aits%2520variants%2520are%2520widely%2520used%2520to%2520train%2520deep%2520neural%2520networks.%2520In%2520contrast%2520to%250Atraditional%2520approaches%2520that%2520focus%2520on%2520tuning%2520the%2520learning%2520rate%252C%2520we%2520propose%2520a%250Anovel%2520adaptive%2520batch%2520size%2520SGD%2520algorithm%252C%2520DiveBatch%252C%2520that%2520dynamically%2520adjusts%250Athe%2520batch%2520size.%2520Adapting%2520the%2520batch%2520size%2520is%2520challenging%253A%2520using%2520large%2520batch%2520sizes%250Ais%2520more%2520efficient%2520due%2520to%2520parallel%2520computation%252C%2520but%2520small-batch%2520training%2520often%250Aconverges%2520in%2520fewer%2520epochs%2520and%2520generalizes%2520better.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520a%2520data-driven%2520adaptation%2520based%2520on%2520gradient%2520diversity%252C%2520enabling%250ADiveBatch%2520to%2520maintain%2520the%2520generalization%2520performance%2520of%2520small-batch%2520training%250Awhile%2520improving%2520convergence%2520speed%2520and%2520computational%2520efficiency.%2520Gradient%250Adiversity%2520has%2520a%2520strong%2520theoretical%2520justification%253A%2520it%2520emerges%2520from%2520the%250Aconvergence%2520analysis%2520of%2520SGD.%2520Evaluations%2520of%2520DiveBatch%2520on%2520synthetic%2520and%250ACiFar-10%252C%2520CiFar-100%252C%2520and%2520Tiny-ImageNet%2520demonstrate%2520that%2520DiveBatch%2520converges%250Asignificantly%2520faster%2520than%2520standard%2520SGD%2520and%2520AdaBatch%2520%25281.06%2520--%25205.0x%2529%252C%2520with%2520a%250Aslight%2520trade-off%2520in%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVEBATCH%3A%20Accelerating%20Model%20Training%20Through%20Gradient-Diversity%20Aware%0A%20%20Batch%20Size%20Adaptation&entry.906535625=Yuen%20Chen%20and%20Yian%20Wang%20and%20Hari%20Sundaram&entry.1292438233=%20%20The%20goal%20of%20this%20paper%20is%20to%20accelerate%20the%20training%20of%20machine%20learning%0Amodels%2C%20a%20critical%20challenge%20since%20the%20training%20of%20large-scale%20deep%20neural%0Amodels%20can%20be%20computationally%20expensive.%20Stochastic%20gradient%20descent%20%28SGD%29%20and%0Aits%20variants%20are%20widely%20used%20to%20train%20deep%20neural%20networks.%20In%20contrast%20to%0Atraditional%20approaches%20that%20focus%20on%20tuning%20the%20learning%20rate%2C%20we%20propose%20a%0Anovel%20adaptive%20batch%20size%20SGD%20algorithm%2C%20DiveBatch%2C%20that%20dynamically%20adjusts%0Athe%20batch%20size.%20Adapting%20the%20batch%20size%20is%20challenging%3A%20using%20large%20batch%20sizes%0Ais%20more%20efficient%20due%20to%20parallel%20computation%2C%20but%20small-batch%20training%20often%0Aconverges%20in%20fewer%20epochs%20and%20generalizes%20better.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20a%20data-driven%20adaptation%20based%20on%20gradient%20diversity%2C%20enabling%0ADiveBatch%20to%20maintain%20the%20generalization%20performance%20of%20small-batch%20training%0Awhile%20improving%20convergence%20speed%20and%20computational%20efficiency.%20Gradient%0Adiversity%20has%20a%20strong%20theoretical%20justification%3A%20it%20emerges%20from%20the%0Aconvergence%20analysis%20of%20SGD.%20Evaluations%20of%20DiveBatch%20on%20synthetic%20and%0ACiFar-10%2C%20CiFar-100%2C%20and%20Tiny-ImageNet%20demonstrate%20that%20DiveBatch%20converges%0Asignificantly%20faster%20than%20standard%20SGD%20and%20AdaBatch%20%281.06%20--%205.0x%29%2C%20with%20a%0Aslight%20trade-off%20in%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16173v1&entry.124074799=Read"},
{"title": "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent\n  Trajectory Modeling in Sports", "author": "Yi Xu and Yun Fu", "abstract": "  Trajectory prediction in multi-agent sports scenarios is inherently\nchallenging due to the structural heterogeneity across agent roles (e.g.,\nplayers vs. ball) and dynamic distribution gaps across different sports\ndomains. Existing unified frameworks often fail to capture these structured\ndistributional shifts, resulting in suboptimal generalization across roles and\ndomains. We propose AdaSports-Traj, an adaptive trajectory modeling framework\nthat explicitly addresses both intra-domain and inter-domain distribution\ndiscrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and\nDomain-Aware Adapter to conditionally adjust latent representations based on\nagent identity and domain context. Additionally, we introduce a Hierarchical\nContrastive Learning objective, which separately supervises role-sensitive and\ndomain-aware representations to encourage disentangled latent structures\nwithout introducing optimization conflict. Experiments on three diverse sports\ndatasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness\nof our adaptive design, achieving strong performance in both unified and\ncross-domain trajectory prediction settings.\n", "link": "http://arxiv.org/abs/2509.16095v1", "date": "2025-09-19", "relevancy": 2.1457, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaSports-Traj%3A%20Role-%20and%20Domain-Aware%20Adaptation%20for%20Multi-Agent%0A%20%20Trajectory%20Modeling%20in%20Sports&body=Title%3A%20AdaSports-Traj%3A%20Role-%20and%20Domain-Aware%20Adaptation%20for%20Multi-Agent%0A%20%20Trajectory%20Modeling%20in%20Sports%0AAuthor%3A%20Yi%20Xu%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Trajectory%20prediction%20in%20multi-agent%20sports%20scenarios%20is%20inherently%0Achallenging%20due%20to%20the%20structural%20heterogeneity%20across%20agent%20roles%20%28e.g.%2C%0Aplayers%20vs.%20ball%29%20and%20dynamic%20distribution%20gaps%20across%20different%20sports%0Adomains.%20Existing%20unified%20frameworks%20often%20fail%20to%20capture%20these%20structured%0Adistributional%20shifts%2C%20resulting%20in%20suboptimal%20generalization%20across%20roles%20and%0Adomains.%20We%20propose%20AdaSports-Traj%2C%20an%20adaptive%20trajectory%20modeling%20framework%0Athat%20explicitly%20addresses%20both%20intra-domain%20and%20inter-domain%20distribution%0Adiscrepancies%20in%20sports.%20At%20its%20core%2C%20AdaSports-Traj%20incorporates%20a%20Role-%20and%0ADomain-Aware%20Adapter%20to%20conditionally%20adjust%20latent%20representations%20based%20on%0Aagent%20identity%20and%20domain%20context.%20Additionally%2C%20we%20introduce%20a%20Hierarchical%0AContrastive%20Learning%20objective%2C%20which%20separately%20supervises%20role-sensitive%20and%0Adomain-aware%20representations%20to%20encourage%20disentangled%20latent%20structures%0Awithout%20introducing%20optimization%20conflict.%20Experiments%20on%20three%20diverse%20sports%0Adatasets%2C%20Basketball-U%2C%20Football-U%2C%20and%20Soccer-U%2C%20demonstrate%20the%20effectiveness%0Aof%20our%20adaptive%20design%2C%20achieving%20strong%20performance%20in%20both%20unified%20and%0Across-domain%20trajectory%20prediction%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaSports-Traj%253A%2520Role-%2520and%2520Domain-Aware%2520Adaptation%2520for%2520Multi-Agent%250A%2520%2520Trajectory%2520Modeling%2520in%2520Sports%26entry.906535625%3DYi%2520Xu%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520in%2520multi-agent%2520sports%2520scenarios%2520is%2520inherently%250Achallenging%2520due%2520to%2520the%2520structural%2520heterogeneity%2520across%2520agent%2520roles%2520%2528e.g.%252C%250Aplayers%2520vs.%2520ball%2529%2520and%2520dynamic%2520distribution%2520gaps%2520across%2520different%2520sports%250Adomains.%2520Existing%2520unified%2520frameworks%2520often%2520fail%2520to%2520capture%2520these%2520structured%250Adistributional%2520shifts%252C%2520resulting%2520in%2520suboptimal%2520generalization%2520across%2520roles%2520and%250Adomains.%2520We%2520propose%2520AdaSports-Traj%252C%2520an%2520adaptive%2520trajectory%2520modeling%2520framework%250Athat%2520explicitly%2520addresses%2520both%2520intra-domain%2520and%2520inter-domain%2520distribution%250Adiscrepancies%2520in%2520sports.%2520At%2520its%2520core%252C%2520AdaSports-Traj%2520incorporates%2520a%2520Role-%2520and%250ADomain-Aware%2520Adapter%2520to%2520conditionally%2520adjust%2520latent%2520representations%2520based%2520on%250Aagent%2520identity%2520and%2520domain%2520context.%2520Additionally%252C%2520we%2520introduce%2520a%2520Hierarchical%250AContrastive%2520Learning%2520objective%252C%2520which%2520separately%2520supervises%2520role-sensitive%2520and%250Adomain-aware%2520representations%2520to%2520encourage%2520disentangled%2520latent%2520structures%250Awithout%2520introducing%2520optimization%2520conflict.%2520Experiments%2520on%2520three%2520diverse%2520sports%250Adatasets%252C%2520Basketball-U%252C%2520Football-U%252C%2520and%2520Soccer-U%252C%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520adaptive%2520design%252C%2520achieving%2520strong%2520performance%2520in%2520both%2520unified%2520and%250Across-domain%2520trajectory%2520prediction%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaSports-Traj%3A%20Role-%20and%20Domain-Aware%20Adaptation%20for%20Multi-Agent%0A%20%20Trajectory%20Modeling%20in%20Sports&entry.906535625=Yi%20Xu%20and%20Yun%20Fu&entry.1292438233=%20%20Trajectory%20prediction%20in%20multi-agent%20sports%20scenarios%20is%20inherently%0Achallenging%20due%20to%20the%20structural%20heterogeneity%20across%20agent%20roles%20%28e.g.%2C%0Aplayers%20vs.%20ball%29%20and%20dynamic%20distribution%20gaps%20across%20different%20sports%0Adomains.%20Existing%20unified%20frameworks%20often%20fail%20to%20capture%20these%20structured%0Adistributional%20shifts%2C%20resulting%20in%20suboptimal%20generalization%20across%20roles%20and%0Adomains.%20We%20propose%20AdaSports-Traj%2C%20an%20adaptive%20trajectory%20modeling%20framework%0Athat%20explicitly%20addresses%20both%20intra-domain%20and%20inter-domain%20distribution%0Adiscrepancies%20in%20sports.%20At%20its%20core%2C%20AdaSports-Traj%20incorporates%20a%20Role-%20and%0ADomain-Aware%20Adapter%20to%20conditionally%20adjust%20latent%20representations%20based%20on%0Aagent%20identity%20and%20domain%20context.%20Additionally%2C%20we%20introduce%20a%20Hierarchical%0AContrastive%20Learning%20objective%2C%20which%20separately%20supervises%20role-sensitive%20and%0Adomain-aware%20representations%20to%20encourage%20disentangled%20latent%20structures%0Awithout%20introducing%20optimization%20conflict.%20Experiments%20on%20three%20diverse%20sports%0Adatasets%2C%20Basketball-U%2C%20Football-U%2C%20and%20Soccer-U%2C%20demonstrate%20the%20effectiveness%0Aof%20our%20adaptive%20design%2C%20achieving%20strong%20performance%20in%20both%20unified%20and%0Across-domain%20trajectory%20prediction%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16095v1&entry.124074799=Read"},
{"title": "AttentionDrop: A Novel Regularization Method for Transformer Models", "author": "Mirza Samad Ahmed Baig and Syeda Anshrah Gillani and Abdul Akbar Khan and Shahid Munir Shah and Muhammad Omer Khan", "abstract": "  Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and speech\nprocessing. However, their immense capacity often leads to overfitting,\nespecially when training data is limited or noisy. In this research, a unified\nfamily of stochastic regularization techniques has been proposed, i.e.\nAttentionDrop with its three different variants, which operate directly on the\nself-attention distributions. Hard Attention Masking randomly zeroes out top-k\nattention logits per query to encourage diverse context utilization, Blurred\nAttention Smoothing applies a dynamic Gaussian convolution over attention\nlogits to diffuse overly peaked distributions, and Consistency-Regularized\nAttentionDrop enforces output stability under multiple independent\nAttentionDrop perturbations via a KL-based consistency loss. Results achieved\nin the study demonstrate that AttentionDrop consistently improves accuracy,\ncalibration, and adversarial robustness over standard Dropout, DropConnect, and\nR-Drop baselines\n", "link": "http://arxiv.org/abs/2504.12088v2", "date": "2025-09-19", "relevancy": 2.1441, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.562}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionDrop%3A%20A%20Novel%20Regularization%20Method%20for%20Transformer%20Models&body=Title%3A%20AttentionDrop%3A%20A%20Novel%20Regularization%20Method%20for%20Transformer%20Models%0AAuthor%3A%20Mirza%20Samad%20Ahmed%20Baig%20and%20Syeda%20Anshrah%20Gillani%20and%20Abdul%20Akbar%20Khan%20and%20Shahid%20Munir%20Shah%20and%20Muhammad%20Omer%20Khan%0AAbstract%3A%20%20%20Transformer-based%20architectures%20achieve%20state-of-the-art%20performance%20across%20a%0Awide%20range%20of%20tasks%20in%20natural%20language%20processing%2C%20computer%20vision%2C%20and%20speech%0Aprocessing.%20However%2C%20their%20immense%20capacity%20often%20leads%20to%20overfitting%2C%0Aespecially%20when%20training%20data%20is%20limited%20or%20noisy.%20In%20this%20research%2C%20a%20unified%0Afamily%20of%20stochastic%20regularization%20techniques%20has%20been%20proposed%2C%20i.e.%0AAttentionDrop%20with%20its%20three%20different%20variants%2C%20which%20operate%20directly%20on%20the%0Aself-attention%20distributions.%20Hard%20Attention%20Masking%20randomly%20zeroes%20out%20top-k%0Aattention%20logits%20per%20query%20to%20encourage%20diverse%20context%20utilization%2C%20Blurred%0AAttention%20Smoothing%20applies%20a%20dynamic%20Gaussian%20convolution%20over%20attention%0Alogits%20to%20diffuse%20overly%20peaked%20distributions%2C%20and%20Consistency-Regularized%0AAttentionDrop%20enforces%20output%20stability%20under%20multiple%20independent%0AAttentionDrop%20perturbations%20via%20a%20KL-based%20consistency%20loss.%20Results%20achieved%0Ain%20the%20study%20demonstrate%20that%20AttentionDrop%20consistently%20improves%20accuracy%2C%0Acalibration%2C%20and%20adversarial%20robustness%20over%20standard%20Dropout%2C%20DropConnect%2C%20and%0AR-Drop%20baselines%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionDrop%253A%2520A%2520Novel%2520Regularization%2520Method%2520for%2520Transformer%2520Models%26entry.906535625%3DMirza%2520Samad%2520Ahmed%2520Baig%2520and%2520Syeda%2520Anshrah%2520Gillani%2520and%2520Abdul%2520Akbar%2520Khan%2520and%2520Shahid%2520Munir%2520Shah%2520and%2520Muhammad%2520Omer%2520Khan%26entry.1292438233%3D%2520%2520Transformer-based%2520architectures%2520achieve%2520state-of-the-art%2520performance%2520across%2520a%250Awide%2520range%2520of%2520tasks%2520in%2520natural%2520language%2520processing%252C%2520computer%2520vision%252C%2520and%2520speech%250Aprocessing.%2520However%252C%2520their%2520immense%2520capacity%2520often%2520leads%2520to%2520overfitting%252C%250Aespecially%2520when%2520training%2520data%2520is%2520limited%2520or%2520noisy.%2520In%2520this%2520research%252C%2520a%2520unified%250Afamily%2520of%2520stochastic%2520regularization%2520techniques%2520has%2520been%2520proposed%252C%2520i.e.%250AAttentionDrop%2520with%2520its%2520three%2520different%2520variants%252C%2520which%2520operate%2520directly%2520on%2520the%250Aself-attention%2520distributions.%2520Hard%2520Attention%2520Masking%2520randomly%2520zeroes%2520out%2520top-k%250Aattention%2520logits%2520per%2520query%2520to%2520encourage%2520diverse%2520context%2520utilization%252C%2520Blurred%250AAttention%2520Smoothing%2520applies%2520a%2520dynamic%2520Gaussian%2520convolution%2520over%2520attention%250Alogits%2520to%2520diffuse%2520overly%2520peaked%2520distributions%252C%2520and%2520Consistency-Regularized%250AAttentionDrop%2520enforces%2520output%2520stability%2520under%2520multiple%2520independent%250AAttentionDrop%2520perturbations%2520via%2520a%2520KL-based%2520consistency%2520loss.%2520Results%2520achieved%250Ain%2520the%2520study%2520demonstrate%2520that%2520AttentionDrop%2520consistently%2520improves%2520accuracy%252C%250Acalibration%252C%2520and%2520adversarial%2520robustness%2520over%2520standard%2520Dropout%252C%2520DropConnect%252C%2520and%250AR-Drop%2520baselines%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionDrop%3A%20A%20Novel%20Regularization%20Method%20for%20Transformer%20Models&entry.906535625=Mirza%20Samad%20Ahmed%20Baig%20and%20Syeda%20Anshrah%20Gillani%20and%20Abdul%20Akbar%20Khan%20and%20Shahid%20Munir%20Shah%20and%20Muhammad%20Omer%20Khan&entry.1292438233=%20%20Transformer-based%20architectures%20achieve%20state-of-the-art%20performance%20across%20a%0Awide%20range%20of%20tasks%20in%20natural%20language%20processing%2C%20computer%20vision%2C%20and%20speech%0Aprocessing.%20However%2C%20their%20immense%20capacity%20often%20leads%20to%20overfitting%2C%0Aespecially%20when%20training%20data%20is%20limited%20or%20noisy.%20In%20this%20research%2C%20a%20unified%0Afamily%20of%20stochastic%20regularization%20techniques%20has%20been%20proposed%2C%20i.e.%0AAttentionDrop%20with%20its%20three%20different%20variants%2C%20which%20operate%20directly%20on%20the%0Aself-attention%20distributions.%20Hard%20Attention%20Masking%20randomly%20zeroes%20out%20top-k%0Aattention%20logits%20per%20query%20to%20encourage%20diverse%20context%20utilization%2C%20Blurred%0AAttention%20Smoothing%20applies%20a%20dynamic%20Gaussian%20convolution%20over%20attention%0Alogits%20to%20diffuse%20overly%20peaked%20distributions%2C%20and%20Consistency-Regularized%0AAttentionDrop%20enforces%20output%20stability%20under%20multiple%20independent%0AAttentionDrop%20perturbations%20via%20a%20KL-based%20consistency%20loss.%20Results%20achieved%0Ain%20the%20study%20demonstrate%20that%20AttentionDrop%20consistently%20improves%20accuracy%2C%0Acalibration%2C%20and%20adversarial%20robustness%20over%20standard%20Dropout%2C%20DropConnect%2C%20and%0AR-Drop%20baselines%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12088v2&entry.124074799=Read"},
{"title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency", "author": "Peng Chen and Hailiang Zhao and Jiaji Zhang and Xueyan Tang and Yixuan Wang and Shuiguang Deng", "abstract": "  The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.\n", "link": "http://arxiv.org/abs/2507.16242v3", "date": "2025-09-19", "relevancy": 2.1383, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4349}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustifying%20Learning-Augmented%20Caching%20Efficiently%20without%20Compromising%0A%20%201-Consistency&body=Title%3A%20Robustifying%20Learning-Augmented%20Caching%20Efficiently%20without%20Compromising%0A%20%201-Consistency%0AAuthor%3A%20Peng%20Chen%20and%20Hailiang%20Zhao%20and%20Jiaji%20Zhang%20and%20Xueyan%20Tang%20and%20Yixuan%20Wang%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20The%20online%20caching%20problem%20aims%20to%20minimize%20cache%20misses%20when%20serving%20a%0Asequence%20of%20requests%20under%20a%20limited%20cache%20size.%20While%20naive%20learning-augmented%0Acaching%20algorithms%20achieve%20ideal%20%241%24-consistency%2C%20they%20lack%20robustness%0Aguarantees.%20Existing%20robustification%20methods%20either%20sacrifice%20%241%24-consistency%0Aor%20introduce%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AGuard%2C%20a%20lightweight%20robustification%20framework%20that%20enhances%20the%20robustness%20of%0Aa%20broad%20class%20of%20learning-augmented%20caching%20algorithms%20to%20%242H_k%20%2B%202%24%2C%20while%0Apreserving%20their%20%241%24-consistency.%20Guard%20achieves%20the%20current%20best-known%0Atrade-off%20between%20consistency%20and%20robustness%2C%20with%20only%20%24O%281%29%24%20additional%0Aper-request%20overhead%2C%20thereby%20maintaining%20the%20original%20time%20complexity%20of%20the%0Abase%20algorithm.%20Extensive%20experiments%20across%20multiple%20real-world%20datasets%20and%0Aprediction%20models%20validate%20the%20effectiveness%20of%20Guard%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16242v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustifying%2520Learning-Augmented%2520Caching%2520Efficiently%2520without%2520Compromising%250A%2520%25201-Consistency%26entry.906535625%3DPeng%2520Chen%2520and%2520Hailiang%2520Zhao%2520and%2520Jiaji%2520Zhang%2520and%2520Xueyan%2520Tang%2520and%2520Yixuan%2520Wang%2520and%2520Shuiguang%2520Deng%26entry.1292438233%3D%2520%2520The%2520online%2520caching%2520problem%2520aims%2520to%2520minimize%2520cache%2520misses%2520when%2520serving%2520a%250Asequence%2520of%2520requests%2520under%2520a%2520limited%2520cache%2520size.%2520While%2520naive%2520learning-augmented%250Acaching%2520algorithms%2520achieve%2520ideal%2520%25241%2524-consistency%252C%2520they%2520lack%2520robustness%250Aguarantees.%2520Existing%2520robustification%2520methods%2520either%2520sacrifice%2520%25241%2524-consistency%250Aor%2520introduce%2520significant%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AGuard%252C%2520a%2520lightweight%2520robustification%2520framework%2520that%2520enhances%2520the%2520robustness%2520of%250Aa%2520broad%2520class%2520of%2520learning-augmented%2520caching%2520algorithms%2520to%2520%25242H_k%2520%252B%25202%2524%252C%2520while%250Apreserving%2520their%2520%25241%2524-consistency.%2520Guard%2520achieves%2520the%2520current%2520best-known%250Atrade-off%2520between%2520consistency%2520and%2520robustness%252C%2520with%2520only%2520%2524O%25281%2529%2524%2520additional%250Aper-request%2520overhead%252C%2520thereby%2520maintaining%2520the%2520original%2520time%2520complexity%2520of%2520the%250Abase%2520algorithm.%2520Extensive%2520experiments%2520across%2520multiple%2520real-world%2520datasets%2520and%250Aprediction%2520models%2520validate%2520the%2520effectiveness%2520of%2520Guard%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16242v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustifying%20Learning-Augmented%20Caching%20Efficiently%20without%20Compromising%0A%20%201-Consistency&entry.906535625=Peng%20Chen%20and%20Hailiang%20Zhao%20and%20Jiaji%20Zhang%20and%20Xueyan%20Tang%20and%20Yixuan%20Wang%20and%20Shuiguang%20Deng&entry.1292438233=%20%20The%20online%20caching%20problem%20aims%20to%20minimize%20cache%20misses%20when%20serving%20a%0Asequence%20of%20requests%20under%20a%20limited%20cache%20size.%20While%20naive%20learning-augmented%0Acaching%20algorithms%20achieve%20ideal%20%241%24-consistency%2C%20they%20lack%20robustness%0Aguarantees.%20Existing%20robustification%20methods%20either%20sacrifice%20%241%24-consistency%0Aor%20introduce%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AGuard%2C%20a%20lightweight%20robustification%20framework%20that%20enhances%20the%20robustness%20of%0Aa%20broad%20class%20of%20learning-augmented%20caching%20algorithms%20to%20%242H_k%20%2B%202%24%2C%20while%0Apreserving%20their%20%241%24-consistency.%20Guard%20achieves%20the%20current%20best-known%0Atrade-off%20between%20consistency%20and%20robustness%2C%20with%20only%20%24O%281%29%24%20additional%0Aper-request%20overhead%2C%20thereby%20maintaining%20the%20original%20time%20complexity%20of%20the%0Abase%20algorithm.%20Extensive%20experiments%20across%20multiple%20real-world%20datasets%20and%0Aprediction%20models%20validate%20the%20effectiveness%20of%20Guard%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16242v3&entry.124074799=Read"},
{"title": "FOVAL: Calibration-Free and Subject-Invariant Fixation Depth Estimation\n  Across Diverse Eye-Tracking Datasets", "author": "Benedikt W. Hosp", "abstract": "  Accurate fixation depth estimation is essential for applications in extended\nreality (XR), robotics, and human-computer interaction. However, current\nmethods heavily depend on user-specific calibration, which limits their\nscalability and usability. We introduce FOVAL, a robust calibration-free\napproach that combines spatiotemporal sequence modelling via Long Short-Term\nMemory (LSTM) networks with subject-invariant feature engineering and\nnormalisation. Compared to Transformers, Temporal Convolutional Networks\n(TCNs), and CNNs, FOVAL achieves superior performance, particularly in\nscenarios with limited and noisy gaze data. Evaluations across three benchmark\ndatasets using Leave-One-Out Cross-Validation (LOOCV) and cross-dataset\nvalidation show a mean absolute error (MAE) of 9.1 cm and strong generalisation\nwithout calibration. We further analyse inter-subject variability and domain\nshifts, providing insight into model robustness and adaptation. FOVAL's\nscalability and accuracy make it highly suitable for real-world deployment.\n", "link": "http://arxiv.org/abs/2408.03591v2", "date": "2025-09-19", "relevancy": 2.1371, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5633}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOVAL%3A%20Calibration-Free%20and%20Subject-Invariant%20Fixation%20Depth%20Estimation%0A%20%20Across%20Diverse%20Eye-Tracking%20Datasets&body=Title%3A%20FOVAL%3A%20Calibration-Free%20and%20Subject-Invariant%20Fixation%20Depth%20Estimation%0A%20%20Across%20Diverse%20Eye-Tracking%20Datasets%0AAuthor%3A%20Benedikt%20W.%20Hosp%0AAbstract%3A%20%20%20Accurate%20fixation%20depth%20estimation%20is%20essential%20for%20applications%20in%20extended%0Areality%20%28XR%29%2C%20robotics%2C%20and%20human-computer%20interaction.%20However%2C%20current%0Amethods%20heavily%20depend%20on%20user-specific%20calibration%2C%20which%20limits%20their%0Ascalability%20and%20usability.%20We%20introduce%20FOVAL%2C%20a%20robust%20calibration-free%0Aapproach%20that%20combines%20spatiotemporal%20sequence%20modelling%20via%20Long%20Short-Term%0AMemory%20%28LSTM%29%20networks%20with%20subject-invariant%20feature%20engineering%20and%0Anormalisation.%20Compared%20to%20Transformers%2C%20Temporal%20Convolutional%20Networks%0A%28TCNs%29%2C%20and%20CNNs%2C%20FOVAL%20achieves%20superior%20performance%2C%20particularly%20in%0Ascenarios%20with%20limited%20and%20noisy%20gaze%20data.%20Evaluations%20across%20three%20benchmark%0Adatasets%20using%20Leave-One-Out%20Cross-Validation%20%28LOOCV%29%20and%20cross-dataset%0Avalidation%20show%20a%20mean%20absolute%20error%20%28MAE%29%20of%209.1%20cm%20and%20strong%20generalisation%0Awithout%20calibration.%20We%20further%20analyse%20inter-subject%20variability%20and%20domain%0Ashifts%2C%20providing%20insight%20into%20model%20robustness%20and%20adaptation.%20FOVAL%27s%0Ascalability%20and%20accuracy%20make%20it%20highly%20suitable%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03591v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOVAL%253A%2520Calibration-Free%2520and%2520Subject-Invariant%2520Fixation%2520Depth%2520Estimation%250A%2520%2520Across%2520Diverse%2520Eye-Tracking%2520Datasets%26entry.906535625%3DBenedikt%2520W.%2520Hosp%26entry.1292438233%3D%2520%2520Accurate%2520fixation%2520depth%2520estimation%2520is%2520essential%2520for%2520applications%2520in%2520extended%250Areality%2520%2528XR%2529%252C%2520robotics%252C%2520and%2520human-computer%2520interaction.%2520However%252C%2520current%250Amethods%2520heavily%2520depend%2520on%2520user-specific%2520calibration%252C%2520which%2520limits%2520their%250Ascalability%2520and%2520usability.%2520We%2520introduce%2520FOVAL%252C%2520a%2520robust%2520calibration-free%250Aapproach%2520that%2520combines%2520spatiotemporal%2520sequence%2520modelling%2520via%2520Long%2520Short-Term%250AMemory%2520%2528LSTM%2529%2520networks%2520with%2520subject-invariant%2520feature%2520engineering%2520and%250Anormalisation.%2520Compared%2520to%2520Transformers%252C%2520Temporal%2520Convolutional%2520Networks%250A%2528TCNs%2529%252C%2520and%2520CNNs%252C%2520FOVAL%2520achieves%2520superior%2520performance%252C%2520particularly%2520in%250Ascenarios%2520with%2520limited%2520and%2520noisy%2520gaze%2520data.%2520Evaluations%2520across%2520three%2520benchmark%250Adatasets%2520using%2520Leave-One-Out%2520Cross-Validation%2520%2528LOOCV%2529%2520and%2520cross-dataset%250Avalidation%2520show%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25209.1%2520cm%2520and%2520strong%2520generalisation%250Awithout%2520calibration.%2520We%2520further%2520analyse%2520inter-subject%2520variability%2520and%2520domain%250Ashifts%252C%2520providing%2520insight%2520into%2520model%2520robustness%2520and%2520adaptation.%2520FOVAL%2527s%250Ascalability%2520and%2520accuracy%2520make%2520it%2520highly%2520suitable%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03591v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOVAL%3A%20Calibration-Free%20and%20Subject-Invariant%20Fixation%20Depth%20Estimation%0A%20%20Across%20Diverse%20Eye-Tracking%20Datasets&entry.906535625=Benedikt%20W.%20Hosp&entry.1292438233=%20%20Accurate%20fixation%20depth%20estimation%20is%20essential%20for%20applications%20in%20extended%0Areality%20%28XR%29%2C%20robotics%2C%20and%20human-computer%20interaction.%20However%2C%20current%0Amethods%20heavily%20depend%20on%20user-specific%20calibration%2C%20which%20limits%20their%0Ascalability%20and%20usability.%20We%20introduce%20FOVAL%2C%20a%20robust%20calibration-free%0Aapproach%20that%20combines%20spatiotemporal%20sequence%20modelling%20via%20Long%20Short-Term%0AMemory%20%28LSTM%29%20networks%20with%20subject-invariant%20feature%20engineering%20and%0Anormalisation.%20Compared%20to%20Transformers%2C%20Temporal%20Convolutional%20Networks%0A%28TCNs%29%2C%20and%20CNNs%2C%20FOVAL%20achieves%20superior%20performance%2C%20particularly%20in%0Ascenarios%20with%20limited%20and%20noisy%20gaze%20data.%20Evaluations%20across%20three%20benchmark%0Adatasets%20using%20Leave-One-Out%20Cross-Validation%20%28LOOCV%29%20and%20cross-dataset%0Avalidation%20show%20a%20mean%20absolute%20error%20%28MAE%29%20of%209.1%20cm%20and%20strong%20generalisation%0Awithout%20calibration.%20We%20further%20analyse%20inter-subject%20variability%20and%20domain%0Ashifts%2C%20providing%20insight%20into%20model%20robustness%20and%20adaptation.%20FOVAL%27s%0Ascalability%20and%20accuracy%20make%20it%20highly%20suitable%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03591v2&entry.124074799=Read"},
{"title": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models", "author": "Renjie Pi and Kehao Miao and Li Peihang and Runtao Liu and Jiahui Gao and Jipeng Zhang and Xiaofang Zhou", "abstract": "  Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions.\n", "link": "http://arxiv.org/abs/2509.16149v1", "date": "2025-09-19", "relevancy": 2.1306, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointing%20to%20a%20Llama%20and%20Call%20it%20a%20Camel%3A%20On%20the%20Sycophancy%20of%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20Pointing%20to%20a%20Llama%20and%20Call%20it%20a%20Camel%3A%20On%20the%20Sycophancy%20of%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Renjie%20Pi%20and%20Kehao%20Miao%20and%20Li%20Peihang%20and%20Runtao%20Liu%20and%20Jiahui%20Gao%20and%20Jipeng%20Zhang%20and%20Xiaofang%20Zhou%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20extraordinary%0Acapabilities%20in%20conducting%20conversations%20based%20on%20image%20inputs.%20However%2C%20we%0Aobserve%20that%20MLLMs%20exhibit%20a%20pronounced%20form%20of%20visual%20sycophantic%20behavior.%0AWhile%20similar%20behavior%20has%20also%20been%20noted%20in%20text-based%20large%20language%20models%0A%28LLMs%29%2C%20it%20becomes%20significantly%20more%20prominent%20when%20MLLMs%20process%20image%0Ainputs.%20We%20refer%20to%20this%20phenomenon%20as%20the%20%22sycophantic%20modality%20gap.%22%20To%0Abetter%20understand%20this%20issue%2C%20we%20further%20analyze%20the%20factors%20that%20contribute%20to%0Athe%20exacerbation%20of%20this%20gap.%20To%20mitigate%20the%20visual%20sycophantic%20behavior%2C%20we%0Afirst%20experiment%20with%20naive%20supervised%20fine-tuning%20to%20help%20the%20MLLM%20resist%0Amisleading%20instructions%20from%20the%20user.%20However%2C%20we%20find%20that%20this%20approach%20also%0Amakes%20the%20MLLM%20overly%20resistant%20to%20corrective%20instructions%20%28i.e.%2C%20stubborn%20even%0Aif%20it%20is%20wrong%29.%20To%20alleviate%20this%20trade-off%2C%20we%20propose%20Sycophantic%20Reflective%0ATuning%20%28SRT%29%2C%20which%20enables%20the%20MLLM%20to%20engage%20in%20reflective%20reasoning%2C%0Aallowing%20it%20to%20determine%20whether%20a%20user%27s%20instruction%20is%20misleading%20or%0Acorrective%20before%20drawing%20a%20conclusion.%20After%20applying%20SRT%2C%20we%20observe%20a%0Asignificant%20reduction%20in%20sycophantic%20behavior%20toward%20misleading%20instructions%2C%0Awithout%20resulting%20in%20excessive%20stubbornness%20when%20receiving%20corrective%0Ainstructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointing%2520to%2520a%2520Llama%2520and%2520Call%2520it%2520a%2520Camel%253A%2520On%2520the%2520Sycophancy%2520of%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DRenjie%2520Pi%2520and%2520Kehao%2520Miao%2520and%2520Li%2520Peihang%2520and%2520Runtao%2520Liu%2520and%2520Jiahui%2520Gao%2520and%2520Jipeng%2520Zhang%2520and%2520Xiaofang%2520Zhou%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520extraordinary%250Acapabilities%2520in%2520conducting%2520conversations%2520based%2520on%2520image%2520inputs.%2520However%252C%2520we%250Aobserve%2520that%2520MLLMs%2520exhibit%2520a%2520pronounced%2520form%2520of%2520visual%2520sycophantic%2520behavior.%250AWhile%2520similar%2520behavior%2520has%2520also%2520been%2520noted%2520in%2520text-based%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520it%2520becomes%2520significantly%2520more%2520prominent%2520when%2520MLLMs%2520process%2520image%250Ainputs.%2520We%2520refer%2520to%2520this%2520phenomenon%2520as%2520the%2520%2522sycophantic%2520modality%2520gap.%2522%2520To%250Abetter%2520understand%2520this%2520issue%252C%2520we%2520further%2520analyze%2520the%2520factors%2520that%2520contribute%2520to%250Athe%2520exacerbation%2520of%2520this%2520gap.%2520To%2520mitigate%2520the%2520visual%2520sycophantic%2520behavior%252C%2520we%250Afirst%2520experiment%2520with%2520naive%2520supervised%2520fine-tuning%2520to%2520help%2520the%2520MLLM%2520resist%250Amisleading%2520instructions%2520from%2520the%2520user.%2520However%252C%2520we%2520find%2520that%2520this%2520approach%2520also%250Amakes%2520the%2520MLLM%2520overly%2520resistant%2520to%2520corrective%2520instructions%2520%2528i.e.%252C%2520stubborn%2520even%250Aif%2520it%2520is%2520wrong%2529.%2520To%2520alleviate%2520this%2520trade-off%252C%2520we%2520propose%2520Sycophantic%2520Reflective%250ATuning%2520%2528SRT%2529%252C%2520which%2520enables%2520the%2520MLLM%2520to%2520engage%2520in%2520reflective%2520reasoning%252C%250Aallowing%2520it%2520to%2520determine%2520whether%2520a%2520user%2527s%2520instruction%2520is%2520misleading%2520or%250Acorrective%2520before%2520drawing%2520a%2520conclusion.%2520After%2520applying%2520SRT%252C%2520we%2520observe%2520a%250Asignificant%2520reduction%2520in%2520sycophantic%2520behavior%2520toward%2520misleading%2520instructions%252C%250Awithout%2520resulting%2520in%2520excessive%2520stubbornness%2520when%2520receiving%2520corrective%250Ainstructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointing%20to%20a%20Llama%20and%20Call%20it%20a%20Camel%3A%20On%20the%20Sycophancy%20of%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Renjie%20Pi%20and%20Kehao%20Miao%20and%20Li%20Peihang%20and%20Runtao%20Liu%20and%20Jiahui%20Gao%20and%20Jipeng%20Zhang%20and%20Xiaofang%20Zhou&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20extraordinary%0Acapabilities%20in%20conducting%20conversations%20based%20on%20image%20inputs.%20However%2C%20we%0Aobserve%20that%20MLLMs%20exhibit%20a%20pronounced%20form%20of%20visual%20sycophantic%20behavior.%0AWhile%20similar%20behavior%20has%20also%20been%20noted%20in%20text-based%20large%20language%20models%0A%28LLMs%29%2C%20it%20becomes%20significantly%20more%20prominent%20when%20MLLMs%20process%20image%0Ainputs.%20We%20refer%20to%20this%20phenomenon%20as%20the%20%22sycophantic%20modality%20gap.%22%20To%0Abetter%20understand%20this%20issue%2C%20we%20further%20analyze%20the%20factors%20that%20contribute%20to%0Athe%20exacerbation%20of%20this%20gap.%20To%20mitigate%20the%20visual%20sycophantic%20behavior%2C%20we%0Afirst%20experiment%20with%20naive%20supervised%20fine-tuning%20to%20help%20the%20MLLM%20resist%0Amisleading%20instructions%20from%20the%20user.%20However%2C%20we%20find%20that%20this%20approach%20also%0Amakes%20the%20MLLM%20overly%20resistant%20to%20corrective%20instructions%20%28i.e.%2C%20stubborn%20even%0Aif%20it%20is%20wrong%29.%20To%20alleviate%20this%20trade-off%2C%20we%20propose%20Sycophantic%20Reflective%0ATuning%20%28SRT%29%2C%20which%20enables%20the%20MLLM%20to%20engage%20in%20reflective%20reasoning%2C%0Aallowing%20it%20to%20determine%20whether%20a%20user%27s%20instruction%20is%20misleading%20or%0Acorrective%20before%20drawing%20a%20conclusion.%20After%20applying%20SRT%2C%20we%20observe%20a%0Asignificant%20reduction%20in%20sycophantic%20behavior%20toward%20misleading%20instructions%2C%0Awithout%20resulting%20in%20excessive%20stubbornness%20when%20receiving%20corrective%0Ainstructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16149v1&entry.124074799=Read"},
{"title": "EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving\n  Brain Network", "author": "Rikuto Kotoge and Zheng Chen and Tasuku Kimura and Yasuko Matsubara and Takufumi Yanagisawa and Haruhiko Kishima and Yasushi Sakurai", "abstract": "  Dynamic GNNs, which integrate temporal and spatial features in\nElectroencephalography (EEG) data, have shown great potential in automating\nseizure detection. However, fully capturing the underlying dynamics necessary\nto represent brain states, such as seizure and non-seizure, remains a\nnon-trivial task and presents two fundamental challenges. First, most existing\ndynamic GNN methods are built on temporally fixed static graphs, which fail to\nreflect the evolving nature of brain connectivity during seizure progression.\nSecond, current efforts to jointly model temporal signals and graph structures\nand, more importantly, their interactions remain nascent, often resulting in\ninconsistent performance. To address these challenges, we present the first\ntheoretical analysis of these two problems, demonstrating the effectiveness and\nnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.\nBuilding on these insights, we propose EvoBrain, a novel seizure detection\nmodel that integrates a two-stream Mamba architecture with a GCN enhanced by\nLaplacian Positional Encoding, following neurological insights. Moreover,\nEvoBrain incorporates explicitly dynamic graph structures, allowing both nodes\nand edges to evolve over time. Our contributions include (a) a theoretical\nanalysis proving the expressivity advantage of explicit dynamic modeling and\ntime-then-graph over other approaches, (b) a novel and efficient model that\nsignificantly improves AUROC by 23% and F1 score by 30%, compared with the\ndynamic GNN baseline, and (c) broad evaluations of our method on the\nchallenging early seizure prediction tasks.\n", "link": "http://arxiv.org/abs/2509.15857v1", "date": "2025-09-19", "relevancy": 2.1184, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.586}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoBrain%3A%20Dynamic%20Multi-channel%20EEG%20Graph%20Modeling%20for%20Time-evolving%0A%20%20Brain%20Network&body=Title%3A%20EvoBrain%3A%20Dynamic%20Multi-channel%20EEG%20Graph%20Modeling%20for%20Time-evolving%0A%20%20Brain%20Network%0AAuthor%3A%20Rikuto%20Kotoge%20and%20Zheng%20Chen%20and%20Tasuku%20Kimura%20and%20Yasuko%20Matsubara%20and%20Takufumi%20Yanagisawa%20and%20Haruhiko%20Kishima%20and%20Yasushi%20Sakurai%0AAbstract%3A%20%20%20Dynamic%20GNNs%2C%20which%20integrate%20temporal%20and%20spatial%20features%20in%0AElectroencephalography%20%28EEG%29%20data%2C%20have%20shown%20great%20potential%20in%20automating%0Aseizure%20detection.%20However%2C%20fully%20capturing%20the%20underlying%20dynamics%20necessary%0Ato%20represent%20brain%20states%2C%20such%20as%20seizure%20and%20non-seizure%2C%20remains%20a%0Anon-trivial%20task%20and%20presents%20two%20fundamental%20challenges.%20First%2C%20most%20existing%0Adynamic%20GNN%20methods%20are%20built%20on%20temporally%20fixed%20static%20graphs%2C%20which%20fail%20to%0Areflect%20the%20evolving%20nature%20of%20brain%20connectivity%20during%20seizure%20progression.%0ASecond%2C%20current%20efforts%20to%20jointly%20model%20temporal%20signals%20and%20graph%20structures%0Aand%2C%20more%20importantly%2C%20their%20interactions%20remain%20nascent%2C%20often%20resulting%20in%0Ainconsistent%20performance.%20To%20address%20these%20challenges%2C%20we%20present%20the%20first%0Atheoretical%20analysis%20of%20these%20two%20problems%2C%20demonstrating%20the%20effectiveness%20and%0Anecessity%20of%20explicit%20dynamic%20modeling%20and%20time-then-graph%20dynamic%20GNN%20method.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20EvoBrain%2C%20a%20novel%20seizure%20detection%0Amodel%20that%20integrates%20a%20two-stream%20Mamba%20architecture%20with%20a%20GCN%20enhanced%20by%0ALaplacian%20Positional%20Encoding%2C%20following%20neurological%20insights.%20Moreover%2C%0AEvoBrain%20incorporates%20explicitly%20dynamic%20graph%20structures%2C%20allowing%20both%20nodes%0Aand%20edges%20to%20evolve%20over%20time.%20Our%20contributions%20include%20%28a%29%20a%20theoretical%0Aanalysis%20proving%20the%20expressivity%20advantage%20of%20explicit%20dynamic%20modeling%20and%0Atime-then-graph%20over%20other%20approaches%2C%20%28b%29%20a%20novel%20and%20efficient%20model%20that%0Asignificantly%20improves%20AUROC%20by%2023%25%20and%20F1%20score%20by%2030%25%2C%20compared%20with%20the%0Adynamic%20GNN%20baseline%2C%20and%20%28c%29%20broad%20evaluations%20of%20our%20method%20on%20the%0Achallenging%20early%20seizure%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoBrain%253A%2520Dynamic%2520Multi-channel%2520EEG%2520Graph%2520Modeling%2520for%2520Time-evolving%250A%2520%2520Brain%2520Network%26entry.906535625%3DRikuto%2520Kotoge%2520and%2520Zheng%2520Chen%2520and%2520Tasuku%2520Kimura%2520and%2520Yasuko%2520Matsubara%2520and%2520Takufumi%2520Yanagisawa%2520and%2520Haruhiko%2520Kishima%2520and%2520Yasushi%2520Sakurai%26entry.1292438233%3D%2520%2520Dynamic%2520GNNs%252C%2520which%2520integrate%2520temporal%2520and%2520spatial%2520features%2520in%250AElectroencephalography%2520%2528EEG%2529%2520data%252C%2520have%2520shown%2520great%2520potential%2520in%2520automating%250Aseizure%2520detection.%2520However%252C%2520fully%2520capturing%2520the%2520underlying%2520dynamics%2520necessary%250Ato%2520represent%2520brain%2520states%252C%2520such%2520as%2520seizure%2520and%2520non-seizure%252C%2520remains%2520a%250Anon-trivial%2520task%2520and%2520presents%2520two%2520fundamental%2520challenges.%2520First%252C%2520most%2520existing%250Adynamic%2520GNN%2520methods%2520are%2520built%2520on%2520temporally%2520fixed%2520static%2520graphs%252C%2520which%2520fail%2520to%250Areflect%2520the%2520evolving%2520nature%2520of%2520brain%2520connectivity%2520during%2520seizure%2520progression.%250ASecond%252C%2520current%2520efforts%2520to%2520jointly%2520model%2520temporal%2520signals%2520and%2520graph%2520structures%250Aand%252C%2520more%2520importantly%252C%2520their%2520interactions%2520remain%2520nascent%252C%2520often%2520resulting%2520in%250Ainconsistent%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520the%2520first%250Atheoretical%2520analysis%2520of%2520these%2520two%2520problems%252C%2520demonstrating%2520the%2520effectiveness%2520and%250Anecessity%2520of%2520explicit%2520dynamic%2520modeling%2520and%2520time-then-graph%2520dynamic%2520GNN%2520method.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520propose%2520EvoBrain%252C%2520a%2520novel%2520seizure%2520detection%250Amodel%2520that%2520integrates%2520a%2520two-stream%2520Mamba%2520architecture%2520with%2520a%2520GCN%2520enhanced%2520by%250ALaplacian%2520Positional%2520Encoding%252C%2520following%2520neurological%2520insights.%2520Moreover%252C%250AEvoBrain%2520incorporates%2520explicitly%2520dynamic%2520graph%2520structures%252C%2520allowing%2520both%2520nodes%250Aand%2520edges%2520to%2520evolve%2520over%2520time.%2520Our%2520contributions%2520include%2520%2528a%2529%2520a%2520theoretical%250Aanalysis%2520proving%2520the%2520expressivity%2520advantage%2520of%2520explicit%2520dynamic%2520modeling%2520and%250Atime-then-graph%2520over%2520other%2520approaches%252C%2520%2528b%2529%2520a%2520novel%2520and%2520efficient%2520model%2520that%250Asignificantly%2520improves%2520AUROC%2520by%252023%2525%2520and%2520F1%2520score%2520by%252030%2525%252C%2520compared%2520with%2520the%250Adynamic%2520GNN%2520baseline%252C%2520and%2520%2528c%2529%2520broad%2520evaluations%2520of%2520our%2520method%2520on%2520the%250Achallenging%2520early%2520seizure%2520prediction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoBrain%3A%20Dynamic%20Multi-channel%20EEG%20Graph%20Modeling%20for%20Time-evolving%0A%20%20Brain%20Network&entry.906535625=Rikuto%20Kotoge%20and%20Zheng%20Chen%20and%20Tasuku%20Kimura%20and%20Yasuko%20Matsubara%20and%20Takufumi%20Yanagisawa%20and%20Haruhiko%20Kishima%20and%20Yasushi%20Sakurai&entry.1292438233=%20%20Dynamic%20GNNs%2C%20which%20integrate%20temporal%20and%20spatial%20features%20in%0AElectroencephalography%20%28EEG%29%20data%2C%20have%20shown%20great%20potential%20in%20automating%0Aseizure%20detection.%20However%2C%20fully%20capturing%20the%20underlying%20dynamics%20necessary%0Ato%20represent%20brain%20states%2C%20such%20as%20seizure%20and%20non-seizure%2C%20remains%20a%0Anon-trivial%20task%20and%20presents%20two%20fundamental%20challenges.%20First%2C%20most%20existing%0Adynamic%20GNN%20methods%20are%20built%20on%20temporally%20fixed%20static%20graphs%2C%20which%20fail%20to%0Areflect%20the%20evolving%20nature%20of%20brain%20connectivity%20during%20seizure%20progression.%0ASecond%2C%20current%20efforts%20to%20jointly%20model%20temporal%20signals%20and%20graph%20structures%0Aand%2C%20more%20importantly%2C%20their%20interactions%20remain%20nascent%2C%20often%20resulting%20in%0Ainconsistent%20performance.%20To%20address%20these%20challenges%2C%20we%20present%20the%20first%0Atheoretical%20analysis%20of%20these%20two%20problems%2C%20demonstrating%20the%20effectiveness%20and%0Anecessity%20of%20explicit%20dynamic%20modeling%20and%20time-then-graph%20dynamic%20GNN%20method.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20EvoBrain%2C%20a%20novel%20seizure%20detection%0Amodel%20that%20integrates%20a%20two-stream%20Mamba%20architecture%20with%20a%20GCN%20enhanced%20by%0ALaplacian%20Positional%20Encoding%2C%20following%20neurological%20insights.%20Moreover%2C%0AEvoBrain%20incorporates%20explicitly%20dynamic%20graph%20structures%2C%20allowing%20both%20nodes%0Aand%20edges%20to%20evolve%20over%20time.%20Our%20contributions%20include%20%28a%29%20a%20theoretical%0Aanalysis%20proving%20the%20expressivity%20advantage%20of%20explicit%20dynamic%20modeling%20and%0Atime-then-graph%20over%20other%20approaches%2C%20%28b%29%20a%20novel%20and%20efficient%20model%20that%0Asignificantly%20improves%20AUROC%20by%2023%25%20and%20F1%20score%20by%2030%25%2C%20compared%20with%20the%0Adynamic%20GNN%20baseline%2C%20and%20%28c%29%20broad%20evaluations%20of%20our%20method%20on%20the%0Achallenging%20early%20seizure%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15857v1&entry.124074799=Read"},
{"title": "FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency\n  Domain Multi-Axis Representation Learning and Dual Attention Mechanisms", "author": "Fang Lu and Jingyu Xu and Qinxiu Sun and Qiong Lou", "abstract": "  Accurate abdominal multi-organ segmentation is critical for clinical\napplications. Although numerous deep learning-based automatic segmentation\nmethods have been developed, they still struggle to segment small, irregular,\nor anatomically complex organs. Moreover, most current methods focus on\nspatial-domain analysis, often overlooking the synergistic potential of\nfrequency-domain representations. To address these limitations, we propose a\nnovel framework named FMD-TransUNet for precise abdominal multi-organ\nsegmentation. It innovatively integrates the Multi-axis External Weight Block\n(MEWB) and the improved dual attention module (DA+) into the TransUNet\nframework. The MEWB extracts multi-axis frequency-domain features to capture\nboth global anatomical structures and local boundary details, providing\ncomplementary information to spatial-domain representations. The DA+ block\nutilizes depthwise separable convolutions and incorporates spatial and channel\nattention mechanisms to enhance feature fusion, reduce redundant information,\nand narrow the semantic gap between the encoder and decoder. Experimental\nvalidation on the Synapse dataset shows that FMD-TransUNet outperforms other\nrecent state-of-the-art methods, achieving an average DSC of 81.32\\% and a HD\nof 16.35 mm across eight abdominal organs. Compared to the baseline model, the\naverage DSC increased by 3.84\\%, and the average HD decreased by 15.34 mm.\nThese results demonstrate the effectiveness of FMD-TransUNet in improving the\naccuracy of abdominal multi-organ segmentation.\n", "link": "http://arxiv.org/abs/2509.16044v1", "date": "2025-09-19", "relevancy": 2.101, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMD-TransUNet%3A%20Abdominal%20Multi-Organ%20Segmentation%20Based%20on%20Frequency%0A%20%20Domain%20Multi-Axis%20Representation%20Learning%20and%20Dual%20Attention%20Mechanisms&body=Title%3A%20FMD-TransUNet%3A%20Abdominal%20Multi-Organ%20Segmentation%20Based%20on%20Frequency%0A%20%20Domain%20Multi-Axis%20Representation%20Learning%20and%20Dual%20Attention%20Mechanisms%0AAuthor%3A%20Fang%20Lu%20and%20Jingyu%20Xu%20and%20Qinxiu%20Sun%20and%20Qiong%20Lou%0AAbstract%3A%20%20%20Accurate%20abdominal%20multi-organ%20segmentation%20is%20critical%20for%20clinical%0Aapplications.%20Although%20numerous%20deep%20learning-based%20automatic%20segmentation%0Amethods%20have%20been%20developed%2C%20they%20still%20struggle%20to%20segment%20small%2C%20irregular%2C%0Aor%20anatomically%20complex%20organs.%20Moreover%2C%20most%20current%20methods%20focus%20on%0Aspatial-domain%20analysis%2C%20often%20overlooking%20the%20synergistic%20potential%20of%0Afrequency-domain%20representations.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20framework%20named%20FMD-TransUNet%20for%20precise%20abdominal%20multi-organ%0Asegmentation.%20It%20innovatively%20integrates%20the%20Multi-axis%20External%20Weight%20Block%0A%28MEWB%29%20and%20the%20improved%20dual%20attention%20module%20%28DA%2B%29%20into%20the%20TransUNet%0Aframework.%20The%20MEWB%20extracts%20multi-axis%20frequency-domain%20features%20to%20capture%0Aboth%20global%20anatomical%20structures%20and%20local%20boundary%20details%2C%20providing%0Acomplementary%20information%20to%20spatial-domain%20representations.%20The%20DA%2B%20block%0Autilizes%20depthwise%20separable%20convolutions%20and%20incorporates%20spatial%20and%20channel%0Aattention%20mechanisms%20to%20enhance%20feature%20fusion%2C%20reduce%20redundant%20information%2C%0Aand%20narrow%20the%20semantic%20gap%20between%20the%20encoder%20and%20decoder.%20Experimental%0Avalidation%20on%20the%20Synapse%20dataset%20shows%20that%20FMD-TransUNet%20outperforms%20other%0Arecent%20state-of-the-art%20methods%2C%20achieving%20an%20average%20DSC%20of%2081.32%5C%25%20and%20a%20HD%0Aof%2016.35%20mm%20across%20eight%20abdominal%20organs.%20Compared%20to%20the%20baseline%20model%2C%20the%0Aaverage%20DSC%20increased%20by%203.84%5C%25%2C%20and%20the%20average%20HD%20decreased%20by%2015.34%20mm.%0AThese%20results%20demonstrate%20the%20effectiveness%20of%20FMD-TransUNet%20in%20improving%20the%0Aaccuracy%20of%20abdominal%20multi-organ%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMD-TransUNet%253A%2520Abdominal%2520Multi-Organ%2520Segmentation%2520Based%2520on%2520Frequency%250A%2520%2520Domain%2520Multi-Axis%2520Representation%2520Learning%2520and%2520Dual%2520Attention%2520Mechanisms%26entry.906535625%3DFang%2520Lu%2520and%2520Jingyu%2520Xu%2520and%2520Qinxiu%2520Sun%2520and%2520Qiong%2520Lou%26entry.1292438233%3D%2520%2520Accurate%2520abdominal%2520multi-organ%2520segmentation%2520is%2520critical%2520for%2520clinical%250Aapplications.%2520Although%2520numerous%2520deep%2520learning-based%2520automatic%2520segmentation%250Amethods%2520have%2520been%2520developed%252C%2520they%2520still%2520struggle%2520to%2520segment%2520small%252C%2520irregular%252C%250Aor%2520anatomically%2520complex%2520organs.%2520Moreover%252C%2520most%2520current%2520methods%2520focus%2520on%250Aspatial-domain%2520analysis%252C%2520often%2520overlooking%2520the%2520synergistic%2520potential%2520of%250Afrequency-domain%2520representations.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520named%2520FMD-TransUNet%2520for%2520precise%2520abdominal%2520multi-organ%250Asegmentation.%2520It%2520innovatively%2520integrates%2520the%2520Multi-axis%2520External%2520Weight%2520Block%250A%2528MEWB%2529%2520and%2520the%2520improved%2520dual%2520attention%2520module%2520%2528DA%252B%2529%2520into%2520the%2520TransUNet%250Aframework.%2520The%2520MEWB%2520extracts%2520multi-axis%2520frequency-domain%2520features%2520to%2520capture%250Aboth%2520global%2520anatomical%2520structures%2520and%2520local%2520boundary%2520details%252C%2520providing%250Acomplementary%2520information%2520to%2520spatial-domain%2520representations.%2520The%2520DA%252B%2520block%250Autilizes%2520depthwise%2520separable%2520convolutions%2520and%2520incorporates%2520spatial%2520and%2520channel%250Aattention%2520mechanisms%2520to%2520enhance%2520feature%2520fusion%252C%2520reduce%2520redundant%2520information%252C%250Aand%2520narrow%2520the%2520semantic%2520gap%2520between%2520the%2520encoder%2520and%2520decoder.%2520Experimental%250Avalidation%2520on%2520the%2520Synapse%2520dataset%2520shows%2520that%2520FMD-TransUNet%2520outperforms%2520other%250Arecent%2520state-of-the-art%2520methods%252C%2520achieving%2520an%2520average%2520DSC%2520of%252081.32%255C%2525%2520and%2520a%2520HD%250Aof%252016.35%2520mm%2520across%2520eight%2520abdominal%2520organs.%2520Compared%2520to%2520the%2520baseline%2520model%252C%2520the%250Aaverage%2520DSC%2520increased%2520by%25203.84%255C%2525%252C%2520and%2520the%2520average%2520HD%2520decreased%2520by%252015.34%2520mm.%250AThese%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520FMD-TransUNet%2520in%2520improving%2520the%250Aaccuracy%2520of%2520abdominal%2520multi-organ%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMD-TransUNet%3A%20Abdominal%20Multi-Organ%20Segmentation%20Based%20on%20Frequency%0A%20%20Domain%20Multi-Axis%20Representation%20Learning%20and%20Dual%20Attention%20Mechanisms&entry.906535625=Fang%20Lu%20and%20Jingyu%20Xu%20and%20Qinxiu%20Sun%20and%20Qiong%20Lou&entry.1292438233=%20%20Accurate%20abdominal%20multi-organ%20segmentation%20is%20critical%20for%20clinical%0Aapplications.%20Although%20numerous%20deep%20learning-based%20automatic%20segmentation%0Amethods%20have%20been%20developed%2C%20they%20still%20struggle%20to%20segment%20small%2C%20irregular%2C%0Aor%20anatomically%20complex%20organs.%20Moreover%2C%20most%20current%20methods%20focus%20on%0Aspatial-domain%20analysis%2C%20often%20overlooking%20the%20synergistic%20potential%20of%0Afrequency-domain%20representations.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20framework%20named%20FMD-TransUNet%20for%20precise%20abdominal%20multi-organ%0Asegmentation.%20It%20innovatively%20integrates%20the%20Multi-axis%20External%20Weight%20Block%0A%28MEWB%29%20and%20the%20improved%20dual%20attention%20module%20%28DA%2B%29%20into%20the%20TransUNet%0Aframework.%20The%20MEWB%20extracts%20multi-axis%20frequency-domain%20features%20to%20capture%0Aboth%20global%20anatomical%20structures%20and%20local%20boundary%20details%2C%20providing%0Acomplementary%20information%20to%20spatial-domain%20representations.%20The%20DA%2B%20block%0Autilizes%20depthwise%20separable%20convolutions%20and%20incorporates%20spatial%20and%20channel%0Aattention%20mechanisms%20to%20enhance%20feature%20fusion%2C%20reduce%20redundant%20information%2C%0Aand%20narrow%20the%20semantic%20gap%20between%20the%20encoder%20and%20decoder.%20Experimental%0Avalidation%20on%20the%20Synapse%20dataset%20shows%20that%20FMD-TransUNet%20outperforms%20other%0Arecent%20state-of-the-art%20methods%2C%20achieving%20an%20average%20DSC%20of%2081.32%5C%25%20and%20a%20HD%0Aof%2016.35%20mm%20across%20eight%20abdominal%20organs.%20Compared%20to%20the%20baseline%20model%2C%20the%0Aaverage%20DSC%20increased%20by%203.84%5C%25%2C%20and%20the%20average%20HD%20decreased%20by%2015.34%20mm.%0AThese%20results%20demonstrate%20the%20effectiveness%20of%20FMD-TransUNet%20in%20improving%20the%0Aaccuracy%20of%20abdominal%20multi-organ%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16044v1&entry.124074799=Read"},
{"title": "Training More Robust Classification Model via Discriminative Loss and\n  Gaussian Noise Injection", "author": "Hai-Vy Nguyen and Fabrice Gamboa and Sixin Zhang and Reda Chhaibi and Serge Gratton and Thierry Giaccone", "abstract": "  Robustness of deep neural networks to input noise remains a critical\nchallenge, as naive noise injection often degrades accuracy on clean\n(uncorrupted) data. We propose a novel training framework that addresses this\ntrade-off through two complementary objectives. First, we introduce a loss\nfunction applied at the penultimate layer that explicitly enforces intra-class\ncompactness and increases the margin to analytically defined decision\nboundaries. This enhances feature discriminativeness and class separability for\nclean data. Second, we propose a class-wise feature alignment mechanism that\nbrings noisy data clusters closer to their clean counterparts. Furthermore, we\nprovide a theoretical analysis demonstrating that improving feature stability\nunder additive Gaussian noise implicitly reduces the curvature of the softmax\nloss landscape in input space, as measured by Hessian eigenvalues.This thus\nnaturally enhances robustness without explicit curvature penalties. Conversely,\nwe also theoretically show that lower curvatures lead to more robust models. We\nvalidate the effectiveness of our method on standard benchmarks and our custom\ndataset. Our approach significantly reinforces model robustness to various\nperturbations while maintaining high accuracy on clean data, advancing the\nunderstanding and practice of noise-robust deep learning.\n", "link": "http://arxiv.org/abs/2405.18499v3", "date": "2025-09-19", "relevancy": 2.0991, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.544}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%0A%20%20Gaussian%20Noise%20Injection&body=Title%3A%20Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%0A%20%20Gaussian%20Noise%20Injection%0AAuthor%3A%20Hai-Vy%20Nguyen%20and%20Fabrice%20Gamboa%20and%20Sixin%20Zhang%20and%20Reda%20Chhaibi%20and%20Serge%20Gratton%20and%20Thierry%20Giaccone%0AAbstract%3A%20%20%20Robustness%20of%20deep%20neural%20networks%20to%20input%20noise%20remains%20a%20critical%0Achallenge%2C%20as%20naive%20noise%20injection%20often%20degrades%20accuracy%20on%20clean%0A%28uncorrupted%29%20data.%20We%20propose%20a%20novel%20training%20framework%20that%20addresses%20this%0Atrade-off%20through%20two%20complementary%20objectives.%20First%2C%20we%20introduce%20a%20loss%0Afunction%20applied%20at%20the%20penultimate%20layer%20that%20explicitly%20enforces%20intra-class%0Acompactness%20and%20increases%20the%20margin%20to%20analytically%20defined%20decision%0Aboundaries.%20This%20enhances%20feature%20discriminativeness%20and%20class%20separability%20for%0Aclean%20data.%20Second%2C%20we%20propose%20a%20class-wise%20feature%20alignment%20mechanism%20that%0Abrings%20noisy%20data%20clusters%20closer%20to%20their%20clean%20counterparts.%20Furthermore%2C%20we%0Aprovide%20a%20theoretical%20analysis%20demonstrating%20that%20improving%20feature%20stability%0Aunder%20additive%20Gaussian%20noise%20implicitly%20reduces%20the%20curvature%20of%20the%20softmax%0Aloss%20landscape%20in%20input%20space%2C%20as%20measured%20by%20Hessian%20eigenvalues.This%20thus%0Anaturally%20enhances%20robustness%20without%20explicit%20curvature%20penalties.%20Conversely%2C%0Awe%20also%20theoretically%20show%20that%20lower%20curvatures%20lead%20to%20more%20robust%20models.%20We%0Avalidate%20the%20effectiveness%20of%20our%20method%20on%20standard%20benchmarks%20and%20our%20custom%0Adataset.%20Our%20approach%20significantly%20reinforces%20model%20robustness%20to%20various%0Aperturbations%20while%20maintaining%20high%20accuracy%20on%20clean%20data%2C%20advancing%20the%0Aunderstanding%20and%20practice%20of%20noise-robust%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18499v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520More%2520Robust%2520Classification%2520Model%2520via%2520Discriminative%2520Loss%2520and%250A%2520%2520Gaussian%2520Noise%2520Injection%26entry.906535625%3DHai-Vy%2520Nguyen%2520and%2520Fabrice%2520Gamboa%2520and%2520Sixin%2520Zhang%2520and%2520Reda%2520Chhaibi%2520and%2520Serge%2520Gratton%2520and%2520Thierry%2520Giaccone%26entry.1292438233%3D%2520%2520Robustness%2520of%2520deep%2520neural%2520networks%2520to%2520input%2520noise%2520remains%2520a%2520critical%250Achallenge%252C%2520as%2520naive%2520noise%2520injection%2520often%2520degrades%2520accuracy%2520on%2520clean%250A%2528uncorrupted%2529%2520data.%2520We%2520propose%2520a%2520novel%2520training%2520framework%2520that%2520addresses%2520this%250Atrade-off%2520through%2520two%2520complementary%2520objectives.%2520First%252C%2520we%2520introduce%2520a%2520loss%250Afunction%2520applied%2520at%2520the%2520penultimate%2520layer%2520that%2520explicitly%2520enforces%2520intra-class%250Acompactness%2520and%2520increases%2520the%2520margin%2520to%2520analytically%2520defined%2520decision%250Aboundaries.%2520This%2520enhances%2520feature%2520discriminativeness%2520and%2520class%2520separability%2520for%250Aclean%2520data.%2520Second%252C%2520we%2520propose%2520a%2520class-wise%2520feature%2520alignment%2520mechanism%2520that%250Abrings%2520noisy%2520data%2520clusters%2520closer%2520to%2520their%2520clean%2520counterparts.%2520Furthermore%252C%2520we%250Aprovide%2520a%2520theoretical%2520analysis%2520demonstrating%2520that%2520improving%2520feature%2520stability%250Aunder%2520additive%2520Gaussian%2520noise%2520implicitly%2520reduces%2520the%2520curvature%2520of%2520the%2520softmax%250Aloss%2520landscape%2520in%2520input%2520space%252C%2520as%2520measured%2520by%2520Hessian%2520eigenvalues.This%2520thus%250Anaturally%2520enhances%2520robustness%2520without%2520explicit%2520curvature%2520penalties.%2520Conversely%252C%250Awe%2520also%2520theoretically%2520show%2520that%2520lower%2520curvatures%2520lead%2520to%2520more%2520robust%2520models.%2520We%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520standard%2520benchmarks%2520and%2520our%2520custom%250Adataset.%2520Our%2520approach%2520significantly%2520reinforces%2520model%2520robustness%2520to%2520various%250Aperturbations%2520while%2520maintaining%2520high%2520accuracy%2520on%2520clean%2520data%252C%2520advancing%2520the%250Aunderstanding%2520and%2520practice%2520of%2520noise-robust%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18499v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20More%20Robust%20Classification%20Model%20via%20Discriminative%20Loss%20and%0A%20%20Gaussian%20Noise%20Injection&entry.906535625=Hai-Vy%20Nguyen%20and%20Fabrice%20Gamboa%20and%20Sixin%20Zhang%20and%20Reda%20Chhaibi%20and%20Serge%20Gratton%20and%20Thierry%20Giaccone&entry.1292438233=%20%20Robustness%20of%20deep%20neural%20networks%20to%20input%20noise%20remains%20a%20critical%0Achallenge%2C%20as%20naive%20noise%20injection%20often%20degrades%20accuracy%20on%20clean%0A%28uncorrupted%29%20data.%20We%20propose%20a%20novel%20training%20framework%20that%20addresses%20this%0Atrade-off%20through%20two%20complementary%20objectives.%20First%2C%20we%20introduce%20a%20loss%0Afunction%20applied%20at%20the%20penultimate%20layer%20that%20explicitly%20enforces%20intra-class%0Acompactness%20and%20increases%20the%20margin%20to%20analytically%20defined%20decision%0Aboundaries.%20This%20enhances%20feature%20discriminativeness%20and%20class%20separability%20for%0Aclean%20data.%20Second%2C%20we%20propose%20a%20class-wise%20feature%20alignment%20mechanism%20that%0Abrings%20noisy%20data%20clusters%20closer%20to%20their%20clean%20counterparts.%20Furthermore%2C%20we%0Aprovide%20a%20theoretical%20analysis%20demonstrating%20that%20improving%20feature%20stability%0Aunder%20additive%20Gaussian%20noise%20implicitly%20reduces%20the%20curvature%20of%20the%20softmax%0Aloss%20landscape%20in%20input%20space%2C%20as%20measured%20by%20Hessian%20eigenvalues.This%20thus%0Anaturally%20enhances%20robustness%20without%20explicit%20curvature%20penalties.%20Conversely%2C%0Awe%20also%20theoretically%20show%20that%20lower%20curvatures%20lead%20to%20more%20robust%20models.%20We%0Avalidate%20the%20effectiveness%20of%20our%20method%20on%20standard%20benchmarks%20and%20our%20custom%0Adataset.%20Our%20approach%20significantly%20reinforces%20model%20robustness%20to%20various%0Aperturbations%20while%20maintaining%20high%20accuracy%20on%20clean%20data%2C%20advancing%20the%0Aunderstanding%20and%20practice%20of%20noise-robust%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18499v3&entry.124074799=Read"},
{"title": "PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement\n  Learning", "author": "Ondrej Bajgar and Dewi S. W. Gould and Jonathon Liu and Alessandro Abate and Konstantinos Gatsis and Michael A. Osborne", "abstract": "  As AI systems become increasingly autonomous, reliably aligning their\ndecision-making with human preferences is essential. Inverse reinforcement\nlearning (IRL) offers a promising approach to infer preferences from\ndemonstrations. These preferences can then be used to produce an apprentice\npolicy that performs well on the demonstrated task. However, in domains like\nautonomous driving or robotics, where errors can have serious consequences, we\nneed not just good average performance but reliable policies with formal\nguarantees -- yet obtaining sufficient human demonstrations for reliability\nguarantees can be costly. Active IRL addresses this challenge by strategically\nselecting the most informative scenarios for human demonstration. We introduce\nPAC-EIG, an information-theoretic acquisition function that directly targets\nprobably-approximately-correct (PAC) guarantees for the learned policy --\nproviding the first such theoretical guarantee for active IRL with noisy expert\ndemonstrations. Our method maximises information gain about the regret of the\napprentice policy, efficiently identifying states requiring further\ndemonstration. We also present Reward-EIG as an alternative when learning the\nreward itself is the primary objective. Focusing on finite state-action spaces,\nwe prove convergence bounds, illustrate failure modes of prior heuristic\nmethods, and demonstrate our method's advantages experimentally.\n", "link": "http://arxiv.org/abs/2508.03693v2", "date": "2025-09-19", "relevancy": 2.0925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5263}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning&body=Title%3A%20PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ondrej%20Bajgar%20and%20Dewi%20S.%20W.%20Gould%20and%20Jonathon%20Liu%20and%20Alessandro%20Abate%20and%20Konstantinos%20Gatsis%20and%20Michael%20A.%20Osborne%0AAbstract%3A%20%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20reliably%20aligning%20their%0Adecision-making%20with%20human%20preferences%20is%20essential.%20Inverse%20reinforcement%0Alearning%20%28IRL%29%20offers%20a%20promising%20approach%20to%20infer%20preferences%20from%0Ademonstrations.%20These%20preferences%20can%20then%20be%20used%20to%20produce%20an%20apprentice%0Apolicy%20that%20performs%20well%20on%20the%20demonstrated%20task.%20However%2C%20in%20domains%20like%0Aautonomous%20driving%20or%20robotics%2C%20where%20errors%20can%20have%20serious%20consequences%2C%20we%0Aneed%20not%20just%20good%20average%20performance%20but%20reliable%20policies%20with%20formal%0Aguarantees%20--%20yet%20obtaining%20sufficient%20human%20demonstrations%20for%20reliability%0Aguarantees%20can%20be%20costly.%20Active%20IRL%20addresses%20this%20challenge%20by%20strategically%0Aselecting%20the%20most%20informative%20scenarios%20for%20human%20demonstration.%20We%20introduce%0APAC-EIG%2C%20an%20information-theoretic%20acquisition%20function%20that%20directly%20targets%0Aprobably-approximately-correct%20%28PAC%29%20guarantees%20for%20the%20learned%20policy%20--%0Aproviding%20the%20first%20such%20theoretical%20guarantee%20for%20active%20IRL%20with%20noisy%20expert%0Ademonstrations.%20Our%20method%20maximises%20information%20gain%20about%20the%20regret%20of%20the%0Aapprentice%20policy%2C%20efficiently%20identifying%20states%20requiring%20further%0Ademonstration.%20We%20also%20present%20Reward-EIG%20as%20an%20alternative%20when%20learning%20the%0Areward%20itself%20is%20the%20primary%20objective.%20Focusing%20on%20finite%20state-action%20spaces%2C%0Awe%20prove%20convergence%20bounds%2C%20illustrate%20failure%20modes%20of%20prior%20heuristic%0Amethods%2C%20and%20demonstrate%20our%20method%27s%20advantages%20experimentally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC%2520Apprenticeship%2520Learning%2520with%2520Bayesian%2520Active%2520Inverse%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DOndrej%2520Bajgar%2520and%2520Dewi%2520S.%2520W.%2520Gould%2520and%2520Jonathon%2520Liu%2520and%2520Alessandro%2520Abate%2520and%2520Konstantinos%2520Gatsis%2520and%2520Michael%2520A.%2520Osborne%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520become%2520increasingly%2520autonomous%252C%2520reliably%2520aligning%2520their%250Adecision-making%2520with%2520human%2520preferences%2520is%2520essential.%2520Inverse%2520reinforcement%250Alearning%2520%2528IRL%2529%2520offers%2520a%2520promising%2520approach%2520to%2520infer%2520preferences%2520from%250Ademonstrations.%2520These%2520preferences%2520can%2520then%2520be%2520used%2520to%2520produce%2520an%2520apprentice%250Apolicy%2520that%2520performs%2520well%2520on%2520the%2520demonstrated%2520task.%2520However%252C%2520in%2520domains%2520like%250Aautonomous%2520driving%2520or%2520robotics%252C%2520where%2520errors%2520can%2520have%2520serious%2520consequences%252C%2520we%250Aneed%2520not%2520just%2520good%2520average%2520performance%2520but%2520reliable%2520policies%2520with%2520formal%250Aguarantees%2520--%2520yet%2520obtaining%2520sufficient%2520human%2520demonstrations%2520for%2520reliability%250Aguarantees%2520can%2520be%2520costly.%2520Active%2520IRL%2520addresses%2520this%2520challenge%2520by%2520strategically%250Aselecting%2520the%2520most%2520informative%2520scenarios%2520for%2520human%2520demonstration.%2520We%2520introduce%250APAC-EIG%252C%2520an%2520information-theoretic%2520acquisition%2520function%2520that%2520directly%2520targets%250Aprobably-approximately-correct%2520%2528PAC%2529%2520guarantees%2520for%2520the%2520learned%2520policy%2520--%250Aproviding%2520the%2520first%2520such%2520theoretical%2520guarantee%2520for%2520active%2520IRL%2520with%2520noisy%2520expert%250Ademonstrations.%2520Our%2520method%2520maximises%2520information%2520gain%2520about%2520the%2520regret%2520of%2520the%250Aapprentice%2520policy%252C%2520efficiently%2520identifying%2520states%2520requiring%2520further%250Ademonstration.%2520We%2520also%2520present%2520Reward-EIG%2520as%2520an%2520alternative%2520when%2520learning%2520the%250Areward%2520itself%2520is%2520the%2520primary%2520objective.%2520Focusing%2520on%2520finite%2520state-action%2520spaces%252C%250Awe%2520prove%2520convergence%2520bounds%252C%2520illustrate%2520failure%2520modes%2520of%2520prior%2520heuristic%250Amethods%252C%2520and%2520demonstrate%2520our%2520method%2527s%2520advantages%2520experimentally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC%20Apprenticeship%20Learning%20with%20Bayesian%20Active%20Inverse%20Reinforcement%0A%20%20Learning&entry.906535625=Ondrej%20Bajgar%20and%20Dewi%20S.%20W.%20Gould%20and%20Jonathon%20Liu%20and%20Alessandro%20Abate%20and%20Konstantinos%20Gatsis%20and%20Michael%20A.%20Osborne&entry.1292438233=%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20reliably%20aligning%20their%0Adecision-making%20with%20human%20preferences%20is%20essential.%20Inverse%20reinforcement%0Alearning%20%28IRL%29%20offers%20a%20promising%20approach%20to%20infer%20preferences%20from%0Ademonstrations.%20These%20preferences%20can%20then%20be%20used%20to%20produce%20an%20apprentice%0Apolicy%20that%20performs%20well%20on%20the%20demonstrated%20task.%20However%2C%20in%20domains%20like%0Aautonomous%20driving%20or%20robotics%2C%20where%20errors%20can%20have%20serious%20consequences%2C%20we%0Aneed%20not%20just%20good%20average%20performance%20but%20reliable%20policies%20with%20formal%0Aguarantees%20--%20yet%20obtaining%20sufficient%20human%20demonstrations%20for%20reliability%0Aguarantees%20can%20be%20costly.%20Active%20IRL%20addresses%20this%20challenge%20by%20strategically%0Aselecting%20the%20most%20informative%20scenarios%20for%20human%20demonstration.%20We%20introduce%0APAC-EIG%2C%20an%20information-theoretic%20acquisition%20function%20that%20directly%20targets%0Aprobably-approximately-correct%20%28PAC%29%20guarantees%20for%20the%20learned%20policy%20--%0Aproviding%20the%20first%20such%20theoretical%20guarantee%20for%20active%20IRL%20with%20noisy%20expert%0Ademonstrations.%20Our%20method%20maximises%20information%20gain%20about%20the%20regret%20of%20the%0Aapprentice%20policy%2C%20efficiently%20identifying%20states%20requiring%20further%0Ademonstration.%20We%20also%20present%20Reward-EIG%20as%20an%20alternative%20when%20learning%20the%0Areward%20itself%20is%20the%20primary%20objective.%20Focusing%20on%20finite%20state-action%20spaces%2C%0Awe%20prove%20convergence%20bounds%2C%20illustrate%20failure%20modes%20of%20prior%20heuristic%0Amethods%2C%20and%20demonstrate%20our%20method%27s%20advantages%20experimentally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03693v2&entry.124074799=Read"},
{"title": "Activation Space Interventions Can Be Transferred Between Large Language\n  Models", "author": "Narmeen Oozeer and Dhruv Nathawani and Nirmalendu Prakash and Michael Lan and Abir Harrasse and Amirali Abdullah", "abstract": "  The study of representation universality in AI models reveals growing\nconvergence across domains, modalities, and architectures. However, the\npractical applications of representation universality remain largely\nunexplored. We bridge this gap by demonstrating that safety interventions can\nbe transferred between models through learned mappings of their shared\nactivation spaces. We demonstrate this approach on two well-established AI\nsafety tasks: backdoor removal and refusal of harmful prompts, showing\nsuccessful transfer of steering vectors that alter the models' outputs in a\npredictable way. Additionally, we propose a new task, \\textit{corrupted\ncapabilities}, where models are fine-tuned to embed knowledge tied to a\nbackdoor. This tests their ability to separate useful skills from backdoors,\nreflecting real-world challenges. Extensive experiments across Llama, Qwen and\nGemma model families show that our method enables using smaller models to\nefficiently align larger ones. Furthermore, we demonstrate that autoencoder\nmappings between base and fine-tuned models can serve as reliable ``lightweight\nsafety switches\", allowing dynamic toggling between model behaviors.\n", "link": "http://arxiv.org/abs/2503.04429v4", "date": "2025-09-19", "relevancy": 2.0922, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models&body=Title%3A%20Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Narmeen%20Oozeer%20and%20Dhruv%20Nathawani%20and%20Nirmalendu%20Prakash%20and%20Michael%20Lan%20and%20Abir%20Harrasse%20and%20Amirali%20Abdullah%0AAbstract%3A%20%20%20The%20study%20of%20representation%20universality%20in%20AI%20models%20reveals%20growing%0Aconvergence%20across%20domains%2C%20modalities%2C%20and%20architectures.%20However%2C%20the%0Apractical%20applications%20of%20representation%20universality%20remain%20largely%0Aunexplored.%20We%20bridge%20this%20gap%20by%20demonstrating%20that%20safety%20interventions%20can%0Abe%20transferred%20between%20models%20through%20learned%20mappings%20of%20their%20shared%0Aactivation%20spaces.%20We%20demonstrate%20this%20approach%20on%20two%20well-established%20AI%0Asafety%20tasks%3A%20backdoor%20removal%20and%20refusal%20of%20harmful%20prompts%2C%20showing%0Asuccessful%20transfer%20of%20steering%20vectors%20that%20alter%20the%20models%27%20outputs%20in%20a%0Apredictable%20way.%20Additionally%2C%20we%20propose%20a%20new%20task%2C%20%5Ctextit%7Bcorrupted%0Acapabilities%7D%2C%20where%20models%20are%20fine-tuned%20to%20embed%20knowledge%20tied%20to%20a%0Abackdoor.%20This%20tests%20their%20ability%20to%20separate%20useful%20skills%20from%20backdoors%2C%0Areflecting%20real-world%20challenges.%20Extensive%20experiments%20across%20Llama%2C%20Qwen%20and%0AGemma%20model%20families%20show%20that%20our%20method%20enables%20using%20smaller%20models%20to%0Aefficiently%20align%20larger%20ones.%20Furthermore%2C%20we%20demonstrate%20that%20autoencoder%0Amappings%20between%20base%20and%20fine-tuned%20models%20can%20serve%20as%20reliable%20%60%60lightweight%0Asafety%20switches%22%2C%20allowing%20dynamic%20toggling%20between%20model%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04429v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Space%2520Interventions%2520Can%2520Be%2520Transferred%2520Between%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DNarmeen%2520Oozeer%2520and%2520Dhruv%2520Nathawani%2520and%2520Nirmalendu%2520Prakash%2520and%2520Michael%2520Lan%2520and%2520Abir%2520Harrasse%2520and%2520Amirali%2520Abdullah%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520representation%2520universality%2520in%2520AI%2520models%2520reveals%2520growing%250Aconvergence%2520across%2520domains%252C%2520modalities%252C%2520and%2520architectures.%2520However%252C%2520the%250Apractical%2520applications%2520of%2520representation%2520universality%2520remain%2520largely%250Aunexplored.%2520We%2520bridge%2520this%2520gap%2520by%2520demonstrating%2520that%2520safety%2520interventions%2520can%250Abe%2520transferred%2520between%2520models%2520through%2520learned%2520mappings%2520of%2520their%2520shared%250Aactivation%2520spaces.%2520We%2520demonstrate%2520this%2520approach%2520on%2520two%2520well-established%2520AI%250Asafety%2520tasks%253A%2520backdoor%2520removal%2520and%2520refusal%2520of%2520harmful%2520prompts%252C%2520showing%250Asuccessful%2520transfer%2520of%2520steering%2520vectors%2520that%2520alter%2520the%2520models%2527%2520outputs%2520in%2520a%250Apredictable%2520way.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520task%252C%2520%255Ctextit%257Bcorrupted%250Acapabilities%257D%252C%2520where%2520models%2520are%2520fine-tuned%2520to%2520embed%2520knowledge%2520tied%2520to%2520a%250Abackdoor.%2520This%2520tests%2520their%2520ability%2520to%2520separate%2520useful%2520skills%2520from%2520backdoors%252C%250Areflecting%2520real-world%2520challenges.%2520Extensive%2520experiments%2520across%2520Llama%252C%2520Qwen%2520and%250AGemma%2520model%2520families%2520show%2520that%2520our%2520method%2520enables%2520using%2520smaller%2520models%2520to%250Aefficiently%2520align%2520larger%2520ones.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520autoencoder%250Amappings%2520between%2520base%2520and%2520fine-tuned%2520models%2520can%2520serve%2520as%2520reliable%2520%2560%2560lightweight%250Asafety%2520switches%2522%252C%2520allowing%2520dynamic%2520toggling%2520between%2520model%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04429v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Space%20Interventions%20Can%20Be%20Transferred%20Between%20Large%20Language%0A%20%20Models&entry.906535625=Narmeen%20Oozeer%20and%20Dhruv%20Nathawani%20and%20Nirmalendu%20Prakash%20and%20Michael%20Lan%20and%20Abir%20Harrasse%20and%20Amirali%20Abdullah&entry.1292438233=%20%20The%20study%20of%20representation%20universality%20in%20AI%20models%20reveals%20growing%0Aconvergence%20across%20domains%2C%20modalities%2C%20and%20architectures.%20However%2C%20the%0Apractical%20applications%20of%20representation%20universality%20remain%20largely%0Aunexplored.%20We%20bridge%20this%20gap%20by%20demonstrating%20that%20safety%20interventions%20can%0Abe%20transferred%20between%20models%20through%20learned%20mappings%20of%20their%20shared%0Aactivation%20spaces.%20We%20demonstrate%20this%20approach%20on%20two%20well-established%20AI%0Asafety%20tasks%3A%20backdoor%20removal%20and%20refusal%20of%20harmful%20prompts%2C%20showing%0Asuccessful%20transfer%20of%20steering%20vectors%20that%20alter%20the%20models%27%20outputs%20in%20a%0Apredictable%20way.%20Additionally%2C%20we%20propose%20a%20new%20task%2C%20%5Ctextit%7Bcorrupted%0Acapabilities%7D%2C%20where%20models%20are%20fine-tuned%20to%20embed%20knowledge%20tied%20to%20a%0Abackdoor.%20This%20tests%20their%20ability%20to%20separate%20useful%20skills%20from%20backdoors%2C%0Areflecting%20real-world%20challenges.%20Extensive%20experiments%20across%20Llama%2C%20Qwen%20and%0AGemma%20model%20families%20show%20that%20our%20method%20enables%20using%20smaller%20models%20to%0Aefficiently%20align%20larger%20ones.%20Furthermore%2C%20we%20demonstrate%20that%20autoencoder%0Amappings%20between%20base%20and%20fine-tuned%20models%20can%20serve%20as%20reliable%20%60%60lightweight%0Asafety%20switches%22%2C%20allowing%20dynamic%20toggling%20between%20model%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04429v4&entry.124074799=Read"},
{"title": "Automated Cyber Defense with Generalizable Graph-based Reinforcement\n  Learning Agents", "author": "Isaiah J. King and Benjamin Bowman and H. Howie Huang", "abstract": "  Deep reinforcement learning (RL) is emerging as a viable strategy for\nautomated cyber defense (ACD). The traditional RL approach represents networks\nas a list of computers in various states of safety or threat. Unfortunately,\nthese models are forced to overfit to specific network topologies, rendering\nthem ineffective when faced with even small environmental perturbations. In\nthis work, we frame ACD as a two-player context-based partially observable\nMarkov decision problem with observations represented as attributed graphs.\nThis approach allows our agents to reason through the lens of relational\ninductive bias. Agents learn how to reason about hosts interacting with other\nsystem entities in a more general manner, and their actions are understood as\nedits to the graph representing the environment. By introducing this bias, we\nwill show that our agents can better reason about the states of networks and\nzero-shot adapt to new ones. We show that this approach outperforms the\nstate-of-the-art by a wide margin, and makes our agents capable of defending\nnever-before-seen networks against a wide range of adversaries in a variety of\ncomplex, and multi-agent environments.\n", "link": "http://arxiv.org/abs/2509.16151v1", "date": "2025-09-19", "relevancy": 2.0884, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5746}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5144}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Cyber%20Defense%20with%20Generalizable%20Graph-based%20Reinforcement%0A%20%20Learning%20Agents&body=Title%3A%20Automated%20Cyber%20Defense%20with%20Generalizable%20Graph-based%20Reinforcement%0A%20%20Learning%20Agents%0AAuthor%3A%20Isaiah%20J.%20King%20and%20Benjamin%20Bowman%20and%20H.%20Howie%20Huang%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20%28RL%29%20is%20emerging%20as%20a%20viable%20strategy%20for%0Aautomated%20cyber%20defense%20%28ACD%29.%20The%20traditional%20RL%20approach%20represents%20networks%0Aas%20a%20list%20of%20computers%20in%20various%20states%20of%20safety%20or%20threat.%20Unfortunately%2C%0Athese%20models%20are%20forced%20to%20overfit%20to%20specific%20network%20topologies%2C%20rendering%0Athem%20ineffective%20when%20faced%20with%20even%20small%20environmental%20perturbations.%20In%0Athis%20work%2C%20we%20frame%20ACD%20as%20a%20two-player%20context-based%20partially%20observable%0AMarkov%20decision%20problem%20with%20observations%20represented%20as%20attributed%20graphs.%0AThis%20approach%20allows%20our%20agents%20to%20reason%20through%20the%20lens%20of%20relational%0Ainductive%20bias.%20Agents%20learn%20how%20to%20reason%20about%20hosts%20interacting%20with%20other%0Asystem%20entities%20in%20a%20more%20general%20manner%2C%20and%20their%20actions%20are%20understood%20as%0Aedits%20to%20the%20graph%20representing%20the%20environment.%20By%20introducing%20this%20bias%2C%20we%0Awill%20show%20that%20our%20agents%20can%20better%20reason%20about%20the%20states%20of%20networks%20and%0Azero-shot%20adapt%20to%20new%20ones.%20We%20show%20that%20this%20approach%20outperforms%20the%0Astate-of-the-art%20by%20a%20wide%20margin%2C%20and%20makes%20our%20agents%20capable%20of%20defending%0Anever-before-seen%20networks%20against%20a%20wide%20range%20of%20adversaries%20in%20a%20variety%20of%0Acomplex%2C%20and%20multi-agent%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Cyber%2520Defense%2520with%2520Generalizable%2520Graph-based%2520Reinforcement%250A%2520%2520Learning%2520Agents%26entry.906535625%3DIsaiah%2520J.%2520King%2520and%2520Benjamin%2520Bowman%2520and%2520H.%2520Howie%2520Huang%26entry.1292438233%3D%2520%2520Deep%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520emerging%2520as%2520a%2520viable%2520strategy%2520for%250Aautomated%2520cyber%2520defense%2520%2528ACD%2529.%2520The%2520traditional%2520RL%2520approach%2520represents%2520networks%250Aas%2520a%2520list%2520of%2520computers%2520in%2520various%2520states%2520of%2520safety%2520or%2520threat.%2520Unfortunately%252C%250Athese%2520models%2520are%2520forced%2520to%2520overfit%2520to%2520specific%2520network%2520topologies%252C%2520rendering%250Athem%2520ineffective%2520when%2520faced%2520with%2520even%2520small%2520environmental%2520perturbations.%2520In%250Athis%2520work%252C%2520we%2520frame%2520ACD%2520as%2520a%2520two-player%2520context-based%2520partially%2520observable%250AMarkov%2520decision%2520problem%2520with%2520observations%2520represented%2520as%2520attributed%2520graphs.%250AThis%2520approach%2520allows%2520our%2520agents%2520to%2520reason%2520through%2520the%2520lens%2520of%2520relational%250Ainductive%2520bias.%2520Agents%2520learn%2520how%2520to%2520reason%2520about%2520hosts%2520interacting%2520with%2520other%250Asystem%2520entities%2520in%2520a%2520more%2520general%2520manner%252C%2520and%2520their%2520actions%2520are%2520understood%2520as%250Aedits%2520to%2520the%2520graph%2520representing%2520the%2520environment.%2520By%2520introducing%2520this%2520bias%252C%2520we%250Awill%2520show%2520that%2520our%2520agents%2520can%2520better%2520reason%2520about%2520the%2520states%2520of%2520networks%2520and%250Azero-shot%2520adapt%2520to%2520new%2520ones.%2520We%2520show%2520that%2520this%2520approach%2520outperforms%2520the%250Astate-of-the-art%2520by%2520a%2520wide%2520margin%252C%2520and%2520makes%2520our%2520agents%2520capable%2520of%2520defending%250Anever-before-seen%2520networks%2520against%2520a%2520wide%2520range%2520of%2520adversaries%2520in%2520a%2520variety%2520of%250Acomplex%252C%2520and%2520multi-agent%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Cyber%20Defense%20with%20Generalizable%20Graph-based%20Reinforcement%0A%20%20Learning%20Agents&entry.906535625=Isaiah%20J.%20King%20and%20Benjamin%20Bowman%20and%20H.%20Howie%20Huang&entry.1292438233=%20%20Deep%20reinforcement%20learning%20%28RL%29%20is%20emerging%20as%20a%20viable%20strategy%20for%0Aautomated%20cyber%20defense%20%28ACD%29.%20The%20traditional%20RL%20approach%20represents%20networks%0Aas%20a%20list%20of%20computers%20in%20various%20states%20of%20safety%20or%20threat.%20Unfortunately%2C%0Athese%20models%20are%20forced%20to%20overfit%20to%20specific%20network%20topologies%2C%20rendering%0Athem%20ineffective%20when%20faced%20with%20even%20small%20environmental%20perturbations.%20In%0Athis%20work%2C%20we%20frame%20ACD%20as%20a%20two-player%20context-based%20partially%20observable%0AMarkov%20decision%20problem%20with%20observations%20represented%20as%20attributed%20graphs.%0AThis%20approach%20allows%20our%20agents%20to%20reason%20through%20the%20lens%20of%20relational%0Ainductive%20bias.%20Agents%20learn%20how%20to%20reason%20about%20hosts%20interacting%20with%20other%0Asystem%20entities%20in%20a%20more%20general%20manner%2C%20and%20their%20actions%20are%20understood%20as%0Aedits%20to%20the%20graph%20representing%20the%20environment.%20By%20introducing%20this%20bias%2C%20we%0Awill%20show%20that%20our%20agents%20can%20better%20reason%20about%20the%20states%20of%20networks%20and%0Azero-shot%20adapt%20to%20new%20ones.%20We%20show%20that%20this%20approach%20outperforms%20the%0Astate-of-the-art%20by%20a%20wide%20margin%2C%20and%20makes%20our%20agents%20capable%20of%20defending%0Anever-before-seen%20networks%20against%20a%20wide%20range%20of%20adversaries%20in%20a%20variety%20of%0Acomplex%2C%20and%20multi-agent%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16151v1&entry.124074799=Read"},
{"title": "Efficient Real-time Refinement of Language Model Text Generation", "author": "Joonho Ko and Jinheon Baek and Sung Ju Hwang", "abstract": "  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n", "link": "http://arxiv.org/abs/2501.07824v5", "date": "2025-09-19", "relevancy": 2.0768, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5315}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5271}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Real-time%20Refinement%20of%20Language%20Model%20Text%20Generation&body=Title%3A%20Efficient%20Real-time%20Refinement%20of%20Language%20Model%20Text%20Generation%0AAuthor%3A%20Joonho%20Ko%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20across%20a%20wide%0Arange%20of%20natural%20language%20tasks.%20However%2C%20a%20critical%20challenge%20remains%20in%20that%0Athey%20sometimes%20generate%20factually%20incorrect%20answers.%20To%20address%20this%2C%20while%0Amany%20previous%20work%20has%20focused%20on%20identifying%20errors%20in%20their%20generation%20and%0Afurther%20refining%20them%2C%20they%20are%20slow%20in%20deployment%20since%20they%20are%20designed%20to%0Averify%20the%20response%20from%20LLMs%20only%20after%20their%20entire%20generation%20%28from%20the%0Afirst%20to%20last%20tokens%29%20is%20done.%20Further%2C%20we%20observe%20that%20once%20LLMs%20generate%0Aincorrect%20tokens%20early%20on%2C%20there%20is%20a%20higher%20likelihood%20that%20subsequent%20tokens%0Awill%20also%20be%20factually%20incorrect.%20To%20this%20end%2C%20in%20this%20work%2C%20we%20propose%0AStreaming-VR%20%28Streaming%20Verification%20and%20Refinement%29%2C%20a%20novel%20approach%20designed%0Ato%20enhance%20the%20efficiency%20of%20verification%20and%20refinement%20of%20LLM%20outputs.%0ASpecifically%2C%20the%20proposed%20Streaming-VR%20enables%20on-the-fly%20verification%20and%0Acorrection%20of%20tokens%20as%20they%20are%20being%20generated%2C%20similar%20to%20a%20streaming%0Aprocess%2C%20ensuring%20that%20each%20subset%20of%20tokens%20is%20checked%20and%20refined%20in%0Areal-time%20by%20another%20LLM%20as%20the%20LLM%20constructs%20its%20response.%20Through%0Acomprehensive%20evaluations%20on%20multiple%20datasets%2C%20we%20demonstrate%20that%20our%0Aapproach%20not%20only%20enhances%20the%20factual%20accuracy%20of%20LLMs%2C%20but%20also%20offers%20a%20more%0Aefficient%20solution%20compared%20to%20prior%20refinement%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07824v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Real-time%2520Refinement%2520of%2520Language%2520Model%2520Text%2520Generation%26entry.906535625%3DJoonho%2520Ko%2520and%2520Jinheon%2520Baek%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520natural%2520language%2520tasks.%2520However%252C%2520a%2520critical%2520challenge%2520remains%2520in%2520that%250Athey%2520sometimes%2520generate%2520factually%2520incorrect%2520answers.%2520To%2520address%2520this%252C%2520while%250Amany%2520previous%2520work%2520has%2520focused%2520on%2520identifying%2520errors%2520in%2520their%2520generation%2520and%250Afurther%2520refining%2520them%252C%2520they%2520are%2520slow%2520in%2520deployment%2520since%2520they%2520are%2520designed%2520to%250Averify%2520the%2520response%2520from%2520LLMs%2520only%2520after%2520their%2520entire%2520generation%2520%2528from%2520the%250Afirst%2520to%2520last%2520tokens%2529%2520is%2520done.%2520Further%252C%2520we%2520observe%2520that%2520once%2520LLMs%2520generate%250Aincorrect%2520tokens%2520early%2520on%252C%2520there%2520is%2520a%2520higher%2520likelihood%2520that%2520subsequent%2520tokens%250Awill%2520also%2520be%2520factually%2520incorrect.%2520To%2520this%2520end%252C%2520in%2520this%2520work%252C%2520we%2520propose%250AStreaming-VR%2520%2528Streaming%2520Verification%2520and%2520Refinement%2529%252C%2520a%2520novel%2520approach%2520designed%250Ato%2520enhance%2520the%2520efficiency%2520of%2520verification%2520and%2520refinement%2520of%2520LLM%2520outputs.%250ASpecifically%252C%2520the%2520proposed%2520Streaming-VR%2520enables%2520on-the-fly%2520verification%2520and%250Acorrection%2520of%2520tokens%2520as%2520they%2520are%2520being%2520generated%252C%2520similar%2520to%2520a%2520streaming%250Aprocess%252C%2520ensuring%2520that%2520each%2520subset%2520of%2520tokens%2520is%2520checked%2520and%2520refined%2520in%250Areal-time%2520by%2520another%2520LLM%2520as%2520the%2520LLM%2520constructs%2520its%2520response.%2520Through%250Acomprehensive%2520evaluations%2520on%2520multiple%2520datasets%252C%2520we%2520demonstrate%2520that%2520our%250Aapproach%2520not%2520only%2520enhances%2520the%2520factual%2520accuracy%2520of%2520LLMs%252C%2520but%2520also%2520offers%2520a%2520more%250Aefficient%2520solution%2520compared%2520to%2520prior%2520refinement%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07824v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Real-time%20Refinement%20of%20Language%20Model%20Text%20Generation&entry.906535625=Joonho%20Ko%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20across%20a%20wide%0Arange%20of%20natural%20language%20tasks.%20However%2C%20a%20critical%20challenge%20remains%20in%20that%0Athey%20sometimes%20generate%20factually%20incorrect%20answers.%20To%20address%20this%2C%20while%0Amany%20previous%20work%20has%20focused%20on%20identifying%20errors%20in%20their%20generation%20and%0Afurther%20refining%20them%2C%20they%20are%20slow%20in%20deployment%20since%20they%20are%20designed%20to%0Averify%20the%20response%20from%20LLMs%20only%20after%20their%20entire%20generation%20%28from%20the%0Afirst%20to%20last%20tokens%29%20is%20done.%20Further%2C%20we%20observe%20that%20once%20LLMs%20generate%0Aincorrect%20tokens%20early%20on%2C%20there%20is%20a%20higher%20likelihood%20that%20subsequent%20tokens%0Awill%20also%20be%20factually%20incorrect.%20To%20this%20end%2C%20in%20this%20work%2C%20we%20propose%0AStreaming-VR%20%28Streaming%20Verification%20and%20Refinement%29%2C%20a%20novel%20approach%20designed%0Ato%20enhance%20the%20efficiency%20of%20verification%20and%20refinement%20of%20LLM%20outputs.%0ASpecifically%2C%20the%20proposed%20Streaming-VR%20enables%20on-the-fly%20verification%20and%0Acorrection%20of%20tokens%20as%20they%20are%20being%20generated%2C%20similar%20to%20a%20streaming%0Aprocess%2C%20ensuring%20that%20each%20subset%20of%20tokens%20is%20checked%20and%20refined%20in%0Areal-time%20by%20another%20LLM%20as%20the%20LLM%20constructs%20its%20response.%20Through%0Acomprehensive%20evaluations%20on%20multiple%20datasets%2C%20we%20demonstrate%20that%20our%0Aapproach%20not%20only%20enhances%20the%20factual%20accuracy%20of%20LLMs%2C%20but%20also%20offers%20a%20more%0Aefficient%20solution%20compared%20to%20prior%20refinement%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07824v5&entry.124074799=Read"},
{"title": "Space Group Equivariant Crystal Diffusion", "author": "Rees Chang and Angela Pak and Alex Guerra and Ni Zhan and Nick Richardson and Elif Ertekin and Ryan P. Adams", "abstract": "  Accelerating inverse design of crystalline materials with generative models\nhas significant implications for a range of technologies. Unlike other atomic\nsystems, 3D crystals are invariant to discrete groups of isometries called the\nspace groups. Crucially, these space group symmetries are known to heavily\ninfluence materials properties. We propose SGEquiDiff, a crystal generative\nmodel which naturally handles space group constraints with space group\ninvariant likelihoods. SGEquiD-iff consists of an SE(3)-invariant, telescoping\ndiscrete sampler of crystal lattices; permutation-invariant, transformer-based\nautoregressive sampling of Wyckoff positions, elements, and numbers of\nsymmetrically unique atoms; and space group equivariant diffusion of atomic\ncoordinates. We show that space group equivariant vector fields automatically\nlive in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves\nstate-of-the-art performance on standard benchmark datasets as assessed by\nquantitative proxy metrics and quantum mechanical calculations. Our code is\navailable at https://github.com/rees-c/sgequidiff.\n", "link": "http://arxiv.org/abs/2505.10994v2", "date": "2025-09-19", "relevancy": 2.0726, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.52}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.52}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Space%20Group%20Equivariant%20Crystal%20Diffusion&body=Title%3A%20Space%20Group%20Equivariant%20Crystal%20Diffusion%0AAuthor%3A%20Rees%20Chang%20and%20Angela%20Pak%20and%20Alex%20Guerra%20and%20Ni%20Zhan%20and%20Nick%20Richardson%20and%20Elif%20Ertekin%20and%20Ryan%20P.%20Adams%0AAbstract%3A%20%20%20Accelerating%20inverse%20design%20of%20crystalline%20materials%20with%20generative%20models%0Ahas%20significant%20implications%20for%20a%20range%20of%20technologies.%20Unlike%20other%20atomic%0Asystems%2C%203D%20crystals%20are%20invariant%20to%20discrete%20groups%20of%20isometries%20called%20the%0Aspace%20groups.%20Crucially%2C%20these%20space%20group%20symmetries%20are%20known%20to%20heavily%0Ainfluence%20materials%20properties.%20We%20propose%20SGEquiDiff%2C%20a%20crystal%20generative%0Amodel%20which%20naturally%20handles%20space%20group%20constraints%20with%20space%20group%0Ainvariant%20likelihoods.%20SGEquiD-iff%20consists%20of%20an%20SE%283%29-invariant%2C%20telescoping%0Adiscrete%20sampler%20of%20crystal%20lattices%3B%20permutation-invariant%2C%20transformer-based%0Aautoregressive%20sampling%20of%20Wyckoff%20positions%2C%20elements%2C%20and%20numbers%20of%0Asymmetrically%20unique%20atoms%3B%20and%20space%20group%20equivariant%20diffusion%20of%20atomic%0Acoordinates.%20We%20show%20that%20space%20group%20equivariant%20vector%20fields%20automatically%0Alive%20in%20the%20tangent%20spaces%20of%20the%20Wyckoff%20positions.%20SGEquiDiff%20achieves%0Astate-of-the-art%20performance%20on%20standard%20benchmark%20datasets%20as%20assessed%20by%0Aquantitative%20proxy%20metrics%20and%20quantum%20mechanical%20calculations.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/rees-c/sgequidiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpace%2520Group%2520Equivariant%2520Crystal%2520Diffusion%26entry.906535625%3DRees%2520Chang%2520and%2520Angela%2520Pak%2520and%2520Alex%2520Guerra%2520and%2520Ni%2520Zhan%2520and%2520Nick%2520Richardson%2520and%2520Elif%2520Ertekin%2520and%2520Ryan%2520P.%2520Adams%26entry.1292438233%3D%2520%2520Accelerating%2520inverse%2520design%2520of%2520crystalline%2520materials%2520with%2520generative%2520models%250Ahas%2520significant%2520implications%2520for%2520a%2520range%2520of%2520technologies.%2520Unlike%2520other%2520atomic%250Asystems%252C%25203D%2520crystals%2520are%2520invariant%2520to%2520discrete%2520groups%2520of%2520isometries%2520called%2520the%250Aspace%2520groups.%2520Crucially%252C%2520these%2520space%2520group%2520symmetries%2520are%2520known%2520to%2520heavily%250Ainfluence%2520materials%2520properties.%2520We%2520propose%2520SGEquiDiff%252C%2520a%2520crystal%2520generative%250Amodel%2520which%2520naturally%2520handles%2520space%2520group%2520constraints%2520with%2520space%2520group%250Ainvariant%2520likelihoods.%2520SGEquiD-iff%2520consists%2520of%2520an%2520SE%25283%2529-invariant%252C%2520telescoping%250Adiscrete%2520sampler%2520of%2520crystal%2520lattices%253B%2520permutation-invariant%252C%2520transformer-based%250Aautoregressive%2520sampling%2520of%2520Wyckoff%2520positions%252C%2520elements%252C%2520and%2520numbers%2520of%250Asymmetrically%2520unique%2520atoms%253B%2520and%2520space%2520group%2520equivariant%2520diffusion%2520of%2520atomic%250Acoordinates.%2520We%2520show%2520that%2520space%2520group%2520equivariant%2520vector%2520fields%2520automatically%250Alive%2520in%2520the%2520tangent%2520spaces%2520of%2520the%2520Wyckoff%2520positions.%2520SGEquiDiff%2520achieves%250Astate-of-the-art%2520performance%2520on%2520standard%2520benchmark%2520datasets%2520as%2520assessed%2520by%250Aquantitative%2520proxy%2520metrics%2520and%2520quantum%2520mechanical%2520calculations.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/rees-c/sgequidiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Space%20Group%20Equivariant%20Crystal%20Diffusion&entry.906535625=Rees%20Chang%20and%20Angela%20Pak%20and%20Alex%20Guerra%20and%20Ni%20Zhan%20and%20Nick%20Richardson%20and%20Elif%20Ertekin%20and%20Ryan%20P.%20Adams&entry.1292438233=%20%20Accelerating%20inverse%20design%20of%20crystalline%20materials%20with%20generative%20models%0Ahas%20significant%20implications%20for%20a%20range%20of%20technologies.%20Unlike%20other%20atomic%0Asystems%2C%203D%20crystals%20are%20invariant%20to%20discrete%20groups%20of%20isometries%20called%20the%0Aspace%20groups.%20Crucially%2C%20these%20space%20group%20symmetries%20are%20known%20to%20heavily%0Ainfluence%20materials%20properties.%20We%20propose%20SGEquiDiff%2C%20a%20crystal%20generative%0Amodel%20which%20naturally%20handles%20space%20group%20constraints%20with%20space%20group%0Ainvariant%20likelihoods.%20SGEquiD-iff%20consists%20of%20an%20SE%283%29-invariant%2C%20telescoping%0Adiscrete%20sampler%20of%20crystal%20lattices%3B%20permutation-invariant%2C%20transformer-based%0Aautoregressive%20sampling%20of%20Wyckoff%20positions%2C%20elements%2C%20and%20numbers%20of%0Asymmetrically%20unique%20atoms%3B%20and%20space%20group%20equivariant%20diffusion%20of%20atomic%0Acoordinates.%20We%20show%20that%20space%20group%20equivariant%20vector%20fields%20automatically%0Alive%20in%20the%20tangent%20spaces%20of%20the%20Wyckoff%20positions.%20SGEquiDiff%20achieves%0Astate-of-the-art%20performance%20on%20standard%20benchmark%20datasets%20as%20assessed%20by%0Aquantitative%20proxy%20metrics%20and%20quantum%20mechanical%20calculations.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/rees-c/sgequidiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10994v2&entry.124074799=Read"},
{"title": "MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time\n  Series Representation Learning", "author": "Yi Xu and Yitian Zhang and Yun Fu", "abstract": "  Unsupervised multivariate time series (MTS) representation learning aims to\nextract compact and informative representations from raw sequences without\nrelying on labels, enabling efficient transfer to diverse downstream tasks. In\nthis paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked\ntime-series modeling framework for unsupervised MTS representation learning.\nDMAE formulates two complementary pretext tasks: (1) reconstructing masked\nvalues based on visible attributes, and (2) estimating latent representations\nof masked features, guided by a teacher encoder. To further improve\nrepresentation quality, we introduce a feature-level alignment constraint that\nencourages the predicted latent representations to align with the teacher's\noutputs. By jointly optimizing these objectives, DMAE learns temporally\ncoherent and semantically rich representations. Comprehensive evaluations\nacross classification, regression, and forecasting tasks demonstrate that our\napproach achieves consistent and superior performance over competitive\nbaselines.\n", "link": "http://arxiv.org/abs/2509.16078v1", "date": "2025-09-19", "relevancy": 2.0689, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4935}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTS-DMAE%3A%20Dual-Masked%20Autoencoder%20for%20Unsupervised%20Multivariate%20Time%0A%20%20Series%20Representation%20Learning&body=Title%3A%20MTS-DMAE%3A%20Dual-Masked%20Autoencoder%20for%20Unsupervised%20Multivariate%20Time%0A%20%20Series%20Representation%20Learning%0AAuthor%3A%20Yi%20Xu%20and%20Yitian%20Zhang%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Unsupervised%20multivariate%20time%20series%20%28MTS%29%20representation%20learning%20aims%20to%0Aextract%20compact%20and%20informative%20representations%20from%20raw%20sequences%20without%0Arelying%20on%20labels%2C%20enabling%20efficient%20transfer%20to%20diverse%20downstream%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20Dual-Masked%20Autoencoder%20%28DMAE%29%2C%20a%20novel%20masked%0Atime-series%20modeling%20framework%20for%20unsupervised%20MTS%20representation%20learning.%0ADMAE%20formulates%20two%20complementary%20pretext%20tasks%3A%20%281%29%20reconstructing%20masked%0Avalues%20based%20on%20visible%20attributes%2C%20and%20%282%29%20estimating%20latent%20representations%0Aof%20masked%20features%2C%20guided%20by%20a%20teacher%20encoder.%20To%20further%20improve%0Arepresentation%20quality%2C%20we%20introduce%20a%20feature-level%20alignment%20constraint%20that%0Aencourages%20the%20predicted%20latent%20representations%20to%20align%20with%20the%20teacher%27s%0Aoutputs.%20By%20jointly%20optimizing%20these%20objectives%2C%20DMAE%20learns%20temporally%0Acoherent%20and%20semantically%20rich%20representations.%20Comprehensive%20evaluations%0Aacross%20classification%2C%20regression%2C%20and%20forecasting%20tasks%20demonstrate%20that%20our%0Aapproach%20achieves%20consistent%20and%20superior%20performance%20over%20competitive%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTS-DMAE%253A%2520Dual-Masked%2520Autoencoder%2520for%2520Unsupervised%2520Multivariate%2520Time%250A%2520%2520Series%2520Representation%2520Learning%26entry.906535625%3DYi%2520Xu%2520and%2520Yitian%2520Zhang%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Unsupervised%2520multivariate%2520time%2520series%2520%2528MTS%2529%2520representation%2520learning%2520aims%2520to%250Aextract%2520compact%2520and%2520informative%2520representations%2520from%2520raw%2520sequences%2520without%250Arelying%2520on%2520labels%252C%2520enabling%2520efficient%2520transfer%2520to%2520diverse%2520downstream%2520tasks.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Dual-Masked%2520Autoencoder%2520%2528DMAE%2529%252C%2520a%2520novel%2520masked%250Atime-series%2520modeling%2520framework%2520for%2520unsupervised%2520MTS%2520representation%2520learning.%250ADMAE%2520formulates%2520two%2520complementary%2520pretext%2520tasks%253A%2520%25281%2529%2520reconstructing%2520masked%250Avalues%2520based%2520on%2520visible%2520attributes%252C%2520and%2520%25282%2529%2520estimating%2520latent%2520representations%250Aof%2520masked%2520features%252C%2520guided%2520by%2520a%2520teacher%2520encoder.%2520To%2520further%2520improve%250Arepresentation%2520quality%252C%2520we%2520introduce%2520a%2520feature-level%2520alignment%2520constraint%2520that%250Aencourages%2520the%2520predicted%2520latent%2520representations%2520to%2520align%2520with%2520the%2520teacher%2527s%250Aoutputs.%2520By%2520jointly%2520optimizing%2520these%2520objectives%252C%2520DMAE%2520learns%2520temporally%250Acoherent%2520and%2520semantically%2520rich%2520representations.%2520Comprehensive%2520evaluations%250Aacross%2520classification%252C%2520regression%252C%2520and%2520forecasting%2520tasks%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520consistent%2520and%2520superior%2520performance%2520over%2520competitive%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTS-DMAE%3A%20Dual-Masked%20Autoencoder%20for%20Unsupervised%20Multivariate%20Time%0A%20%20Series%20Representation%20Learning&entry.906535625=Yi%20Xu%20and%20Yitian%20Zhang%20and%20Yun%20Fu&entry.1292438233=%20%20Unsupervised%20multivariate%20time%20series%20%28MTS%29%20representation%20learning%20aims%20to%0Aextract%20compact%20and%20informative%20representations%20from%20raw%20sequences%20without%0Arelying%20on%20labels%2C%20enabling%20efficient%20transfer%20to%20diverse%20downstream%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20Dual-Masked%20Autoencoder%20%28DMAE%29%2C%20a%20novel%20masked%0Atime-series%20modeling%20framework%20for%20unsupervised%20MTS%20representation%20learning.%0ADMAE%20formulates%20two%20complementary%20pretext%20tasks%3A%20%281%29%20reconstructing%20masked%0Avalues%20based%20on%20visible%20attributes%2C%20and%20%282%29%20estimating%20latent%20representations%0Aof%20masked%20features%2C%20guided%20by%20a%20teacher%20encoder.%20To%20further%20improve%0Arepresentation%20quality%2C%20we%20introduce%20a%20feature-level%20alignment%20constraint%20that%0Aencourages%20the%20predicted%20latent%20representations%20to%20align%20with%20the%20teacher%27s%0Aoutputs.%20By%20jointly%20optimizing%20these%20objectives%2C%20DMAE%20learns%20temporally%0Acoherent%20and%20semantically%20rich%20representations.%20Comprehensive%20evaluations%0Aacross%20classification%2C%20regression%2C%20and%20forecasting%20tasks%20demonstrate%20that%20our%0Aapproach%20achieves%20consistent%20and%20superior%20performance%20over%20competitive%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16078v1&entry.124074799=Read"},
{"title": "MoE-CE: Enhancing Generalization for Deep Learning based Channel\n  Estimation via a Mixture-of-Experts Framework", "author": "Tianyu Li and Yan Xin and  Jianzhong and  Zhang", "abstract": "  Reliable channel estimation (CE) is fundamental for robust communication in\ndynamic wireless environments, where models must generalize across varying\nconditions such as signal-to-noise ratios (SNRs), the number of resource blocks\n(RBs), and channel profiles. Traditional deep learning (DL)-based methods\nstruggle to generalize effectively across such diverse settings, particularly\nunder multitask and zero-shot scenarios. In this work, we propose MoE-CE, a\nflexible mixture-of-experts (MoE) framework designed to enhance the\ngeneralization capability of DL-based CE methods. MoE-CE provides an\nappropriate inductive bias by leveraging multiple expert subnetworks, each\nspecialized in distinct channel characteristics, and a learned router that\ndynamically selects the most relevant experts per input. This architecture\nenhances model capacity and adaptability without a proportional rise in\ncomputational cost while being agnostic to the choice of the backbone model and\nthe learning algorithm. Through extensive experiments on synthetic datasets\ngenerated under diverse SNRs, RB numbers, and channel profiles, including\nmultitask and zero-shot evaluations, we demonstrate that MoE-CE consistently\noutperforms conventional DL approaches, achieving significant performance gains\nwhile maintaining efficiency.\n", "link": "http://arxiv.org/abs/2509.15964v1", "date": "2025-09-19", "relevancy": 2.0683, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5318}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE-CE%3A%20Enhancing%20Generalization%20for%20Deep%20Learning%20based%20Channel%0A%20%20Estimation%20via%20a%20Mixture-of-Experts%20Framework&body=Title%3A%20MoE-CE%3A%20Enhancing%20Generalization%20for%20Deep%20Learning%20based%20Channel%0A%20%20Estimation%20via%20a%20Mixture-of-Experts%20Framework%0AAuthor%3A%20Tianyu%20Li%20and%20Yan%20Xin%20and%20%20Jianzhong%20and%20%20Zhang%0AAbstract%3A%20%20%20Reliable%20channel%20estimation%20%28CE%29%20is%20fundamental%20for%20robust%20communication%20in%0Adynamic%20wireless%20environments%2C%20where%20models%20must%20generalize%20across%20varying%0Aconditions%20such%20as%20signal-to-noise%20ratios%20%28SNRs%29%2C%20the%20number%20of%20resource%20blocks%0A%28RBs%29%2C%20and%20channel%20profiles.%20Traditional%20deep%20learning%20%28DL%29-based%20methods%0Astruggle%20to%20generalize%20effectively%20across%20such%20diverse%20settings%2C%20particularly%0Aunder%20multitask%20and%20zero-shot%20scenarios.%20In%20this%20work%2C%20we%20propose%20MoE-CE%2C%20a%0Aflexible%20mixture-of-experts%20%28MoE%29%20framework%20designed%20to%20enhance%20the%0Ageneralization%20capability%20of%20DL-based%20CE%20methods.%20MoE-CE%20provides%20an%0Aappropriate%20inductive%20bias%20by%20leveraging%20multiple%20expert%20subnetworks%2C%20each%0Aspecialized%20in%20distinct%20channel%20characteristics%2C%20and%20a%20learned%20router%20that%0Adynamically%20selects%20the%20most%20relevant%20experts%20per%20input.%20This%20architecture%0Aenhances%20model%20capacity%20and%20adaptability%20without%20a%20proportional%20rise%20in%0Acomputational%20cost%20while%20being%20agnostic%20to%20the%20choice%20of%20the%20backbone%20model%20and%0Athe%20learning%20algorithm.%20Through%20extensive%20experiments%20on%20synthetic%20datasets%0Agenerated%20under%20diverse%20SNRs%2C%20RB%20numbers%2C%20and%20channel%20profiles%2C%20including%0Amultitask%20and%20zero-shot%20evaluations%2C%20we%20demonstrate%20that%20MoE-CE%20consistently%0Aoutperforms%20conventional%20DL%20approaches%2C%20achieving%20significant%20performance%20gains%0Awhile%20maintaining%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE-CE%253A%2520Enhancing%2520Generalization%2520for%2520Deep%2520Learning%2520based%2520Channel%250A%2520%2520Estimation%2520via%2520a%2520Mixture-of-Experts%2520Framework%26entry.906535625%3DTianyu%2520Li%2520and%2520Yan%2520Xin%2520and%2520%2520Jianzhong%2520and%2520%2520Zhang%26entry.1292438233%3D%2520%2520Reliable%2520channel%2520estimation%2520%2528CE%2529%2520is%2520fundamental%2520for%2520robust%2520communication%2520in%250Adynamic%2520wireless%2520environments%252C%2520where%2520models%2520must%2520generalize%2520across%2520varying%250Aconditions%2520such%2520as%2520signal-to-noise%2520ratios%2520%2528SNRs%2529%252C%2520the%2520number%2520of%2520resource%2520blocks%250A%2528RBs%2529%252C%2520and%2520channel%2520profiles.%2520Traditional%2520deep%2520learning%2520%2528DL%2529-based%2520methods%250Astruggle%2520to%2520generalize%2520effectively%2520across%2520such%2520diverse%2520settings%252C%2520particularly%250Aunder%2520multitask%2520and%2520zero-shot%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520MoE-CE%252C%2520a%250Aflexible%2520mixture-of-experts%2520%2528MoE%2529%2520framework%2520designed%2520to%2520enhance%2520the%250Ageneralization%2520capability%2520of%2520DL-based%2520CE%2520methods.%2520MoE-CE%2520provides%2520an%250Aappropriate%2520inductive%2520bias%2520by%2520leveraging%2520multiple%2520expert%2520subnetworks%252C%2520each%250Aspecialized%2520in%2520distinct%2520channel%2520characteristics%252C%2520and%2520a%2520learned%2520router%2520that%250Adynamically%2520selects%2520the%2520most%2520relevant%2520experts%2520per%2520input.%2520This%2520architecture%250Aenhances%2520model%2520capacity%2520and%2520adaptability%2520without%2520a%2520proportional%2520rise%2520in%250Acomputational%2520cost%2520while%2520being%2520agnostic%2520to%2520the%2520choice%2520of%2520the%2520backbone%2520model%2520and%250Athe%2520learning%2520algorithm.%2520Through%2520extensive%2520experiments%2520on%2520synthetic%2520datasets%250Agenerated%2520under%2520diverse%2520SNRs%252C%2520RB%2520numbers%252C%2520and%2520channel%2520profiles%252C%2520including%250Amultitask%2520and%2520zero-shot%2520evaluations%252C%2520we%2520demonstrate%2520that%2520MoE-CE%2520consistently%250Aoutperforms%2520conventional%2520DL%2520approaches%252C%2520achieving%2520significant%2520performance%2520gains%250Awhile%2520maintaining%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-CE%3A%20Enhancing%20Generalization%20for%20Deep%20Learning%20based%20Channel%0A%20%20Estimation%20via%20a%20Mixture-of-Experts%20Framework&entry.906535625=Tianyu%20Li%20and%20Yan%20Xin%20and%20%20Jianzhong%20and%20%20Zhang&entry.1292438233=%20%20Reliable%20channel%20estimation%20%28CE%29%20is%20fundamental%20for%20robust%20communication%20in%0Adynamic%20wireless%20environments%2C%20where%20models%20must%20generalize%20across%20varying%0Aconditions%20such%20as%20signal-to-noise%20ratios%20%28SNRs%29%2C%20the%20number%20of%20resource%20blocks%0A%28RBs%29%2C%20and%20channel%20profiles.%20Traditional%20deep%20learning%20%28DL%29-based%20methods%0Astruggle%20to%20generalize%20effectively%20across%20such%20diverse%20settings%2C%20particularly%0Aunder%20multitask%20and%20zero-shot%20scenarios.%20In%20this%20work%2C%20we%20propose%20MoE-CE%2C%20a%0Aflexible%20mixture-of-experts%20%28MoE%29%20framework%20designed%20to%20enhance%20the%0Ageneralization%20capability%20of%20DL-based%20CE%20methods.%20MoE-CE%20provides%20an%0Aappropriate%20inductive%20bias%20by%20leveraging%20multiple%20expert%20subnetworks%2C%20each%0Aspecialized%20in%20distinct%20channel%20characteristics%2C%20and%20a%20learned%20router%20that%0Adynamically%20selects%20the%20most%20relevant%20experts%20per%20input.%20This%20architecture%0Aenhances%20model%20capacity%20and%20adaptability%20without%20a%20proportional%20rise%20in%0Acomputational%20cost%20while%20being%20agnostic%20to%20the%20choice%20of%20the%20backbone%20model%20and%0Athe%20learning%20algorithm.%20Through%20extensive%20experiments%20on%20synthetic%20datasets%0Agenerated%20under%20diverse%20SNRs%2C%20RB%20numbers%2C%20and%20channel%20profiles%2C%20including%0Amultitask%20and%20zero-shot%20evaluations%2C%20we%20demonstrate%20that%20MoE-CE%20consistently%0Aoutperforms%20conventional%20DL%20approaches%2C%20achieving%20significant%20performance%20gains%0Awhile%20maintaining%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15964v1&entry.124074799=Read"},
{"title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay\n  Assessment", "author": "Ahmed Karim and Qiao Wang and Zheng Yuan", "abstract": "  Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.\n", "link": "http://arxiv.org/abs/2509.15926v1", "date": "2025-09-19", "relevancy": 2.067, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5745}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5166}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Score%3A%20Uncertainty-Calibrated%20LLMs%20for%20Automated%20Essay%0A%20%20Assessment&body=Title%3A%20Beyond%20the%20Score%3A%20Uncertainty-Calibrated%20LLMs%20for%20Automated%20Essay%0A%20%20Assessment%0AAuthor%3A%20Ahmed%20Karim%20and%20Qiao%20Wang%20and%20Zheng%20Yuan%0AAbstract%3A%20%20%20Automated%20Essay%20Scoring%20%28AES%29%20systems%20now%20reach%20near%20human%20agreement%20on%20some%0Apublic%20benchmarks%2C%20yet%20real-world%20adoption%2C%20especially%20in%20high-stakes%0Aexaminations%2C%20remains%20limited.%20A%20principal%20obstacle%20is%20that%20most%20models%20output%0Aa%20single%20score%20without%20any%20accompanying%20measure%20of%20confidence%20or%20explanation.%0AWe%20address%20this%20gap%20with%20conformal%20prediction%2C%20a%20distribution-free%20wrapper%20that%0Aequips%20any%20classifier%20with%20set-valued%20outputs%20and%20formal%20coverage%20guarantees.%0ATwo%20open-source%20large%20language%20models%20%28Llama-3%208B%20and%20Qwen-2.5%203B%29%20are%0Afine-tuned%20on%20three%20diverse%20corpora%20%28ASAP%2C%20TOEFL11%2C%20Cambridge-FCE%29%20and%0Acalibrated%20at%20a%2090%20percent%20risk%20level.%20Reliability%20is%20assessed%20with%20UAcc%2C%20an%0Auncertainty-aware%20accuracy%20that%20rewards%20models%20for%20being%20both%20correct%20and%0Aconcise.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20combine%20conformal%0Aprediction%20and%20UAcc%20for%20essay%20scoring.%20The%20calibrated%20models%20consistently%20meet%0Athe%20coverage%20target%20while%20keeping%20prediction%20sets%20compact%2C%20indicating%20that%0Aopen-source%2C%20mid-sized%20LLMs%20can%20already%20support%20teacher-in-the-loop%20AES%3B%20we%0Adiscuss%20scaling%20and%20broader%20user%20studies%20as%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Score%253A%2520Uncertainty-Calibrated%2520LLMs%2520for%2520Automated%2520Essay%250A%2520%2520Assessment%26entry.906535625%3DAhmed%2520Karim%2520and%2520Qiao%2520Wang%2520and%2520Zheng%2520Yuan%26entry.1292438233%3D%2520%2520Automated%2520Essay%2520Scoring%2520%2528AES%2529%2520systems%2520now%2520reach%2520near%2520human%2520agreement%2520on%2520some%250Apublic%2520benchmarks%252C%2520yet%2520real-world%2520adoption%252C%2520especially%2520in%2520high-stakes%250Aexaminations%252C%2520remains%2520limited.%2520A%2520principal%2520obstacle%2520is%2520that%2520most%2520models%2520output%250Aa%2520single%2520score%2520without%2520any%2520accompanying%2520measure%2520of%2520confidence%2520or%2520explanation.%250AWe%2520address%2520this%2520gap%2520with%2520conformal%2520prediction%252C%2520a%2520distribution-free%2520wrapper%2520that%250Aequips%2520any%2520classifier%2520with%2520set-valued%2520outputs%2520and%2520formal%2520coverage%2520guarantees.%250ATwo%2520open-source%2520large%2520language%2520models%2520%2528Llama-3%25208B%2520and%2520Qwen-2.5%25203B%2529%2520are%250Afine-tuned%2520on%2520three%2520diverse%2520corpora%2520%2528ASAP%252C%2520TOEFL11%252C%2520Cambridge-FCE%2529%2520and%250Acalibrated%2520at%2520a%252090%2520percent%2520risk%2520level.%2520Reliability%2520is%2520assessed%2520with%2520UAcc%252C%2520an%250Auncertainty-aware%2520accuracy%2520that%2520rewards%2520models%2520for%2520being%2520both%2520correct%2520and%250Aconcise.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520combine%2520conformal%250Aprediction%2520and%2520UAcc%2520for%2520essay%2520scoring.%2520The%2520calibrated%2520models%2520consistently%2520meet%250Athe%2520coverage%2520target%2520while%2520keeping%2520prediction%2520sets%2520compact%252C%2520indicating%2520that%250Aopen-source%252C%2520mid-sized%2520LLMs%2520can%2520already%2520support%2520teacher-in-the-loop%2520AES%253B%2520we%250Adiscuss%2520scaling%2520and%2520broader%2520user%2520studies%2520as%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Score%3A%20Uncertainty-Calibrated%20LLMs%20for%20Automated%20Essay%0A%20%20Assessment&entry.906535625=Ahmed%20Karim%20and%20Qiao%20Wang%20and%20Zheng%20Yuan&entry.1292438233=%20%20Automated%20Essay%20Scoring%20%28AES%29%20systems%20now%20reach%20near%20human%20agreement%20on%20some%0Apublic%20benchmarks%2C%20yet%20real-world%20adoption%2C%20especially%20in%20high-stakes%0Aexaminations%2C%20remains%20limited.%20A%20principal%20obstacle%20is%20that%20most%20models%20output%0Aa%20single%20score%20without%20any%20accompanying%20measure%20of%20confidence%20or%20explanation.%0AWe%20address%20this%20gap%20with%20conformal%20prediction%2C%20a%20distribution-free%20wrapper%20that%0Aequips%20any%20classifier%20with%20set-valued%20outputs%20and%20formal%20coverage%20guarantees.%0ATwo%20open-source%20large%20language%20models%20%28Llama-3%208B%20and%20Qwen-2.5%203B%29%20are%0Afine-tuned%20on%20three%20diverse%20corpora%20%28ASAP%2C%20TOEFL11%2C%20Cambridge-FCE%29%20and%0Acalibrated%20at%20a%2090%20percent%20risk%20level.%20Reliability%20is%20assessed%20with%20UAcc%2C%20an%0Auncertainty-aware%20accuracy%20that%20rewards%20models%20for%20being%20both%20correct%20and%0Aconcise.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20combine%20conformal%0Aprediction%20and%20UAcc%20for%20essay%20scoring.%20The%20calibrated%20models%20consistently%20meet%0Athe%20coverage%20target%20while%20keeping%20prediction%20sets%20compact%2C%20indicating%20that%0Aopen-source%2C%20mid-sized%20LLMs%20can%20already%20support%20teacher-in-the-loop%20AES%3B%20we%0Adiscuss%20scaling%20and%20broader%20user%20studies%20as%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15926v1&entry.124074799=Read"},
{"title": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model\n  Framework for Reinforcement Learning", "author": "Changwei Yao and Xinzi Liu and Chen Li and Marios Savvides", "abstract": "  Designing effective reward functions remains a major challenge in\nreinforcement learning (RL), often requiring considerable human expertise and\niterative refinement. Recent advances leverage Large Language Models (LLMs) for\nautomated reward design, but these approaches are limited by hallucinations,\nreliance on human feedback, and challenges with handling complex, multi-step\ntasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts\n(RE-GoT), a novel bi-level framework that enhances LLMs with structured\ngraph-based reasoning and integrates Visual Language Models (VLMs) for\nautomated rollout evaluation. RE-GoT first decomposes tasks into\ntext-attributed graphs, enabling comprehensive analysis and reward function\ngeneration, and then iteratively refines rewards using visual feedback from\nVLMs without human intervention. Extensive experiments on 10 RoboGen and 4\nManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing\nLLM-based baselines. On RoboGen, our method improves average task success rates\nby 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,\nRE-GoT achieves an average success rate of 93.73% across four diverse\nmanipulation tasks, significantly surpassing prior LLM-based approaches and\neven exceeding expert-designed rewards. Our results indicate that combining\nLLMs and VLMs with graph-of-thoughts reasoning provides a scalable and\neffective solution for autonomous reward evolution in RL.\n", "link": "http://arxiv.org/abs/2509.16136v1", "date": "2025-09-19", "relevancy": 2.0659, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5618}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Evolution%20with%20Graph-of-Thoughts%3A%20A%20Bi-Level%20Language%20Model%0A%20%20Framework%20for%20Reinforcement%20Learning&body=Title%3A%20Reward%20Evolution%20with%20Graph-of-Thoughts%3A%20A%20Bi-Level%20Language%20Model%0A%20%20Framework%20for%20Reinforcement%20Learning%0AAuthor%3A%20Changwei%20Yao%20and%20Xinzi%20Liu%20and%20Chen%20Li%20and%20Marios%20Savvides%0AAbstract%3A%20%20%20Designing%20effective%20reward%20functions%20remains%20a%20major%20challenge%20in%0Areinforcement%20learning%20%28RL%29%2C%20often%20requiring%20considerable%20human%20expertise%20and%0Aiterative%20refinement.%20Recent%20advances%20leverage%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomated%20reward%20design%2C%20but%20these%20approaches%20are%20limited%20by%20hallucinations%2C%0Areliance%20on%20human%20feedback%2C%20and%20challenges%20with%20handling%20complex%2C%20multi-step%0Atasks.%20In%20this%20work%2C%20we%20introduce%20Reward%20Evolution%20with%20Graph-of-Thoughts%0A%28RE-GoT%29%2C%20a%20novel%20bi-level%20framework%20that%20enhances%20LLMs%20with%20structured%0Agraph-based%20reasoning%20and%20integrates%20Visual%20Language%20Models%20%28VLMs%29%20for%0Aautomated%20rollout%20evaluation.%20RE-GoT%20first%20decomposes%20tasks%20into%0Atext-attributed%20graphs%2C%20enabling%20comprehensive%20analysis%20and%20reward%20function%0Ageneration%2C%20and%20then%20iteratively%20refines%20rewards%20using%20visual%20feedback%20from%0AVLMs%20without%20human%20intervention.%20Extensive%20experiments%20on%2010%20RoboGen%20and%204%0AManiSkill2%20tasks%20demonstrate%20that%20RE-GoT%20consistently%20outperforms%20existing%0ALLM-based%20baselines.%20On%20RoboGen%2C%20our%20method%20improves%20average%20task%20success%20rates%0Aby%2032.25%25%2C%20with%20notable%20gains%20on%20complex%20multi-step%20tasks.%20On%20ManiSkill2%2C%0ARE-GoT%20achieves%20an%20average%20success%20rate%20of%2093.73%25%20across%20four%20diverse%0Amanipulation%20tasks%2C%20significantly%20surpassing%20prior%20LLM-based%20approaches%20and%0Aeven%20exceeding%20expert-designed%20rewards.%20Our%20results%20indicate%20that%20combining%0ALLMs%20and%20VLMs%20with%20graph-of-thoughts%20reasoning%20provides%20a%20scalable%20and%0Aeffective%20solution%20for%20autonomous%20reward%20evolution%20in%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Evolution%2520with%2520Graph-of-Thoughts%253A%2520A%2520Bi-Level%2520Language%2520Model%250A%2520%2520Framework%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DChangwei%2520Yao%2520and%2520Xinzi%2520Liu%2520and%2520Chen%2520Li%2520and%2520Marios%2520Savvides%26entry.1292438233%3D%2520%2520Designing%2520effective%2520reward%2520functions%2520remains%2520a%2520major%2520challenge%2520in%250Areinforcement%2520learning%2520%2528RL%2529%252C%2520often%2520requiring%2520considerable%2520human%2520expertise%2520and%250Aiterative%2520refinement.%2520Recent%2520advances%2520leverage%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Aautomated%2520reward%2520design%252C%2520but%2520these%2520approaches%2520are%2520limited%2520by%2520hallucinations%252C%250Areliance%2520on%2520human%2520feedback%252C%2520and%2520challenges%2520with%2520handling%2520complex%252C%2520multi-step%250Atasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Reward%2520Evolution%2520with%2520Graph-of-Thoughts%250A%2528RE-GoT%2529%252C%2520a%2520novel%2520bi-level%2520framework%2520that%2520enhances%2520LLMs%2520with%2520structured%250Agraph-based%2520reasoning%2520and%2520integrates%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520for%250Aautomated%2520rollout%2520evaluation.%2520RE-GoT%2520first%2520decomposes%2520tasks%2520into%250Atext-attributed%2520graphs%252C%2520enabling%2520comprehensive%2520analysis%2520and%2520reward%2520function%250Ageneration%252C%2520and%2520then%2520iteratively%2520refines%2520rewards%2520using%2520visual%2520feedback%2520from%250AVLMs%2520without%2520human%2520intervention.%2520Extensive%2520experiments%2520on%252010%2520RoboGen%2520and%25204%250AManiSkill2%2520tasks%2520demonstrate%2520that%2520RE-GoT%2520consistently%2520outperforms%2520existing%250ALLM-based%2520baselines.%2520On%2520RoboGen%252C%2520our%2520method%2520improves%2520average%2520task%2520success%2520rates%250Aby%252032.25%2525%252C%2520with%2520notable%2520gains%2520on%2520complex%2520multi-step%2520tasks.%2520On%2520ManiSkill2%252C%250ARE-GoT%2520achieves%2520an%2520average%2520success%2520rate%2520of%252093.73%2525%2520across%2520four%2520diverse%250Amanipulation%2520tasks%252C%2520significantly%2520surpassing%2520prior%2520LLM-based%2520approaches%2520and%250Aeven%2520exceeding%2520expert-designed%2520rewards.%2520Our%2520results%2520indicate%2520that%2520combining%250ALLMs%2520and%2520VLMs%2520with%2520graph-of-thoughts%2520reasoning%2520provides%2520a%2520scalable%2520and%250Aeffective%2520solution%2520for%2520autonomous%2520reward%2520evolution%2520in%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Evolution%20with%20Graph-of-Thoughts%3A%20A%20Bi-Level%20Language%20Model%0A%20%20Framework%20for%20Reinforcement%20Learning&entry.906535625=Changwei%20Yao%20and%20Xinzi%20Liu%20and%20Chen%20Li%20and%20Marios%20Savvides&entry.1292438233=%20%20Designing%20effective%20reward%20functions%20remains%20a%20major%20challenge%20in%0Areinforcement%20learning%20%28RL%29%2C%20often%20requiring%20considerable%20human%20expertise%20and%0Aiterative%20refinement.%20Recent%20advances%20leverage%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomated%20reward%20design%2C%20but%20these%20approaches%20are%20limited%20by%20hallucinations%2C%0Areliance%20on%20human%20feedback%2C%20and%20challenges%20with%20handling%20complex%2C%20multi-step%0Atasks.%20In%20this%20work%2C%20we%20introduce%20Reward%20Evolution%20with%20Graph-of-Thoughts%0A%28RE-GoT%29%2C%20a%20novel%20bi-level%20framework%20that%20enhances%20LLMs%20with%20structured%0Agraph-based%20reasoning%20and%20integrates%20Visual%20Language%20Models%20%28VLMs%29%20for%0Aautomated%20rollout%20evaluation.%20RE-GoT%20first%20decomposes%20tasks%20into%0Atext-attributed%20graphs%2C%20enabling%20comprehensive%20analysis%20and%20reward%20function%0Ageneration%2C%20and%20then%20iteratively%20refines%20rewards%20using%20visual%20feedback%20from%0AVLMs%20without%20human%20intervention.%20Extensive%20experiments%20on%2010%20RoboGen%20and%204%0AManiSkill2%20tasks%20demonstrate%20that%20RE-GoT%20consistently%20outperforms%20existing%0ALLM-based%20baselines.%20On%20RoboGen%2C%20our%20method%20improves%20average%20task%20success%20rates%0Aby%2032.25%25%2C%20with%20notable%20gains%20on%20complex%20multi-step%20tasks.%20On%20ManiSkill2%2C%0ARE-GoT%20achieves%20an%20average%20success%20rate%20of%2093.73%25%20across%20four%20diverse%0Amanipulation%20tasks%2C%20significantly%20surpassing%20prior%20LLM-based%20approaches%20and%0Aeven%20exceeding%20expert-designed%20rewards.%20Our%20results%20indicate%20that%20combining%0ALLMs%20and%20VLMs%20with%20graph-of-thoughts%20reasoning%20provides%20a%20scalable%20and%0Aeffective%20solution%20for%20autonomous%20reward%20evolution%20in%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16136v1&entry.124074799=Read"},
{"title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language\n  Models", "author": "Narmeen Oozeer and Luke Marks and Fazl Barez and Amirali Abdullah", "abstract": "  Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors.\n", "link": "http://arxiv.org/abs/2505.24535v2", "date": "2025-09-19", "relevancy": 2.0648, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Linear%20Steering%3A%20Unified%20Multi-Attribute%20Control%20for%20Language%0A%20%20Models&body=Title%3A%20Beyond%20Linear%20Steering%3A%20Unified%20Multi-Attribute%20Control%20for%20Language%0A%20%20Models%0AAuthor%3A%20Narmeen%20Oozeer%20and%20Luke%20Marks%20and%20Fazl%20Barez%20and%20Amirali%20Abdullah%0AAbstract%3A%20%20%20Controlling%20multiple%20behavioral%20attributes%20in%20large%20language%20models%20%28LLMs%29%20at%0Ainference%20time%20is%20a%20challenging%20problem%20due%20to%20interference%20between%20attributes%0Aand%20the%20limitations%20of%20linear%20steering%20methods%2C%20which%20assume%20additive%20behavior%0Ain%20activation%20space%20and%20require%20per-attribute%20tuning.%20We%20introduce%20K-Steering%2C%0Aa%20unified%20and%20flexible%20approach%20that%20trains%20a%20single%20non-linear%20multi-label%0Aclassifier%20on%20hidden%20activations%20and%20computes%20intervention%20directions%20via%0Agradients%20at%20inference%20time.%20This%20avoids%20linearity%20assumptions%2C%20removes%20the%0Aneed%20for%20storing%20and%20tuning%20separate%20attribute%20vectors%2C%20and%20allows%20dynamic%0Acomposition%20of%20behaviors%20without%20retraining.%20To%20evaluate%20our%20method%2C%20we%20propose%0Atwo%20new%20benchmarks%2C%20ToneBank%20and%20DebateMix%2C%20targeting%20compositional%20behavioral%0Acontrol.%20Empirical%20results%20across%203%20model%20families%2C%20validated%20by%20both%0Aactivation-based%20classifiers%20and%20LLM-based%20judges%2C%20demonstrate%20that%20K-Steering%0Aoutperforms%20strong%20baselines%20in%20accurately%20steering%20multiple%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Linear%2520Steering%253A%2520Unified%2520Multi-Attribute%2520Control%2520for%2520Language%250A%2520%2520Models%26entry.906535625%3DNarmeen%2520Oozeer%2520and%2520Luke%2520Marks%2520and%2520Fazl%2520Barez%2520and%2520Amirali%2520Abdullah%26entry.1292438233%3D%2520%2520Controlling%2520multiple%2520behavioral%2520attributes%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520at%250Ainference%2520time%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520interference%2520between%2520attributes%250Aand%2520the%2520limitations%2520of%2520linear%2520steering%2520methods%252C%2520which%2520assume%2520additive%2520behavior%250Ain%2520activation%2520space%2520and%2520require%2520per-attribute%2520tuning.%2520We%2520introduce%2520K-Steering%252C%250Aa%2520unified%2520and%2520flexible%2520approach%2520that%2520trains%2520a%2520single%2520non-linear%2520multi-label%250Aclassifier%2520on%2520hidden%2520activations%2520and%2520computes%2520intervention%2520directions%2520via%250Agradients%2520at%2520inference%2520time.%2520This%2520avoids%2520linearity%2520assumptions%252C%2520removes%2520the%250Aneed%2520for%2520storing%2520and%2520tuning%2520separate%2520attribute%2520vectors%252C%2520and%2520allows%2520dynamic%250Acomposition%2520of%2520behaviors%2520without%2520retraining.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520propose%250Atwo%2520new%2520benchmarks%252C%2520ToneBank%2520and%2520DebateMix%252C%2520targeting%2520compositional%2520behavioral%250Acontrol.%2520Empirical%2520results%2520across%25203%2520model%2520families%252C%2520validated%2520by%2520both%250Aactivation-based%2520classifiers%2520and%2520LLM-based%2520judges%252C%2520demonstrate%2520that%2520K-Steering%250Aoutperforms%2520strong%2520baselines%2520in%2520accurately%2520steering%2520multiple%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Linear%20Steering%3A%20Unified%20Multi-Attribute%20Control%20for%20Language%0A%20%20Models&entry.906535625=Narmeen%20Oozeer%20and%20Luke%20Marks%20and%20Fazl%20Barez%20and%20Amirali%20Abdullah&entry.1292438233=%20%20Controlling%20multiple%20behavioral%20attributes%20in%20large%20language%20models%20%28LLMs%29%20at%0Ainference%20time%20is%20a%20challenging%20problem%20due%20to%20interference%20between%20attributes%0Aand%20the%20limitations%20of%20linear%20steering%20methods%2C%20which%20assume%20additive%20behavior%0Ain%20activation%20space%20and%20require%20per-attribute%20tuning.%20We%20introduce%20K-Steering%2C%0Aa%20unified%20and%20flexible%20approach%20that%20trains%20a%20single%20non-linear%20multi-label%0Aclassifier%20on%20hidden%20activations%20and%20computes%20intervention%20directions%20via%0Agradients%20at%20inference%20time.%20This%20avoids%20linearity%20assumptions%2C%20removes%20the%0Aneed%20for%20storing%20and%20tuning%20separate%20attribute%20vectors%2C%20and%20allows%20dynamic%0Acomposition%20of%20behaviors%20without%20retraining.%20To%20evaluate%20our%20method%2C%20we%20propose%0Atwo%20new%20benchmarks%2C%20ToneBank%20and%20DebateMix%2C%20targeting%20compositional%20behavioral%0Acontrol.%20Empirical%20results%20across%203%20model%20families%2C%20validated%20by%20both%0Aactivation-based%20classifiers%20and%20LLM-based%20judges%2C%20demonstrate%20that%20K-Steering%0Aoutperforms%20strong%20baselines%20in%20accurately%20steering%20multiple%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24535v2&entry.124074799=Read"},
{"title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search", "author": "Zhiyu Mou and Yiqin Lv and Miao Xu and Cheems Wang and Yixiu Mao and Qichen Ye and Chao Li and Rongquan Bai and Chuan Yu and Jian Xu and Bo Zheng", "abstract": "  Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach.\n", "link": "http://arxiv.org/abs/2509.15927v1", "date": "2025-09-19", "relevancy": 1.9015, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Generative%20Auto-bidding%20with%20Offline%20Reward%20Evaluation%20and%0A%20%20Policy%20Search&body=Title%3A%20Enhancing%20Generative%20Auto-bidding%20with%20Offline%20Reward%20Evaluation%20and%0A%20%20Policy%20Search%0AAuthor%3A%20Zhiyu%20Mou%20and%20Yiqin%20Lv%20and%20Miao%20Xu%20and%20Cheems%20Wang%20and%20Yixiu%20Mao%20and%20Qichen%20Ye%20and%20Chao%20Li%20and%20Rongquan%20Bai%20and%20Chuan%20Yu%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Auto-bidding%20is%20an%20essential%20tool%20for%20advertisers%20to%20enhance%20their%0Aadvertising%20performance.%20Recent%20progress%20has%20shown%20that%20AI-Generated%20Bidding%0A%28AIGB%29%2C%20which%20formulates%20the%20auto-bidding%20as%20a%20trajectory%20generation%20task%20and%0Atrains%20a%20conditional%20diffusion-based%20planner%20on%20offline%20data%2C%20achieves%20superior%0Aand%20stable%20performance%20compared%20to%20typical%20offline%20reinforcement%20learning%0A%28RL%29-based%20auto-bidding%20methods.%20However%2C%20existing%20AIGB%20methods%20still%20encounter%0Aa%20performance%20bottleneck%20due%20to%20their%20neglect%20of%20fine-grained%20generation%0Aquality%20evaluation%20and%20inability%20to%20explore%20beyond%20static%20datasets.%20To%20address%0Athis%2C%20we%20propose%20AIGB-Pearl%20%28%5Cemph%7BPlanning%20with%20EvAluator%20via%20RL%7D%29%2C%20a%20novel%0Amethod%20that%20integrates%20generative%20planning%20and%20policy%20optimization.%20The%20key%20to%0AAIGB-Pearl%20is%20to%20construct%20a%20non-bootstrapped%20%5Cemph%7Btrajectory%20evaluator%7D%20to%0Aassign%20rewards%20and%20guide%20policy%20search%2C%20enabling%20the%20planner%20to%20optimize%20its%0Ageneration%20quality%20iteratively%20through%20interaction.%20Furthermore%2C%20to%20enhance%0Atrajectory%20evaluator%20accuracy%20in%20offline%20settings%2C%20we%20incorporate%20three%20key%0Atechniques%3A%20%28i%29%20a%20Large%20Language%20Model%20%28LLM%29-based%20architecture%20for%20better%0Arepresentational%20capacity%2C%20%28ii%29%20hybrid%20point-wise%20and%20pair-wise%20losses%20for%0Abetter%20score%20learning%2C%20and%20%28iii%29%20adaptive%20integration%20of%20expert%20feedback%20for%0Abetter%20generalization%20ability.%20Extensive%20experiments%20on%20both%20simulated%20and%0Areal-world%20advertising%20systems%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Generative%2520Auto-bidding%2520with%2520Offline%2520Reward%2520Evaluation%2520and%250A%2520%2520Policy%2520Search%26entry.906535625%3DZhiyu%2520Mou%2520and%2520Yiqin%2520Lv%2520and%2520Miao%2520Xu%2520and%2520Cheems%2520Wang%2520and%2520Yixiu%2520Mao%2520and%2520Qichen%2520Ye%2520and%2520Chao%2520Li%2520and%2520Rongquan%2520Bai%2520and%2520Chuan%2520Yu%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Auto-bidding%2520is%2520an%2520essential%2520tool%2520for%2520advertisers%2520to%2520enhance%2520their%250Aadvertising%2520performance.%2520Recent%2520progress%2520has%2520shown%2520that%2520AI-Generated%2520Bidding%250A%2528AIGB%2529%252C%2520which%2520formulates%2520the%2520auto-bidding%2520as%2520a%2520trajectory%2520generation%2520task%2520and%250Atrains%2520a%2520conditional%2520diffusion-based%2520planner%2520on%2520offline%2520data%252C%2520achieves%2520superior%250Aand%2520stable%2520performance%2520compared%2520to%2520typical%2520offline%2520reinforcement%2520learning%250A%2528RL%2529-based%2520auto-bidding%2520methods.%2520However%252C%2520existing%2520AIGB%2520methods%2520still%2520encounter%250Aa%2520performance%2520bottleneck%2520due%2520to%2520their%2520neglect%2520of%2520fine-grained%2520generation%250Aquality%2520evaluation%2520and%2520inability%2520to%2520explore%2520beyond%2520static%2520datasets.%2520To%2520address%250Athis%252C%2520we%2520propose%2520AIGB-Pearl%2520%2528%255Cemph%257BPlanning%2520with%2520EvAluator%2520via%2520RL%257D%2529%252C%2520a%2520novel%250Amethod%2520that%2520integrates%2520generative%2520planning%2520and%2520policy%2520optimization.%2520The%2520key%2520to%250AAIGB-Pearl%2520is%2520to%2520construct%2520a%2520non-bootstrapped%2520%255Cemph%257Btrajectory%2520evaluator%257D%2520to%250Aassign%2520rewards%2520and%2520guide%2520policy%2520search%252C%2520enabling%2520the%2520planner%2520to%2520optimize%2520its%250Ageneration%2520quality%2520iteratively%2520through%2520interaction.%2520Furthermore%252C%2520to%2520enhance%250Atrajectory%2520evaluator%2520accuracy%2520in%2520offline%2520settings%252C%2520we%2520incorporate%2520three%2520key%250Atechniques%253A%2520%2528i%2529%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520architecture%2520for%2520better%250Arepresentational%2520capacity%252C%2520%2528ii%2529%2520hybrid%2520point-wise%2520and%2520pair-wise%2520losses%2520for%250Abetter%2520score%2520learning%252C%2520and%2520%2528iii%2529%2520adaptive%2520integration%2520of%2520expert%2520feedback%2520for%250Abetter%2520generalization%2520ability.%2520Extensive%2520experiments%2520on%2520both%2520simulated%2520and%250Areal-world%2520advertising%2520systems%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%250Aour%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Generative%20Auto-bidding%20with%20Offline%20Reward%20Evaluation%20and%0A%20%20Policy%20Search&entry.906535625=Zhiyu%20Mou%20and%20Yiqin%20Lv%20and%20Miao%20Xu%20and%20Cheems%20Wang%20and%20Yixiu%20Mao%20and%20Qichen%20Ye%20and%20Chao%20Li%20and%20Rongquan%20Bai%20and%20Chuan%20Yu%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=%20%20Auto-bidding%20is%20an%20essential%20tool%20for%20advertisers%20to%20enhance%20their%0Aadvertising%20performance.%20Recent%20progress%20has%20shown%20that%20AI-Generated%20Bidding%0A%28AIGB%29%2C%20which%20formulates%20the%20auto-bidding%20as%20a%20trajectory%20generation%20task%20and%0Atrains%20a%20conditional%20diffusion-based%20planner%20on%20offline%20data%2C%20achieves%20superior%0Aand%20stable%20performance%20compared%20to%20typical%20offline%20reinforcement%20learning%0A%28RL%29-based%20auto-bidding%20methods.%20However%2C%20existing%20AIGB%20methods%20still%20encounter%0Aa%20performance%20bottleneck%20due%20to%20their%20neglect%20of%20fine-grained%20generation%0Aquality%20evaluation%20and%20inability%20to%20explore%20beyond%20static%20datasets.%20To%20address%0Athis%2C%20we%20propose%20AIGB-Pearl%20%28%5Cemph%7BPlanning%20with%20EvAluator%20via%20RL%7D%29%2C%20a%20novel%0Amethod%20that%20integrates%20generative%20planning%20and%20policy%20optimization.%20The%20key%20to%0AAIGB-Pearl%20is%20to%20construct%20a%20non-bootstrapped%20%5Cemph%7Btrajectory%20evaluator%7D%20to%0Aassign%20rewards%20and%20guide%20policy%20search%2C%20enabling%20the%20planner%20to%20optimize%20its%0Ageneration%20quality%20iteratively%20through%20interaction.%20Furthermore%2C%20to%20enhance%0Atrajectory%20evaluator%20accuracy%20in%20offline%20settings%2C%20we%20incorporate%20three%20key%0Atechniques%3A%20%28i%29%20a%20Large%20Language%20Model%20%28LLM%29-based%20architecture%20for%20better%0Arepresentational%20capacity%2C%20%28ii%29%20hybrid%20point-wise%20and%20pair-wise%20losses%20for%0Abetter%20score%20learning%2C%20and%20%28iii%29%20adaptive%20integration%20of%20expert%20feedback%20for%0Abetter%20generalization%20ability.%20Extensive%20experiments%20on%20both%20simulated%20and%0Areal-world%20advertising%20systems%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15927v1&entry.124074799=Read"},
{"title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via\n  Macro-to-Micro Flow Transformation", "author": "Chao Yu and Yuanqing Wang and Zhen Guo and Hao Lin and Si Xu and Hongzhi Zang and Quanlu Zhang and Yongji Wu and Chunyang Zhu and Junhao Hu and Zixiao Huang and Mingjie Wei and Yuqing Xie and Ke Yang and Bo Dai and Zhexuan Xu and Xiangyuan Wang and Xu Fu and Zhihao Liu and Kang Chen and Weilin Liu and Gang Liu and Boxun Li and Jianlei Yang and Zhi Yang and Guohao Dai and Yu Wang", "abstract": "  Reinforcement learning (RL) has demonstrated immense potential in advancing\nartificial general intelligence, agentic intelligence, and embodied\nintelligence. However, the inherent heterogeneity and dynamicity of RL\nworkflows often lead to low hardware utilization and slow training on existing\nsystems. In this paper, we present RLinf, a high-performance RL training system\nbased on our key observation that the major roadblock to efficient RL training\nlies in system flexibility. To maximize flexibility and efficiency, RLinf is\nbuilt atop a novel RL system design paradigm called macro-to-micro flow\ntransformation (M2Flow), which automatically breaks down high-level,\neasy-to-compose RL workflows at both the temporal and spatial dimensions, and\nrecomposes them into optimized execution flows. Supported by RLinf worker's\nadaptive communication capability, we devise context switching and elastic\npipelining to realize M2Flow transformation, and a profiling-guided scheduling\npolicy to generate optimal execution plans. Extensive evaluations on both\nreasoning RL and embodied RL tasks demonstrate that RLinf consistently\noutperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in\nend-to-end training throughput.\n", "link": "http://arxiv.org/abs/2509.15965v1", "date": "2025-09-19", "relevancy": 1.5574, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5606}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5219}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLinf%3A%20Flexible%20and%20Efficient%20Large-scale%20Reinforcement%20Learning%20via%0A%20%20Macro-to-Micro%20Flow%20Transformation&body=Title%3A%20RLinf%3A%20Flexible%20and%20Efficient%20Large-scale%20Reinforcement%20Learning%20via%0A%20%20Macro-to-Micro%20Flow%20Transformation%0AAuthor%3A%20Chao%20Yu%20and%20Yuanqing%20Wang%20and%20Zhen%20Guo%20and%20Hao%20Lin%20and%20Si%20Xu%20and%20Hongzhi%20Zang%20and%20Quanlu%20Zhang%20and%20Yongji%20Wu%20and%20Chunyang%20Zhu%20and%20Junhao%20Hu%20and%20Zixiao%20Huang%20and%20Mingjie%20Wei%20and%20Yuqing%20Xie%20and%20Ke%20Yang%20and%20Bo%20Dai%20and%20Zhexuan%20Xu%20and%20Xiangyuan%20Wang%20and%20Xu%20Fu%20and%20Zhihao%20Liu%20and%20Kang%20Chen%20and%20Weilin%20Liu%20and%20Gang%20Liu%20and%20Boxun%20Li%20and%20Jianlei%20Yang%20and%20Zhi%20Yang%20and%20Guohao%20Dai%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20demonstrated%20immense%20potential%20in%20advancing%0Aartificial%20general%20intelligence%2C%20agentic%20intelligence%2C%20and%20embodied%0Aintelligence.%20However%2C%20the%20inherent%20heterogeneity%20and%20dynamicity%20of%20RL%0Aworkflows%20often%20lead%20to%20low%20hardware%20utilization%20and%20slow%20training%20on%20existing%0Asystems.%20In%20this%20paper%2C%20we%20present%20RLinf%2C%20a%20high-performance%20RL%20training%20system%0Abased%20on%20our%20key%20observation%20that%20the%20major%20roadblock%20to%20efficient%20RL%20training%0Alies%20in%20system%20flexibility.%20To%20maximize%20flexibility%20and%20efficiency%2C%20RLinf%20is%0Abuilt%20atop%20a%20novel%20RL%20system%20design%20paradigm%20called%20macro-to-micro%20flow%0Atransformation%20%28M2Flow%29%2C%20which%20automatically%20breaks%20down%20high-level%2C%0Aeasy-to-compose%20RL%20workflows%20at%20both%20the%20temporal%20and%20spatial%20dimensions%2C%20and%0Arecomposes%20them%20into%20optimized%20execution%20flows.%20Supported%20by%20RLinf%20worker%27s%0Aadaptive%20communication%20capability%2C%20we%20devise%20context%20switching%20and%20elastic%0Apipelining%20to%20realize%20M2Flow%20transformation%2C%20and%20a%20profiling-guided%20scheduling%0Apolicy%20to%20generate%20optimal%20execution%20plans.%20Extensive%20evaluations%20on%20both%0Areasoning%20RL%20and%20embodied%20RL%20tasks%20demonstrate%20that%20RLinf%20consistently%0Aoutperforms%20state-of-the-art%20systems%2C%20achieving%201.1x-2.13x%20speedup%20in%0Aend-to-end%20training%20throughput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLinf%253A%2520Flexible%2520and%2520Efficient%2520Large-scale%2520Reinforcement%2520Learning%2520via%250A%2520%2520Macro-to-Micro%2520Flow%2520Transformation%26entry.906535625%3DChao%2520Yu%2520and%2520Yuanqing%2520Wang%2520and%2520Zhen%2520Guo%2520and%2520Hao%2520Lin%2520and%2520Si%2520Xu%2520and%2520Hongzhi%2520Zang%2520and%2520Quanlu%2520Zhang%2520and%2520Yongji%2520Wu%2520and%2520Chunyang%2520Zhu%2520and%2520Junhao%2520Hu%2520and%2520Zixiao%2520Huang%2520and%2520Mingjie%2520Wei%2520and%2520Yuqing%2520Xie%2520and%2520Ke%2520Yang%2520and%2520Bo%2520Dai%2520and%2520Zhexuan%2520Xu%2520and%2520Xiangyuan%2520Wang%2520and%2520Xu%2520Fu%2520and%2520Zhihao%2520Liu%2520and%2520Kang%2520Chen%2520and%2520Weilin%2520Liu%2520and%2520Gang%2520Liu%2520and%2520Boxun%2520Li%2520and%2520Jianlei%2520Yang%2520and%2520Zhi%2520Yang%2520and%2520Guohao%2520Dai%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520demonstrated%2520immense%2520potential%2520in%2520advancing%250Aartificial%2520general%2520intelligence%252C%2520agentic%2520intelligence%252C%2520and%2520embodied%250Aintelligence.%2520However%252C%2520the%2520inherent%2520heterogeneity%2520and%2520dynamicity%2520of%2520RL%250Aworkflows%2520often%2520lead%2520to%2520low%2520hardware%2520utilization%2520and%2520slow%2520training%2520on%2520existing%250Asystems.%2520In%2520this%2520paper%252C%2520we%2520present%2520RLinf%252C%2520a%2520high-performance%2520RL%2520training%2520system%250Abased%2520on%2520our%2520key%2520observation%2520that%2520the%2520major%2520roadblock%2520to%2520efficient%2520RL%2520training%250Alies%2520in%2520system%2520flexibility.%2520To%2520maximize%2520flexibility%2520and%2520efficiency%252C%2520RLinf%2520is%250Abuilt%2520atop%2520a%2520novel%2520RL%2520system%2520design%2520paradigm%2520called%2520macro-to-micro%2520flow%250Atransformation%2520%2528M2Flow%2529%252C%2520which%2520automatically%2520breaks%2520down%2520high-level%252C%250Aeasy-to-compose%2520RL%2520workflows%2520at%2520both%2520the%2520temporal%2520and%2520spatial%2520dimensions%252C%2520and%250Arecomposes%2520them%2520into%2520optimized%2520execution%2520flows.%2520Supported%2520by%2520RLinf%2520worker%2527s%250Aadaptive%2520communication%2520capability%252C%2520we%2520devise%2520context%2520switching%2520and%2520elastic%250Apipelining%2520to%2520realize%2520M2Flow%2520transformation%252C%2520and%2520a%2520profiling-guided%2520scheduling%250Apolicy%2520to%2520generate%2520optimal%2520execution%2520plans.%2520Extensive%2520evaluations%2520on%2520both%250Areasoning%2520RL%2520and%2520embodied%2520RL%2520tasks%2520demonstrate%2520that%2520RLinf%2520consistently%250Aoutperforms%2520state-of-the-art%2520systems%252C%2520achieving%25201.1x-2.13x%2520speedup%2520in%250Aend-to-end%2520training%2520throughput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLinf%3A%20Flexible%20and%20Efficient%20Large-scale%20Reinforcement%20Learning%20via%0A%20%20Macro-to-Micro%20Flow%20Transformation&entry.906535625=Chao%20Yu%20and%20Yuanqing%20Wang%20and%20Zhen%20Guo%20and%20Hao%20Lin%20and%20Si%20Xu%20and%20Hongzhi%20Zang%20and%20Quanlu%20Zhang%20and%20Yongji%20Wu%20and%20Chunyang%20Zhu%20and%20Junhao%20Hu%20and%20Zixiao%20Huang%20and%20Mingjie%20Wei%20and%20Yuqing%20Xie%20and%20Ke%20Yang%20and%20Bo%20Dai%20and%20Zhexuan%20Xu%20and%20Xiangyuan%20Wang%20and%20Xu%20Fu%20and%20Zhihao%20Liu%20and%20Kang%20Chen%20and%20Weilin%20Liu%20and%20Gang%20Liu%20and%20Boxun%20Li%20and%20Jianlei%20Yang%20and%20Zhi%20Yang%20and%20Guohao%20Dai%20and%20Yu%20Wang&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20demonstrated%20immense%20potential%20in%20advancing%0Aartificial%20general%20intelligence%2C%20agentic%20intelligence%2C%20and%20embodied%0Aintelligence.%20However%2C%20the%20inherent%20heterogeneity%20and%20dynamicity%20of%20RL%0Aworkflows%20often%20lead%20to%20low%20hardware%20utilization%20and%20slow%20training%20on%20existing%0Asystems.%20In%20this%20paper%2C%20we%20present%20RLinf%2C%20a%20high-performance%20RL%20training%20system%0Abased%20on%20our%20key%20observation%20that%20the%20major%20roadblock%20to%20efficient%20RL%20training%0Alies%20in%20system%20flexibility.%20To%20maximize%20flexibility%20and%20efficiency%2C%20RLinf%20is%0Abuilt%20atop%20a%20novel%20RL%20system%20design%20paradigm%20called%20macro-to-micro%20flow%0Atransformation%20%28M2Flow%29%2C%20which%20automatically%20breaks%20down%20high-level%2C%0Aeasy-to-compose%20RL%20workflows%20at%20both%20the%20temporal%20and%20spatial%20dimensions%2C%20and%0Arecomposes%20them%20into%20optimized%20execution%20flows.%20Supported%20by%20RLinf%20worker%27s%0Aadaptive%20communication%20capability%2C%20we%20devise%20context%20switching%20and%20elastic%0Apipelining%20to%20realize%20M2Flow%20transformation%2C%20and%20a%20profiling-guided%20scheduling%0Apolicy%20to%20generate%20optimal%20execution%20plans.%20Extensive%20evaluations%20on%20both%0Areasoning%20RL%20and%20embodied%20RL%20tasks%20demonstrate%20that%20RLinf%20consistently%0Aoutperforms%20state-of-the-art%20systems%2C%20achieving%201.1x-2.13x%20speedup%20in%0Aend-to-end%20training%20throughput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15965v1&entry.124074799=Read"},
{"title": "Gradient Alignment in Physics-informed Neural Networks: A Second-Order\n  Optimization Perspective", "author": "Sifan Wang and Ananyae Kumar Bhartari and Bowen Li and Paris Perdikaris", "abstract": "  Multi-task learning through composite loss functions is fundamental to modern\ndeep learning, yet optimizing competing objectives remains challenging. We\npresent new theoretical and practical approaches for addressing directional\nconflicts between loss terms, demonstrating their effectiveness in\nphysics-informed neural networks (PINNs) where such conflicts are particularly\nchallenging to resolve. Through theoretical analysis, we demonstrate how these\nconflicts limit first-order methods and show that second-order optimization\nnaturally resolves them through implicit gradient alignment. We prove that\nSOAP, a recently proposed quasi-Newton method, efficiently approximates the\nHessian preconditioner, enabling breakthrough performance in PINNs:\nstate-of-the-art results on 10 challenging PDE benchmarks, including the first\nsuccessful application to turbulent flows with Reynolds numbers up to 10,000,\nwith 2-10x accuracy improvements over existing methods. We also introduce a\nnovel gradient alignment score that generalizes cosine similarity to multiple\ngradients, providing a practical tool for analyzing optimization dynamics. Our\nfindings establish frameworks for understanding and resolving gradient\nconflicts, with broad implications for optimization beyond scientific\ncomputing.\n", "link": "http://arxiv.org/abs/2502.00604v2", "date": "2025-09-19", "relevancy": 1.9842, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Alignment%20in%20Physics-informed%20Neural%20Networks%3A%20A%20Second-Order%0A%20%20Optimization%20Perspective&body=Title%3A%20Gradient%20Alignment%20in%20Physics-informed%20Neural%20Networks%3A%20A%20Second-Order%0A%20%20Optimization%20Perspective%0AAuthor%3A%20Sifan%20Wang%20and%20Ananyae%20Kumar%20Bhartari%20and%20Bowen%20Li%20and%20Paris%20Perdikaris%0AAbstract%3A%20%20%20Multi-task%20learning%20through%20composite%20loss%20functions%20is%20fundamental%20to%20modern%0Adeep%20learning%2C%20yet%20optimizing%20competing%20objectives%20remains%20challenging.%20We%0Apresent%20new%20theoretical%20and%20practical%20approaches%20for%20addressing%20directional%0Aconflicts%20between%20loss%20terms%2C%20demonstrating%20their%20effectiveness%20in%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20where%20such%20conflicts%20are%20particularly%0Achallenging%20to%20resolve.%20Through%20theoretical%20analysis%2C%20we%20demonstrate%20how%20these%0Aconflicts%20limit%20first-order%20methods%20and%20show%20that%20second-order%20optimization%0Anaturally%20resolves%20them%20through%20implicit%20gradient%20alignment.%20We%20prove%20that%0ASOAP%2C%20a%20recently%20proposed%20quasi-Newton%20method%2C%20efficiently%20approximates%20the%0AHessian%20preconditioner%2C%20enabling%20breakthrough%20performance%20in%20PINNs%3A%0Astate-of-the-art%20results%20on%2010%20challenging%20PDE%20benchmarks%2C%20including%20the%20first%0Asuccessful%20application%20to%20turbulent%20flows%20with%20Reynolds%20numbers%20up%20to%2010%2C000%2C%0Awith%202-10x%20accuracy%20improvements%20over%20existing%20methods.%20We%20also%20introduce%20a%0Anovel%20gradient%20alignment%20score%20that%20generalizes%20cosine%20similarity%20to%20multiple%0Agradients%2C%20providing%20a%20practical%20tool%20for%20analyzing%20optimization%20dynamics.%20Our%0Afindings%20establish%20frameworks%20for%20understanding%20and%20resolving%20gradient%0Aconflicts%2C%20with%20broad%20implications%20for%20optimization%20beyond%20scientific%0Acomputing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Alignment%2520in%2520Physics-informed%2520Neural%2520Networks%253A%2520A%2520Second-Order%250A%2520%2520Optimization%2520Perspective%26entry.906535625%3DSifan%2520Wang%2520and%2520Ananyae%2520Kumar%2520Bhartari%2520and%2520Bowen%2520Li%2520and%2520Paris%2520Perdikaris%26entry.1292438233%3D%2520%2520Multi-task%2520learning%2520through%2520composite%2520loss%2520functions%2520is%2520fundamental%2520to%2520modern%250Adeep%2520learning%252C%2520yet%2520optimizing%2520competing%2520objectives%2520remains%2520challenging.%2520We%250Apresent%2520new%2520theoretical%2520and%2520practical%2520approaches%2520for%2520addressing%2520directional%250Aconflicts%2520between%2520loss%2520terms%252C%2520demonstrating%2520their%2520effectiveness%2520in%250Aphysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520where%2520such%2520conflicts%2520are%2520particularly%250Achallenging%2520to%2520resolve.%2520Through%2520theoretical%2520analysis%252C%2520we%2520demonstrate%2520how%2520these%250Aconflicts%2520limit%2520first-order%2520methods%2520and%2520show%2520that%2520second-order%2520optimization%250Anaturally%2520resolves%2520them%2520through%2520implicit%2520gradient%2520alignment.%2520We%2520prove%2520that%250ASOAP%252C%2520a%2520recently%2520proposed%2520quasi-Newton%2520method%252C%2520efficiently%2520approximates%2520the%250AHessian%2520preconditioner%252C%2520enabling%2520breakthrough%2520performance%2520in%2520PINNs%253A%250Astate-of-the-art%2520results%2520on%252010%2520challenging%2520PDE%2520benchmarks%252C%2520including%2520the%2520first%250Asuccessful%2520application%2520to%2520turbulent%2520flows%2520with%2520Reynolds%2520numbers%2520up%2520to%252010%252C000%252C%250Awith%25202-10x%2520accuracy%2520improvements%2520over%2520existing%2520methods.%2520We%2520also%2520introduce%2520a%250Anovel%2520gradient%2520alignment%2520score%2520that%2520generalizes%2520cosine%2520similarity%2520to%2520multiple%250Agradients%252C%2520providing%2520a%2520practical%2520tool%2520for%2520analyzing%2520optimization%2520dynamics.%2520Our%250Afindings%2520establish%2520frameworks%2520for%2520understanding%2520and%2520resolving%2520gradient%250Aconflicts%252C%2520with%2520broad%2520implications%2520for%2520optimization%2520beyond%2520scientific%250Acomputing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Alignment%20in%20Physics-informed%20Neural%20Networks%3A%20A%20Second-Order%0A%20%20Optimization%20Perspective&entry.906535625=Sifan%20Wang%20and%20Ananyae%20Kumar%20Bhartari%20and%20Bowen%20Li%20and%20Paris%20Perdikaris&entry.1292438233=%20%20Multi-task%20learning%20through%20composite%20loss%20functions%20is%20fundamental%20to%20modern%0Adeep%20learning%2C%20yet%20optimizing%20competing%20objectives%20remains%20challenging.%20We%0Apresent%20new%20theoretical%20and%20practical%20approaches%20for%20addressing%20directional%0Aconflicts%20between%20loss%20terms%2C%20demonstrating%20their%20effectiveness%20in%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20where%20such%20conflicts%20are%20particularly%0Achallenging%20to%20resolve.%20Through%20theoretical%20analysis%2C%20we%20demonstrate%20how%20these%0Aconflicts%20limit%20first-order%20methods%20and%20show%20that%20second-order%20optimization%0Anaturally%20resolves%20them%20through%20implicit%20gradient%20alignment.%20We%20prove%20that%0ASOAP%2C%20a%20recently%20proposed%20quasi-Newton%20method%2C%20efficiently%20approximates%20the%0AHessian%20preconditioner%2C%20enabling%20breakthrough%20performance%20in%20PINNs%3A%0Astate-of-the-art%20results%20on%2010%20challenging%20PDE%20benchmarks%2C%20including%20the%20first%0Asuccessful%20application%20to%20turbulent%20flows%20with%20Reynolds%20numbers%20up%20to%2010%2C000%2C%0Awith%202-10x%20accuracy%20improvements%20over%20existing%20methods.%20We%20also%20introduce%20a%0Anovel%20gradient%20alignment%20score%20that%20generalizes%20cosine%20similarity%20to%20multiple%0Agradients%2C%20providing%20a%20practical%20tool%20for%20analyzing%20optimization%20dynamics.%20Our%0Afindings%20establish%20frameworks%20for%20understanding%20and%20resolving%20gradient%0Aconflicts%2C%20with%20broad%20implications%20for%20optimization%20beyond%20scientific%0Acomputing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00604v2&entry.124074799=Read"},
{"title": "Generalized Deep Multi-view Clustering via Causal Learning with\n  Partially Aligned Cross-view Correspondence", "author": "Xihong Yang and Siwei Wang and Jiaqi Jin and Fangdi Wang and Tianrui Liu and Yueming Jin and Xinwang Liu and En Zhu and Kunlun He", "abstract": "  Multi-view clustering (MVC) aims to explore the common clustering structure\nacross multiple views. Many existing MVC methods heavily rely on the assumption\nof view consistency, where alignments for corresponding samples across\ndifferent views are ordered in advance. However, real-world scenarios often\npresent a challenge as only partial data is consistently aligned across\ndifferent views, restricting the overall clustering performance. In this work,\nwe consider the model performance decreasing phenomenon caused by data order\nshift (i.e., from fully to partially aligned) as a generalized multi-view\nclustering problem. To tackle this problem, we design a causal multi-view\nclustering network, termed CauMVC. We adopt a causal modeling approach to\nunderstand multi-view clustering procedure. To be specific, we formulate the\npartially aligned data as an intervention and multi-view clustering with\npartially aligned data as an post-intervention inference. However, obtaining\ninvariant features directly can be challenging. Thus, we design a Variational\nAuto-Encoder for causal learning by incorporating an encoder from existing\ninformation to estimate the invariant features. Moreover, a decoder is designed\nto perform the post-intervention inference. Lastly, we design a contrastive\nregularizer to capture sample correlations. To the best of our knowledge, this\npaper is the first work to deal generalized multi-view clustering via causal\nlearning. Empirical experiments on both fully and partially aligned data\nillustrate the strong generalization and effectiveness of CauMVC.\n", "link": "http://arxiv.org/abs/2509.16022v1", "date": "2025-09-19", "relevancy": 1.5801, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Deep%20Multi-view%20Clustering%20via%20Causal%20Learning%20with%0A%20%20Partially%20Aligned%20Cross-view%20Correspondence&body=Title%3A%20Generalized%20Deep%20Multi-view%20Clustering%20via%20Causal%20Learning%20with%0A%20%20Partially%20Aligned%20Cross-view%20Correspondence%0AAuthor%3A%20Xihong%20Yang%20and%20Siwei%20Wang%20and%20Jiaqi%20Jin%20and%20Fangdi%20Wang%20and%20Tianrui%20Liu%20and%20Yueming%20Jin%20and%20Xinwang%20Liu%20and%20En%20Zhu%20and%20Kunlun%20He%0AAbstract%3A%20%20%20Multi-view%20clustering%20%28MVC%29%20aims%20to%20explore%20the%20common%20clustering%20structure%0Aacross%20multiple%20views.%20Many%20existing%20MVC%20methods%20heavily%20rely%20on%20the%20assumption%0Aof%20view%20consistency%2C%20where%20alignments%20for%20corresponding%20samples%20across%0Adifferent%20views%20are%20ordered%20in%20advance.%20However%2C%20real-world%20scenarios%20often%0Apresent%20a%20challenge%20as%20only%20partial%20data%20is%20consistently%20aligned%20across%0Adifferent%20views%2C%20restricting%20the%20overall%20clustering%20performance.%20In%20this%20work%2C%0Awe%20consider%20the%20model%20performance%20decreasing%20phenomenon%20caused%20by%20data%20order%0Ashift%20%28i.e.%2C%20from%20fully%20to%20partially%20aligned%29%20as%20a%20generalized%20multi-view%0Aclustering%20problem.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20causal%20multi-view%0Aclustering%20network%2C%20termed%20CauMVC.%20We%20adopt%20a%20causal%20modeling%20approach%20to%0Aunderstand%20multi-view%20clustering%20procedure.%20To%20be%20specific%2C%20we%20formulate%20the%0Apartially%20aligned%20data%20as%20an%20intervention%20and%20multi-view%20clustering%20with%0Apartially%20aligned%20data%20as%20an%20post-intervention%20inference.%20However%2C%20obtaining%0Ainvariant%20features%20directly%20can%20be%20challenging.%20Thus%2C%20we%20design%20a%20Variational%0AAuto-Encoder%20for%20causal%20learning%20by%20incorporating%20an%20encoder%20from%20existing%0Ainformation%20to%20estimate%20the%20invariant%20features.%20Moreover%2C%20a%20decoder%20is%20designed%0Ato%20perform%20the%20post-intervention%20inference.%20Lastly%2C%20we%20design%20a%20contrastive%0Aregularizer%20to%20capture%20sample%20correlations.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Apaper%20is%20the%20first%20work%20to%20deal%20generalized%20multi-view%20clustering%20via%20causal%0Alearning.%20Empirical%20experiments%20on%20both%20fully%20and%20partially%20aligned%20data%0Aillustrate%20the%20strong%20generalization%20and%20effectiveness%20of%20CauMVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Deep%2520Multi-view%2520Clustering%2520via%2520Causal%2520Learning%2520with%250A%2520%2520Partially%2520Aligned%2520Cross-view%2520Correspondence%26entry.906535625%3DXihong%2520Yang%2520and%2520Siwei%2520Wang%2520and%2520Jiaqi%2520Jin%2520and%2520Fangdi%2520Wang%2520and%2520Tianrui%2520Liu%2520and%2520Yueming%2520Jin%2520and%2520Xinwang%2520Liu%2520and%2520En%2520Zhu%2520and%2520Kunlun%2520He%26entry.1292438233%3D%2520%2520Multi-view%2520clustering%2520%2528MVC%2529%2520aims%2520to%2520explore%2520the%2520common%2520clustering%2520structure%250Aacross%2520multiple%2520views.%2520Many%2520existing%2520MVC%2520methods%2520heavily%2520rely%2520on%2520the%2520assumption%250Aof%2520view%2520consistency%252C%2520where%2520alignments%2520for%2520corresponding%2520samples%2520across%250Adifferent%2520views%2520are%2520ordered%2520in%2520advance.%2520However%252C%2520real-world%2520scenarios%2520often%250Apresent%2520a%2520challenge%2520as%2520only%2520partial%2520data%2520is%2520consistently%2520aligned%2520across%250Adifferent%2520views%252C%2520restricting%2520the%2520overall%2520clustering%2520performance.%2520In%2520this%2520work%252C%250Awe%2520consider%2520the%2520model%2520performance%2520decreasing%2520phenomenon%2520caused%2520by%2520data%2520order%250Ashift%2520%2528i.e.%252C%2520from%2520fully%2520to%2520partially%2520aligned%2529%2520as%2520a%2520generalized%2520multi-view%250Aclustering%2520problem.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520design%2520a%2520causal%2520multi-view%250Aclustering%2520network%252C%2520termed%2520CauMVC.%2520We%2520adopt%2520a%2520causal%2520modeling%2520approach%2520to%250Aunderstand%2520multi-view%2520clustering%2520procedure.%2520To%2520be%2520specific%252C%2520we%2520formulate%2520the%250Apartially%2520aligned%2520data%2520as%2520an%2520intervention%2520and%2520multi-view%2520clustering%2520with%250Apartially%2520aligned%2520data%2520as%2520an%2520post-intervention%2520inference.%2520However%252C%2520obtaining%250Ainvariant%2520features%2520directly%2520can%2520be%2520challenging.%2520Thus%252C%2520we%2520design%2520a%2520Variational%250AAuto-Encoder%2520for%2520causal%2520learning%2520by%2520incorporating%2520an%2520encoder%2520from%2520existing%250Ainformation%2520to%2520estimate%2520the%2520invariant%2520features.%2520Moreover%252C%2520a%2520decoder%2520is%2520designed%250Ato%2520perform%2520the%2520post-intervention%2520inference.%2520Lastly%252C%2520we%2520design%2520a%2520contrastive%250Aregularizer%2520to%2520capture%2520sample%2520correlations.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Apaper%2520is%2520the%2520first%2520work%2520to%2520deal%2520generalized%2520multi-view%2520clustering%2520via%2520causal%250Alearning.%2520Empirical%2520experiments%2520on%2520both%2520fully%2520and%2520partially%2520aligned%2520data%250Aillustrate%2520the%2520strong%2520generalization%2520and%2520effectiveness%2520of%2520CauMVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Deep%20Multi-view%20Clustering%20via%20Causal%20Learning%20with%0A%20%20Partially%20Aligned%20Cross-view%20Correspondence&entry.906535625=Xihong%20Yang%20and%20Siwei%20Wang%20and%20Jiaqi%20Jin%20and%20Fangdi%20Wang%20and%20Tianrui%20Liu%20and%20Yueming%20Jin%20and%20Xinwang%20Liu%20and%20En%20Zhu%20and%20Kunlun%20He&entry.1292438233=%20%20Multi-view%20clustering%20%28MVC%29%20aims%20to%20explore%20the%20common%20clustering%20structure%0Aacross%20multiple%20views.%20Many%20existing%20MVC%20methods%20heavily%20rely%20on%20the%20assumption%0Aof%20view%20consistency%2C%20where%20alignments%20for%20corresponding%20samples%20across%0Adifferent%20views%20are%20ordered%20in%20advance.%20However%2C%20real-world%20scenarios%20often%0Apresent%20a%20challenge%20as%20only%20partial%20data%20is%20consistently%20aligned%20across%0Adifferent%20views%2C%20restricting%20the%20overall%20clustering%20performance.%20In%20this%20work%2C%0Awe%20consider%20the%20model%20performance%20decreasing%20phenomenon%20caused%20by%20data%20order%0Ashift%20%28i.e.%2C%20from%20fully%20to%20partially%20aligned%29%20as%20a%20generalized%20multi-view%0Aclustering%20problem.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20causal%20multi-view%0Aclustering%20network%2C%20termed%20CauMVC.%20We%20adopt%20a%20causal%20modeling%20approach%20to%0Aunderstand%20multi-view%20clustering%20procedure.%20To%20be%20specific%2C%20we%20formulate%20the%0Apartially%20aligned%20data%20as%20an%20intervention%20and%20multi-view%20clustering%20with%0Apartially%20aligned%20data%20as%20an%20post-intervention%20inference.%20However%2C%20obtaining%0Ainvariant%20features%20directly%20can%20be%20challenging.%20Thus%2C%20we%20design%20a%20Variational%0AAuto-Encoder%20for%20causal%20learning%20by%20incorporating%20an%20encoder%20from%20existing%0Ainformation%20to%20estimate%20the%20invariant%20features.%20Moreover%2C%20a%20decoder%20is%20designed%0Ato%20perform%20the%20post-intervention%20inference.%20Lastly%2C%20we%20design%20a%20contrastive%0Aregularizer%20to%20capture%20sample%20correlations.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Apaper%20is%20the%20first%20work%20to%20deal%20generalized%20multi-view%20clustering%20via%20causal%0Alearning.%20Empirical%20experiments%20on%20both%20fully%20and%20partially%20aligned%20data%0Aillustrate%20the%20strong%20generalization%20and%20effectiveness%20of%20CauMVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16022v1&entry.124074799=Read"},
{"title": "Inverting Trojans in LLMs", "author": "Zhengxing Li and Guangmingmei Yang and Jayaram Raghuram and David J. Miller and George Kesidis", "abstract": "  While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases.\n", "link": "http://arxiv.org/abs/2509.16203v1", "date": "2025-09-19", "relevancy": 2.0116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4062}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverting%20Trojans%20in%20LLMs&body=Title%3A%20Inverting%20Trojans%20in%20LLMs%0AAuthor%3A%20Zhengxing%20Li%20and%20Guangmingmei%20Yang%20and%20Jayaram%20Raghuram%20and%20David%20J.%20Miller%20and%20George%20Kesidis%0AAbstract%3A%20%20%20While%20effective%20backdoor%20detection%20and%20inversion%20schemes%20have%20been%20developed%0Afor%20AIs%20used%20e.g.%20for%20images%2C%20there%20are%20challenges%20in%20%22porting%22%20these%20methods%0Ato%20LLMs.%20First%2C%20the%20LLM%20input%20space%20is%20discrete%2C%20which%20precludes%20gradient-based%0Asearch%20over%20this%20space%2C%20central%20to%20many%20backdoor%20inversion%20methods.%20Second%2C%0Athere%20are%20~30%2C000%5Ek%20k-tuples%20to%20consider%2C%20k%20the%20token-length%20of%20a%20putative%0Atrigger.%20Third%2C%20for%20LLMs%20there%20is%20the%20need%20to%20blacklist%20tokens%20that%20have%20strong%0Amarginal%20associations%20with%20the%20putative%20target%20response%20%28class%29%20of%20an%20attack%2C%0Aas%20such%20tokens%20give%20false%20detection%20signals.%20However%2C%20good%20blacklists%20may%20not%0Aexist%20for%20some%20domains.%20We%20propose%20a%20LLM%20trigger%20inversion%20approach%20with%20three%0Akey%20components%3A%20i%29%20discrete%20search%2C%20with%20putative%20triggers%20greedily%20accreted%2C%0Astarting%20from%20a%20select%20list%20of%20singletons%3B%20ii%29%20implicit%20blacklisting%2C%20achieved%0Aby%20evaluating%20the%20average%20cosine%20similarity%2C%20in%20activation%20space%2C%20between%20a%0Acandidate%20trigger%20and%20a%20small%20clean%20set%20of%20samples%20from%20the%20putative%20target%0Aclass%3B%20iii%29%20detection%20when%20a%20candidate%20trigger%20elicits%20high%20misclassifications%2C%0Aand%20with%20unusually%20high%20decision%20confidence.%20Unlike%20many%20recent%20works%2C%20we%0Ademonstrate%20that%20our%20approach%20reliably%20detects%20and%20successfully%20inverts%0Aground-truth%20backdoor%20trigger%20phrases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverting%2520Trojans%2520in%2520LLMs%26entry.906535625%3DZhengxing%2520Li%2520and%2520Guangmingmei%2520Yang%2520and%2520Jayaram%2520Raghuram%2520and%2520David%2520J.%2520Miller%2520and%2520George%2520Kesidis%26entry.1292438233%3D%2520%2520While%2520effective%2520backdoor%2520detection%2520and%2520inversion%2520schemes%2520have%2520been%2520developed%250Afor%2520AIs%2520used%2520e.g.%2520for%2520images%252C%2520there%2520are%2520challenges%2520in%2520%2522porting%2522%2520these%2520methods%250Ato%2520LLMs.%2520First%252C%2520the%2520LLM%2520input%2520space%2520is%2520discrete%252C%2520which%2520precludes%2520gradient-based%250Asearch%2520over%2520this%2520space%252C%2520central%2520to%2520many%2520backdoor%2520inversion%2520methods.%2520Second%252C%250Athere%2520are%2520~30%252C000%255Ek%2520k-tuples%2520to%2520consider%252C%2520k%2520the%2520token-length%2520of%2520a%2520putative%250Atrigger.%2520Third%252C%2520for%2520LLMs%2520there%2520is%2520the%2520need%2520to%2520blacklist%2520tokens%2520that%2520have%2520strong%250Amarginal%2520associations%2520with%2520the%2520putative%2520target%2520response%2520%2528class%2529%2520of%2520an%2520attack%252C%250Aas%2520such%2520tokens%2520give%2520false%2520detection%2520signals.%2520However%252C%2520good%2520blacklists%2520may%2520not%250Aexist%2520for%2520some%2520domains.%2520We%2520propose%2520a%2520LLM%2520trigger%2520inversion%2520approach%2520with%2520three%250Akey%2520components%253A%2520i%2529%2520discrete%2520search%252C%2520with%2520putative%2520triggers%2520greedily%2520accreted%252C%250Astarting%2520from%2520a%2520select%2520list%2520of%2520singletons%253B%2520ii%2529%2520implicit%2520blacklisting%252C%2520achieved%250Aby%2520evaluating%2520the%2520average%2520cosine%2520similarity%252C%2520in%2520activation%2520space%252C%2520between%2520a%250Acandidate%2520trigger%2520and%2520a%2520small%2520clean%2520set%2520of%2520samples%2520from%2520the%2520putative%2520target%250Aclass%253B%2520iii%2529%2520detection%2520when%2520a%2520candidate%2520trigger%2520elicits%2520high%2520misclassifications%252C%250Aand%2520with%2520unusually%2520high%2520decision%2520confidence.%2520Unlike%2520many%2520recent%2520works%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520reliably%2520detects%2520and%2520successfully%2520inverts%250Aground-truth%2520backdoor%2520trigger%2520phrases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverting%20Trojans%20in%20LLMs&entry.906535625=Zhengxing%20Li%20and%20Guangmingmei%20Yang%20and%20Jayaram%20Raghuram%20and%20David%20J.%20Miller%20and%20George%20Kesidis&entry.1292438233=%20%20While%20effective%20backdoor%20detection%20and%20inversion%20schemes%20have%20been%20developed%0Afor%20AIs%20used%20e.g.%20for%20images%2C%20there%20are%20challenges%20in%20%22porting%22%20these%20methods%0Ato%20LLMs.%20First%2C%20the%20LLM%20input%20space%20is%20discrete%2C%20which%20precludes%20gradient-based%0Asearch%20over%20this%20space%2C%20central%20to%20many%20backdoor%20inversion%20methods.%20Second%2C%0Athere%20are%20~30%2C000%5Ek%20k-tuples%20to%20consider%2C%20k%20the%20token-length%20of%20a%20putative%0Atrigger.%20Third%2C%20for%20LLMs%20there%20is%20the%20need%20to%20blacklist%20tokens%20that%20have%20strong%0Amarginal%20associations%20with%20the%20putative%20target%20response%20%28class%29%20of%20an%20attack%2C%0Aas%20such%20tokens%20give%20false%20detection%20signals.%20However%2C%20good%20blacklists%20may%20not%0Aexist%20for%20some%20domains.%20We%20propose%20a%20LLM%20trigger%20inversion%20approach%20with%20three%0Akey%20components%3A%20i%29%20discrete%20search%2C%20with%20putative%20triggers%20greedily%20accreted%2C%0Astarting%20from%20a%20select%20list%20of%20singletons%3B%20ii%29%20implicit%20blacklisting%2C%20achieved%0Aby%20evaluating%20the%20average%20cosine%20similarity%2C%20in%20activation%20space%2C%20between%20a%0Acandidate%20trigger%20and%20a%20small%20clean%20set%20of%20samples%20from%20the%20putative%20target%0Aclass%3B%20iii%29%20detection%20when%20a%20candidate%20trigger%20elicits%20high%20misclassifications%2C%0Aand%20with%20unusually%20high%20decision%20confidence.%20Unlike%20many%20recent%20works%2C%20we%0Ademonstrate%20that%20our%20approach%20reliably%20detects%20and%20successfully%20inverts%0Aground-truth%20backdoor%20trigger%20phrases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16203v1&entry.124074799=Read"},
{"title": "FedHK-MVFC: Federated Heat Kernel Multi-View Clustering", "author": "Kristina P. Sinaga", "abstract": "  In the realm of distributed AI and privacy-focused medical applications, we\npropose a framework for multi-view clustering that links quantum field theory\nwith federated healthcare analytics. Our method uses heat-kernel coefficients\nfrom spectral analysis to convert Euclidean distances into geometry-aware\nsimilarity measures, capturing the structure of diverse medical data. We lay\nthis out through the Heat Kernel Distance (HKD) transformation with convergence\nguarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy\nClustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View\nFuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across\nhospitals using differential privacy and secure aggregation to facilitate\nHIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular\npatients show an $8-12 \\%$ increase in clustering accuracy, $70 \\%$ reduced\ncommunication, and $98.2 \\%$ efficiency retention over centralized methods.\nValidated on 10,000 patient records across two hospitals, it proves useful for\ncollaborative phenotyping involving ECG, cardiac imaging, and behavioral data.\nOur theoretical contributions include update rules with proven convergence,\nadaptive view weighting, and privacy-preserving protocols. This presents a new\nstandard for geometry-aware federated learning in healthcare, turning advanced\nmath into workable solutions for analyzing sensitive medical data while\nensuring both rigor and clinical relevance.\n", "link": "http://arxiv.org/abs/2509.15844v1", "date": "2025-09-19", "relevancy": 1.848, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4658}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4618}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedHK-MVFC%3A%20Federated%20Heat%20Kernel%20Multi-View%20Clustering&body=Title%3A%20FedHK-MVFC%3A%20Federated%20Heat%20Kernel%20Multi-View%20Clustering%0AAuthor%3A%20Kristina%20P.%20Sinaga%0AAbstract%3A%20%20%20In%20the%20realm%20of%20distributed%20AI%20and%20privacy-focused%20medical%20applications%2C%20we%0Apropose%20a%20framework%20for%20multi-view%20clustering%20that%20links%20quantum%20field%20theory%0Awith%20federated%20healthcare%20analytics.%20Our%20method%20uses%20heat-kernel%20coefficients%0Afrom%20spectral%20analysis%20to%20convert%20Euclidean%20distances%20into%20geometry-aware%0Asimilarity%20measures%2C%20capturing%20the%20structure%20of%20diverse%20medical%20data.%20We%20lay%0Athis%20out%20through%20the%20Heat%20Kernel%20Distance%20%28HKD%29%20transformation%20with%20convergence%0Aguarantees.%20Two%20algorithms%20are%20developed%3A%20Heat%20Kernel-Enhanced%20Multi-View%20Fuzzy%0AClustering%20%28HK-MVFC%29%20for%20central%20analysis%2C%20and%20Federated%20Heat%20Kernel%20Multi-View%0AFuzzy%20Clustering%20%28FedHK-MVFC%29%20for%20secure%2C%20privacy-preserving%20learning%20across%0Ahospitals%20using%20differential%20privacy%20and%20secure%20aggregation%20to%20facilitate%0AHIPAA-compliant%20collaboration.%20Tests%20on%20synthetic%20datasets%20of%20cardiovascular%0Apatients%20show%20an%20%248-12%20%5C%25%24%20increase%20in%20clustering%20accuracy%2C%20%2470%20%5C%25%24%20reduced%0Acommunication%2C%20and%20%2498.2%20%5C%25%24%20efficiency%20retention%20over%20centralized%20methods.%0AValidated%20on%2010%2C000%20patient%20records%20across%20two%20hospitals%2C%20it%20proves%20useful%20for%0Acollaborative%20phenotyping%20involving%20ECG%2C%20cardiac%20imaging%2C%20and%20behavioral%20data.%0AOur%20theoretical%20contributions%20include%20update%20rules%20with%20proven%20convergence%2C%0Aadaptive%20view%20weighting%2C%20and%20privacy-preserving%20protocols.%20This%20presents%20a%20new%0Astandard%20for%20geometry-aware%20federated%20learning%20in%20healthcare%2C%20turning%20advanced%0Amath%20into%20workable%20solutions%20for%20analyzing%20sensitive%20medical%20data%20while%0Aensuring%20both%20rigor%20and%20clinical%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedHK-MVFC%253A%2520Federated%2520Heat%2520Kernel%2520Multi-View%2520Clustering%26entry.906535625%3DKristina%2520P.%2520Sinaga%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520distributed%2520AI%2520and%2520privacy-focused%2520medical%2520applications%252C%2520we%250Apropose%2520a%2520framework%2520for%2520multi-view%2520clustering%2520that%2520links%2520quantum%2520field%2520theory%250Awith%2520federated%2520healthcare%2520analytics.%2520Our%2520method%2520uses%2520heat-kernel%2520coefficients%250Afrom%2520spectral%2520analysis%2520to%2520convert%2520Euclidean%2520distances%2520into%2520geometry-aware%250Asimilarity%2520measures%252C%2520capturing%2520the%2520structure%2520of%2520diverse%2520medical%2520data.%2520We%2520lay%250Athis%2520out%2520through%2520the%2520Heat%2520Kernel%2520Distance%2520%2528HKD%2529%2520transformation%2520with%2520convergence%250Aguarantees.%2520Two%2520algorithms%2520are%2520developed%253A%2520Heat%2520Kernel-Enhanced%2520Multi-View%2520Fuzzy%250AClustering%2520%2528HK-MVFC%2529%2520for%2520central%2520analysis%252C%2520and%2520Federated%2520Heat%2520Kernel%2520Multi-View%250AFuzzy%2520Clustering%2520%2528FedHK-MVFC%2529%2520for%2520secure%252C%2520privacy-preserving%2520learning%2520across%250Ahospitals%2520using%2520differential%2520privacy%2520and%2520secure%2520aggregation%2520to%2520facilitate%250AHIPAA-compliant%2520collaboration.%2520Tests%2520on%2520synthetic%2520datasets%2520of%2520cardiovascular%250Apatients%2520show%2520an%2520%25248-12%2520%255C%2525%2524%2520increase%2520in%2520clustering%2520accuracy%252C%2520%252470%2520%255C%2525%2524%2520reduced%250Acommunication%252C%2520and%2520%252498.2%2520%255C%2525%2524%2520efficiency%2520retention%2520over%2520centralized%2520methods.%250AValidated%2520on%252010%252C000%2520patient%2520records%2520across%2520two%2520hospitals%252C%2520it%2520proves%2520useful%2520for%250Acollaborative%2520phenotyping%2520involving%2520ECG%252C%2520cardiac%2520imaging%252C%2520and%2520behavioral%2520data.%250AOur%2520theoretical%2520contributions%2520include%2520update%2520rules%2520with%2520proven%2520convergence%252C%250Aadaptive%2520view%2520weighting%252C%2520and%2520privacy-preserving%2520protocols.%2520This%2520presents%2520a%2520new%250Astandard%2520for%2520geometry-aware%2520federated%2520learning%2520in%2520healthcare%252C%2520turning%2520advanced%250Amath%2520into%2520workable%2520solutions%2520for%2520analyzing%2520sensitive%2520medical%2520data%2520while%250Aensuring%2520both%2520rigor%2520and%2520clinical%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedHK-MVFC%3A%20Federated%20Heat%20Kernel%20Multi-View%20Clustering&entry.906535625=Kristina%20P.%20Sinaga&entry.1292438233=%20%20In%20the%20realm%20of%20distributed%20AI%20and%20privacy-focused%20medical%20applications%2C%20we%0Apropose%20a%20framework%20for%20multi-view%20clustering%20that%20links%20quantum%20field%20theory%0Awith%20federated%20healthcare%20analytics.%20Our%20method%20uses%20heat-kernel%20coefficients%0Afrom%20spectral%20analysis%20to%20convert%20Euclidean%20distances%20into%20geometry-aware%0Asimilarity%20measures%2C%20capturing%20the%20structure%20of%20diverse%20medical%20data.%20We%20lay%0Athis%20out%20through%20the%20Heat%20Kernel%20Distance%20%28HKD%29%20transformation%20with%20convergence%0Aguarantees.%20Two%20algorithms%20are%20developed%3A%20Heat%20Kernel-Enhanced%20Multi-View%20Fuzzy%0AClustering%20%28HK-MVFC%29%20for%20central%20analysis%2C%20and%20Federated%20Heat%20Kernel%20Multi-View%0AFuzzy%20Clustering%20%28FedHK-MVFC%29%20for%20secure%2C%20privacy-preserving%20learning%20across%0Ahospitals%20using%20differential%20privacy%20and%20secure%20aggregation%20to%20facilitate%0AHIPAA-compliant%20collaboration.%20Tests%20on%20synthetic%20datasets%20of%20cardiovascular%0Apatients%20show%20an%20%248-12%20%5C%25%24%20increase%20in%20clustering%20accuracy%2C%20%2470%20%5C%25%24%20reduced%0Acommunication%2C%20and%20%2498.2%20%5C%25%24%20efficiency%20retention%20over%20centralized%20methods.%0AValidated%20on%2010%2C000%20patient%20records%20across%20two%20hospitals%2C%20it%20proves%20useful%20for%0Acollaborative%20phenotyping%20involving%20ECG%2C%20cardiac%20imaging%2C%20and%20behavioral%20data.%0AOur%20theoretical%20contributions%20include%20update%20rules%20with%20proven%20convergence%2C%0Aadaptive%20view%20weighting%2C%20and%20privacy-preserving%20protocols.%20This%20presents%20a%20new%0Astandard%20for%20geometry-aware%20federated%20learning%20in%20healthcare%2C%20turning%20advanced%0Amath%20into%20workable%20solutions%20for%20analyzing%20sensitive%20medical%20data%20while%0Aensuring%20both%20rigor%20and%20clinical%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15844v1&entry.124074799=Read"},
{"title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation", "author": "Senkang Hu and Xudong Han and Jinqi Jiang and Yihang Tao and Zihan Fang and Sam Tak Wu Kwong and Yuguang Fang", "abstract": "  Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.\n", "link": "http://arxiv.org/abs/2509.15888v1", "date": "2025-09-19", "relevancy": 1.894, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation&body=Title%3A%20Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation%0AAuthor%3A%20Senkang%20Hu%20and%20Xudong%20Han%20and%20Jinqi%20Jiang%20and%20Yihang%20Tao%20and%20Zihan%20Fang%20and%20Sam%20Tak%20Wu%20Kwong%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20Adapting%20billion-parameter%20language%20models%20to%20a%20downstream%20task%20is%20still%0Acostly%2C%20even%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20We%20re-cast%20task%0Aadaptation%20as%20output-distribution%20alignment%3A%20the%20objective%20is%20to%20steer%20the%0Aoutput%20distribution%20toward%20the%20task%20distribution%20directly%20during%20decoding%0Arather%20than%20indirectly%20through%20weight%20updates.%20Building%20on%20this%20view%2C%20we%0Aintroduce%20Steering%20Vector%20Decoding%20%28SVD%29%2C%20a%20lightweight%2C%20PEFT-compatible%2C%20and%0Atheoretically%20grounded%20method.%20We%20start%20with%20a%20short%20warm-start%20fine-tune%20and%0Aextract%20a%20task-aware%20steering%20vector%20from%20the%20Kullback-Leibler%20%28KL%29%20divergence%0Agradient%20between%20the%20output%20distribution%20of%20the%20warm-started%20and%20pre-trained%0Amodels.%20This%20steering%20vector%20is%20then%20used%20to%20guide%20the%20decoding%20process%20to%0Asteer%20the%20model%27s%20output%20distribution%20towards%20the%20task%20distribution.%20We%0Atheoretically%20prove%20that%20SVD%20is%20first-order%20equivalent%20to%20the%20gradient%20step%20of%0Afull%20fine-tuning%20and%20derive%20a%20globally%20optimal%20solution%20for%20the%20strength%20of%20the%0Asteering%20vector.%20Across%20three%20tasks%20and%20nine%20benchmarks%2C%20SVD%20paired%20with%20four%0Astandard%20PEFT%20methods%20improves%20multiple-choice%20accuracy%20by%20up%20to%205%20points%20and%0Aopen-ended%20truthfulness%20by%202%20points%2C%20with%20similar%20gains%20%281-2%20points%29%20on%0Acommonsense%20datasets%20without%20adding%20trainable%20parameters%20beyond%20the%20PEFT%0Aadapter.%20SVD%20thus%20offers%20a%20lightweight%2C%20theoretically%20grounded%20path%20to%20stronger%0Atask%20adaptation%20for%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-Aligned%2520Decoding%2520for%2520Efficient%2520LLM%2520Task%2520Adaptation%26entry.906535625%3DSenkang%2520Hu%2520and%2520Xudong%2520Han%2520and%2520Jinqi%2520Jiang%2520and%2520Yihang%2520Tao%2520and%2520Zihan%2520Fang%2520and%2520Sam%2520Tak%2520Wu%2520Kwong%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520Adapting%2520billion-parameter%2520language%2520models%2520to%2520a%2520downstream%2520task%2520is%2520still%250Acostly%252C%2520even%2520with%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529.%2520We%2520re-cast%2520task%250Aadaptation%2520as%2520output-distribution%2520alignment%253A%2520the%2520objective%2520is%2520to%2520steer%2520the%250Aoutput%2520distribution%2520toward%2520the%2520task%2520distribution%2520directly%2520during%2520decoding%250Arather%2520than%2520indirectly%2520through%2520weight%2520updates.%2520Building%2520on%2520this%2520view%252C%2520we%250Aintroduce%2520Steering%2520Vector%2520Decoding%2520%2528SVD%2529%252C%2520a%2520lightweight%252C%2520PEFT-compatible%252C%2520and%250Atheoretically%2520grounded%2520method.%2520We%2520start%2520with%2520a%2520short%2520warm-start%2520fine-tune%2520and%250Aextract%2520a%2520task-aware%2520steering%2520vector%2520from%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%250Agradient%2520between%2520the%2520output%2520distribution%2520of%2520the%2520warm-started%2520and%2520pre-trained%250Amodels.%2520This%2520steering%2520vector%2520is%2520then%2520used%2520to%2520guide%2520the%2520decoding%2520process%2520to%250Asteer%2520the%2520model%2527s%2520output%2520distribution%2520towards%2520the%2520task%2520distribution.%2520We%250Atheoretically%2520prove%2520that%2520SVD%2520is%2520first-order%2520equivalent%2520to%2520the%2520gradient%2520step%2520of%250Afull%2520fine-tuning%2520and%2520derive%2520a%2520globally%2520optimal%2520solution%2520for%2520the%2520strength%2520of%2520the%250Asteering%2520vector.%2520Across%2520three%2520tasks%2520and%2520nine%2520benchmarks%252C%2520SVD%2520paired%2520with%2520four%250Astandard%2520PEFT%2520methods%2520improves%2520multiple-choice%2520accuracy%2520by%2520up%2520to%25205%2520points%2520and%250Aopen-ended%2520truthfulness%2520by%25202%2520points%252C%2520with%2520similar%2520gains%2520%25281-2%2520points%2529%2520on%250Acommonsense%2520datasets%2520without%2520adding%2520trainable%2520parameters%2520beyond%2520the%2520PEFT%250Aadapter.%2520SVD%2520thus%2520offers%2520a%2520lightweight%252C%2520theoretically%2520grounded%2520path%2520to%2520stronger%250Atask%2520adaptation%2520for%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation&entry.906535625=Senkang%20Hu%20and%20Xudong%20Han%20and%20Jinqi%20Jiang%20and%20Yihang%20Tao%20and%20Zihan%20Fang%20and%20Sam%20Tak%20Wu%20Kwong%20and%20Yuguang%20Fang&entry.1292438233=%20%20Adapting%20billion-parameter%20language%20models%20to%20a%20downstream%20task%20is%20still%0Acostly%2C%20even%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20We%20re-cast%20task%0Aadaptation%20as%20output-distribution%20alignment%3A%20the%20objective%20is%20to%20steer%20the%0Aoutput%20distribution%20toward%20the%20task%20distribution%20directly%20during%20decoding%0Arather%20than%20indirectly%20through%20weight%20updates.%20Building%20on%20this%20view%2C%20we%0Aintroduce%20Steering%20Vector%20Decoding%20%28SVD%29%2C%20a%20lightweight%2C%20PEFT-compatible%2C%20and%0Atheoretically%20grounded%20method.%20We%20start%20with%20a%20short%20warm-start%20fine-tune%20and%0Aextract%20a%20task-aware%20steering%20vector%20from%20the%20Kullback-Leibler%20%28KL%29%20divergence%0Agradient%20between%20the%20output%20distribution%20of%20the%20warm-started%20and%20pre-trained%0Amodels.%20This%20steering%20vector%20is%20then%20used%20to%20guide%20the%20decoding%20process%20to%0Asteer%20the%20model%27s%20output%20distribution%20towards%20the%20task%20distribution.%20We%0Atheoretically%20prove%20that%20SVD%20is%20first-order%20equivalent%20to%20the%20gradient%20step%20of%0Afull%20fine-tuning%20and%20derive%20a%20globally%20optimal%20solution%20for%20the%20strength%20of%20the%0Asteering%20vector.%20Across%20three%20tasks%20and%20nine%20benchmarks%2C%20SVD%20paired%20with%20four%0Astandard%20PEFT%20methods%20improves%20multiple-choice%20accuracy%20by%20up%20to%205%20points%20and%0Aopen-ended%20truthfulness%20by%202%20points%2C%20with%20similar%20gains%20%281-2%20points%29%20on%0Acommonsense%20datasets%20without%20adding%20trainable%20parameters%20beyond%20the%20PEFT%0Aadapter.%20SVD%20thus%20offers%20a%20lightweight%2C%20theoretically%20grounded%20path%20to%20stronger%0Atask%20adaptation%20for%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15888v1&entry.124074799=Read"},
{"title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning", "author": "Zinan Tang and Xin Gao and Qizhi Pei and Zhuoshi Pan and Mengzhang Cai and Jiang Wu and Conghui He and Lijun Wu", "abstract": "  Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are coming soon. Our datasets,\nmodels, and code are publicly available at https://github.com/Word2VecT/Middo.\n", "link": "http://arxiv.org/abs/2508.21589v3", "date": "2025-09-19", "relevancy": 1.6026, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5442}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning&body=Title%3A%20Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning%0AAuthor%3A%20Zinan%20Tang%20and%20Xin%20Gao%20and%20Qizhi%20Pei%20and%20Zhuoshi%20Pan%20and%20Mengzhang%20Cai%20and%20Jiang%20Wu%20and%20Conghui%20He%20and%20Lijun%20Wu%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20Large%20Language%20Models%20%28LLM%29%20fundamentally%20rely%0Aon%20high-quality%20training%20data.%20While%20data%20selection%20and%20data%20synthesis%20are%20two%0Acommon%20strategies%20to%20improve%20data%20quality%2C%20existing%20approaches%20often%20face%0Alimitations%20in%20static%20dataset%20curation%20that%20fail%20to%20adapt%20to%20evolving%20model%0Acapabilities.%20In%20this%20paper%2C%20we%20introduce%20Middo%2C%20a%20self-evolving%20Model-informed%0Adynamic%20data%20optimization%20framework%20that%20uses%20model-aware%20data%20selection%20and%0Acontext-preserving%20data%20refinement.%20Unlike%20conventional%20one-off%0Afiltering/synthesis%20methods%2C%20our%20framework%20establishes%20a%20closed-loop%0Aoptimization%20system%3A%20%281%29%20A%20self-referential%20diagnostic%20module%20proactively%0Aidentifies%20suboptimal%20samples%20through%20tri-axial%20model%20signals%20-%20loss%20patterns%0A%28complexity%29%2C%20embedding%20cluster%20dynamics%20%28diversity%29%2C%20and%20self-alignment%20scores%0A%28quality%29%3B%20%282%29%20An%20adaptive%20optimization%20engine%20then%20transforms%20suboptimal%0Asamples%20into%20pedagogically%20valuable%20training%20points%20while%20preserving%20semantic%0Aintegrity%3B%20%283%29%20This%20optimization%20process%20continuously%20evolves%20with%20model%0Acapability%20through%20dynamic%20learning%20principles.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20Middo%20consistently%20enhances%20the%20quality%20of%20seed%0Adata%20and%20boosts%20LLM%27s%20performance%20with%20improving%20accuracy%20by%207.15%25%20on%20average%0Awhile%20maintaining%20the%20original%20dataset%20scale.%20This%20work%20establishes%20a%20new%0Aparadigm%20for%20sustainable%20LLM%20training%20through%20dynamic%20human-AI%20co-evolution%20of%0Adata%20and%20models.%20Our%20datasets%2C%20models%2C%20and%20code%20are%20coming%20soon.%20Our%20datasets%2C%0Amodels%2C%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Word2VecT/Middo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21589v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiddo%253A%2520Model-Informed%2520Dynamic%2520Data%2520Optimization%2520for%2520Enhanced%2520LLM%250A%2520%2520Fine-Tuning%2520via%2520Closed-Loop%2520Learning%26entry.906535625%3DZinan%2520Tang%2520and%2520Xin%2520Gao%2520and%2520Qizhi%2520Pei%2520and%2520Zhuoshi%2520Pan%2520and%2520Mengzhang%2520Cai%2520and%2520Jiang%2520Wu%2520and%2520Conghui%2520He%2520and%2520Lijun%2520Wu%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520fundamentally%2520rely%250Aon%2520high-quality%2520training%2520data.%2520While%2520data%2520selection%2520and%2520data%2520synthesis%2520are%2520two%250Acommon%2520strategies%2520to%2520improve%2520data%2520quality%252C%2520existing%2520approaches%2520often%2520face%250Alimitations%2520in%2520static%2520dataset%2520curation%2520that%2520fail%2520to%2520adapt%2520to%2520evolving%2520model%250Acapabilities.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Middo%252C%2520a%2520self-evolving%2520Model-informed%250Adynamic%2520data%2520optimization%2520framework%2520that%2520uses%2520model-aware%2520data%2520selection%2520and%250Acontext-preserving%2520data%2520refinement.%2520Unlike%2520conventional%2520one-off%250Afiltering/synthesis%2520methods%252C%2520our%2520framework%2520establishes%2520a%2520closed-loop%250Aoptimization%2520system%253A%2520%25281%2529%2520A%2520self-referential%2520diagnostic%2520module%2520proactively%250Aidentifies%2520suboptimal%2520samples%2520through%2520tri-axial%2520model%2520signals%2520-%2520loss%2520patterns%250A%2528complexity%2529%252C%2520embedding%2520cluster%2520dynamics%2520%2528diversity%2529%252C%2520and%2520self-alignment%2520scores%250A%2528quality%2529%253B%2520%25282%2529%2520An%2520adaptive%2520optimization%2520engine%2520then%2520transforms%2520suboptimal%250Asamples%2520into%2520pedagogically%2520valuable%2520training%2520points%2520while%2520preserving%2520semantic%250Aintegrity%253B%2520%25283%2529%2520This%2520optimization%2520process%2520continuously%2520evolves%2520with%2520model%250Acapability%2520through%2520dynamic%2520learning%2520principles.%2520Experiments%2520on%2520multiple%250Abenchmarks%2520demonstrate%2520that%2520our%2520Middo%2520consistently%2520enhances%2520the%2520quality%2520of%2520seed%250Adata%2520and%2520boosts%2520LLM%2527s%2520performance%2520with%2520improving%2520accuracy%2520by%25207.15%2525%2520on%2520average%250Awhile%2520maintaining%2520the%2520original%2520dataset%2520scale.%2520This%2520work%2520establishes%2520a%2520new%250Aparadigm%2520for%2520sustainable%2520LLM%2520training%2520through%2520dynamic%2520human-AI%2520co-evolution%2520of%250Adata%2520and%2520models.%2520Our%2520datasets%252C%2520models%252C%2520and%2520code%2520are%2520coming%2520soon.%2520Our%2520datasets%252C%250Amodels%252C%2520and%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Word2VecT/Middo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21589v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning&entry.906535625=Zinan%20Tang%20and%20Xin%20Gao%20and%20Qizhi%20Pei%20and%20Zhuoshi%20Pan%20and%20Mengzhang%20Cai%20and%20Jiang%20Wu%20and%20Conghui%20He%20and%20Lijun%20Wu&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20Large%20Language%20Models%20%28LLM%29%20fundamentally%20rely%0Aon%20high-quality%20training%20data.%20While%20data%20selection%20and%20data%20synthesis%20are%20two%0Acommon%20strategies%20to%20improve%20data%20quality%2C%20existing%20approaches%20often%20face%0Alimitations%20in%20static%20dataset%20curation%20that%20fail%20to%20adapt%20to%20evolving%20model%0Acapabilities.%20In%20this%20paper%2C%20we%20introduce%20Middo%2C%20a%20self-evolving%20Model-informed%0Adynamic%20data%20optimization%20framework%20that%20uses%20model-aware%20data%20selection%20and%0Acontext-preserving%20data%20refinement.%20Unlike%20conventional%20one-off%0Afiltering/synthesis%20methods%2C%20our%20framework%20establishes%20a%20closed-loop%0Aoptimization%20system%3A%20%281%29%20A%20self-referential%20diagnostic%20module%20proactively%0Aidentifies%20suboptimal%20samples%20through%20tri-axial%20model%20signals%20-%20loss%20patterns%0A%28complexity%29%2C%20embedding%20cluster%20dynamics%20%28diversity%29%2C%20and%20self-alignment%20scores%0A%28quality%29%3B%20%282%29%20An%20adaptive%20optimization%20engine%20then%20transforms%20suboptimal%0Asamples%20into%20pedagogically%20valuable%20training%20points%20while%20preserving%20semantic%0Aintegrity%3B%20%283%29%20This%20optimization%20process%20continuously%20evolves%20with%20model%0Acapability%20through%20dynamic%20learning%20principles.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20Middo%20consistently%20enhances%20the%20quality%20of%20seed%0Adata%20and%20boosts%20LLM%27s%20performance%20with%20improving%20accuracy%20by%207.15%25%20on%20average%0Awhile%20maintaining%20the%20original%20dataset%20scale.%20This%20work%20establishes%20a%20new%0Aparadigm%20for%20sustainable%20LLM%20training%20through%20dynamic%20human-AI%20co-evolution%20of%0Adata%20and%20models.%20Our%20datasets%2C%20models%2C%20and%20code%20are%20coming%20soon.%20Our%20datasets%2C%0Amodels%2C%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Word2VecT/Middo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21589v3&entry.124074799=Read"},
{"title": "Modeling Elastic-Body Dynamics of Fish Swimming Using a Variational\n  Framework", "author": "Zhiheng Chen and Wei Wang", "abstract": "  Fish-inspired aquatic robots are gaining increasing attention in research\ncommunities due to their high swimming speeds and efficient propulsion enabled\nby flexible bodies that generate undulatory motions. To support the design\noptimizations and control of such systems, accurate, interpretable, and\ncomputationally tractable modeling of the underlying swimming dynamics is\nindispensable. In this letter, we present a full-body dynamics model for fish\nswimming, rigorously derived from Hamilton's principle. The model captures the\ncontinuously distributed elasticity of a deformable fish body undergoing large\ndeformations and incorporates fluid-structure coupling effects, enabling\nself-propelled motion without prescribing kinematics. A preliminary parameter\nstudy explores the influence of actuation frequency and body stiffness on\nswimming speed and cost of transport (COT). Simulation results indicate that\nswimming speed and energy efficiency exhibit opposing trends with tail-beat\nfrequency and that both body stiffness and body length have distinct optimal\nvalues. These findings provide insights into biological swimming mechanisms and\ninform the design of high-performance soft robotic swimmers.\n", "link": "http://arxiv.org/abs/2509.16145v1", "date": "2025-09-19", "relevancy": 1.8821, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Elastic-Body%20Dynamics%20of%20Fish%20Swimming%20Using%20a%20Variational%0A%20%20Framework&body=Title%3A%20Modeling%20Elastic-Body%20Dynamics%20of%20Fish%20Swimming%20Using%20a%20Variational%0A%20%20Framework%0AAuthor%3A%20Zhiheng%20Chen%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Fish-inspired%20aquatic%20robots%20are%20gaining%20increasing%20attention%20in%20research%0Acommunities%20due%20to%20their%20high%20swimming%20speeds%20and%20efficient%20propulsion%20enabled%0Aby%20flexible%20bodies%20that%20generate%20undulatory%20motions.%20To%20support%20the%20design%0Aoptimizations%20and%20control%20of%20such%20systems%2C%20accurate%2C%20interpretable%2C%20and%0Acomputationally%20tractable%20modeling%20of%20the%20underlying%20swimming%20dynamics%20is%0Aindispensable.%20In%20this%20letter%2C%20we%20present%20a%20full-body%20dynamics%20model%20for%20fish%0Aswimming%2C%20rigorously%20derived%20from%20Hamilton%27s%20principle.%20The%20model%20captures%20the%0Acontinuously%20distributed%20elasticity%20of%20a%20deformable%20fish%20body%20undergoing%20large%0Adeformations%20and%20incorporates%20fluid-structure%20coupling%20effects%2C%20enabling%0Aself-propelled%20motion%20without%20prescribing%20kinematics.%20A%20preliminary%20parameter%0Astudy%20explores%20the%20influence%20of%20actuation%20frequency%20and%20body%20stiffness%20on%0Aswimming%20speed%20and%20cost%20of%20transport%20%28COT%29.%20Simulation%20results%20indicate%20that%0Aswimming%20speed%20and%20energy%20efficiency%20exhibit%20opposing%20trends%20with%20tail-beat%0Afrequency%20and%20that%20both%20body%20stiffness%20and%20body%20length%20have%20distinct%20optimal%0Avalues.%20These%20findings%20provide%20insights%20into%20biological%20swimming%20mechanisms%20and%0Ainform%20the%20design%20of%20high-performance%20soft%20robotic%20swimmers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Elastic-Body%2520Dynamics%2520of%2520Fish%2520Swimming%2520Using%2520a%2520Variational%250A%2520%2520Framework%26entry.906535625%3DZhiheng%2520Chen%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520Fish-inspired%2520aquatic%2520robots%2520are%2520gaining%2520increasing%2520attention%2520in%2520research%250Acommunities%2520due%2520to%2520their%2520high%2520swimming%2520speeds%2520and%2520efficient%2520propulsion%2520enabled%250Aby%2520flexible%2520bodies%2520that%2520generate%2520undulatory%2520motions.%2520To%2520support%2520the%2520design%250Aoptimizations%2520and%2520control%2520of%2520such%2520systems%252C%2520accurate%252C%2520interpretable%252C%2520and%250Acomputationally%2520tractable%2520modeling%2520of%2520the%2520underlying%2520swimming%2520dynamics%2520is%250Aindispensable.%2520In%2520this%2520letter%252C%2520we%2520present%2520a%2520full-body%2520dynamics%2520model%2520for%2520fish%250Aswimming%252C%2520rigorously%2520derived%2520from%2520Hamilton%2527s%2520principle.%2520The%2520model%2520captures%2520the%250Acontinuously%2520distributed%2520elasticity%2520of%2520a%2520deformable%2520fish%2520body%2520undergoing%2520large%250Adeformations%2520and%2520incorporates%2520fluid-structure%2520coupling%2520effects%252C%2520enabling%250Aself-propelled%2520motion%2520without%2520prescribing%2520kinematics.%2520A%2520preliminary%2520parameter%250Astudy%2520explores%2520the%2520influence%2520of%2520actuation%2520frequency%2520and%2520body%2520stiffness%2520on%250Aswimming%2520speed%2520and%2520cost%2520of%2520transport%2520%2528COT%2529.%2520Simulation%2520results%2520indicate%2520that%250Aswimming%2520speed%2520and%2520energy%2520efficiency%2520exhibit%2520opposing%2520trends%2520with%2520tail-beat%250Afrequency%2520and%2520that%2520both%2520body%2520stiffness%2520and%2520body%2520length%2520have%2520distinct%2520optimal%250Avalues.%2520These%2520findings%2520provide%2520insights%2520into%2520biological%2520swimming%2520mechanisms%2520and%250Ainform%2520the%2520design%2520of%2520high-performance%2520soft%2520robotic%2520swimmers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Elastic-Body%20Dynamics%20of%20Fish%20Swimming%20Using%20a%20Variational%0A%20%20Framework&entry.906535625=Zhiheng%20Chen%20and%20Wei%20Wang&entry.1292438233=%20%20Fish-inspired%20aquatic%20robots%20are%20gaining%20increasing%20attention%20in%20research%0Acommunities%20due%20to%20their%20high%20swimming%20speeds%20and%20efficient%20propulsion%20enabled%0Aby%20flexible%20bodies%20that%20generate%20undulatory%20motions.%20To%20support%20the%20design%0Aoptimizations%20and%20control%20of%20such%20systems%2C%20accurate%2C%20interpretable%2C%20and%0Acomputationally%20tractable%20modeling%20of%20the%20underlying%20swimming%20dynamics%20is%0Aindispensable.%20In%20this%20letter%2C%20we%20present%20a%20full-body%20dynamics%20model%20for%20fish%0Aswimming%2C%20rigorously%20derived%20from%20Hamilton%27s%20principle.%20The%20model%20captures%20the%0Acontinuously%20distributed%20elasticity%20of%20a%20deformable%20fish%20body%20undergoing%20large%0Adeformations%20and%20incorporates%20fluid-structure%20coupling%20effects%2C%20enabling%0Aself-propelled%20motion%20without%20prescribing%20kinematics.%20A%20preliminary%20parameter%0Astudy%20explores%20the%20influence%20of%20actuation%20frequency%20and%20body%20stiffness%20on%0Aswimming%20speed%20and%20cost%20of%20transport%20%28COT%29.%20Simulation%20results%20indicate%20that%0Aswimming%20speed%20and%20energy%20efficiency%20exhibit%20opposing%20trends%20with%20tail-beat%0Afrequency%20and%20that%20both%20body%20stiffness%20and%20body%20length%20have%20distinct%20optimal%0Avalues.%20These%20findings%20provide%20insights%20into%20biological%20swimming%20mechanisms%20and%0Ainform%20the%20design%20of%20high-performance%20soft%20robotic%20swimmers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16145v1&entry.124074799=Read"},
{"title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation", "author": "Aur\u00e9lien Cecille and Stefan Duffner and Franck Davoine and R\u00e9mi Agier and Thibault Neveu", "abstract": "  Accurate monocular depth estimation is crucial for 3D scene understanding,\nbut existing methods often blur depth at object boundaries, introducing\nspurious intermediate 3D points. While achieving sharp edges usually requires\nvery fine-grained supervision, our method produces crisp depth discontinuities\nusing only self-supervision. Specifically, we model per-pixel depth as a\nmixture distribution, capturing multiple plausible depths and shifting\nuncertainty from direct regression to the mixture weights. This formulation\nintegrates seamlessly into existing pipelines via variance-aware loss functions\nand uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show\nthat our method achieves up to 35% higher boundary sharpness and improves point\ncloud quality compared to state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2509.15987v1", "date": "2025-09-19", "relevancy": 1.666, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5501}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Sharper%20Object%20Boundaries%20in%20Self-Supervised%20Depth%20Estimation&body=Title%3A%20Towards%20Sharper%20Object%20Boundaries%20in%20Self-Supervised%20Depth%20Estimation%0AAuthor%3A%20Aur%C3%A9lien%20Cecille%20and%20Stefan%20Duffner%20and%20Franck%20Davoine%20and%20R%C3%A9mi%20Agier%20and%20Thibault%20Neveu%0AAbstract%3A%20%20%20Accurate%20monocular%20depth%20estimation%20is%20crucial%20for%203D%20scene%20understanding%2C%0Abut%20existing%20methods%20often%20blur%20depth%20at%20object%20boundaries%2C%20introducing%0Aspurious%20intermediate%203D%20points.%20While%20achieving%20sharp%20edges%20usually%20requires%0Avery%20fine-grained%20supervision%2C%20our%20method%20produces%20crisp%20depth%20discontinuities%0Ausing%20only%20self-supervision.%20Specifically%2C%20we%20model%20per-pixel%20depth%20as%20a%0Amixture%20distribution%2C%20capturing%20multiple%20plausible%20depths%20and%20shifting%0Auncertainty%20from%20direct%20regression%20to%20the%20mixture%20weights.%20This%20formulation%0Aintegrates%20seamlessly%20into%20existing%20pipelines%20via%20variance-aware%20loss%20functions%0Aand%20uncertainty%20propagation.%20Extensive%20evaluations%20on%20KITTI%20and%20VKITTIv2%20show%0Athat%20our%20method%20achieves%20up%20to%2035%25%20higher%20boundary%20sharpness%20and%20improves%20point%0Acloud%20quality%20compared%20to%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Sharper%2520Object%2520Boundaries%2520in%2520Self-Supervised%2520Depth%2520Estimation%26entry.906535625%3DAur%25C3%25A9lien%2520Cecille%2520and%2520Stefan%2520Duffner%2520and%2520Franck%2520Davoine%2520and%2520R%25C3%25A9mi%2520Agier%2520and%2520Thibault%2520Neveu%26entry.1292438233%3D%2520%2520Accurate%2520monocular%2520depth%2520estimation%2520is%2520crucial%2520for%25203D%2520scene%2520understanding%252C%250Abut%2520existing%2520methods%2520often%2520blur%2520depth%2520at%2520object%2520boundaries%252C%2520introducing%250Aspurious%2520intermediate%25203D%2520points.%2520While%2520achieving%2520sharp%2520edges%2520usually%2520requires%250Avery%2520fine-grained%2520supervision%252C%2520our%2520method%2520produces%2520crisp%2520depth%2520discontinuities%250Ausing%2520only%2520self-supervision.%2520Specifically%252C%2520we%2520model%2520per-pixel%2520depth%2520as%2520a%250Amixture%2520distribution%252C%2520capturing%2520multiple%2520plausible%2520depths%2520and%2520shifting%250Auncertainty%2520from%2520direct%2520regression%2520to%2520the%2520mixture%2520weights.%2520This%2520formulation%250Aintegrates%2520seamlessly%2520into%2520existing%2520pipelines%2520via%2520variance-aware%2520loss%2520functions%250Aand%2520uncertainty%2520propagation.%2520Extensive%2520evaluations%2520on%2520KITTI%2520and%2520VKITTIv2%2520show%250Athat%2520our%2520method%2520achieves%2520up%2520to%252035%2525%2520higher%2520boundary%2520sharpness%2520and%2520improves%2520point%250Acloud%2520quality%2520compared%2520to%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Sharper%20Object%20Boundaries%20in%20Self-Supervised%20Depth%20Estimation&entry.906535625=Aur%C3%A9lien%20Cecille%20and%20Stefan%20Duffner%20and%20Franck%20Davoine%20and%20R%C3%A9mi%20Agier%20and%20Thibault%20Neveu&entry.1292438233=%20%20Accurate%20monocular%20depth%20estimation%20is%20crucial%20for%203D%20scene%20understanding%2C%0Abut%20existing%20methods%20often%20blur%20depth%20at%20object%20boundaries%2C%20introducing%0Aspurious%20intermediate%203D%20points.%20While%20achieving%20sharp%20edges%20usually%20requires%0Avery%20fine-grained%20supervision%2C%20our%20method%20produces%20crisp%20depth%20discontinuities%0Ausing%20only%20self-supervision.%20Specifically%2C%20we%20model%20per-pixel%20depth%20as%20a%0Amixture%20distribution%2C%20capturing%20multiple%20plausible%20depths%20and%20shifting%0Auncertainty%20from%20direct%20regression%20to%20the%20mixture%20weights.%20This%20formulation%0Aintegrates%20seamlessly%20into%20existing%20pipelines%20via%20variance-aware%20loss%20functions%0Aand%20uncertainty%20propagation.%20Extensive%20evaluations%20on%20KITTI%20and%20VKITTIv2%20show%0Athat%20our%20method%20achieves%20up%20to%2035%25%20higher%20boundary%20sharpness%20and%20improves%20point%0Acloud%20quality%20compared%20to%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15987v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


