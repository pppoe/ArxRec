<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251215.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence", "author": "Ruiyan Wang and Teng Hu and Kaihui Huang and Zihan Su and Ran Yi and Lizhuang Ma", "abstract": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.", "link": "http://arxiv.org/abs/2512.13465v1", "date": "2025-12-15", "relevancy": 3.3248, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.7267}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6392}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseAnything%3A%20Universal%20Pose-guided%20Video%20Generation%20with%20Part-aware%20Temporal%20Coherence&body=Title%3A%20PoseAnything%3A%20Universal%20Pose-guided%20Video%20Generation%20with%20Part-aware%20Temporal%20Coherence%0AAuthor%3A%20Ruiyan%20Wang%20and%20Teng%20Hu%20and%20Kaihui%20Huang%20and%20Zihan%20Su%20and%20Ran%20Yi%20and%20Lizhuang%20Ma%0AAbstract%3A%20Pose-guided%20video%20generation%20refers%20to%20controlling%20the%20motion%20of%20subjects%20in%20generated%20video%20through%20a%20sequence%20of%20poses.%20It%20enables%20precise%20control%20over%20subject%20motion%20and%20has%20important%20applications%20in%20animation.%20However%2C%20current%20pose-guided%20video%20generation%20methods%20are%20limited%20to%20accepting%20only%20human%20poses%20as%20input%2C%20thus%20generalizing%20poorly%20to%20pose%20of%20other%20subjects.%20To%20address%20this%20issue%2C%20we%20propose%20PoseAnything%2C%20the%20first%20universal%20pose-guided%20video%20generation%20framework%20capable%20of%20handling%20both%20human%20and%20non-human%20characters%2C%20supporting%20arbitrary%20skeletal%20inputs.%20To%20enhance%20consistency%20preservation%20during%20motion%2C%20we%20introduce%20Part-aware%20Temporal%20Coherence%20Module%2C%20which%20divides%20the%20subject%20into%20different%20parts%2C%20establishes%20part%20correspondences%2C%20and%20computes%20cross-attention%20between%20corresponding%20parts%20across%20frames%20to%20achieve%20fine-grained%20part-level%20consistency.%20Additionally%2C%20we%20propose%20Subject%20and%20Camera%20Motion%20Decoupled%20CFG%2C%20a%20novel%20guidance%20strategy%20that%2C%20for%20the%20first%20time%2C%20enables%20independent%20camera%20movement%20control%20in%20pose-guided%20video%20generation%2C%20by%20separately%20injecting%20subject%20and%20camera%20motion%20control%20information%20into%20the%20positive%20and%20negative%20anchors%20of%20CFG.%20Furthermore%2C%20we%20present%20XPose%2C%20a%20high-quality%20public%20dataset%20containing%2050%2C000%20non-human%20pose-video%20pairs%2C%20along%20with%20an%20automated%20pipeline%20for%20annotation%20and%20filtering.%20Extensive%20experiments%20demonstrate%20that%20Pose-Anything%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%20effectiveness%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseAnything%253A%2520Universal%2520Pose-guided%2520Video%2520Generation%2520with%2520Part-aware%2520Temporal%2520Coherence%26entry.906535625%3DRuiyan%2520Wang%2520and%2520Teng%2520Hu%2520and%2520Kaihui%2520Huang%2520and%2520Zihan%2520Su%2520and%2520Ran%2520Yi%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3DPose-guided%2520video%2520generation%2520refers%2520to%2520controlling%2520the%2520motion%2520of%2520subjects%2520in%2520generated%2520video%2520through%2520a%2520sequence%2520of%2520poses.%2520It%2520enables%2520precise%2520control%2520over%2520subject%2520motion%2520and%2520has%2520important%2520applications%2520in%2520animation.%2520However%252C%2520current%2520pose-guided%2520video%2520generation%2520methods%2520are%2520limited%2520to%2520accepting%2520only%2520human%2520poses%2520as%2520input%252C%2520thus%2520generalizing%2520poorly%2520to%2520pose%2520of%2520other%2520subjects.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520PoseAnything%252C%2520the%2520first%2520universal%2520pose-guided%2520video%2520generation%2520framework%2520capable%2520of%2520handling%2520both%2520human%2520and%2520non-human%2520characters%252C%2520supporting%2520arbitrary%2520skeletal%2520inputs.%2520To%2520enhance%2520consistency%2520preservation%2520during%2520motion%252C%2520we%2520introduce%2520Part-aware%2520Temporal%2520Coherence%2520Module%252C%2520which%2520divides%2520the%2520subject%2520into%2520different%2520parts%252C%2520establishes%2520part%2520correspondences%252C%2520and%2520computes%2520cross-attention%2520between%2520corresponding%2520parts%2520across%2520frames%2520to%2520achieve%2520fine-grained%2520part-level%2520consistency.%2520Additionally%252C%2520we%2520propose%2520Subject%2520and%2520Camera%2520Motion%2520Decoupled%2520CFG%252C%2520a%2520novel%2520guidance%2520strategy%2520that%252C%2520for%2520the%2520first%2520time%252C%2520enables%2520independent%2520camera%2520movement%2520control%2520in%2520pose-guided%2520video%2520generation%252C%2520by%2520separately%2520injecting%2520subject%2520and%2520camera%2520motion%2520control%2520information%2520into%2520the%2520positive%2520and%2520negative%2520anchors%2520of%2520CFG.%2520Furthermore%252C%2520we%2520present%2520XPose%252C%2520a%2520high-quality%2520public%2520dataset%2520containing%252050%252C000%2520non-human%2520pose-video%2520pairs%252C%2520along%2520with%2520an%2520automated%2520pipeline%2520for%2520annotation%2520and%2520filtering.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Pose-Anything%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520effectiveness%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseAnything%3A%20Universal%20Pose-guided%20Video%20Generation%20with%20Part-aware%20Temporal%20Coherence&entry.906535625=Ruiyan%20Wang%20and%20Teng%20Hu%20and%20Kaihui%20Huang%20and%20Zihan%20Su%20and%20Ran%20Yi%20and%20Lizhuang%20Ma&entry.1292438233=Pose-guided%20video%20generation%20refers%20to%20controlling%20the%20motion%20of%20subjects%20in%20generated%20video%20through%20a%20sequence%20of%20poses.%20It%20enables%20precise%20control%20over%20subject%20motion%20and%20has%20important%20applications%20in%20animation.%20However%2C%20current%20pose-guided%20video%20generation%20methods%20are%20limited%20to%20accepting%20only%20human%20poses%20as%20input%2C%20thus%20generalizing%20poorly%20to%20pose%20of%20other%20subjects.%20To%20address%20this%20issue%2C%20we%20propose%20PoseAnything%2C%20the%20first%20universal%20pose-guided%20video%20generation%20framework%20capable%20of%20handling%20both%20human%20and%20non-human%20characters%2C%20supporting%20arbitrary%20skeletal%20inputs.%20To%20enhance%20consistency%20preservation%20during%20motion%2C%20we%20introduce%20Part-aware%20Temporal%20Coherence%20Module%2C%20which%20divides%20the%20subject%20into%20different%20parts%2C%20establishes%20part%20correspondences%2C%20and%20computes%20cross-attention%20between%20corresponding%20parts%20across%20frames%20to%20achieve%20fine-grained%20part-level%20consistency.%20Additionally%2C%20we%20propose%20Subject%20and%20Camera%20Motion%20Decoupled%20CFG%2C%20a%20novel%20guidance%20strategy%20that%2C%20for%20the%20first%20time%2C%20enables%20independent%20camera%20movement%20control%20in%20pose-guided%20video%20generation%2C%20by%20separately%20injecting%20subject%20and%20camera%20motion%20control%20information%20into%20the%20positive%20and%20negative%20anchors%20of%20CFG.%20Furthermore%2C%20we%20present%20XPose%2C%20a%20high-quality%20public%20dataset%20containing%2050%2C000%20non-human%20pose-video%20pairs%2C%20along%20with%20an%20automated%20pipeline%20for%20annotation%20and%20filtering.%20Extensive%20experiments%20demonstrate%20that%20Pose-Anything%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%20effectiveness%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2512.13465v1&entry.124074799=Read"},
{"title": "Computer vision training dataset generation for robotic environments using Gaussian splatting", "author": "Patryk Ni\u017ceniec and Marcin Iwanowski", "abstract": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.", "link": "http://arxiv.org/abs/2512.13411v1", "date": "2025-12-15", "relevancy": 3.2713, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6954}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6581}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer%20vision%20training%20dataset%20generation%20for%20robotic%20environments%20using%20Gaussian%20splatting&body=Title%3A%20Computer%20vision%20training%20dataset%20generation%20for%20robotic%20environments%20using%20Gaussian%20splatting%0AAuthor%3A%20Patryk%20Ni%C5%BCeniec%20and%20Marcin%20Iwanowski%0AAbstract%3A%20This%20paper%20introduces%20a%20novel%20pipeline%20for%20generating%20large-scale%2C%20highly%20realistic%2C%20and%20automatically%20labeled%20datasets%20for%20computer%20vision%20tasks%20in%20robotic%20environments.%20Our%20approach%20addresses%20the%20critical%20challenges%20of%20the%20domain%20gap%20between%20synthetic%20and%20real-world%20imagery%20and%20the%20time-consuming%20bottleneck%20of%20manual%20annotation.%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20create%20photorealistic%20representations%20of%20the%20operational%20environment%20and%20objects.%20These%20assets%20are%20then%20used%20in%20a%20game%20engine%20where%20physics%20simulations%20create%20natural%20arrangements.%20A%20novel%2C%20two-pass%20rendering%20technique%20combines%20the%20realism%20of%20splats%20with%20a%20shadow%20map%20generated%20from%20proxy%20meshes.%20This%20map%20is%20then%20algorithmically%20composited%20with%20the%20image%20to%20add%20both%20physically%20plausible%20shadows%20and%20subtle%20highlights%2C%20significantly%20enhancing%20realism.%20Pixel-perfect%20segmentation%20masks%20are%20generated%20automatically%20and%20formatted%20for%20direct%20use%20with%20object%20detection%20models%20like%20YOLO.%20Our%20experiments%20show%20that%20a%20hybrid%20training%20strategy%2C%20combining%20a%20small%20set%20of%20real%20images%20with%20a%20large%20volume%20of%20our%20synthetic%20data%2C%20yields%20the%20best%20detection%20and%20segmentation%20performance%2C%20confirming%20this%20as%20an%20optimal%20strategy%20for%20efficiently%20achieving%20robust%20and%20accurate%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer%2520vision%2520training%2520dataset%2520generation%2520for%2520robotic%2520environments%2520using%2520Gaussian%2520splatting%26entry.906535625%3DPatryk%2520Ni%25C5%25BCeniec%2520and%2520Marcin%2520Iwanowski%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520novel%2520pipeline%2520for%2520generating%2520large-scale%252C%2520highly%2520realistic%252C%2520and%2520automatically%2520labeled%2520datasets%2520for%2520computer%2520vision%2520tasks%2520in%2520robotic%2520environments.%2520Our%2520approach%2520addresses%2520the%2520critical%2520challenges%2520of%2520the%2520domain%2520gap%2520between%2520synthetic%2520and%2520real-world%2520imagery%2520and%2520the%2520time-consuming%2520bottleneck%2520of%2520manual%2520annotation.%2520We%2520leverage%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520create%2520photorealistic%2520representations%2520of%2520the%2520operational%2520environment%2520and%2520objects.%2520These%2520assets%2520are%2520then%2520used%2520in%2520a%2520game%2520engine%2520where%2520physics%2520simulations%2520create%2520natural%2520arrangements.%2520A%2520novel%252C%2520two-pass%2520rendering%2520technique%2520combines%2520the%2520realism%2520of%2520splats%2520with%2520a%2520shadow%2520map%2520generated%2520from%2520proxy%2520meshes.%2520This%2520map%2520is%2520then%2520algorithmically%2520composited%2520with%2520the%2520image%2520to%2520add%2520both%2520physically%2520plausible%2520shadows%2520and%2520subtle%2520highlights%252C%2520significantly%2520enhancing%2520realism.%2520Pixel-perfect%2520segmentation%2520masks%2520are%2520generated%2520automatically%2520and%2520formatted%2520for%2520direct%2520use%2520with%2520object%2520detection%2520models%2520like%2520YOLO.%2520Our%2520experiments%2520show%2520that%2520a%2520hybrid%2520training%2520strategy%252C%2520combining%2520a%2520small%2520set%2520of%2520real%2520images%2520with%2520a%2520large%2520volume%2520of%2520our%2520synthetic%2520data%252C%2520yields%2520the%2520best%2520detection%2520and%2520segmentation%2520performance%252C%2520confirming%2520this%2520as%2520an%2520optimal%2520strategy%2520for%2520efficiently%2520achieving%2520robust%2520and%2520accurate%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20vision%20training%20dataset%20generation%20for%20robotic%20environments%20using%20Gaussian%20splatting&entry.906535625=Patryk%20Ni%C5%BCeniec%20and%20Marcin%20Iwanowski&entry.1292438233=This%20paper%20introduces%20a%20novel%20pipeline%20for%20generating%20large-scale%2C%20highly%20realistic%2C%20and%20automatically%20labeled%20datasets%20for%20computer%20vision%20tasks%20in%20robotic%20environments.%20Our%20approach%20addresses%20the%20critical%20challenges%20of%20the%20domain%20gap%20between%20synthetic%20and%20real-world%20imagery%20and%20the%20time-consuming%20bottleneck%20of%20manual%20annotation.%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20create%20photorealistic%20representations%20of%20the%20operational%20environment%20and%20objects.%20These%20assets%20are%20then%20used%20in%20a%20game%20engine%20where%20physics%20simulations%20create%20natural%20arrangements.%20A%20novel%2C%20two-pass%20rendering%20technique%20combines%20the%20realism%20of%20splats%20with%20a%20shadow%20map%20generated%20from%20proxy%20meshes.%20This%20map%20is%20then%20algorithmically%20composited%20with%20the%20image%20to%20add%20both%20physically%20plausible%20shadows%20and%20subtle%20highlights%2C%20significantly%20enhancing%20realism.%20Pixel-perfect%20segmentation%20masks%20are%20generated%20automatically%20and%20formatted%20for%20direct%20use%20with%20object%20detection%20models%20like%20YOLO.%20Our%20experiments%20show%20that%20a%20hybrid%20training%20strategy%2C%20combining%20a%20small%20set%20of%20real%20images%20with%20a%20large%20volume%20of%20our%20synthetic%20data%2C%20yields%20the%20best%20detection%20and%20segmentation%20performance%2C%20confirming%20this%20as%20an%20optimal%20strategy%20for%20efficiently%20achieving%20robust%20and%20accurate%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.13411v1&entry.124074799=Read"},
{"title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS", "author": "Haiyi Li and Qi Chen and Denis Kalkofen and Hsiang-Ting Chen", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.", "link": "http://arxiv.org/abs/2511.09397v2", "date": "2025-12-15", "relevancy": 3.2376, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.658}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6423}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OUGS%3A%20Active%20View%20Selection%20via%20Object-aware%20Uncertainty%20Estimation%20in%203DGS&body=Title%3A%20OUGS%3A%20Active%20View%20Selection%20via%20Object-aware%20Uncertainty%20Estimation%20in%203DGS%0AAuthor%3A%20Haiyi%20Li%20and%20Qi%20Chen%20and%20Denis%20Kalkofen%20and%20Hsiang-Ting%20Chen%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20achieved%20state-of-the-art%20results%20for%20novel%20view%20synthesis.%20However%2C%20efficiently%20capturing%20high-fidelity%20reconstructions%20of%20specific%20objects%20within%20complex%20scenes%20remains%20a%20significant%20challenge.%20A%20key%20limitation%20of%20existing%20active%20reconstruction%20methods%20is%20their%20reliance%20on%20scene-level%20uncertainty%20metrics%2C%20which%20are%20often%20biased%20by%20irrelevant%20background%20clutter%20and%20lead%20to%20inefficient%20view%20selection%20for%20object-centric%20tasks.%20We%20present%20OUGS%2C%20a%20novel%20framework%20that%20addresses%20this%20challenge%20with%20a%20more%20principled%2C%20physically-grounded%20uncertainty%20formulation%20for%203DGS.%20Our%20core%20innovation%20is%20to%20derive%20uncertainty%20directly%20from%20the%20explicit%20physical%20parameters%20of%20the%203D%20Gaussian%20primitives%20%28e.g.%2C%20position%2C%20scale%2C%20rotation%29.%20By%20propagating%20the%20covariance%20of%20these%20parameters%20through%20the%20rendering%20Jacobian%2C%20we%20establish%20a%20highly%20interpretable%20uncertainty%20model.%20This%20foundation%20allows%20us%20to%20then%20seamlessly%20integrate%20semantic%20segmentation%20masks%20to%20produce%20a%20targeted%2C%20object-aware%20uncertainty%20score%20that%20effectively%20disentangles%20the%20object%20from%20its%20environment.%20This%20allows%20for%20a%20more%20effective%20active%20view%20selection%20strategy%20that%20prioritizes%20views%20critical%20to%20improving%20object%20fidelity.%20Experimental%20evaluations%20on%20public%20datasets%20demonstrate%20that%20our%20approach%20significantly%20improves%20the%20efficiency%20of%20the%203DGS%20reconstruction%20process%20and%20achieves%20higher%20quality%20for%20targeted%20objects%20compared%20to%20existing%20state-of-the-art%20methods%2C%20while%20also%20serving%20as%20a%20robust%20uncertainty%20estimator%20for%20the%20global%20scene.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOUGS%253A%2520Active%2520View%2520Selection%2520via%2520Object-aware%2520Uncertainty%2520Estimation%2520in%25203DGS%26entry.906535625%3DHaiyi%2520Li%2520and%2520Qi%2520Chen%2520and%2520Denis%2520Kalkofen%2520and%2520Hsiang-Ting%2520Chen%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520achieved%2520state-of-the-art%2520results%2520for%2520novel%2520view%2520synthesis.%2520However%252C%2520efficiently%2520capturing%2520high-fidelity%2520reconstructions%2520of%2520specific%2520objects%2520within%2520complex%2520scenes%2520remains%2520a%2520significant%2520challenge.%2520A%2520key%2520limitation%2520of%2520existing%2520active%2520reconstruction%2520methods%2520is%2520their%2520reliance%2520on%2520scene-level%2520uncertainty%2520metrics%252C%2520which%2520are%2520often%2520biased%2520by%2520irrelevant%2520background%2520clutter%2520and%2520lead%2520to%2520inefficient%2520view%2520selection%2520for%2520object-centric%2520tasks.%2520We%2520present%2520OUGS%252C%2520a%2520novel%2520framework%2520that%2520addresses%2520this%2520challenge%2520with%2520a%2520more%2520principled%252C%2520physically-grounded%2520uncertainty%2520formulation%2520for%25203DGS.%2520Our%2520core%2520innovation%2520is%2520to%2520derive%2520uncertainty%2520directly%2520from%2520the%2520explicit%2520physical%2520parameters%2520of%2520the%25203D%2520Gaussian%2520primitives%2520%2528e.g.%252C%2520position%252C%2520scale%252C%2520rotation%2529.%2520By%2520propagating%2520the%2520covariance%2520of%2520these%2520parameters%2520through%2520the%2520rendering%2520Jacobian%252C%2520we%2520establish%2520a%2520highly%2520interpretable%2520uncertainty%2520model.%2520This%2520foundation%2520allows%2520us%2520to%2520then%2520seamlessly%2520integrate%2520semantic%2520segmentation%2520masks%2520to%2520produce%2520a%2520targeted%252C%2520object-aware%2520uncertainty%2520score%2520that%2520effectively%2520disentangles%2520the%2520object%2520from%2520its%2520environment.%2520This%2520allows%2520for%2520a%2520more%2520effective%2520active%2520view%2520selection%2520strategy%2520that%2520prioritizes%2520views%2520critical%2520to%2520improving%2520object%2520fidelity.%2520Experimental%2520evaluations%2520on%2520public%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%2520the%2520efficiency%2520of%2520the%25203DGS%2520reconstruction%2520process%2520and%2520achieves%2520higher%2520quality%2520for%2520targeted%2520objects%2520compared%2520to%2520existing%2520state-of-the-art%2520methods%252C%2520while%2520also%2520serving%2520as%2520a%2520robust%2520uncertainty%2520estimator%2520for%2520the%2520global%2520scene.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OUGS%3A%20Active%20View%20Selection%20via%20Object-aware%20Uncertainty%20Estimation%20in%203DGS&entry.906535625=Haiyi%20Li%20and%20Qi%20Chen%20and%20Denis%20Kalkofen%20and%20Hsiang-Ting%20Chen&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20achieved%20state-of-the-art%20results%20for%20novel%20view%20synthesis.%20However%2C%20efficiently%20capturing%20high-fidelity%20reconstructions%20of%20specific%20objects%20within%20complex%20scenes%20remains%20a%20significant%20challenge.%20A%20key%20limitation%20of%20existing%20active%20reconstruction%20methods%20is%20their%20reliance%20on%20scene-level%20uncertainty%20metrics%2C%20which%20are%20often%20biased%20by%20irrelevant%20background%20clutter%20and%20lead%20to%20inefficient%20view%20selection%20for%20object-centric%20tasks.%20We%20present%20OUGS%2C%20a%20novel%20framework%20that%20addresses%20this%20challenge%20with%20a%20more%20principled%2C%20physically-grounded%20uncertainty%20formulation%20for%203DGS.%20Our%20core%20innovation%20is%20to%20derive%20uncertainty%20directly%20from%20the%20explicit%20physical%20parameters%20of%20the%203D%20Gaussian%20primitives%20%28e.g.%2C%20position%2C%20scale%2C%20rotation%29.%20By%20propagating%20the%20covariance%20of%20these%20parameters%20through%20the%20rendering%20Jacobian%2C%20we%20establish%20a%20highly%20interpretable%20uncertainty%20model.%20This%20foundation%20allows%20us%20to%20then%20seamlessly%20integrate%20semantic%20segmentation%20masks%20to%20produce%20a%20targeted%2C%20object-aware%20uncertainty%20score%20that%20effectively%20disentangles%20the%20object%20from%20its%20environment.%20This%20allows%20for%20a%20more%20effective%20active%20view%20selection%20strategy%20that%20prioritizes%20views%20critical%20to%20improving%20object%20fidelity.%20Experimental%20evaluations%20on%20public%20datasets%20demonstrate%20that%20our%20approach%20significantly%20improves%20the%20efficiency%20of%20the%203DGS%20reconstruction%20process%20and%20achieves%20higher%20quality%20for%20targeted%20objects%20compared%20to%20existing%20state-of-the-art%20methods%2C%20while%20also%20serving%20as%20a%20robust%20uncertainty%20estimator%20for%20the%20global%20scene.&entry.1838667208=http%3A//arxiv.org/abs/2511.09397v2&entry.124074799=Read"},
{"title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting", "author": "Chang Liu and Hongliang Yuan and Lianghao Zhang and Sichao Wang and Jianwei Guo and Shi-Sheng Huang", "abstract": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.", "link": "http://arxiv.org/abs/2512.08334v2", "date": "2025-12-15", "relevancy": 3.2006, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7023}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6387}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridSplat%3A%20Fast%20Reflection-baked%20Gaussian%20Tracing%20using%20Hybrid%20Splatting&body=Title%3A%20HybridSplat%3A%20Fast%20Reflection-baked%20Gaussian%20Tracing%20using%20Hybrid%20Splatting%0AAuthor%3A%20Chang%20Liu%20and%20Hongliang%20Yuan%20and%20Lianghao%20Zhang%20and%20Sichao%20Wang%20and%20Jianwei%20Guo%20and%20Shi-Sheng%20Huang%0AAbstract%3A%20Rendering%20complex%20reflection%20of%20real-world%20scenes%20using%203D%20Gaussian%20splatting%20has%20been%20a%20quite%20promising%20solution%20for%20photorealistic%20novel%20view%20synthesis%2C%20but%20still%20faces%20bottlenecks%20especially%20in%20rendering%20speed%20and%20memory%20storage.%20This%20paper%20proposes%20a%20new%20Hybrid%20Splatting%28HybridSplat%29%20mechanism%20for%20Gaussian%20primitives.%20Our%20key%20idea%20is%20a%20new%20reflection-baked%20Gaussian%20tracing%2C%20which%20bakes%20the%20view-dependent%20reflection%20within%20each%20Gaussian%20primitive%20while%20rendering%20the%20reflection%20using%20tile-based%20Gaussian%20splatting.%20Then%20we%20integrate%20the%20reflective%20Gaussian%20primitives%20with%20base%20Gaussian%20primitives%20using%20a%20unified%20hybrid%20splatting%20framework%20for%20high-fidelity%20scene%20reconstruction.%20Moreover%2C%20we%20further%20introduce%20a%20pipeline-level%20acceleration%20for%20the%20hybrid%20splatting%2C%20and%20reflection-sensitive%20Gaussian%20pruning%20to%20reduce%20the%20model%20size%2C%20thus%20achieving%20much%20faster%20rendering%20speed%20and%20lower%20memory%20storage%20while%20preserving%20the%20reflection%20rendering%20quality.%20By%20extensive%20evaluation%2C%20our%20HybridSplat%20accelerates%20about%207x%20rendering%20speed%20across%20complex%20reflective%20scenes%20from%20Ref-NeRF%2C%20NeRF-Casting%20with%204x%20fewer%20Gaussian%20primitives%20than%20similar%20ray-tracing%20based%20Gaussian%20splatting%20baselines%2C%20serving%20as%20a%20new%20state-of-the-art%20method%20especially%20for%20complex%20reflective%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridSplat%253A%2520Fast%2520Reflection-baked%2520Gaussian%2520Tracing%2520using%2520Hybrid%2520Splatting%26entry.906535625%3DChang%2520Liu%2520and%2520Hongliang%2520Yuan%2520and%2520Lianghao%2520Zhang%2520and%2520Sichao%2520Wang%2520and%2520Jianwei%2520Guo%2520and%2520Shi-Sheng%2520Huang%26entry.1292438233%3DRendering%2520complex%2520reflection%2520of%2520real-world%2520scenes%2520using%25203D%2520Gaussian%2520splatting%2520has%2520been%2520a%2520quite%2520promising%2520solution%2520for%2520photorealistic%2520novel%2520view%2520synthesis%252C%2520but%2520still%2520faces%2520bottlenecks%2520especially%2520in%2520rendering%2520speed%2520and%2520memory%2520storage.%2520This%2520paper%2520proposes%2520a%2520new%2520Hybrid%2520Splatting%2528HybridSplat%2529%2520mechanism%2520for%2520Gaussian%2520primitives.%2520Our%2520key%2520idea%2520is%2520a%2520new%2520reflection-baked%2520Gaussian%2520tracing%252C%2520which%2520bakes%2520the%2520view-dependent%2520reflection%2520within%2520each%2520Gaussian%2520primitive%2520while%2520rendering%2520the%2520reflection%2520using%2520tile-based%2520Gaussian%2520splatting.%2520Then%2520we%2520integrate%2520the%2520reflective%2520Gaussian%2520primitives%2520with%2520base%2520Gaussian%2520primitives%2520using%2520a%2520unified%2520hybrid%2520splatting%2520framework%2520for%2520high-fidelity%2520scene%2520reconstruction.%2520Moreover%252C%2520we%2520further%2520introduce%2520a%2520pipeline-level%2520acceleration%2520for%2520the%2520hybrid%2520splatting%252C%2520and%2520reflection-sensitive%2520Gaussian%2520pruning%2520to%2520reduce%2520the%2520model%2520size%252C%2520thus%2520achieving%2520much%2520faster%2520rendering%2520speed%2520and%2520lower%2520memory%2520storage%2520while%2520preserving%2520the%2520reflection%2520rendering%2520quality.%2520By%2520extensive%2520evaluation%252C%2520our%2520HybridSplat%2520accelerates%2520about%25207x%2520rendering%2520speed%2520across%2520complex%2520reflective%2520scenes%2520from%2520Ref-NeRF%252C%2520NeRF-Casting%2520with%25204x%2520fewer%2520Gaussian%2520primitives%2520than%2520similar%2520ray-tracing%2520based%2520Gaussian%2520splatting%2520baselines%252C%2520serving%2520as%2520a%2520new%2520state-of-the-art%2520method%2520especially%2520for%2520complex%2520reflective%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridSplat%3A%20Fast%20Reflection-baked%20Gaussian%20Tracing%20using%20Hybrid%20Splatting&entry.906535625=Chang%20Liu%20and%20Hongliang%20Yuan%20and%20Lianghao%20Zhang%20and%20Sichao%20Wang%20and%20Jianwei%20Guo%20and%20Shi-Sheng%20Huang&entry.1292438233=Rendering%20complex%20reflection%20of%20real-world%20scenes%20using%203D%20Gaussian%20splatting%20has%20been%20a%20quite%20promising%20solution%20for%20photorealistic%20novel%20view%20synthesis%2C%20but%20still%20faces%20bottlenecks%20especially%20in%20rendering%20speed%20and%20memory%20storage.%20This%20paper%20proposes%20a%20new%20Hybrid%20Splatting%28HybridSplat%29%20mechanism%20for%20Gaussian%20primitives.%20Our%20key%20idea%20is%20a%20new%20reflection-baked%20Gaussian%20tracing%2C%20which%20bakes%20the%20view-dependent%20reflection%20within%20each%20Gaussian%20primitive%20while%20rendering%20the%20reflection%20using%20tile-based%20Gaussian%20splatting.%20Then%20we%20integrate%20the%20reflective%20Gaussian%20primitives%20with%20base%20Gaussian%20primitives%20using%20a%20unified%20hybrid%20splatting%20framework%20for%20high-fidelity%20scene%20reconstruction.%20Moreover%2C%20we%20further%20introduce%20a%20pipeline-level%20acceleration%20for%20the%20hybrid%20splatting%2C%20and%20reflection-sensitive%20Gaussian%20pruning%20to%20reduce%20the%20model%20size%2C%20thus%20achieving%20much%20faster%20rendering%20speed%20and%20lower%20memory%20storage%20while%20preserving%20the%20reflection%20rendering%20quality.%20By%20extensive%20evaluation%2C%20our%20HybridSplat%20accelerates%20about%207x%20rendering%20speed%20across%20complex%20reflective%20scenes%20from%20Ref-NeRF%2C%20NeRF-Casting%20with%204x%20fewer%20Gaussian%20primitives%20than%20similar%20ray-tracing%20based%20Gaussian%20splatting%20baselines%2C%20serving%20as%20a%20new%20state-of-the-art%20method%20especially%20for%20complex%20reflective%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.08334v2&entry.124074799=Read"},
{"title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion", "author": "Minghui Hou and Wei-Hsing Huang and Shaofeng Liang and Daizong Liu and Tai-Hao Wen and Gang Wang and Runwei Guan and Weiping Ding", "abstract": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.", "link": "http://arxiv.org/abs/2512.13177v1", "date": "2025-12-15", "relevancy": 3.1619, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6497}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMDrive%3A%20Interactive%20Scene%20Understanding%20Beyond%20Vision%20with%20Multi-representational%20Fusion&body=Title%3A%20MMDrive%3A%20Interactive%20Scene%20Understanding%20Beyond%20Vision%20with%20Multi-representational%20Fusion%0AAuthor%3A%20Minghui%20Hou%20and%20Wei-Hsing%20Huang%20and%20Shaofeng%20Liang%20and%20Daizong%20Liu%20and%20Tai-Hao%20Wen%20and%20Gang%20Wang%20and%20Runwei%20Guan%20and%20Weiping%20Ding%0AAbstract%3A%20Vision-language%20models%20enable%20the%20understanding%20and%20reasoning%20of%20complex%20traffic%20scenarios%20through%20multi-source%20information%20fusion%2C%20establishing%20it%20as%20a%20core%20technology%20for%20autonomous%20driving.%20However%2C%20existing%20vision-language%20models%20are%20constrained%20by%20the%20image%20understanding%20paradigm%20in%202D%20plane%2C%20which%20restricts%20their%20capability%20to%20perceive%203D%20spatial%20information%20and%20perform%20deep%20semantic%20fusion%2C%20resulting%20in%20suboptimal%20performance%20in%20complex%20autonomous%20driving%20environments.%20This%20study%20proposes%20MMDrive%2C%20an%20multimodal%20vision-language%20model%20framework%20that%20extends%20traditional%20image%20understanding%20to%20a%20generalized%203D%20scene%20understanding%20framework.%20MMDrive%20incorporates%20three%20complementary%20modalities%2C%20including%20occupancy%20maps%2C%20LiDAR%20point%20clouds%2C%20and%20textual%20scene%20descriptions.%20To%20this%20end%2C%20it%20introduces%20two%20novel%20components%20for%20adaptive%20cross-modal%20fusion%20and%20key%20information%20extraction.%20Specifically%2C%20the%20Text-oriented%20Multimodal%20Modulator%20dynamically%20weights%20the%20contributions%20of%20each%20modality%20based%20on%20the%20semantic%20cues%20in%20the%20question%2C%20guiding%20context-aware%20feature%20integration.%20The%20Cross-Modal%20Abstractor%20employs%20learnable%20abstract%20tokens%20to%20generate%20compact%2C%20cross-modal%20summaries%20that%20highlight%20key%20regions%20and%20essential%20semantics.%20Comprehensive%20evaluations%20on%20the%20DriveLM%20and%20NuScenes-QA%20benchmarks%20demonstrate%20that%20MMDrive%20achieves%20significant%20performance%20gains%20over%20existing%20vision-language%20models%20for%20autonomous%20driving%2C%20with%20a%20BLEU-4%20score%20of%2054.56%20and%20METEOR%20of%2041.78%20on%20DriveLM%2C%20and%20an%20accuracy%20score%20of%2062.7%25%20on%20NuScenes-QA.%20MMDrive%20effectively%20breaks%20the%20traditional%20image-only%20understanding%20barrier%2C%20enabling%20robust%20multimodal%20reasoning%20in%20complex%20driving%20environments%20and%20providing%20a%20new%20foundation%20for%20interpretable%20autonomous%20driving%20scene%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMDrive%253A%2520Interactive%2520Scene%2520Understanding%2520Beyond%2520Vision%2520with%2520Multi-representational%2520Fusion%26entry.906535625%3DMinghui%2520Hou%2520and%2520Wei-Hsing%2520Huang%2520and%2520Shaofeng%2520Liang%2520and%2520Daizong%2520Liu%2520and%2520Tai-Hao%2520Wen%2520and%2520Gang%2520Wang%2520and%2520Runwei%2520Guan%2520and%2520Weiping%2520Ding%26entry.1292438233%3DVision-language%2520models%2520enable%2520the%2520understanding%2520and%2520reasoning%2520of%2520complex%2520traffic%2520scenarios%2520through%2520multi-source%2520information%2520fusion%252C%2520establishing%2520it%2520as%2520a%2520core%2520technology%2520for%2520autonomous%2520driving.%2520However%252C%2520existing%2520vision-language%2520models%2520are%2520constrained%2520by%2520the%2520image%2520understanding%2520paradigm%2520in%25202D%2520plane%252C%2520which%2520restricts%2520their%2520capability%2520to%2520perceive%25203D%2520spatial%2520information%2520and%2520perform%2520deep%2520semantic%2520fusion%252C%2520resulting%2520in%2520suboptimal%2520performance%2520in%2520complex%2520autonomous%2520driving%2520environments.%2520This%2520study%2520proposes%2520MMDrive%252C%2520an%2520multimodal%2520vision-language%2520model%2520framework%2520that%2520extends%2520traditional%2520image%2520understanding%2520to%2520a%2520generalized%25203D%2520scene%2520understanding%2520framework.%2520MMDrive%2520incorporates%2520three%2520complementary%2520modalities%252C%2520including%2520occupancy%2520maps%252C%2520LiDAR%2520point%2520clouds%252C%2520and%2520textual%2520scene%2520descriptions.%2520To%2520this%2520end%252C%2520it%2520introduces%2520two%2520novel%2520components%2520for%2520adaptive%2520cross-modal%2520fusion%2520and%2520key%2520information%2520extraction.%2520Specifically%252C%2520the%2520Text-oriented%2520Multimodal%2520Modulator%2520dynamically%2520weights%2520the%2520contributions%2520of%2520each%2520modality%2520based%2520on%2520the%2520semantic%2520cues%2520in%2520the%2520question%252C%2520guiding%2520context-aware%2520feature%2520integration.%2520The%2520Cross-Modal%2520Abstractor%2520employs%2520learnable%2520abstract%2520tokens%2520to%2520generate%2520compact%252C%2520cross-modal%2520summaries%2520that%2520highlight%2520key%2520regions%2520and%2520essential%2520semantics.%2520Comprehensive%2520evaluations%2520on%2520the%2520DriveLM%2520and%2520NuScenes-QA%2520benchmarks%2520demonstrate%2520that%2520MMDrive%2520achieves%2520significant%2520performance%2520gains%2520over%2520existing%2520vision-language%2520models%2520for%2520autonomous%2520driving%252C%2520with%2520a%2520BLEU-4%2520score%2520of%252054.56%2520and%2520METEOR%2520of%252041.78%2520on%2520DriveLM%252C%2520and%2520an%2520accuracy%2520score%2520of%252062.7%2525%2520on%2520NuScenes-QA.%2520MMDrive%2520effectively%2520breaks%2520the%2520traditional%2520image-only%2520understanding%2520barrier%252C%2520enabling%2520robust%2520multimodal%2520reasoning%2520in%2520complex%2520driving%2520environments%2520and%2520providing%2520a%2520new%2520foundation%2520for%2520interpretable%2520autonomous%2520driving%2520scene%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMDrive%3A%20Interactive%20Scene%20Understanding%20Beyond%20Vision%20with%20Multi-representational%20Fusion&entry.906535625=Minghui%20Hou%20and%20Wei-Hsing%20Huang%20and%20Shaofeng%20Liang%20and%20Daizong%20Liu%20and%20Tai-Hao%20Wen%20and%20Gang%20Wang%20and%20Runwei%20Guan%20and%20Weiping%20Ding&entry.1292438233=Vision-language%20models%20enable%20the%20understanding%20and%20reasoning%20of%20complex%20traffic%20scenarios%20through%20multi-source%20information%20fusion%2C%20establishing%20it%20as%20a%20core%20technology%20for%20autonomous%20driving.%20However%2C%20existing%20vision-language%20models%20are%20constrained%20by%20the%20image%20understanding%20paradigm%20in%202D%20plane%2C%20which%20restricts%20their%20capability%20to%20perceive%203D%20spatial%20information%20and%20perform%20deep%20semantic%20fusion%2C%20resulting%20in%20suboptimal%20performance%20in%20complex%20autonomous%20driving%20environments.%20This%20study%20proposes%20MMDrive%2C%20an%20multimodal%20vision-language%20model%20framework%20that%20extends%20traditional%20image%20understanding%20to%20a%20generalized%203D%20scene%20understanding%20framework.%20MMDrive%20incorporates%20three%20complementary%20modalities%2C%20including%20occupancy%20maps%2C%20LiDAR%20point%20clouds%2C%20and%20textual%20scene%20descriptions.%20To%20this%20end%2C%20it%20introduces%20two%20novel%20components%20for%20adaptive%20cross-modal%20fusion%20and%20key%20information%20extraction.%20Specifically%2C%20the%20Text-oriented%20Multimodal%20Modulator%20dynamically%20weights%20the%20contributions%20of%20each%20modality%20based%20on%20the%20semantic%20cues%20in%20the%20question%2C%20guiding%20context-aware%20feature%20integration.%20The%20Cross-Modal%20Abstractor%20employs%20learnable%20abstract%20tokens%20to%20generate%20compact%2C%20cross-modal%20summaries%20that%20highlight%20key%20regions%20and%20essential%20semantics.%20Comprehensive%20evaluations%20on%20the%20DriveLM%20and%20NuScenes-QA%20benchmarks%20demonstrate%20that%20MMDrive%20achieves%20significant%20performance%20gains%20over%20existing%20vision-language%20models%20for%20autonomous%20driving%2C%20with%20a%20BLEU-4%20score%20of%2054.56%20and%20METEOR%20of%2041.78%20on%20DriveLM%2C%20and%20an%20accuracy%20score%20of%2062.7%25%20on%20NuScenes-QA.%20MMDrive%20effectively%20breaks%20the%20traditional%20image-only%20understanding%20barrier%2C%20enabling%20robust%20multimodal%20reasoning%20in%20complex%20driving%20environments%20and%20providing%20a%20new%20foundation%20for%20interpretable%20autonomous%20driving%20scene%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.13177v1&entry.124074799=Read"},
{"title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners", "author": "Lu Ling and Yunhao Ge and Yichen Sheng and Aniket Bera", "abstract": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/", "link": "http://arxiv.org/abs/2512.13683v1", "date": "2025-12-15", "relevancy": 3.1432, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6284}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-Scene%3A%203D%20Instance%20Models%20are%20Implicit%20Generalizable%20Spatial%20Learners&body=Title%3A%20I-Scene%3A%203D%20Instance%20Models%20are%20Implicit%20Generalizable%20Spatial%20Learners%0AAuthor%3A%20Lu%20Ling%20and%20Yunhao%20Ge%20and%20Yichen%20Sheng%20and%20Aniket%20Bera%0AAbstract%3A%20Generalization%20remains%20the%20central%20challenge%20for%20interactive%203D%20scene%20generation.%20Existing%20learning-based%20approaches%20ground%20spatial%20understanding%20in%20limited%20scene%20dataset%2C%20restricting%20generalization%20to%20new%20layouts.%20We%20instead%20reprogram%20a%20pre-trained%203D%20instance%20generator%20to%20act%20as%20a%20scene%20level%20learner%2C%20replacing%20dataset-bounded%20supervision%20with%20model-centric%20spatial%20supervision.%20This%20reprogramming%20unlocks%20the%20generator%20transferable%20spatial%20knowledge%2C%20enabling%20generalization%20to%20unseen%20layouts%20and%20novel%20object%20compositions.%20Remarkably%2C%20spatial%20reasoning%20still%20emerges%20even%20when%20the%20training%20scenes%20are%20randomly%20composed%20objects.%20This%20demonstrates%20that%20the%20generator%27s%20transferable%20scene%20prior%20provides%20a%20rich%20learning%20signal%20for%20inferring%20proximity%2C%20support%2C%20and%20symmetry%20from%20purely%20geometric%20cues.%20Replacing%20widely%20used%20canonical%20space%2C%20we%20instantiate%20this%20insight%20with%20a%20view-centric%20formulation%20of%20the%20scene%20space%2C%20yielding%20a%20fully%20feed-forward%2C%20generalizable%20scene%20generator%20that%20learns%20spatial%20relations%20directly%20from%20the%20instance%20model.%20Quantitative%20and%20qualitative%20results%20show%20that%20a%203D%20instance%20generator%20is%20an%20implicit%20spatial%20learner%20and%20reasoner%2C%20pointing%20toward%20foundation%20models%20for%20interactive%203D%20scene%20understanding%20and%20generation.%20Project%20page%3A%20https%3A//luling06.github.io/I-Scene-project/%0ALink%3A%20http%3A//arxiv.org/abs/2512.13683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-Scene%253A%25203D%2520Instance%2520Models%2520are%2520Implicit%2520Generalizable%2520Spatial%2520Learners%26entry.906535625%3DLu%2520Ling%2520and%2520Yunhao%2520Ge%2520and%2520Yichen%2520Sheng%2520and%2520Aniket%2520Bera%26entry.1292438233%3DGeneralization%2520remains%2520the%2520central%2520challenge%2520for%2520interactive%25203D%2520scene%2520generation.%2520Existing%2520learning-based%2520approaches%2520ground%2520spatial%2520understanding%2520in%2520limited%2520scene%2520dataset%252C%2520restricting%2520generalization%2520to%2520new%2520layouts.%2520We%2520instead%2520reprogram%2520a%2520pre-trained%25203D%2520instance%2520generator%2520to%2520act%2520as%2520a%2520scene%2520level%2520learner%252C%2520replacing%2520dataset-bounded%2520supervision%2520with%2520model-centric%2520spatial%2520supervision.%2520This%2520reprogramming%2520unlocks%2520the%2520generator%2520transferable%2520spatial%2520knowledge%252C%2520enabling%2520generalization%2520to%2520unseen%2520layouts%2520and%2520novel%2520object%2520compositions.%2520Remarkably%252C%2520spatial%2520reasoning%2520still%2520emerges%2520even%2520when%2520the%2520training%2520scenes%2520are%2520randomly%2520composed%2520objects.%2520This%2520demonstrates%2520that%2520the%2520generator%2527s%2520transferable%2520scene%2520prior%2520provides%2520a%2520rich%2520learning%2520signal%2520for%2520inferring%2520proximity%252C%2520support%252C%2520and%2520symmetry%2520from%2520purely%2520geometric%2520cues.%2520Replacing%2520widely%2520used%2520canonical%2520space%252C%2520we%2520instantiate%2520this%2520insight%2520with%2520a%2520view-centric%2520formulation%2520of%2520the%2520scene%2520space%252C%2520yielding%2520a%2520fully%2520feed-forward%252C%2520generalizable%2520scene%2520generator%2520that%2520learns%2520spatial%2520relations%2520directly%2520from%2520the%2520instance%2520model.%2520Quantitative%2520and%2520qualitative%2520results%2520show%2520that%2520a%25203D%2520instance%2520generator%2520is%2520an%2520implicit%2520spatial%2520learner%2520and%2520reasoner%252C%2520pointing%2520toward%2520foundation%2520models%2520for%2520interactive%25203D%2520scene%2520understanding%2520and%2520generation.%2520Project%2520page%253A%2520https%253A//luling06.github.io/I-Scene-project/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-Scene%3A%203D%20Instance%20Models%20are%20Implicit%20Generalizable%20Spatial%20Learners&entry.906535625=Lu%20Ling%20and%20Yunhao%20Ge%20and%20Yichen%20Sheng%20and%20Aniket%20Bera&entry.1292438233=Generalization%20remains%20the%20central%20challenge%20for%20interactive%203D%20scene%20generation.%20Existing%20learning-based%20approaches%20ground%20spatial%20understanding%20in%20limited%20scene%20dataset%2C%20restricting%20generalization%20to%20new%20layouts.%20We%20instead%20reprogram%20a%20pre-trained%203D%20instance%20generator%20to%20act%20as%20a%20scene%20level%20learner%2C%20replacing%20dataset-bounded%20supervision%20with%20model-centric%20spatial%20supervision.%20This%20reprogramming%20unlocks%20the%20generator%20transferable%20spatial%20knowledge%2C%20enabling%20generalization%20to%20unseen%20layouts%20and%20novel%20object%20compositions.%20Remarkably%2C%20spatial%20reasoning%20still%20emerges%20even%20when%20the%20training%20scenes%20are%20randomly%20composed%20objects.%20This%20demonstrates%20that%20the%20generator%27s%20transferable%20scene%20prior%20provides%20a%20rich%20learning%20signal%20for%20inferring%20proximity%2C%20support%2C%20and%20symmetry%20from%20purely%20geometric%20cues.%20Replacing%20widely%20used%20canonical%20space%2C%20we%20instantiate%20this%20insight%20with%20a%20view-centric%20formulation%20of%20the%20scene%20space%2C%20yielding%20a%20fully%20feed-forward%2C%20generalizable%20scene%20generator%20that%20learns%20spatial%20relations%20directly%20from%20the%20instance%20model.%20Quantitative%20and%20qualitative%20results%20show%20that%20a%203D%20instance%20generator%20is%20an%20implicit%20spatial%20learner%20and%20reasoner%2C%20pointing%20toward%20foundation%20models%20for%20interactive%203D%20scene%20understanding%20and%20generation.%20Project%20page%3A%20https%3A//luling06.github.io/I-Scene-project/&entry.1838667208=http%3A//arxiv.org/abs/2512.13683v1&entry.124074799=Read"},
{"title": "Towards Physically Executable 3D Gaussian for Embodied Navigation", "author": "Bingchen Miao and Rong Wei and Zhiqi Ge and Xiaoquan sun and Shiqi Gao and Jingzhe Zhu and Renhan Wang and Siliang Tang and Jun Xiao and Rui Tang and Juncheng Li", "abstract": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.", "link": "http://arxiv.org/abs/2510.21307v2", "date": "2025-12-15", "relevancy": 3.143, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6458}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6253}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Physically%20Executable%203D%20Gaussian%20for%20Embodied%20Navigation&body=Title%3A%20Towards%20Physically%20Executable%203D%20Gaussian%20for%20Embodied%20Navigation%0AAuthor%3A%20Bingchen%20Miao%20and%20Rong%20Wei%20and%20Zhiqi%20Ge%20and%20Xiaoquan%20sun%20and%20Shiqi%20Gao%20and%20Jingzhe%20Zhu%20and%20Renhan%20Wang%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Rui%20Tang%20and%20Juncheng%20Li%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20a%203D%20representation%20method%20with%20photorealistic%20real-time%20rendering%20capabilities%2C%20is%20regarded%20as%20an%20effective%20tool%20for%20narrowing%20the%20sim-to-real%20gap.%20However%2C%20it%20lacks%20fine-grained%20semantics%20and%20physical%20executability%20for%20Visual-Language%20Navigation%20%28VLN%29.%20To%20address%20this%2C%20we%20propose%20SAGE-3D%20%28Semantically%20and%20Physically%20Aligned%20Gaussian%20Environments%20for%203D%20Navigation%29%2C%20a%20new%20paradigm%20that%20upgrades%203DGS%20into%20an%20executable%2C%20semantically%20and%20physically%20aligned%20environment.%20It%20comprises%20two%20components%3A%20%281%29%20Object-Centric%20Semantic%20Grounding%2C%20which%20adds%20object-level%20fine-grained%20annotations%20to%203DGS%3B%20and%20%282%29%20Physics-Aware%20Execution%20Jointing%2C%20which%20embeds%20collision%20objects%20into%203DGS%20and%20constructs%20rich%20physical%20interfaces.%20We%20release%20InteriorGS%2C%20containing%201K%20object-annotated%203DGS%20indoor%20scene%20data%2C%20and%20introduce%20SAGE-Bench%2C%20the%20first%203DGS-based%20VLN%20benchmark%20with%202M%20VLN%20data.%20Experiments%20show%20that%203DGS%20scene%20data%20is%20more%20difficult%20to%20converge%2C%20while%20exhibiting%20strong%20generalizability%2C%20improving%20baseline%20performance%20by%2031%25%20on%20the%20VLN-CE%20Unseen%20task.%20Our%20data%20and%20code%20are%20available%20at%3A%20https%3A//sage-3d.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Physically%2520Executable%25203D%2520Gaussian%2520for%2520Embodied%2520Navigation%26entry.906535625%3DBingchen%2520Miao%2520and%2520Rong%2520Wei%2520and%2520Zhiqi%2520Ge%2520and%2520Xiaoquan%2520sun%2520and%2520Shiqi%2520Gao%2520and%2520Jingzhe%2520Zhu%2520and%2520Renhan%2520Wang%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Rui%2520Tang%2520and%2520Juncheng%2520Li%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520a%25203D%2520representation%2520method%2520with%2520photorealistic%2520real-time%2520rendering%2520capabilities%252C%2520is%2520regarded%2520as%2520an%2520effective%2520tool%2520for%2520narrowing%2520the%2520sim-to-real%2520gap.%2520However%252C%2520it%2520lacks%2520fine-grained%2520semantics%2520and%2520physical%2520executability%2520for%2520Visual-Language%2520Navigation%2520%2528VLN%2529.%2520To%2520address%2520this%252C%2520we%2520propose%2520SAGE-3D%2520%2528Semantically%2520and%2520Physically%2520Aligned%2520Gaussian%2520Environments%2520for%25203D%2520Navigation%2529%252C%2520a%2520new%2520paradigm%2520that%2520upgrades%25203DGS%2520into%2520an%2520executable%252C%2520semantically%2520and%2520physically%2520aligned%2520environment.%2520It%2520comprises%2520two%2520components%253A%2520%25281%2529%2520Object-Centric%2520Semantic%2520Grounding%252C%2520which%2520adds%2520object-level%2520fine-grained%2520annotations%2520to%25203DGS%253B%2520and%2520%25282%2529%2520Physics-Aware%2520Execution%2520Jointing%252C%2520which%2520embeds%2520collision%2520objects%2520into%25203DGS%2520and%2520constructs%2520rich%2520physical%2520interfaces.%2520We%2520release%2520InteriorGS%252C%2520containing%25201K%2520object-annotated%25203DGS%2520indoor%2520scene%2520data%252C%2520and%2520introduce%2520SAGE-Bench%252C%2520the%2520first%25203DGS-based%2520VLN%2520benchmark%2520with%25202M%2520VLN%2520data.%2520Experiments%2520show%2520that%25203DGS%2520scene%2520data%2520is%2520more%2520difficult%2520to%2520converge%252C%2520while%2520exhibiting%2520strong%2520generalizability%252C%2520improving%2520baseline%2520performance%2520by%252031%2525%2520on%2520the%2520VLN-CE%2520Unseen%2520task.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%253A%2520https%253A//sage-3d.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Physically%20Executable%203D%20Gaussian%20for%20Embodied%20Navigation&entry.906535625=Bingchen%20Miao%20and%20Rong%20Wei%20and%20Zhiqi%20Ge%20and%20Xiaoquan%20sun%20and%20Shiqi%20Gao%20and%20Jingzhe%20Zhu%20and%20Renhan%20Wang%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Rui%20Tang%20and%20Juncheng%20Li&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%2C%20a%203D%20representation%20method%20with%20photorealistic%20real-time%20rendering%20capabilities%2C%20is%20regarded%20as%20an%20effective%20tool%20for%20narrowing%20the%20sim-to-real%20gap.%20However%2C%20it%20lacks%20fine-grained%20semantics%20and%20physical%20executability%20for%20Visual-Language%20Navigation%20%28VLN%29.%20To%20address%20this%2C%20we%20propose%20SAGE-3D%20%28Semantically%20and%20Physically%20Aligned%20Gaussian%20Environments%20for%203D%20Navigation%29%2C%20a%20new%20paradigm%20that%20upgrades%203DGS%20into%20an%20executable%2C%20semantically%20and%20physically%20aligned%20environment.%20It%20comprises%20two%20components%3A%20%281%29%20Object-Centric%20Semantic%20Grounding%2C%20which%20adds%20object-level%20fine-grained%20annotations%20to%203DGS%3B%20and%20%282%29%20Physics-Aware%20Execution%20Jointing%2C%20which%20embeds%20collision%20objects%20into%203DGS%20and%20constructs%20rich%20physical%20interfaces.%20We%20release%20InteriorGS%2C%20containing%201K%20object-annotated%203DGS%20indoor%20scene%20data%2C%20and%20introduce%20SAGE-Bench%2C%20the%20first%203DGS-based%20VLN%20benchmark%20with%202M%20VLN%20data.%20Experiments%20show%20that%203DGS%20scene%20data%20is%20more%20difficult%20to%20converge%2C%20while%20exhibiting%20strong%20generalizability%2C%20improving%20baseline%20performance%20by%2031%25%20on%20the%20VLN-CE%20Unseen%20task.%20Our%20data%20and%20code%20are%20available%20at%3A%20https%3A//sage-3d.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2510.21307v2&entry.124074799=Read"},
{"title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation", "author": "Jiangning Zhang and Junwei Zhu and Zhenye Gan and Donghao Luo and Chuming Lin and Feifan Xu and Xu Peng and Jianlong Hu and Yuansen Liu and Yijia Hong and Weijian Cao and Han Feng and Xu Chen and Chencan Fu and Keke He and Xiaobin Hu and Chengjie Wang", "abstract": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/", "link": "http://arxiv.org/abs/2512.13495v1", "date": "2025-12-15", "relevancy": 3.0736, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6653}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.612}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soul%3A%20Breathe%20Life%20into%20Digital%20Human%20for%20High-fidelity%20Long-term%20Multimodal%20Animation&body=Title%3A%20Soul%3A%20Breathe%20Life%20into%20Digital%20Human%20for%20High-fidelity%20Long-term%20Multimodal%20Animation%0AAuthor%3A%20Jiangning%20Zhang%20and%20Junwei%20Zhu%20and%20Zhenye%20Gan%20and%20Donghao%20Luo%20and%20Chuming%20Lin%20and%20Feifan%20Xu%20and%20Xu%20Peng%20and%20Jianlong%20Hu%20and%20Yuansen%20Liu%20and%20Yijia%20Hong%20and%20Weijian%20Cao%20and%20Han%20Feng%20and%20Xu%20Chen%20and%20Chencan%20Fu%20and%20Keke%20He%20and%20Xiaobin%20Hu%20and%20Chengjie%20Wang%0AAbstract%3A%20We%20propose%20a%20multimodal-driven%20framework%20for%20high-fidelity%20long-term%20digital%20human%20animation%20termed%20%24%5Ctextbf%7BSoul%7D%24%2C%20which%20generates%20semantically%20coherent%20videos%20from%20a%20single-frame%20portrait%20image%2C%20text%20prompts%2C%20and%20audio%2C%20achieving%20precise%20lip%20synchronization%2C%20vivid%20facial%20expressions%2C%20and%20robust%20identity%20preservation.%20We%20construct%20Soul-1M%2C%20containing%201%20million%20finely%20annotated%20samples%20with%20a%20precise%20automated%20annotation%20pipeline%20%28covering%20portrait%2C%20upper-body%2C%20full-body%2C%20and%20multi-person%20scenes%29%20to%20mitigate%20data%20scarcity%2C%20and%20we%20carefully%20curate%20Soul-Bench%20for%20comprehensive%20and%20fair%20evaluation%20of%20audio-/text-guided%20animation%20methods.%20The%20model%20is%20built%20on%20the%20Wan2.2-5B%20backbone%2C%20integrating%20audio-injection%20layers%20and%20multiple%20training%20strategies%20together%20with%20threshold-aware%20codebook%20replacement%20to%20ensure%20long-term%20generation%20consistency.%20Meanwhile%2C%20step/CFG%20distillation%20and%20a%20lightweight%20VAE%20are%20used%20to%20optimize%20inference%20efficiency%2C%20achieving%20an%2011.4%24%5Ctimes%24%20speedup%20with%20negligible%20quality%20loss.%20Extensive%20experiments%20show%20that%20Soul%20significantly%20outperforms%20current%20leading%20open-source%20and%20commercial%20models%20on%20video%20quality%2C%20video-text%20alignment%2C%20identity%20preservation%2C%20and%20lip-synchronization%20accuracy%2C%20demonstrating%20broad%20applicability%20in%20real-world%20scenarios%20such%20as%20virtual%20anchors%20and%20film%20production.%20Project%20page%20at%20https%3A//zhangzjn.github.io/projects/Soul/%0ALink%3A%20http%3A//arxiv.org/abs/2512.13495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoul%253A%2520Breathe%2520Life%2520into%2520Digital%2520Human%2520for%2520High-fidelity%2520Long-term%2520Multimodal%2520Animation%26entry.906535625%3DJiangning%2520Zhang%2520and%2520Junwei%2520Zhu%2520and%2520Zhenye%2520Gan%2520and%2520Donghao%2520Luo%2520and%2520Chuming%2520Lin%2520and%2520Feifan%2520Xu%2520and%2520Xu%2520Peng%2520and%2520Jianlong%2520Hu%2520and%2520Yuansen%2520Liu%2520and%2520Yijia%2520Hong%2520and%2520Weijian%2520Cao%2520and%2520Han%2520Feng%2520and%2520Xu%2520Chen%2520and%2520Chencan%2520Fu%2520and%2520Keke%2520He%2520and%2520Xiaobin%2520Hu%2520and%2520Chengjie%2520Wang%26entry.1292438233%3DWe%2520propose%2520a%2520multimodal-driven%2520framework%2520for%2520high-fidelity%2520long-term%2520digital%2520human%2520animation%2520termed%2520%2524%255Ctextbf%257BSoul%257D%2524%252C%2520which%2520generates%2520semantically%2520coherent%2520videos%2520from%2520a%2520single-frame%2520portrait%2520image%252C%2520text%2520prompts%252C%2520and%2520audio%252C%2520achieving%2520precise%2520lip%2520synchronization%252C%2520vivid%2520facial%2520expressions%252C%2520and%2520robust%2520identity%2520preservation.%2520We%2520construct%2520Soul-1M%252C%2520containing%25201%2520million%2520finely%2520annotated%2520samples%2520with%2520a%2520precise%2520automated%2520annotation%2520pipeline%2520%2528covering%2520portrait%252C%2520upper-body%252C%2520full-body%252C%2520and%2520multi-person%2520scenes%2529%2520to%2520mitigate%2520data%2520scarcity%252C%2520and%2520we%2520carefully%2520curate%2520Soul-Bench%2520for%2520comprehensive%2520and%2520fair%2520evaluation%2520of%2520audio-/text-guided%2520animation%2520methods.%2520The%2520model%2520is%2520built%2520on%2520the%2520Wan2.2-5B%2520backbone%252C%2520integrating%2520audio-injection%2520layers%2520and%2520multiple%2520training%2520strategies%2520together%2520with%2520threshold-aware%2520codebook%2520replacement%2520to%2520ensure%2520long-term%2520generation%2520consistency.%2520Meanwhile%252C%2520step/CFG%2520distillation%2520and%2520a%2520lightweight%2520VAE%2520are%2520used%2520to%2520optimize%2520inference%2520efficiency%252C%2520achieving%2520an%252011.4%2524%255Ctimes%2524%2520speedup%2520with%2520negligible%2520quality%2520loss.%2520Extensive%2520experiments%2520show%2520that%2520Soul%2520significantly%2520outperforms%2520current%2520leading%2520open-source%2520and%2520commercial%2520models%2520on%2520video%2520quality%252C%2520video-text%2520alignment%252C%2520identity%2520preservation%252C%2520and%2520lip-synchronization%2520accuracy%252C%2520demonstrating%2520broad%2520applicability%2520in%2520real-world%2520scenarios%2520such%2520as%2520virtual%2520anchors%2520and%2520film%2520production.%2520Project%2520page%2520at%2520https%253A//zhangzjn.github.io/projects/Soul/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soul%3A%20Breathe%20Life%20into%20Digital%20Human%20for%20High-fidelity%20Long-term%20Multimodal%20Animation&entry.906535625=Jiangning%20Zhang%20and%20Junwei%20Zhu%20and%20Zhenye%20Gan%20and%20Donghao%20Luo%20and%20Chuming%20Lin%20and%20Feifan%20Xu%20and%20Xu%20Peng%20and%20Jianlong%20Hu%20and%20Yuansen%20Liu%20and%20Yijia%20Hong%20and%20Weijian%20Cao%20and%20Han%20Feng%20and%20Xu%20Chen%20and%20Chencan%20Fu%20and%20Keke%20He%20and%20Xiaobin%20Hu%20and%20Chengjie%20Wang&entry.1292438233=We%20propose%20a%20multimodal-driven%20framework%20for%20high-fidelity%20long-term%20digital%20human%20animation%20termed%20%24%5Ctextbf%7BSoul%7D%24%2C%20which%20generates%20semantically%20coherent%20videos%20from%20a%20single-frame%20portrait%20image%2C%20text%20prompts%2C%20and%20audio%2C%20achieving%20precise%20lip%20synchronization%2C%20vivid%20facial%20expressions%2C%20and%20robust%20identity%20preservation.%20We%20construct%20Soul-1M%2C%20containing%201%20million%20finely%20annotated%20samples%20with%20a%20precise%20automated%20annotation%20pipeline%20%28covering%20portrait%2C%20upper-body%2C%20full-body%2C%20and%20multi-person%20scenes%29%20to%20mitigate%20data%20scarcity%2C%20and%20we%20carefully%20curate%20Soul-Bench%20for%20comprehensive%20and%20fair%20evaluation%20of%20audio-/text-guided%20animation%20methods.%20The%20model%20is%20built%20on%20the%20Wan2.2-5B%20backbone%2C%20integrating%20audio-injection%20layers%20and%20multiple%20training%20strategies%20together%20with%20threshold-aware%20codebook%20replacement%20to%20ensure%20long-term%20generation%20consistency.%20Meanwhile%2C%20step/CFG%20distillation%20and%20a%20lightweight%20VAE%20are%20used%20to%20optimize%20inference%20efficiency%2C%20achieving%20an%2011.4%24%5Ctimes%24%20speedup%20with%20negligible%20quality%20loss.%20Extensive%20experiments%20show%20that%20Soul%20significantly%20outperforms%20current%20leading%20open-source%20and%20commercial%20models%20on%20video%20quality%2C%20video-text%20alignment%2C%20identity%20preservation%2C%20and%20lip-synchronization%20accuracy%2C%20demonstrating%20broad%20applicability%20in%20real-world%20scenarios%20such%20as%20virtual%20anchors%20and%20film%20production.%20Project%20page%20at%20https%3A//zhangzjn.github.io/projects/Soul/&entry.1838667208=http%3A//arxiv.org/abs/2512.13495v1&entry.124074799=Read"},
{"title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency", "author": "Wenhan Chen and Sezer Karaoglu and Theo Gevers", "abstract": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.", "link": "http://arxiv.org/abs/2512.13665v1", "date": "2025-12-15", "relevancy": 2.9916, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6096}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5953}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grab-3D%3A%20Detecting%20AI-Generated%20Videos%20from%203D%20Geometric%20Temporal%20Consistency&body=Title%3A%20Grab-3D%3A%20Detecting%20AI-Generated%20Videos%20from%203D%20Geometric%20Temporal%20Consistency%0AAuthor%3A%20Wenhan%20Chen%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%20Recent%20advances%20in%20diffusion-based%20generation%20techniques%20enable%20AI%20models%20to%20produce%20highly%20realistic%20videos%2C%20heightening%20the%20need%20for%20reliable%20detection%20mechanisms.%20However%2C%20existing%20detection%20methods%20provide%20only%20limited%20exploration%20of%20the%203D%20geometric%20patterns%20present%20in%20generated%20videos.%20In%20this%20paper%2C%20we%20use%20vanishing%20points%20as%20an%20explicit%20representation%20of%203D%20geometry%20patterns%2C%20revealing%20fundamental%20discrepancies%20in%20geometric%20consistency%20between%20real%20and%20AI-generated%20videos.%20We%20introduce%20Grab-3D%2C%20a%20geometry-aware%20transformer%20framework%20for%20detecting%20AI-generated%20videos%20based%20on%203D%20geometric%20temporal%20consistency.%20To%20enable%20reliable%20evaluation%2C%20we%20construct%20an%20AI-generated%20video%20dataset%20of%20static%20scenes%2C%20allowing%20stable%203D%20geometric%20feature%20extraction.%20We%20propose%20a%20geometry-aware%20transformer%20equipped%20with%20geometric%20positional%20encoding%2C%20temporal-geometric%20attention%2C%20and%20an%20EMA-based%20geometric%20classifier%20head%20to%20explicitly%20inject%203D%20geometric%20awareness%20into%20temporal%20modeling.%20Experiments%20demonstrate%20that%20Grab-3D%20significantly%20outperforms%20state-of-the-art%20detectors%2C%20achieving%20robust%20cross-domain%20generalization%20to%20unseen%20generators.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrab-3D%253A%2520Detecting%2520AI-Generated%2520Videos%2520from%25203D%2520Geometric%2520Temporal%2520Consistency%26entry.906535625%3DWenhan%2520Chen%2520and%2520Sezer%2520Karaoglu%2520and%2520Theo%2520Gevers%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion-based%2520generation%2520techniques%2520enable%2520AI%2520models%2520to%2520produce%2520highly%2520realistic%2520videos%252C%2520heightening%2520the%2520need%2520for%2520reliable%2520detection%2520mechanisms.%2520However%252C%2520existing%2520detection%2520methods%2520provide%2520only%2520limited%2520exploration%2520of%2520the%25203D%2520geometric%2520patterns%2520present%2520in%2520generated%2520videos.%2520In%2520this%2520paper%252C%2520we%2520use%2520vanishing%2520points%2520as%2520an%2520explicit%2520representation%2520of%25203D%2520geometry%2520patterns%252C%2520revealing%2520fundamental%2520discrepancies%2520in%2520geometric%2520consistency%2520between%2520real%2520and%2520AI-generated%2520videos.%2520We%2520introduce%2520Grab-3D%252C%2520a%2520geometry-aware%2520transformer%2520framework%2520for%2520detecting%2520AI-generated%2520videos%2520based%2520on%25203D%2520geometric%2520temporal%2520consistency.%2520To%2520enable%2520reliable%2520evaluation%252C%2520we%2520construct%2520an%2520AI-generated%2520video%2520dataset%2520of%2520static%2520scenes%252C%2520allowing%2520stable%25203D%2520geometric%2520feature%2520extraction.%2520We%2520propose%2520a%2520geometry-aware%2520transformer%2520equipped%2520with%2520geometric%2520positional%2520encoding%252C%2520temporal-geometric%2520attention%252C%2520and%2520an%2520EMA-based%2520geometric%2520classifier%2520head%2520to%2520explicitly%2520inject%25203D%2520geometric%2520awareness%2520into%2520temporal%2520modeling.%2520Experiments%2520demonstrate%2520that%2520Grab-3D%2520significantly%2520outperforms%2520state-of-the-art%2520detectors%252C%2520achieving%2520robust%2520cross-domain%2520generalization%2520to%2520unseen%2520generators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grab-3D%3A%20Detecting%20AI-Generated%20Videos%20from%203D%20Geometric%20Temporal%20Consistency&entry.906535625=Wenhan%20Chen%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=Recent%20advances%20in%20diffusion-based%20generation%20techniques%20enable%20AI%20models%20to%20produce%20highly%20realistic%20videos%2C%20heightening%20the%20need%20for%20reliable%20detection%20mechanisms.%20However%2C%20existing%20detection%20methods%20provide%20only%20limited%20exploration%20of%20the%203D%20geometric%20patterns%20present%20in%20generated%20videos.%20In%20this%20paper%2C%20we%20use%20vanishing%20points%20as%20an%20explicit%20representation%20of%203D%20geometry%20patterns%2C%20revealing%20fundamental%20discrepancies%20in%20geometric%20consistency%20between%20real%20and%20AI-generated%20videos.%20We%20introduce%20Grab-3D%2C%20a%20geometry-aware%20transformer%20framework%20for%20detecting%20AI-generated%20videos%20based%20on%203D%20geometric%20temporal%20consistency.%20To%20enable%20reliable%20evaluation%2C%20we%20construct%20an%20AI-generated%20video%20dataset%20of%20static%20scenes%2C%20allowing%20stable%203D%20geometric%20feature%20extraction.%20We%20propose%20a%20geometry-aware%20transformer%20equipped%20with%20geometric%20positional%20encoding%2C%20temporal-geometric%20attention%2C%20and%20an%20EMA-based%20geometric%20classifier%20head%20to%20explicitly%20inject%203D%20geometric%20awareness%20into%20temporal%20modeling.%20Experiments%20demonstrate%20that%20Grab-3D%20significantly%20outperforms%20state-of-the-art%20detectors%2C%20achieving%20robust%20cross-domain%20generalization%20to%20unseen%20generators.&entry.1838667208=http%3A//arxiv.org/abs/2512.13665v1&entry.124074799=Read"},
{"title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All", "author": "Michal Nazarczuk and Thomas Tanay and Arthur Moreau and Zhensong Zhang and Eduardo P\u00e9rez-Pellitero", "abstract": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.", "link": "http://arxiv.org/abs/2512.13639v1", "date": "2025-12-15", "relevancy": 2.9769, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6061}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Charge%3A%20A%20Comprehensive%20Novel%20View%20Synthesis%20Benchmark%20and%20Dataset%20to%20Bind%20Them%20All&body=Title%3A%20Charge%3A%20A%20Comprehensive%20Novel%20View%20Synthesis%20Benchmark%20and%20Dataset%20to%20Bind%20Them%20All%0AAuthor%3A%20Michal%20Nazarczuk%20and%20Thomas%20Tanay%20and%20Arthur%20Moreau%20and%20Zhensong%20Zhang%20and%20Eduardo%20P%C3%A9rez-Pellitero%0AAbstract%3A%20This%20paper%20presents%20a%20new%20dataset%20for%20Novel%20View%20Synthesis%2C%20generated%20from%20a%20high-quality%2C%20animated%20film%20with%20stunning%20realism%20and%20intricate%20detail.%20Our%20dataset%20captures%20a%20variety%20of%20dynamic%20scenes%2C%20complete%20with%20detailed%20textures%2C%20lighting%2C%20and%20motion%2C%20making%20it%20ideal%20for%20training%20and%20evaluating%20cutting-edge%204D%20scene%20reconstruction%20and%20novel%20view%20generation%20models.%20In%20addition%20to%20high-fidelity%20RGB%20images%2C%20we%20provide%20multiple%20complementary%20modalities%2C%20including%20depth%2C%20surface%20normals%2C%20object%20segmentation%20and%20optical%20flow%2C%20enabling%20a%20deeper%20understanding%20of%20scene%20geometry%20and%20motion.%20The%20dataset%20is%20organised%20into%20three%20distinct%20benchmarking%20scenarios%3A%20a%20dense%20multi-view%20camera%20setup%2C%20a%20sparse%20camera%20arrangement%2C%20and%20monocular%20video%20sequences%2C%20enabling%20a%20wide%20range%20of%20experimentation%20and%20comparison%20across%20varying%20levels%20of%20data%20sparsity.%20With%20its%20combination%20of%20visual%20richness%2C%20high-quality%20annotations%2C%20and%20diverse%20experimental%20setups%2C%20this%20dataset%20offers%20a%20unique%20resource%20for%20pushing%20the%20boundaries%20of%20view%20synthesis%20and%203D%20vision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharge%253A%2520A%2520Comprehensive%2520Novel%2520View%2520Synthesis%2520Benchmark%2520and%2520Dataset%2520to%2520Bind%2520Them%2520All%26entry.906535625%3DMichal%2520Nazarczuk%2520and%2520Thomas%2520Tanay%2520and%2520Arthur%2520Moreau%2520and%2520Zhensong%2520Zhang%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520new%2520dataset%2520for%2520Novel%2520View%2520Synthesis%252C%2520generated%2520from%2520a%2520high-quality%252C%2520animated%2520film%2520with%2520stunning%2520realism%2520and%2520intricate%2520detail.%2520Our%2520dataset%2520captures%2520a%2520variety%2520of%2520dynamic%2520scenes%252C%2520complete%2520with%2520detailed%2520textures%252C%2520lighting%252C%2520and%2520motion%252C%2520making%2520it%2520ideal%2520for%2520training%2520and%2520evaluating%2520cutting-edge%25204D%2520scene%2520reconstruction%2520and%2520novel%2520view%2520generation%2520models.%2520In%2520addition%2520to%2520high-fidelity%2520RGB%2520images%252C%2520we%2520provide%2520multiple%2520complementary%2520modalities%252C%2520including%2520depth%252C%2520surface%2520normals%252C%2520object%2520segmentation%2520and%2520optical%2520flow%252C%2520enabling%2520a%2520deeper%2520understanding%2520of%2520scene%2520geometry%2520and%2520motion.%2520The%2520dataset%2520is%2520organised%2520into%2520three%2520distinct%2520benchmarking%2520scenarios%253A%2520a%2520dense%2520multi-view%2520camera%2520setup%252C%2520a%2520sparse%2520camera%2520arrangement%252C%2520and%2520monocular%2520video%2520sequences%252C%2520enabling%2520a%2520wide%2520range%2520of%2520experimentation%2520and%2520comparison%2520across%2520varying%2520levels%2520of%2520data%2520sparsity.%2520With%2520its%2520combination%2520of%2520visual%2520richness%252C%2520high-quality%2520annotations%252C%2520and%2520diverse%2520experimental%2520setups%252C%2520this%2520dataset%2520offers%2520a%2520unique%2520resource%2520for%2520pushing%2520the%2520boundaries%2520of%2520view%2520synthesis%2520and%25203D%2520vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Charge%3A%20A%20Comprehensive%20Novel%20View%20Synthesis%20Benchmark%20and%20Dataset%20to%20Bind%20Them%20All&entry.906535625=Michal%20Nazarczuk%20and%20Thomas%20Tanay%20and%20Arthur%20Moreau%20and%20Zhensong%20Zhang%20and%20Eduardo%20P%C3%A9rez-Pellitero&entry.1292438233=This%20paper%20presents%20a%20new%20dataset%20for%20Novel%20View%20Synthesis%2C%20generated%20from%20a%20high-quality%2C%20animated%20film%20with%20stunning%20realism%20and%20intricate%20detail.%20Our%20dataset%20captures%20a%20variety%20of%20dynamic%20scenes%2C%20complete%20with%20detailed%20textures%2C%20lighting%2C%20and%20motion%2C%20making%20it%20ideal%20for%20training%20and%20evaluating%20cutting-edge%204D%20scene%20reconstruction%20and%20novel%20view%20generation%20models.%20In%20addition%20to%20high-fidelity%20RGB%20images%2C%20we%20provide%20multiple%20complementary%20modalities%2C%20including%20depth%2C%20surface%20normals%2C%20object%20segmentation%20and%20optical%20flow%2C%20enabling%20a%20deeper%20understanding%20of%20scene%20geometry%20and%20motion.%20The%20dataset%20is%20organised%20into%20three%20distinct%20benchmarking%20scenarios%3A%20a%20dense%20multi-view%20camera%20setup%2C%20a%20sparse%20camera%20arrangement%2C%20and%20monocular%20video%20sequences%2C%20enabling%20a%20wide%20range%20of%20experimentation%20and%20comparison%20across%20varying%20levels%20of%20data%20sparsity.%20With%20its%20combination%20of%20visual%20richness%2C%20high-quality%20annotations%2C%20and%20diverse%20experimental%20setups%2C%20this%20dataset%20offers%20a%20unique%20resource%20for%20pushing%20the%20boundaries%20of%20view%20synthesis%20and%203D%20vision.&entry.1838667208=http%3A//arxiv.org/abs/2512.13639v1&entry.124074799=Read"},
{"title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "author": "Siyan Chen and Yanfei Chen and Ying Chen and Zhuo Chen and Feng Cheng and Xuyan Chi and Jian Cong and Qinpeng Cui and Qide Dong and Junliang Fan and Jing Fang and Zetao Fang and Chengjian Feng and Han Feng and Mingyuan Gao and Yu Gao and Qiushan Guo and Boyang Hao and Qingkai Hao and Bibo He and Qian He and Tuyen Hoang and Ruoqing Hu and Xi Hu and Weilin Huang and Zhaoyang Huang and Zhongyi Huang and Siqi Jiang and Wei Jiang and Yunpu Jiang and Zhuo Jiang and Ashley Kim and Jianan Kong and Zhichao Lai and Shanshan Lao and Ai Li and Feiya Li and Gen Li and Huixia Li and JiaShi Li and Liang Li and Ming Li and Tao Li and Xian Li and Xiaojie Li and Xiaoyang Li and Xingxing Li and Yameng Li and Yifu Li and Yiying Li and Chao Liang and Ying Liang and Zhiqiang Liang and Wang Liao and Yalin Liao and Heng Lin and Kengyu Lin and Shanchuan Lin and Xi Lin and Zhijie Lin and Feng Ling and Fangfang Liu and Gaohong Liu and Jiawei Liu and Jie Liu and Shouda Liu and Shu Liu and Sichao Liu and Songwei Liu and Xin Liu and Xue Liu and Yibo Liu and Zikun Liu and Zuxi Liu and Junlin Lyu and Lecheng Lyu and Qian Lyu and Han Mu and Xiaonan Nie and Jingzhe Ning and Xitong Pan and Yanghua Peng and Lianke Qin and Xueqiong Qu and Yuxi Ren and Yuchen Shen and Guang Shi and Lei Shi and Yan Song and Yinglong Song and Fan Sun and Li Sun and Renfei Sun and Zeyu Sun and Wenjing Tang and Zirui Tao and Feng Wang and Furui Wang and Jinran Wang and Junkai Wang and Ke Wang and Kexin Wang and Qingyi Wang and Rui Wang and Sen Wang and Shuai Wang and Tingru Wang and Weichen Wang and Xin Wang and Yanhui Wang and Yue Wang and Yuping Wang and Yuxuan Wang and Ziyu Wang and Guoqiang Wei and Wanru Wei and Di Wu and Guohong Wu and Hanjie Wu and Jian Wu and Jie Wu and Ruolan Wu and Xinglong Wu and Yonghui Wu and Ruiqi Xia and Liang Xiang and Fei Xiao and XueFeng Xiao and Pan Xie and Shuangyi Xie and Shuang Xu and Jinlan Xue and Bangbang Yang and Ceyuan Yang and Jiaqi Yang and Runkai Yang and Tao Yang and Yang Yang and Yihang Yang and ZhiXian Yang and Ziyan Yang and Yifan Yao and Zilyu Ye and Bowen Yu and Chujie Yuan and Linxiao Yuan and Sichun Zeng and Weihong Zeng and Xuejiao Zeng and Yan Zeng and Chuntao Zhang and Heng Zhang and Jingjie Zhang and Kuo Zhang and Liang Zhang and Liying Zhang and Manlin Zhang and Ting Zhang and Weida Zhang and Xiaohe Zhang and Xinyan Zhang and Yan Zhang and Yuan Zhang and Zixiang Zhang and Fengxuan Zhao and Huating Zhao and Yang Zhao and Hao Zheng and Jianbin Zheng and Xiaozheng Zheng and Yangyang Zheng and Yijie Zheng and Jiexin Zhou and Kuan Zhu and Shenhan Zhu and Wenjia Zhu and Benhui Zou and Feilong Zuo", "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "link": "http://arxiv.org/abs/2512.13507v1", "date": "2025-12-15", "relevancy": 2.9499, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.603}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5852}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&body=Title%3A%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model%0AAuthor%3A%20Siyan%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Yuchen%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo%0AAbstract%3A%20Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedance%25201.5%2520pro%253A%2520A%2520Native%2520Audio-Visual%2520Joint%2520Generation%2520Foundation%2520Model%26entry.906535625%3DSiyan%2520Chen%2520and%2520Yanfei%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Zhuo%2520Chen%2520and%2520Feng%2520Cheng%2520and%2520Xuyan%2520Chi%2520and%2520Jian%2520Cong%2520and%2520Qinpeng%2520Cui%2520and%2520Qide%2520Dong%2520and%2520Junliang%2520Fan%2520and%2520Jing%2520Fang%2520and%2520Zetao%2520Fang%2520and%2520Chengjian%2520Feng%2520and%2520Han%2520Feng%2520and%2520Mingyuan%2520Gao%2520and%2520Yu%2520Gao%2520and%2520Qiushan%2520Guo%2520and%2520Boyang%2520Hao%2520and%2520Qingkai%2520Hao%2520and%2520Bibo%2520He%2520and%2520Qian%2520He%2520and%2520Tuyen%2520Hoang%2520and%2520Ruoqing%2520Hu%2520and%2520Xi%2520Hu%2520and%2520Weilin%2520Huang%2520and%2520Zhaoyang%2520Huang%2520and%2520Zhongyi%2520Huang%2520and%2520Siqi%2520Jiang%2520and%2520Wei%2520Jiang%2520and%2520Yunpu%2520Jiang%2520and%2520Zhuo%2520Jiang%2520and%2520Ashley%2520Kim%2520and%2520Jianan%2520Kong%2520and%2520Zhichao%2520Lai%2520and%2520Shanshan%2520Lao%2520and%2520Ai%2520Li%2520and%2520Feiya%2520Li%2520and%2520Gen%2520Li%2520and%2520Huixia%2520Li%2520and%2520JiaShi%2520Li%2520and%2520Liang%2520Li%2520and%2520Ming%2520Li%2520and%2520Tao%2520Li%2520and%2520Xian%2520Li%2520and%2520Xiaojie%2520Li%2520and%2520Xiaoyang%2520Li%2520and%2520Xingxing%2520Li%2520and%2520Yameng%2520Li%2520and%2520Yifu%2520Li%2520and%2520Yiying%2520Li%2520and%2520Chao%2520Liang%2520and%2520Ying%2520Liang%2520and%2520Zhiqiang%2520Liang%2520and%2520Wang%2520Liao%2520and%2520Yalin%2520Liao%2520and%2520Heng%2520Lin%2520and%2520Kengyu%2520Lin%2520and%2520Shanchuan%2520Lin%2520and%2520Xi%2520Lin%2520and%2520Zhijie%2520Lin%2520and%2520Feng%2520Ling%2520and%2520Fangfang%2520Liu%2520and%2520Gaohong%2520Liu%2520and%2520Jiawei%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Shouda%2520Liu%2520and%2520Shu%2520Liu%2520and%2520Sichao%2520Liu%2520and%2520Songwei%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Xue%2520Liu%2520and%2520Yibo%2520Liu%2520and%2520Zikun%2520Liu%2520and%2520Zuxi%2520Liu%2520and%2520Junlin%2520Lyu%2520and%2520Lecheng%2520Lyu%2520and%2520Qian%2520Lyu%2520and%2520Han%2520Mu%2520and%2520Xiaonan%2520Nie%2520and%2520Jingzhe%2520Ning%2520and%2520Xitong%2520Pan%2520and%2520Yanghua%2520Peng%2520and%2520Lianke%2520Qin%2520and%2520Xueqiong%2520Qu%2520and%2520Yuxi%2520Ren%2520and%2520Yuchen%2520Shen%2520and%2520Guang%2520Shi%2520and%2520Lei%2520Shi%2520and%2520Yan%2520Song%2520and%2520Yinglong%2520Song%2520and%2520Fan%2520Sun%2520and%2520Li%2520Sun%2520and%2520Renfei%2520Sun%2520and%2520Zeyu%2520Sun%2520and%2520Wenjing%2520Tang%2520and%2520Zirui%2520Tao%2520and%2520Feng%2520Wang%2520and%2520Furui%2520Wang%2520and%2520Jinran%2520Wang%2520and%2520Junkai%2520Wang%2520and%2520Ke%2520Wang%2520and%2520Kexin%2520Wang%2520and%2520Qingyi%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Sen%2520Wang%2520and%2520Shuai%2520Wang%2520and%2520Tingru%2520Wang%2520and%2520Weichen%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Yanhui%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Yuping%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Ziyu%2520Wang%2520and%2520Guoqiang%2520Wei%2520and%2520Wanru%2520Wei%2520and%2520Di%2520Wu%2520and%2520Guohong%2520Wu%2520and%2520Hanjie%2520Wu%2520and%2520Jian%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Ruolan%2520Wu%2520and%2520Xinglong%2520Wu%2520and%2520Yonghui%2520Wu%2520and%2520Ruiqi%2520Xia%2520and%2520Liang%2520Xiang%2520and%2520Fei%2520Xiao%2520and%2520XueFeng%2520Xiao%2520and%2520Pan%2520Xie%2520and%2520Shuangyi%2520Xie%2520and%2520Shuang%2520Xu%2520and%2520Jinlan%2520Xue%2520and%2520Bangbang%2520Yang%2520and%2520Ceyuan%2520Yang%2520and%2520Jiaqi%2520Yang%2520and%2520Runkai%2520Yang%2520and%2520Tao%2520Yang%2520and%2520Yang%2520Yang%2520and%2520Yihang%2520Yang%2520and%2520ZhiXian%2520Yang%2520and%2520Ziyan%2520Yang%2520and%2520Yifan%2520Yao%2520and%2520Zilyu%2520Ye%2520and%2520Bowen%2520Yu%2520and%2520Chujie%2520Yuan%2520and%2520Linxiao%2520Yuan%2520and%2520Sichun%2520Zeng%2520and%2520Weihong%2520Zeng%2520and%2520Xuejiao%2520Zeng%2520and%2520Yan%2520Zeng%2520and%2520Chuntao%2520Zhang%2520and%2520Heng%2520Zhang%2520and%2520Jingjie%2520Zhang%2520and%2520Kuo%2520Zhang%2520and%2520Liang%2520Zhang%2520and%2520Liying%2520Zhang%2520and%2520Manlin%2520Zhang%2520and%2520Ting%2520Zhang%2520and%2520Weida%2520Zhang%2520and%2520Xiaohe%2520Zhang%2520and%2520Xinyan%2520Zhang%2520and%2520Yan%2520Zhang%2520and%2520Yuan%2520Zhang%2520and%2520Zixiang%2520Zhang%2520and%2520Fengxuan%2520Zhao%2520and%2520Huating%2520Zhao%2520and%2520Yang%2520Zhao%2520and%2520Hao%2520Zheng%2520and%2520Jianbin%2520Zheng%2520and%2520Xiaozheng%2520Zheng%2520and%2520Yangyang%2520Zheng%2520and%2520Yijie%2520Zheng%2520and%2520Jiexin%2520Zhou%2520and%2520Kuan%2520Zhu%2520and%2520Shenhan%2520Zhu%2520and%2520Wenjia%2520Zhu%2520and%2520Benhui%2520Zou%2520and%2520Feilong%2520Zuo%26entry.1292438233%3DRecent%2520strides%2520in%2520video%2520generation%2520have%2520paved%2520the%2520way%2520for%2520unified%2520audio-visual%2520generation.%2520In%2520this%2520work%252C%2520we%2520present%2520Seedance%25201.5%2520pro%252C%2520a%2520foundational%2520model%2520engineered%2520specifically%2520for%2520native%252C%2520joint%2520audio-video%2520generation.%2520Leveraging%2520a%2520dual-branch%2520Diffusion%2520Transformer%2520architecture%252C%2520the%2520model%2520integrates%2520a%2520cross-modal%2520joint%2520module%2520with%2520a%2520specialized%2520multi-stage%2520data%2520pipeline%252C%2520achieving%2520exceptional%2520audio-visual%2520synchronization%2520and%2520superior%2520generation%2520quality.%2520To%2520ensure%2520practical%2520utility%252C%2520we%2520implement%2520meticulous%2520post-training%2520optimizations%252C%2520including%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520high-quality%2520datasets%2520and%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520with%2520multi-dimensional%2520reward%2520models.%2520Furthermore%252C%2520we%2520introduce%2520an%2520acceleration%2520framework%2520that%2520boosts%2520inference%2520speed%2520by%2520over%252010X.%2520Seedance%25201.5%2520pro%2520distinguishes%2520itself%2520through%2520precise%2520multilingual%2520and%2520dialect%2520lip-syncing%252C%2520dynamic%2520cinematic%2520camera%2520control%252C%2520and%2520enhanced%2520narrative%2520coherence%252C%2520positioning%2520it%2520as%2520a%2520robust%2520engine%2520for%2520professional-grade%2520content%2520creation.%2520Seedance%25201.5%2520pro%2520is%2520now%2520accessible%2520on%2520Volcano%2520Engine%2520at%2520https%253A//console.volcengine.com/ark/region%253Aark%252Bcn-beijing/experience/vision%253Ftype%253DGenVideo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&entry.906535625=Siyan%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Yuchen%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo&entry.1292438233=Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.&entry.1838667208=http%3A//arxiv.org/abs/2512.13507v1&entry.124074799=Read"},
{"title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction", "author": "Peter Kocsis and Lukas H\u00f6llein and Matthias Nie\u00dfner", "abstract": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.", "link": "http://arxiv.org/abs/2512.13157v1", "date": "2025-12-15", "relevancy": 2.9463, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6067}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6067}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Image%20Fusion%20for%20Multi-View%203D%20Material%20Reconstruction&body=Title%3A%20Intrinsic%20Image%20Fusion%20for%20Multi-View%203D%20Material%20Reconstruction%0AAuthor%3A%20Peter%20Kocsis%20and%20Lukas%20H%C3%B6llein%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20We%20introduce%20Intrinsic%20Image%20Fusion%2C%20a%20method%20that%20reconstructs%20high-quality%20physically%20based%20materials%20from%20multi-view%20images.%20Material%20reconstruction%20is%20highly%20underconstrained%20and%20typically%20relies%20on%20analysis-by-synthesis%2C%20which%20requires%20expensive%20and%20noisy%20path%20tracing.%20To%20better%20constrain%20the%20optimization%2C%20we%20incorporate%20single-view%20priors%20into%20the%20reconstruction%20process.%20We%20leverage%20a%20diffusion-based%20material%20estimator%20that%20produces%20multiple%2C%20but%20often%20inconsistent%2C%20candidate%20decompositions%20per%20view.%20To%20reduce%20the%20inconsistency%2C%20we%20fit%20an%20explicit%20low-dimensional%20parametric%20function%20to%20the%20predictions.%20We%20then%20propose%20a%20robust%20optimization%20framework%20using%20soft%20per-view%20prediction%20selection%20together%20with%20confidence-based%20soft%20multi-view%20inlier%20set%20to%20fuse%20the%20most%20consistent%20predictions%20of%20the%20most%20confident%20views%20into%20a%20consistent%20parametric%20material%20space.%20Finally%2C%20we%20use%20inverse%20path%20tracing%20to%20optimize%20for%20the%20low-dimensional%20parameters.%20Our%20results%20outperform%20state-of-the-art%20methods%20in%20material%20disentanglement%20on%20both%20synthetic%20and%20real%20scenes%2C%20producing%20sharp%20and%20clean%20reconstructions%20suitable%20for%20high-quality%20relighting.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Image%2520Fusion%2520for%2520Multi-View%25203D%2520Material%2520Reconstruction%26entry.906535625%3DPeter%2520Kocsis%2520and%2520Lukas%2520H%25C3%25B6llein%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3DWe%2520introduce%2520Intrinsic%2520Image%2520Fusion%252C%2520a%2520method%2520that%2520reconstructs%2520high-quality%2520physically%2520based%2520materials%2520from%2520multi-view%2520images.%2520Material%2520reconstruction%2520is%2520highly%2520underconstrained%2520and%2520typically%2520relies%2520on%2520analysis-by-synthesis%252C%2520which%2520requires%2520expensive%2520and%2520noisy%2520path%2520tracing.%2520To%2520better%2520constrain%2520the%2520optimization%252C%2520we%2520incorporate%2520single-view%2520priors%2520into%2520the%2520reconstruction%2520process.%2520We%2520leverage%2520a%2520diffusion-based%2520material%2520estimator%2520that%2520produces%2520multiple%252C%2520but%2520often%2520inconsistent%252C%2520candidate%2520decompositions%2520per%2520view.%2520To%2520reduce%2520the%2520inconsistency%252C%2520we%2520fit%2520an%2520explicit%2520low-dimensional%2520parametric%2520function%2520to%2520the%2520predictions.%2520We%2520then%2520propose%2520a%2520robust%2520optimization%2520framework%2520using%2520soft%2520per-view%2520prediction%2520selection%2520together%2520with%2520confidence-based%2520soft%2520multi-view%2520inlier%2520set%2520to%2520fuse%2520the%2520most%2520consistent%2520predictions%2520of%2520the%2520most%2520confident%2520views%2520into%2520a%2520consistent%2520parametric%2520material%2520space.%2520Finally%252C%2520we%2520use%2520inverse%2520path%2520tracing%2520to%2520optimize%2520for%2520the%2520low-dimensional%2520parameters.%2520Our%2520results%2520outperform%2520state-of-the-art%2520methods%2520in%2520material%2520disentanglement%2520on%2520both%2520synthetic%2520and%2520real%2520scenes%252C%2520producing%2520sharp%2520and%2520clean%2520reconstructions%2520suitable%2520for%2520high-quality%2520relighting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Image%20Fusion%20for%20Multi-View%203D%20Material%20Reconstruction&entry.906535625=Peter%20Kocsis%20and%20Lukas%20H%C3%B6llein%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=We%20introduce%20Intrinsic%20Image%20Fusion%2C%20a%20method%20that%20reconstructs%20high-quality%20physically%20based%20materials%20from%20multi-view%20images.%20Material%20reconstruction%20is%20highly%20underconstrained%20and%20typically%20relies%20on%20analysis-by-synthesis%2C%20which%20requires%20expensive%20and%20noisy%20path%20tracing.%20To%20better%20constrain%20the%20optimization%2C%20we%20incorporate%20single-view%20priors%20into%20the%20reconstruction%20process.%20We%20leverage%20a%20diffusion-based%20material%20estimator%20that%20produces%20multiple%2C%20but%20often%20inconsistent%2C%20candidate%20decompositions%20per%20view.%20To%20reduce%20the%20inconsistency%2C%20we%20fit%20an%20explicit%20low-dimensional%20parametric%20function%20to%20the%20predictions.%20We%20then%20propose%20a%20robust%20optimization%20framework%20using%20soft%20per-view%20prediction%20selection%20together%20with%20confidence-based%20soft%20multi-view%20inlier%20set%20to%20fuse%20the%20most%20consistent%20predictions%20of%20the%20most%20confident%20views%20into%20a%20consistent%20parametric%20material%20space.%20Finally%2C%20we%20use%20inverse%20path%20tracing%20to%20optimize%20for%20the%20low-dimensional%20parameters.%20Our%20results%20outperform%20state-of-the-art%20methods%20in%20material%20disentanglement%20on%20both%20synthetic%20and%20real%20scenes%2C%20producing%20sharp%20and%20clean%20reconstructions%20suitable%20for%20high-quality%20relighting.&entry.1838667208=http%3A//arxiv.org/abs/2512.13157v1&entry.124074799=Read"},
{"title": "How PARTs assemble into wholes: Learning the relative composition of images", "author": "Melika Ayoughi and Samira Abnar and Chen Huang and Chris Sandino and Sayeri Lala and Eeshan Gunesh Dhekane and Dan Busbridge and Shuangfei Zhai and Vimal Thilak and Josh Susskind and Pascal Mettes and Paul Groth and Hanlin Goh", "abstract": "The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.", "link": "http://arxiv.org/abs/2506.03682v2", "date": "2025-12-15", "relevancy": 2.9147, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20PARTs%20assemble%20into%20wholes%3A%20Learning%20the%20relative%20composition%20of%20images&body=Title%3A%20How%20PARTs%20assemble%20into%20wholes%3A%20Learning%20the%20relative%20composition%20of%20images%0AAuthor%3A%20Melika%20Ayoughi%20and%20Samira%20Abnar%20and%20Chen%20Huang%20and%20Chris%20Sandino%20and%20Sayeri%20Lala%20and%20Eeshan%20Gunesh%20Dhekane%20and%20Dan%20Busbridge%20and%20Shuangfei%20Zhai%20and%20Vimal%20Thilak%20and%20Josh%20Susskind%20and%20Pascal%20Mettes%20and%20Paul%20Groth%20and%20Hanlin%20Goh%0AAbstract%3A%20The%20composition%20of%20objects%20and%20their%20parts%2C%20along%20with%20object-object%20positional%20relationships%2C%20provides%20a%20rich%20source%20of%20information%20for%20representation%20learning.%20Hence%2C%20spatial-aware%20pretext%20tasks%20have%20been%20actively%20explored%20in%20self-supervised%20learning.%20Existing%20works%20commonly%20start%20from%20a%20grid%20structure%2C%20where%20the%20goal%20of%20the%20pretext%20task%20involves%20predicting%20the%20absolute%20position%20index%20of%20patches%20within%20a%20fixed%20grid.%20However%2C%20grid-based%20approaches%20fall%20short%20of%20capturing%20the%20fluid%20and%20continuous%20nature%20of%20real-world%20object%20compositions.%20We%20introduce%20PART%2C%20a%20self-supervised%20learning%20approach%20that%20leverages%20continuous%20relative%20transformations%20between%20off-grid%20patches%20to%20overcome%20these%20limitations.%20By%20modeling%20how%20parts%20relate%20to%20each%20other%20in%20a%20continuous%20space%2C%20PART%20learns%20the%20relative%20composition%20of%20images-an%20off-grid%20structural%20relative%20positioning%20that%20is%20less%20tied%20to%20absolute%20appearance%20and%20can%20remain%20coherent%20under%20variations%20such%20as%20partial%20visibility%20or%20stylistic%20changes.%20In%20tasks%20requiring%20precise%20spatial%20understanding%20such%20as%20object%20detection%20and%20time%20series%20prediction%2C%20PART%20outperforms%20grid-based%20methods%20like%20MAE%20and%20DropPos%2C%20while%20maintaining%20competitive%20performance%20on%20global%20classification%20tasks.%20By%20breaking%20free%20from%20grid%20constraints%2C%20PART%20opens%20up%20a%20new%20trajectory%20for%20universal%20self-supervised%20pretraining%20across%20diverse%20datatypes-from%20images%20to%20EEG%20signals-with%20potential%20in%20medical%20imaging%2C%20video%2C%20and%20audio.%0ALink%3A%20http%3A//arxiv.org/abs/2506.03682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520PARTs%2520assemble%2520into%2520wholes%253A%2520Learning%2520the%2520relative%2520composition%2520of%2520images%26entry.906535625%3DMelika%2520Ayoughi%2520and%2520Samira%2520Abnar%2520and%2520Chen%2520Huang%2520and%2520Chris%2520Sandino%2520and%2520Sayeri%2520Lala%2520and%2520Eeshan%2520Gunesh%2520Dhekane%2520and%2520Dan%2520Busbridge%2520and%2520Shuangfei%2520Zhai%2520and%2520Vimal%2520Thilak%2520and%2520Josh%2520Susskind%2520and%2520Pascal%2520Mettes%2520and%2520Paul%2520Groth%2520and%2520Hanlin%2520Goh%26entry.1292438233%3DThe%2520composition%2520of%2520objects%2520and%2520their%2520parts%252C%2520along%2520with%2520object-object%2520positional%2520relationships%252C%2520provides%2520a%2520rich%2520source%2520of%2520information%2520for%2520representation%2520learning.%2520Hence%252C%2520spatial-aware%2520pretext%2520tasks%2520have%2520been%2520actively%2520explored%2520in%2520self-supervised%2520learning.%2520Existing%2520works%2520commonly%2520start%2520from%2520a%2520grid%2520structure%252C%2520where%2520the%2520goal%2520of%2520the%2520pretext%2520task%2520involves%2520predicting%2520the%2520absolute%2520position%2520index%2520of%2520patches%2520within%2520a%2520fixed%2520grid.%2520However%252C%2520grid-based%2520approaches%2520fall%2520short%2520of%2520capturing%2520the%2520fluid%2520and%2520continuous%2520nature%2520of%2520real-world%2520object%2520compositions.%2520We%2520introduce%2520PART%252C%2520a%2520self-supervised%2520learning%2520approach%2520that%2520leverages%2520continuous%2520relative%2520transformations%2520between%2520off-grid%2520patches%2520to%2520overcome%2520these%2520limitations.%2520By%2520modeling%2520how%2520parts%2520relate%2520to%2520each%2520other%2520in%2520a%2520continuous%2520space%252C%2520PART%2520learns%2520the%2520relative%2520composition%2520of%2520images-an%2520off-grid%2520structural%2520relative%2520positioning%2520that%2520is%2520less%2520tied%2520to%2520absolute%2520appearance%2520and%2520can%2520remain%2520coherent%2520under%2520variations%2520such%2520as%2520partial%2520visibility%2520or%2520stylistic%2520changes.%2520In%2520tasks%2520requiring%2520precise%2520spatial%2520understanding%2520such%2520as%2520object%2520detection%2520and%2520time%2520series%2520prediction%252C%2520PART%2520outperforms%2520grid-based%2520methods%2520like%2520MAE%2520and%2520DropPos%252C%2520while%2520maintaining%2520competitive%2520performance%2520on%2520global%2520classification%2520tasks.%2520By%2520breaking%2520free%2520from%2520grid%2520constraints%252C%2520PART%2520opens%2520up%2520a%2520new%2520trajectory%2520for%2520universal%2520self-supervised%2520pretraining%2520across%2520diverse%2520datatypes-from%2520images%2520to%2520EEG%2520signals-with%2520potential%2520in%2520medical%2520imaging%252C%2520video%252C%2520and%2520audio.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20PARTs%20assemble%20into%20wholes%3A%20Learning%20the%20relative%20composition%20of%20images&entry.906535625=Melika%20Ayoughi%20and%20Samira%20Abnar%20and%20Chen%20Huang%20and%20Chris%20Sandino%20and%20Sayeri%20Lala%20and%20Eeshan%20Gunesh%20Dhekane%20and%20Dan%20Busbridge%20and%20Shuangfei%20Zhai%20and%20Vimal%20Thilak%20and%20Josh%20Susskind%20and%20Pascal%20Mettes%20and%20Paul%20Groth%20and%20Hanlin%20Goh&entry.1292438233=The%20composition%20of%20objects%20and%20their%20parts%2C%20along%20with%20object-object%20positional%20relationships%2C%20provides%20a%20rich%20source%20of%20information%20for%20representation%20learning.%20Hence%2C%20spatial-aware%20pretext%20tasks%20have%20been%20actively%20explored%20in%20self-supervised%20learning.%20Existing%20works%20commonly%20start%20from%20a%20grid%20structure%2C%20where%20the%20goal%20of%20the%20pretext%20task%20involves%20predicting%20the%20absolute%20position%20index%20of%20patches%20within%20a%20fixed%20grid.%20However%2C%20grid-based%20approaches%20fall%20short%20of%20capturing%20the%20fluid%20and%20continuous%20nature%20of%20real-world%20object%20compositions.%20We%20introduce%20PART%2C%20a%20self-supervised%20learning%20approach%20that%20leverages%20continuous%20relative%20transformations%20between%20off-grid%20patches%20to%20overcome%20these%20limitations.%20By%20modeling%20how%20parts%20relate%20to%20each%20other%20in%20a%20continuous%20space%2C%20PART%20learns%20the%20relative%20composition%20of%20images-an%20off-grid%20structural%20relative%20positioning%20that%20is%20less%20tied%20to%20absolute%20appearance%20and%20can%20remain%20coherent%20under%20variations%20such%20as%20partial%20visibility%20or%20stylistic%20changes.%20In%20tasks%20requiring%20precise%20spatial%20understanding%20such%20as%20object%20detection%20and%20time%20series%20prediction%2C%20PART%20outperforms%20grid-based%20methods%20like%20MAE%20and%20DropPos%2C%20while%20maintaining%20competitive%20performance%20on%20global%20classification%20tasks.%20By%20breaking%20free%20from%20grid%20constraints%2C%20PART%20opens%20up%20a%20new%20trajectory%20for%20universal%20self-supervised%20pretraining%20across%20diverse%20datatypes-from%20images%20to%20EEG%20signals-with%20potential%20in%20medical%20imaging%2C%20video%2C%20and%20audio.&entry.1838667208=http%3A//arxiv.org/abs/2506.03682v2&entry.124074799=Read"},
{"title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection", "author": "Juil Koo and Daehyeon Choi and Sangwoo Youn and Phillip Y. Lee and Minhyuk Sung", "abstract": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.", "link": "http://arxiv.org/abs/2512.13250v1", "date": "2025-12-15", "relevancy": 2.9058, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Ambulatory%20Vision%3A%20Learning%20Visually-Grounded%20Active%20View%20Selection&body=Title%3A%20Toward%20Ambulatory%20Vision%3A%20Learning%20Visually-Grounded%20Active%20View%20Selection%0AAuthor%3A%20Juil%20Koo%20and%20Daehyeon%20Choi%20and%20Sangwoo%20Youn%20and%20Phillip%20Y.%20Lee%20and%20Minhyuk%20Sung%0AAbstract%3A%20Vision%20Language%20Models%20%28VLMs%29%20excel%20at%20visual%20question%20answering%20%28VQA%29%20but%20remain%20limited%20to%20snapshot%20vision%2C%20reasoning%20from%20static%20images.%20In%20contrast%2C%20embodied%20agents%20require%20ambulatory%20vision%2C%20actively%20moving%20to%20obtain%20more%20informative%20views.%20We%20introduce%20Visually%20Grounded%20Active%20View%20Selection%20%28VG-AVS%29%2C%20a%20task%20that%20selects%20the%20most%20informative%20next%20viewpoint%20using%20only%20the%20visual%20information%20in%20the%20current%20image%2C%20without%20relying%20on%20scene%20memory%20or%20external%20knowledge.%20To%20support%20this%20task%2C%20we%20construct%20a%20synthetic%20dataset%20with%20automatically%20generated%20paired%20query-target%20views%20and%20question-answer%20prompts.%20We%20also%20propose%20a%20framework%20that%20fine-tunes%20pretrained%20VLMs%20through%20supervised%20fine-tuning%20%28SFT%29%20followed%20by%20RL-based%20policy%20optimization.%20Our%20approach%20achieves%20strong%20question%20answering%20performance%20based%20on%20viewpoint%20selection%20and%20generalizes%20robustly%20to%20unseen%20synthetic%20and%20real%20scenes.%20Furthermore%2C%20incorporating%20our%20learned%20VG-AVS%20framework%20into%20existing%20scene-exploration-based%20EQA%20systems%20improves%20downstream%20question-answering%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Ambulatory%2520Vision%253A%2520Learning%2520Visually-Grounded%2520Active%2520View%2520Selection%26entry.906535625%3DJuil%2520Koo%2520and%2520Daehyeon%2520Choi%2520and%2520Sangwoo%2520Youn%2520and%2520Phillip%2520Y.%2520Lee%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3DVision%2520Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520visual%2520question%2520answering%2520%2528VQA%2529%2520but%2520remain%2520limited%2520to%2520snapshot%2520vision%252C%2520reasoning%2520from%2520static%2520images.%2520In%2520contrast%252C%2520embodied%2520agents%2520require%2520ambulatory%2520vision%252C%2520actively%2520moving%2520to%2520obtain%2520more%2520informative%2520views.%2520We%2520introduce%2520Visually%2520Grounded%2520Active%2520View%2520Selection%2520%2528VG-AVS%2529%252C%2520a%2520task%2520that%2520selects%2520the%2520most%2520informative%2520next%2520viewpoint%2520using%2520only%2520the%2520visual%2520information%2520in%2520the%2520current%2520image%252C%2520without%2520relying%2520on%2520scene%2520memory%2520or%2520external%2520knowledge.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520a%2520synthetic%2520dataset%2520with%2520automatically%2520generated%2520paired%2520query-target%2520views%2520and%2520question-answer%2520prompts.%2520We%2520also%2520propose%2520a%2520framework%2520that%2520fine-tunes%2520pretrained%2520VLMs%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520followed%2520by%2520RL-based%2520policy%2520optimization.%2520Our%2520approach%2520achieves%2520strong%2520question%2520answering%2520performance%2520based%2520on%2520viewpoint%2520selection%2520and%2520generalizes%2520robustly%2520to%2520unseen%2520synthetic%2520and%2520real%2520scenes.%2520Furthermore%252C%2520incorporating%2520our%2520learned%2520VG-AVS%2520framework%2520into%2520existing%2520scene-exploration-based%2520EQA%2520systems%2520improves%2520downstream%2520question-answering%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Ambulatory%20Vision%3A%20Learning%20Visually-Grounded%20Active%20View%20Selection&entry.906535625=Juil%20Koo%20and%20Daehyeon%20Choi%20and%20Sangwoo%20Youn%20and%20Phillip%20Y.%20Lee%20and%20Minhyuk%20Sung&entry.1292438233=Vision%20Language%20Models%20%28VLMs%29%20excel%20at%20visual%20question%20answering%20%28VQA%29%20but%20remain%20limited%20to%20snapshot%20vision%2C%20reasoning%20from%20static%20images.%20In%20contrast%2C%20embodied%20agents%20require%20ambulatory%20vision%2C%20actively%20moving%20to%20obtain%20more%20informative%20views.%20We%20introduce%20Visually%20Grounded%20Active%20View%20Selection%20%28VG-AVS%29%2C%20a%20task%20that%20selects%20the%20most%20informative%20next%20viewpoint%20using%20only%20the%20visual%20information%20in%20the%20current%20image%2C%20without%20relying%20on%20scene%20memory%20or%20external%20knowledge.%20To%20support%20this%20task%2C%20we%20construct%20a%20synthetic%20dataset%20with%20automatically%20generated%20paired%20query-target%20views%20and%20question-answer%20prompts.%20We%20also%20propose%20a%20framework%20that%20fine-tunes%20pretrained%20VLMs%20through%20supervised%20fine-tuning%20%28SFT%29%20followed%20by%20RL-based%20policy%20optimization.%20Our%20approach%20achieves%20strong%20question%20answering%20performance%20based%20on%20viewpoint%20selection%20and%20generalizes%20robustly%20to%20unseen%20synthetic%20and%20real%20scenes.%20Furthermore%2C%20incorporating%20our%20learned%20VG-AVS%20framework%20into%20existing%20scene-exploration-based%20EQA%20systems%20improves%20downstream%20question-answering%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.13250v1&entry.124074799=Read"},
{"title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture", "author": "Xin He and Longhui Wei and Jianbo Ouyang and Minghui Liao and Lingxi Xie and Qi Tian", "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.", "link": "http://arxiv.org/abs/2512.04810v5", "date": "2025-12-15", "relevancy": 2.829, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&body=Title%3A%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture%0AAuthor%3A%20Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Minghui%20Liao%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04810v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Efficient%2520Multimodal%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520with%2520a%2520Unified%2520Architecture%26entry.906535625%3DXin%2520He%2520and%2520Longhui%2520Wei%2520and%2520Jianbo%2520Ouyang%2520and%2520Minghui%2520Liao%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3DWe%2520propose%2520EMMA%252C%2520an%2520efficient%2520and%2520unified%2520architecture%2520for%2520multimodal%2520understanding%252C%2520generation%2520and%2520editing.%2520Specifically%252C%2520EMMA%2520primarily%2520consists%2520of%25201%2529%2520An%2520efficient%2520autoencoder%2520with%2520a%252032x%2520compression%2520ratio%252C%2520which%2520significantly%2520reduces%2520the%2520number%2520of%2520tokens%2520required%2520for%2520generation.%2520This%2520also%2520ensures%2520the%2520training%2520balance%2520between%2520understanding%2520and%2520generation%2520tasks%2520by%2520applying%2520the%2520same%2520compression%2520ratio%2520to%2520images.%25202%2529%2520Channel-wise%2520concatenation%2520instead%2520of%2520token-wise%2520concatenation%2520among%2520visual%2520understanding%2520and%2520generation%2520tokens%252C%2520which%2520further%2520reduces%2520the%2520visual%2520tokens%2520in%2520unified%2520architectures.%25203%2529%2520A%2520shared-and-decoupled%2520network%2520that%2520enables%2520mutual%2520improvements%2520across%2520tasks%2520while%2520meeting%2520the%2520task-specific%2520modeling%2520requirements.%25204%2529%2520A%2520mixture-of-experts%2520mechanism%2520adopted%2520for%2520visual%2520understanding%2520encoder%252C%2520which%2520substantially%2520improves%2520perceptual%2520capabilities%2520with%2520a%2520few%2520parameters%2520increase.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520EMMA-4B%2520can%2520significantly%2520outperform%2520state-of-the-art%2520unified%2520multimodal%2520approaches%2520%2528e.g.%252C%2520BAGEL-7B%2529%2520in%2520both%2520efficiency%2520and%2520performance%252C%2520while%2520also%2520achieving%2520competitive%2520results%2520compared%2520to%2520recent%2520multimodal%2520understanding%2520and%2520generation%2520experts%2520%2528e.g.%252C%2520Qwen3-VL%2520and%2520Qwen-Image%2529.%2520We%2520believe%2520that%2520EMMA%2520lays%2520a%2520solid%2520foundation%2520for%2520the%2520future%2520development%2520of%2520unified%2520multimodal%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04810v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&entry.906535625=Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Minghui%20Liao%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04810v5&entry.124074799=Read"},
{"title": "Towards Interactive Intelligence for Digital Humans", "author": "Yiyi Cai and Xuangeng Chu and Xiwei Gao and Sitong Gong and Yifei Huang and Caixin Kang and Kunhang Li and Haiyang Liu and Ruicong Liu and Yun Liu and Dianwen Ng and Zixiong Su and Erwin Wu and Yuhan Wu and Dingkun Yan and Tianyu Yan and Chang Zeng and Bo Zheng and You Zhou", "abstract": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.", "link": "http://arxiv.org/abs/2512.13674v1", "date": "2025-12-15", "relevancy": 2.8261, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.581}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5777}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interactive%20Intelligence%20for%20Digital%20Humans&body=Title%3A%20Towards%20Interactive%20Intelligence%20for%20Digital%20Humans%0AAuthor%3A%20Yiyi%20Cai%20and%20Xuangeng%20Chu%20and%20Xiwei%20Gao%20and%20Sitong%20Gong%20and%20Yifei%20Huang%20and%20Caixin%20Kang%20and%20Kunhang%20Li%20and%20Haiyang%20Liu%20and%20Ruicong%20Liu%20and%20Yun%20Liu%20and%20Dianwen%20Ng%20and%20Zixiong%20Su%20and%20Erwin%20Wu%20and%20Yuhan%20Wu%20and%20Dingkun%20Yan%20and%20Tianyu%20Yan%20and%20Chang%20Zeng%20and%20Bo%20Zheng%20and%20You%20Zhou%0AAbstract%3A%20We%20introduce%20Interactive%20Intelligence%2C%20a%20novel%20paradigm%20of%20digital%20human%20that%20is%20capable%20of%20personality-aligned%20expression%2C%20adaptive%20interaction%2C%20and%20self-evolution.%20To%20realize%20this%2C%20we%20present%20Mio%20%28Multimodal%20Interactive%20Omni-Avatar%29%2C%20an%20end-to-end%20framework%20composed%20of%20five%20specialized%20modules%3A%20Thinker%2C%20Talker%2C%20Face%20Animator%2C%20Body%20Animator%2C%20and%20Renderer.%20This%20unified%20architecture%20integrates%20cognitive%20reasoning%20with%20real-time%20multimodal%20embodiment%20to%20enable%20fluid%2C%20consistent%20interaction.%20Furthermore%2C%20we%20establish%20a%20new%20benchmark%20to%20rigorously%20evaluate%20the%20capabilities%20of%20interactive%20intelligence.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20methods%20across%20all%20evaluated%20dimensions.%20Together%2C%20these%20contributions%20move%20digital%20humans%20beyond%20superficial%20imitation%20toward%20intelligent%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interactive%2520Intelligence%2520for%2520Digital%2520Humans%26entry.906535625%3DYiyi%2520Cai%2520and%2520Xuangeng%2520Chu%2520and%2520Xiwei%2520Gao%2520and%2520Sitong%2520Gong%2520and%2520Yifei%2520Huang%2520and%2520Caixin%2520Kang%2520and%2520Kunhang%2520Li%2520and%2520Haiyang%2520Liu%2520and%2520Ruicong%2520Liu%2520and%2520Yun%2520Liu%2520and%2520Dianwen%2520Ng%2520and%2520Zixiong%2520Su%2520and%2520Erwin%2520Wu%2520and%2520Yuhan%2520Wu%2520and%2520Dingkun%2520Yan%2520and%2520Tianyu%2520Yan%2520and%2520Chang%2520Zeng%2520and%2520Bo%2520Zheng%2520and%2520You%2520Zhou%26entry.1292438233%3DWe%2520introduce%2520Interactive%2520Intelligence%252C%2520a%2520novel%2520paradigm%2520of%2520digital%2520human%2520that%2520is%2520capable%2520of%2520personality-aligned%2520expression%252C%2520adaptive%2520interaction%252C%2520and%2520self-evolution.%2520To%2520realize%2520this%252C%2520we%2520present%2520Mio%2520%2528Multimodal%2520Interactive%2520Omni-Avatar%2529%252C%2520an%2520end-to-end%2520framework%2520composed%2520of%2520five%2520specialized%2520modules%253A%2520Thinker%252C%2520Talker%252C%2520Face%2520Animator%252C%2520Body%2520Animator%252C%2520and%2520Renderer.%2520This%2520unified%2520architecture%2520integrates%2520cognitive%2520reasoning%2520with%2520real-time%2520multimodal%2520embodiment%2520to%2520enable%2520fluid%252C%2520consistent%2520interaction.%2520Furthermore%252C%2520we%2520establish%2520a%2520new%2520benchmark%2520to%2520rigorously%2520evaluate%2520the%2520capabilities%2520of%2520interactive%2520intelligence.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520methods%2520across%2520all%2520evaluated%2520dimensions.%2520Together%252C%2520these%2520contributions%2520move%2520digital%2520humans%2520beyond%2520superficial%2520imitation%2520toward%2520intelligent%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interactive%20Intelligence%20for%20Digital%20Humans&entry.906535625=Yiyi%20Cai%20and%20Xuangeng%20Chu%20and%20Xiwei%20Gao%20and%20Sitong%20Gong%20and%20Yifei%20Huang%20and%20Caixin%20Kang%20and%20Kunhang%20Li%20and%20Haiyang%20Liu%20and%20Ruicong%20Liu%20and%20Yun%20Liu%20and%20Dianwen%20Ng%20and%20Zixiong%20Su%20and%20Erwin%20Wu%20and%20Yuhan%20Wu%20and%20Dingkun%20Yan%20and%20Tianyu%20Yan%20and%20Chang%20Zeng%20and%20Bo%20Zheng%20and%20You%20Zhou&entry.1292438233=We%20introduce%20Interactive%20Intelligence%2C%20a%20novel%20paradigm%20of%20digital%20human%20that%20is%20capable%20of%20personality-aligned%20expression%2C%20adaptive%20interaction%2C%20and%20self-evolution.%20To%20realize%20this%2C%20we%20present%20Mio%20%28Multimodal%20Interactive%20Omni-Avatar%29%2C%20an%20end-to-end%20framework%20composed%20of%20five%20specialized%20modules%3A%20Thinker%2C%20Talker%2C%20Face%20Animator%2C%20Body%20Animator%2C%20and%20Renderer.%20This%20unified%20architecture%20integrates%20cognitive%20reasoning%20with%20real-time%20multimodal%20embodiment%20to%20enable%20fluid%2C%20consistent%20interaction.%20Furthermore%2C%20we%20establish%20a%20new%20benchmark%20to%20rigorously%20evaluate%20the%20capabilities%20of%20interactive%20intelligence.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20methods%20across%20all%20evaluated%20dimensions.%20Together%2C%20these%20contributions%20move%20digital%20humans%20beyond%20superficial%20imitation%20toward%20intelligent%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2512.13674v1&entry.124074799=Read"},
{"title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "author": "Jiaqi Wang and Weijia Wu and Yi Zhan and Rui Zhao and Ming Hu and James Cheng and Wei Liu and Philip Torr and Kevin Qinghong Lin", "abstract": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "link": "http://arxiv.org/abs/2512.13281v1", "date": "2025-12-15", "relevancy": 2.8251, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5887}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5773}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Reality%20Test%3A%20Can%20AI-Generated%20ASMR%20Videos%20fool%20VLMs%20and%20Humans%3F&body=Title%3A%20Video%20Reality%20Test%3A%20Can%20AI-Generated%20ASMR%20Videos%20fool%20VLMs%20and%20Humans%3F%0AAuthor%3A%20Jiaqi%20Wang%20and%20Weijia%20Wu%20and%20Yi%20Zhan%20and%20Rui%20Zhao%20and%20Ming%20Hu%20and%20James%20Cheng%20and%20Wei%20Liu%20and%20Philip%20Torr%20and%20Kevin%20Qinghong%20Lin%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20have%20produced%20vivid%20content%20that%20are%20often%20indistinguishable%20from%20real%20videos%2C%20making%20AI-generated%20video%20detection%20an%20emerging%20societal%20challenge.%20Prior%20AIGC%20detection%20benchmarks%20mostly%20evaluate%20video%20without%20audio%2C%20target%20broad%20narrative%20domains%2C%20and%20focus%20on%20classification%20solely.%20Yet%20it%20remains%20unclear%20whether%20state-of-the-art%20video%20generation%20models%20can%20produce%20immersive%2C%20audio-paired%20videos%20that%20reliably%20deceive%20humans%20and%20VLMs.%20To%20this%20end%2C%20we%20introduce%20Video%20Reality%20Test%2C%20an%20ASMR-sourced%20video%20benchmark%20suite%20for%20testing%20perceptual%20realism%20under%20tight%20audio-visual%20coupling%2C%20featuring%20the%20following%20dimensions%3A%20%5Ctextbf%7B%28i%29%20Immersive%20ASMR%20video-audio%20sources.%7D%20Built%20on%20carefully%20curated%20real%20ASMR%20videos%2C%20the%20benchmark%20targets%20fine-grained%20action-object%20interactions%20with%20diversity%20across%20objects%2C%20actions%2C%20and%20backgrounds.%20%5Ctextbf%7B%28ii%29%20Peer-Review%20evaluation.%7D%20An%20adversarial%20creator-reviewer%20protocol%20where%20video%20generation%20models%20act%20as%20creators%20aiming%20to%20fool%20reviewers%2C%20while%20VLMs%20serve%20as%20reviewers%20seeking%20to%20identify%20fakeness.%20Our%20experimental%20findings%20show%3A%20The%20best%20creator%20Veo3.1-Fast%20even%20fools%20most%20VLMs%3A%20the%20strongest%20reviewer%20%28Gemini%202.5-Pro%29%20achieves%20only%2056%5C%25%20accuracy%20%28random%2050%5C%25%29%2C%20far%20below%20that%20of%20human%20experts%20%2881.25%5C%25%29.%20Adding%20audio%20improves%20real-fake%20discrimination%2C%20yet%20superficial%20cues%20such%20as%20watermarks%20can%20still%20significantly%20mislead%20models.%20These%20findings%20delineate%20the%20current%20boundary%20of%20video%20generation%20realism%20and%20expose%20limitations%20of%20VLMs%20in%20perceptual%20fidelity%20and%20audio-visual%20consistency.%20Our%20code%20is%20available%20at%20https%3A//github.com/video-reality-test/video-reality-test.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Reality%2520Test%253A%2520Can%2520AI-Generated%2520ASMR%2520Videos%2520fool%2520VLMs%2520and%2520Humans%253F%26entry.906535625%3DJiaqi%2520Wang%2520and%2520Weijia%2520Wu%2520and%2520Yi%2520Zhan%2520and%2520Rui%2520Zhao%2520and%2520Ming%2520Hu%2520and%2520James%2520Cheng%2520and%2520Wei%2520Liu%2520and%2520Philip%2520Torr%2520and%2520Kevin%2520Qinghong%2520Lin%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520have%2520produced%2520vivid%2520content%2520that%2520are%2520often%2520indistinguishable%2520from%2520real%2520videos%252C%2520making%2520AI-generated%2520video%2520detection%2520an%2520emerging%2520societal%2520challenge.%2520Prior%2520AIGC%2520detection%2520benchmarks%2520mostly%2520evaluate%2520video%2520without%2520audio%252C%2520target%2520broad%2520narrative%2520domains%252C%2520and%2520focus%2520on%2520classification%2520solely.%2520Yet%2520it%2520remains%2520unclear%2520whether%2520state-of-the-art%2520video%2520generation%2520models%2520can%2520produce%2520immersive%252C%2520audio-paired%2520videos%2520that%2520reliably%2520deceive%2520humans%2520and%2520VLMs.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Video%2520Reality%2520Test%252C%2520an%2520ASMR-sourced%2520video%2520benchmark%2520suite%2520for%2520testing%2520perceptual%2520realism%2520under%2520tight%2520audio-visual%2520coupling%252C%2520featuring%2520the%2520following%2520dimensions%253A%2520%255Ctextbf%257B%2528i%2529%2520Immersive%2520ASMR%2520video-audio%2520sources.%257D%2520Built%2520on%2520carefully%2520curated%2520real%2520ASMR%2520videos%252C%2520the%2520benchmark%2520targets%2520fine-grained%2520action-object%2520interactions%2520with%2520diversity%2520across%2520objects%252C%2520actions%252C%2520and%2520backgrounds.%2520%255Ctextbf%257B%2528ii%2529%2520Peer-Review%2520evaluation.%257D%2520An%2520adversarial%2520creator-reviewer%2520protocol%2520where%2520video%2520generation%2520models%2520act%2520as%2520creators%2520aiming%2520to%2520fool%2520reviewers%252C%2520while%2520VLMs%2520serve%2520as%2520reviewers%2520seeking%2520to%2520identify%2520fakeness.%2520Our%2520experimental%2520findings%2520show%253A%2520The%2520best%2520creator%2520Veo3.1-Fast%2520even%2520fools%2520most%2520VLMs%253A%2520the%2520strongest%2520reviewer%2520%2528Gemini%25202.5-Pro%2529%2520achieves%2520only%252056%255C%2525%2520accuracy%2520%2528random%252050%255C%2525%2529%252C%2520far%2520below%2520that%2520of%2520human%2520experts%2520%252881.25%255C%2525%2529.%2520Adding%2520audio%2520improves%2520real-fake%2520discrimination%252C%2520yet%2520superficial%2520cues%2520such%2520as%2520watermarks%2520can%2520still%2520significantly%2520mislead%2520models.%2520These%2520findings%2520delineate%2520the%2520current%2520boundary%2520of%2520video%2520generation%2520realism%2520and%2520expose%2520limitations%2520of%2520VLMs%2520in%2520perceptual%2520fidelity%2520and%2520audio-visual%2520consistency.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/video-reality-test/video-reality-test.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Reality%20Test%3A%20Can%20AI-Generated%20ASMR%20Videos%20fool%20VLMs%20and%20Humans%3F&entry.906535625=Jiaqi%20Wang%20and%20Weijia%20Wu%20and%20Yi%20Zhan%20and%20Rui%20Zhao%20and%20Ming%20Hu%20and%20James%20Cheng%20and%20Wei%20Liu%20and%20Philip%20Torr%20and%20Kevin%20Qinghong%20Lin&entry.1292438233=Recent%20advances%20in%20video%20generation%20have%20produced%20vivid%20content%20that%20are%20often%20indistinguishable%20from%20real%20videos%2C%20making%20AI-generated%20video%20detection%20an%20emerging%20societal%20challenge.%20Prior%20AIGC%20detection%20benchmarks%20mostly%20evaluate%20video%20without%20audio%2C%20target%20broad%20narrative%20domains%2C%20and%20focus%20on%20classification%20solely.%20Yet%20it%20remains%20unclear%20whether%20state-of-the-art%20video%20generation%20models%20can%20produce%20immersive%2C%20audio-paired%20videos%20that%20reliably%20deceive%20humans%20and%20VLMs.%20To%20this%20end%2C%20we%20introduce%20Video%20Reality%20Test%2C%20an%20ASMR-sourced%20video%20benchmark%20suite%20for%20testing%20perceptual%20realism%20under%20tight%20audio-visual%20coupling%2C%20featuring%20the%20following%20dimensions%3A%20%5Ctextbf%7B%28i%29%20Immersive%20ASMR%20video-audio%20sources.%7D%20Built%20on%20carefully%20curated%20real%20ASMR%20videos%2C%20the%20benchmark%20targets%20fine-grained%20action-object%20interactions%20with%20diversity%20across%20objects%2C%20actions%2C%20and%20backgrounds.%20%5Ctextbf%7B%28ii%29%20Peer-Review%20evaluation.%7D%20An%20adversarial%20creator-reviewer%20protocol%20where%20video%20generation%20models%20act%20as%20creators%20aiming%20to%20fool%20reviewers%2C%20while%20VLMs%20serve%20as%20reviewers%20seeking%20to%20identify%20fakeness.%20Our%20experimental%20findings%20show%3A%20The%20best%20creator%20Veo3.1-Fast%20even%20fools%20most%20VLMs%3A%20the%20strongest%20reviewer%20%28Gemini%202.5-Pro%29%20achieves%20only%2056%5C%25%20accuracy%20%28random%2050%5C%25%29%2C%20far%20below%20that%20of%20human%20experts%20%2881.25%5C%25%29.%20Adding%20audio%20improves%20real-fake%20discrimination%2C%20yet%20superficial%20cues%20such%20as%20watermarks%20can%20still%20significantly%20mislead%20models.%20These%20findings%20delineate%20the%20current%20boundary%20of%20video%20generation%20realism%20and%20expose%20limitations%20of%20VLMs%20in%20perceptual%20fidelity%20and%20audio-visual%20consistency.%20Our%20code%20is%20available%20at%20https%3A//github.com/video-reality-test/video-reality-test.&entry.1838667208=http%3A//arxiv.org/abs/2512.13281v1&entry.124074799=Read"},
{"title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments", "author": "Raymond Khazoum and Daniela Fernandes and Aleksandr Krylov and Qin Li and Stephane Deny", "abstract": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.", "link": "http://arxiv.org/abs/2512.13517v1", "date": "2025-12-15", "relevancy": 2.7789, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Model%20of%20Mental%20Rotation%20Informed%20by%20Interactive%20VR%20Experiments&body=Title%3A%20A%20Deep%20Learning%20Model%20of%20Mental%20Rotation%20Informed%20by%20Interactive%20VR%20Experiments%0AAuthor%3A%20Raymond%20Khazoum%20and%20Daniela%20Fernandes%20and%20Aleksandr%20Krylov%20and%20Qin%20Li%20and%20Stephane%20Deny%0AAbstract%3A%20Mental%20rotation%20--%20the%20ability%20to%20compare%20objects%20seen%20from%20different%20viewpoints%20--%20is%20a%20fundamental%20example%20of%20mental%20simulation%20and%20spatial%20world%20modelling%20in%20humans.%20Here%20we%20propose%20a%20mechanistic%20model%20of%20human%20mental%20rotation%2C%20leveraging%20advances%20in%20deep%2C%20equivariant%2C%20and%20neuro-symbolic%20learning.%20Our%20model%20consists%20of%20three%20stacked%20components%3A%20%281%29%20an%20equivariant%20neural%20encoder%2C%20taking%20images%20as%20input%20and%20producing%203D%20spatial%20representations%20of%20objects%2C%20%282%29%20a%20neuro-symbolic%20object%20encoder%2C%20deriving%20symbolic%20descriptions%20of%20objects%20from%20these%20spatial%20representations%2C%20and%20%283%29%20a%20neural%20decision%20agent%2C%20comparing%20these%20symbolic%20descriptions%20to%20prescribe%20rotation%20simulations%20in%203D%20latent%20space%20via%20a%20recurrent%20pathway.%20Our%20model%20design%20is%20guided%20by%20the%20abundant%20experimental%20literature%20on%20mental%20rotation%2C%20which%20we%20complemented%20with%20experiments%20in%20VR%20where%20participants%20could%20at%20times%20manipulate%20the%20objects%20to%20compare%2C%20providing%20us%20with%20additional%20insights%20into%20the%20cognitive%20process%20of%20mental%20rotation.%20Our%20model%20captures%20well%20the%20performance%2C%20response%20times%20and%20behavior%20of%20participants%20in%20our%20and%20others%27%20experiments.%20The%20necessity%20of%20each%20model%20component%20is%20shown%20through%20systematic%20ablations.%20Our%20work%20adds%20to%20a%20recent%20collection%20of%20deep%20neural%20models%20of%20human%20spatial%20reasoning%2C%20further%20demonstrating%20the%20potency%20of%20integrating%20deep%2C%20equivariant%2C%20and%20symbolic%20representations%20to%20model%20the%20human%20mind.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Model%2520of%2520Mental%2520Rotation%2520Informed%2520by%2520Interactive%2520VR%2520Experiments%26entry.906535625%3DRaymond%2520Khazoum%2520and%2520Daniela%2520Fernandes%2520and%2520Aleksandr%2520Krylov%2520and%2520Qin%2520Li%2520and%2520Stephane%2520Deny%26entry.1292438233%3DMental%2520rotation%2520--%2520the%2520ability%2520to%2520compare%2520objects%2520seen%2520from%2520different%2520viewpoints%2520--%2520is%2520a%2520fundamental%2520example%2520of%2520mental%2520simulation%2520and%2520spatial%2520world%2520modelling%2520in%2520humans.%2520Here%2520we%2520propose%2520a%2520mechanistic%2520model%2520of%2520human%2520mental%2520rotation%252C%2520leveraging%2520advances%2520in%2520deep%252C%2520equivariant%252C%2520and%2520neuro-symbolic%2520learning.%2520Our%2520model%2520consists%2520of%2520three%2520stacked%2520components%253A%2520%25281%2529%2520an%2520equivariant%2520neural%2520encoder%252C%2520taking%2520images%2520as%2520input%2520and%2520producing%25203D%2520spatial%2520representations%2520of%2520objects%252C%2520%25282%2529%2520a%2520neuro-symbolic%2520object%2520encoder%252C%2520deriving%2520symbolic%2520descriptions%2520of%2520objects%2520from%2520these%2520spatial%2520representations%252C%2520and%2520%25283%2529%2520a%2520neural%2520decision%2520agent%252C%2520comparing%2520these%2520symbolic%2520descriptions%2520to%2520prescribe%2520rotation%2520simulations%2520in%25203D%2520latent%2520space%2520via%2520a%2520recurrent%2520pathway.%2520Our%2520model%2520design%2520is%2520guided%2520by%2520the%2520abundant%2520experimental%2520literature%2520on%2520mental%2520rotation%252C%2520which%2520we%2520complemented%2520with%2520experiments%2520in%2520VR%2520where%2520participants%2520could%2520at%2520times%2520manipulate%2520the%2520objects%2520to%2520compare%252C%2520providing%2520us%2520with%2520additional%2520insights%2520into%2520the%2520cognitive%2520process%2520of%2520mental%2520rotation.%2520Our%2520model%2520captures%2520well%2520the%2520performance%252C%2520response%2520times%2520and%2520behavior%2520of%2520participants%2520in%2520our%2520and%2520others%2527%2520experiments.%2520The%2520necessity%2520of%2520each%2520model%2520component%2520is%2520shown%2520through%2520systematic%2520ablations.%2520Our%2520work%2520adds%2520to%2520a%2520recent%2520collection%2520of%2520deep%2520neural%2520models%2520of%2520human%2520spatial%2520reasoning%252C%2520further%2520demonstrating%2520the%2520potency%2520of%2520integrating%2520deep%252C%2520equivariant%252C%2520and%2520symbolic%2520representations%2520to%2520model%2520the%2520human%2520mind.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Model%20of%20Mental%20Rotation%20Informed%20by%20Interactive%20VR%20Experiments&entry.906535625=Raymond%20Khazoum%20and%20Daniela%20Fernandes%20and%20Aleksandr%20Krylov%20and%20Qin%20Li%20and%20Stephane%20Deny&entry.1292438233=Mental%20rotation%20--%20the%20ability%20to%20compare%20objects%20seen%20from%20different%20viewpoints%20--%20is%20a%20fundamental%20example%20of%20mental%20simulation%20and%20spatial%20world%20modelling%20in%20humans.%20Here%20we%20propose%20a%20mechanistic%20model%20of%20human%20mental%20rotation%2C%20leveraging%20advances%20in%20deep%2C%20equivariant%2C%20and%20neuro-symbolic%20learning.%20Our%20model%20consists%20of%20three%20stacked%20components%3A%20%281%29%20an%20equivariant%20neural%20encoder%2C%20taking%20images%20as%20input%20and%20producing%203D%20spatial%20representations%20of%20objects%2C%20%282%29%20a%20neuro-symbolic%20object%20encoder%2C%20deriving%20symbolic%20descriptions%20of%20objects%20from%20these%20spatial%20representations%2C%20and%20%283%29%20a%20neural%20decision%20agent%2C%20comparing%20these%20symbolic%20descriptions%20to%20prescribe%20rotation%20simulations%20in%203D%20latent%20space%20via%20a%20recurrent%20pathway.%20Our%20model%20design%20is%20guided%20by%20the%20abundant%20experimental%20literature%20on%20mental%20rotation%2C%20which%20we%20complemented%20with%20experiments%20in%20VR%20where%20participants%20could%20at%20times%20manipulate%20the%20objects%20to%20compare%2C%20providing%20us%20with%20additional%20insights%20into%20the%20cognitive%20process%20of%20mental%20rotation.%20Our%20model%20captures%20well%20the%20performance%2C%20response%20times%20and%20behavior%20of%20participants%20in%20our%20and%20others%27%20experiments.%20The%20necessity%20of%20each%20model%20component%20is%20shown%20through%20systematic%20ablations.%20Our%20work%20adds%20to%20a%20recent%20collection%20of%20deep%20neural%20models%20of%20human%20spatial%20reasoning%2C%20further%20demonstrating%20the%20potency%20of%20integrating%20deep%2C%20equivariant%2C%20and%20symbolic%20representations%20to%20model%20the%20human%20mind.&entry.1838667208=http%3A//arxiv.org/abs/2512.13517v1&entry.124074799=Read"},
{"title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving", "author": "Hyunki Seong and Jeong-Kyun Lee and Heesoo Myeong and Yongho Shin and Hyun-Mook Cho and Duck Hoon Kim and Pranav Desai and Monu Surana", "abstract": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.", "link": "http://arxiv.org/abs/2512.13262v1", "date": "2025-12-15", "relevancy": 2.7788, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5769}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5501}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-Training%20and%20Test-Time%20Scaling%20of%20Generative%20Agent%20Behavior%20Models%20for%20Interactive%20Autonomous%20Driving&body=Title%3A%20Post-Training%20and%20Test-Time%20Scaling%20of%20Generative%20Agent%20Behavior%20Models%20for%20Interactive%20Autonomous%20Driving%0AAuthor%3A%20Hyunki%20Seong%20and%20Jeong-Kyun%20Lee%20and%20Heesoo%20Myeong%20and%20Yongho%20Shin%20and%20Hyun-Mook%20Cho%20and%20Duck%20Hoon%20Kim%20and%20Pranav%20Desai%20and%20Monu%20Surana%0AAbstract%3A%20Learning%20interactive%20motion%20behaviors%20among%20multiple%20agents%20is%20a%20core%20challenge%20in%20autonomous%20driving.%20While%20imitation%20learning%20models%20generate%20realistic%20trajectories%2C%20they%20often%20inherit%20biases%20from%20datasets%20dominated%20by%20safe%20demonstrations%2C%20limiting%20robustness%20in%20safety-critical%20cases.%20Moreover%2C%20most%20studies%20rely%20on%20open-loop%20evaluation%2C%20overlooking%20compounding%20errors%20in%20closed-loop%20execution.%20We%20address%20these%20limitations%20with%20two%20complementary%20strategies.%20First%2C%20we%20propose%20Group%20Relative%20Behavior%20Optimization%20%28GRBO%29%2C%20a%20reinforcement%20learning%20post-training%20method%20that%20fine-tunes%20pretrained%20behavior%20models%20via%20group%20relative%20advantage%20maximization%20with%20human%20regularization.%20Using%20only%2010%25%20of%20the%20training%20dataset%2C%20GRBO%20improves%20safety%20performance%20by%20over%2040%25%20while%20preserving%20behavioral%20realism.%20Second%2C%20we%20introduce%20Warm-K%2C%20a%20warm-started%20Top-K%20sampling%20strategy%20that%20balances%20consistency%20and%20diversity%20in%20motion%20selection.%20Our%20Warm-K%20method-based%20test-time%20scaling%20enhances%20behavioral%20consistency%20and%20reactivity%20at%20test%20time%20without%20retraining%2C%20mitigating%20covariate%20shift%20and%20reducing%20performance%20discrepancies.%20Demo%20videos%20are%20available%20in%20the%20supplementary%20material.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-Training%2520and%2520Test-Time%2520Scaling%2520of%2520Generative%2520Agent%2520Behavior%2520Models%2520for%2520Interactive%2520Autonomous%2520Driving%26entry.906535625%3DHyunki%2520Seong%2520and%2520Jeong-Kyun%2520Lee%2520and%2520Heesoo%2520Myeong%2520and%2520Yongho%2520Shin%2520and%2520Hyun-Mook%2520Cho%2520and%2520Duck%2520Hoon%2520Kim%2520and%2520Pranav%2520Desai%2520and%2520Monu%2520Surana%26entry.1292438233%3DLearning%2520interactive%2520motion%2520behaviors%2520among%2520multiple%2520agents%2520is%2520a%2520core%2520challenge%2520in%2520autonomous%2520driving.%2520While%2520imitation%2520learning%2520models%2520generate%2520realistic%2520trajectories%252C%2520they%2520often%2520inherit%2520biases%2520from%2520datasets%2520dominated%2520by%2520safe%2520demonstrations%252C%2520limiting%2520robustness%2520in%2520safety-critical%2520cases.%2520Moreover%252C%2520most%2520studies%2520rely%2520on%2520open-loop%2520evaluation%252C%2520overlooking%2520compounding%2520errors%2520in%2520closed-loop%2520execution.%2520We%2520address%2520these%2520limitations%2520with%2520two%2520complementary%2520strategies.%2520First%252C%2520we%2520propose%2520Group%2520Relative%2520Behavior%2520Optimization%2520%2528GRBO%2529%252C%2520a%2520reinforcement%2520learning%2520post-training%2520method%2520that%2520fine-tunes%2520pretrained%2520behavior%2520models%2520via%2520group%2520relative%2520advantage%2520maximization%2520with%2520human%2520regularization.%2520Using%2520only%252010%2525%2520of%2520the%2520training%2520dataset%252C%2520GRBO%2520improves%2520safety%2520performance%2520by%2520over%252040%2525%2520while%2520preserving%2520behavioral%2520realism.%2520Second%252C%2520we%2520introduce%2520Warm-K%252C%2520a%2520warm-started%2520Top-K%2520sampling%2520strategy%2520that%2520balances%2520consistency%2520and%2520diversity%2520in%2520motion%2520selection.%2520Our%2520Warm-K%2520method-based%2520test-time%2520scaling%2520enhances%2520behavioral%2520consistency%2520and%2520reactivity%2520at%2520test%2520time%2520without%2520retraining%252C%2520mitigating%2520covariate%2520shift%2520and%2520reducing%2520performance%2520discrepancies.%2520Demo%2520videos%2520are%2520available%2520in%2520the%2520supplementary%2520material.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Training%20and%20Test-Time%20Scaling%20of%20Generative%20Agent%20Behavior%20Models%20for%20Interactive%20Autonomous%20Driving&entry.906535625=Hyunki%20Seong%20and%20Jeong-Kyun%20Lee%20and%20Heesoo%20Myeong%20and%20Yongho%20Shin%20and%20Hyun-Mook%20Cho%20and%20Duck%20Hoon%20Kim%20and%20Pranav%20Desai%20and%20Monu%20Surana&entry.1292438233=Learning%20interactive%20motion%20behaviors%20among%20multiple%20agents%20is%20a%20core%20challenge%20in%20autonomous%20driving.%20While%20imitation%20learning%20models%20generate%20realistic%20trajectories%2C%20they%20often%20inherit%20biases%20from%20datasets%20dominated%20by%20safe%20demonstrations%2C%20limiting%20robustness%20in%20safety-critical%20cases.%20Moreover%2C%20most%20studies%20rely%20on%20open-loop%20evaluation%2C%20overlooking%20compounding%20errors%20in%20closed-loop%20execution.%20We%20address%20these%20limitations%20with%20two%20complementary%20strategies.%20First%2C%20we%20propose%20Group%20Relative%20Behavior%20Optimization%20%28GRBO%29%2C%20a%20reinforcement%20learning%20post-training%20method%20that%20fine-tunes%20pretrained%20behavior%20models%20via%20group%20relative%20advantage%20maximization%20with%20human%20regularization.%20Using%20only%2010%25%20of%20the%20training%20dataset%2C%20GRBO%20improves%20safety%20performance%20by%20over%2040%25%20while%20preserving%20behavioral%20realism.%20Second%2C%20we%20introduce%20Warm-K%2C%20a%20warm-started%20Top-K%20sampling%20strategy%20that%20balances%20consistency%20and%20diversity%20in%20motion%20selection.%20Our%20Warm-K%20method-based%20test-time%20scaling%20enhances%20behavioral%20consistency%20and%20reactivity%20at%20test%20time%20without%20retraining%2C%20mitigating%20covariate%20shift%20and%20reducing%20performance%20discrepancies.%20Demo%20videos%20are%20available%20in%20the%20supplementary%20material.&entry.1838667208=http%3A//arxiv.org/abs/2512.13262v1&entry.124074799=Read"},
{"title": "3D Human-Human Interaction Anomaly Detection", "author": "Shun Maeda and Chunzhi Gu and Koichiro Kamide and Katsuya Hotta and Shangce Gao and Chao Zhang", "abstract": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.", "link": "http://arxiv.org/abs/2512.13560v1", "date": "2025-12-15", "relevancy": 2.7716, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human-Human%20Interaction%20Anomaly%20Detection&body=Title%3A%203D%20Human-Human%20Interaction%20Anomaly%20Detection%0AAuthor%3A%20Shun%20Maeda%20and%20Chunzhi%20Gu%20and%20Koichiro%20Kamide%20and%20Katsuya%20Hotta%20and%20Shangce%20Gao%20and%20Chao%20Zhang%0AAbstract%3A%20Human-centric%20anomaly%20detection%20%28AD%29%20has%20been%20primarily%20studied%20to%20specify%20anomalous%20behaviors%20in%20a%20single%20person.%20However%2C%20as%20humans%20by%20nature%20tend%20to%20act%20in%20a%20collaborative%20manner%2C%20behavioral%20anomalies%20can%20also%20arise%20from%20human-human%20interactions.%20Detecting%20such%20anomalies%20using%20existing%20single-person%20AD%20models%20is%20prone%20to%20low%20accuracy%2C%20as%20these%20approaches%20are%20typically%20not%20designed%20to%20capture%20the%20complex%20and%20asymmetric%20dynamics%20of%20interactions.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20task%2C%20Human-Human%20Interaction%20Anomaly%20Detection%20%28H2IAD%29%2C%20which%20aims%20to%20identify%20anomalous%20interactive%20behaviors%20within%20collaborative%203D%20human%20actions.%20To%20address%20H2IAD%2C%20we%20then%20propose%20Interaction%20Anomaly%20Detection%20Network%20%28IADNet%29%2C%20which%20is%20formalized%20with%20a%20Temporal%20Attention%20Sharing%20Module%20%28TASM%29.%20Specifically%2C%20in%20designing%20TASM%2C%20we%20share%20the%20encoded%20motion%20embeddings%20across%20both%20people%20such%20that%20collaborative%20motion%20correlations%20can%20be%20effectively%20synchronized.%20Moreover%2C%20we%20notice%20that%20in%20addition%20to%20temporal%20dynamics%2C%20human%20interactions%20are%20also%20characterized%20by%20spatial%20configurations%20between%20two%20people.%20We%20thus%20introduce%20a%20Distance-Based%20Relational%20Encoding%20Module%20%28DREM%29%20to%20better%20reflect%20social%20cues%20in%20H2IAD.%20The%20normalizing%20flow%20is%20eventually%20employed%20for%20anomaly%20scoring.%20Extensive%20experiments%20on%20human-human%20motion%20benchmarks%20demonstrate%20that%20IADNet%20outperforms%20existing%20Human-centric%20AD%20baselines%20in%20H2IAD.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human-Human%2520Interaction%2520Anomaly%2520Detection%26entry.906535625%3DShun%2520Maeda%2520and%2520Chunzhi%2520Gu%2520and%2520Koichiro%2520Kamide%2520and%2520Katsuya%2520Hotta%2520and%2520Shangce%2520Gao%2520and%2520Chao%2520Zhang%26entry.1292438233%3DHuman-centric%2520anomaly%2520detection%2520%2528AD%2529%2520has%2520been%2520primarily%2520studied%2520to%2520specify%2520anomalous%2520behaviors%2520in%2520a%2520single%2520person.%2520However%252C%2520as%2520humans%2520by%2520nature%2520tend%2520to%2520act%2520in%2520a%2520collaborative%2520manner%252C%2520behavioral%2520anomalies%2520can%2520also%2520arise%2520from%2520human-human%2520interactions.%2520Detecting%2520such%2520anomalies%2520using%2520existing%2520single-person%2520AD%2520models%2520is%2520prone%2520to%2520low%2520accuracy%252C%2520as%2520these%2520approaches%2520are%2520typically%2520not%2520designed%2520to%2520capture%2520the%2520complex%2520and%2520asymmetric%2520dynamics%2520of%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520task%252C%2520Human-Human%2520Interaction%2520Anomaly%2520Detection%2520%2528H2IAD%2529%252C%2520which%2520aims%2520to%2520identify%2520anomalous%2520interactive%2520behaviors%2520within%2520collaborative%25203D%2520human%2520actions.%2520To%2520address%2520H2IAD%252C%2520we%2520then%2520propose%2520Interaction%2520Anomaly%2520Detection%2520Network%2520%2528IADNet%2529%252C%2520which%2520is%2520formalized%2520with%2520a%2520Temporal%2520Attention%2520Sharing%2520Module%2520%2528TASM%2529.%2520Specifically%252C%2520in%2520designing%2520TASM%252C%2520we%2520share%2520the%2520encoded%2520motion%2520embeddings%2520across%2520both%2520people%2520such%2520that%2520collaborative%2520motion%2520correlations%2520can%2520be%2520effectively%2520synchronized.%2520Moreover%252C%2520we%2520notice%2520that%2520in%2520addition%2520to%2520temporal%2520dynamics%252C%2520human%2520interactions%2520are%2520also%2520characterized%2520by%2520spatial%2520configurations%2520between%2520two%2520people.%2520We%2520thus%2520introduce%2520a%2520Distance-Based%2520Relational%2520Encoding%2520Module%2520%2528DREM%2529%2520to%2520better%2520reflect%2520social%2520cues%2520in%2520H2IAD.%2520The%2520normalizing%2520flow%2520is%2520eventually%2520employed%2520for%2520anomaly%2520scoring.%2520Extensive%2520experiments%2520on%2520human-human%2520motion%2520benchmarks%2520demonstrate%2520that%2520IADNet%2520outperforms%2520existing%2520Human-centric%2520AD%2520baselines%2520in%2520H2IAD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human-Human%20Interaction%20Anomaly%20Detection&entry.906535625=Shun%20Maeda%20and%20Chunzhi%20Gu%20and%20Koichiro%20Kamide%20and%20Katsuya%20Hotta%20and%20Shangce%20Gao%20and%20Chao%20Zhang&entry.1292438233=Human-centric%20anomaly%20detection%20%28AD%29%20has%20been%20primarily%20studied%20to%20specify%20anomalous%20behaviors%20in%20a%20single%20person.%20However%2C%20as%20humans%20by%20nature%20tend%20to%20act%20in%20a%20collaborative%20manner%2C%20behavioral%20anomalies%20can%20also%20arise%20from%20human-human%20interactions.%20Detecting%20such%20anomalies%20using%20existing%20single-person%20AD%20models%20is%20prone%20to%20low%20accuracy%2C%20as%20these%20approaches%20are%20typically%20not%20designed%20to%20capture%20the%20complex%20and%20asymmetric%20dynamics%20of%20interactions.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20task%2C%20Human-Human%20Interaction%20Anomaly%20Detection%20%28H2IAD%29%2C%20which%20aims%20to%20identify%20anomalous%20interactive%20behaviors%20within%20collaborative%203D%20human%20actions.%20To%20address%20H2IAD%2C%20we%20then%20propose%20Interaction%20Anomaly%20Detection%20Network%20%28IADNet%29%2C%20which%20is%20formalized%20with%20a%20Temporal%20Attention%20Sharing%20Module%20%28TASM%29.%20Specifically%2C%20in%20designing%20TASM%2C%20we%20share%20the%20encoded%20motion%20embeddings%20across%20both%20people%20such%20that%20collaborative%20motion%20correlations%20can%20be%20effectively%20synchronized.%20Moreover%2C%20we%20notice%20that%20in%20addition%20to%20temporal%20dynamics%2C%20human%20interactions%20are%20also%20characterized%20by%20spatial%20configurations%20between%20two%20people.%20We%20thus%20introduce%20a%20Distance-Based%20Relational%20Encoding%20Module%20%28DREM%29%20to%20better%20reflect%20social%20cues%20in%20H2IAD.%20The%20normalizing%20flow%20is%20eventually%20employed%20for%20anomaly%20scoring.%20Extensive%20experiments%20on%20human-human%20motion%20benchmarks%20demonstrate%20that%20IADNet%20outperforms%20existing%20Human-centric%20AD%20baselines%20in%20H2IAD.&entry.1838667208=http%3A//arxiv.org/abs/2512.13560v1&entry.124074799=Read"},
{"title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery", "author": "Lorenzo Pettinari and Sidaty El Hadramy and Michael Wehrli and Philippe C. Cattin and Daniel Studer and Carol C. Hasler and Maria Licci", "abstract": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.", "link": "http://arxiv.org/abs/2512.13402v1", "date": "2025-12-15", "relevancy": 2.7191, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6114}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5124}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End2Reg%3A%20Learning%20Task-Specific%20Segmentation%20for%20Markerless%20Registration%20in%20Spine%20Surgery&body=Title%3A%20End2Reg%3A%20Learning%20Task-Specific%20Segmentation%20for%20Markerless%20Registration%20in%20Spine%20Surgery%0AAuthor%3A%20Lorenzo%20Pettinari%20and%20Sidaty%20El%20Hadramy%20and%20Michael%20Wehrli%20and%20Philippe%20C.%20Cattin%20and%20Daniel%20Studer%20and%20Carol%20C.%20Hasler%20and%20Maria%20Licci%0AAbstract%3A%20Purpose%3A%20Intraoperative%20navigation%20in%20spine%20surgery%20demands%20millimeter-level%20accuracy.%20Current%20systems%20based%20on%20intraoperative%20radiographic%20imaging%20and%20bone-anchored%20markers%20are%20invasive%2C%20radiation-intensive%20and%20workflow%20disruptive.%20Recent%20markerless%20RGB-D%20registration%20methods%20offer%20a%20promising%20alternative%2C%20but%20existing%20approaches%20rely%20on%20weak%20segmentation%20labels%20to%20isolate%20relevant%20anatomical%20structures%2C%20which%20can%20propagate%20errors%20throughout%20registration.%20Methods%3A%20We%20present%20End2Reg%20an%20end-to-end%20deep%20learning%20framework%20that%20jointly%20optimizes%20segmentation%20and%20registration%2C%20eliminating%20the%20need%20for%20weak%20segmentation%20labels%20and%20manual%20steps.%20The%20network%20learns%20segmentation%20masks%20specifically%20optimized%20for%20registration%2C%20guided%20solely%20by%20the%20registration%20objective%20without%20direct%20segmentation%20supervision.%20Results%3A%20The%20proposed%20framework%20achieves%20state-of-the-art%20performance%20on%20ex-%20and%20in-vivo%20benchmarks%2C%20reducing%20median%20Target%20Registration%20Error%20by%2032%25%20to%201.83mm%20and%20mean%20Root%20Mean%20Square%20Error%20by%2045%25%20to%203.95mm%2C%20respectively.%20An%20ablation%20study%20confirms%20that%20end-to-end%20optimization%20significantly%20improves%20registration%20accuracy.%20Conclusion%3A%20The%20presented%20end-to-end%20RGB-D%20registration%20pipeline%20removes%20dependency%20on%20weak%20labels%20and%20manual%20steps%2C%20advancing%20towards%20fully%20automatic%2C%20markerless%20intraoperative%20navigation.%20Code%20and%20interactive%20visualizations%20are%20available%20at%3A%20https%3A//lorenzopettinari.github.io/end-2-reg/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd2Reg%253A%2520Learning%2520Task-Specific%2520Segmentation%2520for%2520Markerless%2520Registration%2520in%2520Spine%2520Surgery%26entry.906535625%3DLorenzo%2520Pettinari%2520and%2520Sidaty%2520El%2520Hadramy%2520and%2520Michael%2520Wehrli%2520and%2520Philippe%2520C.%2520Cattin%2520and%2520Daniel%2520Studer%2520and%2520Carol%2520C.%2520Hasler%2520and%2520Maria%2520Licci%26entry.1292438233%3DPurpose%253A%2520Intraoperative%2520navigation%2520in%2520spine%2520surgery%2520demands%2520millimeter-level%2520accuracy.%2520Current%2520systems%2520based%2520on%2520intraoperative%2520radiographic%2520imaging%2520and%2520bone-anchored%2520markers%2520are%2520invasive%252C%2520radiation-intensive%2520and%2520workflow%2520disruptive.%2520Recent%2520markerless%2520RGB-D%2520registration%2520methods%2520offer%2520a%2520promising%2520alternative%252C%2520but%2520existing%2520approaches%2520rely%2520on%2520weak%2520segmentation%2520labels%2520to%2520isolate%2520relevant%2520anatomical%2520structures%252C%2520which%2520can%2520propagate%2520errors%2520throughout%2520registration.%2520Methods%253A%2520We%2520present%2520End2Reg%2520an%2520end-to-end%2520deep%2520learning%2520framework%2520that%2520jointly%2520optimizes%2520segmentation%2520and%2520registration%252C%2520eliminating%2520the%2520need%2520for%2520weak%2520segmentation%2520labels%2520and%2520manual%2520steps.%2520The%2520network%2520learns%2520segmentation%2520masks%2520specifically%2520optimized%2520for%2520registration%252C%2520guided%2520solely%2520by%2520the%2520registration%2520objective%2520without%2520direct%2520segmentation%2520supervision.%2520Results%253A%2520The%2520proposed%2520framework%2520achieves%2520state-of-the-art%2520performance%2520on%2520ex-%2520and%2520in-vivo%2520benchmarks%252C%2520reducing%2520median%2520Target%2520Registration%2520Error%2520by%252032%2525%2520to%25201.83mm%2520and%2520mean%2520Root%2520Mean%2520Square%2520Error%2520by%252045%2525%2520to%25203.95mm%252C%2520respectively.%2520An%2520ablation%2520study%2520confirms%2520that%2520end-to-end%2520optimization%2520significantly%2520improves%2520registration%2520accuracy.%2520Conclusion%253A%2520The%2520presented%2520end-to-end%2520RGB-D%2520registration%2520pipeline%2520removes%2520dependency%2520on%2520weak%2520labels%2520and%2520manual%2520steps%252C%2520advancing%2520towards%2520fully%2520automatic%252C%2520markerless%2520intraoperative%2520navigation.%2520Code%2520and%2520interactive%2520visualizations%2520are%2520available%2520at%253A%2520https%253A//lorenzopettinari.github.io/end-2-reg/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End2Reg%3A%20Learning%20Task-Specific%20Segmentation%20for%20Markerless%20Registration%20in%20Spine%20Surgery&entry.906535625=Lorenzo%20Pettinari%20and%20Sidaty%20El%20Hadramy%20and%20Michael%20Wehrli%20and%20Philippe%20C.%20Cattin%20and%20Daniel%20Studer%20and%20Carol%20C.%20Hasler%20and%20Maria%20Licci&entry.1292438233=Purpose%3A%20Intraoperative%20navigation%20in%20spine%20surgery%20demands%20millimeter-level%20accuracy.%20Current%20systems%20based%20on%20intraoperative%20radiographic%20imaging%20and%20bone-anchored%20markers%20are%20invasive%2C%20radiation-intensive%20and%20workflow%20disruptive.%20Recent%20markerless%20RGB-D%20registration%20methods%20offer%20a%20promising%20alternative%2C%20but%20existing%20approaches%20rely%20on%20weak%20segmentation%20labels%20to%20isolate%20relevant%20anatomical%20structures%2C%20which%20can%20propagate%20errors%20throughout%20registration.%20Methods%3A%20We%20present%20End2Reg%20an%20end-to-end%20deep%20learning%20framework%20that%20jointly%20optimizes%20segmentation%20and%20registration%2C%20eliminating%20the%20need%20for%20weak%20segmentation%20labels%20and%20manual%20steps.%20The%20network%20learns%20segmentation%20masks%20specifically%20optimized%20for%20registration%2C%20guided%20solely%20by%20the%20registration%20objective%20without%20direct%20segmentation%20supervision.%20Results%3A%20The%20proposed%20framework%20achieves%20state-of-the-art%20performance%20on%20ex-%20and%20in-vivo%20benchmarks%2C%20reducing%20median%20Target%20Registration%20Error%20by%2032%25%20to%201.83mm%20and%20mean%20Root%20Mean%20Square%20Error%20by%2045%25%20to%203.95mm%2C%20respectively.%20An%20ablation%20study%20confirms%20that%20end-to-end%20optimization%20significantly%20improves%20registration%20accuracy.%20Conclusion%3A%20The%20presented%20end-to-end%20RGB-D%20registration%20pipeline%20removes%20dependency%20on%20weak%20labels%20and%20manual%20steps%2C%20advancing%20towards%20fully%20automatic%2C%20markerless%20intraoperative%20navigation.%20Code%20and%20interactive%20visualizations%20are%20available%20at%3A%20https%3A//lorenzopettinari.github.io/end-2-reg/.&entry.1838667208=http%3A//arxiv.org/abs/2512.13402v1&entry.124074799=Read"},
{"title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "author": "Francesca Da Ros and Luca Di Gaspero and Kevin Roitero", "abstract": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.", "link": "http://arxiv.org/abs/2512.13374v1", "date": "2025-12-15", "relevancy": 2.7049, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behavior%20and%20Representation%20in%20Large%20Language%20Models%20for%20Combinatorial%20Optimization%3A%20From%20Feature%20Extraction%20to%20Algorithm%20Selection&body=Title%3A%20Behavior%20and%20Representation%20in%20Large%20Language%20Models%20for%20Combinatorial%20Optimization%3A%20From%20Feature%20Extraction%20to%20Algorithm%20Selection%0AAuthor%3A%20Francesca%20Da%20Ros%20and%20Luca%20Di%20Gaspero%20and%20Kevin%20Roitero%0AAbstract%3A%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20opened%20new%20perspectives%20for%20automation%20in%20optimization.%20While%20several%20studies%20have%20explored%20how%20LLMs%20can%20generate%20or%20solve%20optimization%20models%2C%20far%20less%20is%20understood%20about%20what%20these%20models%20actually%20learn%20regarding%20problem%20structure%20or%20algorithmic%20behavior.%20This%20study%20investigates%20how%20LLMs%20internally%20represent%20combinatorial%20optimization%20problems%20and%20whether%20such%20representations%20can%20support%20downstream%20decision%20tasks.%20We%20adopt%20a%20twofold%20methodology%20combining%20direct%20querying%2C%20which%20assesses%20LLM%20capacity%20to%20explicitly%20extract%20instance%20features%2C%20with%20probing%20analyses%20that%20examine%20whether%20such%20information%20is%20implicitly%20encoded%20within%20their%20hidden%20layers.%20The%20probing%20framework%20is%20further%20extended%20to%20a%20per-instance%20algorithm%20selection%20task%2C%20evaluating%20whether%20LLM-derived%20representations%20can%20predict%20the%20best-performing%20solver.%20Experiments%20span%20four%20benchmark%20problems%20and%20three%20instance%20representations.%20Results%20show%20that%20LLMs%20exhibit%20moderate%20ability%20to%20recover%20feature%20information%20from%20problem%20instances%2C%20either%20through%20direct%20querying%20or%20probing.%20Notably%2C%20the%20predictive%20power%20of%20LLM%20hidden-layer%20representations%20proves%20comparable%20to%20that%20achieved%20through%20traditional%20feature%20extraction%2C%20suggesting%20that%20LLMs%20capture%20meaningful%20structural%20information%20relevant%20to%20optimization%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehavior%2520and%2520Representation%2520in%2520Large%2520Language%2520Models%2520for%2520Combinatorial%2520Optimization%253A%2520From%2520Feature%2520Extraction%2520to%2520Algorithm%2520Selection%26entry.906535625%3DFrancesca%2520Da%2520Ros%2520and%2520Luca%2520Di%2520Gaspero%2520and%2520Kevin%2520Roitero%26entry.1292438233%3DRecent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520opened%2520new%2520perspectives%2520for%2520automation%2520in%2520optimization.%2520While%2520several%2520studies%2520have%2520explored%2520how%2520LLMs%2520can%2520generate%2520or%2520solve%2520optimization%2520models%252C%2520far%2520less%2520is%2520understood%2520about%2520what%2520these%2520models%2520actually%2520learn%2520regarding%2520problem%2520structure%2520or%2520algorithmic%2520behavior.%2520This%2520study%2520investigates%2520how%2520LLMs%2520internally%2520represent%2520combinatorial%2520optimization%2520problems%2520and%2520whether%2520such%2520representations%2520can%2520support%2520downstream%2520decision%2520tasks.%2520We%2520adopt%2520a%2520twofold%2520methodology%2520combining%2520direct%2520querying%252C%2520which%2520assesses%2520LLM%2520capacity%2520to%2520explicitly%2520extract%2520instance%2520features%252C%2520with%2520probing%2520analyses%2520that%2520examine%2520whether%2520such%2520information%2520is%2520implicitly%2520encoded%2520within%2520their%2520hidden%2520layers.%2520The%2520probing%2520framework%2520is%2520further%2520extended%2520to%2520a%2520per-instance%2520algorithm%2520selection%2520task%252C%2520evaluating%2520whether%2520LLM-derived%2520representations%2520can%2520predict%2520the%2520best-performing%2520solver.%2520Experiments%2520span%2520four%2520benchmark%2520problems%2520and%2520three%2520instance%2520representations.%2520Results%2520show%2520that%2520LLMs%2520exhibit%2520moderate%2520ability%2520to%2520recover%2520feature%2520information%2520from%2520problem%2520instances%252C%2520either%2520through%2520direct%2520querying%2520or%2520probing.%2520Notably%252C%2520the%2520predictive%2520power%2520of%2520LLM%2520hidden-layer%2520representations%2520proves%2520comparable%2520to%2520that%2520achieved%2520through%2520traditional%2520feature%2520extraction%252C%2520suggesting%2520that%2520LLMs%2520capture%2520meaningful%2520structural%2520information%2520relevant%2520to%2520optimization%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavior%20and%20Representation%20in%20Large%20Language%20Models%20for%20Combinatorial%20Optimization%3A%20From%20Feature%20Extraction%20to%20Algorithm%20Selection&entry.906535625=Francesca%20Da%20Ros%20and%20Luca%20Di%20Gaspero%20and%20Kevin%20Roitero&entry.1292438233=Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20opened%20new%20perspectives%20for%20automation%20in%20optimization.%20While%20several%20studies%20have%20explored%20how%20LLMs%20can%20generate%20or%20solve%20optimization%20models%2C%20far%20less%20is%20understood%20about%20what%20these%20models%20actually%20learn%20regarding%20problem%20structure%20or%20algorithmic%20behavior.%20This%20study%20investigates%20how%20LLMs%20internally%20represent%20combinatorial%20optimization%20problems%20and%20whether%20such%20representations%20can%20support%20downstream%20decision%20tasks.%20We%20adopt%20a%20twofold%20methodology%20combining%20direct%20querying%2C%20which%20assesses%20LLM%20capacity%20to%20explicitly%20extract%20instance%20features%2C%20with%20probing%20analyses%20that%20examine%20whether%20such%20information%20is%20implicitly%20encoded%20within%20their%20hidden%20layers.%20The%20probing%20framework%20is%20further%20extended%20to%20a%20per-instance%20algorithm%20selection%20task%2C%20evaluating%20whether%20LLM-derived%20representations%20can%20predict%20the%20best-performing%20solver.%20Experiments%20span%20four%20benchmark%20problems%20and%20three%20instance%20representations.%20Results%20show%20that%20LLMs%20exhibit%20moderate%20ability%20to%20recover%20feature%20information%20from%20problem%20instances%2C%20either%20through%20direct%20querying%20or%20probing.%20Notably%2C%20the%20predictive%20power%20of%20LLM%20hidden-layer%20representations%20proves%20comparable%20to%20that%20achieved%20through%20traditional%20feature%20extraction%2C%20suggesting%20that%20LLMs%20capture%20meaningful%20structural%20information%20relevant%20to%20optimization%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.13374v1&entry.124074799=Read"},
{"title": "Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction", "author": "Chenyu Zhang and Yihao Luo and Lei Zhu and Martyn G Boutelle and Choon Hwai Yap and Guang Yang", "abstract": "Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.", "link": "http://arxiv.org/abs/2503.07874v2", "date": "2025-12-15", "relevancy": 2.6816, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relational%20Anatomical%20Supervision%20for%20Accurate%203D%20Multi-Chamber%20Cardiac%20Mesh%20Reconstruction&body=Title%3A%20Relational%20Anatomical%20Supervision%20for%20Accurate%203D%20Multi-Chamber%20Cardiac%20Mesh%20Reconstruction%0AAuthor%3A%20Chenyu%20Zhang%20and%20Yihao%20Luo%20and%20Lei%20Zhu%20and%20Martyn%20G%20Boutelle%20and%20Choon%20Hwai%20Yap%20and%20Guang%20Yang%0AAbstract%3A%20Accurate%20reconstruction%20of%20multi-chamber%20cardiac%20anatomy%20from%20medical%20images%20is%20a%20cornerstone%20for%20patient-specific%20modeling%2C%20physiological%20simulation%2C%20and%20interventional%20planning.%20However%2C%20current%20reconstruction%20pipelines%20fundamentally%20rely%20on%20surface-wise%20geometric%20supervision%20and%20model%20each%20chamber%20in%20isolation%2C%20resulting%20in%20anatomically%20implausible%20inter-chamber%20violations%20despite%20apparently%20favorable%20overlap%20or%20distance%20metrics.%20In%20this%20work%2C%20we%20propose%20a%20relational%20anatomical%20supervision%20framework%20for%20multi-chamber%20cardiac%20mesh%20reconstruction%20by%20introducing%20a%20Mesh%20Interrelation%20Enhancement%20%28MIE%29%20loss.%20The%20proposed%20formulation%20explicitly%20encodes%20spatial%20relationships%20between%20cardiac%20structures%20into%20a%20differentiable%20occupancy-based%20objective%2C%20thereby%20transforming%20qualitative%20anatomical%20rules%20into%20quantitative%20geometric%20supervision.%20We%20further%20establish%20violation-aware%20evaluation%20metrics%20to%20directly%20quantify%20inter-chamber%20structural%20correctness%2C%20revealing%20systematic%20limitations%20of%20commonly%20used%20geometric%20measures%20such%20as%20Dice%20and%20Chamfer%20distance.%20Extensive%20experiments%20on%20multi-center%20CT%20data%2C%20densely%20sampled%20MR%20data%2C%20and%20two%20independent%20external%20cohorts%2C%20including%20a%20highly%20heterogeneous%20congenital%20heart%20disease%20population%2C%20demonstrate%20that%20the%20proposed%20method%20consistently%20suppresses%20clinically%20critical%20boundary%20violations%20by%20up%20to%2083%5C%25%2C%20while%20maintaining%20competitive%20volumetric%20accuracy%20and%20achieving%20superior%20surface%20fidelity.%20Notably%2C%20the%20proposed%20relational%20supervision%20generalizes%20robustly%20across%20imaging%20modalities%2C%20centers%2C%20and%20pathological%20conditions%2C%20even%20under%20severe%20anatomical%20deformation.%20These%20results%20demonstrate%20that%20distance-based%20supervision%20alone%20is%20insufficient%20to%20guarantee%20anatomically%20faithful%20reconstruction%2C%20and%20that%20explicit%20enforcement%20of%20multi-structure%20anatomical%20relations%20provides%20a%20principled%20and%20robust%20pathway%20toward%20reliable%20patient-specific%20cardiac%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelational%2520Anatomical%2520Supervision%2520for%2520Accurate%25203D%2520Multi-Chamber%2520Cardiac%2520Mesh%2520Reconstruction%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Yihao%2520Luo%2520and%2520Lei%2520Zhu%2520and%2520Martyn%2520G%2520Boutelle%2520and%2520Choon%2520Hwai%2520Yap%2520and%2520Guang%2520Yang%26entry.1292438233%3DAccurate%2520reconstruction%2520of%2520multi-chamber%2520cardiac%2520anatomy%2520from%2520medical%2520images%2520is%2520a%2520cornerstone%2520for%2520patient-specific%2520modeling%252C%2520physiological%2520simulation%252C%2520and%2520interventional%2520planning.%2520However%252C%2520current%2520reconstruction%2520pipelines%2520fundamentally%2520rely%2520on%2520surface-wise%2520geometric%2520supervision%2520and%2520model%2520each%2520chamber%2520in%2520isolation%252C%2520resulting%2520in%2520anatomically%2520implausible%2520inter-chamber%2520violations%2520despite%2520apparently%2520favorable%2520overlap%2520or%2520distance%2520metrics.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520relational%2520anatomical%2520supervision%2520framework%2520for%2520multi-chamber%2520cardiac%2520mesh%2520reconstruction%2520by%2520introducing%2520a%2520Mesh%2520Interrelation%2520Enhancement%2520%2528MIE%2529%2520loss.%2520The%2520proposed%2520formulation%2520explicitly%2520encodes%2520spatial%2520relationships%2520between%2520cardiac%2520structures%2520into%2520a%2520differentiable%2520occupancy-based%2520objective%252C%2520thereby%2520transforming%2520qualitative%2520anatomical%2520rules%2520into%2520quantitative%2520geometric%2520supervision.%2520We%2520further%2520establish%2520violation-aware%2520evaluation%2520metrics%2520to%2520directly%2520quantify%2520inter-chamber%2520structural%2520correctness%252C%2520revealing%2520systematic%2520limitations%2520of%2520commonly%2520used%2520geometric%2520measures%2520such%2520as%2520Dice%2520and%2520Chamfer%2520distance.%2520Extensive%2520experiments%2520on%2520multi-center%2520CT%2520data%252C%2520densely%2520sampled%2520MR%2520data%252C%2520and%2520two%2520independent%2520external%2520cohorts%252C%2520including%2520a%2520highly%2520heterogeneous%2520congenital%2520heart%2520disease%2520population%252C%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520suppresses%2520clinically%2520critical%2520boundary%2520violations%2520by%2520up%2520to%252083%255C%2525%252C%2520while%2520maintaining%2520competitive%2520volumetric%2520accuracy%2520and%2520achieving%2520superior%2520surface%2520fidelity.%2520Notably%252C%2520the%2520proposed%2520relational%2520supervision%2520generalizes%2520robustly%2520across%2520imaging%2520modalities%252C%2520centers%252C%2520and%2520pathological%2520conditions%252C%2520even%2520under%2520severe%2520anatomical%2520deformation.%2520These%2520results%2520demonstrate%2520that%2520distance-based%2520supervision%2520alone%2520is%2520insufficient%2520to%2520guarantee%2520anatomically%2520faithful%2520reconstruction%252C%2520and%2520that%2520explicit%2520enforcement%2520of%2520multi-structure%2520anatomical%2520relations%2520provides%2520a%2520principled%2520and%2520robust%2520pathway%2520toward%2520reliable%2520patient-specific%2520cardiac%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relational%20Anatomical%20Supervision%20for%20Accurate%203D%20Multi-Chamber%20Cardiac%20Mesh%20Reconstruction&entry.906535625=Chenyu%20Zhang%20and%20Yihao%20Luo%20and%20Lei%20Zhu%20and%20Martyn%20G%20Boutelle%20and%20Choon%20Hwai%20Yap%20and%20Guang%20Yang&entry.1292438233=Accurate%20reconstruction%20of%20multi-chamber%20cardiac%20anatomy%20from%20medical%20images%20is%20a%20cornerstone%20for%20patient-specific%20modeling%2C%20physiological%20simulation%2C%20and%20interventional%20planning.%20However%2C%20current%20reconstruction%20pipelines%20fundamentally%20rely%20on%20surface-wise%20geometric%20supervision%20and%20model%20each%20chamber%20in%20isolation%2C%20resulting%20in%20anatomically%20implausible%20inter-chamber%20violations%20despite%20apparently%20favorable%20overlap%20or%20distance%20metrics.%20In%20this%20work%2C%20we%20propose%20a%20relational%20anatomical%20supervision%20framework%20for%20multi-chamber%20cardiac%20mesh%20reconstruction%20by%20introducing%20a%20Mesh%20Interrelation%20Enhancement%20%28MIE%29%20loss.%20The%20proposed%20formulation%20explicitly%20encodes%20spatial%20relationships%20between%20cardiac%20structures%20into%20a%20differentiable%20occupancy-based%20objective%2C%20thereby%20transforming%20qualitative%20anatomical%20rules%20into%20quantitative%20geometric%20supervision.%20We%20further%20establish%20violation-aware%20evaluation%20metrics%20to%20directly%20quantify%20inter-chamber%20structural%20correctness%2C%20revealing%20systematic%20limitations%20of%20commonly%20used%20geometric%20measures%20such%20as%20Dice%20and%20Chamfer%20distance.%20Extensive%20experiments%20on%20multi-center%20CT%20data%2C%20densely%20sampled%20MR%20data%2C%20and%20two%20independent%20external%20cohorts%2C%20including%20a%20highly%20heterogeneous%20congenital%20heart%20disease%20population%2C%20demonstrate%20that%20the%20proposed%20method%20consistently%20suppresses%20clinically%20critical%20boundary%20violations%20by%20up%20to%2083%5C%25%2C%20while%20maintaining%20competitive%20volumetric%20accuracy%20and%20achieving%20superior%20surface%20fidelity.%20Notably%2C%20the%20proposed%20relational%20supervision%20generalizes%20robustly%20across%20imaging%20modalities%2C%20centers%2C%20and%20pathological%20conditions%2C%20even%20under%20severe%20anatomical%20deformation.%20These%20results%20demonstrate%20that%20distance-based%20supervision%20alone%20is%20insufficient%20to%20guarantee%20anatomically%20faithful%20reconstruction%2C%20and%20that%20explicit%20enforcement%20of%20multi-structure%20anatomical%20relations%20provides%20a%20principled%20and%20robust%20pathway%20toward%20reliable%20patient-specific%20cardiac%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2503.07874v2&entry.124074799=Read"},
{"title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis", "author": "Felix J. Dorfner and Manon A. Dorster and Ryan Connolly and Oscar Gentilhomme and Edward Gibbs and Steven Graham and Seth Wander and Thomas Schultz and Manisha Bahl and Dania Daye and Albert E. Kim and Christopher P. Bridge", "abstract": "Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.\n  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.\n  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.\n  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%.\n  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.", "link": "http://arxiv.org/abs/2512.13608v1", "date": "2025-12-15", "relevancy": 2.6777, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBT-DINO%3A%20Towards%20Foundation%20model%20based%20analysis%20of%20Digital%20Breast%20Tomosynthesis&body=Title%3A%20DBT-DINO%3A%20Towards%20Foundation%20model%20based%20analysis%20of%20Digital%20Breast%20Tomosynthesis%0AAuthor%3A%20Felix%20J.%20Dorfner%20and%20Manon%20A.%20Dorster%20and%20Ryan%20Connolly%20and%20Oscar%20Gentilhomme%20and%20Edward%20Gibbs%20and%20Steven%20Graham%20and%20Seth%20Wander%20and%20Thomas%20Schultz%20and%20Manisha%20Bahl%20and%20Dania%20Daye%20and%20Albert%20E.%20Kim%20and%20Christopher%20P.%20Bridge%0AAbstract%3A%20Foundation%20models%20have%20shown%20promise%20in%20medical%20imaging%20but%20remain%20underexplored%20for%20three-dimensional%20imaging%20modalities.%20No%20foundation%20model%20currently%20exists%20for%20Digital%20Breast%20Tomosynthesis%20%28DBT%29%2C%20despite%20its%20use%20for%20breast%20cancer%20screening.%0A%20%20To%20develop%20and%20evaluate%20a%20foundation%20model%20for%20DBT%20%28DBT-DINO%29%20across%20multiple%20clinical%20tasks%20and%20assess%20the%20impact%20of%20domain-specific%20pre-training.%0A%20%20Self-supervised%20pre-training%20was%20performed%20using%20the%20DINOv2%20methodology%20on%20over%2025%20million%202D%20slices%20from%20487%2C975%20DBT%20volumes%20from%2027%2C990%20patients.%20Three%20downstream%20tasks%20were%20evaluated%3A%20%281%29%20breast%20density%20classification%20using%205%2C000%20screening%20exams%3B%20%282%29%205-year%20risk%20of%20developing%20breast%20cancer%20using%20106%2C417%20screening%20exams%3B%20and%20%283%29%20lesion%20detection%20using%20393%20annotated%20volumes.%0A%20%20For%20breast%20density%20classification%2C%20DBT-DINO%20achieved%20an%20accuracy%20of%200.79%20%2895%5C%25%20CI%3A%200.76--0.81%29%2C%20outperforming%20both%20the%20MetaAI%20DINOv2%20baseline%20%280.73%2C%2095%5C%25%20CI%3A%200.70--0.76%2C%20p%3C.001%29%20and%20DenseNet-121%20%280.74%2C%2095%5C%25%20CI%3A%200.71--0.76%2C%20p%3C.001%29.%20For%205-year%20breast%20cancer%20risk%20prediction%2C%20DBT-DINO%20achieved%20an%20AUROC%20of%200.78%20%2895%5C%25%20CI%3A%200.76--0.80%29%20compared%20to%20DINOv2%27s%200.76%20%2895%5C%25%20CI%3A%200.74--0.78%2C%20p%3D.57%29.%20For%20lesion%20detection%2C%20DINOv2%20achieved%20a%20higher%20average%20sensitivity%20of%200.67%20%2895%5C%25%20CI%3A%200.60--0.74%29%20compared%20to%20DBT-DINO%20with%200.62%20%2895%5C%25%20CI%3A%200.53--0.71%2C%20p%3D.60%29.%20DBT-DINO%20demonstrated%20better%20performance%20on%20cancerous%20lesions%20specifically%20with%20a%20detection%20rate%20of%2078.8%5C%25%20compared%20to%20Dinov2%27s%2077.3%5C%25.%0A%20%20Using%20a%20dataset%20of%20unprecedented%20size%2C%20we%20developed%20DBT-DINO%2C%20the%20first%20foundation%20model%20for%20DBT.%20DBT-DINO%20demonstrated%20strong%20performance%20on%20breast%20density%20classification%20and%20cancer%20risk%20prediction.%20However%2C%20domain-specific%20pre-training%20showed%20variable%20benefits%20on%20the%20detection%20task%2C%20with%20ImageNet%20baseline%20outperforming%20DBT-DINO%20on%20general%20lesion%20detection%2C%20indicating%20that%20localized%20detection%20tasks%20require%20further%20methodological%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBT-DINO%253A%2520Towards%2520Foundation%2520model%2520based%2520analysis%2520of%2520Digital%2520Breast%2520Tomosynthesis%26entry.906535625%3DFelix%2520J.%2520Dorfner%2520and%2520Manon%2520A.%2520Dorster%2520and%2520Ryan%2520Connolly%2520and%2520Oscar%2520Gentilhomme%2520and%2520Edward%2520Gibbs%2520and%2520Steven%2520Graham%2520and%2520Seth%2520Wander%2520and%2520Thomas%2520Schultz%2520and%2520Manisha%2520Bahl%2520and%2520Dania%2520Daye%2520and%2520Albert%2520E.%2520Kim%2520and%2520Christopher%2520P.%2520Bridge%26entry.1292438233%3DFoundation%2520models%2520have%2520shown%2520promise%2520in%2520medical%2520imaging%2520but%2520remain%2520underexplored%2520for%2520three-dimensional%2520imaging%2520modalities.%2520No%2520foundation%2520model%2520currently%2520exists%2520for%2520Digital%2520Breast%2520Tomosynthesis%2520%2528DBT%2529%252C%2520despite%2520its%2520use%2520for%2520breast%2520cancer%2520screening.%250A%2520%2520To%2520develop%2520and%2520evaluate%2520a%2520foundation%2520model%2520for%2520DBT%2520%2528DBT-DINO%2529%2520across%2520multiple%2520clinical%2520tasks%2520and%2520assess%2520the%2520impact%2520of%2520domain-specific%2520pre-training.%250A%2520%2520Self-supervised%2520pre-training%2520was%2520performed%2520using%2520the%2520DINOv2%2520methodology%2520on%2520over%252025%2520million%25202D%2520slices%2520from%2520487%252C975%2520DBT%2520volumes%2520from%252027%252C990%2520patients.%2520Three%2520downstream%2520tasks%2520were%2520evaluated%253A%2520%25281%2529%2520breast%2520density%2520classification%2520using%25205%252C000%2520screening%2520exams%253B%2520%25282%2529%25205-year%2520risk%2520of%2520developing%2520breast%2520cancer%2520using%2520106%252C417%2520screening%2520exams%253B%2520and%2520%25283%2529%2520lesion%2520detection%2520using%2520393%2520annotated%2520volumes.%250A%2520%2520For%2520breast%2520density%2520classification%252C%2520DBT-DINO%2520achieved%2520an%2520accuracy%2520of%25200.79%2520%252895%255C%2525%2520CI%253A%25200.76--0.81%2529%252C%2520outperforming%2520both%2520the%2520MetaAI%2520DINOv2%2520baseline%2520%25280.73%252C%252095%255C%2525%2520CI%253A%25200.70--0.76%252C%2520p%253C.001%2529%2520and%2520DenseNet-121%2520%25280.74%252C%252095%255C%2525%2520CI%253A%25200.71--0.76%252C%2520p%253C.001%2529.%2520For%25205-year%2520breast%2520cancer%2520risk%2520prediction%252C%2520DBT-DINO%2520achieved%2520an%2520AUROC%2520of%25200.78%2520%252895%255C%2525%2520CI%253A%25200.76--0.80%2529%2520compared%2520to%2520DINOv2%2527s%25200.76%2520%252895%255C%2525%2520CI%253A%25200.74--0.78%252C%2520p%253D.57%2529.%2520For%2520lesion%2520detection%252C%2520DINOv2%2520achieved%2520a%2520higher%2520average%2520sensitivity%2520of%25200.67%2520%252895%255C%2525%2520CI%253A%25200.60--0.74%2529%2520compared%2520to%2520DBT-DINO%2520with%25200.62%2520%252895%255C%2525%2520CI%253A%25200.53--0.71%252C%2520p%253D.60%2529.%2520DBT-DINO%2520demonstrated%2520better%2520performance%2520on%2520cancerous%2520lesions%2520specifically%2520with%2520a%2520detection%2520rate%2520of%252078.8%255C%2525%2520compared%2520to%2520Dinov2%2527s%252077.3%255C%2525.%250A%2520%2520Using%2520a%2520dataset%2520of%2520unprecedented%2520size%252C%2520we%2520developed%2520DBT-DINO%252C%2520the%2520first%2520foundation%2520model%2520for%2520DBT.%2520DBT-DINO%2520demonstrated%2520strong%2520performance%2520on%2520breast%2520density%2520classification%2520and%2520cancer%2520risk%2520prediction.%2520However%252C%2520domain-specific%2520pre-training%2520showed%2520variable%2520benefits%2520on%2520the%2520detection%2520task%252C%2520with%2520ImageNet%2520baseline%2520outperforming%2520DBT-DINO%2520on%2520general%2520lesion%2520detection%252C%2520indicating%2520that%2520localized%2520detection%2520tasks%2520require%2520further%2520methodological%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBT-DINO%3A%20Towards%20Foundation%20model%20based%20analysis%20of%20Digital%20Breast%20Tomosynthesis&entry.906535625=Felix%20J.%20Dorfner%20and%20Manon%20A.%20Dorster%20and%20Ryan%20Connolly%20and%20Oscar%20Gentilhomme%20and%20Edward%20Gibbs%20and%20Steven%20Graham%20and%20Seth%20Wander%20and%20Thomas%20Schultz%20and%20Manisha%20Bahl%20and%20Dania%20Daye%20and%20Albert%20E.%20Kim%20and%20Christopher%20P.%20Bridge&entry.1292438233=Foundation%20models%20have%20shown%20promise%20in%20medical%20imaging%20but%20remain%20underexplored%20for%20three-dimensional%20imaging%20modalities.%20No%20foundation%20model%20currently%20exists%20for%20Digital%20Breast%20Tomosynthesis%20%28DBT%29%2C%20despite%20its%20use%20for%20breast%20cancer%20screening.%0A%20%20To%20develop%20and%20evaluate%20a%20foundation%20model%20for%20DBT%20%28DBT-DINO%29%20across%20multiple%20clinical%20tasks%20and%20assess%20the%20impact%20of%20domain-specific%20pre-training.%0A%20%20Self-supervised%20pre-training%20was%20performed%20using%20the%20DINOv2%20methodology%20on%20over%2025%20million%202D%20slices%20from%20487%2C975%20DBT%20volumes%20from%2027%2C990%20patients.%20Three%20downstream%20tasks%20were%20evaluated%3A%20%281%29%20breast%20density%20classification%20using%205%2C000%20screening%20exams%3B%20%282%29%205-year%20risk%20of%20developing%20breast%20cancer%20using%20106%2C417%20screening%20exams%3B%20and%20%283%29%20lesion%20detection%20using%20393%20annotated%20volumes.%0A%20%20For%20breast%20density%20classification%2C%20DBT-DINO%20achieved%20an%20accuracy%20of%200.79%20%2895%5C%25%20CI%3A%200.76--0.81%29%2C%20outperforming%20both%20the%20MetaAI%20DINOv2%20baseline%20%280.73%2C%2095%5C%25%20CI%3A%200.70--0.76%2C%20p%3C.001%29%20and%20DenseNet-121%20%280.74%2C%2095%5C%25%20CI%3A%200.71--0.76%2C%20p%3C.001%29.%20For%205-year%20breast%20cancer%20risk%20prediction%2C%20DBT-DINO%20achieved%20an%20AUROC%20of%200.78%20%2895%5C%25%20CI%3A%200.76--0.80%29%20compared%20to%20DINOv2%27s%200.76%20%2895%5C%25%20CI%3A%200.74--0.78%2C%20p%3D.57%29.%20For%20lesion%20detection%2C%20DINOv2%20achieved%20a%20higher%20average%20sensitivity%20of%200.67%20%2895%5C%25%20CI%3A%200.60--0.74%29%20compared%20to%20DBT-DINO%20with%200.62%20%2895%5C%25%20CI%3A%200.53--0.71%2C%20p%3D.60%29.%20DBT-DINO%20demonstrated%20better%20performance%20on%20cancerous%20lesions%20specifically%20with%20a%20detection%20rate%20of%2078.8%5C%25%20compared%20to%20Dinov2%27s%2077.3%5C%25.%0A%20%20Using%20a%20dataset%20of%20unprecedented%20size%2C%20we%20developed%20DBT-DINO%2C%20the%20first%20foundation%20model%20for%20DBT.%20DBT-DINO%20demonstrated%20strong%20performance%20on%20breast%20density%20classification%20and%20cancer%20risk%20prediction.%20However%2C%20domain-specific%20pre-training%20showed%20variable%20benefits%20on%20the%20detection%20task%2C%20with%20ImageNet%20baseline%20outperforming%20DBT-DINO%20on%20general%20lesion%20detection%2C%20indicating%20that%20localized%20detection%20tasks%20require%20further%20methodological%20development.&entry.1838667208=http%3A//arxiv.org/abs/2512.13608v1&entry.124074799=Read"},
{"title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs", "author": "Anran Qi and Changjian Li and Adrien Bousseau and Niloy J. Mitra", "abstract": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/", "link": "http://arxiv.org/abs/2512.13392v1", "date": "2025-12-15", "relevancy": 2.6637, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6876}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.656}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Visible%3A%20Disocclusion-Aware%20Editing%20via%20Proxy%20Dynamic%20Graphs&body=Title%3A%20Beyond%20the%20Visible%3A%20Disocclusion-Aware%20Editing%20via%20Proxy%20Dynamic%20Graphs%0AAuthor%3A%20Anran%20Qi%20and%20Changjian%20Li%20and%20Adrien%20Bousseau%20and%20Niloy%20J.%20Mitra%0AAbstract%3A%20We%20address%20image-to-video%20generation%20with%20explicit%20user%20control%20over%20the%20final%20frame%27s%20disoccluded%20regions.%20Current%20image-to-video%20pipelines%20produce%20plausible%20motion%20but%20struggle%20to%20generate%20predictable%2C%20articulated%20motions%20while%20enforcing%20user-specified%20content%20in%20newly%20revealed%20areas.%20Our%20key%20idea%20is%20to%20separate%20motion%20specification%20from%20appearance%20synthesis%3A%20we%20introduce%20a%20lightweight%2C%20user-editable%20Proxy%20Dynamic%20Graph%20%28PDG%29%20that%20deterministically%20yet%20approximately%20drives%20part%20motion%2C%20while%20a%20frozen%20diffusion%20prior%20is%20used%20to%20synthesize%20plausible%20appearance%20that%20follows%20that%20motion.%20In%20our%20training-free%20pipeline%2C%20the%20user%20loosely%20annotates%20and%20reposes%20a%20PDG%2C%20from%20which%20we%20compute%20a%20dense%20motion%20flow%20to%20leverage%20diffusion%20as%20a%20motion-guided%20shader.%20We%20then%20let%20the%20user%20edit%20appearance%20in%20the%20disoccluded%20areas%20of%20the%20image%2C%20and%20exploit%20the%20visibility%20information%20encoded%20by%20the%20PDG%20to%20perform%20a%20latent-space%20composite%20that%20reconciles%20motion%20with%20user%20intent%20in%20these%20areas.%20This%20design%20yields%20controllable%20articulation%20and%20user%20control%20over%20disocclusions%20without%20fine-tuning.%20We%20demonstrate%20clear%20advantages%20against%20state-of-the-art%20alternatives%20towards%20images%20turned%20into%20short%20videos%20of%20articulated%20objects%2C%20furniture%2C%20vehicles%2C%20and%20deformables.%20Our%20method%20mixes%20generative%20control%2C%20in%20the%20form%20of%20loose%20pose%20and%20structure%2C%20with%20predictable%20controls%2C%20in%20the%20form%20of%20appearance%20specification%20in%20the%20final%20frame%20in%20the%20disoccluded%20regions%2C%20unlocking%20a%20new%20image-to-video%20workflow.%20Code%20will%20be%20released%20on%20acceptance.%20Project%20page%3A%20https%3A//anranqi.github.io/beyondvisible.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.13392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Visible%253A%2520Disocclusion-Aware%2520Editing%2520via%2520Proxy%2520Dynamic%2520Graphs%26entry.906535625%3DAnran%2520Qi%2520and%2520Changjian%2520Li%2520and%2520Adrien%2520Bousseau%2520and%2520Niloy%2520J.%2520Mitra%26entry.1292438233%3DWe%2520address%2520image-to-video%2520generation%2520with%2520explicit%2520user%2520control%2520over%2520the%2520final%2520frame%2527s%2520disoccluded%2520regions.%2520Current%2520image-to-video%2520pipelines%2520produce%2520plausible%2520motion%2520but%2520struggle%2520to%2520generate%2520predictable%252C%2520articulated%2520motions%2520while%2520enforcing%2520user-specified%2520content%2520in%2520newly%2520revealed%2520areas.%2520Our%2520key%2520idea%2520is%2520to%2520separate%2520motion%2520specification%2520from%2520appearance%2520synthesis%253A%2520we%2520introduce%2520a%2520lightweight%252C%2520user-editable%2520Proxy%2520Dynamic%2520Graph%2520%2528PDG%2529%2520that%2520deterministically%2520yet%2520approximately%2520drives%2520part%2520motion%252C%2520while%2520a%2520frozen%2520diffusion%2520prior%2520is%2520used%2520to%2520synthesize%2520plausible%2520appearance%2520that%2520follows%2520that%2520motion.%2520In%2520our%2520training-free%2520pipeline%252C%2520the%2520user%2520loosely%2520annotates%2520and%2520reposes%2520a%2520PDG%252C%2520from%2520which%2520we%2520compute%2520a%2520dense%2520motion%2520flow%2520to%2520leverage%2520diffusion%2520as%2520a%2520motion-guided%2520shader.%2520We%2520then%2520let%2520the%2520user%2520edit%2520appearance%2520in%2520the%2520disoccluded%2520areas%2520of%2520the%2520image%252C%2520and%2520exploit%2520the%2520visibility%2520information%2520encoded%2520by%2520the%2520PDG%2520to%2520perform%2520a%2520latent-space%2520composite%2520that%2520reconciles%2520motion%2520with%2520user%2520intent%2520in%2520these%2520areas.%2520This%2520design%2520yields%2520controllable%2520articulation%2520and%2520user%2520control%2520over%2520disocclusions%2520without%2520fine-tuning.%2520We%2520demonstrate%2520clear%2520advantages%2520against%2520state-of-the-art%2520alternatives%2520towards%2520images%2520turned%2520into%2520short%2520videos%2520of%2520articulated%2520objects%252C%2520furniture%252C%2520vehicles%252C%2520and%2520deformables.%2520Our%2520method%2520mixes%2520generative%2520control%252C%2520in%2520the%2520form%2520of%2520loose%2520pose%2520and%2520structure%252C%2520with%2520predictable%2520controls%252C%2520in%2520the%2520form%2520of%2520appearance%2520specification%2520in%2520the%2520final%2520frame%2520in%2520the%2520disoccluded%2520regions%252C%2520unlocking%2520a%2520new%2520image-to-video%2520workflow.%2520Code%2520will%2520be%2520released%2520on%2520acceptance.%2520Project%2520page%253A%2520https%253A//anranqi.github.io/beyondvisible.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Visible%3A%20Disocclusion-Aware%20Editing%20via%20Proxy%20Dynamic%20Graphs&entry.906535625=Anran%20Qi%20and%20Changjian%20Li%20and%20Adrien%20Bousseau%20and%20Niloy%20J.%20Mitra&entry.1292438233=We%20address%20image-to-video%20generation%20with%20explicit%20user%20control%20over%20the%20final%20frame%27s%20disoccluded%20regions.%20Current%20image-to-video%20pipelines%20produce%20plausible%20motion%20but%20struggle%20to%20generate%20predictable%2C%20articulated%20motions%20while%20enforcing%20user-specified%20content%20in%20newly%20revealed%20areas.%20Our%20key%20idea%20is%20to%20separate%20motion%20specification%20from%20appearance%20synthesis%3A%20we%20introduce%20a%20lightweight%2C%20user-editable%20Proxy%20Dynamic%20Graph%20%28PDG%29%20that%20deterministically%20yet%20approximately%20drives%20part%20motion%2C%20while%20a%20frozen%20diffusion%20prior%20is%20used%20to%20synthesize%20plausible%20appearance%20that%20follows%20that%20motion.%20In%20our%20training-free%20pipeline%2C%20the%20user%20loosely%20annotates%20and%20reposes%20a%20PDG%2C%20from%20which%20we%20compute%20a%20dense%20motion%20flow%20to%20leverage%20diffusion%20as%20a%20motion-guided%20shader.%20We%20then%20let%20the%20user%20edit%20appearance%20in%20the%20disoccluded%20areas%20of%20the%20image%2C%20and%20exploit%20the%20visibility%20information%20encoded%20by%20the%20PDG%20to%20perform%20a%20latent-space%20composite%20that%20reconciles%20motion%20with%20user%20intent%20in%20these%20areas.%20This%20design%20yields%20controllable%20articulation%20and%20user%20control%20over%20disocclusions%20without%20fine-tuning.%20We%20demonstrate%20clear%20advantages%20against%20state-of-the-art%20alternatives%20towards%20images%20turned%20into%20short%20videos%20of%20articulated%20objects%2C%20furniture%2C%20vehicles%2C%20and%20deformables.%20Our%20method%20mixes%20generative%20control%2C%20in%20the%20form%20of%20loose%20pose%20and%20structure%2C%20with%20predictable%20controls%2C%20in%20the%20form%20of%20appearance%20specification%20in%20the%20final%20frame%20in%20the%20disoccluded%20regions%2C%20unlocking%20a%20new%20image-to-video%20workflow.%20Code%20will%20be%20released%20on%20acceptance.%20Project%20page%3A%20https%3A//anranqi.github.io/beyondvisible.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.13392v1&entry.124074799=Read"},
{"title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "author": "Mikhail Zakharov", "abstract": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "link": "http://arxiv.org/abs/2512.13317v1", "date": "2025-12-15", "relevancy": 2.6376, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.539}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Identity%20Unlearning%20for%20Retrieval%20via%20Embedding%20Dispersion&body=Title%3A%20Face%20Identity%20Unlearning%20for%20Retrieval%20via%20Embedding%20Dispersion%0AAuthor%3A%20Mikhail%20Zakharov%0AAbstract%3A%20Face%20recognition%20systems%20rely%20on%20learning%20highly%20discriminative%20and%20compact%20identity%20clusters%20to%20enable%20accurate%20retrieval.%20However%2C%20as%20with%20other%20surveillance-oriented%20technologies%2C%20such%20systems%20raise%20serious%20privacy%20concerns%20due%20to%20their%20potential%20for%20unauthorized%20identity%20tracking.%20While%20several%20works%20have%20explored%20machine%20unlearning%20as%20a%20means%20of%20privacy%20protection%2C%20their%20applicability%20to%20face%20retrieval%20-%20especially%20for%20modern%20embedding-based%20recognition%20models%20-%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20face%20identity%20unlearning%20for%20retrieval%20systems%20and%20present%20its%20inherent%20challenges.%20The%20goal%20is%20to%20make%20selected%20identities%20unretrievable%20by%20dispersing%20their%20embeddings%20on%20the%20hypersphere%20and%20preventing%20the%20formation%20of%20compact%20identity%20clusters%20that%20enable%20re-identification%20in%20the%20gallery.%20The%20primary%20challenge%20is%20to%20achieve%20this%20forgetting%20effect%20while%20preserving%20the%20discriminative%20structure%20of%20the%20embedding%20space%20and%20the%20retrieval%20performance%20of%20the%20model%20for%20the%20remaining%20identities.%20To%20address%20this%2C%20we%20evaluate%20several%20existing%20approximate%20class%20unlearning%20methods%20%28e.g.%2C%20Random%20Labeling%2C%20Gradient%20Ascent%2C%20Boundary%20Unlearning%2C%20and%20other%20recent%20approaches%29%20in%20the%20context%20of%20face%20retrieval%20and%20propose%20a%20simple%20yet%20effective%20dispersion-based%20unlearning%20approach.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28VGGFace2%2C%20CelebA%29%20demonstrate%20that%20our%20method%20achieves%20superior%20forgetting%20behavior%20while%20preserving%20retrieval%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Identity%2520Unlearning%2520for%2520Retrieval%2520via%2520Embedding%2520Dispersion%26entry.906535625%3DMikhail%2520Zakharov%26entry.1292438233%3DFace%2520recognition%2520systems%2520rely%2520on%2520learning%2520highly%2520discriminative%2520and%2520compact%2520identity%2520clusters%2520to%2520enable%2520accurate%2520retrieval.%2520However%252C%2520as%2520with%2520other%2520surveillance-oriented%2520technologies%252C%2520such%2520systems%2520raise%2520serious%2520privacy%2520concerns%2520due%2520to%2520their%2520potential%2520for%2520unauthorized%2520identity%2520tracking.%2520While%2520several%2520works%2520have%2520explored%2520machine%2520unlearning%2520as%2520a%2520means%2520of%2520privacy%2520protection%252C%2520their%2520applicability%2520to%2520face%2520retrieval%2520-%2520especially%2520for%2520modern%2520embedding-based%2520recognition%2520models%2520-%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520problem%2520of%2520face%2520identity%2520unlearning%2520for%2520retrieval%2520systems%2520and%2520present%2520its%2520inherent%2520challenges.%2520The%2520goal%2520is%2520to%2520make%2520selected%2520identities%2520unretrievable%2520by%2520dispersing%2520their%2520embeddings%2520on%2520the%2520hypersphere%2520and%2520preventing%2520the%2520formation%2520of%2520compact%2520identity%2520clusters%2520that%2520enable%2520re-identification%2520in%2520the%2520gallery.%2520The%2520primary%2520challenge%2520is%2520to%2520achieve%2520this%2520forgetting%2520effect%2520while%2520preserving%2520the%2520discriminative%2520structure%2520of%2520the%2520embedding%2520space%2520and%2520the%2520retrieval%2520performance%2520of%2520the%2520model%2520for%2520the%2520remaining%2520identities.%2520To%2520address%2520this%252C%2520we%2520evaluate%2520several%2520existing%2520approximate%2520class%2520unlearning%2520methods%2520%2528e.g.%252C%2520Random%2520Labeling%252C%2520Gradient%2520Ascent%252C%2520Boundary%2520Unlearning%252C%2520and%2520other%2520recent%2520approaches%2529%2520in%2520the%2520context%2520of%2520face%2520retrieval%2520and%2520propose%2520a%2520simple%2520yet%2520effective%2520dispersion-based%2520unlearning%2520approach.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520%2528VGGFace2%252C%2520CelebA%2529%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520forgetting%2520behavior%2520while%2520preserving%2520retrieval%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Identity%20Unlearning%20for%20Retrieval%20via%20Embedding%20Dispersion&entry.906535625=Mikhail%20Zakharov&entry.1292438233=Face%20recognition%20systems%20rely%20on%20learning%20highly%20discriminative%20and%20compact%20identity%20clusters%20to%20enable%20accurate%20retrieval.%20However%2C%20as%20with%20other%20surveillance-oriented%20technologies%2C%20such%20systems%20raise%20serious%20privacy%20concerns%20due%20to%20their%20potential%20for%20unauthorized%20identity%20tracking.%20While%20several%20works%20have%20explored%20machine%20unlearning%20as%20a%20means%20of%20privacy%20protection%2C%20their%20applicability%20to%20face%20retrieval%20-%20especially%20for%20modern%20embedding-based%20recognition%20models%20-%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20face%20identity%20unlearning%20for%20retrieval%20systems%20and%20present%20its%20inherent%20challenges.%20The%20goal%20is%20to%20make%20selected%20identities%20unretrievable%20by%20dispersing%20their%20embeddings%20on%20the%20hypersphere%20and%20preventing%20the%20formation%20of%20compact%20identity%20clusters%20that%20enable%20re-identification%20in%20the%20gallery.%20The%20primary%20challenge%20is%20to%20achieve%20this%20forgetting%20effect%20while%20preserving%20the%20discriminative%20structure%20of%20the%20embedding%20space%20and%20the%20retrieval%20performance%20of%20the%20model%20for%20the%20remaining%20identities.%20To%20address%20this%2C%20we%20evaluate%20several%20existing%20approximate%20class%20unlearning%20methods%20%28e.g.%2C%20Random%20Labeling%2C%20Gradient%20Ascent%2C%20Boundary%20Unlearning%2C%20and%20other%20recent%20approaches%29%20in%20the%20context%20of%20face%20retrieval%20and%20propose%20a%20simple%20yet%20effective%20dispersion-based%20unlearning%20approach.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28VGGFace2%2C%20CelebA%29%20demonstrate%20that%20our%20method%20achieves%20superior%20forgetting%20behavior%20while%20preserving%20retrieval%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2512.13317v1&entry.124074799=Read"},
{"title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "author": "Noa Cohen and Nurit Spingarn-Eliezer and Inbar Huberman-Spiegelglas and Tomer Michaeli", "abstract": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.", "link": "http://arxiv.org/abs/2512.13427v1", "date": "2025-12-15", "relevancy": 2.6136, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5289}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5222}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MineTheGap%3A%20Automatic%20Mining%20of%20Biases%20in%20Text-to-Image%20Models&body=Title%3A%20MineTheGap%3A%20Automatic%20Mining%20of%20Biases%20in%20Text-to-Image%20Models%0AAuthor%3A%20Noa%20Cohen%20and%20Nurit%20Spingarn-Eliezer%20and%20Inbar%20Huberman-Spiegelglas%20and%20Tomer%20Michaeli%0AAbstract%3A%20Text-to-Image%20%28TTI%29%20models%20generate%20images%20based%20on%20text%20prompts%2C%20which%20often%20leave%20certain%20aspects%20of%20the%20desired%20image%20ambiguous.%20When%20faced%20with%20these%20ambiguities%2C%20TTI%20models%20have%20been%20shown%20to%20exhibit%20biases%20in%20their%20interpretations.%20These%20biases%20can%20have%20societal%20impacts%2C%20e.g.%2C%20when%20showing%20only%20a%20certain%20race%20for%20a%20stated%20occupation.%20They%20can%20also%20affect%20user%20experience%20when%20creating%20redundancy%20within%20a%20set%20of%20generated%20images%20instead%20of%20spanning%20diverse%20possibilities.%20Here%2C%20we%20introduce%20MineTheGap%20-%20a%20method%20for%20automatically%20mining%20prompts%20that%20cause%20a%20TTI%20model%20to%20generate%20biased%20outputs.%20Our%20method%20goes%20beyond%20merely%20detecting%20bias%20for%20a%20given%20prompt.%20Rather%2C%20it%20leverages%20a%20genetic%20algorithm%20to%20iteratively%20refine%20a%20pool%20of%20prompts%2C%20seeking%20for%20those%20that%20expose%20biases.%20This%20optimization%20process%20is%20driven%20by%20a%20novel%20bias%20score%2C%20which%20ranks%20biases%20according%20to%20their%20severity%2C%20as%20we%20validate%20on%20a%20dataset%20with%20known%20biases.%20For%20a%20given%20prompt%2C%20this%20score%20is%20obtained%20by%20comparing%20the%20distribution%20of%20generated%20images%20to%20the%20distribution%20of%20LLM-generated%20texts%20that%20constitute%20variations%20on%20the%20prompt.%20Code%20and%20examples%20are%20available%20on%20the%20project%27s%20webpage.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMineTheGap%253A%2520Automatic%2520Mining%2520of%2520Biases%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DNoa%2520Cohen%2520and%2520Nurit%2520Spingarn-Eliezer%2520and%2520Inbar%2520Huberman-Spiegelglas%2520and%2520Tomer%2520Michaeli%26entry.1292438233%3DText-to-Image%2520%2528TTI%2529%2520models%2520generate%2520images%2520based%2520on%2520text%2520prompts%252C%2520which%2520often%2520leave%2520certain%2520aspects%2520of%2520the%2520desired%2520image%2520ambiguous.%2520When%2520faced%2520with%2520these%2520ambiguities%252C%2520TTI%2520models%2520have%2520been%2520shown%2520to%2520exhibit%2520biases%2520in%2520their%2520interpretations.%2520These%2520biases%2520can%2520have%2520societal%2520impacts%252C%2520e.g.%252C%2520when%2520showing%2520only%2520a%2520certain%2520race%2520for%2520a%2520stated%2520occupation.%2520They%2520can%2520also%2520affect%2520user%2520experience%2520when%2520creating%2520redundancy%2520within%2520a%2520set%2520of%2520generated%2520images%2520instead%2520of%2520spanning%2520diverse%2520possibilities.%2520Here%252C%2520we%2520introduce%2520MineTheGap%2520-%2520a%2520method%2520for%2520automatically%2520mining%2520prompts%2520that%2520cause%2520a%2520TTI%2520model%2520to%2520generate%2520biased%2520outputs.%2520Our%2520method%2520goes%2520beyond%2520merely%2520detecting%2520bias%2520for%2520a%2520given%2520prompt.%2520Rather%252C%2520it%2520leverages%2520a%2520genetic%2520algorithm%2520to%2520iteratively%2520refine%2520a%2520pool%2520of%2520prompts%252C%2520seeking%2520for%2520those%2520that%2520expose%2520biases.%2520This%2520optimization%2520process%2520is%2520driven%2520by%2520a%2520novel%2520bias%2520score%252C%2520which%2520ranks%2520biases%2520according%2520to%2520their%2520severity%252C%2520as%2520we%2520validate%2520on%2520a%2520dataset%2520with%2520known%2520biases.%2520For%2520a%2520given%2520prompt%252C%2520this%2520score%2520is%2520obtained%2520by%2520comparing%2520the%2520distribution%2520of%2520generated%2520images%2520to%2520the%2520distribution%2520of%2520LLM-generated%2520texts%2520that%2520constitute%2520variations%2520on%2520the%2520prompt.%2520Code%2520and%2520examples%2520are%2520available%2520on%2520the%2520project%2527s%2520webpage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MineTheGap%3A%20Automatic%20Mining%20of%20Biases%20in%20Text-to-Image%20Models&entry.906535625=Noa%20Cohen%20and%20Nurit%20Spingarn-Eliezer%20and%20Inbar%20Huberman-Spiegelglas%20and%20Tomer%20Michaeli&entry.1292438233=Text-to-Image%20%28TTI%29%20models%20generate%20images%20based%20on%20text%20prompts%2C%20which%20often%20leave%20certain%20aspects%20of%20the%20desired%20image%20ambiguous.%20When%20faced%20with%20these%20ambiguities%2C%20TTI%20models%20have%20been%20shown%20to%20exhibit%20biases%20in%20their%20interpretations.%20These%20biases%20can%20have%20societal%20impacts%2C%20e.g.%2C%20when%20showing%20only%20a%20certain%20race%20for%20a%20stated%20occupation.%20They%20can%20also%20affect%20user%20experience%20when%20creating%20redundancy%20within%20a%20set%20of%20generated%20images%20instead%20of%20spanning%20diverse%20possibilities.%20Here%2C%20we%20introduce%20MineTheGap%20-%20a%20method%20for%20automatically%20mining%20prompts%20that%20cause%20a%20TTI%20model%20to%20generate%20biased%20outputs.%20Our%20method%20goes%20beyond%20merely%20detecting%20bias%20for%20a%20given%20prompt.%20Rather%2C%20it%20leverages%20a%20genetic%20algorithm%20to%20iteratively%20refine%20a%20pool%20of%20prompts%2C%20seeking%20for%20those%20that%20expose%20biases.%20This%20optimization%20process%20is%20driven%20by%20a%20novel%20bias%20score%2C%20which%20ranks%20biases%20according%20to%20their%20severity%2C%20as%20we%20validate%20on%20a%20dataset%20with%20known%20biases.%20For%20a%20given%20prompt%2C%20this%20score%20is%20obtained%20by%20comparing%20the%20distribution%20of%20generated%20images%20to%20the%20distribution%20of%20LLM-generated%20texts%20that%20constitute%20variations%20on%20the%20prompt.%20Code%20and%20examples%20are%20available%20on%20the%20project%27s%20webpage.&entry.1838667208=http%3A//arxiv.org/abs/2512.13427v1&entry.124074799=Read"},
{"title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits", "author": "Foivos Paraperas Papantoniou and Stathis Galanakis and Rolandos Alexandros Potamias and Bernhard Kainz and Stefanos Zafeiriou", "abstract": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.", "link": "http://arxiv.org/abs/2512.13247v1", "date": "2025-12-15", "relevancy": 2.6094, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6782}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6464}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STARCaster%3A%20Spatio-Temporal%20AutoRegressive%20Video%20Diffusion%20for%20Identity-%20and%20View-Aware%20Talking%20Portraits&body=Title%3A%20STARCaster%3A%20Spatio-Temporal%20AutoRegressive%20Video%20Diffusion%20for%20Identity-%20and%20View-Aware%20Talking%20Portraits%0AAuthor%3A%20Foivos%20Paraperas%20Papantoniou%20and%20Stathis%20Galanakis%20and%20Rolandos%20Alexandros%20Potamias%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20This%20paper%20presents%20STARCaster%2C%20an%20identity-aware%20spatio-temporal%20video%20diffusion%20model%20that%20addresses%20both%20speech-driven%20portrait%20animation%20and%20free-viewpoint%20talking%20portrait%20synthesis%2C%20given%20an%20identity%20embedding%20or%20reference%20image%2C%20within%20a%20unified%20framework.%20Existing%202D%20speech-to-video%20diffusion%20models%20depend%20heavily%20on%20reference%20guidance%2C%20leading%20to%20limited%20motion%20diversity.%20At%20the%20same%20time%2C%203D-aware%20animation%20typically%20relies%20on%20inversion%20through%20pre-trained%20tri-plane%20generators%2C%20which%20often%20leads%20to%20imperfect%20reconstructions%20and%20identity%20drift.%20We%20rethink%20reference-%20and%20geometry-based%20paradigms%20in%20two%20ways.%20First%2C%20we%20deviate%20from%20strict%20reference%20conditioning%20at%20pre-training%20by%20introducing%20softer%20identity%20constraints.%20Second%2C%20we%20address%203D%20awareness%20implicitly%20within%20the%202D%20video%20domain%20by%20leveraging%20the%20inherent%20multi-view%20nature%20of%20video%20data.%20STARCaster%20adopts%20a%20compositional%20approach%20progressing%20from%20ID-aware%20motion%20modeling%2C%20to%20audio-visual%20synchronization%20via%20lip%20reading-based%20supervision%2C%20and%20finally%20to%20novel%20view%20animation%20through%20temporal-to-spatial%20adaptation.%20To%20overcome%20the%20scarcity%20of%204D%20audio-visual%20data%2C%20we%20propose%20a%20decoupled%20learning%20approach%20in%20which%20view%20consistency%20and%20temporal%20coherence%20are%20trained%20independently.%20A%20self-forcing%20training%20scheme%20enables%20the%20model%20to%20learn%20from%20longer%20temporal%20contexts%20than%20those%20generated%20at%20inference%2C%20mitigating%20the%20overly%20static%20animations%20common%20in%20existing%20autoregressive%20approaches.%20Comprehensive%20evaluations%20demonstrate%20that%20STARCaster%20generalizes%20effectively%20across%20tasks%20and%20identities%2C%20consistently%20surpassing%20prior%20approaches%20in%20different%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTARCaster%253A%2520Spatio-Temporal%2520AutoRegressive%2520Video%2520Diffusion%2520for%2520Identity-%2520and%2520View-Aware%2520Talking%2520Portraits%26entry.906535625%3DFoivos%2520Paraperas%2520Papantoniou%2520and%2520Stathis%2520Galanakis%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Bernhard%2520Kainz%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3DThis%2520paper%2520presents%2520STARCaster%252C%2520an%2520identity-aware%2520spatio-temporal%2520video%2520diffusion%2520model%2520that%2520addresses%2520both%2520speech-driven%2520portrait%2520animation%2520and%2520free-viewpoint%2520talking%2520portrait%2520synthesis%252C%2520given%2520an%2520identity%2520embedding%2520or%2520reference%2520image%252C%2520within%2520a%2520unified%2520framework.%2520Existing%25202D%2520speech-to-video%2520diffusion%2520models%2520depend%2520heavily%2520on%2520reference%2520guidance%252C%2520leading%2520to%2520limited%2520motion%2520diversity.%2520At%2520the%2520same%2520time%252C%25203D-aware%2520animation%2520typically%2520relies%2520on%2520inversion%2520through%2520pre-trained%2520tri-plane%2520generators%252C%2520which%2520often%2520leads%2520to%2520imperfect%2520reconstructions%2520and%2520identity%2520drift.%2520We%2520rethink%2520reference-%2520and%2520geometry-based%2520paradigms%2520in%2520two%2520ways.%2520First%252C%2520we%2520deviate%2520from%2520strict%2520reference%2520conditioning%2520at%2520pre-training%2520by%2520introducing%2520softer%2520identity%2520constraints.%2520Second%252C%2520we%2520address%25203D%2520awareness%2520implicitly%2520within%2520the%25202D%2520video%2520domain%2520by%2520leveraging%2520the%2520inherent%2520multi-view%2520nature%2520of%2520video%2520data.%2520STARCaster%2520adopts%2520a%2520compositional%2520approach%2520progressing%2520from%2520ID-aware%2520motion%2520modeling%252C%2520to%2520audio-visual%2520synchronization%2520via%2520lip%2520reading-based%2520supervision%252C%2520and%2520finally%2520to%2520novel%2520view%2520animation%2520through%2520temporal-to-spatial%2520adaptation.%2520To%2520overcome%2520the%2520scarcity%2520of%25204D%2520audio-visual%2520data%252C%2520we%2520propose%2520a%2520decoupled%2520learning%2520approach%2520in%2520which%2520view%2520consistency%2520and%2520temporal%2520coherence%2520are%2520trained%2520independently.%2520A%2520self-forcing%2520training%2520scheme%2520enables%2520the%2520model%2520to%2520learn%2520from%2520longer%2520temporal%2520contexts%2520than%2520those%2520generated%2520at%2520inference%252C%2520mitigating%2520the%2520overly%2520static%2520animations%2520common%2520in%2520existing%2520autoregressive%2520approaches.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520STARCaster%2520generalizes%2520effectively%2520across%2520tasks%2520and%2520identities%252C%2520consistently%2520surpassing%2520prior%2520approaches%2520in%2520different%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STARCaster%3A%20Spatio-Temporal%20AutoRegressive%20Video%20Diffusion%20for%20Identity-%20and%20View-Aware%20Talking%20Portraits&entry.906535625=Foivos%20Paraperas%20Papantoniou%20and%20Stathis%20Galanakis%20and%20Rolandos%20Alexandros%20Potamias%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou&entry.1292438233=This%20paper%20presents%20STARCaster%2C%20an%20identity-aware%20spatio-temporal%20video%20diffusion%20model%20that%20addresses%20both%20speech-driven%20portrait%20animation%20and%20free-viewpoint%20talking%20portrait%20synthesis%2C%20given%20an%20identity%20embedding%20or%20reference%20image%2C%20within%20a%20unified%20framework.%20Existing%202D%20speech-to-video%20diffusion%20models%20depend%20heavily%20on%20reference%20guidance%2C%20leading%20to%20limited%20motion%20diversity.%20At%20the%20same%20time%2C%203D-aware%20animation%20typically%20relies%20on%20inversion%20through%20pre-trained%20tri-plane%20generators%2C%20which%20often%20leads%20to%20imperfect%20reconstructions%20and%20identity%20drift.%20We%20rethink%20reference-%20and%20geometry-based%20paradigms%20in%20two%20ways.%20First%2C%20we%20deviate%20from%20strict%20reference%20conditioning%20at%20pre-training%20by%20introducing%20softer%20identity%20constraints.%20Second%2C%20we%20address%203D%20awareness%20implicitly%20within%20the%202D%20video%20domain%20by%20leveraging%20the%20inherent%20multi-view%20nature%20of%20video%20data.%20STARCaster%20adopts%20a%20compositional%20approach%20progressing%20from%20ID-aware%20motion%20modeling%2C%20to%20audio-visual%20synchronization%20via%20lip%20reading-based%20supervision%2C%20and%20finally%20to%20novel%20view%20animation%20through%20temporal-to-spatial%20adaptation.%20To%20overcome%20the%20scarcity%20of%204D%20audio-visual%20data%2C%20we%20propose%20a%20decoupled%20learning%20approach%20in%20which%20view%20consistency%20and%20temporal%20coherence%20are%20trained%20independently.%20A%20self-forcing%20training%20scheme%20enables%20the%20model%20to%20learn%20from%20longer%20temporal%20contexts%20than%20those%20generated%20at%20inference%2C%20mitigating%20the%20overly%20static%20animations%20common%20in%20existing%20autoregressive%20approaches.%20Comprehensive%20evaluations%20demonstrate%20that%20STARCaster%20generalizes%20effectively%20across%20tasks%20and%20identities%2C%20consistently%20surpassing%20prior%20approaches%20in%20different%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.13247v1&entry.124074799=Read"},
{"title": "Semantic-Anchored, Class Variance-Optimized Clustering for Robust Semi-Supervised Few-Shot Learning", "author": "Souvik Maji and Rhythm Baghel and Pratik Mazumder", "abstract": "Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. To further establish its robustness, we conduct extensive experiments under challenging conditions, showing that the model generalizes well to domain shifts and achieves new state-of-the-art performance in open-set settings with distractor classes, highlighting its effectiveness for real-world applications.", "link": "http://arxiv.org/abs/2501.14401v3", "date": "2025-12-15", "relevancy": 2.5936, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5373}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5113}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Anchored%2C%20Class%20Variance-Optimized%20Clustering%20for%20Robust%20Semi-Supervised%20Few-Shot%20Learning&body=Title%3A%20Semantic-Anchored%2C%20Class%20Variance-Optimized%20Clustering%20for%20Robust%20Semi-Supervised%20Few-Shot%20Learning%0AAuthor%3A%20Souvik%20Maji%20and%20Rhythm%20Baghel%20and%20Pratik%20Mazumder%0AAbstract%3A%20Few-shot%20learning%20has%20been%20extensively%20explored%20to%20address%20problems%20where%20the%20amount%20of%20labeled%20samples%20is%20very%20limited%20for%20some%20classes.%20In%20the%20semi-supervised%20few-shot%20learning%20setting%2C%20substantial%20quantities%20of%20unlabeled%20samples%20are%20available.%20Such%20unlabeled%20samples%20are%20generally%20cheaper%20to%20obtain%20and%20can%20be%20used%20to%20improve%20the%20few-shot%20learning%20performance%20of%20the%20model.%20Some%20of%20the%20recent%20methods%20for%20this%20setting%20rely%20on%20clustering%20to%20generate%20pseudo-labels%20for%20the%20unlabeled%20samples.%20Since%20the%20effectiveness%20of%20clustering%20heavily%20influences%20the%20labeling%20of%20the%20unlabeled%20samples%2C%20it%20can%20significantly%20affect%20the%20few-shot%20learning%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20improving%20the%20representation%20learned%20by%20the%20model%20in%20order%20to%20improve%20the%20clustering%20and%2C%20consequently%2C%20the%20model%20performance.%20We%20propose%20an%20approach%20for%20semi-supervised%20few-shot%20learning%20that%20performs%20a%20class-variance%20optimized%20clustering%20coupled%20with%20a%20cluster%20separation%20tuner%20in%20order%20to%20improve%20the%20effectiveness%20of%20clustering%20the%20labeled%20and%20unlabeled%20samples%20in%20this%20setting.%20It%20also%20optimizes%20the%20clustering-based%20pseudo-labeling%20process%20using%20a%20restricted%20pseudo-labeling%20approach%20and%20performs%20semantic%20information%20injection%20in%20order%20to%20improve%20the%20semi-supervised%20few-shot%20learning%20performance%20of%20the%20model.%20We%20experimentally%20demonstrate%20that%20our%20proposed%20approach%20significantly%20outperforms%20recent%20state-of-the-art%20methods%20on%20the%20benchmark%20datasets.%20To%20further%20establish%20its%20robustness%2C%20we%20conduct%20extensive%20experiments%20under%20challenging%20conditions%2C%20showing%20that%20the%20model%20generalizes%20well%20to%20domain%20shifts%20and%20achieves%20new%20state-of-the-art%20performance%20in%20open-set%20settings%20with%20distractor%20classes%2C%20highlighting%20its%20effectiveness%20for%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2501.14401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Anchored%252C%2520Class%2520Variance-Optimized%2520Clustering%2520for%2520Robust%2520Semi-Supervised%2520Few-Shot%2520Learning%26entry.906535625%3DSouvik%2520Maji%2520and%2520Rhythm%2520Baghel%2520and%2520Pratik%2520Mazumder%26entry.1292438233%3DFew-shot%2520learning%2520has%2520been%2520extensively%2520explored%2520to%2520address%2520problems%2520where%2520the%2520amount%2520of%2520labeled%2520samples%2520is%2520very%2520limited%2520for%2520some%2520classes.%2520In%2520the%2520semi-supervised%2520few-shot%2520learning%2520setting%252C%2520substantial%2520quantities%2520of%2520unlabeled%2520samples%2520are%2520available.%2520Such%2520unlabeled%2520samples%2520are%2520generally%2520cheaper%2520to%2520obtain%2520and%2520can%2520be%2520used%2520to%2520improve%2520the%2520few-shot%2520learning%2520performance%2520of%2520the%2520model.%2520Some%2520of%2520the%2520recent%2520methods%2520for%2520this%2520setting%2520rely%2520on%2520clustering%2520to%2520generate%2520pseudo-labels%2520for%2520the%2520unlabeled%2520samples.%2520Since%2520the%2520effectiveness%2520of%2520clustering%2520heavily%2520influences%2520the%2520labeling%2520of%2520the%2520unlabeled%2520samples%252C%2520it%2520can%2520significantly%2520affect%2520the%2520few-shot%2520learning%2520performance.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520improving%2520the%2520representation%2520learned%2520by%2520the%2520model%2520in%2520order%2520to%2520improve%2520the%2520clustering%2520and%252C%2520consequently%252C%2520the%2520model%2520performance.%2520We%2520propose%2520an%2520approach%2520for%2520semi-supervised%2520few-shot%2520learning%2520that%2520performs%2520a%2520class-variance%2520optimized%2520clustering%2520coupled%2520with%2520a%2520cluster%2520separation%2520tuner%2520in%2520order%2520to%2520improve%2520the%2520effectiveness%2520of%2520clustering%2520the%2520labeled%2520and%2520unlabeled%2520samples%2520in%2520this%2520setting.%2520It%2520also%2520optimizes%2520the%2520clustering-based%2520pseudo-labeling%2520process%2520using%2520a%2520restricted%2520pseudo-labeling%2520approach%2520and%2520performs%2520semantic%2520information%2520injection%2520in%2520order%2520to%2520improve%2520the%2520semi-supervised%2520few-shot%2520learning%2520performance%2520of%2520the%2520model.%2520We%2520experimentally%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520significantly%2520outperforms%2520recent%2520state-of-the-art%2520methods%2520on%2520the%2520benchmark%2520datasets.%2520To%2520further%2520establish%2520its%2520robustness%252C%2520we%2520conduct%2520extensive%2520experiments%2520under%2520challenging%2520conditions%252C%2520showing%2520that%2520the%2520model%2520generalizes%2520well%2520to%2520domain%2520shifts%2520and%2520achieves%2520new%2520state-of-the-art%2520performance%2520in%2520open-set%2520settings%2520with%2520distractor%2520classes%252C%2520highlighting%2520its%2520effectiveness%2520for%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Anchored%2C%20Class%20Variance-Optimized%20Clustering%20for%20Robust%20Semi-Supervised%20Few-Shot%20Learning&entry.906535625=Souvik%20Maji%20and%20Rhythm%20Baghel%20and%20Pratik%20Mazumder&entry.1292438233=Few-shot%20learning%20has%20been%20extensively%20explored%20to%20address%20problems%20where%20the%20amount%20of%20labeled%20samples%20is%20very%20limited%20for%20some%20classes.%20In%20the%20semi-supervised%20few-shot%20learning%20setting%2C%20substantial%20quantities%20of%20unlabeled%20samples%20are%20available.%20Such%20unlabeled%20samples%20are%20generally%20cheaper%20to%20obtain%20and%20can%20be%20used%20to%20improve%20the%20few-shot%20learning%20performance%20of%20the%20model.%20Some%20of%20the%20recent%20methods%20for%20this%20setting%20rely%20on%20clustering%20to%20generate%20pseudo-labels%20for%20the%20unlabeled%20samples.%20Since%20the%20effectiveness%20of%20clustering%20heavily%20influences%20the%20labeling%20of%20the%20unlabeled%20samples%2C%20it%20can%20significantly%20affect%20the%20few-shot%20learning%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20improving%20the%20representation%20learned%20by%20the%20model%20in%20order%20to%20improve%20the%20clustering%20and%2C%20consequently%2C%20the%20model%20performance.%20We%20propose%20an%20approach%20for%20semi-supervised%20few-shot%20learning%20that%20performs%20a%20class-variance%20optimized%20clustering%20coupled%20with%20a%20cluster%20separation%20tuner%20in%20order%20to%20improve%20the%20effectiveness%20of%20clustering%20the%20labeled%20and%20unlabeled%20samples%20in%20this%20setting.%20It%20also%20optimizes%20the%20clustering-based%20pseudo-labeling%20process%20using%20a%20restricted%20pseudo-labeling%20approach%20and%20performs%20semantic%20information%20injection%20in%20order%20to%20improve%20the%20semi-supervised%20few-shot%20learning%20performance%20of%20the%20model.%20We%20experimentally%20demonstrate%20that%20our%20proposed%20approach%20significantly%20outperforms%20recent%20state-of-the-art%20methods%20on%20the%20benchmark%20datasets.%20To%20further%20establish%20its%20robustness%2C%20we%20conduct%20extensive%20experiments%20under%20challenging%20conditions%2C%20showing%20that%20the%20model%20generalizes%20well%20to%20domain%20shifts%20and%20achieves%20new%20state-of-the-art%20performance%20in%20open-set%20settings%20with%20distractor%20classes%2C%20highlighting%20its%20effectiveness%20for%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2501.14401v3&entry.124074799=Read"},
{"title": "Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency", "author": "Xinwei Tai and Dongmian Zou and Hongfei Wang", "abstract": "Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT", "link": "http://arxiv.org/abs/2512.13149v1", "date": "2025-12-15", "relevancy": 2.5908, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5259}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5147}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Node-Level%20Graph%20Domain%20Adaptation%20by%20Alleviating%20Local%20Dependency&body=Title%3A%20Enhancing%20Node-Level%20Graph%20Domain%20Adaptation%20by%20Alleviating%20Local%20Dependency%0AAuthor%3A%20Xinwei%20Tai%20and%20Dongmian%20Zou%20and%20Hongfei%20Wang%0AAbstract%3A%20Recent%20years%20have%20witnessed%20significant%20advancements%20in%20machine%20learning%20methods%20on%20graphs.%20However%2C%20transferring%20knowledge%20effectively%20from%20one%20graph%20to%20another%20remains%20a%20critical%20challenge.%20This%20highlights%20the%20need%20for%20algorithms%20capable%20of%20applying%20information%20extracted%20from%20a%20source%20graph%20to%20an%20unlabeled%20target%20graph%2C%20a%20task%20known%20as%20unsupervised%20graph%20domain%20adaptation%20%28GDA%29.%20One%20key%20difficulty%20in%20unsupervised%20GDA%20is%20conditional%20shift%2C%20which%20hinders%20transferability.%20In%20this%20paper%2C%20we%20show%20that%20conditional%20shift%20can%20be%20observed%20only%20if%20there%20exists%20local%20dependencies%20among%20node%20features.%20To%20support%20this%20claim%2C%20we%20perform%20a%20rigorous%20analysis%20and%20also%20further%20provide%20generalization%20bounds%20of%20GDA%20when%20dependent%20node%20features%20are%20modeled%20using%20markov%20chains.%20Guided%20by%20the%20theoretical%20findings%2C%20we%20propose%20to%20improve%20GDA%20by%20decorrelating%20node%20features%2C%20which%20can%20be%20specifically%20implemented%20through%20decorrelated%20GCN%20layers%20and%20graph%20transformer%20layers.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20this%20approach%2C%20showing%20not%20only%20substantial%20performance%20enhancements%20over%20baseline%20GDA%20methods%20but%20also%20clear%20visualizations%20of%20small%20intra-class%20distances%20in%20the%20learned%20representations.%20Our%20code%20is%20available%20at%20https%3A//github.com/TechnologyAiGroup/DFT%0ALink%3A%20http%3A//arxiv.org/abs/2512.13149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Node-Level%2520Graph%2520Domain%2520Adaptation%2520by%2520Alleviating%2520Local%2520Dependency%26entry.906535625%3DXinwei%2520Tai%2520and%2520Dongmian%2520Zou%2520and%2520Hongfei%2520Wang%26entry.1292438233%3DRecent%2520years%2520have%2520witnessed%2520significant%2520advancements%2520in%2520machine%2520learning%2520methods%2520on%2520graphs.%2520However%252C%2520transferring%2520knowledge%2520effectively%2520from%2520one%2520graph%2520to%2520another%2520remains%2520a%2520critical%2520challenge.%2520This%2520highlights%2520the%2520need%2520for%2520algorithms%2520capable%2520of%2520applying%2520information%2520extracted%2520from%2520a%2520source%2520graph%2520to%2520an%2520unlabeled%2520target%2520graph%252C%2520a%2520task%2520known%2520as%2520unsupervised%2520graph%2520domain%2520adaptation%2520%2528GDA%2529.%2520One%2520key%2520difficulty%2520in%2520unsupervised%2520GDA%2520is%2520conditional%2520shift%252C%2520which%2520hinders%2520transferability.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520conditional%2520shift%2520can%2520be%2520observed%2520only%2520if%2520there%2520exists%2520local%2520dependencies%2520among%2520node%2520features.%2520To%2520support%2520this%2520claim%252C%2520we%2520perform%2520a%2520rigorous%2520analysis%2520and%2520also%2520further%2520provide%2520generalization%2520bounds%2520of%2520GDA%2520when%2520dependent%2520node%2520features%2520are%2520modeled%2520using%2520markov%2520chains.%2520Guided%2520by%2520the%2520theoretical%2520findings%252C%2520we%2520propose%2520to%2520improve%2520GDA%2520by%2520decorrelating%2520node%2520features%252C%2520which%2520can%2520be%2520specifically%2520implemented%2520through%2520decorrelated%2520GCN%2520layers%2520and%2520graph%2520transformer%2520layers.%2520Our%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%252C%2520showing%2520not%2520only%2520substantial%2520performance%2520enhancements%2520over%2520baseline%2520GDA%2520methods%2520but%2520also%2520clear%2520visualizations%2520of%2520small%2520intra-class%2520distances%2520in%2520the%2520learned%2520representations.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/TechnologyAiGroup/DFT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Node-Level%20Graph%20Domain%20Adaptation%20by%20Alleviating%20Local%20Dependency&entry.906535625=Xinwei%20Tai%20and%20Dongmian%20Zou%20and%20Hongfei%20Wang&entry.1292438233=Recent%20years%20have%20witnessed%20significant%20advancements%20in%20machine%20learning%20methods%20on%20graphs.%20However%2C%20transferring%20knowledge%20effectively%20from%20one%20graph%20to%20another%20remains%20a%20critical%20challenge.%20This%20highlights%20the%20need%20for%20algorithms%20capable%20of%20applying%20information%20extracted%20from%20a%20source%20graph%20to%20an%20unlabeled%20target%20graph%2C%20a%20task%20known%20as%20unsupervised%20graph%20domain%20adaptation%20%28GDA%29.%20One%20key%20difficulty%20in%20unsupervised%20GDA%20is%20conditional%20shift%2C%20which%20hinders%20transferability.%20In%20this%20paper%2C%20we%20show%20that%20conditional%20shift%20can%20be%20observed%20only%20if%20there%20exists%20local%20dependencies%20among%20node%20features.%20To%20support%20this%20claim%2C%20we%20perform%20a%20rigorous%20analysis%20and%20also%20further%20provide%20generalization%20bounds%20of%20GDA%20when%20dependent%20node%20features%20are%20modeled%20using%20markov%20chains.%20Guided%20by%20the%20theoretical%20findings%2C%20we%20propose%20to%20improve%20GDA%20by%20decorrelating%20node%20features%2C%20which%20can%20be%20specifically%20implemented%20through%20decorrelated%20GCN%20layers%20and%20graph%20transformer%20layers.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20this%20approach%2C%20showing%20not%20only%20substantial%20performance%20enhancements%20over%20baseline%20GDA%20methods%20but%20also%20clear%20visualizations%20of%20small%20intra-class%20distances%20in%20the%20learned%20representations.%20Our%20code%20is%20available%20at%20https%3A//github.com/TechnologyAiGroup/DFT&entry.1838667208=http%3A//arxiv.org/abs/2512.13149v1&entry.124074799=Read"},
{"title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations", "author": "Zhendong Yang and Jie Wang and Liansong Zong and Xiaorong Liu and Quan Qian and Shiqian Chen", "abstract": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN", "link": "http://arxiv.org/abs/2508.16634v5", "date": "2025-12-15", "relevancy": 2.5838, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5219}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations&body=Title%3A%20Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations%0AAuthor%3A%20Zhendong%20Yang%20and%20Jie%20Wang%20and%20Liansong%20Zong%20and%20Xiaorong%20Liu%20and%20Quan%20Qian%20and%20Shiqian%20Chen%0AAbstract%3A%20Few-Shot%20Class-Incremental%20Fault%20Diagnosis%20%28FSC-FD%29%2C%20which%20aims%20to%20continuously%20learn%20from%20new%20fault%20classes%20with%20only%20a%20few%20samples%20without%20forgetting%20old%20ones%2C%20is%20critical%20for%20real-world%20industrial%20systems.%20However%2C%20this%20challenging%20task%20severely%20amplifies%20the%20issues%20of%20catastrophic%20forgetting%20of%20old%20knowledge%20and%20overfitting%20on%20scarce%20new%20data.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20built%20upon%20Dual-Granularity%20Representations%2C%20termed%20the%20Dual-Granularity%20Guidance%20Network%20%28DGGN%29.%20Our%20DGGN%20explicitly%20decouples%20feature%20learning%20into%20two%20parallel%20streams%3A%201%29%20a%20fine-grained%20representation%20stream%2C%20which%20utilizes%20a%20novel%20Multi-Order%20Interaction%20Aggregation%20module%20to%20capture%20discriminative%2C%20class-specific%20features%20from%20the%20limited%20new%20samples.%202%29%20a%20coarse-grained%20representation%20stream%2C%20designed%20to%20model%20and%20preserve%20general%2C%20class-agnostic%20knowledge%20shared%20across%20all%20fault%20types.%20These%20two%20representations%20are%20dynamically%20fused%20by%20a%20multi-semantic%20cross-attention%20mechanism%2C%20where%20the%20stable%20coarse-grained%20knowledge%20guides%20the%20learning%20of%20fine-grained%20features%2C%20preventing%20overfitting%20and%20alleviating%20feature%20conflicts.%20To%20further%20mitigate%20catastrophic%20forgetting%2C%20we%20design%20a%20Boundary-Aware%20Exemplar%20Prioritization%20strategy.%20Moreover%2C%20a%20decoupled%20Balanced%20Random%20Forest%20classifier%20is%20employed%20to%20counter%20the%20decision%20boundary%20bias%20caused%20by%20data%20imbalance.%20Extensive%20experiments%20on%20the%20TEP%20benchmark%20and%20a%20real-world%20MFF%20dataset%20demonstrate%20that%20our%20proposed%20DGGN%20achieves%20superior%20diagnostic%20performance%20and%20stability%20compared%20to%20state-of-the-art%20FSC-FD%20approaches.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/MentaY/DGGN%0ALink%3A%20http%3A//arxiv.org/abs/2508.16634v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Class-incremental%2520Fault%2520Diagnosis%2520by%2520Preserving%2520Class-Agnostic%2520Knowledge%2520with%2520Dual-Granularity%2520Representations%26entry.906535625%3DZhendong%2520Yang%2520and%2520Jie%2520Wang%2520and%2520Liansong%2520Zong%2520and%2520Xiaorong%2520Liu%2520and%2520Quan%2520Qian%2520and%2520Shiqian%2520Chen%26entry.1292438233%3DFew-Shot%2520Class-Incremental%2520Fault%2520Diagnosis%2520%2528FSC-FD%2529%252C%2520which%2520aims%2520to%2520continuously%2520learn%2520from%2520new%2520fault%2520classes%2520with%2520only%2520a%2520few%2520samples%2520without%2520forgetting%2520old%2520ones%252C%2520is%2520critical%2520for%2520real-world%2520industrial%2520systems.%2520However%252C%2520this%2520challenging%2520task%2520severely%2520amplifies%2520the%2520issues%2520of%2520catastrophic%2520forgetting%2520of%2520old%2520knowledge%2520and%2520overfitting%2520on%2520scarce%2520new%2520data.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%2520built%2520upon%2520Dual-Granularity%2520Representations%252C%2520termed%2520the%2520Dual-Granularity%2520Guidance%2520Network%2520%2528DGGN%2529.%2520Our%2520DGGN%2520explicitly%2520decouples%2520feature%2520learning%2520into%2520two%2520parallel%2520streams%253A%25201%2529%2520a%2520fine-grained%2520representation%2520stream%252C%2520which%2520utilizes%2520a%2520novel%2520Multi-Order%2520Interaction%2520Aggregation%2520module%2520to%2520capture%2520discriminative%252C%2520class-specific%2520features%2520from%2520the%2520limited%2520new%2520samples.%25202%2529%2520a%2520coarse-grained%2520representation%2520stream%252C%2520designed%2520to%2520model%2520and%2520preserve%2520general%252C%2520class-agnostic%2520knowledge%2520shared%2520across%2520all%2520fault%2520types.%2520These%2520two%2520representations%2520are%2520dynamically%2520fused%2520by%2520a%2520multi-semantic%2520cross-attention%2520mechanism%252C%2520where%2520the%2520stable%2520coarse-grained%2520knowledge%2520guides%2520the%2520learning%2520of%2520fine-grained%2520features%252C%2520preventing%2520overfitting%2520and%2520alleviating%2520feature%2520conflicts.%2520To%2520further%2520mitigate%2520catastrophic%2520forgetting%252C%2520we%2520design%2520a%2520Boundary-Aware%2520Exemplar%2520Prioritization%2520strategy.%2520Moreover%252C%2520a%2520decoupled%2520Balanced%2520Random%2520Forest%2520classifier%2520is%2520employed%2520to%2520counter%2520the%2520decision%2520boundary%2520bias%2520caused%2520by%2520data%2520imbalance.%2520Extensive%2520experiments%2520on%2520the%2520TEP%2520benchmark%2520and%2520a%2520real-world%2520MFF%2520dataset%2520demonstrate%2520that%2520our%2520proposed%2520DGGN%2520achieves%2520superior%2520diagnostic%2520performance%2520and%2520stability%2520compared%2520to%2520state-of-the-art%2520FSC-FD%2520approaches.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/MentaY/DGGN%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16634v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations&entry.906535625=Zhendong%20Yang%20and%20Jie%20Wang%20and%20Liansong%20Zong%20and%20Xiaorong%20Liu%20and%20Quan%20Qian%20and%20Shiqian%20Chen&entry.1292438233=Few-Shot%20Class-Incremental%20Fault%20Diagnosis%20%28FSC-FD%29%2C%20which%20aims%20to%20continuously%20learn%20from%20new%20fault%20classes%20with%20only%20a%20few%20samples%20without%20forgetting%20old%20ones%2C%20is%20critical%20for%20real-world%20industrial%20systems.%20However%2C%20this%20challenging%20task%20severely%20amplifies%20the%20issues%20of%20catastrophic%20forgetting%20of%20old%20knowledge%20and%20overfitting%20on%20scarce%20new%20data.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20built%20upon%20Dual-Granularity%20Representations%2C%20termed%20the%20Dual-Granularity%20Guidance%20Network%20%28DGGN%29.%20Our%20DGGN%20explicitly%20decouples%20feature%20learning%20into%20two%20parallel%20streams%3A%201%29%20a%20fine-grained%20representation%20stream%2C%20which%20utilizes%20a%20novel%20Multi-Order%20Interaction%20Aggregation%20module%20to%20capture%20discriminative%2C%20class-specific%20features%20from%20the%20limited%20new%20samples.%202%29%20a%20coarse-grained%20representation%20stream%2C%20designed%20to%20model%20and%20preserve%20general%2C%20class-agnostic%20knowledge%20shared%20across%20all%20fault%20types.%20These%20two%20representations%20are%20dynamically%20fused%20by%20a%20multi-semantic%20cross-attention%20mechanism%2C%20where%20the%20stable%20coarse-grained%20knowledge%20guides%20the%20learning%20of%20fine-grained%20features%2C%20preventing%20overfitting%20and%20alleviating%20feature%20conflicts.%20To%20further%20mitigate%20catastrophic%20forgetting%2C%20we%20design%20a%20Boundary-Aware%20Exemplar%20Prioritization%20strategy.%20Moreover%2C%20a%20decoupled%20Balanced%20Random%20Forest%20classifier%20is%20employed%20to%20counter%20the%20decision%20boundary%20bias%20caused%20by%20data%20imbalance.%20Extensive%20experiments%20on%20the%20TEP%20benchmark%20and%20a%20real-world%20MFF%20dataset%20demonstrate%20that%20our%20proposed%20DGGN%20achieves%20superior%20diagnostic%20performance%20and%20stability%20compared%20to%20state-of-the-art%20FSC-FD%20approaches.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/MentaY/DGGN&entry.1838667208=http%3A//arxiv.org/abs/2508.16634v5&entry.124074799=Read"},
{"title": "Progressive Localisation in Localist LLMs", "author": "Joachim Diederich", "abstract": "This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate five locality configurations: two uniform baselines (fully distributed and fully localist) and three progressive polynomial schedules. We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.", "link": "http://arxiv.org/abs/2511.18375v3", "date": "2025-12-15", "relevancy": 2.5713, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5195}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Localisation%20in%20Localist%20LLMs&body=Title%3A%20Progressive%20Localisation%20in%20Localist%20LLMs%0AAuthor%3A%20Joachim%20Diederich%0AAbstract%3A%20This%20paper%20demonstrates%20that%20progressive%20localization%2C%20the%20gradual%20increase%20of%20attention%20locality%20from%20early%20distributed%20layers%20to%20late%20localized%20layers%2C%20represents%20the%20optimal%20architecture%20for%20creating%20interpretable%20large%20language%20models%20%28LLMs%29%20while%20preserving%20performance.%20Through%20systematic%20experimentation%20with%20GPT-2%20fine-tuned%20on%20The%20Psychology%20of%20Artificial%20Superintelligence%2C%20we%20evaluate%20five%20locality%20configurations%3A%20two%20uniform%20baselines%20%28fully%20distributed%20and%20fully%20localist%29%20and%20three%20progressive%20polynomial%20schedules.%20We%20investigate%20whether%20interpretability%20constraints%20can%20be%20aligned%20with%20natural%20semantic%20structure%20while%20being%20applied%20strategically%20across%20network%20depth.%20We%20demonstrate%20that%20progressive%20semantic%20localization%2C%20combining%20adaptive%20semantic%20block%20partitioning%20with%20steep%20polynomial%20locality%20schedules%2C%20achieves%20near-baseline%20language%20modeling%20performance%20while%20providing%20interpretable%20attention%20patterns.%20Multiple%20independent%20training%20runs%20with%20different%20random%20seeds%20establish%20that%20results%20are%20statistically%20robust%20and%20highly%20reproducible.%20The%20approach%20dramatically%20outperforms%20both%20fixed-window%20localization%20and%20naive%20uniform%20locality%20constraints.%20Analysis%20reveals%20that%20maintaining%20flexibility%20through%20low-fidelity%20constraints%20preserves%20model%20capacity%20while%20providing%20interpretability%20benefits%2C%20and%20that%20steep%20schedules%20concentrating%20locality%20in%20decision-critical%20final%20layers%20while%20preserving%20distributed%20learning%20in%20early%20layers%20achieve%20near-baseline%20attention%20distribution%20characteristics.%20These%20findings%20demonstrate%20that%20interpretability%20mechanisms%20should%20align%20with%20semantic%20structure%20to%20achieve%20practical%20performance-interpretability%20tradeoffs%20for%20trustworthy%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18375v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Localisation%2520in%2520Localist%2520LLMs%26entry.906535625%3DJoachim%2520Diederich%26entry.1292438233%3DThis%2520paper%2520demonstrates%2520that%2520progressive%2520localization%252C%2520the%2520gradual%2520increase%2520of%2520attention%2520locality%2520from%2520early%2520distributed%2520layers%2520to%2520late%2520localized%2520layers%252C%2520represents%2520the%2520optimal%2520architecture%2520for%2520creating%2520interpretable%2520large%2520language%2520models%2520%2528LLMs%2529%2520while%2520preserving%2520performance.%2520Through%2520systematic%2520experimentation%2520with%2520GPT-2%2520fine-tuned%2520on%2520The%2520Psychology%2520of%2520Artificial%2520Superintelligence%252C%2520we%2520evaluate%2520five%2520locality%2520configurations%253A%2520two%2520uniform%2520baselines%2520%2528fully%2520distributed%2520and%2520fully%2520localist%2529%2520and%2520three%2520progressive%2520polynomial%2520schedules.%2520We%2520investigate%2520whether%2520interpretability%2520constraints%2520can%2520be%2520aligned%2520with%2520natural%2520semantic%2520structure%2520while%2520being%2520applied%2520strategically%2520across%2520network%2520depth.%2520We%2520demonstrate%2520that%2520progressive%2520semantic%2520localization%252C%2520combining%2520adaptive%2520semantic%2520block%2520partitioning%2520with%2520steep%2520polynomial%2520locality%2520schedules%252C%2520achieves%2520near-baseline%2520language%2520modeling%2520performance%2520while%2520providing%2520interpretable%2520attention%2520patterns.%2520Multiple%2520independent%2520training%2520runs%2520with%2520different%2520random%2520seeds%2520establish%2520that%2520results%2520are%2520statistically%2520robust%2520and%2520highly%2520reproducible.%2520The%2520approach%2520dramatically%2520outperforms%2520both%2520fixed-window%2520localization%2520and%2520naive%2520uniform%2520locality%2520constraints.%2520Analysis%2520reveals%2520that%2520maintaining%2520flexibility%2520through%2520low-fidelity%2520constraints%2520preserves%2520model%2520capacity%2520while%2520providing%2520interpretability%2520benefits%252C%2520and%2520that%2520steep%2520schedules%2520concentrating%2520locality%2520in%2520decision-critical%2520final%2520layers%2520while%2520preserving%2520distributed%2520learning%2520in%2520early%2520layers%2520achieve%2520near-baseline%2520attention%2520distribution%2520characteristics.%2520These%2520findings%2520demonstrate%2520that%2520interpretability%2520mechanisms%2520should%2520align%2520with%2520semantic%2520structure%2520to%2520achieve%2520practical%2520performance-interpretability%2520tradeoffs%2520for%2520trustworthy%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18375v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Localisation%20in%20Localist%20LLMs&entry.906535625=Joachim%20Diederich&entry.1292438233=This%20paper%20demonstrates%20that%20progressive%20localization%2C%20the%20gradual%20increase%20of%20attention%20locality%20from%20early%20distributed%20layers%20to%20late%20localized%20layers%2C%20represents%20the%20optimal%20architecture%20for%20creating%20interpretable%20large%20language%20models%20%28LLMs%29%20while%20preserving%20performance.%20Through%20systematic%20experimentation%20with%20GPT-2%20fine-tuned%20on%20The%20Psychology%20of%20Artificial%20Superintelligence%2C%20we%20evaluate%20five%20locality%20configurations%3A%20two%20uniform%20baselines%20%28fully%20distributed%20and%20fully%20localist%29%20and%20three%20progressive%20polynomial%20schedules.%20We%20investigate%20whether%20interpretability%20constraints%20can%20be%20aligned%20with%20natural%20semantic%20structure%20while%20being%20applied%20strategically%20across%20network%20depth.%20We%20demonstrate%20that%20progressive%20semantic%20localization%2C%20combining%20adaptive%20semantic%20block%20partitioning%20with%20steep%20polynomial%20locality%20schedules%2C%20achieves%20near-baseline%20language%20modeling%20performance%20while%20providing%20interpretable%20attention%20patterns.%20Multiple%20independent%20training%20runs%20with%20different%20random%20seeds%20establish%20that%20results%20are%20statistically%20robust%20and%20highly%20reproducible.%20The%20approach%20dramatically%20outperforms%20both%20fixed-window%20localization%20and%20naive%20uniform%20locality%20constraints.%20Analysis%20reveals%20that%20maintaining%20flexibility%20through%20low-fidelity%20constraints%20preserves%20model%20capacity%20while%20providing%20interpretability%20benefits%2C%20and%20that%20steep%20schedules%20concentrating%20locality%20in%20decision-critical%20final%20layers%20while%20preserving%20distributed%20learning%20in%20early%20layers%20achieve%20near-baseline%20attention%20distribution%20characteristics.%20These%20findings%20demonstrate%20that%20interpretability%20mechanisms%20should%20align%20with%20semantic%20structure%20to%20achieve%20practical%20performance-interpretability%20tradeoffs%20for%20trustworthy%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.18375v3&entry.124074799=Read"},
{"title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance", "author": "Aladin Djuhera and Swanand Ravindra Kadhe and Syed Zawad and Farhan Ahmed and Heiko Ludwig and Holger Boche", "abstract": "Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.", "link": "http://arxiv.org/abs/2506.06522v3", "date": "2025-12-15", "relevancy": 2.5678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fixing%20It%20in%20Post%3A%20A%20Comparative%20Study%20of%20LLM%20Post-Training%20Data%20Quality%20and%20Model%20Performance&body=Title%3A%20Fixing%20It%20in%20Post%3A%20A%20Comparative%20Study%20of%20LLM%20Post-Training%20Data%20Quality%20and%20Model%20Performance%0AAuthor%3A%20Aladin%20Djuhera%20and%20Swanand%20Ravindra%20Kadhe%20and%20Syed%20Zawad%20and%20Farhan%20Ahmed%20and%20Heiko%20Ludwig%20and%20Holger%20Boche%0AAbstract%3A%20Recent%20work%20on%20large%20language%20models%20%28LLMs%29%20has%20increasingly%20focused%20on%20post-training%20and%20alignment%20with%20datasets%20curated%20to%20enhance%20instruction%20following%2C%20world%20knowledge%2C%20and%20specialized%20skills.%20However%2C%20most%20post-training%20datasets%20used%20in%20leading%20open-%20and%20closed-source%20LLMs%20remain%20inaccessible%20to%20the%20public%2C%20with%20limited%20information%20about%20their%20construction%20process.%20This%20lack%20of%20transparency%20has%20motivated%20the%20recent%20development%20of%20open-source%20post-training%20corpora.%20While%20training%20on%20these%20open%20alternatives%20can%20yield%20performance%20comparable%20to%20that%20of%20leading%20models%2C%20systematic%20comparisons%20remain%20challenging%20due%20to%20the%20significant%20computational%20cost%20of%20conducting%20them%20rigorously%20at%20scale%2C%20and%20are%20therefore%20largely%20absent.%20As%20a%20result%2C%20it%20remains%20unclear%20how%20specific%20samples%2C%20task%20types%2C%20or%20curation%20strategies%20influence%20downstream%20performance%20when%20assessing%20data%20quality.%20In%20this%20work%2C%20we%20conduct%20the%20first%20comprehensive%20side-by-side%20analysis%20of%20two%20prominent%20open%20post-training%20datasets%3A%20Tulu-3-SFT-Mix%20and%20SmolTalk.%20Using%20the%20Magpie%20framework%2C%20we%20annotate%20each%20sample%20with%20detailed%20quality%20metrics%2C%20including%20turn%20structure%20%28single-turn%20vs.%20multi-turn%29%2C%20task%20category%2C%20input%20quality%2C%20and%20response%20quality%2C%20and%20we%20derive%20statistics%20that%20reveal%20structural%20and%20qualitative%20similarities%20and%20differences%20between%20the%20two%20datasets.%20Based%20on%20these%20insights%2C%20we%20design%20a%20principled%20curation%20recipe%20that%20produces%20a%20new%20data%20mixture%2C%20TuluTalk%2C%20which%20contains%2014%25%20fewer%20samples%20than%20either%20source%20dataset%20while%20matching%20or%20exceeding%20their%20performance%20on%20key%20benchmarks.%20Our%20findings%20offer%20actionable%20insights%20for%20constructing%20more%20effective%20post-training%20datasets%20that%20improve%20model%20performance%20within%20practical%20resource%20limits.%20To%20support%20future%20research%2C%20we%20publicly%20release%20both%20the%20annotated%20source%20datasets%20and%20our%20curated%20TuluTalk%20mixture.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06522v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFixing%2520It%2520in%2520Post%253A%2520A%2520Comparative%2520Study%2520of%2520LLM%2520Post-Training%2520Data%2520Quality%2520and%2520Model%2520Performance%26entry.906535625%3DAladin%2520Djuhera%2520and%2520Swanand%2520Ravindra%2520Kadhe%2520and%2520Syed%2520Zawad%2520and%2520Farhan%2520Ahmed%2520and%2520Heiko%2520Ludwig%2520and%2520Holger%2520Boche%26entry.1292438233%3DRecent%2520work%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520increasingly%2520focused%2520on%2520post-training%2520and%2520alignment%2520with%2520datasets%2520curated%2520to%2520enhance%2520instruction%2520following%252C%2520world%2520knowledge%252C%2520and%2520specialized%2520skills.%2520However%252C%2520most%2520post-training%2520datasets%2520used%2520in%2520leading%2520open-%2520and%2520closed-source%2520LLMs%2520remain%2520inaccessible%2520to%2520the%2520public%252C%2520with%2520limited%2520information%2520about%2520their%2520construction%2520process.%2520This%2520lack%2520of%2520transparency%2520has%2520motivated%2520the%2520recent%2520development%2520of%2520open-source%2520post-training%2520corpora.%2520While%2520training%2520on%2520these%2520open%2520alternatives%2520can%2520yield%2520performance%2520comparable%2520to%2520that%2520of%2520leading%2520models%252C%2520systematic%2520comparisons%2520remain%2520challenging%2520due%2520to%2520the%2520significant%2520computational%2520cost%2520of%2520conducting%2520them%2520rigorously%2520at%2520scale%252C%2520and%2520are%2520therefore%2520largely%2520absent.%2520As%2520a%2520result%252C%2520it%2520remains%2520unclear%2520how%2520specific%2520samples%252C%2520task%2520types%252C%2520or%2520curation%2520strategies%2520influence%2520downstream%2520performance%2520when%2520assessing%2520data%2520quality.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%2520comprehensive%2520side-by-side%2520analysis%2520of%2520two%2520prominent%2520open%2520post-training%2520datasets%253A%2520Tulu-3-SFT-Mix%2520and%2520SmolTalk.%2520Using%2520the%2520Magpie%2520framework%252C%2520we%2520annotate%2520each%2520sample%2520with%2520detailed%2520quality%2520metrics%252C%2520including%2520turn%2520structure%2520%2528single-turn%2520vs.%2520multi-turn%2529%252C%2520task%2520category%252C%2520input%2520quality%252C%2520and%2520response%2520quality%252C%2520and%2520we%2520derive%2520statistics%2520that%2520reveal%2520structural%2520and%2520qualitative%2520similarities%2520and%2520differences%2520between%2520the%2520two%2520datasets.%2520Based%2520on%2520these%2520insights%252C%2520we%2520design%2520a%2520principled%2520curation%2520recipe%2520that%2520produces%2520a%2520new%2520data%2520mixture%252C%2520TuluTalk%252C%2520which%2520contains%252014%2525%2520fewer%2520samples%2520than%2520either%2520source%2520dataset%2520while%2520matching%2520or%2520exceeding%2520their%2520performance%2520on%2520key%2520benchmarks.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520for%2520constructing%2520more%2520effective%2520post-training%2520datasets%2520that%2520improve%2520model%2520performance%2520within%2520practical%2520resource%2520limits.%2520To%2520support%2520future%2520research%252C%2520we%2520publicly%2520release%2520both%2520the%2520annotated%2520source%2520datasets%2520and%2520our%2520curated%2520TuluTalk%2520mixture.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06522v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fixing%20It%20in%20Post%3A%20A%20Comparative%20Study%20of%20LLM%20Post-Training%20Data%20Quality%20and%20Model%20Performance&entry.906535625=Aladin%20Djuhera%20and%20Swanand%20Ravindra%20Kadhe%20and%20Syed%20Zawad%20and%20Farhan%20Ahmed%20and%20Heiko%20Ludwig%20and%20Holger%20Boche&entry.1292438233=Recent%20work%20on%20large%20language%20models%20%28LLMs%29%20has%20increasingly%20focused%20on%20post-training%20and%20alignment%20with%20datasets%20curated%20to%20enhance%20instruction%20following%2C%20world%20knowledge%2C%20and%20specialized%20skills.%20However%2C%20most%20post-training%20datasets%20used%20in%20leading%20open-%20and%20closed-source%20LLMs%20remain%20inaccessible%20to%20the%20public%2C%20with%20limited%20information%20about%20their%20construction%20process.%20This%20lack%20of%20transparency%20has%20motivated%20the%20recent%20development%20of%20open-source%20post-training%20corpora.%20While%20training%20on%20these%20open%20alternatives%20can%20yield%20performance%20comparable%20to%20that%20of%20leading%20models%2C%20systematic%20comparisons%20remain%20challenging%20due%20to%20the%20significant%20computational%20cost%20of%20conducting%20them%20rigorously%20at%20scale%2C%20and%20are%20therefore%20largely%20absent.%20As%20a%20result%2C%20it%20remains%20unclear%20how%20specific%20samples%2C%20task%20types%2C%20or%20curation%20strategies%20influence%20downstream%20performance%20when%20assessing%20data%20quality.%20In%20this%20work%2C%20we%20conduct%20the%20first%20comprehensive%20side-by-side%20analysis%20of%20two%20prominent%20open%20post-training%20datasets%3A%20Tulu-3-SFT-Mix%20and%20SmolTalk.%20Using%20the%20Magpie%20framework%2C%20we%20annotate%20each%20sample%20with%20detailed%20quality%20metrics%2C%20including%20turn%20structure%20%28single-turn%20vs.%20multi-turn%29%2C%20task%20category%2C%20input%20quality%2C%20and%20response%20quality%2C%20and%20we%20derive%20statistics%20that%20reveal%20structural%20and%20qualitative%20similarities%20and%20differences%20between%20the%20two%20datasets.%20Based%20on%20these%20insights%2C%20we%20design%20a%20principled%20curation%20recipe%20that%20produces%20a%20new%20data%20mixture%2C%20TuluTalk%2C%20which%20contains%2014%25%20fewer%20samples%20than%20either%20source%20dataset%20while%20matching%20or%20exceeding%20their%20performance%20on%20key%20benchmarks.%20Our%20findings%20offer%20actionable%20insights%20for%20constructing%20more%20effective%20post-training%20datasets%20that%20improve%20model%20performance%20within%20practical%20resource%20limits.%20To%20support%20future%20research%2C%20we%20publicly%20release%20both%20the%20annotated%20source%20datasets%20and%20our%20curated%20TuluTalk%20mixture.&entry.1838667208=http%3A//arxiv.org/abs/2506.06522v3&entry.124074799=Read"},
{"title": "SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy", "author": "Yici Liu and Qi Wei Oung and Hoi Leong Lee", "abstract": "Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.", "link": "http://arxiv.org/abs/2512.13458v1", "date": "2025-12-15", "relevancy": 2.5651, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5671}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4981}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSAS%3A%20Cross-subject%20EEG-based%20Emotion%20Recognition%20through%20Source%20Selection%20with%20Adversarial%20Strategy&body=Title%3A%20SSAS%3A%20Cross-subject%20EEG-based%20Emotion%20Recognition%20through%20Source%20Selection%20with%20Adversarial%20Strategy%0AAuthor%3A%20Yici%20Liu%20and%20Qi%20Wei%20Oung%20and%20Hoi%20Leong%20Lee%0AAbstract%3A%20Electroencephalographic%20%28EEG%29%20signals%20have%20long%20been%20applied%20in%20the%20field%20of%20affective%20brain-computer%20interfaces%20%28aBCIs%29.%20Cross-subject%20EEG-based%20emotion%20recognition%20has%20demonstrated%20significant%20potential%20in%20practical%20applications%20due%20to%20its%20suitability%20across%20diverse%20people.%20However%2C%20most%20studies%20on%20cross-subject%20EEG-based%20emotion%20recognition%20neglect%20the%20presence%20of%20inter-individual%20variability%20and%20negative%20transfer%20phenomena%20during%20model%20training.%20To%20address%20this%20issue%2C%20a%20cross-subject%20EEG-based%20emotion%20recognition%20through%20source%20selection%20with%20adversarial%20strategy%20is%20introduced%20in%20this%20paper.%20The%20proposed%20method%20comprises%20two%20modules%3A%20the%20source%20selection%20network%20%28SS%29%20and%20the%20adversarial%20strategies%20network%20%28AS%29.%20The%20SS%20uses%20domain%20labels%20to%20reverse-engineer%20the%20training%20process%20of%20domain%20adaptation.%20Its%20key%20idea%20is%20to%20disrupt%20class%20separability%20and%20magnify%20inter-domain%20differences%2C%20thereby%20raising%20the%20classification%20difficulty%20and%20forcing%20the%20model%20to%20learn%20domain-invariant%20yet%20emotion-relevant%20representations.%20The%20AS%20gets%20the%20source%20domain%20selection%20results%20and%20the%20pretrained%20domain%20discriminators%20from%20SS.%20The%20pretrained%20domain%20discriminators%20compute%20a%20novel%20loss%20aimed%20at%20enhancing%20the%20performance%20of%20domain%20classification%20during%20adversarial%20training%2C%20ensuring%20the%20balance%20of%20adversarial%20strategies.%20This%20paper%20provides%20theoretical%20insights%20into%20the%20proposed%20method%20and%20achieves%20outstanding%20performance%20on%20two%20EEG-based%20emotion%20datasets%2C%20SEED%20and%20SEED-IV.%20The%20code%20can%20be%20found%20at%20https%3A//github.com/liuyici/SSAS.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSAS%253A%2520Cross-subject%2520EEG-based%2520Emotion%2520Recognition%2520through%2520Source%2520Selection%2520with%2520Adversarial%2520Strategy%26entry.906535625%3DYici%2520Liu%2520and%2520Qi%2520Wei%2520Oung%2520and%2520Hoi%2520Leong%2520Lee%26entry.1292438233%3DElectroencephalographic%2520%2528EEG%2529%2520signals%2520have%2520long%2520been%2520applied%2520in%2520the%2520field%2520of%2520affective%2520brain-computer%2520interfaces%2520%2528aBCIs%2529.%2520Cross-subject%2520EEG-based%2520emotion%2520recognition%2520has%2520demonstrated%2520significant%2520potential%2520in%2520practical%2520applications%2520due%2520to%2520its%2520suitability%2520across%2520diverse%2520people.%2520However%252C%2520most%2520studies%2520on%2520cross-subject%2520EEG-based%2520emotion%2520recognition%2520neglect%2520the%2520presence%2520of%2520inter-individual%2520variability%2520and%2520negative%2520transfer%2520phenomena%2520during%2520model%2520training.%2520To%2520address%2520this%2520issue%252C%2520a%2520cross-subject%2520EEG-based%2520emotion%2520recognition%2520through%2520source%2520selection%2520with%2520adversarial%2520strategy%2520is%2520introduced%2520in%2520this%2520paper.%2520The%2520proposed%2520method%2520comprises%2520two%2520modules%253A%2520the%2520source%2520selection%2520network%2520%2528SS%2529%2520and%2520the%2520adversarial%2520strategies%2520network%2520%2528AS%2529.%2520The%2520SS%2520uses%2520domain%2520labels%2520to%2520reverse-engineer%2520the%2520training%2520process%2520of%2520domain%2520adaptation.%2520Its%2520key%2520idea%2520is%2520to%2520disrupt%2520class%2520separability%2520and%2520magnify%2520inter-domain%2520differences%252C%2520thereby%2520raising%2520the%2520classification%2520difficulty%2520and%2520forcing%2520the%2520model%2520to%2520learn%2520domain-invariant%2520yet%2520emotion-relevant%2520representations.%2520The%2520AS%2520gets%2520the%2520source%2520domain%2520selection%2520results%2520and%2520the%2520pretrained%2520domain%2520discriminators%2520from%2520SS.%2520The%2520pretrained%2520domain%2520discriminators%2520compute%2520a%2520novel%2520loss%2520aimed%2520at%2520enhancing%2520the%2520performance%2520of%2520domain%2520classification%2520during%2520adversarial%2520training%252C%2520ensuring%2520the%2520balance%2520of%2520adversarial%2520strategies.%2520This%2520paper%2520provides%2520theoretical%2520insights%2520into%2520the%2520proposed%2520method%2520and%2520achieves%2520outstanding%2520performance%2520on%2520two%2520EEG-based%2520emotion%2520datasets%252C%2520SEED%2520and%2520SEED-IV.%2520The%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/liuyici/SSAS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSAS%3A%20Cross-subject%20EEG-based%20Emotion%20Recognition%20through%20Source%20Selection%20with%20Adversarial%20Strategy&entry.906535625=Yici%20Liu%20and%20Qi%20Wei%20Oung%20and%20Hoi%20Leong%20Lee&entry.1292438233=Electroencephalographic%20%28EEG%29%20signals%20have%20long%20been%20applied%20in%20the%20field%20of%20affective%20brain-computer%20interfaces%20%28aBCIs%29.%20Cross-subject%20EEG-based%20emotion%20recognition%20has%20demonstrated%20significant%20potential%20in%20practical%20applications%20due%20to%20its%20suitability%20across%20diverse%20people.%20However%2C%20most%20studies%20on%20cross-subject%20EEG-based%20emotion%20recognition%20neglect%20the%20presence%20of%20inter-individual%20variability%20and%20negative%20transfer%20phenomena%20during%20model%20training.%20To%20address%20this%20issue%2C%20a%20cross-subject%20EEG-based%20emotion%20recognition%20through%20source%20selection%20with%20adversarial%20strategy%20is%20introduced%20in%20this%20paper.%20The%20proposed%20method%20comprises%20two%20modules%3A%20the%20source%20selection%20network%20%28SS%29%20and%20the%20adversarial%20strategies%20network%20%28AS%29.%20The%20SS%20uses%20domain%20labels%20to%20reverse-engineer%20the%20training%20process%20of%20domain%20adaptation.%20Its%20key%20idea%20is%20to%20disrupt%20class%20separability%20and%20magnify%20inter-domain%20differences%2C%20thereby%20raising%20the%20classification%20difficulty%20and%20forcing%20the%20model%20to%20learn%20domain-invariant%20yet%20emotion-relevant%20representations.%20The%20AS%20gets%20the%20source%20domain%20selection%20results%20and%20the%20pretrained%20domain%20discriminators%20from%20SS.%20The%20pretrained%20domain%20discriminators%20compute%20a%20novel%20loss%20aimed%20at%20enhancing%20the%20performance%20of%20domain%20classification%20during%20adversarial%20training%2C%20ensuring%20the%20balance%20of%20adversarial%20strategies.%20This%20paper%20provides%20theoretical%20insights%20into%20the%20proposed%20method%20and%20achieves%20outstanding%20performance%20on%20two%20EEG-based%20emotion%20datasets%2C%20SEED%20and%20SEED-IV.%20The%20code%20can%20be%20found%20at%20https%3A//github.com/liuyici/SSAS.&entry.1838667208=http%3A//arxiv.org/abs/2512.13458v1&entry.124074799=Read"},
{"title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models", "author": "Shweta Mahajan and Shreya Kadambi and Hoang Le and Munawar Hayat and Fatih Porikli", "abstract": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.", "link": "http://arxiv.org/abs/2512.13609v1", "date": "2025-12-15", "relevancy": 2.5578, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5157}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5146}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do-Undo%3A%20Generating%20and%20Reversing%20Physical%20Actions%20in%20Vision-Language%20Models&body=Title%3A%20Do-Undo%3A%20Generating%20and%20Reversing%20Physical%20Actions%20in%20Vision-Language%20Models%0AAuthor%3A%20Shweta%20Mahajan%20and%20Shreya%20Kadambi%20and%20Hoang%20Le%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli%0AAbstract%3A%20We%20introduce%20the%20Do-Undo%20task%20and%20benchmark%20to%20address%20a%20critical%20gap%20in%20vision-language%20models%3A%20understanding%20and%20generating%20physically%20plausible%20scene%20transformations%20driven%20by%20real-world%20actions.%20Unlike%20prior%20work%20focused%20on%20object-level%20edits%2C%20Do-Undo%20requires%20models%20to%20simulate%20the%20outcome%20of%20a%20physical%20action%20and%20then%20accurately%20reverse%20it%2C%20reflecting%20true%20cause-and-effect%20in%20the%20visual%20world.%20We%20curate%20a%20large-scale%20dataset%20of%20reversible%20actions%20from%20real-world%20videos%20and%20design%20a%20training%20strategy%20enforcing%20consistency%20for%20robust%20action%20grounding.%20Our%20experiments%20reveal%20that%20current%20models%20struggle%20with%20physical%20reversibility%2C%20underscoring%20the%20importance%20of%20this%20task%20for%20embodied%20AI%2C%20robotics%2C%20and%20physics-aware%20generative%20modeling.%20Do-Undo%20establishes%20an%20intuitive%20testbed%20for%20evaluating%20and%20advancing%20physical%20reasoning%20in%20multimodal%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo-Undo%253A%2520Generating%2520and%2520Reversing%2520Physical%2520Actions%2520in%2520Vision-Language%2520Models%26entry.906535625%3DShweta%2520Mahajan%2520and%2520Shreya%2520Kadambi%2520and%2520Hoang%2520Le%2520and%2520Munawar%2520Hayat%2520and%2520Fatih%2520Porikli%26entry.1292438233%3DWe%2520introduce%2520the%2520Do-Undo%2520task%2520and%2520benchmark%2520to%2520address%2520a%2520critical%2520gap%2520in%2520vision-language%2520models%253A%2520understanding%2520and%2520generating%2520physically%2520plausible%2520scene%2520transformations%2520driven%2520by%2520real-world%2520actions.%2520Unlike%2520prior%2520work%2520focused%2520on%2520object-level%2520edits%252C%2520Do-Undo%2520requires%2520models%2520to%2520simulate%2520the%2520outcome%2520of%2520a%2520physical%2520action%2520and%2520then%2520accurately%2520reverse%2520it%252C%2520reflecting%2520true%2520cause-and-effect%2520in%2520the%2520visual%2520world.%2520We%2520curate%2520a%2520large-scale%2520dataset%2520of%2520reversible%2520actions%2520from%2520real-world%2520videos%2520and%2520design%2520a%2520training%2520strategy%2520enforcing%2520consistency%2520for%2520robust%2520action%2520grounding.%2520Our%2520experiments%2520reveal%2520that%2520current%2520models%2520struggle%2520with%2520physical%2520reversibility%252C%2520underscoring%2520the%2520importance%2520of%2520this%2520task%2520for%2520embodied%2520AI%252C%2520robotics%252C%2520and%2520physics-aware%2520generative%2520modeling.%2520Do-Undo%2520establishes%2520an%2520intuitive%2520testbed%2520for%2520evaluating%2520and%2520advancing%2520physical%2520reasoning%2520in%2520multimodal%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do-Undo%3A%20Generating%20and%20Reversing%20Physical%20Actions%20in%20Vision-Language%20Models&entry.906535625=Shweta%20Mahajan%20and%20Shreya%20Kadambi%20and%20Hoang%20Le%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli&entry.1292438233=We%20introduce%20the%20Do-Undo%20task%20and%20benchmark%20to%20address%20a%20critical%20gap%20in%20vision-language%20models%3A%20understanding%20and%20generating%20physically%20plausible%20scene%20transformations%20driven%20by%20real-world%20actions.%20Unlike%20prior%20work%20focused%20on%20object-level%20edits%2C%20Do-Undo%20requires%20models%20to%20simulate%20the%20outcome%20of%20a%20physical%20action%20and%20then%20accurately%20reverse%20it%2C%20reflecting%20true%20cause-and-effect%20in%20the%20visual%20world.%20We%20curate%20a%20large-scale%20dataset%20of%20reversible%20actions%20from%20real-world%20videos%20and%20design%20a%20training%20strategy%20enforcing%20consistency%20for%20robust%20action%20grounding.%20Our%20experiments%20reveal%20that%20current%20models%20struggle%20with%20physical%20reversibility%2C%20underscoring%20the%20importance%20of%20this%20task%20for%20embodied%20AI%2C%20robotics%2C%20and%20physics-aware%20generative%20modeling.%20Do-Undo%20establishes%20an%20intuitive%20testbed%20for%20evaluating%20and%20advancing%20physical%20reasoning%20in%20multimodal%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.13609v1&entry.124074799=Read"},
{"title": "Instance-Level Composed Image Retrieval", "author": "Bill Psomas and George Retsinas and Nikos Efthymiadis and Panagiotis Filntisis and Yannis Avrithis and Petros Maragos and Ondrej Chum and Giorgos Tolias", "abstract": "The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.", "link": "http://arxiv.org/abs/2510.25387v2", "date": "2025-12-15", "relevancy": 2.5419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Level%20Composed%20Image%20Retrieval&body=Title%3A%20Instance-Level%20Composed%20Image%20Retrieval%0AAuthor%3A%20Bill%20Psomas%20and%20George%20Retsinas%20and%20Nikos%20Efthymiadis%20and%20Panagiotis%20Filntisis%20and%20Yannis%20Avrithis%20and%20Petros%20Maragos%20and%20Ondrej%20Chum%20and%20Giorgos%20Tolias%0AAbstract%3A%20The%20progress%20of%20composed%20image%20retrieval%20%28CIR%29%2C%20a%20popular%20research%20direction%20in%20image%20retrieval%2C%20where%20a%20combined%20visual%20and%20textual%20query%20is%20used%2C%20is%20held%20back%20by%20the%20absence%20of%20high-quality%20training%20and%20evaluation%20data.%20We%20introduce%20a%20new%20evaluation%20dataset%2C%20i-CIR%2C%20which%2C%20unlike%20existing%20datasets%2C%20focuses%20on%20an%20instance-level%20class%20definition.%20The%20goal%20is%20to%20retrieve%20images%20that%20contain%20the%20same%20particular%20object%20as%20the%20visual%20query%2C%20presented%20under%20a%20variety%20of%20modifications%20defined%20by%20textual%20queries.%20Its%20design%20and%20curation%20process%20keep%20the%20dataset%20compact%20to%20facilitate%20future%20research%2C%20while%20maintaining%20its%20challenge-comparable%20to%20retrieval%20among%20more%20than%2040M%20random%20distractors-through%20a%20semi-automated%20selection%20of%20hard%20negatives.%0A%20%20To%20overcome%20the%20challenge%20of%20obtaining%20clean%2C%20diverse%2C%20and%20suitable%20training%20data%2C%20we%20leverage%20pre-trained%20vision-and-language%20models%20%28VLMs%29%20in%20a%20training-free%20approach%20called%20BASIC.%20The%20method%20separately%20estimates%20query-image-to-image%20and%20query-text-to-image%20similarities%2C%20performing%20late%20fusion%20to%20upweight%20images%20that%20satisfy%20both%20queries%2C%20while%20down-weighting%20those%20that%20exhibit%20high%20similarity%20with%20only%20one%20of%20the%20two.%20Each%20individual%20similarity%20is%20further%20improved%20by%20a%20set%20of%20components%20that%20are%20simple%20and%20intuitive.%20BASIC%20sets%20a%20new%20state%20of%20the%20art%20on%20i-CIR%20but%20also%20on%20existing%20CIR%20datasets%20that%20follow%20a%20semantic-level%20class%20definition.%20Project%20page%3A%20https%3A//vrg.fel.cvut.cz/icir/.%0ALink%3A%20http%3A//arxiv.org/abs/2510.25387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Level%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DBill%2520Psomas%2520and%2520George%2520Retsinas%2520and%2520Nikos%2520Efthymiadis%2520and%2520Panagiotis%2520Filntisis%2520and%2520Yannis%2520Avrithis%2520and%2520Petros%2520Maragos%2520and%2520Ondrej%2520Chum%2520and%2520Giorgos%2520Tolias%26entry.1292438233%3DThe%2520progress%2520of%2520composed%2520image%2520retrieval%2520%2528CIR%2529%252C%2520a%2520popular%2520research%2520direction%2520in%2520image%2520retrieval%252C%2520where%2520a%2520combined%2520visual%2520and%2520textual%2520query%2520is%2520used%252C%2520is%2520held%2520back%2520by%2520the%2520absence%2520of%2520high-quality%2520training%2520and%2520evaluation%2520data.%2520We%2520introduce%2520a%2520new%2520evaluation%2520dataset%252C%2520i-CIR%252C%2520which%252C%2520unlike%2520existing%2520datasets%252C%2520focuses%2520on%2520an%2520instance-level%2520class%2520definition.%2520The%2520goal%2520is%2520to%2520retrieve%2520images%2520that%2520contain%2520the%2520same%2520particular%2520object%2520as%2520the%2520visual%2520query%252C%2520presented%2520under%2520a%2520variety%2520of%2520modifications%2520defined%2520by%2520textual%2520queries.%2520Its%2520design%2520and%2520curation%2520process%2520keep%2520the%2520dataset%2520compact%2520to%2520facilitate%2520future%2520research%252C%2520while%2520maintaining%2520its%2520challenge-comparable%2520to%2520retrieval%2520among%2520more%2520than%252040M%2520random%2520distractors-through%2520a%2520semi-automated%2520selection%2520of%2520hard%2520negatives.%250A%2520%2520To%2520overcome%2520the%2520challenge%2520of%2520obtaining%2520clean%252C%2520diverse%252C%2520and%2520suitable%2520training%2520data%252C%2520we%2520leverage%2520pre-trained%2520vision-and-language%2520models%2520%2528VLMs%2529%2520in%2520a%2520training-free%2520approach%2520called%2520BASIC.%2520The%2520method%2520separately%2520estimates%2520query-image-to-image%2520and%2520query-text-to-image%2520similarities%252C%2520performing%2520late%2520fusion%2520to%2520upweight%2520images%2520that%2520satisfy%2520both%2520queries%252C%2520while%2520down-weighting%2520those%2520that%2520exhibit%2520high%2520similarity%2520with%2520only%2520one%2520of%2520the%2520two.%2520Each%2520individual%2520similarity%2520is%2520further%2520improved%2520by%2520a%2520set%2520of%2520components%2520that%2520are%2520simple%2520and%2520intuitive.%2520BASIC%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520i-CIR%2520but%2520also%2520on%2520existing%2520CIR%2520datasets%2520that%2520follow%2520a%2520semantic-level%2520class%2520definition.%2520Project%2520page%253A%2520https%253A//vrg.fel.cvut.cz/icir/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Level%20Composed%20Image%20Retrieval&entry.906535625=Bill%20Psomas%20and%20George%20Retsinas%20and%20Nikos%20Efthymiadis%20and%20Panagiotis%20Filntisis%20and%20Yannis%20Avrithis%20and%20Petros%20Maragos%20and%20Ondrej%20Chum%20and%20Giorgos%20Tolias&entry.1292438233=The%20progress%20of%20composed%20image%20retrieval%20%28CIR%29%2C%20a%20popular%20research%20direction%20in%20image%20retrieval%2C%20where%20a%20combined%20visual%20and%20textual%20query%20is%20used%2C%20is%20held%20back%20by%20the%20absence%20of%20high-quality%20training%20and%20evaluation%20data.%20We%20introduce%20a%20new%20evaluation%20dataset%2C%20i-CIR%2C%20which%2C%20unlike%20existing%20datasets%2C%20focuses%20on%20an%20instance-level%20class%20definition.%20The%20goal%20is%20to%20retrieve%20images%20that%20contain%20the%20same%20particular%20object%20as%20the%20visual%20query%2C%20presented%20under%20a%20variety%20of%20modifications%20defined%20by%20textual%20queries.%20Its%20design%20and%20curation%20process%20keep%20the%20dataset%20compact%20to%20facilitate%20future%20research%2C%20while%20maintaining%20its%20challenge-comparable%20to%20retrieval%20among%20more%20than%2040M%20random%20distractors-through%20a%20semi-automated%20selection%20of%20hard%20negatives.%0A%20%20To%20overcome%20the%20challenge%20of%20obtaining%20clean%2C%20diverse%2C%20and%20suitable%20training%20data%2C%20we%20leverage%20pre-trained%20vision-and-language%20models%20%28VLMs%29%20in%20a%20training-free%20approach%20called%20BASIC.%20The%20method%20separately%20estimates%20query-image-to-image%20and%20query-text-to-image%20similarities%2C%20performing%20late%20fusion%20to%20upweight%20images%20that%20satisfy%20both%20queries%2C%20while%20down-weighting%20those%20that%20exhibit%20high%20similarity%20with%20only%20one%20of%20the%20two.%20Each%20individual%20similarity%20is%20further%20improved%20by%20a%20set%20of%20components%20that%20are%20simple%20and%20intuitive.%20BASIC%20sets%20a%20new%20state%20of%20the%20art%20on%20i-CIR%20but%20also%20on%20existing%20CIR%20datasets%20that%20follow%20a%20semantic-level%20class%20definition.%20Project%20page%3A%20https%3A//vrg.fel.cvut.cz/icir/.&entry.1838667208=http%3A//arxiv.org/abs/2510.25387v2&entry.124074799=Read"},
{"title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability", "author": "Leonard Bereska and Zoe Tzifa-Kratira and Reza Samavi and Efstratios Gavves", "abstract": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.", "link": "http://arxiv.org/abs/2512.13568v1", "date": "2025-12-15", "relevancy": 2.507, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.526}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superposition%20as%20Lossy%20Compression%3A%20Measure%20with%20Sparse%20Autoencoders%20and%20Connect%20to%20Adversarial%20Vulnerability&body=Title%3A%20Superposition%20as%20Lossy%20Compression%3A%20Measure%20with%20Sparse%20Autoencoders%20and%20Connect%20to%20Adversarial%20Vulnerability%0AAuthor%3A%20Leonard%20Bereska%20and%20Zoe%20Tzifa-Kratira%20and%20Reza%20Samavi%20and%20Efstratios%20Gavves%0AAbstract%3A%20Neural%20networks%20achieve%20remarkable%20performance%20through%20superposition%3A%20encoding%20multiple%20features%20as%20overlapping%20directions%20in%20activation%20space%20rather%20than%20dedicating%20individual%20neurons%20to%20each%20feature.%20This%20challenges%20interpretability%2C%20yet%20we%20lack%20principled%20methods%20to%20measure%20superposition.%20We%20present%20an%20information-theoretic%20framework%20measuring%20a%20neural%20representation%27s%20effective%20degrees%20of%20freedom.%20We%20apply%20Shannon%20entropy%20to%20sparse%20autoencoder%20activations%20to%20compute%20the%20number%20of%20effective%20features%20as%20the%20minimum%20neurons%20needed%20for%20interference-free%20encoding.%20Equivalently%2C%20this%20measures%20how%20many%20%22virtual%20neurons%22%20the%20network%20simulates%20through%20superposition.%20When%20networks%20encode%20more%20effective%20features%20than%20actual%20neurons%2C%20they%20must%20accept%20interference%20as%20the%20price%20of%20compression.%20Our%20metric%20strongly%20correlates%20with%20ground%20truth%20in%20toy%20models%2C%20detects%20minimal%20superposition%20in%20algorithmic%20tasks%2C%20and%20reveals%20systematic%20reduction%20under%20dropout.%20Layer-wise%20patterns%20mirror%20intrinsic%20dimensionality%20studies%20on%20Pythia-70M.%20The%20metric%20also%20captures%20developmental%20dynamics%2C%20detecting%20sharp%20feature%20consolidation%20during%20grokking.%20Surprisingly%2C%20adversarial%20training%20can%20increase%20effective%20features%20while%20improving%20robustness%2C%20contradicting%20the%20hypothesis%20that%20superposition%20causes%20vulnerability.%20Instead%2C%20the%20effect%20depends%20on%20task%20complexity%20and%20network%20capacity%3A%20simple%20tasks%20with%20ample%20capacity%20allow%20feature%20expansion%20%28abundance%20regime%29%2C%20while%20complex%20tasks%20or%20limited%20capacity%20force%20reduction%20%28scarcity%20regime%29.%20By%20defining%20superposition%20as%20lossy%20compression%2C%20this%20work%20enables%20principled%20measurement%20of%20how%20neural%20networks%20organize%20information%20under%20computational%20constraints%2C%20connecting%20superposition%20to%20adversarial%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperposition%2520as%2520Lossy%2520Compression%253A%2520Measure%2520with%2520Sparse%2520Autoencoders%2520and%2520Connect%2520to%2520Adversarial%2520Vulnerability%26entry.906535625%3DLeonard%2520Bereska%2520and%2520Zoe%2520Tzifa-Kratira%2520and%2520Reza%2520Samavi%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3DNeural%2520networks%2520achieve%2520remarkable%2520performance%2520through%2520superposition%253A%2520encoding%2520multiple%2520features%2520as%2520overlapping%2520directions%2520in%2520activation%2520space%2520rather%2520than%2520dedicating%2520individual%2520neurons%2520to%2520each%2520feature.%2520This%2520challenges%2520interpretability%252C%2520yet%2520we%2520lack%2520principled%2520methods%2520to%2520measure%2520superposition.%2520We%2520present%2520an%2520information-theoretic%2520framework%2520measuring%2520a%2520neural%2520representation%2527s%2520effective%2520degrees%2520of%2520freedom.%2520We%2520apply%2520Shannon%2520entropy%2520to%2520sparse%2520autoencoder%2520activations%2520to%2520compute%2520the%2520number%2520of%2520effective%2520features%2520as%2520the%2520minimum%2520neurons%2520needed%2520for%2520interference-free%2520encoding.%2520Equivalently%252C%2520this%2520measures%2520how%2520many%2520%2522virtual%2520neurons%2522%2520the%2520network%2520simulates%2520through%2520superposition.%2520When%2520networks%2520encode%2520more%2520effective%2520features%2520than%2520actual%2520neurons%252C%2520they%2520must%2520accept%2520interference%2520as%2520the%2520price%2520of%2520compression.%2520Our%2520metric%2520strongly%2520correlates%2520with%2520ground%2520truth%2520in%2520toy%2520models%252C%2520detects%2520minimal%2520superposition%2520in%2520algorithmic%2520tasks%252C%2520and%2520reveals%2520systematic%2520reduction%2520under%2520dropout.%2520Layer-wise%2520patterns%2520mirror%2520intrinsic%2520dimensionality%2520studies%2520on%2520Pythia-70M.%2520The%2520metric%2520also%2520captures%2520developmental%2520dynamics%252C%2520detecting%2520sharp%2520feature%2520consolidation%2520during%2520grokking.%2520Surprisingly%252C%2520adversarial%2520training%2520can%2520increase%2520effective%2520features%2520while%2520improving%2520robustness%252C%2520contradicting%2520the%2520hypothesis%2520that%2520superposition%2520causes%2520vulnerability.%2520Instead%252C%2520the%2520effect%2520depends%2520on%2520task%2520complexity%2520and%2520network%2520capacity%253A%2520simple%2520tasks%2520with%2520ample%2520capacity%2520allow%2520feature%2520expansion%2520%2528abundance%2520regime%2529%252C%2520while%2520complex%2520tasks%2520or%2520limited%2520capacity%2520force%2520reduction%2520%2528scarcity%2520regime%2529.%2520By%2520defining%2520superposition%2520as%2520lossy%2520compression%252C%2520this%2520work%2520enables%2520principled%2520measurement%2520of%2520how%2520neural%2520networks%2520organize%2520information%2520under%2520computational%2520constraints%252C%2520connecting%2520superposition%2520to%2520adversarial%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superposition%20as%20Lossy%20Compression%3A%20Measure%20with%20Sparse%20Autoencoders%20and%20Connect%20to%20Adversarial%20Vulnerability&entry.906535625=Leonard%20Bereska%20and%20Zoe%20Tzifa-Kratira%20and%20Reza%20Samavi%20and%20Efstratios%20Gavves&entry.1292438233=Neural%20networks%20achieve%20remarkable%20performance%20through%20superposition%3A%20encoding%20multiple%20features%20as%20overlapping%20directions%20in%20activation%20space%20rather%20than%20dedicating%20individual%20neurons%20to%20each%20feature.%20This%20challenges%20interpretability%2C%20yet%20we%20lack%20principled%20methods%20to%20measure%20superposition.%20We%20present%20an%20information-theoretic%20framework%20measuring%20a%20neural%20representation%27s%20effective%20degrees%20of%20freedom.%20We%20apply%20Shannon%20entropy%20to%20sparse%20autoencoder%20activations%20to%20compute%20the%20number%20of%20effective%20features%20as%20the%20minimum%20neurons%20needed%20for%20interference-free%20encoding.%20Equivalently%2C%20this%20measures%20how%20many%20%22virtual%20neurons%22%20the%20network%20simulates%20through%20superposition.%20When%20networks%20encode%20more%20effective%20features%20than%20actual%20neurons%2C%20they%20must%20accept%20interference%20as%20the%20price%20of%20compression.%20Our%20metric%20strongly%20correlates%20with%20ground%20truth%20in%20toy%20models%2C%20detects%20minimal%20superposition%20in%20algorithmic%20tasks%2C%20and%20reveals%20systematic%20reduction%20under%20dropout.%20Layer-wise%20patterns%20mirror%20intrinsic%20dimensionality%20studies%20on%20Pythia-70M.%20The%20metric%20also%20captures%20developmental%20dynamics%2C%20detecting%20sharp%20feature%20consolidation%20during%20grokking.%20Surprisingly%2C%20adversarial%20training%20can%20increase%20effective%20features%20while%20improving%20robustness%2C%20contradicting%20the%20hypothesis%20that%20superposition%20causes%20vulnerability.%20Instead%2C%20the%20effect%20depends%20on%20task%20complexity%20and%20network%20capacity%3A%20simple%20tasks%20with%20ample%20capacity%20allow%20feature%20expansion%20%28abundance%20regime%29%2C%20while%20complex%20tasks%20or%20limited%20capacity%20force%20reduction%20%28scarcity%20regime%29.%20By%20defining%20superposition%20as%20lossy%20compression%2C%20this%20work%20enables%20principled%20measurement%20of%20how%20neural%20networks%20organize%20information%20under%20computational%20constraints%2C%20connecting%20superposition%20to%20adversarial%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2512.13568v1&entry.124074799=Read"},
{"title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction", "author": "Robert Reed and Morteza Lahijanian and Luca Laurenti", "abstract": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.", "link": "http://arxiv.org/abs/2512.13593v1", "date": "2025-12-15", "relevancy": 2.5055, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction&body=Title%3A%20Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction%0AAuthor%3A%20Robert%20Reed%20and%20Morteza%20Lahijanian%20and%20Luca%20Laurenti%0AAbstract%3A%20Finite%20Abstraction%20methods%20provide%20a%20powerful%20formal%20framework%20for%20proving%20that%20systems%20satisfy%20their%20specifications.%20However%2C%20these%20techniques%20face%20scalability%20challenges%20for%20high-dimensional%20systems%2C%20as%20they%20rely%20on%20state-space%20discretization%20which%20grows%20exponentially%20with%20dimension.%20Learning-based%20approaches%20to%20dimensionality%20reduction%2C%20utilizing%20neural%20networks%20and%20autoencoders%2C%20have%20shown%20great%20potential%20to%20alleviate%20this%20problem.%20However%2C%20ensuring%20the%20correctness%20of%20the%20resulting%20verification%20results%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%20provide%20a%20formal%20approach%20to%20reduce%20the%20dimensionality%20of%20systems%20via%20convex%20autoencoders%20and%20learn%20the%20dynamics%20in%20the%20latent%20space%20through%20a%20kernel-based%20method.%20We%20then%20construct%20a%20finite%20abstraction%20from%20the%20learned%20model%20in%20the%20latent%20space%20and%20guarantee%20that%20the%20abstraction%20contains%20the%20true%20behaviors%20of%20the%20original%20system.%20We%20show%20that%20the%20verification%20results%20in%20the%20latent%20space%20can%20be%20mapped%20back%20to%20the%20original%20system.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20multiple%20systems%2C%20including%20a%2026D%20system%20controlled%20by%20a%20neural%20network%2C%20showing%20significant%20scalability%20improvements%20without%20loss%20of%20rigor.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Formal%2520Verification%2520via%2520Autoencoder%2520Latent%2520Space%2520Abstraction%26entry.906535625%3DRobert%2520Reed%2520and%2520Morteza%2520Lahijanian%2520and%2520Luca%2520Laurenti%26entry.1292438233%3DFinite%2520Abstraction%2520methods%2520provide%2520a%2520powerful%2520formal%2520framework%2520for%2520proving%2520that%2520systems%2520satisfy%2520their%2520specifications.%2520However%252C%2520these%2520techniques%2520face%2520scalability%2520challenges%2520for%2520high-dimensional%2520systems%252C%2520as%2520they%2520rely%2520on%2520state-space%2520discretization%2520which%2520grows%2520exponentially%2520with%2520dimension.%2520Learning-based%2520approaches%2520to%2520dimensionality%2520reduction%252C%2520utilizing%2520neural%2520networks%2520and%2520autoencoders%252C%2520have%2520shown%2520great%2520potential%2520to%2520alleviate%2520this%2520problem.%2520However%252C%2520ensuring%2520the%2520correctness%2520of%2520the%2520resulting%2520verification%2520results%2520remains%2520an%2520open%2520question.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520formal%2520approach%2520to%2520reduce%2520the%2520dimensionality%2520of%2520systems%2520via%2520convex%2520autoencoders%2520and%2520learn%2520the%2520dynamics%2520in%2520the%2520latent%2520space%2520through%2520a%2520kernel-based%2520method.%2520We%2520then%2520construct%2520a%2520finite%2520abstraction%2520from%2520the%2520learned%2520model%2520in%2520the%2520latent%2520space%2520and%2520guarantee%2520that%2520the%2520abstraction%2520contains%2520the%2520true%2520behaviors%2520of%2520the%2520original%2520system.%2520We%2520show%2520that%2520the%2520verification%2520results%2520in%2520the%2520latent%2520space%2520can%2520be%2520mapped%2520back%2520to%2520the%2520original%2520system.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520multiple%2520systems%252C%2520including%2520a%252026D%2520system%2520controlled%2520by%2520a%2520neural%2520network%252C%2520showing%2520significant%2520scalability%2520improvements%2520without%2520loss%2520of%2520rigor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction&entry.906535625=Robert%20Reed%20and%20Morteza%20Lahijanian%20and%20Luca%20Laurenti&entry.1292438233=Finite%20Abstraction%20methods%20provide%20a%20powerful%20formal%20framework%20for%20proving%20that%20systems%20satisfy%20their%20specifications.%20However%2C%20these%20techniques%20face%20scalability%20challenges%20for%20high-dimensional%20systems%2C%20as%20they%20rely%20on%20state-space%20discretization%20which%20grows%20exponentially%20with%20dimension.%20Learning-based%20approaches%20to%20dimensionality%20reduction%2C%20utilizing%20neural%20networks%20and%20autoencoders%2C%20have%20shown%20great%20potential%20to%20alleviate%20this%20problem.%20However%2C%20ensuring%20the%20correctness%20of%20the%20resulting%20verification%20results%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%20provide%20a%20formal%20approach%20to%20reduce%20the%20dimensionality%20of%20systems%20via%20convex%20autoencoders%20and%20learn%20the%20dynamics%20in%20the%20latent%20space%20through%20a%20kernel-based%20method.%20We%20then%20construct%20a%20finite%20abstraction%20from%20the%20learned%20model%20in%20the%20latent%20space%20and%20guarantee%20that%20the%20abstraction%20contains%20the%20true%20behaviors%20of%20the%20original%20system.%20We%20show%20that%20the%20verification%20results%20in%20the%20latent%20space%20can%20be%20mapped%20back%20to%20the%20original%20system.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20multiple%20systems%2C%20including%20a%2026D%20system%20controlled%20by%20a%20neural%20network%2C%20showing%20significant%20scalability%20improvements%20without%20loss%20of%20rigor.&entry.1838667208=http%3A//arxiv.org/abs/2512.13593v1&entry.124074799=Read"},
{"title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "author": "Jianxiong Gao and Zhaoxi Chen and Xian Liu and Junhao Zhuang and Chengming Xu and Jianfeng Feng and Yu Qiao and Yanwei Fu and Chenyang Si and Ziwei Liu", "abstract": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "link": "http://arxiv.org/abs/2512.13604v1", "date": "2025-12-15", "relevancy": 2.5016, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6675}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6237}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVie%202%3A%20Multimodal%20Controllable%20Ultra-Long%20Video%20World%20Model&body=Title%3A%20LongVie%202%3A%20Multimodal%20Controllable%20Ultra-Long%20Video%20World%20Model%0AAuthor%3A%20Jianxiong%20Gao%20and%20Zhaoxi%20Chen%20and%20Xian%20Liu%20and%20Junhao%20Zhuang%20and%20Chengming%20Xu%20and%20Jianfeng%20Feng%20and%20Yu%20Qiao%20and%20Yanwei%20Fu%20and%20Chenyang%20Si%20and%20Ziwei%20Liu%0AAbstract%3A%20Building%20video%20world%20models%20upon%20pretrained%20video%20generation%20systems%20represents%20an%20important%20yet%20challenging%20step%20toward%20general%20spatiotemporal%20intelligence.%20A%20world%20model%20should%20possess%20three%20essential%20properties%3A%20controllability%2C%20long-term%20visual%20quality%2C%20and%20temporal%20consistency.%20To%20this%20end%2C%20we%20take%20a%20progressive%20approach-first%20enhancing%20controllability%20and%20then%20extending%20toward%20long-term%2C%20high-quality%20generation.%20We%20present%20LongVie%202%2C%20an%20end-to-end%20autoregressive%20framework%20trained%20in%20three%20stages%3A%20%281%29%20Multi-modal%20guidance%2C%20which%20integrates%20dense%20and%20sparse%20control%20signals%20to%20provide%20implicit%20world-level%20supervision%20and%20improve%20controllability%3B%20%282%29%20Degradation-aware%20training%20on%20the%20input%20frame%2C%20bridging%20the%20gap%20between%20training%20and%20long-term%20inference%20to%20maintain%20high%20visual%20quality%3B%20and%20%283%29%20History-context%20guidance%2C%20which%20aligns%20contextual%20information%20across%20adjacent%20clips%20to%20ensure%20temporal%20consistency.%20We%20further%20introduce%20LongVGenBench%2C%20a%20comprehensive%20benchmark%20comprising%20100%20high-resolution%20one-minute%20videos%20covering%20diverse%20real-world%20and%20synthetic%20environments.%20Extensive%20experiments%20demonstrate%20that%20LongVie%202%20achieves%20state-of-the-art%20performance%20in%20long-range%20controllability%2C%20temporal%20coherence%2C%20and%20visual%20fidelity%2C%20and%20supports%20continuous%20video%20generation%20lasting%20up%20to%20five%20minutes%2C%20marking%20a%20significant%20step%20toward%20unified%20video%20world%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVie%25202%253A%2520Multimodal%2520Controllable%2520Ultra-Long%2520Video%2520World%2520Model%26entry.906535625%3DJianxiong%2520Gao%2520and%2520Zhaoxi%2520Chen%2520and%2520Xian%2520Liu%2520and%2520Junhao%2520Zhuang%2520and%2520Chengming%2520Xu%2520and%2520Jianfeng%2520Feng%2520and%2520Yu%2520Qiao%2520and%2520Yanwei%2520Fu%2520and%2520Chenyang%2520Si%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DBuilding%2520video%2520world%2520models%2520upon%2520pretrained%2520video%2520generation%2520systems%2520represents%2520an%2520important%2520yet%2520challenging%2520step%2520toward%2520general%2520spatiotemporal%2520intelligence.%2520A%2520world%2520model%2520should%2520possess%2520three%2520essential%2520properties%253A%2520controllability%252C%2520long-term%2520visual%2520quality%252C%2520and%2520temporal%2520consistency.%2520To%2520this%2520end%252C%2520we%2520take%2520a%2520progressive%2520approach-first%2520enhancing%2520controllability%2520and%2520then%2520extending%2520toward%2520long-term%252C%2520high-quality%2520generation.%2520We%2520present%2520LongVie%25202%252C%2520an%2520end-to-end%2520autoregressive%2520framework%2520trained%2520in%2520three%2520stages%253A%2520%25281%2529%2520Multi-modal%2520guidance%252C%2520which%2520integrates%2520dense%2520and%2520sparse%2520control%2520signals%2520to%2520provide%2520implicit%2520world-level%2520supervision%2520and%2520improve%2520controllability%253B%2520%25282%2529%2520Degradation-aware%2520training%2520on%2520the%2520input%2520frame%252C%2520bridging%2520the%2520gap%2520between%2520training%2520and%2520long-term%2520inference%2520to%2520maintain%2520high%2520visual%2520quality%253B%2520and%2520%25283%2529%2520History-context%2520guidance%252C%2520which%2520aligns%2520contextual%2520information%2520across%2520adjacent%2520clips%2520to%2520ensure%2520temporal%2520consistency.%2520We%2520further%2520introduce%2520LongVGenBench%252C%2520a%2520comprehensive%2520benchmark%2520comprising%2520100%2520high-resolution%2520one-minute%2520videos%2520covering%2520diverse%2520real-world%2520and%2520synthetic%2520environments.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LongVie%25202%2520achieves%2520state-of-the-art%2520performance%2520in%2520long-range%2520controllability%252C%2520temporal%2520coherence%252C%2520and%2520visual%2520fidelity%252C%2520and%2520supports%2520continuous%2520video%2520generation%2520lasting%2520up%2520to%2520five%2520minutes%252C%2520marking%2520a%2520significant%2520step%2520toward%2520unified%2520video%2520world%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVie%202%3A%20Multimodal%20Controllable%20Ultra-Long%20Video%20World%20Model&entry.906535625=Jianxiong%20Gao%20and%20Zhaoxi%20Chen%20and%20Xian%20Liu%20and%20Junhao%20Zhuang%20and%20Chengming%20Xu%20and%20Jianfeng%20Feng%20and%20Yu%20Qiao%20and%20Yanwei%20Fu%20and%20Chenyang%20Si%20and%20Ziwei%20Liu&entry.1292438233=Building%20video%20world%20models%20upon%20pretrained%20video%20generation%20systems%20represents%20an%20important%20yet%20challenging%20step%20toward%20general%20spatiotemporal%20intelligence.%20A%20world%20model%20should%20possess%20three%20essential%20properties%3A%20controllability%2C%20long-term%20visual%20quality%2C%20and%20temporal%20consistency.%20To%20this%20end%2C%20we%20take%20a%20progressive%20approach-first%20enhancing%20controllability%20and%20then%20extending%20toward%20long-term%2C%20high-quality%20generation.%20We%20present%20LongVie%202%2C%20an%20end-to-end%20autoregressive%20framework%20trained%20in%20three%20stages%3A%20%281%29%20Multi-modal%20guidance%2C%20which%20integrates%20dense%20and%20sparse%20control%20signals%20to%20provide%20implicit%20world-level%20supervision%20and%20improve%20controllability%3B%20%282%29%20Degradation-aware%20training%20on%20the%20input%20frame%2C%20bridging%20the%20gap%20between%20training%20and%20long-term%20inference%20to%20maintain%20high%20visual%20quality%3B%20and%20%283%29%20History-context%20guidance%2C%20which%20aligns%20contextual%20information%20across%20adjacent%20clips%20to%20ensure%20temporal%20consistency.%20We%20further%20introduce%20LongVGenBench%2C%20a%20comprehensive%20benchmark%20comprising%20100%20high-resolution%20one-minute%20videos%20covering%20diverse%20real-world%20and%20synthetic%20environments.%20Extensive%20experiments%20demonstrate%20that%20LongVie%202%20achieves%20state-of-the-art%20performance%20in%20long-range%20controllability%2C%20temporal%20coherence%2C%20and%20visual%20fidelity%2C%20and%20supports%20continuous%20video%20generation%20lasting%20up%20to%20five%20minutes%2C%20marking%20a%20significant%20step%20toward%20unified%20video%20world%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.13604v1&entry.124074799=Read"},
{"title": "MALLM: Multi-Agent Large Language Models Framework", "author": "Jonas Becker and Lars Benedikt Kaesberg and Niklas Bauer and Jan Philip Wahle and Terry Ruas and Bela Gipp", "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.", "link": "http://arxiv.org/abs/2509.11656v3", "date": "2025-12-15", "relevancy": 2.5012, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MALLM%3A%20Multi-Agent%20Large%20Language%20Models%20Framework&body=Title%3A%20MALLM%3A%20Multi-Agent%20Large%20Language%20Models%20Framework%0AAuthor%3A%20Jonas%20Becker%20and%20Lars%20Benedikt%20Kaesberg%20and%20Niklas%20Bauer%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20Multi-agent%20debate%20%28MAD%29%20has%20demonstrated%20the%20ability%20to%20augment%20collective%20intelligence%20by%20scaling%20test-time%20compute%20and%20leveraging%20expertise.%20Current%20frameworks%20for%20multi-agent%20debate%20are%20often%20designed%20towards%20tool%20use%2C%20lack%20integrated%20evaluation%2C%20or%20provide%20limited%20configurability%20of%20agent%20personas%2C%20response%20generators%2C%20discussion%20paradigms%2C%20and%20decision%20protocols.%20We%20introduce%20MALLM%20%28Multi-Agent%20Large%20Language%20Models%29%2C%20an%20open-source%20framework%20that%20enables%20systematic%20analysis%20of%20MAD%20components.%20MALLM%20offers%20more%20than%20144%20unique%20configurations%20of%20MAD%2C%20including%20%281%29%20agent%20personas%20%28e.g.%2C%20Expert%2C%20Personality%29%2C%20%282%29%20response%20generators%20%28e.g.%2C%20Critical%2C%20Reasoning%29%2C%20%283%29%20discussion%20paradigms%20%28e.g.%2C%20Memory%2C%20Relay%29%2C%20and%20%284%29%20decision%20protocols%20%28e.g.%2C%20Voting%2C%20Consensus%29.%20MALLM%20uses%20simple%20configuration%20files%20to%20define%20a%20debate.%20Furthermore%2C%20MALLM%20can%20load%20any%20textual%20Hugging%20Face%20dataset%20%28e.g.%2C%20MMLU-Pro%2C%20WinoGrande%29%20and%20provides%20an%20evaluation%20pipeline%20for%20easy%20comparison%20of%20MAD%20configurations.%20MALLM%20enables%20researchers%20to%20systematically%20configure%2C%20run%2C%20and%20evaluate%20debates%20for%20their%20problems%2C%20facilitating%20the%20understanding%20of%20the%20components%20and%20their%20interplay.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11656v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMALLM%253A%2520Multi-Agent%2520Large%2520Language%2520Models%2520Framework%26entry.906535625%3DJonas%2520Becker%2520and%2520Lars%2520Benedikt%2520Kaesberg%2520and%2520Niklas%2520Bauer%2520and%2520Jan%2520Philip%2520Wahle%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3DMulti-agent%2520debate%2520%2528MAD%2529%2520has%2520demonstrated%2520the%2520ability%2520to%2520augment%2520collective%2520intelligence%2520by%2520scaling%2520test-time%2520compute%2520and%2520leveraging%2520expertise.%2520Current%2520frameworks%2520for%2520multi-agent%2520debate%2520are%2520often%2520designed%2520towards%2520tool%2520use%252C%2520lack%2520integrated%2520evaluation%252C%2520or%2520provide%2520limited%2520configurability%2520of%2520agent%2520personas%252C%2520response%2520generators%252C%2520discussion%2520paradigms%252C%2520and%2520decision%2520protocols.%2520We%2520introduce%2520MALLM%2520%2528Multi-Agent%2520Large%2520Language%2520Models%2529%252C%2520an%2520open-source%2520framework%2520that%2520enables%2520systematic%2520analysis%2520of%2520MAD%2520components.%2520MALLM%2520offers%2520more%2520than%2520144%2520unique%2520configurations%2520of%2520MAD%252C%2520including%2520%25281%2529%2520agent%2520personas%2520%2528e.g.%252C%2520Expert%252C%2520Personality%2529%252C%2520%25282%2529%2520response%2520generators%2520%2528e.g.%252C%2520Critical%252C%2520Reasoning%2529%252C%2520%25283%2529%2520discussion%2520paradigms%2520%2528e.g.%252C%2520Memory%252C%2520Relay%2529%252C%2520and%2520%25284%2529%2520decision%2520protocols%2520%2528e.g.%252C%2520Voting%252C%2520Consensus%2529.%2520MALLM%2520uses%2520simple%2520configuration%2520files%2520to%2520define%2520a%2520debate.%2520Furthermore%252C%2520MALLM%2520can%2520load%2520any%2520textual%2520Hugging%2520Face%2520dataset%2520%2528e.g.%252C%2520MMLU-Pro%252C%2520WinoGrande%2529%2520and%2520provides%2520an%2520evaluation%2520pipeline%2520for%2520easy%2520comparison%2520of%2520MAD%2520configurations.%2520MALLM%2520enables%2520researchers%2520to%2520systematically%2520configure%252C%2520run%252C%2520and%2520evaluate%2520debates%2520for%2520their%2520problems%252C%2520facilitating%2520the%2520understanding%2520of%2520the%2520components%2520and%2520their%2520interplay.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11656v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MALLM%3A%20Multi-Agent%20Large%20Language%20Models%20Framework&entry.906535625=Jonas%20Becker%20and%20Lars%20Benedikt%20Kaesberg%20and%20Niklas%20Bauer%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=Multi-agent%20debate%20%28MAD%29%20has%20demonstrated%20the%20ability%20to%20augment%20collective%20intelligence%20by%20scaling%20test-time%20compute%20and%20leveraging%20expertise.%20Current%20frameworks%20for%20multi-agent%20debate%20are%20often%20designed%20towards%20tool%20use%2C%20lack%20integrated%20evaluation%2C%20or%20provide%20limited%20configurability%20of%20agent%20personas%2C%20response%20generators%2C%20discussion%20paradigms%2C%20and%20decision%20protocols.%20We%20introduce%20MALLM%20%28Multi-Agent%20Large%20Language%20Models%29%2C%20an%20open-source%20framework%20that%20enables%20systematic%20analysis%20of%20MAD%20components.%20MALLM%20offers%20more%20than%20144%20unique%20configurations%20of%20MAD%2C%20including%20%281%29%20agent%20personas%20%28e.g.%2C%20Expert%2C%20Personality%29%2C%20%282%29%20response%20generators%20%28e.g.%2C%20Critical%2C%20Reasoning%29%2C%20%283%29%20discussion%20paradigms%20%28e.g.%2C%20Memory%2C%20Relay%29%2C%20and%20%284%29%20decision%20protocols%20%28e.g.%2C%20Voting%2C%20Consensus%29.%20MALLM%20uses%20simple%20configuration%20files%20to%20define%20a%20debate.%20Furthermore%2C%20MALLM%20can%20load%20any%20textual%20Hugging%20Face%20dataset%20%28e.g.%2C%20MMLU-Pro%2C%20WinoGrande%29%20and%20provides%20an%20evaluation%20pipeline%20for%20easy%20comparison%20of%20MAD%20configurations.%20MALLM%20enables%20researchers%20to%20systematically%20configure%2C%20run%2C%20and%20evaluate%20debates%20for%20their%20problems%2C%20facilitating%20the%20understanding%20of%20the%20components%20and%20their%20interplay.&entry.1838667208=http%3A//arxiv.org/abs/2509.11656v3&entry.124074799=Read"},
{"title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning", "author": "Mayank Gulati and Benedikt Gro\u00df and Gerhard Wunder", "abstract": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.\n  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures", "link": "http://arxiv.org/abs/2512.13316v1", "date": "2025-12-15", "relevancy": 2.4933, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5021}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALIGN-FL%3A%20Architecture-independent%20Learning%20through%20Invariant%20Generative%20component%20sharing%20in%20Federated%20Learning&body=Title%3A%20ALIGN-FL%3A%20Architecture-independent%20Learning%20through%20Invariant%20Generative%20component%20sharing%20in%20Federated%20Learning%0AAuthor%3A%20Mayank%20Gulati%20and%20Benedikt%20Gro%C3%9F%20and%20Gerhard%20Wunder%0AAbstract%3A%20We%20present%20ALIGN-FL%2C%20a%20novel%20approach%20to%20distributed%20learning%20that%20addresses%20the%20challenge%20of%20learning%20from%20highly%20disjoint%20data%20distributions%20through%20selective%20sharing%20of%20generative%20components.%20Instead%20of%20exchanging%20full%20model%20parameters%2C%20our%20framework%20enables%20privacy-preserving%20learning%20by%20transferring%20only%20generative%20capabilities%20across%20clients%2C%20while%20the%20server%20performs%20global%20training%20using%20synthetic%20samples.%20Through%20complementary%20privacy%20mechanisms%3A%20DP-SGD%20with%20adaptive%20clipping%20and%20Lipschitz%20regularized%20VAE%20decoders%20and%20a%20stateful%20architecture%20supporting%20heterogeneous%20clients%2C%20we%20experimentally%20validate%20our%20approach%20on%20MNIST%20and%20Fashion-MNIST%20datasets%20with%20cross-domain%20outliers.%20Our%20analysis%20demonstrates%20that%20both%20privacy%20mechanisms%20effectively%20map%20sensitive%20outliers%20to%20typical%20data%20points%20while%20maintaining%20utility%20in%20extreme%20Non-IID%20scenarios%20typical%20of%20cross-silo%20collaborations.%0A%20%20Index%20Terms%3A%20Client-invariant%20Learning%2C%20Federated%20Learning%20%28FL%29%2C%20Privacy-preserving%20Generative%20Models%2C%20Non-Independent%20and%20Identically%20Distributed%20%28Non-IID%29%2C%20Heterogeneous%20Architectures%0ALink%3A%20http%3A//arxiv.org/abs/2512.13316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALIGN-FL%253A%2520Architecture-independent%2520Learning%2520through%2520Invariant%2520Generative%2520component%2520sharing%2520in%2520Federated%2520Learning%26entry.906535625%3DMayank%2520Gulati%2520and%2520Benedikt%2520Gro%25C3%259F%2520and%2520Gerhard%2520Wunder%26entry.1292438233%3DWe%2520present%2520ALIGN-FL%252C%2520a%2520novel%2520approach%2520to%2520distributed%2520learning%2520that%2520addresses%2520the%2520challenge%2520of%2520learning%2520from%2520highly%2520disjoint%2520data%2520distributions%2520through%2520selective%2520sharing%2520of%2520generative%2520components.%2520Instead%2520of%2520exchanging%2520full%2520model%2520parameters%252C%2520our%2520framework%2520enables%2520privacy-preserving%2520learning%2520by%2520transferring%2520only%2520generative%2520capabilities%2520across%2520clients%252C%2520while%2520the%2520server%2520performs%2520global%2520training%2520using%2520synthetic%2520samples.%2520Through%2520complementary%2520privacy%2520mechanisms%253A%2520DP-SGD%2520with%2520adaptive%2520clipping%2520and%2520Lipschitz%2520regularized%2520VAE%2520decoders%2520and%2520a%2520stateful%2520architecture%2520supporting%2520heterogeneous%2520clients%252C%2520we%2520experimentally%2520validate%2520our%2520approach%2520on%2520MNIST%2520and%2520Fashion-MNIST%2520datasets%2520with%2520cross-domain%2520outliers.%2520Our%2520analysis%2520demonstrates%2520that%2520both%2520privacy%2520mechanisms%2520effectively%2520map%2520sensitive%2520outliers%2520to%2520typical%2520data%2520points%2520while%2520maintaining%2520utility%2520in%2520extreme%2520Non-IID%2520scenarios%2520typical%2520of%2520cross-silo%2520collaborations.%250A%2520%2520Index%2520Terms%253A%2520Client-invariant%2520Learning%252C%2520Federated%2520Learning%2520%2528FL%2529%252C%2520Privacy-preserving%2520Generative%2520Models%252C%2520Non-Independent%2520and%2520Identically%2520Distributed%2520%2528Non-IID%2529%252C%2520Heterogeneous%2520Architectures%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALIGN-FL%3A%20Architecture-independent%20Learning%20through%20Invariant%20Generative%20component%20sharing%20in%20Federated%20Learning&entry.906535625=Mayank%20Gulati%20and%20Benedikt%20Gro%C3%9F%20and%20Gerhard%20Wunder&entry.1292438233=We%20present%20ALIGN-FL%2C%20a%20novel%20approach%20to%20distributed%20learning%20that%20addresses%20the%20challenge%20of%20learning%20from%20highly%20disjoint%20data%20distributions%20through%20selective%20sharing%20of%20generative%20components.%20Instead%20of%20exchanging%20full%20model%20parameters%2C%20our%20framework%20enables%20privacy-preserving%20learning%20by%20transferring%20only%20generative%20capabilities%20across%20clients%2C%20while%20the%20server%20performs%20global%20training%20using%20synthetic%20samples.%20Through%20complementary%20privacy%20mechanisms%3A%20DP-SGD%20with%20adaptive%20clipping%20and%20Lipschitz%20regularized%20VAE%20decoders%20and%20a%20stateful%20architecture%20supporting%20heterogeneous%20clients%2C%20we%20experimentally%20validate%20our%20approach%20on%20MNIST%20and%20Fashion-MNIST%20datasets%20with%20cross-domain%20outliers.%20Our%20analysis%20demonstrates%20that%20both%20privacy%20mechanisms%20effectively%20map%20sensitive%20outliers%20to%20typical%20data%20points%20while%20maintaining%20utility%20in%20extreme%20Non-IID%20scenarios%20typical%20of%20cross-silo%20collaborations.%0A%20%20Index%20Terms%3A%20Client-invariant%20Learning%2C%20Federated%20Learning%20%28FL%29%2C%20Privacy-preserving%20Generative%20Models%2C%20Non-Independent%20and%20Identically%20Distributed%20%28Non-IID%29%2C%20Heterogeneous%20Architectures&entry.1838667208=http%3A//arxiv.org/abs/2512.13316v1&entry.124074799=Read"},
{"title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models", "author": "Zefang Liu and Nam Nguyen and Yinzhu Quan and Austin Zhang", "abstract": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.", "link": "http://arxiv.org/abs/2512.13618v1", "date": "2025-12-15", "relevancy": 2.4753, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models&body=Title%3A%20Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models%0AAuthor%3A%20Zefang%20Liu%20and%20Nam%20Nguyen%20and%20Yinzhu%20Quan%20and%20Austin%20Zhang%0AAbstract%3A%20Representing%20continuous%20time%20is%20a%20critical%20and%20under-explored%20challenge%20in%20modeling%20temporal%20event%20sequences%20with%20large%20language%20models%20%28LLMs%29.%20Various%20strategies%20like%20byte-level%20representations%20or%20calendar%20tokens%20have%20been%20proposed.%20However%2C%20the%20optimal%20approach%20remains%20unclear%2C%20especially%20given%20the%20diverse%20statistical%20distributions%20of%20real-world%20event%20data%2C%20which%20range%20from%20smooth%20log-normal%20to%20discrete%2C%20spiky%20patterns.%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20temporal%20tokenization%20for%20event%20sequences%2C%20comparing%20distinct%20encoding%20strategies%3A%20naive%20numeric%20strings%2C%20high-precision%20byte-level%20representations%2C%20human-semantic%20calendar%20tokens%2C%20classic%20uniform%20binning%2C%20and%20adaptive%20residual%20scalar%20quantization.%20We%20evaluate%20these%20strategies%20by%20fine-tuning%20LLMs%20on%20real-world%20datasets%20that%20exemplify%20these%20diverse%20distributions.%20Our%20analysis%20reveals%20that%20no%20single%20strategy%20is%20universally%20superior%3B%20instead%2C%20prediction%20performance%20depends%20heavily%20on%20aligning%20the%20tokenizer%20with%20the%20data%27s%20statistical%20properties%2C%20with%20log-based%20strategies%20excelling%20on%20skewed%20distributions%20and%20human-centric%20formats%20proving%20robust%20for%20mixed%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Tokenization%2520Strategies%2520for%2520Event%2520Sequence%2520Modeling%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DZefang%2520Liu%2520and%2520Nam%2520Nguyen%2520and%2520Yinzhu%2520Quan%2520and%2520Austin%2520Zhang%26entry.1292438233%3DRepresenting%2520continuous%2520time%2520is%2520a%2520critical%2520and%2520under-explored%2520challenge%2520in%2520modeling%2520temporal%2520event%2520sequences%2520with%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Various%2520strategies%2520like%2520byte-level%2520representations%2520or%2520calendar%2520tokens%2520have%2520been%2520proposed.%2520However%252C%2520the%2520optimal%2520approach%2520remains%2520unclear%252C%2520especially%2520given%2520the%2520diverse%2520statistical%2520distributions%2520of%2520real-world%2520event%2520data%252C%2520which%2520range%2520from%2520smooth%2520log-normal%2520to%2520discrete%252C%2520spiky%2520patterns.%2520This%2520paper%2520presents%2520the%2520first%2520empirical%2520study%2520of%2520temporal%2520tokenization%2520for%2520event%2520sequences%252C%2520comparing%2520distinct%2520encoding%2520strategies%253A%2520naive%2520numeric%2520strings%252C%2520high-precision%2520byte-level%2520representations%252C%2520human-semantic%2520calendar%2520tokens%252C%2520classic%2520uniform%2520binning%252C%2520and%2520adaptive%2520residual%2520scalar%2520quantization.%2520We%2520evaluate%2520these%2520strategies%2520by%2520fine-tuning%2520LLMs%2520on%2520real-world%2520datasets%2520that%2520exemplify%2520these%2520diverse%2520distributions.%2520Our%2520analysis%2520reveals%2520that%2520no%2520single%2520strategy%2520is%2520universally%2520superior%253B%2520instead%252C%2520prediction%2520performance%2520depends%2520heavily%2520on%2520aligning%2520the%2520tokenizer%2520with%2520the%2520data%2527s%2520statistical%2520properties%252C%2520with%2520log-based%2520strategies%2520excelling%2520on%2520skewed%2520distributions%2520and%2520human-centric%2520formats%2520proving%2520robust%2520for%2520mixed%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models&entry.906535625=Zefang%20Liu%20and%20Nam%20Nguyen%20and%20Yinzhu%20Quan%20and%20Austin%20Zhang&entry.1292438233=Representing%20continuous%20time%20is%20a%20critical%20and%20under-explored%20challenge%20in%20modeling%20temporal%20event%20sequences%20with%20large%20language%20models%20%28LLMs%29.%20Various%20strategies%20like%20byte-level%20representations%20or%20calendar%20tokens%20have%20been%20proposed.%20However%2C%20the%20optimal%20approach%20remains%20unclear%2C%20especially%20given%20the%20diverse%20statistical%20distributions%20of%20real-world%20event%20data%2C%20which%20range%20from%20smooth%20log-normal%20to%20discrete%2C%20spiky%20patterns.%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20temporal%20tokenization%20for%20event%20sequences%2C%20comparing%20distinct%20encoding%20strategies%3A%20naive%20numeric%20strings%2C%20high-precision%20byte-level%20representations%2C%20human-semantic%20calendar%20tokens%2C%20classic%20uniform%20binning%2C%20and%20adaptive%20residual%20scalar%20quantization.%20We%20evaluate%20these%20strategies%20by%20fine-tuning%20LLMs%20on%20real-world%20datasets%20that%20exemplify%20these%20diverse%20distributions.%20Our%20analysis%20reveals%20that%20no%20single%20strategy%20is%20universally%20superior%3B%20instead%2C%20prediction%20performance%20depends%20heavily%20on%20aligning%20the%20tokenizer%20with%20the%20data%27s%20statistical%20properties%2C%20with%20log-based%20strategies%20excelling%20on%20skewed%20distributions%20and%20human-centric%20formats%20proving%20robust%20for%20mixed%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2512.13618v1&entry.124074799=Read"},
{"title": "World Models Can Leverage Human Videos for Dexterous Manipulation", "author": "Raktim Gautam Goswami and Amir Bar and David Fan and Tsung-Yen Yang and Gaoyue Zhou and Prashanth Krishnamurthy and Michael Rabbat and Farshad Khorrami and Yann LeCun", "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.", "link": "http://arxiv.org/abs/2512.13644v1", "date": "2025-12-15", "relevancy": 2.4386, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6497}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.586}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20World%20Models%20Can%20Leverage%20Human%20Videos%20for%20Dexterous%20Manipulation&body=Title%3A%20World%20Models%20Can%20Leverage%20Human%20Videos%20for%20Dexterous%20Manipulation%0AAuthor%3A%20Raktim%20Gautam%20Goswami%20and%20Amir%20Bar%20and%20David%20Fan%20and%20Tsung-Yen%20Yang%20and%20Gaoyue%20Zhou%20and%20Prashanth%20Krishnamurthy%20and%20Michael%20Rabbat%20and%20Farshad%20Khorrami%20and%20Yann%20LeCun%0AAbstract%3A%20Dexterous%20manipulation%20is%20challenging%20because%20it%20requires%20understanding%20how%20subtle%20hand%20motion%20influences%20the%20environment%20through%20contact%20with%20objects.%20We%20introduce%20DexWM%2C%20a%20Dexterous%20Manipulation%20World%20Model%20that%20predicts%20the%20next%20latent%20state%20of%20the%20environment%20conditioned%20on%20past%20states%20and%20dexterous%20actions.%20To%20overcome%20the%20scarcity%20of%20dexterous%20manipulation%20datasets%2C%20DexWM%20is%20trained%20on%20over%20900%20hours%20of%20human%20and%20non-dexterous%20robot%20videos.%20To%20enable%20fine-grained%20dexterity%2C%20we%20find%20that%20predicting%20visual%20features%20alone%20is%20insufficient%3B%20therefore%2C%20we%20introduce%20an%20auxiliary%20hand%20consistency%20loss%20that%20enforces%20accurate%20hand%20configurations.%20DexWM%20outperforms%20prior%20world%20models%20conditioned%20on%20text%2C%20navigation%2C%20and%20full-body%20actions%2C%20achieving%20more%20accurate%20predictions%20of%20future%20states.%20DexWM%20also%20demonstrates%20strong%20zero-shot%20generalization%20to%20unseen%20manipulation%20skills%20when%20deployed%20on%20a%20Franka%20Panda%20arm%20equipped%20with%20an%20Allegro%20gripper%2C%20outperforming%20Diffusion%20Policy%20by%20over%2050%25%20on%20average%20in%20grasping%2C%20placing%2C%20and%20reaching%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorld%2520Models%2520Can%2520Leverage%2520Human%2520Videos%2520for%2520Dexterous%2520Manipulation%26entry.906535625%3DRaktim%2520Gautam%2520Goswami%2520and%2520Amir%2520Bar%2520and%2520David%2520Fan%2520and%2520Tsung-Yen%2520Yang%2520and%2520Gaoyue%2520Zhou%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Michael%2520Rabbat%2520and%2520Farshad%2520Khorrami%2520and%2520Yann%2520LeCun%26entry.1292438233%3DDexterous%2520manipulation%2520is%2520challenging%2520because%2520it%2520requires%2520understanding%2520how%2520subtle%2520hand%2520motion%2520influences%2520the%2520environment%2520through%2520contact%2520with%2520objects.%2520We%2520introduce%2520DexWM%252C%2520a%2520Dexterous%2520Manipulation%2520World%2520Model%2520that%2520predicts%2520the%2520next%2520latent%2520state%2520of%2520the%2520environment%2520conditioned%2520on%2520past%2520states%2520and%2520dexterous%2520actions.%2520To%2520overcome%2520the%2520scarcity%2520of%2520dexterous%2520manipulation%2520datasets%252C%2520DexWM%2520is%2520trained%2520on%2520over%2520900%2520hours%2520of%2520human%2520and%2520non-dexterous%2520robot%2520videos.%2520To%2520enable%2520fine-grained%2520dexterity%252C%2520we%2520find%2520that%2520predicting%2520visual%2520features%2520alone%2520is%2520insufficient%253B%2520therefore%252C%2520we%2520introduce%2520an%2520auxiliary%2520hand%2520consistency%2520loss%2520that%2520enforces%2520accurate%2520hand%2520configurations.%2520DexWM%2520outperforms%2520prior%2520world%2520models%2520conditioned%2520on%2520text%252C%2520navigation%252C%2520and%2520full-body%2520actions%252C%2520achieving%2520more%2520accurate%2520predictions%2520of%2520future%2520states.%2520DexWM%2520also%2520demonstrates%2520strong%2520zero-shot%2520generalization%2520to%2520unseen%2520manipulation%2520skills%2520when%2520deployed%2520on%2520a%2520Franka%2520Panda%2520arm%2520equipped%2520with%2520an%2520Allegro%2520gripper%252C%2520outperforming%2520Diffusion%2520Policy%2520by%2520over%252050%2525%2520on%2520average%2520in%2520grasping%252C%2520placing%252C%2520and%2520reaching%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=World%20Models%20Can%20Leverage%20Human%20Videos%20for%20Dexterous%20Manipulation&entry.906535625=Raktim%20Gautam%20Goswami%20and%20Amir%20Bar%20and%20David%20Fan%20and%20Tsung-Yen%20Yang%20and%20Gaoyue%20Zhou%20and%20Prashanth%20Krishnamurthy%20and%20Michael%20Rabbat%20and%20Farshad%20Khorrami%20and%20Yann%20LeCun&entry.1292438233=Dexterous%20manipulation%20is%20challenging%20because%20it%20requires%20understanding%20how%20subtle%20hand%20motion%20influences%20the%20environment%20through%20contact%20with%20objects.%20We%20introduce%20DexWM%2C%20a%20Dexterous%20Manipulation%20World%20Model%20that%20predicts%20the%20next%20latent%20state%20of%20the%20environment%20conditioned%20on%20past%20states%20and%20dexterous%20actions.%20To%20overcome%20the%20scarcity%20of%20dexterous%20manipulation%20datasets%2C%20DexWM%20is%20trained%20on%20over%20900%20hours%20of%20human%20and%20non-dexterous%20robot%20videos.%20To%20enable%20fine-grained%20dexterity%2C%20we%20find%20that%20predicting%20visual%20features%20alone%20is%20insufficient%3B%20therefore%2C%20we%20introduce%20an%20auxiliary%20hand%20consistency%20loss%20that%20enforces%20accurate%20hand%20configurations.%20DexWM%20outperforms%20prior%20world%20models%20conditioned%20on%20text%2C%20navigation%2C%20and%20full-body%20actions%2C%20achieving%20more%20accurate%20predictions%20of%20future%20states.%20DexWM%20also%20demonstrates%20strong%20zero-shot%20generalization%20to%20unseen%20manipulation%20skills%20when%20deployed%20on%20a%20Franka%20Panda%20arm%20equipped%20with%20an%20Allegro%20gripper%2C%20outperforming%20Diffusion%20Policy%20by%20over%2050%25%20on%20average%20in%20grasping%2C%20placing%2C%20and%20reaching%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.13644v1&entry.124074799=Read"},
{"title": "Feedforward 3D Editing via Text-Steerable Image-to-3D", "author": "Ziqi Ma and Hongqiao Chen and Yisong Yue and Georgia Gkioxari", "abstract": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/", "link": "http://arxiv.org/abs/2512.13678v1", "date": "2025-12-15", "relevancy": 2.4301, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.616}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6027}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedforward%203D%20Editing%20via%20Text-Steerable%20Image-to-3D&body=Title%3A%20Feedforward%203D%20Editing%20via%20Text-Steerable%20Image-to-3D%0AAuthor%3A%20Ziqi%20Ma%20and%20Hongqiao%20Chen%20and%20Yisong%20Yue%20and%20Georgia%20Gkioxari%0AAbstract%3A%20Recent%20progress%20in%20image-to-3D%20has%20opened%20up%20immense%20possibilities%20for%20design%2C%20AR/VR%2C%20and%20robotics.%20However%2C%20to%20use%20AI-generated%203D%20assets%20in%20real%20applications%2C%20a%20critical%20requirement%20is%20the%20capability%20to%20edit%20them%20easily.%20We%20present%20a%20feedforward%20method%2C%20Steer3D%2C%20to%20add%20text%20steerability%20to%20image-to-3D%20models%2C%20which%20enables%20editing%20of%20generated%203D%20assets%20with%20language.%20Our%20approach%20is%20inspired%20by%20ControlNet%2C%20which%20we%20adapt%20to%20image-to-3D%20generation%20to%20enable%20text%20steering%20directly%20in%20a%20forward%20pass.%20We%20build%20a%20scalable%20data%20engine%20for%20automatic%20data%20generation%2C%20and%20develop%20a%20two-stage%20training%20recipe%20based%20on%20flow-matching%20training%20and%20Direct%20Preference%20Optimization%20%28DPO%29.%20Compared%20to%20competing%20methods%2C%20Steer3D%20more%20faithfully%20follows%20the%20language%20instruction%20and%20maintains%20better%20consistency%20with%20the%20original%203D%20asset%2C%20while%20being%202.4x%20to%2028.5x%20faster.%20Steer3D%20demonstrates%20that%20it%20is%20possible%20to%20add%20a%20new%20modality%20%28text%29%20to%20steer%20the%20generation%20of%20pretrained%20image-to-3D%20generative%20models%20with%20100k%20data.%20Project%20website%3A%20https%3A//glab-caltech.github.io/steer3d/%0ALink%3A%20http%3A//arxiv.org/abs/2512.13678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedforward%25203D%2520Editing%2520via%2520Text-Steerable%2520Image-to-3D%26entry.906535625%3DZiqi%2520Ma%2520and%2520Hongqiao%2520Chen%2520and%2520Yisong%2520Yue%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3DRecent%2520progress%2520in%2520image-to-3D%2520has%2520opened%2520up%2520immense%2520possibilities%2520for%2520design%252C%2520AR/VR%252C%2520and%2520robotics.%2520However%252C%2520to%2520use%2520AI-generated%25203D%2520assets%2520in%2520real%2520applications%252C%2520a%2520critical%2520requirement%2520is%2520the%2520capability%2520to%2520edit%2520them%2520easily.%2520We%2520present%2520a%2520feedforward%2520method%252C%2520Steer3D%252C%2520to%2520add%2520text%2520steerability%2520to%2520image-to-3D%2520models%252C%2520which%2520enables%2520editing%2520of%2520generated%25203D%2520assets%2520with%2520language.%2520Our%2520approach%2520is%2520inspired%2520by%2520ControlNet%252C%2520which%2520we%2520adapt%2520to%2520image-to-3D%2520generation%2520to%2520enable%2520text%2520steering%2520directly%2520in%2520a%2520forward%2520pass.%2520We%2520build%2520a%2520scalable%2520data%2520engine%2520for%2520automatic%2520data%2520generation%252C%2520and%2520develop%2520a%2520two-stage%2520training%2520recipe%2520based%2520on%2520flow-matching%2520training%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520Compared%2520to%2520competing%2520methods%252C%2520Steer3D%2520more%2520faithfully%2520follows%2520the%2520language%2520instruction%2520and%2520maintains%2520better%2520consistency%2520with%2520the%2520original%25203D%2520asset%252C%2520while%2520being%25202.4x%2520to%252028.5x%2520faster.%2520Steer3D%2520demonstrates%2520that%2520it%2520is%2520possible%2520to%2520add%2520a%2520new%2520modality%2520%2528text%2529%2520to%2520steer%2520the%2520generation%2520of%2520pretrained%2520image-to-3D%2520generative%2520models%2520with%2520100k%2520data.%2520Project%2520website%253A%2520https%253A//glab-caltech.github.io/steer3d/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedforward%203D%20Editing%20via%20Text-Steerable%20Image-to-3D&entry.906535625=Ziqi%20Ma%20and%20Hongqiao%20Chen%20and%20Yisong%20Yue%20and%20Georgia%20Gkioxari&entry.1292438233=Recent%20progress%20in%20image-to-3D%20has%20opened%20up%20immense%20possibilities%20for%20design%2C%20AR/VR%2C%20and%20robotics.%20However%2C%20to%20use%20AI-generated%203D%20assets%20in%20real%20applications%2C%20a%20critical%20requirement%20is%20the%20capability%20to%20edit%20them%20easily.%20We%20present%20a%20feedforward%20method%2C%20Steer3D%2C%20to%20add%20text%20steerability%20to%20image-to-3D%20models%2C%20which%20enables%20editing%20of%20generated%203D%20assets%20with%20language.%20Our%20approach%20is%20inspired%20by%20ControlNet%2C%20which%20we%20adapt%20to%20image-to-3D%20generation%20to%20enable%20text%20steering%20directly%20in%20a%20forward%20pass.%20We%20build%20a%20scalable%20data%20engine%20for%20automatic%20data%20generation%2C%20and%20develop%20a%20two-stage%20training%20recipe%20based%20on%20flow-matching%20training%20and%20Direct%20Preference%20Optimization%20%28DPO%29.%20Compared%20to%20competing%20methods%2C%20Steer3D%20more%20faithfully%20follows%20the%20language%20instruction%20and%20maintains%20better%20consistency%20with%20the%20original%203D%20asset%2C%20while%20being%202.4x%20to%2028.5x%20faster.%20Steer3D%20demonstrates%20that%20it%20is%20possible%20to%20add%20a%20new%20modality%20%28text%29%20to%20steer%20the%20generation%20of%20pretrained%20image-to-3D%20generative%20models%20with%20100k%20data.%20Project%20website%3A%20https%3A//glab-caltech.github.io/steer3d/&entry.1838667208=http%3A//arxiv.org/abs/2512.13678v1&entry.124074799=Read"},
{"title": "Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks", "author": "V\u00edtor M. Hanriot and Luiz C. B. Torres and Ant\u00f4nio P. Braga", "abstract": "While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.", "link": "http://arxiv.org/abs/2512.13410v1", "date": "2025-12-15", "relevancy": 2.4282, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4917}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4893}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiclass%20Graph-Based%20Large%20Margin%20Classifiers%3A%20Unified%20Approach%20for%20Support%20Vectors%20and%20Neural%20Networks&body=Title%3A%20Multiclass%20Graph-Based%20Large%20Margin%20Classifiers%3A%20Unified%20Approach%20for%20Support%20Vectors%20and%20Neural%20Networks%0AAuthor%3A%20V%C3%ADtor%20M.%20Hanriot%20and%20Luiz%20C.%20B.%20Torres%20and%20Ant%C3%B4nio%20P.%20Braga%0AAbstract%3A%20While%20large%20margin%20classifiers%20are%20originally%20an%20outcome%20of%20an%20optimization%20framework%2C%20support%20vectors%20%28SVs%29%20can%20be%20obtained%20from%20geometric%20approaches.%20This%20article%20presents%20advances%20in%20the%20use%20of%20Gabriel%20graphs%20%28GGs%29%20in%20binary%20and%20multiclass%20classification%20problems.%20For%20Chipclass%2C%20a%20hyperparameter-less%20and%20optimization-less%20GG-based%20binary%20classifier%2C%20we%20discuss%20how%20activation%20functions%20and%20support%20edge%20%28SE%29-centered%20neurons%20affect%20the%20classification%2C%20proposing%20smoother%20functions%20and%20structural%20SV%20%28SSV%29-centered%20neurons%20to%20achieve%20margins%20with%20low%20probabilities%20and%20smoother%20classification%20contours.%20We%20extend%20the%20neural%20network%20architecture%2C%20which%20can%20be%20trained%20with%20backpropagation%20with%20a%20softmax%20function%20and%20a%20cross-entropy%20loss%2C%20or%20by%20solving%20a%20system%20of%20linear%20equations.%20A%20new%20subgraph-/distance-based%20membership%20function%20for%20graph%20regularization%20is%20also%20proposed%2C%20along%20with%20a%20new%20GG%20recomputation%20algorithm%20that%20is%20less%20computationally%20expensive%20than%20the%20standard%20approach.%20Experimental%20results%20with%20the%20Friedman%20test%20show%20that%20our%20method%20was%20better%20than%20previous%20GG-based%20classifiers%20and%20statistically%20equivalent%20to%20tree-based%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticlass%2520Graph-Based%2520Large%2520Margin%2520Classifiers%253A%2520Unified%2520Approach%2520for%2520Support%2520Vectors%2520and%2520Neural%2520Networks%26entry.906535625%3DV%25C3%25ADtor%2520M.%2520Hanriot%2520and%2520Luiz%2520C.%2520B.%2520Torres%2520and%2520Ant%25C3%25B4nio%2520P.%2520Braga%26entry.1292438233%3DWhile%2520large%2520margin%2520classifiers%2520are%2520originally%2520an%2520outcome%2520of%2520an%2520optimization%2520framework%252C%2520support%2520vectors%2520%2528SVs%2529%2520can%2520be%2520obtained%2520from%2520geometric%2520approaches.%2520This%2520article%2520presents%2520advances%2520in%2520the%2520use%2520of%2520Gabriel%2520graphs%2520%2528GGs%2529%2520in%2520binary%2520and%2520multiclass%2520classification%2520problems.%2520For%2520Chipclass%252C%2520a%2520hyperparameter-less%2520and%2520optimization-less%2520GG-based%2520binary%2520classifier%252C%2520we%2520discuss%2520how%2520activation%2520functions%2520and%2520support%2520edge%2520%2528SE%2529-centered%2520neurons%2520affect%2520the%2520classification%252C%2520proposing%2520smoother%2520functions%2520and%2520structural%2520SV%2520%2528SSV%2529-centered%2520neurons%2520to%2520achieve%2520margins%2520with%2520low%2520probabilities%2520and%2520smoother%2520classification%2520contours.%2520We%2520extend%2520the%2520neural%2520network%2520architecture%252C%2520which%2520can%2520be%2520trained%2520with%2520backpropagation%2520with%2520a%2520softmax%2520function%2520and%2520a%2520cross-entropy%2520loss%252C%2520or%2520by%2520solving%2520a%2520system%2520of%2520linear%2520equations.%2520A%2520new%2520subgraph-/distance-based%2520membership%2520function%2520for%2520graph%2520regularization%2520is%2520also%2520proposed%252C%2520along%2520with%2520a%2520new%2520GG%2520recomputation%2520algorithm%2520that%2520is%2520less%2520computationally%2520expensive%2520than%2520the%2520standard%2520approach.%2520Experimental%2520results%2520with%2520the%2520Friedman%2520test%2520show%2520that%2520our%2520method%2520was%2520better%2520than%2520previous%2520GG-based%2520classifiers%2520and%2520statistically%2520equivalent%2520to%2520tree-based%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiclass%20Graph-Based%20Large%20Margin%20Classifiers%3A%20Unified%20Approach%20for%20Support%20Vectors%20and%20Neural%20Networks&entry.906535625=V%C3%ADtor%20M.%20Hanriot%20and%20Luiz%20C.%20B.%20Torres%20and%20Ant%C3%B4nio%20P.%20Braga&entry.1292438233=While%20large%20margin%20classifiers%20are%20originally%20an%20outcome%20of%20an%20optimization%20framework%2C%20support%20vectors%20%28SVs%29%20can%20be%20obtained%20from%20geometric%20approaches.%20This%20article%20presents%20advances%20in%20the%20use%20of%20Gabriel%20graphs%20%28GGs%29%20in%20binary%20and%20multiclass%20classification%20problems.%20For%20Chipclass%2C%20a%20hyperparameter-less%20and%20optimization-less%20GG-based%20binary%20classifier%2C%20we%20discuss%20how%20activation%20functions%20and%20support%20edge%20%28SE%29-centered%20neurons%20affect%20the%20classification%2C%20proposing%20smoother%20functions%20and%20structural%20SV%20%28SSV%29-centered%20neurons%20to%20achieve%20margins%20with%20low%20probabilities%20and%20smoother%20classification%20contours.%20We%20extend%20the%20neural%20network%20architecture%2C%20which%20can%20be%20trained%20with%20backpropagation%20with%20a%20softmax%20function%20and%20a%20cross-entropy%20loss%2C%20or%20by%20solving%20a%20system%20of%20linear%20equations.%20A%20new%20subgraph-/distance-based%20membership%20function%20for%20graph%20regularization%20is%20also%20proposed%2C%20along%20with%20a%20new%20GG%20recomputation%20algorithm%20that%20is%20less%20computationally%20expensive%20than%20the%20standard%20approach.%20Experimental%20results%20with%20the%20Friedman%20test%20show%20that%20our%20method%20was%20better%20than%20previous%20GG-based%20classifiers%20and%20statistically%20equivalent%20to%20tree-based%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.13410v1&entry.124074799=Read"},
{"title": "Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging", "author": "Paul Weiser and Georg Langs and Wolfgang Bogner and Stanislav Motyka and Bernhard Strasser and Polina Golland and Nalini Singh and Jorg Dietrich and Erik Uhlmann and Tracy Batchelor and Daniel Cahill and Malte Hoffmann and Antoine Klauser and Ovidiu C. Andronesi", "abstract": "Introduction: Altered neurometabolism is an important pathological mechanism in many neurological diseases and brain cancer, which can be mapped non-invasively by Magnetic Resonance Spectroscopic Imaging (MRSI). Advanced MRSI using non-cartesian compressed-sense acquisition enables fast high-resolution metabolic imaging but has lengthy reconstruction times that limits throughput and needs expert user interaction. Here, we present a robust and efficient Deep Learning reconstruction to obtain high-quality metabolic maps.\n  Methods: Fast high-resolution whole-brain metabolic imaging was performed at 3.4 mm$^3$ isotropic resolution with acquisition times between 4:11-9:21 min:s using ECCENTRIC pulse sequence on a 7T MRI scanner. Data were acquired in a high-resolution phantom and 27 human participants, including 22 healthy volunteers and 5 glioma patients. A deep neural network using recurring interlaced convolutional layers with joint dual-space feature representation was developed for deep learning ECCENTRIC reconstruction (Deep-ER). 21 subjects were used for training and 6 subjects for testing. Deep-ER performance was compared to conventional iterative Total Generalized Variation reconstruction using image and spectral quality metrics.\n  Results: Deep-ER demonstrated 600-fold faster reconstruction than conventional methods, providing improved spatial-spectral quality and metabolite quantification with 12%-45% (P<0.05) higher signal-to-noise and 8%-50% (P<0.05) smaller Cramer-Rao lower bounds. Metabolic images clearly visualize glioma tumor heterogeneity and boundary.\n  Conclusion: Deep-ER provides efficient and robust reconstruction for sparse-sampled MRSI. The accelerated acquisition-reconstruction MRSI is compatible with high-throughput imaging workflow. It is expected that such improved performance will facilitate basic and clinical MRSI applications.", "link": "http://arxiv.org/abs/2409.18303v2", "date": "2025-12-15", "relevancy": 2.4276, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5064}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4776}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-ER%3A%20Deep%20Learning%20ECCENTRIC%20Reconstruction%20for%20fast%20high-resolution%20neurometabolic%20imaging&body=Title%3A%20Deep-ER%3A%20Deep%20Learning%20ECCENTRIC%20Reconstruction%20for%20fast%20high-resolution%20neurometabolic%20imaging%0AAuthor%3A%20Paul%20Weiser%20and%20Georg%20Langs%20and%20Wolfgang%20Bogner%20and%20Stanislav%20Motyka%20and%20Bernhard%20Strasser%20and%20Polina%20Golland%20and%20Nalini%20Singh%20and%20Jorg%20Dietrich%20and%20Erik%20Uhlmann%20and%20Tracy%20Batchelor%20and%20Daniel%20Cahill%20and%20Malte%20Hoffmann%20and%20Antoine%20Klauser%20and%20Ovidiu%20C.%20Andronesi%0AAbstract%3A%20Introduction%3A%20Altered%20neurometabolism%20is%20an%20important%20pathological%20mechanism%20in%20many%20neurological%20diseases%20and%20brain%20cancer%2C%20which%20can%20be%20mapped%20non-invasively%20by%20Magnetic%20Resonance%20Spectroscopic%20Imaging%20%28MRSI%29.%20Advanced%20MRSI%20using%20non-cartesian%20compressed-sense%20acquisition%20enables%20fast%20high-resolution%20metabolic%20imaging%20but%20has%20lengthy%20reconstruction%20times%20that%20limits%20throughput%20and%20needs%20expert%20user%20interaction.%20Here%2C%20we%20present%20a%20robust%20and%20efficient%20Deep%20Learning%20reconstruction%20to%20obtain%20high-quality%20metabolic%20maps.%0A%20%20Methods%3A%20Fast%20high-resolution%20whole-brain%20metabolic%20imaging%20was%20performed%20at%203.4%20mm%24%5E3%24%20isotropic%20resolution%20with%20acquisition%20times%20between%204%3A11-9%3A21%20min%3As%20using%20ECCENTRIC%20pulse%20sequence%20on%20a%207T%20MRI%20scanner.%20Data%20were%20acquired%20in%20a%20high-resolution%20phantom%20and%2027%20human%20participants%2C%20including%2022%20healthy%20volunteers%20and%205%20glioma%20patients.%20A%20deep%20neural%20network%20using%20recurring%20interlaced%20convolutional%20layers%20with%20joint%20dual-space%20feature%20representation%20was%20developed%20for%20deep%20learning%20ECCENTRIC%20reconstruction%20%28Deep-ER%29.%2021%20subjects%20were%20used%20for%20training%20and%206%20subjects%20for%20testing.%20Deep-ER%20performance%20was%20compared%20to%20conventional%20iterative%20Total%20Generalized%20Variation%20reconstruction%20using%20image%20and%20spectral%20quality%20metrics.%0A%20%20Results%3A%20Deep-ER%20demonstrated%20600-fold%20faster%20reconstruction%20than%20conventional%20methods%2C%20providing%20improved%20spatial-spectral%20quality%20and%20metabolite%20quantification%20with%2012%25-45%25%20%28P%3C0.05%29%20higher%20signal-to-noise%20and%208%25-50%25%20%28P%3C0.05%29%20smaller%20Cramer-Rao%20lower%20bounds.%20Metabolic%20images%20clearly%20visualize%20glioma%20tumor%20heterogeneity%20and%20boundary.%0A%20%20Conclusion%3A%20Deep-ER%20provides%20efficient%20and%20robust%20reconstruction%20for%20sparse-sampled%20MRSI.%20The%20accelerated%20acquisition-reconstruction%20MRSI%20is%20compatible%20with%20high-throughput%20imaging%20workflow.%20It%20is%20expected%20that%20such%20improved%20performance%20will%20facilitate%20basic%20and%20clinical%20MRSI%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2409.18303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-ER%253A%2520Deep%2520Learning%2520ECCENTRIC%2520Reconstruction%2520for%2520fast%2520high-resolution%2520neurometabolic%2520imaging%26entry.906535625%3DPaul%2520Weiser%2520and%2520Georg%2520Langs%2520and%2520Wolfgang%2520Bogner%2520and%2520Stanislav%2520Motyka%2520and%2520Bernhard%2520Strasser%2520and%2520Polina%2520Golland%2520and%2520Nalini%2520Singh%2520and%2520Jorg%2520Dietrich%2520and%2520Erik%2520Uhlmann%2520and%2520Tracy%2520Batchelor%2520and%2520Daniel%2520Cahill%2520and%2520Malte%2520Hoffmann%2520and%2520Antoine%2520Klauser%2520and%2520Ovidiu%2520C.%2520Andronesi%26entry.1292438233%3DIntroduction%253A%2520Altered%2520neurometabolism%2520is%2520an%2520important%2520pathological%2520mechanism%2520in%2520many%2520neurological%2520diseases%2520and%2520brain%2520cancer%252C%2520which%2520can%2520be%2520mapped%2520non-invasively%2520by%2520Magnetic%2520Resonance%2520Spectroscopic%2520Imaging%2520%2528MRSI%2529.%2520Advanced%2520MRSI%2520using%2520non-cartesian%2520compressed-sense%2520acquisition%2520enables%2520fast%2520high-resolution%2520metabolic%2520imaging%2520but%2520has%2520lengthy%2520reconstruction%2520times%2520that%2520limits%2520throughput%2520and%2520needs%2520expert%2520user%2520interaction.%2520Here%252C%2520we%2520present%2520a%2520robust%2520and%2520efficient%2520Deep%2520Learning%2520reconstruction%2520to%2520obtain%2520high-quality%2520metabolic%2520maps.%250A%2520%2520Methods%253A%2520Fast%2520high-resolution%2520whole-brain%2520metabolic%2520imaging%2520was%2520performed%2520at%25203.4%2520mm%2524%255E3%2524%2520isotropic%2520resolution%2520with%2520acquisition%2520times%2520between%25204%253A11-9%253A21%2520min%253As%2520using%2520ECCENTRIC%2520pulse%2520sequence%2520on%2520a%25207T%2520MRI%2520scanner.%2520Data%2520were%2520acquired%2520in%2520a%2520high-resolution%2520phantom%2520and%252027%2520human%2520participants%252C%2520including%252022%2520healthy%2520volunteers%2520and%25205%2520glioma%2520patients.%2520A%2520deep%2520neural%2520network%2520using%2520recurring%2520interlaced%2520convolutional%2520layers%2520with%2520joint%2520dual-space%2520feature%2520representation%2520was%2520developed%2520for%2520deep%2520learning%2520ECCENTRIC%2520reconstruction%2520%2528Deep-ER%2529.%252021%2520subjects%2520were%2520used%2520for%2520training%2520and%25206%2520subjects%2520for%2520testing.%2520Deep-ER%2520performance%2520was%2520compared%2520to%2520conventional%2520iterative%2520Total%2520Generalized%2520Variation%2520reconstruction%2520using%2520image%2520and%2520spectral%2520quality%2520metrics.%250A%2520%2520Results%253A%2520Deep-ER%2520demonstrated%2520600-fold%2520faster%2520reconstruction%2520than%2520conventional%2520methods%252C%2520providing%2520improved%2520spatial-spectral%2520quality%2520and%2520metabolite%2520quantification%2520with%252012%2525-45%2525%2520%2528P%253C0.05%2529%2520higher%2520signal-to-noise%2520and%25208%2525-50%2525%2520%2528P%253C0.05%2529%2520smaller%2520Cramer-Rao%2520lower%2520bounds.%2520Metabolic%2520images%2520clearly%2520visualize%2520glioma%2520tumor%2520heterogeneity%2520and%2520boundary.%250A%2520%2520Conclusion%253A%2520Deep-ER%2520provides%2520efficient%2520and%2520robust%2520reconstruction%2520for%2520sparse-sampled%2520MRSI.%2520The%2520accelerated%2520acquisition-reconstruction%2520MRSI%2520is%2520compatible%2520with%2520high-throughput%2520imaging%2520workflow.%2520It%2520is%2520expected%2520that%2520such%2520improved%2520performance%2520will%2520facilitate%2520basic%2520and%2520clinical%2520MRSI%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-ER%3A%20Deep%20Learning%20ECCENTRIC%20Reconstruction%20for%20fast%20high-resolution%20neurometabolic%20imaging&entry.906535625=Paul%20Weiser%20and%20Georg%20Langs%20and%20Wolfgang%20Bogner%20and%20Stanislav%20Motyka%20and%20Bernhard%20Strasser%20and%20Polina%20Golland%20and%20Nalini%20Singh%20and%20Jorg%20Dietrich%20and%20Erik%20Uhlmann%20and%20Tracy%20Batchelor%20and%20Daniel%20Cahill%20and%20Malte%20Hoffmann%20and%20Antoine%20Klauser%20and%20Ovidiu%20C.%20Andronesi&entry.1292438233=Introduction%3A%20Altered%20neurometabolism%20is%20an%20important%20pathological%20mechanism%20in%20many%20neurological%20diseases%20and%20brain%20cancer%2C%20which%20can%20be%20mapped%20non-invasively%20by%20Magnetic%20Resonance%20Spectroscopic%20Imaging%20%28MRSI%29.%20Advanced%20MRSI%20using%20non-cartesian%20compressed-sense%20acquisition%20enables%20fast%20high-resolution%20metabolic%20imaging%20but%20has%20lengthy%20reconstruction%20times%20that%20limits%20throughput%20and%20needs%20expert%20user%20interaction.%20Here%2C%20we%20present%20a%20robust%20and%20efficient%20Deep%20Learning%20reconstruction%20to%20obtain%20high-quality%20metabolic%20maps.%0A%20%20Methods%3A%20Fast%20high-resolution%20whole-brain%20metabolic%20imaging%20was%20performed%20at%203.4%20mm%24%5E3%24%20isotropic%20resolution%20with%20acquisition%20times%20between%204%3A11-9%3A21%20min%3As%20using%20ECCENTRIC%20pulse%20sequence%20on%20a%207T%20MRI%20scanner.%20Data%20were%20acquired%20in%20a%20high-resolution%20phantom%20and%2027%20human%20participants%2C%20including%2022%20healthy%20volunteers%20and%205%20glioma%20patients.%20A%20deep%20neural%20network%20using%20recurring%20interlaced%20convolutional%20layers%20with%20joint%20dual-space%20feature%20representation%20was%20developed%20for%20deep%20learning%20ECCENTRIC%20reconstruction%20%28Deep-ER%29.%2021%20subjects%20were%20used%20for%20training%20and%206%20subjects%20for%20testing.%20Deep-ER%20performance%20was%20compared%20to%20conventional%20iterative%20Total%20Generalized%20Variation%20reconstruction%20using%20image%20and%20spectral%20quality%20metrics.%0A%20%20Results%3A%20Deep-ER%20demonstrated%20600-fold%20faster%20reconstruction%20than%20conventional%20methods%2C%20providing%20improved%20spatial-spectral%20quality%20and%20metabolite%20quantification%20with%2012%25-45%25%20%28P%3C0.05%29%20higher%20signal-to-noise%20and%208%25-50%25%20%28P%3C0.05%29%20smaller%20Cramer-Rao%20lower%20bounds.%20Metabolic%20images%20clearly%20visualize%20glioma%20tumor%20heterogeneity%20and%20boundary.%0A%20%20Conclusion%3A%20Deep-ER%20provides%20efficient%20and%20robust%20reconstruction%20for%20sparse-sampled%20MRSI.%20The%20accelerated%20acquisition-reconstruction%20MRSI%20is%20compatible%20with%20high-throughput%20imaging%20workflow.%20It%20is%20expected%20that%20such%20improved%20performance%20will%20facilitate%20basic%20and%20clinical%20MRSI%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2409.18303v2&entry.124074799=Read"},
{"title": "Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames", "author": "Jun Yang and Wenjie Xue and Sahar Ghavidel and Steven L. Waslander", "abstract": "Estimating the 6D pose of textureless objects from RGB images is an important problem in robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions, single-view based 6D pose estimators are still unable to handle a wide range of objects, motivating research towards multi-view pose estimation and next-best-view prediction that addresses these limitations. In this work, we propose a comprehensive active perception framework for estimating the 6D poses of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and efficiency. First, we estimate the 3D translation of each object, resolving scale and depth ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent task of determining the 3D orientation, which we achieve through canonical scale template matching. Building on this formulation, we then introduce an active perception strategy that predicts the next best camera viewpoint to capture an RGB image, effectively reducing object pose uncertainty and enhancing pose accuracy. We evaluate our method on the public ROBI and TOD datasets, as well as on our reconstructed transparent object dataset, T-ROBI. Under the same camera viewpoints, our multi-view pose estimation significantly outperforms state-of-the-art approaches. Furthermore, by leveraging our next-best-view strategy, our approach achieves high pose accuracy with fewer viewpoints than heuristic-based policies across all evaluated datasets. The accompanying video and T-ROBI dataset will be released on our project page: https://trailab.github.io/ActiveODPE.", "link": "http://arxiv.org/abs/2503.03726v2", "date": "2025-12-15", "relevancy": 2.4204, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6189}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%206D%20Pose%20Estimation%20for%20Textureless%20Objects%20using%20Multi-View%20RGB%20Frames&body=Title%3A%20Active%206D%20Pose%20Estimation%20for%20Textureless%20Objects%20using%20Multi-View%20RGB%20Frames%0AAuthor%3A%20Jun%20Yang%20and%20Wenjie%20Xue%20and%20Sahar%20Ghavidel%20and%20Steven%20L.%20Waslander%0AAbstract%3A%20Estimating%20the%206D%20pose%20of%20textureless%20objects%20from%20RGB%20images%20is%20an%20important%20problem%20in%20robotics.%20Due%20to%20appearance%20ambiguities%2C%20rotational%20symmetries%2C%20and%20severe%20occlusions%2C%20single-view%20based%206D%20pose%20estimators%20are%20still%20unable%20to%20handle%20a%20wide%20range%20of%20objects%2C%20motivating%20research%20towards%20multi-view%20pose%20estimation%20and%20next-best-view%20prediction%20that%20addresses%20these%20limitations.%20In%20this%20work%2C%20we%20propose%20a%20comprehensive%20active%20perception%20framework%20for%20estimating%20the%206D%20poses%20of%20textureless%20objects%20using%20only%20RGB%20images.%20Our%20approach%20is%20built%20upon%20a%20key%20idea%3A%20decoupling%20the%206D%20pose%20estimation%20into%20a%20two-step%20sequential%20process%20can%20greatly%20improve%20both%20accuracy%20and%20efficiency.%20First%2C%20we%20estimate%20the%203D%20translation%20of%20each%20object%2C%20resolving%20scale%20and%20depth%20ambiguities%20inherent%20to%20RGB%20images.%20These%20estimates%20are%20then%20used%20to%20simplify%20the%20subsequent%20task%20of%20determining%20the%203D%20orientation%2C%20which%20we%20achieve%20through%20canonical%20scale%20template%20matching.%20Building%20on%20this%20formulation%2C%20we%20then%20introduce%20an%20active%20perception%20strategy%20that%20predicts%20the%20next%20best%20camera%20viewpoint%20to%20capture%20an%20RGB%20image%2C%20effectively%20reducing%20object%20pose%20uncertainty%20and%20enhancing%20pose%20accuracy.%20We%20evaluate%20our%20method%20on%20the%20public%20ROBI%20and%20TOD%20datasets%2C%20as%20well%20as%20on%20our%20reconstructed%20transparent%20object%20dataset%2C%20T-ROBI.%20Under%20the%20same%20camera%20viewpoints%2C%20our%20multi-view%20pose%20estimation%20significantly%20outperforms%20state-of-the-art%20approaches.%20Furthermore%2C%20by%20leveraging%20our%20next-best-view%20strategy%2C%20our%20approach%20achieves%20high%20pose%20accuracy%20with%20fewer%20viewpoints%20than%20heuristic-based%20policies%20across%20all%20evaluated%20datasets.%20The%20accompanying%20video%20and%20T-ROBI%20dataset%20will%20be%20released%20on%20our%20project%20page%3A%20https%3A//trailab.github.io/ActiveODPE.%0ALink%3A%20http%3A//arxiv.org/abs/2503.03726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%25206D%2520Pose%2520Estimation%2520for%2520Textureless%2520Objects%2520using%2520Multi-View%2520RGB%2520Frames%26entry.906535625%3DJun%2520Yang%2520and%2520Wenjie%2520Xue%2520and%2520Sahar%2520Ghavidel%2520and%2520Steven%2520L.%2520Waslander%26entry.1292438233%3DEstimating%2520the%25206D%2520pose%2520of%2520textureless%2520objects%2520from%2520RGB%2520images%2520is%2520an%2520important%2520problem%2520in%2520robotics.%2520Due%2520to%2520appearance%2520ambiguities%252C%2520rotational%2520symmetries%252C%2520and%2520severe%2520occlusions%252C%2520single-view%2520based%25206D%2520pose%2520estimators%2520are%2520still%2520unable%2520to%2520handle%2520a%2520wide%2520range%2520of%2520objects%252C%2520motivating%2520research%2520towards%2520multi-view%2520pose%2520estimation%2520and%2520next-best-view%2520prediction%2520that%2520addresses%2520these%2520limitations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520comprehensive%2520active%2520perception%2520framework%2520for%2520estimating%2520the%25206D%2520poses%2520of%2520textureless%2520objects%2520using%2520only%2520RGB%2520images.%2520Our%2520approach%2520is%2520built%2520upon%2520a%2520key%2520idea%253A%2520decoupling%2520the%25206D%2520pose%2520estimation%2520into%2520a%2520two-step%2520sequential%2520process%2520can%2520greatly%2520improve%2520both%2520accuracy%2520and%2520efficiency.%2520First%252C%2520we%2520estimate%2520the%25203D%2520translation%2520of%2520each%2520object%252C%2520resolving%2520scale%2520and%2520depth%2520ambiguities%2520inherent%2520to%2520RGB%2520images.%2520These%2520estimates%2520are%2520then%2520used%2520to%2520simplify%2520the%2520subsequent%2520task%2520of%2520determining%2520the%25203D%2520orientation%252C%2520which%2520we%2520achieve%2520through%2520canonical%2520scale%2520template%2520matching.%2520Building%2520on%2520this%2520formulation%252C%2520we%2520then%2520introduce%2520an%2520active%2520perception%2520strategy%2520that%2520predicts%2520the%2520next%2520best%2520camera%2520viewpoint%2520to%2520capture%2520an%2520RGB%2520image%252C%2520effectively%2520reducing%2520object%2520pose%2520uncertainty%2520and%2520enhancing%2520pose%2520accuracy.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520public%2520ROBI%2520and%2520TOD%2520datasets%252C%2520as%2520well%2520as%2520on%2520our%2520reconstructed%2520transparent%2520object%2520dataset%252C%2520T-ROBI.%2520Under%2520the%2520same%2520camera%2520viewpoints%252C%2520our%2520multi-view%2520pose%2520estimation%2520significantly%2520outperforms%2520state-of-the-art%2520approaches.%2520Furthermore%252C%2520by%2520leveraging%2520our%2520next-best-view%2520strategy%252C%2520our%2520approach%2520achieves%2520high%2520pose%2520accuracy%2520with%2520fewer%2520viewpoints%2520than%2520heuristic-based%2520policies%2520across%2520all%2520evaluated%2520datasets.%2520The%2520accompanying%2520video%2520and%2520T-ROBI%2520dataset%2520will%2520be%2520released%2520on%2520our%2520project%2520page%253A%2520https%253A//trailab.github.io/ActiveODPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%206D%20Pose%20Estimation%20for%20Textureless%20Objects%20using%20Multi-View%20RGB%20Frames&entry.906535625=Jun%20Yang%20and%20Wenjie%20Xue%20and%20Sahar%20Ghavidel%20and%20Steven%20L.%20Waslander&entry.1292438233=Estimating%20the%206D%20pose%20of%20textureless%20objects%20from%20RGB%20images%20is%20an%20important%20problem%20in%20robotics.%20Due%20to%20appearance%20ambiguities%2C%20rotational%20symmetries%2C%20and%20severe%20occlusions%2C%20single-view%20based%206D%20pose%20estimators%20are%20still%20unable%20to%20handle%20a%20wide%20range%20of%20objects%2C%20motivating%20research%20towards%20multi-view%20pose%20estimation%20and%20next-best-view%20prediction%20that%20addresses%20these%20limitations.%20In%20this%20work%2C%20we%20propose%20a%20comprehensive%20active%20perception%20framework%20for%20estimating%20the%206D%20poses%20of%20textureless%20objects%20using%20only%20RGB%20images.%20Our%20approach%20is%20built%20upon%20a%20key%20idea%3A%20decoupling%20the%206D%20pose%20estimation%20into%20a%20two-step%20sequential%20process%20can%20greatly%20improve%20both%20accuracy%20and%20efficiency.%20First%2C%20we%20estimate%20the%203D%20translation%20of%20each%20object%2C%20resolving%20scale%20and%20depth%20ambiguities%20inherent%20to%20RGB%20images.%20These%20estimates%20are%20then%20used%20to%20simplify%20the%20subsequent%20task%20of%20determining%20the%203D%20orientation%2C%20which%20we%20achieve%20through%20canonical%20scale%20template%20matching.%20Building%20on%20this%20formulation%2C%20we%20then%20introduce%20an%20active%20perception%20strategy%20that%20predicts%20the%20next%20best%20camera%20viewpoint%20to%20capture%20an%20RGB%20image%2C%20effectively%20reducing%20object%20pose%20uncertainty%20and%20enhancing%20pose%20accuracy.%20We%20evaluate%20our%20method%20on%20the%20public%20ROBI%20and%20TOD%20datasets%2C%20as%20well%20as%20on%20our%20reconstructed%20transparent%20object%20dataset%2C%20T-ROBI.%20Under%20the%20same%20camera%20viewpoints%2C%20our%20multi-view%20pose%20estimation%20significantly%20outperforms%20state-of-the-art%20approaches.%20Furthermore%2C%20by%20leveraging%20our%20next-best-view%20strategy%2C%20our%20approach%20achieves%20high%20pose%20accuracy%20with%20fewer%20viewpoints%20than%20heuristic-based%20policies%20across%20all%20evaluated%20datasets.%20The%20accompanying%20video%20and%20T-ROBI%20dataset%20will%20be%20released%20on%20our%20project%20page%3A%20https%3A//trailab.github.io/ActiveODPE.&entry.1838667208=http%3A//arxiv.org/abs/2503.03726v2&entry.124074799=Read"},
{"title": "DynamiX: Large-Scale Dynamic Social Network Simulator", "author": "Yanhui Sun and Wu Liu and Wentao Wang and Hantao Yao and Jiebo Luo and Yongdong Zhang", "abstract": "Understanding the intrinsic mechanisms of social platforms is an urgent demand to maintain social stability. The rise of large language models provides significant potential for social network simulations to capture attitude dynamics and reproduce collective behaviors. However, existing studies mainly focus on scaling up agent populations, neglecting the dynamic evolution of social relationships. To address this gap, we introduce DynamiX, a novel large-scale social network simulator dedicated to dynamic social network modeling. DynamiX uses a dynamic hierarchy module for selecting core agents with key characteristics at each timestep, enabling accurate alignment of real-world adaptive switching of user roles. Furthermore, we design distinct dynamic social relationship modeling strategies for different user types. For opinion leaders, we propose an information-stream-based link prediction method recommending potential users with similar stances, simulating homogeneous connections, and autonomous behavior decisions. For ordinary users, we construct an inequality-oriented behavior decision-making module, effectively addressing unequal social interactions and capturing the patterns of relationship adjustments driven by multi-dimensional factors. Experimental results demonstrate that DynamiX exhibits marked improvements in attitude evolution simulation and collective behavior analysis compared to static networks. Besides, DynamiX opens a new theoretical perspective on follower growth prediction, providing empirical evidence for opinion leaders cultivation.", "link": "http://arxiv.org/abs/2507.19929v2", "date": "2025-12-15", "relevancy": 2.4103, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4942}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4871}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamiX%3A%20Large-Scale%20Dynamic%20Social%20Network%20Simulator&body=Title%3A%20DynamiX%3A%20Large-Scale%20Dynamic%20Social%20Network%20Simulator%0AAuthor%3A%20Yanhui%20Sun%20and%20Wu%20Liu%20and%20Wentao%20Wang%20and%20Hantao%20Yao%20and%20Jiebo%20Luo%20and%20Yongdong%20Zhang%0AAbstract%3A%20Understanding%20the%20intrinsic%20mechanisms%20of%20social%20platforms%20is%20an%20urgent%20demand%20to%20maintain%20social%20stability.%20The%20rise%20of%20large%20language%20models%20provides%20significant%20potential%20for%20social%20network%20simulations%20to%20capture%20attitude%20dynamics%20and%20reproduce%20collective%20behaviors.%20However%2C%20existing%20studies%20mainly%20focus%20on%20scaling%20up%20agent%20populations%2C%20neglecting%20the%20dynamic%20evolution%20of%20social%20relationships.%20To%20address%20this%20gap%2C%20we%20introduce%20DynamiX%2C%20a%20novel%20large-scale%20social%20network%20simulator%20dedicated%20to%20dynamic%20social%20network%20modeling.%20DynamiX%20uses%20a%20dynamic%20hierarchy%20module%20for%20selecting%20core%20agents%20with%20key%20characteristics%20at%20each%20timestep%2C%20enabling%20accurate%20alignment%20of%20real-world%20adaptive%20switching%20of%20user%20roles.%20Furthermore%2C%20we%20design%20distinct%20dynamic%20social%20relationship%20modeling%20strategies%20for%20different%20user%20types.%20For%20opinion%20leaders%2C%20we%20propose%20an%20information-stream-based%20link%20prediction%20method%20recommending%20potential%20users%20with%20similar%20stances%2C%20simulating%20homogeneous%20connections%2C%20and%20autonomous%20behavior%20decisions.%20For%20ordinary%20users%2C%20we%20construct%20an%20inequality-oriented%20behavior%20decision-making%20module%2C%20effectively%20addressing%20unequal%20social%20interactions%20and%20capturing%20the%20patterns%20of%20relationship%20adjustments%20driven%20by%20multi-dimensional%20factors.%20Experimental%20results%20demonstrate%20that%20DynamiX%20exhibits%20marked%20improvements%20in%20attitude%20evolution%20simulation%20and%20collective%20behavior%20analysis%20compared%20to%20static%20networks.%20Besides%2C%20DynamiX%20opens%20a%20new%20theoretical%20perspective%20on%20follower%20growth%20prediction%2C%20providing%20empirical%20evidence%20for%20opinion%20leaders%20cultivation.%0ALink%3A%20http%3A//arxiv.org/abs/2507.19929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamiX%253A%2520Large-Scale%2520Dynamic%2520Social%2520Network%2520Simulator%26entry.906535625%3DYanhui%2520Sun%2520and%2520Wu%2520Liu%2520and%2520Wentao%2520Wang%2520and%2520Hantao%2520Yao%2520and%2520Jiebo%2520Luo%2520and%2520Yongdong%2520Zhang%26entry.1292438233%3DUnderstanding%2520the%2520intrinsic%2520mechanisms%2520of%2520social%2520platforms%2520is%2520an%2520urgent%2520demand%2520to%2520maintain%2520social%2520stability.%2520The%2520rise%2520of%2520large%2520language%2520models%2520provides%2520significant%2520potential%2520for%2520social%2520network%2520simulations%2520to%2520capture%2520attitude%2520dynamics%2520and%2520reproduce%2520collective%2520behaviors.%2520However%252C%2520existing%2520studies%2520mainly%2520focus%2520on%2520scaling%2520up%2520agent%2520populations%252C%2520neglecting%2520the%2520dynamic%2520evolution%2520of%2520social%2520relationships.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520DynamiX%252C%2520a%2520novel%2520large-scale%2520social%2520network%2520simulator%2520dedicated%2520to%2520dynamic%2520social%2520network%2520modeling.%2520DynamiX%2520uses%2520a%2520dynamic%2520hierarchy%2520module%2520for%2520selecting%2520core%2520agents%2520with%2520key%2520characteristics%2520at%2520each%2520timestep%252C%2520enabling%2520accurate%2520alignment%2520of%2520real-world%2520adaptive%2520switching%2520of%2520user%2520roles.%2520Furthermore%252C%2520we%2520design%2520distinct%2520dynamic%2520social%2520relationship%2520modeling%2520strategies%2520for%2520different%2520user%2520types.%2520For%2520opinion%2520leaders%252C%2520we%2520propose%2520an%2520information-stream-based%2520link%2520prediction%2520method%2520recommending%2520potential%2520users%2520with%2520similar%2520stances%252C%2520simulating%2520homogeneous%2520connections%252C%2520and%2520autonomous%2520behavior%2520decisions.%2520For%2520ordinary%2520users%252C%2520we%2520construct%2520an%2520inequality-oriented%2520behavior%2520decision-making%2520module%252C%2520effectively%2520addressing%2520unequal%2520social%2520interactions%2520and%2520capturing%2520the%2520patterns%2520of%2520relationship%2520adjustments%2520driven%2520by%2520multi-dimensional%2520factors.%2520Experimental%2520results%2520demonstrate%2520that%2520DynamiX%2520exhibits%2520marked%2520improvements%2520in%2520attitude%2520evolution%2520simulation%2520and%2520collective%2520behavior%2520analysis%2520compared%2520to%2520static%2520networks.%2520Besides%252C%2520DynamiX%2520opens%2520a%2520new%2520theoretical%2520perspective%2520on%2520follower%2520growth%2520prediction%252C%2520providing%2520empirical%2520evidence%2520for%2520opinion%2520leaders%2520cultivation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamiX%3A%20Large-Scale%20Dynamic%20Social%20Network%20Simulator&entry.906535625=Yanhui%20Sun%20and%20Wu%20Liu%20and%20Wentao%20Wang%20and%20Hantao%20Yao%20and%20Jiebo%20Luo%20and%20Yongdong%20Zhang&entry.1292438233=Understanding%20the%20intrinsic%20mechanisms%20of%20social%20platforms%20is%20an%20urgent%20demand%20to%20maintain%20social%20stability.%20The%20rise%20of%20large%20language%20models%20provides%20significant%20potential%20for%20social%20network%20simulations%20to%20capture%20attitude%20dynamics%20and%20reproduce%20collective%20behaviors.%20However%2C%20existing%20studies%20mainly%20focus%20on%20scaling%20up%20agent%20populations%2C%20neglecting%20the%20dynamic%20evolution%20of%20social%20relationships.%20To%20address%20this%20gap%2C%20we%20introduce%20DynamiX%2C%20a%20novel%20large-scale%20social%20network%20simulator%20dedicated%20to%20dynamic%20social%20network%20modeling.%20DynamiX%20uses%20a%20dynamic%20hierarchy%20module%20for%20selecting%20core%20agents%20with%20key%20characteristics%20at%20each%20timestep%2C%20enabling%20accurate%20alignment%20of%20real-world%20adaptive%20switching%20of%20user%20roles.%20Furthermore%2C%20we%20design%20distinct%20dynamic%20social%20relationship%20modeling%20strategies%20for%20different%20user%20types.%20For%20opinion%20leaders%2C%20we%20propose%20an%20information-stream-based%20link%20prediction%20method%20recommending%20potential%20users%20with%20similar%20stances%2C%20simulating%20homogeneous%20connections%2C%20and%20autonomous%20behavior%20decisions.%20For%20ordinary%20users%2C%20we%20construct%20an%20inequality-oriented%20behavior%20decision-making%20module%2C%20effectively%20addressing%20unequal%20social%20interactions%20and%20capturing%20the%20patterns%20of%20relationship%20adjustments%20driven%20by%20multi-dimensional%20factors.%20Experimental%20results%20demonstrate%20that%20DynamiX%20exhibits%20marked%20improvements%20in%20attitude%20evolution%20simulation%20and%20collective%20behavior%20analysis%20compared%20to%20static%20networks.%20Besides%2C%20DynamiX%20opens%20a%20new%20theoretical%20perspective%20on%20follower%20growth%20prediction%2C%20providing%20empirical%20evidence%20for%20opinion%20leaders%20cultivation.&entry.1838667208=http%3A//arxiv.org/abs/2507.19929v2&entry.124074799=Read"},
{"title": "Large-Language Memorization During the Classification of United States Supreme Court Cases", "author": "John E. Ortega and Dhruv D. Joshi and Matt P. Borkowski", "abstract": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.", "link": "http://arxiv.org/abs/2512.13654v1", "date": "2025-12-15", "relevancy": 2.4029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Language%20Memorization%20During%20the%20Classification%20of%20United%20States%20Supreme%20Court%20Cases&body=Title%3A%20Large-Language%20Memorization%20During%20the%20Classification%20of%20United%20States%20Supreme%20Court%20Cases%0AAuthor%3A%20John%20E.%20Ortega%20and%20Dhruv%20D.%20Joshi%20and%20Matt%20P.%20Borkowski%0AAbstract%3A%20Large-language%20models%20%28LLMs%29%20have%20been%20shown%20to%20respond%20in%20a%20variety%20of%20ways%20for%20classification%20tasks%20outside%20of%20question-answering.%20LLM%20responses%20are%20sometimes%20called%20%22hallucinations%22%20since%20the%20output%20is%20not%20what%20is%20ex%20pected.%20Memorization%20strategies%20in%20LLMs%20are%20being%20studied%20in%20detail%2C%20with%20the%20goal%20of%20understanding%20how%20LLMs%20respond.%20We%20perform%20a%20deep%20dive%20into%20a%20classification%20task%20based%20on%20United%20States%20Supreme%20Court%20%28SCOTUS%29%20decisions.%20The%20SCOTUS%20corpus%20is%20an%20ideal%20classification%20task%20to%20study%20for%20LLM%20memory%20accuracy%20because%20it%20presents%20significant%20challenges%20due%20to%20extensive%20sentence%20length%2C%20complex%20legal%20terminology%2C%20non-standard%20structure%2C%20and%20domain-specific%20vocabulary.%20Experimentation%20is%20performed%20with%20the%20latest%20LLM%20fine%20tuning%20and%20retrieval-based%20approaches%2C%20such%20as%20parameter-efficient%20fine-tuning%2C%20auto-modeling%2C%20and%20others%2C%20on%20two%20traditional%20category-based%20SCOTUS%20classification%20tasks%3A%20one%20with%2015%20labeled%20topics%20and%20another%20with%20279.%20We%20show%20that%20prompt-based%20models%20with%20memories%2C%20such%20as%20DeepSeek%2C%20can%20be%20more%20robust%20than%20previous%20BERT-based%20models%20on%20both%20tasks%20scoring%20about%202%20points%20better%20than%20previous%20models%20not%20based%20on%20prompting.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Language%2520Memorization%2520During%2520the%2520Classification%2520of%2520United%2520States%2520Supreme%2520Court%2520Cases%26entry.906535625%3DJohn%2520E.%2520Ortega%2520and%2520Dhruv%2520D.%2520Joshi%2520and%2520Matt%2520P.%2520Borkowski%26entry.1292438233%3DLarge-language%2520models%2520%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520respond%2520in%2520a%2520variety%2520of%2520ways%2520for%2520classification%2520tasks%2520outside%2520of%2520question-answering.%2520LLM%2520responses%2520are%2520sometimes%2520called%2520%2522hallucinations%2522%2520since%2520the%2520output%2520is%2520not%2520what%2520is%2520ex%2520pected.%2520Memorization%2520strategies%2520in%2520LLMs%2520are%2520being%2520studied%2520in%2520detail%252C%2520with%2520the%2520goal%2520of%2520understanding%2520how%2520LLMs%2520respond.%2520We%2520perform%2520a%2520deep%2520dive%2520into%2520a%2520classification%2520task%2520based%2520on%2520United%2520States%2520Supreme%2520Court%2520%2528SCOTUS%2529%2520decisions.%2520The%2520SCOTUS%2520corpus%2520is%2520an%2520ideal%2520classification%2520task%2520to%2520study%2520for%2520LLM%2520memory%2520accuracy%2520because%2520it%2520presents%2520significant%2520challenges%2520due%2520to%2520extensive%2520sentence%2520length%252C%2520complex%2520legal%2520terminology%252C%2520non-standard%2520structure%252C%2520and%2520domain-specific%2520vocabulary.%2520Experimentation%2520is%2520performed%2520with%2520the%2520latest%2520LLM%2520fine%2520tuning%2520and%2520retrieval-based%2520approaches%252C%2520such%2520as%2520parameter-efficient%2520fine-tuning%252C%2520auto-modeling%252C%2520and%2520others%252C%2520on%2520two%2520traditional%2520category-based%2520SCOTUS%2520classification%2520tasks%253A%2520one%2520with%252015%2520labeled%2520topics%2520and%2520another%2520with%2520279.%2520We%2520show%2520that%2520prompt-based%2520models%2520with%2520memories%252C%2520such%2520as%2520DeepSeek%252C%2520can%2520be%2520more%2520robust%2520than%2520previous%2520BERT-based%2520models%2520on%2520both%2520tasks%2520scoring%2520about%25202%2520points%2520better%2520than%2520previous%2520models%2520not%2520based%2520on%2520prompting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Language%20Memorization%20During%20the%20Classification%20of%20United%20States%20Supreme%20Court%20Cases&entry.906535625=John%20E.%20Ortega%20and%20Dhruv%20D.%20Joshi%20and%20Matt%20P.%20Borkowski&entry.1292438233=Large-language%20models%20%28LLMs%29%20have%20been%20shown%20to%20respond%20in%20a%20variety%20of%20ways%20for%20classification%20tasks%20outside%20of%20question-answering.%20LLM%20responses%20are%20sometimes%20called%20%22hallucinations%22%20since%20the%20output%20is%20not%20what%20is%20ex%20pected.%20Memorization%20strategies%20in%20LLMs%20are%20being%20studied%20in%20detail%2C%20with%20the%20goal%20of%20understanding%20how%20LLMs%20respond.%20We%20perform%20a%20deep%20dive%20into%20a%20classification%20task%20based%20on%20United%20States%20Supreme%20Court%20%28SCOTUS%29%20decisions.%20The%20SCOTUS%20corpus%20is%20an%20ideal%20classification%20task%20to%20study%20for%20LLM%20memory%20accuracy%20because%20it%20presents%20significant%20challenges%20due%20to%20extensive%20sentence%20length%2C%20complex%20legal%20terminology%2C%20non-standard%20structure%2C%20and%20domain-specific%20vocabulary.%20Experimentation%20is%20performed%20with%20the%20latest%20LLM%20fine%20tuning%20and%20retrieval-based%20approaches%2C%20such%20as%20parameter-efficient%20fine-tuning%2C%20auto-modeling%2C%20and%20others%2C%20on%20two%20traditional%20category-based%20SCOTUS%20classification%20tasks%3A%20one%20with%2015%20labeled%20topics%20and%20another%20with%20279.%20We%20show%20that%20prompt-based%20models%20with%20memories%2C%20such%20as%20DeepSeek%2C%20can%20be%20more%20robust%20than%20previous%20BERT-based%20models%20on%20both%20tasks%20scoring%20about%202%20points%20better%20than%20previous%20models%20not%20based%20on%20prompting.&entry.1838667208=http%3A//arxiv.org/abs/2512.13654v1&entry.124074799=Read"},
{"title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "author": "Jingfeng Yao and Yuda Song and Yucong Zhou and Xinggang Wang", "abstract": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "link": "http://arxiv.org/abs/2512.13687v1", "date": "2025-12-15", "relevancy": 2.3731, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6173}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6161}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20Pre-training%20of%20Visual%20Tokenizers%20for%20Generation&body=Title%3A%20Towards%20Scalable%20Pre-training%20of%20Visual%20Tokenizers%20for%20Generation%0AAuthor%3A%20Jingfeng%20Yao%20and%20Yuda%20Song%20and%20Yucong%20Zhou%20and%20Xinggang%20Wang%0AAbstract%3A%20The%20quality%20of%20the%20latent%20space%20in%20visual%20tokenizers%20%28e.g.%2C%20VAEs%29%20is%20crucial%20for%20modern%20generative%20models.%20However%2C%20the%20standard%20reconstruction-based%20training%20paradigm%20produces%20a%20latent%20space%20that%20is%20biased%20towards%20low-level%20information%2C%20leading%20to%20a%20foundation%20flaw%3A%20better%20pixel-level%20accuracy%20does%20not%20lead%20to%20higher-quality%20generation.%20This%20implies%20that%20pouring%20extensive%20compute%20into%20visual%20tokenizer%20pre-training%20translates%20poorly%20to%20improved%20performance%20in%20generation.%20We%20identify%20this%20as%20the%20%60%60pre-training%20scaling%20problem%60%60%20and%20suggest%20a%20necessary%20shift%3A%20to%20be%20effective%20for%20generation%2C%20a%20latent%20space%20must%20concisely%20represent%20high-level%20semantics.%20We%20present%20VTP%2C%20a%20unified%20visual%20tokenizer%20pre-training%20framework%2C%20pioneering%20the%20joint%20optimization%20of%20image-text%20contrastive%2C%20self-supervised%2C%20and%20reconstruction%20losses.%20Our%20large-scale%20study%20reveals%20two%20principal%20findings%3A%20%281%29%20understanding%20is%20a%20key%20driver%20of%20generation%2C%20and%20%282%29%20much%20better%20scaling%20properties%2C%20where%20generative%20performance%20scales%20effectively%20with%20compute%2C%20parameters%2C%20and%20data%20allocated%20to%20the%20pretraining%20of%20the%20visual%20tokenizer.%20After%20large-scale%20pre-training%2C%20our%20tokenizer%20delivers%20a%20competitive%20profile%20%2878.2%20zero-shot%20accuracy%20and%200.36%20rFID%20on%20ImageNet%29%20and%204.1%20times%20faster%20convergence%20on%20generation%20compared%20to%20advanced%20distillation%20methods.%20More%20importantly%2C%20it%20scales%20effectively%3A%20without%20modifying%20standard%20DiT%20training%20specs%2C%20solely%20investing%20more%20FLOPS%20in%20pretraining%20VTP%20achieves%2065.8%5C%25%20FID%20improvement%20in%20downstream%20generation%2C%20while%20conventional%20autoencoder%20stagnates%20very%20early%20at%201/10%20FLOPS.%20Our%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/MiniMax-AI/VTP.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520Pre-training%2520of%2520Visual%2520Tokenizers%2520for%2520Generation%26entry.906535625%3DJingfeng%2520Yao%2520and%2520Yuda%2520Song%2520and%2520Yucong%2520Zhou%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DThe%2520quality%2520of%2520the%2520latent%2520space%2520in%2520visual%2520tokenizers%2520%2528e.g.%252C%2520VAEs%2529%2520is%2520crucial%2520for%2520modern%2520generative%2520models.%2520However%252C%2520the%2520standard%2520reconstruction-based%2520training%2520paradigm%2520produces%2520a%2520latent%2520space%2520that%2520is%2520biased%2520towards%2520low-level%2520information%252C%2520leading%2520to%2520a%2520foundation%2520flaw%253A%2520better%2520pixel-level%2520accuracy%2520does%2520not%2520lead%2520to%2520higher-quality%2520generation.%2520This%2520implies%2520that%2520pouring%2520extensive%2520compute%2520into%2520visual%2520tokenizer%2520pre-training%2520translates%2520poorly%2520to%2520improved%2520performance%2520in%2520generation.%2520We%2520identify%2520this%2520as%2520the%2520%2560%2560pre-training%2520scaling%2520problem%2560%2560%2520and%2520suggest%2520a%2520necessary%2520shift%253A%2520to%2520be%2520effective%2520for%2520generation%252C%2520a%2520latent%2520space%2520must%2520concisely%2520represent%2520high-level%2520semantics.%2520We%2520present%2520VTP%252C%2520a%2520unified%2520visual%2520tokenizer%2520pre-training%2520framework%252C%2520pioneering%2520the%2520joint%2520optimization%2520of%2520image-text%2520contrastive%252C%2520self-supervised%252C%2520and%2520reconstruction%2520losses.%2520Our%2520large-scale%2520study%2520reveals%2520two%2520principal%2520findings%253A%2520%25281%2529%2520understanding%2520is%2520a%2520key%2520driver%2520of%2520generation%252C%2520and%2520%25282%2529%2520much%2520better%2520scaling%2520properties%252C%2520where%2520generative%2520performance%2520scales%2520effectively%2520with%2520compute%252C%2520parameters%252C%2520and%2520data%2520allocated%2520to%2520the%2520pretraining%2520of%2520the%2520visual%2520tokenizer.%2520After%2520large-scale%2520pre-training%252C%2520our%2520tokenizer%2520delivers%2520a%2520competitive%2520profile%2520%252878.2%2520zero-shot%2520accuracy%2520and%25200.36%2520rFID%2520on%2520ImageNet%2529%2520and%25204.1%2520times%2520faster%2520convergence%2520on%2520generation%2520compared%2520to%2520advanced%2520distillation%2520methods.%2520More%2520importantly%252C%2520it%2520scales%2520effectively%253A%2520without%2520modifying%2520standard%2520DiT%2520training%2520specs%252C%2520solely%2520investing%2520more%2520FLOPS%2520in%2520pretraining%2520VTP%2520achieves%252065.8%255C%2525%2520FID%2520improvement%2520in%2520downstream%2520generation%252C%2520while%2520conventional%2520autoencoder%2520stagnates%2520very%2520early%2520at%25201/10%2520FLOPS.%2520Our%2520pre-trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/MiniMax-AI/VTP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20Pre-training%20of%20Visual%20Tokenizers%20for%20Generation&entry.906535625=Jingfeng%20Yao%20and%20Yuda%20Song%20and%20Yucong%20Zhou%20and%20Xinggang%20Wang&entry.1292438233=The%20quality%20of%20the%20latent%20space%20in%20visual%20tokenizers%20%28e.g.%2C%20VAEs%29%20is%20crucial%20for%20modern%20generative%20models.%20However%2C%20the%20standard%20reconstruction-based%20training%20paradigm%20produces%20a%20latent%20space%20that%20is%20biased%20towards%20low-level%20information%2C%20leading%20to%20a%20foundation%20flaw%3A%20better%20pixel-level%20accuracy%20does%20not%20lead%20to%20higher-quality%20generation.%20This%20implies%20that%20pouring%20extensive%20compute%20into%20visual%20tokenizer%20pre-training%20translates%20poorly%20to%20improved%20performance%20in%20generation.%20We%20identify%20this%20as%20the%20%60%60pre-training%20scaling%20problem%60%60%20and%20suggest%20a%20necessary%20shift%3A%20to%20be%20effective%20for%20generation%2C%20a%20latent%20space%20must%20concisely%20represent%20high-level%20semantics.%20We%20present%20VTP%2C%20a%20unified%20visual%20tokenizer%20pre-training%20framework%2C%20pioneering%20the%20joint%20optimization%20of%20image-text%20contrastive%2C%20self-supervised%2C%20and%20reconstruction%20losses.%20Our%20large-scale%20study%20reveals%20two%20principal%20findings%3A%20%281%29%20understanding%20is%20a%20key%20driver%20of%20generation%2C%20and%20%282%29%20much%20better%20scaling%20properties%2C%20where%20generative%20performance%20scales%20effectively%20with%20compute%2C%20parameters%2C%20and%20data%20allocated%20to%20the%20pretraining%20of%20the%20visual%20tokenizer.%20After%20large-scale%20pre-training%2C%20our%20tokenizer%20delivers%20a%20competitive%20profile%20%2878.2%20zero-shot%20accuracy%20and%200.36%20rFID%20on%20ImageNet%29%20and%204.1%20times%20faster%20convergence%20on%20generation%20compared%20to%20advanced%20distillation%20methods.%20More%20importantly%2C%20it%20scales%20effectively%3A%20without%20modifying%20standard%20DiT%20training%20specs%2C%20solely%20investing%20more%20FLOPS%20in%20pretraining%20VTP%20achieves%2065.8%5C%25%20FID%20improvement%20in%20downstream%20generation%2C%20while%20conventional%20autoencoder%20stagnates%20very%20early%20at%201/10%20FLOPS.%20Our%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/MiniMax-AI/VTP.&entry.1838667208=http%3A//arxiv.org/abs/2512.13687v1&entry.124074799=Read"},
{"title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution", "author": "Long Cui and Weiyun Wang and Jie Shao and Zichen Wen and Gen Luo and Linfeng Zhang and Yanting Zhang and Yu Qiao and Wenhai Wang", "abstract": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.", "link": "http://arxiv.org/abs/2510.12793v2", "date": "2025-12-15", "relevancy": 2.3681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution&body=Title%3A%20ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution%0AAuthor%3A%20Long%20Cui%20and%20Weiyun%20Wang%20and%20Jie%20Shao%20and%20Zichen%20Wen%20and%20Gen%20Luo%20and%20Linfeng%20Zhang%20and%20Yanting%20Zhang%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%0AAbstract%3A%20Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20suffer%20from%20increased%20inference%20costs%20due%20to%20the%20additional%20vision%20tokens%20introduced%20by%20image%20inputs.%20In%20this%20work%2C%20we%20propose%20Visual%20Consistency%20Learning%20%28ViCO%29%2C%20a%20novel%20training%20algorithm%20that%20enables%20the%20model%20to%20represent%20images%20of%20varying%20semantic%20complexities%20using%20different%20numbers%20of%20vision%20tokens.%20The%20key%20idea%20behind%20our%20method%20is%20to%20employ%20multiple%20MLP%20connectors%2C%20each%20with%20a%20different%20image%20compression%20ratio%2C%20to%20downsample%20the%20vision%20tokens%20based%20on%20the%20semantic%20complexity%20of%20the%20image.%20During%20training%2C%20we%20minimize%20the%20KL%20divergence%20between%20the%20responses%20conditioned%20on%20different%20MLP%20connectors.%20At%20inference%20time%2C%20we%20introduce%20an%20image%20router%2C%20termed%20Visual%20Resolution%20Router%20%28ViR%29%2C%20that%20automatically%20selects%20the%20appropriate%20compression%20rate%20for%20each%20image%20patch.%20Compared%20with%20existing%20dynamic%20high-resolution%20strategies%2C%20which%20adjust%20the%20number%20of%20visual%20tokens%20based%20on%20image%20resolutions%2C%20our%20method%20dynamically%20adapts%20the%20number%20of%20visual%20tokens%20according%20to%20semantic%20complexity.%20Experimental%20results%20demonstrate%20that%20our%20method%20can%20reduce%20the%20number%20of%20vision%20tokens%20by%20up%20to%2050%25%20while%20maintaining%20the%20model%27s%20perception%2C%20reasoning%2C%20and%20OCR%20capabilities.%20We%20hope%20this%20work%20will%20contribute%20to%20the%20development%20of%20more%20efficient%20MLLMs.%20The%20code%20and%20models%20will%20be%20released%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViCO%253A%2520A%2520Training%2520Strategy%2520towards%2520Semantic%2520Aware%2520Dynamic%2520High-Resolution%26entry.906535625%3DLong%2520Cui%2520and%2520Weiyun%2520Wang%2520and%2520Jie%2520Shao%2520and%2520Zichen%2520Wen%2520and%2520Gen%2520Luo%2520and%2520Linfeng%2520Zhang%2520and%2520Yanting%2520Zhang%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%26entry.1292438233%3DExisting%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520suffer%2520from%2520increased%2520inference%2520costs%2520due%2520to%2520the%2520additional%2520vision%2520tokens%2520introduced%2520by%2520image%2520inputs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Visual%2520Consistency%2520Learning%2520%2528ViCO%2529%252C%2520a%2520novel%2520training%2520algorithm%2520that%2520enables%2520the%2520model%2520to%2520represent%2520images%2520of%2520varying%2520semantic%2520complexities%2520using%2520different%2520numbers%2520of%2520vision%2520tokens.%2520The%2520key%2520idea%2520behind%2520our%2520method%2520is%2520to%2520employ%2520multiple%2520MLP%2520connectors%252C%2520each%2520with%2520a%2520different%2520image%2520compression%2520ratio%252C%2520to%2520downsample%2520the%2520vision%2520tokens%2520based%2520on%2520the%2520semantic%2520complexity%2520of%2520the%2520image.%2520During%2520training%252C%2520we%2520minimize%2520the%2520KL%2520divergence%2520between%2520the%2520responses%2520conditioned%2520on%2520different%2520MLP%2520connectors.%2520At%2520inference%2520time%252C%2520we%2520introduce%2520an%2520image%2520router%252C%2520termed%2520Visual%2520Resolution%2520Router%2520%2528ViR%2529%252C%2520that%2520automatically%2520selects%2520the%2520appropriate%2520compression%2520rate%2520for%2520each%2520image%2520patch.%2520Compared%2520with%2520existing%2520dynamic%2520high-resolution%2520strategies%252C%2520which%2520adjust%2520the%2520number%2520of%2520visual%2520tokens%2520based%2520on%2520image%2520resolutions%252C%2520our%2520method%2520dynamically%2520adapts%2520the%2520number%2520of%2520visual%2520tokens%2520according%2520to%2520semantic%2520complexity.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520can%2520reduce%2520the%2520number%2520of%2520vision%2520tokens%2520by%2520up%2520to%252050%2525%2520while%2520maintaining%2520the%2520model%2527s%2520perception%252C%2520reasoning%252C%2520and%2520OCR%2520capabilities.%2520We%2520hope%2520this%2520work%2520will%2520contribute%2520to%2520the%2520development%2520of%2520more%2520efficient%2520MLLMs.%2520The%2520code%2520and%2520models%2520will%2520be%2520released%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViCO%3A%20A%20Training%20Strategy%20towards%20Semantic%20Aware%20Dynamic%20High-Resolution&entry.906535625=Long%20Cui%20and%20Weiyun%20Wang%20and%20Jie%20Shao%20and%20Zichen%20Wen%20and%20Gen%20Luo%20and%20Linfeng%20Zhang%20and%20Yanting%20Zhang%20and%20Yu%20Qiao%20and%20Wenhai%20Wang&entry.1292438233=Existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20suffer%20from%20increased%20inference%20costs%20due%20to%20the%20additional%20vision%20tokens%20introduced%20by%20image%20inputs.%20In%20this%20work%2C%20we%20propose%20Visual%20Consistency%20Learning%20%28ViCO%29%2C%20a%20novel%20training%20algorithm%20that%20enables%20the%20model%20to%20represent%20images%20of%20varying%20semantic%20complexities%20using%20different%20numbers%20of%20vision%20tokens.%20The%20key%20idea%20behind%20our%20method%20is%20to%20employ%20multiple%20MLP%20connectors%2C%20each%20with%20a%20different%20image%20compression%20ratio%2C%20to%20downsample%20the%20vision%20tokens%20based%20on%20the%20semantic%20complexity%20of%20the%20image.%20During%20training%2C%20we%20minimize%20the%20KL%20divergence%20between%20the%20responses%20conditioned%20on%20different%20MLP%20connectors.%20At%20inference%20time%2C%20we%20introduce%20an%20image%20router%2C%20termed%20Visual%20Resolution%20Router%20%28ViR%29%2C%20that%20automatically%20selects%20the%20appropriate%20compression%20rate%20for%20each%20image%20patch.%20Compared%20with%20existing%20dynamic%20high-resolution%20strategies%2C%20which%20adjust%20the%20number%20of%20visual%20tokens%20based%20on%20image%20resolutions%2C%20our%20method%20dynamically%20adapts%20the%20number%20of%20visual%20tokens%20according%20to%20semantic%20complexity.%20Experimental%20results%20demonstrate%20that%20our%20method%20can%20reduce%20the%20number%20of%20vision%20tokens%20by%20up%20to%2050%25%20while%20maintaining%20the%20model%27s%20perception%2C%20reasoning%2C%20and%20OCR%20capabilities.%20We%20hope%20this%20work%20will%20contribute%20to%20the%20development%20of%20more%20efficient%20MLLMs.%20The%20code%20and%20models%20will%20be%20released%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2510.12793v2&entry.124074799=Read"},
{"title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training", "author": "Reza Shirkavand and Peiran Yu and Qi He and Heng Huang", "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning (PEFT) methods address these by freezing most model parameters and training only a small subset. However, PEFT often underperforms compared to full fine-tuning when high task-specific accuracy is required. Zeroth-Order (ZO) methods fine-tune the entire pre-trained model without back-propagation, estimating gradients through forward passes only. While memory-efficient, ZO methods suffer from slow convergence and high sensitivity to prompt selection. We bridge these two worlds with Bilevel-ZOFO, a bilevel optimization method that couples fast, local FO-PEFT adaptation at the inner level with stable, memory-efficient ZO updates of the full backbone at the outer level. The FO-PEFT inner loop performs fast, low-memory local adaptation that reduces the variance of ZO estimates and stabilizes the search, guiding the outer ZO updates of the full backbone and reducing prompt sensitivity. In the mean time, the outer ZO provides better generalization ability for PEFT. We provide theoretical convergence guarantees and empirically demonstrate that Bilevel-ZOFO significantly outperforms existing ZO and FO-PEFT methods, achieving 2-4 times faster training while maintaining similar memory efficiency. Additionally, we show by updating the backbone with ZO and adapting only a tiny FO-PEFT block per task, Bilevel-ZOFO combines full-model capacity with few-shot efficiency, making it a very efficient meta-learning algorithm that quickly adapts to new tasks.", "link": "http://arxiv.org/abs/2502.03604v3", "date": "2025-12-15", "relevancy": 2.3437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bilevel%20ZOFO%3A%20Efficient%20LLM%20Fine-Tuning%20and%20Meta-Training&body=Title%3A%20Bilevel%20ZOFO%3A%20Efficient%20LLM%20Fine-Tuning%20and%20Meta-Training%0AAuthor%3A%20Reza%20Shirkavand%20and%20Peiran%20Yu%20and%20Qi%20He%20and%20Heng%20Huang%0AAbstract%3A%20Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20for%20downstream%20tasks%20using%20First-Order%20%28FO%29%20optimizers%20presents%20significant%20computational%20challenges.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20these%20by%20freezing%20most%20model%20parameters%20and%20training%20only%20a%20small%20subset.%20However%2C%20PEFT%20often%20underperforms%20compared%20to%20full%20fine-tuning%20when%20high%20task-specific%20accuracy%20is%20required.%20Zeroth-Order%20%28ZO%29%20methods%20fine-tune%20the%20entire%20pre-trained%20model%20without%20back-propagation%2C%20estimating%20gradients%20through%20forward%20passes%20only.%20While%20memory-efficient%2C%20ZO%20methods%20suffer%20from%20slow%20convergence%20and%20high%20sensitivity%20to%20prompt%20selection.%20We%20bridge%20these%20two%20worlds%20with%20Bilevel-ZOFO%2C%20a%20bilevel%20optimization%20method%20that%20couples%20fast%2C%20local%20FO-PEFT%20adaptation%20at%20the%20inner%20level%20with%20stable%2C%20memory-efficient%20ZO%20updates%20of%20the%20full%20backbone%20at%20the%20outer%20level.%20The%20FO-PEFT%20inner%20loop%20performs%20fast%2C%20low-memory%20local%20adaptation%20that%20reduces%20the%20variance%20of%20ZO%20estimates%20and%20stabilizes%20the%20search%2C%20guiding%20the%20outer%20ZO%20updates%20of%20the%20full%20backbone%20and%20reducing%20prompt%20sensitivity.%20In%20the%20mean%20time%2C%20the%20outer%20ZO%20provides%20better%20generalization%20ability%20for%20PEFT.%20We%20provide%20theoretical%20convergence%20guarantees%20and%20empirically%20demonstrate%20that%20Bilevel-ZOFO%20significantly%20outperforms%20existing%20ZO%20and%20FO-PEFT%20methods%2C%20achieving%202-4%20times%20faster%20training%20while%20maintaining%20similar%20memory%20efficiency.%20Additionally%2C%20we%20show%20by%20updating%20the%20backbone%20with%20ZO%20and%20adapting%20only%20a%20tiny%20FO-PEFT%20block%20per%20task%2C%20Bilevel-ZOFO%20combines%20full-model%20capacity%20with%20few-shot%20efficiency%2C%20making%20it%20a%20very%20efficient%20meta-learning%20algorithm%20that%20quickly%20adapts%20to%20new%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2502.03604v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBilevel%2520ZOFO%253A%2520Efficient%2520LLM%2520Fine-Tuning%2520and%2520Meta-Training%26entry.906535625%3DReza%2520Shirkavand%2520and%2520Peiran%2520Yu%2520and%2520Qi%2520He%2520and%2520Heng%2520Huang%26entry.1292438233%3DFine-tuning%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520downstream%2520tasks%2520using%2520First-Order%2520%2528FO%2529%2520optimizers%2520presents%2520significant%2520computational%2520challenges.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520address%2520these%2520by%2520freezing%2520most%2520model%2520parameters%2520and%2520training%2520only%2520a%2520small%2520subset.%2520However%252C%2520PEFT%2520often%2520underperforms%2520compared%2520to%2520full%2520fine-tuning%2520when%2520high%2520task-specific%2520accuracy%2520is%2520required.%2520Zeroth-Order%2520%2528ZO%2529%2520methods%2520fine-tune%2520the%2520entire%2520pre-trained%2520model%2520without%2520back-propagation%252C%2520estimating%2520gradients%2520through%2520forward%2520passes%2520only.%2520While%2520memory-efficient%252C%2520ZO%2520methods%2520suffer%2520from%2520slow%2520convergence%2520and%2520high%2520sensitivity%2520to%2520prompt%2520selection.%2520We%2520bridge%2520these%2520two%2520worlds%2520with%2520Bilevel-ZOFO%252C%2520a%2520bilevel%2520optimization%2520method%2520that%2520couples%2520fast%252C%2520local%2520FO-PEFT%2520adaptation%2520at%2520the%2520inner%2520level%2520with%2520stable%252C%2520memory-efficient%2520ZO%2520updates%2520of%2520the%2520full%2520backbone%2520at%2520the%2520outer%2520level.%2520The%2520FO-PEFT%2520inner%2520loop%2520performs%2520fast%252C%2520low-memory%2520local%2520adaptation%2520that%2520reduces%2520the%2520variance%2520of%2520ZO%2520estimates%2520and%2520stabilizes%2520the%2520search%252C%2520guiding%2520the%2520outer%2520ZO%2520updates%2520of%2520the%2520full%2520backbone%2520and%2520reducing%2520prompt%2520sensitivity.%2520In%2520the%2520mean%2520time%252C%2520the%2520outer%2520ZO%2520provides%2520better%2520generalization%2520ability%2520for%2520PEFT.%2520We%2520provide%2520theoretical%2520convergence%2520guarantees%2520and%2520empirically%2520demonstrate%2520that%2520Bilevel-ZOFO%2520significantly%2520outperforms%2520existing%2520ZO%2520and%2520FO-PEFT%2520methods%252C%2520achieving%25202-4%2520times%2520faster%2520training%2520while%2520maintaining%2520similar%2520memory%2520efficiency.%2520Additionally%252C%2520we%2520show%2520by%2520updating%2520the%2520backbone%2520with%2520ZO%2520and%2520adapting%2520only%2520a%2520tiny%2520FO-PEFT%2520block%2520per%2520task%252C%2520Bilevel-ZOFO%2520combines%2520full-model%2520capacity%2520with%2520few-shot%2520efficiency%252C%2520making%2520it%2520a%2520very%2520efficient%2520meta-learning%2520algorithm%2520that%2520quickly%2520adapts%2520to%2520new%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03604v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilevel%20ZOFO%3A%20Efficient%20LLM%20Fine-Tuning%20and%20Meta-Training&entry.906535625=Reza%20Shirkavand%20and%20Peiran%20Yu%20and%20Qi%20He%20and%20Heng%20Huang&entry.1292438233=Fine-tuning%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20for%20downstream%20tasks%20using%20First-Order%20%28FO%29%20optimizers%20presents%20significant%20computational%20challenges.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20these%20by%20freezing%20most%20model%20parameters%20and%20training%20only%20a%20small%20subset.%20However%2C%20PEFT%20often%20underperforms%20compared%20to%20full%20fine-tuning%20when%20high%20task-specific%20accuracy%20is%20required.%20Zeroth-Order%20%28ZO%29%20methods%20fine-tune%20the%20entire%20pre-trained%20model%20without%20back-propagation%2C%20estimating%20gradients%20through%20forward%20passes%20only.%20While%20memory-efficient%2C%20ZO%20methods%20suffer%20from%20slow%20convergence%20and%20high%20sensitivity%20to%20prompt%20selection.%20We%20bridge%20these%20two%20worlds%20with%20Bilevel-ZOFO%2C%20a%20bilevel%20optimization%20method%20that%20couples%20fast%2C%20local%20FO-PEFT%20adaptation%20at%20the%20inner%20level%20with%20stable%2C%20memory-efficient%20ZO%20updates%20of%20the%20full%20backbone%20at%20the%20outer%20level.%20The%20FO-PEFT%20inner%20loop%20performs%20fast%2C%20low-memory%20local%20adaptation%20that%20reduces%20the%20variance%20of%20ZO%20estimates%20and%20stabilizes%20the%20search%2C%20guiding%20the%20outer%20ZO%20updates%20of%20the%20full%20backbone%20and%20reducing%20prompt%20sensitivity.%20In%20the%20mean%20time%2C%20the%20outer%20ZO%20provides%20better%20generalization%20ability%20for%20PEFT.%20We%20provide%20theoretical%20convergence%20guarantees%20and%20empirically%20demonstrate%20that%20Bilevel-ZOFO%20significantly%20outperforms%20existing%20ZO%20and%20FO-PEFT%20methods%2C%20achieving%202-4%20times%20faster%20training%20while%20maintaining%20similar%20memory%20efficiency.%20Additionally%2C%20we%20show%20by%20updating%20the%20backbone%20with%20ZO%20and%20adapting%20only%20a%20tiny%20FO-PEFT%20block%20per%20task%2C%20Bilevel-ZOFO%20combines%20full-model%20capacity%20with%20few-shot%20efficiency%2C%20making%20it%20a%20very%20efficient%20meta-learning%20algorithm%20that%20quickly%20adapts%20to%20new%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2502.03604v3&entry.124074799=Read"},
{"title": "LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification", "author": "Ankit Sharma and Sayan Roy Gupta", "abstract": "Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.", "link": "http://arxiv.org/abs/2512.13617v1", "date": "2025-12-15", "relevancy": 2.3429, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4702}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4684}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightTopoGAT%3A%20Enhancing%20Graph%20Attention%20Networks%20with%20Topological%20Features%20for%20Efficient%20Graph%20Classification&body=Title%3A%20LightTopoGAT%3A%20Enhancing%20Graph%20Attention%20Networks%20with%20Topological%20Features%20for%20Efficient%20Graph%20Classification%0AAuthor%3A%20Ankit%20Sharma%20and%20Sayan%20Roy%20Gupta%0AAbstract%3A%20Graph%20Neural%20Networks%20have%20demonstrated%20significant%20success%20in%20graph%20classification%20tasks%2C%20yet%20they%20often%20require%20substantial%20computational%20resources%20and%20struggle%20to%20capture%20global%20graph%20properties%20effectively.%20We%20introduce%20LightTopoGAT%2C%20a%20lightweight%20graph%20attention%20network%20that%20enhances%20node%20features%20through%20topological%20augmentation%20by%20incorporating%20node%20degree%20and%20local%20clustering%20coefficient%20to%20improve%20graph%20representation%20learning.%20The%20proposed%20approach%20maintains%20parameter%20efficiency%20through%20streamlined%20attention%20mechanisms%20while%20integrating%20structural%20information%20that%20is%20typically%20overlooked%20by%20local%20message%20passing%20schemes.%20Through%20comprehensive%20experiments%20on%20three%20benchmark%20datasets%2C%20MUTAG%2C%20ENZYMES%2C%20and%20PROTEINS%2C%20we%20show%20that%20LightTopoGAT%20achieves%20superior%20performance%20compared%20to%20established%20baselines%20including%20GCN%2C%20GraphSAGE%2C%20and%20standard%20GAT%2C%20with%20a%206.6%20percent%20improvement%20in%20accuracy%20on%20MUTAG%20and%20a%202.2%20percent%20improvement%20on%20PROTEINS.%20Ablation%20studies%20further%20confirm%20that%20these%20performance%20gains%20arise%20directly%20from%20the%20inclusion%20of%20topological%20features%2C%20demonstrating%20a%20simple%20yet%20effective%20strategy%20for%20enhancing%20graph%20neural%20network%20performance%20without%20increasing%20architectural%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightTopoGAT%253A%2520Enhancing%2520Graph%2520Attention%2520Networks%2520with%2520Topological%2520Features%2520for%2520Efficient%2520Graph%2520Classification%26entry.906535625%3DAnkit%2520Sharma%2520and%2520Sayan%2520Roy%2520Gupta%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520have%2520demonstrated%2520significant%2520success%2520in%2520graph%2520classification%2520tasks%252C%2520yet%2520they%2520often%2520require%2520substantial%2520computational%2520resources%2520and%2520struggle%2520to%2520capture%2520global%2520graph%2520properties%2520effectively.%2520We%2520introduce%2520LightTopoGAT%252C%2520a%2520lightweight%2520graph%2520attention%2520network%2520that%2520enhances%2520node%2520features%2520through%2520topological%2520augmentation%2520by%2520incorporating%2520node%2520degree%2520and%2520local%2520clustering%2520coefficient%2520to%2520improve%2520graph%2520representation%2520learning.%2520The%2520proposed%2520approach%2520maintains%2520parameter%2520efficiency%2520through%2520streamlined%2520attention%2520mechanisms%2520while%2520integrating%2520structural%2520information%2520that%2520is%2520typically%2520overlooked%2520by%2520local%2520message%2520passing%2520schemes.%2520Through%2520comprehensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%252C%2520MUTAG%252C%2520ENZYMES%252C%2520and%2520PROTEINS%252C%2520we%2520show%2520that%2520LightTopoGAT%2520achieves%2520superior%2520performance%2520compared%2520to%2520established%2520baselines%2520including%2520GCN%252C%2520GraphSAGE%252C%2520and%2520standard%2520GAT%252C%2520with%2520a%25206.6%2520percent%2520improvement%2520in%2520accuracy%2520on%2520MUTAG%2520and%2520a%25202.2%2520percent%2520improvement%2520on%2520PROTEINS.%2520Ablation%2520studies%2520further%2520confirm%2520that%2520these%2520performance%2520gains%2520arise%2520directly%2520from%2520the%2520inclusion%2520of%2520topological%2520features%252C%2520demonstrating%2520a%2520simple%2520yet%2520effective%2520strategy%2520for%2520enhancing%2520graph%2520neural%2520network%2520performance%2520without%2520increasing%2520architectural%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightTopoGAT%3A%20Enhancing%20Graph%20Attention%20Networks%20with%20Topological%20Features%20for%20Efficient%20Graph%20Classification&entry.906535625=Ankit%20Sharma%20and%20Sayan%20Roy%20Gupta&entry.1292438233=Graph%20Neural%20Networks%20have%20demonstrated%20significant%20success%20in%20graph%20classification%20tasks%2C%20yet%20they%20often%20require%20substantial%20computational%20resources%20and%20struggle%20to%20capture%20global%20graph%20properties%20effectively.%20We%20introduce%20LightTopoGAT%2C%20a%20lightweight%20graph%20attention%20network%20that%20enhances%20node%20features%20through%20topological%20augmentation%20by%20incorporating%20node%20degree%20and%20local%20clustering%20coefficient%20to%20improve%20graph%20representation%20learning.%20The%20proposed%20approach%20maintains%20parameter%20efficiency%20through%20streamlined%20attention%20mechanisms%20while%20integrating%20structural%20information%20that%20is%20typically%20overlooked%20by%20local%20message%20passing%20schemes.%20Through%20comprehensive%20experiments%20on%20three%20benchmark%20datasets%2C%20MUTAG%2C%20ENZYMES%2C%20and%20PROTEINS%2C%20we%20show%20that%20LightTopoGAT%20achieves%20superior%20performance%20compared%20to%20established%20baselines%20including%20GCN%2C%20GraphSAGE%2C%20and%20standard%20GAT%2C%20with%20a%206.6%20percent%20improvement%20in%20accuracy%20on%20MUTAG%20and%20a%202.2%20percent%20improvement%20on%20PROTEINS.%20Ablation%20studies%20further%20confirm%20that%20these%20performance%20gains%20arise%20directly%20from%20the%20inclusion%20of%20topological%20features%2C%20demonstrating%20a%20simple%20yet%20effective%20strategy%20for%20enhancing%20graph%20neural%20network%20performance%20without%20increasing%20architectural%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2512.13617v1&entry.124074799=Read"},
{"title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning", "author": "Aayush Mishra and Daniel Khashabi and Anqi Liu", "abstract": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and two model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.", "link": "http://arxiv.org/abs/2509.22621v2", "date": "2025-12-15", "relevancy": 2.3395, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4739}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning&body=Title%3A%20IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning%0AAuthor%3A%20Aayush%20Mishra%20and%20Daniel%20Khashabi%20and%20Anqi%20Liu%0AAbstract%3A%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20used%20to%20specialize%20model%20behavior%20by%20training%20weights%20to%20produce%20intended%20target%20responses%20for%20queries.%20In%20contrast%2C%20In-Context%20Learning%20%28ICL%29%20adapts%20models%20during%20inference%20with%20instructions%20or%20demonstrations%20in%20the%20prompt.%20ICL%20can%20offer%20better%20generalizability%20and%20more%20calibrated%20responses%20compared%20to%20SFT%20in%20data%20scarce%20settings%2C%20at%20the%20cost%20of%20more%20inference%20compute.%20In%20this%20work%2C%20we%20ask%20the%20question%3A%20Can%20ICL%27s%20internal%20computations%20be%20used%20to%20improve%20the%20qualities%20of%20SFT%3F%20We%20first%20show%20that%20ICL%20and%20SFT%20produce%20distinct%20activation%20patterns%2C%20indicating%20that%20the%20two%20methods%20achieve%20adaptation%20through%20different%20functional%20mechanisms.%20Motivated%20by%20this%20observation%20and%20to%20use%20ICL%27s%20rich%20functionality%2C%20we%20introduce%20ICL%20Activation%20Alignment%20%28IA2%29%2C%20a%20self-distillation%20technique%20which%20aims%20to%20replicate%20ICL%27s%20activation%20patterns%20in%20SFT%20models%20and%20incentivizes%20ICL-like%20internal%20reasoning.%20Performing%20IA2%20as%20a%20priming%20step%20before%20SFT%20significantly%20improves%20the%20accuracy%20and%20calibration%20of%20model%20outputs%2C%20as%20shown%20by%20our%20extensive%20empirical%20results%20on%2012%20popular%20benchmarks%20and%20two%20model%20families.%20This%20finding%20is%20not%20only%20practically%20useful%2C%20but%20also%20offers%20a%20conceptual%20window%20into%20the%20inner%20mechanics%20of%20model%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIA2%253A%2520Alignment%2520with%2520ICL%2520Activations%2520Improves%2520Supervised%2520Fine-Tuning%26entry.906535625%3DAayush%2520Mishra%2520and%2520Daniel%2520Khashabi%2520and%2520Anqi%2520Liu%26entry.1292438233%3DSupervised%2520Fine-Tuning%2520%2528SFT%2529%2520is%2520used%2520to%2520specialize%2520model%2520behavior%2520by%2520training%2520weights%2520to%2520produce%2520intended%2520target%2520responses%2520for%2520queries.%2520In%2520contrast%252C%2520In-Context%2520Learning%2520%2528ICL%2529%2520adapts%2520models%2520during%2520inference%2520with%2520instructions%2520or%2520demonstrations%2520in%2520the%2520prompt.%2520ICL%2520can%2520offer%2520better%2520generalizability%2520and%2520more%2520calibrated%2520responses%2520compared%2520to%2520SFT%2520in%2520data%2520scarce%2520settings%252C%2520at%2520the%2520cost%2520of%2520more%2520inference%2520compute.%2520In%2520this%2520work%252C%2520we%2520ask%2520the%2520question%253A%2520Can%2520ICL%2527s%2520internal%2520computations%2520be%2520used%2520to%2520improve%2520the%2520qualities%2520of%2520SFT%253F%2520We%2520first%2520show%2520that%2520ICL%2520and%2520SFT%2520produce%2520distinct%2520activation%2520patterns%252C%2520indicating%2520that%2520the%2520two%2520methods%2520achieve%2520adaptation%2520through%2520different%2520functional%2520mechanisms.%2520Motivated%2520by%2520this%2520observation%2520and%2520to%2520use%2520ICL%2527s%2520rich%2520functionality%252C%2520we%2520introduce%2520ICL%2520Activation%2520Alignment%2520%2528IA2%2529%252C%2520a%2520self-distillation%2520technique%2520which%2520aims%2520to%2520replicate%2520ICL%2527s%2520activation%2520patterns%2520in%2520SFT%2520models%2520and%2520incentivizes%2520ICL-like%2520internal%2520reasoning.%2520Performing%2520IA2%2520as%2520a%2520priming%2520step%2520before%2520SFT%2520significantly%2520improves%2520the%2520accuracy%2520and%2520calibration%2520of%2520model%2520outputs%252C%2520as%2520shown%2520by%2520our%2520extensive%2520empirical%2520results%2520on%252012%2520popular%2520benchmarks%2520and%2520two%2520model%2520families.%2520This%2520finding%2520is%2520not%2520only%2520practically%2520useful%252C%2520but%2520also%2520offers%2520a%2520conceptual%2520window%2520into%2520the%2520inner%2520mechanics%2520of%2520model%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning&entry.906535625=Aayush%20Mishra%20and%20Daniel%20Khashabi%20and%20Anqi%20Liu&entry.1292438233=Supervised%20Fine-Tuning%20%28SFT%29%20is%20used%20to%20specialize%20model%20behavior%20by%20training%20weights%20to%20produce%20intended%20target%20responses%20for%20queries.%20In%20contrast%2C%20In-Context%20Learning%20%28ICL%29%20adapts%20models%20during%20inference%20with%20instructions%20or%20demonstrations%20in%20the%20prompt.%20ICL%20can%20offer%20better%20generalizability%20and%20more%20calibrated%20responses%20compared%20to%20SFT%20in%20data%20scarce%20settings%2C%20at%20the%20cost%20of%20more%20inference%20compute.%20In%20this%20work%2C%20we%20ask%20the%20question%3A%20Can%20ICL%27s%20internal%20computations%20be%20used%20to%20improve%20the%20qualities%20of%20SFT%3F%20We%20first%20show%20that%20ICL%20and%20SFT%20produce%20distinct%20activation%20patterns%2C%20indicating%20that%20the%20two%20methods%20achieve%20adaptation%20through%20different%20functional%20mechanisms.%20Motivated%20by%20this%20observation%20and%20to%20use%20ICL%27s%20rich%20functionality%2C%20we%20introduce%20ICL%20Activation%20Alignment%20%28IA2%29%2C%20a%20self-distillation%20technique%20which%20aims%20to%20replicate%20ICL%27s%20activation%20patterns%20in%20SFT%20models%20and%20incentivizes%20ICL-like%20internal%20reasoning.%20Performing%20IA2%20as%20a%20priming%20step%20before%20SFT%20significantly%20improves%20the%20accuracy%20and%20calibration%20of%20model%20outputs%2C%20as%20shown%20by%20our%20extensive%20empirical%20results%20on%2012%20popular%20benchmarks%20and%20two%20model%20families.%20This%20finding%20is%20not%20only%20practically%20useful%2C%20but%20also%20offers%20a%20conceptual%20window%20into%20the%20inner%20mechanics%20of%20model%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2509.22621v2&entry.124074799=Read"},
{"title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "author": "Enshen Zhou and Cheng Chi and Yibo Li and Jingkun An and Jiayuan Zhang and Shanyu Rong and Yi Han and Yuheng Ji and Mengzhen Liu and Pengwei Wang and Zhongyuan Wang and Lu Sheng and Shanghang Zhang", "abstract": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "link": "http://arxiv.org/abs/2512.13660v1", "date": "2025-12-15", "relevancy": 2.3333, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboTracer%3A%20Mastering%20Spatial%20Trace%20with%20Reasoning%20in%20Vision-Language%20Models%20for%20Robotics&body=Title%3A%20RoboTracer%3A%20Mastering%20Spatial%20Trace%20with%20Reasoning%20in%20Vision-Language%20Models%20for%20Robotics%0AAuthor%3A%20Enshen%20Zhou%20and%20Cheng%20Chi%20and%20Yibo%20Li%20and%20Jingkun%20An%20and%20Jiayuan%20Zhang%20and%20Shanyu%20Rong%20and%20Yi%20Han%20and%20Yuheng%20Ji%20and%20Mengzhen%20Liu%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Lu%20Sheng%20and%20Shanghang%20Zhang%0AAbstract%3A%20Spatial%20tracing%2C%20as%20a%20fundamental%20embodied%20interaction%20ability%20for%20robots%2C%20is%20inherently%20challenging%20as%20it%20requires%20multi-step%20metric-grounded%20reasoning%20compounded%20with%20complex%20spatial%20referring%20and%20real-world%20metric%20measurement.%20However%2C%20existing%20methods%20struggle%20with%20this%20compositional%20task.%20To%20this%20end%2C%20we%20propose%20RoboTracer%2C%20a%203D-aware%20VLM%20that%20first%20achieves%20both%203D%20spatial%20referring%20and%20measuring%20via%20a%20universal%20spatial%20encoder%20and%20a%20regression-supervised%20decoder%20to%20enhance%20scale%20awareness%20during%20supervised%20fine-tuning%20%28SFT%29.%20Moreover%2C%20RoboTracer%20advances%20multi-step%20metric-grounded%20reasoning%20via%20reinforcement%20fine-tuning%20%28RFT%29%20with%20metric-sensitive%20process%20rewards%2C%20supervising%20key%20intermediate%20perceptual%20cues%20to%20accurately%20generate%20spatial%20traces.%20To%20support%20SFT%20and%20RFT%20training%2C%20we%20introduce%20TraceSpatial%2C%20a%20large-scale%20dataset%20of%2030M%20QA%20pairs%2C%20spanning%20outdoor/indoor/tabletop%20scenes%20and%20supporting%20complex%20reasoning%20processes%20%28up%20to%209%20steps%29.%20We%20further%20present%20TraceSpatial-Bench%2C%20a%20challenging%20benchmark%20filling%20the%20gap%20to%20evaluate%20spatial%20tracing.%20Experimental%20results%20show%20that%20RoboTracer%20surpasses%20baselines%20in%20spatial%20understanding%2C%20measuring%2C%20and%20referring%2C%20with%20an%20average%20success%20rate%20of%2079.1%25%2C%20and%20also%20achieves%20SOTA%20performance%20on%20TraceSpatial-Bench%20by%20a%20large%20margin%2C%20exceeding%20Gemini-2.5-Pro%20by%2036%25%20accuracy.%20Notably%2C%20RoboTracer%20can%20be%20integrated%20with%20various%20control%20policies%20to%20execute%20long-horizon%2C%20dynamic%20tasks%20across%20diverse%20robots%20%28UR5%2C%20G1%20humanoid%29%20in%20cluttered%20real-world%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboTracer%253A%2520Mastering%2520Spatial%2520Trace%2520with%2520Reasoning%2520in%2520Vision-Language%2520Models%2520for%2520Robotics%26entry.906535625%3DEnshen%2520Zhou%2520and%2520Cheng%2520Chi%2520and%2520Yibo%2520Li%2520and%2520Jingkun%2520An%2520and%2520Jiayuan%2520Zhang%2520and%2520Shanyu%2520Rong%2520and%2520Yi%2520Han%2520and%2520Yuheng%2520Ji%2520and%2520Mengzhen%2520Liu%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Lu%2520Sheng%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DSpatial%2520tracing%252C%2520as%2520a%2520fundamental%2520embodied%2520interaction%2520ability%2520for%2520robots%252C%2520is%2520inherently%2520challenging%2520as%2520it%2520requires%2520multi-step%2520metric-grounded%2520reasoning%2520compounded%2520with%2520complex%2520spatial%2520referring%2520and%2520real-world%2520metric%2520measurement.%2520However%252C%2520existing%2520methods%2520struggle%2520with%2520this%2520compositional%2520task.%2520To%2520this%2520end%252C%2520we%2520propose%2520RoboTracer%252C%2520a%25203D-aware%2520VLM%2520that%2520first%2520achieves%2520both%25203D%2520spatial%2520referring%2520and%2520measuring%2520via%2520a%2520universal%2520spatial%2520encoder%2520and%2520a%2520regression-supervised%2520decoder%2520to%2520enhance%2520scale%2520awareness%2520during%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Moreover%252C%2520RoboTracer%2520advances%2520multi-step%2520metric-grounded%2520reasoning%2520via%2520reinforcement%2520fine-tuning%2520%2528RFT%2529%2520with%2520metric-sensitive%2520process%2520rewards%252C%2520supervising%2520key%2520intermediate%2520perceptual%2520cues%2520to%2520accurately%2520generate%2520spatial%2520traces.%2520To%2520support%2520SFT%2520and%2520RFT%2520training%252C%2520we%2520introduce%2520TraceSpatial%252C%2520a%2520large-scale%2520dataset%2520of%252030M%2520QA%2520pairs%252C%2520spanning%2520outdoor/indoor/tabletop%2520scenes%2520and%2520supporting%2520complex%2520reasoning%2520processes%2520%2528up%2520to%25209%2520steps%2529.%2520We%2520further%2520present%2520TraceSpatial-Bench%252C%2520a%2520challenging%2520benchmark%2520filling%2520the%2520gap%2520to%2520evaluate%2520spatial%2520tracing.%2520Experimental%2520results%2520show%2520that%2520RoboTracer%2520surpasses%2520baselines%2520in%2520spatial%2520understanding%252C%2520measuring%252C%2520and%2520referring%252C%2520with%2520an%2520average%2520success%2520rate%2520of%252079.1%2525%252C%2520and%2520also%2520achieves%2520SOTA%2520performance%2520on%2520TraceSpatial-Bench%2520by%2520a%2520large%2520margin%252C%2520exceeding%2520Gemini-2.5-Pro%2520by%252036%2525%2520accuracy.%2520Notably%252C%2520RoboTracer%2520can%2520be%2520integrated%2520with%2520various%2520control%2520policies%2520to%2520execute%2520long-horizon%252C%2520dynamic%2520tasks%2520across%2520diverse%2520robots%2520%2528UR5%252C%2520G1%2520humanoid%2529%2520in%2520cluttered%2520real-world%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboTracer%3A%20Mastering%20Spatial%20Trace%20with%20Reasoning%20in%20Vision-Language%20Models%20for%20Robotics&entry.906535625=Enshen%20Zhou%20and%20Cheng%20Chi%20and%20Yibo%20Li%20and%20Jingkun%20An%20and%20Jiayuan%20Zhang%20and%20Shanyu%20Rong%20and%20Yi%20Han%20and%20Yuheng%20Ji%20and%20Mengzhen%20Liu%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Lu%20Sheng%20and%20Shanghang%20Zhang&entry.1292438233=Spatial%20tracing%2C%20as%20a%20fundamental%20embodied%20interaction%20ability%20for%20robots%2C%20is%20inherently%20challenging%20as%20it%20requires%20multi-step%20metric-grounded%20reasoning%20compounded%20with%20complex%20spatial%20referring%20and%20real-world%20metric%20measurement.%20However%2C%20existing%20methods%20struggle%20with%20this%20compositional%20task.%20To%20this%20end%2C%20we%20propose%20RoboTracer%2C%20a%203D-aware%20VLM%20that%20first%20achieves%20both%203D%20spatial%20referring%20and%20measuring%20via%20a%20universal%20spatial%20encoder%20and%20a%20regression-supervised%20decoder%20to%20enhance%20scale%20awareness%20during%20supervised%20fine-tuning%20%28SFT%29.%20Moreover%2C%20RoboTracer%20advances%20multi-step%20metric-grounded%20reasoning%20via%20reinforcement%20fine-tuning%20%28RFT%29%20with%20metric-sensitive%20process%20rewards%2C%20supervising%20key%20intermediate%20perceptual%20cues%20to%20accurately%20generate%20spatial%20traces.%20To%20support%20SFT%20and%20RFT%20training%2C%20we%20introduce%20TraceSpatial%2C%20a%20large-scale%20dataset%20of%2030M%20QA%20pairs%2C%20spanning%20outdoor/indoor/tabletop%20scenes%20and%20supporting%20complex%20reasoning%20processes%20%28up%20to%209%20steps%29.%20We%20further%20present%20TraceSpatial-Bench%2C%20a%20challenging%20benchmark%20filling%20the%20gap%20to%20evaluate%20spatial%20tracing.%20Experimental%20results%20show%20that%20RoboTracer%20surpasses%20baselines%20in%20spatial%20understanding%2C%20measuring%2C%20and%20referring%2C%20with%20an%20average%20success%20rate%20of%2079.1%25%2C%20and%20also%20achieves%20SOTA%20performance%20on%20TraceSpatial-Bench%20by%20a%20large%20margin%2C%20exceeding%20Gemini-2.5-Pro%20by%2036%25%20accuracy.%20Notably%2C%20RoboTracer%20can%20be%20integrated%20with%20various%20control%20policies%20to%20execute%20long-horizon%2C%20dynamic%20tasks%20across%20diverse%20robots%20%28UR5%2C%20G1%20humanoid%29%20in%20cluttered%20real-world%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.13660v1&entry.124074799=Read"},
{"title": "KlingAvatar 2.0 Technical Report", "author": " Kling Team and Jialu Chen and Yikang Ding and Zhixue Fang and Kun Gai and Yuan Gao and Kang He and Jingyun Hua and Boyuan Jiang and Mingming Lao and Xiaohan Li and Hui Liu and Jiwen Liu and Xiaoqiang Liu and Yuan Liu and Shun Lu and Yongsen Mao and Yingchao Shao and Huafeng Shi and Xiaoyu Shi and Peiqin Sun and Songlin Tang and Pengfei Wan and Chao Wang and Xuebo Wang and Haoxian Zhang and Yuanxing Zhang and Yan Zhou", "abstract": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "link": "http://arxiv.org/abs/2512.13313v1", "date": "2025-12-15", "relevancy": 2.3317, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5916}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5873}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KlingAvatar%202.0%20Technical%20Report&body=Title%3A%20KlingAvatar%202.0%20Technical%20Report%0AAuthor%3A%20%20Kling%20Team%20and%20Jialu%20Chen%20and%20Yikang%20Ding%20and%20Zhixue%20Fang%20and%20Kun%20Gai%20and%20Yuan%20Gao%20and%20Kang%20He%20and%20Jingyun%20Hua%20and%20Boyuan%20Jiang%20and%20Mingming%20Lao%20and%20Xiaohan%20Li%20and%20Hui%20Liu%20and%20Jiwen%20Liu%20and%20Xiaoqiang%20Liu%20and%20Yuan%20Liu%20and%20Shun%20Lu%20and%20Yongsen%20Mao%20and%20Yingchao%20Shao%20and%20Huafeng%20Shi%20and%20Xiaoyu%20Shi%20and%20Peiqin%20Sun%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Chao%20Wang%20and%20Xuebo%20Wang%20and%20Haoxian%20Zhang%20and%20Yuanxing%20Zhang%20and%20Yan%20Zhou%0AAbstract%3A%20Avatar%20video%20generation%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%20However%2C%20prior%20work%20exhibits%20limited%20efficiency%20in%20generating%20long-duration%20high-resolution%20videos%2C%20suffering%20from%20temporal%20drifting%2C%20quality%20degradation%2C%20and%20weak%20prompt%20following%20as%20video%20length%20increases.%20To%20address%20these%20challenges%2C%20we%20propose%20KlingAvatar%202.0%2C%20a%20spatio-temporal%20cascade%20framework%20that%20performs%20upscaling%20in%20both%20spatial%20resolution%20and%20temporal%20dimension.%20The%20framework%20first%20generates%20low-resolution%20blueprint%20video%20keyframes%20that%20capture%20global%20semantics%20and%20motion%2C%20and%20then%20refines%20them%20into%20high-resolution%2C%20temporally%20coherent%20sub-clips%20using%20a%20first-last%20frame%20strategy%2C%20while%20retaining%20smooth%20temporal%20transitions%20in%20long-form%20videos.%20To%20enhance%20cross-modal%20instruction%20fusion%20and%20alignment%20in%20extended%20videos%2C%20we%20introduce%20a%20Co-Reasoning%20Director%20composed%20of%20three%20modality-specific%20large%20language%20model%20%28LLM%29%20experts.%20These%20experts%20reason%20about%20modality%20priorities%20and%20infer%20underlying%20user%20intent%2C%20converting%20inputs%20into%20detailed%20storylines%20through%20multi-turn%20dialogue.%20A%20Negative%20Director%20further%20refines%20negative%20prompts%20to%20improve%20instruction%20alignment.%20Building%20on%20these%20components%2C%20we%20extend%20the%20framework%20to%20support%20ID-specific%20multi-character%20control.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20effectively%20addresses%20the%20challenges%20of%20efficient%2C%20multimodally%20aligned%20long-form%20high-resolution%20video%20generation%2C%20delivering%20enhanced%20visual%20clarity%2C%20realistic%20lip-teeth%20rendering%20with%20accurate%20lip%20synchronization%2C%20strong%20identity%20preservation%2C%20and%20coherent%20multimodal%20instruction%20following.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKlingAvatar%25202.0%2520Technical%2520Report%26entry.906535625%3D%2520Kling%2520Team%2520and%2520Jialu%2520Chen%2520and%2520Yikang%2520Ding%2520and%2520Zhixue%2520Fang%2520and%2520Kun%2520Gai%2520and%2520Yuan%2520Gao%2520and%2520Kang%2520He%2520and%2520Jingyun%2520Hua%2520and%2520Boyuan%2520Jiang%2520and%2520Mingming%2520Lao%2520and%2520Xiaohan%2520Li%2520and%2520Hui%2520Liu%2520and%2520Jiwen%2520Liu%2520and%2520Xiaoqiang%2520Liu%2520and%2520Yuan%2520Liu%2520and%2520Shun%2520Lu%2520and%2520Yongsen%2520Mao%2520and%2520Yingchao%2520Shao%2520and%2520Huafeng%2520Shi%2520and%2520Xiaoyu%2520Shi%2520and%2520Peiqin%2520Sun%2520and%2520Songlin%2520Tang%2520and%2520Pengfei%2520Wan%2520and%2520Chao%2520Wang%2520and%2520Xuebo%2520Wang%2520and%2520Haoxian%2520Zhang%2520and%2520Yuanxing%2520Zhang%2520and%2520Yan%2520Zhou%26entry.1292438233%3DAvatar%2520video%2520generation%2520models%2520have%2520achieved%2520remarkable%2520progress%2520in%2520recent%2520years.%2520However%252C%2520prior%2520work%2520exhibits%2520limited%2520efficiency%2520in%2520generating%2520long-duration%2520high-resolution%2520videos%252C%2520suffering%2520from%2520temporal%2520drifting%252C%2520quality%2520degradation%252C%2520and%2520weak%2520prompt%2520following%2520as%2520video%2520length%2520increases.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520KlingAvatar%25202.0%252C%2520a%2520spatio-temporal%2520cascade%2520framework%2520that%2520performs%2520upscaling%2520in%2520both%2520spatial%2520resolution%2520and%2520temporal%2520dimension.%2520The%2520framework%2520first%2520generates%2520low-resolution%2520blueprint%2520video%2520keyframes%2520that%2520capture%2520global%2520semantics%2520and%2520motion%252C%2520and%2520then%2520refines%2520them%2520into%2520high-resolution%252C%2520temporally%2520coherent%2520sub-clips%2520using%2520a%2520first-last%2520frame%2520strategy%252C%2520while%2520retaining%2520smooth%2520temporal%2520transitions%2520in%2520long-form%2520videos.%2520To%2520enhance%2520cross-modal%2520instruction%2520fusion%2520and%2520alignment%2520in%2520extended%2520videos%252C%2520we%2520introduce%2520a%2520Co-Reasoning%2520Director%2520composed%2520of%2520three%2520modality-specific%2520large%2520language%2520model%2520%2528LLM%2529%2520experts.%2520These%2520experts%2520reason%2520about%2520modality%2520priorities%2520and%2520infer%2520underlying%2520user%2520intent%252C%2520converting%2520inputs%2520into%2520detailed%2520storylines%2520through%2520multi-turn%2520dialogue.%2520A%2520Negative%2520Director%2520further%2520refines%2520negative%2520prompts%2520to%2520improve%2520instruction%2520alignment.%2520Building%2520on%2520these%2520components%252C%2520we%2520extend%2520the%2520framework%2520to%2520support%2520ID-specific%2520multi-character%2520control.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520effectively%2520addresses%2520the%2520challenges%2520of%2520efficient%252C%2520multimodally%2520aligned%2520long-form%2520high-resolution%2520video%2520generation%252C%2520delivering%2520enhanced%2520visual%2520clarity%252C%2520realistic%2520lip-teeth%2520rendering%2520with%2520accurate%2520lip%2520synchronization%252C%2520strong%2520identity%2520preservation%252C%2520and%2520coherent%2520multimodal%2520instruction%2520following.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KlingAvatar%202.0%20Technical%20Report&entry.906535625=%20Kling%20Team%20and%20Jialu%20Chen%20and%20Yikang%20Ding%20and%20Zhixue%20Fang%20and%20Kun%20Gai%20and%20Yuan%20Gao%20and%20Kang%20He%20and%20Jingyun%20Hua%20and%20Boyuan%20Jiang%20and%20Mingming%20Lao%20and%20Xiaohan%20Li%20and%20Hui%20Liu%20and%20Jiwen%20Liu%20and%20Xiaoqiang%20Liu%20and%20Yuan%20Liu%20and%20Shun%20Lu%20and%20Yongsen%20Mao%20and%20Yingchao%20Shao%20and%20Huafeng%20Shi%20and%20Xiaoyu%20Shi%20and%20Peiqin%20Sun%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Chao%20Wang%20and%20Xuebo%20Wang%20and%20Haoxian%20Zhang%20and%20Yuanxing%20Zhang%20and%20Yan%20Zhou&entry.1292438233=Avatar%20video%20generation%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%20However%2C%20prior%20work%20exhibits%20limited%20efficiency%20in%20generating%20long-duration%20high-resolution%20videos%2C%20suffering%20from%20temporal%20drifting%2C%20quality%20degradation%2C%20and%20weak%20prompt%20following%20as%20video%20length%20increases.%20To%20address%20these%20challenges%2C%20we%20propose%20KlingAvatar%202.0%2C%20a%20spatio-temporal%20cascade%20framework%20that%20performs%20upscaling%20in%20both%20spatial%20resolution%20and%20temporal%20dimension.%20The%20framework%20first%20generates%20low-resolution%20blueprint%20video%20keyframes%20that%20capture%20global%20semantics%20and%20motion%2C%20and%20then%20refines%20them%20into%20high-resolution%2C%20temporally%20coherent%20sub-clips%20using%20a%20first-last%20frame%20strategy%2C%20while%20retaining%20smooth%20temporal%20transitions%20in%20long-form%20videos.%20To%20enhance%20cross-modal%20instruction%20fusion%20and%20alignment%20in%20extended%20videos%2C%20we%20introduce%20a%20Co-Reasoning%20Director%20composed%20of%20three%20modality-specific%20large%20language%20model%20%28LLM%29%20experts.%20These%20experts%20reason%20about%20modality%20priorities%20and%20infer%20underlying%20user%20intent%2C%20converting%20inputs%20into%20detailed%20storylines%20through%20multi-turn%20dialogue.%20A%20Negative%20Director%20further%20refines%20negative%20prompts%20to%20improve%20instruction%20alignment.%20Building%20on%20these%20components%2C%20we%20extend%20the%20framework%20to%20support%20ID-specific%20multi-character%20control.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20effectively%20addresses%20the%20challenges%20of%20efficient%2C%20multimodally%20aligned%20long-form%20high-resolution%20video%20generation%2C%20delivering%20enhanced%20visual%20clarity%2C%20realistic%20lip-teeth%20rendering%20with%20accurate%20lip%20synchronization%2C%20strong%20identity%20preservation%2C%20and%20coherent%20multimodal%20instruction%20following.&entry.1838667208=http%3A//arxiv.org/abs/2512.13313v1&entry.124074799=Read"},
{"title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees", "author": "Junye Du and Zhenghao Li and Zhutong Gu and Long Feng", "abstract": "This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{\u00f6}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = \u03a9(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = \u03a9(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.", "link": "http://arxiv.org/abs/2512.13565v1", "date": "2025-12-15", "relevancy": 2.3282, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Nonparametric%20Statistics%20Approach%20to%20Feature%20Selection%20in%20Deep%20Neural%20Networks%20with%20Theoretical%20Guarantees&body=Title%3A%20A%20Nonparametric%20Statistics%20Approach%20to%20Feature%20Selection%20in%20Deep%20Neural%20Networks%20with%20Theoretical%20Guarantees%0AAuthor%3A%20Junye%20Du%20and%20Zhenghao%20Li%20and%20Zhutong%20Gu%20and%20Long%20Feng%0AAbstract%3A%20This%20paper%20tackles%20the%20problem%20of%20feature%20selection%20in%20a%20highly%20challenging%20setting%3A%20%24%5Cmathbb%7BE%7D%28y%20%7C%20%5Cboldsymbol%7Bx%7D%29%20%3D%20G%28%5Cboldsymbol%7Bx%7D_%7B%5Cmathcal%7BS%7D_0%7D%29%24%2C%20where%20%24%5Cmathcal%7BS%7D_0%24%20is%20the%20set%20of%20relevant%20features%20and%20%24G%24%20is%20an%20unknown%2C%20potentially%20nonlinear%20function%20subject%20to%20mild%20smoothness%20conditions.%20Our%20approach%20begins%20with%20feature%20selection%20in%20deep%20neural%20networks%2C%20then%20generalizes%20the%20results%20to%20H%7B%C3%B6%7Dlder%20smooth%20functions%20by%20exploiting%20the%20strong%20approximation%20capabilities%20of%20neural%20networks.%20Unlike%20conventional%20optimization-based%20deep%20learning%20methods%2C%20we%20reformulate%20neural%20networks%20as%20index%20models%20and%20estimate%20%24%5Cmathcal%7BS%7D_0%24%20using%20the%20second-order%20Stein%27s%20formula.%20This%20gradient-descent-free%20strategy%20guarantees%20feature%20selection%20consistency%20with%20a%20sample%20size%20requirement%20of%20%24n%20%3D%20%CE%A9%28p%5E2%29%24%2C%20where%20%24p%24%20is%20the%20feature%20dimension.%20To%20handle%20high-dimensional%20scenarios%2C%20we%20further%20introduce%20a%20screening-and-selection%20mechanism%20that%20achieves%20nonlinear%20selection%20consistency%20when%20%24n%20%3D%20%CE%A9%28s%20%5Clog%20p%29%24%2C%20with%20%24s%24%20representing%20the%20sparsity%20level.%20Additionally%2C%20we%20refit%20a%20neural%20network%20on%20the%20selected%20features%20for%20prediction%20and%20establish%20performance%20guarantees%20under%20a%20relaxed%20sparsity%20assumption.%20Extensive%20simulations%20and%20real-data%20analyses%20demonstrate%20the%20strong%20performance%20of%20our%20method%20even%20in%20the%20presence%20of%20complex%20feature%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Nonparametric%2520Statistics%2520Approach%2520to%2520Feature%2520Selection%2520in%2520Deep%2520Neural%2520Networks%2520with%2520Theoretical%2520Guarantees%26entry.906535625%3DJunye%2520Du%2520and%2520Zhenghao%2520Li%2520and%2520Zhutong%2520Gu%2520and%2520Long%2520Feng%26entry.1292438233%3DThis%2520paper%2520tackles%2520the%2520problem%2520of%2520feature%2520selection%2520in%2520a%2520highly%2520challenging%2520setting%253A%2520%2524%255Cmathbb%257BE%257D%2528y%2520%257C%2520%255Cboldsymbol%257Bx%257D%2529%2520%253D%2520G%2528%255Cboldsymbol%257Bx%257D_%257B%255Cmathcal%257BS%257D_0%257D%2529%2524%252C%2520where%2520%2524%255Cmathcal%257BS%257D_0%2524%2520is%2520the%2520set%2520of%2520relevant%2520features%2520and%2520%2524G%2524%2520is%2520an%2520unknown%252C%2520potentially%2520nonlinear%2520function%2520subject%2520to%2520mild%2520smoothness%2520conditions.%2520Our%2520approach%2520begins%2520with%2520feature%2520selection%2520in%2520deep%2520neural%2520networks%252C%2520then%2520generalizes%2520the%2520results%2520to%2520H%257B%25C3%25B6%257Dlder%2520smooth%2520functions%2520by%2520exploiting%2520the%2520strong%2520approximation%2520capabilities%2520of%2520neural%2520networks.%2520Unlike%2520conventional%2520optimization-based%2520deep%2520learning%2520methods%252C%2520we%2520reformulate%2520neural%2520networks%2520as%2520index%2520models%2520and%2520estimate%2520%2524%255Cmathcal%257BS%257D_0%2524%2520using%2520the%2520second-order%2520Stein%2527s%2520formula.%2520This%2520gradient-descent-free%2520strategy%2520guarantees%2520feature%2520selection%2520consistency%2520with%2520a%2520sample%2520size%2520requirement%2520of%2520%2524n%2520%253D%2520%25CE%25A9%2528p%255E2%2529%2524%252C%2520where%2520%2524p%2524%2520is%2520the%2520feature%2520dimension.%2520To%2520handle%2520high-dimensional%2520scenarios%252C%2520we%2520further%2520introduce%2520a%2520screening-and-selection%2520mechanism%2520that%2520achieves%2520nonlinear%2520selection%2520consistency%2520when%2520%2524n%2520%253D%2520%25CE%25A9%2528s%2520%255Clog%2520p%2529%2524%252C%2520with%2520%2524s%2524%2520representing%2520the%2520sparsity%2520level.%2520Additionally%252C%2520we%2520refit%2520a%2520neural%2520network%2520on%2520the%2520selected%2520features%2520for%2520prediction%2520and%2520establish%2520performance%2520guarantees%2520under%2520a%2520relaxed%2520sparsity%2520assumption.%2520Extensive%2520simulations%2520and%2520real-data%2520analyses%2520demonstrate%2520the%2520strong%2520performance%2520of%2520our%2520method%2520even%2520in%2520the%2520presence%2520of%2520complex%2520feature%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Nonparametric%20Statistics%20Approach%20to%20Feature%20Selection%20in%20Deep%20Neural%20Networks%20with%20Theoretical%20Guarantees&entry.906535625=Junye%20Du%20and%20Zhenghao%20Li%20and%20Zhutong%20Gu%20and%20Long%20Feng&entry.1292438233=This%20paper%20tackles%20the%20problem%20of%20feature%20selection%20in%20a%20highly%20challenging%20setting%3A%20%24%5Cmathbb%7BE%7D%28y%20%7C%20%5Cboldsymbol%7Bx%7D%29%20%3D%20G%28%5Cboldsymbol%7Bx%7D_%7B%5Cmathcal%7BS%7D_0%7D%29%24%2C%20where%20%24%5Cmathcal%7BS%7D_0%24%20is%20the%20set%20of%20relevant%20features%20and%20%24G%24%20is%20an%20unknown%2C%20potentially%20nonlinear%20function%20subject%20to%20mild%20smoothness%20conditions.%20Our%20approach%20begins%20with%20feature%20selection%20in%20deep%20neural%20networks%2C%20then%20generalizes%20the%20results%20to%20H%7B%C3%B6%7Dlder%20smooth%20functions%20by%20exploiting%20the%20strong%20approximation%20capabilities%20of%20neural%20networks.%20Unlike%20conventional%20optimization-based%20deep%20learning%20methods%2C%20we%20reformulate%20neural%20networks%20as%20index%20models%20and%20estimate%20%24%5Cmathcal%7BS%7D_0%24%20using%20the%20second-order%20Stein%27s%20formula.%20This%20gradient-descent-free%20strategy%20guarantees%20feature%20selection%20consistency%20with%20a%20sample%20size%20requirement%20of%20%24n%20%3D%20%CE%A9%28p%5E2%29%24%2C%20where%20%24p%24%20is%20the%20feature%20dimension.%20To%20handle%20high-dimensional%20scenarios%2C%20we%20further%20introduce%20a%20screening-and-selection%20mechanism%20that%20achieves%20nonlinear%20selection%20consistency%20when%20%24n%20%3D%20%CE%A9%28s%20%5Clog%20p%29%24%2C%20with%20%24s%24%20representing%20the%20sparsity%20level.%20Additionally%2C%20we%20refit%20a%20neural%20network%20on%20the%20selected%20features%20for%20prediction%20and%20establish%20performance%20guarantees%20under%20a%20relaxed%20sparsity%20assumption.%20Extensive%20simulations%20and%20real-data%20analyses%20demonstrate%20the%20strong%20performance%20of%20our%20method%20even%20in%20the%20presence%20of%20complex%20feature%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2512.13565v1&entry.124074799=Read"},
{"title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs", "author": "Zheng Qin and Ruobing Zheng and Yabing Wang and Tianqi Li and Yi Yuan and Jingdong Chen and Le Wang", "abstract": "While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.Furthermore, grounded in the observation that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, we posit that reasoning ability serves as the key to unlocking it. We devise a multi-stage, modality-progressive reinforcement learning approach, resulting in HumanSense-Omni-Reasoning, which substantially enhances performance on higher-level understanding and interactive tasks. Additionally, we observe that successful reasoning processes appear to exhibit consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.Project page: \\textcolor{brightpink}{https://digital-avatar.github.io/ai/HumanSense/}", "link": "http://arxiv.org/abs/2508.10576v4", "date": "2025-12-15", "relevancy": 2.3211, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs&body=Title%3A%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs%0AAuthor%3A%20Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%20achieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%20fine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%20both%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%20empathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%20interaction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%20understanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%20feedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%20room%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%20Supplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%20improvements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.Furthermore%2C%20grounded%20in%20the%20observation%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%20analysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20we%20posit%20that%20reasoning%20ability%20serves%20as%20the%20key%20to%20unlocking%20it.%20We%20devise%20a%20multi-stage%2C%20modality-progressive%20reinforcement%20learning%20approach%2C%20resulting%20in%20HumanSense-Omni-Reasoning%2C%20which%20substantially%20enhances%20performance%20on%20higher-level%20understanding%20and%20interactive%20tasks.%20Additionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20appear%20to%20exhibit%20consistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%20enhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.Project%20page%3A%20%5Ctextcolor%7Bbrightpink%7D%7Bhttps%3A//digital-avatar.github.io/ai/HumanSense/%7D%0ALink%3A%20http%3A//arxiv.org/abs/2508.10576v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanSense%253A%2520From%2520Multimodal%2520Perception%2520to%2520Empathetic%2520Context-Aware%2520Responses%2520through%2520Reasoning%2520MLLMs%26entry.906535625%3DZheng%2520Qin%2520and%2520Ruobing%2520Zheng%2520and%2520Yabing%2520Wang%2520and%2520Tianqi%2520Li%2520and%2520Yi%2520Yuan%2520and%2520Jingdong%2520Chen%2520and%2520Le%2520Wang%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520show%2520immense%2520promise%2520for%2520achieving%2520truly%2520human-like%2520interactions%252C%2520progress%2520is%2520hindered%2520by%2520the%2520lack%2520of%2520fine-grained%2520evaluation%2520frameworks%2520for%2520human-centered%2520scenarios%252C%2520encompassing%2520both%2520the%2520understanding%2520of%2520complex%2520human%2520intentions%2520and%2520the%2520provision%2520of%2520empathetic%252C%2520context-aware%2520responses.%2520Here%2520we%2520introduce%2520HumanSense%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520human-centered%2520perception%2520and%2520interaction%2520capabilities%2520of%2520MLLMs%252C%2520with%2520a%2520particular%2520focus%2520on%2520deep%2520understanding%2520of%2520extended%2520multimodal%2520contexts%2520and%2520the%2520formulation%2520of%2520rational%2520feedback.%2520Our%2520evaluation%2520reveals%2520that%2520leading%2520MLLMs%2520still%2520have%2520considerable%2520room%2520for%2520improvement%252C%2520particularly%2520for%2520advanced%2520interaction-oriented%2520tasks.%2520Supplementing%2520visual%2520input%2520with%2520audio%2520and%2520text%2520information%2520yields%2520substantial%2520improvements%252C%2520and%2520Omni-modal%2520models%2520show%2520advantages%2520on%2520these%2520tasks.Furthermore%252C%2520grounded%2520in%2520the%2520observation%2520that%2520appropriate%2520feedback%2520stems%2520from%2520a%2520contextual%2520analysis%2520of%2520the%2520interlocutor%2527s%2520needs%2520and%2520emotions%252C%2520we%2520posit%2520that%2520reasoning%2520ability%2520serves%2520as%2520the%2520key%2520to%2520unlocking%2520it.%2520We%2520devise%2520a%2520multi-stage%252C%2520modality-progressive%2520reinforcement%2520learning%2520approach%252C%2520resulting%2520in%2520HumanSense-Omni-Reasoning%252C%2520which%2520substantially%2520enhances%2520performance%2520on%2520higher-level%2520understanding%2520and%2520interactive%2520tasks.%2520Additionally%252C%2520we%2520observe%2520that%2520successful%2520reasoning%2520processes%2520appear%2520to%2520exhibit%2520consistent%2520thought%2520patterns.%2520By%2520designing%2520corresponding%2520prompts%252C%2520we%2520also%2520enhance%2520the%2520performance%2520of%2520non-reasoning%2520models%2520in%2520a%2520training-free%2520manner.Project%2520page%253A%2520%255Ctextcolor%257Bbrightpink%257D%257Bhttps%253A//digital-avatar.github.io/ai/HumanSense/%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10576v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs&entry.906535625=Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%20achieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%20fine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%20both%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%20empathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%20interaction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%20understanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%20feedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%20room%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%20Supplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%20improvements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.Furthermore%2C%20grounded%20in%20the%20observation%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%20analysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20we%20posit%20that%20reasoning%20ability%20serves%20as%20the%20key%20to%20unlocking%20it.%20We%20devise%20a%20multi-stage%2C%20modality-progressive%20reinforcement%20learning%20approach%2C%20resulting%20in%20HumanSense-Omni-Reasoning%2C%20which%20substantially%20enhances%20performance%20on%20higher-level%20understanding%20and%20interactive%20tasks.%20Additionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20appear%20to%20exhibit%20consistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%20enhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.Project%20page%3A%20%5Ctextcolor%7Bbrightpink%7D%7Bhttps%3A//digital-avatar.github.io/ai/HumanSense/%7D&entry.1838667208=http%3A//arxiv.org/abs/2508.10576v4&entry.124074799=Read"},
{"title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "author": "Zhangcheng Qiang and Kerry Taylor and Weiqing Wang", "abstract": "Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many views treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary modifications, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and less explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignment(s) from OM to reduce the number of matching candidates and improve overall OV performance.", "link": "http://arxiv.org/abs/2409.20302v5", "date": "2025-12-15", "relevancy": 2.3195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&body=Title%3A%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang%0AAbstract%3A%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20capture%20time-varying%20information%20for%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20views%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20the%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20modifications%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20less%20explainability%20for%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20building%20upon%20the%20existing%20alignment%28s%29%20from%20OM%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20improve%20overall%20OV%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2409.20302v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOM4OV%253A%2520Leveraging%2520Ontology%2520Matching%2520for%2520Ontology%2520Versioning%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Kerry%2520Taylor%2520and%2520Weiqing%2520Wang%26entry.1292438233%3DDue%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520Semantic%2520Web%252C%2520version%2520control%2520is%2520necessary%2520to%2520capture%2520time-varying%2520information%2520for%2520widely%2520used%2520ontologies.%2520Despite%2520the%2520long-standing%2520recognition%2520of%2520ontology%2520versioning%2520%2528OV%2529%2520as%2520a%2520crucial%2520component%2520of%2520efficient%2520ontology%2520management%252C%2520many%2520views%2520treat%2520OV%2520as%2520similar%2520to%2520ontology%2520matching%2520%2528OM%2529%2520and%2520directly%2520reuse%2520OM%2520systems%2520for%2520OV%2520tasks.%2520In%2520this%2520study%252C%2520we%2520systematically%2520analyse%2520the%2520similarities%2520and%2520differences%2520between%2520OM%2520and%2520OV%2520and%2520formalise%2520the%2520OM4OV%2520pipeline.%2520The%2520pipeline%2520is%2520implemented%2520and%2520evaluated%2520in%2520the%2520state-of-the-art%2520OM%2520system%2520Agent-OM.%2520The%2520experimental%2520results%2520indicate%2520that%2520OM%2520systems%2520can%2520be%2520reused%2520for%2520OV%2520tasks%252C%2520but%2520without%2520necessary%2520modifications%252C%2520the%2520current%2520OM4OV%2520pipeline%2520can%2520produce%2520skewed%2520measurements%252C%2520poor%2520performance%2520in%2520detecting%2520update%2520entities%252C%2520and%2520less%2520explainability%2520for%2520false%2520mappings.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520an%2520optimisation%2520method%2520called%2520the%2520cross-reference%2520%2528CR%2529%2520mechanism%252C%2520building%2520upon%2520the%2520existing%2520alignment%2528s%2529%2520from%2520OM%2520to%2520reduce%2520the%2520number%2520of%2520matching%2520candidates%2520and%2520improve%2520overall%2520OV%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20302v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&entry.906535625=Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang&entry.1292438233=Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20capture%20time-varying%20information%20for%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20views%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20the%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20modifications%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20less%20explainability%20for%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20building%20upon%20the%20existing%20alignment%28s%29%20from%20OM%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20improve%20overall%20OV%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2409.20302v5&entry.124074799=Read"},
{"title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction", "author": "Tianye Ding and Yiming Xie and Yiqing Liang and Moitreya Chatterjee and Pedro Miraldo and Huaizu Jiang", "abstract": "Recent feed-forward reconstruction models like VGGT and $\u03c0^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$", "link": "http://arxiv.org/abs/2512.13680v1", "date": "2025-12-15", "relevancy": 2.3127, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6161}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5539}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASER%3A%20Layer-wise%20Scale%20Alignment%20for%20Training-Free%20Streaming%204D%20Reconstruction&body=Title%3A%20LASER%3A%20Layer-wise%20Scale%20Alignment%20for%20Training-Free%20Streaming%204D%20Reconstruction%0AAuthor%3A%20Tianye%20Ding%20and%20Yiming%20Xie%20and%20Yiqing%20Liang%20and%20Moitreya%20Chatterjee%20and%20Pedro%20Miraldo%20and%20Huaizu%20Jiang%0AAbstract%3A%20Recent%20feed-forward%20reconstruction%20models%20like%20VGGT%20and%20%24%CF%80%5E3%24%20achieve%20impressive%20reconstruction%20quality%20but%20cannot%20process%20streaming%20videos%20due%20to%20quadratic%20memory%20complexity%2C%20limiting%20their%20practical%20deployment.%20While%20existing%20streaming%20methods%20address%20this%20through%20learned%20memory%20mechanisms%20or%20causal%20attention%2C%20they%20require%20extensive%20retraining%20and%20may%20not%20fully%20leverage%20the%20strong%20geometric%20priors%20of%20state-of-the-art%20offline%20models.%20We%20propose%20LASER%2C%20a%20training-free%20framework%20that%20converts%20an%20offline%20reconstruction%20model%20into%20a%20streaming%20system%20by%20aligning%20predictions%20across%20consecutive%20temporal%20windows.%20We%20observe%20that%20simple%20similarity%20transformation%20%28%24%5Cmathrm%7BSim%7D%283%29%24%29%20alignment%20fails%20due%20to%20layer%20depth%20misalignment%3A%20monocular%20scale%20ambiguity%20causes%20relative%20depth%20scales%20of%20different%20scene%20layers%20to%20vary%20inconsistently%20between%20windows.%20To%20address%20this%2C%20we%20introduce%20layer-wise%20scale%20alignment%2C%20which%20segments%20depth%20predictions%20into%20discrete%20layers%2C%20computes%20per-layer%20scale%20factors%2C%20and%20propagates%20them%20across%20both%20adjacent%20windows%20and%20timestamps.%20Extensive%20experiments%20show%20that%20LASER%20achieves%20state-of-the-art%20performance%20on%20camera%20pose%20estimation%20and%20point%20map%20reconstruction%20%25quality%20with%20offline%20models%20while%20operating%20at%2014%20FPS%20with%206%20GB%20peak%20memory%20on%20a%20RTX%20A6000%20GPU%2C%20enabling%20practical%20deployment%20for%20kilometer-scale%20streaming%20videos.%20Project%20website%3A%20%24%5Chref%7Bhttps%3A//neu-vi.github.io/LASER/%7D%7B%5Ctexttt%7Bhttps%3A//neu-vi.github.io/LASER/%7D%7D%24%0ALink%3A%20http%3A//arxiv.org/abs/2512.13680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASER%253A%2520Layer-wise%2520Scale%2520Alignment%2520for%2520Training-Free%2520Streaming%25204D%2520Reconstruction%26entry.906535625%3DTianye%2520Ding%2520and%2520Yiming%2520Xie%2520and%2520Yiqing%2520Liang%2520and%2520Moitreya%2520Chatterjee%2520and%2520Pedro%2520Miraldo%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3DRecent%2520feed-forward%2520reconstruction%2520models%2520like%2520VGGT%2520and%2520%2524%25CF%2580%255E3%2524%2520achieve%2520impressive%2520reconstruction%2520quality%2520but%2520cannot%2520process%2520streaming%2520videos%2520due%2520to%2520quadratic%2520memory%2520complexity%252C%2520limiting%2520their%2520practical%2520deployment.%2520While%2520existing%2520streaming%2520methods%2520address%2520this%2520through%2520learned%2520memory%2520mechanisms%2520or%2520causal%2520attention%252C%2520they%2520require%2520extensive%2520retraining%2520and%2520may%2520not%2520fully%2520leverage%2520the%2520strong%2520geometric%2520priors%2520of%2520state-of-the-art%2520offline%2520models.%2520We%2520propose%2520LASER%252C%2520a%2520training-free%2520framework%2520that%2520converts%2520an%2520offline%2520reconstruction%2520model%2520into%2520a%2520streaming%2520system%2520by%2520aligning%2520predictions%2520across%2520consecutive%2520temporal%2520windows.%2520We%2520observe%2520that%2520simple%2520similarity%2520transformation%2520%2528%2524%255Cmathrm%257BSim%257D%25283%2529%2524%2529%2520alignment%2520fails%2520due%2520to%2520layer%2520depth%2520misalignment%253A%2520monocular%2520scale%2520ambiguity%2520causes%2520relative%2520depth%2520scales%2520of%2520different%2520scene%2520layers%2520to%2520vary%2520inconsistently%2520between%2520windows.%2520To%2520address%2520this%252C%2520we%2520introduce%2520layer-wise%2520scale%2520alignment%252C%2520which%2520segments%2520depth%2520predictions%2520into%2520discrete%2520layers%252C%2520computes%2520per-layer%2520scale%2520factors%252C%2520and%2520propagates%2520them%2520across%2520both%2520adjacent%2520windows%2520and%2520timestamps.%2520Extensive%2520experiments%2520show%2520that%2520LASER%2520achieves%2520state-of-the-art%2520performance%2520on%2520camera%2520pose%2520estimation%2520and%2520point%2520map%2520reconstruction%2520%2525quality%2520with%2520offline%2520models%2520while%2520operating%2520at%252014%2520FPS%2520with%25206%2520GB%2520peak%2520memory%2520on%2520a%2520RTX%2520A6000%2520GPU%252C%2520enabling%2520practical%2520deployment%2520for%2520kilometer-scale%2520streaming%2520videos.%2520Project%2520website%253A%2520%2524%255Chref%257Bhttps%253A//neu-vi.github.io/LASER/%257D%257B%255Ctexttt%257Bhttps%253A//neu-vi.github.io/LASER/%257D%257D%2524%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASER%3A%20Layer-wise%20Scale%20Alignment%20for%20Training-Free%20Streaming%204D%20Reconstruction&entry.906535625=Tianye%20Ding%20and%20Yiming%20Xie%20and%20Yiqing%20Liang%20and%20Moitreya%20Chatterjee%20and%20Pedro%20Miraldo%20and%20Huaizu%20Jiang&entry.1292438233=Recent%20feed-forward%20reconstruction%20models%20like%20VGGT%20and%20%24%CF%80%5E3%24%20achieve%20impressive%20reconstruction%20quality%20but%20cannot%20process%20streaming%20videos%20due%20to%20quadratic%20memory%20complexity%2C%20limiting%20their%20practical%20deployment.%20While%20existing%20streaming%20methods%20address%20this%20through%20learned%20memory%20mechanisms%20or%20causal%20attention%2C%20they%20require%20extensive%20retraining%20and%20may%20not%20fully%20leverage%20the%20strong%20geometric%20priors%20of%20state-of-the-art%20offline%20models.%20We%20propose%20LASER%2C%20a%20training-free%20framework%20that%20converts%20an%20offline%20reconstruction%20model%20into%20a%20streaming%20system%20by%20aligning%20predictions%20across%20consecutive%20temporal%20windows.%20We%20observe%20that%20simple%20similarity%20transformation%20%28%24%5Cmathrm%7BSim%7D%283%29%24%29%20alignment%20fails%20due%20to%20layer%20depth%20misalignment%3A%20monocular%20scale%20ambiguity%20causes%20relative%20depth%20scales%20of%20different%20scene%20layers%20to%20vary%20inconsistently%20between%20windows.%20To%20address%20this%2C%20we%20introduce%20layer-wise%20scale%20alignment%2C%20which%20segments%20depth%20predictions%20into%20discrete%20layers%2C%20computes%20per-layer%20scale%20factors%2C%20and%20propagates%20them%20across%20both%20adjacent%20windows%20and%20timestamps.%20Extensive%20experiments%20show%20that%20LASER%20achieves%20state-of-the-art%20performance%20on%20camera%20pose%20estimation%20and%20point%20map%20reconstruction%20%25quality%20with%20offline%20models%20while%20operating%20at%2014%20FPS%20with%206%20GB%20peak%20memory%20on%20a%20RTX%20A6000%20GPU%2C%20enabling%20practical%20deployment%20for%20kilometer-scale%20streaming%20videos.%20Project%20website%3A%20%24%5Chref%7Bhttps%3A//neu-vi.github.io/LASER/%7D%7B%5Ctexttt%7Bhttps%3A//neu-vi.github.io/LASER/%7D%7D%24&entry.1838667208=http%3A//arxiv.org/abs/2512.13680v1&entry.124074799=Read"},
{"title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation", "author": "Xiaohu Huang and Hao Zhou and Qiangpeng Yang and Shilei Wen and Kai Han", "abstract": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.", "link": "http://arxiv.org/abs/2512.13677v1", "date": "2025-12-15", "relevancy": 2.3114, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6069}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5582}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JoVA%3A%20Unified%20Multimodal%20Learning%20for%20Joint%20Video-Audio%20Generation&body=Title%3A%20JoVA%3A%20Unified%20Multimodal%20Learning%20for%20Joint%20Video-Audio%20Generation%0AAuthor%3A%20Xiaohu%20Huang%20and%20Hao%20Zhou%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Kai%20Han%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20JoVA%2C%20a%20unified%20framework%20for%20joint%20video-audio%20generation.%20Despite%20recent%20encouraging%20advances%2C%20existing%20methods%20face%20two%20critical%20limitations.%20First%2C%20most%20existing%20approaches%20can%20only%20generate%20ambient%20sounds%20and%20lack%20the%20capability%20to%20produce%20human%20speech%20synchronized%20with%20lip%20movements.%20Second%2C%20recent%20attempts%20at%20unified%20human%20video-audio%20generation%20typically%20rely%20on%20explicit%20fusion%20or%20modality-specific%20alignment%20modules%2C%20which%20introduce%20additional%20architecture%20design%20and%20weaken%20the%20model%20simplicity%20of%20the%20original%20transformers.%20To%20address%20these%20issues%2C%20JoVA%20employs%20joint%20self-attention%20across%20video%20and%20audio%20tokens%20within%20each%20transformer%20layer%2C%20enabling%20direct%20and%20efficient%20cross-modal%20interaction%20without%20the%20need%20for%20additional%20alignment%20modules.%20Furthermore%2C%20to%20enable%20high-quality%20lip-speech%20synchronization%2C%20we%20introduce%20a%20simple%20yet%20effective%20mouth-area%20loss%20based%20on%20facial%20keypoint%20detection%2C%20which%20enhances%20supervision%20on%20the%20critical%20mouth%20region%20during%20training%20without%20compromising%20architectural%20simplicity.%20Extensive%20experiments%20on%20benchmarks%20demonstrate%20that%20JoVA%20outperforms%20or%20is%20competitive%20with%20both%20unified%20and%20audio-driven%20state-of-the-art%20methods%20in%20lip-sync%20accuracy%2C%20speech%20quality%2C%20and%20overall%20video-audio%20generation%20fidelity.%20Our%20results%20establish%20JoVA%20as%20an%20elegant%20framework%20for%20high-quality%20multimodal%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoVA%253A%2520Unified%2520Multimodal%2520Learning%2520for%2520Joint%2520Video-Audio%2520Generation%26entry.906535625%3DXiaohu%2520Huang%2520and%2520Hao%2520Zhou%2520and%2520Qiangpeng%2520Yang%2520and%2520Shilei%2520Wen%2520and%2520Kai%2520Han%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520JoVA%252C%2520a%2520unified%2520framework%2520for%2520joint%2520video-audio%2520generation.%2520Despite%2520recent%2520encouraging%2520advances%252C%2520existing%2520methods%2520face%2520two%2520critical%2520limitations.%2520First%252C%2520most%2520existing%2520approaches%2520can%2520only%2520generate%2520ambient%2520sounds%2520and%2520lack%2520the%2520capability%2520to%2520produce%2520human%2520speech%2520synchronized%2520with%2520lip%2520movements.%2520Second%252C%2520recent%2520attempts%2520at%2520unified%2520human%2520video-audio%2520generation%2520typically%2520rely%2520on%2520explicit%2520fusion%2520or%2520modality-specific%2520alignment%2520modules%252C%2520which%2520introduce%2520additional%2520architecture%2520design%2520and%2520weaken%2520the%2520model%2520simplicity%2520of%2520the%2520original%2520transformers.%2520To%2520address%2520these%2520issues%252C%2520JoVA%2520employs%2520joint%2520self-attention%2520across%2520video%2520and%2520audio%2520tokens%2520within%2520each%2520transformer%2520layer%252C%2520enabling%2520direct%2520and%2520efficient%2520cross-modal%2520interaction%2520without%2520the%2520need%2520for%2520additional%2520alignment%2520modules.%2520Furthermore%252C%2520to%2520enable%2520high-quality%2520lip-speech%2520synchronization%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520mouth-area%2520loss%2520based%2520on%2520facial%2520keypoint%2520detection%252C%2520which%2520enhances%2520supervision%2520on%2520the%2520critical%2520mouth%2520region%2520during%2520training%2520without%2520compromising%2520architectural%2520simplicity.%2520Extensive%2520experiments%2520on%2520benchmarks%2520demonstrate%2520that%2520JoVA%2520outperforms%2520or%2520is%2520competitive%2520with%2520both%2520unified%2520and%2520audio-driven%2520state-of-the-art%2520methods%2520in%2520lip-sync%2520accuracy%252C%2520speech%2520quality%252C%2520and%2520overall%2520video-audio%2520generation%2520fidelity.%2520Our%2520results%2520establish%2520JoVA%2520as%2520an%2520elegant%2520framework%2520for%2520high-quality%2520multimodal%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JoVA%3A%20Unified%20Multimodal%20Learning%20for%20Joint%20Video-Audio%20Generation&entry.906535625=Xiaohu%20Huang%20and%20Hao%20Zhou%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Kai%20Han&entry.1292438233=In%20this%20paper%2C%20we%20present%20JoVA%2C%20a%20unified%20framework%20for%20joint%20video-audio%20generation.%20Despite%20recent%20encouraging%20advances%2C%20existing%20methods%20face%20two%20critical%20limitations.%20First%2C%20most%20existing%20approaches%20can%20only%20generate%20ambient%20sounds%20and%20lack%20the%20capability%20to%20produce%20human%20speech%20synchronized%20with%20lip%20movements.%20Second%2C%20recent%20attempts%20at%20unified%20human%20video-audio%20generation%20typically%20rely%20on%20explicit%20fusion%20or%20modality-specific%20alignment%20modules%2C%20which%20introduce%20additional%20architecture%20design%20and%20weaken%20the%20model%20simplicity%20of%20the%20original%20transformers.%20To%20address%20these%20issues%2C%20JoVA%20employs%20joint%20self-attention%20across%20video%20and%20audio%20tokens%20within%20each%20transformer%20layer%2C%20enabling%20direct%20and%20efficient%20cross-modal%20interaction%20without%20the%20need%20for%20additional%20alignment%20modules.%20Furthermore%2C%20to%20enable%20high-quality%20lip-speech%20synchronization%2C%20we%20introduce%20a%20simple%20yet%20effective%20mouth-area%20loss%20based%20on%20facial%20keypoint%20detection%2C%20which%20enhances%20supervision%20on%20the%20critical%20mouth%20region%20during%20training%20without%20compromising%20architectural%20simplicity.%20Extensive%20experiments%20on%20benchmarks%20demonstrate%20that%20JoVA%20outperforms%20or%20is%20competitive%20with%20both%20unified%20and%20audio-driven%20state-of-the-art%20methods%20in%20lip-sync%20accuracy%2C%20speech%20quality%2C%20and%20overall%20video-audio%20generation%20fidelity.%20Our%20results%20establish%20JoVA%20as%20an%20elegant%20framework%20for%20high-quality%20multimodal%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.13677v1&entry.124074799=Read"},
{"title": "Learning to Generate Cross-Task Unexploitable Examples", "author": "Haoxuan Qu and Qiuchi Xiang and Yujun Cai and Yirui Wu and Majid Mirmehdi and Hossein Rahmani and Jun Liu", "abstract": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "link": "http://arxiv.org/abs/2512.13416v1", "date": "2025-12-15", "relevancy": 2.2869, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5786}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5753}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Cross-Task%20Unexploitable%20Examples&body=Title%3A%20Learning%20to%20Generate%20Cross-Task%20Unexploitable%20Examples%0AAuthor%3A%20Haoxuan%20Qu%20and%20Qiuchi%20Xiang%20and%20Yujun%20Cai%20and%20Yirui%20Wu%20and%20Majid%20Mirmehdi%20and%20Hossein%20Rahmani%20and%20Jun%20Liu%0AAbstract%3A%20Unexploitable%20example%20generation%20aims%20to%20transform%20personal%20images%20into%20their%20unexploitable%20%28unlearnable%29%20versions%20before%20they%20are%20uploaded%20online%2C%20thereby%20preventing%20unauthorized%20exploitation%20of%20online%20personal%20images.%20Recently%2C%20this%20task%20has%20garnered%20significant%20research%20attention%20due%20to%20its%20critical%20relevance%20to%20personal%20data%20privacy.%20Yet%2C%20despite%20recent%20progress%2C%20existing%20methods%20for%20this%20task%20can%20still%20suffer%20from%20limited%20practical%20applicability%2C%20as%20they%20can%20fail%20to%20generate%20examples%20that%20are%20broadly%20unexploitable%20across%20different%20real-world%20computer%20vision%20tasks.%20To%20deal%20with%20this%20problem%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20Meta%20Cross-Task%20Unexploitable%20Example%20Generation%20%28MCT-UEG%29%20framework.%20At%20the%20core%20of%20our%20framework%2C%20to%20optimize%20the%20unexploitable%20example%20generator%20for%20effectively%20producing%20broadly%20unexploitable%20examples%2C%20we%20design%20a%20flat-minima-oriented%20meta%20training%20and%20testing%20scheme.%20Extensive%20experiments%20show%20the%20efficacy%20of%20our%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Cross-Task%2520Unexploitable%2520Examples%26entry.906535625%3DHaoxuan%2520Qu%2520and%2520Qiuchi%2520Xiang%2520and%2520Yujun%2520Cai%2520and%2520Yirui%2520Wu%2520and%2520Majid%2520Mirmehdi%2520and%2520Hossein%2520Rahmani%2520and%2520Jun%2520Liu%26entry.1292438233%3DUnexploitable%2520example%2520generation%2520aims%2520to%2520transform%2520personal%2520images%2520into%2520their%2520unexploitable%2520%2528unlearnable%2529%2520versions%2520before%2520they%2520are%2520uploaded%2520online%252C%2520thereby%2520preventing%2520unauthorized%2520exploitation%2520of%2520online%2520personal%2520images.%2520Recently%252C%2520this%2520task%2520has%2520garnered%2520significant%2520research%2520attention%2520due%2520to%2520its%2520critical%2520relevance%2520to%2520personal%2520data%2520privacy.%2520Yet%252C%2520despite%2520recent%2520progress%252C%2520existing%2520methods%2520for%2520this%2520task%2520can%2520still%2520suffer%2520from%2520limited%2520practical%2520applicability%252C%2520as%2520they%2520can%2520fail%2520to%2520generate%2520examples%2520that%2520are%2520broadly%2520unexploitable%2520across%2520different%2520real-world%2520computer%2520vision%2520tasks.%2520To%2520deal%2520with%2520this%2520problem%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Meta%2520Cross-Task%2520Unexploitable%2520Example%2520Generation%2520%2528MCT-UEG%2529%2520framework.%2520At%2520the%2520core%2520of%2520our%2520framework%252C%2520to%2520optimize%2520the%2520unexploitable%2520example%2520generator%2520for%2520effectively%2520producing%2520broadly%2520unexploitable%2520examples%252C%2520we%2520design%2520a%2520flat-minima-oriented%2520meta%2520training%2520and%2520testing%2520scheme.%2520Extensive%2520experiments%2520show%2520the%2520efficacy%2520of%2520our%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Cross-Task%20Unexploitable%20Examples&entry.906535625=Haoxuan%20Qu%20and%20Qiuchi%20Xiang%20and%20Yujun%20Cai%20and%20Yirui%20Wu%20and%20Majid%20Mirmehdi%20and%20Hossein%20Rahmani%20and%20Jun%20Liu&entry.1292438233=Unexploitable%20example%20generation%20aims%20to%20transform%20personal%20images%20into%20their%20unexploitable%20%28unlearnable%29%20versions%20before%20they%20are%20uploaded%20online%2C%20thereby%20preventing%20unauthorized%20exploitation%20of%20online%20personal%20images.%20Recently%2C%20this%20task%20has%20garnered%20significant%20research%20attention%20due%20to%20its%20critical%20relevance%20to%20personal%20data%20privacy.%20Yet%2C%20despite%20recent%20progress%2C%20existing%20methods%20for%20this%20task%20can%20still%20suffer%20from%20limited%20practical%20applicability%2C%20as%20they%20can%20fail%20to%20generate%20examples%20that%20are%20broadly%20unexploitable%20across%20different%20real-world%20computer%20vision%20tasks.%20To%20deal%20with%20this%20problem%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20Meta%20Cross-Task%20Unexploitable%20Example%20Generation%20%28MCT-UEG%29%20framework.%20At%20the%20core%20of%20our%20framework%2C%20to%20optimize%20the%20unexploitable%20example%20generator%20for%20effectively%20producing%20broadly%20unexploitable%20examples%2C%20we%20design%20a%20flat-minima-oriented%20meta%20training%20and%20testing%20scheme.%20Extensive%20experiments%20show%20the%20efficacy%20of%20our%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.13416v1&entry.124074799=Read"},
{"title": "Recurrent Video Masked Autoencoders", "author": "Daniel Zoran and Nikhil Parthasarathy and Yi Yang and Drew A Hudson and Joao Carreira and Andrew Zisserman", "abstract": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.", "link": "http://arxiv.org/abs/2512.13684v1", "date": "2025-12-15", "relevancy": 2.2851, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5753}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5741}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Video%20Masked%20Autoencoders&body=Title%3A%20Recurrent%20Video%20Masked%20Autoencoders%0AAuthor%3A%20Daniel%20Zoran%20and%20Nikhil%20Parthasarathy%20and%20Yi%20Yang%20and%20Drew%20A%20Hudson%20and%20Joao%20Carreira%20and%20Andrew%20Zisserman%0AAbstract%3A%20We%20present%20Recurrent%20Video%20Masked-Autoencoders%20%28RVM%29%3A%20a%20novel%20video%20representation%20learning%20approach%20that%20uses%20a%20transformer-based%20recurrent%20neural%20network%20to%20aggregate%20dense%20image%20features%20over%20time%2C%20effectively%20capturing%20the%20spatio-temporal%20structure%20of%20natural%20video%20data.%20RVM%20learns%20via%20an%20asymmetric%20masked%20prediction%20task%20requiring%20only%20a%20standard%20pixel%20reconstruction%20objective.%20This%20design%20yields%20a%20highly%20efficient%20%60%60generalist%27%27%20encoder%3A%20RVM%20achieves%20competitive%20performance%20with%20state-of-the-art%20video%20models%20%28e.g.%20VideoMAE%2C%20V-JEPA%29%20on%20video-level%20tasks%20like%20action%20recognition%20and%20point/object%20tracking%2C%20while%20also%20performing%20favorably%20against%20image%20models%20%28e.g.%20DINOv2%29%20on%20tasks%20that%20test%20geometric%20and%20dense%20spatial%20understanding.%20Notably%2C%20RVM%20achieves%20strong%20performance%20in%20the%20small-model%20regime%20without%20requiring%20knowledge%20distillation%2C%20exhibiting%20up%20to%2030x%20greater%20parameter%20efficiency%20than%20competing%20video%20masked%20autoencoders.%20Moreover%2C%20we%20demonstrate%20that%20RVM%27s%20recurrent%20nature%20allows%20for%20stable%20feature%20propagation%20over%20long%20temporal%20horizons%20with%20linear%20computational%20cost%2C%20overcoming%20some%20of%20the%20limitations%20of%20standard%20spatio-temporal%20attention-based%20architectures.%20Finally%2C%20we%20use%20qualitative%20visualizations%20to%20highlight%20that%20RVM%20learns%20rich%20representations%20of%20scene%20semantics%2C%20structure%2C%20and%20motion.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Video%2520Masked%2520Autoencoders%26entry.906535625%3DDaniel%2520Zoran%2520and%2520Nikhil%2520Parthasarathy%2520and%2520Yi%2520Yang%2520and%2520Drew%2520A%2520Hudson%2520and%2520Joao%2520Carreira%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3DWe%2520present%2520Recurrent%2520Video%2520Masked-Autoencoders%2520%2528RVM%2529%253A%2520a%2520novel%2520video%2520representation%2520learning%2520approach%2520that%2520uses%2520a%2520transformer-based%2520recurrent%2520neural%2520network%2520to%2520aggregate%2520dense%2520image%2520features%2520over%2520time%252C%2520effectively%2520capturing%2520the%2520spatio-temporal%2520structure%2520of%2520natural%2520video%2520data.%2520RVM%2520learns%2520via%2520an%2520asymmetric%2520masked%2520prediction%2520task%2520requiring%2520only%2520a%2520standard%2520pixel%2520reconstruction%2520objective.%2520This%2520design%2520yields%2520a%2520highly%2520efficient%2520%2560%2560generalist%2527%2527%2520encoder%253A%2520RVM%2520achieves%2520competitive%2520performance%2520with%2520state-of-the-art%2520video%2520models%2520%2528e.g.%2520VideoMAE%252C%2520V-JEPA%2529%2520on%2520video-level%2520tasks%2520like%2520action%2520recognition%2520and%2520point/object%2520tracking%252C%2520while%2520also%2520performing%2520favorably%2520against%2520image%2520models%2520%2528e.g.%2520DINOv2%2529%2520on%2520tasks%2520that%2520test%2520geometric%2520and%2520dense%2520spatial%2520understanding.%2520Notably%252C%2520RVM%2520achieves%2520strong%2520performance%2520in%2520the%2520small-model%2520regime%2520without%2520requiring%2520knowledge%2520distillation%252C%2520exhibiting%2520up%2520to%252030x%2520greater%2520parameter%2520efficiency%2520than%2520competing%2520video%2520masked%2520autoencoders.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520RVM%2527s%2520recurrent%2520nature%2520allows%2520for%2520stable%2520feature%2520propagation%2520over%2520long%2520temporal%2520horizons%2520with%2520linear%2520computational%2520cost%252C%2520overcoming%2520some%2520of%2520the%2520limitations%2520of%2520standard%2520spatio-temporal%2520attention-based%2520architectures.%2520Finally%252C%2520we%2520use%2520qualitative%2520visualizations%2520to%2520highlight%2520that%2520RVM%2520learns%2520rich%2520representations%2520of%2520scene%2520semantics%252C%2520structure%252C%2520and%2520motion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Video%20Masked%20Autoencoders&entry.906535625=Daniel%20Zoran%20and%20Nikhil%20Parthasarathy%20and%20Yi%20Yang%20and%20Drew%20A%20Hudson%20and%20Joao%20Carreira%20and%20Andrew%20Zisserman&entry.1292438233=We%20present%20Recurrent%20Video%20Masked-Autoencoders%20%28RVM%29%3A%20a%20novel%20video%20representation%20learning%20approach%20that%20uses%20a%20transformer-based%20recurrent%20neural%20network%20to%20aggregate%20dense%20image%20features%20over%20time%2C%20effectively%20capturing%20the%20spatio-temporal%20structure%20of%20natural%20video%20data.%20RVM%20learns%20via%20an%20asymmetric%20masked%20prediction%20task%20requiring%20only%20a%20standard%20pixel%20reconstruction%20objective.%20This%20design%20yields%20a%20highly%20efficient%20%60%60generalist%27%27%20encoder%3A%20RVM%20achieves%20competitive%20performance%20with%20state-of-the-art%20video%20models%20%28e.g.%20VideoMAE%2C%20V-JEPA%29%20on%20video-level%20tasks%20like%20action%20recognition%20and%20point/object%20tracking%2C%20while%20also%20performing%20favorably%20against%20image%20models%20%28e.g.%20DINOv2%29%20on%20tasks%20that%20test%20geometric%20and%20dense%20spatial%20understanding.%20Notably%2C%20RVM%20achieves%20strong%20performance%20in%20the%20small-model%20regime%20without%20requiring%20knowledge%20distillation%2C%20exhibiting%20up%20to%2030x%20greater%20parameter%20efficiency%20than%20competing%20video%20masked%20autoencoders.%20Moreover%2C%20we%20demonstrate%20that%20RVM%27s%20recurrent%20nature%20allows%20for%20stable%20feature%20propagation%20over%20long%20temporal%20horizons%20with%20linear%20computational%20cost%2C%20overcoming%20some%20of%20the%20limitations%20of%20standard%20spatio-temporal%20attention-based%20architectures.%20Finally%2C%20we%20use%20qualitative%20visualizations%20to%20highlight%20that%20RVM%20learns%20rich%20representations%20of%20scene%20semantics%2C%20structure%2C%20and%20motion.&entry.1838667208=http%3A//arxiv.org/abs/2512.13684v1&entry.124074799=Read"},
{"title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition", "author": "Ahmed Abul Hasanaath and Hamzah Luqman", "abstract": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM", "link": "http://arxiv.org/abs/2512.13415v1", "date": "2025-12-15", "relevancy": 2.2546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5794}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5544}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USTM%3A%20Unified%20Spatial%20and%20Temporal%20Modeling%20for%20Continuous%20Sign%20Language%20Recognition&body=Title%3A%20USTM%3A%20Unified%20Spatial%20and%20Temporal%20Modeling%20for%20Continuous%20Sign%20Language%20Recognition%0AAuthor%3A%20Ahmed%20Abul%20Hasanaath%20and%20Hamzah%20Luqman%0AAbstract%3A%20Continuous%20sign%20language%20recognition%20%28CSLR%29%20requires%20precise%20spatio-temporal%20modeling%20to%20accurately%20recognize%20sequences%20of%20gestures%20in%20videos.%20Existing%20frameworks%20often%20rely%20on%20CNN-based%20spatial%20backbones%20combined%20with%20temporal%20convolution%20or%20recurrent%20modules.%20These%20techniques%20fail%20in%20capturing%20fine-grained%20hand%20and%20facial%20cues%20and%20modeling%20long-range%20temporal%20dependencies.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Unified%20Spatio-Temporal%20Modeling%20%28USTM%29%20framework%2C%20a%20spatio-temporal%20encoder%20that%20effectively%20models%20complex%20patterns%20using%20a%20combination%20of%20a%20Swin%20Transformer%20backbone%20enhanced%20with%20lightweight%20temporal%20adapter%20with%20positional%20embeddings%20%28TAPE%29.%20Our%20framework%20captures%20fine-grained%20spatial%20features%20alongside%20short%20and%20long-term%20temporal%20context%2C%20enabling%20robust%20sign%20language%20recognition%20from%20RGB%20videos%20without%20relying%20on%20multi-stream%20inputs%20or%20auxiliary%20modalities.%20Extensive%20experiments%20on%20benchmarked%20datasets%20including%20PHOENIX14%2C%20PHOENIX14T%2C%20and%20CSL-Daily%20demonstrate%20that%20USTM%20achieves%20state-of-the-art%20performance%20against%20RGB-based%20as%20well%20as%20multi-modal%20CSLR%20approaches%2C%20while%20maintaining%20competitive%20performance%20against%20multi-stream%20approaches.%20These%20results%20highlight%20the%20strength%20and%20efficacy%20of%20the%20USTM%20framework%20for%20CSLR.%20The%20code%20is%20available%20at%20https%3A//github.com/gufranSabri/USTM%0ALink%3A%20http%3A//arxiv.org/abs/2512.13415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSTM%253A%2520Unified%2520Spatial%2520and%2520Temporal%2520Modeling%2520for%2520Continuous%2520Sign%2520Language%2520Recognition%26entry.906535625%3DAhmed%2520Abul%2520Hasanaath%2520and%2520Hamzah%2520Luqman%26entry.1292438233%3DContinuous%2520sign%2520language%2520recognition%2520%2528CSLR%2529%2520requires%2520precise%2520spatio-temporal%2520modeling%2520to%2520accurately%2520recognize%2520sequences%2520of%2520gestures%2520in%2520videos.%2520Existing%2520frameworks%2520often%2520rely%2520on%2520CNN-based%2520spatial%2520backbones%2520combined%2520with%2520temporal%2520convolution%2520or%2520recurrent%2520modules.%2520These%2520techniques%2520fail%2520in%2520capturing%2520fine-grained%2520hand%2520and%2520facial%2520cues%2520and%2520modeling%2520long-range%2520temporal%2520dependencies.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Unified%2520Spatio-Temporal%2520Modeling%2520%2528USTM%2529%2520framework%252C%2520a%2520spatio-temporal%2520encoder%2520that%2520effectively%2520models%2520complex%2520patterns%2520using%2520a%2520combination%2520of%2520a%2520Swin%2520Transformer%2520backbone%2520enhanced%2520with%2520lightweight%2520temporal%2520adapter%2520with%2520positional%2520embeddings%2520%2528TAPE%2529.%2520Our%2520framework%2520captures%2520fine-grained%2520spatial%2520features%2520alongside%2520short%2520and%2520long-term%2520temporal%2520context%252C%2520enabling%2520robust%2520sign%2520language%2520recognition%2520from%2520RGB%2520videos%2520without%2520relying%2520on%2520multi-stream%2520inputs%2520or%2520auxiliary%2520modalities.%2520Extensive%2520experiments%2520on%2520benchmarked%2520datasets%2520including%2520PHOENIX14%252C%2520PHOENIX14T%252C%2520and%2520CSL-Daily%2520demonstrate%2520that%2520USTM%2520achieves%2520state-of-the-art%2520performance%2520against%2520RGB-based%2520as%2520well%2520as%2520multi-modal%2520CSLR%2520approaches%252C%2520while%2520maintaining%2520competitive%2520performance%2520against%2520multi-stream%2520approaches.%2520These%2520results%2520highlight%2520the%2520strength%2520and%2520efficacy%2520of%2520the%2520USTM%2520framework%2520for%2520CSLR.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/gufranSabri/USTM%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USTM%3A%20Unified%20Spatial%20and%20Temporal%20Modeling%20for%20Continuous%20Sign%20Language%20Recognition&entry.906535625=Ahmed%20Abul%20Hasanaath%20and%20Hamzah%20Luqman&entry.1292438233=Continuous%20sign%20language%20recognition%20%28CSLR%29%20requires%20precise%20spatio-temporal%20modeling%20to%20accurately%20recognize%20sequences%20of%20gestures%20in%20videos.%20Existing%20frameworks%20often%20rely%20on%20CNN-based%20spatial%20backbones%20combined%20with%20temporal%20convolution%20or%20recurrent%20modules.%20These%20techniques%20fail%20in%20capturing%20fine-grained%20hand%20and%20facial%20cues%20and%20modeling%20long-range%20temporal%20dependencies.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Unified%20Spatio-Temporal%20Modeling%20%28USTM%29%20framework%2C%20a%20spatio-temporal%20encoder%20that%20effectively%20models%20complex%20patterns%20using%20a%20combination%20of%20a%20Swin%20Transformer%20backbone%20enhanced%20with%20lightweight%20temporal%20adapter%20with%20positional%20embeddings%20%28TAPE%29.%20Our%20framework%20captures%20fine-grained%20spatial%20features%20alongside%20short%20and%20long-term%20temporal%20context%2C%20enabling%20robust%20sign%20language%20recognition%20from%20RGB%20videos%20without%20relying%20on%20multi-stream%20inputs%20or%20auxiliary%20modalities.%20Extensive%20experiments%20on%20benchmarked%20datasets%20including%20PHOENIX14%2C%20PHOENIX14T%2C%20and%20CSL-Daily%20demonstrate%20that%20USTM%20achieves%20state-of-the-art%20performance%20against%20RGB-based%20as%20well%20as%20multi-modal%20CSLR%20approaches%2C%20while%20maintaining%20competitive%20performance%20against%20multi-stream%20approaches.%20These%20results%20highlight%20the%20strength%20and%20efficacy%20of%20the%20USTM%20framework%20for%20CSLR.%20The%20code%20is%20available%20at%20https%3A//github.com/gufranSabri/USTM&entry.1838667208=http%3A//arxiv.org/abs/2512.13415v1&entry.124074799=Read"},
{"title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding", "author": "Piyush Bagad and Andrew Zisserman", "abstract": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.", "link": "http://arxiv.org/abs/2512.13511v1", "date": "2025-12-15", "relevancy": 2.2488, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5894}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TARA%3A%20Simple%20and%20Efficient%20Time%20Aware%20Retrieval%20Adaptation%20of%20MLLMs%20for%20Video%20Understanding&body=Title%3A%20TARA%3A%20Simple%20and%20Efficient%20Time%20Aware%20Retrieval%20Adaptation%20of%20MLLMs%20for%20Video%20Understanding%0AAuthor%3A%20Piyush%20Bagad%20and%20Andrew%20Zisserman%0AAbstract%3A%20Our%20objective%20is%20to%20build%20a%20general%20time-aware%20video-text%20embedding%20model%20for%20retrieval.%20To%20that%20end%2C%20we%20propose%20a%20simple%20and%20efficient%20recipe%2C%20dubbed%20TARA%20%28Time%20Aware%20Retrieval%20Adaptation%29%2C%20to%20adapt%20Multimodal%20LLMs%20%28MLLMs%29%20to%20a%20time-aware%20video-text%20embedding%20model%20without%20using%20any%20video%20data%20at%20all.%20For%20evaluating%20time-awareness%20in%20retrieval%2C%20we%20propose%20a%20new%20benchmark%20with%20temporally%20opposite%20%28chiral%29%20actions%20as%20hard%20negatives%20and%20curated%20splits%20for%20chiral%20and%20non-chiral%20actions.%20We%20show%20that%20TARA%20outperforms%20all%20existing%20video-text%20models%20on%20this%20chiral%20benchmark%20while%20also%20achieving%20strong%20results%20on%20standard%20benchmarks.%20Furthermore%2C%20we%20discover%20additional%20benefits%20of%20TARA%20beyond%20time-awareness%3A%20%28i%29%20TARA%20embeddings%20are%20negation-aware%20as%20shown%20in%20NegBench%20benchmark%20that%20evaluates%20negation%20in%20video%20retrieval%2C%20%28ii%29%20TARA%20achieves%20state%20of%20the%20art%20performance%20on%20verb%20and%20adverb%20understanding%20in%20videos.%20Overall%2C%20TARA%20yields%20a%20strong%2C%20versatile%2C%20time-aware%20video-text%20embedding%20model%20with%20state%20of%20the%20art%20zero-shot%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTARA%253A%2520Simple%2520and%2520Efficient%2520Time%2520Aware%2520Retrieval%2520Adaptation%2520of%2520MLLMs%2520for%2520Video%2520Understanding%26entry.906535625%3DPiyush%2520Bagad%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3DOur%2520objective%2520is%2520to%2520build%2520a%2520general%2520time-aware%2520video-text%2520embedding%2520model%2520for%2520retrieval.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%2520simple%2520and%2520efficient%2520recipe%252C%2520dubbed%2520TARA%2520%2528Time%2520Aware%2520Retrieval%2520Adaptation%2529%252C%2520to%2520adapt%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520to%2520a%2520time-aware%2520video-text%2520embedding%2520model%2520without%2520using%2520any%2520video%2520data%2520at%2520all.%2520For%2520evaluating%2520time-awareness%2520in%2520retrieval%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520with%2520temporally%2520opposite%2520%2528chiral%2529%2520actions%2520as%2520hard%2520negatives%2520and%2520curated%2520splits%2520for%2520chiral%2520and%2520non-chiral%2520actions.%2520We%2520show%2520that%2520TARA%2520outperforms%2520all%2520existing%2520video-text%2520models%2520on%2520this%2520chiral%2520benchmark%2520while%2520also%2520achieving%2520strong%2520results%2520on%2520standard%2520benchmarks.%2520Furthermore%252C%2520we%2520discover%2520additional%2520benefits%2520of%2520TARA%2520beyond%2520time-awareness%253A%2520%2528i%2529%2520TARA%2520embeddings%2520are%2520negation-aware%2520as%2520shown%2520in%2520NegBench%2520benchmark%2520that%2520evaluates%2520negation%2520in%2520video%2520retrieval%252C%2520%2528ii%2529%2520TARA%2520achieves%2520state%2520of%2520the%2520art%2520performance%2520on%2520verb%2520and%2520adverb%2520understanding%2520in%2520videos.%2520Overall%252C%2520TARA%2520yields%2520a%2520strong%252C%2520versatile%252C%2520time-aware%2520video-text%2520embedding%2520model%2520with%2520state%2520of%2520the%2520art%2520zero-shot%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TARA%3A%20Simple%20and%20Efficient%20Time%20Aware%20Retrieval%20Adaptation%20of%20MLLMs%20for%20Video%20Understanding&entry.906535625=Piyush%20Bagad%20and%20Andrew%20Zisserman&entry.1292438233=Our%20objective%20is%20to%20build%20a%20general%20time-aware%20video-text%20embedding%20model%20for%20retrieval.%20To%20that%20end%2C%20we%20propose%20a%20simple%20and%20efficient%20recipe%2C%20dubbed%20TARA%20%28Time%20Aware%20Retrieval%20Adaptation%29%2C%20to%20adapt%20Multimodal%20LLMs%20%28MLLMs%29%20to%20a%20time-aware%20video-text%20embedding%20model%20without%20using%20any%20video%20data%20at%20all.%20For%20evaluating%20time-awareness%20in%20retrieval%2C%20we%20propose%20a%20new%20benchmark%20with%20temporally%20opposite%20%28chiral%29%20actions%20as%20hard%20negatives%20and%20curated%20splits%20for%20chiral%20and%20non-chiral%20actions.%20We%20show%20that%20TARA%20outperforms%20all%20existing%20video-text%20models%20on%20this%20chiral%20benchmark%20while%20also%20achieving%20strong%20results%20on%20standard%20benchmarks.%20Furthermore%2C%20we%20discover%20additional%20benefits%20of%20TARA%20beyond%20time-awareness%3A%20%28i%29%20TARA%20embeddings%20are%20negation-aware%20as%20shown%20in%20NegBench%20benchmark%20that%20evaluates%20negation%20in%20video%20retrieval%2C%20%28ii%29%20TARA%20achieves%20state%20of%20the%20art%20performance%20on%20verb%20and%20adverb%20understanding%20in%20videos.%20Overall%2C%20TARA%20yields%20a%20strong%2C%20versatile%2C%20time-aware%20video-text%20embedding%20model%20with%20state%20of%20the%20art%20zero-shot%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.13511v1&entry.124074799=Read"},
{"title": "IPR-1: Interactive Physical Reasoner", "author": "Mingyu Zhang and Lifeng Zhuo and Tianxi Tan and Guocan Xie and Xian Nie and Yan Li and Renjie Zhao and Zizhu He and Ziyu Wang and Jiting Cai and Yong-Lu Li", "abstract": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.", "link": "http://arxiv.org/abs/2511.15407v2", "date": "2025-12-15", "relevancy": 2.2386, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5793}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPR-1%3A%20Interactive%20Physical%20Reasoner&body=Title%3A%20IPR-1%3A%20Interactive%20Physical%20Reasoner%0AAuthor%3A%20Mingyu%20Zhang%20and%20Lifeng%20Zhuo%20and%20Tianxi%20Tan%20and%20Guocan%20Xie%20and%20Xian%20Nie%20and%20Yan%20Li%20and%20Renjie%20Zhao%20and%20Zizhu%20He%20and%20Ziyu%20Wang%20and%20Jiting%20Cai%20and%20Yong-Lu%20Li%0AAbstract%3A%20Humans%20learn%20by%20observing%2C%20interacting%20with%20environments%2C%20and%20internalizing%20physics%20and%20causality.%20Here%2C%20we%20aim%20to%20ask%20whether%20an%20agent%20can%20similarly%20acquire%20human-like%20reasoning%20from%20interaction%20and%20keep%20improving%20with%20more%20experience.%20To%20study%20this%2C%20we%20introduce%20a%20Game-to-Unseen%20%28G2U%29%20benchmark%20of%201%2C000%2B%20heterogeneous%20games%20that%20exhibit%20significant%20visual%20domain%20gaps.%20Existing%20approaches%2C%20including%20VLMs%20and%20world%20models%2C%20struggle%20to%20capture%20underlying%20physics%20and%20causality%20since%20they%20are%20not%20focused%20on%20core%20mechanisms%20and%20overfit%20to%20visual%20details.%20VLM/VLA%20agents%20reason%20but%20lack%20look-ahead%20in%20interactive%20settings%2C%20while%20world%20models%20imagine%20but%20imitate%20visual%20patterns%20rather%20than%20analyze%20physics%20and%20causality.%20We%20therefore%20propose%20IPR%20%28Interactive%20Physical%20Reasoner%29%2C%20using%20world-model%20rollouts%20to%20score%20and%20reinforce%20a%20VLM%27s%20policy%2C%20and%20introduce%20PhysCode%2C%20a%20physics-centric%20action%20code%20aligning%20semantic%20intent%20with%20dynamics%20to%20provide%20a%20shared%20action%20space%20for%20prediction%20and%20reasoning.%20Pretrained%20on%201%2C000%2B%20games%2C%20our%20IPR%20performs%20robustly%20on%20levels%20from%20primitive%20intuition%20to%20goal-driven%20reasoning%2C%20and%20even%20surpasses%20GPT-5%20overall.%20We%20find%20that%20performance%20improves%20with%20more%20training%20games%20and%20interaction%20steps%2C%20and%20that%20the%20model%20also%20zero-shot%20transfers%20to%20unseen%20games.%20These%20results%20support%20physics-centric%20interaction%20as%20a%20path%20to%20steadily%20improving%20physical%20reasoning.%20Further%20demos%20and%20project%20details%20can%20be%20found%20at%20https%3A//mybearyzhang.github.io/ipr-1.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPR-1%253A%2520Interactive%2520Physical%2520Reasoner%26entry.906535625%3DMingyu%2520Zhang%2520and%2520Lifeng%2520Zhuo%2520and%2520Tianxi%2520Tan%2520and%2520Guocan%2520Xie%2520and%2520Xian%2520Nie%2520and%2520Yan%2520Li%2520and%2520Renjie%2520Zhao%2520and%2520Zizhu%2520He%2520and%2520Ziyu%2520Wang%2520and%2520Jiting%2520Cai%2520and%2520Yong-Lu%2520Li%26entry.1292438233%3DHumans%2520learn%2520by%2520observing%252C%2520interacting%2520with%2520environments%252C%2520and%2520internalizing%2520physics%2520and%2520causality.%2520Here%252C%2520we%2520aim%2520to%2520ask%2520whether%2520an%2520agent%2520can%2520similarly%2520acquire%2520human-like%2520reasoning%2520from%2520interaction%2520and%2520keep%2520improving%2520with%2520more%2520experience.%2520To%2520study%2520this%252C%2520we%2520introduce%2520a%2520Game-to-Unseen%2520%2528G2U%2529%2520benchmark%2520of%25201%252C000%252B%2520heterogeneous%2520games%2520that%2520exhibit%2520significant%2520visual%2520domain%2520gaps.%2520Existing%2520approaches%252C%2520including%2520VLMs%2520and%2520world%2520models%252C%2520struggle%2520to%2520capture%2520underlying%2520physics%2520and%2520causality%2520since%2520they%2520are%2520not%2520focused%2520on%2520core%2520mechanisms%2520and%2520overfit%2520to%2520visual%2520details.%2520VLM/VLA%2520agents%2520reason%2520but%2520lack%2520look-ahead%2520in%2520interactive%2520settings%252C%2520while%2520world%2520models%2520imagine%2520but%2520imitate%2520visual%2520patterns%2520rather%2520than%2520analyze%2520physics%2520and%2520causality.%2520We%2520therefore%2520propose%2520IPR%2520%2528Interactive%2520Physical%2520Reasoner%2529%252C%2520using%2520world-model%2520rollouts%2520to%2520score%2520and%2520reinforce%2520a%2520VLM%2527s%2520policy%252C%2520and%2520introduce%2520PhysCode%252C%2520a%2520physics-centric%2520action%2520code%2520aligning%2520semantic%2520intent%2520with%2520dynamics%2520to%2520provide%2520a%2520shared%2520action%2520space%2520for%2520prediction%2520and%2520reasoning.%2520Pretrained%2520on%25201%252C000%252B%2520games%252C%2520our%2520IPR%2520performs%2520robustly%2520on%2520levels%2520from%2520primitive%2520intuition%2520to%2520goal-driven%2520reasoning%252C%2520and%2520even%2520surpasses%2520GPT-5%2520overall.%2520We%2520find%2520that%2520performance%2520improves%2520with%2520more%2520training%2520games%2520and%2520interaction%2520steps%252C%2520and%2520that%2520the%2520model%2520also%2520zero-shot%2520transfers%2520to%2520unseen%2520games.%2520These%2520results%2520support%2520physics-centric%2520interaction%2520as%2520a%2520path%2520to%2520steadily%2520improving%2520physical%2520reasoning.%2520Further%2520demos%2520and%2520project%2520details%2520can%2520be%2520found%2520at%2520https%253A//mybearyzhang.github.io/ipr-1.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPR-1%3A%20Interactive%20Physical%20Reasoner&entry.906535625=Mingyu%20Zhang%20and%20Lifeng%20Zhuo%20and%20Tianxi%20Tan%20and%20Guocan%20Xie%20and%20Xian%20Nie%20and%20Yan%20Li%20and%20Renjie%20Zhao%20and%20Zizhu%20He%20and%20Ziyu%20Wang%20and%20Jiting%20Cai%20and%20Yong-Lu%20Li&entry.1292438233=Humans%20learn%20by%20observing%2C%20interacting%20with%20environments%2C%20and%20internalizing%20physics%20and%20causality.%20Here%2C%20we%20aim%20to%20ask%20whether%20an%20agent%20can%20similarly%20acquire%20human-like%20reasoning%20from%20interaction%20and%20keep%20improving%20with%20more%20experience.%20To%20study%20this%2C%20we%20introduce%20a%20Game-to-Unseen%20%28G2U%29%20benchmark%20of%201%2C000%2B%20heterogeneous%20games%20that%20exhibit%20significant%20visual%20domain%20gaps.%20Existing%20approaches%2C%20including%20VLMs%20and%20world%20models%2C%20struggle%20to%20capture%20underlying%20physics%20and%20causality%20since%20they%20are%20not%20focused%20on%20core%20mechanisms%20and%20overfit%20to%20visual%20details.%20VLM/VLA%20agents%20reason%20but%20lack%20look-ahead%20in%20interactive%20settings%2C%20while%20world%20models%20imagine%20but%20imitate%20visual%20patterns%20rather%20than%20analyze%20physics%20and%20causality.%20We%20therefore%20propose%20IPR%20%28Interactive%20Physical%20Reasoner%29%2C%20using%20world-model%20rollouts%20to%20score%20and%20reinforce%20a%20VLM%27s%20policy%2C%20and%20introduce%20PhysCode%2C%20a%20physics-centric%20action%20code%20aligning%20semantic%20intent%20with%20dynamics%20to%20provide%20a%20shared%20action%20space%20for%20prediction%20and%20reasoning.%20Pretrained%20on%201%2C000%2B%20games%2C%20our%20IPR%20performs%20robustly%20on%20levels%20from%20primitive%20intuition%20to%20goal-driven%20reasoning%2C%20and%20even%20surpasses%20GPT-5%20overall.%20We%20find%20that%20performance%20improves%20with%20more%20training%20games%20and%20interaction%20steps%2C%20and%20that%20the%20model%20also%20zero-shot%20transfers%20to%20unseen%20games.%20These%20results%20support%20physics-centric%20interaction%20as%20a%20path%20to%20steadily%20improving%20physical%20reasoning.%20Further%20demos%20and%20project%20details%20can%20be%20found%20at%20https%3A//mybearyzhang.github.io/ipr-1.&entry.1838667208=http%3A//arxiv.org/abs/2511.15407v2&entry.124074799=Read"},
{"title": "CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors", "author": "Jian Sun and Wei Sun and Genwei Zhang and Kailun Yang and Song Li and Xiangqi Meng and Na Deng and Chongbin Tan", "abstract": "Ultra-wideband (UWB) based positioning with fewer anchors has attracted significant research interest in recent years, especially under energy-constrained conditions. However, most existing methods rely on discrete-time representations and smoothness priors to infer a robot's motion states, which often struggle with ensuring multi-sensor data synchronization. In this article, we present a continuous-time UWB-Inertial-Odometer localization system (CT-UIO), utilizing a non-uniform B-spline framework with fewer anchors. Unlike traditional uniform B-spline-based continuous-time methods, we introduce an adaptive knot-span adjustment strategy for non-uniform continuous-time trajectory representation. This is accomplished by adjusting control points dynamically based on movement speed. To enable efficient fusion of {inertial measurement unit (IMU) and odometer data, we propose an improved extended Kalman filter (EKF) with innovation-based adaptive estimation to provide short-term accurate motion prior. Furthermore, to address the challenge of achieving a fully observable UWB localization system under few-anchor conditions, the virtual anchor (VA) generation method based on multiple hypotheses is proposed. At the backend, we propose an adaptive sliding window strategy for global trajectory estimation. Comprehensive experiments are conducted on three self-collected datasets with different UWB anchor numbers and motion modes. The result shows that the proposed CT-UIO achieves 0.403m, 0.150m, and 0.189m localization accuracy in corridor, exhibition hall, and office environments, yielding 17.2%, 26.1%, and 15.2% improvements compared with competing state-of-the-art UIO systems, respectively. The codebase and datasets of this work will be open-sourced at https://github.com/JasonSun623/CT-UIO.", "link": "http://arxiv.org/abs/2502.06287v2", "date": "2025-12-15", "relevancy": 2.2297, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5792}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-UIO%3A%20Continuous-Time%20UWB-Inertial-Odometer%20Localization%20Using%20Non-Uniform%20B-spline%20with%20Fewer%20Anchors&body=Title%3A%20CT-UIO%3A%20Continuous-Time%20UWB-Inertial-Odometer%20Localization%20Using%20Non-Uniform%20B-spline%20with%20Fewer%20Anchors%0AAuthor%3A%20Jian%20Sun%20and%20Wei%20Sun%20and%20Genwei%20Zhang%20and%20Kailun%20Yang%20and%20Song%20Li%20and%20Xiangqi%20Meng%20and%20Na%20Deng%20and%20Chongbin%20Tan%0AAbstract%3A%20Ultra-wideband%20%28UWB%29%20based%20positioning%20with%20fewer%20anchors%20has%20attracted%20significant%20research%20interest%20in%20recent%20years%2C%20especially%20under%20energy-constrained%20conditions.%20However%2C%20most%20existing%20methods%20rely%20on%20discrete-time%20representations%20and%20smoothness%20priors%20to%20infer%20a%20robot%27s%20motion%20states%2C%20which%20often%20struggle%20with%20ensuring%20multi-sensor%20data%20synchronization.%20In%20this%20article%2C%20we%20present%20a%20continuous-time%20UWB-Inertial-Odometer%20localization%20system%20%28CT-UIO%29%2C%20utilizing%20a%20non-uniform%20B-spline%20framework%20with%20fewer%20anchors.%20Unlike%20traditional%20uniform%20B-spline-based%20continuous-time%20methods%2C%20we%20introduce%20an%20adaptive%20knot-span%20adjustment%20strategy%20for%20non-uniform%20continuous-time%20trajectory%20representation.%20This%20is%20accomplished%20by%20adjusting%20control%20points%20dynamically%20based%20on%20movement%20speed.%20To%20enable%20efficient%20fusion%20of%20%7Binertial%20measurement%20unit%20%28IMU%29%20and%20odometer%20data%2C%20we%20propose%20an%20improved%20extended%20Kalman%20filter%20%28EKF%29%20with%20innovation-based%20adaptive%20estimation%20to%20provide%20short-term%20accurate%20motion%20prior.%20Furthermore%2C%20to%20address%20the%20challenge%20of%20achieving%20a%20fully%20observable%20UWB%20localization%20system%20under%20few-anchor%20conditions%2C%20the%20virtual%20anchor%20%28VA%29%20generation%20method%20based%20on%20multiple%20hypotheses%20is%20proposed.%20At%20the%20backend%2C%20we%20propose%20an%20adaptive%20sliding%20window%20strategy%20for%20global%20trajectory%20estimation.%20Comprehensive%20experiments%20are%20conducted%20on%20three%20self-collected%20datasets%20with%20different%20UWB%20anchor%20numbers%20and%20motion%20modes.%20The%20result%20shows%20that%20the%20proposed%20CT-UIO%20achieves%200.403m%2C%200.150m%2C%20and%200.189m%20localization%20accuracy%20in%20corridor%2C%20exhibition%20hall%2C%20and%20office%20environments%2C%20yielding%2017.2%25%2C%2026.1%25%2C%20and%2015.2%25%20improvements%20compared%20with%20competing%20state-of-the-art%20UIO%20systems%2C%20respectively.%20The%20codebase%20and%20datasets%20of%20this%20work%20will%20be%20open-sourced%20at%20https%3A//github.com/JasonSun623/CT-UIO.%0ALink%3A%20http%3A//arxiv.org/abs/2502.06287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-UIO%253A%2520Continuous-Time%2520UWB-Inertial-Odometer%2520Localization%2520Using%2520Non-Uniform%2520B-spline%2520with%2520Fewer%2520Anchors%26entry.906535625%3DJian%2520Sun%2520and%2520Wei%2520Sun%2520and%2520Genwei%2520Zhang%2520and%2520Kailun%2520Yang%2520and%2520Song%2520Li%2520and%2520Xiangqi%2520Meng%2520and%2520Na%2520Deng%2520and%2520Chongbin%2520Tan%26entry.1292438233%3DUltra-wideband%2520%2528UWB%2529%2520based%2520positioning%2520with%2520fewer%2520anchors%2520has%2520attracted%2520significant%2520research%2520interest%2520in%2520recent%2520years%252C%2520especially%2520under%2520energy-constrained%2520conditions.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%2520discrete-time%2520representations%2520and%2520smoothness%2520priors%2520to%2520infer%2520a%2520robot%2527s%2520motion%2520states%252C%2520which%2520often%2520struggle%2520with%2520ensuring%2520multi-sensor%2520data%2520synchronization.%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520continuous-time%2520UWB-Inertial-Odometer%2520localization%2520system%2520%2528CT-UIO%2529%252C%2520utilizing%2520a%2520non-uniform%2520B-spline%2520framework%2520with%2520fewer%2520anchors.%2520Unlike%2520traditional%2520uniform%2520B-spline-based%2520continuous-time%2520methods%252C%2520we%2520introduce%2520an%2520adaptive%2520knot-span%2520adjustment%2520strategy%2520for%2520non-uniform%2520continuous-time%2520trajectory%2520representation.%2520This%2520is%2520accomplished%2520by%2520adjusting%2520control%2520points%2520dynamically%2520based%2520on%2520movement%2520speed.%2520To%2520enable%2520efficient%2520fusion%2520of%2520%257Binertial%2520measurement%2520unit%2520%2528IMU%2529%2520and%2520odometer%2520data%252C%2520we%2520propose%2520an%2520improved%2520extended%2520Kalman%2520filter%2520%2528EKF%2529%2520with%2520innovation-based%2520adaptive%2520estimation%2520to%2520provide%2520short-term%2520accurate%2520motion%2520prior.%2520Furthermore%252C%2520to%2520address%2520the%2520challenge%2520of%2520achieving%2520a%2520fully%2520observable%2520UWB%2520localization%2520system%2520under%2520few-anchor%2520conditions%252C%2520the%2520virtual%2520anchor%2520%2528VA%2529%2520generation%2520method%2520based%2520on%2520multiple%2520hypotheses%2520is%2520proposed.%2520At%2520the%2520backend%252C%2520we%2520propose%2520an%2520adaptive%2520sliding%2520window%2520strategy%2520for%2520global%2520trajectory%2520estimation.%2520Comprehensive%2520experiments%2520are%2520conducted%2520on%2520three%2520self-collected%2520datasets%2520with%2520different%2520UWB%2520anchor%2520numbers%2520and%2520motion%2520modes.%2520The%2520result%2520shows%2520that%2520the%2520proposed%2520CT-UIO%2520achieves%25200.403m%252C%25200.150m%252C%2520and%25200.189m%2520localization%2520accuracy%2520in%2520corridor%252C%2520exhibition%2520hall%252C%2520and%2520office%2520environments%252C%2520yielding%252017.2%2525%252C%252026.1%2525%252C%2520and%252015.2%2525%2520improvements%2520compared%2520with%2520competing%2520state-of-the-art%2520UIO%2520systems%252C%2520respectively.%2520The%2520codebase%2520and%2520datasets%2520of%2520this%2520work%2520will%2520be%2520open-sourced%2520at%2520https%253A//github.com/JasonSun623/CT-UIO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-UIO%3A%20Continuous-Time%20UWB-Inertial-Odometer%20Localization%20Using%20Non-Uniform%20B-spline%20with%20Fewer%20Anchors&entry.906535625=Jian%20Sun%20and%20Wei%20Sun%20and%20Genwei%20Zhang%20and%20Kailun%20Yang%20and%20Song%20Li%20and%20Xiangqi%20Meng%20and%20Na%20Deng%20and%20Chongbin%20Tan&entry.1292438233=Ultra-wideband%20%28UWB%29%20based%20positioning%20with%20fewer%20anchors%20has%20attracted%20significant%20research%20interest%20in%20recent%20years%2C%20especially%20under%20energy-constrained%20conditions.%20However%2C%20most%20existing%20methods%20rely%20on%20discrete-time%20representations%20and%20smoothness%20priors%20to%20infer%20a%20robot%27s%20motion%20states%2C%20which%20often%20struggle%20with%20ensuring%20multi-sensor%20data%20synchronization.%20In%20this%20article%2C%20we%20present%20a%20continuous-time%20UWB-Inertial-Odometer%20localization%20system%20%28CT-UIO%29%2C%20utilizing%20a%20non-uniform%20B-spline%20framework%20with%20fewer%20anchors.%20Unlike%20traditional%20uniform%20B-spline-based%20continuous-time%20methods%2C%20we%20introduce%20an%20adaptive%20knot-span%20adjustment%20strategy%20for%20non-uniform%20continuous-time%20trajectory%20representation.%20This%20is%20accomplished%20by%20adjusting%20control%20points%20dynamically%20based%20on%20movement%20speed.%20To%20enable%20efficient%20fusion%20of%20%7Binertial%20measurement%20unit%20%28IMU%29%20and%20odometer%20data%2C%20we%20propose%20an%20improved%20extended%20Kalman%20filter%20%28EKF%29%20with%20innovation-based%20adaptive%20estimation%20to%20provide%20short-term%20accurate%20motion%20prior.%20Furthermore%2C%20to%20address%20the%20challenge%20of%20achieving%20a%20fully%20observable%20UWB%20localization%20system%20under%20few-anchor%20conditions%2C%20the%20virtual%20anchor%20%28VA%29%20generation%20method%20based%20on%20multiple%20hypotheses%20is%20proposed.%20At%20the%20backend%2C%20we%20propose%20an%20adaptive%20sliding%20window%20strategy%20for%20global%20trajectory%20estimation.%20Comprehensive%20experiments%20are%20conducted%20on%20three%20self-collected%20datasets%20with%20different%20UWB%20anchor%20numbers%20and%20motion%20modes.%20The%20result%20shows%20that%20the%20proposed%20CT-UIO%20achieves%200.403m%2C%200.150m%2C%20and%200.189m%20localization%20accuracy%20in%20corridor%2C%20exhibition%20hall%2C%20and%20office%20environments%2C%20yielding%2017.2%25%2C%2026.1%25%2C%20and%2015.2%25%20improvements%20compared%20with%20competing%20state-of-the-art%20UIO%20systems%2C%20respectively.%20The%20codebase%20and%20datasets%20of%20this%20work%20will%20be%20open-sourced%20at%20https%3A//github.com/JasonSun623/CT-UIO.&entry.1838667208=http%3A//arxiv.org/abs/2502.06287v2&entry.124074799=Read"},
{"title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction", "author": "Qinglin Jia and Zhaocheng Du and Chuhan Wu and Huifeng Guo and Ruiming Tang and Shuting Shi and Muyu Zhang", "abstract": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.", "link": "http://arxiv.org/abs/2512.13300v1", "date": "2025-12-15", "relevancy": 2.2287, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5616}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5565}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20One%20Left%20Behind%3A%20How%20to%20Exploit%20the%20Incomplete%20and%20Skewed%20Multi-Label%20Data%20for%20Conversion%20Rate%20Prediction&body=Title%3A%20No%20One%20Left%20Behind%3A%20How%20to%20Exploit%20the%20Incomplete%20and%20Skewed%20Multi-Label%20Data%20for%20Conversion%20Rate%20Prediction%0AAuthor%3A%20Qinglin%20Jia%20and%20Zhaocheng%20Du%20and%20Chuhan%20Wu%20and%20Huifeng%20Guo%20and%20Ruiming%20Tang%20and%20Shuting%20Shi%20and%20Muyu%20Zhang%0AAbstract%3A%20In%20most%20real-world%20online%20advertising%20systems%2C%20advertisers%20typically%20have%20diverse%20customer%20acquisition%20goals.%20A%20common%20solution%20is%20to%20use%20multi-task%20learning%20%28MTL%29%20to%20train%20a%20unified%20model%20on%20post-click%20data%20to%20estimate%20the%20conversion%20rate%20%28CVR%29%20for%20these%20diverse%20targets.%20In%20practice%2C%20CVR%20prediction%20often%20encounters%20missing%20conversion%20data%20as%20many%20advertisers%20submit%20only%20a%20subset%20of%20user%20conversion%20actions%20due%20to%20privacy%20or%20other%20constraints%2C%20making%20the%20labels%20of%20multi-task%20data%20incomplete.%20If%20the%20model%20is%20trained%20on%20all%20available%20samples%20where%20advertisers%20submit%20user%20conversion%20actions%2C%20it%20may%20struggle%20when%20deployed%20to%20serve%20a%20subset%20of%20advertisers%20targeting%20specific%20conversion%20actions%2C%20as%20the%20training%20and%20deployment%20data%20distributions%20are%20mismatched.%20While%20considerable%20MTL%20efforts%20have%20been%20made%2C%20a%20long-standing%20challenge%20is%20how%20to%20effectively%20train%20a%20unified%20model%20with%20the%20incomplete%20and%20skewed%20multi-label%20data.%20In%20this%20paper%2C%20we%20propose%20a%20fine-grained%20Knowledge%20transfer%20framework%20for%20Asymmetric%20Multi-Label%20data%20%28KAML%29.%20We%20introduce%20an%20attribution-driven%20masking%20strategy%20%28ADM%29%20to%20better%20utilize%20data%20with%20asymmetric%20multi-label%20data%20in%20training.%20However%2C%20the%20more%20relaxed%20masking%20in%20ADM%20is%20a%20double-edged%20sword%3A%20it%20provides%20additional%20training%20signals%20but%20also%20introduces%20noise%20due%20to%20skewed%20data.%20To%20address%20this%2C%20we%20propose%20a%20hierarchical%20knowledge%20extraction%20mechanism%20%28HKE%29%20to%20model%20the%20sample%20discrepancy%20within%20the%20target%20task%20tower.%20Finally%2C%20to%20maximize%20the%20utility%20of%20unlabeled%20samples%2C%20we%20incorporate%20ranking%20loss%20strategy%20to%20further%20enhance%20our%20model.%20The%20effectiveness%20of%20KAML%20has%20been%20demonstrated%20through%20comprehensive%20evaluations%20on%20offline%20industry%20datasets%20and%20online%20A/B%20tests%2C%20which%20show%20significant%20performance%20improvements%20over%20existing%20MTL%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520One%2520Left%2520Behind%253A%2520How%2520to%2520Exploit%2520the%2520Incomplete%2520and%2520Skewed%2520Multi-Label%2520Data%2520for%2520Conversion%2520Rate%2520Prediction%26entry.906535625%3DQinglin%2520Jia%2520and%2520Zhaocheng%2520Du%2520and%2520Chuhan%2520Wu%2520and%2520Huifeng%2520Guo%2520and%2520Ruiming%2520Tang%2520and%2520Shuting%2520Shi%2520and%2520Muyu%2520Zhang%26entry.1292438233%3DIn%2520most%2520real-world%2520online%2520advertising%2520systems%252C%2520advertisers%2520typically%2520have%2520diverse%2520customer%2520acquisition%2520goals.%2520A%2520common%2520solution%2520is%2520to%2520use%2520multi-task%2520learning%2520%2528MTL%2529%2520to%2520train%2520a%2520unified%2520model%2520on%2520post-click%2520data%2520to%2520estimate%2520the%2520conversion%2520rate%2520%2528CVR%2529%2520for%2520these%2520diverse%2520targets.%2520In%2520practice%252C%2520CVR%2520prediction%2520often%2520encounters%2520missing%2520conversion%2520data%2520as%2520many%2520advertisers%2520submit%2520only%2520a%2520subset%2520of%2520user%2520conversion%2520actions%2520due%2520to%2520privacy%2520or%2520other%2520constraints%252C%2520making%2520the%2520labels%2520of%2520multi-task%2520data%2520incomplete.%2520If%2520the%2520model%2520is%2520trained%2520on%2520all%2520available%2520samples%2520where%2520advertisers%2520submit%2520user%2520conversion%2520actions%252C%2520it%2520may%2520struggle%2520when%2520deployed%2520to%2520serve%2520a%2520subset%2520of%2520advertisers%2520targeting%2520specific%2520conversion%2520actions%252C%2520as%2520the%2520training%2520and%2520deployment%2520data%2520distributions%2520are%2520mismatched.%2520While%2520considerable%2520MTL%2520efforts%2520have%2520been%2520made%252C%2520a%2520long-standing%2520challenge%2520is%2520how%2520to%2520effectively%2520train%2520a%2520unified%2520model%2520with%2520the%2520incomplete%2520and%2520skewed%2520multi-label%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520fine-grained%2520Knowledge%2520transfer%2520framework%2520for%2520Asymmetric%2520Multi-Label%2520data%2520%2528KAML%2529.%2520We%2520introduce%2520an%2520attribution-driven%2520masking%2520strategy%2520%2528ADM%2529%2520to%2520better%2520utilize%2520data%2520with%2520asymmetric%2520multi-label%2520data%2520in%2520training.%2520However%252C%2520the%2520more%2520relaxed%2520masking%2520in%2520ADM%2520is%2520a%2520double-edged%2520sword%253A%2520it%2520provides%2520additional%2520training%2520signals%2520but%2520also%2520introduces%2520noise%2520due%2520to%2520skewed%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520hierarchical%2520knowledge%2520extraction%2520mechanism%2520%2528HKE%2529%2520to%2520model%2520the%2520sample%2520discrepancy%2520within%2520the%2520target%2520task%2520tower.%2520Finally%252C%2520to%2520maximize%2520the%2520utility%2520of%2520unlabeled%2520samples%252C%2520we%2520incorporate%2520ranking%2520loss%2520strategy%2520to%2520further%2520enhance%2520our%2520model.%2520The%2520effectiveness%2520of%2520KAML%2520has%2520been%2520demonstrated%2520through%2520comprehensive%2520evaluations%2520on%2520offline%2520industry%2520datasets%2520and%2520online%2520A/B%2520tests%252C%2520which%2520show%2520significant%2520performance%2520improvements%2520over%2520existing%2520MTL%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20One%20Left%20Behind%3A%20How%20to%20Exploit%20the%20Incomplete%20and%20Skewed%20Multi-Label%20Data%20for%20Conversion%20Rate%20Prediction&entry.906535625=Qinglin%20Jia%20and%20Zhaocheng%20Du%20and%20Chuhan%20Wu%20and%20Huifeng%20Guo%20and%20Ruiming%20Tang%20and%20Shuting%20Shi%20and%20Muyu%20Zhang&entry.1292438233=In%20most%20real-world%20online%20advertising%20systems%2C%20advertisers%20typically%20have%20diverse%20customer%20acquisition%20goals.%20A%20common%20solution%20is%20to%20use%20multi-task%20learning%20%28MTL%29%20to%20train%20a%20unified%20model%20on%20post-click%20data%20to%20estimate%20the%20conversion%20rate%20%28CVR%29%20for%20these%20diverse%20targets.%20In%20practice%2C%20CVR%20prediction%20often%20encounters%20missing%20conversion%20data%20as%20many%20advertisers%20submit%20only%20a%20subset%20of%20user%20conversion%20actions%20due%20to%20privacy%20or%20other%20constraints%2C%20making%20the%20labels%20of%20multi-task%20data%20incomplete.%20If%20the%20model%20is%20trained%20on%20all%20available%20samples%20where%20advertisers%20submit%20user%20conversion%20actions%2C%20it%20may%20struggle%20when%20deployed%20to%20serve%20a%20subset%20of%20advertisers%20targeting%20specific%20conversion%20actions%2C%20as%20the%20training%20and%20deployment%20data%20distributions%20are%20mismatched.%20While%20considerable%20MTL%20efforts%20have%20been%20made%2C%20a%20long-standing%20challenge%20is%20how%20to%20effectively%20train%20a%20unified%20model%20with%20the%20incomplete%20and%20skewed%20multi-label%20data.%20In%20this%20paper%2C%20we%20propose%20a%20fine-grained%20Knowledge%20transfer%20framework%20for%20Asymmetric%20Multi-Label%20data%20%28KAML%29.%20We%20introduce%20an%20attribution-driven%20masking%20strategy%20%28ADM%29%20to%20better%20utilize%20data%20with%20asymmetric%20multi-label%20data%20in%20training.%20However%2C%20the%20more%20relaxed%20masking%20in%20ADM%20is%20a%20double-edged%20sword%3A%20it%20provides%20additional%20training%20signals%20but%20also%20introduces%20noise%20due%20to%20skewed%20data.%20To%20address%20this%2C%20we%20propose%20a%20hierarchical%20knowledge%20extraction%20mechanism%20%28HKE%29%20to%20model%20the%20sample%20discrepancy%20within%20the%20target%20task%20tower.%20Finally%2C%20to%20maximize%20the%20utility%20of%20unlabeled%20samples%2C%20we%20incorporate%20ranking%20loss%20strategy%20to%20further%20enhance%20our%20model.%20The%20effectiveness%20of%20KAML%20has%20been%20demonstrated%20through%20comprehensive%20evaluations%20on%20offline%20industry%20datasets%20and%20online%20A/B%20tests%2C%20which%20show%20significant%20performance%20improvements%20over%20existing%20MTL%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.13300v1&entry.124074799=Read"},
{"title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments", "author": "Li-Wei Shih and Ruo-Syuan Mei and Jesse Heidrich and Hui-Ping Wang and Joel Hooton and Joshua Solomon and Jorge Arinez and Guangze Li and Chenhui Shao", "abstract": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.", "link": "http://arxiv.org/abs/2512.13561v1", "date": "2025-12-15", "relevancy": 2.228, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5904}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5512}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Field%20Perception%20for%20Safety%20Enhancement%20of%20Autonomous%20Mobile%20Robots%20in%20Manufacturing%20Environments&body=Title%3A%20Near-Field%20Perception%20for%20Safety%20Enhancement%20of%20Autonomous%20Mobile%20Robots%20in%20Manufacturing%20Environments%0AAuthor%3A%20Li-Wei%20Shih%20and%20Ruo-Syuan%20Mei%20and%20Jesse%20Heidrich%20and%20Hui-Ping%20Wang%20and%20Joel%20Hooton%20and%20Joshua%20Solomon%20and%20Jorge%20Arinez%20and%20Guangze%20Li%20and%20Chenhui%20Shao%0AAbstract%3A%20Near-field%20perception%20is%20essential%20for%20the%20safe%20operation%20of%20autonomous%20mobile%20robots%20%28AMRs%29%20in%20manufacturing%20environments.%20Conventional%20ranging%20sensors%20such%20as%20light%20detection%20and%20ranging%20%28LiDAR%29%20and%20ultrasonic%20devices%20provide%20broad%20situational%20awareness%20but%20often%20fail%20to%20detect%20small%20objects%20near%20the%20robot%20base.%20To%20address%20this%20limitation%2C%20this%20paper%20presents%20a%20three-tier%20near-field%20perception%20framework.%20The%20first%20approach%20employs%20light-discontinuity%20detection%2C%20which%20projects%20a%20laser%20stripe%20across%20the%20near-field%20zone%20and%20identifies%20interruptions%20in%20the%20stripe%20to%20perform%20fast%2C%20binary%20cutoff%20sensing%20for%20obstacle%20presence.%20The%20second%20approach%20utilizes%20light-displacement%20measurement%20to%20estimate%20object%20height%20by%20analyzing%20the%20geometric%20displacement%20of%20a%20projected%20stripe%20in%20the%20camera%20image%2C%20which%20provides%20quantitative%20obstacle%20height%20information%20with%20minimal%20computational%20overhead.%20The%20third%20approach%20employs%20a%20computer%20vision-based%20object%20detection%20model%20on%20embedded%20AI%20hardware%20to%20classify%20objects%2C%20enabling%20semantic%20perception%20and%20context-aware%20safety%20decisions.%20All%20methods%20are%20implemented%20on%20a%20Raspberry%20Pi%205%20system%2C%20achieving%20real-time%20performance%20at%2025%20or%2050%20frames%20per%20second.%20Experimental%20evaluation%20and%20comparative%20analysis%20demonstrate%20that%20the%20proposed%20hierarchy%20balances%20precision%2C%20computation%2C%20and%20cost%2C%20thereby%20providing%20a%20scalable%20perception%20solution%20for%20enabling%20safe%20operations%20of%20AMRs%20in%20manufacturing%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Field%2520Perception%2520for%2520Safety%2520Enhancement%2520of%2520Autonomous%2520Mobile%2520Robots%2520in%2520Manufacturing%2520Environments%26entry.906535625%3DLi-Wei%2520Shih%2520and%2520Ruo-Syuan%2520Mei%2520and%2520Jesse%2520Heidrich%2520and%2520Hui-Ping%2520Wang%2520and%2520Joel%2520Hooton%2520and%2520Joshua%2520Solomon%2520and%2520Jorge%2520Arinez%2520and%2520Guangze%2520Li%2520and%2520Chenhui%2520Shao%26entry.1292438233%3DNear-field%2520perception%2520is%2520essential%2520for%2520the%2520safe%2520operation%2520of%2520autonomous%2520mobile%2520robots%2520%2528AMRs%2529%2520in%2520manufacturing%2520environments.%2520Conventional%2520ranging%2520sensors%2520such%2520as%2520light%2520detection%2520and%2520ranging%2520%2528LiDAR%2529%2520and%2520ultrasonic%2520devices%2520provide%2520broad%2520situational%2520awareness%2520but%2520often%2520fail%2520to%2520detect%2520small%2520objects%2520near%2520the%2520robot%2520base.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%2520presents%2520a%2520three-tier%2520near-field%2520perception%2520framework.%2520The%2520first%2520approach%2520employs%2520light-discontinuity%2520detection%252C%2520which%2520projects%2520a%2520laser%2520stripe%2520across%2520the%2520near-field%2520zone%2520and%2520identifies%2520interruptions%2520in%2520the%2520stripe%2520to%2520perform%2520fast%252C%2520binary%2520cutoff%2520sensing%2520for%2520obstacle%2520presence.%2520The%2520second%2520approach%2520utilizes%2520light-displacement%2520measurement%2520to%2520estimate%2520object%2520height%2520by%2520analyzing%2520the%2520geometric%2520displacement%2520of%2520a%2520projected%2520stripe%2520in%2520the%2520camera%2520image%252C%2520which%2520provides%2520quantitative%2520obstacle%2520height%2520information%2520with%2520minimal%2520computational%2520overhead.%2520The%2520third%2520approach%2520employs%2520a%2520computer%2520vision-based%2520object%2520detection%2520model%2520on%2520embedded%2520AI%2520hardware%2520to%2520classify%2520objects%252C%2520enabling%2520semantic%2520perception%2520and%2520context-aware%2520safety%2520decisions.%2520All%2520methods%2520are%2520implemented%2520on%2520a%2520Raspberry%2520Pi%25205%2520system%252C%2520achieving%2520real-time%2520performance%2520at%252025%2520or%252050%2520frames%2520per%2520second.%2520Experimental%2520evaluation%2520and%2520comparative%2520analysis%2520demonstrate%2520that%2520the%2520proposed%2520hierarchy%2520balances%2520precision%252C%2520computation%252C%2520and%2520cost%252C%2520thereby%2520providing%2520a%2520scalable%2520perception%2520solution%2520for%2520enabling%2520safe%2520operations%2520of%2520AMRs%2520in%2520manufacturing%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Field%20Perception%20for%20Safety%20Enhancement%20of%20Autonomous%20Mobile%20Robots%20in%20Manufacturing%20Environments&entry.906535625=Li-Wei%20Shih%20and%20Ruo-Syuan%20Mei%20and%20Jesse%20Heidrich%20and%20Hui-Ping%20Wang%20and%20Joel%20Hooton%20and%20Joshua%20Solomon%20and%20Jorge%20Arinez%20and%20Guangze%20Li%20and%20Chenhui%20Shao&entry.1292438233=Near-field%20perception%20is%20essential%20for%20the%20safe%20operation%20of%20autonomous%20mobile%20robots%20%28AMRs%29%20in%20manufacturing%20environments.%20Conventional%20ranging%20sensors%20such%20as%20light%20detection%20and%20ranging%20%28LiDAR%29%20and%20ultrasonic%20devices%20provide%20broad%20situational%20awareness%20but%20often%20fail%20to%20detect%20small%20objects%20near%20the%20robot%20base.%20To%20address%20this%20limitation%2C%20this%20paper%20presents%20a%20three-tier%20near-field%20perception%20framework.%20The%20first%20approach%20employs%20light-discontinuity%20detection%2C%20which%20projects%20a%20laser%20stripe%20across%20the%20near-field%20zone%20and%20identifies%20interruptions%20in%20the%20stripe%20to%20perform%20fast%2C%20binary%20cutoff%20sensing%20for%20obstacle%20presence.%20The%20second%20approach%20utilizes%20light-displacement%20measurement%20to%20estimate%20object%20height%20by%20analyzing%20the%20geometric%20displacement%20of%20a%20projected%20stripe%20in%20the%20camera%20image%2C%20which%20provides%20quantitative%20obstacle%20height%20information%20with%20minimal%20computational%20overhead.%20The%20third%20approach%20employs%20a%20computer%20vision-based%20object%20detection%20model%20on%20embedded%20AI%20hardware%20to%20classify%20objects%2C%20enabling%20semantic%20perception%20and%20context-aware%20safety%20decisions.%20All%20methods%20are%20implemented%20on%20a%20Raspberry%20Pi%205%20system%2C%20achieving%20real-time%20performance%20at%2025%20or%2050%20frames%20per%20second.%20Experimental%20evaluation%20and%20comparative%20analysis%20demonstrate%20that%20the%20proposed%20hierarchy%20balances%20precision%2C%20computation%2C%20and%20cost%2C%20thereby%20providing%20a%20scalable%20perception%20solution%20for%20enabling%20safe%20operations%20of%20AMRs%20in%20manufacturing%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.13561v1&entry.124074799=Read"},
{"title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders", "author": "Khawla Elhadri and J\u00f6rg Schl\u00f6tterer and Christin Seifert", "abstract": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.", "link": "http://arxiv.org/abs/2512.13442v1", "date": "2025-12-15", "relevancy": 2.2216, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4575}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XNNTab%20--%20Interpretable%20Neural%20Networks%20for%20Tabular%20Data%20using%20Sparse%20Autoencoders&body=Title%3A%20XNNTab%20--%20Interpretable%20Neural%20Networks%20for%20Tabular%20Data%20using%20Sparse%20Autoencoders%0AAuthor%3A%20Khawla%20Elhadri%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Christin%20Seifert%0AAbstract%3A%20In%20data-driven%20applications%20relying%20on%20tabular%20data%2C%20where%20interpretability%20is%20key%2C%20machine%20learning%20models%20such%20as%20decision%20trees%20and%20linear%20regression%20are%20applied.%20Although%20neural%20networks%20can%20provide%20higher%20predictive%20performance%2C%20they%20are%20not%20used%20because%20of%20their%20blackbox%20nature.%20In%20this%20work%2C%20we%20present%20XNNTab%2C%20a%20neural%20architecture%20that%20combines%20the%20expressiveness%20of%20neural%20networks%20and%20interpretability.%20XNNTab%20first%20learns%20highly%20non-linear%20feature%20representations%2C%20which%20are%20decomposed%20into%20monosemantic%20features%20using%20a%20sparse%20autoencoder%20%28SAE%29.%20These%20features%20are%20then%20assigned%20human-interpretable%20concepts%2C%20making%20the%20overall%20model%20prediction%20intrinsically%20interpretable.%20XNNTab%20outperforms%20interpretable%20predictive%20models%2C%20and%20achieves%20comparable%20performance%20to%20its%20non-interpretable%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXNNTab%2520--%2520Interpretable%2520Neural%2520Networks%2520for%2520Tabular%2520Data%2520using%2520Sparse%2520Autoencoders%26entry.906535625%3DKhawla%2520Elhadri%2520and%2520J%25C3%25B6rg%2520Schl%25C3%25B6tterer%2520and%2520Christin%2520Seifert%26entry.1292438233%3DIn%2520data-driven%2520applications%2520relying%2520on%2520tabular%2520data%252C%2520where%2520interpretability%2520is%2520key%252C%2520machine%2520learning%2520models%2520such%2520as%2520decision%2520trees%2520and%2520linear%2520regression%2520are%2520applied.%2520Although%2520neural%2520networks%2520can%2520provide%2520higher%2520predictive%2520performance%252C%2520they%2520are%2520not%2520used%2520because%2520of%2520their%2520blackbox%2520nature.%2520In%2520this%2520work%252C%2520we%2520present%2520XNNTab%252C%2520a%2520neural%2520architecture%2520that%2520combines%2520the%2520expressiveness%2520of%2520neural%2520networks%2520and%2520interpretability.%2520XNNTab%2520first%2520learns%2520highly%2520non-linear%2520feature%2520representations%252C%2520which%2520are%2520decomposed%2520into%2520monosemantic%2520features%2520using%2520a%2520sparse%2520autoencoder%2520%2528SAE%2529.%2520These%2520features%2520are%2520then%2520assigned%2520human-interpretable%2520concepts%252C%2520making%2520the%2520overall%2520model%2520prediction%2520intrinsically%2520interpretable.%2520XNNTab%2520outperforms%2520interpretable%2520predictive%2520models%252C%2520and%2520achieves%2520comparable%2520performance%2520to%2520its%2520non-interpretable%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XNNTab%20--%20Interpretable%20Neural%20Networks%20for%20Tabular%20Data%20using%20Sparse%20Autoencoders&entry.906535625=Khawla%20Elhadri%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Christin%20Seifert&entry.1292438233=In%20data-driven%20applications%20relying%20on%20tabular%20data%2C%20where%20interpretability%20is%20key%2C%20machine%20learning%20models%20such%20as%20decision%20trees%20and%20linear%20regression%20are%20applied.%20Although%20neural%20networks%20can%20provide%20higher%20predictive%20performance%2C%20they%20are%20not%20used%20because%20of%20their%20blackbox%20nature.%20In%20this%20work%2C%20we%20present%20XNNTab%2C%20a%20neural%20architecture%20that%20combines%20the%20expressiveness%20of%20neural%20networks%20and%20interpretability.%20XNNTab%20first%20learns%20highly%20non-linear%20feature%20representations%2C%20which%20are%20decomposed%20into%20monosemantic%20features%20using%20a%20sparse%20autoencoder%20%28SAE%29.%20These%20features%20are%20then%20assigned%20human-interpretable%20concepts%2C%20making%20the%20overall%20model%20prediction%20intrinsically%20interpretable.%20XNNTab%20outperforms%20interpretable%20predictive%20models%2C%20and%20achieves%20comparable%20performance%20to%20its%20non-interpretable%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2512.13442v1&entry.124074799=Read"},
{"title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"", "author": "Carla Monteiro and Valentina Corbetta and Regina Beets-Tan and Lu\u00eds F. Teixeira and Wilson Silva", "abstract": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.", "link": "http://arxiv.org/abs/2512.13376v1", "date": "2025-12-15", "relevancy": 2.2204, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5729}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Generalization%20in%20Polyp%20Segmentation%20with%20DINO%20Self-Attention%20%22keys%22&body=Title%3A%20Unlocking%20Generalization%20in%20Polyp%20Segmentation%20with%20DINO%20Self-Attention%20%22keys%22%0AAuthor%3A%20Carla%20Monteiro%20and%20Valentina%20Corbetta%20and%20Regina%20Beets-Tan%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Wilson%20Silva%0AAbstract%3A%20Automatic%20polyp%20segmentation%20is%20crucial%20for%20improving%20the%20clinical%20identification%20of%20colorectal%20cancer%20%28CRC%29.%20While%20Deep%20Learning%20%28DL%29%20techniques%20have%20been%20extensively%20researched%20for%20this%20problem%2C%20current%20methods%20frequently%20struggle%20with%20generalization%2C%20particularly%20in%20data-constrained%20or%20challenging%20settings.%20Moreover%2C%20many%20existing%20polyp%20segmentation%20methods%20rely%20on%20complex%2C%20task-specific%20architectures.%20To%20address%20these%20limitations%2C%20we%20present%20a%20framework%20that%20leverages%20the%20intrinsic%20robustness%20of%20DINO%20self-attention%20%22key%22%20features%20for%20robust%20segmentation.%20Unlike%20traditional%20methods%20that%20extract%20tokens%20from%20the%20deepest%20layers%20of%20the%20Vision%20Transformer%20%28ViT%29%2C%20our%20approach%20leverages%20the%20key%20features%20of%20the%20self-attention%20module%20with%20a%20simple%20convolutional%20decoder%20to%20predict%20polyp%20masks%2C%20resulting%20in%20enhanced%20performance%20and%20better%20generalizability.%20We%20validate%20our%20approach%20using%20a%20multi-center%20dataset%20under%20two%20rigorous%20protocols%3A%20Domain%20Generalization%20%28DG%29%20and%20Extreme%20Single%20Domain%20Generalization%20%28ESDG%29.%20Our%20results%2C%20supported%20by%20a%20comprehensive%20statistical%20analysis%2C%20demonstrate%20that%20this%20pipeline%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20significantly%20enhancing%20generalization%2C%20particularly%20in%20data-scarce%20and%20challenging%20scenarios.%20While%20avoiding%20a%20polyp-specific%20architecture%2C%20we%20surpass%20well-established%20models%20like%20nnU-Net%20and%20UM-Net.%20Additionally%2C%20we%20provide%20a%20systematic%20benchmark%20of%20the%20DINO%20framework%27s%20evolution%2C%20quantifying%20the%20specific%20impact%20of%20architectural%20advancements%20on%20downstream%20polyp%20segmentation%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Generalization%2520in%2520Polyp%2520Segmentation%2520with%2520DINO%2520Self-Attention%2520%2522keys%2522%26entry.906535625%3DCarla%2520Monteiro%2520and%2520Valentina%2520Corbetta%2520and%2520Regina%2520Beets-Tan%2520and%2520Lu%25C3%25ADs%2520F.%2520Teixeira%2520and%2520Wilson%2520Silva%26entry.1292438233%3DAutomatic%2520polyp%2520segmentation%2520is%2520crucial%2520for%2520improving%2520the%2520clinical%2520identification%2520of%2520colorectal%2520cancer%2520%2528CRC%2529.%2520While%2520Deep%2520Learning%2520%2528DL%2529%2520techniques%2520have%2520been%2520extensively%2520researched%2520for%2520this%2520problem%252C%2520current%2520methods%2520frequently%2520struggle%2520with%2520generalization%252C%2520particularly%2520in%2520data-constrained%2520or%2520challenging%2520settings.%2520Moreover%252C%2520many%2520existing%2520polyp%2520segmentation%2520methods%2520rely%2520on%2520complex%252C%2520task-specific%2520architectures.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520a%2520framework%2520that%2520leverages%2520the%2520intrinsic%2520robustness%2520of%2520DINO%2520self-attention%2520%2522key%2522%2520features%2520for%2520robust%2520segmentation.%2520Unlike%2520traditional%2520methods%2520that%2520extract%2520tokens%2520from%2520the%2520deepest%2520layers%2520of%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%252C%2520our%2520approach%2520leverages%2520the%2520key%2520features%2520of%2520the%2520self-attention%2520module%2520with%2520a%2520simple%2520convolutional%2520decoder%2520to%2520predict%2520polyp%2520masks%252C%2520resulting%2520in%2520enhanced%2520performance%2520and%2520better%2520generalizability.%2520We%2520validate%2520our%2520approach%2520using%2520a%2520multi-center%2520dataset%2520under%2520two%2520rigorous%2520protocols%253A%2520Domain%2520Generalization%2520%2528DG%2529%2520and%2520Extreme%2520Single%2520Domain%2520Generalization%2520%2528ESDG%2529.%2520Our%2520results%252C%2520supported%2520by%2520a%2520comprehensive%2520statistical%2520analysis%252C%2520demonstrate%2520that%2520this%2520pipeline%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%252C%2520significantly%2520enhancing%2520generalization%252C%2520particularly%2520in%2520data-scarce%2520and%2520challenging%2520scenarios.%2520While%2520avoiding%2520a%2520polyp-specific%2520architecture%252C%2520we%2520surpass%2520well-established%2520models%2520like%2520nnU-Net%2520and%2520UM-Net.%2520Additionally%252C%2520we%2520provide%2520a%2520systematic%2520benchmark%2520of%2520the%2520DINO%2520framework%2527s%2520evolution%252C%2520quantifying%2520the%2520specific%2520impact%2520of%2520architectural%2520advancements%2520on%2520downstream%2520polyp%2520segmentation%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Generalization%20in%20Polyp%20Segmentation%20with%20DINO%20Self-Attention%20%22keys%22&entry.906535625=Carla%20Monteiro%20and%20Valentina%20Corbetta%20and%20Regina%20Beets-Tan%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Wilson%20Silva&entry.1292438233=Automatic%20polyp%20segmentation%20is%20crucial%20for%20improving%20the%20clinical%20identification%20of%20colorectal%20cancer%20%28CRC%29.%20While%20Deep%20Learning%20%28DL%29%20techniques%20have%20been%20extensively%20researched%20for%20this%20problem%2C%20current%20methods%20frequently%20struggle%20with%20generalization%2C%20particularly%20in%20data-constrained%20or%20challenging%20settings.%20Moreover%2C%20many%20existing%20polyp%20segmentation%20methods%20rely%20on%20complex%2C%20task-specific%20architectures.%20To%20address%20these%20limitations%2C%20we%20present%20a%20framework%20that%20leverages%20the%20intrinsic%20robustness%20of%20DINO%20self-attention%20%22key%22%20features%20for%20robust%20segmentation.%20Unlike%20traditional%20methods%20that%20extract%20tokens%20from%20the%20deepest%20layers%20of%20the%20Vision%20Transformer%20%28ViT%29%2C%20our%20approach%20leverages%20the%20key%20features%20of%20the%20self-attention%20module%20with%20a%20simple%20convolutional%20decoder%20to%20predict%20polyp%20masks%2C%20resulting%20in%20enhanced%20performance%20and%20better%20generalizability.%20We%20validate%20our%20approach%20using%20a%20multi-center%20dataset%20under%20two%20rigorous%20protocols%3A%20Domain%20Generalization%20%28DG%29%20and%20Extreme%20Single%20Domain%20Generalization%20%28ESDG%29.%20Our%20results%2C%20supported%20by%20a%20comprehensive%20statistical%20analysis%2C%20demonstrate%20that%20this%20pipeline%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20significantly%20enhancing%20generalization%2C%20particularly%20in%20data-scarce%20and%20challenging%20scenarios.%20While%20avoiding%20a%20polyp-specific%20architecture%2C%20we%20surpass%20well-established%20models%20like%20nnU-Net%20and%20UM-Net.%20Additionally%2C%20we%20provide%20a%20systematic%20benchmark%20of%20the%20DINO%20framework%27s%20evolution%2C%20quantifying%20the%20specific%20impact%20of%20architectural%20advancements%20on%20downstream%20polyp%20segmentation%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.13376v1&entry.124074799=Read"},
{"title": "Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning", "author": "Yewei Liu and Xiyuan Wang and Muhan Zhang", "abstract": "We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning", "link": "http://arxiv.org/abs/2506.12041v3", "date": "2025-12-15", "relevancy": 2.201, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4559}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.435}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%20Pruning%20via%20Graph%20Metanetworks%20%3A%20A%20Universal%20Meta%20Learning%20Framework%20for%20Network%20Pruning&body=Title%3A%20Meta%20Pruning%20via%20Graph%20Metanetworks%20%3A%20A%20Universal%20Meta%20Learning%20Framework%20for%20Network%20Pruning%0AAuthor%3A%20Yewei%20Liu%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang%0AAbstract%3A%20We%20propose%20an%20entirely%20new%20meta-learning%20framework%20for%20network%20pruning.%20It%20is%20a%20general%20framework%20that%20can%20be%20theoretically%20applied%20to%20almost%20all%20types%20of%20networks%20with%20all%20kinds%20of%20pruning%20and%20has%20great%20generality%20and%20transferability.%20Experiments%20have%20shown%20that%20it%20can%20achieve%20outstanding%20results%20on%20many%20popular%20and%20representative%20pruning%20tasks%20%28including%20both%20CNNs%20and%20Transformers%29.%20Unlike%20all%20prior%20works%20that%20either%20rely%20on%20fixed%2C%20hand-crafted%20criteria%20to%20prune%20in%20a%20coarse%20manner%2C%20or%20employ%20learning%20to%20prune%20ways%20that%20require%20special%20training%20during%20each%20pruning%20and%20lack%20generality.%20Our%20framework%20can%20learn%20complex%20pruning%20rules%20automatically%20via%20a%20neural%20network%20%28metanetwork%29%20and%20has%20great%20generality%20that%20can%20prune%20without%20any%20special%20training.%20More%20specifically%2C%20we%20introduce%20the%20newly%20developed%20idea%20of%20metanetwork%20from%20meta-learning%20into%20pruning.%20A%20metanetwork%20is%20a%20network%20that%20takes%20another%20network%20as%20input%20and%20produces%20a%20modified%20network%20as%20output.%20In%20this%20paper%2C%20we%20first%20establish%20a%20bijective%20mapping%20between%20neural%20networks%20and%20graphs%2C%20and%20then%20employ%20a%20graph%20neural%20network%20as%20our%20metanetwork.%20We%20train%20a%20metanetwork%20that%20learns%20the%20pruning%20strategy%20automatically%20and%20can%20transform%20a%20network%20that%20is%20hard%20to%20prune%20into%20another%20network%20that%20is%20much%20easier%20to%20prune.%20Once%20the%20metanetwork%20is%20trained%2C%20our%20pruning%20needs%20nothing%20more%20than%20a%20feedforward%20through%20the%20metanetwork%20and%20some%20standard%20finetuning%20to%20prune%20at%20state-of-the-art.%20Our%20code%20is%20available%20at%20https%3A//github.com/Yewei-Liu/MetaPruning%0ALink%3A%20http%3A//arxiv.org/abs/2506.12041v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%2520Pruning%2520via%2520Graph%2520Metanetworks%2520%253A%2520A%2520Universal%2520Meta%2520Learning%2520Framework%2520for%2520Network%2520Pruning%26entry.906535625%3DYewei%2520Liu%2520and%2520Xiyuan%2520Wang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3DWe%2520propose%2520an%2520entirely%2520new%2520meta-learning%2520framework%2520for%2520network%2520pruning.%2520It%2520is%2520a%2520general%2520framework%2520that%2520can%2520be%2520theoretically%2520applied%2520to%2520almost%2520all%2520types%2520of%2520networks%2520with%2520all%2520kinds%2520of%2520pruning%2520and%2520has%2520great%2520generality%2520and%2520transferability.%2520Experiments%2520have%2520shown%2520that%2520it%2520can%2520achieve%2520outstanding%2520results%2520on%2520many%2520popular%2520and%2520representative%2520pruning%2520tasks%2520%2528including%2520both%2520CNNs%2520and%2520Transformers%2529.%2520Unlike%2520all%2520prior%2520works%2520that%2520either%2520rely%2520on%2520fixed%252C%2520hand-crafted%2520criteria%2520to%2520prune%2520in%2520a%2520coarse%2520manner%252C%2520or%2520employ%2520learning%2520to%2520prune%2520ways%2520that%2520require%2520special%2520training%2520during%2520each%2520pruning%2520and%2520lack%2520generality.%2520Our%2520framework%2520can%2520learn%2520complex%2520pruning%2520rules%2520automatically%2520via%2520a%2520neural%2520network%2520%2528metanetwork%2529%2520and%2520has%2520great%2520generality%2520that%2520can%2520prune%2520without%2520any%2520special%2520training.%2520More%2520specifically%252C%2520we%2520introduce%2520the%2520newly%2520developed%2520idea%2520of%2520metanetwork%2520from%2520meta-learning%2520into%2520pruning.%2520A%2520metanetwork%2520is%2520a%2520network%2520that%2520takes%2520another%2520network%2520as%2520input%2520and%2520produces%2520a%2520modified%2520network%2520as%2520output.%2520In%2520this%2520paper%252C%2520we%2520first%2520establish%2520a%2520bijective%2520mapping%2520between%2520neural%2520networks%2520and%2520graphs%252C%2520and%2520then%2520employ%2520a%2520graph%2520neural%2520network%2520as%2520our%2520metanetwork.%2520We%2520train%2520a%2520metanetwork%2520that%2520learns%2520the%2520pruning%2520strategy%2520automatically%2520and%2520can%2520transform%2520a%2520network%2520that%2520is%2520hard%2520to%2520prune%2520into%2520another%2520network%2520that%2520is%2520much%2520easier%2520to%2520prune.%2520Once%2520the%2520metanetwork%2520is%2520trained%252C%2520our%2520pruning%2520needs%2520nothing%2520more%2520than%2520a%2520feedforward%2520through%2520the%2520metanetwork%2520and%2520some%2520standard%2520finetuning%2520to%2520prune%2520at%2520state-of-the-art.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Yewei-Liu/MetaPruning%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12041v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20Pruning%20via%20Graph%20Metanetworks%20%3A%20A%20Universal%20Meta%20Learning%20Framework%20for%20Network%20Pruning&entry.906535625=Yewei%20Liu%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang&entry.1292438233=We%20propose%20an%20entirely%20new%20meta-learning%20framework%20for%20network%20pruning.%20It%20is%20a%20general%20framework%20that%20can%20be%20theoretically%20applied%20to%20almost%20all%20types%20of%20networks%20with%20all%20kinds%20of%20pruning%20and%20has%20great%20generality%20and%20transferability.%20Experiments%20have%20shown%20that%20it%20can%20achieve%20outstanding%20results%20on%20many%20popular%20and%20representative%20pruning%20tasks%20%28including%20both%20CNNs%20and%20Transformers%29.%20Unlike%20all%20prior%20works%20that%20either%20rely%20on%20fixed%2C%20hand-crafted%20criteria%20to%20prune%20in%20a%20coarse%20manner%2C%20or%20employ%20learning%20to%20prune%20ways%20that%20require%20special%20training%20during%20each%20pruning%20and%20lack%20generality.%20Our%20framework%20can%20learn%20complex%20pruning%20rules%20automatically%20via%20a%20neural%20network%20%28metanetwork%29%20and%20has%20great%20generality%20that%20can%20prune%20without%20any%20special%20training.%20More%20specifically%2C%20we%20introduce%20the%20newly%20developed%20idea%20of%20metanetwork%20from%20meta-learning%20into%20pruning.%20A%20metanetwork%20is%20a%20network%20that%20takes%20another%20network%20as%20input%20and%20produces%20a%20modified%20network%20as%20output.%20In%20this%20paper%2C%20we%20first%20establish%20a%20bijective%20mapping%20between%20neural%20networks%20and%20graphs%2C%20and%20then%20employ%20a%20graph%20neural%20network%20as%20our%20metanetwork.%20We%20train%20a%20metanetwork%20that%20learns%20the%20pruning%20strategy%20automatically%20and%20can%20transform%20a%20network%20that%20is%20hard%20to%20prune%20into%20another%20network%20that%20is%20much%20easier%20to%20prune.%20Once%20the%20metanetwork%20is%20trained%2C%20our%20pruning%20needs%20nothing%20more%20than%20a%20feedforward%20through%20the%20metanetwork%20and%20some%20standard%20finetuning%20to%20prune%20at%20state-of-the-art.%20Our%20code%20is%20available%20at%20https%3A//github.com/Yewei-Liu/MetaPruning&entry.1838667208=http%3A//arxiv.org/abs/2506.12041v3&entry.124074799=Read"},
{"title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning", "author": "Li Wang and Changhao Zhang and Zengqi Xiu and Kai Lu and Xin Yu and Kui Zhang and Wenjun Wu", "abstract": "Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., up to 1.5B parameters) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.", "link": "http://arxiv.org/abs/2508.10019v2", "date": "2025-12-15", "relevancy": 2.1814, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Understanding%20from%20Reasoning%20via%20Problem%20Space%20Mapping%20for%20Small-Scale%20Model%20Reasoning&body=Title%3A%20Decoupling%20Understanding%20from%20Reasoning%20via%20Problem%20Space%20Mapping%20for%20Small-Scale%20Model%20Reasoning%0AAuthor%3A%20Li%20Wang%20and%20Changhao%20Zhang%20and%20Zengqi%20Xiu%20and%20Kai%20Lu%20and%20Xin%20Yu%20and%20Kui%20Zhang%20and%20Wenjun%20Wu%0AAbstract%3A%20Despite%20recent%20advances%20in%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20improving%20the%20reasoning%20ability%20of%20Small%20Language%20Models%20%28SLMs%2C%20e.g.%2C%20up%20to%201.5B%20parameters%29%20remains%20challenging.%20A%20key%20obstacle%20lies%20in%20the%20complexity%20and%20variability%20of%20natural%20language%3A%20essentially%20equivalent%20problems%20often%20appear%20in%20diverse%20surface%20forms%2C%20often%20obscured%20by%20redundant%20or%20distracting%20details.%20This%20imposes%20a%20dual%20burden%20on%20SLMs%3A%20they%20must%20first%20extract%20the%20core%20problem%20from%20complex%20linguistic%20input%2C%20and%20then%20perform%20reasoning%20based%20on%20that%20understanding.%20The%20resulting%20vast%20and%20noisy%20problem%20space%20hinders%20optimization%2C%20particularly%20for%20models%20with%20limited%20capacity.%20To%20address%20this%2C%20we%20propose%20a%20new%20framework%20that%20decouples%20understanding%20from%20reasoning%20by%20mapping%20natural%20language%20problems%20into%20a%20canonical%20problem%20space-a%20semantically%20simplified%20yet%20expressive%20domain.%20This%20enables%20SLMs%20to%20focus%20on%20reasoning%20over%20standardized%20inputs%2C%20free%20from%20linguistic%20variability.%20Within%20this%20framework%2C%20we%20introduce%20DURIT%20%28Decoupled%20Understanding%20from%20Reasoning%20via%20Iterative%20Training%29%2C%20a%20three-step%20algorithm%20that%20iteratively%3A%20%281%29%20mapping%20natural%20language%20problems%20via%20reinforcement%20learning%2C%20%282%29%20aligns%20reasoning%20trajectories%20through%20self-distillation%2C%20and%20%283%29%20trains%20reasoning%20policies%20in%20the%20problem%20space.%20The%20mapper%20and%20reasoner%20are%20co-trained%20in%20an%20alternating%20loop%20throughout%20this%20process.%20Experiments%20show%20that%20DURIT%20substantially%20improves%20SLMs%27%20performance%20on%20both%20in-domain%20and%20out-of-domain%20mathematical%20and%20logical%20reasoning%20tasks.%20Beyond%20improving%20reasoning%20capabilities%2C%20DURIT%20also%20improves%20the%20robustness%20of%20reasoning%2C%20validating%20decoupling%20understanding%20from%20reasoning%20as%20an%20effective%20strategy%20for%20strengthening%20SLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Understanding%2520from%2520Reasoning%2520via%2520Problem%2520Space%2520Mapping%2520for%2520Small-Scale%2520Model%2520Reasoning%26entry.906535625%3DLi%2520Wang%2520and%2520Changhao%2520Zhang%2520and%2520Zengqi%2520Xiu%2520and%2520Kai%2520Lu%2520and%2520Xin%2520Yu%2520and%2520Kui%2520Zhang%2520and%2520Wenjun%2520Wu%26entry.1292438233%3DDespite%2520recent%2520advances%2520in%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520improving%2520the%2520reasoning%2520ability%2520of%2520Small%2520Language%2520Models%2520%2528SLMs%252C%2520e.g.%252C%2520up%2520to%25201.5B%2520parameters%2529%2520remains%2520challenging.%2520A%2520key%2520obstacle%2520lies%2520in%2520the%2520complexity%2520and%2520variability%2520of%2520natural%2520language%253A%2520essentially%2520equivalent%2520problems%2520often%2520appear%2520in%2520diverse%2520surface%2520forms%252C%2520often%2520obscured%2520by%2520redundant%2520or%2520distracting%2520details.%2520This%2520imposes%2520a%2520dual%2520burden%2520on%2520SLMs%253A%2520they%2520must%2520first%2520extract%2520the%2520core%2520problem%2520from%2520complex%2520linguistic%2520input%252C%2520and%2520then%2520perform%2520reasoning%2520based%2520on%2520that%2520understanding.%2520The%2520resulting%2520vast%2520and%2520noisy%2520problem%2520space%2520hinders%2520optimization%252C%2520particularly%2520for%2520models%2520with%2520limited%2520capacity.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520framework%2520that%2520decouples%2520understanding%2520from%2520reasoning%2520by%2520mapping%2520natural%2520language%2520problems%2520into%2520a%2520canonical%2520problem%2520space-a%2520semantically%2520simplified%2520yet%2520expressive%2520domain.%2520This%2520enables%2520SLMs%2520to%2520focus%2520on%2520reasoning%2520over%2520standardized%2520inputs%252C%2520free%2520from%2520linguistic%2520variability.%2520Within%2520this%2520framework%252C%2520we%2520introduce%2520DURIT%2520%2528Decoupled%2520Understanding%2520from%2520Reasoning%2520via%2520Iterative%2520Training%2529%252C%2520a%2520three-step%2520algorithm%2520that%2520iteratively%253A%2520%25281%2529%2520mapping%2520natural%2520language%2520problems%2520via%2520reinforcement%2520learning%252C%2520%25282%2529%2520aligns%2520reasoning%2520trajectories%2520through%2520self-distillation%252C%2520and%2520%25283%2529%2520trains%2520reasoning%2520policies%2520in%2520the%2520problem%2520space.%2520The%2520mapper%2520and%2520reasoner%2520are%2520co-trained%2520in%2520an%2520alternating%2520loop%2520throughout%2520this%2520process.%2520Experiments%2520show%2520that%2520DURIT%2520substantially%2520improves%2520SLMs%2527%2520performance%2520on%2520both%2520in-domain%2520and%2520out-of-domain%2520mathematical%2520and%2520logical%2520reasoning%2520tasks.%2520Beyond%2520improving%2520reasoning%2520capabilities%252C%2520DURIT%2520also%2520improves%2520the%2520robustness%2520of%2520reasoning%252C%2520validating%2520decoupling%2520understanding%2520from%2520reasoning%2520as%2520an%2520effective%2520strategy%2520for%2520strengthening%2520SLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Understanding%20from%20Reasoning%20via%20Problem%20Space%20Mapping%20for%20Small-Scale%20Model%20Reasoning&entry.906535625=Li%20Wang%20and%20Changhao%20Zhang%20and%20Zengqi%20Xiu%20and%20Kai%20Lu%20and%20Xin%20Yu%20and%20Kui%20Zhang%20and%20Wenjun%20Wu&entry.1292438233=Despite%20recent%20advances%20in%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20improving%20the%20reasoning%20ability%20of%20Small%20Language%20Models%20%28SLMs%2C%20e.g.%2C%20up%20to%201.5B%20parameters%29%20remains%20challenging.%20A%20key%20obstacle%20lies%20in%20the%20complexity%20and%20variability%20of%20natural%20language%3A%20essentially%20equivalent%20problems%20often%20appear%20in%20diverse%20surface%20forms%2C%20often%20obscured%20by%20redundant%20or%20distracting%20details.%20This%20imposes%20a%20dual%20burden%20on%20SLMs%3A%20they%20must%20first%20extract%20the%20core%20problem%20from%20complex%20linguistic%20input%2C%20and%20then%20perform%20reasoning%20based%20on%20that%20understanding.%20The%20resulting%20vast%20and%20noisy%20problem%20space%20hinders%20optimization%2C%20particularly%20for%20models%20with%20limited%20capacity.%20To%20address%20this%2C%20we%20propose%20a%20new%20framework%20that%20decouples%20understanding%20from%20reasoning%20by%20mapping%20natural%20language%20problems%20into%20a%20canonical%20problem%20space-a%20semantically%20simplified%20yet%20expressive%20domain.%20This%20enables%20SLMs%20to%20focus%20on%20reasoning%20over%20standardized%20inputs%2C%20free%20from%20linguistic%20variability.%20Within%20this%20framework%2C%20we%20introduce%20DURIT%20%28Decoupled%20Understanding%20from%20Reasoning%20via%20Iterative%20Training%29%2C%20a%20three-step%20algorithm%20that%20iteratively%3A%20%281%29%20mapping%20natural%20language%20problems%20via%20reinforcement%20learning%2C%20%282%29%20aligns%20reasoning%20trajectories%20through%20self-distillation%2C%20and%20%283%29%20trains%20reasoning%20policies%20in%20the%20problem%20space.%20The%20mapper%20and%20reasoner%20are%20co-trained%20in%20an%20alternating%20loop%20throughout%20this%20process.%20Experiments%20show%20that%20DURIT%20substantially%20improves%20SLMs%27%20performance%20on%20both%20in-domain%20and%20out-of-domain%20mathematical%20and%20logical%20reasoning%20tasks.%20Beyond%20improving%20reasoning%20capabilities%2C%20DURIT%20also%20improves%20the%20robustness%20of%20reasoning%2C%20validating%20decoupling%20understanding%20from%20reasoning%20as%20an%20effective%20strategy%20for%20strengthening%20SLMs.&entry.1838667208=http%3A//arxiv.org/abs/2508.10019v2&entry.124074799=Read"},
{"title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception", "author": "Gong Chen and Chaokun Zhang and Pengcheng Lv and Xiaohui Xie", "abstract": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.", "link": "http://arxiv.org/abs/2512.13191v1", "date": "2025-12-15", "relevancy": 2.1791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRA%3A%20A%20Collaborative%20Robust%20Architecture%20with%20Hybrid%20Fusion%20for%20Efficient%20Perception&body=Title%3A%20CoRA%3A%20A%20Collaborative%20Robust%20Architecture%20with%20Hybrid%20Fusion%20for%20Efficient%20Perception%0AAuthor%3A%20Gong%20Chen%20and%20Chaokun%20Zhang%20and%20Pengcheng%20Lv%20and%20Xiaohui%20Xie%0AAbstract%3A%20Collaborative%20perception%20has%20garnered%20significant%20attention%20as%20a%20crucial%20technology%20to%20overcome%20the%20perceptual%20limitations%20of%20single-agent%20systems.%20Many%20state-of-the-art%20%28SOTA%29%20methods%20have%20achieved%20communication%20efficiency%20and%20high%20performance%20via%20intermediate%20fusion.%20However%2C%20they%20share%20a%20critical%20vulnerability%3A%20their%20performance%20degrades%20under%20adverse%20communication%20conditions%20due%20to%20the%20misalignment%20induced%20by%20data%20transmission%2C%20which%20severely%20hampers%20their%20practical%20deployment.%20To%20bridge%20this%20gap%2C%20we%20re-examine%20different%20fusion%20paradigms%2C%20and%20recover%20that%20the%20strengths%20of%20intermediate%20and%20late%20fusion%20are%20not%20a%20trade-off%2C%20but%20a%20complementary%20pairing.%20Based%20on%20this%20key%20insight%2C%20we%20propose%20CoRA%2C%20a%20novel%20collaborative%20robust%20architecture%20with%20a%20hybrid%20approach%20to%20decouple%20performance%20from%20robustness%20with%20low%20communication.%20It%20is%20composed%20of%20two%20components%3A%20a%20feature-level%20fusion%20branch%20and%20an%20object-level%20correction%20branch.%20Its%20first%20branch%20selects%20critical%20features%20and%20fuses%20them%20efficiently%20to%20ensure%20both%20performance%20and%20scalability.%20The%20second%20branch%20leverages%20semantic%20relevance%20to%20correct%20spatial%20displacements%2C%20guaranteeing%20resilience%20against%20pose%20errors.%20Experiments%20demonstrate%20the%20superiority%20of%20CoRA.%20Under%20extreme%20scenarios%2C%20CoRA%20improves%20upon%20its%20baseline%20performance%20by%20approximately%2019%25%20in%20AP%400.7%20with%20more%20than%205x%20less%20communication%20volume%2C%20which%20makes%20it%20a%20promising%20solution%20for%20robust%20collaborative%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRA%253A%2520A%2520Collaborative%2520Robust%2520Architecture%2520with%2520Hybrid%2520Fusion%2520for%2520Efficient%2520Perception%26entry.906535625%3DGong%2520Chen%2520and%2520Chaokun%2520Zhang%2520and%2520Pengcheng%2520Lv%2520and%2520Xiaohui%2520Xie%26entry.1292438233%3DCollaborative%2520perception%2520has%2520garnered%2520significant%2520attention%2520as%2520a%2520crucial%2520technology%2520to%2520overcome%2520the%2520perceptual%2520limitations%2520of%2520single-agent%2520systems.%2520Many%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520have%2520achieved%2520communication%2520efficiency%2520and%2520high%2520performance%2520via%2520intermediate%2520fusion.%2520However%252C%2520they%2520share%2520a%2520critical%2520vulnerability%253A%2520their%2520performance%2520degrades%2520under%2520adverse%2520communication%2520conditions%2520due%2520to%2520the%2520misalignment%2520induced%2520by%2520data%2520transmission%252C%2520which%2520severely%2520hampers%2520their%2520practical%2520deployment.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520re-examine%2520different%2520fusion%2520paradigms%252C%2520and%2520recover%2520that%2520the%2520strengths%2520of%2520intermediate%2520and%2520late%2520fusion%2520are%2520not%2520a%2520trade-off%252C%2520but%2520a%2520complementary%2520pairing.%2520Based%2520on%2520this%2520key%2520insight%252C%2520we%2520propose%2520CoRA%252C%2520a%2520novel%2520collaborative%2520robust%2520architecture%2520with%2520a%2520hybrid%2520approach%2520to%2520decouple%2520performance%2520from%2520robustness%2520with%2520low%2520communication.%2520It%2520is%2520composed%2520of%2520two%2520components%253A%2520a%2520feature-level%2520fusion%2520branch%2520and%2520an%2520object-level%2520correction%2520branch.%2520Its%2520first%2520branch%2520selects%2520critical%2520features%2520and%2520fuses%2520them%2520efficiently%2520to%2520ensure%2520both%2520performance%2520and%2520scalability.%2520The%2520second%2520branch%2520leverages%2520semantic%2520relevance%2520to%2520correct%2520spatial%2520displacements%252C%2520guaranteeing%2520resilience%2520against%2520pose%2520errors.%2520Experiments%2520demonstrate%2520the%2520superiority%2520of%2520CoRA.%2520Under%2520extreme%2520scenarios%252C%2520CoRA%2520improves%2520upon%2520its%2520baseline%2520performance%2520by%2520approximately%252019%2525%2520in%2520AP%25400.7%2520with%2520more%2520than%25205x%2520less%2520communication%2520volume%252C%2520which%2520makes%2520it%2520a%2520promising%2520solution%2520for%2520robust%2520collaborative%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRA%3A%20A%20Collaborative%20Robust%20Architecture%20with%20Hybrid%20Fusion%20for%20Efficient%20Perception&entry.906535625=Gong%20Chen%20and%20Chaokun%20Zhang%20and%20Pengcheng%20Lv%20and%20Xiaohui%20Xie&entry.1292438233=Collaborative%20perception%20has%20garnered%20significant%20attention%20as%20a%20crucial%20technology%20to%20overcome%20the%20perceptual%20limitations%20of%20single-agent%20systems.%20Many%20state-of-the-art%20%28SOTA%29%20methods%20have%20achieved%20communication%20efficiency%20and%20high%20performance%20via%20intermediate%20fusion.%20However%2C%20they%20share%20a%20critical%20vulnerability%3A%20their%20performance%20degrades%20under%20adverse%20communication%20conditions%20due%20to%20the%20misalignment%20induced%20by%20data%20transmission%2C%20which%20severely%20hampers%20their%20practical%20deployment.%20To%20bridge%20this%20gap%2C%20we%20re-examine%20different%20fusion%20paradigms%2C%20and%20recover%20that%20the%20strengths%20of%20intermediate%20and%20late%20fusion%20are%20not%20a%20trade-off%2C%20but%20a%20complementary%20pairing.%20Based%20on%20this%20key%20insight%2C%20we%20propose%20CoRA%2C%20a%20novel%20collaborative%20robust%20architecture%20with%20a%20hybrid%20approach%20to%20decouple%20performance%20from%20robustness%20with%20low%20communication.%20It%20is%20composed%20of%20two%20components%3A%20a%20feature-level%20fusion%20branch%20and%20an%20object-level%20correction%20branch.%20Its%20first%20branch%20selects%20critical%20features%20and%20fuses%20them%20efficiently%20to%20ensure%20both%20performance%20and%20scalability.%20The%20second%20branch%20leverages%20semantic%20relevance%20to%20correct%20spatial%20displacements%2C%20guaranteeing%20resilience%20against%20pose%20errors.%20Experiments%20demonstrate%20the%20superiority%20of%20CoRA.%20Under%20extreme%20scenarios%2C%20CoRA%20improves%20upon%20its%20baseline%20performance%20by%20approximately%2019%25%20in%20AP%400.7%20with%20more%20than%205x%20less%20communication%20volume%2C%20which%20makes%20it%20a%20promising%20solution%20for%20robust%20collaborative%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2512.13191v1&entry.124074799=Read"},
{"title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models", "author": "Kei Saito", "abstract": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $\u03c1$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "link": "http://arxiv.org/abs/2512.13478v1", "date": "2025-12-15", "relevancy": 2.1717, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Resolution%20Reasoning%3A%20A%20Framework%20for%20Preserving%20Semantic%20Ambiguity%20in%20Language%20Models&body=Title%3A%20Non-Resolution%20Reasoning%3A%20A%20Framework%20for%20Preserving%20Semantic%20Ambiguity%20in%20Language%20Models%0AAuthor%3A%20Kei%20Saito%0AAbstract%3A%20Premature%20semantic%20collapse%20--%20the%20forced%20early%20commitment%20to%20a%20single%20meaning%20--%20remains%20a%20core%20architectural%20limitation%20of%20current%20language%20models.%20Softmax-driven%20competition%20and%20greedy%20decoding%20cause%20models%20to%20discard%20valid%20interpretations%20before%20sufficient%20context%20is%20available%2C%20resulting%20in%20brittle%20reasoning%20and%20context%20failures.%20We%20introduce%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20a%20general%20computational%20framework%20that%20preserves%20semantic%20ambiguity%20during%20inference%20and%20performs%20resolution%20only%20when%20explicitly%20required.%20NRR%20integrates%20three%20components%3A%20%281%29%20Multi-Vector%20Embeddings%20that%20maintain%20multiple%20viable%20interpretations%20per%20token%2C%20%282%29%20Non-Collapsing%20Attention%20that%20prevents%20winner-take-all%20dynamics%20across%20layers%2C%20and%20%283%29%20Contextual%20Identity%20Tracking%20%28CIT%29%2C%20which%20assigns%20context-specific%20identities%20to%20recurring%20entities%20%28e.g.%2C%20distinguishing%20%22Dr.%20Smith%20the%20cardiologist%22%20from%20%22Dr.%20Smith%20the%20researcher%22%29.%20These%20mechanisms%20are%20unified%20by%20an%20external%20Resolution%20Operator%20%24%CF%81%24%20that%20makes%20semantic%20commitment%20explicit%2C%20controllable%2C%20and%20task-dependent.%20Unlike%20standard%20architectures%2C%20NRR%20separates%20representation%20from%20resolution%2C%20allowing%20a%20single%20model%20to%20shift%20between%20creative%2C%20factual%2C%20and%20ambiguity-preserving%20reasoning%20without%20retraining.%20A%20synthetic%20evaluation%20demonstrates%20NRR%27s%20ability%20to%20preserve%20ambiguity%20and%20track%20context%3A%20CIT-enhanced%20models%20achieve%2090.9%25%20accuracy%20on%20out-of-distribution%20identity-shift%20tasks%2C%20compared%20to%209.1%25%20for%20transformer%20baselines.%20NRR%20provides%20a%20principled%20alternative%20to%20premature%20collapse%2C%20reframing%20ambiguity%20as%20an%20explicit%20representational%20state%20rather%20than%20a%20failure%20mode.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Resolution%2520Reasoning%253A%2520A%2520Framework%2520for%2520Preserving%2520Semantic%2520Ambiguity%2520in%2520Language%2520Models%26entry.906535625%3DKei%2520Saito%26entry.1292438233%3DPremature%2520semantic%2520collapse%2520--%2520the%2520forced%2520early%2520commitment%2520to%2520a%2520single%2520meaning%2520--%2520remains%2520a%2520core%2520architectural%2520limitation%2520of%2520current%2520language%2520models.%2520Softmax-driven%2520competition%2520and%2520greedy%2520decoding%2520cause%2520models%2520to%2520discard%2520valid%2520interpretations%2520before%2520sufficient%2520context%2520is%2520available%252C%2520resulting%2520in%2520brittle%2520reasoning%2520and%2520context%2520failures.%2520We%2520introduce%2520Non-Resolution%2520Reasoning%2520%2528NRR%2529%252C%2520a%2520general%2520computational%2520framework%2520that%2520preserves%2520semantic%2520ambiguity%2520during%2520inference%2520and%2520performs%2520resolution%2520only%2520when%2520explicitly%2520required.%2520NRR%2520integrates%2520three%2520components%253A%2520%25281%2529%2520Multi-Vector%2520Embeddings%2520that%2520maintain%2520multiple%2520viable%2520interpretations%2520per%2520token%252C%2520%25282%2529%2520Non-Collapsing%2520Attention%2520that%2520prevents%2520winner-take-all%2520dynamics%2520across%2520layers%252C%2520and%2520%25283%2529%2520Contextual%2520Identity%2520Tracking%2520%2528CIT%2529%252C%2520which%2520assigns%2520context-specific%2520identities%2520to%2520recurring%2520entities%2520%2528e.g.%252C%2520distinguishing%2520%2522Dr.%2520Smith%2520the%2520cardiologist%2522%2520from%2520%2522Dr.%2520Smith%2520the%2520researcher%2522%2529.%2520These%2520mechanisms%2520are%2520unified%2520by%2520an%2520external%2520Resolution%2520Operator%2520%2524%25CF%2581%2524%2520that%2520makes%2520semantic%2520commitment%2520explicit%252C%2520controllable%252C%2520and%2520task-dependent.%2520Unlike%2520standard%2520architectures%252C%2520NRR%2520separates%2520representation%2520from%2520resolution%252C%2520allowing%2520a%2520single%2520model%2520to%2520shift%2520between%2520creative%252C%2520factual%252C%2520and%2520ambiguity-preserving%2520reasoning%2520without%2520retraining.%2520A%2520synthetic%2520evaluation%2520demonstrates%2520NRR%2527s%2520ability%2520to%2520preserve%2520ambiguity%2520and%2520track%2520context%253A%2520CIT-enhanced%2520models%2520achieve%252090.9%2525%2520accuracy%2520on%2520out-of-distribution%2520identity-shift%2520tasks%252C%2520compared%2520to%25209.1%2525%2520for%2520transformer%2520baselines.%2520NRR%2520provides%2520a%2520principled%2520alternative%2520to%2520premature%2520collapse%252C%2520reframing%2520ambiguity%2520as%2520an%2520explicit%2520representational%2520state%2520rather%2520than%2520a%2520failure%2520mode.%2520The%2520question%2520is%2520not%2520whether%2520AI%2520should%2520resolve%2520ambiguity%252C%2520but%2520when%252C%2520how%252C%2520and%2520under%2520whose%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Resolution%20Reasoning%3A%20A%20Framework%20for%20Preserving%20Semantic%20Ambiguity%20in%20Language%20Models&entry.906535625=Kei%20Saito&entry.1292438233=Premature%20semantic%20collapse%20--%20the%20forced%20early%20commitment%20to%20a%20single%20meaning%20--%20remains%20a%20core%20architectural%20limitation%20of%20current%20language%20models.%20Softmax-driven%20competition%20and%20greedy%20decoding%20cause%20models%20to%20discard%20valid%20interpretations%20before%20sufficient%20context%20is%20available%2C%20resulting%20in%20brittle%20reasoning%20and%20context%20failures.%20We%20introduce%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20a%20general%20computational%20framework%20that%20preserves%20semantic%20ambiguity%20during%20inference%20and%20performs%20resolution%20only%20when%20explicitly%20required.%20NRR%20integrates%20three%20components%3A%20%281%29%20Multi-Vector%20Embeddings%20that%20maintain%20multiple%20viable%20interpretations%20per%20token%2C%20%282%29%20Non-Collapsing%20Attention%20that%20prevents%20winner-take-all%20dynamics%20across%20layers%2C%20and%20%283%29%20Contextual%20Identity%20Tracking%20%28CIT%29%2C%20which%20assigns%20context-specific%20identities%20to%20recurring%20entities%20%28e.g.%2C%20distinguishing%20%22Dr.%20Smith%20the%20cardiologist%22%20from%20%22Dr.%20Smith%20the%20researcher%22%29.%20These%20mechanisms%20are%20unified%20by%20an%20external%20Resolution%20Operator%20%24%CF%81%24%20that%20makes%20semantic%20commitment%20explicit%2C%20controllable%2C%20and%20task-dependent.%20Unlike%20standard%20architectures%2C%20NRR%20separates%20representation%20from%20resolution%2C%20allowing%20a%20single%20model%20to%20shift%20between%20creative%2C%20factual%2C%20and%20ambiguity-preserving%20reasoning%20without%20retraining.%20A%20synthetic%20evaluation%20demonstrates%20NRR%27s%20ability%20to%20preserve%20ambiguity%20and%20track%20context%3A%20CIT-enhanced%20models%20achieve%2090.9%25%20accuracy%20on%20out-of-distribution%20identity-shift%20tasks%2C%20compared%20to%209.1%25%20for%20transformer%20baselines.%20NRR%20provides%20a%20principled%20alternative%20to%20premature%20collapse%2C%20reframing%20ambiguity%20as%20an%20explicit%20representational%20state%20rather%20than%20a%20failure%20mode.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.13478v1&entry.124074799=Read"},
{"title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection", "author": "Junwen Miao and Penghui Du and Yi Liu and Yu Wang and Yan Wang", "abstract": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.", "link": "http://arxiv.org/abs/2512.13671v1", "date": "2025-12-15", "relevancy": 2.1641, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5493}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentIAD%3A%20Tool-Augmented%20Single-Agent%20for%20Industrial%20Anomaly%20Detection&body=Title%3A%20AgentIAD%3A%20Tool-Augmented%20Single-Agent%20for%20Industrial%20Anomaly%20Detection%0AAuthor%3A%20Junwen%20Miao%20and%20Penghui%20Du%20and%20Yi%20Liu%20and%20Yu%20Wang%20and%20Yan%20Wang%0AAbstract%3A%20Industrial%20anomaly%20detection%20%28IAD%29%20is%20difficult%20due%20to%20the%20scarcity%20of%20normal%20reference%20samples%20and%20the%20subtle%2C%20localized%20nature%20of%20many%20defects.%20Single-pass%20vision-language%20models%20%28VLMs%29%20often%20overlook%20small%20abnormalities%20and%20lack%20explicit%20mechanisms%20to%20compare%20against%20canonical%20normal%20patterns.%20We%20propose%20AgentIAD%2C%20a%20tool-driven%20agentic%20framework%20that%20enables%20multi-stage%20visual%20inspection.%20The%20agent%20is%20equipped%20with%20a%20Perceptive%20Zoomer%20%28PZ%29%20for%20localized%20fine-grained%20analysis%20and%20a%20Comparative%20Retriever%20%28CR%29%20for%20querying%20normal%20exemplars%20when%20evidence%20is%20ambiguous.%20To%20teach%20these%20inspection%20behaviors%2C%20we%20construct%20structured%20perceptive%20and%20comparative%20trajectories%20from%20the%20MMAD%20dataset%20and%20train%20the%20model%20in%20two%20stages%3A%20supervised%20fine-tuning%20followed%20by%20reinforcement%20learning.%20A%20two-part%20reward%20design%20drives%20this%20process%3A%20a%20perception%20reward%20that%20supervises%20classification%20accuracy%2C%20spatial%20alignment%2C%20and%20type%20correctness%2C%20and%20a%20behavior%20reward%20that%20encourages%20efficient%20tool%20use.%20Together%2C%20these%20components%20enable%20the%20model%20to%20refine%20its%20judgment%20through%20step-wise%20observation%2C%20zooming%2C%20and%20verification.%20AgentIAD%20achieves%20a%20new%20state-of-the-art%2097.62%25%20classification%20accuracy%20on%20MMAD%2C%20surpassing%20prior%20MLLM-based%20approaches%20while%20producing%20transparent%20and%20interpretable%20inspection%20traces.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentIAD%253A%2520Tool-Augmented%2520Single-Agent%2520for%2520Industrial%2520Anomaly%2520Detection%26entry.906535625%3DJunwen%2520Miao%2520and%2520Penghui%2520Du%2520and%2520Yi%2520Liu%2520and%2520Yu%2520Wang%2520and%2520Yan%2520Wang%26entry.1292438233%3DIndustrial%2520anomaly%2520detection%2520%2528IAD%2529%2520is%2520difficult%2520due%2520to%2520the%2520scarcity%2520of%2520normal%2520reference%2520samples%2520and%2520the%2520subtle%252C%2520localized%2520nature%2520of%2520many%2520defects.%2520Single-pass%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520overlook%2520small%2520abnormalities%2520and%2520lack%2520explicit%2520mechanisms%2520to%2520compare%2520against%2520canonical%2520normal%2520patterns.%2520We%2520propose%2520AgentIAD%252C%2520a%2520tool-driven%2520agentic%2520framework%2520that%2520enables%2520multi-stage%2520visual%2520inspection.%2520The%2520agent%2520is%2520equipped%2520with%2520a%2520Perceptive%2520Zoomer%2520%2528PZ%2529%2520for%2520localized%2520fine-grained%2520analysis%2520and%2520a%2520Comparative%2520Retriever%2520%2528CR%2529%2520for%2520querying%2520normal%2520exemplars%2520when%2520evidence%2520is%2520ambiguous.%2520To%2520teach%2520these%2520inspection%2520behaviors%252C%2520we%2520construct%2520structured%2520perceptive%2520and%2520comparative%2520trajectories%2520from%2520the%2520MMAD%2520dataset%2520and%2520train%2520the%2520model%2520in%2520two%2520stages%253A%2520supervised%2520fine-tuning%2520followed%2520by%2520reinforcement%2520learning.%2520A%2520two-part%2520reward%2520design%2520drives%2520this%2520process%253A%2520a%2520perception%2520reward%2520that%2520supervises%2520classification%2520accuracy%252C%2520spatial%2520alignment%252C%2520and%2520type%2520correctness%252C%2520and%2520a%2520behavior%2520reward%2520that%2520encourages%2520efficient%2520tool%2520use.%2520Together%252C%2520these%2520components%2520enable%2520the%2520model%2520to%2520refine%2520its%2520judgment%2520through%2520step-wise%2520observation%252C%2520zooming%252C%2520and%2520verification.%2520AgentIAD%2520achieves%2520a%2520new%2520state-of-the-art%252097.62%2525%2520classification%2520accuracy%2520on%2520MMAD%252C%2520surpassing%2520prior%2520MLLM-based%2520approaches%2520while%2520producing%2520transparent%2520and%2520interpretable%2520inspection%2520traces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentIAD%3A%20Tool-Augmented%20Single-Agent%20for%20Industrial%20Anomaly%20Detection&entry.906535625=Junwen%20Miao%20and%20Penghui%20Du%20and%20Yi%20Liu%20and%20Yu%20Wang%20and%20Yan%20Wang&entry.1292438233=Industrial%20anomaly%20detection%20%28IAD%29%20is%20difficult%20due%20to%20the%20scarcity%20of%20normal%20reference%20samples%20and%20the%20subtle%2C%20localized%20nature%20of%20many%20defects.%20Single-pass%20vision-language%20models%20%28VLMs%29%20often%20overlook%20small%20abnormalities%20and%20lack%20explicit%20mechanisms%20to%20compare%20against%20canonical%20normal%20patterns.%20We%20propose%20AgentIAD%2C%20a%20tool-driven%20agentic%20framework%20that%20enables%20multi-stage%20visual%20inspection.%20The%20agent%20is%20equipped%20with%20a%20Perceptive%20Zoomer%20%28PZ%29%20for%20localized%20fine-grained%20analysis%20and%20a%20Comparative%20Retriever%20%28CR%29%20for%20querying%20normal%20exemplars%20when%20evidence%20is%20ambiguous.%20To%20teach%20these%20inspection%20behaviors%2C%20we%20construct%20structured%20perceptive%20and%20comparative%20trajectories%20from%20the%20MMAD%20dataset%20and%20train%20the%20model%20in%20two%20stages%3A%20supervised%20fine-tuning%20followed%20by%20reinforcement%20learning.%20A%20two-part%20reward%20design%20drives%20this%20process%3A%20a%20perception%20reward%20that%20supervises%20classification%20accuracy%2C%20spatial%20alignment%2C%20and%20type%20correctness%2C%20and%20a%20behavior%20reward%20that%20encourages%20efficient%20tool%20use.%20Together%2C%20these%20components%20enable%20the%20model%20to%20refine%20its%20judgment%20through%20step-wise%20observation%2C%20zooming%2C%20and%20verification.%20AgentIAD%20achieves%20a%20new%20state-of-the-art%2097.62%25%20classification%20accuracy%20on%20MMAD%2C%20surpassing%20prior%20MLLM-based%20approaches%20while%20producing%20transparent%20and%20interpretable%20inspection%20traces.&entry.1838667208=http%3A//arxiv.org/abs/2512.13671v1&entry.124074799=Read"},
{"title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs", "author": "Minghao LI and Ruihang Wang and Rui Tan and Yonggang Wen", "abstract": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.", "link": "http://arxiv.org/abs/2512.10611v2", "date": "2025-12-15", "relevancy": 2.1495, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5485}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5367}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs&body=Title%3A%20Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs%0AAuthor%3A%20Minghao%20LI%20and%20Ruihang%20Wang%20and%20Rui%20Tan%20and%20Yonggang%20Wen%0AAbstract%3A%20Data%20center%20%28DC%29%20infrastructure%20serves%20as%20the%20backbone%20to%20support%20the%20escalating%20demand%20for%20computing%20capacity.%20Traditional%20design%20methodologies%20that%20blend%20human%20expertise%20with%20specialized%20simulation%20tools%20scale%20poorly%20with%20the%20increasing%20system%20complexity.%20Recent%20studies%20adopt%20generative%20artificial%20intelligence%20to%20design%20plausible%20human-centric%20indoor%20layouts.%20However%2C%20they%20do%20not%20consider%20the%20underlying%20physics%2C%20making%20them%20unsuitable%20for%20the%20DC%20design%20that%20sets%20quantifiable%20operational%20objectives%20and%20strict%20physical%20constraints.%20To%20bridge%20the%20gap%2C%20we%20propose%20Phythesis%2C%20a%20novel%20framework%20that%20synergizes%20large%20language%20models%20%28LLMs%29%20and%20physics-guided%20evolutionary%20optimization%20to%20automate%20simulation-ready%20%28SimReady%29%20scene%20synthesis%20for%20energy-efficient%20DC%20design.%20Phythesis%20employs%20an%20iterative%20bi-level%20optimization%20architecture%2C%20where%20%28i%29%20the%20LLM-driven%20optimization%20level%20generates%20physically%20plausible%20three-dimensional%20layouts%20and%20self-criticizes%20them%20to%20refine%20the%20scene%20topology%2C%20and%20%28ii%29%20the%20physics-informed%20optimization%20level%20identifies%20the%20optimal%20asset%20parameters%20and%20selects%20the%20best%20asset%20combination.%20Experiments%20on%20three%20generation%20scales%20show%20that%20Phythesis%20achieves%2057.3%25%20generation%20success%20rate%20increase%20and%2011.5%25%20power%20usage%20effectiveness%20%28PUE%29%20improvement%2C%20compared%20with%20the%20vanilla%20LLM-based%20solution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhythesis%253A%2520Physics-Guided%2520Evolutionary%2520Scene%2520Synthesis%2520for%2520Energy-Efficient%2520Data%2520Center%2520Design%2520via%2520LLMs%26entry.906535625%3DMinghao%2520LI%2520and%2520Ruihang%2520Wang%2520and%2520Rui%2520Tan%2520and%2520Yonggang%2520Wen%26entry.1292438233%3DData%2520center%2520%2528DC%2529%2520infrastructure%2520serves%2520as%2520the%2520backbone%2520to%2520support%2520the%2520escalating%2520demand%2520for%2520computing%2520capacity.%2520Traditional%2520design%2520methodologies%2520that%2520blend%2520human%2520expertise%2520with%2520specialized%2520simulation%2520tools%2520scale%2520poorly%2520with%2520the%2520increasing%2520system%2520complexity.%2520Recent%2520studies%2520adopt%2520generative%2520artificial%2520intelligence%2520to%2520design%2520plausible%2520human-centric%2520indoor%2520layouts.%2520However%252C%2520they%2520do%2520not%2520consider%2520the%2520underlying%2520physics%252C%2520making%2520them%2520unsuitable%2520for%2520the%2520DC%2520design%2520that%2520sets%2520quantifiable%2520operational%2520objectives%2520and%2520strict%2520physical%2520constraints.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520propose%2520Phythesis%252C%2520a%2520novel%2520framework%2520that%2520synergizes%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520physics-guided%2520evolutionary%2520optimization%2520to%2520automate%2520simulation-ready%2520%2528SimReady%2529%2520scene%2520synthesis%2520for%2520energy-efficient%2520DC%2520design.%2520Phythesis%2520employs%2520an%2520iterative%2520bi-level%2520optimization%2520architecture%252C%2520where%2520%2528i%2529%2520the%2520LLM-driven%2520optimization%2520level%2520generates%2520physically%2520plausible%2520three-dimensional%2520layouts%2520and%2520self-criticizes%2520them%2520to%2520refine%2520the%2520scene%2520topology%252C%2520and%2520%2528ii%2529%2520the%2520physics-informed%2520optimization%2520level%2520identifies%2520the%2520optimal%2520asset%2520parameters%2520and%2520selects%2520the%2520best%2520asset%2520combination.%2520Experiments%2520on%2520three%2520generation%2520scales%2520show%2520that%2520Phythesis%2520achieves%252057.3%2525%2520generation%2520success%2520rate%2520increase%2520and%252011.5%2525%2520power%2520usage%2520effectiveness%2520%2528PUE%2529%2520improvement%252C%2520compared%2520with%2520the%2520vanilla%2520LLM-based%2520solution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs&entry.906535625=Minghao%20LI%20and%20Ruihang%20Wang%20and%20Rui%20Tan%20and%20Yonggang%20Wen&entry.1292438233=Data%20center%20%28DC%29%20infrastructure%20serves%20as%20the%20backbone%20to%20support%20the%20escalating%20demand%20for%20computing%20capacity.%20Traditional%20design%20methodologies%20that%20blend%20human%20expertise%20with%20specialized%20simulation%20tools%20scale%20poorly%20with%20the%20increasing%20system%20complexity.%20Recent%20studies%20adopt%20generative%20artificial%20intelligence%20to%20design%20plausible%20human-centric%20indoor%20layouts.%20However%2C%20they%20do%20not%20consider%20the%20underlying%20physics%2C%20making%20them%20unsuitable%20for%20the%20DC%20design%20that%20sets%20quantifiable%20operational%20objectives%20and%20strict%20physical%20constraints.%20To%20bridge%20the%20gap%2C%20we%20propose%20Phythesis%2C%20a%20novel%20framework%20that%20synergizes%20large%20language%20models%20%28LLMs%29%20and%20physics-guided%20evolutionary%20optimization%20to%20automate%20simulation-ready%20%28SimReady%29%20scene%20synthesis%20for%20energy-efficient%20DC%20design.%20Phythesis%20employs%20an%20iterative%20bi-level%20optimization%20architecture%2C%20where%20%28i%29%20the%20LLM-driven%20optimization%20level%20generates%20physically%20plausible%20three-dimensional%20layouts%20and%20self-criticizes%20them%20to%20refine%20the%20scene%20topology%2C%20and%20%28ii%29%20the%20physics-informed%20optimization%20level%20identifies%20the%20optimal%20asset%20parameters%20and%20selects%20the%20best%20asset%20combination.%20Experiments%20on%20three%20generation%20scales%20show%20that%20Phythesis%20achieves%2057.3%25%20generation%20success%20rate%20increase%20and%2011.5%25%20power%20usage%20effectiveness%20%28PUE%29%20improvement%2C%20compared%20with%20the%20vanilla%20LLM-based%20solution.&entry.1838667208=http%3A//arxiv.org/abs/2512.10611v2&entry.124074799=Read"},
{"title": "Towards a Common Framework for Autoformalization", "author": "Agnieszka Mensfelt and David Tena Cucala and Santiago Franco and Angeliki Koutsoukou-Argyraki and Vince Trencsenyi and Kostas Stathis", "abstract": "Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.", "link": "http://arxiv.org/abs/2509.09810v3", "date": "2025-12-15", "relevancy": 2.1484, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.432}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Common%20Framework%20for%20Autoformalization&body=Title%3A%20Towards%20a%20Common%20Framework%20for%20Autoformalization%0AAuthor%3A%20Agnieszka%20Mensfelt%20and%20David%20Tena%20Cucala%20and%20Santiago%20Franco%20and%20Angeliki%20Koutsoukou-Argyraki%20and%20Vince%20Trencsenyi%20and%20Kostas%20Stathis%0AAbstract%3A%20Autoformalization%20has%20emerged%20as%20a%20term%20referring%20to%20the%20automation%20of%20formalization%20-%20specifically%2C%20the%20formalization%20of%20mathematics%20using%20interactive%20theorem%20provers%20%28proof%20assistants%29.%20Its%20rapid%20development%20has%20been%20driven%20by%20progress%20in%20deep%20learning%2C%20especially%20large%20language%20models%20%28LLMs%29.%20More%20recently%2C%20the%20term%20has%20expanded%20beyond%20mathematics%20to%20describe%20the%20broader%20task%20of%20translating%20informal%20input%20into%20formal%20logical%20representations.%20At%20the%20same%20time%2C%20a%20growing%20body%20of%20research%20explores%20using%20LLMs%20to%20translate%20informal%20language%20into%20formal%20representations%20for%20reasoning%2C%20planning%2C%20and%20knowledge%20representation%20-%20often%20without%20explicitly%20referring%20to%20this%20process%20as%20autoformalization.%20As%20a%20result%2C%20despite%20addressing%20similar%20tasks%2C%20the%20largely%20independent%20development%20of%20these%20research%20areas%20has%20limited%20opportunities%20for%20shared%20methodologies%2C%20benchmarks%2C%20and%20theoretical%20frameworks%20that%20could%20accelerate%20progress.%20The%20goal%20of%20this%20paper%20is%20to%20review%20-%20explicit%20or%20implicit%20-%20instances%20of%20what%20can%20be%20considered%20autoformalization%20and%20to%20propose%20a%20unified%20framework%2C%20encouraging%20cross-pollination%20between%20different%20fields%20to%20advance%20the%20development%20of%20next%20generation%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.09810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Common%2520Framework%2520for%2520Autoformalization%26entry.906535625%3DAgnieszka%2520Mensfelt%2520and%2520David%2520Tena%2520Cucala%2520and%2520Santiago%2520Franco%2520and%2520Angeliki%2520Koutsoukou-Argyraki%2520and%2520Vince%2520Trencsenyi%2520and%2520Kostas%2520Stathis%26entry.1292438233%3DAutoformalization%2520has%2520emerged%2520as%2520a%2520term%2520referring%2520to%2520the%2520automation%2520of%2520formalization%2520-%2520specifically%252C%2520the%2520formalization%2520of%2520mathematics%2520using%2520interactive%2520theorem%2520provers%2520%2528proof%2520assistants%2529.%2520Its%2520rapid%2520development%2520has%2520been%2520driven%2520by%2520progress%2520in%2520deep%2520learning%252C%2520especially%2520large%2520language%2520models%2520%2528LLMs%2529.%2520More%2520recently%252C%2520the%2520term%2520has%2520expanded%2520beyond%2520mathematics%2520to%2520describe%2520the%2520broader%2520task%2520of%2520translating%2520informal%2520input%2520into%2520formal%2520logical%2520representations.%2520At%2520the%2520same%2520time%252C%2520a%2520growing%2520body%2520of%2520research%2520explores%2520using%2520LLMs%2520to%2520translate%2520informal%2520language%2520into%2520formal%2520representations%2520for%2520reasoning%252C%2520planning%252C%2520and%2520knowledge%2520representation%2520-%2520often%2520without%2520explicitly%2520referring%2520to%2520this%2520process%2520as%2520autoformalization.%2520As%2520a%2520result%252C%2520despite%2520addressing%2520similar%2520tasks%252C%2520the%2520largely%2520independent%2520development%2520of%2520these%2520research%2520areas%2520has%2520limited%2520opportunities%2520for%2520shared%2520methodologies%252C%2520benchmarks%252C%2520and%2520theoretical%2520frameworks%2520that%2520could%2520accelerate%2520progress.%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520review%2520-%2520explicit%2520or%2520implicit%2520-%2520instances%2520of%2520what%2520can%2520be%2520considered%2520autoformalization%2520and%2520to%2520propose%2520a%2520unified%2520framework%252C%2520encouraging%2520cross-pollination%2520between%2520different%2520fields%2520to%2520advance%2520the%2520development%2520of%2520next%2520generation%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Common%20Framework%20for%20Autoformalization&entry.906535625=Agnieszka%20Mensfelt%20and%20David%20Tena%20Cucala%20and%20Santiago%20Franco%20and%20Angeliki%20Koutsoukou-Argyraki%20and%20Vince%20Trencsenyi%20and%20Kostas%20Stathis&entry.1292438233=Autoformalization%20has%20emerged%20as%20a%20term%20referring%20to%20the%20automation%20of%20formalization%20-%20specifically%2C%20the%20formalization%20of%20mathematics%20using%20interactive%20theorem%20provers%20%28proof%20assistants%29.%20Its%20rapid%20development%20has%20been%20driven%20by%20progress%20in%20deep%20learning%2C%20especially%20large%20language%20models%20%28LLMs%29.%20More%20recently%2C%20the%20term%20has%20expanded%20beyond%20mathematics%20to%20describe%20the%20broader%20task%20of%20translating%20informal%20input%20into%20formal%20logical%20representations.%20At%20the%20same%20time%2C%20a%20growing%20body%20of%20research%20explores%20using%20LLMs%20to%20translate%20informal%20language%20into%20formal%20representations%20for%20reasoning%2C%20planning%2C%20and%20knowledge%20representation%20-%20often%20without%20explicitly%20referring%20to%20this%20process%20as%20autoformalization.%20As%20a%20result%2C%20despite%20addressing%20similar%20tasks%2C%20the%20largely%20independent%20development%20of%20these%20research%20areas%20has%20limited%20opportunities%20for%20shared%20methodologies%2C%20benchmarks%2C%20and%20theoretical%20frameworks%20that%20could%20accelerate%20progress.%20The%20goal%20of%20this%20paper%20is%20to%20review%20-%20explicit%20or%20implicit%20-%20instances%20of%20what%20can%20be%20considered%20autoformalization%20and%20to%20propose%20a%20unified%20framework%2C%20encouraging%20cross-pollination%20between%20different%20fields%20to%20advance%20the%20development%20of%20next%20generation%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.09810v3&entry.124074799=Read"},
{"title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images", "author": "Bo Liu and Qiao Qin and Qinghui He", "abstract": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.", "link": "http://arxiv.org/abs/2512.13285v1", "date": "2025-12-15", "relevancy": 2.1369, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5442}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5293}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalCLIP%3A%20Causally-Informed%20Feature%20Disentanglement%20and%20Filtering%20for%20Generalizable%20Detection%20of%20Generated%20Images&body=Title%3A%20CausalCLIP%3A%20Causally-Informed%20Feature%20Disentanglement%20and%20Filtering%20for%20Generalizable%20Detection%20of%20Generated%20Images%0AAuthor%3A%20Bo%20Liu%20and%20Qiao%20Qin%20and%20Qinghui%20He%0AAbstract%3A%20The%20rapid%20advancement%20of%20generative%20models%20has%20increased%20the%20demand%20for%20generated%20image%20detectors%20capable%20of%20generalizing%20across%20diverse%20and%20evolving%20generation%20techniques.%20However%2C%20existing%20methods%2C%20including%20those%20leveraging%20pre-trained%20vision-language%20models%2C%20often%20produce%20highly%20entangled%20representations%2C%20mixing%20task-relevant%20forensic%20cues%20%28causal%20features%29%20with%20spurious%20or%20irrelevant%20patterns%20%28non-causal%20features%29%2C%20thus%20limiting%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20CausalCLIP%2C%20a%20framework%20that%20explicitly%20disentangles%20causal%20from%20non-causal%20features%20and%20employs%20targeted%20filtering%20guided%20by%20causal%20inference%20principles%20to%20retain%20only%20the%20most%20transferable%20and%20discriminative%20forensic%20cues.%20By%20modeling%20the%20generation%20process%20with%20a%20structural%20causal%20model%20and%20enforcing%20statistical%20independence%20through%20Gumbel-Softmax-based%20feature%20masking%20and%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29%20constraints%2C%20CausalCLIP%20isolates%20stable%20causal%20features%20robust%20to%20distribution%20shifts.%20When%20tested%20on%20unseen%20generative%20models%20from%20different%20series%2C%20CausalCLIP%20demonstrates%20strong%20generalization%20ability%2C%20achieving%20improvements%20of%206.83%25%20in%20accuracy%20and%204.06%25%20in%20average%20precision%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalCLIP%253A%2520Causally-Informed%2520Feature%2520Disentanglement%2520and%2520Filtering%2520for%2520Generalizable%2520Detection%2520of%2520Generated%2520Images%26entry.906535625%3DBo%2520Liu%2520and%2520Qiao%2520Qin%2520and%2520Qinghui%2520He%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520generative%2520models%2520has%2520increased%2520the%2520demand%2520for%2520generated%2520image%2520detectors%2520capable%2520of%2520generalizing%2520across%2520diverse%2520and%2520evolving%2520generation%2520techniques.%2520However%252C%2520existing%2520methods%252C%2520including%2520those%2520leveraging%2520pre-trained%2520vision-language%2520models%252C%2520often%2520produce%2520highly%2520entangled%2520representations%252C%2520mixing%2520task-relevant%2520forensic%2520cues%2520%2528causal%2520features%2529%2520with%2520spurious%2520or%2520irrelevant%2520patterns%2520%2528non-causal%2520features%2529%252C%2520thus%2520limiting%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520CausalCLIP%252C%2520a%2520framework%2520that%2520explicitly%2520disentangles%2520causal%2520from%2520non-causal%2520features%2520and%2520employs%2520targeted%2520filtering%2520guided%2520by%2520causal%2520inference%2520principles%2520to%2520retain%2520only%2520the%2520most%2520transferable%2520and%2520discriminative%2520forensic%2520cues.%2520By%2520modeling%2520the%2520generation%2520process%2520with%2520a%2520structural%2520causal%2520model%2520and%2520enforcing%2520statistical%2520independence%2520through%2520Gumbel-Softmax-based%2520feature%2520masking%2520and%2520Hilbert-Schmidt%2520Independence%2520Criterion%2520%2528HSIC%2529%2520constraints%252C%2520CausalCLIP%2520isolates%2520stable%2520causal%2520features%2520robust%2520to%2520distribution%2520shifts.%2520When%2520tested%2520on%2520unseen%2520generative%2520models%2520from%2520different%2520series%252C%2520CausalCLIP%2520demonstrates%2520strong%2520generalization%2520ability%252C%2520achieving%2520improvements%2520of%25206.83%2525%2520in%2520accuracy%2520and%25204.06%2525%2520in%2520average%2520precision%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalCLIP%3A%20Causally-Informed%20Feature%20Disentanglement%20and%20Filtering%20for%20Generalizable%20Detection%20of%20Generated%20Images&entry.906535625=Bo%20Liu%20and%20Qiao%20Qin%20and%20Qinghui%20He&entry.1292438233=The%20rapid%20advancement%20of%20generative%20models%20has%20increased%20the%20demand%20for%20generated%20image%20detectors%20capable%20of%20generalizing%20across%20diverse%20and%20evolving%20generation%20techniques.%20However%2C%20existing%20methods%2C%20including%20those%20leveraging%20pre-trained%20vision-language%20models%2C%20often%20produce%20highly%20entangled%20representations%2C%20mixing%20task-relevant%20forensic%20cues%20%28causal%20features%29%20with%20spurious%20or%20irrelevant%20patterns%20%28non-causal%20features%29%2C%20thus%20limiting%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20CausalCLIP%2C%20a%20framework%20that%20explicitly%20disentangles%20causal%20from%20non-causal%20features%20and%20employs%20targeted%20filtering%20guided%20by%20causal%20inference%20principles%20to%20retain%20only%20the%20most%20transferable%20and%20discriminative%20forensic%20cues.%20By%20modeling%20the%20generation%20process%20with%20a%20structural%20causal%20model%20and%20enforcing%20statistical%20independence%20through%20Gumbel-Softmax-based%20feature%20masking%20and%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29%20constraints%2C%20CausalCLIP%20isolates%20stable%20causal%20features%20robust%20to%20distribution%20shifts.%20When%20tested%20on%20unseen%20generative%20models%20from%20different%20series%2C%20CausalCLIP%20demonstrates%20strong%20generalization%20ability%2C%20achieving%20improvements%20of%206.83%25%20in%20accuracy%20and%204.06%25%20in%20average%20precision%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.13285v1&entry.124074799=Read"},
{"title": "DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides", "author": "Haoyue Zhang and Meera Chappidi and Erolcan Sayar and Helen Richards and Zhijun Chen and Lucas Liu and Roxanne Wadia and Peter A Humphrey and Fady Ghali and Alberto Contreras-Sanz and Peter Black and Jonathan Wright and Stephanie Harmon and Michael Haffner", "abstract": "Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.", "link": "http://arxiv.org/abs/2512.13600v1", "date": "2025-12-15", "relevancy": 2.1298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-SSL%3A%20self-supervised%20domain%20adaptor%20to%20leverage%20foundational%20models%20in%20turbt%20histopathology%20slides&body=Title%3A%20DA-SSL%3A%20self-supervised%20domain%20adaptor%20to%20leverage%20foundational%20models%20in%20turbt%20histopathology%20slides%0AAuthor%3A%20Haoyue%20Zhang%20and%20Meera%20Chappidi%20and%20Erolcan%20Sayar%20and%20Helen%20Richards%20and%20Zhijun%20Chen%20and%20Lucas%20Liu%20and%20Roxanne%20Wadia%20and%20Peter%20A%20Humphrey%20and%20Fady%20Ghali%20and%20Alberto%20Contreras-Sanz%20and%20Peter%20Black%20and%20Jonathan%20Wright%20and%20Stephanie%20Harmon%20and%20Michael%20Haffner%0AAbstract%3A%20Recent%20deep%20learning%20frameworks%20in%20histopathology%2C%20particularly%20multiple%20instance%20learning%20%28MIL%29%20combined%20with%20pathology%20foundational%20models%20%28PFMs%29%2C%20have%20shown%20strong%20performance.%20However%2C%20PFMs%20exhibit%20limitations%20on%20certain%20cancer%20or%20specimen%20types%20due%20to%20domain%20shifts%20-%20these%20cancer%20types%20were%20rarely%20used%20for%20pretraining%20or%20specimens%20contain%20tissue-based%20artifacts%20rarely%20seen%20within%20the%20pretraining%20population.%20Such%20is%20the%20case%20for%20transurethral%20resection%20of%20bladder%20tumor%20%28TURBT%29%2C%20which%20are%20essential%20for%20diagnosing%20muscle-invasive%20bladder%20cancer%20%28MIBC%29%2C%20but%20contain%20fragmented%20tissue%20chips%20and%20electrocautery%20artifacts%20and%20were%20not%20widely%20used%20in%20publicly%20available%20PFMs.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%20domain-adaptive%20self-supervised%20adaptor%20%28DA-SSL%29%20that%20realigns%20pretrained%20PFM%20features%20to%20the%20TURBT%20domain%20without%20fine-tuning%20the%20foundational%20model%20itself.%20We%20pilot%20this%20framework%20for%20predicting%20treatment%20response%20in%20TURBT%2C%20where%20histomorphological%20features%20are%20currently%20underutilized%20and%20identifying%20patients%20who%20will%20benefit%20from%20neoadjuvant%20chemotherapy%20%28NAC%29%20is%20challenging.%20In%20our%20multi-center%20study%2C%20DA-SSL%20achieved%20an%20AUC%20of%200.77%2B/-0.04%20in%20five-fold%20cross-validation%20and%20an%20external%20test%20accuracy%20of%200.84%2C%20sensitivity%20of%200.71%2C%20and%20specificity%20of%200.91%20using%20majority%20voting.%20Our%20results%20demonstrate%20that%20lightweight%20domain%20adaptation%20with%20self-supervision%20can%20effectively%20enhance%20PFM-based%20MIL%20pipelines%20for%20clinically%20challenging%20histopathology%20tasks.%20Code%20is%20Available%20at%20https%3A//github.com/zhanghaoyue/DA_SSL_TURBT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-SSL%253A%2520self-supervised%2520domain%2520adaptor%2520to%2520leverage%2520foundational%2520models%2520in%2520turbt%2520histopathology%2520slides%26entry.906535625%3DHaoyue%2520Zhang%2520and%2520Meera%2520Chappidi%2520and%2520Erolcan%2520Sayar%2520and%2520Helen%2520Richards%2520and%2520Zhijun%2520Chen%2520and%2520Lucas%2520Liu%2520and%2520Roxanne%2520Wadia%2520and%2520Peter%2520A%2520Humphrey%2520and%2520Fady%2520Ghali%2520and%2520Alberto%2520Contreras-Sanz%2520and%2520Peter%2520Black%2520and%2520Jonathan%2520Wright%2520and%2520Stephanie%2520Harmon%2520and%2520Michael%2520Haffner%26entry.1292438233%3DRecent%2520deep%2520learning%2520frameworks%2520in%2520histopathology%252C%2520particularly%2520multiple%2520instance%2520learning%2520%2528MIL%2529%2520combined%2520with%2520pathology%2520foundational%2520models%2520%2528PFMs%2529%252C%2520have%2520shown%2520strong%2520performance.%2520However%252C%2520PFMs%2520exhibit%2520limitations%2520on%2520certain%2520cancer%2520or%2520specimen%2520types%2520due%2520to%2520domain%2520shifts%2520-%2520these%2520cancer%2520types%2520were%2520rarely%2520used%2520for%2520pretraining%2520or%2520specimens%2520contain%2520tissue-based%2520artifacts%2520rarely%2520seen%2520within%2520the%2520pretraining%2520population.%2520Such%2520is%2520the%2520case%2520for%2520transurethral%2520resection%2520of%2520bladder%2520tumor%2520%2528TURBT%2529%252C%2520which%2520are%2520essential%2520for%2520diagnosing%2520muscle-invasive%2520bladder%2520cancer%2520%2528MIBC%2529%252C%2520but%2520contain%2520fragmented%2520tissue%2520chips%2520and%2520electrocautery%2520artifacts%2520and%2520were%2520not%2520widely%2520used%2520in%2520publicly%2520available%2520PFMs.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520domain-adaptive%2520self-supervised%2520adaptor%2520%2528DA-SSL%2529%2520that%2520realigns%2520pretrained%2520PFM%2520features%2520to%2520the%2520TURBT%2520domain%2520without%2520fine-tuning%2520the%2520foundational%2520model%2520itself.%2520We%2520pilot%2520this%2520framework%2520for%2520predicting%2520treatment%2520response%2520in%2520TURBT%252C%2520where%2520histomorphological%2520features%2520are%2520currently%2520underutilized%2520and%2520identifying%2520patients%2520who%2520will%2520benefit%2520from%2520neoadjuvant%2520chemotherapy%2520%2528NAC%2529%2520is%2520challenging.%2520In%2520our%2520multi-center%2520study%252C%2520DA-SSL%2520achieved%2520an%2520AUC%2520of%25200.77%252B/-0.04%2520in%2520five-fold%2520cross-validation%2520and%2520an%2520external%2520test%2520accuracy%2520of%25200.84%252C%2520sensitivity%2520of%25200.71%252C%2520and%2520specificity%2520of%25200.91%2520using%2520majority%2520voting.%2520Our%2520results%2520demonstrate%2520that%2520lightweight%2520domain%2520adaptation%2520with%2520self-supervision%2520can%2520effectively%2520enhance%2520PFM-based%2520MIL%2520pipelines%2520for%2520clinically%2520challenging%2520histopathology%2520tasks.%2520Code%2520is%2520Available%2520at%2520https%253A//github.com/zhanghaoyue/DA_SSL_TURBT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-SSL%3A%20self-supervised%20domain%20adaptor%20to%20leverage%20foundational%20models%20in%20turbt%20histopathology%20slides&entry.906535625=Haoyue%20Zhang%20and%20Meera%20Chappidi%20and%20Erolcan%20Sayar%20and%20Helen%20Richards%20and%20Zhijun%20Chen%20and%20Lucas%20Liu%20and%20Roxanne%20Wadia%20and%20Peter%20A%20Humphrey%20and%20Fady%20Ghali%20and%20Alberto%20Contreras-Sanz%20and%20Peter%20Black%20and%20Jonathan%20Wright%20and%20Stephanie%20Harmon%20and%20Michael%20Haffner&entry.1292438233=Recent%20deep%20learning%20frameworks%20in%20histopathology%2C%20particularly%20multiple%20instance%20learning%20%28MIL%29%20combined%20with%20pathology%20foundational%20models%20%28PFMs%29%2C%20have%20shown%20strong%20performance.%20However%2C%20PFMs%20exhibit%20limitations%20on%20certain%20cancer%20or%20specimen%20types%20due%20to%20domain%20shifts%20-%20these%20cancer%20types%20were%20rarely%20used%20for%20pretraining%20or%20specimens%20contain%20tissue-based%20artifacts%20rarely%20seen%20within%20the%20pretraining%20population.%20Such%20is%20the%20case%20for%20transurethral%20resection%20of%20bladder%20tumor%20%28TURBT%29%2C%20which%20are%20essential%20for%20diagnosing%20muscle-invasive%20bladder%20cancer%20%28MIBC%29%2C%20but%20contain%20fragmented%20tissue%20chips%20and%20electrocautery%20artifacts%20and%20were%20not%20widely%20used%20in%20publicly%20available%20PFMs.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%20domain-adaptive%20self-supervised%20adaptor%20%28DA-SSL%29%20that%20realigns%20pretrained%20PFM%20features%20to%20the%20TURBT%20domain%20without%20fine-tuning%20the%20foundational%20model%20itself.%20We%20pilot%20this%20framework%20for%20predicting%20treatment%20response%20in%20TURBT%2C%20where%20histomorphological%20features%20are%20currently%20underutilized%20and%20identifying%20patients%20who%20will%20benefit%20from%20neoadjuvant%20chemotherapy%20%28NAC%29%20is%20challenging.%20In%20our%20multi-center%20study%2C%20DA-SSL%20achieved%20an%20AUC%20of%200.77%2B/-0.04%20in%20five-fold%20cross-validation%20and%20an%20external%20test%20accuracy%20of%200.84%2C%20sensitivity%20of%200.71%2C%20and%20specificity%20of%200.91%20using%20majority%20voting.%20Our%20results%20demonstrate%20that%20lightweight%20domain%20adaptation%20with%20self-supervision%20can%20effectively%20enhance%20PFM-based%20MIL%20pipelines%20for%20clinically%20challenging%20histopathology%20tasks.%20Code%20is%20Available%20at%20https%3A//github.com/zhanghaoyue/DA_SSL_TURBT.&entry.1838667208=http%3A//arxiv.org/abs/2512.13600v1&entry.124074799=Read"},
{"title": "LitePT: Lighter Yet Stronger Point Transformer", "author": "Yuanwen Yue and Damien Robert and Jianyuan Wang and Sunghwan Hong and Jan Dirk Wegner and Christian Rupprecht and Konrad Schindler", "abstract": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.", "link": "http://arxiv.org/abs/2512.13689v1", "date": "2025-12-15", "relevancy": 2.1239, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5287}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LitePT%3A%20Lighter%20Yet%20Stronger%20Point%20Transformer&body=Title%3A%20LitePT%3A%20Lighter%20Yet%20Stronger%20Point%20Transformer%0AAuthor%3A%20Yuanwen%20Yue%20and%20Damien%20Robert%20and%20Jianyuan%20Wang%20and%20Sunghwan%20Hong%20and%20Jan%20Dirk%20Wegner%20and%20Christian%20Rupprecht%20and%20Konrad%20Schindler%0AAbstract%3A%20Modern%20neural%20architectures%20for%203D%20point%20cloud%20processing%20contain%20both%20convolutional%20layers%20and%20attention%20blocks%2C%20but%20the%20best%20way%20to%20assemble%20them%20remains%20unclear.%20We%20analyse%20the%20role%20of%20different%20computational%20blocks%20in%203D%20point%20cloud%20networks%20and%20find%20an%20intuitive%20behaviour%3A%20convolution%20is%20adequate%20to%20extract%20low-level%20geometry%20at%20high-resolution%20in%20early%20layers%2C%20where%20attention%20is%20expensive%20without%20bringing%20any%20benefits%3B%20attention%20captures%20high-level%20semantics%20and%20context%20in%20low-resolution%2C%20deep%20layers%20more%20efficiently.%20Guided%20by%20this%20design%20principle%2C%20we%20propose%20a%20new%2C%20improved%203D%20point%20cloud%20backbone%20that%20employs%20convolutions%20in%20early%20stages%20and%20switches%20to%20attention%20for%20deeper%20layers.%20To%20avoid%20the%20loss%20of%20spatial%20layout%20information%20when%20discarding%20redundant%20convolution%20layers%2C%20we%20introduce%20a%20novel%2C%20training-free%203D%20positional%20encoding%2C%20PointROPE.%20The%20resulting%20LitePT%20model%20has%20%243.6%5Ctimes%24%20fewer%20parameters%2C%20runs%20%242%5Ctimes%24%20faster%2C%20and%20uses%20%242%5Ctimes%24%20less%20memory%20than%20the%20state-of-the-art%20Point%20Transformer%20V3%2C%20but%20nonetheless%20matches%20or%20even%20outperforms%20it%20on%20a%20range%20of%20tasks%20and%20datasets.%20Code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/prs-eth/LitePT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLitePT%253A%2520Lighter%2520Yet%2520Stronger%2520Point%2520Transformer%26entry.906535625%3DYuanwen%2520Yue%2520and%2520Damien%2520Robert%2520and%2520Jianyuan%2520Wang%2520and%2520Sunghwan%2520Hong%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Christian%2520Rupprecht%2520and%2520Konrad%2520Schindler%26entry.1292438233%3DModern%2520neural%2520architectures%2520for%25203D%2520point%2520cloud%2520processing%2520contain%2520both%2520convolutional%2520layers%2520and%2520attention%2520blocks%252C%2520but%2520the%2520best%2520way%2520to%2520assemble%2520them%2520remains%2520unclear.%2520We%2520analyse%2520the%2520role%2520of%2520different%2520computational%2520blocks%2520in%25203D%2520point%2520cloud%2520networks%2520and%2520find%2520an%2520intuitive%2520behaviour%253A%2520convolution%2520is%2520adequate%2520to%2520extract%2520low-level%2520geometry%2520at%2520high-resolution%2520in%2520early%2520layers%252C%2520where%2520attention%2520is%2520expensive%2520without%2520bringing%2520any%2520benefits%253B%2520attention%2520captures%2520high-level%2520semantics%2520and%2520context%2520in%2520low-resolution%252C%2520deep%2520layers%2520more%2520efficiently.%2520Guided%2520by%2520this%2520design%2520principle%252C%2520we%2520propose%2520a%2520new%252C%2520improved%25203D%2520point%2520cloud%2520backbone%2520that%2520employs%2520convolutions%2520in%2520early%2520stages%2520and%2520switches%2520to%2520attention%2520for%2520deeper%2520layers.%2520To%2520avoid%2520the%2520loss%2520of%2520spatial%2520layout%2520information%2520when%2520discarding%2520redundant%2520convolution%2520layers%252C%2520we%2520introduce%2520a%2520novel%252C%2520training-free%25203D%2520positional%2520encoding%252C%2520PointROPE.%2520The%2520resulting%2520LitePT%2520model%2520has%2520%25243.6%255Ctimes%2524%2520fewer%2520parameters%252C%2520runs%2520%25242%255Ctimes%2524%2520faster%252C%2520and%2520uses%2520%25242%255Ctimes%2524%2520less%2520memory%2520than%2520the%2520state-of-the-art%2520Point%2520Transformer%2520V3%252C%2520but%2520nonetheless%2520matches%2520or%2520even%2520outperforms%2520it%2520on%2520a%2520range%2520of%2520tasks%2520and%2520datasets.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/prs-eth/LitePT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LitePT%3A%20Lighter%20Yet%20Stronger%20Point%20Transformer&entry.906535625=Yuanwen%20Yue%20and%20Damien%20Robert%20and%20Jianyuan%20Wang%20and%20Sunghwan%20Hong%20and%20Jan%20Dirk%20Wegner%20and%20Christian%20Rupprecht%20and%20Konrad%20Schindler&entry.1292438233=Modern%20neural%20architectures%20for%203D%20point%20cloud%20processing%20contain%20both%20convolutional%20layers%20and%20attention%20blocks%2C%20but%20the%20best%20way%20to%20assemble%20them%20remains%20unclear.%20We%20analyse%20the%20role%20of%20different%20computational%20blocks%20in%203D%20point%20cloud%20networks%20and%20find%20an%20intuitive%20behaviour%3A%20convolution%20is%20adequate%20to%20extract%20low-level%20geometry%20at%20high-resolution%20in%20early%20layers%2C%20where%20attention%20is%20expensive%20without%20bringing%20any%20benefits%3B%20attention%20captures%20high-level%20semantics%20and%20context%20in%20low-resolution%2C%20deep%20layers%20more%20efficiently.%20Guided%20by%20this%20design%20principle%2C%20we%20propose%20a%20new%2C%20improved%203D%20point%20cloud%20backbone%20that%20employs%20convolutions%20in%20early%20stages%20and%20switches%20to%20attention%20for%20deeper%20layers.%20To%20avoid%20the%20loss%20of%20spatial%20layout%20information%20when%20discarding%20redundant%20convolution%20layers%2C%20we%20introduce%20a%20novel%2C%20training-free%203D%20positional%20encoding%2C%20PointROPE.%20The%20resulting%20LitePT%20model%20has%20%243.6%5Ctimes%24%20fewer%20parameters%2C%20runs%20%242%5Ctimes%24%20faster%2C%20and%20uses%20%242%5Ctimes%24%20less%20memory%20than%20the%20state-of-the-art%20Point%20Transformer%20V3%2C%20but%20nonetheless%20matches%20or%20even%20outperforms%20it%20on%20a%20range%20of%20tasks%20and%20datasets.%20Code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/prs-eth/LitePT.&entry.1838667208=http%3A//arxiv.org/abs/2512.13689v1&entry.124074799=Read"},
{"title": "WakeupUrban: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imagery", "author": "Tianxiang Hao and Lixian Zhang and Yingjia Zhang and Mengxuan Chen and Jinxiao Zhang and Runmin Dong and Haohuan Fu", "abstract": "Historical satellite imagery archive, such as Keyhole satellite data, offers rare insights into understanding early urban development and long-term transformation. However, severe quality degradation ($\\textit{e.g.}$, distortion, misalignment, and spectral scarcity) and the absence of annotations have long hindered its analysis. To bridge this gap and enhance understanding of urban development, we introduce $\\textbf{WakeupUrbanBench}$, an annotated segmentation dataset based on historical satellite imagery with the earliest observation time among all existing remote sensing (RS) datasets, along with a framework for unsupervised segmentation tasks, $\\textbf{WakeupUSM}$. First, WakeupUrbanBench serves as a pioneer, expertly annotated dataset built on mid-$20^{\\text{th}}$ century RS imagery, involving four key urban classes and spanning 4 cities across 2 continents with nearly 1000 km$^2$ area of diverse urban morphologies, and additionally introducing one present-day city. Second, WakeupUSM is a novel unsupervised semantic segmentation framework for historical RS imagery. It employs a confidence-aware alignment mechanism and focal-confidence loss based on a self-supervised learning architecture, which generates robust pseudo-labels and adaptively prioritizes prediction difficulty and label reliability to improve unsupervised segmentation on noisy historical data without manual supervision. Comprehensive experiments demonstrate WakeupUSM significantly outperforms existing unsupervised segmentation methods $\\textbf{both WakeupUrbanBench and public dataset}$, promising to pave the way for quantitative studies of long-term urban change using modern computer vision. Our benchmark and codes will be released at https://github.com/Tianxiang-Hao/WakeupUrban.", "link": "http://arxiv.org/abs/2506.09476v3", "date": "2025-12-15", "relevancy": 2.1188, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5603}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WakeupUrban%3A%20Unsupervised%20Semantic%20Segmentation%20of%20Mid-20%24%5E%7Bth%7D%24%20century%20Urban%20Landscapes%20with%20Satellite%20Imagery&body=Title%3A%20WakeupUrban%3A%20Unsupervised%20Semantic%20Segmentation%20of%20Mid-20%24%5E%7Bth%7D%24%20century%20Urban%20Landscapes%20with%20Satellite%20Imagery%0AAuthor%3A%20Tianxiang%20Hao%20and%20Lixian%20Zhang%20and%20Yingjia%20Zhang%20and%20Mengxuan%20Chen%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu%0AAbstract%3A%20Historical%20satellite%20imagery%20archive%2C%20such%20as%20Keyhole%20satellite%20data%2C%20offers%20rare%20insights%20into%20understanding%20early%20urban%20development%20and%20long-term%20transformation.%20However%2C%20severe%20quality%20degradation%20%28%24%5Ctextit%7Be.g.%7D%24%2C%20distortion%2C%20misalignment%2C%20and%20spectral%20scarcity%29%20and%20the%20absence%20of%20annotations%20have%20long%20hindered%20its%20analysis.%20To%20bridge%20this%20gap%20and%20enhance%20understanding%20of%20urban%20development%2C%20we%20introduce%20%24%5Ctextbf%7BWakeupUrbanBench%7D%24%2C%20an%20annotated%20segmentation%20dataset%20based%20on%20historical%20satellite%20imagery%20with%20the%20earliest%20observation%20time%20among%20all%20existing%20remote%20sensing%20%28RS%29%20datasets%2C%20along%20with%20a%20framework%20for%20unsupervised%20segmentation%20tasks%2C%20%24%5Ctextbf%7BWakeupUSM%7D%24.%20First%2C%20WakeupUrbanBench%20serves%20as%20a%20pioneer%2C%20expertly%20annotated%20dataset%20built%20on%20mid-%2420%5E%7B%5Ctext%7Bth%7D%7D%24%20century%20RS%20imagery%2C%20involving%20four%20key%20urban%20classes%20and%20spanning%204%20cities%20across%202%20continents%20with%20nearly%201000%20km%24%5E2%24%20area%20of%20diverse%20urban%20morphologies%2C%20and%20additionally%20introducing%20one%20present-day%20city.%20Second%2C%20WakeupUSM%20is%20a%20novel%20unsupervised%20semantic%20segmentation%20framework%20for%20historical%20RS%20imagery.%20It%20employs%20a%20confidence-aware%20alignment%20mechanism%20and%20focal-confidence%20loss%20based%20on%20a%20self-supervised%20learning%20architecture%2C%20which%20generates%20robust%20pseudo-labels%20and%20adaptively%20prioritizes%20prediction%20difficulty%20and%20label%20reliability%20to%20improve%20unsupervised%20segmentation%20on%20noisy%20historical%20data%20without%20manual%20supervision.%20Comprehensive%20experiments%20demonstrate%20WakeupUSM%20significantly%20outperforms%20existing%20unsupervised%20segmentation%20methods%20%24%5Ctextbf%7Bboth%20WakeupUrbanBench%20and%20public%20dataset%7D%24%2C%20promising%20to%20pave%20the%20way%20for%20quantitative%20studies%20of%20long-term%20urban%20change%20using%20modern%20computer%20vision.%20Our%20benchmark%20and%20codes%20will%20be%20released%20at%20https%3A//github.com/Tianxiang-Hao/WakeupUrban.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09476v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWakeupUrban%253A%2520Unsupervised%2520Semantic%2520Segmentation%2520of%2520Mid-20%2524%255E%257Bth%257D%2524%2520century%2520Urban%2520Landscapes%2520with%2520Satellite%2520Imagery%26entry.906535625%3DTianxiang%2520Hao%2520and%2520Lixian%2520Zhang%2520and%2520Yingjia%2520Zhang%2520and%2520Mengxuan%2520Chen%2520and%2520Jinxiao%2520Zhang%2520and%2520Runmin%2520Dong%2520and%2520Haohuan%2520Fu%26entry.1292438233%3DHistorical%2520satellite%2520imagery%2520archive%252C%2520such%2520as%2520Keyhole%2520satellite%2520data%252C%2520offers%2520rare%2520insights%2520into%2520understanding%2520early%2520urban%2520development%2520and%2520long-term%2520transformation.%2520However%252C%2520severe%2520quality%2520degradation%2520%2528%2524%255Ctextit%257Be.g.%257D%2524%252C%2520distortion%252C%2520misalignment%252C%2520and%2520spectral%2520scarcity%2529%2520and%2520the%2520absence%2520of%2520annotations%2520have%2520long%2520hindered%2520its%2520analysis.%2520To%2520bridge%2520this%2520gap%2520and%2520enhance%2520understanding%2520of%2520urban%2520development%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BWakeupUrbanBench%257D%2524%252C%2520an%2520annotated%2520segmentation%2520dataset%2520based%2520on%2520historical%2520satellite%2520imagery%2520with%2520the%2520earliest%2520observation%2520time%2520among%2520all%2520existing%2520remote%2520sensing%2520%2528RS%2529%2520datasets%252C%2520along%2520with%2520a%2520framework%2520for%2520unsupervised%2520segmentation%2520tasks%252C%2520%2524%255Ctextbf%257BWakeupUSM%257D%2524.%2520First%252C%2520WakeupUrbanBench%2520serves%2520as%2520a%2520pioneer%252C%2520expertly%2520annotated%2520dataset%2520built%2520on%2520mid-%252420%255E%257B%255Ctext%257Bth%257D%257D%2524%2520century%2520RS%2520imagery%252C%2520involving%2520four%2520key%2520urban%2520classes%2520and%2520spanning%25204%2520cities%2520across%25202%2520continents%2520with%2520nearly%25201000%2520km%2524%255E2%2524%2520area%2520of%2520diverse%2520urban%2520morphologies%252C%2520and%2520additionally%2520introducing%2520one%2520present-day%2520city.%2520Second%252C%2520WakeupUSM%2520is%2520a%2520novel%2520unsupervised%2520semantic%2520segmentation%2520framework%2520for%2520historical%2520RS%2520imagery.%2520It%2520employs%2520a%2520confidence-aware%2520alignment%2520mechanism%2520and%2520focal-confidence%2520loss%2520based%2520on%2520a%2520self-supervised%2520learning%2520architecture%252C%2520which%2520generates%2520robust%2520pseudo-labels%2520and%2520adaptively%2520prioritizes%2520prediction%2520difficulty%2520and%2520label%2520reliability%2520to%2520improve%2520unsupervised%2520segmentation%2520on%2520noisy%2520historical%2520data%2520without%2520manual%2520supervision.%2520Comprehensive%2520experiments%2520demonstrate%2520WakeupUSM%2520significantly%2520outperforms%2520existing%2520unsupervised%2520segmentation%2520methods%2520%2524%255Ctextbf%257Bboth%2520WakeupUrbanBench%2520and%2520public%2520dataset%257D%2524%252C%2520promising%2520to%2520pave%2520the%2520way%2520for%2520quantitative%2520studies%2520of%2520long-term%2520urban%2520change%2520using%2520modern%2520computer%2520vision.%2520Our%2520benchmark%2520and%2520codes%2520will%2520be%2520released%2520at%2520https%253A//github.com/Tianxiang-Hao/WakeupUrban.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09476v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WakeupUrban%3A%20Unsupervised%20Semantic%20Segmentation%20of%20Mid-20%24%5E%7Bth%7D%24%20century%20Urban%20Landscapes%20with%20Satellite%20Imagery&entry.906535625=Tianxiang%20Hao%20and%20Lixian%20Zhang%20and%20Yingjia%20Zhang%20and%20Mengxuan%20Chen%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu&entry.1292438233=Historical%20satellite%20imagery%20archive%2C%20such%20as%20Keyhole%20satellite%20data%2C%20offers%20rare%20insights%20into%20understanding%20early%20urban%20development%20and%20long-term%20transformation.%20However%2C%20severe%20quality%20degradation%20%28%24%5Ctextit%7Be.g.%7D%24%2C%20distortion%2C%20misalignment%2C%20and%20spectral%20scarcity%29%20and%20the%20absence%20of%20annotations%20have%20long%20hindered%20its%20analysis.%20To%20bridge%20this%20gap%20and%20enhance%20understanding%20of%20urban%20development%2C%20we%20introduce%20%24%5Ctextbf%7BWakeupUrbanBench%7D%24%2C%20an%20annotated%20segmentation%20dataset%20based%20on%20historical%20satellite%20imagery%20with%20the%20earliest%20observation%20time%20among%20all%20existing%20remote%20sensing%20%28RS%29%20datasets%2C%20along%20with%20a%20framework%20for%20unsupervised%20segmentation%20tasks%2C%20%24%5Ctextbf%7BWakeupUSM%7D%24.%20First%2C%20WakeupUrbanBench%20serves%20as%20a%20pioneer%2C%20expertly%20annotated%20dataset%20built%20on%20mid-%2420%5E%7B%5Ctext%7Bth%7D%7D%24%20century%20RS%20imagery%2C%20involving%20four%20key%20urban%20classes%20and%20spanning%204%20cities%20across%202%20continents%20with%20nearly%201000%20km%24%5E2%24%20area%20of%20diverse%20urban%20morphologies%2C%20and%20additionally%20introducing%20one%20present-day%20city.%20Second%2C%20WakeupUSM%20is%20a%20novel%20unsupervised%20semantic%20segmentation%20framework%20for%20historical%20RS%20imagery.%20It%20employs%20a%20confidence-aware%20alignment%20mechanism%20and%20focal-confidence%20loss%20based%20on%20a%20self-supervised%20learning%20architecture%2C%20which%20generates%20robust%20pseudo-labels%20and%20adaptively%20prioritizes%20prediction%20difficulty%20and%20label%20reliability%20to%20improve%20unsupervised%20segmentation%20on%20noisy%20historical%20data%20without%20manual%20supervision.%20Comprehensive%20experiments%20demonstrate%20WakeupUSM%20significantly%20outperforms%20existing%20unsupervised%20segmentation%20methods%20%24%5Ctextbf%7Bboth%20WakeupUrbanBench%20and%20public%20dataset%7D%24%2C%20promising%20to%20pave%20the%20way%20for%20quantitative%20studies%20of%20long-term%20urban%20change%20using%20modern%20computer%20vision.%20Our%20benchmark%20and%20codes%20will%20be%20released%20at%20https%3A//github.com/Tianxiang-Hao/WakeupUrban.&entry.1838667208=http%3A//arxiv.org/abs/2506.09476v3&entry.124074799=Read"},
{"title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance", "author": "Francesco Ragusa and Michele Mazzamuto and Rosario Forte and Irene D'Ambra and James Fort and Jakob Engel and Antonino Furnari and Giovanni Maria Farinella", "abstract": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.", "link": "http://arxiv.org/abs/2512.13238v1", "date": "2025-12-15", "relevancy": 2.1034, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5339}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5235}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ego-EXTRA%3A%20video-language%20Egocentric%20Dataset%20for%20EXpert-TRAinee%20assistance&body=Title%3A%20Ego-EXTRA%3A%20video-language%20Egocentric%20Dataset%20for%20EXpert-TRAinee%20assistance%0AAuthor%3A%20Francesco%20Ragusa%20and%20Michele%20Mazzamuto%20and%20Rosario%20Forte%20and%20Irene%20D%27Ambra%20and%20James%20Fort%20and%20Jakob%20Engel%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20We%20present%20Ego-EXTRA%2C%20a%20video-language%20Egocentric%20Dataset%20for%20EXpert-TRAinee%20assistance.%20Ego-EXTRA%20features%2050%20hours%20of%20unscripted%20egocentric%20videos%20of%20subjects%20performing%20procedural%20activities%20%28the%20trainees%29%20while%20guided%20by%20real-world%20experts%20who%20provide%20guidance%20and%20answer%20specific%20questions%20using%20natural%20language.%20Following%20a%20%60%60Wizard%20of%20OZ%27%27%20data%20collection%20paradigm%2C%20the%20expert%20enacts%20a%20wearable%20intelligent%20assistant%2C%20looking%20at%20the%20activities%20performed%20by%20the%20trainee%20exclusively%20from%20their%20egocentric%20point%20of%20view%2C%20answering%20questions%20when%20asked%20by%20the%20trainee%2C%20or%20proactively%20interacting%20with%20suggestions%20during%20the%20procedures.%20This%20unique%20data%20collection%20protocol%20enables%20Ego-EXTRA%20to%20capture%20a%20high-quality%20dialogue%20in%20which%20expert-level%20feedback%20is%20provided%20to%20the%20trainee.%20Two-way%20dialogues%20between%20experts%20and%20trainees%20are%20recorded%2C%20transcribed%2C%20and%20used%20to%20create%20a%20novel%20benchmark%20comprising%20more%20than%2015k%20high-quality%20Visual%20Question%20Answer%20sets%2C%20which%20we%20use%20to%20evaluate%20Multimodal%20Large%20Language%20Models.%20The%20results%20show%20that%20Ego-EXTRA%20is%20challenging%20and%20highlight%20the%20limitations%20of%20current%20models%20when%20used%20to%20provide%20expert-level%20assistance%20to%20the%20user.%20The%20Ego-EXTRA%20dataset%20is%20publicly%20available%20to%20support%20the%20benchmark%20of%20egocentric%20video-language%20assistants%3A%20https%3A//fpv-iplab.github.io/Ego-EXTRA/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgo-EXTRA%253A%2520video-language%2520Egocentric%2520Dataset%2520for%2520EXpert-TRAinee%2520assistance%26entry.906535625%3DFrancesco%2520Ragusa%2520and%2520Michele%2520Mazzamuto%2520and%2520Rosario%2520Forte%2520and%2520Irene%2520D%2527Ambra%2520and%2520James%2520Fort%2520and%2520Jakob%2520Engel%2520and%2520Antonino%2520Furnari%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3DWe%2520present%2520Ego-EXTRA%252C%2520a%2520video-language%2520Egocentric%2520Dataset%2520for%2520EXpert-TRAinee%2520assistance.%2520Ego-EXTRA%2520features%252050%2520hours%2520of%2520unscripted%2520egocentric%2520videos%2520of%2520subjects%2520performing%2520procedural%2520activities%2520%2528the%2520trainees%2529%2520while%2520guided%2520by%2520real-world%2520experts%2520who%2520provide%2520guidance%2520and%2520answer%2520specific%2520questions%2520using%2520natural%2520language.%2520Following%2520a%2520%2560%2560Wizard%2520of%2520OZ%2527%2527%2520data%2520collection%2520paradigm%252C%2520the%2520expert%2520enacts%2520a%2520wearable%2520intelligent%2520assistant%252C%2520looking%2520at%2520the%2520activities%2520performed%2520by%2520the%2520trainee%2520exclusively%2520from%2520their%2520egocentric%2520point%2520of%2520view%252C%2520answering%2520questions%2520when%2520asked%2520by%2520the%2520trainee%252C%2520or%2520proactively%2520interacting%2520with%2520suggestions%2520during%2520the%2520procedures.%2520This%2520unique%2520data%2520collection%2520protocol%2520enables%2520Ego-EXTRA%2520to%2520capture%2520a%2520high-quality%2520dialogue%2520in%2520which%2520expert-level%2520feedback%2520is%2520provided%2520to%2520the%2520trainee.%2520Two-way%2520dialogues%2520between%2520experts%2520and%2520trainees%2520are%2520recorded%252C%2520transcribed%252C%2520and%2520used%2520to%2520create%2520a%2520novel%2520benchmark%2520comprising%2520more%2520than%252015k%2520high-quality%2520Visual%2520Question%2520Answer%2520sets%252C%2520which%2520we%2520use%2520to%2520evaluate%2520Multimodal%2520Large%2520Language%2520Models.%2520The%2520results%2520show%2520that%2520Ego-EXTRA%2520is%2520challenging%2520and%2520highlight%2520the%2520limitations%2520of%2520current%2520models%2520when%2520used%2520to%2520provide%2520expert-level%2520assistance%2520to%2520the%2520user.%2520The%2520Ego-EXTRA%2520dataset%2520is%2520publicly%2520available%2520to%2520support%2520the%2520benchmark%2520of%2520egocentric%2520video-language%2520assistants%253A%2520https%253A//fpv-iplab.github.io/Ego-EXTRA/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ego-EXTRA%3A%20video-language%20Egocentric%20Dataset%20for%20EXpert-TRAinee%20assistance&entry.906535625=Francesco%20Ragusa%20and%20Michele%20Mazzamuto%20and%20Rosario%20Forte%20and%20Irene%20D%27Ambra%20and%20James%20Fort%20and%20Jakob%20Engel%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=We%20present%20Ego-EXTRA%2C%20a%20video-language%20Egocentric%20Dataset%20for%20EXpert-TRAinee%20assistance.%20Ego-EXTRA%20features%2050%20hours%20of%20unscripted%20egocentric%20videos%20of%20subjects%20performing%20procedural%20activities%20%28the%20trainees%29%20while%20guided%20by%20real-world%20experts%20who%20provide%20guidance%20and%20answer%20specific%20questions%20using%20natural%20language.%20Following%20a%20%60%60Wizard%20of%20OZ%27%27%20data%20collection%20paradigm%2C%20the%20expert%20enacts%20a%20wearable%20intelligent%20assistant%2C%20looking%20at%20the%20activities%20performed%20by%20the%20trainee%20exclusively%20from%20their%20egocentric%20point%20of%20view%2C%20answering%20questions%20when%20asked%20by%20the%20trainee%2C%20or%20proactively%20interacting%20with%20suggestions%20during%20the%20procedures.%20This%20unique%20data%20collection%20protocol%20enables%20Ego-EXTRA%20to%20capture%20a%20high-quality%20dialogue%20in%20which%20expert-level%20feedback%20is%20provided%20to%20the%20trainee.%20Two-way%20dialogues%20between%20experts%20and%20trainees%20are%20recorded%2C%20transcribed%2C%20and%20used%20to%20create%20a%20novel%20benchmark%20comprising%20more%20than%2015k%20high-quality%20Visual%20Question%20Answer%20sets%2C%20which%20we%20use%20to%20evaluate%20Multimodal%20Large%20Language%20Models.%20The%20results%20show%20that%20Ego-EXTRA%20is%20challenging%20and%20highlight%20the%20limitations%20of%20current%20models%20when%20used%20to%20provide%20expert-level%20assistance%20to%20the%20user.%20The%20Ego-EXTRA%20dataset%20is%20publicly%20available%20to%20support%20the%20benchmark%20of%20egocentric%20video-language%20assistants%3A%20https%3A//fpv-iplab.github.io/Ego-EXTRA/.&entry.1838667208=http%3A//arxiv.org/abs/2512.13238v1&entry.124074799=Read"},
{"title": "MMhops-R1: Multimodal Multi-hop Reasoning", "author": "Tao Zhang and Ziqi Zhang and Zongyang Ma and Yuxin Chen and Bing Li and Chunfeng Yuan and Guangting Wang and Fengyun Rao and Ying Shan and Weiming Hu", "abstract": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.", "link": "http://arxiv.org/abs/2512.13573v1", "date": "2025-12-15", "relevancy": 2.0906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMhops-R1%3A%20Multimodal%20Multi-hop%20Reasoning&body=Title%3A%20MMhops-R1%3A%20Multimodal%20Multi-hop%20Reasoning%0AAuthor%3A%20Tao%20Zhang%20and%20Ziqi%20Zhang%20and%20Zongyang%20Ma%20and%20Yuxin%20Chen%20and%20Bing%20Li%20and%20Chunfeng%20Yuan%20and%20Guangting%20Wang%20and%20Fengyun%20Rao%20and%20Ying%20Shan%20and%20Weiming%20Hu%0AAbstract%3A%20The%20ability%20to%20perform%20multi-modal%20multi-hop%20reasoning%20by%20iteratively%20integrating%20information%20across%20various%20modalities%20and%20external%20knowledge%20is%20critical%20for%20addressing%20complex%20real-world%20challenges.%20However%2C%20existing%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20are%20predominantly%20limited%20to%20single-step%20reasoning%2C%20as%20existing%20benchmarks%20lack%20the%20complexity%20needed%20to%20evaluate%20and%20drive%20multi-hop%20abilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MMhops%2C%20a%20novel%2C%20large-scale%20benchmark%20designed%20to%20systematically%20evaluate%20and%20foster%20multi-modal%20multi-hop%20reasoning.%20MMhops%20dataset%20comprises%20two%20challenging%20task%20formats%2C%20Bridging%20and%20Comparison%2C%20which%20necessitate%20that%20models%20dynamically%20construct%20complex%20reasoning%20chains%20by%20integrating%20external%20knowledge.%20To%20tackle%20the%20challenges%20posed%20by%20MMhops%2C%20we%20propose%20MMhops-R1%2C%20a%20novel%20multi-modal%20Retrieval-Augmented%20Generation%20%28mRAG%29%20framework%20for%20dynamic%20reasoning.%20Our%20framework%20utilizes%20reinforcement%20learning%20to%20optimize%20the%20model%20for%20autonomously%20planning%20reasoning%20paths%2C%20formulating%20targeted%20queries%2C%20and%20synthesizing%20multi-level%20information.%20Comprehensive%20experiments%20demonstrate%20that%20MMhops-R1%20significantly%20outperforms%20strong%20baselines%20on%20MMhops%2C%20highlighting%20that%20dynamic%20planning%20and%20multi-modal%20knowledge%20integration%20are%20crucial%20for%20complex%20reasoning.%20Moreover%2C%20MMhops-R1%20demonstrates%20strong%20generalization%20to%20tasks%20requiring%20fixed-hop%20reasoning%2C%20underscoring%20the%20robustness%20of%20our%20dynamic%20planning%20approach.%20In%20conclusion%2C%20our%20work%20contributes%20a%20challenging%20new%20benchmark%20and%20a%20powerful%20baseline%20model%2C%20and%20we%20will%20release%20the%20associated%20code%2C%20data%2C%20and%20weights%20to%20catalyze%20future%20research%20in%20this%20critical%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMhops-R1%253A%2520Multimodal%2520Multi-hop%2520Reasoning%26entry.906535625%3DTao%2520Zhang%2520and%2520Ziqi%2520Zhang%2520and%2520Zongyang%2520Ma%2520and%2520Yuxin%2520Chen%2520and%2520Bing%2520Li%2520and%2520Chunfeng%2520Yuan%2520and%2520Guangting%2520Wang%2520and%2520Fengyun%2520Rao%2520and%2520Ying%2520Shan%2520and%2520Weiming%2520Hu%26entry.1292438233%3DThe%2520ability%2520to%2520perform%2520multi-modal%2520multi-hop%2520reasoning%2520by%2520iteratively%2520integrating%2520information%2520across%2520various%2520modalities%2520and%2520external%2520knowledge%2520is%2520critical%2520for%2520addressing%2520complex%2520real-world%2520challenges.%2520However%252C%2520existing%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520predominantly%2520limited%2520to%2520single-step%2520reasoning%252C%2520as%2520existing%2520benchmarks%2520lack%2520the%2520complexity%2520needed%2520to%2520evaluate%2520and%2520drive%2520multi-hop%2520abilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MMhops%252C%2520a%2520novel%252C%2520large-scale%2520benchmark%2520designed%2520to%2520systematically%2520evaluate%2520and%2520foster%2520multi-modal%2520multi-hop%2520reasoning.%2520MMhops%2520dataset%2520comprises%2520two%2520challenging%2520task%2520formats%252C%2520Bridging%2520and%2520Comparison%252C%2520which%2520necessitate%2520that%2520models%2520dynamically%2520construct%2520complex%2520reasoning%2520chains%2520by%2520integrating%2520external%2520knowledge.%2520To%2520tackle%2520the%2520challenges%2520posed%2520by%2520MMhops%252C%2520we%2520propose%2520MMhops-R1%252C%2520a%2520novel%2520multi-modal%2520Retrieval-Augmented%2520Generation%2520%2528mRAG%2529%2520framework%2520for%2520dynamic%2520reasoning.%2520Our%2520framework%2520utilizes%2520reinforcement%2520learning%2520to%2520optimize%2520the%2520model%2520for%2520autonomously%2520planning%2520reasoning%2520paths%252C%2520formulating%2520targeted%2520queries%252C%2520and%2520synthesizing%2520multi-level%2520information.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520MMhops-R1%2520significantly%2520outperforms%2520strong%2520baselines%2520on%2520MMhops%252C%2520highlighting%2520that%2520dynamic%2520planning%2520and%2520multi-modal%2520knowledge%2520integration%2520are%2520crucial%2520for%2520complex%2520reasoning.%2520Moreover%252C%2520MMhops-R1%2520demonstrates%2520strong%2520generalization%2520to%2520tasks%2520requiring%2520fixed-hop%2520reasoning%252C%2520underscoring%2520the%2520robustness%2520of%2520our%2520dynamic%2520planning%2520approach.%2520In%2520conclusion%252C%2520our%2520work%2520contributes%2520a%2520challenging%2520new%2520benchmark%2520and%2520a%2520powerful%2520baseline%2520model%252C%2520and%2520we%2520will%2520release%2520the%2520associated%2520code%252C%2520data%252C%2520and%2520weights%2520to%2520catalyze%2520future%2520research%2520in%2520this%2520critical%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMhops-R1%3A%20Multimodal%20Multi-hop%20Reasoning&entry.906535625=Tao%20Zhang%20and%20Ziqi%20Zhang%20and%20Zongyang%20Ma%20and%20Yuxin%20Chen%20and%20Bing%20Li%20and%20Chunfeng%20Yuan%20and%20Guangting%20Wang%20and%20Fengyun%20Rao%20and%20Ying%20Shan%20and%20Weiming%20Hu&entry.1292438233=The%20ability%20to%20perform%20multi-modal%20multi-hop%20reasoning%20by%20iteratively%20integrating%20information%20across%20various%20modalities%20and%20external%20knowledge%20is%20critical%20for%20addressing%20complex%20real-world%20challenges.%20However%2C%20existing%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20are%20predominantly%20limited%20to%20single-step%20reasoning%2C%20as%20existing%20benchmarks%20lack%20the%20complexity%20needed%20to%20evaluate%20and%20drive%20multi-hop%20abilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MMhops%2C%20a%20novel%2C%20large-scale%20benchmark%20designed%20to%20systematically%20evaluate%20and%20foster%20multi-modal%20multi-hop%20reasoning.%20MMhops%20dataset%20comprises%20two%20challenging%20task%20formats%2C%20Bridging%20and%20Comparison%2C%20which%20necessitate%20that%20models%20dynamically%20construct%20complex%20reasoning%20chains%20by%20integrating%20external%20knowledge.%20To%20tackle%20the%20challenges%20posed%20by%20MMhops%2C%20we%20propose%20MMhops-R1%2C%20a%20novel%20multi-modal%20Retrieval-Augmented%20Generation%20%28mRAG%29%20framework%20for%20dynamic%20reasoning.%20Our%20framework%20utilizes%20reinforcement%20learning%20to%20optimize%20the%20model%20for%20autonomously%20planning%20reasoning%20paths%2C%20formulating%20targeted%20queries%2C%20and%20synthesizing%20multi-level%20information.%20Comprehensive%20experiments%20demonstrate%20that%20MMhops-R1%20significantly%20outperforms%20strong%20baselines%20on%20MMhops%2C%20highlighting%20that%20dynamic%20planning%20and%20multi-modal%20knowledge%20integration%20are%20crucial%20for%20complex%20reasoning.%20Moreover%2C%20MMhops-R1%20demonstrates%20strong%20generalization%20to%20tasks%20requiring%20fixed-hop%20reasoning%2C%20underscoring%20the%20robustness%20of%20our%20dynamic%20planning%20approach.%20In%20conclusion%2C%20our%20work%20contributes%20a%20challenging%20new%20benchmark%20and%20a%20powerful%20baseline%20model%2C%20and%20we%20will%20release%20the%20associated%20code%2C%20data%2C%20and%20weights%20to%20catalyze%20future%20research%20in%20this%20critical%20area.&entry.1838667208=http%3A//arxiv.org/abs/2512.13573v1&entry.124074799=Read"},
{"title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles", "author": "S\u00fcmer Tun\u00e7ay and Alain Andres and Ignacio Carlucho", "abstract": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.", "link": "http://arxiv.org/abs/2512.13359v1", "date": "2025-12-15", "relevancy": 2.0843, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Policy%20Learning%20for%206-DOF%20Position%20Control%20of%20Underwater%20Vehicles&body=Title%3A%20Fast%20Policy%20Learning%20for%206-DOF%20Position%20Control%20of%20Underwater%20Vehicles%0AAuthor%3A%20S%C3%BCmer%20Tun%C3%A7ay%20and%20Alain%20Andres%20and%20Ignacio%20Carlucho%0AAbstract%3A%20Autonomous%20Underwater%20Vehicles%20%28AUVs%29%20require%20reliable%20six-degree-of-freedom%20%286-DOF%29%20position%20control%20to%20operate%20effectively%20in%20complex%20and%20dynamic%20marine%20environments.%20Traditional%20controllers%20are%20effective%20under%20nominal%20conditions%20but%20exhibit%20degraded%20performance%20when%20faced%20with%20unmodeled%20dynamics%20or%20environmental%20disturbances.%20Reinforcement%20learning%20%28RL%29%20provides%20a%20powerful%20alternative%20but%20training%20is%20typically%20slow%20and%20sim-to-real%20transfer%20remains%20challenging.%20This%20work%20introduces%20a%20GPU-accelerated%20RL%20training%20pipeline%20built%20in%20JAX%20and%20MuJoCo-XLA%20%28MJX%29.%20By%20jointly%20JIT-compiling%20large-scale%20parallel%20physics%20simulation%20and%20learning%20updates%2C%20we%20achieve%20training%20times%20of%20under%20two%20minutes.Through%20systematic%20evaluation%20of%20multiple%20RL%20algorithms%2C%20we%20show%20robust%206-DOF%20trajectory%20tracking%20and%20effective%20disturbance%20rejection%20in%20real%20underwater%20experiments%2C%20with%20policies%20transferred%20zero-shot%20from%20simulation.%20Our%20results%20provide%20the%20first%20explicit%20real-world%20demonstration%20of%20RL-based%20AUV%20position%20control%20across%20all%20six%20degrees%20of%20freedom.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Policy%2520Learning%2520for%25206-DOF%2520Position%2520Control%2520of%2520Underwater%2520Vehicles%26entry.906535625%3DS%25C3%25BCmer%2520Tun%25C3%25A7ay%2520and%2520Alain%2520Andres%2520and%2520Ignacio%2520Carlucho%26entry.1292438233%3DAutonomous%2520Underwater%2520Vehicles%2520%2528AUVs%2529%2520require%2520reliable%2520six-degree-of-freedom%2520%25286-DOF%2529%2520position%2520control%2520to%2520operate%2520effectively%2520in%2520complex%2520and%2520dynamic%2520marine%2520environments.%2520Traditional%2520controllers%2520are%2520effective%2520under%2520nominal%2520conditions%2520but%2520exhibit%2520degraded%2520performance%2520when%2520faced%2520with%2520unmodeled%2520dynamics%2520or%2520environmental%2520disturbances.%2520Reinforcement%2520learning%2520%2528RL%2529%2520provides%2520a%2520powerful%2520alternative%2520but%2520training%2520is%2520typically%2520slow%2520and%2520sim-to-real%2520transfer%2520remains%2520challenging.%2520This%2520work%2520introduces%2520a%2520GPU-accelerated%2520RL%2520training%2520pipeline%2520built%2520in%2520JAX%2520and%2520MuJoCo-XLA%2520%2528MJX%2529.%2520By%2520jointly%2520JIT-compiling%2520large-scale%2520parallel%2520physics%2520simulation%2520and%2520learning%2520updates%252C%2520we%2520achieve%2520training%2520times%2520of%2520under%2520two%2520minutes.Through%2520systematic%2520evaluation%2520of%2520multiple%2520RL%2520algorithms%252C%2520we%2520show%2520robust%25206-DOF%2520trajectory%2520tracking%2520and%2520effective%2520disturbance%2520rejection%2520in%2520real%2520underwater%2520experiments%252C%2520with%2520policies%2520transferred%2520zero-shot%2520from%2520simulation.%2520Our%2520results%2520provide%2520the%2520first%2520explicit%2520real-world%2520demonstration%2520of%2520RL-based%2520AUV%2520position%2520control%2520across%2520all%2520six%2520degrees%2520of%2520freedom.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Policy%20Learning%20for%206-DOF%20Position%20Control%20of%20Underwater%20Vehicles&entry.906535625=S%C3%BCmer%20Tun%C3%A7ay%20and%20Alain%20Andres%20and%20Ignacio%20Carlucho&entry.1292438233=Autonomous%20Underwater%20Vehicles%20%28AUVs%29%20require%20reliable%20six-degree-of-freedom%20%286-DOF%29%20position%20control%20to%20operate%20effectively%20in%20complex%20and%20dynamic%20marine%20environments.%20Traditional%20controllers%20are%20effective%20under%20nominal%20conditions%20but%20exhibit%20degraded%20performance%20when%20faced%20with%20unmodeled%20dynamics%20or%20environmental%20disturbances.%20Reinforcement%20learning%20%28RL%29%20provides%20a%20powerful%20alternative%20but%20training%20is%20typically%20slow%20and%20sim-to-real%20transfer%20remains%20challenging.%20This%20work%20introduces%20a%20GPU-accelerated%20RL%20training%20pipeline%20built%20in%20JAX%20and%20MuJoCo-XLA%20%28MJX%29.%20By%20jointly%20JIT-compiling%20large-scale%20parallel%20physics%20simulation%20and%20learning%20updates%2C%20we%20achieve%20training%20times%20of%20under%20two%20minutes.Through%20systematic%20evaluation%20of%20multiple%20RL%20algorithms%2C%20we%20show%20robust%206-DOF%20trajectory%20tracking%20and%20effective%20disturbance%20rejection%20in%20real%20underwater%20experiments%2C%20with%20policies%20transferred%20zero-shot%20from%20simulation.%20Our%20results%20provide%20the%20first%20explicit%20real-world%20demonstration%20of%20RL-based%20AUV%20position%20control%20across%20all%20six%20degrees%20of%20freedom.&entry.1838667208=http%3A//arxiv.org/abs/2512.13359v1&entry.124074799=Read"},
{"title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models", "author": "Chendong Sun", "abstract": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.", "link": "http://arxiv.org/abs/2512.13194v1", "date": "2025-12-15", "relevancy": 2.0751, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5285}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5129}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptive%20Rejection%20Sampling%20for%20Accelerating%20Speculative%20Decoding%20in%20Large%20Language%20Models&body=Title%3A%20Efficient%20Adaptive%20Rejection%20Sampling%20for%20Accelerating%20Speculative%20Decoding%20in%20Large%20Language%20Models%0AAuthor%3A%20Chendong%20Sun%0AAbstract%3A%20Speculative%20Decoding%20is%20a%20prominent%20technique%20for%20accelerating%20the%20autoregressive%20inference%20of%20large%20language%20models%20%28LLMs%29%20by%20employing%20a%20fast%20draft%20model%20to%20propose%20candidate%20token%20sequences%20and%20a%20large%20target%20model%20to%20verify%20them%20in%20parallel.%20However%2C%20its%20core%20component%20--%20the%20rejection%20sampling%20mechanism%20--%20relies%20on%20a%20fixed%2C%20context-independent%20random%20threshold.%20This%20leads%20to%20a%20significant%20%22random%20rejection%22%20problem%20in%20high-uncertainty%20generation%20scenarios%2C%20where%20plausible%20candidate%20tokens%20are%20frequently%20rejected%20due%20to%20random%20chance%2C%20undermining%20inference%20efficiency.%20This%20paper%20introduces%20Efficient%20Adaptive%20Rejection%20Sampling%20%28EARS%29%2C%20a%20novel%20method%20that%20dynamically%20adjusts%20the%20acceptance%20threshold%20by%20incorporating%20the%20target%20model%27s%20own%20predictive%20uncertainty%2C%20measured%20as%20%5C%281%20-%20%5Cmax%28P_%7B%5Cmathrm%7Btarget%7D%7D%29%5C%29.%20By%20introducing%20a%20tolerance%20term%20proportional%20to%20this%20uncertainty%2C%20EARS%20intelligently%20relaxes%20the%20acceptance%20criterion%20when%20the%20model%20is%20uncertain%2C%20effectively%20reducing%20random%20rejections%20while%20maintaining%20strict%20standards%20when%20the%20model%20is%20confident.%20Experiments%20on%20creative%20writing%20and%20open-domain%20QA%20tasks%20demonstrate%20that%20EARS%20significantly%20enhances%20the%20efficiency%20of%20speculative%20decoding%2C%20achieving%20up%20to%20an%2018.12%25%20increase%20in%20throughput%20with%20a%20negligible%200.84%25%20accuracy%20drop%20on%20the%20GSM8K%20benchmark.%20The%20method%20requires%20no%20modifications%20to%20model%20architectures%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20speculative%20decoding%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptive%2520Rejection%2520Sampling%2520for%2520Accelerating%2520Speculative%2520Decoding%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DChendong%2520Sun%26entry.1292438233%3DSpeculative%2520Decoding%2520is%2520a%2520prominent%2520technique%2520for%2520accelerating%2520the%2520autoregressive%2520inference%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520employing%2520a%2520fast%2520draft%2520model%2520to%2520propose%2520candidate%2520token%2520sequences%2520and%2520a%2520large%2520target%2520model%2520to%2520verify%2520them%2520in%2520parallel.%2520However%252C%2520its%2520core%2520component%2520--%2520the%2520rejection%2520sampling%2520mechanism%2520--%2520relies%2520on%2520a%2520fixed%252C%2520context-independent%2520random%2520threshold.%2520This%2520leads%2520to%2520a%2520significant%2520%2522random%2520rejection%2522%2520problem%2520in%2520high-uncertainty%2520generation%2520scenarios%252C%2520where%2520plausible%2520candidate%2520tokens%2520are%2520frequently%2520rejected%2520due%2520to%2520random%2520chance%252C%2520undermining%2520inference%2520efficiency.%2520This%2520paper%2520introduces%2520Efficient%2520Adaptive%2520Rejection%2520Sampling%2520%2528EARS%2529%252C%2520a%2520novel%2520method%2520that%2520dynamically%2520adjusts%2520the%2520acceptance%2520threshold%2520by%2520incorporating%2520the%2520target%2520model%2527s%2520own%2520predictive%2520uncertainty%252C%2520measured%2520as%2520%255C%25281%2520-%2520%255Cmax%2528P_%257B%255Cmathrm%257Btarget%257D%257D%2529%255C%2529.%2520By%2520introducing%2520a%2520tolerance%2520term%2520proportional%2520to%2520this%2520uncertainty%252C%2520EARS%2520intelligently%2520relaxes%2520the%2520acceptance%2520criterion%2520when%2520the%2520model%2520is%2520uncertain%252C%2520effectively%2520reducing%2520random%2520rejections%2520while%2520maintaining%2520strict%2520standards%2520when%2520the%2520model%2520is%2520confident.%2520Experiments%2520on%2520creative%2520writing%2520and%2520open-domain%2520QA%2520tasks%2520demonstrate%2520that%2520EARS%2520significantly%2520enhances%2520the%2520efficiency%2520of%2520speculative%2520decoding%252C%2520achieving%2520up%2520to%2520an%252018.12%2525%2520increase%2520in%2520throughput%2520with%2520a%2520negligible%25200.84%2525%2520accuracy%2520drop%2520on%2520the%2520GSM8K%2520benchmark.%2520The%2520method%2520requires%2520no%2520modifications%2520to%2520model%2520architectures%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520speculative%2520decoding%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptive%20Rejection%20Sampling%20for%20Accelerating%20Speculative%20Decoding%20in%20Large%20Language%20Models&entry.906535625=Chendong%20Sun&entry.1292438233=Speculative%20Decoding%20is%20a%20prominent%20technique%20for%20accelerating%20the%20autoregressive%20inference%20of%20large%20language%20models%20%28LLMs%29%20by%20employing%20a%20fast%20draft%20model%20to%20propose%20candidate%20token%20sequences%20and%20a%20large%20target%20model%20to%20verify%20them%20in%20parallel.%20However%2C%20its%20core%20component%20--%20the%20rejection%20sampling%20mechanism%20--%20relies%20on%20a%20fixed%2C%20context-independent%20random%20threshold.%20This%20leads%20to%20a%20significant%20%22random%20rejection%22%20problem%20in%20high-uncertainty%20generation%20scenarios%2C%20where%20plausible%20candidate%20tokens%20are%20frequently%20rejected%20due%20to%20random%20chance%2C%20undermining%20inference%20efficiency.%20This%20paper%20introduces%20Efficient%20Adaptive%20Rejection%20Sampling%20%28EARS%29%2C%20a%20novel%20method%20that%20dynamically%20adjusts%20the%20acceptance%20threshold%20by%20incorporating%20the%20target%20model%27s%20own%20predictive%20uncertainty%2C%20measured%20as%20%5C%281%20-%20%5Cmax%28P_%7B%5Cmathrm%7Btarget%7D%7D%29%5C%29.%20By%20introducing%20a%20tolerance%20term%20proportional%20to%20this%20uncertainty%2C%20EARS%20intelligently%20relaxes%20the%20acceptance%20criterion%20when%20the%20model%20is%20uncertain%2C%20effectively%20reducing%20random%20rejections%20while%20maintaining%20strict%20standards%20when%20the%20model%20is%20confident.%20Experiments%20on%20creative%20writing%20and%20open-domain%20QA%20tasks%20demonstrate%20that%20EARS%20significantly%20enhances%20the%20efficiency%20of%20speculative%20decoding%2C%20achieving%20up%20to%20an%2018.12%25%20increase%20in%20throughput%20with%20a%20negligible%200.84%25%20accuracy%20drop%20on%20the%20GSM8K%20benchmark.%20The%20method%20requires%20no%20modifications%20to%20model%20architectures%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20speculative%20decoding%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2512.13194v1&entry.124074799=Read"},
{"title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging", "author": "Youssef Megahed and Inok Lee and Robin Ducharme and Kevin Dick and Adrian D. C. Chan and Steven Hawken and Mark C. Walker", "abstract": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.", "link": "http://arxiv.org/abs/2512.13434v1", "date": "2025-12-15", "relevancy": 2.0737, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5322}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Ultrasound%20Representation%20Learning%20for%20Renal%20Anomaly%20Prediction%20in%20Prenatal%20Imaging&body=Title%3A%20Self-Supervised%20Ultrasound%20Representation%20Learning%20for%20Renal%20Anomaly%20Prediction%20in%20Prenatal%20Imaging%0AAuthor%3A%20Youssef%20Megahed%20and%20Inok%20Lee%20and%20Robin%20Ducharme%20and%20Kevin%20Dick%20and%20Adrian%20D.%20C.%20Chan%20and%20Steven%20Hawken%20and%20Mark%20C.%20Walker%0AAbstract%3A%20Prenatal%20ultrasound%20is%20the%20cornerstone%20for%20detecting%20congenital%20anomalies%20of%20the%20kidneys%20and%20urinary%20tract%2C%20but%20diagnosis%20is%20limited%20by%20operator%20dependence%20and%20suboptimal%20imaging%20conditions.%20We%20sought%20to%20assess%20the%20performance%20of%20a%20self-supervised%20ultrasound%20foundation%20model%20for%20automated%20fetal%20renal%20anomaly%20classification%20using%20a%20curated%20dataset%20of%20969%20two-dimensional%20ultrasound%20images.%20A%20pretrained%20Ultrasound%20Self-Supervised%20Foundation%20Model%20with%20Masked%20Autoencoding%20%28USF-MAE%29%20was%20fine-tuned%20for%20binary%20and%20multi-class%20classification%20of%20normal%20kidneys%2C%20urinary%20tract%20dilation%2C%20and%20multicystic%20dysplastic%20kidney.%20Models%20were%20compared%20with%20a%20DenseNet-169%20convolutional%20baseline%20using%20cross-validation%20and%20an%20independent%20test%20set.%20USF-MAE%20consistently%20improved%20upon%20the%20baseline%20across%20all%20evaluation%20metrics%20in%20both%20binary%20and%20multi-class%20settings.%20USF-MAE%20achieved%20an%20improvement%20of%20about%201.87%25%20%28AUC%29%20and%207.8%25%20%28F1-score%29%20on%20the%20validation%20set%2C%202.32%25%20%28AUC%29%20and%204.33%25%20%28F1-score%29%20on%20the%20independent%20holdout%20test%20set.%20The%20largest%20gains%20were%20observed%20in%20the%20multi-class%20setting%2C%20where%20the%20improvement%20in%20AUC%20was%2016.28%25%20and%2046.15%25%20in%20F1-score.%20To%20facilitate%20model%20interpretability%2C%20Score-CAM%20visualizations%20were%20adapted%20for%20a%20transformer%20architecture%20and%20show%20that%20model%20predictions%20were%20informed%20by%20known%2C%20clinically%20relevant%20renal%20structures%2C%20including%20the%20renal%20pelvis%20in%20urinary%20tract%20dilation%20and%20cystic%20regions%20in%20multicystic%20dysplastic%20kidney.%20These%20results%20show%20that%20ultrasound-specific%20self-supervised%20learning%20can%20generate%20a%20useful%20representation%20as%20a%20foundation%20for%20downstream%20diagnostic%20tasks.%20The%20proposed%20framework%20offers%20a%20robust%2C%20interpretable%20approach%20to%20support%20the%20prenatal%20detection%20of%20renal%20anomalies%20and%20demonstrates%20the%20promise%20of%20foundation%20models%20in%20obstetric%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Ultrasound%2520Representation%2520Learning%2520for%2520Renal%2520Anomaly%2520Prediction%2520in%2520Prenatal%2520Imaging%26entry.906535625%3DYoussef%2520Megahed%2520and%2520Inok%2520Lee%2520and%2520Robin%2520Ducharme%2520and%2520Kevin%2520Dick%2520and%2520Adrian%2520D.%2520C.%2520Chan%2520and%2520Steven%2520Hawken%2520and%2520Mark%2520C.%2520Walker%26entry.1292438233%3DPrenatal%2520ultrasound%2520is%2520the%2520cornerstone%2520for%2520detecting%2520congenital%2520anomalies%2520of%2520the%2520kidneys%2520and%2520urinary%2520tract%252C%2520but%2520diagnosis%2520is%2520limited%2520by%2520operator%2520dependence%2520and%2520suboptimal%2520imaging%2520conditions.%2520We%2520sought%2520to%2520assess%2520the%2520performance%2520of%2520a%2520self-supervised%2520ultrasound%2520foundation%2520model%2520for%2520automated%2520fetal%2520renal%2520anomaly%2520classification%2520using%2520a%2520curated%2520dataset%2520of%2520969%2520two-dimensional%2520ultrasound%2520images.%2520A%2520pretrained%2520Ultrasound%2520Self-Supervised%2520Foundation%2520Model%2520with%2520Masked%2520Autoencoding%2520%2528USF-MAE%2529%2520was%2520fine-tuned%2520for%2520binary%2520and%2520multi-class%2520classification%2520of%2520normal%2520kidneys%252C%2520urinary%2520tract%2520dilation%252C%2520and%2520multicystic%2520dysplastic%2520kidney.%2520Models%2520were%2520compared%2520with%2520a%2520DenseNet-169%2520convolutional%2520baseline%2520using%2520cross-validation%2520and%2520an%2520independent%2520test%2520set.%2520USF-MAE%2520consistently%2520improved%2520upon%2520the%2520baseline%2520across%2520all%2520evaluation%2520metrics%2520in%2520both%2520binary%2520and%2520multi-class%2520settings.%2520USF-MAE%2520achieved%2520an%2520improvement%2520of%2520about%25201.87%2525%2520%2528AUC%2529%2520and%25207.8%2525%2520%2528F1-score%2529%2520on%2520the%2520validation%2520set%252C%25202.32%2525%2520%2528AUC%2529%2520and%25204.33%2525%2520%2528F1-score%2529%2520on%2520the%2520independent%2520holdout%2520test%2520set.%2520The%2520largest%2520gains%2520were%2520observed%2520in%2520the%2520multi-class%2520setting%252C%2520where%2520the%2520improvement%2520in%2520AUC%2520was%252016.28%2525%2520and%252046.15%2525%2520in%2520F1-score.%2520To%2520facilitate%2520model%2520interpretability%252C%2520Score-CAM%2520visualizations%2520were%2520adapted%2520for%2520a%2520transformer%2520architecture%2520and%2520show%2520that%2520model%2520predictions%2520were%2520informed%2520by%2520known%252C%2520clinically%2520relevant%2520renal%2520structures%252C%2520including%2520the%2520renal%2520pelvis%2520in%2520urinary%2520tract%2520dilation%2520and%2520cystic%2520regions%2520in%2520multicystic%2520dysplastic%2520kidney.%2520These%2520results%2520show%2520that%2520ultrasound-specific%2520self-supervised%2520learning%2520can%2520generate%2520a%2520useful%2520representation%2520as%2520a%2520foundation%2520for%2520downstream%2520diagnostic%2520tasks.%2520The%2520proposed%2520framework%2520offers%2520a%2520robust%252C%2520interpretable%2520approach%2520to%2520support%2520the%2520prenatal%2520detection%2520of%2520renal%2520anomalies%2520and%2520demonstrates%2520the%2520promise%2520of%2520foundation%2520models%2520in%2520obstetric%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Ultrasound%20Representation%20Learning%20for%20Renal%20Anomaly%20Prediction%20in%20Prenatal%20Imaging&entry.906535625=Youssef%20Megahed%20and%20Inok%20Lee%20and%20Robin%20Ducharme%20and%20Kevin%20Dick%20and%20Adrian%20D.%20C.%20Chan%20and%20Steven%20Hawken%20and%20Mark%20C.%20Walker&entry.1292438233=Prenatal%20ultrasound%20is%20the%20cornerstone%20for%20detecting%20congenital%20anomalies%20of%20the%20kidneys%20and%20urinary%20tract%2C%20but%20diagnosis%20is%20limited%20by%20operator%20dependence%20and%20suboptimal%20imaging%20conditions.%20We%20sought%20to%20assess%20the%20performance%20of%20a%20self-supervised%20ultrasound%20foundation%20model%20for%20automated%20fetal%20renal%20anomaly%20classification%20using%20a%20curated%20dataset%20of%20969%20two-dimensional%20ultrasound%20images.%20A%20pretrained%20Ultrasound%20Self-Supervised%20Foundation%20Model%20with%20Masked%20Autoencoding%20%28USF-MAE%29%20was%20fine-tuned%20for%20binary%20and%20multi-class%20classification%20of%20normal%20kidneys%2C%20urinary%20tract%20dilation%2C%20and%20multicystic%20dysplastic%20kidney.%20Models%20were%20compared%20with%20a%20DenseNet-169%20convolutional%20baseline%20using%20cross-validation%20and%20an%20independent%20test%20set.%20USF-MAE%20consistently%20improved%20upon%20the%20baseline%20across%20all%20evaluation%20metrics%20in%20both%20binary%20and%20multi-class%20settings.%20USF-MAE%20achieved%20an%20improvement%20of%20about%201.87%25%20%28AUC%29%20and%207.8%25%20%28F1-score%29%20on%20the%20validation%20set%2C%202.32%25%20%28AUC%29%20and%204.33%25%20%28F1-score%29%20on%20the%20independent%20holdout%20test%20set.%20The%20largest%20gains%20were%20observed%20in%20the%20multi-class%20setting%2C%20where%20the%20improvement%20in%20AUC%20was%2016.28%25%20and%2046.15%25%20in%20F1-score.%20To%20facilitate%20model%20interpretability%2C%20Score-CAM%20visualizations%20were%20adapted%20for%20a%20transformer%20architecture%20and%20show%20that%20model%20predictions%20were%20informed%20by%20known%2C%20clinically%20relevant%20renal%20structures%2C%20including%20the%20renal%20pelvis%20in%20urinary%20tract%20dilation%20and%20cystic%20regions%20in%20multicystic%20dysplastic%20kidney.%20These%20results%20show%20that%20ultrasound-specific%20self-supervised%20learning%20can%20generate%20a%20useful%20representation%20as%20a%20foundation%20for%20downstream%20diagnostic%20tasks.%20The%20proposed%20framework%20offers%20a%20robust%2C%20interpretable%20approach%20to%20support%20the%20prenatal%20detection%20of%20renal%20anomalies%20and%20demonstrates%20the%20promise%20of%20foundation%20models%20in%20obstetric%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2512.13434v1&entry.124074799=Read"},
{"title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions", "author": "Kangxian Xie and Yufei Zhu and Kaiming Kuang and Li Zhang and Hongwei Bran Li and Mingchen Gao and Jiancheng Yang", "abstract": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.", "link": "http://arxiv.org/abs/2505.08919v2", "date": "2025-12-15", "relevancy": 2.0655, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5306}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5108}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Template-Guided%20Reconstruction%20of%20Pulmonary%20Segments%20with%20Neural%20Implicit%20Functions&body=Title%3A%20Template-Guided%20Reconstruction%20of%20Pulmonary%20Segments%20with%20Neural%20Implicit%20Functions%0AAuthor%3A%20Kangxian%20Xie%20and%20Yufei%20Zhu%20and%20Kaiming%20Kuang%20and%20Li%20Zhang%20and%20Hongwei%20Bran%20Li%20and%20Mingchen%20Gao%20and%20Jiancheng%20Yang%0AAbstract%3A%20High-quality%203D%20reconstruction%20of%20pulmonary%20segments%20plays%20a%20crucial%20role%20in%20segmentectomy%20and%20surgical%20planning%20for%20the%20treatment%20of%20lung%20cancer.%20Due%20to%20the%20resolution%20requirement%20of%20the%20target%20reconstruction%2C%20conventional%20deep%20learning-based%20methods%20often%20suffer%20from%20computational%20resource%20constraints%20or%20limited%20granularity.%20Conversely%2C%20implicit%20modeling%20is%20favored%20due%20to%20its%20computational%20efficiency%20and%20continuous%20representation%20at%20any%20resolution.%20We%20propose%20a%20neural%20implicit%20function-based%20method%20to%20learn%20a%203D%20surface%20to%20achieve%20anatomy-aware%2C%20precise%20pulmonary%20segment%20reconstruction%2C%20represented%20as%20a%20shape%20by%20deforming%20a%20learnable%20template.%20Additionally%2C%20we%20introduce%20two%20clinically%20relevant%20evaluation%20metrics%20to%20comprehensively%20assess%20the%20quality%20of%20the%20reconstruction.%20Furthermore%2C%20to%20address%20the%20lack%20of%20publicly%20available%20shape%20datasets%20for%20benchmarking%20reconstruction%20algorithms%2C%20we%20developed%20a%20shape%20dataset%20named%20Lung3D%2C%20which%20includes%20the%203D%20models%20of%20800%20labeled%20pulmonary%20segments%20and%20their%20corresponding%20airways%2C%20arteries%2C%20veins%2C%20and%20intersegmental%20veins.%20We%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20existing%20methods%2C%20providing%20a%20new%20perspective%20for%20pulmonary%20segment%20reconstruction.%20Code%20and%20data%20will%20be%20available%20at%20https%3A//github.com/HINTLab/ImPulSe.%0ALink%3A%20http%3A//arxiv.org/abs/2505.08919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemplate-Guided%2520Reconstruction%2520of%2520Pulmonary%2520Segments%2520with%2520Neural%2520Implicit%2520Functions%26entry.906535625%3DKangxian%2520Xie%2520and%2520Yufei%2520Zhu%2520and%2520Kaiming%2520Kuang%2520and%2520Li%2520Zhang%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Mingchen%2520Gao%2520and%2520Jiancheng%2520Yang%26entry.1292438233%3DHigh-quality%25203D%2520reconstruction%2520of%2520pulmonary%2520segments%2520plays%2520a%2520crucial%2520role%2520in%2520segmentectomy%2520and%2520surgical%2520planning%2520for%2520the%2520treatment%2520of%2520lung%2520cancer.%2520Due%2520to%2520the%2520resolution%2520requirement%2520of%2520the%2520target%2520reconstruction%252C%2520conventional%2520deep%2520learning-based%2520methods%2520often%2520suffer%2520from%2520computational%2520resource%2520constraints%2520or%2520limited%2520granularity.%2520Conversely%252C%2520implicit%2520modeling%2520is%2520favored%2520due%2520to%2520its%2520computational%2520efficiency%2520and%2520continuous%2520representation%2520at%2520any%2520resolution.%2520We%2520propose%2520a%2520neural%2520implicit%2520function-based%2520method%2520to%2520learn%2520a%25203D%2520surface%2520to%2520achieve%2520anatomy-aware%252C%2520precise%2520pulmonary%2520segment%2520reconstruction%252C%2520represented%2520as%2520a%2520shape%2520by%2520deforming%2520a%2520learnable%2520template.%2520Additionally%252C%2520we%2520introduce%2520two%2520clinically%2520relevant%2520evaluation%2520metrics%2520to%2520comprehensively%2520assess%2520the%2520quality%2520of%2520the%2520reconstruction.%2520Furthermore%252C%2520to%2520address%2520the%2520lack%2520of%2520publicly%2520available%2520shape%2520datasets%2520for%2520benchmarking%2520reconstruction%2520algorithms%252C%2520we%2520developed%2520a%2520shape%2520dataset%2520named%2520Lung3D%252C%2520which%2520includes%2520the%25203D%2520models%2520of%2520800%2520labeled%2520pulmonary%2520segments%2520and%2520their%2520corresponding%2520airways%252C%2520arteries%252C%2520veins%252C%2520and%2520intersegmental%2520veins.%2520We%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520existing%2520methods%252C%2520providing%2520a%2520new%2520perspective%2520for%2520pulmonary%2520segment%2520reconstruction.%2520Code%2520and%2520data%2520will%2520be%2520available%2520at%2520https%253A//github.com/HINTLab/ImPulSe.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Template-Guided%20Reconstruction%20of%20Pulmonary%20Segments%20with%20Neural%20Implicit%20Functions&entry.906535625=Kangxian%20Xie%20and%20Yufei%20Zhu%20and%20Kaiming%20Kuang%20and%20Li%20Zhang%20and%20Hongwei%20Bran%20Li%20and%20Mingchen%20Gao%20and%20Jiancheng%20Yang&entry.1292438233=High-quality%203D%20reconstruction%20of%20pulmonary%20segments%20plays%20a%20crucial%20role%20in%20segmentectomy%20and%20surgical%20planning%20for%20the%20treatment%20of%20lung%20cancer.%20Due%20to%20the%20resolution%20requirement%20of%20the%20target%20reconstruction%2C%20conventional%20deep%20learning-based%20methods%20often%20suffer%20from%20computational%20resource%20constraints%20or%20limited%20granularity.%20Conversely%2C%20implicit%20modeling%20is%20favored%20due%20to%20its%20computational%20efficiency%20and%20continuous%20representation%20at%20any%20resolution.%20We%20propose%20a%20neural%20implicit%20function-based%20method%20to%20learn%20a%203D%20surface%20to%20achieve%20anatomy-aware%2C%20precise%20pulmonary%20segment%20reconstruction%2C%20represented%20as%20a%20shape%20by%20deforming%20a%20learnable%20template.%20Additionally%2C%20we%20introduce%20two%20clinically%20relevant%20evaluation%20metrics%20to%20comprehensively%20assess%20the%20quality%20of%20the%20reconstruction.%20Furthermore%2C%20to%20address%20the%20lack%20of%20publicly%20available%20shape%20datasets%20for%20benchmarking%20reconstruction%20algorithms%2C%20we%20developed%20a%20shape%20dataset%20named%20Lung3D%2C%20which%20includes%20the%203D%20models%20of%20800%20labeled%20pulmonary%20segments%20and%20their%20corresponding%20airways%2C%20arteries%2C%20veins%2C%20and%20intersegmental%20veins.%20We%20demonstrate%20that%20the%20proposed%20approach%20outperforms%20existing%20methods%2C%20providing%20a%20new%20perspective%20for%20pulmonary%20segment%20reconstruction.%20Code%20and%20data%20will%20be%20available%20at%20https%3A//github.com/HINTLab/ImPulSe.&entry.1838667208=http%3A//arxiv.org/abs/2505.08919v2&entry.124074799=Read"},
{"title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction", "author": "Ahmed Mujtaba and Gleb Radchenko and Marc Masana and Radu Prodan", "abstract": "Driver fatigue remains a leading cause of road accidents, with 24% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6% and mAP by 5% over video-level supervision, achieving 99.34% classification accuracy and 95.69% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.", "link": "http://arxiv.org/abs/2512.11446v2", "date": "2025-12-15", "relevancy": 2.0614, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5491}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction&body=Title%3A%20YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction%0AAuthor%3A%20Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Marc%20Masana%20and%20Radu%20Prodan%0AAbstract%3A%20Driver%20fatigue%20remains%20a%20leading%20cause%20of%20road%20accidents%2C%20with%2024%25%20of%20crashes%20involving%20drowsy%20drivers.%20While%20yawning%20serves%20as%20an%20early%20behavioral%20indicator%20of%20fatigue%2C%20existing%20machine%20learning%20approaches%20face%20significant%20challenges%20due%20to%20video-annotated%20datasets%20that%20introduce%20systematic%20noise%20from%20coarse%20temporal%20annotations.%20We%20develop%20a%20semi-automated%20labeling%20pipeline%20with%20human-in-the-loop%20verification%2C%20which%20we%20apply%20to%20YawDD%2C%20enabling%20more%20accurate%20model%20training.%20Training%20the%20established%20MNasNet%20classifier%20and%20YOLOv11%20detector%20architectures%20on%20YawDD%2B%20improves%20frame%20accuracy%20by%20up%20to%206%25%20and%20mAP%20by%205%25%20over%20video-level%20supervision%2C%20achieving%2099.34%25%20classification%20accuracy%20and%2095.69%25%20detection%20mAP.%20The%20resulting%20approach%20deliver%20up%20to%2059.8%20FPS%20on%20edge%20AI%20hardware%20%28NVIDIA%20Jetson%20Nano%29%2C%20confirming%20that%20enhanced%20data%20quality%20alone%20supports%20on-device%20yawning%20monitoring%20without%20server-side%20computation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYawDD%252B%253A%2520Frame-level%2520Annotations%2520for%2520Accurate%2520Yawn%2520Prediction%26entry.906535625%3DAhmed%2520Mujtaba%2520and%2520Gleb%2520Radchenko%2520and%2520Marc%2520Masana%2520and%2520Radu%2520Prodan%26entry.1292438233%3DDriver%2520fatigue%2520remains%2520a%2520leading%2520cause%2520of%2520road%2520accidents%252C%2520with%252024%2525%2520of%2520crashes%2520involving%2520drowsy%2520drivers.%2520While%2520yawning%2520serves%2520as%2520an%2520early%2520behavioral%2520indicator%2520of%2520fatigue%252C%2520existing%2520machine%2520learning%2520approaches%2520face%2520significant%2520challenges%2520due%2520to%2520video-annotated%2520datasets%2520that%2520introduce%2520systematic%2520noise%2520from%2520coarse%2520temporal%2520annotations.%2520We%2520develop%2520a%2520semi-automated%2520labeling%2520pipeline%2520with%2520human-in-the-loop%2520verification%252C%2520which%2520we%2520apply%2520to%2520YawDD%252C%2520enabling%2520more%2520accurate%2520model%2520training.%2520Training%2520the%2520established%2520MNasNet%2520classifier%2520and%2520YOLOv11%2520detector%2520architectures%2520on%2520YawDD%252B%2520improves%2520frame%2520accuracy%2520by%2520up%2520to%25206%2525%2520and%2520mAP%2520by%25205%2525%2520over%2520video-level%2520supervision%252C%2520achieving%252099.34%2525%2520classification%2520accuracy%2520and%252095.69%2525%2520detection%2520mAP.%2520The%2520resulting%2520approach%2520deliver%2520up%2520to%252059.8%2520FPS%2520on%2520edge%2520AI%2520hardware%2520%2528NVIDIA%2520Jetson%2520Nano%2529%252C%2520confirming%2520that%2520enhanced%2520data%2520quality%2520alone%2520supports%2520on-device%2520yawning%2520monitoring%2520without%2520server-side%2520computation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction&entry.906535625=Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Marc%20Masana%20and%20Radu%20Prodan&entry.1292438233=Driver%20fatigue%20remains%20a%20leading%20cause%20of%20road%20accidents%2C%20with%2024%25%20of%20crashes%20involving%20drowsy%20drivers.%20While%20yawning%20serves%20as%20an%20early%20behavioral%20indicator%20of%20fatigue%2C%20existing%20machine%20learning%20approaches%20face%20significant%20challenges%20due%20to%20video-annotated%20datasets%20that%20introduce%20systematic%20noise%20from%20coarse%20temporal%20annotations.%20We%20develop%20a%20semi-automated%20labeling%20pipeline%20with%20human-in-the-loop%20verification%2C%20which%20we%20apply%20to%20YawDD%2C%20enabling%20more%20accurate%20model%20training.%20Training%20the%20established%20MNasNet%20classifier%20and%20YOLOv11%20detector%20architectures%20on%20YawDD%2B%20improves%20frame%20accuracy%20by%20up%20to%206%25%20and%20mAP%20by%205%25%20over%20video-level%20supervision%2C%20achieving%2099.34%25%20classification%20accuracy%20and%2095.69%25%20detection%20mAP.%20The%20resulting%20approach%20deliver%20up%20to%2059.8%20FPS%20on%20edge%20AI%20hardware%20%28NVIDIA%20Jetson%20Nano%29%2C%20confirming%20that%20enhanced%20data%20quality%20alone%20supports%20on-device%20yawning%20monitoring%20without%20server-side%20computation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11446v2&entry.124074799=Read"},
{"title": "SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning", "author": "Junchao Zhu and Ruining Deng and Junlin Guo and Tianyuan Yao and Chongyu Qu and Juming Xiong and Siqi Lu and Zhengyi Lu and Yanfan Zhu and Marilyn Lionts and Yuechen Yang and Yalin Zheng and Yu Wang and Shilin Zhao and Haichun Yang and Yuankai Huo", "abstract": "Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST", "link": "http://arxiv.org/abs/2512.13635v1", "date": "2025-12-15", "relevancy": 2.0568, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.531}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5035}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCR2-ST%3A%20Combine%20Single%20Cell%20with%20Spatial%20Transcriptomics%20for%20Efficient%20Active%20Sampling%20via%20Reinforcement%20Learning&body=Title%3A%20SCR2-ST%3A%20Combine%20Single%20Cell%20with%20Spatial%20Transcriptomics%20for%20Efficient%20Active%20Sampling%20via%20Reinforcement%20Learning%0AAuthor%3A%20Junchao%20Zhu%20and%20Ruining%20Deng%20and%20Junlin%20Guo%20and%20Tianyuan%20Yao%20and%20Chongyu%20Qu%20and%20Juming%20Xiong%20and%20Siqi%20Lu%20and%20Zhengyi%20Lu%20and%20Yanfan%20Zhu%20and%20Marilyn%20Lionts%20and%20Yuechen%20Yang%20and%20Yalin%20Zheng%20and%20Yu%20Wang%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20Spatial%20transcriptomics%20%28ST%29%20is%20an%20emerging%20technology%20that%20enables%20researchers%20to%20investigate%20the%20molecular%20relationships%20underlying%20tissue%20morphology.%20However%2C%20acquiring%20ST%20data%20remains%20prohibitively%20expensive%2C%20and%20traditional%20fixed-grid%20sampling%20strategies%20lead%20to%20redundant%20measurements%20of%20morphologically%20similar%20or%20biologically%20uninformative%20regions%2C%20thus%20resulting%20in%20scarce%20data%20that%20constrain%20current%20methods.%20The%20well-established%20single-cell%20sequencing%20field%2C%20however%2C%20could%20provide%20rich%20biological%20data%20as%20an%20effective%20auxiliary%20source%20to%20mitigate%20this%20limitation.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20SCR2-ST%2C%20a%20unified%20framework%20that%20leverages%20single-cell%20prior%20knowledge%20to%20guide%20efficient%20data%20acquisition%20and%20accurate%20expression%20prediction.%20SCR2-ST%20integrates%20a%20single-cell%20guided%20reinforcement%20learning-based%20%28SCRL%29%20active%20sampling%20and%20a%20hybrid%20regression-retrieval%20prediction%20network%20SCR2Net.%20SCRL%20combines%20single-cell%20foundation%20model%20embeddings%20with%20spatial%20density%20information%20to%20construct%20biologically%20grounded%20reward%20signals%2C%20enabling%20selective%20acquisition%20of%20informative%20tissue%20regions%20under%20constrained%20sequencing%20budgets.%20SCR2Net%20then%20leverages%20the%20actively%20sampled%20data%20through%20a%20hybrid%20architecture%20combining%20regression-based%20modeling%20with%20retrieval-augmented%20inference%2C%20where%20a%20majority%20cell-type%20filtering%20mechanism%20suppresses%20noisy%20matches%20and%20retrieved%20expression%20profiles%20serve%20as%20soft%20labels%20for%20auxiliary%20supervision.%20We%20evaluated%20SCR2-ST%20on%20three%20public%20ST%20datasets%2C%20demonstrating%20SOTA%20performance%20in%20both%20sampling%20efficiency%20and%20prediction%20accuracy%2C%20particularly%20under%20low-budget%20scenarios.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/hrlblab/SCR2ST%0ALink%3A%20http%3A//arxiv.org/abs/2512.13635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCR2-ST%253A%2520Combine%2520Single%2520Cell%2520with%2520Spatial%2520Transcriptomics%2520for%2520Efficient%2520Active%2520Sampling%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DJunchao%2520Zhu%2520and%2520Ruining%2520Deng%2520and%2520Junlin%2520Guo%2520and%2520Tianyuan%2520Yao%2520and%2520Chongyu%2520Qu%2520and%2520Juming%2520Xiong%2520and%2520Siqi%2520Lu%2520and%2520Zhengyi%2520Lu%2520and%2520Yanfan%2520Zhu%2520and%2520Marilyn%2520Lionts%2520and%2520Yuechen%2520Yang%2520and%2520Yalin%2520Zheng%2520and%2520Yu%2520Wang%2520and%2520Shilin%2520Zhao%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3DSpatial%2520transcriptomics%2520%2528ST%2529%2520is%2520an%2520emerging%2520technology%2520that%2520enables%2520researchers%2520to%2520investigate%2520the%2520molecular%2520relationships%2520underlying%2520tissue%2520morphology.%2520However%252C%2520acquiring%2520ST%2520data%2520remains%2520prohibitively%2520expensive%252C%2520and%2520traditional%2520fixed-grid%2520sampling%2520strategies%2520lead%2520to%2520redundant%2520measurements%2520of%2520morphologically%2520similar%2520or%2520biologically%2520uninformative%2520regions%252C%2520thus%2520resulting%2520in%2520scarce%2520data%2520that%2520constrain%2520current%2520methods.%2520The%2520well-established%2520single-cell%2520sequencing%2520field%252C%2520however%252C%2520could%2520provide%2520rich%2520biological%2520data%2520as%2520an%2520effective%2520auxiliary%2520source%2520to%2520mitigate%2520this%2520limitation.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520introduce%2520SCR2-ST%252C%2520a%2520unified%2520framework%2520that%2520leverages%2520single-cell%2520prior%2520knowledge%2520to%2520guide%2520efficient%2520data%2520acquisition%2520and%2520accurate%2520expression%2520prediction.%2520SCR2-ST%2520integrates%2520a%2520single-cell%2520guided%2520reinforcement%2520learning-based%2520%2528SCRL%2529%2520active%2520sampling%2520and%2520a%2520hybrid%2520regression-retrieval%2520prediction%2520network%2520SCR2Net.%2520SCRL%2520combines%2520single-cell%2520foundation%2520model%2520embeddings%2520with%2520spatial%2520density%2520information%2520to%2520construct%2520biologically%2520grounded%2520reward%2520signals%252C%2520enabling%2520selective%2520acquisition%2520of%2520informative%2520tissue%2520regions%2520under%2520constrained%2520sequencing%2520budgets.%2520SCR2Net%2520then%2520leverages%2520the%2520actively%2520sampled%2520data%2520through%2520a%2520hybrid%2520architecture%2520combining%2520regression-based%2520modeling%2520with%2520retrieval-augmented%2520inference%252C%2520where%2520a%2520majority%2520cell-type%2520filtering%2520mechanism%2520suppresses%2520noisy%2520matches%2520and%2520retrieved%2520expression%2520profiles%2520serve%2520as%2520soft%2520labels%2520for%2520auxiliary%2520supervision.%2520We%2520evaluated%2520SCR2-ST%2520on%2520three%2520public%2520ST%2520datasets%252C%2520demonstrating%2520SOTA%2520performance%2520in%2520both%2520sampling%2520efficiency%2520and%2520prediction%2520accuracy%252C%2520particularly%2520under%2520low-budget%2520scenarios.%2520Code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/hrlblab/SCR2ST%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCR2-ST%3A%20Combine%20Single%20Cell%20with%20Spatial%20Transcriptomics%20for%20Efficient%20Active%20Sampling%20via%20Reinforcement%20Learning&entry.906535625=Junchao%20Zhu%20and%20Ruining%20Deng%20and%20Junlin%20Guo%20and%20Tianyuan%20Yao%20and%20Chongyu%20Qu%20and%20Juming%20Xiong%20and%20Siqi%20Lu%20and%20Zhengyi%20Lu%20and%20Yanfan%20Zhu%20and%20Marilyn%20Lionts%20and%20Yuechen%20Yang%20and%20Yalin%20Zheng%20and%20Yu%20Wang%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=Spatial%20transcriptomics%20%28ST%29%20is%20an%20emerging%20technology%20that%20enables%20researchers%20to%20investigate%20the%20molecular%20relationships%20underlying%20tissue%20morphology.%20However%2C%20acquiring%20ST%20data%20remains%20prohibitively%20expensive%2C%20and%20traditional%20fixed-grid%20sampling%20strategies%20lead%20to%20redundant%20measurements%20of%20morphologically%20similar%20or%20biologically%20uninformative%20regions%2C%20thus%20resulting%20in%20scarce%20data%20that%20constrain%20current%20methods.%20The%20well-established%20single-cell%20sequencing%20field%2C%20however%2C%20could%20provide%20rich%20biological%20data%20as%20an%20effective%20auxiliary%20source%20to%20mitigate%20this%20limitation.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20SCR2-ST%2C%20a%20unified%20framework%20that%20leverages%20single-cell%20prior%20knowledge%20to%20guide%20efficient%20data%20acquisition%20and%20accurate%20expression%20prediction.%20SCR2-ST%20integrates%20a%20single-cell%20guided%20reinforcement%20learning-based%20%28SCRL%29%20active%20sampling%20and%20a%20hybrid%20regression-retrieval%20prediction%20network%20SCR2Net.%20SCRL%20combines%20single-cell%20foundation%20model%20embeddings%20with%20spatial%20density%20information%20to%20construct%20biologically%20grounded%20reward%20signals%2C%20enabling%20selective%20acquisition%20of%20informative%20tissue%20regions%20under%20constrained%20sequencing%20budgets.%20SCR2Net%20then%20leverages%20the%20actively%20sampled%20data%20through%20a%20hybrid%20architecture%20combining%20regression-based%20modeling%20with%20retrieval-augmented%20inference%2C%20where%20a%20majority%20cell-type%20filtering%20mechanism%20suppresses%20noisy%20matches%20and%20retrieved%20expression%20profiles%20serve%20as%20soft%20labels%20for%20auxiliary%20supervision.%20We%20evaluated%20SCR2-ST%20on%20three%20public%20ST%20datasets%2C%20demonstrating%20SOTA%20performance%20in%20both%20sampling%20efficiency%20and%20prediction%20accuracy%2C%20particularly%20under%20low-budget%20scenarios.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/hrlblab/SCR2ST&entry.1838667208=http%3A//arxiv.org/abs/2512.13635v1&entry.124074799=Read"},
{"title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data", "author": "Melvin Barbaux", "abstract": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.", "link": "http://arxiv.org/abs/2512.13228v1", "date": "2025-12-15", "relevancy": 1.9183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ModSSC%3A%20A%20Modular%20Framework%20for%20Semi-Supervised%20Classification%20on%20Heterogeneous%20Data&body=Title%3A%20ModSSC%3A%20A%20Modular%20Framework%20for%20Semi-Supervised%20Classification%20on%20Heterogeneous%20Data%0AAuthor%3A%20Melvin%20Barbaux%0AAbstract%3A%20Semi-supervised%20classification%20leverages%20both%20labeled%20and%20unlabeled%20data%20to%20improve%20predictive%20performance%2C%20but%20existing%20software%20support%20is%20fragmented%20across%20methods%20and%20modalities.%20We%20introduce%20ModSSC%2C%20an%20open%20source%20Python%20framework%20that%20unifies%20inductive%20and%20transductive%20semi-supervised%20classification%20in%20a%20modular%20code%20base.%20ModSSC%20implements%20a%20broad%20range%20of%20classical%20and%20recent%20algorithms%2C%20provides%20loaders%20for%20tabular%2C%20image%2C%20text%2C%20audio%20and%20graph%20datasets%2C%20and%20exposes%20a%20single%20configuration%20interface%20for%20specifying%20datasets%2C%20models%20and%20evaluation%20protocols.%20It%20supports%20both%20lightweight%20classical%20methods%20on%20small%20datasets%20running%20on%20CPU%20and%20recent%20deep%20approaches%20that%20can%20exploit%20multiple%20GPUs%20within%20the%20same%20experimental%20framework.%20Experiments%20are%20described%20declaratively%20in%20YAML%2C%20which%20facilitates%20reproducing%20existing%20work%20and%20running%20large%20comparative%20studies.%20ModSSC%201.0.0%20is%20released%20under%20the%20MIT%20license%20with%20extensive%20documentation%20and%20tests%2C%20and%20is%20available%20at%20https%3A//github.com/ModSSC/ModSSC.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModSSC%253A%2520A%2520Modular%2520Framework%2520for%2520Semi-Supervised%2520Classification%2520on%2520Heterogeneous%2520Data%26entry.906535625%3DMelvin%2520Barbaux%26entry.1292438233%3DSemi-supervised%2520classification%2520leverages%2520both%2520labeled%2520and%2520unlabeled%2520data%2520to%2520improve%2520predictive%2520performance%252C%2520but%2520existing%2520software%2520support%2520is%2520fragmented%2520across%2520methods%2520and%2520modalities.%2520We%2520introduce%2520ModSSC%252C%2520an%2520open%2520source%2520Python%2520framework%2520that%2520unifies%2520inductive%2520and%2520transductive%2520semi-supervised%2520classification%2520in%2520a%2520modular%2520code%2520base.%2520ModSSC%2520implements%2520a%2520broad%2520range%2520of%2520classical%2520and%2520recent%2520algorithms%252C%2520provides%2520loaders%2520for%2520tabular%252C%2520image%252C%2520text%252C%2520audio%2520and%2520graph%2520datasets%252C%2520and%2520exposes%2520a%2520single%2520configuration%2520interface%2520for%2520specifying%2520datasets%252C%2520models%2520and%2520evaluation%2520protocols.%2520It%2520supports%2520both%2520lightweight%2520classical%2520methods%2520on%2520small%2520datasets%2520running%2520on%2520CPU%2520and%2520recent%2520deep%2520approaches%2520that%2520can%2520exploit%2520multiple%2520GPUs%2520within%2520the%2520same%2520experimental%2520framework.%2520Experiments%2520are%2520described%2520declaratively%2520in%2520YAML%252C%2520which%2520facilitates%2520reproducing%2520existing%2520work%2520and%2520running%2520large%2520comparative%2520studies.%2520ModSSC%25201.0.0%2520is%2520released%2520under%2520the%2520MIT%2520license%2520with%2520extensive%2520documentation%2520and%2520tests%252C%2520and%2520is%2520available%2520at%2520https%253A//github.com/ModSSC/ModSSC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModSSC%3A%20A%20Modular%20Framework%20for%20Semi-Supervised%20Classification%20on%20Heterogeneous%20Data&entry.906535625=Melvin%20Barbaux&entry.1292438233=Semi-supervised%20classification%20leverages%20both%20labeled%20and%20unlabeled%20data%20to%20improve%20predictive%20performance%2C%20but%20existing%20software%20support%20is%20fragmented%20across%20methods%20and%20modalities.%20We%20introduce%20ModSSC%2C%20an%20open%20source%20Python%20framework%20that%20unifies%20inductive%20and%20transductive%20semi-supervised%20classification%20in%20a%20modular%20code%20base.%20ModSSC%20implements%20a%20broad%20range%20of%20classical%20and%20recent%20algorithms%2C%20provides%20loaders%20for%20tabular%2C%20image%2C%20text%2C%20audio%20and%20graph%20datasets%2C%20and%20exposes%20a%20single%20configuration%20interface%20for%20specifying%20datasets%2C%20models%20and%20evaluation%20protocols.%20It%20supports%20both%20lightweight%20classical%20methods%20on%20small%20datasets%20running%20on%20CPU%20and%20recent%20deep%20approaches%20that%20can%20exploit%20multiple%20GPUs%20within%20the%20same%20experimental%20framework.%20Experiments%20are%20described%20declaratively%20in%20YAML%2C%20which%20facilitates%20reproducing%20existing%20work%20and%20running%20large%20comparative%20studies.%20ModSSC%201.0.0%20is%20released%20under%20the%20MIT%20license%20with%20extensive%20documentation%20and%20tests%2C%20and%20is%20available%20at%20https%3A//github.com/ModSSC/ModSSC.&entry.1838667208=http%3A//arxiv.org/abs/2512.13228v1&entry.124074799=Read"},
{"title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "author": "Ojas Pungalia and Rashi Upadhyay and Abhishek Mishra and Abhiram H and Tejasvi Alladi and Sujan Yenuganti and Dhruv Kumar", "abstract": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.", "link": "http://arxiv.org/abs/2512.13481v1", "date": "2025-12-15", "relevancy": 1.9019, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20neuralFOMO%3A%20Can%20LLMs%20Handle%20Being%20Second%20Best%3F%20Measuring%20Envy-Like%20Preferences%20in%20Multi-Agent%20Settings&body=Title%3A%20neuralFOMO%3A%20Can%20LLMs%20Handle%20Being%20Second%20Best%3F%20Measuring%20Envy-Like%20Preferences%20in%20Multi-Agent%20Settings%0AAuthor%3A%20Ojas%20Pungalia%20and%20Rashi%20Upadhyay%20and%20Abhishek%20Mishra%20and%20Abhiram%20H%20and%20Tejasvi%20Alladi%20and%20Sujan%20Yenuganti%20and%20Dhruv%20Kumar%0AAbstract%3A%20Envy%20is%20a%20common%20human%20behavior%20that%20shapes%20competitiveness%20and%20can%20alter%20outcomes%20in%20team%20settings.%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20act%20on%20behalf%20of%20humans%20in%20collaborative%20and%20competitive%20workflows%2C%20there%20is%20a%20pressing%20need%20to%20evaluate%20whether%20and%20under%20what%20conditions%20they%20exhibit%20envy-like%20preferences.%20In%20this%20paper%2C%20we%20test%20whether%20LLMs%20show%20envy-like%20behavior%20toward%20each%20other.%20We%20considered%20two%20scenarios%3A%20%281%29%20A%20point%20allocation%20game%20that%20tests%20whether%20a%20model%20tries%20to%20win%20over%20its%20peer.%20%282%29%20A%20workplace%20setting%20observing%20behaviour%20when%20recognition%20is%20unfair.%20Our%20findings%20reveal%20consistent%20evidence%20of%20envy-like%20patterns%20in%20certain%20LLMs%2C%20with%20large%20variation%20across%20models%20and%20contexts.%20For%20instance%2C%20GPT-5-mini%20and%20Claude-3.7-Sonnet%20show%20a%20clear%20tendency%20to%20pull%20down%20the%20peer%20model%20to%20equalize%20outcomes%2C%20whereas%20Mistral-Small-3.2-24B%20instead%20focuses%20on%20maximizing%20its%20own%20individual%20gains.%20These%20results%20highlight%20the%20need%20to%20consider%20competitive%20dispositions%20as%20a%20safety%20and%20design%20factor%20in%20LLM-based%20multi-agent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DneuralFOMO%253A%2520Can%2520LLMs%2520Handle%2520Being%2520Second%2520Best%253F%2520Measuring%2520Envy-Like%2520Preferences%2520in%2520Multi-Agent%2520Settings%26entry.906535625%3DOjas%2520Pungalia%2520and%2520Rashi%2520Upadhyay%2520and%2520Abhishek%2520Mishra%2520and%2520Abhiram%2520H%2520and%2520Tejasvi%2520Alladi%2520and%2520Sujan%2520Yenuganti%2520and%2520Dhruv%2520Kumar%26entry.1292438233%3DEnvy%2520is%2520a%2520common%2520human%2520behavior%2520that%2520shapes%2520competitiveness%2520and%2520can%2520alter%2520outcomes%2520in%2520team%2520settings.%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520act%2520on%2520behalf%2520of%2520humans%2520in%2520collaborative%2520and%2520competitive%2520workflows%252C%2520there%2520is%2520a%2520pressing%2520need%2520to%2520evaluate%2520whether%2520and%2520under%2520what%2520conditions%2520they%2520exhibit%2520envy-like%2520preferences.%2520In%2520this%2520paper%252C%2520we%2520test%2520whether%2520LLMs%2520show%2520envy-like%2520behavior%2520toward%2520each%2520other.%2520We%2520considered%2520two%2520scenarios%253A%2520%25281%2529%2520A%2520point%2520allocation%2520game%2520that%2520tests%2520whether%2520a%2520model%2520tries%2520to%2520win%2520over%2520its%2520peer.%2520%25282%2529%2520A%2520workplace%2520setting%2520observing%2520behaviour%2520when%2520recognition%2520is%2520unfair.%2520Our%2520findings%2520reveal%2520consistent%2520evidence%2520of%2520envy-like%2520patterns%2520in%2520certain%2520LLMs%252C%2520with%2520large%2520variation%2520across%2520models%2520and%2520contexts.%2520For%2520instance%252C%2520GPT-5-mini%2520and%2520Claude-3.7-Sonnet%2520show%2520a%2520clear%2520tendency%2520to%2520pull%2520down%2520the%2520peer%2520model%2520to%2520equalize%2520outcomes%252C%2520whereas%2520Mistral-Small-3.2-24B%2520instead%2520focuses%2520on%2520maximizing%2520its%2520own%2520individual%2520gains.%2520These%2520results%2520highlight%2520the%2520need%2520to%2520consider%2520competitive%2520dispositions%2520as%2520a%2520safety%2520and%2520design%2520factor%2520in%2520LLM-based%2520multi-agent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=neuralFOMO%3A%20Can%20LLMs%20Handle%20Being%20Second%20Best%3F%20Measuring%20Envy-Like%20Preferences%20in%20Multi-Agent%20Settings&entry.906535625=Ojas%20Pungalia%20and%20Rashi%20Upadhyay%20and%20Abhishek%20Mishra%20and%20Abhiram%20H%20and%20Tejasvi%20Alladi%20and%20Sujan%20Yenuganti%20and%20Dhruv%20Kumar&entry.1292438233=Envy%20is%20a%20common%20human%20behavior%20that%20shapes%20competitiveness%20and%20can%20alter%20outcomes%20in%20team%20settings.%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20act%20on%20behalf%20of%20humans%20in%20collaborative%20and%20competitive%20workflows%2C%20there%20is%20a%20pressing%20need%20to%20evaluate%20whether%20and%20under%20what%20conditions%20they%20exhibit%20envy-like%20preferences.%20In%20this%20paper%2C%20we%20test%20whether%20LLMs%20show%20envy-like%20behavior%20toward%20each%20other.%20We%20considered%20two%20scenarios%3A%20%281%29%20A%20point%20allocation%20game%20that%20tests%20whether%20a%20model%20tries%20to%20win%20over%20its%20peer.%20%282%29%20A%20workplace%20setting%20observing%20behaviour%20when%20recognition%20is%20unfair.%20Our%20findings%20reveal%20consistent%20evidence%20of%20envy-like%20patterns%20in%20certain%20LLMs%2C%20with%20large%20variation%20across%20models%20and%20contexts.%20For%20instance%2C%20GPT-5-mini%20and%20Claude-3.7-Sonnet%20show%20a%20clear%20tendency%20to%20pull%20down%20the%20peer%20model%20to%20equalize%20outcomes%2C%20whereas%20Mistral-Small-3.2-24B%20instead%20focuses%20on%20maximizing%20its%20own%20individual%20gains.%20These%20results%20highlight%20the%20need%20to%20consider%20competitive%20dispositions%20as%20a%20safety%20and%20design%20factor%20in%20LLM-based%20multi-agent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.13481v1&entry.124074799=Read"},
{"title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs", "author": "Si Qi Goh and Yongsen Zheng and Ziyao Liu and Sami Hormi and Kwok-Yan Lam", "abstract": "Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.", "link": "http://arxiv.org/abs/2512.13337v1", "date": "2025-12-15", "relevancy": 1.9811, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.502}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FROC%3A%20A%20Unified%20Framework%20with%20Risk-Optimized%20Control%20for%20Machine%20Unlearning%20in%20LLMs&body=Title%3A%20FROC%3A%20A%20Unified%20Framework%20with%20Risk-Optimized%20Control%20for%20Machine%20Unlearning%20in%20LLMs%0AAuthor%3A%20Si%20Qi%20Goh%20and%20Yongsen%20Zheng%20and%20Ziyao%20Liu%20and%20Sami%20Hormi%20and%20Kwok-Yan%20Lam%0AAbstract%3A%20Machine%20unlearning%20%28MU%29%20seeks%20to%20eliminate%20the%20influence%20of%20specific%20training%20examples%20from%20deployed%20models.%20As%20large%20language%20models%20%28LLMs%29%20become%20widely%20used%2C%20managing%20risks%20arising%20from%20insufficient%20forgetting%20or%20utility%20loss%20is%20increasingly%20crucial.%20Current%20MU%20techniques%20lack%20effective%20mechanisms%20for%20evaluating%20and%20controlling%20these%20risks%2C%20hindering%20the%20selection%20of%20strategies%20that%20appropriately%20balance%20safety%20and%20utility%2C%20and%20raising%20trust%20concerns%20surrounding%20the%20%22right%20to%20be%20forgotten.%22%20To%20address%20these%20issues%2C%20we%20propose%20FROC%2C%20a%20unified%20framework%20with%20Risk-Optimized%20Control%20for%20machine%20unlearning%20in%20LLMs.%20FROC%20is%20built%20around%20a%20conformal-style%20risk-control%20formulation%20that%20expresses%20a%20user-specified%20risk%20budget%20on%20unlearning%20behavior.%20This%20probability-based%20constraint%20enables%20FROC%20to%20compare%20MU%20strategies%2C%20identify%20feasible%20operating%20regions%2C%20and%20guide%20hyperparameter%20selection%20according%20to%20desired%20trade-offs%20between%20forgetting%20sufficiency%20and%20utility%20preservation.%20To%20operationalize%20this%20constraint%2C%20FROC%20introduces%20a%20smoothly%20varying%20continuous%20risk%20model%20that%20aggregates%20forgetting%20deficiency%20and%20utility%20degradation%20into%20a%20single%20configuration-level%20score.%20Building%20on%20conformal%20risk%20analysis%2C%20FROC%20computes%20%281%29%20the%20Conformal%20Unlearning%20Risk%20%28CUR%29%2C%20a%20data-driven%20estimated%20value%20on%20the%20probability%20that%20forgotten%20samples%20continue%20to%20influence%20model%20predictions%2C%20and%20%282%29%20risk-controlled%20configuration%20sets%2C%20which%20identify%20unlearning%20hyperparameters%20that%20are%20valid%20under%20the%20specified%20risk%20budget.%20Experiments%20across%20multiple%20LLM%20MU%20methods%20demonstrate%20that%20FROC%20produces%20stable%2C%20interpretable%20risk%20landscapes%20and%20reveals%20consistent%20relationships%20between%20unlearning%20configurations%2C%20semantic%20shift%2C%20and%20utility%20impact.%20FROC%20reframes%20MU%20as%20a%20controllable%2C%20risk-aware%20process%20and%20offers%20a%20practical%20foundation%20for%20managing%20unlearning%20behavior%20in%20large-scale%20LLM%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFROC%253A%2520A%2520Unified%2520Framework%2520with%2520Risk-Optimized%2520Control%2520for%2520Machine%2520Unlearning%2520in%2520LLMs%26entry.906535625%3DSi%2520Qi%2520Goh%2520and%2520Yongsen%2520Zheng%2520and%2520Ziyao%2520Liu%2520and%2520Sami%2520Hormi%2520and%2520Kwok-Yan%2520Lam%26entry.1292438233%3DMachine%2520unlearning%2520%2528MU%2529%2520seeks%2520to%2520eliminate%2520the%2520influence%2520of%2520specific%2520training%2520examples%2520from%2520deployed%2520models.%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520widely%2520used%252C%2520managing%2520risks%2520arising%2520from%2520insufficient%2520forgetting%2520or%2520utility%2520loss%2520is%2520increasingly%2520crucial.%2520Current%2520MU%2520techniques%2520lack%2520effective%2520mechanisms%2520for%2520evaluating%2520and%2520controlling%2520these%2520risks%252C%2520hindering%2520the%2520selection%2520of%2520strategies%2520that%2520appropriately%2520balance%2520safety%2520and%2520utility%252C%2520and%2520raising%2520trust%2520concerns%2520surrounding%2520the%2520%2522right%2520to%2520be%2520forgotten.%2522%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FROC%252C%2520a%2520unified%2520framework%2520with%2520Risk-Optimized%2520Control%2520for%2520machine%2520unlearning%2520in%2520LLMs.%2520FROC%2520is%2520built%2520around%2520a%2520conformal-style%2520risk-control%2520formulation%2520that%2520expresses%2520a%2520user-specified%2520risk%2520budget%2520on%2520unlearning%2520behavior.%2520This%2520probability-based%2520constraint%2520enables%2520FROC%2520to%2520compare%2520MU%2520strategies%252C%2520identify%2520feasible%2520operating%2520regions%252C%2520and%2520guide%2520hyperparameter%2520selection%2520according%2520to%2520desired%2520trade-offs%2520between%2520forgetting%2520sufficiency%2520and%2520utility%2520preservation.%2520To%2520operationalize%2520this%2520constraint%252C%2520FROC%2520introduces%2520a%2520smoothly%2520varying%2520continuous%2520risk%2520model%2520that%2520aggregates%2520forgetting%2520deficiency%2520and%2520utility%2520degradation%2520into%2520a%2520single%2520configuration-level%2520score.%2520Building%2520on%2520conformal%2520risk%2520analysis%252C%2520FROC%2520computes%2520%25281%2529%2520the%2520Conformal%2520Unlearning%2520Risk%2520%2528CUR%2529%252C%2520a%2520data-driven%2520estimated%2520value%2520on%2520the%2520probability%2520that%2520forgotten%2520samples%2520continue%2520to%2520influence%2520model%2520predictions%252C%2520and%2520%25282%2529%2520risk-controlled%2520configuration%2520sets%252C%2520which%2520identify%2520unlearning%2520hyperparameters%2520that%2520are%2520valid%2520under%2520the%2520specified%2520risk%2520budget.%2520Experiments%2520across%2520multiple%2520LLM%2520MU%2520methods%2520demonstrate%2520that%2520FROC%2520produces%2520stable%252C%2520interpretable%2520risk%2520landscapes%2520and%2520reveals%2520consistent%2520relationships%2520between%2520unlearning%2520configurations%252C%2520semantic%2520shift%252C%2520and%2520utility%2520impact.%2520FROC%2520reframes%2520MU%2520as%2520a%2520controllable%252C%2520risk-aware%2520process%2520and%2520offers%2520a%2520practical%2520foundation%2520for%2520managing%2520unlearning%2520behavior%2520in%2520large-scale%2520LLM%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FROC%3A%20A%20Unified%20Framework%20with%20Risk-Optimized%20Control%20for%20Machine%20Unlearning%20in%20LLMs&entry.906535625=Si%20Qi%20Goh%20and%20Yongsen%20Zheng%20and%20Ziyao%20Liu%20and%20Sami%20Hormi%20and%20Kwok-Yan%20Lam&entry.1292438233=Machine%20unlearning%20%28MU%29%20seeks%20to%20eliminate%20the%20influence%20of%20specific%20training%20examples%20from%20deployed%20models.%20As%20large%20language%20models%20%28LLMs%29%20become%20widely%20used%2C%20managing%20risks%20arising%20from%20insufficient%20forgetting%20or%20utility%20loss%20is%20increasingly%20crucial.%20Current%20MU%20techniques%20lack%20effective%20mechanisms%20for%20evaluating%20and%20controlling%20these%20risks%2C%20hindering%20the%20selection%20of%20strategies%20that%20appropriately%20balance%20safety%20and%20utility%2C%20and%20raising%20trust%20concerns%20surrounding%20the%20%22right%20to%20be%20forgotten.%22%20To%20address%20these%20issues%2C%20we%20propose%20FROC%2C%20a%20unified%20framework%20with%20Risk-Optimized%20Control%20for%20machine%20unlearning%20in%20LLMs.%20FROC%20is%20built%20around%20a%20conformal-style%20risk-control%20formulation%20that%20expresses%20a%20user-specified%20risk%20budget%20on%20unlearning%20behavior.%20This%20probability-based%20constraint%20enables%20FROC%20to%20compare%20MU%20strategies%2C%20identify%20feasible%20operating%20regions%2C%20and%20guide%20hyperparameter%20selection%20according%20to%20desired%20trade-offs%20between%20forgetting%20sufficiency%20and%20utility%20preservation.%20To%20operationalize%20this%20constraint%2C%20FROC%20introduces%20a%20smoothly%20varying%20continuous%20risk%20model%20that%20aggregates%20forgetting%20deficiency%20and%20utility%20degradation%20into%20a%20single%20configuration-level%20score.%20Building%20on%20conformal%20risk%20analysis%2C%20FROC%20computes%20%281%29%20the%20Conformal%20Unlearning%20Risk%20%28CUR%29%2C%20a%20data-driven%20estimated%20value%20on%20the%20probability%20that%20forgotten%20samples%20continue%20to%20influence%20model%20predictions%2C%20and%20%282%29%20risk-controlled%20configuration%20sets%2C%20which%20identify%20unlearning%20hyperparameters%20that%20are%20valid%20under%20the%20specified%20risk%20budget.%20Experiments%20across%20multiple%20LLM%20MU%20methods%20demonstrate%20that%20FROC%20produces%20stable%2C%20interpretable%20risk%20landscapes%20and%20reveals%20consistent%20relationships%20between%20unlearning%20configurations%2C%20semantic%20shift%2C%20and%20utility%20impact.%20FROC%20reframes%20MU%20as%20a%20controllable%2C%20risk-aware%20process%20and%20offers%20a%20practical%20foundation%20for%20managing%20unlearning%20behavior%20in%20large-scale%20LLM%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2512.13337v1&entry.124074799=Read"},
{"title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion", "author": "Sangmin Hong and Suyoung Lee and Kyoung Mu Lee", "abstract": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.", "link": "http://arxiv.org/abs/2512.13147v1", "date": "2025-12-15", "relevancy": 1.7009, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.57}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5666}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StarryGazer%3A%20Leveraging%20Monocular%20Depth%20Estimation%20Models%20for%20Domain-Agnostic%20Single%20Depth%20Image%20Completion&body=Title%3A%20StarryGazer%3A%20Leveraging%20Monocular%20Depth%20Estimation%20Models%20for%20Domain-Agnostic%20Single%20Depth%20Image%20Completion%0AAuthor%3A%20Sangmin%20Hong%20and%20Suyoung%20Lee%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20The%20problem%20of%20depth%20completion%20involves%20predicting%20a%20dense%20depth%20image%20from%20a%20single%20sparse%20depth%20map%20and%20an%20RGB%20image.%20Unsupervised%20depth%20completion%20methods%20have%20been%20proposed%20for%20various%20datasets%20where%20ground%20truth%20depth%20data%20is%20unavailable%20and%20supervised%20methods%20cannot%20be%20applied.%20However%2C%20these%20models%20require%20auxiliary%20data%20to%20estimate%20depth%20values%2C%20which%20is%20far%20from%20real%20scenarios.%20Monocular%20depth%20estimation%20%28MDE%29%20models%20can%20produce%20a%20plausible%20relative%20depth%20map%20from%20a%20single%20image%2C%20but%20there%20is%20no%20work%20to%20properly%20combine%20the%20sparse%20depth%20map%20with%20MDE%20for%20depth%20completion%3B%20a%20simple%20affine%20transformation%20to%20the%20depth%20map%20will%20yield%20a%20high%20error%20since%20MDE%20are%20inaccurate%20at%20estimating%20depth%20difference%20between%20objects.%20We%20introduce%20StarryGazer%2C%20a%20domain-agnostic%20framework%20that%20predicts%20dense%20depth%20images%20from%20a%20single%20sparse%20depth%20image%20and%20an%20RGB%20image%20without%20relying%20on%20ground-truth%20depth%20by%20leveraging%20the%20power%20of%20large%20MDE%20models.%20First%2C%20we%20employ%20a%20pre-trained%20MDE%20model%20to%20produce%20relative%20depth%20images.%20These%20images%20are%20segmented%20and%20randomly%20rescaled%20to%20form%20synthetic%20pairs%20for%20dense%20pseudo-ground%20truth%20and%20corresponding%20sparse%20depths.%20A%20refinement%20network%20is%20trained%20with%20the%20synthetic%20pairs%2C%20incorporating%20the%20relative%20depth%20maps%20and%20RGB%20images%20to%20improve%20the%20model%27s%20accuracy%20and%20robustness.%20StarryGazer%20shows%20superior%20results%20over%20existing%20unsupervised%20methods%20and%20transformed%20MDE%20results%20on%20various%20datasets%2C%20demonstrating%20that%20our%20framework%20exploits%20the%20power%20of%20MDE%20models%20while%20appropriately%20fixing%20errors%20using%20sparse%20depth%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStarryGazer%253A%2520Leveraging%2520Monocular%2520Depth%2520Estimation%2520Models%2520for%2520Domain-Agnostic%2520Single%2520Depth%2520Image%2520Completion%26entry.906535625%3DSangmin%2520Hong%2520and%2520Suyoung%2520Lee%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3DThe%2520problem%2520of%2520depth%2520completion%2520involves%2520predicting%2520a%2520dense%2520depth%2520image%2520from%2520a%2520single%2520sparse%2520depth%2520map%2520and%2520an%2520RGB%2520image.%2520Unsupervised%2520depth%2520completion%2520methods%2520have%2520been%2520proposed%2520for%2520various%2520datasets%2520where%2520ground%2520truth%2520depth%2520data%2520is%2520unavailable%2520and%2520supervised%2520methods%2520cannot%2520be%2520applied.%2520However%252C%2520these%2520models%2520require%2520auxiliary%2520data%2520to%2520estimate%2520depth%2520values%252C%2520which%2520is%2520far%2520from%2520real%2520scenarios.%2520Monocular%2520depth%2520estimation%2520%2528MDE%2529%2520models%2520can%2520produce%2520a%2520plausible%2520relative%2520depth%2520map%2520from%2520a%2520single%2520image%252C%2520but%2520there%2520is%2520no%2520work%2520to%2520properly%2520combine%2520the%2520sparse%2520depth%2520map%2520with%2520MDE%2520for%2520depth%2520completion%253B%2520a%2520simple%2520affine%2520transformation%2520to%2520the%2520depth%2520map%2520will%2520yield%2520a%2520high%2520error%2520since%2520MDE%2520are%2520inaccurate%2520at%2520estimating%2520depth%2520difference%2520between%2520objects.%2520We%2520introduce%2520StarryGazer%252C%2520a%2520domain-agnostic%2520framework%2520that%2520predicts%2520dense%2520depth%2520images%2520from%2520a%2520single%2520sparse%2520depth%2520image%2520and%2520an%2520RGB%2520image%2520without%2520relying%2520on%2520ground-truth%2520depth%2520by%2520leveraging%2520the%2520power%2520of%2520large%2520MDE%2520models.%2520First%252C%2520we%2520employ%2520a%2520pre-trained%2520MDE%2520model%2520to%2520produce%2520relative%2520depth%2520images.%2520These%2520images%2520are%2520segmented%2520and%2520randomly%2520rescaled%2520to%2520form%2520synthetic%2520pairs%2520for%2520dense%2520pseudo-ground%2520truth%2520and%2520corresponding%2520sparse%2520depths.%2520A%2520refinement%2520network%2520is%2520trained%2520with%2520the%2520synthetic%2520pairs%252C%2520incorporating%2520the%2520relative%2520depth%2520maps%2520and%2520RGB%2520images%2520to%2520improve%2520the%2520model%2527s%2520accuracy%2520and%2520robustness.%2520StarryGazer%2520shows%2520superior%2520results%2520over%2520existing%2520unsupervised%2520methods%2520and%2520transformed%2520MDE%2520results%2520on%2520various%2520datasets%252C%2520demonstrating%2520that%2520our%2520framework%2520exploits%2520the%2520power%2520of%2520MDE%2520models%2520while%2520appropriately%2520fixing%2520errors%2520using%2520sparse%2520depth%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StarryGazer%3A%20Leveraging%20Monocular%20Depth%20Estimation%20Models%20for%20Domain-Agnostic%20Single%20Depth%20Image%20Completion&entry.906535625=Sangmin%20Hong%20and%20Suyoung%20Lee%20and%20Kyoung%20Mu%20Lee&entry.1292438233=The%20problem%20of%20depth%20completion%20involves%20predicting%20a%20dense%20depth%20image%20from%20a%20single%20sparse%20depth%20map%20and%20an%20RGB%20image.%20Unsupervised%20depth%20completion%20methods%20have%20been%20proposed%20for%20various%20datasets%20where%20ground%20truth%20depth%20data%20is%20unavailable%20and%20supervised%20methods%20cannot%20be%20applied.%20However%2C%20these%20models%20require%20auxiliary%20data%20to%20estimate%20depth%20values%2C%20which%20is%20far%20from%20real%20scenarios.%20Monocular%20depth%20estimation%20%28MDE%29%20models%20can%20produce%20a%20plausible%20relative%20depth%20map%20from%20a%20single%20image%2C%20but%20there%20is%20no%20work%20to%20properly%20combine%20the%20sparse%20depth%20map%20with%20MDE%20for%20depth%20completion%3B%20a%20simple%20affine%20transformation%20to%20the%20depth%20map%20will%20yield%20a%20high%20error%20since%20MDE%20are%20inaccurate%20at%20estimating%20depth%20difference%20between%20objects.%20We%20introduce%20StarryGazer%2C%20a%20domain-agnostic%20framework%20that%20predicts%20dense%20depth%20images%20from%20a%20single%20sparse%20depth%20image%20and%20an%20RGB%20image%20without%20relying%20on%20ground-truth%20depth%20by%20leveraging%20the%20power%20of%20large%20MDE%20models.%20First%2C%20we%20employ%20a%20pre-trained%20MDE%20model%20to%20produce%20relative%20depth%20images.%20These%20images%20are%20segmented%20and%20randomly%20rescaled%20to%20form%20synthetic%20pairs%20for%20dense%20pseudo-ground%20truth%20and%20corresponding%20sparse%20depths.%20A%20refinement%20network%20is%20trained%20with%20the%20synthetic%20pairs%2C%20incorporating%20the%20relative%20depth%20maps%20and%20RGB%20images%20to%20improve%20the%20model%27s%20accuracy%20and%20robustness.%20StarryGazer%20shows%20superior%20results%20over%20existing%20unsupervised%20methods%20and%20transformed%20MDE%20results%20on%20various%20datasets%2C%20demonstrating%20that%20our%20framework%20exploits%20the%20power%20of%20MDE%20models%20while%20appropriately%20fixing%20errors%20using%20sparse%20depth%20information.&entry.1838667208=http%3A//arxiv.org/abs/2512.13147v1&entry.124074799=Read"},
{"title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images", "author": "Thalyssa Baiocco-Rodrigues and Antoine Olivier and Reda Belbahri and Thomas Duboudin and Pierre-Antoine Bannier and Benjamin Adjadj and Katharina Von Loga and Nathan Noiry and Maxime Touzot and Hector Roux de Bezieux", "abstract": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.", "link": "http://arxiv.org/abs/2512.13440v1", "date": "2025-12-15", "relevancy": 1.7656, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4561}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMILIA%3A%20interpretable%20multiple%20instance%20learning%20for%20inflammation%20prediction%20in%20IBD%20from%20H%26E%20whole%20slide%20images&body=Title%3A%20IMILIA%3A%20interpretable%20multiple%20instance%20learning%20for%20inflammation%20prediction%20in%20IBD%20from%20H%26E%20whole%20slide%20images%0AAuthor%3A%20Thalyssa%20Baiocco-Rodrigues%20and%20Antoine%20Olivier%20and%20Reda%20Belbahri%20and%20Thomas%20Duboudin%20and%20Pierre-Antoine%20Bannier%20and%20Benjamin%20Adjadj%20and%20Katharina%20Von%20Loga%20and%20Nathan%20Noiry%20and%20Maxime%20Touzot%20and%20Hector%20Roux%20de%20Bezieux%0AAbstract%3A%20As%20the%20therapeutic%20target%20for%20Inflammatory%20Bowel%20Disease%20%28IBD%29%20shifts%20toward%20histologic%20remission%2C%20the%20accurate%20assessment%20of%20microscopic%20inflammation%20has%20become%20increasingly%20central%20for%20evaluating%20disease%20activity%20and%20response%20to%20treatment.%20In%20this%20work%2C%20we%20introduce%20IMILIA%20%28Interpretable%20Multiple%20Instance%20Learning%20for%20Inflammation%20Analysis%29%2C%20an%20end-to-end%20framework%20designed%20for%20the%20prediction%20of%20inflammation%20presence%20in%20IBD%20digitized%20slides%20stained%20with%20hematoxylin%20and%20eosin%20%28H%26E%29%2C%20followed%20by%20the%20automated%20computation%20of%20markers%20characterizing%20tissue%20regions%20driving%20the%20predictions.%20IMILIA%20is%20composed%20of%20an%20inflammation%20prediction%20module%2C%20consisting%20of%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20model%2C%20and%20an%20interpretability%20module%2C%20divided%20in%20two%20blocks%3A%20HistoPLUS%2C%20for%20cell%20instance%20detection%2C%20segmentation%20and%20classification%3B%20and%20EpiSeg%2C%20for%20epithelium%20segmentation.%20IMILIA%20achieves%20a%20cross-validation%20ROC-AUC%20of%200.83%20on%20the%20discovery%20cohort%2C%20and%20a%20ROC-AUC%20of%200.99%20and%200.84%20on%20two%20external%20validation%20cohorts.%20The%20interpretability%20module%20yields%20biologically%20consistent%20insights%3A%20tiles%20with%20higher%20predicted%20scores%20show%20increased%20densities%20of%20immune%20cells%20%28lymphocytes%2C%20plasmocytes%2C%20neutrophils%20and%20eosinophils%29%2C%20whereas%20lower-scored%20tiles%20predominantly%20contain%20normal%20epithelial%20cells.%20Notably%2C%20these%20patterns%20were%20consistent%20across%20all%20datasets.%20Code%20and%20models%20to%20partially%20replicate%20the%20results%20on%20the%20public%20IBDColEpi%20dataset%20can%20be%20found%20at%20https%3A//github.com/owkin/imilia.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMILIA%253A%2520interpretable%2520multiple%2520instance%2520learning%2520for%2520inflammation%2520prediction%2520in%2520IBD%2520from%2520H%2526E%2520whole%2520slide%2520images%26entry.906535625%3DThalyssa%2520Baiocco-Rodrigues%2520and%2520Antoine%2520Olivier%2520and%2520Reda%2520Belbahri%2520and%2520Thomas%2520Duboudin%2520and%2520Pierre-Antoine%2520Bannier%2520and%2520Benjamin%2520Adjadj%2520and%2520Katharina%2520Von%2520Loga%2520and%2520Nathan%2520Noiry%2520and%2520Maxime%2520Touzot%2520and%2520Hector%2520Roux%2520de%2520Bezieux%26entry.1292438233%3DAs%2520the%2520therapeutic%2520target%2520for%2520Inflammatory%2520Bowel%2520Disease%2520%2528IBD%2529%2520shifts%2520toward%2520histologic%2520remission%252C%2520the%2520accurate%2520assessment%2520of%2520microscopic%2520inflammation%2520has%2520become%2520increasingly%2520central%2520for%2520evaluating%2520disease%2520activity%2520and%2520response%2520to%2520treatment.%2520In%2520this%2520work%252C%2520we%2520introduce%2520IMILIA%2520%2528Interpretable%2520Multiple%2520Instance%2520Learning%2520for%2520Inflammation%2520Analysis%2529%252C%2520an%2520end-to-end%2520framework%2520designed%2520for%2520the%2520prediction%2520of%2520inflammation%2520presence%2520in%2520IBD%2520digitized%2520slides%2520stained%2520with%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%252C%2520followed%2520by%2520the%2520automated%2520computation%2520of%2520markers%2520characterizing%2520tissue%2520regions%2520driving%2520the%2520predictions.%2520IMILIA%2520is%2520composed%2520of%2520an%2520inflammation%2520prediction%2520module%252C%2520consisting%2520of%2520a%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520model%252C%2520and%2520an%2520interpretability%2520module%252C%2520divided%2520in%2520two%2520blocks%253A%2520HistoPLUS%252C%2520for%2520cell%2520instance%2520detection%252C%2520segmentation%2520and%2520classification%253B%2520and%2520EpiSeg%252C%2520for%2520epithelium%2520segmentation.%2520IMILIA%2520achieves%2520a%2520cross-validation%2520ROC-AUC%2520of%25200.83%2520on%2520the%2520discovery%2520cohort%252C%2520and%2520a%2520ROC-AUC%2520of%25200.99%2520and%25200.84%2520on%2520two%2520external%2520validation%2520cohorts.%2520The%2520interpretability%2520module%2520yields%2520biologically%2520consistent%2520insights%253A%2520tiles%2520with%2520higher%2520predicted%2520scores%2520show%2520increased%2520densities%2520of%2520immune%2520cells%2520%2528lymphocytes%252C%2520plasmocytes%252C%2520neutrophils%2520and%2520eosinophils%2529%252C%2520whereas%2520lower-scored%2520tiles%2520predominantly%2520contain%2520normal%2520epithelial%2520cells.%2520Notably%252C%2520these%2520patterns%2520were%2520consistent%2520across%2520all%2520datasets.%2520Code%2520and%2520models%2520to%2520partially%2520replicate%2520the%2520results%2520on%2520the%2520public%2520IBDColEpi%2520dataset%2520can%2520be%2520found%2520at%2520https%253A//github.com/owkin/imilia.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMILIA%3A%20interpretable%20multiple%20instance%20learning%20for%20inflammation%20prediction%20in%20IBD%20from%20H%26E%20whole%20slide%20images&entry.906535625=Thalyssa%20Baiocco-Rodrigues%20and%20Antoine%20Olivier%20and%20Reda%20Belbahri%20and%20Thomas%20Duboudin%20and%20Pierre-Antoine%20Bannier%20and%20Benjamin%20Adjadj%20and%20Katharina%20Von%20Loga%20and%20Nathan%20Noiry%20and%20Maxime%20Touzot%20and%20Hector%20Roux%20de%20Bezieux&entry.1292438233=As%20the%20therapeutic%20target%20for%20Inflammatory%20Bowel%20Disease%20%28IBD%29%20shifts%20toward%20histologic%20remission%2C%20the%20accurate%20assessment%20of%20microscopic%20inflammation%20has%20become%20increasingly%20central%20for%20evaluating%20disease%20activity%20and%20response%20to%20treatment.%20In%20this%20work%2C%20we%20introduce%20IMILIA%20%28Interpretable%20Multiple%20Instance%20Learning%20for%20Inflammation%20Analysis%29%2C%20an%20end-to-end%20framework%20designed%20for%20the%20prediction%20of%20inflammation%20presence%20in%20IBD%20digitized%20slides%20stained%20with%20hematoxylin%20and%20eosin%20%28H%26E%29%2C%20followed%20by%20the%20automated%20computation%20of%20markers%20characterizing%20tissue%20regions%20driving%20the%20predictions.%20IMILIA%20is%20composed%20of%20an%20inflammation%20prediction%20module%2C%20consisting%20of%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20model%2C%20and%20an%20interpretability%20module%2C%20divided%20in%20two%20blocks%3A%20HistoPLUS%2C%20for%20cell%20instance%20detection%2C%20segmentation%20and%20classification%3B%20and%20EpiSeg%2C%20for%20epithelium%20segmentation.%20IMILIA%20achieves%20a%20cross-validation%20ROC-AUC%20of%200.83%20on%20the%20discovery%20cohort%2C%20and%20a%20ROC-AUC%20of%200.99%20and%200.84%20on%20two%20external%20validation%20cohorts.%20The%20interpretability%20module%20yields%20biologically%20consistent%20insights%3A%20tiles%20with%20higher%20predicted%20scores%20show%20increased%20densities%20of%20immune%20cells%20%28lymphocytes%2C%20plasmocytes%2C%20neutrophils%20and%20eosinophils%29%2C%20whereas%20lower-scored%20tiles%20predominantly%20contain%20normal%20epithelial%20cells.%20Notably%2C%20these%20patterns%20were%20consistent%20across%20all%20datasets.%20Code%20and%20models%20to%20partially%20replicate%20the%20results%20on%20the%20public%20IBDColEpi%20dataset%20can%20be%20found%20at%20https%3A//github.com/owkin/imilia.&entry.1838667208=http%3A//arxiv.org/abs/2512.13440v1&entry.124074799=Read"},
{"title": "DEAR: Dataset for Evaluating the Aesthetics of Rendering", "author": "Vsevolod Plohotnuk and Artyom Panshin and Nikola Bani\u0107 and Simone Bianco and Michael Freeman and Egor Ershov", "abstract": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).", "link": "http://arxiv.org/abs/2512.05209v2", "date": "2025-12-15", "relevancy": 2.0315, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5249}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5194}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering&body=Title%3A%20DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%0AAuthor%3A%20Vsevolod%20Plohotnuk%20and%20Artyom%20Panshin%20and%20Nikola%20Bani%C4%87%20and%20Simone%20Bianco%20and%20Michael%20Freeman%20and%20Egor%20Ershov%0AAbstract%3A%20Traditional%20Image%20Quality%20Assessment~%28IQA%29%20focuses%20on%20quantifying%20technical%20degradations%20such%20as%20noise%2C%20blur%2C%20or%20compression%20artifacts%2C%20using%20both%20full-reference%20and%20no-reference%20objective%20metrics.%20However%2C%20evaluation%20of%20rendering%20aesthetics%2C%20a%20growing%20domain%20relevant%20to%20photographic%20editing%2C%20content%20creation%2C%20and%20AI-generated%20imagery%2C%20remains%20underexplored%20due%20to%20the%20lack%20of%20datasets%20that%20reflect%20the%20inherently%20subjective%20nature%20of%20style%20preference.%20In%20this%20work%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20model%20human%20aesthetic%20judgments%20of%20image%20rendering%20styles%20is%20introduced%3A%20the%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%20%28DEAR%29.%20Built%20upon%20the%20MIT-Adobe%20FiveK%20dataset%2C%20DEAR%20incorporates%20pairwise%20human%20preference%20scores%20collected%20via%20large-scale%20crowdsourcing%2C%20with%20each%20image%20pair%20evaluated%20by%2025%20distinct%20human%20evaluators%20with%20a%20total%20of%2013%2C648%20of%20them%20participating%20overall.%20These%20annotations%20capture%20nuanced%2C%20context-sensitive%20aesthetic%20preferences%2C%20enabling%20the%20development%20and%20evaluation%20of%20models%20that%20go%20beyond%20traditional%20distortion-based%20IQA%2C%20focusing%20on%20a%20new%20task%3A%20Evaluation%20of%20Aesthetics%20of%20Rendering%20%28EAR%29.%20The%20data%20collection%20pipeline%20is%20described%2C%20human%20voting%20patterns%20are%20analyzed%2C%20and%20multiple%20use%20cases%20are%20outlined%2C%20including%20style%20preference%20prediction%2C%20aesthetic%20benchmarking%2C%20and%20personalized%20aesthetic%20modeling.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20DEAR%20is%20the%20first%20dataset%20to%20systematically%20address%20image%20aesthetics%20of%20rendering%20assessment%20grounded%20in%20subjective%20human%20preferences.%20A%20subset%20of%20100%20images%20with%20markup%20for%20them%20is%20published%20on%20HuggingFace%20%28huggingface.co/datasets/vsevolodpl/DEAR%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAR%253A%2520Dataset%2520for%2520Evaluating%2520the%2520Aesthetics%2520of%2520Rendering%26entry.906535625%3DVsevolod%2520Plohotnuk%2520and%2520Artyom%2520Panshin%2520and%2520Nikola%2520Bani%25C4%2587%2520and%2520Simone%2520Bianco%2520and%2520Michael%2520Freeman%2520and%2520Egor%2520Ershov%26entry.1292438233%3DTraditional%2520Image%2520Quality%2520Assessment~%2528IQA%2529%2520focuses%2520on%2520quantifying%2520technical%2520degradations%2520such%2520as%2520noise%252C%2520blur%252C%2520or%2520compression%2520artifacts%252C%2520using%2520both%2520full-reference%2520and%2520no-reference%2520objective%2520metrics.%2520However%252C%2520evaluation%2520of%2520rendering%2520aesthetics%252C%2520a%2520growing%2520domain%2520relevant%2520to%2520photographic%2520editing%252C%2520content%2520creation%252C%2520and%2520AI-generated%2520imagery%252C%2520remains%2520underexplored%2520due%2520to%2520the%2520lack%2520of%2520datasets%2520that%2520reflect%2520the%2520inherently%2520subjective%2520nature%2520of%2520style%2520preference.%2520In%2520this%2520work%252C%2520a%2520novel%2520benchmark%2520dataset%2520designed%2520to%2520model%2520human%2520aesthetic%2520judgments%2520of%2520image%2520rendering%2520styles%2520is%2520introduced%253A%2520the%2520Dataset%2520for%2520Evaluating%2520the%2520Aesthetics%2520of%2520Rendering%2520%2528DEAR%2529.%2520Built%2520upon%2520the%2520MIT-Adobe%2520FiveK%2520dataset%252C%2520DEAR%2520incorporates%2520pairwise%2520human%2520preference%2520scores%2520collected%2520via%2520large-scale%2520crowdsourcing%252C%2520with%2520each%2520image%2520pair%2520evaluated%2520by%252025%2520distinct%2520human%2520evaluators%2520with%2520a%2520total%2520of%252013%252C648%2520of%2520them%2520participating%2520overall.%2520These%2520annotations%2520capture%2520nuanced%252C%2520context-sensitive%2520aesthetic%2520preferences%252C%2520enabling%2520the%2520development%2520and%2520evaluation%2520of%2520models%2520that%2520go%2520beyond%2520traditional%2520distortion-based%2520IQA%252C%2520focusing%2520on%2520a%2520new%2520task%253A%2520Evaluation%2520of%2520Aesthetics%2520of%2520Rendering%2520%2528EAR%2529.%2520The%2520data%2520collection%2520pipeline%2520is%2520described%252C%2520human%2520voting%2520patterns%2520are%2520analyzed%252C%2520and%2520multiple%2520use%2520cases%2520are%2520outlined%252C%2520including%2520style%2520preference%2520prediction%252C%2520aesthetic%2520benchmarking%252C%2520and%2520personalized%2520aesthetic%2520modeling.%2520To%2520the%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520DEAR%2520is%2520the%2520first%2520dataset%2520to%2520systematically%2520address%2520image%2520aesthetics%2520of%2520rendering%2520assessment%2520grounded%2520in%2520subjective%2520human%2520preferences.%2520A%2520subset%2520of%2520100%2520images%2520with%2520markup%2520for%2520them%2520is%2520published%2520on%2520HuggingFace%2520%2528huggingface.co/datasets/vsevolodpl/DEAR%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering&entry.906535625=Vsevolod%20Plohotnuk%20and%20Artyom%20Panshin%20and%20Nikola%20Bani%C4%87%20and%20Simone%20Bianco%20and%20Michael%20Freeman%20and%20Egor%20Ershov&entry.1292438233=Traditional%20Image%20Quality%20Assessment~%28IQA%29%20focuses%20on%20quantifying%20technical%20degradations%20such%20as%20noise%2C%20blur%2C%20or%20compression%20artifacts%2C%20using%20both%20full-reference%20and%20no-reference%20objective%20metrics.%20However%2C%20evaluation%20of%20rendering%20aesthetics%2C%20a%20growing%20domain%20relevant%20to%20photographic%20editing%2C%20content%20creation%2C%20and%20AI-generated%20imagery%2C%20remains%20underexplored%20due%20to%20the%20lack%20of%20datasets%20that%20reflect%20the%20inherently%20subjective%20nature%20of%20style%20preference.%20In%20this%20work%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20model%20human%20aesthetic%20judgments%20of%20image%20rendering%20styles%20is%20introduced%3A%20the%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%20%28DEAR%29.%20Built%20upon%20the%20MIT-Adobe%20FiveK%20dataset%2C%20DEAR%20incorporates%20pairwise%20human%20preference%20scores%20collected%20via%20large-scale%20crowdsourcing%2C%20with%20each%20image%20pair%20evaluated%20by%2025%20distinct%20human%20evaluators%20with%20a%20total%20of%2013%2C648%20of%20them%20participating%20overall.%20These%20annotations%20capture%20nuanced%2C%20context-sensitive%20aesthetic%20preferences%2C%20enabling%20the%20development%20and%20evaluation%20of%20models%20that%20go%20beyond%20traditional%20distortion-based%20IQA%2C%20focusing%20on%20a%20new%20task%3A%20Evaluation%20of%20Aesthetics%20of%20Rendering%20%28EAR%29.%20The%20data%20collection%20pipeline%20is%20described%2C%20human%20voting%20patterns%20are%20analyzed%2C%20and%20multiple%20use%20cases%20are%20outlined%2C%20including%20style%20preference%20prediction%2C%20aesthetic%20benchmarking%2C%20and%20personalized%20aesthetic%20modeling.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20DEAR%20is%20the%20first%20dataset%20to%20systematically%20address%20image%20aesthetics%20of%20rendering%20assessment%20grounded%20in%20subjective%20human%20preferences.%20A%20subset%20of%20100%20images%20with%20markup%20for%20them%20is%20published%20on%20HuggingFace%20%28huggingface.co/datasets/vsevolodpl/DEAR%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.05209v2&entry.124074799=Read"},
{"title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning", "author": "Jiaru Zou and Ling Yang and Yunzhe Qi and Sirui Chen and Mengting Ai and Ke Shen and Jingrui He and Mengdi Wang", "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.", "link": "http://arxiv.org/abs/2512.13278v1", "date": "2025-12-15", "relevancy": 1.5669, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5117}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoTool%3A%20Dynamic%20Tool%20Selection%20and%20Integration%20for%20Agentic%20Reasoning&body=Title%3A%20AutoTool%3A%20Dynamic%20Tool%20Selection%20and%20Integration%20for%20Agentic%20Reasoning%0AAuthor%3A%20Jiaru%20Zou%20and%20Ling%20Yang%20and%20Yunzhe%20Qi%20and%20Sirui%20Chen%20and%20Mengting%20Ai%20and%20Ke%20Shen%20and%20Jingrui%20He%20and%20Mengdi%20Wang%0AAbstract%3A%20Agentic%20reinforcement%20learning%20has%20advanced%20large%20language%20models%20%28LLMs%29%20to%20reason%20through%20long%20chain-of-thought%20trajectories%20while%20interleaving%20external%20tool%20use.%20Existing%20approaches%20assume%20a%20fixed%20inventory%20of%20tools%2C%20limiting%20LLM%20agents%27%20adaptability%20to%20new%20or%20evolving%20toolsets.%20We%20present%20AutoTool%2C%20a%20framework%20that%20equips%20LLM%20agents%20with%20dynamic%20tool-selection%20capabilities%20throughout%20their%20reasoning%20trajectories.%20We%20first%20construct%20a%20200k%20dataset%20with%20explicit%20tool-selection%20rationales%20across%201%2C000%2B%20tools%20and%20100%2B%20tasks%20spanning%20mathematics%2C%20science%2C%20code%20generation%2C%20and%20multimodal%20reasoning.%20Building%20on%20this%20data%20foundation%2C%20AutoTool%20employs%20a%20dual-phase%20optimization%20pipeline%3A%20%28i%29%20supervised%20and%20RL-based%20trajectory%20stabilization%20for%20coherent%20reasoning%2C%20and%20%28ii%29%20KL-regularized%20Plackett-Luce%20ranking%20to%20refine%20consistent%20multi-step%20tool%20selection.%20Across%20ten%20diverse%20benchmarks%2C%20we%20train%20two%20base%20models%2C%20Qwen3-8B%20and%20Qwen2.5-VL-7B%2C%20with%20AutoTool.%20With%20fewer%20parameters%2C%20AutoTool%20consistently%20outperforms%20advanced%20LLM%20agents%20and%20tool-integration%20methods%2C%20yielding%20average%20gains%20of%206.4%25%20in%20math%20%26%20science%20reasoning%2C%204.5%25%20in%20search-based%20QA%2C%207.7%25%20in%20code%20generation%2C%20and%206.9%25%20in%20multimodal%20understanding.%20In%20addition%2C%20AutoTool%20exhibits%20stronger%20generalization%20by%20dynamically%20leveraging%20unseen%20tools%20from%20evolving%20toolsets%20during%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoTool%253A%2520Dynamic%2520Tool%2520Selection%2520and%2520Integration%2520for%2520Agentic%2520Reasoning%26entry.906535625%3DJiaru%2520Zou%2520and%2520Ling%2520Yang%2520and%2520Yunzhe%2520Qi%2520and%2520Sirui%2520Chen%2520and%2520Mengting%2520Ai%2520and%2520Ke%2520Shen%2520and%2520Jingrui%2520He%2520and%2520Mengdi%2520Wang%26entry.1292438233%3DAgentic%2520reinforcement%2520learning%2520has%2520advanced%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reason%2520through%2520long%2520chain-of-thought%2520trajectories%2520while%2520interleaving%2520external%2520tool%2520use.%2520Existing%2520approaches%2520assume%2520a%2520fixed%2520inventory%2520of%2520tools%252C%2520limiting%2520LLM%2520agents%2527%2520adaptability%2520to%2520new%2520or%2520evolving%2520toolsets.%2520We%2520present%2520AutoTool%252C%2520a%2520framework%2520that%2520equips%2520LLM%2520agents%2520with%2520dynamic%2520tool-selection%2520capabilities%2520throughout%2520their%2520reasoning%2520trajectories.%2520We%2520first%2520construct%2520a%2520200k%2520dataset%2520with%2520explicit%2520tool-selection%2520rationales%2520across%25201%252C000%252B%2520tools%2520and%2520100%252B%2520tasks%2520spanning%2520mathematics%252C%2520science%252C%2520code%2520generation%252C%2520and%2520multimodal%2520reasoning.%2520Building%2520on%2520this%2520data%2520foundation%252C%2520AutoTool%2520employs%2520a%2520dual-phase%2520optimization%2520pipeline%253A%2520%2528i%2529%2520supervised%2520and%2520RL-based%2520trajectory%2520stabilization%2520for%2520coherent%2520reasoning%252C%2520and%2520%2528ii%2529%2520KL-regularized%2520Plackett-Luce%2520ranking%2520to%2520refine%2520consistent%2520multi-step%2520tool%2520selection.%2520Across%2520ten%2520diverse%2520benchmarks%252C%2520we%2520train%2520two%2520base%2520models%252C%2520Qwen3-8B%2520and%2520Qwen2.5-VL-7B%252C%2520with%2520AutoTool.%2520With%2520fewer%2520parameters%252C%2520AutoTool%2520consistently%2520outperforms%2520advanced%2520LLM%2520agents%2520and%2520tool-integration%2520methods%252C%2520yielding%2520average%2520gains%2520of%25206.4%2525%2520in%2520math%2520%2526%2520science%2520reasoning%252C%25204.5%2525%2520in%2520search-based%2520QA%252C%25207.7%2525%2520in%2520code%2520generation%252C%2520and%25206.9%2525%2520in%2520multimodal%2520understanding.%2520In%2520addition%252C%2520AutoTool%2520exhibits%2520stronger%2520generalization%2520by%2520dynamically%2520leveraging%2520unseen%2520tools%2520from%2520evolving%2520toolsets%2520during%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoTool%3A%20Dynamic%20Tool%20Selection%20and%20Integration%20for%20Agentic%20Reasoning&entry.906535625=Jiaru%20Zou%20and%20Ling%20Yang%20and%20Yunzhe%20Qi%20and%20Sirui%20Chen%20and%20Mengting%20Ai%20and%20Ke%20Shen%20and%20Jingrui%20He%20and%20Mengdi%20Wang&entry.1292438233=Agentic%20reinforcement%20learning%20has%20advanced%20large%20language%20models%20%28LLMs%29%20to%20reason%20through%20long%20chain-of-thought%20trajectories%20while%20interleaving%20external%20tool%20use.%20Existing%20approaches%20assume%20a%20fixed%20inventory%20of%20tools%2C%20limiting%20LLM%20agents%27%20adaptability%20to%20new%20or%20evolving%20toolsets.%20We%20present%20AutoTool%2C%20a%20framework%20that%20equips%20LLM%20agents%20with%20dynamic%20tool-selection%20capabilities%20throughout%20their%20reasoning%20trajectories.%20We%20first%20construct%20a%20200k%20dataset%20with%20explicit%20tool-selection%20rationales%20across%201%2C000%2B%20tools%20and%20100%2B%20tasks%20spanning%20mathematics%2C%20science%2C%20code%20generation%2C%20and%20multimodal%20reasoning.%20Building%20on%20this%20data%20foundation%2C%20AutoTool%20employs%20a%20dual-phase%20optimization%20pipeline%3A%20%28i%29%20supervised%20and%20RL-based%20trajectory%20stabilization%20for%20coherent%20reasoning%2C%20and%20%28ii%29%20KL-regularized%20Plackett-Luce%20ranking%20to%20refine%20consistent%20multi-step%20tool%20selection.%20Across%20ten%20diverse%20benchmarks%2C%20we%20train%20two%20base%20models%2C%20Qwen3-8B%20and%20Qwen2.5-VL-7B%2C%20with%20AutoTool.%20With%20fewer%20parameters%2C%20AutoTool%20consistently%20outperforms%20advanced%20LLM%20agents%20and%20tool-integration%20methods%2C%20yielding%20average%20gains%20of%206.4%25%20in%20math%20%26%20science%20reasoning%2C%204.5%25%20in%20search-based%20QA%2C%207.7%25%20in%20code%20generation%2C%20and%206.9%25%20in%20multimodal%20understanding.%20In%20addition%2C%20AutoTool%20exhibits%20stronger%20generalization%20by%20dynamically%20leveraging%20unseen%20tools%20from%20evolving%20toolsets%20during%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2512.13278v1&entry.124074799=Read"},
{"title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation", "author": "Christophe Bolduc and Julien Philip and Li Ma and Mingming He and Paul Debevec and Jean-Fran\u00e7ois Lalonde", "abstract": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.", "link": "http://arxiv.org/abs/2512.13597v1", "date": "2025-12-15", "relevancy": 1.6846, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5811}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lighting%20in%20Motion%3A%20Spatiotemporal%20HDR%20Lighting%20Estimation&body=Title%3A%20Lighting%20in%20Motion%3A%20Spatiotemporal%20HDR%20Lighting%20Estimation%0AAuthor%3A%20Christophe%20Bolduc%20and%20Julien%20Philip%20and%20Li%20Ma%20and%20Mingming%20He%20and%20Paul%20Debevec%20and%20Jean-Fran%C3%A7ois%20Lalonde%0AAbstract%3A%20We%20present%20Lighting%20in%20Motion%20%28LiMo%29%2C%20a%20diffusion-based%20approach%20to%20spatiotemporal%20lighting%20estimation.%20LiMo%20targets%20both%20realistic%20high-frequency%20detail%20prediction%20and%20accurate%20illuminance%20estimation.%20To%20account%20for%20both%2C%20we%20propose%20generating%20a%20set%20of%20mirrored%20and%20diffuse%20spheres%20at%20different%20exposures%2C%20based%20on%20their%203D%20positions%20in%20the%20input.%20Making%20use%20of%20diffusion%20priors%2C%20we%20fine-tune%20powerful%20existing%20diffusion%20models%20on%20a%20large-scale%20customized%20dataset%20of%20indoor%20and%20outdoor%20scenes%2C%20paired%20with%20spatiotemporal%20light%20probes.%20For%20accurate%20spatial%20conditioning%2C%20we%20demonstrate%20that%20depth%20alone%20is%20insufficient%20and%20we%20introduce%20a%20new%20geometric%20condition%20to%20provide%20the%20relative%20position%20of%20the%20scene%20to%20the%20target%203D%20position.%20Finally%2C%20we%20combine%20diffuse%20and%20mirror%20predictions%20at%20different%20exposures%20into%20a%20single%20HDRI%20map%20leveraging%20differentiable%20rendering.%20We%20thoroughly%20evaluate%20our%20method%20and%20design%20choices%20to%20establish%20LiMo%20as%20state-of-the-art%20for%20both%20spatial%20control%20and%20prediction%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLighting%2520in%2520Motion%253A%2520Spatiotemporal%2520HDR%2520Lighting%2520Estimation%26entry.906535625%3DChristophe%2520Bolduc%2520and%2520Julien%2520Philip%2520and%2520Li%2520Ma%2520and%2520Mingming%2520He%2520and%2520Paul%2520Debevec%2520and%2520Jean-Fran%25C3%25A7ois%2520Lalonde%26entry.1292438233%3DWe%2520present%2520Lighting%2520in%2520Motion%2520%2528LiMo%2529%252C%2520a%2520diffusion-based%2520approach%2520to%2520spatiotemporal%2520lighting%2520estimation.%2520LiMo%2520targets%2520both%2520realistic%2520high-frequency%2520detail%2520prediction%2520and%2520accurate%2520illuminance%2520estimation.%2520To%2520account%2520for%2520both%252C%2520we%2520propose%2520generating%2520a%2520set%2520of%2520mirrored%2520and%2520diffuse%2520spheres%2520at%2520different%2520exposures%252C%2520based%2520on%2520their%25203D%2520positions%2520in%2520the%2520input.%2520Making%2520use%2520of%2520diffusion%2520priors%252C%2520we%2520fine-tune%2520powerful%2520existing%2520diffusion%2520models%2520on%2520a%2520large-scale%2520customized%2520dataset%2520of%2520indoor%2520and%2520outdoor%2520scenes%252C%2520paired%2520with%2520spatiotemporal%2520light%2520probes.%2520For%2520accurate%2520spatial%2520conditioning%252C%2520we%2520demonstrate%2520that%2520depth%2520alone%2520is%2520insufficient%2520and%2520we%2520introduce%2520a%2520new%2520geometric%2520condition%2520to%2520provide%2520the%2520relative%2520position%2520of%2520the%2520scene%2520to%2520the%2520target%25203D%2520position.%2520Finally%252C%2520we%2520combine%2520diffuse%2520and%2520mirror%2520predictions%2520at%2520different%2520exposures%2520into%2520a%2520single%2520HDRI%2520map%2520leveraging%2520differentiable%2520rendering.%2520We%2520thoroughly%2520evaluate%2520our%2520method%2520and%2520design%2520choices%2520to%2520establish%2520LiMo%2520as%2520state-of-the-art%2520for%2520both%2520spatial%2520control%2520and%2520prediction%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lighting%20in%20Motion%3A%20Spatiotemporal%20HDR%20Lighting%20Estimation&entry.906535625=Christophe%20Bolduc%20and%20Julien%20Philip%20and%20Li%20Ma%20and%20Mingming%20He%20and%20Paul%20Debevec%20and%20Jean-Fran%C3%A7ois%20Lalonde&entry.1292438233=We%20present%20Lighting%20in%20Motion%20%28LiMo%29%2C%20a%20diffusion-based%20approach%20to%20spatiotemporal%20lighting%20estimation.%20LiMo%20targets%20both%20realistic%20high-frequency%20detail%20prediction%20and%20accurate%20illuminance%20estimation.%20To%20account%20for%20both%2C%20we%20propose%20generating%20a%20set%20of%20mirrored%20and%20diffuse%20spheres%20at%20different%20exposures%2C%20based%20on%20their%203D%20positions%20in%20the%20input.%20Making%20use%20of%20diffusion%20priors%2C%20we%20fine-tune%20powerful%20existing%20diffusion%20models%20on%20a%20large-scale%20customized%20dataset%20of%20indoor%20and%20outdoor%20scenes%2C%20paired%20with%20spatiotemporal%20light%20probes.%20For%20accurate%20spatial%20conditioning%2C%20we%20demonstrate%20that%20depth%20alone%20is%20insufficient%20and%20we%20introduce%20a%20new%20geometric%20condition%20to%20provide%20the%20relative%20position%20of%20the%20scene%20to%20the%20target%203D%20position.%20Finally%2C%20we%20combine%20diffuse%20and%20mirror%20predictions%20at%20different%20exposures%20into%20a%20single%20HDRI%20map%20leveraging%20differentiable%20rendering.%20We%20thoroughly%20evaluate%20our%20method%20and%20design%20choices%20to%20establish%20LiMo%20as%20state-of-the-art%20for%20both%20spatial%20control%20and%20prediction%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.13597v1&entry.124074799=Read"},
{"title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation", "author": "Hyun-Gi Lee and Jaekyeong Han and Minjun Kwon and Hyeonuk Kwon and Jooha Park and Hoe Jin Ha and Dong-Hwa Seo", "abstract": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 \u03a9 in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.", "link": "http://arxiv.org/abs/2512.13198v1", "date": "2025-12-15", "relevancy": 1.4096, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.387}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3516}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALBATROSS%3A%20A%20robotised%20system%20for%20high-throughput%20electrolyte%20screening%20via%20automated%20electrolyte%20formulation%2C%20coin-cell%20fabrication%2C%20and%20electrochemical%20evaluation&body=Title%3A%20ALBATROSS%3A%20A%20robotised%20system%20for%20high-throughput%20electrolyte%20screening%20via%20automated%20electrolyte%20formulation%2C%20coin-cell%20fabrication%2C%20and%20electrochemical%20evaluation%0AAuthor%3A%20Hyun-Gi%20Lee%20and%20Jaekyeong%20Han%20and%20Minjun%20Kwon%20and%20Hyeonuk%20Kwon%20and%20Jooha%20Park%20and%20Hoe%20Jin%20Ha%20and%20Dong-Hwa%20Seo%0AAbstract%3A%20As%20battery%20technologies%20advance%20toward%20higher%20stability%20and%20energy%20density%2C%20the%20need%20for%20extensive%20cell-level%20testing%20across%20various%20component%20configurations%20becomes%20critical.%20To%20evaluate%20performance%20and%20understand%20the%20operating%20principles%20of%20batteries%20in%20laboratory%20scale%2C%20fabrication%20and%20evaluation%20of%20coin%20cells%20are%20essential%20processes.%20However%2C%20the%20conventional%20coin-cell%20assembly%20and%20testing%20processes%20require%20significant%20time%20and%20labor%20from%20researchers%2C%20posing%20challenges%20to%20high-throughput%20screening%20research.%20In%20this%20study%2C%20we%20introduce%20an%20Automated%20Li-ion%20BAttery%20Testing%20RObot%20SyStem%20%28ALBATROSS%29%2C%20an%20automated%20system%20capable%20of%20electrolyte%20formulation%2C%20coin-cell%20assembly%2C%20and%20electrochemical%20evaluation.%20The%20system%2C%20integrated%20within%20a%20argon-filled%20glovebox%2C%20enables%20fully%20automated%20assembly%20and%20testing%20of%20up%20to%2048%20cells%20without%20researcher%20intervention.%20By%20incorporating%20custom-designed%20robot%20gripper%20and%203D-printed%20structures%20optimized%20for%20precise%20cell%20handling%2C%20ALBATROSS%20achieved%20high%20assembly%20reliability%2C%20yielding%20a%20relative%20standard%20deviation%20%28RSD%29%20of%20less%20than%201.2%25%20in%20discharge%20capacity%20and%20a%20standard%20deviation%20of%20less%20than%203%20%CE%A9%20in%20EIS%20measurements%20for%20NCM811%7C%7CLi%20half%20cells.%20Owing%20to%20its%20high%20reliability%20and%20automation%20capability%2C%20ALBATROSS%20allows%20for%20the%20acquisition%20of%20high-quality%20coin-cell%20datasets%2C%20which%20are%20expected%20to%20accelerate%20the%20development%20of%20next-generation%20electrolytes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALBATROSS%253A%2520A%2520robotised%2520system%2520for%2520high-throughput%2520electrolyte%2520screening%2520via%2520automated%2520electrolyte%2520formulation%252C%2520coin-cell%2520fabrication%252C%2520and%2520electrochemical%2520evaluation%26entry.906535625%3DHyun-Gi%2520Lee%2520and%2520Jaekyeong%2520Han%2520and%2520Minjun%2520Kwon%2520and%2520Hyeonuk%2520Kwon%2520and%2520Jooha%2520Park%2520and%2520Hoe%2520Jin%2520Ha%2520and%2520Dong-Hwa%2520Seo%26entry.1292438233%3DAs%2520battery%2520technologies%2520advance%2520toward%2520higher%2520stability%2520and%2520energy%2520density%252C%2520the%2520need%2520for%2520extensive%2520cell-level%2520testing%2520across%2520various%2520component%2520configurations%2520becomes%2520critical.%2520To%2520evaluate%2520performance%2520and%2520understand%2520the%2520operating%2520principles%2520of%2520batteries%2520in%2520laboratory%2520scale%252C%2520fabrication%2520and%2520evaluation%2520of%2520coin%2520cells%2520are%2520essential%2520processes.%2520However%252C%2520the%2520conventional%2520coin-cell%2520assembly%2520and%2520testing%2520processes%2520require%2520significant%2520time%2520and%2520labor%2520from%2520researchers%252C%2520posing%2520challenges%2520to%2520high-throughput%2520screening%2520research.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520Automated%2520Li-ion%2520BAttery%2520Testing%2520RObot%2520SyStem%2520%2528ALBATROSS%2529%252C%2520an%2520automated%2520system%2520capable%2520of%2520electrolyte%2520formulation%252C%2520coin-cell%2520assembly%252C%2520and%2520electrochemical%2520evaluation.%2520The%2520system%252C%2520integrated%2520within%2520a%2520argon-filled%2520glovebox%252C%2520enables%2520fully%2520automated%2520assembly%2520and%2520testing%2520of%2520up%2520to%252048%2520cells%2520without%2520researcher%2520intervention.%2520By%2520incorporating%2520custom-designed%2520robot%2520gripper%2520and%25203D-printed%2520structures%2520optimized%2520for%2520precise%2520cell%2520handling%252C%2520ALBATROSS%2520achieved%2520high%2520assembly%2520reliability%252C%2520yielding%2520a%2520relative%2520standard%2520deviation%2520%2528RSD%2529%2520of%2520less%2520than%25201.2%2525%2520in%2520discharge%2520capacity%2520and%2520a%2520standard%2520deviation%2520of%2520less%2520than%25203%2520%25CE%25A9%2520in%2520EIS%2520measurements%2520for%2520NCM811%257C%257CLi%2520half%2520cells.%2520Owing%2520to%2520its%2520high%2520reliability%2520and%2520automation%2520capability%252C%2520ALBATROSS%2520allows%2520for%2520the%2520acquisition%2520of%2520high-quality%2520coin-cell%2520datasets%252C%2520which%2520are%2520expected%2520to%2520accelerate%2520the%2520development%2520of%2520next-generation%2520electrolytes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALBATROSS%3A%20A%20robotised%20system%20for%20high-throughput%20electrolyte%20screening%20via%20automated%20electrolyte%20formulation%2C%20coin-cell%20fabrication%2C%20and%20electrochemical%20evaluation&entry.906535625=Hyun-Gi%20Lee%20and%20Jaekyeong%20Han%20and%20Minjun%20Kwon%20and%20Hyeonuk%20Kwon%20and%20Jooha%20Park%20and%20Hoe%20Jin%20Ha%20and%20Dong-Hwa%20Seo&entry.1292438233=As%20battery%20technologies%20advance%20toward%20higher%20stability%20and%20energy%20density%2C%20the%20need%20for%20extensive%20cell-level%20testing%20across%20various%20component%20configurations%20becomes%20critical.%20To%20evaluate%20performance%20and%20understand%20the%20operating%20principles%20of%20batteries%20in%20laboratory%20scale%2C%20fabrication%20and%20evaluation%20of%20coin%20cells%20are%20essential%20processes.%20However%2C%20the%20conventional%20coin-cell%20assembly%20and%20testing%20processes%20require%20significant%20time%20and%20labor%20from%20researchers%2C%20posing%20challenges%20to%20high-throughput%20screening%20research.%20In%20this%20study%2C%20we%20introduce%20an%20Automated%20Li-ion%20BAttery%20Testing%20RObot%20SyStem%20%28ALBATROSS%29%2C%20an%20automated%20system%20capable%20of%20electrolyte%20formulation%2C%20coin-cell%20assembly%2C%20and%20electrochemical%20evaluation.%20The%20system%2C%20integrated%20within%20a%20argon-filled%20glovebox%2C%20enables%20fully%20automated%20assembly%20and%20testing%20of%20up%20to%2048%20cells%20without%20researcher%20intervention.%20By%20incorporating%20custom-designed%20robot%20gripper%20and%203D-printed%20structures%20optimized%20for%20precise%20cell%20handling%2C%20ALBATROSS%20achieved%20high%20assembly%20reliability%2C%20yielding%20a%20relative%20standard%20deviation%20%28RSD%29%20of%20less%20than%201.2%25%20in%20discharge%20capacity%20and%20a%20standard%20deviation%20of%20less%20than%203%20%CE%A9%20in%20EIS%20measurements%20for%20NCM811%7C%7CLi%20half%20cells.%20Owing%20to%20its%20high%20reliability%20and%20automation%20capability%2C%20ALBATROSS%20allows%20for%20the%20acquisition%20of%20high-quality%20coin-cell%20datasets%2C%20which%20are%20expected%20to%20accelerate%20the%20development%20of%20next-generation%20electrolytes.&entry.1838667208=http%3A//arxiv.org/abs/2512.13198v1&entry.124074799=Read"},
{"title": "Can Language Models Discover Scaling Laws?", "author": "Haowei Lin and Haotian Ye and Wenzheng Feng and Quzhe Huang and Yujun Li and Hubert Lim and Zhengrui Li and Xiangyu Wang and Jianzhu Ma and James Zou and Yitao Liang", "abstract": "Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.", "link": "http://arxiv.org/abs/2507.21184v3", "date": "2025-12-15", "relevancy": 1.4148, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Language%20Models%20Discover%20Scaling%20Laws%3F&body=Title%3A%20Can%20Language%20Models%20Discover%20Scaling%20Laws%3F%0AAuthor%3A%20Haowei%20Lin%20and%20Haotian%20Ye%20and%20Wenzheng%20Feng%20and%20Quzhe%20Huang%20and%20Yujun%20Li%20and%20Hubert%20Lim%20and%20Zhengrui%20Li%20and%20Xiangyu%20Wang%20and%20Jianzhu%20Ma%20and%20James%20Zou%20and%20Yitao%20Liang%0AAbstract%3A%20Discovering%20scaling%20laws%20for%20predicting%20model%20performance%20at%20scale%20is%20a%20fundamental%20and%20open-ended%20challenge%2C%20mostly%20reliant%20on%20slow%2C%20case%20specific%20human%20experimentation.%20To%20investigate%20the%20potential%20for%20LLMs%20to%20automate%20this%20process%2C%20we%20collect%20over%205%2C000%20experiments%20from%20existing%20literature%20and%20curate%20seven%20diverse%20scaling%20law%20discovery%20tasks.%20While%20existing%20agents%20struggle%20to%20produce%20accurate%20law%20formulas%2C%20this%20paper%20introduces%20SLDAgent%2C%20an%20evolution-based%20agent%20that%20co-optimize%20the%20scaling%20law%20model%20and%20the%20parameters%2C%20enabling%20it%20to%20autonomously%20explore%20complex%20relationships%20between%20variables.%20For%20the%20first%20time%2C%20we%20demonstrates%20that%20SLDAgent%20can%20automatically%20discover%20laws%20that%20exhibit%20consistently%20more%20accurate%20extrapolation%20than%20their%20established%2C%20human-derived%20counterparts%20across%20all%20tasks.%20Through%20comprehensive%20analysis%2C%20we%20elucidate%20why%20these%20discovered%20laws%20are%20superior%20and%20verify%20their%20practical%20utility%20in%20both%20pretraining%20and%20finetuning%20applications.%20This%20work%20establishes%20a%20new%20paradigm%20for%20agentic%20scientific%20discovery%2C%20showing%20that%20AI%20systems%20can%20understand%20their%20own%20scaling%20behavior%2C%20and%20can%20contribute%20novel%20and%20practical%20knowledge%20back%20to%20the%20research%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2507.21184v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Language%2520Models%2520Discover%2520Scaling%2520Laws%253F%26entry.906535625%3DHaowei%2520Lin%2520and%2520Haotian%2520Ye%2520and%2520Wenzheng%2520Feng%2520and%2520Quzhe%2520Huang%2520and%2520Yujun%2520Li%2520and%2520Hubert%2520Lim%2520and%2520Zhengrui%2520Li%2520and%2520Xiangyu%2520Wang%2520and%2520Jianzhu%2520Ma%2520and%2520James%2520Zou%2520and%2520Yitao%2520Liang%26entry.1292438233%3DDiscovering%2520scaling%2520laws%2520for%2520predicting%2520model%2520performance%2520at%2520scale%2520is%2520a%2520fundamental%2520and%2520open-ended%2520challenge%252C%2520mostly%2520reliant%2520on%2520slow%252C%2520case%2520specific%2520human%2520experimentation.%2520To%2520investigate%2520the%2520potential%2520for%2520LLMs%2520to%2520automate%2520this%2520process%252C%2520we%2520collect%2520over%25205%252C000%2520experiments%2520from%2520existing%2520literature%2520and%2520curate%2520seven%2520diverse%2520scaling%2520law%2520discovery%2520tasks.%2520While%2520existing%2520agents%2520struggle%2520to%2520produce%2520accurate%2520law%2520formulas%252C%2520this%2520paper%2520introduces%2520SLDAgent%252C%2520an%2520evolution-based%2520agent%2520that%2520co-optimize%2520the%2520scaling%2520law%2520model%2520and%2520the%2520parameters%252C%2520enabling%2520it%2520to%2520autonomously%2520explore%2520complex%2520relationships%2520between%2520variables.%2520For%2520the%2520first%2520time%252C%2520we%2520demonstrates%2520that%2520SLDAgent%2520can%2520automatically%2520discover%2520laws%2520that%2520exhibit%2520consistently%2520more%2520accurate%2520extrapolation%2520than%2520their%2520established%252C%2520human-derived%2520counterparts%2520across%2520all%2520tasks.%2520Through%2520comprehensive%2520analysis%252C%2520we%2520elucidate%2520why%2520these%2520discovered%2520laws%2520are%2520superior%2520and%2520verify%2520their%2520practical%2520utility%2520in%2520both%2520pretraining%2520and%2520finetuning%2520applications.%2520This%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520agentic%2520scientific%2520discovery%252C%2520showing%2520that%2520AI%2520systems%2520can%2520understand%2520their%2520own%2520scaling%2520behavior%252C%2520and%2520can%2520contribute%2520novel%2520and%2520practical%2520knowledge%2520back%2520to%2520the%2520research%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21184v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Language%20Models%20Discover%20Scaling%20Laws%3F&entry.906535625=Haowei%20Lin%20and%20Haotian%20Ye%20and%20Wenzheng%20Feng%20and%20Quzhe%20Huang%20and%20Yujun%20Li%20and%20Hubert%20Lim%20and%20Zhengrui%20Li%20and%20Xiangyu%20Wang%20and%20Jianzhu%20Ma%20and%20James%20Zou%20and%20Yitao%20Liang&entry.1292438233=Discovering%20scaling%20laws%20for%20predicting%20model%20performance%20at%20scale%20is%20a%20fundamental%20and%20open-ended%20challenge%2C%20mostly%20reliant%20on%20slow%2C%20case%20specific%20human%20experimentation.%20To%20investigate%20the%20potential%20for%20LLMs%20to%20automate%20this%20process%2C%20we%20collect%20over%205%2C000%20experiments%20from%20existing%20literature%20and%20curate%20seven%20diverse%20scaling%20law%20discovery%20tasks.%20While%20existing%20agents%20struggle%20to%20produce%20accurate%20law%20formulas%2C%20this%20paper%20introduces%20SLDAgent%2C%20an%20evolution-based%20agent%20that%20co-optimize%20the%20scaling%20law%20model%20and%20the%20parameters%2C%20enabling%20it%20to%20autonomously%20explore%20complex%20relationships%20between%20variables.%20For%20the%20first%20time%2C%20we%20demonstrates%20that%20SLDAgent%20can%20automatically%20discover%20laws%20that%20exhibit%20consistently%20more%20accurate%20extrapolation%20than%20their%20established%2C%20human-derived%20counterparts%20across%20all%20tasks.%20Through%20comprehensive%20analysis%2C%20we%20elucidate%20why%20these%20discovered%20laws%20are%20superior%20and%20verify%20their%20practical%20utility%20in%20both%20pretraining%20and%20finetuning%20applications.%20This%20work%20establishes%20a%20new%20paradigm%20for%20agentic%20scientific%20discovery%2C%20showing%20that%20AI%20systems%20can%20understand%20their%20own%20scaling%20behavior%2C%20and%20can%20contribute%20novel%20and%20practical%20knowledge%20back%20to%20the%20research%20community.&entry.1838667208=http%3A//arxiv.org/abs/2507.21184v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


