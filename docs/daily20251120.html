<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251119.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression", "author": "Keshav Gupta and Akshat Sanghvi and Shreyas Reddy Palley and Astitva Srivastava and Charu Sharma and Avinash Sharma", "abstract": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, SymGS, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at symgs.github.io", "link": "http://arxiv.org/abs/2511.13264v2", "date": "2025-11-19", "relevancy": 3.2194, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6636}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.641}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression&body=Title%3A%20SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression%0AAuthor%3A%20Keshav%20Gupta%20and%20Akshat%20Sanghvi%20and%20Shreyas%20Reddy%20Palley%20and%20Astitva%20Srivastava%20and%20Charu%20Sharma%20and%20Avinash%20Sharma%0AAbstract%3A%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20transformative%20technique%20in%20novel%20view%20synthesis%2C%20primarily%20due%20to%20its%20high%20rendering%20speed%20and%20photorealistic%20fidelity.%20However%2C%20its%20memory%20footprint%20scales%20rapidly%20with%20scene%20complexity%2C%20often%20reaching%20several%20gigabytes.%20Existing%20methods%20address%20this%20issue%20by%20introducing%20compression%20strategies%20that%20exploit%20primitive-level%20redundancy%20through%20similarity%20detection%20and%20quantization.%20We%20aim%20to%20surpass%20the%20compression%20limits%20of%20such%20methods%20by%20incorporating%20symmetry-aware%20techniques%2C%20specifically%20targeting%20mirror%20symmetries%20to%20eliminate%20redundant%20primitives.%20We%20propose%20a%20novel%20compression%20framework%2C%20SymGS%2C%20introducing%20learnable%20mirrors%20into%20the%20scene%2C%20thereby%20eliminating%20local%20and%20global%20reflective%20redundancies%20for%20compression.%20Our%20framework%20functions%20as%20a%20plug-and-play%20enhancement%20to%20state-of-the-art%20compression%20methods%2C%20%28e.g.%20HAC%29%20to%20achieve%20further%20compression.%20Compared%20to%20HAC%2C%20we%20achieve%20%241.66%20%5Ctimes%24%20compression%20across%20benchmark%20datasets%20%28upto%20%243%5Ctimes%24%20on%20large-scale%20scenes%29.%20On%20an%20average%2C%20SymGS%20enables%20%24%5Cbf%7B108%5Ctimes%7D%24%20compression%20of%20a%203DGS%20scene%2C%20while%20preserving%20rendering%20quality.%20The%20project%20page%20and%20supplementary%20can%20be%20found%20at%20symgs.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2511.13264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymGS%2520%253A%2520Leveraging%2520Local%2520Symmetries%2520for%25203D%2520Gaussian%2520Splatting%2520Compression%26entry.906535625%3DKeshav%2520Gupta%2520and%2520Akshat%2520Sanghvi%2520and%2520Shreyas%2520Reddy%2520Palley%2520and%2520Astitva%2520Srivastava%2520and%2520Charu%2520Sharma%2520and%2520Avinash%2520Sharma%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520transformative%2520technique%2520in%2520novel%2520view%2520synthesis%252C%2520primarily%2520due%2520to%2520its%2520high%2520rendering%2520speed%2520and%2520photorealistic%2520fidelity.%2520However%252C%2520its%2520memory%2520footprint%2520scales%2520rapidly%2520with%2520scene%2520complexity%252C%2520often%2520reaching%2520several%2520gigabytes.%2520Existing%2520methods%2520address%2520this%2520issue%2520by%2520introducing%2520compression%2520strategies%2520that%2520exploit%2520primitive-level%2520redundancy%2520through%2520similarity%2520detection%2520and%2520quantization.%2520We%2520aim%2520to%2520surpass%2520the%2520compression%2520limits%2520of%2520such%2520methods%2520by%2520incorporating%2520symmetry-aware%2520techniques%252C%2520specifically%2520targeting%2520mirror%2520symmetries%2520to%2520eliminate%2520redundant%2520primitives.%2520We%2520propose%2520a%2520novel%2520compression%2520framework%252C%2520SymGS%252C%2520introducing%2520learnable%2520mirrors%2520into%2520the%2520scene%252C%2520thereby%2520eliminating%2520local%2520and%2520global%2520reflective%2520redundancies%2520for%2520compression.%2520Our%2520framework%2520functions%2520as%2520a%2520plug-and-play%2520enhancement%2520to%2520state-of-the-art%2520compression%2520methods%252C%2520%2528e.g.%2520HAC%2529%2520to%2520achieve%2520further%2520compression.%2520Compared%2520to%2520HAC%252C%2520we%2520achieve%2520%25241.66%2520%255Ctimes%2524%2520compression%2520across%2520benchmark%2520datasets%2520%2528upto%2520%25243%255Ctimes%2524%2520on%2520large-scale%2520scenes%2529.%2520On%2520an%2520average%252C%2520SymGS%2520enables%2520%2524%255Cbf%257B108%255Ctimes%257D%2524%2520compression%2520of%2520a%25203DGS%2520scene%252C%2520while%2520preserving%2520rendering%2520quality.%2520The%2520project%2520page%2520and%2520supplementary%2520can%2520be%2520found%2520at%2520symgs.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression&entry.906535625=Keshav%20Gupta%20and%20Akshat%20Sanghvi%20and%20Shreyas%20Reddy%20Palley%20and%20Astitva%20Srivastava%20and%20Charu%20Sharma%20and%20Avinash%20Sharma&entry.1292438233=3D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20transformative%20technique%20in%20novel%20view%20synthesis%2C%20primarily%20due%20to%20its%20high%20rendering%20speed%20and%20photorealistic%20fidelity.%20However%2C%20its%20memory%20footprint%20scales%20rapidly%20with%20scene%20complexity%2C%20often%20reaching%20several%20gigabytes.%20Existing%20methods%20address%20this%20issue%20by%20introducing%20compression%20strategies%20that%20exploit%20primitive-level%20redundancy%20through%20similarity%20detection%20and%20quantization.%20We%20aim%20to%20surpass%20the%20compression%20limits%20of%20such%20methods%20by%20incorporating%20symmetry-aware%20techniques%2C%20specifically%20targeting%20mirror%20symmetries%20to%20eliminate%20redundant%20primitives.%20We%20propose%20a%20novel%20compression%20framework%2C%20SymGS%2C%20introducing%20learnable%20mirrors%20into%20the%20scene%2C%20thereby%20eliminating%20local%20and%20global%20reflective%20redundancies%20for%20compression.%20Our%20framework%20functions%20as%20a%20plug-and-play%20enhancement%20to%20state-of-the-art%20compression%20methods%2C%20%28e.g.%20HAC%29%20to%20achieve%20further%20compression.%20Compared%20to%20HAC%2C%20we%20achieve%20%241.66%20%5Ctimes%24%20compression%20across%20benchmark%20datasets%20%28upto%20%243%5Ctimes%24%20on%20large-scale%20scenes%29.%20On%20an%20average%2C%20SymGS%20enables%20%24%5Cbf%7B108%5Ctimes%7D%24%20compression%20of%20a%203DGS%20scene%2C%20while%20preserving%20rendering%20quality.%20The%20project%20page%20and%20supplementary%20can%20be%20found%20at%20symgs.github.io&entry.1838667208=http%3A//arxiv.org/abs/2511.13264v2&entry.124074799=Read"},
{"title": "Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language", "author": "Yan Xia and Letian Shi and Yilin Di and Joao F. Henriques and Daniel Cremers", "abstract": "We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.", "link": "http://arxiv.org/abs/2511.15308v1", "date": "2025-11-19", "relevancy": 3.167, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6742}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Loc%2B%2B%3A%20Generalizing%203D%20Point%20Cloud%20Localization%20from%20Natural%20Language&body=Title%3A%20Text2Loc%2B%2B%3A%20Generalizing%203D%20Point%20Cloud%20Localization%20from%20Natural%20Language%0AAuthor%3A%20Yan%20Xia%20and%20Letian%20Shi%20and%20Yilin%20Di%20and%20Joao%20F.%20Henriques%20and%20Daniel%20Cremers%0AAbstract%3A%20We%20tackle%20the%20problem%20of%20localizing%203D%20point%20cloud%20submaps%20using%20complex%20and%20diverse%20natural%20language%20descriptions%2C%20and%20present%20Text2Loc%2B%2B%2C%20a%20novel%20neural%20network%20designed%20for%20effective%20cross-modal%20alignment%20between%20language%20and%20point%20clouds%20in%20a%20coarse-to-fine%20localization%20pipeline.%20To%20support%20benchmarking%2C%20we%20introduce%20a%20new%20city-scale%20dataset%20covering%20both%20color%20and%20non-color%20point%20clouds%20from%20diverse%20urban%20scenes%2C%20and%20organize%20location%20descriptions%20into%20three%20levels%20of%20linguistic%20complexity.%20In%20the%20global%20place%20recognition%20stage%2C%20Text2Loc%2B%2B%20combines%20a%20pretrained%20language%20model%20with%20a%20Hierarchical%20Transformer%20with%20Max%20pooling%20%28HTM%29%20for%20sentence-level%20semantics%2C%20and%20employs%20an%20attention-based%20point%20cloud%20encoder%20for%20spatial%20understanding.%20We%20further%20propose%20Masked%20Instance%20Training%20%28MIT%29%20to%20filter%20out%20non-aligned%20objects%20and%20improve%20multimodal%20robustness.%20To%20enhance%20the%20embedding%20space%2C%20we%20introduce%20Modality-aware%20Hierarchical%20Contrastive%20Learning%20%28MHCL%29%2C%20incorporating%20cross-modal%2C%20submap-%2C%20text-%2C%20and%20instance-level%20losses.%20In%20the%20fine%20localization%20stage%2C%20we%20completely%20remove%20explicit%20text-instance%20matching%20and%20design%20a%20lightweight%20yet%20powerful%20framework%20based%20on%20Prototype-based%20Map%20Cloning%20%28PMC%29%20and%20a%20Cascaded%20Cross-Attention%20Transformer%20%28CCAT%29.%20Extensive%20experiments%20on%20the%20KITTI360Pose%20dataset%20show%20that%20Text2Loc%2B%2B%20outperforms%20existing%20methods%20by%20up%20to%2015%25.%20In%20addition%2C%20the%20proposed%20model%20exhibits%20robust%20generalization%20when%20evaluated%20on%20the%20new%20dataset%2C%20effectively%20handling%20complex%20linguistic%20expressions%20and%20a%20wide%20variety%20of%20urban%20environments.%20The%20code%20and%20dataset%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Loc%252B%252B%253A%2520Generalizing%25203D%2520Point%2520Cloud%2520Localization%2520from%2520Natural%2520Language%26entry.906535625%3DYan%2520Xia%2520and%2520Letian%2520Shi%2520and%2520Yilin%2520Di%2520and%2520Joao%2520F.%2520Henriques%2520and%2520Daniel%2520Cremers%26entry.1292438233%3DWe%2520tackle%2520the%2520problem%2520of%2520localizing%25203D%2520point%2520cloud%2520submaps%2520using%2520complex%2520and%2520diverse%2520natural%2520language%2520descriptions%252C%2520and%2520present%2520Text2Loc%252B%252B%252C%2520a%2520novel%2520neural%2520network%2520designed%2520for%2520effective%2520cross-modal%2520alignment%2520between%2520language%2520and%2520point%2520clouds%2520in%2520a%2520coarse-to-fine%2520localization%2520pipeline.%2520To%2520support%2520benchmarking%252C%2520we%2520introduce%2520a%2520new%2520city-scale%2520dataset%2520covering%2520both%2520color%2520and%2520non-color%2520point%2520clouds%2520from%2520diverse%2520urban%2520scenes%252C%2520and%2520organize%2520location%2520descriptions%2520into%2520three%2520levels%2520of%2520linguistic%2520complexity.%2520In%2520the%2520global%2520place%2520recognition%2520stage%252C%2520Text2Loc%252B%252B%2520combines%2520a%2520pretrained%2520language%2520model%2520with%2520a%2520Hierarchical%2520Transformer%2520with%2520Max%2520pooling%2520%2528HTM%2529%2520for%2520sentence-level%2520semantics%252C%2520and%2520employs%2520an%2520attention-based%2520point%2520cloud%2520encoder%2520for%2520spatial%2520understanding.%2520We%2520further%2520propose%2520Masked%2520Instance%2520Training%2520%2528MIT%2529%2520to%2520filter%2520out%2520non-aligned%2520objects%2520and%2520improve%2520multimodal%2520robustness.%2520To%2520enhance%2520the%2520embedding%2520space%252C%2520we%2520introduce%2520Modality-aware%2520Hierarchical%2520Contrastive%2520Learning%2520%2528MHCL%2529%252C%2520incorporating%2520cross-modal%252C%2520submap-%252C%2520text-%252C%2520and%2520instance-level%2520losses.%2520In%2520the%2520fine%2520localization%2520stage%252C%2520we%2520completely%2520remove%2520explicit%2520text-instance%2520matching%2520and%2520design%2520a%2520lightweight%2520yet%2520powerful%2520framework%2520based%2520on%2520Prototype-based%2520Map%2520Cloning%2520%2528PMC%2529%2520and%2520a%2520Cascaded%2520Cross-Attention%2520Transformer%2520%2528CCAT%2529.%2520Extensive%2520experiments%2520on%2520the%2520KITTI360Pose%2520dataset%2520show%2520that%2520Text2Loc%252B%252B%2520outperforms%2520existing%2520methods%2520by%2520up%2520to%252015%2525.%2520In%2520addition%252C%2520the%2520proposed%2520model%2520exhibits%2520robust%2520generalization%2520when%2520evaluated%2520on%2520the%2520new%2520dataset%252C%2520effectively%2520handling%2520complex%2520linguistic%2520expressions%2520and%2520a%2520wide%2520variety%2520of%2520urban%2520environments.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Loc%2B%2B%3A%20Generalizing%203D%20Point%20Cloud%20Localization%20from%20Natural%20Language&entry.906535625=Yan%20Xia%20and%20Letian%20Shi%20and%20Yilin%20Di%20and%20Joao%20F.%20Henriques%20and%20Daniel%20Cremers&entry.1292438233=We%20tackle%20the%20problem%20of%20localizing%203D%20point%20cloud%20submaps%20using%20complex%20and%20diverse%20natural%20language%20descriptions%2C%20and%20present%20Text2Loc%2B%2B%2C%20a%20novel%20neural%20network%20designed%20for%20effective%20cross-modal%20alignment%20between%20language%20and%20point%20clouds%20in%20a%20coarse-to-fine%20localization%20pipeline.%20To%20support%20benchmarking%2C%20we%20introduce%20a%20new%20city-scale%20dataset%20covering%20both%20color%20and%20non-color%20point%20clouds%20from%20diverse%20urban%20scenes%2C%20and%20organize%20location%20descriptions%20into%20three%20levels%20of%20linguistic%20complexity.%20In%20the%20global%20place%20recognition%20stage%2C%20Text2Loc%2B%2B%20combines%20a%20pretrained%20language%20model%20with%20a%20Hierarchical%20Transformer%20with%20Max%20pooling%20%28HTM%29%20for%20sentence-level%20semantics%2C%20and%20employs%20an%20attention-based%20point%20cloud%20encoder%20for%20spatial%20understanding.%20We%20further%20propose%20Masked%20Instance%20Training%20%28MIT%29%20to%20filter%20out%20non-aligned%20objects%20and%20improve%20multimodal%20robustness.%20To%20enhance%20the%20embedding%20space%2C%20we%20introduce%20Modality-aware%20Hierarchical%20Contrastive%20Learning%20%28MHCL%29%2C%20incorporating%20cross-modal%2C%20submap-%2C%20text-%2C%20and%20instance-level%20losses.%20In%20the%20fine%20localization%20stage%2C%20we%20completely%20remove%20explicit%20text-instance%20matching%20and%20design%20a%20lightweight%20yet%20powerful%20framework%20based%20on%20Prototype-based%20Map%20Cloning%20%28PMC%29%20and%20a%20Cascaded%20Cross-Attention%20Transformer%20%28CCAT%29.%20Extensive%20experiments%20on%20the%20KITTI360Pose%20dataset%20show%20that%20Text2Loc%2B%2B%20outperforms%20existing%20methods%20by%20up%20to%2015%25.%20In%20addition%2C%20the%20proposed%20model%20exhibits%20robust%20generalization%20when%20evaluated%20on%20the%20new%20dataset%2C%20effectively%20handling%20complex%20linguistic%20expressions%20and%20a%20wide%20variety%20of%20urban%20environments.%20The%20code%20and%20dataset%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.15308v1&entry.124074799=Read"},
{"title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery", "author": "Yiming Zeng and Xi-Le Zhao and Wei-Hao Wu and Teng-Yu Ji and Chao Wang", "abstract": "Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.", "link": "http://arxiv.org/abs/2511.14270v2", "date": "2025-11-19", "relevancy": 3.1216, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6622}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6414}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting-based%20Low-Rank%20Tensor%20Representation%20for%20Multi-Dimensional%20Image%20Recovery&body=Title%3A%20Gaussian%20Splatting-based%20Low-Rank%20Tensor%20Representation%20for%20Multi-Dimensional%20Image%20Recovery%0AAuthor%3A%20Yiming%20Zeng%20and%20Xi-Le%20Zhao%20and%20Wei-Hao%20Wu%20and%20Teng-Yu%20Ji%20and%20Chao%20Wang%0AAbstract%3A%20Tensor%20singular%20value%20decomposition%20%28t-SVD%29%20is%20a%20promising%20tool%20for%20multi-dimensional%20image%20representation%2C%20which%20decomposes%20a%20multi-dimensional%20image%20into%20a%20latent%20tensor%20and%20an%20accompanying%20transform%20matrix.%20However%2C%20two%20critical%20limitations%20of%20t-SVD%20methods%20persist%3A%20%281%29%20the%20approximation%20of%20the%20latent%20tensor%20%28e.g.%2C%20tensor%20factorizations%29%20is%20coarse%20and%20fails%20to%20accurately%20capture%20spatial%20local%20high-frequency%20information%3B%20%282%29%20The%20transform%20matrix%20is%20composed%20of%20fixed%20basis%20atoms%20%28e.g.%2C%20complex%20exponential%20atoms%20in%20DFT%20and%20cosine%20atoms%20in%20DCT%29%20and%20cannot%20precisely%20capture%20local%20high-frequency%20information%20along%20the%20mode-3%20fibers.%20To%20address%20these%20two%20limitations%2C%20we%20propose%20a%20Gaussian%20Splatting-based%20Low-rank%20tensor%20Representation%20%28GSLR%29%20framework%2C%20which%20compactly%20and%20continuously%20represents%20multi-dimensional%20images.%20Specifically%2C%20we%20leverage%20tailored%202D%20Gaussian%20splatting%20and%201D%20Gaussian%20splatting%20to%20generate%20the%20latent%20tensor%20and%20transform%20matrix%2C%20respectively.%20The%202D%20and%201D%20Gaussian%20splatting%20are%20indispensable%20and%20complementary%20under%20this%20representation%20framework%2C%20which%20enjoys%20a%20powerful%20representation%20capability%2C%20especially%20for%20local%20high-frequency%20information.%20To%20evaluate%20the%20representation%20ability%20of%20the%20proposed%20GSLR%2C%20we%20develop%20an%20unsupervised%20GSLR-based%20multi-dimensional%20image%20recovery%20model.%20Extensive%20experiments%20on%20multi-dimensional%20image%20recovery%20demonstrate%20that%20GSLR%20consistently%20outperforms%20state-of-the-art%20methods%2C%20particularly%20in%20capturing%20local%20high-frequency%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting-based%2520Low-Rank%2520Tensor%2520Representation%2520for%2520Multi-Dimensional%2520Image%2520Recovery%26entry.906535625%3DYiming%2520Zeng%2520and%2520Xi-Le%2520Zhao%2520and%2520Wei-Hao%2520Wu%2520and%2520Teng-Yu%2520Ji%2520and%2520Chao%2520Wang%26entry.1292438233%3DTensor%2520singular%2520value%2520decomposition%2520%2528t-SVD%2529%2520is%2520a%2520promising%2520tool%2520for%2520multi-dimensional%2520image%2520representation%252C%2520which%2520decomposes%2520a%2520multi-dimensional%2520image%2520into%2520a%2520latent%2520tensor%2520and%2520an%2520accompanying%2520transform%2520matrix.%2520However%252C%2520two%2520critical%2520limitations%2520of%2520t-SVD%2520methods%2520persist%253A%2520%25281%2529%2520the%2520approximation%2520of%2520the%2520latent%2520tensor%2520%2528e.g.%252C%2520tensor%2520factorizations%2529%2520is%2520coarse%2520and%2520fails%2520to%2520accurately%2520capture%2520spatial%2520local%2520high-frequency%2520information%253B%2520%25282%2529%2520The%2520transform%2520matrix%2520is%2520composed%2520of%2520fixed%2520basis%2520atoms%2520%2528e.g.%252C%2520complex%2520exponential%2520atoms%2520in%2520DFT%2520and%2520cosine%2520atoms%2520in%2520DCT%2529%2520and%2520cannot%2520precisely%2520capture%2520local%2520high-frequency%2520information%2520along%2520the%2520mode-3%2520fibers.%2520To%2520address%2520these%2520two%2520limitations%252C%2520we%2520propose%2520a%2520Gaussian%2520Splatting-based%2520Low-rank%2520tensor%2520Representation%2520%2528GSLR%2529%2520framework%252C%2520which%2520compactly%2520and%2520continuously%2520represents%2520multi-dimensional%2520images.%2520Specifically%252C%2520we%2520leverage%2520tailored%25202D%2520Gaussian%2520splatting%2520and%25201D%2520Gaussian%2520splatting%2520to%2520generate%2520the%2520latent%2520tensor%2520and%2520transform%2520matrix%252C%2520respectively.%2520The%25202D%2520and%25201D%2520Gaussian%2520splatting%2520are%2520indispensable%2520and%2520complementary%2520under%2520this%2520representation%2520framework%252C%2520which%2520enjoys%2520a%2520powerful%2520representation%2520capability%252C%2520especially%2520for%2520local%2520high-frequency%2520information.%2520To%2520evaluate%2520the%2520representation%2520ability%2520of%2520the%2520proposed%2520GSLR%252C%2520we%2520develop%2520an%2520unsupervised%2520GSLR-based%2520multi-dimensional%2520image%2520recovery%2520model.%2520Extensive%2520experiments%2520on%2520multi-dimensional%2520image%2520recovery%2520demonstrate%2520that%2520GSLR%2520consistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520particularly%2520in%2520capturing%2520local%2520high-frequency%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting-based%20Low-Rank%20Tensor%20Representation%20for%20Multi-Dimensional%20Image%20Recovery&entry.906535625=Yiming%20Zeng%20and%20Xi-Le%20Zhao%20and%20Wei-Hao%20Wu%20and%20Teng-Yu%20Ji%20and%20Chao%20Wang&entry.1292438233=Tensor%20singular%20value%20decomposition%20%28t-SVD%29%20is%20a%20promising%20tool%20for%20multi-dimensional%20image%20representation%2C%20which%20decomposes%20a%20multi-dimensional%20image%20into%20a%20latent%20tensor%20and%20an%20accompanying%20transform%20matrix.%20However%2C%20two%20critical%20limitations%20of%20t-SVD%20methods%20persist%3A%20%281%29%20the%20approximation%20of%20the%20latent%20tensor%20%28e.g.%2C%20tensor%20factorizations%29%20is%20coarse%20and%20fails%20to%20accurately%20capture%20spatial%20local%20high-frequency%20information%3B%20%282%29%20The%20transform%20matrix%20is%20composed%20of%20fixed%20basis%20atoms%20%28e.g.%2C%20complex%20exponential%20atoms%20in%20DFT%20and%20cosine%20atoms%20in%20DCT%29%20and%20cannot%20precisely%20capture%20local%20high-frequency%20information%20along%20the%20mode-3%20fibers.%20To%20address%20these%20two%20limitations%2C%20we%20propose%20a%20Gaussian%20Splatting-based%20Low-rank%20tensor%20Representation%20%28GSLR%29%20framework%2C%20which%20compactly%20and%20continuously%20represents%20multi-dimensional%20images.%20Specifically%2C%20we%20leverage%20tailored%202D%20Gaussian%20splatting%20and%201D%20Gaussian%20splatting%20to%20generate%20the%20latent%20tensor%20and%20transform%20matrix%2C%20respectively.%20The%202D%20and%201D%20Gaussian%20splatting%20are%20indispensable%20and%20complementary%20under%20this%20representation%20framework%2C%20which%20enjoys%20a%20powerful%20representation%20capability%2C%20especially%20for%20local%20high-frequency%20information.%20To%20evaluate%20the%20representation%20ability%20of%20the%20proposed%20GSLR%2C%20we%20develop%20an%20unsupervised%20GSLR-based%20multi-dimensional%20image%20recovery%20model.%20Extensive%20experiments%20on%20multi-dimensional%20image%20recovery%20demonstrate%20that%20GSLR%20consistently%20outperforms%20state-of-the-art%20methods%2C%20particularly%20in%20capturing%20local%20high-frequency%20information.&entry.1838667208=http%3A//arxiv.org/abs/2511.14270v2&entry.124074799=Read"},
{"title": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models", "author": "Mehran Tamjidi and Hamidreza Dastmalchi and Mohammadreza Alimoradijazi and Ali Cheraghian and Aijun An and Morteza Saberi", "abstract": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.", "link": "http://arxiv.org/abs/2511.15311v1", "date": "2025-11-19", "relevancy": 3.0411, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt-As-You-Walk%20Through%20the%20Clouds%3A%20Training-Free%20Online%20Test-Time%20Adaptation%20of%203D%20Vision-Language%20Foundation%20Models&body=Title%3A%20Adapt-As-You-Walk%20Through%20the%20Clouds%3A%20Training-Free%20Online%20Test-Time%20Adaptation%20of%203D%20Vision-Language%20Foundation%20Models%0AAuthor%3A%20Mehran%20Tamjidi%20and%20Hamidreza%20Dastmalchi%20and%20Mohammadreza%20Alimoradijazi%20and%20Ali%20Cheraghian%20and%20Aijun%20An%20and%20Morteza%20Saberi%0AAbstract%3A%203D%20Vision-Language%20Foundation%20Models%20%28VLFMs%29%20have%20shown%20strong%20generalization%20and%20zero-shot%20recognition%20capabilities%20in%20open-world%20point%20cloud%20processing%20tasks.%20However%2C%20these%20models%20often%20underperform%20in%20practical%20scenarios%20where%20data%20are%20noisy%2C%20incomplete%2C%20or%20drawn%20from%20a%20different%20distribution%20than%20the%20training%20data.%20To%20address%20this%2C%20we%20propose%20Uni-Adapter%2C%20a%20novel%20training-free%20online%20test-time%20adaptation%20%28TTA%29%20strategy%20for%203D%20VLFMs%20based%20on%20dynamic%20prototype%20learning.%20We%20define%20a%203D%20cache%20to%20store%20class-specific%20cluster%20centers%20as%20prototypes%2C%20which%20are%20continuously%20updated%20to%20capture%20intra-class%20variability%20in%20heterogeneous%20data%20distributions.%20These%20dynamic%20prototypes%20serve%20as%20anchors%20for%20cache-based%20logit%20computation%20via%20similarity%20scoring.%20Simultaneously%2C%20a%20graph-based%20label%20smoothing%20module%20captures%20inter-prototype%20similarities%20to%20enforce%20label%20consistency%20among%20similar%20prototypes.%20Finally%2C%20we%20unify%20predictions%20from%20the%20original%203D%20VLFM%20and%20the%20refined%203D%20cache%20using%20entropy-weighted%20aggregation%20for%20reliable%20adaptation.%20Without%20retraining%2C%20Uni-Adapter%20effectively%20mitigates%20distribution%20shifts%2C%20achieving%20state-of-the-art%20performance%20on%20diverse%203D%20benchmarks%20over%20different%203D%20VLFMs%2C%20improving%20ModelNet-40C%20by%2010.55%25%2C%20ScanObjectNN-C%20by%208.26%25%2C%20and%20ShapeNet-C%20by%204.49%25%20over%20the%20source%203D%20VLFMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt-As-You-Walk%2520Through%2520the%2520Clouds%253A%2520Training-Free%2520Online%2520Test-Time%2520Adaptation%2520of%25203D%2520Vision-Language%2520Foundation%2520Models%26entry.906535625%3DMehran%2520Tamjidi%2520and%2520Hamidreza%2520Dastmalchi%2520and%2520Mohammadreza%2520Alimoradijazi%2520and%2520Ali%2520Cheraghian%2520and%2520Aijun%2520An%2520and%2520Morteza%2520Saberi%26entry.1292438233%3D3D%2520Vision-Language%2520Foundation%2520Models%2520%2528VLFMs%2529%2520have%2520shown%2520strong%2520generalization%2520and%2520zero-shot%2520recognition%2520capabilities%2520in%2520open-world%2520point%2520cloud%2520processing%2520tasks.%2520However%252C%2520these%2520models%2520often%2520underperform%2520in%2520practical%2520scenarios%2520where%2520data%2520are%2520noisy%252C%2520incomplete%252C%2520or%2520drawn%2520from%2520a%2520different%2520distribution%2520than%2520the%2520training%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520Uni-Adapter%252C%2520a%2520novel%2520training-free%2520online%2520test-time%2520adaptation%2520%2528TTA%2529%2520strategy%2520for%25203D%2520VLFMs%2520based%2520on%2520dynamic%2520prototype%2520learning.%2520We%2520define%2520a%25203D%2520cache%2520to%2520store%2520class-specific%2520cluster%2520centers%2520as%2520prototypes%252C%2520which%2520are%2520continuously%2520updated%2520to%2520capture%2520intra-class%2520variability%2520in%2520heterogeneous%2520data%2520distributions.%2520These%2520dynamic%2520prototypes%2520serve%2520as%2520anchors%2520for%2520cache-based%2520logit%2520computation%2520via%2520similarity%2520scoring.%2520Simultaneously%252C%2520a%2520graph-based%2520label%2520smoothing%2520module%2520captures%2520inter-prototype%2520similarities%2520to%2520enforce%2520label%2520consistency%2520among%2520similar%2520prototypes.%2520Finally%252C%2520we%2520unify%2520predictions%2520from%2520the%2520original%25203D%2520VLFM%2520and%2520the%2520refined%25203D%2520cache%2520using%2520entropy-weighted%2520aggregation%2520for%2520reliable%2520adaptation.%2520Without%2520retraining%252C%2520Uni-Adapter%2520effectively%2520mitigates%2520distribution%2520shifts%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520diverse%25203D%2520benchmarks%2520over%2520different%25203D%2520VLFMs%252C%2520improving%2520ModelNet-40C%2520by%252010.55%2525%252C%2520ScanObjectNN-C%2520by%25208.26%2525%252C%2520and%2520ShapeNet-C%2520by%25204.49%2525%2520over%2520the%2520source%25203D%2520VLFMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt-As-You-Walk%20Through%20the%20Clouds%3A%20Training-Free%20Online%20Test-Time%20Adaptation%20of%203D%20Vision-Language%20Foundation%20Models&entry.906535625=Mehran%20Tamjidi%20and%20Hamidreza%20Dastmalchi%20and%20Mohammadreza%20Alimoradijazi%20and%20Ali%20Cheraghian%20and%20Aijun%20An%20and%20Morteza%20Saberi&entry.1292438233=3D%20Vision-Language%20Foundation%20Models%20%28VLFMs%29%20have%20shown%20strong%20generalization%20and%20zero-shot%20recognition%20capabilities%20in%20open-world%20point%20cloud%20processing%20tasks.%20However%2C%20these%20models%20often%20underperform%20in%20practical%20scenarios%20where%20data%20are%20noisy%2C%20incomplete%2C%20or%20drawn%20from%20a%20different%20distribution%20than%20the%20training%20data.%20To%20address%20this%2C%20we%20propose%20Uni-Adapter%2C%20a%20novel%20training-free%20online%20test-time%20adaptation%20%28TTA%29%20strategy%20for%203D%20VLFMs%20based%20on%20dynamic%20prototype%20learning.%20We%20define%20a%203D%20cache%20to%20store%20class-specific%20cluster%20centers%20as%20prototypes%2C%20which%20are%20continuously%20updated%20to%20capture%20intra-class%20variability%20in%20heterogeneous%20data%20distributions.%20These%20dynamic%20prototypes%20serve%20as%20anchors%20for%20cache-based%20logit%20computation%20via%20similarity%20scoring.%20Simultaneously%2C%20a%20graph-based%20label%20smoothing%20module%20captures%20inter-prototype%20similarities%20to%20enforce%20label%20consistency%20among%20similar%20prototypes.%20Finally%2C%20we%20unify%20predictions%20from%20the%20original%203D%20VLFM%20and%20the%20refined%203D%20cache%20using%20entropy-weighted%20aggregation%20for%20reliable%20adaptation.%20Without%20retraining%2C%20Uni-Adapter%20effectively%20mitigates%20distribution%20shifts%2C%20achieving%20state-of-the-art%20performance%20on%20diverse%203D%20benchmarks%20over%20different%203D%20VLFMs%2C%20improving%20ModelNet-40C%20by%2010.55%25%2C%20ScanObjectNN-C%20by%208.26%25%2C%20and%20ShapeNet-C%20by%204.49%25%20over%20the%20source%203D%20VLFMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.15311v1&entry.124074799=Read"},
{"title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks", "author": "Shijie Lian and Changti Wu and Laurence Tianruo Yang and Hang Yuan and Bin Yu and Lei Zhang and Kai Chen", "abstract": "Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\\% to 41.8\\% (+5.2\\%), and the mean MindCube accuracy rose from 31.4\\% to 38.1\\% (+6.7\\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \\href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.", "link": "http://arxiv.org/abs/2509.24473v3", "date": "2025-11-19", "relevancy": 3.0004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Euclid%27s%20Gift%3A%20Enhancing%20Spatial%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models%20via%20Geometric%20Surrogate%20Tasks&body=Title%3A%20Euclid%27s%20Gift%3A%20Enhancing%20Spatial%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models%20via%20Geometric%20Surrogate%20Tasks%0AAuthor%3A%20Shijie%20Lian%20and%20Changti%20Wu%20and%20Laurence%20Tianruo%20Yang%20and%20Hang%20Yuan%20and%20Bin%20Yu%20and%20Lei%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20Spatial%20intelligence%20spans%20a%20rich%20suite%20of%20abilities%2C%20including%20visualising%20and%20transforming%20shapes%2C%20mentally%20rotating%20objects%2C%20judging%20relational%20positions%20and%20containment%2C%20and%20estimating%20numerosity.%20However%2C%20it%20still%20remains%20a%20critical%20unresolved%20challenge%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20To%20fill%20this%20gap%2C%20we%20propose%20to%20treat%20Euclidean%20geometry%20problem-solving%20as%20a%20surrogate%20task.%20Specifically%2C%20we%20meticulously%20constructed%20a%20curated%20multimodal%20dataset%2C%20called%20Euclid30K%2C%20comprising%20approximately%2030K%20plane%20and%20solid%20geometry%20problems.%20Furthermore%2C%20to%20enable%20the%20model%20to%20learn%20and%20apply%20Euclidean%20principles%20from%20these%20geometry%20problems%2C%20we%20fine-tuned%20seven%20model%20variants%20%28spanning%203--72B%20parameters%29%20from%20the%20Qwen2.5VL%2C%20Qwen3VL%2C%20and%20RoboBrain2.0%20families%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20inspiring%20the%20models%20to%20identify%20shapes%2C%20count%2C%20and%20relate%20entities%2C%20and%20perform%20multi-step%20deductive%20reasoning%20using%20Euclidean%20principles.%20Our%20experiments%20demonstrate%20that%20the%20resulting%20models%20achieve%20substantial%20zero-shot%20gains%20across%20four%20spatial%20reasoning%20benchmarks%20%28Super-CLEVR%2C%20Omni3DBench%2C%20VSI-Bench%2C%20and%20MindCube%29%20without%20any%20task-specific%20adaptations.%20Notably%2C%20after%20training%20on%20the%20Euclid30K%2C%20the%20mean%20VSI-Bench%20accuracy%20rose%20from%2036.6%5C%25%20to%2041.8%5C%25%20%28%2B5.2%5C%25%29%2C%20and%20the%20mean%20MindCube%20accuracy%20rose%20from%2031.4%5C%25%20to%2038.1%5C%25%20%28%2B6.7%5C%25%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20study%20showing%20that%20geometry-centric%20fine-tuning%20can%20confer%20vision-language%20models%20with%20broadly%20transferable%20spatial%20skills.%20Code%20and%20Euclid30K%20dataset%20can%20be%20found%20in%20%5Chref%7Bhttps%3A//zgca-ai4edu.github.io/Euclids_Gift%7D%7Bthis%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24473v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuclid%2527s%2520Gift%253A%2520Enhancing%2520Spatial%2520Perception%2520and%2520Reasoning%2520in%2520Vision-Language%2520Models%2520via%2520Geometric%2520Surrogate%2520Tasks%26entry.906535625%3DShijie%2520Lian%2520and%2520Changti%2520Wu%2520and%2520Laurence%2520Tianruo%2520Yang%2520and%2520Hang%2520Yuan%2520and%2520Bin%2520Yu%2520and%2520Lei%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3DSpatial%2520intelligence%2520spans%2520a%2520rich%2520suite%2520of%2520abilities%252C%2520including%2520visualising%2520and%2520transforming%2520shapes%252C%2520mentally%2520rotating%2520objects%252C%2520judging%2520relational%2520positions%2520and%2520containment%252C%2520and%2520estimating%2520numerosity.%2520However%252C%2520it%2520still%2520remains%2520a%2520critical%2520unresolved%2520challenge%2520for%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520to%2520treat%2520Euclidean%2520geometry%2520problem-solving%2520as%2520a%2520surrogate%2520task.%2520Specifically%252C%2520we%2520meticulously%2520constructed%2520a%2520curated%2520multimodal%2520dataset%252C%2520called%2520Euclid30K%252C%2520comprising%2520approximately%252030K%2520plane%2520and%2520solid%2520geometry%2520problems.%2520Furthermore%252C%2520to%2520enable%2520the%2520model%2520to%2520learn%2520and%2520apply%2520Euclidean%2520principles%2520from%2520these%2520geometry%2520problems%252C%2520we%2520fine-tuned%2520seven%2520model%2520variants%2520%2528spanning%25203--72B%2520parameters%2529%2520from%2520the%2520Qwen2.5VL%252C%2520Qwen3VL%252C%2520and%2520RoboBrain2.0%2520families%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520inspiring%2520the%2520models%2520to%2520identify%2520shapes%252C%2520count%252C%2520and%2520relate%2520entities%252C%2520and%2520perform%2520multi-step%2520deductive%2520reasoning%2520using%2520Euclidean%2520principles.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520resulting%2520models%2520achieve%2520substantial%2520zero-shot%2520gains%2520across%2520four%2520spatial%2520reasoning%2520benchmarks%2520%2528Super-CLEVR%252C%2520Omni3DBench%252C%2520VSI-Bench%252C%2520and%2520MindCube%2529%2520without%2520any%2520task-specific%2520adaptations.%2520Notably%252C%2520after%2520training%2520on%2520the%2520Euclid30K%252C%2520the%2520mean%2520VSI-Bench%2520accuracy%2520rose%2520from%252036.6%255C%2525%2520to%252041.8%255C%2525%2520%2528%252B5.2%255C%2525%2529%252C%2520and%2520the%2520mean%2520MindCube%2520accuracy%2520rose%2520from%252031.4%255C%2525%2520to%252038.1%255C%2525%2520%2528%252B6.7%255C%2525%2529.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520systematic%2520study%2520showing%2520that%2520geometry-centric%2520fine-tuning%2520can%2520confer%2520vision-language%2520models%2520with%2520broadly%2520transferable%2520spatial%2520skills.%2520Code%2520and%2520Euclid30K%2520dataset%2520can%2520be%2520found%2520in%2520%255Chref%257Bhttps%253A//zgca-ai4edu.github.io/Euclids_Gift%257D%257Bthis%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24473v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Euclid%27s%20Gift%3A%20Enhancing%20Spatial%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models%20via%20Geometric%20Surrogate%20Tasks&entry.906535625=Shijie%20Lian%20and%20Changti%20Wu%20and%20Laurence%20Tianruo%20Yang%20and%20Hang%20Yuan%20and%20Bin%20Yu%20and%20Lei%20Zhang%20and%20Kai%20Chen&entry.1292438233=Spatial%20intelligence%20spans%20a%20rich%20suite%20of%20abilities%2C%20including%20visualising%20and%20transforming%20shapes%2C%20mentally%20rotating%20objects%2C%20judging%20relational%20positions%20and%20containment%2C%20and%20estimating%20numerosity.%20However%2C%20it%20still%20remains%20a%20critical%20unresolved%20challenge%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20To%20fill%20this%20gap%2C%20we%20propose%20to%20treat%20Euclidean%20geometry%20problem-solving%20as%20a%20surrogate%20task.%20Specifically%2C%20we%20meticulously%20constructed%20a%20curated%20multimodal%20dataset%2C%20called%20Euclid30K%2C%20comprising%20approximately%2030K%20plane%20and%20solid%20geometry%20problems.%20Furthermore%2C%20to%20enable%20the%20model%20to%20learn%20and%20apply%20Euclidean%20principles%20from%20these%20geometry%20problems%2C%20we%20fine-tuned%20seven%20model%20variants%20%28spanning%203--72B%20parameters%29%20from%20the%20Qwen2.5VL%2C%20Qwen3VL%2C%20and%20RoboBrain2.0%20families%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20inspiring%20the%20models%20to%20identify%20shapes%2C%20count%2C%20and%20relate%20entities%2C%20and%20perform%20multi-step%20deductive%20reasoning%20using%20Euclidean%20principles.%20Our%20experiments%20demonstrate%20that%20the%20resulting%20models%20achieve%20substantial%20zero-shot%20gains%20across%20four%20spatial%20reasoning%20benchmarks%20%28Super-CLEVR%2C%20Omni3DBench%2C%20VSI-Bench%2C%20and%20MindCube%29%20without%20any%20task-specific%20adaptations.%20Notably%2C%20after%20training%20on%20the%20Euclid30K%2C%20the%20mean%20VSI-Bench%20accuracy%20rose%20from%2036.6%5C%25%20to%2041.8%5C%25%20%28%2B5.2%5C%25%29%2C%20and%20the%20mean%20MindCube%20accuracy%20rose%20from%2031.4%5C%25%20to%2038.1%5C%25%20%28%2B6.7%5C%25%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20study%20showing%20that%20geometry-centric%20fine-tuning%20can%20confer%20vision-language%20models%20with%20broadly%20transferable%20spatial%20skills.%20Code%20and%20Euclid30K%20dataset%20can%20be%20found%20in%20%5Chref%7Bhttps%3A//zgca-ai4edu.github.io/Euclids_Gift%7D%7Bthis%7D.&entry.1838667208=http%3A//arxiv.org/abs/2509.24473v3&entry.124074799=Read"},
{"title": "Edge-Centric Relational Reasoning for 3D Scene Graph Prediction", "author": "Yanni Ma and Hao Liu and Yulan Guo and Theo Gevers and Martin R. Oswald", "abstract": "3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.", "link": "http://arxiv.org/abs/2511.15288v1", "date": "2025-11-19", "relevancy": 2.8943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Centric%20Relational%20Reasoning%20for%203D%20Scene%20Graph%20Prediction&body=Title%3A%20Edge-Centric%20Relational%20Reasoning%20for%203D%20Scene%20Graph%20Prediction%0AAuthor%3A%20Yanni%20Ma%20and%20Hao%20Liu%20and%20Yulan%20Guo%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%0AAbstract%3A%203D%20scene%20graph%20prediction%20aims%20to%20abstract%20complex%203D%20environments%20into%20structured%20graphs%20consisting%20of%20objects%20and%20their%20pairwise%20relationships.%20Existing%20approaches%20typically%20adopt%20object-centric%20graph%20neural%20networks%2C%20where%20relation%20edge%20features%20are%20iteratively%20updated%20by%20aggregating%20messages%20from%20connected%20object%20nodes.%20However%2C%20this%20design%20inherently%20restricts%20relation%20representations%20to%20pairwise%20object%20context%2C%20making%20it%20difficult%20to%20capture%20high-order%20relational%20dependencies%20that%20are%20essential%20for%20accurate%20relation%20prediction.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20Link-guided%20Edge-centric%20relational%20reasoning%20framework%20with%20Object-aware%20fusion%2C%20namely%20LEO%2C%20which%20enables%20progressive%20reasoning%20from%20relation-level%20context%20to%20object-level%20understanding.%20Specifically%2C%20LEO%20first%20predicts%20potential%20links%20between%20object%20pairs%20to%20suppress%20irrelevant%20edges%2C%20and%20then%20transforms%20the%20original%20scene%20graph%20into%20a%20line%20graph%20where%20each%20relation%20is%20treated%20as%20a%20node.%20A%20line%20graph%20neural%20network%20is%20applied%20to%20perform%20edge-centric%20relational%20reasoning%20to%20capture%20inter-relation%20context.%20The%20enriched%20relation%20features%20are%20subsequently%20integrated%20into%20the%20original%20object-centric%20graph%20to%20enhance%20object-level%20reasoning%20and%20improve%20relation%20prediction.%20Our%20framework%20is%20model-agnostic%20and%20can%20be%20integrated%20with%20any%20existing%20object-centric%20method.%20Experiments%20on%20the%203DSSG%20dataset%20with%20two%20competitive%20baselines%20show%20consistent%20improvements%2C%20highlighting%20the%20effectiveness%20of%20our%20edge-to-object%20reasoning%20paradigm.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Centric%2520Relational%2520Reasoning%2520for%25203D%2520Scene%2520Graph%2520Prediction%26entry.906535625%3DYanni%2520Ma%2520and%2520Hao%2520Liu%2520and%2520Yulan%2520Guo%2520and%2520Theo%2520Gevers%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D3D%2520scene%2520graph%2520prediction%2520aims%2520to%2520abstract%2520complex%25203D%2520environments%2520into%2520structured%2520graphs%2520consisting%2520of%2520objects%2520and%2520their%2520pairwise%2520relationships.%2520Existing%2520approaches%2520typically%2520adopt%2520object-centric%2520graph%2520neural%2520networks%252C%2520where%2520relation%2520edge%2520features%2520are%2520iteratively%2520updated%2520by%2520aggregating%2520messages%2520from%2520connected%2520object%2520nodes.%2520However%252C%2520this%2520design%2520inherently%2520restricts%2520relation%2520representations%2520to%2520pairwise%2520object%2520context%252C%2520making%2520it%2520difficult%2520to%2520capture%2520high-order%2520relational%2520dependencies%2520that%2520are%2520essential%2520for%2520accurate%2520relation%2520prediction.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Link-guided%2520Edge-centric%2520relational%2520reasoning%2520framework%2520with%2520Object-aware%2520fusion%252C%2520namely%2520LEO%252C%2520which%2520enables%2520progressive%2520reasoning%2520from%2520relation-level%2520context%2520to%2520object-level%2520understanding.%2520Specifically%252C%2520LEO%2520first%2520predicts%2520potential%2520links%2520between%2520object%2520pairs%2520to%2520suppress%2520irrelevant%2520edges%252C%2520and%2520then%2520transforms%2520the%2520original%2520scene%2520graph%2520into%2520a%2520line%2520graph%2520where%2520each%2520relation%2520is%2520treated%2520as%2520a%2520node.%2520A%2520line%2520graph%2520neural%2520network%2520is%2520applied%2520to%2520perform%2520edge-centric%2520relational%2520reasoning%2520to%2520capture%2520inter-relation%2520context.%2520The%2520enriched%2520relation%2520features%2520are%2520subsequently%2520integrated%2520into%2520the%2520original%2520object-centric%2520graph%2520to%2520enhance%2520object-level%2520reasoning%2520and%2520improve%2520relation%2520prediction.%2520Our%2520framework%2520is%2520model-agnostic%2520and%2520can%2520be%2520integrated%2520with%2520any%2520existing%2520object-centric%2520method.%2520Experiments%2520on%2520the%25203DSSG%2520dataset%2520with%2520two%2520competitive%2520baselines%2520show%2520consistent%2520improvements%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520edge-to-object%2520reasoning%2520paradigm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Centric%20Relational%20Reasoning%20for%203D%20Scene%20Graph%20Prediction&entry.906535625=Yanni%20Ma%20and%20Hao%20Liu%20and%20Yulan%20Guo%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald&entry.1292438233=3D%20scene%20graph%20prediction%20aims%20to%20abstract%20complex%203D%20environments%20into%20structured%20graphs%20consisting%20of%20objects%20and%20their%20pairwise%20relationships.%20Existing%20approaches%20typically%20adopt%20object-centric%20graph%20neural%20networks%2C%20where%20relation%20edge%20features%20are%20iteratively%20updated%20by%20aggregating%20messages%20from%20connected%20object%20nodes.%20However%2C%20this%20design%20inherently%20restricts%20relation%20representations%20to%20pairwise%20object%20context%2C%20making%20it%20difficult%20to%20capture%20high-order%20relational%20dependencies%20that%20are%20essential%20for%20accurate%20relation%20prediction.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20Link-guided%20Edge-centric%20relational%20reasoning%20framework%20with%20Object-aware%20fusion%2C%20namely%20LEO%2C%20which%20enables%20progressive%20reasoning%20from%20relation-level%20context%20to%20object-level%20understanding.%20Specifically%2C%20LEO%20first%20predicts%20potential%20links%20between%20object%20pairs%20to%20suppress%20irrelevant%20edges%2C%20and%20then%20transforms%20the%20original%20scene%20graph%20into%20a%20line%20graph%20where%20each%20relation%20is%20treated%20as%20a%20node.%20A%20line%20graph%20neural%20network%20is%20applied%20to%20perform%20edge-centric%20relational%20reasoning%20to%20capture%20inter-relation%20context.%20The%20enriched%20relation%20features%20are%20subsequently%20integrated%20into%20the%20original%20object-centric%20graph%20to%20enhance%20object-level%20reasoning%20and%20improve%20relation%20prediction.%20Our%20framework%20is%20model-agnostic%20and%20can%20be%20integrated%20with%20any%20existing%20object-centric%20method.%20Experiments%20on%20the%203DSSG%20dataset%20with%20two%20competitive%20baselines%20show%20consistent%20improvements%2C%20highlighting%20the%20effectiveness%20of%20our%20edge-to-object%20reasoning%20paradigm.&entry.1838667208=http%3A//arxiv.org/abs/2511.15288v1&entry.124074799=Read"},
{"title": "VisPlay: Self-Evolving Vision-Language Models from Images", "author": "Yicheng He and Chengsong Huang and Zongxia Li and Jiaxin Huang and Yonghui Yang", "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/", "link": "http://arxiv.org/abs/2511.15661v1", "date": "2025-11-19", "relevancy": 2.8804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images&body=Title%3A%20VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images%0AAuthor%3A%20Yicheng%20He%20and%20Chengsong%20Huang%20and%20Zongxia%20Li%20and%20Jiaxin%20Huang%20and%20Yonghui%20Yang%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20provides%20a%20principled%20framework%20for%20improving%20Vision-Language%20Models%20%28VLMs%29%20on%20complex%20reasoning%20tasks.%20However%2C%20existing%20RL%20approaches%20often%20rely%20on%20human-annotated%20labels%20or%20task-specific%20heuristics%20to%20define%20verifiable%20rewards%2C%20both%20of%20which%20are%20costly%20and%20difficult%20to%20scale.%20We%20introduce%20VisPlay%2C%20a%20self-evolving%20RL%20framework%20that%20enables%20VLMs%20to%20autonomously%20improve%20their%20reasoning%20abilities%20using%20large%20amounts%20of%20unlabeled%20image%20data.%20Starting%20from%20a%20single%20base%20VLM%2C%20VisPlay%20assigns%20the%20model%20into%20two%20interacting%20roles%3A%20an%20Image-Conditioned%20Questioner%20that%20formulates%20challenging%20yet%20answerable%20visual%20questions%2C%20and%20a%20Multimodal%20Reasoner%20that%20generates%20silver%20responses.%20These%20roles%20are%20jointly%20trained%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20which%20incorporates%20diversity%20and%20difficulty%20rewards%20to%20balance%20the%20complexity%20of%20generated%20questions%20with%20the%20quality%20of%20the%20silver%20answers.%20VisPlay%20scales%20efficiently%20across%20two%20model%20families.%20When%20trained%20on%20Qwen2.5-VL%20and%20MiMo-VL%2C%20VisPlay%20achieves%20consistent%20improvements%20in%20visual%20reasoning%2C%20compositional%20generalization%2C%20and%20hallucination%20reduction%20across%20eight%20benchmarks%2C%20including%20MM-Vet%20and%20MMMU%2C%20demonstrating%20a%20scalable%20path%20toward%20self-evolving%20multimodal%20intelligence.%20The%20project%20page%20is%20available%20at%20https%3A//bruno686.github.io/VisPlay/%0ALink%3A%20http%3A//arxiv.org/abs/2511.15661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisPlay%253A%2520Self-Evolving%2520Vision-Language%2520Models%2520from%2520Images%26entry.906535625%3DYicheng%2520He%2520and%2520Chengsong%2520Huang%2520and%2520Zongxia%2520Li%2520and%2520Jiaxin%2520Huang%2520and%2520Yonghui%2520Yang%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520provides%2520a%2520principled%2520framework%2520for%2520improving%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520on%2520complex%2520reasoning%2520tasks.%2520However%252C%2520existing%2520RL%2520approaches%2520often%2520rely%2520on%2520human-annotated%2520labels%2520or%2520task-specific%2520heuristics%2520to%2520define%2520verifiable%2520rewards%252C%2520both%2520of%2520which%2520are%2520costly%2520and%2520difficult%2520to%2520scale.%2520We%2520introduce%2520VisPlay%252C%2520a%2520self-evolving%2520RL%2520framework%2520that%2520enables%2520VLMs%2520to%2520autonomously%2520improve%2520their%2520reasoning%2520abilities%2520using%2520large%2520amounts%2520of%2520unlabeled%2520image%2520data.%2520Starting%2520from%2520a%2520single%2520base%2520VLM%252C%2520VisPlay%2520assigns%2520the%2520model%2520into%2520two%2520interacting%2520roles%253A%2520an%2520Image-Conditioned%2520Questioner%2520that%2520formulates%2520challenging%2520yet%2520answerable%2520visual%2520questions%252C%2520and%2520a%2520Multimodal%2520Reasoner%2520that%2520generates%2520silver%2520responses.%2520These%2520roles%2520are%2520jointly%2520trained%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520which%2520incorporates%2520diversity%2520and%2520difficulty%2520rewards%2520to%2520balance%2520the%2520complexity%2520of%2520generated%2520questions%2520with%2520the%2520quality%2520of%2520the%2520silver%2520answers.%2520VisPlay%2520scales%2520efficiently%2520across%2520two%2520model%2520families.%2520When%2520trained%2520on%2520Qwen2.5-VL%2520and%2520MiMo-VL%252C%2520VisPlay%2520achieves%2520consistent%2520improvements%2520in%2520visual%2520reasoning%252C%2520compositional%2520generalization%252C%2520and%2520hallucination%2520reduction%2520across%2520eight%2520benchmarks%252C%2520including%2520MM-Vet%2520and%2520MMMU%252C%2520demonstrating%2520a%2520scalable%2520path%2520toward%2520self-evolving%2520multimodal%2520intelligence.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//bruno686.github.io/VisPlay/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images&entry.906535625=Yicheng%20He%20and%20Chengsong%20Huang%20and%20Zongxia%20Li%20and%20Jiaxin%20Huang%20and%20Yonghui%20Yang&entry.1292438233=Reinforcement%20learning%20%28RL%29%20provides%20a%20principled%20framework%20for%20improving%20Vision-Language%20Models%20%28VLMs%29%20on%20complex%20reasoning%20tasks.%20However%2C%20existing%20RL%20approaches%20often%20rely%20on%20human-annotated%20labels%20or%20task-specific%20heuristics%20to%20define%20verifiable%20rewards%2C%20both%20of%20which%20are%20costly%20and%20difficult%20to%20scale.%20We%20introduce%20VisPlay%2C%20a%20self-evolving%20RL%20framework%20that%20enables%20VLMs%20to%20autonomously%20improve%20their%20reasoning%20abilities%20using%20large%20amounts%20of%20unlabeled%20image%20data.%20Starting%20from%20a%20single%20base%20VLM%2C%20VisPlay%20assigns%20the%20model%20into%20two%20interacting%20roles%3A%20an%20Image-Conditioned%20Questioner%20that%20formulates%20challenging%20yet%20answerable%20visual%20questions%2C%20and%20a%20Multimodal%20Reasoner%20that%20generates%20silver%20responses.%20These%20roles%20are%20jointly%20trained%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20which%20incorporates%20diversity%20and%20difficulty%20rewards%20to%20balance%20the%20complexity%20of%20generated%20questions%20with%20the%20quality%20of%20the%20silver%20answers.%20VisPlay%20scales%20efficiently%20across%20two%20model%20families.%20When%20trained%20on%20Qwen2.5-VL%20and%20MiMo-VL%2C%20VisPlay%20achieves%20consistent%20improvements%20in%20visual%20reasoning%2C%20compositional%20generalization%2C%20and%20hallucination%20reduction%20across%20eight%20benchmarks%2C%20including%20MM-Vet%20and%20MMMU%2C%20demonstrating%20a%20scalable%20path%20toward%20self-evolving%20multimodal%20intelligence.%20The%20project%20page%20is%20available%20at%20https%3A//bruno686.github.io/VisPlay/&entry.1838667208=http%3A//arxiv.org/abs/2511.15661v1&entry.124074799=Read"},
{"title": "Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training", "author": "Yunjiao Zhou and Xinyan Chen and Junlang Qian and Lihua Xie and Jianfei Yang", "abstract": "Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.", "link": "http://arxiv.org/abs/2511.15379v1", "date": "2025-11-19", "relevancy": 2.8486, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5866}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Open-Vocabulary%20Human%20Motion%20Grounding%20with%20Test-Time%20Training&body=Title%3A%20Zero-Shot%20Open-Vocabulary%20Human%20Motion%20Grounding%20with%20Test-Time%20Training%0AAuthor%3A%20Yunjiao%20Zhou%20and%20Xinyan%20Chen%20and%20Junlang%20Qian%20and%20Lihua%20Xie%20and%20Jianfei%20Yang%0AAbstract%3A%20Understanding%20complex%20human%20activities%20demands%20the%20ability%20to%20decompose%20motion%20into%20fine-grained%2C%20semantic-aligned%20sub-actions.%20This%20motion%20grounding%20process%20is%20crucial%20for%20behavior%20analysis%2C%20embodied%20AI%20and%20virtual%20reality.%20Yet%2C%20most%20existing%20methods%20rely%20on%20dense%20supervision%20with%20predefined%20action%20classes%2C%20which%20are%20infeasible%20in%20open-vocabulary%2C%20real-world%20settings.%20In%20this%20paper%2C%20we%20propose%20ZOMG%2C%20a%20zero-shot%2C%20open-vocabulary%20framework%20that%20segments%20motion%20sequences%20into%20semantically%20meaningful%20sub-actions%20without%20requiring%20any%20annotations%20or%20fine-tuning.%20Technically%2C%20ZOMG%20integrates%20%281%29%20language%20semantic%20partition%2C%20which%20leverages%20large%20language%20models%20to%20decompose%20instructions%20into%20ordered%20sub-action%20units%2C%20and%20%282%29%20soft%20masking%20optimization%2C%20which%20learns%20instance-specific%20temporal%20masks%20to%20focus%20on%20frames%20critical%20to%20sub-actions%2C%20while%20maintaining%20intra-segment%20continuity%20and%20enforcing%20inter-segment%20separation%2C%20all%20without%20altering%20the%20pretrained%20encoder.%20Experiments%20on%20three%20motion-language%20datasets%20demonstrate%20state-of-the-art%20effectiveness%20and%20efficiency%20of%20motion%20grounding%20performance%2C%20outperforming%20prior%20methods%20by%20%2B8.7%5C%25%20mAP%20on%20HumanML3D%20benchmark.%20Meanwhile%2C%20significant%20improvements%20also%20exist%20in%20downstream%20retrieval%2C%20establishing%20a%20new%20paradigm%20for%20annotation-free%20motion%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Open-Vocabulary%2520Human%2520Motion%2520Grounding%2520with%2520Test-Time%2520Training%26entry.906535625%3DYunjiao%2520Zhou%2520and%2520Xinyan%2520Chen%2520and%2520Junlang%2520Qian%2520and%2520Lihua%2520Xie%2520and%2520Jianfei%2520Yang%26entry.1292438233%3DUnderstanding%2520complex%2520human%2520activities%2520demands%2520the%2520ability%2520to%2520decompose%2520motion%2520into%2520fine-grained%252C%2520semantic-aligned%2520sub-actions.%2520This%2520motion%2520grounding%2520process%2520is%2520crucial%2520for%2520behavior%2520analysis%252C%2520embodied%2520AI%2520and%2520virtual%2520reality.%2520Yet%252C%2520most%2520existing%2520methods%2520rely%2520on%2520dense%2520supervision%2520with%2520predefined%2520action%2520classes%252C%2520which%2520are%2520infeasible%2520in%2520open-vocabulary%252C%2520real-world%2520settings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ZOMG%252C%2520a%2520zero-shot%252C%2520open-vocabulary%2520framework%2520that%2520segments%2520motion%2520sequences%2520into%2520semantically%2520meaningful%2520sub-actions%2520without%2520requiring%2520any%2520annotations%2520or%2520fine-tuning.%2520Technically%252C%2520ZOMG%2520integrates%2520%25281%2529%2520language%2520semantic%2520partition%252C%2520which%2520leverages%2520large%2520language%2520models%2520to%2520decompose%2520instructions%2520into%2520ordered%2520sub-action%2520units%252C%2520and%2520%25282%2529%2520soft%2520masking%2520optimization%252C%2520which%2520learns%2520instance-specific%2520temporal%2520masks%2520to%2520focus%2520on%2520frames%2520critical%2520to%2520sub-actions%252C%2520while%2520maintaining%2520intra-segment%2520continuity%2520and%2520enforcing%2520inter-segment%2520separation%252C%2520all%2520without%2520altering%2520the%2520pretrained%2520encoder.%2520Experiments%2520on%2520three%2520motion-language%2520datasets%2520demonstrate%2520state-of-the-art%2520effectiveness%2520and%2520efficiency%2520of%2520motion%2520grounding%2520performance%252C%2520outperforming%2520prior%2520methods%2520by%2520%252B8.7%255C%2525%2520mAP%2520on%2520HumanML3D%2520benchmark.%2520Meanwhile%252C%2520significant%2520improvements%2520also%2520exist%2520in%2520downstream%2520retrieval%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520annotation-free%2520motion%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Open-Vocabulary%20Human%20Motion%20Grounding%20with%20Test-Time%20Training&entry.906535625=Yunjiao%20Zhou%20and%20Xinyan%20Chen%20and%20Junlang%20Qian%20and%20Lihua%20Xie%20and%20Jianfei%20Yang&entry.1292438233=Understanding%20complex%20human%20activities%20demands%20the%20ability%20to%20decompose%20motion%20into%20fine-grained%2C%20semantic-aligned%20sub-actions.%20This%20motion%20grounding%20process%20is%20crucial%20for%20behavior%20analysis%2C%20embodied%20AI%20and%20virtual%20reality.%20Yet%2C%20most%20existing%20methods%20rely%20on%20dense%20supervision%20with%20predefined%20action%20classes%2C%20which%20are%20infeasible%20in%20open-vocabulary%2C%20real-world%20settings.%20In%20this%20paper%2C%20we%20propose%20ZOMG%2C%20a%20zero-shot%2C%20open-vocabulary%20framework%20that%20segments%20motion%20sequences%20into%20semantically%20meaningful%20sub-actions%20without%20requiring%20any%20annotations%20or%20fine-tuning.%20Technically%2C%20ZOMG%20integrates%20%281%29%20language%20semantic%20partition%2C%20which%20leverages%20large%20language%20models%20to%20decompose%20instructions%20into%20ordered%20sub-action%20units%2C%20and%20%282%29%20soft%20masking%20optimization%2C%20which%20learns%20instance-specific%20temporal%20masks%20to%20focus%20on%20frames%20critical%20to%20sub-actions%2C%20while%20maintaining%20intra-segment%20continuity%20and%20enforcing%20inter-segment%20separation%2C%20all%20without%20altering%20the%20pretrained%20encoder.%20Experiments%20on%20three%20motion-language%20datasets%20demonstrate%20state-of-the-art%20effectiveness%20and%20efficiency%20of%20motion%20grounding%20performance%2C%20outperforming%20prior%20methods%20by%20%2B8.7%5C%25%20mAP%20on%20HumanML3D%20benchmark.%20Meanwhile%2C%20significant%20improvements%20also%20exist%20in%20downstream%20retrieval%2C%20establishing%20a%20new%20paradigm%20for%20annotation-free%20motion%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2511.15379v1&entry.124074799=Read"},
{"title": "RoMa v2: Harder Better Faster Denser Feature Matching", "author": "Johan Edstedt and David Nordstr\u00f6m and Yushan Zhang and Georg B\u00f6kman and Jonathan Astermark and Viktor Larsson and Anders Heyden and Fredrik Kahl and M\u00e5rten Wadenb\u00e4ck and Michael Felsberg", "abstract": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2", "link": "http://arxiv.org/abs/2511.15706v1", "date": "2025-11-19", "relevancy": 2.8206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching&body=Title%3A%20RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching%0AAuthor%3A%20Johan%20Edstedt%20and%20David%20Nordstr%C3%B6m%20and%20Yushan%20Zhang%20and%20Georg%20B%C3%B6kman%20and%20Jonathan%20Astermark%20and%20Viktor%20Larsson%20and%20Anders%20Heyden%20and%20Fredrik%20Kahl%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Michael%20Felsberg%0AAbstract%3A%20Dense%20feature%20matching%20aims%20to%20estimate%20all%20correspondences%20between%20two%20images%20of%20a%203D%20scene%20and%20has%20recently%20been%20established%20as%20the%20gold-standard%20due%20to%20its%20high%20accuracy%20and%20robustness.%20However%2C%20existing%20dense%20matchers%20still%20fail%20or%20perform%20poorly%20for%20many%20hard%20real-world%20scenarios%2C%20and%20high-precision%20models%20are%20often%20slow%2C%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20attack%20these%20weaknesses%20on%20a%20wide%20front%20through%20a%20series%20of%20systematic%20improvements%20that%20together%20yield%20a%20significantly%20better%20model.%20In%20particular%2C%20we%20construct%20a%20novel%20matching%20architecture%20and%20loss%2C%20which%2C%20combined%20with%20a%20curated%20diverse%20training%20distribution%2C%20enables%20our%20model%20to%20solve%20many%20complex%20matching%20tasks.%20We%20further%20make%20training%20faster%20through%20a%20decoupled%20two-stage%20matching-then-refinement%20pipeline%2C%20and%20at%20the%20same%20time%2C%20significantly%20reduce%20refinement%20memory%20usage%20through%20a%20custom%20CUDA%20kernel.%20Finally%2C%20we%20leverage%20the%20recent%20DINOv3%20foundation%20model%20along%20with%20multiple%20other%20insights%20to%20make%20the%20model%20more%20robust%20and%20unbiased.%20In%20our%20extensive%20set%20of%20experiments%20we%20show%20that%20the%20resulting%20novel%20matcher%20sets%20a%20new%20state-of-the-art%2C%20being%20significantly%20more%20accurate%20than%20its%20predecessors.%20Code%20is%20available%20at%20https%3A//github.com/Parskatt/romav2%0ALink%3A%20http%3A//arxiv.org/abs/2511.15706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoMa%2520v2%253A%2520Harder%2520Better%2520Faster%2520Denser%2520Feature%2520Matching%26entry.906535625%3DJohan%2520Edstedt%2520and%2520David%2520Nordstr%25C3%25B6m%2520and%2520Yushan%2520Zhang%2520and%2520Georg%2520B%25C3%25B6kman%2520and%2520Jonathan%2520Astermark%2520and%2520Viktor%2520Larsson%2520and%2520Anders%2520Heyden%2520and%2520Fredrik%2520Kahl%2520and%2520M%25C3%25A5rten%2520Wadenb%25C3%25A4ck%2520and%2520Michael%2520Felsberg%26entry.1292438233%3DDense%2520feature%2520matching%2520aims%2520to%2520estimate%2520all%2520correspondences%2520between%2520two%2520images%2520of%2520a%25203D%2520scene%2520and%2520has%2520recently%2520been%2520established%2520as%2520the%2520gold-standard%2520due%2520to%2520its%2520high%2520accuracy%2520and%2520robustness.%2520However%252C%2520existing%2520dense%2520matchers%2520still%2520fail%2520or%2520perform%2520poorly%2520for%2520many%2520hard%2520real-world%2520scenarios%252C%2520and%2520high-precision%2520models%2520are%2520often%2520slow%252C%2520limiting%2520their%2520applicability.%2520In%2520this%2520paper%252C%2520we%2520attack%2520these%2520weaknesses%2520on%2520a%2520wide%2520front%2520through%2520a%2520series%2520of%2520systematic%2520improvements%2520that%2520together%2520yield%2520a%2520significantly%2520better%2520model.%2520In%2520particular%252C%2520we%2520construct%2520a%2520novel%2520matching%2520architecture%2520and%2520loss%252C%2520which%252C%2520combined%2520with%2520a%2520curated%2520diverse%2520training%2520distribution%252C%2520enables%2520our%2520model%2520to%2520solve%2520many%2520complex%2520matching%2520tasks.%2520We%2520further%2520make%2520training%2520faster%2520through%2520a%2520decoupled%2520two-stage%2520matching-then-refinement%2520pipeline%252C%2520and%2520at%2520the%2520same%2520time%252C%2520significantly%2520reduce%2520refinement%2520memory%2520usage%2520through%2520a%2520custom%2520CUDA%2520kernel.%2520Finally%252C%2520we%2520leverage%2520the%2520recent%2520DINOv3%2520foundation%2520model%2520along%2520with%2520multiple%2520other%2520insights%2520to%2520make%2520the%2520model%2520more%2520robust%2520and%2520unbiased.%2520In%2520our%2520extensive%2520set%2520of%2520experiments%2520we%2520show%2520that%2520the%2520resulting%2520novel%2520matcher%2520sets%2520a%2520new%2520state-of-the-art%252C%2520being%2520significantly%2520more%2520accurate%2520than%2520its%2520predecessors.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Parskatt/romav2%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching&entry.906535625=Johan%20Edstedt%20and%20David%20Nordstr%C3%B6m%20and%20Yushan%20Zhang%20and%20Georg%20B%C3%B6kman%20and%20Jonathan%20Astermark%20and%20Viktor%20Larsson%20and%20Anders%20Heyden%20and%20Fredrik%20Kahl%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Michael%20Felsberg&entry.1292438233=Dense%20feature%20matching%20aims%20to%20estimate%20all%20correspondences%20between%20two%20images%20of%20a%203D%20scene%20and%20has%20recently%20been%20established%20as%20the%20gold-standard%20due%20to%20its%20high%20accuracy%20and%20robustness.%20However%2C%20existing%20dense%20matchers%20still%20fail%20or%20perform%20poorly%20for%20many%20hard%20real-world%20scenarios%2C%20and%20high-precision%20models%20are%20often%20slow%2C%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20attack%20these%20weaknesses%20on%20a%20wide%20front%20through%20a%20series%20of%20systematic%20improvements%20that%20together%20yield%20a%20significantly%20better%20model.%20In%20particular%2C%20we%20construct%20a%20novel%20matching%20architecture%20and%20loss%2C%20which%2C%20combined%20with%20a%20curated%20diverse%20training%20distribution%2C%20enables%20our%20model%20to%20solve%20many%20complex%20matching%20tasks.%20We%20further%20make%20training%20faster%20through%20a%20decoupled%20two-stage%20matching-then-refinement%20pipeline%2C%20and%20at%20the%20same%20time%2C%20significantly%20reduce%20refinement%20memory%20usage%20through%20a%20custom%20CUDA%20kernel.%20Finally%2C%20we%20leverage%20the%20recent%20DINOv3%20foundation%20model%20along%20with%20multiple%20other%20insights%20to%20make%20the%20model%20more%20robust%20and%20unbiased.%20In%20our%20extensive%20set%20of%20experiments%20we%20show%20that%20the%20resulting%20novel%20matcher%20sets%20a%20new%20state-of-the-art%2C%20being%20significantly%20more%20accurate%20than%20its%20predecessors.%20Code%20is%20available%20at%20https%3A//github.com/Parskatt/romav2&entry.1838667208=http%3A//arxiv.org/abs/2511.15706v1&entry.124074799=Read"},
{"title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction", "author": "Adeela Islam and Stefano Fiorini and Stuart James and Pietro Morerio and Alessio Del Bue", "abstract": "The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 57% and 87% for RMSE Rotation and Translation, respectively.", "link": "http://arxiv.org/abs/2505.21117v3", "date": "2025-11-19", "relevancy": 2.8012, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6042}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5402}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%20Reconstruction&body=Title%3A%20ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%20Reconstruction%0AAuthor%3A%20Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%20including%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%20placement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%20this%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%20for%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%20applicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%20erosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%20reduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%20keypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%20Networks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%20computational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%20multiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%20enhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%20diffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%20on%20prior%20methods%20by%2057%25%20and%2087%25%20for%20RMSE%20Rotation%20and%20Translation%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21117v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReassembleNet%253A%2520Learnable%2520Keypoints%2520and%2520Diffusion%2520for%25202D%2520Fresco%2520Reconstruction%26entry.906535625%3DAdeela%2520Islam%2520and%2520Stefano%2520Fiorini%2520and%2520Stuart%2520James%2520and%2520Pietro%2520Morerio%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3DThe%2520task%2520of%2520reassembly%2520is%2520a%2520significant%2520challenge%2520across%2520multiple%2520domains%252C%2520including%2520archaeology%252C%2520genomics%252C%2520and%2520molecular%2520docking%252C%2520requiring%2520the%2520precise%2520placement%2520and%2520orientation%2520of%2520elements%2520to%2520reconstruct%2520an%2520original%2520structure.%2520In%2520this%2520work%252C%2520we%2520address%2520key%2520limitations%2520in%2520state-of-the-art%2520Deep%2520Learning%2520methods%2520for%2520reassembly%252C%2520namely%2520i%2529%2520scalability%253B%2520ii%2529%2520multimodality%253B%2520and%2520iii%2529%2520real-world%2520applicability%253A%2520beyond%2520square%2520or%2520simple%2520geometric%2520shapes%252C%2520realistic%2520and%2520complex%2520erosion%252C%2520or%2520other%2520real-world%2520problems.%2520We%2520propose%2520ReassembleNet%252C%2520a%2520method%2520that%2520reduces%2520complexity%2520by%2520representing%2520each%2520input%2520piece%2520as%2520a%2520set%2520of%2520contour%2520keypoints%2520and%2520learning%2520to%2520select%2520the%2520most%2520informative%2520ones%2520by%2520Graph%2520Neural%2520Networks%2520pooling%2520inspired%2520techniques.%2520ReassembleNet%2520effectively%2520lowers%2520computational%2520complexity%2520while%2520enabling%2520the%2520integration%2520of%2520features%2520from%2520multiple%2520modalities%252C%2520including%2520both%2520geometric%2520and%2520texture%2520data.%2520Further%2520enhanced%2520through%2520pretraining%2520on%2520a%2520semi-synthetic%2520dataset.%2520We%2520then%2520apply%2520diffusion-based%2520pose%2520estimation%2520to%2520recover%2520the%2520original%2520structure.%2520We%2520improve%2520on%2520prior%2520methods%2520by%252057%2525%2520and%252087%2525%2520for%2520RMSE%2520Rotation%2520and%2520Translation%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21117v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReassembleNet%3A%20Learnable%20Keypoints%20and%20Diffusion%20for%202D%20Fresco%20Reconstruction&entry.906535625=Adeela%20Islam%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue&entry.1292438233=The%20task%20of%20reassembly%20is%20a%20significant%20challenge%20across%20multiple%20domains%2C%20including%20archaeology%2C%20genomics%2C%20and%20molecular%20docking%2C%20requiring%20the%20precise%20placement%20and%20orientation%20of%20elements%20to%20reconstruct%20an%20original%20structure.%20In%20this%20work%2C%20we%20address%20key%20limitations%20in%20state-of-the-art%20Deep%20Learning%20methods%20for%20reassembly%2C%20namely%20i%29%20scalability%3B%20ii%29%20multimodality%3B%20and%20iii%29%20real-world%20applicability%3A%20beyond%20square%20or%20simple%20geometric%20shapes%2C%20realistic%20and%20complex%20erosion%2C%20or%20other%20real-world%20problems.%20We%20propose%20ReassembleNet%2C%20a%20method%20that%20reduces%20complexity%20by%20representing%20each%20input%20piece%20as%20a%20set%20of%20contour%20keypoints%20and%20learning%20to%20select%20the%20most%20informative%20ones%20by%20Graph%20Neural%20Networks%20pooling%20inspired%20techniques.%20ReassembleNet%20effectively%20lowers%20computational%20complexity%20while%20enabling%20the%20integration%20of%20features%20from%20multiple%20modalities%2C%20including%20both%20geometric%20and%20texture%20data.%20Further%20enhanced%20through%20pretraining%20on%20a%20semi-synthetic%20dataset.%20We%20then%20apply%20diffusion-based%20pose%20estimation%20to%20recover%20the%20original%20structure.%20We%20improve%20on%20prior%20methods%20by%2057%25%20and%2087%25%20for%20RMSE%20Rotation%20and%20Translation%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2505.21117v3&entry.124074799=Read"},
{"title": "When to Think and When to Look: Uncertainty-Guided Lookback", "author": "Jing Bi and Filippos Bellos and Junjia Guo and Yayuan Li and Chao Huang and  Yunlong and  Tang and Luchuan Song and Susan Liang and  Zhongfei and  Zhang and Jason J. Corso and Chenliang Xu", "abstract": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.", "link": "http://arxiv.org/abs/2511.15613v1", "date": "2025-11-19", "relevancy": 2.7831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback&body=Title%3A%20When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback%0AAuthor%3A%20Jing%20Bi%20and%20Filippos%20Bellos%20and%20Junjia%20Guo%20and%20Yayuan%20Li%20and%20Chao%20Huang%20and%20%20Yunlong%20and%20%20Tang%20and%20Luchuan%20Song%20and%20Susan%20Liang%20and%20%20Zhongfei%20and%20%20Zhang%20and%20Jason%20J.%20Corso%20and%20Chenliang%20Xu%0AAbstract%3A%20Test-time%20thinking%20%28that%20is%2C%20generating%20explicit%20intermediate%20reasoning%20chains%29%20is%20known%20to%20boost%20performance%20in%20large%20language%20models%20and%20has%20recently%20shown%20strong%20gains%20for%20large%20vision%20language%20models%20%28LVLMs%29.%20However%2C%20despite%20these%20promising%20results%2C%20there%20is%20still%20no%20systematic%20analysis%20of%20how%20thinking%20actually%20affects%20visual%20reasoning.%20We%20provide%20the%20first%20such%20analysis%20with%20a%20large%20scale%2C%20controlled%20comparison%20of%20thinking%20for%20LVLMs%2C%20evaluating%20ten%20variants%20from%20the%20InternVL3.5%20and%20Qwen3-VL%20families%20on%20MMMU-val%20under%20generous%20token%20budgets%20and%20multi%20pass%20decoding.%20We%20show%20that%20more%20thinking%20is%20not%20always%20better%3B%20long%20chains%20often%20yield%20long%20wrong%20trajectories%20that%20ignore%20the%20image%20and%20underperform%20the%20same%20models%20run%20in%20standard%20instruct%20mode.%20A%20deeper%20analysis%20reveals%20that%20certain%20short%20lookback%20phrases%2C%20which%20explicitly%20refer%20back%20to%20the%20image%2C%20are%20strongly%20enriched%20in%20successful%20trajectories%20and%20correlate%20with%20better%20visual%20grounding.%20Building%20on%20this%20insight%2C%20we%20propose%20uncertainty%20guided%20lookback%2C%20a%20training%20free%20decoding%20strategy%20that%20combines%20an%20uncertainty%20signal%20with%20adaptive%20lookback%20prompts%20and%20breadth%20search.%20Our%20method%20improves%20overall%20MMMU%20performance%2C%20delivers%20the%20largest%20gains%20in%20categories%20where%20standard%20thinking%20is%20weak%2C%20and%20outperforms%20several%20strong%20decoding%20baselines%2C%20setting%20a%20new%20state%20of%20the%20art%20under%20fixed%20model%20families%20and%20token%20budgets.%20We%20further%20show%20that%20this%20decoding%20strategy%20generalizes%2C%20yielding%20consistent%20improvements%20on%20five%20additional%20benchmarks%2C%20including%20two%20broad%20multimodal%20suites%20and%20math%20focused%20visual%20reasoning%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520to%2520Think%2520and%2520When%2520to%2520Look%253A%2520Uncertainty-Guided%2520Lookback%26entry.906535625%3DJing%2520Bi%2520and%2520Filippos%2520Bellos%2520and%2520Junjia%2520Guo%2520and%2520Yayuan%2520Li%2520and%2520Chao%2520Huang%2520and%2520%2520Yunlong%2520and%2520%2520Tang%2520and%2520Luchuan%2520Song%2520and%2520Susan%2520Liang%2520and%2520%2520Zhongfei%2520and%2520%2520Zhang%2520and%2520Jason%2520J.%2520Corso%2520and%2520Chenliang%2520Xu%26entry.1292438233%3DTest-time%2520thinking%2520%2528that%2520is%252C%2520generating%2520explicit%2520intermediate%2520reasoning%2520chains%2529%2520is%2520known%2520to%2520boost%2520performance%2520in%2520large%2520language%2520models%2520and%2520has%2520recently%2520shown%2520strong%2520gains%2520for%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529.%2520However%252C%2520despite%2520these%2520promising%2520results%252C%2520there%2520is%2520still%2520no%2520systematic%2520analysis%2520of%2520how%2520thinking%2520actually%2520affects%2520visual%2520reasoning.%2520We%2520provide%2520the%2520first%2520such%2520analysis%2520with%2520a%2520large%2520scale%252C%2520controlled%2520comparison%2520of%2520thinking%2520for%2520LVLMs%252C%2520evaluating%2520ten%2520variants%2520from%2520the%2520InternVL3.5%2520and%2520Qwen3-VL%2520families%2520on%2520MMMU-val%2520under%2520generous%2520token%2520budgets%2520and%2520multi%2520pass%2520decoding.%2520We%2520show%2520that%2520more%2520thinking%2520is%2520not%2520always%2520better%253B%2520long%2520chains%2520often%2520yield%2520long%2520wrong%2520trajectories%2520that%2520ignore%2520the%2520image%2520and%2520underperform%2520the%2520same%2520models%2520run%2520in%2520standard%2520instruct%2520mode.%2520A%2520deeper%2520analysis%2520reveals%2520that%2520certain%2520short%2520lookback%2520phrases%252C%2520which%2520explicitly%2520refer%2520back%2520to%2520the%2520image%252C%2520are%2520strongly%2520enriched%2520in%2520successful%2520trajectories%2520and%2520correlate%2520with%2520better%2520visual%2520grounding.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520uncertainty%2520guided%2520lookback%252C%2520a%2520training%2520free%2520decoding%2520strategy%2520that%2520combines%2520an%2520uncertainty%2520signal%2520with%2520adaptive%2520lookback%2520prompts%2520and%2520breadth%2520search.%2520Our%2520method%2520improves%2520overall%2520MMMU%2520performance%252C%2520delivers%2520the%2520largest%2520gains%2520in%2520categories%2520where%2520standard%2520thinking%2520is%2520weak%252C%2520and%2520outperforms%2520several%2520strong%2520decoding%2520baselines%252C%2520setting%2520a%2520new%2520state%2520of%2520the%2520art%2520under%2520fixed%2520model%2520families%2520and%2520token%2520budgets.%2520We%2520further%2520show%2520that%2520this%2520decoding%2520strategy%2520generalizes%252C%2520yielding%2520consistent%2520improvements%2520on%2520five%2520additional%2520benchmarks%252C%2520including%2520two%2520broad%2520multimodal%2520suites%2520and%2520math%2520focused%2520visual%2520reasoning%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback&entry.906535625=Jing%20Bi%20and%20Filippos%20Bellos%20and%20Junjia%20Guo%20and%20Yayuan%20Li%20and%20Chao%20Huang%20and%20%20Yunlong%20and%20%20Tang%20and%20Luchuan%20Song%20and%20Susan%20Liang%20and%20%20Zhongfei%20and%20%20Zhang%20and%20Jason%20J.%20Corso%20and%20Chenliang%20Xu&entry.1292438233=Test-time%20thinking%20%28that%20is%2C%20generating%20explicit%20intermediate%20reasoning%20chains%29%20is%20known%20to%20boost%20performance%20in%20large%20language%20models%20and%20has%20recently%20shown%20strong%20gains%20for%20large%20vision%20language%20models%20%28LVLMs%29.%20However%2C%20despite%20these%20promising%20results%2C%20there%20is%20still%20no%20systematic%20analysis%20of%20how%20thinking%20actually%20affects%20visual%20reasoning.%20We%20provide%20the%20first%20such%20analysis%20with%20a%20large%20scale%2C%20controlled%20comparison%20of%20thinking%20for%20LVLMs%2C%20evaluating%20ten%20variants%20from%20the%20InternVL3.5%20and%20Qwen3-VL%20families%20on%20MMMU-val%20under%20generous%20token%20budgets%20and%20multi%20pass%20decoding.%20We%20show%20that%20more%20thinking%20is%20not%20always%20better%3B%20long%20chains%20often%20yield%20long%20wrong%20trajectories%20that%20ignore%20the%20image%20and%20underperform%20the%20same%20models%20run%20in%20standard%20instruct%20mode.%20A%20deeper%20analysis%20reveals%20that%20certain%20short%20lookback%20phrases%2C%20which%20explicitly%20refer%20back%20to%20the%20image%2C%20are%20strongly%20enriched%20in%20successful%20trajectories%20and%20correlate%20with%20better%20visual%20grounding.%20Building%20on%20this%20insight%2C%20we%20propose%20uncertainty%20guided%20lookback%2C%20a%20training%20free%20decoding%20strategy%20that%20combines%20an%20uncertainty%20signal%20with%20adaptive%20lookback%20prompts%20and%20breadth%20search.%20Our%20method%20improves%20overall%20MMMU%20performance%2C%20delivers%20the%20largest%20gains%20in%20categories%20where%20standard%20thinking%20is%20weak%2C%20and%20outperforms%20several%20strong%20decoding%20baselines%2C%20setting%20a%20new%20state%20of%20the%20art%20under%20fixed%20model%20families%20and%20token%20budgets.%20We%20further%20show%20that%20this%20decoding%20strategy%20generalizes%2C%20yielding%20consistent%20improvements%20on%20five%20additional%20benchmarks%2C%20including%20two%20broad%20multimodal%20suites%20and%20math%20focused%20visual%20reasoning%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.15613v1&entry.124074799=Read"},
{"title": "Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition", "author": "Xufei Wang and Junqiao Zhao and Siyue Tao and Qiwen Gu and Wonbong Kim and Tiantian Feng", "abstract": "LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.", "link": "http://arxiv.org/abs/2511.15597v1", "date": "2025-11-19", "relevancy": 2.7477, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Mistakes%3A%20Loss-Aware%20Memory%20Enhanced%20Continual%20Learning%20for%20LiDAR%20Place%20Recognition&body=Title%3A%20Learning%20from%20Mistakes%3A%20Loss-Aware%20Memory%20Enhanced%20Continual%20Learning%20for%20LiDAR%20Place%20Recognition%0AAuthor%3A%20Xufei%20Wang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Qiwen%20Gu%20and%20Wonbong%20Kim%20and%20Tiantian%20Feng%0AAbstract%3A%20LiDAR%20place%20recognition%20plays%20a%20crucial%20role%20in%20SLAM%2C%20robot%20navigation%2C%20and%20autonomous%20driving.%20However%2C%20existing%20LiDAR%20place%20recognition%20methods%20often%20struggle%20to%20adapt%20to%20new%20environments%20without%20forgetting%20previously%20learned%20knowledge%2C%20a%20challenge%20widely%20known%20as%20catastrophic%20forgetting.%20To%20address%20this%20issue%2C%20we%20propose%20KDF%2B%2C%20a%20novel%20continual%20learning%20framework%20for%20LiDAR%20place%20recognition%20that%20extends%20the%20KDF%20paradigm%20with%20a%20loss-aware%20sampling%20strategy%20and%20a%20rehearsal%20enhancement%20mechanism.%20The%20proposed%20sampling%20strategy%20estimates%20the%20learning%20difficulty%20of%20each%20sample%20via%20its%20loss%20value%20and%20selects%20samples%20for%20replay%20according%20to%20their%20estimated%20difficulty.%20Harder%20samples%2C%20which%20tend%20to%20encode%20more%20discriminative%20information%2C%20are%20sampled%20with%20higher%20probability%20while%20maintaining%20distributional%20coverage%20across%20the%20dataset.%20In%20addition%2C%20the%20rehearsal%20enhancement%20mechanism%20encourages%20memory%20samples%20to%20be%20further%20refined%20during%20new-task%20training%20by%20slightly%20reducing%20their%20loss%20relative%20to%20previous%20tasks%2C%20thereby%20reinforcing%20long-term%20knowledge%20retention.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20KDF%2B%20consistently%20outperforms%20existing%20continual%20learning%20methods%20and%20can%20be%20seamlessly%20integrated%20into%20state-of-the-art%20continual%20learning%20for%20LiDAR%20place%20recognition%20frameworks%20to%20yield%20significant%20and%20stable%20performance%20gains.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/repo/KDF-plus.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Mistakes%253A%2520Loss-Aware%2520Memory%2520Enhanced%2520Continual%2520Learning%2520for%2520LiDAR%2520Place%2520Recognition%26entry.906535625%3DXufei%2520Wang%2520and%2520Junqiao%2520Zhao%2520and%2520Siyue%2520Tao%2520and%2520Qiwen%2520Gu%2520and%2520Wonbong%2520Kim%2520and%2520Tiantian%2520Feng%26entry.1292438233%3DLiDAR%2520place%2520recognition%2520plays%2520a%2520crucial%2520role%2520in%2520SLAM%252C%2520robot%2520navigation%252C%2520and%2520autonomous%2520driving.%2520However%252C%2520existing%2520LiDAR%2520place%2520recognition%2520methods%2520often%2520struggle%2520to%2520adapt%2520to%2520new%2520environments%2520without%2520forgetting%2520previously%2520learned%2520knowledge%252C%2520a%2520challenge%2520widely%2520known%2520as%2520catastrophic%2520forgetting.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520KDF%252B%252C%2520a%2520novel%2520continual%2520learning%2520framework%2520for%2520LiDAR%2520place%2520recognition%2520that%2520extends%2520the%2520KDF%2520paradigm%2520with%2520a%2520loss-aware%2520sampling%2520strategy%2520and%2520a%2520rehearsal%2520enhancement%2520mechanism.%2520The%2520proposed%2520sampling%2520strategy%2520estimates%2520the%2520learning%2520difficulty%2520of%2520each%2520sample%2520via%2520its%2520loss%2520value%2520and%2520selects%2520samples%2520for%2520replay%2520according%2520to%2520their%2520estimated%2520difficulty.%2520Harder%2520samples%252C%2520which%2520tend%2520to%2520encode%2520more%2520discriminative%2520information%252C%2520are%2520sampled%2520with%2520higher%2520probability%2520while%2520maintaining%2520distributional%2520coverage%2520across%2520the%2520dataset.%2520In%2520addition%252C%2520the%2520rehearsal%2520enhancement%2520mechanism%2520encourages%2520memory%2520samples%2520to%2520be%2520further%2520refined%2520during%2520new-task%2520training%2520by%2520slightly%2520reducing%2520their%2520loss%2520relative%2520to%2520previous%2520tasks%252C%2520thereby%2520reinforcing%2520long-term%2520knowledge%2520retention.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520KDF%252B%2520consistently%2520outperforms%2520existing%2520continual%2520learning%2520methods%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520state-of-the-art%2520continual%2520learning%2520for%2520LiDAR%2520place%2520recognition%2520frameworks%2520to%2520yield%2520significant%2520and%2520stable%2520performance%2520gains.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/repo/KDF-plus.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Mistakes%3A%20Loss-Aware%20Memory%20Enhanced%20Continual%20Learning%20for%20LiDAR%20Place%20Recognition&entry.906535625=Xufei%20Wang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Qiwen%20Gu%20and%20Wonbong%20Kim%20and%20Tiantian%20Feng&entry.1292438233=LiDAR%20place%20recognition%20plays%20a%20crucial%20role%20in%20SLAM%2C%20robot%20navigation%2C%20and%20autonomous%20driving.%20However%2C%20existing%20LiDAR%20place%20recognition%20methods%20often%20struggle%20to%20adapt%20to%20new%20environments%20without%20forgetting%20previously%20learned%20knowledge%2C%20a%20challenge%20widely%20known%20as%20catastrophic%20forgetting.%20To%20address%20this%20issue%2C%20we%20propose%20KDF%2B%2C%20a%20novel%20continual%20learning%20framework%20for%20LiDAR%20place%20recognition%20that%20extends%20the%20KDF%20paradigm%20with%20a%20loss-aware%20sampling%20strategy%20and%20a%20rehearsal%20enhancement%20mechanism.%20The%20proposed%20sampling%20strategy%20estimates%20the%20learning%20difficulty%20of%20each%20sample%20via%20its%20loss%20value%20and%20selects%20samples%20for%20replay%20according%20to%20their%20estimated%20difficulty.%20Harder%20samples%2C%20which%20tend%20to%20encode%20more%20discriminative%20information%2C%20are%20sampled%20with%20higher%20probability%20while%20maintaining%20distributional%20coverage%20across%20the%20dataset.%20In%20addition%2C%20the%20rehearsal%20enhancement%20mechanism%20encourages%20memory%20samples%20to%20be%20further%20refined%20during%20new-task%20training%20by%20slightly%20reducing%20their%20loss%20relative%20to%20previous%20tasks%2C%20thereby%20reinforcing%20long-term%20knowledge%20retention.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20KDF%2B%20consistently%20outperforms%20existing%20continual%20learning%20methods%20and%20can%20be%20seamlessly%20integrated%20into%20state-of-the-art%20continual%20learning%20for%20LiDAR%20place%20recognition%20frameworks%20to%20yield%20significant%20and%20stable%20performance%20gains.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/repo/KDF-plus.&entry.1838667208=http%3A//arxiv.org/abs/2511.15597v1&entry.124074799=Read"},
{"title": "US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery", "author": "Miruna-Alexandra Gafencu and Yordanka Velikova and Nassir Navab and Mohammad Farid Azampour", "abstract": "Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete", "link": "http://arxiv.org/abs/2511.15600v1", "date": "2025-11-19", "relevancy": 2.7259, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20US-X%20Complete%3A%20A%20Multi-Modal%20Approach%20to%20Anatomical%203D%20Shape%20Recovery&body=Title%3A%20US-X%20Complete%3A%20A%20Multi-Modal%20Approach%20to%20Anatomical%203D%20Shape%20Recovery%0AAuthor%3A%20Miruna-Alexandra%20Gafencu%20and%20Yordanka%20Velikova%20and%20Nassir%20Navab%20and%20Mohammad%20Farid%20Azampour%0AAbstract%3A%20Ultrasound%20offers%20a%20radiation-free%2C%20cost-effective%20solution%20for%20real-time%20visualization%20of%20spinal%20landmarks%2C%20paraspinal%20soft%20tissues%20and%20neurovascular%20structures%2C%20making%20it%20valuable%20for%20intraoperative%20guidance%20during%20spinal%20procedures.%20However%2C%20ultrasound%20suffers%20from%20inherent%20limitations%20in%20visualizing%20complete%20vertebral%20anatomy%2C%20in%20particular%20vertebral%20bodies%2C%20due%20to%20acoustic%20shadowing%20effects%20caused%20by%20bone.%20In%20this%20work%2C%20we%20present%20a%20novel%20multi-modal%20deep%20learning%20method%20for%20completing%20occluded%20anatomical%20structures%20in%203D%20ultrasound%20by%20leveraging%20complementary%20information%20from%20a%20single%20X-ray%20image.%20To%20enable%20training%2C%20we%20generate%20paired%20training%20data%20consisting%20of%3A%20%281%29%202D%20lateral%20vertebral%20views%20that%20simulate%20X-ray%20scans%2C%20and%20%282%29%203D%20partial%20vertebrae%20representations%20that%20mimic%20the%20limited%20visibility%20and%20occlusions%20encountered%20during%20ultrasound%20spine%20imaging.%20Our%20method%20integrates%20morphological%20information%20from%20both%20imaging%20modalities%20and%20demonstrates%20significant%20improvements%20in%20vertebral%20reconstruction%20%28p%20%3C%200.001%29%20compared%20to%20state%20of%20art%20in%203D%20ultrasound%20vertebral%20completion.%20We%20perform%20phantom%20studies%20as%20an%20initial%20step%20to%20future%20clinical%20translation%2C%20and%20achieve%20a%20more%20accurate%2C%20complete%20volumetric%20lumbar%20spine%20visualization%20overlayed%20on%20the%20ultrasound%20scan%20without%20the%20need%20for%20registration%20with%20preoperative%20modalities%20such%20as%20computed%20tomography.%20This%20demonstrates%20that%20integrating%20a%20single%20X-ray%20projection%20mitigates%20ultrasound%27s%20key%20limitation%20while%20preserving%20its%20strengths%20as%20the%20primary%20imaging%20modality.%20Code%20and%20data%20can%20be%20found%20at%20https%3A//github.com/miruna20/US-X-Complete%0ALink%3A%20http%3A//arxiv.org/abs/2511.15600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUS-X%2520Complete%253A%2520A%2520Multi-Modal%2520Approach%2520to%2520Anatomical%25203D%2520Shape%2520Recovery%26entry.906535625%3DMiruna-Alexandra%2520Gafencu%2520and%2520Yordanka%2520Velikova%2520and%2520Nassir%2520Navab%2520and%2520Mohammad%2520Farid%2520Azampour%26entry.1292438233%3DUltrasound%2520offers%2520a%2520radiation-free%252C%2520cost-effective%2520solution%2520for%2520real-time%2520visualization%2520of%2520spinal%2520landmarks%252C%2520paraspinal%2520soft%2520tissues%2520and%2520neurovascular%2520structures%252C%2520making%2520it%2520valuable%2520for%2520intraoperative%2520guidance%2520during%2520spinal%2520procedures.%2520However%252C%2520ultrasound%2520suffers%2520from%2520inherent%2520limitations%2520in%2520visualizing%2520complete%2520vertebral%2520anatomy%252C%2520in%2520particular%2520vertebral%2520bodies%252C%2520due%2520to%2520acoustic%2520shadowing%2520effects%2520caused%2520by%2520bone.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520multi-modal%2520deep%2520learning%2520method%2520for%2520completing%2520occluded%2520anatomical%2520structures%2520in%25203D%2520ultrasound%2520by%2520leveraging%2520complementary%2520information%2520from%2520a%2520single%2520X-ray%2520image.%2520To%2520enable%2520training%252C%2520we%2520generate%2520paired%2520training%2520data%2520consisting%2520of%253A%2520%25281%2529%25202D%2520lateral%2520vertebral%2520views%2520that%2520simulate%2520X-ray%2520scans%252C%2520and%2520%25282%2529%25203D%2520partial%2520vertebrae%2520representations%2520that%2520mimic%2520the%2520limited%2520visibility%2520and%2520occlusions%2520encountered%2520during%2520ultrasound%2520spine%2520imaging.%2520Our%2520method%2520integrates%2520morphological%2520information%2520from%2520both%2520imaging%2520modalities%2520and%2520demonstrates%2520significant%2520improvements%2520in%2520vertebral%2520reconstruction%2520%2528p%2520%253C%25200.001%2529%2520compared%2520to%2520state%2520of%2520art%2520in%25203D%2520ultrasound%2520vertebral%2520completion.%2520We%2520perform%2520phantom%2520studies%2520as%2520an%2520initial%2520step%2520to%2520future%2520clinical%2520translation%252C%2520and%2520achieve%2520a%2520more%2520accurate%252C%2520complete%2520volumetric%2520lumbar%2520spine%2520visualization%2520overlayed%2520on%2520the%2520ultrasound%2520scan%2520without%2520the%2520need%2520for%2520registration%2520with%2520preoperative%2520modalities%2520such%2520as%2520computed%2520tomography.%2520This%2520demonstrates%2520that%2520integrating%2520a%2520single%2520X-ray%2520projection%2520mitigates%2520ultrasound%2527s%2520key%2520limitation%2520while%2520preserving%2520its%2520strengths%2520as%2520the%2520primary%2520imaging%2520modality.%2520Code%2520and%2520data%2520can%2520be%2520found%2520at%2520https%253A//github.com/miruna20/US-X-Complete%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=US-X%20Complete%3A%20A%20Multi-Modal%20Approach%20to%20Anatomical%203D%20Shape%20Recovery&entry.906535625=Miruna-Alexandra%20Gafencu%20and%20Yordanka%20Velikova%20and%20Nassir%20Navab%20and%20Mohammad%20Farid%20Azampour&entry.1292438233=Ultrasound%20offers%20a%20radiation-free%2C%20cost-effective%20solution%20for%20real-time%20visualization%20of%20spinal%20landmarks%2C%20paraspinal%20soft%20tissues%20and%20neurovascular%20structures%2C%20making%20it%20valuable%20for%20intraoperative%20guidance%20during%20spinal%20procedures.%20However%2C%20ultrasound%20suffers%20from%20inherent%20limitations%20in%20visualizing%20complete%20vertebral%20anatomy%2C%20in%20particular%20vertebral%20bodies%2C%20due%20to%20acoustic%20shadowing%20effects%20caused%20by%20bone.%20In%20this%20work%2C%20we%20present%20a%20novel%20multi-modal%20deep%20learning%20method%20for%20completing%20occluded%20anatomical%20structures%20in%203D%20ultrasound%20by%20leveraging%20complementary%20information%20from%20a%20single%20X-ray%20image.%20To%20enable%20training%2C%20we%20generate%20paired%20training%20data%20consisting%20of%3A%20%281%29%202D%20lateral%20vertebral%20views%20that%20simulate%20X-ray%20scans%2C%20and%20%282%29%203D%20partial%20vertebrae%20representations%20that%20mimic%20the%20limited%20visibility%20and%20occlusions%20encountered%20during%20ultrasound%20spine%20imaging.%20Our%20method%20integrates%20morphological%20information%20from%20both%20imaging%20modalities%20and%20demonstrates%20significant%20improvements%20in%20vertebral%20reconstruction%20%28p%20%3C%200.001%29%20compared%20to%20state%20of%20art%20in%203D%20ultrasound%20vertebral%20completion.%20We%20perform%20phantom%20studies%20as%20an%20initial%20step%20to%20future%20clinical%20translation%2C%20and%20achieve%20a%20more%20accurate%2C%20complete%20volumetric%20lumbar%20spine%20visualization%20overlayed%20on%20the%20ultrasound%20scan%20without%20the%20need%20for%20registration%20with%20preoperative%20modalities%20such%20as%20computed%20tomography.%20This%20demonstrates%20that%20integrating%20a%20single%20X-ray%20projection%20mitigates%20ultrasound%27s%20key%20limitation%20while%20preserving%20its%20strengths%20as%20the%20primary%20imaging%20modality.%20Code%20and%20data%20can%20be%20found%20at%20https%3A//github.com/miruna20/US-X-Complete&entry.1838667208=http%3A//arxiv.org/abs/2511.15600v1&entry.124074799=Read"},
{"title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization", "author": "Yikun Wang and Zuyan Liu and Ziyi Wang and Pengfei Liu and Han Hu and Yongming Rao", "abstract": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.", "link": "http://arxiv.org/abs/2511.15705v1", "date": "2025-11-19", "relevancy": 2.6924, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization&body=Title%3A%20GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization%0AAuthor%3A%20Yikun%20Wang%20and%20Zuyan%20Liu%20and%20Ziyi%20Wang%20and%20Pengfei%20Liu%20and%20Han%20Hu%20and%20Yongming%20Rao%0AAbstract%3A%20Current%20research%20on%20agentic%20visual%20reasoning%20enables%20deep%20multimodal%20understanding%20but%20primarily%20focuses%20on%20image%20manipulation%20tools%2C%20leaving%20a%20gap%20toward%20more%20general-purpose%20agentic%20models.%20In%20this%20work%2C%20we%20revisit%20the%20geolocalization%20task%2C%20which%20requires%20not%20only%20nuanced%20visual%20grounding%20but%20also%20web%20search%20to%20confirm%20or%20refine%20hypotheses%20during%20reasoning.%20Since%20existing%20geolocalization%20benchmarks%20fail%20to%20meet%20the%20need%20for%20high-resolution%20imagery%20and%20the%20localization%20challenge%20for%20deep%20agentic%20reasoning%2C%20we%20curate%20GeoBench%2C%20a%20benchmark%20that%20includes%20photos%20and%20panoramas%20from%20around%20the%20world%2C%20along%20with%20a%20subset%20of%20satellite%20images%20of%20different%20cities%20to%20rigorously%20evaluate%20the%20geolocalization%20ability%20of%20agentic%20models.%20We%20also%20propose%20GeoVista%2C%20an%20agentic%20model%20that%20seamlessly%20integrates%20tool%20invocation%20within%20the%20reasoning%20loop%2C%20including%20an%20image-zoom-in%20tool%20to%20magnify%20regions%20of%20interest%20and%20a%20web-search%20tool%20to%20retrieve%20related%20web%20information.%20We%20develop%20a%20complete%20training%20pipeline%20for%20it%2C%20including%20a%20cold-start%20supervised%20fine-tuning%20%28SFT%29%20stage%20to%20learn%20reasoning%20patterns%20and%20tool-use%20priors%2C%20followed%20by%20a%20reinforcement%20learning%20%28RL%29%20stage%20to%20further%20enhance%20reasoning%20ability.%20We%20adopt%20a%20hierarchical%20reward%20to%20leverage%20multi-level%20geographical%20information%20and%20improve%20overall%20geolocalization%20performance.%20Experimental%20results%20show%20that%20GeoVista%20surpasses%20other%20open-source%20agentic%20models%20on%20the%20geolocalization%20task%20greatly%20and%20achieves%20performance%20comparable%20to%20closed-source%20models%20such%20as%20Gemini-2.5-flash%20and%20GPT-5%20on%20most%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoVista%253A%2520Web-Augmented%2520Agentic%2520Visual%2520Reasoning%2520for%2520Geolocalization%26entry.906535625%3DYikun%2520Wang%2520and%2520Zuyan%2520Liu%2520and%2520Ziyi%2520Wang%2520and%2520Pengfei%2520Liu%2520and%2520Han%2520Hu%2520and%2520Yongming%2520Rao%26entry.1292438233%3DCurrent%2520research%2520on%2520agentic%2520visual%2520reasoning%2520enables%2520deep%2520multimodal%2520understanding%2520but%2520primarily%2520focuses%2520on%2520image%2520manipulation%2520tools%252C%2520leaving%2520a%2520gap%2520toward%2520more%2520general-purpose%2520agentic%2520models.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520geolocalization%2520task%252C%2520which%2520requires%2520not%2520only%2520nuanced%2520visual%2520grounding%2520but%2520also%2520web%2520search%2520to%2520confirm%2520or%2520refine%2520hypotheses%2520during%2520reasoning.%2520Since%2520existing%2520geolocalization%2520benchmarks%2520fail%2520to%2520meet%2520the%2520need%2520for%2520high-resolution%2520imagery%2520and%2520the%2520localization%2520challenge%2520for%2520deep%2520agentic%2520reasoning%252C%2520we%2520curate%2520GeoBench%252C%2520a%2520benchmark%2520that%2520includes%2520photos%2520and%2520panoramas%2520from%2520around%2520the%2520world%252C%2520along%2520with%2520a%2520subset%2520of%2520satellite%2520images%2520of%2520different%2520cities%2520to%2520rigorously%2520evaluate%2520the%2520geolocalization%2520ability%2520of%2520agentic%2520models.%2520We%2520also%2520propose%2520GeoVista%252C%2520an%2520agentic%2520model%2520that%2520seamlessly%2520integrates%2520tool%2520invocation%2520within%2520the%2520reasoning%2520loop%252C%2520including%2520an%2520image-zoom-in%2520tool%2520to%2520magnify%2520regions%2520of%2520interest%2520and%2520a%2520web-search%2520tool%2520to%2520retrieve%2520related%2520web%2520information.%2520We%2520develop%2520a%2520complete%2520training%2520pipeline%2520for%2520it%252C%2520including%2520a%2520cold-start%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520stage%2520to%2520learn%2520reasoning%2520patterns%2520and%2520tool-use%2520priors%252C%2520followed%2520by%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520stage%2520to%2520further%2520enhance%2520reasoning%2520ability.%2520We%2520adopt%2520a%2520hierarchical%2520reward%2520to%2520leverage%2520multi-level%2520geographical%2520information%2520and%2520improve%2520overall%2520geolocalization%2520performance.%2520Experimental%2520results%2520show%2520that%2520GeoVista%2520surpasses%2520other%2520open-source%2520agentic%2520models%2520on%2520the%2520geolocalization%2520task%2520greatly%2520and%2520achieves%2520performance%2520comparable%2520to%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-flash%2520and%2520GPT-5%2520on%2520most%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoVista%3A%20Web-Augmented%20Agentic%20Visual%20Reasoning%20for%20Geolocalization&entry.906535625=Yikun%20Wang%20and%20Zuyan%20Liu%20and%20Ziyi%20Wang%20and%20Pengfei%20Liu%20and%20Han%20Hu%20and%20Yongming%20Rao&entry.1292438233=Current%20research%20on%20agentic%20visual%20reasoning%20enables%20deep%20multimodal%20understanding%20but%20primarily%20focuses%20on%20image%20manipulation%20tools%2C%20leaving%20a%20gap%20toward%20more%20general-purpose%20agentic%20models.%20In%20this%20work%2C%20we%20revisit%20the%20geolocalization%20task%2C%20which%20requires%20not%20only%20nuanced%20visual%20grounding%20but%20also%20web%20search%20to%20confirm%20or%20refine%20hypotheses%20during%20reasoning.%20Since%20existing%20geolocalization%20benchmarks%20fail%20to%20meet%20the%20need%20for%20high-resolution%20imagery%20and%20the%20localization%20challenge%20for%20deep%20agentic%20reasoning%2C%20we%20curate%20GeoBench%2C%20a%20benchmark%20that%20includes%20photos%20and%20panoramas%20from%20around%20the%20world%2C%20along%20with%20a%20subset%20of%20satellite%20images%20of%20different%20cities%20to%20rigorously%20evaluate%20the%20geolocalization%20ability%20of%20agentic%20models.%20We%20also%20propose%20GeoVista%2C%20an%20agentic%20model%20that%20seamlessly%20integrates%20tool%20invocation%20within%20the%20reasoning%20loop%2C%20including%20an%20image-zoom-in%20tool%20to%20magnify%20regions%20of%20interest%20and%20a%20web-search%20tool%20to%20retrieve%20related%20web%20information.%20We%20develop%20a%20complete%20training%20pipeline%20for%20it%2C%20including%20a%20cold-start%20supervised%20fine-tuning%20%28SFT%29%20stage%20to%20learn%20reasoning%20patterns%20and%20tool-use%20priors%2C%20followed%20by%20a%20reinforcement%20learning%20%28RL%29%20stage%20to%20further%20enhance%20reasoning%20ability.%20We%20adopt%20a%20hierarchical%20reward%20to%20leverage%20multi-level%20geographical%20information%20and%20improve%20overall%20geolocalization%20performance.%20Experimental%20results%20show%20that%20GeoVista%20surpasses%20other%20open-source%20agentic%20models%20on%20the%20geolocalization%20task%20greatly%20and%20achieves%20performance%20comparable%20to%20closed-source%20models%20such%20as%20Gemini-2.5-flash%20and%20GPT-5%20on%20most%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.15705v1&entry.124074799=Read"},
{"title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome", "author": "Dabin Jeong and Amirhossein Vahidi and Ciro Ram\u00edrez-Su\u00e1stegui and Marie Moullet and Kevin Ly and Mohammad Vali Sanian and Sebastian Birk and Yinshui Chang and Adam Boxall and Daniyal Jafree and Lloyd Steele and Vijaya Baskar MS and Muzlifah Haniffa and Mohammad Lotfollahi", "abstract": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.", "link": "http://arxiv.org/abs/2511.15464v1", "date": "2025-11-19", "relevancy": 2.6851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5839}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5151}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome&body=Title%3A%20SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome%0AAuthor%3A%20Dabin%20Jeong%20and%20Amirhossein%20Vahidi%20and%20Ciro%20Ram%C3%ADrez-Su%C3%A1stegui%20and%20Marie%20Moullet%20and%20Kevin%20Ly%20and%20Mohammad%20Vali%20Sanian%20and%20Sebastian%20Birk%20and%20Yinshui%20Chang%20and%20Adam%20Boxall%20and%20Daniyal%20Jafree%20and%20Lloyd%20Steele%20and%20Vijaya%20Baskar%20MS%20and%20Muzlifah%20Haniffa%20and%20Mohammad%20Lotfollahi%0AAbstract%3A%20Recent%20advances%20in%20computational%20pathology%20have%20leveraged%20vision-language%20models%20to%20learn%20joint%20representations%20of%20Hematoxylin%20and%20Eosin%20%28HE%29%20images%20with%20spatial%20transcriptomic%20%28ST%29%20profiles.%20However%2C%20existing%20approaches%20typically%20align%20HE%20tiles%20with%20their%20corresponding%20ST%20profiles%20at%20a%20single%20scale%2C%20overlooking%20fine-grained%20cellular%20structures%20and%20their%20spatial%20organization.%20To%20address%20this%2C%20we%20propose%20Sigmma%2C%20a%20multi-modal%20contrastive%20alignment%20framework%20for%20learning%20hierarchical%20representations%20of%20HE%20images%20and%20spatial%20transcriptome%20profiles%20across%20multiple%20scales.%20Sigmma%20introduces%20multi-scale%20contrastive%20alignment%2C%20ensuring%20that%20representations%20learned%20at%20different%20scales%20remain%20coherent%20across%20modalities.%20Furthermore%2C%20by%20representing%20cell%20interactions%20as%20a%20graph%20and%20integrating%20inter-%20and%20intra-subgraph%20relationships%2C%20our%20approach%20effectively%20captures%20cell-cell%20interactions%2C%20ranging%20from%20fine%20to%20coarse%2C%20within%20the%20tissue%20microenvironment.%20We%20demonstrate%20that%20Sigmm%20learns%20representations%20that%20better%20capture%20cross-modal%20correspondences%2C%20leading%20to%20an%20improvement%20of%20avg.%209.78%5C%25%20in%20the%20gene-expression%20prediction%20task%20and%20avg.%2026.93%5C%25%20in%20the%20cross-modal%20retrieval%20task%20across%20datasets.%20We%20further%20show%20that%20it%20learns%20meaningful%20multi-tissue%20organization%20in%20downstream%20analyses.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGMMA%253A%2520Hierarchical%2520Graph-Based%2520Multi-Scale%2520Multi-modal%2520Contrastive%2520Alignment%2520of%2520Histopathology%2520Image%2520and%2520Spatial%2520Transcriptome%26entry.906535625%3DDabin%2520Jeong%2520and%2520Amirhossein%2520Vahidi%2520and%2520Ciro%2520Ram%25C3%25ADrez-Su%25C3%25A1stegui%2520and%2520Marie%2520Moullet%2520and%2520Kevin%2520Ly%2520and%2520Mohammad%2520Vali%2520Sanian%2520and%2520Sebastian%2520Birk%2520and%2520Yinshui%2520Chang%2520and%2520Adam%2520Boxall%2520and%2520Daniyal%2520Jafree%2520and%2520Lloyd%2520Steele%2520and%2520Vijaya%2520Baskar%2520MS%2520and%2520Muzlifah%2520Haniffa%2520and%2520Mohammad%2520Lotfollahi%26entry.1292438233%3DRecent%2520advances%2520in%2520computational%2520pathology%2520have%2520leveraged%2520vision-language%2520models%2520to%2520learn%2520joint%2520representations%2520of%2520Hematoxylin%2520and%2520Eosin%2520%2528HE%2529%2520images%2520with%2520spatial%2520transcriptomic%2520%2528ST%2529%2520profiles.%2520However%252C%2520existing%2520approaches%2520typically%2520align%2520HE%2520tiles%2520with%2520their%2520corresponding%2520ST%2520profiles%2520at%2520a%2520single%2520scale%252C%2520overlooking%2520fine-grained%2520cellular%2520structures%2520and%2520their%2520spatial%2520organization.%2520To%2520address%2520this%252C%2520we%2520propose%2520Sigmma%252C%2520a%2520multi-modal%2520contrastive%2520alignment%2520framework%2520for%2520learning%2520hierarchical%2520representations%2520of%2520HE%2520images%2520and%2520spatial%2520transcriptome%2520profiles%2520across%2520multiple%2520scales.%2520Sigmma%2520introduces%2520multi-scale%2520contrastive%2520alignment%252C%2520ensuring%2520that%2520representations%2520learned%2520at%2520different%2520scales%2520remain%2520coherent%2520across%2520modalities.%2520Furthermore%252C%2520by%2520representing%2520cell%2520interactions%2520as%2520a%2520graph%2520and%2520integrating%2520inter-%2520and%2520intra-subgraph%2520relationships%252C%2520our%2520approach%2520effectively%2520captures%2520cell-cell%2520interactions%252C%2520ranging%2520from%2520fine%2520to%2520coarse%252C%2520within%2520the%2520tissue%2520microenvironment.%2520We%2520demonstrate%2520that%2520Sigmm%2520learns%2520representations%2520that%2520better%2520capture%2520cross-modal%2520correspondences%252C%2520leading%2520to%2520an%2520improvement%2520of%2520avg.%25209.78%255C%2525%2520in%2520the%2520gene-expression%2520prediction%2520task%2520and%2520avg.%252026.93%255C%2525%2520in%2520the%2520cross-modal%2520retrieval%2520task%2520across%2520datasets.%2520We%2520further%2520show%2520that%2520it%2520learns%2520meaningful%2520multi-tissue%2520organization%2520in%2520downstream%2520analyses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome&entry.906535625=Dabin%20Jeong%20and%20Amirhossein%20Vahidi%20and%20Ciro%20Ram%C3%ADrez-Su%C3%A1stegui%20and%20Marie%20Moullet%20and%20Kevin%20Ly%20and%20Mohammad%20Vali%20Sanian%20and%20Sebastian%20Birk%20and%20Yinshui%20Chang%20and%20Adam%20Boxall%20and%20Daniyal%20Jafree%20and%20Lloyd%20Steele%20and%20Vijaya%20Baskar%20MS%20and%20Muzlifah%20Haniffa%20and%20Mohammad%20Lotfollahi&entry.1292438233=Recent%20advances%20in%20computational%20pathology%20have%20leveraged%20vision-language%20models%20to%20learn%20joint%20representations%20of%20Hematoxylin%20and%20Eosin%20%28HE%29%20images%20with%20spatial%20transcriptomic%20%28ST%29%20profiles.%20However%2C%20existing%20approaches%20typically%20align%20HE%20tiles%20with%20their%20corresponding%20ST%20profiles%20at%20a%20single%20scale%2C%20overlooking%20fine-grained%20cellular%20structures%20and%20their%20spatial%20organization.%20To%20address%20this%2C%20we%20propose%20Sigmma%2C%20a%20multi-modal%20contrastive%20alignment%20framework%20for%20learning%20hierarchical%20representations%20of%20HE%20images%20and%20spatial%20transcriptome%20profiles%20across%20multiple%20scales.%20Sigmma%20introduces%20multi-scale%20contrastive%20alignment%2C%20ensuring%20that%20representations%20learned%20at%20different%20scales%20remain%20coherent%20across%20modalities.%20Furthermore%2C%20by%20representing%20cell%20interactions%20as%20a%20graph%20and%20integrating%20inter-%20and%20intra-subgraph%20relationships%2C%20our%20approach%20effectively%20captures%20cell-cell%20interactions%2C%20ranging%20from%20fine%20to%20coarse%2C%20within%20the%20tissue%20microenvironment.%20We%20demonstrate%20that%20Sigmm%20learns%20representations%20that%20better%20capture%20cross-modal%20correspondences%2C%20leading%20to%20an%20improvement%20of%20avg.%209.78%5C%25%20in%20the%20gene-expression%20prediction%20task%20and%20avg.%2026.93%5C%25%20in%20the%20cross-modal%20retrieval%20task%20across%20datasets.%20We%20further%20show%20that%20it%20learns%20meaningful%20multi-tissue%20organization%20in%20downstream%20analyses.&entry.1838667208=http%3A//arxiv.org/abs/2511.15464v1&entry.124074799=Read"},
{"title": "Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning", "author": "Tao Hu and Lan Li and Zhen-Hao Xie and Da-Wei Zhou", "abstract": "Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.", "link": "http://arxiv.org/abs/2511.15633v1", "date": "2025-11-19", "relevancy": 2.6831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Semantic%20Tree%20Anchoring%20for%20CLIP-Based%20Class-Incremental%20Learning&body=Title%3A%20Hierarchical%20Semantic%20Tree%20Anchoring%20for%20CLIP-Based%20Class-Incremental%20Learning%0AAuthor%3A%20Tao%20Hu%20and%20Lan%20Li%20and%20Zhen-Hao%20Xie%20and%20Da-Wei%20Zhou%0AAbstract%3A%20Class-Incremental%20Learning%20%28CIL%29%20enables%20models%20to%20learn%20new%20classes%20continually%20while%20preserving%20past%20knowledge.%20Recently%2C%20vision-language%20models%20like%20CLIP%20offer%20transferable%20features%20via%20multi-modal%20pre-training%2C%20making%20them%20well-suited%20for%20CIL.%20However%2C%20real-world%20visual%20and%20linguistic%20concepts%20are%20inherently%20hierarchical%3A%20a%20textual%20concept%20like%20%22dog%22%20subsumes%20fine-grained%20categories%20such%20as%20%22Labrador%22%20and%20%22Golden%20Retriever%2C%22%20and%20each%20category%20entails%20its%20images.%20But%20existing%20CLIP-based%20CIL%20methods%20fail%20to%20explicitly%20capture%20this%20inherent%20hierarchy%2C%20leading%20to%20fine-grained%20class%20features%20drift%20during%20incremental%20updates%20and%20ultimately%20to%20catastrophic%20forgetting.%20To%20address%20this%20challenge%2C%20we%20propose%20HASTEN%20%28Hierarchical%20Semantic%20Tree%20Anchoring%29%20that%20anchors%20hierarchical%20information%20into%20CIL%20to%20reduce%20catastrophic%20forgetting.%20First%2C%20we%20employ%20an%20external%20knowledge%20graph%20as%20supervision%20to%20embed%20visual%20and%20textual%20features%20in%20hyperbolic%20space%2C%20effectively%20preserving%20hierarchical%20structure%20as%20data%20evolves.%20Second%2C%20to%20mitigate%20catastrophic%20forgetting%2C%20we%20project%20gradients%20onto%20the%20null%20space%20of%20the%20shared%20hyperbolic%20mapper%2C%20preventing%20interference%20with%20prior%20tasks.%20These%20two%20steps%20work%20synergistically%20to%20enable%20the%20model%20to%20resist%20forgetting%20by%20maintaining%20hierarchical%20relationships.%20Extensive%20experiments%20show%20that%20HASTEN%20consistently%20outperforms%20existing%20methods%20while%20providing%20a%20unified%20structured%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Semantic%2520Tree%2520Anchoring%2520for%2520CLIP-Based%2520Class-Incremental%2520Learning%26entry.906535625%3DTao%2520Hu%2520and%2520Lan%2520Li%2520and%2520Zhen-Hao%2520Xie%2520and%2520Da-Wei%2520Zhou%26entry.1292438233%3DClass-Incremental%2520Learning%2520%2528CIL%2529%2520enables%2520models%2520to%2520learn%2520new%2520classes%2520continually%2520while%2520preserving%2520past%2520knowledge.%2520Recently%252C%2520vision-language%2520models%2520like%2520CLIP%2520offer%2520transferable%2520features%2520via%2520multi-modal%2520pre-training%252C%2520making%2520them%2520well-suited%2520for%2520CIL.%2520However%252C%2520real-world%2520visual%2520and%2520linguistic%2520concepts%2520are%2520inherently%2520hierarchical%253A%2520a%2520textual%2520concept%2520like%2520%2522dog%2522%2520subsumes%2520fine-grained%2520categories%2520such%2520as%2520%2522Labrador%2522%2520and%2520%2522Golden%2520Retriever%252C%2522%2520and%2520each%2520category%2520entails%2520its%2520images.%2520But%2520existing%2520CLIP-based%2520CIL%2520methods%2520fail%2520to%2520explicitly%2520capture%2520this%2520inherent%2520hierarchy%252C%2520leading%2520to%2520fine-grained%2520class%2520features%2520drift%2520during%2520incremental%2520updates%2520and%2520ultimately%2520to%2520catastrophic%2520forgetting.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520HASTEN%2520%2528Hierarchical%2520Semantic%2520Tree%2520Anchoring%2529%2520that%2520anchors%2520hierarchical%2520information%2520into%2520CIL%2520to%2520reduce%2520catastrophic%2520forgetting.%2520First%252C%2520we%2520employ%2520an%2520external%2520knowledge%2520graph%2520as%2520supervision%2520to%2520embed%2520visual%2520and%2520textual%2520features%2520in%2520hyperbolic%2520space%252C%2520effectively%2520preserving%2520hierarchical%2520structure%2520as%2520data%2520evolves.%2520Second%252C%2520to%2520mitigate%2520catastrophic%2520forgetting%252C%2520we%2520project%2520gradients%2520onto%2520the%2520null%2520space%2520of%2520the%2520shared%2520hyperbolic%2520mapper%252C%2520preventing%2520interference%2520with%2520prior%2520tasks.%2520These%2520two%2520steps%2520work%2520synergistically%2520to%2520enable%2520the%2520model%2520to%2520resist%2520forgetting%2520by%2520maintaining%2520hierarchical%2520relationships.%2520Extensive%2520experiments%2520show%2520that%2520HASTEN%2520consistently%2520outperforms%2520existing%2520methods%2520while%2520providing%2520a%2520unified%2520structured%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Semantic%20Tree%20Anchoring%20for%20CLIP-Based%20Class-Incremental%20Learning&entry.906535625=Tao%20Hu%20and%20Lan%20Li%20and%20Zhen-Hao%20Xie%20and%20Da-Wei%20Zhou&entry.1292438233=Class-Incremental%20Learning%20%28CIL%29%20enables%20models%20to%20learn%20new%20classes%20continually%20while%20preserving%20past%20knowledge.%20Recently%2C%20vision-language%20models%20like%20CLIP%20offer%20transferable%20features%20via%20multi-modal%20pre-training%2C%20making%20them%20well-suited%20for%20CIL.%20However%2C%20real-world%20visual%20and%20linguistic%20concepts%20are%20inherently%20hierarchical%3A%20a%20textual%20concept%20like%20%22dog%22%20subsumes%20fine-grained%20categories%20such%20as%20%22Labrador%22%20and%20%22Golden%20Retriever%2C%22%20and%20each%20category%20entails%20its%20images.%20But%20existing%20CLIP-based%20CIL%20methods%20fail%20to%20explicitly%20capture%20this%20inherent%20hierarchy%2C%20leading%20to%20fine-grained%20class%20features%20drift%20during%20incremental%20updates%20and%20ultimately%20to%20catastrophic%20forgetting.%20To%20address%20this%20challenge%2C%20we%20propose%20HASTEN%20%28Hierarchical%20Semantic%20Tree%20Anchoring%29%20that%20anchors%20hierarchical%20information%20into%20CIL%20to%20reduce%20catastrophic%20forgetting.%20First%2C%20we%20employ%20an%20external%20knowledge%20graph%20as%20supervision%20to%20embed%20visual%20and%20textual%20features%20in%20hyperbolic%20space%2C%20effectively%20preserving%20hierarchical%20structure%20as%20data%20evolves.%20Second%2C%20to%20mitigate%20catastrophic%20forgetting%2C%20we%20project%20gradients%20onto%20the%20null%20space%20of%20the%20shared%20hyperbolic%20mapper%2C%20preventing%20interference%20with%20prior%20tasks.%20These%20two%20steps%20work%20synergistically%20to%20enable%20the%20model%20to%20resist%20forgetting%20by%20maintaining%20hierarchical%20relationships.%20Extensive%20experiments%20show%20that%20HASTEN%20consistently%20outperforms%20existing%20methods%20while%20providing%20a%20unified%20structured%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2511.15633v1&entry.124074799=Read"},
{"title": "UltraDP: Generalizable Carotid Ultrasound Scanning with Force-Aware Diffusion Policy", "author": "Ruoqu Chen and Xiangjie Yan and Kangchen Lv and Gao Huang and Zheng Li and Xiang Li", "abstract": "Ultrasound scanning is a critical imaging technique for real-time, non-invasive diagnostics. However, variations in patient anatomy and complex human-in-the-loop interactions pose significant challenges for autonomous robotic scanning. Existing ultrasound scanning robots are commonly limited to relatively low generalization and inefficient data utilization. To overcome these limitations, we present UltraDP, a Diffusion-Policy-based method that receives multi-sensory inputs (ultrasound images, wrist camera images, contact wrench, and probe pose) and generates actions that are fit for multi-modal action distributions in autonomous ultrasound scanning of carotid artery. We propose a specialized guidance module to enable the policy to output actions that center the artery in ultrasound images. To ensure stable contact and safe interaction between the robot and the human subject, a hybrid force-impedance controller is utilized to drive the robot to track such trajectories. Also, we have built a large-scale training dataset for carotid scanning comprising 210 scans with 460k sample pairs from 21 volunteers of both genders. By exploring our guidance module and DP's strong generalization ability, UltraDP achieves a 95% success rate in transverse scanning on previously unseen subjects, demonstrating its effectiveness.", "link": "http://arxiv.org/abs/2511.15550v1", "date": "2025-11-19", "relevancy": 2.6555, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5423}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5337}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraDP%3A%20Generalizable%20Carotid%20Ultrasound%20Scanning%20with%20Force-Aware%20Diffusion%20Policy&body=Title%3A%20UltraDP%3A%20Generalizable%20Carotid%20Ultrasound%20Scanning%20with%20Force-Aware%20Diffusion%20Policy%0AAuthor%3A%20Ruoqu%20Chen%20and%20Xiangjie%20Yan%20and%20Kangchen%20Lv%20and%20Gao%20Huang%20and%20Zheng%20Li%20and%20Xiang%20Li%0AAbstract%3A%20Ultrasound%20scanning%20is%20a%20critical%20imaging%20technique%20for%20real-time%2C%20non-invasive%20diagnostics.%20However%2C%20variations%20in%20patient%20anatomy%20and%20complex%20human-in-the-loop%20interactions%20pose%20significant%20challenges%20for%20autonomous%20robotic%20scanning.%20Existing%20ultrasound%20scanning%20robots%20are%20commonly%20limited%20to%20relatively%20low%20generalization%20and%20inefficient%20data%20utilization.%20To%20overcome%20these%20limitations%2C%20we%20present%20UltraDP%2C%20a%20Diffusion-Policy-based%20method%20that%20receives%20multi-sensory%20inputs%20%28ultrasound%20images%2C%20wrist%20camera%20images%2C%20contact%20wrench%2C%20and%20probe%20pose%29%20and%20generates%20actions%20that%20are%20fit%20for%20multi-modal%20action%20distributions%20in%20autonomous%20ultrasound%20scanning%20of%20carotid%20artery.%20We%20propose%20a%20specialized%20guidance%20module%20to%20enable%20the%20policy%20to%20output%20actions%20that%20center%20the%20artery%20in%20ultrasound%20images.%20To%20ensure%20stable%20contact%20and%20safe%20interaction%20between%20the%20robot%20and%20the%20human%20subject%2C%20a%20hybrid%20force-impedance%20controller%20is%20utilized%20to%20drive%20the%20robot%20to%20track%20such%20trajectories.%20Also%2C%20we%20have%20built%20a%20large-scale%20training%20dataset%20for%20carotid%20scanning%20comprising%20210%20scans%20with%20460k%20sample%20pairs%20from%2021%20volunteers%20of%20both%20genders.%20By%20exploring%20our%20guidance%20module%20and%20DP%27s%20strong%20generalization%20ability%2C%20UltraDP%20achieves%20a%2095%25%20success%20rate%20in%20transverse%20scanning%20on%20previously%20unseen%20subjects%2C%20demonstrating%20its%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraDP%253A%2520Generalizable%2520Carotid%2520Ultrasound%2520Scanning%2520with%2520Force-Aware%2520Diffusion%2520Policy%26entry.906535625%3DRuoqu%2520Chen%2520and%2520Xiangjie%2520Yan%2520and%2520Kangchen%2520Lv%2520and%2520Gao%2520Huang%2520and%2520Zheng%2520Li%2520and%2520Xiang%2520Li%26entry.1292438233%3DUltrasound%2520scanning%2520is%2520a%2520critical%2520imaging%2520technique%2520for%2520real-time%252C%2520non-invasive%2520diagnostics.%2520However%252C%2520variations%2520in%2520patient%2520anatomy%2520and%2520complex%2520human-in-the-loop%2520interactions%2520pose%2520significant%2520challenges%2520for%2520autonomous%2520robotic%2520scanning.%2520Existing%2520ultrasound%2520scanning%2520robots%2520are%2520commonly%2520limited%2520to%2520relatively%2520low%2520generalization%2520and%2520inefficient%2520data%2520utilization.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%2520UltraDP%252C%2520a%2520Diffusion-Policy-based%2520method%2520that%2520receives%2520multi-sensory%2520inputs%2520%2528ultrasound%2520images%252C%2520wrist%2520camera%2520images%252C%2520contact%2520wrench%252C%2520and%2520probe%2520pose%2529%2520and%2520generates%2520actions%2520that%2520are%2520fit%2520for%2520multi-modal%2520action%2520distributions%2520in%2520autonomous%2520ultrasound%2520scanning%2520of%2520carotid%2520artery.%2520We%2520propose%2520a%2520specialized%2520guidance%2520module%2520to%2520enable%2520the%2520policy%2520to%2520output%2520actions%2520that%2520center%2520the%2520artery%2520in%2520ultrasound%2520images.%2520To%2520ensure%2520stable%2520contact%2520and%2520safe%2520interaction%2520between%2520the%2520robot%2520and%2520the%2520human%2520subject%252C%2520a%2520hybrid%2520force-impedance%2520controller%2520is%2520utilized%2520to%2520drive%2520the%2520robot%2520to%2520track%2520such%2520trajectories.%2520Also%252C%2520we%2520have%2520built%2520a%2520large-scale%2520training%2520dataset%2520for%2520carotid%2520scanning%2520comprising%2520210%2520scans%2520with%2520460k%2520sample%2520pairs%2520from%252021%2520volunteers%2520of%2520both%2520genders.%2520By%2520exploring%2520our%2520guidance%2520module%2520and%2520DP%2527s%2520strong%2520generalization%2520ability%252C%2520UltraDP%2520achieves%2520a%252095%2525%2520success%2520rate%2520in%2520transverse%2520scanning%2520on%2520previously%2520unseen%2520subjects%252C%2520demonstrating%2520its%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraDP%3A%20Generalizable%20Carotid%20Ultrasound%20Scanning%20with%20Force-Aware%20Diffusion%20Policy&entry.906535625=Ruoqu%20Chen%20and%20Xiangjie%20Yan%20and%20Kangchen%20Lv%20and%20Gao%20Huang%20and%20Zheng%20Li%20and%20Xiang%20Li&entry.1292438233=Ultrasound%20scanning%20is%20a%20critical%20imaging%20technique%20for%20real-time%2C%20non-invasive%20diagnostics.%20However%2C%20variations%20in%20patient%20anatomy%20and%20complex%20human-in-the-loop%20interactions%20pose%20significant%20challenges%20for%20autonomous%20robotic%20scanning.%20Existing%20ultrasound%20scanning%20robots%20are%20commonly%20limited%20to%20relatively%20low%20generalization%20and%20inefficient%20data%20utilization.%20To%20overcome%20these%20limitations%2C%20we%20present%20UltraDP%2C%20a%20Diffusion-Policy-based%20method%20that%20receives%20multi-sensory%20inputs%20%28ultrasound%20images%2C%20wrist%20camera%20images%2C%20contact%20wrench%2C%20and%20probe%20pose%29%20and%20generates%20actions%20that%20are%20fit%20for%20multi-modal%20action%20distributions%20in%20autonomous%20ultrasound%20scanning%20of%20carotid%20artery.%20We%20propose%20a%20specialized%20guidance%20module%20to%20enable%20the%20policy%20to%20output%20actions%20that%20center%20the%20artery%20in%20ultrasound%20images.%20To%20ensure%20stable%20contact%20and%20safe%20interaction%20between%20the%20robot%20and%20the%20human%20subject%2C%20a%20hybrid%20force-impedance%20controller%20is%20utilized%20to%20drive%20the%20robot%20to%20track%20such%20trajectories.%20Also%2C%20we%20have%20built%20a%20large-scale%20training%20dataset%20for%20carotid%20scanning%20comprising%20210%20scans%20with%20460k%20sample%20pairs%20from%2021%20volunteers%20of%20both%20genders.%20By%20exploring%20our%20guidance%20module%20and%20DP%27s%20strong%20generalization%20ability%2C%20UltraDP%20achieves%20a%2095%25%20success%20rate%20in%20transverse%20scanning%20on%20previously%20unseen%20subjects%2C%20demonstrating%20its%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2511.15550v1&entry.124074799=Read"},
{"title": "Computer-Use Agents as Judges for Generative User Interface", "author": "Kevin Qinghong Lin and Siyuan Hu and Linjie Li and Zhengyuan Yang and Lijuan Wang and Philip Torr and Mike Zheng Shou", "abstract": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.", "link": "http://arxiv.org/abs/2511.15567v1", "date": "2025-11-19", "relevancy": 2.6442, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5362}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5279}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer-Use%20Agents%20as%20Judges%20for%20Generative%20User%20Interface&body=Title%3A%20Computer-Use%20Agents%20as%20Judges%20for%20Generative%20User%20Interface%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Siyuan%20Hu%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Philip%20Torr%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20Computer-Use%20Agents%20%28CUA%29%20are%20becoming%20increasingly%20capable%20of%20autonomously%20operating%20digital%20environments%20through%20Graphical%20User%20Interfaces%20%28GUI%29.%20Yet%2C%20most%20GUI%20remain%20designed%20primarily%20for%20humans--prioritizing%20aesthetics%20and%20usability--forcing%20agents%20to%20adopt%20human-oriented%20behaviors%20that%20are%20unnecessary%20for%20efficient%20task%20execution.%20At%20the%20same%20time%2C%20rapid%20advances%20in%20coding-oriented%20language%20models%20%28Coder%29%20have%20transformed%20automatic%20GUI%20design.%20This%20raises%20a%20fundamental%20question%3A%20Can%20CUA%20as%20judges%20to%20assist%20Coder%20for%20automatic%20GUI%20design%3F%20To%20investigate%2C%20we%20introduce%20AUI-Gym%2C%20a%20benchmark%20for%20Automatic%20GUI%20development%20spanning%2052%20applications%20across%20diverse%20domains.%20Using%20language%20models%2C%20we%20synthesize%201560%20tasks%20that%20simulate%20real-world%20scenarios.%20To%20ensure%20task%20reliability%2C%20we%20further%20develop%20a%20verifier%20that%20programmatically%20checks%20whether%20each%20task%20is%20executable%20within%20its%20environment.%20Building%20on%20this%2C%20we%20propose%20a%20Coder-CUA%20in%20Collaboration%20framework%3A%20the%20Coder%20acts%20as%20Designer%2C%20generating%20and%20revising%20websites%2C%20while%20the%20CUA%20serves%20as%20Judge%2C%20evaluating%20functionality%20and%20refining%20designs.%20Success%20is%20measured%20not%20by%20visual%20appearance%2C%20but%20by%20task%20solvability%20and%20CUA%20navigation%20success%20rate.%20To%20turn%20CUA%20feedback%20into%20usable%20guidance%2C%20we%20design%20a%20CUA%20Dashboard%20that%20compresses%20multi-step%20navigation%20histories%20into%20concise%20visual%20summaries%2C%20offering%20interpretable%20guidance%20for%20iterative%20redesign.%20By%20positioning%20agents%20as%20both%20designers%20and%20judges%2C%20our%20framework%20shifts%20interface%20design%20toward%20agent-native%20efficiency%20and%20reliability.%20Our%20work%20takes%20a%20step%20toward%20shifting%20agents%20from%20passive%20use%20toward%20active%20participation%20in%20digital%20environments.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/showlab/AUI.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer-Use%2520Agents%2520as%2520Judges%2520for%2520Generative%2520User%2520Interface%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Siyuan%2520Hu%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Lijuan%2520Wang%2520and%2520Philip%2520Torr%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3DComputer-Use%2520Agents%2520%2528CUA%2529%2520are%2520becoming%2520increasingly%2520capable%2520of%2520autonomously%2520operating%2520digital%2520environments%2520through%2520Graphical%2520User%2520Interfaces%2520%2528GUI%2529.%2520Yet%252C%2520most%2520GUI%2520remain%2520designed%2520primarily%2520for%2520humans--prioritizing%2520aesthetics%2520and%2520usability--forcing%2520agents%2520to%2520adopt%2520human-oriented%2520behaviors%2520that%2520are%2520unnecessary%2520for%2520efficient%2520task%2520execution.%2520At%2520the%2520same%2520time%252C%2520rapid%2520advances%2520in%2520coding-oriented%2520language%2520models%2520%2528Coder%2529%2520have%2520transformed%2520automatic%2520GUI%2520design.%2520This%2520raises%2520a%2520fundamental%2520question%253A%2520Can%2520CUA%2520as%2520judges%2520to%2520assist%2520Coder%2520for%2520automatic%2520GUI%2520design%253F%2520To%2520investigate%252C%2520we%2520introduce%2520AUI-Gym%252C%2520a%2520benchmark%2520for%2520Automatic%2520GUI%2520development%2520spanning%252052%2520applications%2520across%2520diverse%2520domains.%2520Using%2520language%2520models%252C%2520we%2520synthesize%25201560%2520tasks%2520that%2520simulate%2520real-world%2520scenarios.%2520To%2520ensure%2520task%2520reliability%252C%2520we%2520further%2520develop%2520a%2520verifier%2520that%2520programmatically%2520checks%2520whether%2520each%2520task%2520is%2520executable%2520within%2520its%2520environment.%2520Building%2520on%2520this%252C%2520we%2520propose%2520a%2520Coder-CUA%2520in%2520Collaboration%2520framework%253A%2520the%2520Coder%2520acts%2520as%2520Designer%252C%2520generating%2520and%2520revising%2520websites%252C%2520while%2520the%2520CUA%2520serves%2520as%2520Judge%252C%2520evaluating%2520functionality%2520and%2520refining%2520designs.%2520Success%2520is%2520measured%2520not%2520by%2520visual%2520appearance%252C%2520but%2520by%2520task%2520solvability%2520and%2520CUA%2520navigation%2520success%2520rate.%2520To%2520turn%2520CUA%2520feedback%2520into%2520usable%2520guidance%252C%2520we%2520design%2520a%2520CUA%2520Dashboard%2520that%2520compresses%2520multi-step%2520navigation%2520histories%2520into%2520concise%2520visual%2520summaries%252C%2520offering%2520interpretable%2520guidance%2520for%2520iterative%2520redesign.%2520By%2520positioning%2520agents%2520as%2520both%2520designers%2520and%2520judges%252C%2520our%2520framework%2520shifts%2520interface%2520design%2520toward%2520agent-native%2520efficiency%2520and%2520reliability.%2520Our%2520work%2520takes%2520a%2520step%2520toward%2520shifting%2520agents%2520from%2520passive%2520use%2520toward%2520active%2520participation%2520in%2520digital%2520environments.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/showlab/AUI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer-Use%20Agents%20as%20Judges%20for%20Generative%20User%20Interface&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Siyuan%20Hu%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Philip%20Torr%20and%20Mike%20Zheng%20Shou&entry.1292438233=Computer-Use%20Agents%20%28CUA%29%20are%20becoming%20increasingly%20capable%20of%20autonomously%20operating%20digital%20environments%20through%20Graphical%20User%20Interfaces%20%28GUI%29.%20Yet%2C%20most%20GUI%20remain%20designed%20primarily%20for%20humans--prioritizing%20aesthetics%20and%20usability--forcing%20agents%20to%20adopt%20human-oriented%20behaviors%20that%20are%20unnecessary%20for%20efficient%20task%20execution.%20At%20the%20same%20time%2C%20rapid%20advances%20in%20coding-oriented%20language%20models%20%28Coder%29%20have%20transformed%20automatic%20GUI%20design.%20This%20raises%20a%20fundamental%20question%3A%20Can%20CUA%20as%20judges%20to%20assist%20Coder%20for%20automatic%20GUI%20design%3F%20To%20investigate%2C%20we%20introduce%20AUI-Gym%2C%20a%20benchmark%20for%20Automatic%20GUI%20development%20spanning%2052%20applications%20across%20diverse%20domains.%20Using%20language%20models%2C%20we%20synthesize%201560%20tasks%20that%20simulate%20real-world%20scenarios.%20To%20ensure%20task%20reliability%2C%20we%20further%20develop%20a%20verifier%20that%20programmatically%20checks%20whether%20each%20task%20is%20executable%20within%20its%20environment.%20Building%20on%20this%2C%20we%20propose%20a%20Coder-CUA%20in%20Collaboration%20framework%3A%20the%20Coder%20acts%20as%20Designer%2C%20generating%20and%20revising%20websites%2C%20while%20the%20CUA%20serves%20as%20Judge%2C%20evaluating%20functionality%20and%20refining%20designs.%20Success%20is%20measured%20not%20by%20visual%20appearance%2C%20but%20by%20task%20solvability%20and%20CUA%20navigation%20success%20rate.%20To%20turn%20CUA%20feedback%20into%20usable%20guidance%2C%20we%20design%20a%20CUA%20Dashboard%20that%20compresses%20multi-step%20navigation%20histories%20into%20concise%20visual%20summaries%2C%20offering%20interpretable%20guidance%20for%20iterative%20redesign.%20By%20positioning%20agents%20as%20both%20designers%20and%20judges%2C%20our%20framework%20shifts%20interface%20design%20toward%20agent-native%20efficiency%20and%20reliability.%20Our%20work%20takes%20a%20step%20toward%20shifting%20agents%20from%20passive%20use%20toward%20active%20participation%20in%20digital%20environments.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/showlab/AUI.&entry.1838667208=http%3A//arxiv.org/abs/2511.15567v1&entry.124074799=Read"},
{"title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework", "author": "Shanlin Zhou and Xinpeng Wang and Jianxun Lian and Zhenghao Liu and Laks V. S. Lakshmanan and Xiaoyuan Yi and Yongtao Hao", "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.", "link": "http://arxiv.org/abs/2511.15408v1", "date": "2025-11-19", "relevancy": 2.6438, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5408}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5299}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAMeGEn%3A%20Creative%20Name%20Generation%20via%20A%20Novel%20Agent-based%20Multiple%20Personalized%20Goal%20Enhancement%20Framework&body=Title%3A%20NAMeGEn%3A%20Creative%20Name%20Generation%20via%20A%20Novel%20Agent-based%20Multiple%20Personalized%20Goal%20Enhancement%20Framework%0AAuthor%3A%20Shanlin%20Zhou%20and%20Xinpeng%20Wang%20and%20Jianxun%20Lian%20and%20Zhenghao%20Liu%20and%20Laks%20V.%20S.%20Lakshmanan%20and%20Xiaoyuan%20Yi%20and%20Yongtao%20Hao%0AAbstract%3A%20Trained%20on%20diverse%20human-authored%20texts%2C%20Large%20Language%20Models%20%28LLMs%29%20unlocked%20the%20potential%20for%20Creative%20Natural%20Language%20Generation%20%28CNLG%29%2C%20benefiting%20various%20applications%20like%20advertising%20and%20storytelling.%20Nevertheless%2C%20CNLG%20still%20remains%20difficult%20due%20to%20two%20main%20challenges.%20%281%29%20Multi-objective%20flexibility%3A%20user%20requirements%20are%20often%20personalized%2C%20fine-grained%2C%20and%20pluralistic%2C%20which%20LLMs%20struggle%20to%20satisfy%20simultaneously%3B%20%282%29%20Interpretive%20complexity%3A%20beyond%20generation%2C%20creativity%20also%20involves%20understanding%20and%20interpreting%20implicit%20meaning%20to%20enhance%20users%27%20perception.%20These%20challenges%20significantly%20limit%20current%20methods%2C%20especially%20in%20short-form%20text%20generation%2C%20in%20generating%20creative%20and%20insightful%20content.%20To%20address%20this%2C%20we%20focus%20on%20Chinese%20baby%20naming%2C%20a%20representative%20short-form%20CNLG%20task%20requiring%20adherence%20to%20explicit%20user%20constraints%20%28e.g.%2C%20length%2C%20semantics%2C%20anthroponymy%29%20while%20offering%20meaningful%20aesthetic%20explanations.%20We%20propose%20NAMeGEn%2C%20a%20novel%20multi-agent%20optimization%20framework%20that%20iteratively%20alternates%20between%20objective%20extraction%2C%20name%20generation%2C%20and%20evaluation%20to%20meet%20diverse%20requirements%20and%20generate%20accurate%20explanations.%20To%20support%20this%20task%2C%20we%20further%20construct%20a%20classical%20Chinese%20poetry%20corpus%20with%2017k%2B%20poems%20to%20enhance%20aesthetics%2C%20and%20introduce%20CBNames%2C%20a%20new%20benchmark%20with%20tailored%20metrics.%20Extensive%20experiments%20demonstrate%20that%20NAMeGEn%20effectively%20generates%20creative%20names%20that%20meet%20diverse%2C%20personalized%20requirements%20while%20providing%20meaningful%20explanations%2C%20outperforming%20six%20baseline%20methods%20spanning%20various%20LLM%20backbones%20without%20any%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAMeGEn%253A%2520Creative%2520Name%2520Generation%2520via%2520A%2520Novel%2520Agent-based%2520Multiple%2520Personalized%2520Goal%2520Enhancement%2520Framework%26entry.906535625%3DShanlin%2520Zhou%2520and%2520Xinpeng%2520Wang%2520and%2520Jianxun%2520Lian%2520and%2520Zhenghao%2520Liu%2520and%2520Laks%2520V.%2520S.%2520Lakshmanan%2520and%2520Xiaoyuan%2520Yi%2520and%2520Yongtao%2520Hao%26entry.1292438233%3DTrained%2520on%2520diverse%2520human-authored%2520texts%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520unlocked%2520the%2520potential%2520for%2520Creative%2520Natural%2520Language%2520Generation%2520%2528CNLG%2529%252C%2520benefiting%2520various%2520applications%2520like%2520advertising%2520and%2520storytelling.%2520Nevertheless%252C%2520CNLG%2520still%2520remains%2520difficult%2520due%2520to%2520two%2520main%2520challenges.%2520%25281%2529%2520Multi-objective%2520flexibility%253A%2520user%2520requirements%2520are%2520often%2520personalized%252C%2520fine-grained%252C%2520and%2520pluralistic%252C%2520which%2520LLMs%2520struggle%2520to%2520satisfy%2520simultaneously%253B%2520%25282%2529%2520Interpretive%2520complexity%253A%2520beyond%2520generation%252C%2520creativity%2520also%2520involves%2520understanding%2520and%2520interpreting%2520implicit%2520meaning%2520to%2520enhance%2520users%2527%2520perception.%2520These%2520challenges%2520significantly%2520limit%2520current%2520methods%252C%2520especially%2520in%2520short-form%2520text%2520generation%252C%2520in%2520generating%2520creative%2520and%2520insightful%2520content.%2520To%2520address%2520this%252C%2520we%2520focus%2520on%2520Chinese%2520baby%2520naming%252C%2520a%2520representative%2520short-form%2520CNLG%2520task%2520requiring%2520adherence%2520to%2520explicit%2520user%2520constraints%2520%2528e.g.%252C%2520length%252C%2520semantics%252C%2520anthroponymy%2529%2520while%2520offering%2520meaningful%2520aesthetic%2520explanations.%2520We%2520propose%2520NAMeGEn%252C%2520a%2520novel%2520multi-agent%2520optimization%2520framework%2520that%2520iteratively%2520alternates%2520between%2520objective%2520extraction%252C%2520name%2520generation%252C%2520and%2520evaluation%2520to%2520meet%2520diverse%2520requirements%2520and%2520generate%2520accurate%2520explanations.%2520To%2520support%2520this%2520task%252C%2520we%2520further%2520construct%2520a%2520classical%2520Chinese%2520poetry%2520corpus%2520with%252017k%252B%2520poems%2520to%2520enhance%2520aesthetics%252C%2520and%2520introduce%2520CBNames%252C%2520a%2520new%2520benchmark%2520with%2520tailored%2520metrics.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NAMeGEn%2520effectively%2520generates%2520creative%2520names%2520that%2520meet%2520diverse%252C%2520personalized%2520requirements%2520while%2520providing%2520meaningful%2520explanations%252C%2520outperforming%2520six%2520baseline%2520methods%2520spanning%2520various%2520LLM%2520backbones%2520without%2520any%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAMeGEn%3A%20Creative%20Name%20Generation%20via%20A%20Novel%20Agent-based%20Multiple%20Personalized%20Goal%20Enhancement%20Framework&entry.906535625=Shanlin%20Zhou%20and%20Xinpeng%20Wang%20and%20Jianxun%20Lian%20and%20Zhenghao%20Liu%20and%20Laks%20V.%20S.%20Lakshmanan%20and%20Xiaoyuan%20Yi%20and%20Yongtao%20Hao&entry.1292438233=Trained%20on%20diverse%20human-authored%20texts%2C%20Large%20Language%20Models%20%28LLMs%29%20unlocked%20the%20potential%20for%20Creative%20Natural%20Language%20Generation%20%28CNLG%29%2C%20benefiting%20various%20applications%20like%20advertising%20and%20storytelling.%20Nevertheless%2C%20CNLG%20still%20remains%20difficult%20due%20to%20two%20main%20challenges.%20%281%29%20Multi-objective%20flexibility%3A%20user%20requirements%20are%20often%20personalized%2C%20fine-grained%2C%20and%20pluralistic%2C%20which%20LLMs%20struggle%20to%20satisfy%20simultaneously%3B%20%282%29%20Interpretive%20complexity%3A%20beyond%20generation%2C%20creativity%20also%20involves%20understanding%20and%20interpreting%20implicit%20meaning%20to%20enhance%20users%27%20perception.%20These%20challenges%20significantly%20limit%20current%20methods%2C%20especially%20in%20short-form%20text%20generation%2C%20in%20generating%20creative%20and%20insightful%20content.%20To%20address%20this%2C%20we%20focus%20on%20Chinese%20baby%20naming%2C%20a%20representative%20short-form%20CNLG%20task%20requiring%20adherence%20to%20explicit%20user%20constraints%20%28e.g.%2C%20length%2C%20semantics%2C%20anthroponymy%29%20while%20offering%20meaningful%20aesthetic%20explanations.%20We%20propose%20NAMeGEn%2C%20a%20novel%20multi-agent%20optimization%20framework%20that%20iteratively%20alternates%20between%20objective%20extraction%2C%20name%20generation%2C%20and%20evaluation%20to%20meet%20diverse%20requirements%20and%20generate%20accurate%20explanations.%20To%20support%20this%20task%2C%20we%20further%20construct%20a%20classical%20Chinese%20poetry%20corpus%20with%2017k%2B%20poems%20to%20enhance%20aesthetics%2C%20and%20introduce%20CBNames%2C%20a%20new%20benchmark%20with%20tailored%20metrics.%20Extensive%20experiments%20demonstrate%20that%20NAMeGEn%20effectively%20generates%20creative%20names%20that%20meet%20diverse%2C%20personalized%20requirements%20while%20providing%20meaningful%20explanations%2C%20outperforming%20six%20baseline%20methods%20spanning%20various%20LLM%20backbones%20without%20any%20training.&entry.1838667208=http%3A//arxiv.org/abs/2511.15408v1&entry.124074799=Read"},
{"title": "Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models", "author": "Davide Marincione and Donato Crisostomi and Roberto Dessi and Emanuele Rodol\u00e0 and Emanuele Rossi", "abstract": "Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.", "link": "http://arxiv.org/abs/2511.05171v2", "date": "2025-11-19", "relevancy": 2.6141, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Merging%20Improves%20Zero-Shot%20Generalization%20in%20Bioacoustic%20Foundation%20Models&body=Title%3A%20Model%20Merging%20Improves%20Zero-Shot%20Generalization%20in%20Bioacoustic%20Foundation%20Models%0AAuthor%3A%20Davide%20Marincione%20and%20Donato%20Crisostomi%20and%20Roberto%20Dessi%20and%20Emanuele%20Rodol%C3%A0%20and%20Emanuele%20Rossi%0AAbstract%3A%20Foundation%20models%20capable%20of%20generalizing%20across%20species%20and%20tasks%20represent%20a%20promising%20new%20frontier%20in%20bioacoustics%2C%20with%20NatureLM%20being%20one%20of%20the%20most%20prominent%20examples.%20While%20its%20domain-specific%20fine-tuning%20yields%20strong%20performance%20on%20bioacoustic%20benchmarks%2C%20we%20observe%20that%20it%20also%20introduces%20trade-offs%20in%20instruction-following%20flexibility.%20For%20instance%2C%20NatureLM%20achieves%20high%20accuracy%20when%20prompted%20for%20either%20the%20common%20or%20scientific%20name%20individually%2C%20but%20its%20accuracy%20drops%20significantly%20when%20both%20are%20requested%20in%20a%20single%20prompt.%20We%20address%20this%20by%20applying%20a%20simple%20model%20merging%20strategy%20that%20interpolates%20NatureLM%20with%20its%20base%20language%20model%2C%20recovering%20instruction-following%20capabilities%20with%20minimal%20loss%20of%20domain%20expertise.%20Finally%2C%20we%20show%20that%20the%20merged%20model%20exhibits%20markedly%20stronger%20zero-shot%20generalization%2C%20achieving%20over%20a%20200%25%20relative%20improvement%20and%20setting%20a%20new%20state-of-the-art%20in%20closed-set%20zero-shot%20classification%20of%20unseen%20species.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Merging%2520Improves%2520Zero-Shot%2520Generalization%2520in%2520Bioacoustic%2520Foundation%2520Models%26entry.906535625%3DDavide%2520Marincione%2520and%2520Donato%2520Crisostomi%2520and%2520Roberto%2520Dessi%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Emanuele%2520Rossi%26entry.1292438233%3DFoundation%2520models%2520capable%2520of%2520generalizing%2520across%2520species%2520and%2520tasks%2520represent%2520a%2520promising%2520new%2520frontier%2520in%2520bioacoustics%252C%2520with%2520NatureLM%2520being%2520one%2520of%2520the%2520most%2520prominent%2520examples.%2520While%2520its%2520domain-specific%2520fine-tuning%2520yields%2520strong%2520performance%2520on%2520bioacoustic%2520benchmarks%252C%2520we%2520observe%2520that%2520it%2520also%2520introduces%2520trade-offs%2520in%2520instruction-following%2520flexibility.%2520For%2520instance%252C%2520NatureLM%2520achieves%2520high%2520accuracy%2520when%2520prompted%2520for%2520either%2520the%2520common%2520or%2520scientific%2520name%2520individually%252C%2520but%2520its%2520accuracy%2520drops%2520significantly%2520when%2520both%2520are%2520requested%2520in%2520a%2520single%2520prompt.%2520We%2520address%2520this%2520by%2520applying%2520a%2520simple%2520model%2520merging%2520strategy%2520that%2520interpolates%2520NatureLM%2520with%2520its%2520base%2520language%2520model%252C%2520recovering%2520instruction-following%2520capabilities%2520with%2520minimal%2520loss%2520of%2520domain%2520expertise.%2520Finally%252C%2520we%2520show%2520that%2520the%2520merged%2520model%2520exhibits%2520markedly%2520stronger%2520zero-shot%2520generalization%252C%2520achieving%2520over%2520a%2520200%2525%2520relative%2520improvement%2520and%2520setting%2520a%2520new%2520state-of-the-art%2520in%2520closed-set%2520zero-shot%2520classification%2520of%2520unseen%2520species.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Merging%20Improves%20Zero-Shot%20Generalization%20in%20Bioacoustic%20Foundation%20Models&entry.906535625=Davide%20Marincione%20and%20Donato%20Crisostomi%20and%20Roberto%20Dessi%20and%20Emanuele%20Rodol%C3%A0%20and%20Emanuele%20Rossi&entry.1292438233=Foundation%20models%20capable%20of%20generalizing%20across%20species%20and%20tasks%20represent%20a%20promising%20new%20frontier%20in%20bioacoustics%2C%20with%20NatureLM%20being%20one%20of%20the%20most%20prominent%20examples.%20While%20its%20domain-specific%20fine-tuning%20yields%20strong%20performance%20on%20bioacoustic%20benchmarks%2C%20we%20observe%20that%20it%20also%20introduces%20trade-offs%20in%20instruction-following%20flexibility.%20For%20instance%2C%20NatureLM%20achieves%20high%20accuracy%20when%20prompted%20for%20either%20the%20common%20or%20scientific%20name%20individually%2C%20but%20its%20accuracy%20drops%20significantly%20when%20both%20are%20requested%20in%20a%20single%20prompt.%20We%20address%20this%20by%20applying%20a%20simple%20model%20merging%20strategy%20that%20interpolates%20NatureLM%20with%20its%20base%20language%20model%2C%20recovering%20instruction-following%20capabilities%20with%20minimal%20loss%20of%20domain%20expertise.%20Finally%2C%20we%20show%20that%20the%20merged%20model%20exhibits%20markedly%20stronger%20zero-shot%20generalization%2C%20achieving%20over%20a%20200%25%20relative%20improvement%20and%20setting%20a%20new%20state-of-the-art%20in%20closed-set%20zero-shot%20classification%20of%20unseen%20species.&entry.1838667208=http%3A//arxiv.org/abs/2511.05171v2&entry.124074799=Read"},
{"title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?", "author": "Miao Zhang and Sherif Abdulatif and Benedikt Loesch and Marco Altmann and Bin Yang", "abstract": "Due to the significant effort required for data collection and annotation in 3D perception tasks, mixed sample data augmentation (MSDA) has been widely studied to generate diverse training samples by mixing existing data. Recently, many MSDA techniques have been developed for point clouds, but they mainly target LiDAR data, leaving their application to radar point clouds largely unexplored. In this paper, we examine the feasibility of applying existing MSDA methods to radar point clouds and identify several challenges in adapting these techniques. These obstacles stem from the radar's irregular angular distribution, deviations from a single-sensor polar layout in multi-radar setups, and point sparsity. To address these issues, we propose Class-Aware PillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar level in 3D point clouds, guided by class labels. Unlike methods that rely a single mix ratio to the entire sample, CAPMix assigns an independent ratio to each pillar, boosting sample diversity. To account for the density of different classes, we use class-specific distributions: for dense objects (e.g., large vehicles), we skew ratios to favor points from another sample, while for sparse objects (e.g., pedestrians), we sample more points from the original. This class-aware mixing retains critical details and enriches each sample with new information, ultimately generating more diverse training data. Experimental results demonstrate that our method not only significantly boosts performance but also outperforms existing MSDA approaches across two datasets (Bosch Street and K-Radar). We believe that this straightforward yet effective approach will spark further investigation into MSDA techniques for radar data.", "link": "http://arxiv.org/abs/2503.02687v3", "date": "2025-11-19", "relevancy": 2.5951, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5204}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5204}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-Aware%20PillarMix%3A%20Can%20Mixed%20Sample%20Data%20Augmentation%20Enhance%203D%20Object%20Detection%20with%20Radar%20Point%20Clouds%3F&body=Title%3A%20Class-Aware%20PillarMix%3A%20Can%20Mixed%20Sample%20Data%20Augmentation%20Enhance%203D%20Object%20Detection%20with%20Radar%20Point%20Clouds%3F%0AAuthor%3A%20Miao%20Zhang%20and%20Sherif%20Abdulatif%20and%20Benedikt%20Loesch%20and%20Marco%20Altmann%20and%20Bin%20Yang%0AAbstract%3A%20Due%20to%20the%20significant%20effort%20required%20for%20data%20collection%20and%20annotation%20in%203D%20perception%20tasks%2C%20mixed%20sample%20data%20augmentation%20%28MSDA%29%20has%20been%20widely%20studied%20to%20generate%20diverse%20training%20samples%20by%20mixing%20existing%20data.%20Recently%2C%20many%20MSDA%20techniques%20have%20been%20developed%20for%20point%20clouds%2C%20but%20they%20mainly%20target%20LiDAR%20data%2C%20leaving%20their%20application%20to%20radar%20point%20clouds%20largely%20unexplored.%20In%20this%20paper%2C%20we%20examine%20the%20feasibility%20of%20applying%20existing%20MSDA%20methods%20to%20radar%20point%20clouds%20and%20identify%20several%20challenges%20in%20adapting%20these%20techniques.%20These%20obstacles%20stem%20from%20the%20radar%27s%20irregular%20angular%20distribution%2C%20deviations%20from%20a%20single-sensor%20polar%20layout%20in%20multi-radar%20setups%2C%20and%20point%20sparsity.%20To%20address%20these%20issues%2C%20we%20propose%20Class-Aware%20PillarMix%20%28CAPMix%29%2C%20a%20novel%20MSDA%20approach%20that%20applies%20MixUp%20at%20the%20pillar%20level%20in%203D%20point%20clouds%2C%20guided%20by%20class%20labels.%20Unlike%20methods%20that%20rely%20a%20single%20mix%20ratio%20to%20the%20entire%20sample%2C%20CAPMix%20assigns%20an%20independent%20ratio%20to%20each%20pillar%2C%20boosting%20sample%20diversity.%20To%20account%20for%20the%20density%20of%20different%20classes%2C%20we%20use%20class-specific%20distributions%3A%20for%20dense%20objects%20%28e.g.%2C%20large%20vehicles%29%2C%20we%20skew%20ratios%20to%20favor%20points%20from%20another%20sample%2C%20while%20for%20sparse%20objects%20%28e.g.%2C%20pedestrians%29%2C%20we%20sample%20more%20points%20from%20the%20original.%20This%20class-aware%20mixing%20retains%20critical%20details%20and%20enriches%20each%20sample%20with%20new%20information%2C%20ultimately%20generating%20more%20diverse%20training%20data.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%20significantly%20boosts%20performance%20but%20also%20outperforms%20existing%20MSDA%20approaches%20across%20two%20datasets%20%28Bosch%20Street%20and%20K-Radar%29.%20We%20believe%20that%20this%20straightforward%20yet%20effective%20approach%20will%20spark%20further%20investigation%20into%20MSDA%20techniques%20for%20radar%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2503.02687v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-Aware%2520PillarMix%253A%2520Can%2520Mixed%2520Sample%2520Data%2520Augmentation%2520Enhance%25203D%2520Object%2520Detection%2520with%2520Radar%2520Point%2520Clouds%253F%26entry.906535625%3DMiao%2520Zhang%2520and%2520Sherif%2520Abdulatif%2520and%2520Benedikt%2520Loesch%2520and%2520Marco%2520Altmann%2520and%2520Bin%2520Yang%26entry.1292438233%3DDue%2520to%2520the%2520significant%2520effort%2520required%2520for%2520data%2520collection%2520and%2520annotation%2520in%25203D%2520perception%2520tasks%252C%2520mixed%2520sample%2520data%2520augmentation%2520%2528MSDA%2529%2520has%2520been%2520widely%2520studied%2520to%2520generate%2520diverse%2520training%2520samples%2520by%2520mixing%2520existing%2520data.%2520Recently%252C%2520many%2520MSDA%2520techniques%2520have%2520been%2520developed%2520for%2520point%2520clouds%252C%2520but%2520they%2520mainly%2520target%2520LiDAR%2520data%252C%2520leaving%2520their%2520application%2520to%2520radar%2520point%2520clouds%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520feasibility%2520of%2520applying%2520existing%2520MSDA%2520methods%2520to%2520radar%2520point%2520clouds%2520and%2520identify%2520several%2520challenges%2520in%2520adapting%2520these%2520techniques.%2520These%2520obstacles%2520stem%2520from%2520the%2520radar%2527s%2520irregular%2520angular%2520distribution%252C%2520deviations%2520from%2520a%2520single-sensor%2520polar%2520layout%2520in%2520multi-radar%2520setups%252C%2520and%2520point%2520sparsity.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Class-Aware%2520PillarMix%2520%2528CAPMix%2529%252C%2520a%2520novel%2520MSDA%2520approach%2520that%2520applies%2520MixUp%2520at%2520the%2520pillar%2520level%2520in%25203D%2520point%2520clouds%252C%2520guided%2520by%2520class%2520labels.%2520Unlike%2520methods%2520that%2520rely%2520a%2520single%2520mix%2520ratio%2520to%2520the%2520entire%2520sample%252C%2520CAPMix%2520assigns%2520an%2520independent%2520ratio%2520to%2520each%2520pillar%252C%2520boosting%2520sample%2520diversity.%2520To%2520account%2520for%2520the%2520density%2520of%2520different%2520classes%252C%2520we%2520use%2520class-specific%2520distributions%253A%2520for%2520dense%2520objects%2520%2528e.g.%252C%2520large%2520vehicles%2529%252C%2520we%2520skew%2520ratios%2520to%2520favor%2520points%2520from%2520another%2520sample%252C%2520while%2520for%2520sparse%2520objects%2520%2528e.g.%252C%2520pedestrians%2529%252C%2520we%2520sample%2520more%2520points%2520from%2520the%2520original.%2520This%2520class-aware%2520mixing%2520retains%2520critical%2520details%2520and%2520enriches%2520each%2520sample%2520with%2520new%2520information%252C%2520ultimately%2520generating%2520more%2520diverse%2520training%2520data.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520not%2520only%2520significantly%2520boosts%2520performance%2520but%2520also%2520outperforms%2520existing%2520MSDA%2520approaches%2520across%2520two%2520datasets%2520%2528Bosch%2520Street%2520and%2520K-Radar%2529.%2520We%2520believe%2520that%2520this%2520straightforward%2520yet%2520effective%2520approach%2520will%2520spark%2520further%2520investigation%2520into%2520MSDA%2520techniques%2520for%2520radar%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02687v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-Aware%20PillarMix%3A%20Can%20Mixed%20Sample%20Data%20Augmentation%20Enhance%203D%20Object%20Detection%20with%20Radar%20Point%20Clouds%3F&entry.906535625=Miao%20Zhang%20and%20Sherif%20Abdulatif%20and%20Benedikt%20Loesch%20and%20Marco%20Altmann%20and%20Bin%20Yang&entry.1292438233=Due%20to%20the%20significant%20effort%20required%20for%20data%20collection%20and%20annotation%20in%203D%20perception%20tasks%2C%20mixed%20sample%20data%20augmentation%20%28MSDA%29%20has%20been%20widely%20studied%20to%20generate%20diverse%20training%20samples%20by%20mixing%20existing%20data.%20Recently%2C%20many%20MSDA%20techniques%20have%20been%20developed%20for%20point%20clouds%2C%20but%20they%20mainly%20target%20LiDAR%20data%2C%20leaving%20their%20application%20to%20radar%20point%20clouds%20largely%20unexplored.%20In%20this%20paper%2C%20we%20examine%20the%20feasibility%20of%20applying%20existing%20MSDA%20methods%20to%20radar%20point%20clouds%20and%20identify%20several%20challenges%20in%20adapting%20these%20techniques.%20These%20obstacles%20stem%20from%20the%20radar%27s%20irregular%20angular%20distribution%2C%20deviations%20from%20a%20single-sensor%20polar%20layout%20in%20multi-radar%20setups%2C%20and%20point%20sparsity.%20To%20address%20these%20issues%2C%20we%20propose%20Class-Aware%20PillarMix%20%28CAPMix%29%2C%20a%20novel%20MSDA%20approach%20that%20applies%20MixUp%20at%20the%20pillar%20level%20in%203D%20point%20clouds%2C%20guided%20by%20class%20labels.%20Unlike%20methods%20that%20rely%20a%20single%20mix%20ratio%20to%20the%20entire%20sample%2C%20CAPMix%20assigns%20an%20independent%20ratio%20to%20each%20pillar%2C%20boosting%20sample%20diversity.%20To%20account%20for%20the%20density%20of%20different%20classes%2C%20we%20use%20class-specific%20distributions%3A%20for%20dense%20objects%20%28e.g.%2C%20large%20vehicles%29%2C%20we%20skew%20ratios%20to%20favor%20points%20from%20another%20sample%2C%20while%20for%20sparse%20objects%20%28e.g.%2C%20pedestrians%29%2C%20we%20sample%20more%20points%20from%20the%20original.%20This%20class-aware%20mixing%20retains%20critical%20details%20and%20enriches%20each%20sample%20with%20new%20information%2C%20ultimately%20generating%20more%20diverse%20training%20data.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%20significantly%20boosts%20performance%20but%20also%20outperforms%20existing%20MSDA%20approaches%20across%20two%20datasets%20%28Bosch%20Street%20and%20K-Radar%29.%20We%20believe%20that%20this%20straightforward%20yet%20effective%20approach%20will%20spark%20further%20investigation%20into%20MSDA%20techniques%20for%20radar%20data.&entry.1838667208=http%3A//arxiv.org/abs/2503.02687v3&entry.124074799=Read"},
{"title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?", "author": "Yang Chen and Minghao Liu and Yufan Shen and Yunwen Li and Tianyuan Huang and Xinyu Fang and Tianyu Zheng and Wenxuan Huang and Cheng Yang and Daocheng Fu and Jianbiao Mei and Rong Wu and Yunfei Zhao and Licheng Wen and Xuemeng Yang and Song Mao and Qunshu Lin and Zhi Yu and Yongliang Shen and Yu Qiao and Botian Shi", "abstract": "The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available at https://github.com/SIGMME/IWR-Bench.", "link": "http://arxiv.org/abs/2509.24709v3", "date": "2025-11-19", "relevancy": 2.5894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IWR-Bench%3A%20Can%20LVLMs%20reconstruct%20interactive%20webpage%20from%20a%20user%20interaction%20video%3F&body=Title%3A%20IWR-Bench%3A%20Can%20LVLMs%20reconstruct%20interactive%20webpage%20from%20a%20user%20interaction%20video%3F%0AAuthor%3A%20Yang%20Chen%20and%20Minghao%20Liu%20and%20Yufan%20Shen%20and%20Yunwen%20Li%20and%20Tianyuan%20Huang%20and%20Xinyu%20Fang%20and%20Tianyu%20Zheng%20and%20Wenxuan%20Huang%20and%20Cheng%20Yang%20and%20Daocheng%20Fu%20and%20Jianbiao%20Mei%20and%20Rong%20Wu%20and%20Yunfei%20Zhao%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Song%20Mao%20and%20Qunshu%20Lin%20and%20Zhi%20Yu%20and%20Yongliang%20Shen%20and%20Yu%20Qiao%20and%20Botian%20Shi%0AAbstract%3A%20The%20webpage-to-code%20task%20requires%20models%20to%20understand%20visual%20representations%20of%20webpages%20and%20generate%20corresponding%20code.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20static%20screenshot-to-code%20tasks%2C%20thereby%20overlooking%20the%20dynamic%20interactions%20fundamental%20to%20real-world%20web%20applications.%20To%20address%20this%20limitation%2C%20this%20paper%20introduces%20IWR-Bench%2C%20a%20novel%20benchmark%20for%20evaluating%20the%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20in%20interactive%20webpage%20reconstruction%20from%20video.%20IWR-Bench%20comprises%20113%20meticulously%20curated%20tasks%20from%20100%20real-world%20websites%2C%20with%201%2C001%20actions%20and%20featuring%20diverse%20interaction%20complexities%20%28e.g.%2C%20web%20games%29%2C%20visual%20styles%2C%20and%20domains.%20Aligning%20with%20standard%20web%20development%20practices%2C%20each%20task%20includes%20not%20only%20user%20interaction%20videos%20but%20also%20all%20crawled%20static%20assets%20%28e.g.%2C%20images%2C%20videos%29.%20This%20benchmark%20evaluates%20models%20on%20two%20fundamental%20challenges%3A%20comprehensive%20multi-modal%20reasoning%20to%20infer%20interaction%20logic%20from%20video%20and%20assets%2C%20and%20advanced%20code%20generation%20to%20translate%20this%20logic%20into%20functional%20code.%20An%20agent-as-a-judge%20framework%20with%20a%20comprehensive%20metric%20system%20automatically%20assesses%20the%20functional%20correctness%20and%20visual%20fidelity%20of%20generated%20webpages.%20Extensive%20experiments%20on%2028%20LVLMs%20reveal%20a%20significant%20challenge%3A%20the%20best%20model%20achieves%20an%20overall%20score%20of%20only%2036.35%25%2C%20as%20functional%20correctness%20%2824.39%25%20IFS%29%20lags%20significantly%20behind%20visual%20fidelity%20%2864.25%25%20VFS%29.%20These%20results%20highlight%20critical%20limitations%20in%20current%20models%27%20ability%20to%20reason%20about%20temporal%20dynamics%20and%20synthesize%20event-driven%20logic%2C%20establishing%20IWR-Bench%20as%20a%20challenging%20frontier%20for%20vision-language%20research.%20The%20benchmark%20and%20evaluation%20code%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/SIGMME/IWR-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIWR-Bench%253A%2520Can%2520LVLMs%2520reconstruct%2520interactive%2520webpage%2520from%2520a%2520user%2520interaction%2520video%253F%26entry.906535625%3DYang%2520Chen%2520and%2520Minghao%2520Liu%2520and%2520Yufan%2520Shen%2520and%2520Yunwen%2520Li%2520and%2520Tianyuan%2520Huang%2520and%2520Xinyu%2520Fang%2520and%2520Tianyu%2520Zheng%2520and%2520Wenxuan%2520Huang%2520and%2520Cheng%2520Yang%2520and%2520Daocheng%2520Fu%2520and%2520Jianbiao%2520Mei%2520and%2520Rong%2520Wu%2520and%2520Yunfei%2520Zhao%2520and%2520Licheng%2520Wen%2520and%2520Xuemeng%2520Yang%2520and%2520Song%2520Mao%2520and%2520Qunshu%2520Lin%2520and%2520Zhi%2520Yu%2520and%2520Yongliang%2520Shen%2520and%2520Yu%2520Qiao%2520and%2520Botian%2520Shi%26entry.1292438233%3DThe%2520webpage-to-code%2520task%2520requires%2520models%2520to%2520understand%2520visual%2520representations%2520of%2520webpages%2520and%2520generate%2520corresponding%2520code.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520static%2520screenshot-to-code%2520tasks%252C%2520thereby%2520overlooking%2520the%2520dynamic%2520interactions%2520fundamental%2520to%2520real-world%2520web%2520applications.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%2520introduces%2520IWR-Bench%252C%2520a%2520novel%2520benchmark%2520for%2520evaluating%2520the%2520capabilities%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520in%2520interactive%2520webpage%2520reconstruction%2520from%2520video.%2520IWR-Bench%2520comprises%2520113%2520meticulously%2520curated%2520tasks%2520from%2520100%2520real-world%2520websites%252C%2520with%25201%252C001%2520actions%2520and%2520featuring%2520diverse%2520interaction%2520complexities%2520%2528e.g.%252C%2520web%2520games%2529%252C%2520visual%2520styles%252C%2520and%2520domains.%2520Aligning%2520with%2520standard%2520web%2520development%2520practices%252C%2520each%2520task%2520includes%2520not%2520only%2520user%2520interaction%2520videos%2520but%2520also%2520all%2520crawled%2520static%2520assets%2520%2528e.g.%252C%2520images%252C%2520videos%2529.%2520This%2520benchmark%2520evaluates%2520models%2520on%2520two%2520fundamental%2520challenges%253A%2520comprehensive%2520multi-modal%2520reasoning%2520to%2520infer%2520interaction%2520logic%2520from%2520video%2520and%2520assets%252C%2520and%2520advanced%2520code%2520generation%2520to%2520translate%2520this%2520logic%2520into%2520functional%2520code.%2520An%2520agent-as-a-judge%2520framework%2520with%2520a%2520comprehensive%2520metric%2520system%2520automatically%2520assesses%2520the%2520functional%2520correctness%2520and%2520visual%2520fidelity%2520of%2520generated%2520webpages.%2520Extensive%2520experiments%2520on%252028%2520LVLMs%2520reveal%2520a%2520significant%2520challenge%253A%2520the%2520best%2520model%2520achieves%2520an%2520overall%2520score%2520of%2520only%252036.35%2525%252C%2520as%2520functional%2520correctness%2520%252824.39%2525%2520IFS%2529%2520lags%2520significantly%2520behind%2520visual%2520fidelity%2520%252864.25%2525%2520VFS%2529.%2520These%2520results%2520highlight%2520critical%2520limitations%2520in%2520current%2520models%2527%2520ability%2520to%2520reason%2520about%2520temporal%2520dynamics%2520and%2520synthesize%2520event-driven%2520logic%252C%2520establishing%2520IWR-Bench%2520as%2520a%2520challenging%2520frontier%2520for%2520vision-language%2520research.%2520The%2520benchmark%2520and%2520evaluation%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/SIGMME/IWR-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IWR-Bench%3A%20Can%20LVLMs%20reconstruct%20interactive%20webpage%20from%20a%20user%20interaction%20video%3F&entry.906535625=Yang%20Chen%20and%20Minghao%20Liu%20and%20Yufan%20Shen%20and%20Yunwen%20Li%20and%20Tianyuan%20Huang%20and%20Xinyu%20Fang%20and%20Tianyu%20Zheng%20and%20Wenxuan%20Huang%20and%20Cheng%20Yang%20and%20Daocheng%20Fu%20and%20Jianbiao%20Mei%20and%20Rong%20Wu%20and%20Yunfei%20Zhao%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Song%20Mao%20and%20Qunshu%20Lin%20and%20Zhi%20Yu%20and%20Yongliang%20Shen%20and%20Yu%20Qiao%20and%20Botian%20Shi&entry.1292438233=The%20webpage-to-code%20task%20requires%20models%20to%20understand%20visual%20representations%20of%20webpages%20and%20generate%20corresponding%20code.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20static%20screenshot-to-code%20tasks%2C%20thereby%20overlooking%20the%20dynamic%20interactions%20fundamental%20to%20real-world%20web%20applications.%20To%20address%20this%20limitation%2C%20this%20paper%20introduces%20IWR-Bench%2C%20a%20novel%20benchmark%20for%20evaluating%20the%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20in%20interactive%20webpage%20reconstruction%20from%20video.%20IWR-Bench%20comprises%20113%20meticulously%20curated%20tasks%20from%20100%20real-world%20websites%2C%20with%201%2C001%20actions%20and%20featuring%20diverse%20interaction%20complexities%20%28e.g.%2C%20web%20games%29%2C%20visual%20styles%2C%20and%20domains.%20Aligning%20with%20standard%20web%20development%20practices%2C%20each%20task%20includes%20not%20only%20user%20interaction%20videos%20but%20also%20all%20crawled%20static%20assets%20%28e.g.%2C%20images%2C%20videos%29.%20This%20benchmark%20evaluates%20models%20on%20two%20fundamental%20challenges%3A%20comprehensive%20multi-modal%20reasoning%20to%20infer%20interaction%20logic%20from%20video%20and%20assets%2C%20and%20advanced%20code%20generation%20to%20translate%20this%20logic%20into%20functional%20code.%20An%20agent-as-a-judge%20framework%20with%20a%20comprehensive%20metric%20system%20automatically%20assesses%20the%20functional%20correctness%20and%20visual%20fidelity%20of%20generated%20webpages.%20Extensive%20experiments%20on%2028%20LVLMs%20reveal%20a%20significant%20challenge%3A%20the%20best%20model%20achieves%20an%20overall%20score%20of%20only%2036.35%25%2C%20as%20functional%20correctness%20%2824.39%25%20IFS%29%20lags%20significantly%20behind%20visual%20fidelity%20%2864.25%25%20VFS%29.%20These%20results%20highlight%20critical%20limitations%20in%20current%20models%27%20ability%20to%20reason%20about%20temporal%20dynamics%20and%20synthesize%20event-driven%20logic%2C%20establishing%20IWR-Bench%20as%20a%20challenging%20frontier%20for%20vision-language%20research.%20The%20benchmark%20and%20evaluation%20code%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/SIGMME/IWR-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2509.24709v3&entry.124074799=Read"},
{"title": "NTK-Guided Implicit Neural Teaching", "author": "Chen Zhang and Wei Zuo and Bingyang Cheng and Yikun Wang and Wei-Bin Kou and Yik Chung WU and Ngai Wong", "abstract": "Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.", "link": "http://arxiv.org/abs/2511.15487v1", "date": "2025-11-19", "relevancy": 2.5764, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5232}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5137}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTK-Guided%20Implicit%20Neural%20Teaching&body=Title%3A%20NTK-Guided%20Implicit%20Neural%20Teaching%0AAuthor%3A%20Chen%20Zhang%20and%20Wei%20Zuo%20and%20Bingyang%20Cheng%20and%20Yikun%20Wang%20and%20Wei-Bin%20Kou%20and%20Yik%20Chung%20WU%20and%20Ngai%20Wong%0AAbstract%3A%20Implicit%20Neural%20Representations%20%28INRs%29%20parameterize%20continuous%20signals%20via%20multilayer%20perceptrons%20%28MLPs%29%2C%20enabling%20compact%2C%20resolution-independent%20modeling%20for%20tasks%20like%20image%2C%20audio%2C%20and%203D%20reconstruction.%20However%2C%20fitting%20high-resolution%20signals%20demands%20optimizing%20over%20millions%20of%20coordinates%2C%20incurring%20prohibitive%20computational%20costs.%20To%20address%20it%2C%20we%20propose%20NTK-Guided%20Implicit%20Neural%20Teaching%20%28NINT%29%2C%20which%20accelerates%20training%20by%20dynamically%20selecting%20coordinates%20that%20maximize%20global%20functional%20updates.%20Leveraging%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%2C%20NINT%20scores%20examples%20by%20the%20norm%20of%20their%20NTK-augmented%20loss%20gradients%2C%20capturing%20both%20fitting%20errors%20and%20heterogeneous%20leverage%20%28self-influence%20and%20cross-coordinate%20coupling%29.%20This%20dual%20consideration%20enables%20faster%20convergence%20compared%20to%20existing%20methods.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20NINT%20significantly%20reduces%20training%20time%20by%20nearly%20half%20while%20maintaining%20or%20improving%20representation%20quality%2C%20establishing%20state-of-the-art%20acceleration%20among%20recent%20sampling-based%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTK-Guided%2520Implicit%2520Neural%2520Teaching%26entry.906535625%3DChen%2520Zhang%2520and%2520Wei%2520Zuo%2520and%2520Bingyang%2520Cheng%2520and%2520Yikun%2520Wang%2520and%2520Wei-Bin%2520Kou%2520and%2520Yik%2520Chung%2520WU%2520and%2520Ngai%2520Wong%26entry.1292438233%3DImplicit%2520Neural%2520Representations%2520%2528INRs%2529%2520parameterize%2520continuous%2520signals%2520via%2520multilayer%2520perceptrons%2520%2528MLPs%2529%252C%2520enabling%2520compact%252C%2520resolution-independent%2520modeling%2520for%2520tasks%2520like%2520image%252C%2520audio%252C%2520and%25203D%2520reconstruction.%2520However%252C%2520fitting%2520high-resolution%2520signals%2520demands%2520optimizing%2520over%2520millions%2520of%2520coordinates%252C%2520incurring%2520prohibitive%2520computational%2520costs.%2520To%2520address%2520it%252C%2520we%2520propose%2520NTK-Guided%2520Implicit%2520Neural%2520Teaching%2520%2528NINT%2529%252C%2520which%2520accelerates%2520training%2520by%2520dynamically%2520selecting%2520coordinates%2520that%2520maximize%2520global%2520functional%2520updates.%2520Leveraging%2520the%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%252C%2520NINT%2520scores%2520examples%2520by%2520the%2520norm%2520of%2520their%2520NTK-augmented%2520loss%2520gradients%252C%2520capturing%2520both%2520fitting%2520errors%2520and%2520heterogeneous%2520leverage%2520%2528self-influence%2520and%2520cross-coordinate%2520coupling%2529.%2520This%2520dual%2520consideration%2520enables%2520faster%2520convergence%2520compared%2520to%2520existing%2520methods.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520NINT%2520significantly%2520reduces%2520training%2520time%2520by%2520nearly%2520half%2520while%2520maintaining%2520or%2520improving%2520representation%2520quality%252C%2520establishing%2520state-of-the-art%2520acceleration%2520among%2520recent%2520sampling-based%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTK-Guided%20Implicit%20Neural%20Teaching&entry.906535625=Chen%20Zhang%20and%20Wei%20Zuo%20and%20Bingyang%20Cheng%20and%20Yikun%20Wang%20and%20Wei-Bin%20Kou%20and%20Yik%20Chung%20WU%20and%20Ngai%20Wong&entry.1292438233=Implicit%20Neural%20Representations%20%28INRs%29%20parameterize%20continuous%20signals%20via%20multilayer%20perceptrons%20%28MLPs%29%2C%20enabling%20compact%2C%20resolution-independent%20modeling%20for%20tasks%20like%20image%2C%20audio%2C%20and%203D%20reconstruction.%20However%2C%20fitting%20high-resolution%20signals%20demands%20optimizing%20over%20millions%20of%20coordinates%2C%20incurring%20prohibitive%20computational%20costs.%20To%20address%20it%2C%20we%20propose%20NTK-Guided%20Implicit%20Neural%20Teaching%20%28NINT%29%2C%20which%20accelerates%20training%20by%20dynamically%20selecting%20coordinates%20that%20maximize%20global%20functional%20updates.%20Leveraging%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%2C%20NINT%20scores%20examples%20by%20the%20norm%20of%20their%20NTK-augmented%20loss%20gradients%2C%20capturing%20both%20fitting%20errors%20and%20heterogeneous%20leverage%20%28self-influence%20and%20cross-coordinate%20coupling%29.%20This%20dual%20consideration%20enables%20faster%20convergence%20compared%20to%20existing%20methods.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20NINT%20significantly%20reduces%20training%20time%20by%20nearly%20half%20while%20maintaining%20or%20improving%20representation%20quality%2C%20establishing%20state-of-the-art%20acceleration%20among%20recent%20sampling-based%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2511.15487v1&entry.124074799=Read"},
{"title": "MHR: Momentum Human Rig", "author": "Aaron Ferguson and Ahmed A. A. Osman and Berta Bescos and Carsten Stoll and Chris Twigg and Christoph Lassner and David Otte and Eric Vignola and Federica Bogo and Igor Santesteban and Javier Romero and Jenna Zarate and Jeongseok Lee and Jinhyung Park and Jinlong Yang and John Doublestein and Kishore Venkateshan and Kris Kitani and Ladislav Kavan and Marco Dal Farra and Matthew Hu and Matthew Cioffi and Michael Fabris and Michael Ranieri and Mohammad Modarres and Petr Kadlecek and Rinat Abdrashitov and Romain Pr\u00e9vost and Roman Rajbhandari and Ronald Mallet and Russel Pearsall and Sandy Kao and Sanjeev Kumar and Scott Parrish and Te-Li Wang and Tony Tung and Yuan Dong and Yuhua Chen and Yuanlu Xu and Yuting Ye and Zhongshi Jiang", "abstract": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.", "link": "http://arxiv.org/abs/2511.15586v1", "date": "2025-11-19", "relevancy": 2.5513, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5694}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4982}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHR%3A%20Momentum%20Human%20Rig&body=Title%3A%20MHR%3A%20Momentum%20Human%20Rig%0AAuthor%3A%20Aaron%20Ferguson%20and%20Ahmed%20A.%20A.%20Osman%20and%20Berta%20Bescos%20and%20Carsten%20Stoll%20and%20Chris%20Twigg%20and%20Christoph%20Lassner%20and%20David%20Otte%20and%20Eric%20Vignola%20and%20Federica%20Bogo%20and%20Igor%20Santesteban%20and%20Javier%20Romero%20and%20Jenna%20Zarate%20and%20Jeongseok%20Lee%20and%20Jinhyung%20Park%20and%20Jinlong%20Yang%20and%20John%20Doublestein%20and%20Kishore%20Venkateshan%20and%20Kris%20Kitani%20and%20Ladislav%20Kavan%20and%20Marco%20Dal%20Farra%20and%20Matthew%20Hu%20and%20Matthew%20Cioffi%20and%20Michael%20Fabris%20and%20Michael%20Ranieri%20and%20Mohammad%20Modarres%20and%20Petr%20Kadlecek%20and%20Rinat%20Abdrashitov%20and%20Romain%20Pr%C3%A9vost%20and%20Roman%20Rajbhandari%20and%20Ronald%20Mallet%20and%20Russel%20Pearsall%20and%20Sandy%20Kao%20and%20Sanjeev%20Kumar%20and%20Scott%20Parrish%20and%20Te-Li%20Wang%20and%20Tony%20Tung%20and%20Yuan%20Dong%20and%20Yuhua%20Chen%20and%20Yuanlu%20Xu%20and%20Yuting%20Ye%20and%20Zhongshi%20Jiang%0AAbstract%3A%20We%20present%20MHR%2C%20a%20parametric%20human%20body%20model%20that%20combines%20the%20decoupled%20skeleton/shape%20paradigm%20of%20ATLAS%20with%20a%20flexible%2C%20modern%20rig%20and%20pose%20corrective%20system%20inspired%20by%20the%20Momentum%20library.%20Our%20model%20enables%20expressive%2C%20anatomically%20plausible%20human%20animation%2C%20supporting%20non-linear%20pose%20correctives%2C%20and%20is%20designed%20for%20robust%20integration%20in%20AR/VR%20and%20graphics%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHR%253A%2520Momentum%2520Human%2520Rig%26entry.906535625%3DAaron%2520Ferguson%2520and%2520Ahmed%2520A.%2520A.%2520Osman%2520and%2520Berta%2520Bescos%2520and%2520Carsten%2520Stoll%2520and%2520Chris%2520Twigg%2520and%2520Christoph%2520Lassner%2520and%2520David%2520Otte%2520and%2520Eric%2520Vignola%2520and%2520Federica%2520Bogo%2520and%2520Igor%2520Santesteban%2520and%2520Javier%2520Romero%2520and%2520Jenna%2520Zarate%2520and%2520Jeongseok%2520Lee%2520and%2520Jinhyung%2520Park%2520and%2520Jinlong%2520Yang%2520and%2520John%2520Doublestein%2520and%2520Kishore%2520Venkateshan%2520and%2520Kris%2520Kitani%2520and%2520Ladislav%2520Kavan%2520and%2520Marco%2520Dal%2520Farra%2520and%2520Matthew%2520Hu%2520and%2520Matthew%2520Cioffi%2520and%2520Michael%2520Fabris%2520and%2520Michael%2520Ranieri%2520and%2520Mohammad%2520Modarres%2520and%2520Petr%2520Kadlecek%2520and%2520Rinat%2520Abdrashitov%2520and%2520Romain%2520Pr%25C3%25A9vost%2520and%2520Roman%2520Rajbhandari%2520and%2520Ronald%2520Mallet%2520and%2520Russel%2520Pearsall%2520and%2520Sandy%2520Kao%2520and%2520Sanjeev%2520Kumar%2520and%2520Scott%2520Parrish%2520and%2520Te-Li%2520Wang%2520and%2520Tony%2520Tung%2520and%2520Yuan%2520Dong%2520and%2520Yuhua%2520Chen%2520and%2520Yuanlu%2520Xu%2520and%2520Yuting%2520Ye%2520and%2520Zhongshi%2520Jiang%26entry.1292438233%3DWe%2520present%2520MHR%252C%2520a%2520parametric%2520human%2520body%2520model%2520that%2520combines%2520the%2520decoupled%2520skeleton/shape%2520paradigm%2520of%2520ATLAS%2520with%2520a%2520flexible%252C%2520modern%2520rig%2520and%2520pose%2520corrective%2520system%2520inspired%2520by%2520the%2520Momentum%2520library.%2520Our%2520model%2520enables%2520expressive%252C%2520anatomically%2520plausible%2520human%2520animation%252C%2520supporting%2520non-linear%2520pose%2520correctives%252C%2520and%2520is%2520designed%2520for%2520robust%2520integration%2520in%2520AR/VR%2520and%2520graphics%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHR%3A%20Momentum%20Human%20Rig&entry.906535625=Aaron%20Ferguson%20and%20Ahmed%20A.%20A.%20Osman%20and%20Berta%20Bescos%20and%20Carsten%20Stoll%20and%20Chris%20Twigg%20and%20Christoph%20Lassner%20and%20David%20Otte%20and%20Eric%20Vignola%20and%20Federica%20Bogo%20and%20Igor%20Santesteban%20and%20Javier%20Romero%20and%20Jenna%20Zarate%20and%20Jeongseok%20Lee%20and%20Jinhyung%20Park%20and%20Jinlong%20Yang%20and%20John%20Doublestein%20and%20Kishore%20Venkateshan%20and%20Kris%20Kitani%20and%20Ladislav%20Kavan%20and%20Marco%20Dal%20Farra%20and%20Matthew%20Hu%20and%20Matthew%20Cioffi%20and%20Michael%20Fabris%20and%20Michael%20Ranieri%20and%20Mohammad%20Modarres%20and%20Petr%20Kadlecek%20and%20Rinat%20Abdrashitov%20and%20Romain%20Pr%C3%A9vost%20and%20Roman%20Rajbhandari%20and%20Ronald%20Mallet%20and%20Russel%20Pearsall%20and%20Sandy%20Kao%20and%20Sanjeev%20Kumar%20and%20Scott%20Parrish%20and%20Te-Li%20Wang%20and%20Tony%20Tung%20and%20Yuan%20Dong%20and%20Yuhua%20Chen%20and%20Yuanlu%20Xu%20and%20Yuting%20Ye%20and%20Zhongshi%20Jiang&entry.1292438233=We%20present%20MHR%2C%20a%20parametric%20human%20body%20model%20that%20combines%20the%20decoupled%20skeleton/shape%20paradigm%20of%20ATLAS%20with%20a%20flexible%2C%20modern%20rig%20and%20pose%20corrective%20system%20inspired%20by%20the%20Momentum%20library.%20Our%20model%20enables%20expressive%2C%20anatomically%20plausible%20human%20animation%2C%20supporting%20non-linear%20pose%20correctives%2C%20and%20is%20designed%20for%20robust%20integration%20in%20AR/VR%20and%20graphics%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2511.15586v1&entry.124074799=Read"},
{"title": "Parameter Importance-Driven Continual Learning for Foundation Models", "author": "Lingxiang Wang and Hainan Zhang and Zhiming Zheng", "abstract": "Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.", "link": "http://arxiv.org/abs/2511.15375v1", "date": "2025-11-19", "relevancy": 2.5416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter%20Importance-Driven%20Continual%20Learning%20for%20Foundation%20Models&body=Title%3A%20Parameter%20Importance-Driven%20Continual%20Learning%20for%20Foundation%20Models%0AAuthor%3A%20Lingxiang%20Wang%20and%20Hainan%20Zhang%20and%20Zhiming%20Zheng%0AAbstract%3A%20Domain-specific%20post-training%20often%20causes%20catastrophic%20forgetting%2C%20making%20foundation%20models%20lose%20their%20general%20reasoning%20ability%20and%20limiting%20their%20adaptability%20to%20dynamic%20real-world%20environments.%20Preserving%20general%20capabilities%20while%20acquiring%20downstream%20domain%20knowledge%20is%20a%20central%20challenge%20for%20large%20language%20and%20multimodal%20models.%20Traditional%20continual%20learning%20methods%2C%20such%20as%20regularization%2C%20replay%20and%20architectural%20isolation%2C%20suffer%20from%20poor%20downstream%20performance%2C%20reliance%20on%20inaccessible%20historical%20data%2C%20or%20additional%20parameter%20overhead.%20While%20recent%20parameter-efficient%20tuning%20%28PET%29%20methods%20can%20alleviate%20forgetting%2C%20their%20effectiveness%20strongly%20depends%20on%20the%20choice%20of%20parameters%20and%20update%20strategies.%20In%20this%20paper%2C%20we%20introduce%20PIECE%2C%20a%20Parameter%20Importance%20Estimation-based%20Continual%20Enhancement%20method%20that%20preserves%20general%20ability%20while%20efficiently%20learning%20domain%20knowledge%20without%20accessing%20prior%20training%20data%20or%20increasing%20model%20parameters.%20PIECE%20selectively%20updates%20only%200.1%25%20of%20core%20parameters%20most%20relevant%20to%20new%20tasks%2C%20guided%20by%20two%20importance%20estimators%3A%20PIECE-F%20based%20on%20Fisher%20Information%2C%20and%20PIECE-S%20based%20on%20a%20second-order%20normalization%20that%20combines%20gradient%20and%20curvature%20information.%20Experiments%20across%20three%20language%20models%20and%20two%20multimodal%20models%20show%20that%20PIECE%20maintains%20general%20capabilities%20and%20achieves%20state-of-the-art%20continual%20learning%20performance%20across%20diverse%20downstream%20tasks.%20Our%20results%20highlight%20a%20practical%20path%20to%20scalable%2C%20domain-adaptive%20foundation%20models%20without%20catastrophic%20forgetting.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter%2520Importance-Driven%2520Continual%2520Learning%2520for%2520Foundation%2520Models%26entry.906535625%3DLingxiang%2520Wang%2520and%2520Hainan%2520Zhang%2520and%2520Zhiming%2520Zheng%26entry.1292438233%3DDomain-specific%2520post-training%2520often%2520causes%2520catastrophic%2520forgetting%252C%2520making%2520foundation%2520models%2520lose%2520their%2520general%2520reasoning%2520ability%2520and%2520limiting%2520their%2520adaptability%2520to%2520dynamic%2520real-world%2520environments.%2520Preserving%2520general%2520capabilities%2520while%2520acquiring%2520downstream%2520domain%2520knowledge%2520is%2520a%2520central%2520challenge%2520for%2520large%2520language%2520and%2520multimodal%2520models.%2520Traditional%2520continual%2520learning%2520methods%252C%2520such%2520as%2520regularization%252C%2520replay%2520and%2520architectural%2520isolation%252C%2520suffer%2520from%2520poor%2520downstream%2520performance%252C%2520reliance%2520on%2520inaccessible%2520historical%2520data%252C%2520or%2520additional%2520parameter%2520overhead.%2520While%2520recent%2520parameter-efficient%2520tuning%2520%2528PET%2529%2520methods%2520can%2520alleviate%2520forgetting%252C%2520their%2520effectiveness%2520strongly%2520depends%2520on%2520the%2520choice%2520of%2520parameters%2520and%2520update%2520strategies.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PIECE%252C%2520a%2520Parameter%2520Importance%2520Estimation-based%2520Continual%2520Enhancement%2520method%2520that%2520preserves%2520general%2520ability%2520while%2520efficiently%2520learning%2520domain%2520knowledge%2520without%2520accessing%2520prior%2520training%2520data%2520or%2520increasing%2520model%2520parameters.%2520PIECE%2520selectively%2520updates%2520only%25200.1%2525%2520of%2520core%2520parameters%2520most%2520relevant%2520to%2520new%2520tasks%252C%2520guided%2520by%2520two%2520importance%2520estimators%253A%2520PIECE-F%2520based%2520on%2520Fisher%2520Information%252C%2520and%2520PIECE-S%2520based%2520on%2520a%2520second-order%2520normalization%2520that%2520combines%2520gradient%2520and%2520curvature%2520information.%2520Experiments%2520across%2520three%2520language%2520models%2520and%2520two%2520multimodal%2520models%2520show%2520that%2520PIECE%2520maintains%2520general%2520capabilities%2520and%2520achieves%2520state-of-the-art%2520continual%2520learning%2520performance%2520across%2520diverse%2520downstream%2520tasks.%2520Our%2520results%2520highlight%2520a%2520practical%2520path%2520to%2520scalable%252C%2520domain-adaptive%2520foundation%2520models%2520without%2520catastrophic%2520forgetting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Importance-Driven%20Continual%20Learning%20for%20Foundation%20Models&entry.906535625=Lingxiang%20Wang%20and%20Hainan%20Zhang%20and%20Zhiming%20Zheng&entry.1292438233=Domain-specific%20post-training%20often%20causes%20catastrophic%20forgetting%2C%20making%20foundation%20models%20lose%20their%20general%20reasoning%20ability%20and%20limiting%20their%20adaptability%20to%20dynamic%20real-world%20environments.%20Preserving%20general%20capabilities%20while%20acquiring%20downstream%20domain%20knowledge%20is%20a%20central%20challenge%20for%20large%20language%20and%20multimodal%20models.%20Traditional%20continual%20learning%20methods%2C%20such%20as%20regularization%2C%20replay%20and%20architectural%20isolation%2C%20suffer%20from%20poor%20downstream%20performance%2C%20reliance%20on%20inaccessible%20historical%20data%2C%20or%20additional%20parameter%20overhead.%20While%20recent%20parameter-efficient%20tuning%20%28PET%29%20methods%20can%20alleviate%20forgetting%2C%20their%20effectiveness%20strongly%20depends%20on%20the%20choice%20of%20parameters%20and%20update%20strategies.%20In%20this%20paper%2C%20we%20introduce%20PIECE%2C%20a%20Parameter%20Importance%20Estimation-based%20Continual%20Enhancement%20method%20that%20preserves%20general%20ability%20while%20efficiently%20learning%20domain%20knowledge%20without%20accessing%20prior%20training%20data%20or%20increasing%20model%20parameters.%20PIECE%20selectively%20updates%20only%200.1%25%20of%20core%20parameters%20most%20relevant%20to%20new%20tasks%2C%20guided%20by%20two%20importance%20estimators%3A%20PIECE-F%20based%20on%20Fisher%20Information%2C%20and%20PIECE-S%20based%20on%20a%20second-order%20normalization%20that%20combines%20gradient%20and%20curvature%20information.%20Experiments%20across%20three%20language%20models%20and%20two%20multimodal%20models%20show%20that%20PIECE%20maintains%20general%20capabilities%20and%20achieves%20state-of-the-art%20continual%20learning%20performance%20across%20diverse%20downstream%20tasks.%20Our%20results%20highlight%20a%20practical%20path%20to%20scalable%2C%20domain-adaptive%20foundation%20models%20without%20catastrophic%20forgetting.&entry.1838667208=http%3A//arxiv.org/abs/2511.15375v1&entry.124074799=Read"},
{"title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation", "author": "Linyin Luo and Yujuan Ding and Yunshan Ma and Wenqi Fan and Hanjiang Lai", "abstract": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.", "link": "http://arxiv.org/abs/2511.15435v1", "date": "2025-11-19", "relevancy": 2.5347, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.51}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HV-Attack%3A%20Hierarchical%20Visual%20Attack%20for%20Multimodal%20Retrieval%20Augmented%20Generation&body=Title%3A%20HV-Attack%3A%20Hierarchical%20Visual%20Attack%20for%20Multimodal%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Linyin%20Luo%20and%20Yujuan%20Ding%20and%20Yunshan%20Ma%20and%20Wenqi%20Fan%20and%20Hanjiang%20Lai%0AAbstract%3A%20Advanced%20multimodal%20Retrieval-Augmented%20Generation%20%28MRAG%29%20techniques%20have%20been%20widely%20applied%20to%20enhance%20the%20capabilities%20of%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20but%20they%20also%20bring%20along%20novel%20safety%20issues.%20Existing%20adversarial%20research%20has%20revealed%20the%20vulnerability%20of%20MRAG%20systems%20to%20knowledge%20poisoning%20attacks%2C%20which%20fool%20the%20retriever%20into%20recalling%20injected%20poisoned%20contents.%20However%2C%20our%20work%20considers%20a%20different%20setting%3A%20visual%20attack%20of%20MRAG%20by%20solely%20adding%20imperceptible%20perturbations%20at%20the%20image%20inputs%20of%20users%2C%20without%20manipulating%20any%20other%20components.%20This%20is%20challenging%20due%20to%20the%20robustness%20of%20fine-tuned%20retrievers%20and%20large-scale%20generators%2C%20and%20the%20effect%20of%20visual%20perturbation%20may%20be%20further%20weakened%20by%20propagation%20through%20the%20RAG%20chain.%20We%20propose%20a%20novel%20Hierarchical%20Visual%20Attack%20that%20misaligns%20and%20disrupts%20the%20two%20inputs%20%28the%20multimodal%20query%20and%20the%20augmented%20knowledge%29%20of%20MRAG%27s%20generator%20to%20confuse%20its%20generation.%20We%20further%20design%20a%20hierarchical%20two-stage%20strategy%20to%20obtain%20misaligned%20augmented%20knowledge.%20We%20disrupt%20the%20image%20input%20of%20the%20retriever%20to%20make%20it%20recall%20irrelevant%20knowledge%20from%20the%20original%20database%2C%20by%20optimizing%20the%20perturbation%20which%20first%20breaks%20the%20cross-modal%20alignment%20and%20then%20disrupts%20the%20multimodal%20semantic%20alignment.%20We%20conduct%20extensive%20experiments%20on%20two%20widely-used%20MRAG%20datasets%3A%20OK-VQA%20and%20InfoSeek.%20We%20use%20CLIP-based%20retrievers%20and%20two%20LMMs%20BLIP-2%20and%20LLaVA%20as%20generators.%20Results%20demonstrate%20the%20effectiveness%20of%20our%20visual%20attack%20on%20MRAG%20through%20the%20significant%20decrease%20in%20both%20retrieval%20and%20generation%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHV-Attack%253A%2520Hierarchical%2520Visual%2520Attack%2520for%2520Multimodal%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DLinyin%2520Luo%2520and%2520Yujuan%2520Ding%2520and%2520Yunshan%2520Ma%2520and%2520Wenqi%2520Fan%2520and%2520Hanjiang%2520Lai%26entry.1292438233%3DAdvanced%2520multimodal%2520Retrieval-Augmented%2520Generation%2520%2528MRAG%2529%2520techniques%2520have%2520been%2520widely%2520applied%2520to%2520enhance%2520the%2520capabilities%2520of%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%252C%2520but%2520they%2520also%2520bring%2520along%2520novel%2520safety%2520issues.%2520Existing%2520adversarial%2520research%2520has%2520revealed%2520the%2520vulnerability%2520of%2520MRAG%2520systems%2520to%2520knowledge%2520poisoning%2520attacks%252C%2520which%2520fool%2520the%2520retriever%2520into%2520recalling%2520injected%2520poisoned%2520contents.%2520However%252C%2520our%2520work%2520considers%2520a%2520different%2520setting%253A%2520visual%2520attack%2520of%2520MRAG%2520by%2520solely%2520adding%2520imperceptible%2520perturbations%2520at%2520the%2520image%2520inputs%2520of%2520users%252C%2520without%2520manipulating%2520any%2520other%2520components.%2520This%2520is%2520challenging%2520due%2520to%2520the%2520robustness%2520of%2520fine-tuned%2520retrievers%2520and%2520large-scale%2520generators%252C%2520and%2520the%2520effect%2520of%2520visual%2520perturbation%2520may%2520be%2520further%2520weakened%2520by%2520propagation%2520through%2520the%2520RAG%2520chain.%2520We%2520propose%2520a%2520novel%2520Hierarchical%2520Visual%2520Attack%2520that%2520misaligns%2520and%2520disrupts%2520the%2520two%2520inputs%2520%2528the%2520multimodal%2520query%2520and%2520the%2520augmented%2520knowledge%2529%2520of%2520MRAG%2527s%2520generator%2520to%2520confuse%2520its%2520generation.%2520We%2520further%2520design%2520a%2520hierarchical%2520two-stage%2520strategy%2520to%2520obtain%2520misaligned%2520augmented%2520knowledge.%2520We%2520disrupt%2520the%2520image%2520input%2520of%2520the%2520retriever%2520to%2520make%2520it%2520recall%2520irrelevant%2520knowledge%2520from%2520the%2520original%2520database%252C%2520by%2520optimizing%2520the%2520perturbation%2520which%2520first%2520breaks%2520the%2520cross-modal%2520alignment%2520and%2520then%2520disrupts%2520the%2520multimodal%2520semantic%2520alignment.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520two%2520widely-used%2520MRAG%2520datasets%253A%2520OK-VQA%2520and%2520InfoSeek.%2520We%2520use%2520CLIP-based%2520retrievers%2520and%2520two%2520LMMs%2520BLIP-2%2520and%2520LLaVA%2520as%2520generators.%2520Results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520visual%2520attack%2520on%2520MRAG%2520through%2520the%2520significant%2520decrease%2520in%2520both%2520retrieval%2520and%2520generation%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HV-Attack%3A%20Hierarchical%20Visual%20Attack%20for%20Multimodal%20Retrieval%20Augmented%20Generation&entry.906535625=Linyin%20Luo%20and%20Yujuan%20Ding%20and%20Yunshan%20Ma%20and%20Wenqi%20Fan%20and%20Hanjiang%20Lai&entry.1292438233=Advanced%20multimodal%20Retrieval-Augmented%20Generation%20%28MRAG%29%20techniques%20have%20been%20widely%20applied%20to%20enhance%20the%20capabilities%20of%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20but%20they%20also%20bring%20along%20novel%20safety%20issues.%20Existing%20adversarial%20research%20has%20revealed%20the%20vulnerability%20of%20MRAG%20systems%20to%20knowledge%20poisoning%20attacks%2C%20which%20fool%20the%20retriever%20into%20recalling%20injected%20poisoned%20contents.%20However%2C%20our%20work%20considers%20a%20different%20setting%3A%20visual%20attack%20of%20MRAG%20by%20solely%20adding%20imperceptible%20perturbations%20at%20the%20image%20inputs%20of%20users%2C%20without%20manipulating%20any%20other%20components.%20This%20is%20challenging%20due%20to%20the%20robustness%20of%20fine-tuned%20retrievers%20and%20large-scale%20generators%2C%20and%20the%20effect%20of%20visual%20perturbation%20may%20be%20further%20weakened%20by%20propagation%20through%20the%20RAG%20chain.%20We%20propose%20a%20novel%20Hierarchical%20Visual%20Attack%20that%20misaligns%20and%20disrupts%20the%20two%20inputs%20%28the%20multimodal%20query%20and%20the%20augmented%20knowledge%29%20of%20MRAG%27s%20generator%20to%20confuse%20its%20generation.%20We%20further%20design%20a%20hierarchical%20two-stage%20strategy%20to%20obtain%20misaligned%20augmented%20knowledge.%20We%20disrupt%20the%20image%20input%20of%20the%20retriever%20to%20make%20it%20recall%20irrelevant%20knowledge%20from%20the%20original%20database%2C%20by%20optimizing%20the%20perturbation%20which%20first%20breaks%20the%20cross-modal%20alignment%20and%20then%20disrupts%20the%20multimodal%20semantic%20alignment.%20We%20conduct%20extensive%20experiments%20on%20two%20widely-used%20MRAG%20datasets%3A%20OK-VQA%20and%20InfoSeek.%20We%20use%20CLIP-based%20retrievers%20and%20two%20LMMs%20BLIP-2%20and%20LLaVA%20as%20generators.%20Results%20demonstrate%20the%20effectiveness%20of%20our%20visual%20attack%20on%20MRAG%20through%20the%20significant%20decrease%20in%20both%20retrieval%20and%20generation%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.15435v1&entry.124074799=Read"},
{"title": "On the Internal Semantics of Time-Series Foundation Models", "author": "Atharva Pandey and Abhilash Neog and Gautam Jajoo", "abstract": "Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.", "link": "http://arxiv.org/abs/2511.15324v1", "date": "2025-11-19", "relevancy": 2.5223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Internal%20Semantics%20of%20Time-Series%20Foundation%20Models&body=Title%3A%20On%20the%20Internal%20Semantics%20of%20Time-Series%20Foundation%20Models%0AAuthor%3A%20Atharva%20Pandey%20and%20Abhilash%20Neog%20and%20Gautam%20Jajoo%0AAbstract%3A%20Time-series%20Foundation%20Models%20%28TSFMs%29%20have%20recently%20emerged%20as%20a%20universal%20paradigm%20for%20learning%20across%20diverse%20temporal%20domains.%20However%2C%20despite%20their%20empirical%20success%2C%20the%20internal%20mechanisms%20by%20which%20these%20models%20represent%20fundamental%20time-series%20concepts%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20undertake%20a%20systematic%20investigation%20of%20concept%20interpretability%20in%20TSFMs.%20Specifically%2C%20we%20examine%3A%20%28i%29%20which%20layers%20encode%20which%20concepts%2C%20%28ii%29%20whether%20concept%20parameters%20are%20linearly%20recoverable%2C%20%28iii%29%20how%20representations%20evolve%20in%20terms%20of%20concept%20disentanglement%20and%20abstraction%20across%20model%20depth%2C%20and%20%28iv%29%20how%20models%20process%20compositions%20of%20concepts.%20We%20systematically%20probe%20these%20questions%20using%20layer-wise%20analyses%2C%20linear%20recoverability%20tests%2C%20and%20representation%20similarity%20measures%2C%20providing%20a%20structured%20account%20of%20TSFM%20semantics.%20The%20resulting%20insights%20show%20that%20early%20layers%20mainly%20capture%20local%2C%20time-domain%20patterns%20%28e.g.%2C%20AR%281%29%2C%20level%20shifts%2C%20trends%29%2C%20while%20deeper%20layers%20encode%20dispersion%20and%20change-time%20signals%2C%20with%20spectral%20and%20warping%20factors%20remaining%20the%20hardest%20to%20recover%20linearly.%20In%20compositional%20settings%2C%20however%2C%20probe%20performance%20degrades%2C%20revealing%20interference%20between%20concepts.%20This%20highlights%20that%20while%20atomic%20concepts%20are%20reliably%20localized%2C%20composition%20remains%20a%20challenge%2C%20underscoring%20a%20key%20limitation%20in%20current%20TSFMs%27%20ability%20to%20represent%20interacting%20temporal%20phenomena.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Internal%2520Semantics%2520of%2520Time-Series%2520Foundation%2520Models%26entry.906535625%3DAtharva%2520Pandey%2520and%2520Abhilash%2520Neog%2520and%2520Gautam%2520Jajoo%26entry.1292438233%3DTime-series%2520Foundation%2520Models%2520%2528TSFMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520universal%2520paradigm%2520for%2520learning%2520across%2520diverse%2520temporal%2520domains.%2520However%252C%2520despite%2520their%2520empirical%2520success%252C%2520the%2520internal%2520mechanisms%2520by%2520which%2520these%2520models%2520represent%2520fundamental%2520time-series%2520concepts%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520undertake%2520a%2520systematic%2520investigation%2520of%2520concept%2520interpretability%2520in%2520TSFMs.%2520Specifically%252C%2520we%2520examine%253A%2520%2528i%2529%2520which%2520layers%2520encode%2520which%2520concepts%252C%2520%2528ii%2529%2520whether%2520concept%2520parameters%2520are%2520linearly%2520recoverable%252C%2520%2528iii%2529%2520how%2520representations%2520evolve%2520in%2520terms%2520of%2520concept%2520disentanglement%2520and%2520abstraction%2520across%2520model%2520depth%252C%2520and%2520%2528iv%2529%2520how%2520models%2520process%2520compositions%2520of%2520concepts.%2520We%2520systematically%2520probe%2520these%2520questions%2520using%2520layer-wise%2520analyses%252C%2520linear%2520recoverability%2520tests%252C%2520and%2520representation%2520similarity%2520measures%252C%2520providing%2520a%2520structured%2520account%2520of%2520TSFM%2520semantics.%2520The%2520resulting%2520insights%2520show%2520that%2520early%2520layers%2520mainly%2520capture%2520local%252C%2520time-domain%2520patterns%2520%2528e.g.%252C%2520AR%25281%2529%252C%2520level%2520shifts%252C%2520trends%2529%252C%2520while%2520deeper%2520layers%2520encode%2520dispersion%2520and%2520change-time%2520signals%252C%2520with%2520spectral%2520and%2520warping%2520factors%2520remaining%2520the%2520hardest%2520to%2520recover%2520linearly.%2520In%2520compositional%2520settings%252C%2520however%252C%2520probe%2520performance%2520degrades%252C%2520revealing%2520interference%2520between%2520concepts.%2520This%2520highlights%2520that%2520while%2520atomic%2520concepts%2520are%2520reliably%2520localized%252C%2520composition%2520remains%2520a%2520challenge%252C%2520underscoring%2520a%2520key%2520limitation%2520in%2520current%2520TSFMs%2527%2520ability%2520to%2520represent%2520interacting%2520temporal%2520phenomena.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Internal%20Semantics%20of%20Time-Series%20Foundation%20Models&entry.906535625=Atharva%20Pandey%20and%20Abhilash%20Neog%20and%20Gautam%20Jajoo&entry.1292438233=Time-series%20Foundation%20Models%20%28TSFMs%29%20have%20recently%20emerged%20as%20a%20universal%20paradigm%20for%20learning%20across%20diverse%20temporal%20domains.%20However%2C%20despite%20their%20empirical%20success%2C%20the%20internal%20mechanisms%20by%20which%20these%20models%20represent%20fundamental%20time-series%20concepts%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20undertake%20a%20systematic%20investigation%20of%20concept%20interpretability%20in%20TSFMs.%20Specifically%2C%20we%20examine%3A%20%28i%29%20which%20layers%20encode%20which%20concepts%2C%20%28ii%29%20whether%20concept%20parameters%20are%20linearly%20recoverable%2C%20%28iii%29%20how%20representations%20evolve%20in%20terms%20of%20concept%20disentanglement%20and%20abstraction%20across%20model%20depth%2C%20and%20%28iv%29%20how%20models%20process%20compositions%20of%20concepts.%20We%20systematically%20probe%20these%20questions%20using%20layer-wise%20analyses%2C%20linear%20recoverability%20tests%2C%20and%20representation%20similarity%20measures%2C%20providing%20a%20structured%20account%20of%20TSFM%20semantics.%20The%20resulting%20insights%20show%20that%20early%20layers%20mainly%20capture%20local%2C%20time-domain%20patterns%20%28e.g.%2C%20AR%281%29%2C%20level%20shifts%2C%20trends%29%2C%20while%20deeper%20layers%20encode%20dispersion%20and%20change-time%20signals%2C%20with%20spectral%20and%20warping%20factors%20remaining%20the%20hardest%20to%20recover%20linearly.%20In%20compositional%20settings%2C%20however%2C%20probe%20performance%20degrades%2C%20revealing%20interference%20between%20concepts.%20This%20highlights%20that%20while%20atomic%20concepts%20are%20reliably%20localized%2C%20composition%20remains%20a%20challenge%2C%20underscoring%20a%20key%20limitation%20in%20current%20TSFMs%27%20ability%20to%20represent%20interacting%20temporal%20phenomena.&entry.1838667208=http%3A//arxiv.org/abs/2511.15324v1&entry.124074799=Read"},
{"title": "LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials", "author": "Huseyin Goksu", "abstract": "Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on \"heterophilic\" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.", "link": "http://arxiv.org/abs/2511.15328v1", "date": "2025-11-19", "relevancy": 2.5149, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5301}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5036}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaguerreNet%3A%20Advancing%20a%20Unified%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Continuous%20Polynomials&body=Title%3A%20LaguerreNet%3A%20Advancing%20a%20Unified%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Continuous%20Polynomials%0AAuthor%3A%20Huseyin%20Goksu%0AAbstract%3A%20Spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20suffer%20from%20two%20critical%20limitations%3A%20poor%20performance%20on%20%22heterophilic%22%20graphs%20and%20performance%20collapse%20at%20high%20polynomial%20degrees%20%28K%29%2C%20known%20as%20over-smoothing.%20Both%20issues%20stem%20from%20the%20static%2C%20low-pass%20nature%20of%20standard%20filters%20%28e.g.%2C%20ChebyNet%29.%20While%20adaptive%20polynomial%20filters%2C%20such%20as%20the%20discrete%20MeixnerNet%2C%20have%20emerged%20as%20a%20potential%20unified%20solution%2C%20their%20extension%20to%20the%20continuous%20domain%20and%20stability%20with%20unbounded%20coefficients%20remain%20open%20questions.%20In%20this%20work%2C%20we%20propose%20%60LaguerreNet%60%2C%20a%20novel%20GNN%20filter%20based%20on%20continuous%20Laguerre%20polynomials.%20%60LaguerreNet%60%20learns%20the%20filter%27s%20spectral%20shape%20by%20making%20its%20core%20alpha%20parameter%20trainable%2C%20thereby%20advancing%20the%20adaptive%20polynomial%20approach.%20We%20solve%20the%20severe%20O%28k%5E2%29%20numerical%20instability%20of%20these%20unbounded%20polynomials%20using%20a%20%60LayerNorm%60-based%20stabilization%20technique.%20We%20demonstrate%20experimentally%20that%20this%20approach%20is%20highly%20effective%3A%201%29%20%60LaguerreNet%60%20achieves%20state-of-the-art%20results%20on%20challenging%20heterophilic%20benchmarks.%202%29%20It%20is%20exceptionally%20robust%20to%20over-smoothing%2C%20with%20performance%20peaking%20at%20K%3D10%2C%20an%20order%20of%20magnitude%20beyond%20where%20ChebyNet%20collapses.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaguerreNet%253A%2520Advancing%2520a%2520Unified%2520Solution%2520for%2520Heterophily%2520and%2520Over-smoothing%2520with%2520Adaptive%2520Continuous%2520Polynomials%26entry.906535625%3DHuseyin%2520Goksu%26entry.1292438233%3DSpectral%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520suffer%2520from%2520two%2520critical%2520limitations%253A%2520poor%2520performance%2520on%2520%2522heterophilic%2522%2520graphs%2520and%2520performance%2520collapse%2520at%2520high%2520polynomial%2520degrees%2520%2528K%2529%252C%2520known%2520as%2520over-smoothing.%2520Both%2520issues%2520stem%2520from%2520the%2520static%252C%2520low-pass%2520nature%2520of%2520standard%2520filters%2520%2528e.g.%252C%2520ChebyNet%2529.%2520While%2520adaptive%2520polynomial%2520filters%252C%2520such%2520as%2520the%2520discrete%2520MeixnerNet%252C%2520have%2520emerged%2520as%2520a%2520potential%2520unified%2520solution%252C%2520their%2520extension%2520to%2520the%2520continuous%2520domain%2520and%2520stability%2520with%2520unbounded%2520coefficients%2520remain%2520open%2520questions.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2560LaguerreNet%2560%252C%2520a%2520novel%2520GNN%2520filter%2520based%2520on%2520continuous%2520Laguerre%2520polynomials.%2520%2560LaguerreNet%2560%2520learns%2520the%2520filter%2527s%2520spectral%2520shape%2520by%2520making%2520its%2520core%2520alpha%2520parameter%2520trainable%252C%2520thereby%2520advancing%2520the%2520adaptive%2520polynomial%2520approach.%2520We%2520solve%2520the%2520severe%2520O%2528k%255E2%2529%2520numerical%2520instability%2520of%2520these%2520unbounded%2520polynomials%2520using%2520a%2520%2560LayerNorm%2560-based%2520stabilization%2520technique.%2520We%2520demonstrate%2520experimentally%2520that%2520this%2520approach%2520is%2520highly%2520effective%253A%25201%2529%2520%2560LaguerreNet%2560%2520achieves%2520state-of-the-art%2520results%2520on%2520challenging%2520heterophilic%2520benchmarks.%25202%2529%2520It%2520is%2520exceptionally%2520robust%2520to%2520over-smoothing%252C%2520with%2520performance%2520peaking%2520at%2520K%253D10%252C%2520an%2520order%2520of%2520magnitude%2520beyond%2520where%2520ChebyNet%2520collapses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaguerreNet%3A%20Advancing%20a%20Unified%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Continuous%20Polynomials&entry.906535625=Huseyin%20Goksu&entry.1292438233=Spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20suffer%20from%20two%20critical%20limitations%3A%20poor%20performance%20on%20%22heterophilic%22%20graphs%20and%20performance%20collapse%20at%20high%20polynomial%20degrees%20%28K%29%2C%20known%20as%20over-smoothing.%20Both%20issues%20stem%20from%20the%20static%2C%20low-pass%20nature%20of%20standard%20filters%20%28e.g.%2C%20ChebyNet%29.%20While%20adaptive%20polynomial%20filters%2C%20such%20as%20the%20discrete%20MeixnerNet%2C%20have%20emerged%20as%20a%20potential%20unified%20solution%2C%20their%20extension%20to%20the%20continuous%20domain%20and%20stability%20with%20unbounded%20coefficients%20remain%20open%20questions.%20In%20this%20work%2C%20we%20propose%20%60LaguerreNet%60%2C%20a%20novel%20GNN%20filter%20based%20on%20continuous%20Laguerre%20polynomials.%20%60LaguerreNet%60%20learns%20the%20filter%27s%20spectral%20shape%20by%20making%20its%20core%20alpha%20parameter%20trainable%2C%20thereby%20advancing%20the%20adaptive%20polynomial%20approach.%20We%20solve%20the%20severe%20O%28k%5E2%29%20numerical%20instability%20of%20these%20unbounded%20polynomials%20using%20a%20%60LayerNorm%60-based%20stabilization%20technique.%20We%20demonstrate%20experimentally%20that%20this%20approach%20is%20highly%20effective%3A%201%29%20%60LaguerreNet%60%20achieves%20state-of-the-art%20results%20on%20challenging%20heterophilic%20benchmarks.%202%29%20It%20is%20exceptionally%20robust%20to%20over-smoothing%2C%20with%20performance%20peaking%20at%20K%3D10%2C%20an%20order%20of%20magnitude%20beyond%20where%20ChebyNet%20collapses.&entry.1838667208=http%3A//arxiv.org/abs/2511.15328v1&entry.124074799=Read"},
{"title": "Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras", "author": "Ziyan Liu and Qi Su and Lulu Tang and Zhaofei Yu and Tiejun Huang", "abstract": "Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.", "link": "http://arxiv.org/abs/2511.15459v1", "date": "2025-11-19", "relevancy": 2.5132, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5043}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5039}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20in%20Spikes%3A%20An%20Entropy-Guided%20Object%20Detector%20for%20Spike%20Cameras&body=Title%3A%20Driving%20in%20Spikes%3A%20An%20Entropy-Guided%20Object%20Detector%20for%20Spike%20Cameras%0AAuthor%3A%20Ziyan%20Liu%20and%20Qi%20Su%20and%20Lulu%20Tang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang%0AAbstract%3A%20Object%20detection%20in%20autonomous%20driving%20suffers%20from%20motion%20blur%20and%20saturation%20under%20fast%20motion%20and%20extreme%20lighting.%20Spike%20cameras%2C%20offer%20microsecond%20latency%20and%20ultra%20high%20dynamic%20range%20for%20object%20detection%20by%20using%20per%20pixel%20asynchronous%20integrate%20and%20fire.%20However%2C%20their%20sparse%2C%20discrete%20output%20cannot%20be%20processed%20by%20standard%20image-based%20detectors%2C%20posing%20a%20critical%20challenge%20for%20end%20to%20end%20spike%20stream%20detection.%20We%20propose%20EASD%2C%20an%20end%20to%20end%20spike%20camera%20detector%20with%20a%20dual%20branch%20design%3A%20a%20Temporal%20Based%20Texture%20plus%20Feature%20Fusion%20branch%20for%20global%20cross%20slice%20semantics%2C%20and%20an%20Entropy%20Selective%20Attention%20branch%20for%20object%20centric%20details.%20To%20close%20the%20data%20gap%2C%20we%20introduce%20DSEC%20Spike%2C%20the%20first%20driving%20oriented%20simulated%20spike%20detection%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520in%2520Spikes%253A%2520An%2520Entropy-Guided%2520Object%2520Detector%2520for%2520Spike%2520Cameras%26entry.906535625%3DZiyan%2520Liu%2520and%2520Qi%2520Su%2520and%2520Lulu%2520Tang%2520and%2520Zhaofei%2520Yu%2520and%2520Tiejun%2520Huang%26entry.1292438233%3DObject%2520detection%2520in%2520autonomous%2520driving%2520suffers%2520from%2520motion%2520blur%2520and%2520saturation%2520under%2520fast%2520motion%2520and%2520extreme%2520lighting.%2520Spike%2520cameras%252C%2520offer%2520microsecond%2520latency%2520and%2520ultra%2520high%2520dynamic%2520range%2520for%2520object%2520detection%2520by%2520using%2520per%2520pixel%2520asynchronous%2520integrate%2520and%2520fire.%2520However%252C%2520their%2520sparse%252C%2520discrete%2520output%2520cannot%2520be%2520processed%2520by%2520standard%2520image-based%2520detectors%252C%2520posing%2520a%2520critical%2520challenge%2520for%2520end%2520to%2520end%2520spike%2520stream%2520detection.%2520We%2520propose%2520EASD%252C%2520an%2520end%2520to%2520end%2520spike%2520camera%2520detector%2520with%2520a%2520dual%2520branch%2520design%253A%2520a%2520Temporal%2520Based%2520Texture%2520plus%2520Feature%2520Fusion%2520branch%2520for%2520global%2520cross%2520slice%2520semantics%252C%2520and%2520an%2520Entropy%2520Selective%2520Attention%2520branch%2520for%2520object%2520centric%2520details.%2520To%2520close%2520the%2520data%2520gap%252C%2520we%2520introduce%2520DSEC%2520Spike%252C%2520the%2520first%2520driving%2520oriented%2520simulated%2520spike%2520detection%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20in%20Spikes%3A%20An%20Entropy-Guided%20Object%20Detector%20for%20Spike%20Cameras&entry.906535625=Ziyan%20Liu%20and%20Qi%20Su%20and%20Lulu%20Tang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang&entry.1292438233=Object%20detection%20in%20autonomous%20driving%20suffers%20from%20motion%20blur%20and%20saturation%20under%20fast%20motion%20and%20extreme%20lighting.%20Spike%20cameras%2C%20offer%20microsecond%20latency%20and%20ultra%20high%20dynamic%20range%20for%20object%20detection%20by%20using%20per%20pixel%20asynchronous%20integrate%20and%20fire.%20However%2C%20their%20sparse%2C%20discrete%20output%20cannot%20be%20processed%20by%20standard%20image-based%20detectors%2C%20posing%20a%20critical%20challenge%20for%20end%20to%20end%20spike%20stream%20detection.%20We%20propose%20EASD%2C%20an%20end%20to%20end%20spike%20camera%20detector%20with%20a%20dual%20branch%20design%3A%20a%20Temporal%20Based%20Texture%20plus%20Feature%20Fusion%20branch%20for%20global%20cross%20slice%20semantics%2C%20and%20an%20Entropy%20Selective%20Attention%20branch%20for%20object%20centric%20details.%20To%20close%20the%20data%20gap%2C%20we%20introduce%20DSEC%20Spike%2C%20the%20first%20driving%20oriented%20simulated%20spike%20detection%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2511.15459v1&entry.124074799=Read"},
{"title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification", "author": "Dante Francisco Wasmuht and Otto Brookes and Maximillian Schall and Pablo Palencia and Chris Beirne and Tilo Burghardt and Majid Mirmehdi and Hjalmar K\u00fchl and Mimi Arandjelovic and Sam Pottie and Peter Bermant and Brandon Asheim and Yi Jin Toh and Adam Elzinga and Jason Holmberg and Andrew Whitworth and Eleanor Flatt and Laura Gustafson and Chaitanya Ryali and Yuan-Ting Hu and Baishan Guo and Andrew Westbury and Kate Saenko and Didac Suris", "abstract": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$.", "link": "http://arxiv.org/abs/2511.15622v1", "date": "2025-11-19", "relevancy": 2.4987, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5027}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4984}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification&body=Title%3A%20The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification%0AAuthor%3A%20Dante%20Francisco%20Wasmuht%20and%20Otto%20Brookes%20and%20Maximillian%20Schall%20and%20Pablo%20Palencia%20and%20Chris%20Beirne%20and%20Tilo%20Burghardt%20and%20Majid%20Mirmehdi%20and%20Hjalmar%20K%C3%BChl%20and%20Mimi%20Arandjelovic%20and%20Sam%20Pottie%20and%20Peter%20Bermant%20and%20Brandon%20Asheim%20and%20Yi%20Jin%20Toh%20and%20Adam%20Elzinga%20and%20Jason%20Holmberg%20and%20Andrew%20Whitworth%20and%20Eleanor%20Flatt%20and%20Laura%20Gustafson%20and%20Chaitanya%20Ryali%20and%20Yuan-Ting%20Hu%20and%20Baishan%20Guo%20and%20Andrew%20Westbury%20and%20Kate%20Saenko%20and%20Didac%20Suris%0AAbstract%3A%20Automated%20video%20analysis%20is%20critical%20for%20wildlife%20conservation.%20A%20foundational%20task%20in%20this%20domain%20is%20multi-animal%20tracking%20%28MAT%29%2C%20which%20underpins%20applications%20such%20as%20individual%20re-identification%20and%20behavior%20recognition.%20However%2C%20existing%20datasets%20are%20limited%20in%20scale%2C%20constrained%20to%20a%20few%20species%2C%20or%20lack%20sufficient%20temporal%20and%20geographical%20diversity%20-%20leaving%20no%20suitable%20benchmark%20for%20training%20general-purpose%20MAT%20models%20applicable%20across%20wild%20animal%20populations.%20To%20address%20this%2C%20we%20introduce%20SA-FARI%2C%20the%20largest%20open-source%20MAT%20dataset%20for%20wild%20animals.%20It%20comprises%2011%2C609%20camera%20trap%20videos%20collected%20over%20approximately%2010%20years%20%282014-2024%29%20from%20741%20locations%20across%204%20continents%2C%20spanning%2099%20species%20categories.%20Each%20video%20is%20exhaustively%20annotated%20culminating%20in%20~46%20hours%20of%20densely%20annotated%20footage%20containing%2016%2C224%20masklet%20identities%20and%20942%2C702%20individual%20bounding%20boxes%2C%20segmentation%20masks%2C%20and%20species%20labels.%20Alongside%20the%20task-specific%20annotations%2C%20we%20publish%20anonymized%20camera%20trap%20locations%20for%20each%20video.%20Finally%2C%20we%20present%20comprehensive%20benchmarks%20on%20SA-FARI%20using%20state-of-the-art%20vision-language%20models%20for%20detection%20and%20tracking%2C%20including%20SAM%203%2C%20evaluated%20with%20both%20species-specific%20and%20generic%20animal%20prompts.%20We%20also%20compare%20against%20vision-only%20methods%20developed%20specifically%20for%20wildlife%20analysis.%20SA-FARI%20is%20the%20first%20large-scale%20dataset%20to%20combine%20high%20species%20diversity%2C%20multi-region%20coverage%2C%20and%20high-quality%20spatio-temporal%20annotations%2C%20offering%20a%20new%20foundation%20for%20advancing%20generalizable%20multianimal%20tracking%20in%20the%20wild.%20The%20dataset%20is%20available%20at%20%24%5Chref%7Bhttps%3A//www.conservationxlabs.com/sa-fari%7D%7B%5Ctext%7Bconservationxlabs.com/SA-FARI%7D%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520SA-FARI%2520Dataset%253A%2520Segment%2520Anything%2520in%2520Footage%2520of%2520Animals%2520for%2520Recognition%2520and%2520Identification%26entry.906535625%3DDante%2520Francisco%2520Wasmuht%2520and%2520Otto%2520Brookes%2520and%2520Maximillian%2520Schall%2520and%2520Pablo%2520Palencia%2520and%2520Chris%2520Beirne%2520and%2520Tilo%2520Burghardt%2520and%2520Majid%2520Mirmehdi%2520and%2520Hjalmar%2520K%25C3%25BChl%2520and%2520Mimi%2520Arandjelovic%2520and%2520Sam%2520Pottie%2520and%2520Peter%2520Bermant%2520and%2520Brandon%2520Asheim%2520and%2520Yi%2520Jin%2520Toh%2520and%2520Adam%2520Elzinga%2520and%2520Jason%2520Holmberg%2520and%2520Andrew%2520Whitworth%2520and%2520Eleanor%2520Flatt%2520and%2520Laura%2520Gustafson%2520and%2520Chaitanya%2520Ryali%2520and%2520Yuan-Ting%2520Hu%2520and%2520Baishan%2520Guo%2520and%2520Andrew%2520Westbury%2520and%2520Kate%2520Saenko%2520and%2520Didac%2520Suris%26entry.1292438233%3DAutomated%2520video%2520analysis%2520is%2520critical%2520for%2520wildlife%2520conservation.%2520A%2520foundational%2520task%2520in%2520this%2520domain%2520is%2520multi-animal%2520tracking%2520%2528MAT%2529%252C%2520which%2520underpins%2520applications%2520such%2520as%2520individual%2520re-identification%2520and%2520behavior%2520recognition.%2520However%252C%2520existing%2520datasets%2520are%2520limited%2520in%2520scale%252C%2520constrained%2520to%2520a%2520few%2520species%252C%2520or%2520lack%2520sufficient%2520temporal%2520and%2520geographical%2520diversity%2520-%2520leaving%2520no%2520suitable%2520benchmark%2520for%2520training%2520general-purpose%2520MAT%2520models%2520applicable%2520across%2520wild%2520animal%2520populations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SA-FARI%252C%2520the%2520largest%2520open-source%2520MAT%2520dataset%2520for%2520wild%2520animals.%2520It%2520comprises%252011%252C609%2520camera%2520trap%2520videos%2520collected%2520over%2520approximately%252010%2520years%2520%25282014-2024%2529%2520from%2520741%2520locations%2520across%25204%2520continents%252C%2520spanning%252099%2520species%2520categories.%2520Each%2520video%2520is%2520exhaustively%2520annotated%2520culminating%2520in%2520~46%2520hours%2520of%2520densely%2520annotated%2520footage%2520containing%252016%252C224%2520masklet%2520identities%2520and%2520942%252C702%2520individual%2520bounding%2520boxes%252C%2520segmentation%2520masks%252C%2520and%2520species%2520labels.%2520Alongside%2520the%2520task-specific%2520annotations%252C%2520we%2520publish%2520anonymized%2520camera%2520trap%2520locations%2520for%2520each%2520video.%2520Finally%252C%2520we%2520present%2520comprehensive%2520benchmarks%2520on%2520SA-FARI%2520using%2520state-of-the-art%2520vision-language%2520models%2520for%2520detection%2520and%2520tracking%252C%2520including%2520SAM%25203%252C%2520evaluated%2520with%2520both%2520species-specific%2520and%2520generic%2520animal%2520prompts.%2520We%2520also%2520compare%2520against%2520vision-only%2520methods%2520developed%2520specifically%2520for%2520wildlife%2520analysis.%2520SA-FARI%2520is%2520the%2520first%2520large-scale%2520dataset%2520to%2520combine%2520high%2520species%2520diversity%252C%2520multi-region%2520coverage%252C%2520and%2520high-quality%2520spatio-temporal%2520annotations%252C%2520offering%2520a%2520new%2520foundation%2520for%2520advancing%2520generalizable%2520multianimal%2520tracking%2520in%2520the%2520wild.%2520The%2520dataset%2520is%2520available%2520at%2520%2524%255Chref%257Bhttps%253A//www.conservationxlabs.com/sa-fari%257D%257B%255Ctext%257Bconservationxlabs.com/SA-FARI%257D%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification&entry.906535625=Dante%20Francisco%20Wasmuht%20and%20Otto%20Brookes%20and%20Maximillian%20Schall%20and%20Pablo%20Palencia%20and%20Chris%20Beirne%20and%20Tilo%20Burghardt%20and%20Majid%20Mirmehdi%20and%20Hjalmar%20K%C3%BChl%20and%20Mimi%20Arandjelovic%20and%20Sam%20Pottie%20and%20Peter%20Bermant%20and%20Brandon%20Asheim%20and%20Yi%20Jin%20Toh%20and%20Adam%20Elzinga%20and%20Jason%20Holmberg%20and%20Andrew%20Whitworth%20and%20Eleanor%20Flatt%20and%20Laura%20Gustafson%20and%20Chaitanya%20Ryali%20and%20Yuan-Ting%20Hu%20and%20Baishan%20Guo%20and%20Andrew%20Westbury%20and%20Kate%20Saenko%20and%20Didac%20Suris&entry.1292438233=Automated%20video%20analysis%20is%20critical%20for%20wildlife%20conservation.%20A%20foundational%20task%20in%20this%20domain%20is%20multi-animal%20tracking%20%28MAT%29%2C%20which%20underpins%20applications%20such%20as%20individual%20re-identification%20and%20behavior%20recognition.%20However%2C%20existing%20datasets%20are%20limited%20in%20scale%2C%20constrained%20to%20a%20few%20species%2C%20or%20lack%20sufficient%20temporal%20and%20geographical%20diversity%20-%20leaving%20no%20suitable%20benchmark%20for%20training%20general-purpose%20MAT%20models%20applicable%20across%20wild%20animal%20populations.%20To%20address%20this%2C%20we%20introduce%20SA-FARI%2C%20the%20largest%20open-source%20MAT%20dataset%20for%20wild%20animals.%20It%20comprises%2011%2C609%20camera%20trap%20videos%20collected%20over%20approximately%2010%20years%20%282014-2024%29%20from%20741%20locations%20across%204%20continents%2C%20spanning%2099%20species%20categories.%20Each%20video%20is%20exhaustively%20annotated%20culminating%20in%20~46%20hours%20of%20densely%20annotated%20footage%20containing%2016%2C224%20masklet%20identities%20and%20942%2C702%20individual%20bounding%20boxes%2C%20segmentation%20masks%2C%20and%20species%20labels.%20Alongside%20the%20task-specific%20annotations%2C%20we%20publish%20anonymized%20camera%20trap%20locations%20for%20each%20video.%20Finally%2C%20we%20present%20comprehensive%20benchmarks%20on%20SA-FARI%20using%20state-of-the-art%20vision-language%20models%20for%20detection%20and%20tracking%2C%20including%20SAM%203%2C%20evaluated%20with%20both%20species-specific%20and%20generic%20animal%20prompts.%20We%20also%20compare%20against%20vision-only%20methods%20developed%20specifically%20for%20wildlife%20analysis.%20SA-FARI%20is%20the%20first%20large-scale%20dataset%20to%20combine%20high%20species%20diversity%2C%20multi-region%20coverage%2C%20and%20high-quality%20spatio-temporal%20annotations%2C%20offering%20a%20new%20foundation%20for%20advancing%20generalizable%20multianimal%20tracking%20in%20the%20wild.%20The%20dataset%20is%20available%20at%20%24%5Chref%7Bhttps%3A//www.conservationxlabs.com/sa-fari%7D%7B%5Ctext%7Bconservationxlabs.com/SA-FARI%7D%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.15622v1&entry.124074799=Read"},
{"title": "Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning", "author": "Kyle Domico and Jean-Charles Noirot Ferrand and Ryan Sheatsley and Eric Pauley and Josiah Hanna and Patrick McDaniel", "abstract": "Attacks on machine learning models have been extensively studied through stateless optimization. In this paper, we demonstrate how a reinforcement learning (RL) agent can learn a new class of attack algorithms that generate adversarial samples. Unlike traditional adversarial machine learning (AML) methods that craft adversarial samples independently, our RL-based approach retains and exploits past attack experience to improve the effectiveness and efficiency of future attacks. We formulate adversarial sample generation as a Markov Decision Process and evaluate RL's ability to (a) learn effective and efficient attack strategies and (b) compete with state-of-the-art AML. On two image classification benchmarks, our agent increases attack success rate by up to 13.2% and decreases the average number of victim model queries per attack by up to 16.9% from the start to the end of training. In a head-to-head comparison with state-of-the-art image attacks, our approach enables an adversary to generate adversarial samples with 17% more success on unseen inputs post-training. From a security perspective, this work demonstrates a powerful new attack vector that uses RL to train agents that attack ML models efficiently and at scale.", "link": "http://arxiv.org/abs/2503.01734v2", "date": "2025-11-19", "relevancy": 2.4936, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5372}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4904}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Agents%3A%20Black-Box%20Evasion%20Attacks%20with%20Reinforcement%20Learning&body=Title%3A%20Adversarial%20Agents%3A%20Black-Box%20Evasion%20Attacks%20with%20Reinforcement%20Learning%0AAuthor%3A%20Kyle%20Domico%20and%20Jean-Charles%20Noirot%20Ferrand%20and%20Ryan%20Sheatsley%20and%20Eric%20Pauley%20and%20Josiah%20Hanna%20and%20Patrick%20McDaniel%0AAbstract%3A%20Attacks%20on%20machine%20learning%20models%20have%20been%20extensively%20studied%20through%20stateless%20optimization.%20In%20this%20paper%2C%20we%20demonstrate%20how%20a%20reinforcement%20learning%20%28RL%29%20agent%20can%20learn%20a%20new%20class%20of%20attack%20algorithms%20that%20generate%20adversarial%20samples.%20Unlike%20traditional%20adversarial%20machine%20learning%20%28AML%29%20methods%20that%20craft%20adversarial%20samples%20independently%2C%20our%20RL-based%20approach%20retains%20and%20exploits%20past%20attack%20experience%20to%20improve%20the%20effectiveness%20and%20efficiency%20of%20future%20attacks.%20We%20formulate%20adversarial%20sample%20generation%20as%20a%20Markov%20Decision%20Process%20and%20evaluate%20RL%27s%20ability%20to%20%28a%29%20learn%20effective%20and%20efficient%20attack%20strategies%20and%20%28b%29%20compete%20with%20state-of-the-art%20AML.%20On%20two%20image%20classification%20benchmarks%2C%20our%20agent%20increases%20attack%20success%20rate%20by%20up%20to%2013.2%25%20and%20decreases%20the%20average%20number%20of%20victim%20model%20queries%20per%20attack%20by%20up%20to%2016.9%25%20from%20the%20start%20to%20the%20end%20of%20training.%20In%20a%20head-to-head%20comparison%20with%20state-of-the-art%20image%20attacks%2C%20our%20approach%20enables%20an%20adversary%20to%20generate%20adversarial%20samples%20with%2017%25%20more%20success%20on%20unseen%20inputs%20post-training.%20From%20a%20security%20perspective%2C%20this%20work%20demonstrates%20a%20powerful%20new%20attack%20vector%20that%20uses%20RL%20to%20train%20agents%20that%20attack%20ML%20models%20efficiently%20and%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2503.01734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Agents%253A%2520Black-Box%2520Evasion%2520Attacks%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DKyle%2520Domico%2520and%2520Jean-Charles%2520Noirot%2520Ferrand%2520and%2520Ryan%2520Sheatsley%2520and%2520Eric%2520Pauley%2520and%2520Josiah%2520Hanna%2520and%2520Patrick%2520McDaniel%26entry.1292438233%3DAttacks%2520on%2520machine%2520learning%2520models%2520have%2520been%2520extensively%2520studied%2520through%2520stateless%2520optimization.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520how%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520can%2520learn%2520a%2520new%2520class%2520of%2520attack%2520algorithms%2520that%2520generate%2520adversarial%2520samples.%2520Unlike%2520traditional%2520adversarial%2520machine%2520learning%2520%2528AML%2529%2520methods%2520that%2520craft%2520adversarial%2520samples%2520independently%252C%2520our%2520RL-based%2520approach%2520retains%2520and%2520exploits%2520past%2520attack%2520experience%2520to%2520improve%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520future%2520attacks.%2520We%2520formulate%2520adversarial%2520sample%2520generation%2520as%2520a%2520Markov%2520Decision%2520Process%2520and%2520evaluate%2520RL%2527s%2520ability%2520to%2520%2528a%2529%2520learn%2520effective%2520and%2520efficient%2520attack%2520strategies%2520and%2520%2528b%2529%2520compete%2520with%2520state-of-the-art%2520AML.%2520On%2520two%2520image%2520classification%2520benchmarks%252C%2520our%2520agent%2520increases%2520attack%2520success%2520rate%2520by%2520up%2520to%252013.2%2525%2520and%2520decreases%2520the%2520average%2520number%2520of%2520victim%2520model%2520queries%2520per%2520attack%2520by%2520up%2520to%252016.9%2525%2520from%2520the%2520start%2520to%2520the%2520end%2520of%2520training.%2520In%2520a%2520head-to-head%2520comparison%2520with%2520state-of-the-art%2520image%2520attacks%252C%2520our%2520approach%2520enables%2520an%2520adversary%2520to%2520generate%2520adversarial%2520samples%2520with%252017%2525%2520more%2520success%2520on%2520unseen%2520inputs%2520post-training.%2520From%2520a%2520security%2520perspective%252C%2520this%2520work%2520demonstrates%2520a%2520powerful%2520new%2520attack%2520vector%2520that%2520uses%2520RL%2520to%2520train%2520agents%2520that%2520attack%2520ML%2520models%2520efficiently%2520and%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Agents%3A%20Black-Box%20Evasion%20Attacks%20with%20Reinforcement%20Learning&entry.906535625=Kyle%20Domico%20and%20Jean-Charles%20Noirot%20Ferrand%20and%20Ryan%20Sheatsley%20and%20Eric%20Pauley%20and%20Josiah%20Hanna%20and%20Patrick%20McDaniel&entry.1292438233=Attacks%20on%20machine%20learning%20models%20have%20been%20extensively%20studied%20through%20stateless%20optimization.%20In%20this%20paper%2C%20we%20demonstrate%20how%20a%20reinforcement%20learning%20%28RL%29%20agent%20can%20learn%20a%20new%20class%20of%20attack%20algorithms%20that%20generate%20adversarial%20samples.%20Unlike%20traditional%20adversarial%20machine%20learning%20%28AML%29%20methods%20that%20craft%20adversarial%20samples%20independently%2C%20our%20RL-based%20approach%20retains%20and%20exploits%20past%20attack%20experience%20to%20improve%20the%20effectiveness%20and%20efficiency%20of%20future%20attacks.%20We%20formulate%20adversarial%20sample%20generation%20as%20a%20Markov%20Decision%20Process%20and%20evaluate%20RL%27s%20ability%20to%20%28a%29%20learn%20effective%20and%20efficient%20attack%20strategies%20and%20%28b%29%20compete%20with%20state-of-the-art%20AML.%20On%20two%20image%20classification%20benchmarks%2C%20our%20agent%20increases%20attack%20success%20rate%20by%20up%20to%2013.2%25%20and%20decreases%20the%20average%20number%20of%20victim%20model%20queries%20per%20attack%20by%20up%20to%2016.9%25%20from%20the%20start%20to%20the%20end%20of%20training.%20In%20a%20head-to-head%20comparison%20with%20state-of-the-art%20image%20attacks%2C%20our%20approach%20enables%20an%20adversary%20to%20generate%20adversarial%20samples%20with%2017%25%20more%20success%20on%20unseen%20inputs%20post-training.%20From%20a%20security%20perspective%2C%20this%20work%20demonstrates%20a%20powerful%20new%20attack%20vector%20that%20uses%20RL%20to%20train%20agents%20that%20attack%20ML%20models%20efficiently%20and%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2503.01734v2&entry.124074799=Read"},
{"title": "Towards Understanding Layer Contributions in Tabular In-Context Learning Models", "author": "Amir Rezaei Balef and Mykhailo Koshil and Katharina Eggensperger", "abstract": "Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the \"layers as painters\" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.", "link": "http://arxiv.org/abs/2511.15432v1", "date": "2025-11-19", "relevancy": 2.474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Layer%20Contributions%20in%20Tabular%20In-Context%20Learning%20Models&body=Title%3A%20Towards%20Understanding%20Layer%20Contributions%20in%20Tabular%20In-Context%20Learning%20Models%0AAuthor%3A%20Amir%20Rezaei%20Balef%20and%20Mykhailo%20Koshil%20and%20Katharina%20Eggensperger%0AAbstract%3A%20Despite%20the%20architectural%20similarities%20between%20tabular%20in-context%20learning%20%28ICL%29%20models%20and%20large%20language%20models%20%28LLMs%29%2C%20little%20is%20known%20about%20how%20individual%20layers%20contribute%20to%20tabular%20prediction.%20In%20this%20paper%2C%20we%20investigate%20how%20the%20latent%20spaces%20evolve%20across%20layers%20in%20tabular%20ICL%20models%2C%20identify%20potential%20redundant%20layers%2C%20and%20compare%20these%20dynamics%20with%20those%20observed%20in%20LLMs.%20We%20analyze%20TabPFN%20and%20TabICL%20through%20the%20%22layers%20as%20painters%22%20perspective%2C%20finding%20that%20only%20subsets%20of%20layers%20share%20a%20common%20representational%20language%2C%20suggesting%20structural%20redundancy%20and%20offering%20opportunities%20for%20model%20compression%20and%20improved%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Layer%2520Contributions%2520in%2520Tabular%2520In-Context%2520Learning%2520Models%26entry.906535625%3DAmir%2520Rezaei%2520Balef%2520and%2520Mykhailo%2520Koshil%2520and%2520Katharina%2520Eggensperger%26entry.1292438233%3DDespite%2520the%2520architectural%2520similarities%2520between%2520tabular%2520in-context%2520learning%2520%2528ICL%2529%2520models%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520little%2520is%2520known%2520about%2520how%2520individual%2520layers%2520contribute%2520to%2520tabular%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520the%2520latent%2520spaces%2520evolve%2520across%2520layers%2520in%2520tabular%2520ICL%2520models%252C%2520identify%2520potential%2520redundant%2520layers%252C%2520and%2520compare%2520these%2520dynamics%2520with%2520those%2520observed%2520in%2520LLMs.%2520We%2520analyze%2520TabPFN%2520and%2520TabICL%2520through%2520the%2520%2522layers%2520as%2520painters%2522%2520perspective%252C%2520finding%2520that%2520only%2520subsets%2520of%2520layers%2520share%2520a%2520common%2520representational%2520language%252C%2520suggesting%2520structural%2520redundancy%2520and%2520offering%2520opportunities%2520for%2520model%2520compression%2520and%2520improved%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Layer%20Contributions%20in%20Tabular%20In-Context%20Learning%20Models&entry.906535625=Amir%20Rezaei%20Balef%20and%20Mykhailo%20Koshil%20and%20Katharina%20Eggensperger&entry.1292438233=Despite%20the%20architectural%20similarities%20between%20tabular%20in-context%20learning%20%28ICL%29%20models%20and%20large%20language%20models%20%28LLMs%29%2C%20little%20is%20known%20about%20how%20individual%20layers%20contribute%20to%20tabular%20prediction.%20In%20this%20paper%2C%20we%20investigate%20how%20the%20latent%20spaces%20evolve%20across%20layers%20in%20tabular%20ICL%20models%2C%20identify%20potential%20redundant%20layers%2C%20and%20compare%20these%20dynamics%20with%20those%20observed%20in%20LLMs.%20We%20analyze%20TabPFN%20and%20TabICL%20through%20the%20%22layers%20as%20painters%22%20perspective%2C%20finding%20that%20only%20subsets%20of%20layers%20share%20a%20common%20representational%20language%2C%20suggesting%20structural%20redundancy%20and%20offering%20opportunities%20for%20model%20compression%20and%20improved%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2511.15432v1&entry.124074799=Read"},
{"title": "Coresets from Trajectories: Selecting Data via Correlation of Loss Differences", "author": "Manish Nagaraj and Deepak Ravikumar and Kaushik Roy", "abstract": "Deep learning models achieve state-of-the-art performance across domains but face scalability challenges in real-time or resource-constrained scenarios. To address this, we propose Correlation of Loss Differences (CLD), a simple and scalable metric for coreset selection that identifies the most impactful training samples by measuring their alignment with the loss trajectories of a held-out validation set. CLD is highly efficient, requiring only per-sample loss values computed at training checkpoints, and avoiding the costly gradient and curvature computations used in many existing subset selection methods. We develop a general theoretical framework that establishes convergence guarantees for CLD-based coresets, demonstrating that the convergence error is upper-bounded by the alignment of the selected samples and the representativeness of the validation set. On CIFAR-100 and ImageNet-1k, CLD-based coresets typically outperform or closely match state-of-the-art methods across subset sizes, and remain within 1% of more computationally expensive baselines even when not leading. CLD transfers effectively across architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with <1% degradation. Moreover, CLD is stable when using only early checkpoints, incurring negligible accuracy loss. Finally, CLD exhibits inherent bias reduction via per-class validation alignment, obviating the need for additional stratified sampling. Together, these properties make CLD a principled, efficient, stable, and transferable tool for scalable dataset optimization.", "link": "http://arxiv.org/abs/2508.20230v2", "date": "2025-11-19", "relevancy": 2.4421, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coresets%20from%20Trajectories%3A%20Selecting%20Data%20via%20Correlation%20of%20Loss%20Differences&body=Title%3A%20Coresets%20from%20Trajectories%3A%20Selecting%20Data%20via%20Correlation%20of%20Loss%20Differences%0AAuthor%3A%20Manish%20Nagaraj%20and%20Deepak%20Ravikumar%20and%20Kaushik%20Roy%0AAbstract%3A%20Deep%20learning%20models%20achieve%20state-of-the-art%20performance%20across%20domains%20but%20face%20scalability%20challenges%20in%20real-time%20or%20resource-constrained%20scenarios.%20To%20address%20this%2C%20we%20propose%20Correlation%20of%20Loss%20Differences%20%28CLD%29%2C%20a%20simple%20and%20scalable%20metric%20for%20coreset%20selection%20that%20identifies%20the%20most%20impactful%20training%20samples%20by%20measuring%20their%20alignment%20with%20the%20loss%20trajectories%20of%20a%20held-out%20validation%20set.%20CLD%20is%20highly%20efficient%2C%20requiring%20only%20per-sample%20loss%20values%20computed%20at%20training%20checkpoints%2C%20and%20avoiding%20the%20costly%20gradient%20and%20curvature%20computations%20used%20in%20many%20existing%20subset%20selection%20methods.%20We%20develop%20a%20general%20theoretical%20framework%20that%20establishes%20convergence%20guarantees%20for%20CLD-based%20coresets%2C%20demonstrating%20that%20the%20convergence%20error%20is%20upper-bounded%20by%20the%20alignment%20of%20the%20selected%20samples%20and%20the%20representativeness%20of%20the%20validation%20set.%20On%20CIFAR-100%20and%20ImageNet-1k%2C%20CLD-based%20coresets%20typically%20outperform%20or%20closely%20match%20state-of-the-art%20methods%20across%20subset%20sizes%2C%20and%20remain%20within%201%25%20of%20more%20computationally%20expensive%20baselines%20even%20when%20not%20leading.%20CLD%20transfers%20effectively%20across%20architectures%20%28ResNet%2C%20VGG%2C%20DenseNet%29%2C%20enabling%20proxy-to-target%20selection%20with%20%3C1%25%20degradation.%20Moreover%2C%20CLD%20is%20stable%20when%20using%20only%20early%20checkpoints%2C%20incurring%20negligible%20accuracy%20loss.%20Finally%2C%20CLD%20exhibits%20inherent%20bias%20reduction%20via%20per-class%20validation%20alignment%2C%20obviating%20the%20need%20for%20additional%20stratified%20sampling.%20Together%2C%20these%20properties%20make%20CLD%20a%20principled%2C%20efficient%2C%20stable%2C%20and%20transferable%20tool%20for%20scalable%20dataset%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoresets%2520from%2520Trajectories%253A%2520Selecting%2520Data%2520via%2520Correlation%2520of%2520Loss%2520Differences%26entry.906535625%3DManish%2520Nagaraj%2520and%2520Deepak%2520Ravikumar%2520and%2520Kaushik%2520Roy%26entry.1292438233%3DDeep%2520learning%2520models%2520achieve%2520state-of-the-art%2520performance%2520across%2520domains%2520but%2520face%2520scalability%2520challenges%2520in%2520real-time%2520or%2520resource-constrained%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520Correlation%2520of%2520Loss%2520Differences%2520%2528CLD%2529%252C%2520a%2520simple%2520and%2520scalable%2520metric%2520for%2520coreset%2520selection%2520that%2520identifies%2520the%2520most%2520impactful%2520training%2520samples%2520by%2520measuring%2520their%2520alignment%2520with%2520the%2520loss%2520trajectories%2520of%2520a%2520held-out%2520validation%2520set.%2520CLD%2520is%2520highly%2520efficient%252C%2520requiring%2520only%2520per-sample%2520loss%2520values%2520computed%2520at%2520training%2520checkpoints%252C%2520and%2520avoiding%2520the%2520costly%2520gradient%2520and%2520curvature%2520computations%2520used%2520in%2520many%2520existing%2520subset%2520selection%2520methods.%2520We%2520develop%2520a%2520general%2520theoretical%2520framework%2520that%2520establishes%2520convergence%2520guarantees%2520for%2520CLD-based%2520coresets%252C%2520demonstrating%2520that%2520the%2520convergence%2520error%2520is%2520upper-bounded%2520by%2520the%2520alignment%2520of%2520the%2520selected%2520samples%2520and%2520the%2520representativeness%2520of%2520the%2520validation%2520set.%2520On%2520CIFAR-100%2520and%2520ImageNet-1k%252C%2520CLD-based%2520coresets%2520typically%2520outperform%2520or%2520closely%2520match%2520state-of-the-art%2520methods%2520across%2520subset%2520sizes%252C%2520and%2520remain%2520within%25201%2525%2520of%2520more%2520computationally%2520expensive%2520baselines%2520even%2520when%2520not%2520leading.%2520CLD%2520transfers%2520effectively%2520across%2520architectures%2520%2528ResNet%252C%2520VGG%252C%2520DenseNet%2529%252C%2520enabling%2520proxy-to-target%2520selection%2520with%2520%253C1%2525%2520degradation.%2520Moreover%252C%2520CLD%2520is%2520stable%2520when%2520using%2520only%2520early%2520checkpoints%252C%2520incurring%2520negligible%2520accuracy%2520loss.%2520Finally%252C%2520CLD%2520exhibits%2520inherent%2520bias%2520reduction%2520via%2520per-class%2520validation%2520alignment%252C%2520obviating%2520the%2520need%2520for%2520additional%2520stratified%2520sampling.%2520Together%252C%2520these%2520properties%2520make%2520CLD%2520a%2520principled%252C%2520efficient%252C%2520stable%252C%2520and%2520transferable%2520tool%2520for%2520scalable%2520dataset%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coresets%20from%20Trajectories%3A%20Selecting%20Data%20via%20Correlation%20of%20Loss%20Differences&entry.906535625=Manish%20Nagaraj%20and%20Deepak%20Ravikumar%20and%20Kaushik%20Roy&entry.1292438233=Deep%20learning%20models%20achieve%20state-of-the-art%20performance%20across%20domains%20but%20face%20scalability%20challenges%20in%20real-time%20or%20resource-constrained%20scenarios.%20To%20address%20this%2C%20we%20propose%20Correlation%20of%20Loss%20Differences%20%28CLD%29%2C%20a%20simple%20and%20scalable%20metric%20for%20coreset%20selection%20that%20identifies%20the%20most%20impactful%20training%20samples%20by%20measuring%20their%20alignment%20with%20the%20loss%20trajectories%20of%20a%20held-out%20validation%20set.%20CLD%20is%20highly%20efficient%2C%20requiring%20only%20per-sample%20loss%20values%20computed%20at%20training%20checkpoints%2C%20and%20avoiding%20the%20costly%20gradient%20and%20curvature%20computations%20used%20in%20many%20existing%20subset%20selection%20methods.%20We%20develop%20a%20general%20theoretical%20framework%20that%20establishes%20convergence%20guarantees%20for%20CLD-based%20coresets%2C%20demonstrating%20that%20the%20convergence%20error%20is%20upper-bounded%20by%20the%20alignment%20of%20the%20selected%20samples%20and%20the%20representativeness%20of%20the%20validation%20set.%20On%20CIFAR-100%20and%20ImageNet-1k%2C%20CLD-based%20coresets%20typically%20outperform%20or%20closely%20match%20state-of-the-art%20methods%20across%20subset%20sizes%2C%20and%20remain%20within%201%25%20of%20more%20computationally%20expensive%20baselines%20even%20when%20not%20leading.%20CLD%20transfers%20effectively%20across%20architectures%20%28ResNet%2C%20VGG%2C%20DenseNet%29%2C%20enabling%20proxy-to-target%20selection%20with%20%3C1%25%20degradation.%20Moreover%2C%20CLD%20is%20stable%20when%20using%20only%20early%20checkpoints%2C%20incurring%20negligible%20accuracy%20loss.%20Finally%2C%20CLD%20exhibits%20inherent%20bias%20reduction%20via%20per-class%20validation%20alignment%2C%20obviating%20the%20need%20for%20additional%20stratified%20sampling.%20Together%2C%20these%20properties%20make%20CLD%20a%20principled%2C%20efficient%2C%20stable%2C%20and%20transferable%20tool%20for%20scalable%20dataset%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2508.20230v2&entry.124074799=Read"},
{"title": "KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials", "author": "Huseyin Goksu", "abstract": "Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on \"heterophilic\" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \\textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.", "link": "http://arxiv.org/abs/2511.15327v1", "date": "2025-11-19", "relevancy": 2.441, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5055}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5024}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KrawtchoukNet%3A%20A%20Unified%20GNN%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Bounded%20Polynomials&body=Title%3A%20KrawtchoukNet%3A%20A%20Unified%20GNN%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Bounded%20Polynomials%0AAuthor%3A%20Huseyin%20Goksu%0AAbstract%3A%20Spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20based%20on%20polynomial%20filters%2C%20such%20as%20ChebyNet%2C%20suffer%20from%20two%20critical%20limitations%3A%201%29%20performance%20collapse%20on%20%22heterophilic%22%20graphs%20and%202%29%20performance%20collapse%20at%20high%20polynomial%20degrees%20%28K%29%2C%20known%20as%20over-smoothing.%20Both%20issues%20stem%20from%20the%20static%2C%20low-pass%20nature%20of%20standard%20filters.%20In%20this%20work%2C%20we%20propose%20%60KrawtchoukNet%60%2C%20a%20GNN%20filter%20based%20on%20the%20discrete%20Krawtchouk%20polynomials.%20We%20demonstrate%20that%20%60KrawtchoukNet%60%20provides%20a%20unified%20solution%20to%20both%20problems%20through%20two%20key%20design%20choices.%20First%2C%20by%20fixing%20the%20polynomial%27s%20domain%20N%20to%20a%20small%20constant%20%28e.g.%2C%20N%3D20%29%2C%20we%20create%20the%20first%20GNN%20filter%20whose%20recurrence%20coefficients%20are%20%5Ctextit%7Binherently%20bounded%7D%2C%20making%20it%20exceptionally%20robust%20to%20over-smoothing%20%28achieving%20SOTA%20results%20at%20K%3D10%29.%20Second%2C%20by%20making%20the%20filter%27s%20shape%20parameter%20p%20learnable%2C%20the%20filter%20adapts%20its%20spectral%20response%20to%20the%20graph%20data.%20We%20show%20this%20adaptive%20nature%20allows%20%60KrawtchoukNet%60%20to%20achieve%20SOTA%20performance%20on%20challenging%20heterophilic%20benchmarks%20%28Texas%2C%20Cornell%29%2C%20decisively%20outperforming%20standard%20GNNs%20like%20GAT%20and%20APPNP.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKrawtchoukNet%253A%2520A%2520Unified%2520GNN%2520Solution%2520for%2520Heterophily%2520and%2520Over-smoothing%2520with%2520Adaptive%2520Bounded%2520Polynomials%26entry.906535625%3DHuseyin%2520Goksu%26entry.1292438233%3DSpectral%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520based%2520on%2520polynomial%2520filters%252C%2520such%2520as%2520ChebyNet%252C%2520suffer%2520from%2520two%2520critical%2520limitations%253A%25201%2529%2520performance%2520collapse%2520on%2520%2522heterophilic%2522%2520graphs%2520and%25202%2529%2520performance%2520collapse%2520at%2520high%2520polynomial%2520degrees%2520%2528K%2529%252C%2520known%2520as%2520over-smoothing.%2520Both%2520issues%2520stem%2520from%2520the%2520static%252C%2520low-pass%2520nature%2520of%2520standard%2520filters.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2560KrawtchoukNet%2560%252C%2520a%2520GNN%2520filter%2520based%2520on%2520the%2520discrete%2520Krawtchouk%2520polynomials.%2520We%2520demonstrate%2520that%2520%2560KrawtchoukNet%2560%2520provides%2520a%2520unified%2520solution%2520to%2520both%2520problems%2520through%2520two%2520key%2520design%2520choices.%2520First%252C%2520by%2520fixing%2520the%2520polynomial%2527s%2520domain%2520N%2520to%2520a%2520small%2520constant%2520%2528e.g.%252C%2520N%253D20%2529%252C%2520we%2520create%2520the%2520first%2520GNN%2520filter%2520whose%2520recurrence%2520coefficients%2520are%2520%255Ctextit%257Binherently%2520bounded%257D%252C%2520making%2520it%2520exceptionally%2520robust%2520to%2520over-smoothing%2520%2528achieving%2520SOTA%2520results%2520at%2520K%253D10%2529.%2520Second%252C%2520by%2520making%2520the%2520filter%2527s%2520shape%2520parameter%2520p%2520learnable%252C%2520the%2520filter%2520adapts%2520its%2520spectral%2520response%2520to%2520the%2520graph%2520data.%2520We%2520show%2520this%2520adaptive%2520nature%2520allows%2520%2560KrawtchoukNet%2560%2520to%2520achieve%2520SOTA%2520performance%2520on%2520challenging%2520heterophilic%2520benchmarks%2520%2528Texas%252C%2520Cornell%2529%252C%2520decisively%2520outperforming%2520standard%2520GNNs%2520like%2520GAT%2520and%2520APPNP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KrawtchoukNet%3A%20A%20Unified%20GNN%20Solution%20for%20Heterophily%20and%20Over-smoothing%20with%20Adaptive%20Bounded%20Polynomials&entry.906535625=Huseyin%20Goksu&entry.1292438233=Spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20based%20on%20polynomial%20filters%2C%20such%20as%20ChebyNet%2C%20suffer%20from%20two%20critical%20limitations%3A%201%29%20performance%20collapse%20on%20%22heterophilic%22%20graphs%20and%202%29%20performance%20collapse%20at%20high%20polynomial%20degrees%20%28K%29%2C%20known%20as%20over-smoothing.%20Both%20issues%20stem%20from%20the%20static%2C%20low-pass%20nature%20of%20standard%20filters.%20In%20this%20work%2C%20we%20propose%20%60KrawtchoukNet%60%2C%20a%20GNN%20filter%20based%20on%20the%20discrete%20Krawtchouk%20polynomials.%20We%20demonstrate%20that%20%60KrawtchoukNet%60%20provides%20a%20unified%20solution%20to%20both%20problems%20through%20two%20key%20design%20choices.%20First%2C%20by%20fixing%20the%20polynomial%27s%20domain%20N%20to%20a%20small%20constant%20%28e.g.%2C%20N%3D20%29%2C%20we%20create%20the%20first%20GNN%20filter%20whose%20recurrence%20coefficients%20are%20%5Ctextit%7Binherently%20bounded%7D%2C%20making%20it%20exceptionally%20robust%20to%20over-smoothing%20%28achieving%20SOTA%20results%20at%20K%3D10%29.%20Second%2C%20by%20making%20the%20filter%27s%20shape%20parameter%20p%20learnable%2C%20the%20filter%20adapts%20its%20spectral%20response%20to%20the%20graph%20data.%20We%20show%20this%20adaptive%20nature%20allows%20%60KrawtchoukNet%60%20to%20achieve%20SOTA%20performance%20on%20challenging%20heterophilic%20benchmarks%20%28Texas%2C%20Cornell%29%2C%20decisively%20outperforming%20standard%20GNNs%20like%20GAT%20and%20APPNP.&entry.1838667208=http%3A//arxiv.org/abs/2511.15327v1&entry.124074799=Read"},
{"title": "A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture", "author": "Pandiyaraju V and Abishek Karthik and Sreya Mynampati and Poovarasan L and D. Saraswathi", "abstract": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.", "link": "http://arxiv.org/abs/2511.15535v1", "date": "2025-11-19", "relevancy": 2.4191, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4938}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20CNN-ViT-GNN%20Framework%20with%20GAN-Based%20Augmentation%20for%20Intelligent%20Weed%20Detection%20in%20Precision%20Agriculture&body=Title%3A%20A%20Hybrid%20CNN-ViT-GNN%20Framework%20with%20GAN-Based%20Augmentation%20for%20Intelligent%20Weed%20Detection%20in%20Precision%20Agriculture%0AAuthor%3A%20Pandiyaraju%20V%20and%20Abishek%20Karthik%20and%20Sreya%20Mynampati%20and%20Poovarasan%20L%20and%20D.%20Saraswathi%0AAbstract%3A%20The%20task%20of%20weed%20detection%20is%20an%20essential%20element%20of%20precision%20agriculture%20since%20accurate%20species%20identification%20allows%20a%20farmer%20to%20selectively%20apply%20herbicides%20and%20fits%20into%20sustainable%20agriculture%20crop%20management.%20This%20paper%20proposes%20a%20hybrid%20deep%20learning%20framework%20recipe%20for%20weed%20detection%20that%20utilizes%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20Vision%20Transformers%20%28ViTs%29%2C%20and%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20build%20robustness%20to%20multiple%20field%20conditions.%20A%20Generative%20Adversarial%20Network%20%28GAN%29-based%20augmentation%20method%20was%20imposed%20to%20balance%20class%20distributions%20and%20better%20generalize%20the%20model.%20Further%2C%20a%20self-supervised%20contrastive%20pre-training%20method%20helps%20to%20learn%20more%20features%20from%20limited%20annotated%20data.%20Experimental%20results%20yield%20superior%20results%20with%2099.33%25%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score%20on%20multi-benchmark%20datasets.%20The%20proposed%20model%20architecture%20enables%20local%2C%20global%2C%20and%20relational%20feature%20representations%20and%20offers%20high%20interpretability%20and%20adaptability.%20Practically%2C%20the%20framework%20allows%20real-time%2C%20efficient%20deployment%20to%20edge%20devices%20for%20automated%20weed%20detecting%2C%20reducing%20over-reliance%20on%20herbicides%20and%20providing%20scalable%2C%20sustainable%20precision-farming%20options.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520CNN-ViT-GNN%2520Framework%2520with%2520GAN-Based%2520Augmentation%2520for%2520Intelligent%2520Weed%2520Detection%2520in%2520Precision%2520Agriculture%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Abishek%2520Karthik%2520and%2520Sreya%2520Mynampati%2520and%2520Poovarasan%2520L%2520and%2520D.%2520Saraswathi%26entry.1292438233%3DThe%2520task%2520of%2520weed%2520detection%2520is%2520an%2520essential%2520element%2520of%2520precision%2520agriculture%2520since%2520accurate%2520species%2520identification%2520allows%2520a%2520farmer%2520to%2520selectively%2520apply%2520herbicides%2520and%2520fits%2520into%2520sustainable%2520agriculture%2520crop%2520management.%2520This%2520paper%2520proposes%2520a%2520hybrid%2520deep%2520learning%2520framework%2520recipe%2520for%2520weed%2520detection%2520that%2520utilizes%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520and%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520build%2520robustness%2520to%2520multiple%2520field%2520conditions.%2520A%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529-based%2520augmentation%2520method%2520was%2520imposed%2520to%2520balance%2520class%2520distributions%2520and%2520better%2520generalize%2520the%2520model.%2520Further%252C%2520a%2520self-supervised%2520contrastive%2520pre-training%2520method%2520helps%2520to%2520learn%2520more%2520features%2520from%2520limited%2520annotated%2520data.%2520Experimental%2520results%2520yield%2520superior%2520results%2520with%252099.33%2525%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%2520F1-score%2520on%2520multi-benchmark%2520datasets.%2520The%2520proposed%2520model%2520architecture%2520enables%2520local%252C%2520global%252C%2520and%2520relational%2520feature%2520representations%2520and%2520offers%2520high%2520interpretability%2520and%2520adaptability.%2520Practically%252C%2520the%2520framework%2520allows%2520real-time%252C%2520efficient%2520deployment%2520to%2520edge%2520devices%2520for%2520automated%2520weed%2520detecting%252C%2520reducing%2520over-reliance%2520on%2520herbicides%2520and%2520providing%2520scalable%252C%2520sustainable%2520precision-farming%2520options.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20CNN-ViT-GNN%20Framework%20with%20GAN-Based%20Augmentation%20for%20Intelligent%20Weed%20Detection%20in%20Precision%20Agriculture&entry.906535625=Pandiyaraju%20V%20and%20Abishek%20Karthik%20and%20Sreya%20Mynampati%20and%20Poovarasan%20L%20and%20D.%20Saraswathi&entry.1292438233=The%20task%20of%20weed%20detection%20is%20an%20essential%20element%20of%20precision%20agriculture%20since%20accurate%20species%20identification%20allows%20a%20farmer%20to%20selectively%20apply%20herbicides%20and%20fits%20into%20sustainable%20agriculture%20crop%20management.%20This%20paper%20proposes%20a%20hybrid%20deep%20learning%20framework%20recipe%20for%20weed%20detection%20that%20utilizes%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20Vision%20Transformers%20%28ViTs%29%2C%20and%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20build%20robustness%20to%20multiple%20field%20conditions.%20A%20Generative%20Adversarial%20Network%20%28GAN%29-based%20augmentation%20method%20was%20imposed%20to%20balance%20class%20distributions%20and%20better%20generalize%20the%20model.%20Further%2C%20a%20self-supervised%20contrastive%20pre-training%20method%20helps%20to%20learn%20more%20features%20from%20limited%20annotated%20data.%20Experimental%20results%20yield%20superior%20results%20with%2099.33%25%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score%20on%20multi-benchmark%20datasets.%20The%20proposed%20model%20architecture%20enables%20local%2C%20global%2C%20and%20relational%20feature%20representations%20and%20offers%20high%20interpretability%20and%20adaptability.%20Practically%2C%20the%20framework%20allows%20real-time%2C%20efficient%20deployment%20to%20edge%20devices%20for%20automated%20weed%20detecting%2C%20reducing%20over-reliance%20on%20herbicides%20and%20providing%20scalable%2C%20sustainable%20precision-farming%20options.&entry.1838667208=http%3A//arxiv.org/abs/2511.15535v1&entry.124074799=Read"},
{"title": "ViewBridge:Revisiting Cross-View Localization from Image Matching", "author": "Panwang Xia and Qiong Wu and Lei Yu and Yi Liu and Mingtao Xiong and Xudong Lu and Yi Liu and Haoyu Guo and Yongxiang Yao and Junjian Zhang and Xiangyuan Cai and Hongwei Hu and Zhi Zheng and Yongjun Zhang and Yi Wan", "abstract": "Cross-view localization aims to estimate the 3-DoF pose of a ground-view image by aligning it with aerial or satellite imagery. Existing methods typically address this task through direct regression or feature alignment in a shared bird's-eye view (BEV) space. Although effective for coarse alignment, these methods fail to establish fine-grained and geometrically reliable correspondences under large viewpoint variations, thereby limiting both the accuracy and interpretability of localization results. Consequently, we revisit cross-view localization from the perspective of image matching and propose a unified framework that enhances both matching and localization. Specifically, we introduce a Surface Model that constrains BEV feature projection to physically valid regions for geometric consistency, and a SimRefiner that adaptively refines similarity distributions to enhance match reliability. To further support research in this area, we present CVFM, the first benchmark with 32,509 cross-view image pairs annotated with pixel-level correspondences. Extensive experiments demonstrate that our approach achieves geometry-consistent and fine-grained correspondences across extreme viewpoints and further improves the accuracy and stability of cross-view localization.", "link": "http://arxiv.org/abs/2508.10716v2", "date": "2025-11-19", "relevancy": 2.406, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6508}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5765}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewBridge%3ARevisiting%20Cross-View%20Localization%20from%20Image%20Matching&body=Title%3A%20ViewBridge%3ARevisiting%20Cross-View%20Localization%20from%20Image%20Matching%0AAuthor%3A%20Panwang%20Xia%20and%20Qiong%20Wu%20and%20Lei%20Yu%20and%20Yi%20Liu%20and%20Mingtao%20Xiong%20and%20Xudong%20Lu%20and%20Yi%20Liu%20and%20Haoyu%20Guo%20and%20Yongxiang%20Yao%20and%20Junjian%20Zhang%20and%20Xiangyuan%20Cai%20and%20Hongwei%20Hu%20and%20Zhi%20Zheng%20and%20Yongjun%20Zhang%20and%20Yi%20Wan%0AAbstract%3A%20Cross-view%20localization%20aims%20to%20estimate%20the%203-DoF%20pose%20of%20a%20ground-view%20image%20by%20aligning%20it%20with%20aerial%20or%20satellite%20imagery.%20Existing%20methods%20typically%20address%20this%20task%20through%20direct%20regression%20or%20feature%20alignment%20in%20a%20shared%20bird%27s-eye%20view%20%28BEV%29%20space.%20Although%20effective%20for%20coarse%20alignment%2C%20these%20methods%20fail%20to%20establish%20fine-grained%20and%20geometrically%20reliable%20correspondences%20under%20large%20viewpoint%20variations%2C%20thereby%20limiting%20both%20the%20accuracy%20and%20interpretability%20of%20localization%20results.%20Consequently%2C%20we%20revisit%20cross-view%20localization%20from%20the%20perspective%20of%20image%20matching%20and%20propose%20a%20unified%20framework%20that%20enhances%20both%20matching%20and%20localization.%20Specifically%2C%20we%20introduce%20a%20Surface%20Model%20that%20constrains%20BEV%20feature%20projection%20to%20physically%20valid%20regions%20for%20geometric%20consistency%2C%20and%20a%20SimRefiner%20that%20adaptively%20refines%20similarity%20distributions%20to%20enhance%20match%20reliability.%20To%20further%20support%20research%20in%20this%20area%2C%20we%20present%20CVFM%2C%20the%20first%20benchmark%20with%2032%2C509%20cross-view%20image%20pairs%20annotated%20with%20pixel-level%20correspondences.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20geometry-consistent%20and%20fine-grained%20correspondences%20across%20extreme%20viewpoints%20and%20further%20improves%20the%20accuracy%20and%20stability%20of%20cross-view%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewBridge%253ARevisiting%2520Cross-View%2520Localization%2520from%2520Image%2520Matching%26entry.906535625%3DPanwang%2520Xia%2520and%2520Qiong%2520Wu%2520and%2520Lei%2520Yu%2520and%2520Yi%2520Liu%2520and%2520Mingtao%2520Xiong%2520and%2520Xudong%2520Lu%2520and%2520Yi%2520Liu%2520and%2520Haoyu%2520Guo%2520and%2520Yongxiang%2520Yao%2520and%2520Junjian%2520Zhang%2520and%2520Xiangyuan%2520Cai%2520and%2520Hongwei%2520Hu%2520and%2520Zhi%2520Zheng%2520and%2520Yongjun%2520Zhang%2520and%2520Yi%2520Wan%26entry.1292438233%3DCross-view%2520localization%2520aims%2520to%2520estimate%2520the%25203-DoF%2520pose%2520of%2520a%2520ground-view%2520image%2520by%2520aligning%2520it%2520with%2520aerial%2520or%2520satellite%2520imagery.%2520Existing%2520methods%2520typically%2520address%2520this%2520task%2520through%2520direct%2520regression%2520or%2520feature%2520alignment%2520in%2520a%2520shared%2520bird%2527s-eye%2520view%2520%2528BEV%2529%2520space.%2520Although%2520effective%2520for%2520coarse%2520alignment%252C%2520these%2520methods%2520fail%2520to%2520establish%2520fine-grained%2520and%2520geometrically%2520reliable%2520correspondences%2520under%2520large%2520viewpoint%2520variations%252C%2520thereby%2520limiting%2520both%2520the%2520accuracy%2520and%2520interpretability%2520of%2520localization%2520results.%2520Consequently%252C%2520we%2520revisit%2520cross-view%2520localization%2520from%2520the%2520perspective%2520of%2520image%2520matching%2520and%2520propose%2520a%2520unified%2520framework%2520that%2520enhances%2520both%2520matching%2520and%2520localization.%2520Specifically%252C%2520we%2520introduce%2520a%2520Surface%2520Model%2520that%2520constrains%2520BEV%2520feature%2520projection%2520to%2520physically%2520valid%2520regions%2520for%2520geometric%2520consistency%252C%2520and%2520a%2520SimRefiner%2520that%2520adaptively%2520refines%2520similarity%2520distributions%2520to%2520enhance%2520match%2520reliability.%2520To%2520further%2520support%2520research%2520in%2520this%2520area%252C%2520we%2520present%2520CVFM%252C%2520the%2520first%2520benchmark%2520with%252032%252C509%2520cross-view%2520image%2520pairs%2520annotated%2520with%2520pixel-level%2520correspondences.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520geometry-consistent%2520and%2520fine-grained%2520correspondences%2520across%2520extreme%2520viewpoints%2520and%2520further%2520improves%2520the%2520accuracy%2520and%2520stability%2520of%2520cross-view%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewBridge%3ARevisiting%20Cross-View%20Localization%20from%20Image%20Matching&entry.906535625=Panwang%20Xia%20and%20Qiong%20Wu%20and%20Lei%20Yu%20and%20Yi%20Liu%20and%20Mingtao%20Xiong%20and%20Xudong%20Lu%20and%20Yi%20Liu%20and%20Haoyu%20Guo%20and%20Yongxiang%20Yao%20and%20Junjian%20Zhang%20and%20Xiangyuan%20Cai%20and%20Hongwei%20Hu%20and%20Zhi%20Zheng%20and%20Yongjun%20Zhang%20and%20Yi%20Wan&entry.1292438233=Cross-view%20localization%20aims%20to%20estimate%20the%203-DoF%20pose%20of%20a%20ground-view%20image%20by%20aligning%20it%20with%20aerial%20or%20satellite%20imagery.%20Existing%20methods%20typically%20address%20this%20task%20through%20direct%20regression%20or%20feature%20alignment%20in%20a%20shared%20bird%27s-eye%20view%20%28BEV%29%20space.%20Although%20effective%20for%20coarse%20alignment%2C%20these%20methods%20fail%20to%20establish%20fine-grained%20and%20geometrically%20reliable%20correspondences%20under%20large%20viewpoint%20variations%2C%20thereby%20limiting%20both%20the%20accuracy%20and%20interpretability%20of%20localization%20results.%20Consequently%2C%20we%20revisit%20cross-view%20localization%20from%20the%20perspective%20of%20image%20matching%20and%20propose%20a%20unified%20framework%20that%20enhances%20both%20matching%20and%20localization.%20Specifically%2C%20we%20introduce%20a%20Surface%20Model%20that%20constrains%20BEV%20feature%20projection%20to%20physically%20valid%20regions%20for%20geometric%20consistency%2C%20and%20a%20SimRefiner%20that%20adaptively%20refines%20similarity%20distributions%20to%20enhance%20match%20reliability.%20To%20further%20support%20research%20in%20this%20area%2C%20we%20present%20CVFM%2C%20the%20first%20benchmark%20with%2032%2C509%20cross-view%20image%20pairs%20annotated%20with%20pixel-level%20correspondences.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20geometry-consistent%20and%20fine-grained%20correspondences%20across%20extreme%20viewpoints%20and%20further%20improves%20the%20accuracy%20and%20stability%20of%20cross-view%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2508.10716v2&entry.124074799=Read"},
{"title": "Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models", "author": "Haidong Kang and Lihong Lin and Enneng Yang and Hongning Dai and Hao Wang", "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \\textit{huge labor costs} and \\textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \\textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \\textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.", "link": "http://arxiv.org/abs/2511.15390v1", "date": "2025-11-19", "relevancy": 2.4048, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Expert%20Knowledge%20Limits%3A%20Self-Pruning%20for%20Large%20Language%20Models&body=Title%3A%20Breaking%20Expert%20Knowledge%20Limits%3A%20Self-Pruning%20for%20Large%20Language%20Models%0AAuthor%3A%20Haidong%20Kang%20and%20Lihong%20Lin%20and%20Enneng%20Yang%20and%20Hongning%20Dai%20and%20Hao%20Wang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20a%20wide%20range%20of%20tasks%2C%20hindering%20real-world%20deployment%20due%20to%20their%20massive%20size.%20Existing%20pruning%20methods%20%28e.g.%2C%20Wanda%29%20tailored%20for%20LLMs%20rely%20heavily%20on%20manual%20design%20pruning%20algorithms%2C%20thereby%20leading%20to%20%5Ctextit%7Bhuge%20labor%20costs%7D%20and%20%5Ctextit%7Brequires%20expert%20knowledge%7D.%20Furthermore%2C%20we%20are%20the%20first%20to%20identify%20the%20serious%20%5Ctextit%7Boutlier%20value%20issue%7D%20behind%20dramatic%20performance%20degradation%20under%20high%20pruning%20ratios%20that%20are%20caused%20by%20uniform%20sparsity%2C%20raising%20an%20additional%20concern%20about%20how%20to%20design%20adaptive%20pruning%20sparsity%20ideal%20for%20LLMs.%20Can%20LLMs%20prune%20by%20themselves%3F%20In%20this%20work%2C%20we%20introduce%20an%20affirmative%20answer%20by%20proposing%20a%20novel%20pruning%20method%20called%20%5Ctextbf%7BAutoPrune%7D%2C%20which%20first%20overcomes%20expert%20knowledge%20limits%20by%20leveraging%20LLMs%20to%20design%20optimal%20pruning%20algorithms%20for%20themselves%20automatically%20without%20any%20expert%20knowledge.%20Specifically%2C%20to%20mitigate%20the%20black-box%20nature%20of%20LLMs%2C%20we%20propose%20a%20Graph-driven%20Chain-of-Thought%20%28GCoT%29%20to%20optimize%20prompts%2C%20significantly%20enhancing%20the%20reasoning%20process%20in%20learning%20the%20pruning%20algorithm%20and%20enabling%20us%20to%20generate%20pruning%20algorithms%20with%20superior%20performance%20and%20interpretability%20in%20the%20next%20generation.%20Finally%2C%20grounded%20in%20insights%20of%20outlier%20value%20issue%2C%20we%20introduce%20Skew-aware%20Dynamic%20Sparsity%20Allocation%20%28SDSA%29%20to%20overcome%20the%20outlier%20value%20issue%2C%20mitigating%20performance%20degradation%20under%20high%20pruning%20ratios.%20We%20conduct%20extensive%20experiments%20on%20mainstream%20LLMs%20benchmarks%2C%20demonstrating%20the%20superiority%20of%20AutoPrune%2C%20which%20consistently%20excels%20state-of-the-art%20competitors.%20The%20code%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/AutoPrune.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Expert%2520Knowledge%2520Limits%253A%2520Self-Pruning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DHaidong%2520Kang%2520and%2520Lihong%2520Lin%2520and%2520Enneng%2520Yang%2520and%2520Hongning%2520Dai%2520and%2520Hao%2520Wang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520on%2520a%2520wide%2520range%2520of%2520tasks%252C%2520hindering%2520real-world%2520deployment%2520due%2520to%2520their%2520massive%2520size.%2520Existing%2520pruning%2520methods%2520%2528e.g.%252C%2520Wanda%2529%2520tailored%2520for%2520LLMs%2520rely%2520heavily%2520on%2520manual%2520design%2520pruning%2520algorithms%252C%2520thereby%2520leading%2520to%2520%255Ctextit%257Bhuge%2520labor%2520costs%257D%2520and%2520%255Ctextit%257Brequires%2520expert%2520knowledge%257D.%2520Furthermore%252C%2520we%2520are%2520the%2520first%2520to%2520identify%2520the%2520serious%2520%255Ctextit%257Boutlier%2520value%2520issue%257D%2520behind%2520dramatic%2520performance%2520degradation%2520under%2520high%2520pruning%2520ratios%2520that%2520are%2520caused%2520by%2520uniform%2520sparsity%252C%2520raising%2520an%2520additional%2520concern%2520about%2520how%2520to%2520design%2520adaptive%2520pruning%2520sparsity%2520ideal%2520for%2520LLMs.%2520Can%2520LLMs%2520prune%2520by%2520themselves%253F%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520affirmative%2520answer%2520by%2520proposing%2520a%2520novel%2520pruning%2520method%2520called%2520%255Ctextbf%257BAutoPrune%257D%252C%2520which%2520first%2520overcomes%2520expert%2520knowledge%2520limits%2520by%2520leveraging%2520LLMs%2520to%2520design%2520optimal%2520pruning%2520algorithms%2520for%2520themselves%2520automatically%2520without%2520any%2520expert%2520knowledge.%2520Specifically%252C%2520to%2520mitigate%2520the%2520black-box%2520nature%2520of%2520LLMs%252C%2520we%2520propose%2520a%2520Graph-driven%2520Chain-of-Thought%2520%2528GCoT%2529%2520to%2520optimize%2520prompts%252C%2520significantly%2520enhancing%2520the%2520reasoning%2520process%2520in%2520learning%2520the%2520pruning%2520algorithm%2520and%2520enabling%2520us%2520to%2520generate%2520pruning%2520algorithms%2520with%2520superior%2520performance%2520and%2520interpretability%2520in%2520the%2520next%2520generation.%2520Finally%252C%2520grounded%2520in%2520insights%2520of%2520outlier%2520value%2520issue%252C%2520we%2520introduce%2520Skew-aware%2520Dynamic%2520Sparsity%2520Allocation%2520%2528SDSA%2529%2520to%2520overcome%2520the%2520outlier%2520value%2520issue%252C%2520mitigating%2520performance%2520degradation%2520under%2520high%2520pruning%2520ratios.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520mainstream%2520LLMs%2520benchmarks%252C%2520demonstrating%2520the%2520superiority%2520of%2520AutoPrune%252C%2520which%2520consistently%2520excels%2520state-of-the-art%2520competitors.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//anonymous.4open.science/r/AutoPrune.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Expert%20Knowledge%20Limits%3A%20Self-Pruning%20for%20Large%20Language%20Models&entry.906535625=Haidong%20Kang%20and%20Lihong%20Lin%20and%20Enneng%20Yang%20and%20Hongning%20Dai%20and%20Hao%20Wang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20a%20wide%20range%20of%20tasks%2C%20hindering%20real-world%20deployment%20due%20to%20their%20massive%20size.%20Existing%20pruning%20methods%20%28e.g.%2C%20Wanda%29%20tailored%20for%20LLMs%20rely%20heavily%20on%20manual%20design%20pruning%20algorithms%2C%20thereby%20leading%20to%20%5Ctextit%7Bhuge%20labor%20costs%7D%20and%20%5Ctextit%7Brequires%20expert%20knowledge%7D.%20Furthermore%2C%20we%20are%20the%20first%20to%20identify%20the%20serious%20%5Ctextit%7Boutlier%20value%20issue%7D%20behind%20dramatic%20performance%20degradation%20under%20high%20pruning%20ratios%20that%20are%20caused%20by%20uniform%20sparsity%2C%20raising%20an%20additional%20concern%20about%20how%20to%20design%20adaptive%20pruning%20sparsity%20ideal%20for%20LLMs.%20Can%20LLMs%20prune%20by%20themselves%3F%20In%20this%20work%2C%20we%20introduce%20an%20affirmative%20answer%20by%20proposing%20a%20novel%20pruning%20method%20called%20%5Ctextbf%7BAutoPrune%7D%2C%20which%20first%20overcomes%20expert%20knowledge%20limits%20by%20leveraging%20LLMs%20to%20design%20optimal%20pruning%20algorithms%20for%20themselves%20automatically%20without%20any%20expert%20knowledge.%20Specifically%2C%20to%20mitigate%20the%20black-box%20nature%20of%20LLMs%2C%20we%20propose%20a%20Graph-driven%20Chain-of-Thought%20%28GCoT%29%20to%20optimize%20prompts%2C%20significantly%20enhancing%20the%20reasoning%20process%20in%20learning%20the%20pruning%20algorithm%20and%20enabling%20us%20to%20generate%20pruning%20algorithms%20with%20superior%20performance%20and%20interpretability%20in%20the%20next%20generation.%20Finally%2C%20grounded%20in%20insights%20of%20outlier%20value%20issue%2C%20we%20introduce%20Skew-aware%20Dynamic%20Sparsity%20Allocation%20%28SDSA%29%20to%20overcome%20the%20outlier%20value%20issue%2C%20mitigating%20performance%20degradation%20under%20high%20pruning%20ratios.%20We%20conduct%20extensive%20experiments%20on%20mainstream%20LLMs%20benchmarks%2C%20demonstrating%20the%20superiority%20of%20AutoPrune%2C%20which%20consistently%20excels%20state-of-the-art%20competitors.%20The%20code%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/AutoPrune.&entry.1838667208=http%3A//arxiv.org/abs/2511.15390v1&entry.124074799=Read"},
{"title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC", "author": "Beichen Zhang and Yuhang Zang and Xiaoyi Dong and Yuhang Cao and Haodong Duan and Dahua Lin and Jiaqi Wang", "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.", "link": "http://arxiv.org/abs/2511.15703v1", "date": "2025-11-19", "relevancy": 2.386, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC&body=Title%3A%20Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC%0AAuthor%3A%20Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20Abstract%20reasoning%20from%20minimal%20examples%20remains%20a%20core%20unsolved%20problem%20for%20frontier%20foundation%20models%20such%20as%20GPT-5%20and%20Grok%204.%20These%20models%20still%20fail%20to%20infer%20structured%20transformation%20rules%20from%20a%20handful%20of%20examples%2C%20which%20is%20a%20key%20hallmark%20of%20human%20intelligence.%20The%20Abstraction%20and%20Reasoning%20Corpus%20for%20Artificial%20General%20Intelligence%20%28ARC-AGI%29%20provides%20a%20rigorous%20testbed%20for%20this%20capability%2C%20demanding%20conceptual%20rule%20induction%20and%20transfer%20to%20novel%20tasks.%20Most%20existing%20methods%20treat%20ARC-AGI%20as%20a%20purely%20textual%20reasoning%20task%2C%20overlooking%20the%20fact%20that%20humans%20rely%20heavily%20on%20visual%20abstraction%20when%20solving%20such%20puzzles.%20However%2C%20our%20pilot%20experiments%20reveal%20a%20paradox%3A%20naively%20rendering%20ARC-AGI%20grids%20as%20images%20degrades%20performance%20due%20to%20imprecise%20rule%20execution.%20This%20leads%20to%20our%20central%20hypothesis%20that%20vision%20and%20language%20possess%20complementary%20strengths%20across%20distinct%20reasoning%20stages%3A%20vision%20supports%20global%20pattern%20abstraction%20and%20verification%2C%20whereas%20language%20specializes%20in%20symbolic%20rule%20formulation%20and%20precise%20execution.%20Building%20on%20this%20insight%2C%20we%20introduce%20two%20synergistic%20strategies%3A%20%281%29%20Vision-Language%20Synergy%20Reasoning%20%28VLSR%29%2C%20which%20decomposes%20ARC-AGI%20into%20modality-aligned%20subtasks%3B%20and%20%282%29%20Modality-Switch%20Self-Correction%20%28MSSC%29%2C%20which%20leverages%20vision%20to%20verify%20text-based%20reasoning%20for%20intrinsic%20error%20correction.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20yields%20up%20to%20a%204.33%25%20improvement%20over%20text-only%20baselines%20across%20diverse%20flagship%20models%20and%20multiple%20ARC-AGI%20tasks.%20Our%20findings%20suggest%20that%20unifying%20visual%20abstraction%20with%20linguistic%20reasoning%20is%20a%20crucial%20step%20toward%20achieving%20generalizable%2C%20human-like%20intelligence%20in%20future%20foundation%20models.%20Source%20code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Visually%252C%2520Reason%2520Textually%253A%2520Vision-Language%2520Synergy%2520in%2520ARC%26entry.906535625%3DBeichen%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Haodong%2520Duan%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DAbstract%2520reasoning%2520from%2520minimal%2520examples%2520remains%2520a%2520core%2520unsolved%2520problem%2520for%2520frontier%2520foundation%2520models%2520such%2520as%2520GPT-5%2520and%2520Grok%25204.%2520These%2520models%2520still%2520fail%2520to%2520infer%2520structured%2520transformation%2520rules%2520from%2520a%2520handful%2520of%2520examples%252C%2520which%2520is%2520a%2520key%2520hallmark%2520of%2520human%2520intelligence.%2520The%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520for%2520Artificial%2520General%2520Intelligence%2520%2528ARC-AGI%2529%2520provides%2520a%2520rigorous%2520testbed%2520for%2520this%2520capability%252C%2520demanding%2520conceptual%2520rule%2520induction%2520and%2520transfer%2520to%2520novel%2520tasks.%2520Most%2520existing%2520methods%2520treat%2520ARC-AGI%2520as%2520a%2520purely%2520textual%2520reasoning%2520task%252C%2520overlooking%2520the%2520fact%2520that%2520humans%2520rely%2520heavily%2520on%2520visual%2520abstraction%2520when%2520solving%2520such%2520puzzles.%2520However%252C%2520our%2520pilot%2520experiments%2520reveal%2520a%2520paradox%253A%2520naively%2520rendering%2520ARC-AGI%2520grids%2520as%2520images%2520degrades%2520performance%2520due%2520to%2520imprecise%2520rule%2520execution.%2520This%2520leads%2520to%2520our%2520central%2520hypothesis%2520that%2520vision%2520and%2520language%2520possess%2520complementary%2520strengths%2520across%2520distinct%2520reasoning%2520stages%253A%2520vision%2520supports%2520global%2520pattern%2520abstraction%2520and%2520verification%252C%2520whereas%2520language%2520specializes%2520in%2520symbolic%2520rule%2520formulation%2520and%2520precise%2520execution.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520two%2520synergistic%2520strategies%253A%2520%25281%2529%2520Vision-Language%2520Synergy%2520Reasoning%2520%2528VLSR%2529%252C%2520which%2520decomposes%2520ARC-AGI%2520into%2520modality-aligned%2520subtasks%253B%2520and%2520%25282%2529%2520Modality-Switch%2520Self-Correction%2520%2528MSSC%2529%252C%2520which%2520leverages%2520vision%2520to%2520verify%2520text-based%2520reasoning%2520for%2520intrinsic%2520error%2520correction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520yields%2520up%2520to%2520a%25204.33%2525%2520improvement%2520over%2520text-only%2520baselines%2520across%2520diverse%2520flagship%2520models%2520and%2520multiple%2520ARC-AGI%2520tasks.%2520Our%2520findings%2520suggest%2520that%2520unifying%2520visual%2520abstraction%2520with%2520linguistic%2520reasoning%2520is%2520a%2520crucial%2520step%2520toward%2520achieving%2520generalizable%252C%2520human-like%2520intelligence%2520in%2520future%2520foundation%2520models.%2520Source%2520code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Visually%2C%20Reason%20Textually%3A%20Vision-Language%20Synergy%20in%20ARC&entry.906535625=Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=Abstract%20reasoning%20from%20minimal%20examples%20remains%20a%20core%20unsolved%20problem%20for%20frontier%20foundation%20models%20such%20as%20GPT-5%20and%20Grok%204.%20These%20models%20still%20fail%20to%20infer%20structured%20transformation%20rules%20from%20a%20handful%20of%20examples%2C%20which%20is%20a%20key%20hallmark%20of%20human%20intelligence.%20The%20Abstraction%20and%20Reasoning%20Corpus%20for%20Artificial%20General%20Intelligence%20%28ARC-AGI%29%20provides%20a%20rigorous%20testbed%20for%20this%20capability%2C%20demanding%20conceptual%20rule%20induction%20and%20transfer%20to%20novel%20tasks.%20Most%20existing%20methods%20treat%20ARC-AGI%20as%20a%20purely%20textual%20reasoning%20task%2C%20overlooking%20the%20fact%20that%20humans%20rely%20heavily%20on%20visual%20abstraction%20when%20solving%20such%20puzzles.%20However%2C%20our%20pilot%20experiments%20reveal%20a%20paradox%3A%20naively%20rendering%20ARC-AGI%20grids%20as%20images%20degrades%20performance%20due%20to%20imprecise%20rule%20execution.%20This%20leads%20to%20our%20central%20hypothesis%20that%20vision%20and%20language%20possess%20complementary%20strengths%20across%20distinct%20reasoning%20stages%3A%20vision%20supports%20global%20pattern%20abstraction%20and%20verification%2C%20whereas%20language%20specializes%20in%20symbolic%20rule%20formulation%20and%20precise%20execution.%20Building%20on%20this%20insight%2C%20we%20introduce%20two%20synergistic%20strategies%3A%20%281%29%20Vision-Language%20Synergy%20Reasoning%20%28VLSR%29%2C%20which%20decomposes%20ARC-AGI%20into%20modality-aligned%20subtasks%3B%20and%20%282%29%20Modality-Switch%20Self-Correction%20%28MSSC%29%2C%20which%20leverages%20vision%20to%20verify%20text-based%20reasoning%20for%20intrinsic%20error%20correction.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20yields%20up%20to%20a%204.33%25%20improvement%20over%20text-only%20baselines%20across%20diverse%20flagship%20models%20and%20multiple%20ARC-AGI%20tasks.%20Our%20findings%20suggest%20that%20unifying%20visual%20abstraction%20with%20linguistic%20reasoning%20is%20a%20crucial%20step%20toward%20achieving%20generalizable%2C%20human-like%20intelligence%20in%20future%20foundation%20models.%20Source%20code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2511.15703v1&entry.124074799=Read"},
{"title": "Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays", "author": "Haowei Hua and Hong Jiao and Xinyi Wang", "abstract": "BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.", "link": "http://arxiv.org/abs/2510.22830v4", "date": "2025-11-19", "relevancy": 2.3765, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4801}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploration%20of%20Summarization%20by%20Generative%20Language%20Models%20for%20Automated%20Scoring%20of%20Long%20Essays&body=Title%3A%20Exploration%20of%20Summarization%20by%20Generative%20Language%20Models%20for%20Automated%20Scoring%20of%20Long%20Essays%0AAuthor%3A%20Haowei%20Hua%20and%20Hong%20Jiao%20and%20Xinyi%20Wang%0AAbstract%3A%20BERT%20and%20its%20variants%20are%20extensively%20explored%20for%20automated%20scoring.%20However%2C%20a%20limit%20of%20512%20tokens%20for%20these%20encoder-based%20models%20showed%20the%20deficiency%20in%20automated%20scoring%20of%20long%20essays.%20Thus%2C%20this%20research%20explores%20generative%20language%20models%20for%20automated%20scoring%20of%20long%20essays%20via%20summarization%20and%20prompting.%20The%20results%20revealed%20great%20improvement%20of%20scoring%20accuracy%20with%20QWK%20increased%20from%200.822%20to%200.8878%20for%20the%20Learning%20Agency%20Lab%20Automated%20Essay%20Scoring%202.0%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22830v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploration%2520of%2520Summarization%2520by%2520Generative%2520Language%2520Models%2520for%2520Automated%2520Scoring%2520of%2520Long%2520Essays%26entry.906535625%3DHaowei%2520Hua%2520and%2520Hong%2520Jiao%2520and%2520Xinyi%2520Wang%26entry.1292438233%3DBERT%2520and%2520its%2520variants%2520are%2520extensively%2520explored%2520for%2520automated%2520scoring.%2520However%252C%2520a%2520limit%2520of%2520512%2520tokens%2520for%2520these%2520encoder-based%2520models%2520showed%2520the%2520deficiency%2520in%2520automated%2520scoring%2520of%2520long%2520essays.%2520Thus%252C%2520this%2520research%2520explores%2520generative%2520language%2520models%2520for%2520automated%2520scoring%2520of%2520long%2520essays%2520via%2520summarization%2520and%2520prompting.%2520The%2520results%2520revealed%2520great%2520improvement%2520of%2520scoring%2520accuracy%2520with%2520QWK%2520increased%2520from%25200.822%2520to%25200.8878%2520for%2520the%2520Learning%2520Agency%2520Lab%2520Automated%2520Essay%2520Scoring%25202.0%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22830v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploration%20of%20Summarization%20by%20Generative%20Language%20Models%20for%20Automated%20Scoring%20of%20Long%20Essays&entry.906535625=Haowei%20Hua%20and%20Hong%20Jiao%20and%20Xinyi%20Wang&entry.1292438233=BERT%20and%20its%20variants%20are%20extensively%20explored%20for%20automated%20scoring.%20However%2C%20a%20limit%20of%20512%20tokens%20for%20these%20encoder-based%20models%20showed%20the%20deficiency%20in%20automated%20scoring%20of%20long%20essays.%20Thus%2C%20this%20research%20explores%20generative%20language%20models%20for%20automated%20scoring%20of%20long%20essays%20via%20summarization%20and%20prompting.%20The%20results%20revealed%20great%20improvement%20of%20scoring%20accuracy%20with%20QWK%20increased%20from%200.822%20to%200.8878%20for%20the%20Learning%20Agency%20Lab%20Automated%20Essay%20Scoring%202.0%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2510.22830v4&entry.124074799=Read"},
{"title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning", "author": "Qihao Yang and Xuelin Wang and Jiale Chen and Xuelian Dong and Yuxin Hao and Tianyong Hao", "abstract": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.", "link": "http://arxiv.org/abs/2511.15574v1", "date": "2025-11-19", "relevancy": 2.3708, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSKBenchmark%3A%20Modeling%20and%20Benchmarking%20Chinese%20Second%20Language%20Acquisition%20in%20Large%20Language%20Models%20through%20Curriculum%20Tuning&body=Title%3A%20HSKBenchmark%3A%20Modeling%20and%20Benchmarking%20Chinese%20Second%20Language%20Acquisition%20in%20Large%20Language%20Models%20through%20Curriculum%20Tuning%0AAuthor%3A%20Qihao%20Yang%20and%20Xuelin%20Wang%20and%20Jiale%20Chen%20and%20Xuelian%20Dong%20and%20Yuxin%20Hao%20and%20Tianyong%20Hao%0AAbstract%3A%20Language%20acquisition%20is%20vital%20to%20revealing%20the%20nature%20of%20human%20language%20intelligence%20and%20has%20recently%20emerged%20as%20a%20promising%20perspective%20for%20improving%20the%20interpretability%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20ethically%20and%20practically%20infeasible%20to%20conduct%20experiments%20that%20require%20controlling%20human%20learners%27%20language%20inputs.%20This%20poses%20challenges%20for%20the%20verifiability%20and%20scalability%20of%20language%20acquisition%20modeling%2C%20particularly%20in%20Chinese%20second%20language%20acquisition%20%28SLA%29.%20While%20LLMs%20provide%20a%20controllable%20and%20reproducible%20alternative%2C%20a%20systematic%20benchmark%20to%20support%20phase-wise%20modeling%20and%20assessment%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20present%20HSKBenchmark%2C%20the%20first%20benchmark%20for%20staged%20modeling%20and%20writing%20assessment%20of%20LLMs%20in%20Chinese%20SLA.%20It%20covers%20HSK%20levels%203%20to%206%20and%20includes%20authentic%20textbooks%20with%206.76%20million%20tokens%2C%2016K%20synthetic%20instruction%20samples%2C%2030%20test%20topics%2C%20and%20a%20linguistically%20grounded%20evaluation%20system.%20To%20simulate%20human%20learning%20trajectories%2C%20we%20introduce%20a%20curriculum-tuning%20framework%20that%20trains%20models%20from%20beginner%20to%20advanced%20levels.%20An%20evaluation%20system%20is%20created%20to%20examine%20level-based%20grammar%20coverage%2C%20writing%20errors%2C%20lexical%20and%20syntactic%20complexity%2C%20and%20holistic%20scoring.%20We%20also%20build%20HSKAgent%2C%20fine-tuned%20on%2010K%20learner%20compositions.%20Extensive%20experimental%20results%20demonstrate%20that%20HSKBenchmark%20not%20only%20models%20Chinese%20SLA%20effectively%2C%20but%20also%20serves%20as%20a%20reliable%20benchmark%20for%20dynamic%20writing%20assessment%20in%20LLMs.%20Our%20fine-tuned%20LLMs%20have%20writing%20performance%20on%20par%20with%20advanced%20human%20learners%20and%20exhibit%20human-like%20acquisition%20characteristics.%20The%20HSKBenchmark%2C%20HSKAgent%2C%20and%20checkpoints%20serve%20as%20foundational%20tools%20and%20resources%2C%20with%20the%20potential%20to%20pave%20the%20way%20for%20future%20research%20on%20language%20acquisition%20modeling%20and%20LLMs%20interpretability.%20Code%20and%20data%20are%20publicly%20available%20at%3A%20https%3A//github.com/CharlesYang030/HSKB.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSKBenchmark%253A%2520Modeling%2520and%2520Benchmarking%2520Chinese%2520Second%2520Language%2520Acquisition%2520in%2520Large%2520Language%2520Models%2520through%2520Curriculum%2520Tuning%26entry.906535625%3DQihao%2520Yang%2520and%2520Xuelin%2520Wang%2520and%2520Jiale%2520Chen%2520and%2520Xuelian%2520Dong%2520and%2520Yuxin%2520Hao%2520and%2520Tianyong%2520Hao%26entry.1292438233%3DLanguage%2520acquisition%2520is%2520vital%2520to%2520revealing%2520the%2520nature%2520of%2520human%2520language%2520intelligence%2520and%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520perspective%2520for%2520improving%2520the%2520interpretability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520it%2520is%2520ethically%2520and%2520practically%2520infeasible%2520to%2520conduct%2520experiments%2520that%2520require%2520controlling%2520human%2520learners%2527%2520language%2520inputs.%2520This%2520poses%2520challenges%2520for%2520the%2520verifiability%2520and%2520scalability%2520of%2520language%2520acquisition%2520modeling%252C%2520particularly%2520in%2520Chinese%2520second%2520language%2520acquisition%2520%2528SLA%2529.%2520While%2520LLMs%2520provide%2520a%2520controllable%2520and%2520reproducible%2520alternative%252C%2520a%2520systematic%2520benchmark%2520to%2520support%2520phase-wise%2520modeling%2520and%2520assessment%2520is%2520still%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520present%2520HSKBenchmark%252C%2520the%2520first%2520benchmark%2520for%2520staged%2520modeling%2520and%2520writing%2520assessment%2520of%2520LLMs%2520in%2520Chinese%2520SLA.%2520It%2520covers%2520HSK%2520levels%25203%2520to%25206%2520and%2520includes%2520authentic%2520textbooks%2520with%25206.76%2520million%2520tokens%252C%252016K%2520synthetic%2520instruction%2520samples%252C%252030%2520test%2520topics%252C%2520and%2520a%2520linguistically%2520grounded%2520evaluation%2520system.%2520To%2520simulate%2520human%2520learning%2520trajectories%252C%2520we%2520introduce%2520a%2520curriculum-tuning%2520framework%2520that%2520trains%2520models%2520from%2520beginner%2520to%2520advanced%2520levels.%2520An%2520evaluation%2520system%2520is%2520created%2520to%2520examine%2520level-based%2520grammar%2520coverage%252C%2520writing%2520errors%252C%2520lexical%2520and%2520syntactic%2520complexity%252C%2520and%2520holistic%2520scoring.%2520We%2520also%2520build%2520HSKAgent%252C%2520fine-tuned%2520on%252010K%2520learner%2520compositions.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520HSKBenchmark%2520not%2520only%2520models%2520Chinese%2520SLA%2520effectively%252C%2520but%2520also%2520serves%2520as%2520a%2520reliable%2520benchmark%2520for%2520dynamic%2520writing%2520assessment%2520in%2520LLMs.%2520Our%2520fine-tuned%2520LLMs%2520have%2520writing%2520performance%2520on%2520par%2520with%2520advanced%2520human%2520learners%2520and%2520exhibit%2520human-like%2520acquisition%2520characteristics.%2520The%2520HSKBenchmark%252C%2520HSKAgent%252C%2520and%2520checkpoints%2520serve%2520as%2520foundational%2520tools%2520and%2520resources%252C%2520with%2520the%2520potential%2520to%2520pave%2520the%2520way%2520for%2520future%2520research%2520on%2520language%2520acquisition%2520modeling%2520and%2520LLMs%2520interpretability.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520at%253A%2520https%253A//github.com/CharlesYang030/HSKB.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSKBenchmark%3A%20Modeling%20and%20Benchmarking%20Chinese%20Second%20Language%20Acquisition%20in%20Large%20Language%20Models%20through%20Curriculum%20Tuning&entry.906535625=Qihao%20Yang%20and%20Xuelin%20Wang%20and%20Jiale%20Chen%20and%20Xuelian%20Dong%20and%20Yuxin%20Hao%20and%20Tianyong%20Hao&entry.1292438233=Language%20acquisition%20is%20vital%20to%20revealing%20the%20nature%20of%20human%20language%20intelligence%20and%20has%20recently%20emerged%20as%20a%20promising%20perspective%20for%20improving%20the%20interpretability%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20it%20is%20ethically%20and%20practically%20infeasible%20to%20conduct%20experiments%20that%20require%20controlling%20human%20learners%27%20language%20inputs.%20This%20poses%20challenges%20for%20the%20verifiability%20and%20scalability%20of%20language%20acquisition%20modeling%2C%20particularly%20in%20Chinese%20second%20language%20acquisition%20%28SLA%29.%20While%20LLMs%20provide%20a%20controllable%20and%20reproducible%20alternative%2C%20a%20systematic%20benchmark%20to%20support%20phase-wise%20modeling%20and%20assessment%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20present%20HSKBenchmark%2C%20the%20first%20benchmark%20for%20staged%20modeling%20and%20writing%20assessment%20of%20LLMs%20in%20Chinese%20SLA.%20It%20covers%20HSK%20levels%203%20to%206%20and%20includes%20authentic%20textbooks%20with%206.76%20million%20tokens%2C%2016K%20synthetic%20instruction%20samples%2C%2030%20test%20topics%2C%20and%20a%20linguistically%20grounded%20evaluation%20system.%20To%20simulate%20human%20learning%20trajectories%2C%20we%20introduce%20a%20curriculum-tuning%20framework%20that%20trains%20models%20from%20beginner%20to%20advanced%20levels.%20An%20evaluation%20system%20is%20created%20to%20examine%20level-based%20grammar%20coverage%2C%20writing%20errors%2C%20lexical%20and%20syntactic%20complexity%2C%20and%20holistic%20scoring.%20We%20also%20build%20HSKAgent%2C%20fine-tuned%20on%2010K%20learner%20compositions.%20Extensive%20experimental%20results%20demonstrate%20that%20HSKBenchmark%20not%20only%20models%20Chinese%20SLA%20effectively%2C%20but%20also%20serves%20as%20a%20reliable%20benchmark%20for%20dynamic%20writing%20assessment%20in%20LLMs.%20Our%20fine-tuned%20LLMs%20have%20writing%20performance%20on%20par%20with%20advanced%20human%20learners%20and%20exhibit%20human-like%20acquisition%20characteristics.%20The%20HSKBenchmark%2C%20HSKAgent%2C%20and%20checkpoints%20serve%20as%20foundational%20tools%20and%20resources%2C%20with%20the%20potential%20to%20pave%20the%20way%20for%20future%20research%20on%20language%20acquisition%20modeling%20and%20LLMs%20interpretability.%20Code%20and%20data%20are%20publicly%20available%20at%3A%20https%3A//github.com/CharlesYang030/HSKB.&entry.1838667208=http%3A//arxiv.org/abs/2511.15574v1&entry.124074799=Read"},
{"title": "First Frame Is the Place to Go for Video Content Customization", "author": "Jingxi Chen and Zongxia Li and Zhichao Liu and Guangyao Shi and Xiyang Wu and Fuxiao Liu and Cornelia Fermuller and Brandon Y. Feng and Yiannis Aloimonos", "abstract": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.", "link": "http://arxiv.org/abs/2511.15700v1", "date": "2025-11-19", "relevancy": 2.3519, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6721}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6102}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%20Frame%20Is%20the%20Place%20to%20Go%20for%20Video%20Content%20Customization&body=Title%3A%20First%20Frame%20Is%20the%20Place%20to%20Go%20for%20Video%20Content%20Customization%0AAuthor%3A%20Jingxi%20Chen%20and%20Zongxia%20Li%20and%20Zhichao%20Liu%20and%20Guangyao%20Shi%20and%20Xiyang%20Wu%20and%20Fuxiao%20Liu%20and%20Cornelia%20Fermuller%20and%20Brandon%20Y.%20Feng%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20What%20role%20does%20the%20first%20frame%20play%20in%20video%20generation%20models%3F%20Traditionally%2C%20it%27s%20viewed%20as%20the%20spatial-temporal%20starting%20point%20of%20a%20video%2C%20merely%20a%20seed%20for%20subsequent%20animation.%20In%20this%20work%2C%20we%20reveal%20a%20fundamentally%20different%20perspective%3A%20video%20models%20implicitly%20treat%20the%20first%20frame%20as%20a%20conceptual%20memory%20buffer%20that%20stores%20visual%20entities%20for%20later%20reuse%20during%20generation.%20Leveraging%20this%20insight%2C%20we%20show%20that%20it%27s%20possible%20to%20achieve%20robust%20and%20generalized%20video%20content%20customization%20in%20diverse%20scenarios%2C%20using%20only%2020-50%20training%20examples%20without%20architectural%20changes%20or%20large-scale%20finetuning.%20This%20unveils%20a%20powerful%2C%20overlooked%20capability%20of%20video%20generation%20models%20for%20reference-based%20video%20customization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%2520Frame%2520Is%2520the%2520Place%2520to%2520Go%2520for%2520Video%2520Content%2520Customization%26entry.906535625%3DJingxi%2520Chen%2520and%2520Zongxia%2520Li%2520and%2520Zhichao%2520Liu%2520and%2520Guangyao%2520Shi%2520and%2520Xiyang%2520Wu%2520and%2520Fuxiao%2520Liu%2520and%2520Cornelia%2520Fermuller%2520and%2520Brandon%2520Y.%2520Feng%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3DWhat%2520role%2520does%2520the%2520first%2520frame%2520play%2520in%2520video%2520generation%2520models%253F%2520Traditionally%252C%2520it%2527s%2520viewed%2520as%2520the%2520spatial-temporal%2520starting%2520point%2520of%2520a%2520video%252C%2520merely%2520a%2520seed%2520for%2520subsequent%2520animation.%2520In%2520this%2520work%252C%2520we%2520reveal%2520a%2520fundamentally%2520different%2520perspective%253A%2520video%2520models%2520implicitly%2520treat%2520the%2520first%2520frame%2520as%2520a%2520conceptual%2520memory%2520buffer%2520that%2520stores%2520visual%2520entities%2520for%2520later%2520reuse%2520during%2520generation.%2520Leveraging%2520this%2520insight%252C%2520we%2520show%2520that%2520it%2527s%2520possible%2520to%2520achieve%2520robust%2520and%2520generalized%2520video%2520content%2520customization%2520in%2520diverse%2520scenarios%252C%2520using%2520only%252020-50%2520training%2520examples%2520without%2520architectural%2520changes%2520or%2520large-scale%2520finetuning.%2520This%2520unveils%2520a%2520powerful%252C%2520overlooked%2520capability%2520of%2520video%2520generation%2520models%2520for%2520reference-based%2520video%2520customization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%20Frame%20Is%20the%20Place%20to%20Go%20for%20Video%20Content%20Customization&entry.906535625=Jingxi%20Chen%20and%20Zongxia%20Li%20and%20Zhichao%20Liu%20and%20Guangyao%20Shi%20and%20Xiyang%20Wu%20and%20Fuxiao%20Liu%20and%20Cornelia%20Fermuller%20and%20Brandon%20Y.%20Feng%20and%20Yiannis%20Aloimonos&entry.1292438233=What%20role%20does%20the%20first%20frame%20play%20in%20video%20generation%20models%3F%20Traditionally%2C%20it%27s%20viewed%20as%20the%20spatial-temporal%20starting%20point%20of%20a%20video%2C%20merely%20a%20seed%20for%20subsequent%20animation.%20In%20this%20work%2C%20we%20reveal%20a%20fundamentally%20different%20perspective%3A%20video%20models%20implicitly%20treat%20the%20first%20frame%20as%20a%20conceptual%20memory%20buffer%20that%20stores%20visual%20entities%20for%20later%20reuse%20during%20generation.%20Leveraging%20this%20insight%2C%20we%20show%20that%20it%27s%20possible%20to%20achieve%20robust%20and%20generalized%20video%20content%20customization%20in%20diverse%20scenarios%2C%20using%20only%2020-50%20training%20examples%20without%20architectural%20changes%20or%20large-scale%20finetuning.%20This%20unveils%20a%20powerful%2C%20overlooked%20capability%20of%20video%20generation%20models%20for%20reference-based%20video%20customization.&entry.1838667208=http%3A//arxiv.org/abs/2511.15700v1&entry.124074799=Read"},
{"title": "OODTE: A Differential Testing Engine for the ONNX Optimizer", "author": "Nikolaos Louloudakis and Ajitha Rajan", "abstract": "With over 760 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.", "link": "http://arxiv.org/abs/2505.01892v4", "date": "2025-11-19", "relevancy": 2.3518, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OODTE%3A%20A%20Differential%20Testing%20Engine%20for%20the%20ONNX%20Optimizer&body=Title%3A%20OODTE%3A%20A%20Differential%20Testing%20Engine%20for%20the%20ONNX%20Optimizer%0AAuthor%3A%20Nikolaos%20Louloudakis%20and%20Ajitha%20Rajan%0AAbstract%3A%20With%20over%20760%20stars%20on%20GitHub%20and%20being%20part%20of%20the%20official%20ONNX%20repository%2C%20the%20ONNX%20Optimizer%20is%20the%20default%20tool%20for%20applying%20graph-based%20optimizations%20to%20ONNX%20models.%20Despite%20its%20widespread%20use%2C%20its%20ability%20to%20maintain%20model%20accuracy%20during%20optimization%20has%20not%20been%20thoroughly%20investigated.%20In%20this%20work%2C%20we%20present%20OODTE%2C%20a%20utility%20designed%20to%20automatically%20and%20comprehensively%20evaluate%20the%20correctness%20of%20the%20ONNX%20Optimizer.%20OODTE%20adopts%20a%20straightforward%20yet%20powerful%20differential%20testing%20and%20evaluation%20methodology%2C%20which%20can%20be%20readily%20adapted%20for%20use%20with%20other%20compiler%20optimizers.%20Specifically%2C%20OODTE%20takes%20a%20collection%20of%20ONNX%20models%2C%20applies%20optimizations%2C%20and%20executes%20both%20the%20original%20and%20optimized%20versions%20across%20a%20user-defined%20input%20set%2C%20automatically%20capturing%20any%20issues%20encountered%20during%20optimization.%20When%20discrepancies%20in%20accuracy%20arise%2C%20OODTE%20iteratively%20isolates%20the%20responsible%20optimization%20pass%20by%20repeating%20the%20process%20at%20a%20finer%20granularity.%20We%20applied%20OODTE%20to%20130%20well-known%20models%20from%20the%20official%20ONNX%20Model%20Hub%2C%20spanning%20diverse%20tasks%20including%20classification%2C%20object%20detection%2C%20semantic%20segmentation%2C%20text%20summarization%2C%20question%20answering%2C%20and%20sentiment%20analysis.%20Our%20evaluation%20revealed%20that%209.2%25%20of%20the%20model%20instances%20either%20caused%20the%20optimizer%20to%20crash%20or%20led%20to%20the%20generation%20of%20invalid%20models%20using%20default%20optimization%20strategies.%20Additionally%2C%2030%25%20of%20classification%20models%20and%2016.6%25%20of%20object%20detection%20and%20segmentation%20models%20exhibited%20differing%20outputs%20across%20original%20and%20optimized%20versions%2C%20whereas%20models%20focused%20on%20text-related%20tasks%20were%20generally%20robust%20to%20optimization.%20OODTE%20uncovered%2015%20issues-14%20previously%20unknown-affecting%209%20of%2047%20optimization%20passes%20and%20the%20optimizer%20overall.%20All%20issues%20were%20reported%20to%20the%20ONNX%20Optimizer%20team.%20OODTE%20offers%20a%20simple%20but%20effective%20framework%20for%20validating%20AI%20model%20optimizers%2C%20applicable%20beyond%20the%20ONNX%20ecosystem.%0ALink%3A%20http%3A//arxiv.org/abs/2505.01892v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOODTE%253A%2520A%2520Differential%2520Testing%2520Engine%2520for%2520the%2520ONNX%2520Optimizer%26entry.906535625%3DNikolaos%2520Louloudakis%2520and%2520Ajitha%2520Rajan%26entry.1292438233%3DWith%2520over%2520760%2520stars%2520on%2520GitHub%2520and%2520being%2520part%2520of%2520the%2520official%2520ONNX%2520repository%252C%2520the%2520ONNX%2520Optimizer%2520is%2520the%2520default%2520tool%2520for%2520applying%2520graph-based%2520optimizations%2520to%2520ONNX%2520models.%2520Despite%2520its%2520widespread%2520use%252C%2520its%2520ability%2520to%2520maintain%2520model%2520accuracy%2520during%2520optimization%2520has%2520not%2520been%2520thoroughly%2520investigated.%2520In%2520this%2520work%252C%2520we%2520present%2520OODTE%252C%2520a%2520utility%2520designed%2520to%2520automatically%2520and%2520comprehensively%2520evaluate%2520the%2520correctness%2520of%2520the%2520ONNX%2520Optimizer.%2520OODTE%2520adopts%2520a%2520straightforward%2520yet%2520powerful%2520differential%2520testing%2520and%2520evaluation%2520methodology%252C%2520which%2520can%2520be%2520readily%2520adapted%2520for%2520use%2520with%2520other%2520compiler%2520optimizers.%2520Specifically%252C%2520OODTE%2520takes%2520a%2520collection%2520of%2520ONNX%2520models%252C%2520applies%2520optimizations%252C%2520and%2520executes%2520both%2520the%2520original%2520and%2520optimized%2520versions%2520across%2520a%2520user-defined%2520input%2520set%252C%2520automatically%2520capturing%2520any%2520issues%2520encountered%2520during%2520optimization.%2520When%2520discrepancies%2520in%2520accuracy%2520arise%252C%2520OODTE%2520iteratively%2520isolates%2520the%2520responsible%2520optimization%2520pass%2520by%2520repeating%2520the%2520process%2520at%2520a%2520finer%2520granularity.%2520We%2520applied%2520OODTE%2520to%2520130%2520well-known%2520models%2520from%2520the%2520official%2520ONNX%2520Model%2520Hub%252C%2520spanning%2520diverse%2520tasks%2520including%2520classification%252C%2520object%2520detection%252C%2520semantic%2520segmentation%252C%2520text%2520summarization%252C%2520question%2520answering%252C%2520and%2520sentiment%2520analysis.%2520Our%2520evaluation%2520revealed%2520that%25209.2%2525%2520of%2520the%2520model%2520instances%2520either%2520caused%2520the%2520optimizer%2520to%2520crash%2520or%2520led%2520to%2520the%2520generation%2520of%2520invalid%2520models%2520using%2520default%2520optimization%2520strategies.%2520Additionally%252C%252030%2525%2520of%2520classification%2520models%2520and%252016.6%2525%2520of%2520object%2520detection%2520and%2520segmentation%2520models%2520exhibited%2520differing%2520outputs%2520across%2520original%2520and%2520optimized%2520versions%252C%2520whereas%2520models%2520focused%2520on%2520text-related%2520tasks%2520were%2520generally%2520robust%2520to%2520optimization.%2520OODTE%2520uncovered%252015%2520issues-14%2520previously%2520unknown-affecting%25209%2520of%252047%2520optimization%2520passes%2520and%2520the%2520optimizer%2520overall.%2520All%2520issues%2520were%2520reported%2520to%2520the%2520ONNX%2520Optimizer%2520team.%2520OODTE%2520offers%2520a%2520simple%2520but%2520effective%2520framework%2520for%2520validating%2520AI%2520model%2520optimizers%252C%2520applicable%2520beyond%2520the%2520ONNX%2520ecosystem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01892v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OODTE%3A%20A%20Differential%20Testing%20Engine%20for%20the%20ONNX%20Optimizer&entry.906535625=Nikolaos%20Louloudakis%20and%20Ajitha%20Rajan&entry.1292438233=With%20over%20760%20stars%20on%20GitHub%20and%20being%20part%20of%20the%20official%20ONNX%20repository%2C%20the%20ONNX%20Optimizer%20is%20the%20default%20tool%20for%20applying%20graph-based%20optimizations%20to%20ONNX%20models.%20Despite%20its%20widespread%20use%2C%20its%20ability%20to%20maintain%20model%20accuracy%20during%20optimization%20has%20not%20been%20thoroughly%20investigated.%20In%20this%20work%2C%20we%20present%20OODTE%2C%20a%20utility%20designed%20to%20automatically%20and%20comprehensively%20evaluate%20the%20correctness%20of%20the%20ONNX%20Optimizer.%20OODTE%20adopts%20a%20straightforward%20yet%20powerful%20differential%20testing%20and%20evaluation%20methodology%2C%20which%20can%20be%20readily%20adapted%20for%20use%20with%20other%20compiler%20optimizers.%20Specifically%2C%20OODTE%20takes%20a%20collection%20of%20ONNX%20models%2C%20applies%20optimizations%2C%20and%20executes%20both%20the%20original%20and%20optimized%20versions%20across%20a%20user-defined%20input%20set%2C%20automatically%20capturing%20any%20issues%20encountered%20during%20optimization.%20When%20discrepancies%20in%20accuracy%20arise%2C%20OODTE%20iteratively%20isolates%20the%20responsible%20optimization%20pass%20by%20repeating%20the%20process%20at%20a%20finer%20granularity.%20We%20applied%20OODTE%20to%20130%20well-known%20models%20from%20the%20official%20ONNX%20Model%20Hub%2C%20spanning%20diverse%20tasks%20including%20classification%2C%20object%20detection%2C%20semantic%20segmentation%2C%20text%20summarization%2C%20question%20answering%2C%20and%20sentiment%20analysis.%20Our%20evaluation%20revealed%20that%209.2%25%20of%20the%20model%20instances%20either%20caused%20the%20optimizer%20to%20crash%20or%20led%20to%20the%20generation%20of%20invalid%20models%20using%20default%20optimization%20strategies.%20Additionally%2C%2030%25%20of%20classification%20models%20and%2016.6%25%20of%20object%20detection%20and%20segmentation%20models%20exhibited%20differing%20outputs%20across%20original%20and%20optimized%20versions%2C%20whereas%20models%20focused%20on%20text-related%20tasks%20were%20generally%20robust%20to%20optimization.%20OODTE%20uncovered%2015%20issues-14%20previously%20unknown-affecting%209%20of%2047%20optimization%20passes%20and%20the%20optimizer%20overall.%20All%20issues%20were%20reported%20to%20the%20ONNX%20Optimizer%20team.%20OODTE%20offers%20a%20simple%20but%20effective%20framework%20for%20validating%20AI%20model%20optimizers%2C%20applicable%20beyond%20the%20ONNX%20ecosystem.&entry.1838667208=http%3A//arxiv.org/abs/2505.01892v4&entry.124074799=Read"},
{"title": "ExDAG: an MIQP Algorithm for Learning DAGs", "author": "Pavel Rytir and Ales Wodecki and Jakub Marecek", "abstract": "There has been a growing interest in causal learning in recent years. Commonly used representations of causal structures, including Bayesian networks and structural equation models (SEM), take the form of directed acyclic graphs (DAGs). We provide a novel mixed-integer quadratic programming formulation and an associated algorithm that identifies DAGs with a low structural Hamming distance between the identified DAG and the ground truth, under identifiability assumptions. The eventual exact learning is guaranteed by the global convergence of the branch-and-bound-and-cut algorithm, which is utilized. In addition to this, integer programming techniques give us access to the dual bound, which allows for a real time assessment of the quality of solution. Previously, integer programming techniques have been shown to lead to limited scaling in the case of DAG identification due to the super exponential number of constraints, which prevent the formation of cycles. The algorithm proposed circumvents this by selectively generating only the violated constraints using the so-called \"lazy\" constraints methodology. Our empirical results show that ExDAG outperforms state-of-the-art solvers in terms of structural Hamming distance and $F_1$ score when considering Gaussian noise on medium-sized graphs.", "link": "http://arxiv.org/abs/2406.15229v2", "date": "2025-11-19", "relevancy": 2.3512, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4762}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4674}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExDAG%3A%20an%20MIQP%20Algorithm%20for%20Learning%20DAGs&body=Title%3A%20ExDAG%3A%20an%20MIQP%20Algorithm%20for%20Learning%20DAGs%0AAuthor%3A%20Pavel%20Rytir%20and%20Ales%20Wodecki%20and%20Jakub%20Marecek%0AAbstract%3A%20There%20has%20been%20a%20growing%20interest%20in%20causal%20learning%20in%20recent%20years.%20Commonly%20used%20representations%20of%20causal%20structures%2C%20including%20Bayesian%20networks%20and%20structural%20equation%20models%20%28SEM%29%2C%20take%20the%20form%20of%20directed%20acyclic%20graphs%20%28DAGs%29.%20We%20provide%20a%20novel%20mixed-integer%20quadratic%20programming%20formulation%20and%20an%20associated%20algorithm%20that%20identifies%20DAGs%20with%20a%20low%20structural%20Hamming%20distance%20between%20the%20identified%20DAG%20and%20the%20ground%20truth%2C%20under%20identifiability%20assumptions.%20The%20eventual%20exact%20learning%20is%20guaranteed%20by%20the%20global%20convergence%20of%20the%20branch-and-bound-and-cut%20algorithm%2C%20which%20is%20utilized.%20In%20addition%20to%20this%2C%20integer%20programming%20techniques%20give%20us%20access%20to%20the%20dual%20bound%2C%20which%20allows%20for%20a%20real%20time%20assessment%20of%20the%20quality%20of%20solution.%20Previously%2C%20integer%20programming%20techniques%20have%20been%20shown%20to%20lead%20to%20limited%20scaling%20in%20the%20case%20of%20DAG%20identification%20due%20to%20the%20super%20exponential%20number%20of%20constraints%2C%20which%20prevent%20the%20formation%20of%20cycles.%20The%20algorithm%20proposed%20circumvents%20this%20by%20selectively%20generating%20only%20the%20violated%20constraints%20using%20the%20so-called%20%22lazy%22%20constraints%20methodology.%20Our%20empirical%20results%20show%20that%20ExDAG%20outperforms%20state-of-the-art%20solvers%20in%20terms%20of%20structural%20Hamming%20distance%20and%20%24F_1%24%20score%20when%20considering%20Gaussian%20noise%20on%20medium-sized%20graphs.%0ALink%3A%20http%3A//arxiv.org/abs/2406.15229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExDAG%253A%2520an%2520MIQP%2520Algorithm%2520for%2520Learning%2520DAGs%26entry.906535625%3DPavel%2520Rytir%2520and%2520Ales%2520Wodecki%2520and%2520Jakub%2520Marecek%26entry.1292438233%3DThere%2520has%2520been%2520a%2520growing%2520interest%2520in%2520causal%2520learning%2520in%2520recent%2520years.%2520Commonly%2520used%2520representations%2520of%2520causal%2520structures%252C%2520including%2520Bayesian%2520networks%2520and%2520structural%2520equation%2520models%2520%2528SEM%2529%252C%2520take%2520the%2520form%2520of%2520directed%2520acyclic%2520graphs%2520%2528DAGs%2529.%2520We%2520provide%2520a%2520novel%2520mixed-integer%2520quadratic%2520programming%2520formulation%2520and%2520an%2520associated%2520algorithm%2520that%2520identifies%2520DAGs%2520with%2520a%2520low%2520structural%2520Hamming%2520distance%2520between%2520the%2520identified%2520DAG%2520and%2520the%2520ground%2520truth%252C%2520under%2520identifiability%2520assumptions.%2520The%2520eventual%2520exact%2520learning%2520is%2520guaranteed%2520by%2520the%2520global%2520convergence%2520of%2520the%2520branch-and-bound-and-cut%2520algorithm%252C%2520which%2520is%2520utilized.%2520In%2520addition%2520to%2520this%252C%2520integer%2520programming%2520techniques%2520give%2520us%2520access%2520to%2520the%2520dual%2520bound%252C%2520which%2520allows%2520for%2520a%2520real%2520time%2520assessment%2520of%2520the%2520quality%2520of%2520solution.%2520Previously%252C%2520integer%2520programming%2520techniques%2520have%2520been%2520shown%2520to%2520lead%2520to%2520limited%2520scaling%2520in%2520the%2520case%2520of%2520DAG%2520identification%2520due%2520to%2520the%2520super%2520exponential%2520number%2520of%2520constraints%252C%2520which%2520prevent%2520the%2520formation%2520of%2520cycles.%2520The%2520algorithm%2520proposed%2520circumvents%2520this%2520by%2520selectively%2520generating%2520only%2520the%2520violated%2520constraints%2520using%2520the%2520so-called%2520%2522lazy%2522%2520constraints%2520methodology.%2520Our%2520empirical%2520results%2520show%2520that%2520ExDAG%2520outperforms%2520state-of-the-art%2520solvers%2520in%2520terms%2520of%2520structural%2520Hamming%2520distance%2520and%2520%2524F_1%2524%2520score%2520when%2520considering%2520Gaussian%2520noise%2520on%2520medium-sized%2520graphs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExDAG%3A%20an%20MIQP%20Algorithm%20for%20Learning%20DAGs&entry.906535625=Pavel%20Rytir%20and%20Ales%20Wodecki%20and%20Jakub%20Marecek&entry.1292438233=There%20has%20been%20a%20growing%20interest%20in%20causal%20learning%20in%20recent%20years.%20Commonly%20used%20representations%20of%20causal%20structures%2C%20including%20Bayesian%20networks%20and%20structural%20equation%20models%20%28SEM%29%2C%20take%20the%20form%20of%20directed%20acyclic%20graphs%20%28DAGs%29.%20We%20provide%20a%20novel%20mixed-integer%20quadratic%20programming%20formulation%20and%20an%20associated%20algorithm%20that%20identifies%20DAGs%20with%20a%20low%20structural%20Hamming%20distance%20between%20the%20identified%20DAG%20and%20the%20ground%20truth%2C%20under%20identifiability%20assumptions.%20The%20eventual%20exact%20learning%20is%20guaranteed%20by%20the%20global%20convergence%20of%20the%20branch-and-bound-and-cut%20algorithm%2C%20which%20is%20utilized.%20In%20addition%20to%20this%2C%20integer%20programming%20techniques%20give%20us%20access%20to%20the%20dual%20bound%2C%20which%20allows%20for%20a%20real%20time%20assessment%20of%20the%20quality%20of%20solution.%20Previously%2C%20integer%20programming%20techniques%20have%20been%20shown%20to%20lead%20to%20limited%20scaling%20in%20the%20case%20of%20DAG%20identification%20due%20to%20the%20super%20exponential%20number%20of%20constraints%2C%20which%20prevent%20the%20formation%20of%20cycles.%20The%20algorithm%20proposed%20circumvents%20this%20by%20selectively%20generating%20only%20the%20violated%20constraints%20using%20the%20so-called%20%22lazy%22%20constraints%20methodology.%20Our%20empirical%20results%20show%20that%20ExDAG%20outperforms%20state-of-the-art%20solvers%20in%20terms%20of%20structural%20Hamming%20distance%20and%20%24F_1%24%20score%20when%20considering%20Gaussian%20noise%20on%20medium-sized%20graphs.&entry.1838667208=http%3A//arxiv.org/abs/2406.15229v2&entry.124074799=Read"},
{"title": "Scriboora: Rethinking Human Pose Forecasting", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.", "link": "http://arxiv.org/abs/2511.15565v1", "date": "2025-11-19", "relevancy": 2.3142, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scriboora%3A%20Rethinking%20Human%20Pose%20Forecasting&body=Title%3A%20Scriboora%3A%20Rethinking%20Human%20Pose%20Forecasting%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20Human%20pose%20forecasting%20predicts%20future%20poses%20based%20on%20past%20observations%2C%20and%20has%20many%20significant%20applications%20in%20areas%20such%20as%20action%20recognition%2C%20autonomous%20driving%20or%20human-robot%20interaction.%20This%20paper%20evaluates%20a%20wide%20range%20of%20pose%20forecasting%20algorithms%20in%20the%20task%20of%20absolute%20pose%20forecasting%2C%20revealing%20many%20reproducibility%20issues%2C%20and%20provides%20a%20unified%20training%20and%20evaluation%20pipeline.%20After%20drawing%20a%20high-level%20analogy%20to%20the%20task%20of%20speech%20understanding%2C%20it%20is%20shown%20that%20recent%20speech%20models%20can%20be%20efficiently%20adapted%20to%20the%20task%20of%20pose%20forecasting%2C%20and%20improve%20current%20state-of-the-art%20performance.%20At%20last%20the%20robustness%20of%20the%20models%20is%20evaluated%2C%20using%20noisy%20joint%20coordinates%20obtained%20from%20a%20pose%20estimator%20model%2C%20to%20reflect%20a%20realistic%20type%20of%20noise%2C%20which%20is%20more%20close%20to%20real-world%20applications.%20For%20this%20a%20new%20dataset%20variation%20is%20introduced%2C%20and%20it%20is%20shown%20that%20estimated%20poses%20result%20in%20a%20substantial%20performance%20degradation%2C%20and%20how%20much%20of%20it%20can%20be%20recovered%20again%20by%20unsupervised%20finetuning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScriboora%253A%2520Rethinking%2520Human%2520Pose%2520Forecasting%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3DHuman%2520pose%2520forecasting%2520predicts%2520future%2520poses%2520based%2520on%2520past%2520observations%252C%2520and%2520has%2520many%2520significant%2520applications%2520in%2520areas%2520such%2520as%2520action%2520recognition%252C%2520autonomous%2520driving%2520or%2520human-robot%2520interaction.%2520This%2520paper%2520evaluates%2520a%2520wide%2520range%2520of%2520pose%2520forecasting%2520algorithms%2520in%2520the%2520task%2520of%2520absolute%2520pose%2520forecasting%252C%2520revealing%2520many%2520reproducibility%2520issues%252C%2520and%2520provides%2520a%2520unified%2520training%2520and%2520evaluation%2520pipeline.%2520After%2520drawing%2520a%2520high-level%2520analogy%2520to%2520the%2520task%2520of%2520speech%2520understanding%252C%2520it%2520is%2520shown%2520that%2520recent%2520speech%2520models%2520can%2520be%2520efficiently%2520adapted%2520to%2520the%2520task%2520of%2520pose%2520forecasting%252C%2520and%2520improve%2520current%2520state-of-the-art%2520performance.%2520At%2520last%2520the%2520robustness%2520of%2520the%2520models%2520is%2520evaluated%252C%2520using%2520noisy%2520joint%2520coordinates%2520obtained%2520from%2520a%2520pose%2520estimator%2520model%252C%2520to%2520reflect%2520a%2520realistic%2520type%2520of%2520noise%252C%2520which%2520is%2520more%2520close%2520to%2520real-world%2520applications.%2520For%2520this%2520a%2520new%2520dataset%2520variation%2520is%2520introduced%252C%2520and%2520it%2520is%2520shown%2520that%2520estimated%2520poses%2520result%2520in%2520a%2520substantial%2520performance%2520degradation%252C%2520and%2520how%2520much%2520of%2520it%2520can%2520be%2520recovered%2520again%2520by%2520unsupervised%2520finetuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scriboora%3A%20Rethinking%20Human%20Pose%20Forecasting&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=Human%20pose%20forecasting%20predicts%20future%20poses%20based%20on%20past%20observations%2C%20and%20has%20many%20significant%20applications%20in%20areas%20such%20as%20action%20recognition%2C%20autonomous%20driving%20or%20human-robot%20interaction.%20This%20paper%20evaluates%20a%20wide%20range%20of%20pose%20forecasting%20algorithms%20in%20the%20task%20of%20absolute%20pose%20forecasting%2C%20revealing%20many%20reproducibility%20issues%2C%20and%20provides%20a%20unified%20training%20and%20evaluation%20pipeline.%20After%20drawing%20a%20high-level%20analogy%20to%20the%20task%20of%20speech%20understanding%2C%20it%20is%20shown%20that%20recent%20speech%20models%20can%20be%20efficiently%20adapted%20to%20the%20task%20of%20pose%20forecasting%2C%20and%20improve%20current%20state-of-the-art%20performance.%20At%20last%20the%20robustness%20of%20the%20models%20is%20evaluated%2C%20using%20noisy%20joint%20coordinates%20obtained%20from%20a%20pose%20estimator%20model%2C%20to%20reflect%20a%20realistic%20type%20of%20noise%2C%20which%20is%20more%20close%20to%20real-world%20applications.%20For%20this%20a%20new%20dataset%20variation%20is%20introduced%2C%20and%20it%20is%20shown%20that%20estimated%20poses%20result%20in%20a%20substantial%20performance%20degradation%2C%20and%20how%20much%20of%20it%20can%20be%20recovered%20again%20by%20unsupervised%20finetuning.&entry.1838667208=http%3A//arxiv.org/abs/2511.15565v1&entry.124074799=Read"},
{"title": "ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation", "author": "Simon Boeder and Fabian Gigengack and Simon Roesler and Holger Caesar and Benjamin Risse", "abstract": "Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.", "link": "http://arxiv.org/abs/2511.15396v1", "date": "2025-11-19", "relevancy": 2.3105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShelfOcc%3A%20Native%203D%20Supervision%20beyond%20LiDAR%20for%20Vision-Based%20Occupancy%20Estimation&body=Title%3A%20ShelfOcc%3A%20Native%203D%20Supervision%20beyond%20LiDAR%20for%20Vision-Based%20Occupancy%20Estimation%0AAuthor%3A%20Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Simon%20Roesler%20and%20Holger%20Caesar%20and%20Benjamin%20Risse%0AAbstract%3A%20Recent%20progress%20in%20self-%20and%20weakly%20supervised%20occupancy%20estimation%20has%20largely%20relied%20on%202D%20projection%20or%20rendering-based%20supervision%2C%20which%20suffers%20from%20geometric%20inconsistencies%20and%20severe%20depth%20bleeding.%20We%20thus%20introduce%20ShelfOcc%2C%20a%20vision-only%20method%20that%20overcomes%20these%20limitations%20without%20relying%20on%20LiDAR.%20ShelfOcc%20brings%20supervision%20into%20native%203D%20space%20by%20generating%20metrically%20consistent%20semantic%20voxel%20labels%20from%20video%2C%20enabling%20true%203D%20supervision%20without%20any%20additional%20sensors%20or%20manual%203D%20annotations.%20While%20recent%20vision-based%203D%20geometry%20foundation%20models%20provide%20a%20promising%20source%20of%20prior%20knowledge%2C%20they%20do%20not%20work%20out%20of%20the%20box%20as%20a%20prediction%20due%20to%20sparse%20or%20noisy%20and%20inconsistent%20geometry%2C%20especially%20in%20dynamic%20driving%20scenes.%20Our%20method%20introduces%20a%20dedicated%20framework%20that%20mitigates%20these%20issues%20by%20filtering%20and%20accumulating%20static%20geometry%20consistently%20across%20frames%2C%20handling%20dynamic%20content%20and%20propagating%20semantic%20information%20into%20a%20stable%20voxel%20representation.%20This%20data-centric%20shift%20in%20supervision%20for%20weakly/shelf-supervised%20occupancy%20estimation%20allows%20the%20use%20of%20essentially%20any%20SOTA%20occupancy%20model%20architecture%20without%20relying%20on%20LiDAR%20data.%20We%20argue%20that%20such%20high-quality%20supervision%20is%20essential%20for%20robust%20occupancy%20learning%20and%20constitutes%20an%20important%20complementary%20avenue%20to%20architectural%20innovation.%20On%20the%20Occ3D-nuScenes%20benchmark%2C%20ShelfOcc%20substantially%20outperforms%20all%20previous%20weakly/shelf-supervised%20methods%20%28up%20to%20a%2034%25%20relative%20improvement%29%2C%20establishing%20a%20new%20data-driven%20direction%20for%20LiDAR-free%203D%20scene%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShelfOcc%253A%2520Native%25203D%2520Supervision%2520beyond%2520LiDAR%2520for%2520Vision-Based%2520Occupancy%2520Estimation%26entry.906535625%3DSimon%2520Boeder%2520and%2520Fabian%2520Gigengack%2520and%2520Simon%2520Roesler%2520and%2520Holger%2520Caesar%2520and%2520Benjamin%2520Risse%26entry.1292438233%3DRecent%2520progress%2520in%2520self-%2520and%2520weakly%2520supervised%2520occupancy%2520estimation%2520has%2520largely%2520relied%2520on%25202D%2520projection%2520or%2520rendering-based%2520supervision%252C%2520which%2520suffers%2520from%2520geometric%2520inconsistencies%2520and%2520severe%2520depth%2520bleeding.%2520We%2520thus%2520introduce%2520ShelfOcc%252C%2520a%2520vision-only%2520method%2520that%2520overcomes%2520these%2520limitations%2520without%2520relying%2520on%2520LiDAR.%2520ShelfOcc%2520brings%2520supervision%2520into%2520native%25203D%2520space%2520by%2520generating%2520metrically%2520consistent%2520semantic%2520voxel%2520labels%2520from%2520video%252C%2520enabling%2520true%25203D%2520supervision%2520without%2520any%2520additional%2520sensors%2520or%2520manual%25203D%2520annotations.%2520While%2520recent%2520vision-based%25203D%2520geometry%2520foundation%2520models%2520provide%2520a%2520promising%2520source%2520of%2520prior%2520knowledge%252C%2520they%2520do%2520not%2520work%2520out%2520of%2520the%2520box%2520as%2520a%2520prediction%2520due%2520to%2520sparse%2520or%2520noisy%2520and%2520inconsistent%2520geometry%252C%2520especially%2520in%2520dynamic%2520driving%2520scenes.%2520Our%2520method%2520introduces%2520a%2520dedicated%2520framework%2520that%2520mitigates%2520these%2520issues%2520by%2520filtering%2520and%2520accumulating%2520static%2520geometry%2520consistently%2520across%2520frames%252C%2520handling%2520dynamic%2520content%2520and%2520propagating%2520semantic%2520information%2520into%2520a%2520stable%2520voxel%2520representation.%2520This%2520data-centric%2520shift%2520in%2520supervision%2520for%2520weakly/shelf-supervised%2520occupancy%2520estimation%2520allows%2520the%2520use%2520of%2520essentially%2520any%2520SOTA%2520occupancy%2520model%2520architecture%2520without%2520relying%2520on%2520LiDAR%2520data.%2520We%2520argue%2520that%2520such%2520high-quality%2520supervision%2520is%2520essential%2520for%2520robust%2520occupancy%2520learning%2520and%2520constitutes%2520an%2520important%2520complementary%2520avenue%2520to%2520architectural%2520innovation.%2520On%2520the%2520Occ3D-nuScenes%2520benchmark%252C%2520ShelfOcc%2520substantially%2520outperforms%2520all%2520previous%2520weakly/shelf-supervised%2520methods%2520%2528up%2520to%2520a%252034%2525%2520relative%2520improvement%2529%252C%2520establishing%2520a%2520new%2520data-driven%2520direction%2520for%2520LiDAR-free%25203D%2520scene%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShelfOcc%3A%20Native%203D%20Supervision%20beyond%20LiDAR%20for%20Vision-Based%20Occupancy%20Estimation&entry.906535625=Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Simon%20Roesler%20and%20Holger%20Caesar%20and%20Benjamin%20Risse&entry.1292438233=Recent%20progress%20in%20self-%20and%20weakly%20supervised%20occupancy%20estimation%20has%20largely%20relied%20on%202D%20projection%20or%20rendering-based%20supervision%2C%20which%20suffers%20from%20geometric%20inconsistencies%20and%20severe%20depth%20bleeding.%20We%20thus%20introduce%20ShelfOcc%2C%20a%20vision-only%20method%20that%20overcomes%20these%20limitations%20without%20relying%20on%20LiDAR.%20ShelfOcc%20brings%20supervision%20into%20native%203D%20space%20by%20generating%20metrically%20consistent%20semantic%20voxel%20labels%20from%20video%2C%20enabling%20true%203D%20supervision%20without%20any%20additional%20sensors%20or%20manual%203D%20annotations.%20While%20recent%20vision-based%203D%20geometry%20foundation%20models%20provide%20a%20promising%20source%20of%20prior%20knowledge%2C%20they%20do%20not%20work%20out%20of%20the%20box%20as%20a%20prediction%20due%20to%20sparse%20or%20noisy%20and%20inconsistent%20geometry%2C%20especially%20in%20dynamic%20driving%20scenes.%20Our%20method%20introduces%20a%20dedicated%20framework%20that%20mitigates%20these%20issues%20by%20filtering%20and%20accumulating%20static%20geometry%20consistently%20across%20frames%2C%20handling%20dynamic%20content%20and%20propagating%20semantic%20information%20into%20a%20stable%20voxel%20representation.%20This%20data-centric%20shift%20in%20supervision%20for%20weakly/shelf-supervised%20occupancy%20estimation%20allows%20the%20use%20of%20essentially%20any%20SOTA%20occupancy%20model%20architecture%20without%20relying%20on%20LiDAR%20data.%20We%20argue%20that%20such%20high-quality%20supervision%20is%20essential%20for%20robust%20occupancy%20learning%20and%20constitutes%20an%20important%20complementary%20avenue%20to%20architectural%20innovation.%20On%20the%20Occ3D-nuScenes%20benchmark%2C%20ShelfOcc%20substantially%20outperforms%20all%20previous%20weakly/shelf-supervised%20methods%20%28up%20to%20a%2034%25%20relative%20improvement%29%2C%20establishing%20a%20new%20data-driven%20direction%20for%20LiDAR-free%203D%20scene%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2511.15396v1&entry.124074799=Read"},
{"title": "Importance Ranking in Complex Networks via Influence-aware Causal Node Embedding", "author": "Jiahui Gao and Kuang Zhou and Yuchen Zhu and Keyu Wu", "abstract": "Understanding and quantifying node importance is a fundamental problem in network science and engineering, underpinning a wide range of applications such as influence maximization, social recommendation, and network dismantling. Prior research often relies on centrality measures or advanced graph embedding techniques using structural information, followed by downstream classification or regression tasks to identify critical nodes. However, these methods typically decouple node representation learning from the ranking objective and rely on the topological structure of target networks, leading to feature-task inconsistency and limited generalization across networks. This paper proposes a novel framework that leverages causal representation learning to get robust, invariant node embeddings for cross-network ranking tasks. Firstly, we introduce an influence-aware causal node embedding module within an autoencoder architecture to extract node embeddings that are causally related to node importance. Moreover, we introduce a causal ranking loss and design a unified optimization framework that jointly optimizes the reconstruction and ranking objectives, enabling mutual reinforcement between node representation learning and ranking optimization. This design allows the proposed model to be trained on synthetic networks and to generalize effectively across diverse real-world networks. Extensive experiments on multiple benchmark datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines in terms of both ranking accuracy and cross-network transferability, offering new insights for network analysis and engineering applications-particularly in scenarios where the target network's structure is inaccessible in advance due to privacy or security constraints.", "link": "http://arxiv.org/abs/2511.01228v2", "date": "2025-11-19", "relevancy": 2.301, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4669}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Importance%20Ranking%20in%20Complex%20Networks%20via%20Influence-aware%20Causal%20Node%20Embedding&body=Title%3A%20Importance%20Ranking%20in%20Complex%20Networks%20via%20Influence-aware%20Causal%20Node%20Embedding%0AAuthor%3A%20Jiahui%20Gao%20and%20Kuang%20Zhou%20and%20Yuchen%20Zhu%20and%20Keyu%20Wu%0AAbstract%3A%20Understanding%20and%20quantifying%20node%20importance%20is%20a%20fundamental%20problem%20in%20network%20science%20and%20engineering%2C%20underpinning%20a%20wide%20range%20of%20applications%20such%20as%20influence%20maximization%2C%20social%20recommendation%2C%20and%20network%20dismantling.%20Prior%20research%20often%20relies%20on%20centrality%20measures%20or%20advanced%20graph%20embedding%20techniques%20using%20structural%20information%2C%20followed%20by%20downstream%20classification%20or%20regression%20tasks%20to%20identify%20critical%20nodes.%20However%2C%20these%20methods%20typically%20decouple%20node%20representation%20learning%20from%20the%20ranking%20objective%20and%20rely%20on%20the%20topological%20structure%20of%20target%20networks%2C%20leading%20to%20feature-task%20inconsistency%20and%20limited%20generalization%20across%20networks.%20This%20paper%20proposes%20a%20novel%20framework%20that%20leverages%20causal%20representation%20learning%20to%20get%20robust%2C%20invariant%20node%20embeddings%20for%20cross-network%20ranking%20tasks.%20Firstly%2C%20we%20introduce%20an%20influence-aware%20causal%20node%20embedding%20module%20within%20an%20autoencoder%20architecture%20to%20extract%20node%20embeddings%20that%20are%20causally%20related%20to%20node%20importance.%20Moreover%2C%20we%20introduce%20a%20causal%20ranking%20loss%20and%20design%20a%20unified%20optimization%20framework%20that%20jointly%20optimizes%20the%20reconstruction%20and%20ranking%20objectives%2C%20enabling%20mutual%20reinforcement%20between%20node%20representation%20learning%20and%20ranking%20optimization.%20This%20design%20allows%20the%20proposed%20model%20to%20be%20trained%20on%20synthetic%20networks%20and%20to%20generalize%20effectively%20across%20diverse%20real-world%20networks.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20model%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20terms%20of%20both%20ranking%20accuracy%20and%20cross-network%20transferability%2C%20offering%20new%20insights%20for%20network%20analysis%20and%20engineering%20applications-particularly%20in%20scenarios%20where%20the%20target%20network%27s%20structure%20is%20inaccessible%20in%20advance%20due%20to%20privacy%20or%20security%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImportance%2520Ranking%2520in%2520Complex%2520Networks%2520via%2520Influence-aware%2520Causal%2520Node%2520Embedding%26entry.906535625%3DJiahui%2520Gao%2520and%2520Kuang%2520Zhou%2520and%2520Yuchen%2520Zhu%2520and%2520Keyu%2520Wu%26entry.1292438233%3DUnderstanding%2520and%2520quantifying%2520node%2520importance%2520is%2520a%2520fundamental%2520problem%2520in%2520network%2520science%2520and%2520engineering%252C%2520underpinning%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%2520influence%2520maximization%252C%2520social%2520recommendation%252C%2520and%2520network%2520dismantling.%2520Prior%2520research%2520often%2520relies%2520on%2520centrality%2520measures%2520or%2520advanced%2520graph%2520embedding%2520techniques%2520using%2520structural%2520information%252C%2520followed%2520by%2520downstream%2520classification%2520or%2520regression%2520tasks%2520to%2520identify%2520critical%2520nodes.%2520However%252C%2520these%2520methods%2520typically%2520decouple%2520node%2520representation%2520learning%2520from%2520the%2520ranking%2520objective%2520and%2520rely%2520on%2520the%2520topological%2520structure%2520of%2520target%2520networks%252C%2520leading%2520to%2520feature-task%2520inconsistency%2520and%2520limited%2520generalization%2520across%2520networks.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%2520that%2520leverages%2520causal%2520representation%2520learning%2520to%2520get%2520robust%252C%2520invariant%2520node%2520embeddings%2520for%2520cross-network%2520ranking%2520tasks.%2520Firstly%252C%2520we%2520introduce%2520an%2520influence-aware%2520causal%2520node%2520embedding%2520module%2520within%2520an%2520autoencoder%2520architecture%2520to%2520extract%2520node%2520embeddings%2520that%2520are%2520causally%2520related%2520to%2520node%2520importance.%2520Moreover%252C%2520we%2520introduce%2520a%2520causal%2520ranking%2520loss%2520and%2520design%2520a%2520unified%2520optimization%2520framework%2520that%2520jointly%2520optimizes%2520the%2520reconstruction%2520and%2520ranking%2520objectives%252C%2520enabling%2520mutual%2520reinforcement%2520between%2520node%2520representation%2520learning%2520and%2520ranking%2520optimization.%2520This%2520design%2520allows%2520the%2520proposed%2520model%2520to%2520be%2520trained%2520on%2520synthetic%2520networks%2520and%2520to%2520generalize%2520effectively%2520across%2520diverse%2520real-world%2520networks.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520model%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520terms%2520of%2520both%2520ranking%2520accuracy%2520and%2520cross-network%2520transferability%252C%2520offering%2520new%2520insights%2520for%2520network%2520analysis%2520and%2520engineering%2520applications-particularly%2520in%2520scenarios%2520where%2520the%2520target%2520network%2527s%2520structure%2520is%2520inaccessible%2520in%2520advance%2520due%2520to%2520privacy%2520or%2520security%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Importance%20Ranking%20in%20Complex%20Networks%20via%20Influence-aware%20Causal%20Node%20Embedding&entry.906535625=Jiahui%20Gao%20and%20Kuang%20Zhou%20and%20Yuchen%20Zhu%20and%20Keyu%20Wu&entry.1292438233=Understanding%20and%20quantifying%20node%20importance%20is%20a%20fundamental%20problem%20in%20network%20science%20and%20engineering%2C%20underpinning%20a%20wide%20range%20of%20applications%20such%20as%20influence%20maximization%2C%20social%20recommendation%2C%20and%20network%20dismantling.%20Prior%20research%20often%20relies%20on%20centrality%20measures%20or%20advanced%20graph%20embedding%20techniques%20using%20structural%20information%2C%20followed%20by%20downstream%20classification%20or%20regression%20tasks%20to%20identify%20critical%20nodes.%20However%2C%20these%20methods%20typically%20decouple%20node%20representation%20learning%20from%20the%20ranking%20objective%20and%20rely%20on%20the%20topological%20structure%20of%20target%20networks%2C%20leading%20to%20feature-task%20inconsistency%20and%20limited%20generalization%20across%20networks.%20This%20paper%20proposes%20a%20novel%20framework%20that%20leverages%20causal%20representation%20learning%20to%20get%20robust%2C%20invariant%20node%20embeddings%20for%20cross-network%20ranking%20tasks.%20Firstly%2C%20we%20introduce%20an%20influence-aware%20causal%20node%20embedding%20module%20within%20an%20autoencoder%20architecture%20to%20extract%20node%20embeddings%20that%20are%20causally%20related%20to%20node%20importance.%20Moreover%2C%20we%20introduce%20a%20causal%20ranking%20loss%20and%20design%20a%20unified%20optimization%20framework%20that%20jointly%20optimizes%20the%20reconstruction%20and%20ranking%20objectives%2C%20enabling%20mutual%20reinforcement%20between%20node%20representation%20learning%20and%20ranking%20optimization.%20This%20design%20allows%20the%20proposed%20model%20to%20be%20trained%20on%20synthetic%20networks%20and%20to%20generalize%20effectively%20across%20diverse%20real-world%20networks.%20Extensive%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20model%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20terms%20of%20both%20ranking%20accuracy%20and%20cross-network%20transferability%2C%20offering%20new%20insights%20for%20network%20analysis%20and%20engineering%20applications-particularly%20in%20scenarios%20where%20the%20target%20network%27s%20structure%20is%20inaccessible%20in%20advance%20due%20to%20privacy%20or%20security%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2511.01228v2&entry.124074799=Read"},
{"title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning", "author": "Amir Rezaei Balef and Claire Vernade and Katharina Eggensperger", "abstract": "The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a challenging resource allocation problem in the field of AutoML. We propose MaxUCB, a max k-armed bandit method to trade off exploring different model classes and conducting hyperparameter optimization. MaxUCB is specifically designed for the light-tailed and bounded reward distributions arising in this setting and, thus, provides an efficient alternative compared to classic max k-armed bandit methods assuming heavy-tailed reward distributions. We theoretically and empirically evaluate our method on four standard AutoML benchmarks, demonstrating superior performance over prior approaches. We make our code and data available at https://github.com/amirbalef/CASH_with_Bandits", "link": "http://arxiv.org/abs/2505.05226v2", "date": "2025-11-19", "relevancy": 2.2992, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4844}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Put%20CASH%20on%20Bandits%3A%20A%20Max%20K-Armed%20Problem%20for%20Automated%20Machine%20Learning&body=Title%3A%20Put%20CASH%20on%20Bandits%3A%20A%20Max%20K-Armed%20Problem%20for%20Automated%20Machine%20Learning%0AAuthor%3A%20Amir%20Rezaei%20Balef%20and%20Claire%20Vernade%20and%20Katharina%20Eggensperger%0AAbstract%3A%20The%20Combined%20Algorithm%20Selection%20and%20Hyperparameter%20optimization%20%28CASH%29%20is%20a%20challenging%20resource%20allocation%20problem%20in%20the%20field%20of%20AutoML.%20We%20propose%20MaxUCB%2C%20a%20max%20k-armed%20bandit%20method%20to%20trade%20off%20exploring%20different%20model%20classes%20and%20conducting%20hyperparameter%20optimization.%20MaxUCB%20is%20specifically%20designed%20for%20the%20light-tailed%20and%20bounded%20reward%20distributions%20arising%20in%20this%20setting%20and%2C%20thus%2C%20provides%20an%20efficient%20alternative%20compared%20to%20classic%20max%20k-armed%20bandit%20methods%20assuming%20heavy-tailed%20reward%20distributions.%20We%20theoretically%20and%20empirically%20evaluate%20our%20method%20on%20four%20standard%20AutoML%20benchmarks%2C%20demonstrating%20superior%20performance%20over%20prior%20approaches.%20We%20make%20our%20code%20and%20data%20available%20at%20https%3A//github.com/amirbalef/CASH_with_Bandits%0ALink%3A%20http%3A//arxiv.org/abs/2505.05226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPut%2520CASH%2520on%2520Bandits%253A%2520A%2520Max%2520K-Armed%2520Problem%2520for%2520Automated%2520Machine%2520Learning%26entry.906535625%3DAmir%2520Rezaei%2520Balef%2520and%2520Claire%2520Vernade%2520and%2520Katharina%2520Eggensperger%26entry.1292438233%3DThe%2520Combined%2520Algorithm%2520Selection%2520and%2520Hyperparameter%2520optimization%2520%2528CASH%2529%2520is%2520a%2520challenging%2520resource%2520allocation%2520problem%2520in%2520the%2520field%2520of%2520AutoML.%2520We%2520propose%2520MaxUCB%252C%2520a%2520max%2520k-armed%2520bandit%2520method%2520to%2520trade%2520off%2520exploring%2520different%2520model%2520classes%2520and%2520conducting%2520hyperparameter%2520optimization.%2520MaxUCB%2520is%2520specifically%2520designed%2520for%2520the%2520light-tailed%2520and%2520bounded%2520reward%2520distributions%2520arising%2520in%2520this%2520setting%2520and%252C%2520thus%252C%2520provides%2520an%2520efficient%2520alternative%2520compared%2520to%2520classic%2520max%2520k-armed%2520bandit%2520methods%2520assuming%2520heavy-tailed%2520reward%2520distributions.%2520We%2520theoretically%2520and%2520empirically%2520evaluate%2520our%2520method%2520on%2520four%2520standard%2520AutoML%2520benchmarks%252C%2520demonstrating%2520superior%2520performance%2520over%2520prior%2520approaches.%2520We%2520make%2520our%2520code%2520and%2520data%2520available%2520at%2520https%253A//github.com/amirbalef/CASH_with_Bandits%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Put%20CASH%20on%20Bandits%3A%20A%20Max%20K-Armed%20Problem%20for%20Automated%20Machine%20Learning&entry.906535625=Amir%20Rezaei%20Balef%20and%20Claire%20Vernade%20and%20Katharina%20Eggensperger&entry.1292438233=The%20Combined%20Algorithm%20Selection%20and%20Hyperparameter%20optimization%20%28CASH%29%20is%20a%20challenging%20resource%20allocation%20problem%20in%20the%20field%20of%20AutoML.%20We%20propose%20MaxUCB%2C%20a%20max%20k-armed%20bandit%20method%20to%20trade%20off%20exploring%20different%20model%20classes%20and%20conducting%20hyperparameter%20optimization.%20MaxUCB%20is%20specifically%20designed%20for%20the%20light-tailed%20and%20bounded%20reward%20distributions%20arising%20in%20this%20setting%20and%2C%20thus%2C%20provides%20an%20efficient%20alternative%20compared%20to%20classic%20max%20k-armed%20bandit%20methods%20assuming%20heavy-tailed%20reward%20distributions.%20We%20theoretically%20and%20empirically%20evaluate%20our%20method%20on%20four%20standard%20AutoML%20benchmarks%2C%20demonstrating%20superior%20performance%20over%20prior%20approaches.%20We%20make%20our%20code%20and%20data%20available%20at%20https%3A//github.com/amirbalef/CASH_with_Bandits&entry.1838667208=http%3A//arxiv.org/abs/2505.05226v2&entry.124074799=Read"},
{"title": "FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning", "author": "Ouiame Marnissi and Hajar EL Hammouti and El Houcine Bergou", "abstract": "Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\\% compared to baseline strategies.", "link": "http://arxiv.org/abs/2511.15454v1", "date": "2025-11-19", "relevancy": 2.2958, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4884}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairEnergy%3A%20Contribution-Based%20Fairness%20meets%20Energy%20Efficiency%20in%20Federated%20Learning&body=Title%3A%20FairEnergy%3A%20Contribution-Based%20Fairness%20meets%20Energy%20Efficiency%20in%20Federated%20Learning%0AAuthor%3A%20Ouiame%20Marnissi%20and%20Hajar%20EL%20Hammouti%20and%20El%20Houcine%20Bergou%0AAbstract%3A%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20distributed%20devices%20while%20preserving%20data%20privacy.%20However%2C%20balancing%20energy%20efficiency%20and%20fair%20participation%20while%20ensuring%20high%20model%20accuracy%20remains%20challenging%20in%20wireless%20edge%20systems%20due%20to%20heterogeneous%20resources%2C%20unequal%20client%20contributions%2C%20and%20limited%20communication%20capacity.%20To%20address%20these%20challenges%2C%20we%20propose%20FairEnergy%2C%20a%20fairness-aware%20energy%20minimization%20framework%20that%20integrates%20a%20contribution%20score%20capturing%20both%20the%20magnitude%20of%20updates%20and%20their%20compression%20ratio%20into%20the%20joint%20optimization%20of%20device%20selection%2C%20bandwidth%20allocation%2C%20and%20compression%20level.%20The%20resulting%20mixed-integer%20non-convex%20problem%20is%20solved%20by%20relaxing%20binary%20selection%20variables%20and%20applying%20Lagrangian%20decomposition%20to%20handle%20global%20bandwidth%20coupling%2C%20followed%20by%20per-device%20subproblem%20optimization.%20Experiments%20on%20non-IID%20data%20show%20that%20FairEnergy%20achieves%20higher%20accuracy%20while%20reducing%20energy%20consumption%20by%20up%20to%2079%5C%25%20compared%20to%20baseline%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairEnergy%253A%2520Contribution-Based%2520Fairness%2520meets%2520Energy%2520Efficiency%2520in%2520Federated%2520Learning%26entry.906535625%3DOuiame%2520Marnissi%2520and%2520Hajar%2520EL%2520Hammouti%2520and%2520El%2520Houcine%2520Bergou%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%2520distributed%2520devices%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520balancing%2520energy%2520efficiency%2520and%2520fair%2520participation%2520while%2520ensuring%2520high%2520model%2520accuracy%2520remains%2520challenging%2520in%2520wireless%2520edge%2520systems%2520due%2520to%2520heterogeneous%2520resources%252C%2520unequal%2520client%2520contributions%252C%2520and%2520limited%2520communication%2520capacity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FairEnergy%252C%2520a%2520fairness-aware%2520energy%2520minimization%2520framework%2520that%2520integrates%2520a%2520contribution%2520score%2520capturing%2520both%2520the%2520magnitude%2520of%2520updates%2520and%2520their%2520compression%2520ratio%2520into%2520the%2520joint%2520optimization%2520of%2520device%2520selection%252C%2520bandwidth%2520allocation%252C%2520and%2520compression%2520level.%2520The%2520resulting%2520mixed-integer%2520non-convex%2520problem%2520is%2520solved%2520by%2520relaxing%2520binary%2520selection%2520variables%2520and%2520applying%2520Lagrangian%2520decomposition%2520to%2520handle%2520global%2520bandwidth%2520coupling%252C%2520followed%2520by%2520per-device%2520subproblem%2520optimization.%2520Experiments%2520on%2520non-IID%2520data%2520show%2520that%2520FairEnergy%2520achieves%2520higher%2520accuracy%2520while%2520reducing%2520energy%2520consumption%2520by%2520up%2520to%252079%255C%2525%2520compared%2520to%2520baseline%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairEnergy%3A%20Contribution-Based%20Fairness%20meets%20Energy%20Efficiency%20in%20Federated%20Learning&entry.906535625=Ouiame%20Marnissi%20and%20Hajar%20EL%20Hammouti%20and%20El%20Houcine%20Bergou&entry.1292438233=Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20distributed%20devices%20while%20preserving%20data%20privacy.%20However%2C%20balancing%20energy%20efficiency%20and%20fair%20participation%20while%20ensuring%20high%20model%20accuracy%20remains%20challenging%20in%20wireless%20edge%20systems%20due%20to%20heterogeneous%20resources%2C%20unequal%20client%20contributions%2C%20and%20limited%20communication%20capacity.%20To%20address%20these%20challenges%2C%20we%20propose%20FairEnergy%2C%20a%20fairness-aware%20energy%20minimization%20framework%20that%20integrates%20a%20contribution%20score%20capturing%20both%20the%20magnitude%20of%20updates%20and%20their%20compression%20ratio%20into%20the%20joint%20optimization%20of%20device%20selection%2C%20bandwidth%20allocation%2C%20and%20compression%20level.%20The%20resulting%20mixed-integer%20non-convex%20problem%20is%20solved%20by%20relaxing%20binary%20selection%20variables%20and%20applying%20Lagrangian%20decomposition%20to%20handle%20global%20bandwidth%20coupling%2C%20followed%20by%20per-device%20subproblem%20optimization.%20Experiments%20on%20non-IID%20data%20show%20that%20FairEnergy%20achieves%20higher%20accuracy%20while%20reducing%20energy%20consumption%20by%20up%20to%2079%5C%25%20compared%20to%20baseline%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2511.15454v1&entry.124074799=Read"},
{"title": "C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models", "author": "Nayoung Oh and Dohyun Kim and Junhyeong Bang and Rohan Paul and Daehyung Park", "abstract": "Space grounding refers to localizing a set of spatial references described in natural language instructions. Traditional methods often fail to account for complex reasoning -- such as distance, geometry, and inter-object relationships -- while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs. To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization. For the coarse estimation, we design a grid-based visual-grounding prompt with a propose-validate strategy, maximizing VLM's spatial understanding and yielding physically and semantically valid canonical region (i.e., ellipses). For the refinement, we locally adapt the region to surrounding environment without over-relaxed to free space. We construct a new space-grounding benchmark and compare C2F-Space with five state-of-the-art baselines using success rate and intersection-over-union. Our C2F-Space significantly outperforms all baselines. Our ablation study confirms the effectiveness of each module in the two-step process and their synergistic effect of the combined framework. We finally demonstrate the applicability of C2F-Space to simulated robotic pick-and-place tasks.", "link": "http://arxiv.org/abs/2511.15333v1", "date": "2025-11-19", "relevancy": 2.2909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2F-Space%3A%20Coarse-to-Fine%20Space%20Grounding%20for%20Spatial%20Instructions%20using%20Vision-Language%20Models&body=Title%3A%20C2F-Space%3A%20Coarse-to-Fine%20Space%20Grounding%20for%20Spatial%20Instructions%20using%20Vision-Language%20Models%0AAuthor%3A%20Nayoung%20Oh%20and%20Dohyun%20Kim%20and%20Junhyeong%20Bang%20and%20Rohan%20Paul%20and%20Daehyung%20Park%0AAbstract%3A%20Space%20grounding%20refers%20to%20localizing%20a%20set%20of%20spatial%20references%20described%20in%20natural%20language%20instructions.%20Traditional%20methods%20often%20fail%20to%20account%20for%20complex%20reasoning%20--%20such%20as%20distance%2C%20geometry%2C%20and%20inter-object%20relationships%20--%20while%20vision-language%20models%20%28VLMs%29%2C%20despite%20strong%20reasoning%20abilities%2C%20struggle%20to%20produce%20a%20fine-grained%20region%20of%20outputs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20C2F-Space%2C%20a%20novel%20coarse-to-fine%20space-grounding%20framework%20that%20%28i%29%20estimates%20an%20approximated%20yet%20spatially%20consistent%20region%20using%20a%20VLM%2C%20then%20%28ii%29%20refines%20the%20region%20to%20align%20with%20the%20local%20environment%20through%20superpixelization.%20For%20the%20coarse%20estimation%2C%20we%20design%20a%20grid-based%20visual-grounding%20prompt%20with%20a%20propose-validate%20strategy%2C%20maximizing%20VLM%27s%20spatial%20understanding%20and%20yielding%20physically%20and%20semantically%20valid%20canonical%20region%20%28i.e.%2C%20ellipses%29.%20For%20the%20refinement%2C%20we%20locally%20adapt%20the%20region%20to%20surrounding%20environment%20without%20over-relaxed%20to%20free%20space.%20We%20construct%20a%20new%20space-grounding%20benchmark%20and%20compare%20C2F-Space%20with%20five%20state-of-the-art%20baselines%20using%20success%20rate%20and%20intersection-over-union.%20Our%20C2F-Space%20significantly%20outperforms%20all%20baselines.%20Our%20ablation%20study%20confirms%20the%20effectiveness%20of%20each%20module%20in%20the%20two-step%20process%20and%20their%20synergistic%20effect%20of%20the%20combined%20framework.%20We%20finally%20demonstrate%20the%20applicability%20of%20C2F-Space%20to%20simulated%20robotic%20pick-and-place%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2F-Space%253A%2520Coarse-to-Fine%2520Space%2520Grounding%2520for%2520Spatial%2520Instructions%2520using%2520Vision-Language%2520Models%26entry.906535625%3DNayoung%2520Oh%2520and%2520Dohyun%2520Kim%2520and%2520Junhyeong%2520Bang%2520and%2520Rohan%2520Paul%2520and%2520Daehyung%2520Park%26entry.1292438233%3DSpace%2520grounding%2520refers%2520to%2520localizing%2520a%2520set%2520of%2520spatial%2520references%2520described%2520in%2520natural%2520language%2520instructions.%2520Traditional%2520methods%2520often%2520fail%2520to%2520account%2520for%2520complex%2520reasoning%2520--%2520such%2520as%2520distance%252C%2520geometry%252C%2520and%2520inter-object%2520relationships%2520--%2520while%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520despite%2520strong%2520reasoning%2520abilities%252C%2520struggle%2520to%2520produce%2520a%2520fine-grained%2520region%2520of%2520outputs.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520C2F-Space%252C%2520a%2520novel%2520coarse-to-fine%2520space-grounding%2520framework%2520that%2520%2528i%2529%2520estimates%2520an%2520approximated%2520yet%2520spatially%2520consistent%2520region%2520using%2520a%2520VLM%252C%2520then%2520%2528ii%2529%2520refines%2520the%2520region%2520to%2520align%2520with%2520the%2520local%2520environment%2520through%2520superpixelization.%2520For%2520the%2520coarse%2520estimation%252C%2520we%2520design%2520a%2520grid-based%2520visual-grounding%2520prompt%2520with%2520a%2520propose-validate%2520strategy%252C%2520maximizing%2520VLM%2527s%2520spatial%2520understanding%2520and%2520yielding%2520physically%2520and%2520semantically%2520valid%2520canonical%2520region%2520%2528i.e.%252C%2520ellipses%2529.%2520For%2520the%2520refinement%252C%2520we%2520locally%2520adapt%2520the%2520region%2520to%2520surrounding%2520environment%2520without%2520over-relaxed%2520to%2520free%2520space.%2520We%2520construct%2520a%2520new%2520space-grounding%2520benchmark%2520and%2520compare%2520C2F-Space%2520with%2520five%2520state-of-the-art%2520baselines%2520using%2520success%2520rate%2520and%2520intersection-over-union.%2520Our%2520C2F-Space%2520significantly%2520outperforms%2520all%2520baselines.%2520Our%2520ablation%2520study%2520confirms%2520the%2520effectiveness%2520of%2520each%2520module%2520in%2520the%2520two-step%2520process%2520and%2520their%2520synergistic%2520effect%2520of%2520the%2520combined%2520framework.%2520We%2520finally%2520demonstrate%2520the%2520applicability%2520of%2520C2F-Space%2520to%2520simulated%2520robotic%2520pick-and-place%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2F-Space%3A%20Coarse-to-Fine%20Space%20Grounding%20for%20Spatial%20Instructions%20using%20Vision-Language%20Models&entry.906535625=Nayoung%20Oh%20and%20Dohyun%20Kim%20and%20Junhyeong%20Bang%20and%20Rohan%20Paul%20and%20Daehyung%20Park&entry.1292438233=Space%20grounding%20refers%20to%20localizing%20a%20set%20of%20spatial%20references%20described%20in%20natural%20language%20instructions.%20Traditional%20methods%20often%20fail%20to%20account%20for%20complex%20reasoning%20--%20such%20as%20distance%2C%20geometry%2C%20and%20inter-object%20relationships%20--%20while%20vision-language%20models%20%28VLMs%29%2C%20despite%20strong%20reasoning%20abilities%2C%20struggle%20to%20produce%20a%20fine-grained%20region%20of%20outputs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20C2F-Space%2C%20a%20novel%20coarse-to-fine%20space-grounding%20framework%20that%20%28i%29%20estimates%20an%20approximated%20yet%20spatially%20consistent%20region%20using%20a%20VLM%2C%20then%20%28ii%29%20refines%20the%20region%20to%20align%20with%20the%20local%20environment%20through%20superpixelization.%20For%20the%20coarse%20estimation%2C%20we%20design%20a%20grid-based%20visual-grounding%20prompt%20with%20a%20propose-validate%20strategy%2C%20maximizing%20VLM%27s%20spatial%20understanding%20and%20yielding%20physically%20and%20semantically%20valid%20canonical%20region%20%28i.e.%2C%20ellipses%29.%20For%20the%20refinement%2C%20we%20locally%20adapt%20the%20region%20to%20surrounding%20environment%20without%20over-relaxed%20to%20free%20space.%20We%20construct%20a%20new%20space-grounding%20benchmark%20and%20compare%20C2F-Space%20with%20five%20state-of-the-art%20baselines%20using%20success%20rate%20and%20intersection-over-union.%20Our%20C2F-Space%20significantly%20outperforms%20all%20baselines.%20Our%20ablation%20study%20confirms%20the%20effectiveness%20of%20each%20module%20in%20the%20two-step%20process%20and%20their%20synergistic%20effect%20of%20the%20combined%20framework.%20We%20finally%20demonstrate%20the%20applicability%20of%20C2F-Space%20to%20simulated%20robotic%20pick-and-place%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.15333v1&entry.124074799=Read"},
{"title": "Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion", "author": "Evgeniia Vu and Andrei Boiarov and Dmitry Vetrov", "abstract": "Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce a novel framework for streaming gesture generation that extends Rolling Diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. Our framework is universally compatible with existing diffusion-based gesture generation model, transforming them into streaming methods capable of continuous generation without requiring post-processing. We evaluate our framework on ZEGGS and BEAT, strong benchmarks for real-world applicability. Applied to state-of-the-art baselines on both datasets, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time co-speech gesture synthesis. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that employs a ladder-based noise scheduling strategy to simultaneously denoise multiple frames. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 4x speedup with high visual fidelity and temporal coherence in our experiments. Comprehensive user studies further validate our framework ability to generate realistic, diverse gestures closely synchronized with the audio input.", "link": "http://arxiv.org/abs/2503.10488v3", "date": "2025-11-19", "relevancy": 2.2854, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.583}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5646}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%20Generation%20of%20Co-Speech%20Gestures%20via%20Accelerated%20Rolling%20Diffusion&body=Title%3A%20Streaming%20Generation%20of%20Co-Speech%20Gestures%20via%20Accelerated%20Rolling%20Diffusion%0AAuthor%3A%20Evgeniia%20Vu%20and%20Andrei%20Boiarov%20and%20Dmitry%20Vetrov%0AAbstract%3A%20Generating%20co-speech%20gestures%20in%20real%20time%20requires%20both%20temporal%20coherence%20and%20efficient%20sampling.%20We%20introduce%20a%20novel%20framework%20for%20streaming%20gesture%20generation%20that%20extends%20Rolling%20Diffusion%20models%20with%20structured%20progressive%20noise%20scheduling%2C%20enabling%20seamless%20long-sequence%20motion%20synthesis%20while%20preserving%20realism%20and%20diversity.%20Our%20framework%20is%20universally%20compatible%20with%20existing%20diffusion-based%20gesture%20generation%20model%2C%20transforming%20them%20into%20streaming%20methods%20capable%20of%20continuous%20generation%20without%20requiring%20post-processing.%20We%20evaluate%20our%20framework%20on%20ZEGGS%20and%20BEAT%2C%20strong%20benchmarks%20for%20real-world%20applicability.%20Applied%20to%20state-of-the-art%20baselines%20on%20both%20datasets%2C%20it%20consistently%20outperforms%20them%2C%20demonstrating%20its%20effectiveness%20as%20a%20generalizable%20and%20efficient%20solution%20for%20real-time%20co-speech%20gesture%20synthesis.%20We%20further%20propose%20Rolling%20Diffusion%20Ladder%20Acceleration%20%28RDLA%29%2C%20a%20new%20approach%20that%20employs%20a%20ladder-based%20noise%20scheduling%20strategy%20to%20simultaneously%20denoise%20multiple%20frames.%20This%20significantly%20improves%20sampling%20efficiency%20while%20maintaining%20motion%20consistency%2C%20achieving%20up%20to%20a%204x%20speedup%20with%20high%20visual%20fidelity%20and%20temporal%20coherence%20in%20our%20experiments.%20Comprehensive%20user%20studies%20further%20validate%20our%20framework%20ability%20to%20generate%20realistic%2C%20diverse%20gestures%20closely%20synchronized%20with%20the%20audio%20input.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10488v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%2520Generation%2520of%2520Co-Speech%2520Gestures%2520via%2520Accelerated%2520Rolling%2520Diffusion%26entry.906535625%3DEvgeniia%2520Vu%2520and%2520Andrei%2520Boiarov%2520and%2520Dmitry%2520Vetrov%26entry.1292438233%3DGenerating%2520co-speech%2520gestures%2520in%2520real%2520time%2520requires%2520both%2520temporal%2520coherence%2520and%2520efficient%2520sampling.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520streaming%2520gesture%2520generation%2520that%2520extends%2520Rolling%2520Diffusion%2520models%2520with%2520structured%2520progressive%2520noise%2520scheduling%252C%2520enabling%2520seamless%2520long-sequence%2520motion%2520synthesis%2520while%2520preserving%2520realism%2520and%2520diversity.%2520Our%2520framework%2520is%2520universally%2520compatible%2520with%2520existing%2520diffusion-based%2520gesture%2520generation%2520model%252C%2520transforming%2520them%2520into%2520streaming%2520methods%2520capable%2520of%2520continuous%2520generation%2520without%2520requiring%2520post-processing.%2520We%2520evaluate%2520our%2520framework%2520on%2520ZEGGS%2520and%2520BEAT%252C%2520strong%2520benchmarks%2520for%2520real-world%2520applicability.%2520Applied%2520to%2520state-of-the-art%2520baselines%2520on%2520both%2520datasets%252C%2520it%2520consistently%2520outperforms%2520them%252C%2520demonstrating%2520its%2520effectiveness%2520as%2520a%2520generalizable%2520and%2520efficient%2520solution%2520for%2520real-time%2520co-speech%2520gesture%2520synthesis.%2520We%2520further%2520propose%2520Rolling%2520Diffusion%2520Ladder%2520Acceleration%2520%2528RDLA%2529%252C%2520a%2520new%2520approach%2520that%2520employs%2520a%2520ladder-based%2520noise%2520scheduling%2520strategy%2520to%2520simultaneously%2520denoise%2520multiple%2520frames.%2520This%2520significantly%2520improves%2520sampling%2520efficiency%2520while%2520maintaining%2520motion%2520consistency%252C%2520achieving%2520up%2520to%2520a%25204x%2520speedup%2520with%2520high%2520visual%2520fidelity%2520and%2520temporal%2520coherence%2520in%2520our%2520experiments.%2520Comprehensive%2520user%2520studies%2520further%2520validate%2520our%2520framework%2520ability%2520to%2520generate%2520realistic%252C%2520diverse%2520gestures%2520closely%2520synchronized%2520with%2520the%2520audio%2520input.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10488v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%20Generation%20of%20Co-Speech%20Gestures%20via%20Accelerated%20Rolling%20Diffusion&entry.906535625=Evgeniia%20Vu%20and%20Andrei%20Boiarov%20and%20Dmitry%20Vetrov&entry.1292438233=Generating%20co-speech%20gestures%20in%20real%20time%20requires%20both%20temporal%20coherence%20and%20efficient%20sampling.%20We%20introduce%20a%20novel%20framework%20for%20streaming%20gesture%20generation%20that%20extends%20Rolling%20Diffusion%20models%20with%20structured%20progressive%20noise%20scheduling%2C%20enabling%20seamless%20long-sequence%20motion%20synthesis%20while%20preserving%20realism%20and%20diversity.%20Our%20framework%20is%20universally%20compatible%20with%20existing%20diffusion-based%20gesture%20generation%20model%2C%20transforming%20them%20into%20streaming%20methods%20capable%20of%20continuous%20generation%20without%20requiring%20post-processing.%20We%20evaluate%20our%20framework%20on%20ZEGGS%20and%20BEAT%2C%20strong%20benchmarks%20for%20real-world%20applicability.%20Applied%20to%20state-of-the-art%20baselines%20on%20both%20datasets%2C%20it%20consistently%20outperforms%20them%2C%20demonstrating%20its%20effectiveness%20as%20a%20generalizable%20and%20efficient%20solution%20for%20real-time%20co-speech%20gesture%20synthesis.%20We%20further%20propose%20Rolling%20Diffusion%20Ladder%20Acceleration%20%28RDLA%29%2C%20a%20new%20approach%20that%20employs%20a%20ladder-based%20noise%20scheduling%20strategy%20to%20simultaneously%20denoise%20multiple%20frames.%20This%20significantly%20improves%20sampling%20efficiency%20while%20maintaining%20motion%20consistency%2C%20achieving%20up%20to%20a%204x%20speedup%20with%20high%20visual%20fidelity%20and%20temporal%20coherence%20in%20our%20experiments.%20Comprehensive%20user%20studies%20further%20validate%20our%20framework%20ability%20to%20generate%20realistic%2C%20diverse%20gestures%20closely%20synchronized%20with%20the%20audio%20input.&entry.1838667208=http%3A//arxiv.org/abs/2503.10488v3&entry.124074799=Read"},
{"title": "Do Large Language Models (LLMs) Understand Chronology?", "author": "Pattaraphon Kenny Wongchamcharoen and Paul Glasserman", "abstract": "Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.", "link": "http://arxiv.org/abs/2511.14214v2", "date": "2025-11-19", "relevancy": 2.2852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20%28LLMs%29%20Understand%20Chronology%3F&body=Title%3A%20Do%20Large%20Language%20Models%20%28LLMs%29%20Understand%20Chronology%3F%0AAuthor%3A%20Pattaraphon%20Kenny%20Wongchamcharoen%20and%20Paul%20Glasserman%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20in%20finance%20and%20economics%2C%20where%20prompt-based%20attempts%20against%20look-ahead%20bias%20implicitly%20assume%20that%20models%20understand%20chronology.%20We%20test%20this%20fundamental%20question%20with%20a%20series%20of%20chronological%20ordering%20tasks%20with%20increasing%20complexities%20over%20facts%20the%20model%20already%20knows%20from%20pre-training.%20Our%20tasks%20cover%20%281%29%20chronological%20ordering%2C%20%282%29%20conditional%20sorting%20%28filter%2C%20then%20order%29%2C%20and%20%283%29%20anachronism%20detection.%20We%20evaluate%20GPT-4.1%2C%20Claude-3.7%20Sonnet%2C%20with%20and%20without%20Extended%20Thinking%20%28ET%29%2C%20and%20GPT-5%20across%20multiple%20reasoning-effort%20settings.%20Across%20models%2C%20Exact%20match%20rate%20drops%20sharply%20as%20sequences%20lengthen%20even%20while%20rank%20correlations%20stay%20high%20as%20LLMs%20largely%20preserve%20local%20order%20but%20struggle%20to%20maintain%20a%20single%20globally%20consistent%20timeline.%20In%20conditional%20sorting%2C%20most%20failures%20stem%20from%20the%20filtering%20step%20rather%20than%20the%20ordering%20step%2C%20but%20GPT-5%20and%20Claude-3.7%20Sonnet%20with%20Extended%20Thinking%20outshine%20normal%20models%20significantly.%20Lastly%2C%20anachronism%20detection%20is%20found%20to%20be%20the%20easiest%20task%20for%20the%20LLMs%20but%20performance%20still%20declines%20with%20increasingly%20overlapping%20timelines%20or%20entities.%20Overall%2C%20our%20main%20contribution%20is%20showing%20that%20allocating%20explicit%20reasoning%20budget%20helps%20with%20chronological%20ordering%20with%20GPT-5%20at%20medium/high%20reasoning%20effort%20achieving%20flawless%20ordering%20at%20all%20lengths%20and%20perfect%20conditional%20sorting%20%28both%20self-filtered%20and%20given-subset%29%2C%20whereas%20low/minimal%20effort%20degrades%20with%20longer%20lists%2C%20mirroring%20earlier%20models.%20Our%20findings%20delineate%20limits%20of%20current%20LLMs%20on%20chronological%20tasks%2C%20providing%20insights%20into%20task%20complexity%2C%20and%20demonstrate%20scenarios%20in%20which%20reasoning%20helps.%20These%20patterns%20are%20important%20for%20the%20real-time%20application%20of%20LLMs%20in%20finance.%20We%20release%20all%20code%20and%20evaluation%20templates%20to%20support%20full%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520Understand%2520Chronology%253F%26entry.906535625%3DPattaraphon%2520Kenny%2520Wongchamcharoen%2520and%2520Paul%2520Glasserman%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520finance%2520and%2520economics%252C%2520where%2520prompt-based%2520attempts%2520against%2520look-ahead%2520bias%2520implicitly%2520assume%2520that%2520models%2520understand%2520chronology.%2520We%2520test%2520this%2520fundamental%2520question%2520with%2520a%2520series%2520of%2520chronological%2520ordering%2520tasks%2520with%2520increasing%2520complexities%2520over%2520facts%2520the%2520model%2520already%2520knows%2520from%2520pre-training.%2520Our%2520tasks%2520cover%2520%25281%2529%2520chronological%2520ordering%252C%2520%25282%2529%2520conditional%2520sorting%2520%2528filter%252C%2520then%2520order%2529%252C%2520and%2520%25283%2529%2520anachronism%2520detection.%2520We%2520evaluate%2520GPT-4.1%252C%2520Claude-3.7%2520Sonnet%252C%2520with%2520and%2520without%2520Extended%2520Thinking%2520%2528ET%2529%252C%2520and%2520GPT-5%2520across%2520multiple%2520reasoning-effort%2520settings.%2520Across%2520models%252C%2520Exact%2520match%2520rate%2520drops%2520sharply%2520as%2520sequences%2520lengthen%2520even%2520while%2520rank%2520correlations%2520stay%2520high%2520as%2520LLMs%2520largely%2520preserve%2520local%2520order%2520but%2520struggle%2520to%2520maintain%2520a%2520single%2520globally%2520consistent%2520timeline.%2520In%2520conditional%2520sorting%252C%2520most%2520failures%2520stem%2520from%2520the%2520filtering%2520step%2520rather%2520than%2520the%2520ordering%2520step%252C%2520but%2520GPT-5%2520and%2520Claude-3.7%2520Sonnet%2520with%2520Extended%2520Thinking%2520outshine%2520normal%2520models%2520significantly.%2520Lastly%252C%2520anachronism%2520detection%2520is%2520found%2520to%2520be%2520the%2520easiest%2520task%2520for%2520the%2520LLMs%2520but%2520performance%2520still%2520declines%2520with%2520increasingly%2520overlapping%2520timelines%2520or%2520entities.%2520Overall%252C%2520our%2520main%2520contribution%2520is%2520showing%2520that%2520allocating%2520explicit%2520reasoning%2520budget%2520helps%2520with%2520chronological%2520ordering%2520with%2520GPT-5%2520at%2520medium/high%2520reasoning%2520effort%2520achieving%2520flawless%2520ordering%2520at%2520all%2520lengths%2520and%2520perfect%2520conditional%2520sorting%2520%2528both%2520self-filtered%2520and%2520given-subset%2529%252C%2520whereas%2520low/minimal%2520effort%2520degrades%2520with%2520longer%2520lists%252C%2520mirroring%2520earlier%2520models.%2520Our%2520findings%2520delineate%2520limits%2520of%2520current%2520LLMs%2520on%2520chronological%2520tasks%252C%2520providing%2520insights%2520into%2520task%2520complexity%252C%2520and%2520demonstrate%2520scenarios%2520in%2520which%2520reasoning%2520helps.%2520These%2520patterns%2520are%2520important%2520for%2520the%2520real-time%2520application%2520of%2520LLMs%2520in%2520finance.%2520We%2520release%2520all%2520code%2520and%2520evaluation%2520templates%2520to%2520support%2520full%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20%28LLMs%29%20Understand%20Chronology%3F&entry.906535625=Pattaraphon%20Kenny%20Wongchamcharoen%20and%20Paul%20Glasserman&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20in%20finance%20and%20economics%2C%20where%20prompt-based%20attempts%20against%20look-ahead%20bias%20implicitly%20assume%20that%20models%20understand%20chronology.%20We%20test%20this%20fundamental%20question%20with%20a%20series%20of%20chronological%20ordering%20tasks%20with%20increasing%20complexities%20over%20facts%20the%20model%20already%20knows%20from%20pre-training.%20Our%20tasks%20cover%20%281%29%20chronological%20ordering%2C%20%282%29%20conditional%20sorting%20%28filter%2C%20then%20order%29%2C%20and%20%283%29%20anachronism%20detection.%20We%20evaluate%20GPT-4.1%2C%20Claude-3.7%20Sonnet%2C%20with%20and%20without%20Extended%20Thinking%20%28ET%29%2C%20and%20GPT-5%20across%20multiple%20reasoning-effort%20settings.%20Across%20models%2C%20Exact%20match%20rate%20drops%20sharply%20as%20sequences%20lengthen%20even%20while%20rank%20correlations%20stay%20high%20as%20LLMs%20largely%20preserve%20local%20order%20but%20struggle%20to%20maintain%20a%20single%20globally%20consistent%20timeline.%20In%20conditional%20sorting%2C%20most%20failures%20stem%20from%20the%20filtering%20step%20rather%20than%20the%20ordering%20step%2C%20but%20GPT-5%20and%20Claude-3.7%20Sonnet%20with%20Extended%20Thinking%20outshine%20normal%20models%20significantly.%20Lastly%2C%20anachronism%20detection%20is%20found%20to%20be%20the%20easiest%20task%20for%20the%20LLMs%20but%20performance%20still%20declines%20with%20increasingly%20overlapping%20timelines%20or%20entities.%20Overall%2C%20our%20main%20contribution%20is%20showing%20that%20allocating%20explicit%20reasoning%20budget%20helps%20with%20chronological%20ordering%20with%20GPT-5%20at%20medium/high%20reasoning%20effort%20achieving%20flawless%20ordering%20at%20all%20lengths%20and%20perfect%20conditional%20sorting%20%28both%20self-filtered%20and%20given-subset%29%2C%20whereas%20low/minimal%20effort%20degrades%20with%20longer%20lists%2C%20mirroring%20earlier%20models.%20Our%20findings%20delineate%20limits%20of%20current%20LLMs%20on%20chronological%20tasks%2C%20providing%20insights%20into%20task%20complexity%2C%20and%20demonstrate%20scenarios%20in%20which%20reasoning%20helps.%20These%20patterns%20are%20important%20for%20the%20real-time%20application%20of%20LLMs%20in%20finance.%20We%20release%20all%20code%20and%20evaluation%20templates%20to%20support%20full%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2511.14214v2&entry.124074799=Read"},
{"title": "Self Pre-training with Topology- and Spatiality-aware Masked Autoencoders for 3D Medical Image Segmentation", "author": "Pengfei Gu and Huimin Li and Yejia Zhang and Chaoli Wang and Danny Z. Chen", "abstract": "Masked Autoencoders (MAEs) have been shown to be effective in pre-training Vision Transformers (ViTs) for natural and medical image analysis problems. By reconstructing missing pixel/voxel information in visible patches, a ViT encoder can aggregate contextual information for downstream tasks. But, existing MAE pre-training methods, which were specifically developed with the ViT architecture, lack the ability to capture geometric shape and spatial information, which is critical for medical image segmentation tasks. In this paper, we propose a novel extension of known MAEs for self pre-training (i.e., models pre-trained on the same target dataset) for 3D medical image segmentation. (1) We propose a new topological loss to preserve geometric shape information by computing topological signatures of both the input and reconstructed volumes, learning geometric shape information. (2) We introduce a pre-text task that predicts the positions of the centers and eight corners of 3D crops, enabling the MAE to aggregate spatial information. (3) We extend the MAE pre-training strategy to a hybrid state-of-the-art (SOTA) medical image segmentation architecture and co-pretrain it alongside the ViT. (4) We develop a fine-tuned model for downstream segmentation tasks by complementing the pre-trained ViT encoder with our pre-trained SOTA model. Extensive experiments on five public 3D segmentation datasets show the effectiveness of our new approach.", "link": "http://arxiv.org/abs/2406.10519v3", "date": "2025-11-19", "relevancy": 2.2806, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self%20Pre-training%20with%20Topology-%20and%20Spatiality-aware%20Masked%20Autoencoders%20for%203D%20Medical%20Image%20Segmentation&body=Title%3A%20Self%20Pre-training%20with%20Topology-%20and%20Spatiality-aware%20Masked%20Autoencoders%20for%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Pengfei%20Gu%20and%20Huimin%20Li%20and%20Yejia%20Zhang%20and%20Chaoli%20Wang%20and%20Danny%20Z.%20Chen%0AAbstract%3A%20Masked%20Autoencoders%20%28MAEs%29%20have%20been%20shown%20to%20be%20effective%20in%20pre-training%20Vision%20Transformers%20%28ViTs%29%20for%20natural%20and%20medical%20image%20analysis%20problems.%20By%20reconstructing%20missing%20pixel/voxel%20information%20in%20visible%20patches%2C%20a%20ViT%20encoder%20can%20aggregate%20contextual%20information%20for%20downstream%20tasks.%20But%2C%20existing%20MAE%20pre-training%20methods%2C%20which%20were%20specifically%20developed%20with%20the%20ViT%20architecture%2C%20lack%20the%20ability%20to%20capture%20geometric%20shape%20and%20spatial%20information%2C%20which%20is%20critical%20for%20medical%20image%20segmentation%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20extension%20of%20known%20MAEs%20for%20self%20pre-training%20%28i.e.%2C%20models%20pre-trained%20on%20the%20same%20target%20dataset%29%20for%203D%20medical%20image%20segmentation.%20%281%29%20We%20propose%20a%20new%20topological%20loss%20to%20preserve%20geometric%20shape%20information%20by%20computing%20topological%20signatures%20of%20both%20the%20input%20and%20reconstructed%20volumes%2C%20learning%20geometric%20shape%20information.%20%282%29%20We%20introduce%20a%20pre-text%20task%20that%20predicts%20the%20positions%20of%20the%20centers%20and%20eight%20corners%20of%203D%20crops%2C%20enabling%20the%20MAE%20to%20aggregate%20spatial%20information.%20%283%29%20We%20extend%20the%20MAE%20pre-training%20strategy%20to%20a%20hybrid%20state-of-the-art%20%28SOTA%29%20medical%20image%20segmentation%20architecture%20and%20co-pretrain%20it%20alongside%20the%20ViT.%20%284%29%20We%20develop%20a%20fine-tuned%20model%20for%20downstream%20segmentation%20tasks%20by%20complementing%20the%20pre-trained%20ViT%20encoder%20with%20our%20pre-trained%20SOTA%20model.%20Extensive%20experiments%20on%20five%20public%203D%20segmentation%20datasets%20show%20the%20effectiveness%20of%20our%20new%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2406.10519v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf%2520Pre-training%2520with%2520Topology-%2520and%2520Spatiality-aware%2520Masked%2520Autoencoders%2520for%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DPengfei%2520Gu%2520and%2520Huimin%2520Li%2520and%2520Yejia%2520Zhang%2520and%2520Chaoli%2520Wang%2520and%2520Danny%2520Z.%2520Chen%26entry.1292438233%3DMasked%2520Autoencoders%2520%2528MAEs%2529%2520have%2520been%2520shown%2520to%2520be%2520effective%2520in%2520pre-training%2520Vision%2520Transformers%2520%2528ViTs%2529%2520for%2520natural%2520and%2520medical%2520image%2520analysis%2520problems.%2520By%2520reconstructing%2520missing%2520pixel/voxel%2520information%2520in%2520visible%2520patches%252C%2520a%2520ViT%2520encoder%2520can%2520aggregate%2520contextual%2520information%2520for%2520downstream%2520tasks.%2520But%252C%2520existing%2520MAE%2520pre-training%2520methods%252C%2520which%2520were%2520specifically%2520developed%2520with%2520the%2520ViT%2520architecture%252C%2520lack%2520the%2520ability%2520to%2520capture%2520geometric%2520shape%2520and%2520spatial%2520information%252C%2520which%2520is%2520critical%2520for%2520medical%2520image%2520segmentation%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520extension%2520of%2520known%2520MAEs%2520for%2520self%2520pre-training%2520%2528i.e.%252C%2520models%2520pre-trained%2520on%2520the%2520same%2520target%2520dataset%2529%2520for%25203D%2520medical%2520image%2520segmentation.%2520%25281%2529%2520We%2520propose%2520a%2520new%2520topological%2520loss%2520to%2520preserve%2520geometric%2520shape%2520information%2520by%2520computing%2520topological%2520signatures%2520of%2520both%2520the%2520input%2520and%2520reconstructed%2520volumes%252C%2520learning%2520geometric%2520shape%2520information.%2520%25282%2529%2520We%2520introduce%2520a%2520pre-text%2520task%2520that%2520predicts%2520the%2520positions%2520of%2520the%2520centers%2520and%2520eight%2520corners%2520of%25203D%2520crops%252C%2520enabling%2520the%2520MAE%2520to%2520aggregate%2520spatial%2520information.%2520%25283%2529%2520We%2520extend%2520the%2520MAE%2520pre-training%2520strategy%2520to%2520a%2520hybrid%2520state-of-the-art%2520%2528SOTA%2529%2520medical%2520image%2520segmentation%2520architecture%2520and%2520co-pretrain%2520it%2520alongside%2520the%2520ViT.%2520%25284%2529%2520We%2520develop%2520a%2520fine-tuned%2520model%2520for%2520downstream%2520segmentation%2520tasks%2520by%2520complementing%2520the%2520pre-trained%2520ViT%2520encoder%2520with%2520our%2520pre-trained%2520SOTA%2520model.%2520Extensive%2520experiments%2520on%2520five%2520public%25203D%2520segmentation%2520datasets%2520show%2520the%2520effectiveness%2520of%2520our%2520new%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10519v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self%20Pre-training%20with%20Topology-%20and%20Spatiality-aware%20Masked%20Autoencoders%20for%203D%20Medical%20Image%20Segmentation&entry.906535625=Pengfei%20Gu%20and%20Huimin%20Li%20and%20Yejia%20Zhang%20and%20Chaoli%20Wang%20and%20Danny%20Z.%20Chen&entry.1292438233=Masked%20Autoencoders%20%28MAEs%29%20have%20been%20shown%20to%20be%20effective%20in%20pre-training%20Vision%20Transformers%20%28ViTs%29%20for%20natural%20and%20medical%20image%20analysis%20problems.%20By%20reconstructing%20missing%20pixel/voxel%20information%20in%20visible%20patches%2C%20a%20ViT%20encoder%20can%20aggregate%20contextual%20information%20for%20downstream%20tasks.%20But%2C%20existing%20MAE%20pre-training%20methods%2C%20which%20were%20specifically%20developed%20with%20the%20ViT%20architecture%2C%20lack%20the%20ability%20to%20capture%20geometric%20shape%20and%20spatial%20information%2C%20which%20is%20critical%20for%20medical%20image%20segmentation%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20extension%20of%20known%20MAEs%20for%20self%20pre-training%20%28i.e.%2C%20models%20pre-trained%20on%20the%20same%20target%20dataset%29%20for%203D%20medical%20image%20segmentation.%20%281%29%20We%20propose%20a%20new%20topological%20loss%20to%20preserve%20geometric%20shape%20information%20by%20computing%20topological%20signatures%20of%20both%20the%20input%20and%20reconstructed%20volumes%2C%20learning%20geometric%20shape%20information.%20%282%29%20We%20introduce%20a%20pre-text%20task%20that%20predicts%20the%20positions%20of%20the%20centers%20and%20eight%20corners%20of%203D%20crops%2C%20enabling%20the%20MAE%20to%20aggregate%20spatial%20information.%20%283%29%20We%20extend%20the%20MAE%20pre-training%20strategy%20to%20a%20hybrid%20state-of-the-art%20%28SOTA%29%20medical%20image%20segmentation%20architecture%20and%20co-pretrain%20it%20alongside%20the%20ViT.%20%284%29%20We%20develop%20a%20fine-tuned%20model%20for%20downstream%20segmentation%20tasks%20by%20complementing%20the%20pre-trained%20ViT%20encoder%20with%20our%20pre-trained%20SOTA%20model.%20Extensive%20experiments%20on%20five%20public%203D%20segmentation%20datasets%20show%20the%20effectiveness%20of%20our%20new%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2406.10519v3&entry.124074799=Read"},
{"title": "Multi-Text Guided Few-Shot Semantic Segmentation", "author": "Qiang Jiao and Bin Yan and Yi Yang and Mengrui Shi and Qiang Zhang", "abstract": "Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.", "link": "http://arxiv.org/abs/2511.15515v1", "date": "2025-11-19", "relevancy": 2.2562, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6049}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Text%20Guided%20Few-Shot%20Semantic%20Segmentation&body=Title%3A%20Multi-Text%20Guided%20Few-Shot%20Semantic%20Segmentation%0AAuthor%3A%20Qiang%20Jiao%20and%20Bin%20Yan%20and%20Yi%20Yang%20and%20Mengrui%20Shi%20and%20Qiang%20Zhang%0AAbstract%3A%20Recent%20CLIP-based%20few-shot%20semantic%20segmentation%20methods%20introduce%20class-level%20textual%20priors%20to%20assist%20segmentation%20by%20typically%20using%20a%20single%20prompt%20%28e.g.%2C%20a%20photo%20of%20class%29.%20However%2C%20these%20approaches%20often%20result%20in%20incomplete%20activation%20of%20target%20regions%2C%20as%20a%20single%20textual%20description%20cannot%20fully%20capture%20the%20semantic%20diversity%20of%20complex%20categories.%20Moreover%2C%20they%20lack%20explicit%20cross-modal%20interaction%20and%20are%20vulnerable%20to%20noisy%20support%20features%2C%20further%20degrading%20visual%20prior%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Multi-Text%20Guided%20Few-Shot%20Semantic%20Segmentation%20Network%20%28MTGNet%29%2C%20a%20dual-branch%20framework%20that%20enhances%20segmentation%20performance%20by%20fusing%20diverse%20textual%20prompts%20to%20refine%20textual%20priors%20and%20guide%20the%20cross-modal%20optimization%20of%20visual%20priors.%20Specifically%2C%20we%20design%20a%20Multi-Textual%20Prior%20Refinement%20%28MTPR%29%20module%20that%20suppresses%20interference%20and%20aggregates%20complementary%20semantic%20cues%20to%20enhance%20foreground%20activation%20and%20expand%20semantic%20coverage%20for%20structurally%20complex%20objects.%20We%20introduce%20a%20Text%20Anchor%20Feature%20Fusion%20%28TAFF%29%20module%2C%20which%20leverages%20multi-text%20embeddings%20as%20semantic%20anchors%20to%20facilitate%20the%20transfer%20of%20discriminative%20local%20prototypes%20from%20support%20images%20to%20query%20images%2C%20thereby%20improving%20semantic%20consistency%20and%20alleviating%20intra-class%20variations.%20Furthermore%2C%20a%20Foreground%20Confidence-Weighted%20Attention%20%28FCWA%29%20module%20is%20presented%20to%20enhance%20visual%20prior%20robustness%20by%20leveraging%20internal%20self-similarity%20within%20support%20foreground%20features.%20It%20adaptively%20down-weights%20inconsistent%20regions%20and%20effectively%20suppresses%20interference%20in%20the%20query%20segmentation%20process.%20Extensive%20experiments%20on%20standard%20FSS%20benchmarks%20validate%20the%20effectiveness%20of%20MTGNet.%20In%20the%201-shot%20setting%2C%20it%20achieves%2076.8%25%20mIoU%20on%20PASCAL-5i%20and%2057.4%25%20on%20COCO-20i%2C%20with%20notable%20improvements%20in%20folds%20exhibiting%20high%20intra-class%20variations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Text%2520Guided%2520Few-Shot%2520Semantic%2520Segmentation%26entry.906535625%3DQiang%2520Jiao%2520and%2520Bin%2520Yan%2520and%2520Yi%2520Yang%2520and%2520Mengrui%2520Shi%2520and%2520Qiang%2520Zhang%26entry.1292438233%3DRecent%2520CLIP-based%2520few-shot%2520semantic%2520segmentation%2520methods%2520introduce%2520class-level%2520textual%2520priors%2520to%2520assist%2520segmentation%2520by%2520typically%2520using%2520a%2520single%2520prompt%2520%2528e.g.%252C%2520a%2520photo%2520of%2520class%2529.%2520However%252C%2520these%2520approaches%2520often%2520result%2520in%2520incomplete%2520activation%2520of%2520target%2520regions%252C%2520as%2520a%2520single%2520textual%2520description%2520cannot%2520fully%2520capture%2520the%2520semantic%2520diversity%2520of%2520complex%2520categories.%2520Moreover%252C%2520they%2520lack%2520explicit%2520cross-modal%2520interaction%2520and%2520are%2520vulnerable%2520to%2520noisy%2520support%2520features%252C%2520further%2520degrading%2520visual%2520prior%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520Multi-Text%2520Guided%2520Few-Shot%2520Semantic%2520Segmentation%2520Network%2520%2528MTGNet%2529%252C%2520a%2520dual-branch%2520framework%2520that%2520enhances%2520segmentation%2520performance%2520by%2520fusing%2520diverse%2520textual%2520prompts%2520to%2520refine%2520textual%2520priors%2520and%2520guide%2520the%2520cross-modal%2520optimization%2520of%2520visual%2520priors.%2520Specifically%252C%2520we%2520design%2520a%2520Multi-Textual%2520Prior%2520Refinement%2520%2528MTPR%2529%2520module%2520that%2520suppresses%2520interference%2520and%2520aggregates%2520complementary%2520semantic%2520cues%2520to%2520enhance%2520foreground%2520activation%2520and%2520expand%2520semantic%2520coverage%2520for%2520structurally%2520complex%2520objects.%2520We%2520introduce%2520a%2520Text%2520Anchor%2520Feature%2520Fusion%2520%2528TAFF%2529%2520module%252C%2520which%2520leverages%2520multi-text%2520embeddings%2520as%2520semantic%2520anchors%2520to%2520facilitate%2520the%2520transfer%2520of%2520discriminative%2520local%2520prototypes%2520from%2520support%2520images%2520to%2520query%2520images%252C%2520thereby%2520improving%2520semantic%2520consistency%2520and%2520alleviating%2520intra-class%2520variations.%2520Furthermore%252C%2520a%2520Foreground%2520Confidence-Weighted%2520Attention%2520%2528FCWA%2529%2520module%2520is%2520presented%2520to%2520enhance%2520visual%2520prior%2520robustness%2520by%2520leveraging%2520internal%2520self-similarity%2520within%2520support%2520foreground%2520features.%2520It%2520adaptively%2520down-weights%2520inconsistent%2520regions%2520and%2520effectively%2520suppresses%2520interference%2520in%2520the%2520query%2520segmentation%2520process.%2520Extensive%2520experiments%2520on%2520standard%2520FSS%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520MTGNet.%2520In%2520the%25201-shot%2520setting%252C%2520it%2520achieves%252076.8%2525%2520mIoU%2520on%2520PASCAL-5i%2520and%252057.4%2525%2520on%2520COCO-20i%252C%2520with%2520notable%2520improvements%2520in%2520folds%2520exhibiting%2520high%2520intra-class%2520variations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Text%20Guided%20Few-Shot%20Semantic%20Segmentation&entry.906535625=Qiang%20Jiao%20and%20Bin%20Yan%20and%20Yi%20Yang%20and%20Mengrui%20Shi%20and%20Qiang%20Zhang&entry.1292438233=Recent%20CLIP-based%20few-shot%20semantic%20segmentation%20methods%20introduce%20class-level%20textual%20priors%20to%20assist%20segmentation%20by%20typically%20using%20a%20single%20prompt%20%28e.g.%2C%20a%20photo%20of%20class%29.%20However%2C%20these%20approaches%20often%20result%20in%20incomplete%20activation%20of%20target%20regions%2C%20as%20a%20single%20textual%20description%20cannot%20fully%20capture%20the%20semantic%20diversity%20of%20complex%20categories.%20Moreover%2C%20they%20lack%20explicit%20cross-modal%20interaction%20and%20are%20vulnerable%20to%20noisy%20support%20features%2C%20further%20degrading%20visual%20prior%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Multi-Text%20Guided%20Few-Shot%20Semantic%20Segmentation%20Network%20%28MTGNet%29%2C%20a%20dual-branch%20framework%20that%20enhances%20segmentation%20performance%20by%20fusing%20diverse%20textual%20prompts%20to%20refine%20textual%20priors%20and%20guide%20the%20cross-modal%20optimization%20of%20visual%20priors.%20Specifically%2C%20we%20design%20a%20Multi-Textual%20Prior%20Refinement%20%28MTPR%29%20module%20that%20suppresses%20interference%20and%20aggregates%20complementary%20semantic%20cues%20to%20enhance%20foreground%20activation%20and%20expand%20semantic%20coverage%20for%20structurally%20complex%20objects.%20We%20introduce%20a%20Text%20Anchor%20Feature%20Fusion%20%28TAFF%29%20module%2C%20which%20leverages%20multi-text%20embeddings%20as%20semantic%20anchors%20to%20facilitate%20the%20transfer%20of%20discriminative%20local%20prototypes%20from%20support%20images%20to%20query%20images%2C%20thereby%20improving%20semantic%20consistency%20and%20alleviating%20intra-class%20variations.%20Furthermore%2C%20a%20Foreground%20Confidence-Weighted%20Attention%20%28FCWA%29%20module%20is%20presented%20to%20enhance%20visual%20prior%20robustness%20by%20leveraging%20internal%20self-similarity%20within%20support%20foreground%20features.%20It%20adaptively%20down-weights%20inconsistent%20regions%20and%20effectively%20suppresses%20interference%20in%20the%20query%20segmentation%20process.%20Extensive%20experiments%20on%20standard%20FSS%20benchmarks%20validate%20the%20effectiveness%20of%20MTGNet.%20In%20the%201-shot%20setting%2C%20it%20achieves%2076.8%25%20mIoU%20on%20PASCAL-5i%20and%2057.4%25%20on%20COCO-20i%2C%20with%20notable%20improvements%20in%20folds%20exhibiting%20high%20intra-class%20variations.&entry.1838667208=http%3A//arxiv.org/abs/2511.15515v1&entry.124074799=Read"},
{"title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking", "author": "Sifan Zhou and Yichao Cao and Jiahao Nie and Yuqian Fu and Ziyu Zhao and Xiaobo Lu and Shuo Wang", "abstract": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.", "link": "http://arxiv.org/abs/2511.15580v1", "date": "2025-11-19", "relevancy": 2.2556, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5708}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5619}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompTrack%3A%20Information%20Bottleneck-Guided%20Low-Rank%20Dynamic%20Token%20Compression%20for%20Point%20Cloud%20Tracking&body=Title%3A%20CompTrack%3A%20Information%20Bottleneck-Guided%20Low-Rank%20Dynamic%20Token%20Compression%20for%20Point%20Cloud%20Tracking%0AAuthor%3A%20Sifan%20Zhou%20and%20Yichao%20Cao%20and%20Jiahao%20Nie%20and%20Yuqian%20Fu%20and%20Ziyu%20Zhao%20and%20Xiaobo%20Lu%20and%20Shuo%20Wang%0AAbstract%3A%203D%20single%20object%20tracking%20%28SOT%29%20in%20LiDAR%20point%20clouds%20is%20a%20critical%20task%20in%20computer%20vision%20and%20autonomous%20driving.%20Despite%20great%20success%20having%20been%20achieved%2C%20the%20inherent%20sparsity%20of%20point%20clouds%20introduces%20a%20dual-redundancy%20challenge%20that%20limits%20existing%20trackers%3A%20%281%29%20vast%20spatial%20redundancy%20from%20background%20noise%20impairs%20accuracy%2C%20and%20%282%29%20informational%20redundancy%20within%20the%20foreground%20hinders%20efficiency.%20To%20tackle%20these%20issues%2C%20we%20propose%20CompTrack%2C%20a%20novel%20end-to-end%20framework%20that%20systematically%20eliminates%20both%20forms%20of%20redundancy%20in%20point%20clouds.%20First%2C%20CompTrack%20incorporates%20a%20Spatial%20Foreground%20Predictor%20%28SFP%29%20module%20to%20filter%20out%20irrelevant%20background%20noise%20based%20on%20information%20entropy%2C%20addressing%20spatial%20redundancy.%20Subsequently%2C%20its%20core%20is%20an%20Information%20Bottleneck-guided%20Dynamic%20Token%20Compression%20%28IB-DTC%29%20module%20that%20eliminates%20the%20informational%20redundancy%20within%20the%20foreground.%20Theoretically%20grounded%20in%20low-rank%20approximation%2C%20this%20module%20leverages%20an%20online%20SVD%20analysis%20to%20adaptively%20compress%20the%20redundant%20foreground%20into%20a%20compact%20and%20highly%20informative%20set%20of%20proxy%20tokens.%20Extensive%20experiments%20on%20KITTI%2C%20nuScenes%20and%20Waymo%20datasets%20demonstrate%20that%20CompTrack%20achieves%20top-performing%20tracking%20performance%20with%20superior%20efficiency%2C%20running%20at%20a%20real-time%2090%20FPS%20on%20a%20single%20RTX%203090%20GPU.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompTrack%253A%2520Information%2520Bottleneck-Guided%2520Low-Rank%2520Dynamic%2520Token%2520Compression%2520for%2520Point%2520Cloud%2520Tracking%26entry.906535625%3DSifan%2520Zhou%2520and%2520Yichao%2520Cao%2520and%2520Jiahao%2520Nie%2520and%2520Yuqian%2520Fu%2520and%2520Ziyu%2520Zhao%2520and%2520Xiaobo%2520Lu%2520and%2520Shuo%2520Wang%26entry.1292438233%3D3D%2520single%2520object%2520tracking%2520%2528SOT%2529%2520in%2520LiDAR%2520point%2520clouds%2520is%2520a%2520critical%2520task%2520in%2520computer%2520vision%2520and%2520autonomous%2520driving.%2520Despite%2520great%2520success%2520having%2520been%2520achieved%252C%2520the%2520inherent%2520sparsity%2520of%2520point%2520clouds%2520introduces%2520a%2520dual-redundancy%2520challenge%2520that%2520limits%2520existing%2520trackers%253A%2520%25281%2529%2520vast%2520spatial%2520redundancy%2520from%2520background%2520noise%2520impairs%2520accuracy%252C%2520and%2520%25282%2529%2520informational%2520redundancy%2520within%2520the%2520foreground%2520hinders%2520efficiency.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520CompTrack%252C%2520a%2520novel%2520end-to-end%2520framework%2520that%2520systematically%2520eliminates%2520both%2520forms%2520of%2520redundancy%2520in%2520point%2520clouds.%2520First%252C%2520CompTrack%2520incorporates%2520a%2520Spatial%2520Foreground%2520Predictor%2520%2528SFP%2529%2520module%2520to%2520filter%2520out%2520irrelevant%2520background%2520noise%2520based%2520on%2520information%2520entropy%252C%2520addressing%2520spatial%2520redundancy.%2520Subsequently%252C%2520its%2520core%2520is%2520an%2520Information%2520Bottleneck-guided%2520Dynamic%2520Token%2520Compression%2520%2528IB-DTC%2529%2520module%2520that%2520eliminates%2520the%2520informational%2520redundancy%2520within%2520the%2520foreground.%2520Theoretically%2520grounded%2520in%2520low-rank%2520approximation%252C%2520this%2520module%2520leverages%2520an%2520online%2520SVD%2520analysis%2520to%2520adaptively%2520compress%2520the%2520redundant%2520foreground%2520into%2520a%2520compact%2520and%2520highly%2520informative%2520set%2520of%2520proxy%2520tokens.%2520Extensive%2520experiments%2520on%2520KITTI%252C%2520nuScenes%2520and%2520Waymo%2520datasets%2520demonstrate%2520that%2520CompTrack%2520achieves%2520top-performing%2520tracking%2520performance%2520with%2520superior%2520efficiency%252C%2520running%2520at%2520a%2520real-time%252090%2520FPS%2520on%2520a%2520single%2520RTX%25203090%2520GPU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompTrack%3A%20Information%20Bottleneck-Guided%20Low-Rank%20Dynamic%20Token%20Compression%20for%20Point%20Cloud%20Tracking&entry.906535625=Sifan%20Zhou%20and%20Yichao%20Cao%20and%20Jiahao%20Nie%20and%20Yuqian%20Fu%20and%20Ziyu%20Zhao%20and%20Xiaobo%20Lu%20and%20Shuo%20Wang&entry.1292438233=3D%20single%20object%20tracking%20%28SOT%29%20in%20LiDAR%20point%20clouds%20is%20a%20critical%20task%20in%20computer%20vision%20and%20autonomous%20driving.%20Despite%20great%20success%20having%20been%20achieved%2C%20the%20inherent%20sparsity%20of%20point%20clouds%20introduces%20a%20dual-redundancy%20challenge%20that%20limits%20existing%20trackers%3A%20%281%29%20vast%20spatial%20redundancy%20from%20background%20noise%20impairs%20accuracy%2C%20and%20%282%29%20informational%20redundancy%20within%20the%20foreground%20hinders%20efficiency.%20To%20tackle%20these%20issues%2C%20we%20propose%20CompTrack%2C%20a%20novel%20end-to-end%20framework%20that%20systematically%20eliminates%20both%20forms%20of%20redundancy%20in%20point%20clouds.%20First%2C%20CompTrack%20incorporates%20a%20Spatial%20Foreground%20Predictor%20%28SFP%29%20module%20to%20filter%20out%20irrelevant%20background%20noise%20based%20on%20information%20entropy%2C%20addressing%20spatial%20redundancy.%20Subsequently%2C%20its%20core%20is%20an%20Information%20Bottleneck-guided%20Dynamic%20Token%20Compression%20%28IB-DTC%29%20module%20that%20eliminates%20the%20informational%20redundancy%20within%20the%20foreground.%20Theoretically%20grounded%20in%20low-rank%20approximation%2C%20this%20module%20leverages%20an%20online%20SVD%20analysis%20to%20adaptively%20compress%20the%20redundant%20foreground%20into%20a%20compact%20and%20highly%20informative%20set%20of%20proxy%20tokens.%20Extensive%20experiments%20on%20KITTI%2C%20nuScenes%20and%20Waymo%20datasets%20demonstrate%20that%20CompTrack%20achieves%20top-performing%20tracking%20performance%20with%20superior%20efficiency%2C%20running%20at%20a%20real-time%2090%20FPS%20on%20a%20single%20RTX%203090%20GPU.&entry.1838667208=http%3A//arxiv.org/abs/2511.15580v1&entry.124074799=Read"},
{"title": "Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection", "author": "Spyridon Loukovitis and Vasileios Karampinis and Athanasios Voulodimos", "abstract": "Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.", "link": "http://arxiv.org/abs/2511.15343v1", "date": "2025-11-19", "relevancy": 2.2301, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5774}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5603}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Post-Hoc%20Confidence%20Fusion%20for%203-Class%20Open-Set%20Aerial%20Object%20Detection&body=Title%3A%20Fast%20Post-Hoc%20Confidence%20Fusion%20for%203-Class%20Open-Set%20Aerial%20Object%20Detection%0AAuthor%3A%20Spyridon%20Loukovitis%20and%20Vasileios%20Karampinis%20and%20Athanasios%20Voulodimos%0AAbstract%3A%20Developing%20reliable%20UAV%20navigation%20systems%20requires%20robust%20air-to-air%20object%20detectors%20capable%20of%20distinguishing%20between%20objects%20seen%20during%20training%20and%20previously%20unseen%20objects.%20While%20many%20methods%20address%20closed-set%20detection%20and%20achieve%20high-confidence%20recognition%20of%20in-domain%20%28ID%29%20targets%2C%20they%20generally%20do%20not%20tackle%20open-set%20detection%2C%20which%20requires%20simultaneous%20handling%20of%20both%20ID%20and%20out-of-distribution%20%28OOD%29%20objects.%20Existing%20open-set%20approaches%20typically%20rely%20on%20a%20single%20uncertainty%20score%20with%20thresholding%2C%20limiting%20flexibility%20and%20often%20conflating%20OOD%20objects%20with%20background%20clutter.%20In%20contrast%2C%20we%20propose%20a%20lightweight%2C%20model-agnostic%20post-processing%20framework%20that%20explicitly%20separates%20background%20from%20unknown%20objects%20while%20preserving%20the%20base%20detector%27s%20performance.%20Our%20approach%20extends%20open-set%20detection%20beyond%20binary%20ID/OOD%20classification%20to%20real-time%20three-way%20classification%20among%20ID%20targets%2C%20OOD%20objects%2C%20and%20background.%20To%20this%20end%2C%20we%20employ%20a%20fusion%20scheme%20that%20aggregates%20multiple%20confidence%20estimates%20and%20per-detection%20features%20using%20a%20compact%20multilayer%20perceptron%20%28MLP%29.%20Incorporating%20different%20logit%20variants%20into%20the%20MLP%20consistently%20enhances%20performance%20across%20both%20binary%20and%20three-class%20classification%20without%20compromising%20throughput.%20Extensive%20ablation%20and%20comparative%20experiments%20confirm%20that%20our%20method%20surpasses%20threshold-based%20baselines%20in%20two-class%20classification%20by%20an%20average%20of%202.7%25%20AUROC%2C%20while%20retaining%20or%20improving%20open-set%20mAP.%20Furthermore%2C%20our%20study%20uniquely%20enables%20robust%20three-class%20classification%2C%20a%20critical%20capability%20for%20safe%20UAV%20navigation%2C%20where%20OOD%20objects%20must%20be%20actively%20avoided%20and%20background%20regions%20safely%20ignored.%20Comparative%20analysis%20highlights%20that%20our%20method%20surpasses%20competitive%20techniques%20in%20AUROC%20across%20datasets%2C%20while%20improving%20closed-set%20mAP%20by%20up%20to%209%20points%2C%20an%2018%25%20relative%20gain.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Post-Hoc%2520Confidence%2520Fusion%2520for%25203-Class%2520Open-Set%2520Aerial%2520Object%2520Detection%26entry.906535625%3DSpyridon%2520Loukovitis%2520and%2520Vasileios%2520Karampinis%2520and%2520Athanasios%2520Voulodimos%26entry.1292438233%3DDeveloping%2520reliable%2520UAV%2520navigation%2520systems%2520requires%2520robust%2520air-to-air%2520object%2520detectors%2520capable%2520of%2520distinguishing%2520between%2520objects%2520seen%2520during%2520training%2520and%2520previously%2520unseen%2520objects.%2520While%2520many%2520methods%2520address%2520closed-set%2520detection%2520and%2520achieve%2520high-confidence%2520recognition%2520of%2520in-domain%2520%2528ID%2529%2520targets%252C%2520they%2520generally%2520do%2520not%2520tackle%2520open-set%2520detection%252C%2520which%2520requires%2520simultaneous%2520handling%2520of%2520both%2520ID%2520and%2520out-of-distribution%2520%2528OOD%2529%2520objects.%2520Existing%2520open-set%2520approaches%2520typically%2520rely%2520on%2520a%2520single%2520uncertainty%2520score%2520with%2520thresholding%252C%2520limiting%2520flexibility%2520and%2520often%2520conflating%2520OOD%2520objects%2520with%2520background%2520clutter.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520lightweight%252C%2520model-agnostic%2520post-processing%2520framework%2520that%2520explicitly%2520separates%2520background%2520from%2520unknown%2520objects%2520while%2520preserving%2520the%2520base%2520detector%2527s%2520performance.%2520Our%2520approach%2520extends%2520open-set%2520detection%2520beyond%2520binary%2520ID/OOD%2520classification%2520to%2520real-time%2520three-way%2520classification%2520among%2520ID%2520targets%252C%2520OOD%2520objects%252C%2520and%2520background.%2520To%2520this%2520end%252C%2520we%2520employ%2520a%2520fusion%2520scheme%2520that%2520aggregates%2520multiple%2520confidence%2520estimates%2520and%2520per-detection%2520features%2520using%2520a%2520compact%2520multilayer%2520perceptron%2520%2528MLP%2529.%2520Incorporating%2520different%2520logit%2520variants%2520into%2520the%2520MLP%2520consistently%2520enhances%2520performance%2520across%2520both%2520binary%2520and%2520three-class%2520classification%2520without%2520compromising%2520throughput.%2520Extensive%2520ablation%2520and%2520comparative%2520experiments%2520confirm%2520that%2520our%2520method%2520surpasses%2520threshold-based%2520baselines%2520in%2520two-class%2520classification%2520by%2520an%2520average%2520of%25202.7%2525%2520AUROC%252C%2520while%2520retaining%2520or%2520improving%2520open-set%2520mAP.%2520Furthermore%252C%2520our%2520study%2520uniquely%2520enables%2520robust%2520three-class%2520classification%252C%2520a%2520critical%2520capability%2520for%2520safe%2520UAV%2520navigation%252C%2520where%2520OOD%2520objects%2520must%2520be%2520actively%2520avoided%2520and%2520background%2520regions%2520safely%2520ignored.%2520Comparative%2520analysis%2520highlights%2520that%2520our%2520method%2520surpasses%2520competitive%2520techniques%2520in%2520AUROC%2520across%2520datasets%252C%2520while%2520improving%2520closed-set%2520mAP%2520by%2520up%2520to%25209%2520points%252C%2520an%252018%2525%2520relative%2520gain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Post-Hoc%20Confidence%20Fusion%20for%203-Class%20Open-Set%20Aerial%20Object%20Detection&entry.906535625=Spyridon%20Loukovitis%20and%20Vasileios%20Karampinis%20and%20Athanasios%20Voulodimos&entry.1292438233=Developing%20reliable%20UAV%20navigation%20systems%20requires%20robust%20air-to-air%20object%20detectors%20capable%20of%20distinguishing%20between%20objects%20seen%20during%20training%20and%20previously%20unseen%20objects.%20While%20many%20methods%20address%20closed-set%20detection%20and%20achieve%20high-confidence%20recognition%20of%20in-domain%20%28ID%29%20targets%2C%20they%20generally%20do%20not%20tackle%20open-set%20detection%2C%20which%20requires%20simultaneous%20handling%20of%20both%20ID%20and%20out-of-distribution%20%28OOD%29%20objects.%20Existing%20open-set%20approaches%20typically%20rely%20on%20a%20single%20uncertainty%20score%20with%20thresholding%2C%20limiting%20flexibility%20and%20often%20conflating%20OOD%20objects%20with%20background%20clutter.%20In%20contrast%2C%20we%20propose%20a%20lightweight%2C%20model-agnostic%20post-processing%20framework%20that%20explicitly%20separates%20background%20from%20unknown%20objects%20while%20preserving%20the%20base%20detector%27s%20performance.%20Our%20approach%20extends%20open-set%20detection%20beyond%20binary%20ID/OOD%20classification%20to%20real-time%20three-way%20classification%20among%20ID%20targets%2C%20OOD%20objects%2C%20and%20background.%20To%20this%20end%2C%20we%20employ%20a%20fusion%20scheme%20that%20aggregates%20multiple%20confidence%20estimates%20and%20per-detection%20features%20using%20a%20compact%20multilayer%20perceptron%20%28MLP%29.%20Incorporating%20different%20logit%20variants%20into%20the%20MLP%20consistently%20enhances%20performance%20across%20both%20binary%20and%20three-class%20classification%20without%20compromising%20throughput.%20Extensive%20ablation%20and%20comparative%20experiments%20confirm%20that%20our%20method%20surpasses%20threshold-based%20baselines%20in%20two-class%20classification%20by%20an%20average%20of%202.7%25%20AUROC%2C%20while%20retaining%20or%20improving%20open-set%20mAP.%20Furthermore%2C%20our%20study%20uniquely%20enables%20robust%20three-class%20classification%2C%20a%20critical%20capability%20for%20safe%20UAV%20navigation%2C%20where%20OOD%20objects%20must%20be%20actively%20avoided%20and%20background%20regions%20safely%20ignored.%20Comparative%20analysis%20highlights%20that%20our%20method%20surpasses%20competitive%20techniques%20in%20AUROC%20across%20datasets%2C%20while%20improving%20closed-set%20mAP%20by%20up%20to%209%20points%2C%20an%2018%25%20relative%20gain.&entry.1838667208=http%3A//arxiv.org/abs/2511.15343v1&entry.124074799=Read"},
{"title": "Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation", "author": "Victorita Dolean and Daria Hrebenshchykova and St\u00e9phane Lanteri and Victor Michel-Dansac", "abstract": "Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.", "link": "http://arxiv.org/abs/2511.15445v1", "date": "2025-11-19", "relevancy": 2.2285, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4458}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation&body=Title%3A%20Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation%0AAuthor%3A%20Victorita%20Dolean%20and%20Daria%20Hrebenshchykova%20and%20St%C3%A9phane%20Lanteri%20and%20Victor%20Michel-Dansac%0AAbstract%3A%20Accurately%20simulating%20wave%20propagation%20is%20crucial%20in%20fields%20such%20as%20acoustics%2C%20electromagnetism%2C%20and%20seismic%20analysis.%20Traditional%20numerical%20methods%2C%20like%20finite%20difference%20and%20finite%20element%20approaches%2C%20are%20widely%20used%20to%20solve%20governing%20partial%20differential%20equations%20%28PDEs%29%20such%20as%20the%20Helmholtz%20equation.%20However%2C%20these%20methods%20face%20significant%20computational%20challenges%20when%20applied%20to%20high-frequency%20wave%20problems%20in%20complex%20two-dimensional%20domains.%20This%20work%20investigates%20Finite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%20and%20their%20multilevel%20extensions%20as%20a%20promising%20alternative.%20These%20methods%20leverage%20domain%20decomposition%2C%20partitioning%20the%20computational%20domain%20into%20overlapping%20sub-domains%2C%20each%20governed%20by%20a%20local%20neural%20network.%20We%20assess%20their%20accuracy%20and%20computational%20efficiency%20in%20solving%20the%20Helmholtz%20equation%20for%20the%20homogeneous%20case%2C%20demonstrating%20their%20potential%20to%20mitigate%20the%20limitations%20of%20traditional%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520network-driven%2520domain%2520decomposition%2520for%2520efficient%2520solutions%2520to%2520the%2520Helmholtz%2520equation%26entry.906535625%3DVictorita%2520Dolean%2520and%2520Daria%2520Hrebenshchykova%2520and%2520St%25C3%25A9phane%2520Lanteri%2520and%2520Victor%2520Michel-Dansac%26entry.1292438233%3DAccurately%2520simulating%2520wave%2520propagation%2520is%2520crucial%2520in%2520fields%2520such%2520as%2520acoustics%252C%2520electromagnetism%252C%2520and%2520seismic%2520analysis.%2520Traditional%2520numerical%2520methods%252C%2520like%2520finite%2520difference%2520and%2520finite%2520element%2520approaches%252C%2520are%2520widely%2520used%2520to%2520solve%2520governing%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520such%2520as%2520the%2520Helmholtz%2520equation.%2520However%252C%2520these%2520methods%2520face%2520significant%2520computational%2520challenges%2520when%2520applied%2520to%2520high-frequency%2520wave%2520problems%2520in%2520complex%2520two-dimensional%2520domains.%2520This%2520work%2520investigates%2520Finite%2520Basis%2520Physics-Informed%2520Neural%2520Networks%2520%2528FBPINNs%2529%2520and%2520their%2520multilevel%2520extensions%2520as%2520a%2520promising%2520alternative.%2520These%2520methods%2520leverage%2520domain%2520decomposition%252C%2520partitioning%2520the%2520computational%2520domain%2520into%2520overlapping%2520sub-domains%252C%2520each%2520governed%2520by%2520a%2520local%2520neural%2520network.%2520We%2520assess%2520their%2520accuracy%2520and%2520computational%2520efficiency%2520in%2520solving%2520the%2520Helmholtz%2520equation%2520for%2520the%2520homogeneous%2520case%252C%2520demonstrating%2520their%2520potential%2520to%2520mitigate%2520the%2520limitations%2520of%2520traditional%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20network-driven%20domain%20decomposition%20for%20efficient%20solutions%20to%20the%20Helmholtz%20equation&entry.906535625=Victorita%20Dolean%20and%20Daria%20Hrebenshchykova%20and%20St%C3%A9phane%20Lanteri%20and%20Victor%20Michel-Dansac&entry.1292438233=Accurately%20simulating%20wave%20propagation%20is%20crucial%20in%20fields%20such%20as%20acoustics%2C%20electromagnetism%2C%20and%20seismic%20analysis.%20Traditional%20numerical%20methods%2C%20like%20finite%20difference%20and%20finite%20element%20approaches%2C%20are%20widely%20used%20to%20solve%20governing%20partial%20differential%20equations%20%28PDEs%29%20such%20as%20the%20Helmholtz%20equation.%20However%2C%20these%20methods%20face%20significant%20computational%20challenges%20when%20applied%20to%20high-frequency%20wave%20problems%20in%20complex%20two-dimensional%20domains.%20This%20work%20investigates%20Finite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%20and%20their%20multilevel%20extensions%20as%20a%20promising%20alternative.%20These%20methods%20leverage%20domain%20decomposition%2C%20partitioning%20the%20computational%20domain%20into%20overlapping%20sub-domains%2C%20each%20governed%20by%20a%20local%20neural%20network.%20We%20assess%20their%20accuracy%20and%20computational%20efficiency%20in%20solving%20the%20Helmholtz%20equation%20for%20the%20homogeneous%20case%2C%20demonstrating%20their%20potential%20to%20mitigate%20the%20limitations%20of%20traditional%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2511.15445v1&entry.124074799=Read"},
{"title": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index", "author": "Selim Furkan Tekin and Rajesh Bordawekar", "abstract": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.", "link": "http://arxiv.org/abs/2511.15557v1", "date": "2025-11-19", "relevancy": 2.2192, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.459}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B%2BANN%3A%20A%20Fast%20Billion-Scale%20Disk-based%20Nearest-Neighbor%20Index&body=Title%3A%20B%2BANN%3A%20A%20Fast%20Billion-Scale%20Disk-based%20Nearest-Neighbor%20Index%0AAuthor%3A%20Selim%20Furkan%20Tekin%20and%20Rajesh%20Bordawekar%0AAbstract%3A%20Storing%20and%20processing%20of%20embedding%20vectors%20by%20specialized%20Vector%20databases%20%28VDBs%29%20has%20become%20the%20linchpin%20in%20building%20modern%20AI%20pipelines.%20Most%20current%20VDBs%20employ%20variants%20of%20a%20graph-based%20ap-%20proximate%20nearest-neighbor%20%28ANN%29%20index%20algorithm%2C%20HNSW%2C%20to%20an-%20swer%20semantic%20queries%20over%20stored%20vectors.%20Inspite%20of%20its%20wide-spread%20use%2C%20the%20HNSW%20algorithm%20suffers%20from%20several%20issues%3A%20in-memory%20design%20and%20implementation%2C%20random%20memory%20accesses%20leading%20to%20degradation%20in%20cache%20behavior%2C%20limited%20acceleration%20scope%20due%20to%20fine-grained%20pairwise%20computations%2C%20and%20support%20of%20only%20semantic%20similarity%20queries.%20In%20this%20paper%2C%20we%20present%20a%20novel%20disk-based%20ANN%20index%2C%20B%2BANN%2C%20to%20address%20these%20issues%3A%20it%20first%20partitions%20input%20data%20into%20blocks%20containing%20semantically%20similar%20items%2C%20then%20builds%20an%20B%2B%20tree%20variant%20to%20store%20blocks%20both%20in-memory%20and%20on%20disks%2C%20and%20finally%2C%20enables%20hybrid%20edge-%20and%20block-based%20in-memory%20traversals.%20As%20demonstrated%20by%20our%20experimantal%20evaluation%2C%20the%20proposed%20B%2BANN%20disk-based%20index%20improves%20both%20quality%20%28Recall%20value%29%2C%20and%20execution%20performance%20%28Queries%20per%20second/QPS%29%20over%20HNSW%2C%20by%20improving%20spatial%20and%20temporal%20locality%20for%20semantic%20operations%2C%20reducing%20cache%20misses%20%2819.23%25%20relative%20gain%29%2C%20and%20decreasing%20the%20memory%20consumption%20and%20disk-based%20build%20time%20by%2024x%20over%20the%20DiskANN%20algorithm.%20Finally%2C%20it%20enables%20dissimilarity%20queries%2C%20which%20are%20not%20supported%20by%20similarity-oriented%20ANN%20indices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB%252BANN%253A%2520A%2520Fast%2520Billion-Scale%2520Disk-based%2520Nearest-Neighbor%2520Index%26entry.906535625%3DSelim%2520Furkan%2520Tekin%2520and%2520Rajesh%2520Bordawekar%26entry.1292438233%3DStoring%2520and%2520processing%2520of%2520embedding%2520vectors%2520by%2520specialized%2520Vector%2520databases%2520%2528VDBs%2529%2520has%2520become%2520the%2520linchpin%2520in%2520building%2520modern%2520AI%2520pipelines.%2520Most%2520current%2520VDBs%2520employ%2520variants%2520of%2520a%2520graph-based%2520ap-%2520proximate%2520nearest-neighbor%2520%2528ANN%2529%2520index%2520algorithm%252C%2520HNSW%252C%2520to%2520an-%2520swer%2520semantic%2520queries%2520over%2520stored%2520vectors.%2520Inspite%2520of%2520its%2520wide-spread%2520use%252C%2520the%2520HNSW%2520algorithm%2520suffers%2520from%2520several%2520issues%253A%2520in-memory%2520design%2520and%2520implementation%252C%2520random%2520memory%2520accesses%2520leading%2520to%2520degradation%2520in%2520cache%2520behavior%252C%2520limited%2520acceleration%2520scope%2520due%2520to%2520fine-grained%2520pairwise%2520computations%252C%2520and%2520support%2520of%2520only%2520semantic%2520similarity%2520queries.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520disk-based%2520ANN%2520index%252C%2520B%252BANN%252C%2520to%2520address%2520these%2520issues%253A%2520it%2520first%2520partitions%2520input%2520data%2520into%2520blocks%2520containing%2520semantically%2520similar%2520items%252C%2520then%2520builds%2520an%2520B%252B%2520tree%2520variant%2520to%2520store%2520blocks%2520both%2520in-memory%2520and%2520on%2520disks%252C%2520and%2520finally%252C%2520enables%2520hybrid%2520edge-%2520and%2520block-based%2520in-memory%2520traversals.%2520As%2520demonstrated%2520by%2520our%2520experimantal%2520evaluation%252C%2520the%2520proposed%2520B%252BANN%2520disk-based%2520index%2520improves%2520both%2520quality%2520%2528Recall%2520value%2529%252C%2520and%2520execution%2520performance%2520%2528Queries%2520per%2520second/QPS%2529%2520over%2520HNSW%252C%2520by%2520improving%2520spatial%2520and%2520temporal%2520locality%2520for%2520semantic%2520operations%252C%2520reducing%2520cache%2520misses%2520%252819.23%2525%2520relative%2520gain%2529%252C%2520and%2520decreasing%2520the%2520memory%2520consumption%2520and%2520disk-based%2520build%2520time%2520by%252024x%2520over%2520the%2520DiskANN%2520algorithm.%2520Finally%252C%2520it%2520enables%2520dissimilarity%2520queries%252C%2520which%2520are%2520not%2520supported%2520by%2520similarity-oriented%2520ANN%2520indices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B%2BANN%3A%20A%20Fast%20Billion-Scale%20Disk-based%20Nearest-Neighbor%20Index&entry.906535625=Selim%20Furkan%20Tekin%20and%20Rajesh%20Bordawekar&entry.1292438233=Storing%20and%20processing%20of%20embedding%20vectors%20by%20specialized%20Vector%20databases%20%28VDBs%29%20has%20become%20the%20linchpin%20in%20building%20modern%20AI%20pipelines.%20Most%20current%20VDBs%20employ%20variants%20of%20a%20graph-based%20ap-%20proximate%20nearest-neighbor%20%28ANN%29%20index%20algorithm%2C%20HNSW%2C%20to%20an-%20swer%20semantic%20queries%20over%20stored%20vectors.%20Inspite%20of%20its%20wide-spread%20use%2C%20the%20HNSW%20algorithm%20suffers%20from%20several%20issues%3A%20in-memory%20design%20and%20implementation%2C%20random%20memory%20accesses%20leading%20to%20degradation%20in%20cache%20behavior%2C%20limited%20acceleration%20scope%20due%20to%20fine-grained%20pairwise%20computations%2C%20and%20support%20of%20only%20semantic%20similarity%20queries.%20In%20this%20paper%2C%20we%20present%20a%20novel%20disk-based%20ANN%20index%2C%20B%2BANN%2C%20to%20address%20these%20issues%3A%20it%20first%20partitions%20input%20data%20into%20blocks%20containing%20semantically%20similar%20items%2C%20then%20builds%20an%20B%2B%20tree%20variant%20to%20store%20blocks%20both%20in-memory%20and%20on%20disks%2C%20and%20finally%2C%20enables%20hybrid%20edge-%20and%20block-based%20in-memory%20traversals.%20As%20demonstrated%20by%20our%20experimantal%20evaluation%2C%20the%20proposed%20B%2BANN%20disk-based%20index%20improves%20both%20quality%20%28Recall%20value%29%2C%20and%20execution%20performance%20%28Queries%20per%20second/QPS%29%20over%20HNSW%2C%20by%20improving%20spatial%20and%20temporal%20locality%20for%20semantic%20operations%2C%20reducing%20cache%20misses%20%2819.23%25%20relative%20gain%29%2C%20and%20decreasing%20the%20memory%20consumption%20and%20disk-based%20build%20time%20by%2024x%20over%20the%20DiskANN%20algorithm.%20Finally%2C%20it%20enables%20dissimilarity%20queries%2C%20which%20are%20not%20supported%20by%20similarity-oriented%20ANN%20indices.&entry.1838667208=http%3A//arxiv.org/abs/2511.15557v1&entry.124074799=Read"},
{"title": "MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling", "author": "Shanshan Zhang", "abstract": "Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.", "link": "http://arxiv.org/abs/2511.15645v1", "date": "2025-11-19", "relevancy": 2.2111, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5537}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaIO%3A%20Global-Coordinate%20Inertial%20Odometry%20for%20Pedestrians%20via%20Multi-Scale%20Frequency-Decoupled%20Modeling&body=Title%3A%20MambaIO%3A%20Global-Coordinate%20Inertial%20Odometry%20for%20Pedestrians%20via%20Multi-Scale%20Frequency-Decoupled%20Modeling%0AAuthor%3A%20Shanshan%20Zhang%0AAbstract%3A%20Inertial%20Odometry%20%28IO%29%20enables%20real-time%20localization%20using%20only%20acceleration%20and%20angular%20velocity%20measurements%20from%20an%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20making%20it%20a%20promising%20solution%20for%20localization%20in%20consumer-grade%20applications.%20Traditionally%2C%20IMU%20measurements%20in%20IO%20have%20been%20processed%20under%20two%20coordinate%20system%20paradigms%3A%20the%20body%20coordinate%20frame%20and%20the%20global%20coordinate%20frame%2C%20with%20the%20latter%20being%20widely%20adopted.%20However%2C%20recent%20studies%20in%20drone%20scenarios%20have%20demonstrated%20that%20the%20body%20frame%20can%20significantly%20improve%20localization%20accuracy%2C%20prompting%20a%20re-evaluation%20of%20the%20suitability%20of%20the%20global%20frame%20for%20pedestrian%20IO.%20To%20address%20this%20issue%2C%20this%20paper%20systematically%20evaluates%20the%20effectiveness%20of%20the%20global%20coordinate%20frame%20in%20pedestrian%20IO%20through%20theoretical%20analysis%2C%20qualitative%20inspection%2C%20and%20quantitative%20experiments.%20Building%20upon%20these%20findings%2C%20we%20further%20propose%20MambaIO%2C%20which%20decomposes%20IMU%20measurements%20into%20high-frequency%20and%20low-frequency%20components%20using%20a%20Laplacian%20pyramid.%20The%20low-frequency%20component%20is%20processed%20by%20a%20Mamba%20architecture%20to%20extract%20implicit%20contextual%20motion%20cues%2C%20while%20the%20high-frequency%20component%20is%20handled%20by%20a%20convolutional%20structure%20to%20capture%20fine-grained%20local%20motion%20details.%20Experiments%20on%20multiple%20public%20datasets%20show%20that%20MambaIO%20substantially%20reduces%20localization%20error%20and%20achieves%20state-of-the-art%20%28SOTA%29%20performance.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20the%20Mamba%20architecture%20to%20the%20inertial%20odometry%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaIO%253A%2520Global-Coordinate%2520Inertial%2520Odometry%2520for%2520Pedestrians%2520via%2520Multi-Scale%2520Frequency-Decoupled%2520Modeling%26entry.906535625%3DShanshan%2520Zhang%26entry.1292438233%3DInertial%2520Odometry%2520%2528IO%2529%2520enables%2520real-time%2520localization%2520using%2520only%2520acceleration%2520and%2520angular%2520velocity%2520measurements%2520from%2520an%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%2520localization%2520in%2520consumer-grade%2520applications.%2520Traditionally%252C%2520IMU%2520measurements%2520in%2520IO%2520have%2520been%2520processed%2520under%2520two%2520coordinate%2520system%2520paradigms%253A%2520the%2520body%2520coordinate%2520frame%2520and%2520the%2520global%2520coordinate%2520frame%252C%2520with%2520the%2520latter%2520being%2520widely%2520adopted.%2520However%252C%2520recent%2520studies%2520in%2520drone%2520scenarios%2520have%2520demonstrated%2520that%2520the%2520body%2520frame%2520can%2520significantly%2520improve%2520localization%2520accuracy%252C%2520prompting%2520a%2520re-evaluation%2520of%2520the%2520suitability%2520of%2520the%2520global%2520frame%2520for%2520pedestrian%2520IO.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520systematically%2520evaluates%2520the%2520effectiveness%2520of%2520the%2520global%2520coordinate%2520frame%2520in%2520pedestrian%2520IO%2520through%2520theoretical%2520analysis%252C%2520qualitative%2520inspection%252C%2520and%2520quantitative%2520experiments.%2520Building%2520upon%2520these%2520findings%252C%2520we%2520further%2520propose%2520MambaIO%252C%2520which%2520decomposes%2520IMU%2520measurements%2520into%2520high-frequency%2520and%2520low-frequency%2520components%2520using%2520a%2520Laplacian%2520pyramid.%2520The%2520low-frequency%2520component%2520is%2520processed%2520by%2520a%2520Mamba%2520architecture%2520to%2520extract%2520implicit%2520contextual%2520motion%2520cues%252C%2520while%2520the%2520high-frequency%2520component%2520is%2520handled%2520by%2520a%2520convolutional%2520structure%2520to%2520capture%2520fine-grained%2520local%2520motion%2520details.%2520Experiments%2520on%2520multiple%2520public%2520datasets%2520show%2520that%2520MambaIO%2520substantially%2520reduces%2520localization%2520error%2520and%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520application%2520of%2520the%2520Mamba%2520architecture%2520to%2520the%2520inertial%2520odometry%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaIO%3A%20Global-Coordinate%20Inertial%20Odometry%20for%20Pedestrians%20via%20Multi-Scale%20Frequency-Decoupled%20Modeling&entry.906535625=Shanshan%20Zhang&entry.1292438233=Inertial%20Odometry%20%28IO%29%20enables%20real-time%20localization%20using%20only%20acceleration%20and%20angular%20velocity%20measurements%20from%20an%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20making%20it%20a%20promising%20solution%20for%20localization%20in%20consumer-grade%20applications.%20Traditionally%2C%20IMU%20measurements%20in%20IO%20have%20been%20processed%20under%20two%20coordinate%20system%20paradigms%3A%20the%20body%20coordinate%20frame%20and%20the%20global%20coordinate%20frame%2C%20with%20the%20latter%20being%20widely%20adopted.%20However%2C%20recent%20studies%20in%20drone%20scenarios%20have%20demonstrated%20that%20the%20body%20frame%20can%20significantly%20improve%20localization%20accuracy%2C%20prompting%20a%20re-evaluation%20of%20the%20suitability%20of%20the%20global%20frame%20for%20pedestrian%20IO.%20To%20address%20this%20issue%2C%20this%20paper%20systematically%20evaluates%20the%20effectiveness%20of%20the%20global%20coordinate%20frame%20in%20pedestrian%20IO%20through%20theoretical%20analysis%2C%20qualitative%20inspection%2C%20and%20quantitative%20experiments.%20Building%20upon%20these%20findings%2C%20we%20further%20propose%20MambaIO%2C%20which%20decomposes%20IMU%20measurements%20into%20high-frequency%20and%20low-frequency%20components%20using%20a%20Laplacian%20pyramid.%20The%20low-frequency%20component%20is%20processed%20by%20a%20Mamba%20architecture%20to%20extract%20implicit%20contextual%20motion%20cues%2C%20while%20the%20high-frequency%20component%20is%20handled%20by%20a%20convolutional%20structure%20to%20capture%20fine-grained%20local%20motion%20details.%20Experiments%20on%20multiple%20public%20datasets%20show%20that%20MambaIO%20substantially%20reduces%20localization%20error%20and%20achieves%20state-of-the-art%20%28SOTA%29%20performance.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20the%20Mamba%20architecture%20to%20the%20inertial%20odometry%20task.&entry.1838667208=http%3A//arxiv.org/abs/2511.15645v1&entry.124074799=Read"},
{"title": "SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference", "author": "Jintao Zhang and Chendong Xiang and Haofeng Huang and Jia Wei and Haocheng Xi and Jun Zhu and Jianfei Chen", "abstract": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The code is available at https://github.com/thu-ml/SpargeAttn.", "link": "http://arxiv.org/abs/2502.18137v8", "date": "2025-11-19", "relevancy": 2.1974, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6046}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5149}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference&body=Title%3A%20SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference%0AAuthor%3A%20Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%20its%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%20sparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%20the%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%20sparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%20optimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%20patterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%20both%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%20In%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%20attention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%20first%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%20skip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%20design%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%20skips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%20significantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%20generation%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20code%20is%20available%20at%20https%3A//github.com/thu-ml/SpargeAttn.%0ALink%3A%20http%3A//arxiv.org/abs/2502.18137v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpargeAttention%253A%2520Accurate%2520and%2520Training-free%2520Sparse%2520Attention%2520Accelerating%2520Any%2520Model%2520Inference%26entry.906535625%3DJintao%2520Zhang%2520and%2520Chendong%2520Xiang%2520and%2520Haofeng%2520Huang%2520and%2520Jia%2520Wei%2520and%2520Haocheng%2520Xi%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3DAn%2520efficient%2520attention%2520implementation%2520is%2520essential%2520for%2520large%2520models%2520due%2520to%2520its%2520quadratic%2520time%2520complexity.%2520Fortunately%252C%2520attention%2520commonly%2520exhibits%2520sparsity%252C%2520i.e.%252C%2520many%2520values%2520in%2520the%2520attention%2520map%2520are%2520near%2520zero%252C%2520allowing%2520for%2520the%2520omission%2520of%2520corresponding%2520computations.%2520Many%2520studies%2520have%2520utilized%2520the%2520sparse%2520pattern%2520to%2520accelerate%2520attention.%2520However%252C%2520most%2520existing%2520works%2520focus%2520on%2520optimizing%2520attention%2520within%2520specific%2520models%2520by%2520exploiting%2520certain%2520sparse%2520patterns%2520of%2520the%2520attention%2520map.%2520A%2520universal%2520sparse%2520attention%2520that%2520guarantees%2520both%2520the%2520speedup%2520and%2520end-to-end%2520performance%2520of%2520diverse%2520models%2520remains%2520elusive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SpargeAttn%252C%2520a%2520universal%2520sparse%2520and%2520quantized%2520attention%2520for%2520any%2520model.%2520Our%2520method%2520uses%2520a%2520two-stage%2520online%2520filter%253A%2520in%2520the%2520first%2520stage%252C%2520we%2520rapidly%2520and%2520accurately%2520predict%2520the%2520attention%2520map%252C%2520enabling%2520the%2520skip%2520of%2520some%2520matrix%2520multiplications%2520in%2520attention.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%2520an%2520online%2520softmax-aware%2520filter%2520that%2520incurs%2520no%2520extra%2520overhead%2520and%2520further%2520skips%2520some%2520matrix%2520multiplications.%2520Experiments%2520show%2520that%2520our%2520method%2520significantly%2520accelerates%2520diverse%2520models%252C%2520including%2520language%252C%2520image%252C%2520and%2520video%2520generation%252C%2520without%2520sacrificing%2520end-to-end%2520metrics.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/thu-ml/SpargeAttn.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18137v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference&entry.906535625=Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%20its%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%20sparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%20the%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%20sparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%20optimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%20patterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%20both%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%20In%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%20attention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%20first%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%20skip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%20design%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%20skips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%20significantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%20generation%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20code%20is%20available%20at%20https%3A//github.com/thu-ml/SpargeAttn.&entry.1838667208=http%3A//arxiv.org/abs/2502.18137v8&entry.124074799=Read"},
{"title": "Building Robust and Scalable Multilingual ASR for Indian Languages", "author": "Arjun Gangwar and Kaousheik Jayakumar and S. Umesh", "abstract": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).", "link": "http://arxiv.org/abs/2511.15418v1", "date": "2025-11-19", "relevancy": 2.1809, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Robust%20and%20Scalable%20Multilingual%20ASR%20for%20Indian%20Languages&body=Title%3A%20Building%20Robust%20and%20Scalable%20Multilingual%20ASR%20for%20Indian%20Languages%0AAuthor%3A%20Arjun%20Gangwar%20and%20Kaousheik%20Jayakumar%20and%20S.%20Umesh%0AAbstract%3A%20This%20paper%20describes%20the%20systems%20developed%20by%20SPRING%20Lab%2C%20Indian%20Institute%20of%20Technology%20Madras%2C%20for%20the%20ASRU%20MADASR%202.0%20challenge.%20The%20systems%20developed%20focuses%20on%20adapting%20ASR%20systems%20to%20improve%20in%20predicting%20the%20language%20and%20dialect%20of%20the%20utterance%20among%208%20languages%20across%2033%20dialects.%20We%20participated%20in%20Track%201%20and%20Track%202%2C%20which%20restricts%20the%20use%20of%20additional%20data%20and%20develop%20from-the-scratch%20multilingual%20systems.%20We%20presented%20a%20novel%20training%20approach%20using%20Multi-Decoder%20architecture%20with%20phonemic%20Common%20Label%20Set%20%28CLS%29%20as%20intermediate%20representation.%20It%20improved%20the%20performance%20over%20the%20baseline%20%28in%20the%20CLS%20space%29.%20We%20also%20discuss%20various%20methods%20used%20to%20retain%20the%20gain%20obtained%20in%20the%20phonemic%20space%20while%20converting%20them%20back%20to%20the%20corresponding%20grapheme%20representations.%20Our%20systems%20beat%20the%20baseline%20in%203%20languages%20%28Track%202%29%20in%20terms%20of%20WER/CER%20and%20achieved%20the%20highest%20language%20ID%20and%20dialect%20ID%20accuracy%20among%20all%20participating%20teams%20%28Track%202%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Robust%2520and%2520Scalable%2520Multilingual%2520ASR%2520for%2520Indian%2520Languages%26entry.906535625%3DArjun%2520Gangwar%2520and%2520Kaousheik%2520Jayakumar%2520and%2520S.%2520Umesh%26entry.1292438233%3DThis%2520paper%2520describes%2520the%2520systems%2520developed%2520by%2520SPRING%2520Lab%252C%2520Indian%2520Institute%2520of%2520Technology%2520Madras%252C%2520for%2520the%2520ASRU%2520MADASR%25202.0%2520challenge.%2520The%2520systems%2520developed%2520focuses%2520on%2520adapting%2520ASR%2520systems%2520to%2520improve%2520in%2520predicting%2520the%2520language%2520and%2520dialect%2520of%2520the%2520utterance%2520among%25208%2520languages%2520across%252033%2520dialects.%2520We%2520participated%2520in%2520Track%25201%2520and%2520Track%25202%252C%2520which%2520restricts%2520the%2520use%2520of%2520additional%2520data%2520and%2520develop%2520from-the-scratch%2520multilingual%2520systems.%2520We%2520presented%2520a%2520novel%2520training%2520approach%2520using%2520Multi-Decoder%2520architecture%2520with%2520phonemic%2520Common%2520Label%2520Set%2520%2528CLS%2529%2520as%2520intermediate%2520representation.%2520It%2520improved%2520the%2520performance%2520over%2520the%2520baseline%2520%2528in%2520the%2520CLS%2520space%2529.%2520We%2520also%2520discuss%2520various%2520methods%2520used%2520to%2520retain%2520the%2520gain%2520obtained%2520in%2520the%2520phonemic%2520space%2520while%2520converting%2520them%2520back%2520to%2520the%2520corresponding%2520grapheme%2520representations.%2520Our%2520systems%2520beat%2520the%2520baseline%2520in%25203%2520languages%2520%2528Track%25202%2529%2520in%2520terms%2520of%2520WER/CER%2520and%2520achieved%2520the%2520highest%2520language%2520ID%2520and%2520dialect%2520ID%2520accuracy%2520among%2520all%2520participating%2520teams%2520%2528Track%25202%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Robust%20and%20Scalable%20Multilingual%20ASR%20for%20Indian%20Languages&entry.906535625=Arjun%20Gangwar%20and%20Kaousheik%20Jayakumar%20and%20S.%20Umesh&entry.1292438233=This%20paper%20describes%20the%20systems%20developed%20by%20SPRING%20Lab%2C%20Indian%20Institute%20of%20Technology%20Madras%2C%20for%20the%20ASRU%20MADASR%202.0%20challenge.%20The%20systems%20developed%20focuses%20on%20adapting%20ASR%20systems%20to%20improve%20in%20predicting%20the%20language%20and%20dialect%20of%20the%20utterance%20among%208%20languages%20across%2033%20dialects.%20We%20participated%20in%20Track%201%20and%20Track%202%2C%20which%20restricts%20the%20use%20of%20additional%20data%20and%20develop%20from-the-scratch%20multilingual%20systems.%20We%20presented%20a%20novel%20training%20approach%20using%20Multi-Decoder%20architecture%20with%20phonemic%20Common%20Label%20Set%20%28CLS%29%20as%20intermediate%20representation.%20It%20improved%20the%20performance%20over%20the%20baseline%20%28in%20the%20CLS%20space%29.%20We%20also%20discuss%20various%20methods%20used%20to%20retain%20the%20gain%20obtained%20in%20the%20phonemic%20space%20while%20converting%20them%20back%20to%20the%20corresponding%20grapheme%20representations.%20Our%20systems%20beat%20the%20baseline%20in%203%20languages%20%28Track%202%29%20in%20terms%20of%20WER/CER%20and%20achieved%20the%20highest%20language%20ID%20and%20dialect%20ID%20accuracy%20among%20all%20participating%20teams%20%28Track%202%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.15418v1&entry.124074799=Read"},
{"title": "Causal Representation Learning with Observational Grouping for CXR Classification", "author": "Rajat Rasal and Avinash Kori and Ben Glocker", "abstract": "Identifiable causal representation learning seeks to uncover the true causal relationships underlying a data generation process. In medical imaging, this presents opportunities to improve the generalisability and robustness of task-specific latent features. This work introduces the concept of grouping observations to learn identifiable representations for disease classification in chest X-rays via an end-to-end framework. Our experiments demonstrate that these causal representations improve generalisability and robustness across multiple classification tasks when grouping is used to enforce invariance w.r.t race, sex, and imaging views.", "link": "http://arxiv.org/abs/2506.20582v2", "date": "2025-11-19", "relevancy": 2.1745, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4335}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Representation%20Learning%20with%20Observational%20Grouping%20for%20CXR%20Classification&body=Title%3A%20Causal%20Representation%20Learning%20with%20Observational%20Grouping%20for%20CXR%20Classification%0AAuthor%3A%20Rajat%20Rasal%20and%20Avinash%20Kori%20and%20Ben%20Glocker%0AAbstract%3A%20Identifiable%20causal%20representation%20learning%20seeks%20to%20uncover%20the%20true%20causal%20relationships%20underlying%20a%20data%20generation%20process.%20In%20medical%20imaging%2C%20this%20presents%20opportunities%20to%20improve%20the%20generalisability%20and%20robustness%20of%20task-specific%20latent%20features.%20This%20work%20introduces%20the%20concept%20of%20grouping%20observations%20to%20learn%20identifiable%20representations%20for%20disease%20classification%20in%20chest%20X-rays%20via%20an%20end-to-end%20framework.%20Our%20experiments%20demonstrate%20that%20these%20causal%20representations%20improve%20generalisability%20and%20robustness%20across%20multiple%20classification%20tasks%20when%20grouping%20is%20used%20to%20enforce%20invariance%20w.r.t%20race%2C%20sex%2C%20and%20imaging%20views.%0ALink%3A%20http%3A//arxiv.org/abs/2506.20582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Representation%2520Learning%2520with%2520Observational%2520Grouping%2520for%2520CXR%2520Classification%26entry.906535625%3DRajat%2520Rasal%2520and%2520Avinash%2520Kori%2520and%2520Ben%2520Glocker%26entry.1292438233%3DIdentifiable%2520causal%2520representation%2520learning%2520seeks%2520to%2520uncover%2520the%2520true%2520causal%2520relationships%2520underlying%2520a%2520data%2520generation%2520process.%2520In%2520medical%2520imaging%252C%2520this%2520presents%2520opportunities%2520to%2520improve%2520the%2520generalisability%2520and%2520robustness%2520of%2520task-specific%2520latent%2520features.%2520This%2520work%2520introduces%2520the%2520concept%2520of%2520grouping%2520observations%2520to%2520learn%2520identifiable%2520representations%2520for%2520disease%2520classification%2520in%2520chest%2520X-rays%2520via%2520an%2520end-to-end%2520framework.%2520Our%2520experiments%2520demonstrate%2520that%2520these%2520causal%2520representations%2520improve%2520generalisability%2520and%2520robustness%2520across%2520multiple%2520classification%2520tasks%2520when%2520grouping%2520is%2520used%2520to%2520enforce%2520invariance%2520w.r.t%2520race%252C%2520sex%252C%2520and%2520imaging%2520views.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Representation%20Learning%20with%20Observational%20Grouping%20for%20CXR%20Classification&entry.906535625=Rajat%20Rasal%20and%20Avinash%20Kori%20and%20Ben%20Glocker&entry.1292438233=Identifiable%20causal%20representation%20learning%20seeks%20to%20uncover%20the%20true%20causal%20relationships%20underlying%20a%20data%20generation%20process.%20In%20medical%20imaging%2C%20this%20presents%20opportunities%20to%20improve%20the%20generalisability%20and%20robustness%20of%20task-specific%20latent%20features.%20This%20work%20introduces%20the%20concept%20of%20grouping%20observations%20to%20learn%20identifiable%20representations%20for%20disease%20classification%20in%20chest%20X-rays%20via%20an%20end-to-end%20framework.%20Our%20experiments%20demonstrate%20that%20these%20causal%20representations%20improve%20generalisability%20and%20robustness%20across%20multiple%20classification%20tasks%20when%20grouping%20is%20used%20to%20enforce%20invariance%20w.r.t%20race%2C%20sex%2C%20and%20imaging%20views.&entry.1838667208=http%3A//arxiv.org/abs/2506.20582v2&entry.124074799=Read"},
{"title": "AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning", "author": "Urjitkumar Patel and Fang-Chun Yeh and Chinmay Gondhalekar", "abstract": "With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.", "link": "http://arxiv.org/abs/2511.15578v1", "date": "2025-11-19", "relevancy": 2.173, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AVATAAR%3A%20Agentic%20Video%20Answering%20via%20Temporal%20Adaptive%20Alignment%20and%20Reasoning&body=Title%3A%20AVATAAR%3A%20Agentic%20Video%20Answering%20via%20Temporal%20Adaptive%20Alignment%20and%20Reasoning%0AAuthor%3A%20Urjitkumar%20Patel%20and%20Fang-Chun%20Yeh%20and%20Chinmay%20Gondhalekar%0AAbstract%3A%20With%20the%20increasing%20prevalence%20of%20video%20content%2C%20effectively%20understanding%20and%20answering%20questions%20about%20long%20form%20videos%20has%20become%20essential%20for%20numerous%20applications.%20Although%20large%20vision%20language%20models%20%28LVLMs%29%20have%20enhanced%20performance%2C%20they%20often%20face%20challenges%20with%20nuanced%20queries%20that%20demand%20both%20a%20comprehensive%20understanding%20and%20detailed%20analysis.%20To%20overcome%20these%20obstacles%2C%20we%20introduce%20AVATAAR%2C%20a%20modular%20and%20interpretable%20framework%20that%20combines%20global%20and%20local%20video%20context%2C%20along%20with%20a%20Pre%20Retrieval%20Thinking%20Agent%20and%20a%20Rethink%20Module.%20AVATAAR%20creates%20a%20persistent%20global%20summary%20and%20establishes%20a%20feedback%20loop%20between%20the%20Rethink%20Module%20and%20the%20Pre%20Retrieval%20Thinking%20Agent%2C%20allowing%20the%20system%20to%20refine%20its%20retrieval%20strategies%20based%20on%20partial%20answers%20and%20replicate%20human-like%20iterative%20reasoning.%20On%20the%20CinePile%20benchmark%2C%20AVATAAR%20demonstrates%20significant%20improvements%20over%20a%20baseline%2C%20achieving%20relative%20gains%20of%20%2B5.6%25%20in%20temporal%20reasoning%2C%20%2B5%25%20in%20technical%20queries%2C%20%2B8%25%20in%20theme-based%20questions%2C%20and%20%2B8.2%25%20in%20narrative%20comprehension.%20Our%20experiments%20confirm%20that%20each%20module%20contributes%20positively%20to%20the%20overall%20performance%2C%20with%20the%20feedback%20loop%20being%20crucial%20for%20adaptability.%20These%20findings%20highlight%20AVATAAR%27s%20effectiveness%20in%20enhancing%20video%20understanding%20capabilities.%20Ultimately%2C%20AVATAAR%20presents%20a%20scalable%20solution%20for%20long-form%20Video%20Question%20Answering%20%28QA%29%2C%20merging%20accuracy%2C%20interpretability%2C%20and%20extensibility.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAVATAAR%253A%2520Agentic%2520Video%2520Answering%2520via%2520Temporal%2520Adaptive%2520Alignment%2520and%2520Reasoning%26entry.906535625%3DUrjitkumar%2520Patel%2520and%2520Fang-Chun%2520Yeh%2520and%2520Chinmay%2520Gondhalekar%26entry.1292438233%3DWith%2520the%2520increasing%2520prevalence%2520of%2520video%2520content%252C%2520effectively%2520understanding%2520and%2520answering%2520questions%2520about%2520long%2520form%2520videos%2520has%2520become%2520essential%2520for%2520numerous%2520applications.%2520Although%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520have%2520enhanced%2520performance%252C%2520they%2520often%2520face%2520challenges%2520with%2520nuanced%2520queries%2520that%2520demand%2520both%2520a%2520comprehensive%2520understanding%2520and%2520detailed%2520analysis.%2520To%2520overcome%2520these%2520obstacles%252C%2520we%2520introduce%2520AVATAAR%252C%2520a%2520modular%2520and%2520interpretable%2520framework%2520that%2520combines%2520global%2520and%2520local%2520video%2520context%252C%2520along%2520with%2520a%2520Pre%2520Retrieval%2520Thinking%2520Agent%2520and%2520a%2520Rethink%2520Module.%2520AVATAAR%2520creates%2520a%2520persistent%2520global%2520summary%2520and%2520establishes%2520a%2520feedback%2520loop%2520between%2520the%2520Rethink%2520Module%2520and%2520the%2520Pre%2520Retrieval%2520Thinking%2520Agent%252C%2520allowing%2520the%2520system%2520to%2520refine%2520its%2520retrieval%2520strategies%2520based%2520on%2520partial%2520answers%2520and%2520replicate%2520human-like%2520iterative%2520reasoning.%2520On%2520the%2520CinePile%2520benchmark%252C%2520AVATAAR%2520demonstrates%2520significant%2520improvements%2520over%2520a%2520baseline%252C%2520achieving%2520relative%2520gains%2520of%2520%252B5.6%2525%2520in%2520temporal%2520reasoning%252C%2520%252B5%2525%2520in%2520technical%2520queries%252C%2520%252B8%2525%2520in%2520theme-based%2520questions%252C%2520and%2520%252B8.2%2525%2520in%2520narrative%2520comprehension.%2520Our%2520experiments%2520confirm%2520that%2520each%2520module%2520contributes%2520positively%2520to%2520the%2520overall%2520performance%252C%2520with%2520the%2520feedback%2520loop%2520being%2520crucial%2520for%2520adaptability.%2520These%2520findings%2520highlight%2520AVATAAR%2527s%2520effectiveness%2520in%2520enhancing%2520video%2520understanding%2520capabilities.%2520Ultimately%252C%2520AVATAAR%2520presents%2520a%2520scalable%2520solution%2520for%2520long-form%2520Video%2520Question%2520Answering%2520%2528QA%2529%252C%2520merging%2520accuracy%252C%2520interpretability%252C%2520and%2520extensibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AVATAAR%3A%20Agentic%20Video%20Answering%20via%20Temporal%20Adaptive%20Alignment%20and%20Reasoning&entry.906535625=Urjitkumar%20Patel%20and%20Fang-Chun%20Yeh%20and%20Chinmay%20Gondhalekar&entry.1292438233=With%20the%20increasing%20prevalence%20of%20video%20content%2C%20effectively%20understanding%20and%20answering%20questions%20about%20long%20form%20videos%20has%20become%20essential%20for%20numerous%20applications.%20Although%20large%20vision%20language%20models%20%28LVLMs%29%20have%20enhanced%20performance%2C%20they%20often%20face%20challenges%20with%20nuanced%20queries%20that%20demand%20both%20a%20comprehensive%20understanding%20and%20detailed%20analysis.%20To%20overcome%20these%20obstacles%2C%20we%20introduce%20AVATAAR%2C%20a%20modular%20and%20interpretable%20framework%20that%20combines%20global%20and%20local%20video%20context%2C%20along%20with%20a%20Pre%20Retrieval%20Thinking%20Agent%20and%20a%20Rethink%20Module.%20AVATAAR%20creates%20a%20persistent%20global%20summary%20and%20establishes%20a%20feedback%20loop%20between%20the%20Rethink%20Module%20and%20the%20Pre%20Retrieval%20Thinking%20Agent%2C%20allowing%20the%20system%20to%20refine%20its%20retrieval%20strategies%20based%20on%20partial%20answers%20and%20replicate%20human-like%20iterative%20reasoning.%20On%20the%20CinePile%20benchmark%2C%20AVATAAR%20demonstrates%20significant%20improvements%20over%20a%20baseline%2C%20achieving%20relative%20gains%20of%20%2B5.6%25%20in%20temporal%20reasoning%2C%20%2B5%25%20in%20technical%20queries%2C%20%2B8%25%20in%20theme-based%20questions%2C%20and%20%2B8.2%25%20in%20narrative%20comprehension.%20Our%20experiments%20confirm%20that%20each%20module%20contributes%20positively%20to%20the%20overall%20performance%2C%20with%20the%20feedback%20loop%20being%20crucial%20for%20adaptability.%20These%20findings%20highlight%20AVATAAR%27s%20effectiveness%20in%20enhancing%20video%20understanding%20capabilities.%20Ultimately%2C%20AVATAAR%20presents%20a%20scalable%20solution%20for%20long-form%20Video%20Question%20Answering%20%28QA%29%2C%20merging%20accuracy%2C%20interpretability%2C%20and%20extensibility.&entry.1838667208=http%3A//arxiv.org/abs/2511.15578v1&entry.124074799=Read"},
{"title": "FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation", "author": "Tingrui Shen and Yiheng Zhang and Chen Tang and Chuan Ping and Zixing Zhao and Le Wan and Yuwang Wang and Ronggang Wang and Shengfeng He", "abstract": "Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.", "link": "http://arxiv.org/abs/2511.15618v1", "date": "2025-11-19", "relevancy": 2.1705, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5854}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5292}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashMesh%3A%20Faster%20and%20Better%20Autoregressive%20Mesh%20Synthesis%20via%20Structured%20Speculation&body=Title%3A%20FlashMesh%3A%20Faster%20and%20Better%20Autoregressive%20Mesh%20Synthesis%20via%20Structured%20Speculation%0AAuthor%3A%20Tingrui%20Shen%20and%20Yiheng%20Zhang%20and%20Chen%20Tang%20and%20Chuan%20Ping%20and%20Zixing%20Zhao%20and%20Le%20Wan%20and%20Yuwang%20Wang%20and%20Ronggang%20Wang%20and%20Shengfeng%20He%0AAbstract%3A%20Autoregressive%20models%20can%20generate%20high-quality%203D%20meshes%20by%20sequentially%20producing%20vertices%20and%20faces%2C%20but%20their%20token-by-token%20decoding%20results%20in%20slow%20inference%2C%20limiting%20practical%20use%20in%20interactive%20and%20large-scale%20applications.%20We%20present%20FlashMesh%2C%20a%20fast%20and%20high-fidelity%20mesh%20generation%20framework%20that%20rethinks%20autoregressive%20decoding%20through%20a%20predict-correct-verify%20paradigm.%20The%20key%20insight%20is%20that%20mesh%20tokens%20exhibit%20strong%20structural%20and%20geometric%20correlations%20that%20enable%20confident%20multi-token%20speculation.%20FlashMesh%20leverages%20this%20by%20introducing%20a%20speculative%20decoding%20scheme%20tailored%20to%20the%20commonly%20used%20hourglass%20transformer%20architecture%2C%20enabling%20parallel%20prediction%20across%20face%2C%20point%2C%20and%20coordinate%20levels.%20Extensive%20experiments%20show%20that%20FlashMesh%20achieves%20up%20to%20a%202%20x%20speedup%20over%20standard%20autoregressive%20models%20while%20also%20improving%20generation%20fidelity.%20Our%20results%20demonstrate%20that%20structural%20priors%20in%20mesh%20data%20can%20be%20systematically%20harnessed%20to%20accelerate%20and%20enhance%20autoregressive%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashMesh%253A%2520Faster%2520and%2520Better%2520Autoregressive%2520Mesh%2520Synthesis%2520via%2520Structured%2520Speculation%26entry.906535625%3DTingrui%2520Shen%2520and%2520Yiheng%2520Zhang%2520and%2520Chen%2520Tang%2520and%2520Chuan%2520Ping%2520and%2520Zixing%2520Zhao%2520and%2520Le%2520Wan%2520and%2520Yuwang%2520Wang%2520and%2520Ronggang%2520Wang%2520and%2520Shengfeng%2520He%26entry.1292438233%3DAutoregressive%2520models%2520can%2520generate%2520high-quality%25203D%2520meshes%2520by%2520sequentially%2520producing%2520vertices%2520and%2520faces%252C%2520but%2520their%2520token-by-token%2520decoding%2520results%2520in%2520slow%2520inference%252C%2520limiting%2520practical%2520use%2520in%2520interactive%2520and%2520large-scale%2520applications.%2520We%2520present%2520FlashMesh%252C%2520a%2520fast%2520and%2520high-fidelity%2520mesh%2520generation%2520framework%2520that%2520rethinks%2520autoregressive%2520decoding%2520through%2520a%2520predict-correct-verify%2520paradigm.%2520The%2520key%2520insight%2520is%2520that%2520mesh%2520tokens%2520exhibit%2520strong%2520structural%2520and%2520geometric%2520correlations%2520that%2520enable%2520confident%2520multi-token%2520speculation.%2520FlashMesh%2520leverages%2520this%2520by%2520introducing%2520a%2520speculative%2520decoding%2520scheme%2520tailored%2520to%2520the%2520commonly%2520used%2520hourglass%2520transformer%2520architecture%252C%2520enabling%2520parallel%2520prediction%2520across%2520face%252C%2520point%252C%2520and%2520coordinate%2520levels.%2520Extensive%2520experiments%2520show%2520that%2520FlashMesh%2520achieves%2520up%2520to%2520a%25202%2520x%2520speedup%2520over%2520standard%2520autoregressive%2520models%2520while%2520also%2520improving%2520generation%2520fidelity.%2520Our%2520results%2520demonstrate%2520that%2520structural%2520priors%2520in%2520mesh%2520data%2520can%2520be%2520systematically%2520harnessed%2520to%2520accelerate%2520and%2520enhance%2520autoregressive%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashMesh%3A%20Faster%20and%20Better%20Autoregressive%20Mesh%20Synthesis%20via%20Structured%20Speculation&entry.906535625=Tingrui%20Shen%20and%20Yiheng%20Zhang%20and%20Chen%20Tang%20and%20Chuan%20Ping%20and%20Zixing%20Zhao%20and%20Le%20Wan%20and%20Yuwang%20Wang%20and%20Ronggang%20Wang%20and%20Shengfeng%20He&entry.1292438233=Autoregressive%20models%20can%20generate%20high-quality%203D%20meshes%20by%20sequentially%20producing%20vertices%20and%20faces%2C%20but%20their%20token-by-token%20decoding%20results%20in%20slow%20inference%2C%20limiting%20practical%20use%20in%20interactive%20and%20large-scale%20applications.%20We%20present%20FlashMesh%2C%20a%20fast%20and%20high-fidelity%20mesh%20generation%20framework%20that%20rethinks%20autoregressive%20decoding%20through%20a%20predict-correct-verify%20paradigm.%20The%20key%20insight%20is%20that%20mesh%20tokens%20exhibit%20strong%20structural%20and%20geometric%20correlations%20that%20enable%20confident%20multi-token%20speculation.%20FlashMesh%20leverages%20this%20by%20introducing%20a%20speculative%20decoding%20scheme%20tailored%20to%20the%20commonly%20used%20hourglass%20transformer%20architecture%2C%20enabling%20parallel%20prediction%20across%20face%2C%20point%2C%20and%20coordinate%20levels.%20Extensive%20experiments%20show%20that%20FlashMesh%20achieves%20up%20to%20a%202%20x%20speedup%20over%20standard%20autoregressive%20models%20while%20also%20improving%20generation%20fidelity.%20Our%20results%20demonstrate%20that%20structural%20priors%20in%20mesh%20data%20can%20be%20systematically%20harnessed%20to%20accelerate%20and%20enhance%20autoregressive%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.15618v1&entry.124074799=Read"},
{"title": "From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers", "author": "Huiyuan Tian and Bonan Xu and Shijian Li and Xin Jin", "abstract": "Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\\%/95\\%/90\\%/80\\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\\%$ to $77.53\\%$ and $78.23\\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.", "link": "http://arxiv.org/abs/2511.15572v1", "date": "2025-11-19", "relevancy": 2.1677, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Low-Rank%20Features%20to%20Encoding%20Mismatch%3A%20Rethinking%20Feature%20Distillation%20in%20Vision%20Transformers&body=Title%3A%20From%20Low-Rank%20Features%20to%20Encoding%20Mismatch%3A%20Rethinking%20Feature%20Distillation%20in%20Vision%20Transformers%0AAuthor%3A%20Huiyuan%20Tian%20and%20Bonan%20Xu%20and%20Shijian%20Li%20and%20Xin%20Jin%0AAbstract%3A%20Feature-map%20knowledge%20distillation%20%28KD%29%20is%20highly%20effective%20for%20convolutional%20networks%20but%20often%20fails%20for%20Vision%20Transformers%20%28ViTs%29.%20To%20understand%20this%20failure%20and%20guide%20method%20design%2C%20we%20conduct%20a%20two-view%20representation%20analysis%20of%20ViTs.%20First%2C%20a%20layer-wise%20Singular%20Value%20Decomposition%20%28SVD%29%20of%20full%20feature%20matrices%20shows%20that%20final-layer%20representations%20are%20globally%20low-rank%3A%20for%20CaiT-S24%2C%20only%20%24121/61/34/14%24%20dimensions%20suffice%20to%20capture%20%2499%5C%25/95%5C%25/90%5C%25/80%5C%25%24%20of%20the%20energy.%20In%20principle%2C%20this%20suggests%20that%20a%20compact%20student%20plus%20a%20simple%20linear%20projector%20should%20be%20enough%20for%20feature%20alignment%2C%20contradicting%20the%20weak%20empirical%20performance%20of%20standard%20feature%20KD.%20To%20resolve%20this%20paradox%2C%20we%20introduce%20a%20token-level%20Spectral%20Energy%20Pattern%20%28SEP%29%20analysis%20that%20measures%20how%20each%20token%20uses%20channel%20capacity.%20SEP%20reveals%20that%2C%20despite%20the%20global%20low-rank%20structure%2C%20individual%20tokens%20distribute%20energy%20over%20most%20channels%2C%20forming%20a%20high-bandwidth%20encoding%20pattern.%20This%20results%20in%20an%20encoding%20mismatch%20between%20wide%20teachers%20and%20narrow%20students.%20Motivated%20by%20this%20insight%2C%20we%20propose%20two%20minimal%2C%20mismatch-driven%20strategies%3A%20%281%29%20post-hoc%20feature%20lifting%20with%20a%20lightweight%20projector%20retained%20during%20inference%2C%20or%20%282%29%20native%20width%20alignment%20that%20widens%20only%20the%20student%27s%20last%20block%20to%20the%20teacher%27s%20width.%20On%20ImageNet-1K%2C%20these%20strategies%20reactivate%20simple%20feature-map%20distillation%20in%20ViTs%2C%20raising%20DeiT-Tiny%20accuracy%20from%20%2474.86%5C%25%24%20to%20%2477.53%5C%25%24%20and%20%2478.23%5C%25%24%20when%20distilling%20from%20CaiT-S24%2C%20while%20also%20improving%20standalone%20students%20trained%20without%20any%20teacher.%20Our%20analysis%20thus%20explains%20why%20ViT%20feature%20distillation%20fails%20and%20shows%20how%20exploiting%20low-rank%20structure%20yields%20effective%2C%20interpretable%20remedies%20and%20concrete%20design%20guidance%20for%20compact%20ViTs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Low-Rank%2520Features%2520to%2520Encoding%2520Mismatch%253A%2520Rethinking%2520Feature%2520Distillation%2520in%2520Vision%2520Transformers%26entry.906535625%3DHuiyuan%2520Tian%2520and%2520Bonan%2520Xu%2520and%2520Shijian%2520Li%2520and%2520Xin%2520Jin%26entry.1292438233%3DFeature-map%2520knowledge%2520distillation%2520%2528KD%2529%2520is%2520highly%2520effective%2520for%2520convolutional%2520networks%2520but%2520often%2520fails%2520for%2520Vision%2520Transformers%2520%2528ViTs%2529.%2520To%2520understand%2520this%2520failure%2520and%2520guide%2520method%2520design%252C%2520we%2520conduct%2520a%2520two-view%2520representation%2520analysis%2520of%2520ViTs.%2520First%252C%2520a%2520layer-wise%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529%2520of%2520full%2520feature%2520matrices%2520shows%2520that%2520final-layer%2520representations%2520are%2520globally%2520low-rank%253A%2520for%2520CaiT-S24%252C%2520only%2520%2524121/61/34/14%2524%2520dimensions%2520suffice%2520to%2520capture%2520%252499%255C%2525/95%255C%2525/90%255C%2525/80%255C%2525%2524%2520of%2520the%2520energy.%2520In%2520principle%252C%2520this%2520suggests%2520that%2520a%2520compact%2520student%2520plus%2520a%2520simple%2520linear%2520projector%2520should%2520be%2520enough%2520for%2520feature%2520alignment%252C%2520contradicting%2520the%2520weak%2520empirical%2520performance%2520of%2520standard%2520feature%2520KD.%2520To%2520resolve%2520this%2520paradox%252C%2520we%2520introduce%2520a%2520token-level%2520Spectral%2520Energy%2520Pattern%2520%2528SEP%2529%2520analysis%2520that%2520measures%2520how%2520each%2520token%2520uses%2520channel%2520capacity.%2520SEP%2520reveals%2520that%252C%2520despite%2520the%2520global%2520low-rank%2520structure%252C%2520individual%2520tokens%2520distribute%2520energy%2520over%2520most%2520channels%252C%2520forming%2520a%2520high-bandwidth%2520encoding%2520pattern.%2520This%2520results%2520in%2520an%2520encoding%2520mismatch%2520between%2520wide%2520teachers%2520and%2520narrow%2520students.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520two%2520minimal%252C%2520mismatch-driven%2520strategies%253A%2520%25281%2529%2520post-hoc%2520feature%2520lifting%2520with%2520a%2520lightweight%2520projector%2520retained%2520during%2520inference%252C%2520or%2520%25282%2529%2520native%2520width%2520alignment%2520that%2520widens%2520only%2520the%2520student%2527s%2520last%2520block%2520to%2520the%2520teacher%2527s%2520width.%2520On%2520ImageNet-1K%252C%2520these%2520strategies%2520reactivate%2520simple%2520feature-map%2520distillation%2520in%2520ViTs%252C%2520raising%2520DeiT-Tiny%2520accuracy%2520from%2520%252474.86%255C%2525%2524%2520to%2520%252477.53%255C%2525%2524%2520and%2520%252478.23%255C%2525%2524%2520when%2520distilling%2520from%2520CaiT-S24%252C%2520while%2520also%2520improving%2520standalone%2520students%2520trained%2520without%2520any%2520teacher.%2520Our%2520analysis%2520thus%2520explains%2520why%2520ViT%2520feature%2520distillation%2520fails%2520and%2520shows%2520how%2520exploiting%2520low-rank%2520structure%2520yields%2520effective%252C%2520interpretable%2520remedies%2520and%2520concrete%2520design%2520guidance%2520for%2520compact%2520ViTs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Low-Rank%20Features%20to%20Encoding%20Mismatch%3A%20Rethinking%20Feature%20Distillation%20in%20Vision%20Transformers&entry.906535625=Huiyuan%20Tian%20and%20Bonan%20Xu%20and%20Shijian%20Li%20and%20Xin%20Jin&entry.1292438233=Feature-map%20knowledge%20distillation%20%28KD%29%20is%20highly%20effective%20for%20convolutional%20networks%20but%20often%20fails%20for%20Vision%20Transformers%20%28ViTs%29.%20To%20understand%20this%20failure%20and%20guide%20method%20design%2C%20we%20conduct%20a%20two-view%20representation%20analysis%20of%20ViTs.%20First%2C%20a%20layer-wise%20Singular%20Value%20Decomposition%20%28SVD%29%20of%20full%20feature%20matrices%20shows%20that%20final-layer%20representations%20are%20globally%20low-rank%3A%20for%20CaiT-S24%2C%20only%20%24121/61/34/14%24%20dimensions%20suffice%20to%20capture%20%2499%5C%25/95%5C%25/90%5C%25/80%5C%25%24%20of%20the%20energy.%20In%20principle%2C%20this%20suggests%20that%20a%20compact%20student%20plus%20a%20simple%20linear%20projector%20should%20be%20enough%20for%20feature%20alignment%2C%20contradicting%20the%20weak%20empirical%20performance%20of%20standard%20feature%20KD.%20To%20resolve%20this%20paradox%2C%20we%20introduce%20a%20token-level%20Spectral%20Energy%20Pattern%20%28SEP%29%20analysis%20that%20measures%20how%20each%20token%20uses%20channel%20capacity.%20SEP%20reveals%20that%2C%20despite%20the%20global%20low-rank%20structure%2C%20individual%20tokens%20distribute%20energy%20over%20most%20channels%2C%20forming%20a%20high-bandwidth%20encoding%20pattern.%20This%20results%20in%20an%20encoding%20mismatch%20between%20wide%20teachers%20and%20narrow%20students.%20Motivated%20by%20this%20insight%2C%20we%20propose%20two%20minimal%2C%20mismatch-driven%20strategies%3A%20%281%29%20post-hoc%20feature%20lifting%20with%20a%20lightweight%20projector%20retained%20during%20inference%2C%20or%20%282%29%20native%20width%20alignment%20that%20widens%20only%20the%20student%27s%20last%20block%20to%20the%20teacher%27s%20width.%20On%20ImageNet-1K%2C%20these%20strategies%20reactivate%20simple%20feature-map%20distillation%20in%20ViTs%2C%20raising%20DeiT-Tiny%20accuracy%20from%20%2474.86%5C%25%24%20to%20%2477.53%5C%25%24%20and%20%2478.23%5C%25%24%20when%20distilling%20from%20CaiT-S24%2C%20while%20also%20improving%20standalone%20students%20trained%20without%20any%20teacher.%20Our%20analysis%20thus%20explains%20why%20ViT%20feature%20distillation%20fails%20and%20shows%20how%20exploiting%20low-rank%20structure%20yields%20effective%2C%20interpretable%20remedies%20and%20concrete%20design%20guidance%20for%20compact%20ViTs.&entry.1838667208=http%3A//arxiv.org/abs/2511.15572v1&entry.124074799=Read"},
{"title": "Robust Adaptive Safe Robotic Grasping with Tactile Sensing", "author": "Yitaek Kim and Jeeseop Kim and Albert H. Li and Aaron D. Ames and Christoffer Sloth", "abstract": "Robotic grasping requires safe force interaction to prevent a grasped object from being damaged or slipping out of the hand. In this vein, this paper proposes an integrated framework for grasping with formal safety guarantees based on Control Barrier Functions. We first design contact force and force closure constraints, which are enforced by a safety filter to accomplish safe grasping with finger force control. For sensory feedback, we develop a technique to estimate contact point, force, and torque from tactile sensors at each finger. We verify the framework with various safety filters in a numerical simulation under a two-finger grasping scenario. We then experimentally validate the framework by grasping multiple objects, including fragile lab glassware, in a real robotic setup, showing that safe grasping can be successfully achieved in the real world. We evaluate the performance of each safety filter in the context of safety violation and conservatism, and find that disturbance observer-based control barrier functions provide superior performance for safety guarantees with minimum conservatism.", "link": "http://arxiv.org/abs/2411.07833v2", "date": "2025-11-19", "relevancy": 2.166, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5468}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing&body=Title%3A%20Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing%0AAuthor%3A%20Yitaek%20Kim%20and%20Jeeseop%20Kim%20and%20Albert%20H.%20Li%20and%20Aaron%20D.%20Ames%20and%20Christoffer%20Sloth%0AAbstract%3A%20Robotic%20grasping%20requires%20safe%20force%20interaction%20to%20prevent%20a%20grasped%20object%20from%20being%20damaged%20or%20slipping%20out%20of%20the%20hand.%20In%20this%20vein%2C%20this%20paper%20proposes%20an%20integrated%20framework%20for%20grasping%20with%20formal%20safety%20guarantees%20based%20on%20Control%20Barrier%20Functions.%20We%20first%20design%20contact%20force%20and%20force%20closure%20constraints%2C%20which%20are%20enforced%20by%20a%20safety%20filter%20to%20accomplish%20safe%20grasping%20with%20finger%20force%20control.%20For%20sensory%20feedback%2C%20we%20develop%20a%20technique%20to%20estimate%20contact%20point%2C%20force%2C%20and%20torque%20from%20tactile%20sensors%20at%20each%20finger.%20We%20verify%20the%20framework%20with%20various%20safety%20filters%20in%20a%20numerical%20simulation%20under%20a%20two-finger%20grasping%20scenario.%20We%20then%20experimentally%20validate%20the%20framework%20by%20grasping%20multiple%20objects%2C%20including%20fragile%20lab%20glassware%2C%20in%20a%20real%20robotic%20setup%2C%20showing%20that%20safe%20grasping%20can%20be%20successfully%20achieved%20in%20the%20real%20world.%20We%20evaluate%20the%20performance%20of%20each%20safety%20filter%20in%20the%20context%20of%20safety%20violation%20and%20conservatism%2C%20and%20find%20that%20disturbance%20observer-based%20control%20barrier%20functions%20provide%20superior%20performance%20for%20safety%20guarantees%20with%20minimum%20conservatism.%0ALink%3A%20http%3A//arxiv.org/abs/2411.07833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Adaptive%2520Safe%2520Robotic%2520Grasping%2520with%2520Tactile%2520Sensing%26entry.906535625%3DYitaek%2520Kim%2520and%2520Jeeseop%2520Kim%2520and%2520Albert%2520H.%2520Li%2520and%2520Aaron%2520D.%2520Ames%2520and%2520Christoffer%2520Sloth%26entry.1292438233%3DRobotic%2520grasping%2520requires%2520safe%2520force%2520interaction%2520to%2520prevent%2520a%2520grasped%2520object%2520from%2520being%2520damaged%2520or%2520slipping%2520out%2520of%2520the%2520hand.%2520In%2520this%2520vein%252C%2520this%2520paper%2520proposes%2520an%2520integrated%2520framework%2520for%2520grasping%2520with%2520formal%2520safety%2520guarantees%2520based%2520on%2520Control%2520Barrier%2520Functions.%2520We%2520first%2520design%2520contact%2520force%2520and%2520force%2520closure%2520constraints%252C%2520which%2520are%2520enforced%2520by%2520a%2520safety%2520filter%2520to%2520accomplish%2520safe%2520grasping%2520with%2520finger%2520force%2520control.%2520For%2520sensory%2520feedback%252C%2520we%2520develop%2520a%2520technique%2520to%2520estimate%2520contact%2520point%252C%2520force%252C%2520and%2520torque%2520from%2520tactile%2520sensors%2520at%2520each%2520finger.%2520We%2520verify%2520the%2520framework%2520with%2520various%2520safety%2520filters%2520in%2520a%2520numerical%2520simulation%2520under%2520a%2520two-finger%2520grasping%2520scenario.%2520We%2520then%2520experimentally%2520validate%2520the%2520framework%2520by%2520grasping%2520multiple%2520objects%252C%2520including%2520fragile%2520lab%2520glassware%252C%2520in%2520a%2520real%2520robotic%2520setup%252C%2520showing%2520that%2520safe%2520grasping%2520can%2520be%2520successfully%2520achieved%2520in%2520the%2520real%2520world.%2520We%2520evaluate%2520the%2520performance%2520of%2520each%2520safety%2520filter%2520in%2520the%2520context%2520of%2520safety%2520violation%2520and%2520conservatism%252C%2520and%2520find%2520that%2520disturbance%2520observer-based%2520control%2520barrier%2520functions%2520provide%2520superior%2520performance%2520for%2520safety%2520guarantees%2520with%2520minimum%2520conservatism.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing&entry.906535625=Yitaek%20Kim%20and%20Jeeseop%20Kim%20and%20Albert%20H.%20Li%20and%20Aaron%20D.%20Ames%20and%20Christoffer%20Sloth&entry.1292438233=Robotic%20grasping%20requires%20safe%20force%20interaction%20to%20prevent%20a%20grasped%20object%20from%20being%20damaged%20or%20slipping%20out%20of%20the%20hand.%20In%20this%20vein%2C%20this%20paper%20proposes%20an%20integrated%20framework%20for%20grasping%20with%20formal%20safety%20guarantees%20based%20on%20Control%20Barrier%20Functions.%20We%20first%20design%20contact%20force%20and%20force%20closure%20constraints%2C%20which%20are%20enforced%20by%20a%20safety%20filter%20to%20accomplish%20safe%20grasping%20with%20finger%20force%20control.%20For%20sensory%20feedback%2C%20we%20develop%20a%20technique%20to%20estimate%20contact%20point%2C%20force%2C%20and%20torque%20from%20tactile%20sensors%20at%20each%20finger.%20We%20verify%20the%20framework%20with%20various%20safety%20filters%20in%20a%20numerical%20simulation%20under%20a%20two-finger%20grasping%20scenario.%20We%20then%20experimentally%20validate%20the%20framework%20by%20grasping%20multiple%20objects%2C%20including%20fragile%20lab%20glassware%2C%20in%20a%20real%20robotic%20setup%2C%20showing%20that%20safe%20grasping%20can%20be%20successfully%20achieved%20in%20the%20real%20world.%20We%20evaluate%20the%20performance%20of%20each%20safety%20filter%20in%20the%20context%20of%20safety%20violation%20and%20conservatism%2C%20and%20find%20that%20disturbance%20observer-based%20control%20barrier%20functions%20provide%20superior%20performance%20for%20safety%20guarantees%20with%20minimum%20conservatism.&entry.1838667208=http%3A//arxiv.org/abs/2411.07833v2&entry.124074799=Read"},
{"title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "author": "Steven Bedrick and A. Seza Do\u011fru\u00f6z and Sergiu Nisioi", "abstract": "Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.", "link": "http://arxiv.org/abs/2505.03025v2", "date": "2025-11-19", "relevancy": 2.1575, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4654}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Typology%20of%20Synthetic%20Datasets%20for%20Dialogue%20Processing%20in%20Clinical%20Contexts&body=Title%3A%20A%20Typology%20of%20Synthetic%20Datasets%20for%20Dialogue%20Processing%20in%20Clinical%20Contexts%0AAuthor%3A%20Steven%20Bedrick%20and%20A.%20Seza%20Do%C4%9Fru%C3%B6z%20and%20Sergiu%20Nisioi%0AAbstract%3A%20Synthetic%20data%20sets%20are%20used%20across%20linguistic%20domains%20and%20NLP%20tasks%2C%20particularly%20in%20scenarios%20where%20authentic%20data%20is%20limited%20%28or%20even%20non-existent%29.%20One%20such%20domain%20is%20that%20of%20clinical%20%28healthcare%29%20contexts%2C%20where%20there%20exist%20significant%20and%20long-standing%20challenges%20%28e.g.%2C%20privacy%2C%20anonymization%2C%20and%20data%20governance%29%20which%20have%20led%20to%20the%20development%20of%20an%20increasing%20number%20of%20synthetic%20datasets.%20One%20increasingly%20important%20category%20of%20clinical%20dataset%20is%20that%20of%20clinical%20dialogues%20which%20are%20especially%20sensitive%20and%20difficult%20to%20collect%2C%20and%20as%20such%20are%20commonly%20synthesized.%0A%20%20While%20such%20synthetic%20datasets%20have%20been%20shown%20to%20be%20sufficient%20in%20some%20situations%2C%20little%20theory%20exists%20to%20inform%20how%20they%20may%20be%20best%20used%20and%20generalized%20to%20new%20applications.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20how%20synthetic%20datasets%20are%20created%2C%20evaluated%20and%20being%20used%20for%20dialogue%20related%20tasks%20in%20the%20medical%20domain.%20Additionally%2C%20we%20propose%20a%20novel%20typology%20for%20use%20in%20classifying%20types%20and%20degrees%20of%20data%20synthesis%2C%20to%20facilitate%20comparison%20and%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2505.03025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Typology%2520of%2520Synthetic%2520Datasets%2520for%2520Dialogue%2520Processing%2520in%2520Clinical%2520Contexts%26entry.906535625%3DSteven%2520Bedrick%2520and%2520A.%2520Seza%2520Do%25C4%259Fru%25C3%25B6z%2520and%2520Sergiu%2520Nisioi%26entry.1292438233%3DSynthetic%2520data%2520sets%2520are%2520used%2520across%2520linguistic%2520domains%2520and%2520NLP%2520tasks%252C%2520particularly%2520in%2520scenarios%2520where%2520authentic%2520data%2520is%2520limited%2520%2528or%2520even%2520non-existent%2529.%2520One%2520such%2520domain%2520is%2520that%2520of%2520clinical%2520%2528healthcare%2529%2520contexts%252C%2520where%2520there%2520exist%2520significant%2520and%2520long-standing%2520challenges%2520%2528e.g.%252C%2520privacy%252C%2520anonymization%252C%2520and%2520data%2520governance%2529%2520which%2520have%2520led%2520to%2520the%2520development%2520of%2520an%2520increasing%2520number%2520of%2520synthetic%2520datasets.%2520One%2520increasingly%2520important%2520category%2520of%2520clinical%2520dataset%2520is%2520that%2520of%2520clinical%2520dialogues%2520which%2520are%2520especially%2520sensitive%2520and%2520difficult%2520to%2520collect%252C%2520and%2520as%2520such%2520are%2520commonly%2520synthesized.%250A%2520%2520While%2520such%2520synthetic%2520datasets%2520have%2520been%2520shown%2520to%2520be%2520sufficient%2520in%2520some%2520situations%252C%2520little%2520theory%2520exists%2520to%2520inform%2520how%2520they%2520may%2520be%2520best%2520used%2520and%2520generalized%2520to%2520new%2520applications.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520overview%2520of%2520how%2520synthetic%2520datasets%2520are%2520created%252C%2520evaluated%2520and%2520being%2520used%2520for%2520dialogue%2520related%2520tasks%2520in%2520the%2520medical%2520domain.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520typology%2520for%2520use%2520in%2520classifying%2520types%2520and%2520degrees%2520of%2520data%2520synthesis%252C%2520to%2520facilitate%2520comparison%2520and%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Typology%20of%20Synthetic%20Datasets%20for%20Dialogue%20Processing%20in%20Clinical%20Contexts&entry.906535625=Steven%20Bedrick%20and%20A.%20Seza%20Do%C4%9Fru%C3%B6z%20and%20Sergiu%20Nisioi&entry.1292438233=Synthetic%20data%20sets%20are%20used%20across%20linguistic%20domains%20and%20NLP%20tasks%2C%20particularly%20in%20scenarios%20where%20authentic%20data%20is%20limited%20%28or%20even%20non-existent%29.%20One%20such%20domain%20is%20that%20of%20clinical%20%28healthcare%29%20contexts%2C%20where%20there%20exist%20significant%20and%20long-standing%20challenges%20%28e.g.%2C%20privacy%2C%20anonymization%2C%20and%20data%20governance%29%20which%20have%20led%20to%20the%20development%20of%20an%20increasing%20number%20of%20synthetic%20datasets.%20One%20increasingly%20important%20category%20of%20clinical%20dataset%20is%20that%20of%20clinical%20dialogues%20which%20are%20especially%20sensitive%20and%20difficult%20to%20collect%2C%20and%20as%20such%20are%20commonly%20synthesized.%0A%20%20While%20such%20synthetic%20datasets%20have%20been%20shown%20to%20be%20sufficient%20in%20some%20situations%2C%20little%20theory%20exists%20to%20inform%20how%20they%20may%20be%20best%20used%20and%20generalized%20to%20new%20applications.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20how%20synthetic%20datasets%20are%20created%2C%20evaluated%20and%20being%20used%20for%20dialogue%20related%20tasks%20in%20the%20medical%20domain.%20Additionally%2C%20we%20propose%20a%20novel%20typology%20for%20use%20in%20classifying%20types%20and%20degrees%20of%20data%20synthesis%2C%20to%20facilitate%20comparison%20and%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2505.03025v2&entry.124074799=Read"},
{"title": "WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes", "author": "Marc-Emmanuel Coupvent des Graviers and Hejer Ammar and Christophe Guettier and Yann Dumortier and Romaric Audigier", "abstract": "We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.", "link": "http://arxiv.org/abs/2511.15429v1", "date": "2025-11-19", "relevancy": 2.1543, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5535}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.532}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WarNav%3A%20An%20Autonomous%20Driving%20Benchmark%20for%20Segmentation%20of%20Navigable%20Zones%20in%20War%20Scenes&body=Title%3A%20WarNav%3A%20An%20Autonomous%20Driving%20Benchmark%20for%20Segmentation%20of%20Navigable%20Zones%20in%20War%20Scenes%0AAuthor%3A%20Marc-Emmanuel%20Coupvent%20des%20Graviers%20and%20Hejer%20Ammar%20and%20Christophe%20Guettier%20and%20Yann%20Dumortier%20and%20Romaric%20Audigier%0AAbstract%3A%20We%20introduce%20WarNav%2C%20a%20novel%20real-world%20dataset%20constructed%20from%20images%20of%20the%20open-source%20DATTALION%20repository%2C%20specifically%20tailored%20to%20enable%20the%20development%20and%20benchmarking%20of%20semantic%20segmentation%20models%20for%20autonomous%20ground%20vehicle%20navigation%20in%20unstructured%2C%20conflict-affected%20environments.%20This%20dataset%20addresses%20a%20critical%20gap%20between%20conventional%20urban%20driving%20resources%20and%20the%20unique%20operational%20scenarios%20encountered%20by%20unmanned%20systems%20in%20hazardous%20and%20damaged%20war-zones.%20We%20detail%20the%20methodological%20challenges%20encountered%2C%20ranging%20from%20data%20heterogeneity%20to%20ethical%20considerations%2C%20providing%20guidance%20for%20future%20efforts%20that%20target%20extreme%20operational%20contexts.%20To%20establish%20performance%20references%2C%20we%20report%20baseline%20results%20on%20WarNav%20using%20several%20state-of-the-art%20semantic%20segmentation%20models%20trained%20on%20structured%20urban%20scenes.%20We%20further%20analyse%20the%20impact%20of%20training%20data%20environments%20and%20propose%20a%20first%20step%20towards%20effective%20navigability%20in%20challenging%20environments%20with%20the%20constraint%20of%20having%20no%20annotation%20of%20the%20targeted%20images.%20Our%20goal%20is%20to%20foster%20impactful%20research%20that%20enhances%20the%20robustness%20and%20safety%20of%20autonomous%20vehicles%20in%20high-risk%20scenarios%20while%20being%20frugal%20in%20annotated%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarNav%253A%2520An%2520Autonomous%2520Driving%2520Benchmark%2520for%2520Segmentation%2520of%2520Navigable%2520Zones%2520in%2520War%2520Scenes%26entry.906535625%3DMarc-Emmanuel%2520Coupvent%2520des%2520Graviers%2520and%2520Hejer%2520Ammar%2520and%2520Christophe%2520Guettier%2520and%2520Yann%2520Dumortier%2520and%2520Romaric%2520Audigier%26entry.1292438233%3DWe%2520introduce%2520WarNav%252C%2520a%2520novel%2520real-world%2520dataset%2520constructed%2520from%2520images%2520of%2520the%2520open-source%2520DATTALION%2520repository%252C%2520specifically%2520tailored%2520to%2520enable%2520the%2520development%2520and%2520benchmarking%2520of%2520semantic%2520segmentation%2520models%2520for%2520autonomous%2520ground%2520vehicle%2520navigation%2520in%2520unstructured%252C%2520conflict-affected%2520environments.%2520This%2520dataset%2520addresses%2520a%2520critical%2520gap%2520between%2520conventional%2520urban%2520driving%2520resources%2520and%2520the%2520unique%2520operational%2520scenarios%2520encountered%2520by%2520unmanned%2520systems%2520in%2520hazardous%2520and%2520damaged%2520war-zones.%2520We%2520detail%2520the%2520methodological%2520challenges%2520encountered%252C%2520ranging%2520from%2520data%2520heterogeneity%2520to%2520ethical%2520considerations%252C%2520providing%2520guidance%2520for%2520future%2520efforts%2520that%2520target%2520extreme%2520operational%2520contexts.%2520To%2520establish%2520performance%2520references%252C%2520we%2520report%2520baseline%2520results%2520on%2520WarNav%2520using%2520several%2520state-of-the-art%2520semantic%2520segmentation%2520models%2520trained%2520on%2520structured%2520urban%2520scenes.%2520We%2520further%2520analyse%2520the%2520impact%2520of%2520training%2520data%2520environments%2520and%2520propose%2520a%2520first%2520step%2520towards%2520effective%2520navigability%2520in%2520challenging%2520environments%2520with%2520the%2520constraint%2520of%2520having%2520no%2520annotation%2520of%2520the%2520targeted%2520images.%2520Our%2520goal%2520is%2520to%2520foster%2520impactful%2520research%2520that%2520enhances%2520the%2520robustness%2520and%2520safety%2520of%2520autonomous%2520vehicles%2520in%2520high-risk%2520scenarios%2520while%2520being%2520frugal%2520in%2520annotated%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WarNav%3A%20An%20Autonomous%20Driving%20Benchmark%20for%20Segmentation%20of%20Navigable%20Zones%20in%20War%20Scenes&entry.906535625=Marc-Emmanuel%20Coupvent%20des%20Graviers%20and%20Hejer%20Ammar%20and%20Christophe%20Guettier%20and%20Yann%20Dumortier%20and%20Romaric%20Audigier&entry.1292438233=We%20introduce%20WarNav%2C%20a%20novel%20real-world%20dataset%20constructed%20from%20images%20of%20the%20open-source%20DATTALION%20repository%2C%20specifically%20tailored%20to%20enable%20the%20development%20and%20benchmarking%20of%20semantic%20segmentation%20models%20for%20autonomous%20ground%20vehicle%20navigation%20in%20unstructured%2C%20conflict-affected%20environments.%20This%20dataset%20addresses%20a%20critical%20gap%20between%20conventional%20urban%20driving%20resources%20and%20the%20unique%20operational%20scenarios%20encountered%20by%20unmanned%20systems%20in%20hazardous%20and%20damaged%20war-zones.%20We%20detail%20the%20methodological%20challenges%20encountered%2C%20ranging%20from%20data%20heterogeneity%20to%20ethical%20considerations%2C%20providing%20guidance%20for%20future%20efforts%20that%20target%20extreme%20operational%20contexts.%20To%20establish%20performance%20references%2C%20we%20report%20baseline%20results%20on%20WarNav%20using%20several%20state-of-the-art%20semantic%20segmentation%20models%20trained%20on%20structured%20urban%20scenes.%20We%20further%20analyse%20the%20impact%20of%20training%20data%20environments%20and%20propose%20a%20first%20step%20towards%20effective%20navigability%20in%20challenging%20environments%20with%20the%20constraint%20of%20having%20no%20annotation%20of%20the%20targeted%20images.%20Our%20goal%20is%20to%20foster%20impactful%20research%20that%20enhances%20the%20robustness%20and%20safety%20of%20autonomous%20vehicles%20in%20high-risk%20scenarios%20while%20being%20frugal%20in%20annotated%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.15429v1&entry.124074799=Read"},
{"title": "ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning", "author": "Wenjie Zhu and Yabin Zhang and Xin Jin and Wenjun Zeng and Lei Zhang", "abstract": "The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.", "link": "http://arxiv.org/abs/2509.03951v3", "date": "2025-11-19", "relevancy": 2.1513, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANTS%3A%20Adaptive%20Negative%20Textual%20Space%20Shaping%20for%20OOD%20Detection%20via%20Test-Time%20MLLM%20Understanding%20and%20Reasoning&body=Title%3A%20ANTS%3A%20Adaptive%20Negative%20Textual%20Space%20Shaping%20for%20OOD%20Detection%20via%20Test-Time%20MLLM%20Understanding%20and%20Reasoning%0AAuthor%3A%20Wenjie%20Zhu%20and%20Yabin%20Zhang%20and%20Xin%20Jin%20and%20Wenjun%20Zeng%20and%20Lei%20Zhang%0AAbstract%3A%20The%20introduction%20of%20negative%20labels%20%28NLs%29%20has%20proven%20effective%20in%20enhancing%20Out-of-Distribution%20%28OOD%29%20detection.%20However%2C%20existing%20methods%20often%20lack%20an%20understanding%20of%20OOD%20images%2C%20making%20it%20difficult%20to%20construct%20an%20accurate%20negative%20space.%20Furthermore%2C%20the%20absence%20of%20negative%20labels%20semantically%20similar%20to%20ID%20labels%20constrains%20their%20capability%20in%20near-OOD%20detection.%20To%20address%20these%20issues%2C%20we%20propose%20shaping%20an%20Adaptive%20Negative%20Textual%20Space%20%28ANTS%29%20by%20leveraging%20the%20understanding%20and%20reasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Specifically%2C%20we%20cache%20images%20likely%20to%20be%20OOD%20samples%20from%20the%20historical%20test%20images%20and%20prompt%20the%20MLLM%20to%20describe%20these%20images%2C%20generating%20expressive%20negative%20sentences%20that%20precisely%20characterize%20the%20OOD%20distribution%20and%20enhance%20far-OOD%20detection.%20For%20the%20near-OOD%20setting%2C%20where%20OOD%20samples%20resemble%20the%20in-distribution%20%28ID%29%20subset%2C%20we%20cache%20the%20subset%20of%20ID%20classes%20that%20are%20visually%20similar%20to%20historical%20test%20images%20and%20then%20leverage%20MLLM%20reasoning%20to%20generate%20visually%20similar%20negative%20labels%20tailored%20to%20this%20subset%2C%20effectively%20reducing%20false%20negatives%20and%20improving%20near-OOD%20detection.%20To%20balance%20these%20two%20types%20of%20negative%20textual%20spaces%2C%20we%20design%20an%20adaptive%20weighted%20score%20that%20enables%20the%20method%20to%20handle%20different%20OOD%20task%20settings%20%28near-OOD%20and%20far-OOD%29%2C%20making%20it%20highly%20adaptable%20in%20open%20environments.%20On%20the%20ImageNet%20benchmark%2C%20our%20ANTS%20significantly%20reduces%20the%20FPR95%20by%203.1%5C%25%2C%20establishing%20a%20new%20state-of-the-art.%20Furthermore%2C%20our%20method%20is%20training-free%20and%20zero-shot%2C%20enabling%20high%20scalability.%0ALink%3A%20http%3A//arxiv.org/abs/2509.03951v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANTS%253A%2520Adaptive%2520Negative%2520Textual%2520Space%2520Shaping%2520for%2520OOD%2520Detection%2520via%2520Test-Time%2520MLLM%2520Understanding%2520and%2520Reasoning%26entry.906535625%3DWenjie%2520Zhu%2520and%2520Yabin%2520Zhang%2520and%2520Xin%2520Jin%2520and%2520Wenjun%2520Zeng%2520and%2520Lei%2520Zhang%26entry.1292438233%3DThe%2520introduction%2520of%2520negative%2520labels%2520%2528NLs%2529%2520has%2520proven%2520effective%2520in%2520enhancing%2520Out-of-Distribution%2520%2528OOD%2529%2520detection.%2520However%252C%2520existing%2520methods%2520often%2520lack%2520an%2520understanding%2520of%2520OOD%2520images%252C%2520making%2520it%2520difficult%2520to%2520construct%2520an%2520accurate%2520negative%2520space.%2520Furthermore%252C%2520the%2520absence%2520of%2520negative%2520labels%2520semantically%2520similar%2520to%2520ID%2520labels%2520constrains%2520their%2520capability%2520in%2520near-OOD%2520detection.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520shaping%2520an%2520Adaptive%2520Negative%2520Textual%2520Space%2520%2528ANTS%2529%2520by%2520leveraging%2520the%2520understanding%2520and%2520reasoning%2520capabilities%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Specifically%252C%2520we%2520cache%2520images%2520likely%2520to%2520be%2520OOD%2520samples%2520from%2520the%2520historical%2520test%2520images%2520and%2520prompt%2520the%2520MLLM%2520to%2520describe%2520these%2520images%252C%2520generating%2520expressive%2520negative%2520sentences%2520that%2520precisely%2520characterize%2520the%2520OOD%2520distribution%2520and%2520enhance%2520far-OOD%2520detection.%2520For%2520the%2520near-OOD%2520setting%252C%2520where%2520OOD%2520samples%2520resemble%2520the%2520in-distribution%2520%2528ID%2529%2520subset%252C%2520we%2520cache%2520the%2520subset%2520of%2520ID%2520classes%2520that%2520are%2520visually%2520similar%2520to%2520historical%2520test%2520images%2520and%2520then%2520leverage%2520MLLM%2520reasoning%2520to%2520generate%2520visually%2520similar%2520negative%2520labels%2520tailored%2520to%2520this%2520subset%252C%2520effectively%2520reducing%2520false%2520negatives%2520and%2520improving%2520near-OOD%2520detection.%2520To%2520balance%2520these%2520two%2520types%2520of%2520negative%2520textual%2520spaces%252C%2520we%2520design%2520an%2520adaptive%2520weighted%2520score%2520that%2520enables%2520the%2520method%2520to%2520handle%2520different%2520OOD%2520task%2520settings%2520%2528near-OOD%2520and%2520far-OOD%2529%252C%2520making%2520it%2520highly%2520adaptable%2520in%2520open%2520environments.%2520On%2520the%2520ImageNet%2520benchmark%252C%2520our%2520ANTS%2520significantly%2520reduces%2520the%2520FPR95%2520by%25203.1%255C%2525%252C%2520establishing%2520a%2520new%2520state-of-the-art.%2520Furthermore%252C%2520our%2520method%2520is%2520training-free%2520and%2520zero-shot%252C%2520enabling%2520high%2520scalability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03951v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANTS%3A%20Adaptive%20Negative%20Textual%20Space%20Shaping%20for%20OOD%20Detection%20via%20Test-Time%20MLLM%20Understanding%20and%20Reasoning&entry.906535625=Wenjie%20Zhu%20and%20Yabin%20Zhang%20and%20Xin%20Jin%20and%20Wenjun%20Zeng%20and%20Lei%20Zhang&entry.1292438233=The%20introduction%20of%20negative%20labels%20%28NLs%29%20has%20proven%20effective%20in%20enhancing%20Out-of-Distribution%20%28OOD%29%20detection.%20However%2C%20existing%20methods%20often%20lack%20an%20understanding%20of%20OOD%20images%2C%20making%20it%20difficult%20to%20construct%20an%20accurate%20negative%20space.%20Furthermore%2C%20the%20absence%20of%20negative%20labels%20semantically%20similar%20to%20ID%20labels%20constrains%20their%20capability%20in%20near-OOD%20detection.%20To%20address%20these%20issues%2C%20we%20propose%20shaping%20an%20Adaptive%20Negative%20Textual%20Space%20%28ANTS%29%20by%20leveraging%20the%20understanding%20and%20reasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Specifically%2C%20we%20cache%20images%20likely%20to%20be%20OOD%20samples%20from%20the%20historical%20test%20images%20and%20prompt%20the%20MLLM%20to%20describe%20these%20images%2C%20generating%20expressive%20negative%20sentences%20that%20precisely%20characterize%20the%20OOD%20distribution%20and%20enhance%20far-OOD%20detection.%20For%20the%20near-OOD%20setting%2C%20where%20OOD%20samples%20resemble%20the%20in-distribution%20%28ID%29%20subset%2C%20we%20cache%20the%20subset%20of%20ID%20classes%20that%20are%20visually%20similar%20to%20historical%20test%20images%20and%20then%20leverage%20MLLM%20reasoning%20to%20generate%20visually%20similar%20negative%20labels%20tailored%20to%20this%20subset%2C%20effectively%20reducing%20false%20negatives%20and%20improving%20near-OOD%20detection.%20To%20balance%20these%20two%20types%20of%20negative%20textual%20spaces%2C%20we%20design%20an%20adaptive%20weighted%20score%20that%20enables%20the%20method%20to%20handle%20different%20OOD%20task%20settings%20%28near-OOD%20and%20far-OOD%29%2C%20making%20it%20highly%20adaptable%20in%20open%20environments.%20On%20the%20ImageNet%20benchmark%2C%20our%20ANTS%20significantly%20reduces%20the%20FPR95%20by%203.1%5C%25%2C%20establishing%20a%20new%20state-of-the-art.%20Furthermore%2C%20our%20method%20is%20training-free%20and%20zero-shot%2C%20enabling%20high%20scalability.&entry.1838667208=http%3A//arxiv.org/abs/2509.03951v3&entry.124074799=Read"},
{"title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data", "author": "Xiongyi Cai and Ri-Zhao Qiu and Geng Chen and Lai Wei and Isabella Liu and Tianshu Huang and Xuxin Cheng and Xiaolong Wang", "abstract": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/", "link": "http://arxiv.org/abs/2511.15704v1", "date": "2025-11-19", "relevancy": 2.1511, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5463}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5409}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-N-On%3A%20Scaling%20Egocentric%20Manipulation%20with%20in-the-wild%20and%20on-task%20Data&body=Title%3A%20In-N-On%3A%20Scaling%20Egocentric%20Manipulation%20with%20in-the-wild%20and%20on-task%20Data%0AAuthor%3A%20Xiongyi%20Cai%20and%20Ri-Zhao%20Qiu%20and%20Geng%20Chen%20and%20Lai%20Wei%20and%20Isabella%20Liu%20and%20Tianshu%20Huang%20and%20Xuxin%20Cheng%20and%20Xiaolong%20Wang%0AAbstract%3A%20Egocentric%20videos%20are%20a%20valuable%20and%20scalable%20data%20source%20to%20learn%20manipulation%20policies.%20However%2C%20due%20to%20significant%20data%20heterogeneity%2C%20most%20existing%20approaches%20utilize%20human%20data%20for%20simple%20pre-training%2C%20which%20does%20not%20unlock%20its%20full%20potential.%20This%20paper%20first%20provides%20a%20scalable%20recipe%20for%20collecting%20and%20using%20egocentric%20data%20by%20categorizing%20human%20data%20into%20two%20categories%3A%20in-the-wild%20and%20on-task%20alongside%20with%20systematic%20analysis%20on%20how%20to%20use%20the%20data.%20We%20first%20curate%20a%20dataset%2C%20PHSD%2C%20which%20contains%20over%201%2C000%20hours%20of%20diverse%20in-the-wild%20egocentric%20data%20and%20over%2020%20hours%20of%20on-task%20data%20directly%20aligned%20to%20the%20target%20manipulation%20tasks.%20This%20enables%20learning%20a%20large%20egocentric%20language-conditioned%20flow%20matching%20policy%2C%20Human0.%20With%20domain%20adaptation%20techniques%2C%20Human0%20minimizes%20the%20gap%20between%20humans%20and%20humanoids.%20Empirically%2C%20we%20show%20Human0%20achieves%20several%20novel%20properties%20from%20scaling%20human%20data%2C%20including%20language%20following%20of%20instructions%20from%20only%20human%20data%2C%20few-shot%20learning%2C%20and%20improved%20robustness%20using%20on-task%20data.%20Project%20website%3A%20https%3A//xiongyicai.github.io/In-N-On/%0ALink%3A%20http%3A//arxiv.org/abs/2511.15704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-N-On%253A%2520Scaling%2520Egocentric%2520Manipulation%2520with%2520in-the-wild%2520and%2520on-task%2520Data%26entry.906535625%3DXiongyi%2520Cai%2520and%2520Ri-Zhao%2520Qiu%2520and%2520Geng%2520Chen%2520and%2520Lai%2520Wei%2520and%2520Isabella%2520Liu%2520and%2520Tianshu%2520Huang%2520and%2520Xuxin%2520Cheng%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3DEgocentric%2520videos%2520are%2520a%2520valuable%2520and%2520scalable%2520data%2520source%2520to%2520learn%2520manipulation%2520policies.%2520However%252C%2520due%2520to%2520significant%2520data%2520heterogeneity%252C%2520most%2520existing%2520approaches%2520utilize%2520human%2520data%2520for%2520simple%2520pre-training%252C%2520which%2520does%2520not%2520unlock%2520its%2520full%2520potential.%2520This%2520paper%2520first%2520provides%2520a%2520scalable%2520recipe%2520for%2520collecting%2520and%2520using%2520egocentric%2520data%2520by%2520categorizing%2520human%2520data%2520into%2520two%2520categories%253A%2520in-the-wild%2520and%2520on-task%2520alongside%2520with%2520systematic%2520analysis%2520on%2520how%2520to%2520use%2520the%2520data.%2520We%2520first%2520curate%2520a%2520dataset%252C%2520PHSD%252C%2520which%2520contains%2520over%25201%252C000%2520hours%2520of%2520diverse%2520in-the-wild%2520egocentric%2520data%2520and%2520over%252020%2520hours%2520of%2520on-task%2520data%2520directly%2520aligned%2520to%2520the%2520target%2520manipulation%2520tasks.%2520This%2520enables%2520learning%2520a%2520large%2520egocentric%2520language-conditioned%2520flow%2520matching%2520policy%252C%2520Human0.%2520With%2520domain%2520adaptation%2520techniques%252C%2520Human0%2520minimizes%2520the%2520gap%2520between%2520humans%2520and%2520humanoids.%2520Empirically%252C%2520we%2520show%2520Human0%2520achieves%2520several%2520novel%2520properties%2520from%2520scaling%2520human%2520data%252C%2520including%2520language%2520following%2520of%2520instructions%2520from%2520only%2520human%2520data%252C%2520few-shot%2520learning%252C%2520and%2520improved%2520robustness%2520using%2520on-task%2520data.%2520Project%2520website%253A%2520https%253A//xiongyicai.github.io/In-N-On/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-N-On%3A%20Scaling%20Egocentric%20Manipulation%20with%20in-the-wild%20and%20on-task%20Data&entry.906535625=Xiongyi%20Cai%20and%20Ri-Zhao%20Qiu%20and%20Geng%20Chen%20and%20Lai%20Wei%20and%20Isabella%20Liu%20and%20Tianshu%20Huang%20and%20Xuxin%20Cheng%20and%20Xiaolong%20Wang&entry.1292438233=Egocentric%20videos%20are%20a%20valuable%20and%20scalable%20data%20source%20to%20learn%20manipulation%20policies.%20However%2C%20due%20to%20significant%20data%20heterogeneity%2C%20most%20existing%20approaches%20utilize%20human%20data%20for%20simple%20pre-training%2C%20which%20does%20not%20unlock%20its%20full%20potential.%20This%20paper%20first%20provides%20a%20scalable%20recipe%20for%20collecting%20and%20using%20egocentric%20data%20by%20categorizing%20human%20data%20into%20two%20categories%3A%20in-the-wild%20and%20on-task%20alongside%20with%20systematic%20analysis%20on%20how%20to%20use%20the%20data.%20We%20first%20curate%20a%20dataset%2C%20PHSD%2C%20which%20contains%20over%201%2C000%20hours%20of%20diverse%20in-the-wild%20egocentric%20data%20and%20over%2020%20hours%20of%20on-task%20data%20directly%20aligned%20to%20the%20target%20manipulation%20tasks.%20This%20enables%20learning%20a%20large%20egocentric%20language-conditioned%20flow%20matching%20policy%2C%20Human0.%20With%20domain%20adaptation%20techniques%2C%20Human0%20minimizes%20the%20gap%20between%20humans%20and%20humanoids.%20Empirically%2C%20we%20show%20Human0%20achieves%20several%20novel%20properties%20from%20scaling%20human%20data%2C%20including%20language%20following%20of%20instructions%20from%20only%20human%20data%2C%20few-shot%20learning%2C%20and%20improved%20robustness%20using%20on-task%20data.%20Project%20website%3A%20https%3A//xiongyicai.github.io/In-N-On/&entry.1838667208=http%3A//arxiv.org/abs/2511.15704v1&entry.124074799=Read"},
{"title": "Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection", "author": "Fanxiao Li and Jiaying Wu and Tingchao Fu and Yunyun Dong and Bingbing Song and Wei Zhou", "abstract": "The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.", "link": "http://arxiv.org/abs/2508.12711v3", "date": "2025-11-19", "relevancy": 2.1394, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5525}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5346}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection&body=Title%3A%20Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection%0AAuthor%3A%20Fanxiao%20Li%20and%20Jiaying%20Wu%20and%20Tingchao%20Fu%20and%20Yunyun%20Dong%20and%20Bingbing%20Song%20and%20Wei%20Zhou%0AAbstract%3A%20The%20proliferation%20of%20multimodal%20misinformation%20poses%20growing%20threats%20to%20public%20discourse%20and%20societal%20trust.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20enabled%20recent%20progress%20in%20multimodal%20misinformation%20detection%20%28MMD%29%2C%20the%20rise%20of%20generative%20AI%20%28GenAI%29%20tools%20introduces%20a%20new%20challenge%3A%20GenAI-driven%20news%20diversity%2C%20characterized%20by%20highly%20varied%20and%20complex%20content.%20We%20show%20that%20this%20diversity%20induces%20multi-level%20drift%2C%20comprising%20%281%29%20model-level%20misperception%20drift%2C%20where%20stylistic%20variations%20disrupt%20a%20model%27s%20internal%20reasoning%2C%20and%20%282%29%20evidence-level%20drift%2C%20where%20expression%20diversity%20degrades%20the%20quality%20or%20relevance%20of%20retrieved%20external%20evidence.%20These%20drifts%20significantly%20degrade%20the%20robustness%20of%20current%20LVLM-based%20MMD%20systems.%20To%20systematically%20study%20this%20problem%2C%20we%20introduce%20DriftBench%2C%20a%20large-scale%20benchmark%20comprising%2016%2C000%20news%20instances%20across%20six%20categories%20of%20diversification.%20We%20design%20three%20evaluation%20tasks%3A%20%281%29%20robustness%20of%20truth%20verification%20under%20multi-level%20drift%3B%20%282%29%20susceptibility%20to%20adversarial%20evidence%20contamination%20generated%20by%20GenAI%3B%20and%20%283%29%20analysis%20of%20reasoning%20consistency%20across%20diverse%20inputs.%20Experiments%20with%20six%20state-of-the-art%20LVLM-based%20detectors%20show%20substantial%20performance%20drops%20%28average%20F1%20-14.8%25%29%20and%20increasingly%20unstable%20reasoning%20traces%2C%20with%20even%20more%20severe%20failures%20under%20adversarial%20evidence%20injection.%20Our%20findings%20uncover%20fundamental%20vulnerabilities%20in%20existing%20MMD%20systems%20and%20suggest%20an%20urgent%20need%20for%20more%20resilient%20approaches%20in%20the%20GenAI%20era.%0ALink%3A%20http%3A//arxiv.org/abs/2508.12711v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrifting%2520Away%2520from%2520Truth%253A%2520GenAI-Driven%2520News%2520Diversity%2520Challenges%2520LVLM-Based%2520Misinformation%2520Detection%26entry.906535625%3DFanxiao%2520Li%2520and%2520Jiaying%2520Wu%2520and%2520Tingchao%2520Fu%2520and%2520Yunyun%2520Dong%2520and%2520Bingbing%2520Song%2520and%2520Wei%2520Zhou%26entry.1292438233%3DThe%2520proliferation%2520of%2520multimodal%2520misinformation%2520poses%2520growing%2520threats%2520to%2520public%2520discourse%2520and%2520societal%2520trust.%2520While%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520enabled%2520recent%2520progress%2520in%2520multimodal%2520misinformation%2520detection%2520%2528MMD%2529%252C%2520the%2520rise%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520tools%2520introduces%2520a%2520new%2520challenge%253A%2520GenAI-driven%2520news%2520diversity%252C%2520characterized%2520by%2520highly%2520varied%2520and%2520complex%2520content.%2520We%2520show%2520that%2520this%2520diversity%2520induces%2520multi-level%2520drift%252C%2520comprising%2520%25281%2529%2520model-level%2520misperception%2520drift%252C%2520where%2520stylistic%2520variations%2520disrupt%2520a%2520model%2527s%2520internal%2520reasoning%252C%2520and%2520%25282%2529%2520evidence-level%2520drift%252C%2520where%2520expression%2520diversity%2520degrades%2520the%2520quality%2520or%2520relevance%2520of%2520retrieved%2520external%2520evidence.%2520These%2520drifts%2520significantly%2520degrade%2520the%2520robustness%2520of%2520current%2520LVLM-based%2520MMD%2520systems.%2520To%2520systematically%2520study%2520this%2520problem%252C%2520we%2520introduce%2520DriftBench%252C%2520a%2520large-scale%2520benchmark%2520comprising%252016%252C000%2520news%2520instances%2520across%2520six%2520categories%2520of%2520diversification.%2520We%2520design%2520three%2520evaluation%2520tasks%253A%2520%25281%2529%2520robustness%2520of%2520truth%2520verification%2520under%2520multi-level%2520drift%253B%2520%25282%2529%2520susceptibility%2520to%2520adversarial%2520evidence%2520contamination%2520generated%2520by%2520GenAI%253B%2520and%2520%25283%2529%2520analysis%2520of%2520reasoning%2520consistency%2520across%2520diverse%2520inputs.%2520Experiments%2520with%2520six%2520state-of-the-art%2520LVLM-based%2520detectors%2520show%2520substantial%2520performance%2520drops%2520%2528average%2520F1%2520-14.8%2525%2529%2520and%2520increasingly%2520unstable%2520reasoning%2520traces%252C%2520with%2520even%2520more%2520severe%2520failures%2520under%2520adversarial%2520evidence%2520injection.%2520Our%2520findings%2520uncover%2520fundamental%2520vulnerabilities%2520in%2520existing%2520MMD%2520systems%2520and%2520suggest%2520an%2520urgent%2520need%2520for%2520more%2520resilient%2520approaches%2520in%2520the%2520GenAI%2520era.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12711v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection&entry.906535625=Fanxiao%20Li%20and%20Jiaying%20Wu%20and%20Tingchao%20Fu%20and%20Yunyun%20Dong%20and%20Bingbing%20Song%20and%20Wei%20Zhou&entry.1292438233=The%20proliferation%20of%20multimodal%20misinformation%20poses%20growing%20threats%20to%20public%20discourse%20and%20societal%20trust.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20enabled%20recent%20progress%20in%20multimodal%20misinformation%20detection%20%28MMD%29%2C%20the%20rise%20of%20generative%20AI%20%28GenAI%29%20tools%20introduces%20a%20new%20challenge%3A%20GenAI-driven%20news%20diversity%2C%20characterized%20by%20highly%20varied%20and%20complex%20content.%20We%20show%20that%20this%20diversity%20induces%20multi-level%20drift%2C%20comprising%20%281%29%20model-level%20misperception%20drift%2C%20where%20stylistic%20variations%20disrupt%20a%20model%27s%20internal%20reasoning%2C%20and%20%282%29%20evidence-level%20drift%2C%20where%20expression%20diversity%20degrades%20the%20quality%20or%20relevance%20of%20retrieved%20external%20evidence.%20These%20drifts%20significantly%20degrade%20the%20robustness%20of%20current%20LVLM-based%20MMD%20systems.%20To%20systematically%20study%20this%20problem%2C%20we%20introduce%20DriftBench%2C%20a%20large-scale%20benchmark%20comprising%2016%2C000%20news%20instances%20across%20six%20categories%20of%20diversification.%20We%20design%20three%20evaluation%20tasks%3A%20%281%29%20robustness%20of%20truth%20verification%20under%20multi-level%20drift%3B%20%282%29%20susceptibility%20to%20adversarial%20evidence%20contamination%20generated%20by%20GenAI%3B%20and%20%283%29%20analysis%20of%20reasoning%20consistency%20across%20diverse%20inputs.%20Experiments%20with%20six%20state-of-the-art%20LVLM-based%20detectors%20show%20substantial%20performance%20drops%20%28average%20F1%20-14.8%25%29%20and%20increasingly%20unstable%20reasoning%20traces%2C%20with%20even%20more%20severe%20failures%20under%20adversarial%20evidence%20injection.%20Our%20findings%20uncover%20fundamental%20vulnerabilities%20in%20existing%20MMD%20systems%20and%20suggest%20an%20urgent%20need%20for%20more%20resilient%20approaches%20in%20the%20GenAI%20era.&entry.1838667208=http%3A//arxiv.org/abs/2508.12711v3&entry.124074799=Read"},
{"title": "MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation", "author": "Bin Xie and Gady Agam", "abstract": "Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.", "link": "http://arxiv.org/abs/2511.15603v1", "date": "2025-11-19", "relevancy": 2.1365, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5588}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskMed%3A%20Decoupled%20Mask%20and%20Class%20Prediction%20for%20Medical%20Image%20Segmentation&body=Title%3A%20MaskMed%3A%20Decoupled%20Mask%20and%20Class%20Prediction%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Bin%20Xie%20and%20Gady%20Agam%0AAbstract%3A%20Medical%20image%20segmentation%20typically%20adopts%20a%20point-wise%20convolutional%20segmentation%20head%20to%20predict%20dense%20labels%2C%20where%20each%20output%20channel%20is%20heuristically%20tied%20to%20a%20specific%20class.%20This%20rigid%20design%20limits%20both%20feature%20sharing%20and%20semantic%20generalization.%20In%20this%20work%2C%20we%20propose%20a%20unified%20decoupled%20segmentation%20head%20that%20separates%20multi-class%20prediction%20into%20class-agnostic%20mask%20prediction%20and%20class%20label%20prediction%20using%20shared%20object%20queries.%20Furthermore%2C%20we%20introduce%20a%20Full-Scale%20Aware%20Deformable%20Transformer%20module%20that%20enables%20low-resolution%20encoder%20features%20to%20attend%20across%20full-resolution%20encoder%20features%20via%20deformable%20attention%2C%20achieving%20memory-efficient%20and%20spatially%20aligned%20full-scale%20fusion.%20Our%20proposed%20method%2C%20named%20MaskMed%2C%20achieves%20state-of-the-art%20performance%2C%20surpassing%20nnUNet%20by%20%2B2.0%25%20Dice%20on%20AMOS%202022%20and%20%2B6.9%25%20Dice%20on%20BTCV.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskMed%253A%2520Decoupled%2520Mask%2520and%2520Class%2520Prediction%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DBin%2520Xie%2520and%2520Gady%2520Agam%26entry.1292438233%3DMedical%2520image%2520segmentation%2520typically%2520adopts%2520a%2520point-wise%2520convolutional%2520segmentation%2520head%2520to%2520predict%2520dense%2520labels%252C%2520where%2520each%2520output%2520channel%2520is%2520heuristically%2520tied%2520to%2520a%2520specific%2520class.%2520This%2520rigid%2520design%2520limits%2520both%2520feature%2520sharing%2520and%2520semantic%2520generalization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520decoupled%2520segmentation%2520head%2520that%2520separates%2520multi-class%2520prediction%2520into%2520class-agnostic%2520mask%2520prediction%2520and%2520class%2520label%2520prediction%2520using%2520shared%2520object%2520queries.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Full-Scale%2520Aware%2520Deformable%2520Transformer%2520module%2520that%2520enables%2520low-resolution%2520encoder%2520features%2520to%2520attend%2520across%2520full-resolution%2520encoder%2520features%2520via%2520deformable%2520attention%252C%2520achieving%2520memory-efficient%2520and%2520spatially%2520aligned%2520full-scale%2520fusion.%2520Our%2520proposed%2520method%252C%2520named%2520MaskMed%252C%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520nnUNet%2520by%2520%252B2.0%2525%2520Dice%2520on%2520AMOS%25202022%2520and%2520%252B6.9%2525%2520Dice%2520on%2520BTCV.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskMed%3A%20Decoupled%20Mask%20and%20Class%20Prediction%20for%20Medical%20Image%20Segmentation&entry.906535625=Bin%20Xie%20and%20Gady%20Agam&entry.1292438233=Medical%20image%20segmentation%20typically%20adopts%20a%20point-wise%20convolutional%20segmentation%20head%20to%20predict%20dense%20labels%2C%20where%20each%20output%20channel%20is%20heuristically%20tied%20to%20a%20specific%20class.%20This%20rigid%20design%20limits%20both%20feature%20sharing%20and%20semantic%20generalization.%20In%20this%20work%2C%20we%20propose%20a%20unified%20decoupled%20segmentation%20head%20that%20separates%20multi-class%20prediction%20into%20class-agnostic%20mask%20prediction%20and%20class%20label%20prediction%20using%20shared%20object%20queries.%20Furthermore%2C%20we%20introduce%20a%20Full-Scale%20Aware%20Deformable%20Transformer%20module%20that%20enables%20low-resolution%20encoder%20features%20to%20attend%20across%20full-resolution%20encoder%20features%20via%20deformable%20attention%2C%20achieving%20memory-efficient%20and%20spatially%20aligned%20full-scale%20fusion.%20Our%20proposed%20method%2C%20named%20MaskMed%2C%20achieves%20state-of-the-art%20performance%2C%20surpassing%20nnUNet%20by%20%2B2.0%25%20Dice%20on%20AMOS%202022%20and%20%2B6.9%25%20Dice%20on%20BTCV.&entry.1838667208=http%3A//arxiv.org/abs/2511.15603v1&entry.124074799=Read"},
{"title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping", "author": "Yushi Huang and Zining Wang and Zhihang Yuan and Yifu Ding and Ruihao Gong and Jinyang Guo and Xianglong Liu and Jun Zhang", "abstract": "Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$.", "link": "http://arxiv.org/abs/2511.15690v1", "date": "2025-11-19", "relevancy": 2.1348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDES%3A%20Accelerating%20Mixture-of-Experts%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Expert%20Skipping&body=Title%3A%20MoDES%3A%20Accelerating%20Mixture-of-Experts%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Expert%20Skipping%0AAuthor%3A%20Yushi%20Huang%20and%20Zining%20Wang%20and%20Zhihang%20Yuan%20and%20Yifu%20Ding%20and%20Ruihao%20Gong%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Jun%20Zhang%0AAbstract%3A%20Mixture-of-Experts%20%28MoE%29%20Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20vision-language%20tasks%2C%20but%20they%20suffer%20from%20high%20computational%20inefficiency.%20To%20reduce%20inference%20overhead%2C%20expert%20skipping%20methods%20have%20been%20proposed%20to%20deactivate%20redundant%20experts%20based%20on%20the%20current%20input%20tokens.%20However%2C%20we%20find%20that%20applying%20these%20methods-originally%20designed%20for%20unimodal%20large%20language%20models%20%28LLMs%29-to%20MLLMs%20results%20in%20considerable%20performance%20degradation.%20This%20is%20primarily%20because%20such%20methods%20fail%20to%20account%20for%20the%20heterogeneous%20contributions%20of%20experts%20across%20MoE%20layers%20and%20modality-specific%20behaviors%20of%20tokens%20within%20these%20layers.%20Motivated%20by%20these%20findings%2C%20we%20propose%20MoDES%2C%20the%20first%20training-free%20framework%20that%20adaptively%20skips%20experts%20to%20enable%20efficient%20and%20accurate%20MoE%20MLLM%20inference.%20It%20incorporates%20a%20globally-modulated%20local%20gating%20%28GMLG%29%20mechanism%20that%20integrates%20global%20layer-wise%20importance%20into%20local%20routing%20probabilities%20to%20accurately%20estimate%20per-token%20expert%20importance.%20A%20dual-modality%20thresholding%20%28DMT%29%20method%20is%20then%20applied%2C%20which%20processes%20tokens%20from%20each%20modality%20separately%2C%20to%20derive%20the%20skipping%20schedule.%20To%20set%20the%20optimal%20thresholds%2C%20we%20introduce%20a%20frontier%20search%20algorithm%20that%20exploits%20monotonicity%20properties%2C%20cutting%20convergence%20time%20from%20several%20days%20to%20a%20few%20hours.%20Extensive%20experiments%20for%203%20model%20series%20across%2013%20benchmarks%20demonstrate%20that%20MoDES%20far%20outperforms%20previous%20approaches.%20For%20instance%2C%20when%20skipping%2088%25%20experts%20for%20Qwen3-VL-MoE-30B-A3B-Instruct%2C%20the%20performance%20boost%20is%20up%20to%2010.67%25%20%2897.33%25%20vs.%2086.66%25%29.%20Furthermore%2C%20MoDES%20significantly%20enhances%20inference%20speed%2C%20improving%20the%20prefilling%20time%20by%202.16%24%5Ctimes%24%20and%20the%20decoding%20time%20by%201.26%24%5Ctimes%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDES%253A%2520Accelerating%2520Mixture-of-Experts%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Dynamic%2520Expert%2520Skipping%26entry.906535625%3DYushi%2520Huang%2520and%2520Zining%2520Wang%2520and%2520Zhihang%2520Yuan%2520and%2520Yifu%2520Ding%2520and%2520Ruihao%2520Gong%2520and%2520Jinyang%2520Guo%2520and%2520Xianglong%2520Liu%2520and%2520Jun%2520Zhang%26entry.1292438233%3DMixture-of-Experts%2520%2528MoE%2529%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520excel%2520at%2520vision-language%2520tasks%252C%2520but%2520they%2520suffer%2520from%2520high%2520computational%2520inefficiency.%2520To%2520reduce%2520inference%2520overhead%252C%2520expert%2520skipping%2520methods%2520have%2520been%2520proposed%2520to%2520deactivate%2520redundant%2520experts%2520based%2520on%2520the%2520current%2520input%2520tokens.%2520However%252C%2520we%2520find%2520that%2520applying%2520these%2520methods-originally%2520designed%2520for%2520unimodal%2520large%2520language%2520models%2520%2528LLMs%2529-to%2520MLLMs%2520results%2520in%2520considerable%2520performance%2520degradation.%2520This%2520is%2520primarily%2520because%2520such%2520methods%2520fail%2520to%2520account%2520for%2520the%2520heterogeneous%2520contributions%2520of%2520experts%2520across%2520MoE%2520layers%2520and%2520modality-specific%2520behaviors%2520of%2520tokens%2520within%2520these%2520layers.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520MoDES%252C%2520the%2520first%2520training-free%2520framework%2520that%2520adaptively%2520skips%2520experts%2520to%2520enable%2520efficient%2520and%2520accurate%2520MoE%2520MLLM%2520inference.%2520It%2520incorporates%2520a%2520globally-modulated%2520local%2520gating%2520%2528GMLG%2529%2520mechanism%2520that%2520integrates%2520global%2520layer-wise%2520importance%2520into%2520local%2520routing%2520probabilities%2520to%2520accurately%2520estimate%2520per-token%2520expert%2520importance.%2520A%2520dual-modality%2520thresholding%2520%2528DMT%2529%2520method%2520is%2520then%2520applied%252C%2520which%2520processes%2520tokens%2520from%2520each%2520modality%2520separately%252C%2520to%2520derive%2520the%2520skipping%2520schedule.%2520To%2520set%2520the%2520optimal%2520thresholds%252C%2520we%2520introduce%2520a%2520frontier%2520search%2520algorithm%2520that%2520exploits%2520monotonicity%2520properties%252C%2520cutting%2520convergence%2520time%2520from%2520several%2520days%2520to%2520a%2520few%2520hours.%2520Extensive%2520experiments%2520for%25203%2520model%2520series%2520across%252013%2520benchmarks%2520demonstrate%2520that%2520MoDES%2520far%2520outperforms%2520previous%2520approaches.%2520For%2520instance%252C%2520when%2520skipping%252088%2525%2520experts%2520for%2520Qwen3-VL-MoE-30B-A3B-Instruct%252C%2520the%2520performance%2520boost%2520is%2520up%2520to%252010.67%2525%2520%252897.33%2525%2520vs.%252086.66%2525%2529.%2520Furthermore%252C%2520MoDES%2520significantly%2520enhances%2520inference%2520speed%252C%2520improving%2520the%2520prefilling%2520time%2520by%25202.16%2524%255Ctimes%2524%2520and%2520the%2520decoding%2520time%2520by%25201.26%2524%255Ctimes%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDES%3A%20Accelerating%20Mixture-of-Experts%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Expert%20Skipping&entry.906535625=Yushi%20Huang%20and%20Zining%20Wang%20and%20Zhihang%20Yuan%20and%20Yifu%20Ding%20and%20Ruihao%20Gong%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Jun%20Zhang&entry.1292438233=Mixture-of-Experts%20%28MoE%29%20Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20vision-language%20tasks%2C%20but%20they%20suffer%20from%20high%20computational%20inefficiency.%20To%20reduce%20inference%20overhead%2C%20expert%20skipping%20methods%20have%20been%20proposed%20to%20deactivate%20redundant%20experts%20based%20on%20the%20current%20input%20tokens.%20However%2C%20we%20find%20that%20applying%20these%20methods-originally%20designed%20for%20unimodal%20large%20language%20models%20%28LLMs%29-to%20MLLMs%20results%20in%20considerable%20performance%20degradation.%20This%20is%20primarily%20because%20such%20methods%20fail%20to%20account%20for%20the%20heterogeneous%20contributions%20of%20experts%20across%20MoE%20layers%20and%20modality-specific%20behaviors%20of%20tokens%20within%20these%20layers.%20Motivated%20by%20these%20findings%2C%20we%20propose%20MoDES%2C%20the%20first%20training-free%20framework%20that%20adaptively%20skips%20experts%20to%20enable%20efficient%20and%20accurate%20MoE%20MLLM%20inference.%20It%20incorporates%20a%20globally-modulated%20local%20gating%20%28GMLG%29%20mechanism%20that%20integrates%20global%20layer-wise%20importance%20into%20local%20routing%20probabilities%20to%20accurately%20estimate%20per-token%20expert%20importance.%20A%20dual-modality%20thresholding%20%28DMT%29%20method%20is%20then%20applied%2C%20which%20processes%20tokens%20from%20each%20modality%20separately%2C%20to%20derive%20the%20skipping%20schedule.%20To%20set%20the%20optimal%20thresholds%2C%20we%20introduce%20a%20frontier%20search%20algorithm%20that%20exploits%20monotonicity%20properties%2C%20cutting%20convergence%20time%20from%20several%20days%20to%20a%20few%20hours.%20Extensive%20experiments%20for%203%20model%20series%20across%2013%20benchmarks%20demonstrate%20that%20MoDES%20far%20outperforms%20previous%20approaches.%20For%20instance%2C%20when%20skipping%2088%25%20experts%20for%20Qwen3-VL-MoE-30B-A3B-Instruct%2C%20the%20performance%20boost%20is%20up%20to%2010.67%25%20%2897.33%25%20vs.%2086.66%25%29.%20Furthermore%2C%20MoDES%20significantly%20enhances%20inference%20speed%2C%20improving%20the%20prefilling%20time%20by%202.16%24%5Ctimes%24%20and%20the%20decoding%20time%20by%201.26%24%5Ctimes%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.15690v1&entry.124074799=Read"},
{"title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation", "author": "Georgios Venianakis and Constantinos Theodoropoulos and Michail Kavousanakis", "abstract": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.", "link": "http://arxiv.org/abs/2511.15543v1", "date": "2025-11-19", "relevancy": 2.1329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5694}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5441}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics%20Informed%20Machine%20Learning%20Framework%20for%20Optimal%20Sensor%20Placement%20and%20Parameter%20Estimation&body=Title%3A%20A%20Physics%20Informed%20Machine%20Learning%20Framework%20for%20Optimal%20Sensor%20Placement%20and%20Parameter%20Estimation%0AAuthor%3A%20Georgios%20Venianakis%20and%20Constantinos%20Theodoropoulos%20and%20Michail%20Kavousanakis%0AAbstract%3A%20Parameter%20estimation%20remains%20a%20challenging%20task%20across%20many%20areas%20of%20engineering.%20Because%20data%20acquisition%20can%20often%20be%20costly%2C%20limited%2C%20or%20prone%20to%20inaccuracies%20%28noise%2C%20uncertainty%29%20it%20is%20crucial%20to%20identify%20sensor%20configurations%20that%20provide%20the%20maximum%20amount%20of%20information%20about%20the%20unknown%20parameters%2C%20in%20particular%20for%20the%20case%20of%20distributed-parameter%20systems%2C%20where%20spatial%20variations%20are%20important.%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20powerful%20machine-learning%20%28ML%29%20tool%20for%20parameter%20estimation%2C%20particularly%20in%20cases%20with%20sparse%20or%20noisy%20measurements%2C%20overcoming%20some%20of%20the%20limitations%20of%20traditional%20optimization-based%20and%20Bayesian%20approaches.%20Despite%20the%20widespread%20use%20of%20PINNs%20for%20solving%20inverse%20problems%2C%20relatively%20little%20attention%20has%20been%20given%20to%20how%20their%20performance%20depends%20on%20sensor%20placement.%20This%20study%20addresses%20this%20gap%20by%20introducing%20a%20comprehensive%20PINN-based%20framework%20that%20simultaneously%20tackles%20optimal%20sensor%20placement%20and%20parameter%20estimation.%20Our%20approach%20involves%20training%20a%20PINN%20model%20in%20which%20the%20parameters%20of%20interest%20are%20included%20as%20additional%20inputs.%20This%20enables%20the%20efficient%20computation%20of%20sensitivity%20functions%20through%20automatic%20differentiation%2C%20which%20are%20then%20used%20to%20determine%20optimal%20sensor%20locations%20exploiting%20the%20D-optimality%20criterion.%20The%20framework%20is%20validated%20on%20two%20illustrative%20distributed-parameter%20reaction-diffusion-advection%20problems%20of%20increasing%20complexity.%20The%20results%20demonstrate%20that%20our%20PINNs-based%20methodology%20consistently%20achieves%20higher%20accuracy%20compared%20to%20parameter%20values%20estimated%20from%20intuitively%20or%20randomly%20selected%20sensor%20positions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics%2520Informed%2520Machine%2520Learning%2520Framework%2520for%2520Optimal%2520Sensor%2520Placement%2520and%2520Parameter%2520Estimation%26entry.906535625%3DGeorgios%2520Venianakis%2520and%2520Constantinos%2520Theodoropoulos%2520and%2520Michail%2520Kavousanakis%26entry.1292438233%3DParameter%2520estimation%2520remains%2520a%2520challenging%2520task%2520across%2520many%2520areas%2520of%2520engineering.%2520Because%2520data%2520acquisition%2520can%2520often%2520be%2520costly%252C%2520limited%252C%2520or%2520prone%2520to%2520inaccuracies%2520%2528noise%252C%2520uncertainty%2529%2520it%2520is%2520crucial%2520to%2520identify%2520sensor%2520configurations%2520that%2520provide%2520the%2520maximum%2520amount%2520of%2520information%2520about%2520the%2520unknown%2520parameters%252C%2520in%2520particular%2520for%2520the%2520case%2520of%2520distributed-parameter%2520systems%252C%2520where%2520spatial%2520variations%2520are%2520important.%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520machine-learning%2520%2528ML%2529%2520tool%2520for%2520parameter%2520estimation%252C%2520particularly%2520in%2520cases%2520with%2520sparse%2520or%2520noisy%2520measurements%252C%2520overcoming%2520some%2520of%2520the%2520limitations%2520of%2520traditional%2520optimization-based%2520and%2520Bayesian%2520approaches.%2520Despite%2520the%2520widespread%2520use%2520of%2520PINNs%2520for%2520solving%2520inverse%2520problems%252C%2520relatively%2520little%2520attention%2520has%2520been%2520given%2520to%2520how%2520their%2520performance%2520depends%2520on%2520sensor%2520placement.%2520This%2520study%2520addresses%2520this%2520gap%2520by%2520introducing%2520a%2520comprehensive%2520PINN-based%2520framework%2520that%2520simultaneously%2520tackles%2520optimal%2520sensor%2520placement%2520and%2520parameter%2520estimation.%2520Our%2520approach%2520involves%2520training%2520a%2520PINN%2520model%2520in%2520which%2520the%2520parameters%2520of%2520interest%2520are%2520included%2520as%2520additional%2520inputs.%2520This%2520enables%2520the%2520efficient%2520computation%2520of%2520sensitivity%2520functions%2520through%2520automatic%2520differentiation%252C%2520which%2520are%2520then%2520used%2520to%2520determine%2520optimal%2520sensor%2520locations%2520exploiting%2520the%2520D-optimality%2520criterion.%2520The%2520framework%2520is%2520validated%2520on%2520two%2520illustrative%2520distributed-parameter%2520reaction-diffusion-advection%2520problems%2520of%2520increasing%2520complexity.%2520The%2520results%2520demonstrate%2520that%2520our%2520PINNs-based%2520methodology%2520consistently%2520achieves%2520higher%2520accuracy%2520compared%2520to%2520parameter%2520values%2520estimated%2520from%2520intuitively%2520or%2520randomly%2520selected%2520sensor%2520positions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics%20Informed%20Machine%20Learning%20Framework%20for%20Optimal%20Sensor%20Placement%20and%20Parameter%20Estimation&entry.906535625=Georgios%20Venianakis%20and%20Constantinos%20Theodoropoulos%20and%20Michail%20Kavousanakis&entry.1292438233=Parameter%20estimation%20remains%20a%20challenging%20task%20across%20many%20areas%20of%20engineering.%20Because%20data%20acquisition%20can%20often%20be%20costly%2C%20limited%2C%20or%20prone%20to%20inaccuracies%20%28noise%2C%20uncertainty%29%20it%20is%20crucial%20to%20identify%20sensor%20configurations%20that%20provide%20the%20maximum%20amount%20of%20information%20about%20the%20unknown%20parameters%2C%20in%20particular%20for%20the%20case%20of%20distributed-parameter%20systems%2C%20where%20spatial%20variations%20are%20important.%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20recently%20emerged%20as%20a%20powerful%20machine-learning%20%28ML%29%20tool%20for%20parameter%20estimation%2C%20particularly%20in%20cases%20with%20sparse%20or%20noisy%20measurements%2C%20overcoming%20some%20of%20the%20limitations%20of%20traditional%20optimization-based%20and%20Bayesian%20approaches.%20Despite%20the%20widespread%20use%20of%20PINNs%20for%20solving%20inverse%20problems%2C%20relatively%20little%20attention%20has%20been%20given%20to%20how%20their%20performance%20depends%20on%20sensor%20placement.%20This%20study%20addresses%20this%20gap%20by%20introducing%20a%20comprehensive%20PINN-based%20framework%20that%20simultaneously%20tackles%20optimal%20sensor%20placement%20and%20parameter%20estimation.%20Our%20approach%20involves%20training%20a%20PINN%20model%20in%20which%20the%20parameters%20of%20interest%20are%20included%20as%20additional%20inputs.%20This%20enables%20the%20efficient%20computation%20of%20sensitivity%20functions%20through%20automatic%20differentiation%2C%20which%20are%20then%20used%20to%20determine%20optimal%20sensor%20locations%20exploiting%20the%20D-optimality%20criterion.%20The%20framework%20is%20validated%20on%20two%20illustrative%20distributed-parameter%20reaction-diffusion-advection%20problems%20of%20increasing%20complexity.%20The%20results%20demonstrate%20that%20our%20PINNs-based%20methodology%20consistently%20achieves%20higher%20accuracy%20compared%20to%20parameter%20values%20estimated%20from%20intuitively%20or%20randomly%20selected%20sensor%20positions.&entry.1838667208=http%3A//arxiv.org/abs/2511.15543v1&entry.124074799=Read"},
{"title": "Decentralized Gaussian Process Classification and an Application in Subsea Robotics", "author": "Yifei Gao and Hans J. He and Daniel J. Stilwell and James McMahon", "abstract": "Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.", "link": "http://arxiv.org/abs/2511.15529v1", "date": "2025-11-19", "relevancy": 2.1129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5357}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Gaussian%20Process%20Classification%20and%20an%20Application%20in%20Subsea%20Robotics&body=Title%3A%20Decentralized%20Gaussian%20Process%20Classification%20and%20an%20Application%20in%20Subsea%20Robotics%0AAuthor%3A%20Yifei%20Gao%20and%20Hans%20J.%20He%20and%20Daniel%20J.%20Stilwell%20and%20James%20McMahon%0AAbstract%3A%20Teams%20of%20cooperating%20autonomous%20underwater%20vehicles%20%28AUVs%29%20rely%20on%20acoustic%20communication%20for%20coordination%2C%20yet%20this%20communication%20medium%20is%20constrained%20by%20limited%20range%2C%20multi-path%20effects%2C%20and%20low%20bandwidth.%20One%20way%20to%20address%20the%20uncertainty%20associated%20with%20acoustic%20communication%20is%20to%20learn%20the%20communication%20environment%20in%20real-time.%20We%20address%20the%20challenge%20of%20a%20team%20of%20robots%20building%20a%20map%20of%20the%20probability%20of%20communication%20success%20from%20one%20location%20to%20another%20in%20real-time.%20This%20is%20a%20decentralized%20classification%20problem%20--%20communication%20events%20are%20either%20successful%20or%20unsuccessful%20--%20where%20AUVs%20share%20a%20subset%20of%20their%20communication%20measurements%20to%20build%20the%20map.%20The%20main%20contribution%20of%20this%20work%20is%20a%20rigorously%20derived%20data%20sharing%20policy%20that%20selects%20measurements%20to%20be%20shared%20among%20AUVs.%20We%20experimentally%20validate%20our%20proposed%20sharing%20policy%20using%20real%20acoustic%20communication%20data%20collected%20from%20teams%20of%20Virginia%20Tech%20690%20AUVs%2C%20demonstrating%20its%20effectiveness%20in%20underwater%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Gaussian%2520Process%2520Classification%2520and%2520an%2520Application%2520in%2520Subsea%2520Robotics%26entry.906535625%3DYifei%2520Gao%2520and%2520Hans%2520J.%2520He%2520and%2520Daniel%2520J.%2520Stilwell%2520and%2520James%2520McMahon%26entry.1292438233%3DTeams%2520of%2520cooperating%2520autonomous%2520underwater%2520vehicles%2520%2528AUVs%2529%2520rely%2520on%2520acoustic%2520communication%2520for%2520coordination%252C%2520yet%2520this%2520communication%2520medium%2520is%2520constrained%2520by%2520limited%2520range%252C%2520multi-path%2520effects%252C%2520and%2520low%2520bandwidth.%2520One%2520way%2520to%2520address%2520the%2520uncertainty%2520associated%2520with%2520acoustic%2520communication%2520is%2520to%2520learn%2520the%2520communication%2520environment%2520in%2520real-time.%2520We%2520address%2520the%2520challenge%2520of%2520a%2520team%2520of%2520robots%2520building%2520a%2520map%2520of%2520the%2520probability%2520of%2520communication%2520success%2520from%2520one%2520location%2520to%2520another%2520in%2520real-time.%2520This%2520is%2520a%2520decentralized%2520classification%2520problem%2520--%2520communication%2520events%2520are%2520either%2520successful%2520or%2520unsuccessful%2520--%2520where%2520AUVs%2520share%2520a%2520subset%2520of%2520their%2520communication%2520measurements%2520to%2520build%2520the%2520map.%2520The%2520main%2520contribution%2520of%2520this%2520work%2520is%2520a%2520rigorously%2520derived%2520data%2520sharing%2520policy%2520that%2520selects%2520measurements%2520to%2520be%2520shared%2520among%2520AUVs.%2520We%2520experimentally%2520validate%2520our%2520proposed%2520sharing%2520policy%2520using%2520real%2520acoustic%2520communication%2520data%2520collected%2520from%2520teams%2520of%2520Virginia%2520Tech%2520690%2520AUVs%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520underwater%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Gaussian%20Process%20Classification%20and%20an%20Application%20in%20Subsea%20Robotics&entry.906535625=Yifei%20Gao%20and%20Hans%20J.%20He%20and%20Daniel%20J.%20Stilwell%20and%20James%20McMahon&entry.1292438233=Teams%20of%20cooperating%20autonomous%20underwater%20vehicles%20%28AUVs%29%20rely%20on%20acoustic%20communication%20for%20coordination%2C%20yet%20this%20communication%20medium%20is%20constrained%20by%20limited%20range%2C%20multi-path%20effects%2C%20and%20low%20bandwidth.%20One%20way%20to%20address%20the%20uncertainty%20associated%20with%20acoustic%20communication%20is%20to%20learn%20the%20communication%20environment%20in%20real-time.%20We%20address%20the%20challenge%20of%20a%20team%20of%20robots%20building%20a%20map%20of%20the%20probability%20of%20communication%20success%20from%20one%20location%20to%20another%20in%20real-time.%20This%20is%20a%20decentralized%20classification%20problem%20--%20communication%20events%20are%20either%20successful%20or%20unsuccessful%20--%20where%20AUVs%20share%20a%20subset%20of%20their%20communication%20measurements%20to%20build%20the%20map.%20The%20main%20contribution%20of%20this%20work%20is%20a%20rigorously%20derived%20data%20sharing%20policy%20that%20selects%20measurements%20to%20be%20shared%20among%20AUVs.%20We%20experimentally%20validate%20our%20proposed%20sharing%20policy%20using%20real%20acoustic%20communication%20data%20collected%20from%20teams%20of%20Virginia%20Tech%20690%20AUVs%2C%20demonstrating%20its%20effectiveness%20in%20underwater%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.15529v1&entry.124074799=Read"},
{"title": "Energy-based generator matching: A neural sampler for general state space", "author": "Dongyeop Woo and Minsu Kim and Minkyu Kim and Kiyoung Seong and Sungsoo Ahn", "abstract": "We propose Energy-based generator matching (EGM), a modality-agnostic approach to train generative models from energy functions in the absence of data. Extending the recently proposed generator matching, EGM enables training of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump, and can generate data from continuous, discrete, and a mixture of two modalities. To this end, we propose estimating the generator matching loss using self-normalized importance sampling with an additional bootstrapping trick to reduce variance in the importance weight. We validate EGM on both discrete and multimodal tasks up to 100 and 20 dimensions, respectively.", "link": "http://arxiv.org/abs/2505.19646v3", "date": "2025-11-19", "relevancy": 2.1108, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5561}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-based%20generator%20matching%3A%20A%20neural%20sampler%20for%20general%20state%20space&body=Title%3A%20Energy-based%20generator%20matching%3A%20A%20neural%20sampler%20for%20general%20state%20space%0AAuthor%3A%20Dongyeop%20Woo%20and%20Minsu%20Kim%20and%20Minkyu%20Kim%20and%20Kiyoung%20Seong%20and%20Sungsoo%20Ahn%0AAbstract%3A%20We%20propose%20Energy-based%20generator%20matching%20%28EGM%29%2C%20a%20modality-agnostic%20approach%20to%20train%20generative%20models%20from%20energy%20functions%20in%20the%20absence%20of%20data.%20Extending%20the%20recently%20proposed%20generator%20matching%2C%20EGM%20enables%20training%20of%20arbitrary%20continuous-time%20Markov%20processes%2C%20e.g.%2C%20diffusion%2C%20flow%2C%20and%20jump%2C%20and%20can%20generate%20data%20from%20continuous%2C%20discrete%2C%20and%20a%20mixture%20of%20two%20modalities.%20To%20this%20end%2C%20we%20propose%20estimating%20the%20generator%20matching%20loss%20using%20self-normalized%20importance%20sampling%20with%20an%20additional%20bootstrapping%20trick%20to%20reduce%20variance%20in%20the%20importance%20weight.%20We%20validate%20EGM%20on%20both%20discrete%20and%20multimodal%20tasks%20up%20to%20100%20and%2020%20dimensions%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-based%2520generator%2520matching%253A%2520A%2520neural%2520sampler%2520for%2520general%2520state%2520space%26entry.906535625%3DDongyeop%2520Woo%2520and%2520Minsu%2520Kim%2520and%2520Minkyu%2520Kim%2520and%2520Kiyoung%2520Seong%2520and%2520Sungsoo%2520Ahn%26entry.1292438233%3DWe%2520propose%2520Energy-based%2520generator%2520matching%2520%2528EGM%2529%252C%2520a%2520modality-agnostic%2520approach%2520to%2520train%2520generative%2520models%2520from%2520energy%2520functions%2520in%2520the%2520absence%2520of%2520data.%2520Extending%2520the%2520recently%2520proposed%2520generator%2520matching%252C%2520EGM%2520enables%2520training%2520of%2520arbitrary%2520continuous-time%2520Markov%2520processes%252C%2520e.g.%252C%2520diffusion%252C%2520flow%252C%2520and%2520jump%252C%2520and%2520can%2520generate%2520data%2520from%2520continuous%252C%2520discrete%252C%2520and%2520a%2520mixture%2520of%2520two%2520modalities.%2520To%2520this%2520end%252C%2520we%2520propose%2520estimating%2520the%2520generator%2520matching%2520loss%2520using%2520self-normalized%2520importance%2520sampling%2520with%2520an%2520additional%2520bootstrapping%2520trick%2520to%2520reduce%2520variance%2520in%2520the%2520importance%2520weight.%2520We%2520validate%2520EGM%2520on%2520both%2520discrete%2520and%2520multimodal%2520tasks%2520up%2520to%2520100%2520and%252020%2520dimensions%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-based%20generator%20matching%3A%20A%20neural%20sampler%20for%20general%20state%20space&entry.906535625=Dongyeop%20Woo%20and%20Minsu%20Kim%20and%20Minkyu%20Kim%20and%20Kiyoung%20Seong%20and%20Sungsoo%20Ahn&entry.1292438233=We%20propose%20Energy-based%20generator%20matching%20%28EGM%29%2C%20a%20modality-agnostic%20approach%20to%20train%20generative%20models%20from%20energy%20functions%20in%20the%20absence%20of%20data.%20Extending%20the%20recently%20proposed%20generator%20matching%2C%20EGM%20enables%20training%20of%20arbitrary%20continuous-time%20Markov%20processes%2C%20e.g.%2C%20diffusion%2C%20flow%2C%20and%20jump%2C%20and%20can%20generate%20data%20from%20continuous%2C%20discrete%2C%20and%20a%20mixture%20of%20two%20modalities.%20To%20this%20end%2C%20we%20propose%20estimating%20the%20generator%20matching%20loss%20using%20self-normalized%20importance%20sampling%20with%20an%20additional%20bootstrapping%20trick%20to%20reduce%20variance%20in%20the%20importance%20weight.%20We%20validate%20EGM%20on%20both%20discrete%20and%20multimodal%20tasks%20up%20to%20100%20and%2020%20dimensions%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2505.19646v3&entry.124074799=Read"},
{"title": "When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling", "author": "Alessio Pellegrino and Jacopo Mauro", "abstract": "One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.", "link": "http://arxiv.org/abs/2511.14334v2", "date": "2025-11-19", "relevancy": 2.0996, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Words%20Change%20the%20Model%3A%20Sensitivity%20of%20LLMs%20for%20Constraint%20Programming%20Modelling&body=Title%3A%20When%20Words%20Change%20the%20Model%3A%20Sensitivity%20of%20LLMs%20for%20Constraint%20Programming%20Modelling%0AAuthor%3A%20Alessio%20Pellegrino%20and%20Jacopo%20Mauro%0AAbstract%3A%20One%20of%20the%20long-standing%20goals%20in%20optimisation%20and%20constraint%20programming%20is%20to%20describe%20a%20problem%20in%20natural%20language%20and%20automatically%20obtain%20an%20executable%2C%20efficient%20model.%20Large%20language%20models%20appear%20to%20bring%20this%20vision%20closer%2C%20showing%20impressive%20results%20in%20automatically%20generating%20models%20for%20classical%20benchmarks.%20However%2C%20much%20of%20this%20apparent%20success%20may%20derive%20from%20data%20contamination%20rather%20than%20genuine%20reasoning%3A%20many%20standard%20CP%20problems%20are%20likely%20included%20in%20the%20training%20data%20of%20these%20models.%20To%20examine%20this%20hypothesis%2C%20we%20systematically%20rephrased%20and%20perturbed%20a%20set%20of%20well-known%20CSPLib%20problems%20to%20preserve%20their%20structure%20while%20modifying%20their%20context%20and%20introducing%20misleading%20elements.%20We%20then%20compared%20the%20models%20produced%20by%20three%20representative%20LLMs%20across%20original%20and%20modified%20descriptions.%20Our%20qualitative%20analysis%20shows%20that%20while%20LLMs%20can%20produce%20syntactically%20valid%20and%20semantically%20plausible%20models%2C%20their%20performance%20drops%20sharply%20under%20contextual%20and%20linguistic%20variation%2C%20revealing%20shallow%20understanding%20and%20sensitivity%20to%20wording.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Words%2520Change%2520the%2520Model%253A%2520Sensitivity%2520of%2520LLMs%2520for%2520Constraint%2520Programming%2520Modelling%26entry.906535625%3DAlessio%2520Pellegrino%2520and%2520Jacopo%2520Mauro%26entry.1292438233%3DOne%2520of%2520the%2520long-standing%2520goals%2520in%2520optimisation%2520and%2520constraint%2520programming%2520is%2520to%2520describe%2520a%2520problem%2520in%2520natural%2520language%2520and%2520automatically%2520obtain%2520an%2520executable%252C%2520efficient%2520model.%2520Large%2520language%2520models%2520appear%2520to%2520bring%2520this%2520vision%2520closer%252C%2520showing%2520impressive%2520results%2520in%2520automatically%2520generating%2520models%2520for%2520classical%2520benchmarks.%2520However%252C%2520much%2520of%2520this%2520apparent%2520success%2520may%2520derive%2520from%2520data%2520contamination%2520rather%2520than%2520genuine%2520reasoning%253A%2520many%2520standard%2520CP%2520problems%2520are%2520likely%2520included%2520in%2520the%2520training%2520data%2520of%2520these%2520models.%2520To%2520examine%2520this%2520hypothesis%252C%2520we%2520systematically%2520rephrased%2520and%2520perturbed%2520a%2520set%2520of%2520well-known%2520CSPLib%2520problems%2520to%2520preserve%2520their%2520structure%2520while%2520modifying%2520their%2520context%2520and%2520introducing%2520misleading%2520elements.%2520We%2520then%2520compared%2520the%2520models%2520produced%2520by%2520three%2520representative%2520LLMs%2520across%2520original%2520and%2520modified%2520descriptions.%2520Our%2520qualitative%2520analysis%2520shows%2520that%2520while%2520LLMs%2520can%2520produce%2520syntactically%2520valid%2520and%2520semantically%2520plausible%2520models%252C%2520their%2520performance%2520drops%2520sharply%2520under%2520contextual%2520and%2520linguistic%2520variation%252C%2520revealing%2520shallow%2520understanding%2520and%2520sensitivity%2520to%2520wording.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Words%20Change%20the%20Model%3A%20Sensitivity%20of%20LLMs%20for%20Constraint%20Programming%20Modelling&entry.906535625=Alessio%20Pellegrino%20and%20Jacopo%20Mauro&entry.1292438233=One%20of%20the%20long-standing%20goals%20in%20optimisation%20and%20constraint%20programming%20is%20to%20describe%20a%20problem%20in%20natural%20language%20and%20automatically%20obtain%20an%20executable%2C%20efficient%20model.%20Large%20language%20models%20appear%20to%20bring%20this%20vision%20closer%2C%20showing%20impressive%20results%20in%20automatically%20generating%20models%20for%20classical%20benchmarks.%20However%2C%20much%20of%20this%20apparent%20success%20may%20derive%20from%20data%20contamination%20rather%20than%20genuine%20reasoning%3A%20many%20standard%20CP%20problems%20are%20likely%20included%20in%20the%20training%20data%20of%20these%20models.%20To%20examine%20this%20hypothesis%2C%20we%20systematically%20rephrased%20and%20perturbed%20a%20set%20of%20well-known%20CSPLib%20problems%20to%20preserve%20their%20structure%20while%20modifying%20their%20context%20and%20introducing%20misleading%20elements.%20We%20then%20compared%20the%20models%20produced%20by%20three%20representative%20LLMs%20across%20original%20and%20modified%20descriptions.%20Our%20qualitative%20analysis%20shows%20that%20while%20LLMs%20can%20produce%20syntactically%20valid%20and%20semantically%20plausible%20models%2C%20their%20performance%20drops%20sharply%20under%20contextual%20and%20linguistic%20variation%2C%20revealing%20shallow%20understanding%20and%20sensitivity%20to%20wording.&entry.1838667208=http%3A//arxiv.org/abs/2511.14334v2&entry.124074799=Read"},
{"title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics", "author": "Michael McCabe and Payel Mukhopadhyay and Tanya Marwah and Bruno Regaldo-Saint Blancard and Francois Rozet and Cristiana Diaconu and Lucas Meyer and Kaze W. K. Wong and Hadi Sotoudeh and Alberto Bietti and Irina Espejo and Rio Fear and Siavash Golkar and Tom Hehir and Keiya Hirashima and Geraud Krawezik and Francois Lanusse and Rudy Morel and Ruben Ohana and Liam Parker and Mariel Pettee and Jeff Shen and Kyunghyun Cho and Miles Cranmer and Shirley Ho", "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.", "link": "http://arxiv.org/abs/2511.15684v1", "date": "2025-11-19", "relevancy": 2.0816, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Walrus%3A%20A%20Cross-Domain%20Foundation%20Model%20for%20Continuum%20Dynamics&body=Title%3A%20Walrus%3A%20A%20Cross-Domain%20Foundation%20Model%20for%20Continuum%20Dynamics%0AAuthor%3A%20Michael%20McCabe%20and%20Payel%20Mukhopadhyay%20and%20Tanya%20Marwah%20and%20Bruno%20Regaldo-Saint%20Blancard%20and%20Francois%20Rozet%20and%20Cristiana%20Diaconu%20and%20Lucas%20Meyer%20and%20Kaze%20W.%20K.%20Wong%20and%20Hadi%20Sotoudeh%20and%20Alberto%20Bietti%20and%20Irina%20Espejo%20and%20Rio%20Fear%20and%20Siavash%20Golkar%20and%20Tom%20Hehir%20and%20Keiya%20Hirashima%20and%20Geraud%20Krawezik%20and%20Francois%20Lanusse%20and%20Rudy%20Morel%20and%20Ruben%20Ohana%20and%20Liam%20Parker%20and%20Mariel%20Pettee%20and%20Jeff%20Shen%20and%20Kyunghyun%20Cho%20and%20Miles%20Cranmer%20and%20Shirley%20Ho%0AAbstract%3A%20Foundation%20models%20have%20transformed%20machine%20learning%20for%20language%20and%20vision%2C%20but%20achieving%20comparable%20impact%20in%20physical%20simulation%20remains%20a%20challenge.%20Data%20heterogeneity%20and%20unstable%20long-term%20dynamics%20inhibit%20learning%20from%20sufficiently%20diverse%20dynamics%2C%20while%20varying%20resolutions%20and%20dimensionalities%20challenge%20efficient%20training%20on%20modern%20hardware.%20Through%20empirical%20and%20theoretical%20analysis%2C%20we%20incorporate%20new%20approaches%20to%20mitigate%20these%20obstacles%2C%20including%20a%20harmonic-analysis-based%20stabilization%20method%2C%20load-balanced%20distributed%202D%20and%203D%20training%20strategies%2C%20and%20compute-adaptive%20tokenization.%20Using%20these%20tools%2C%20we%20develop%20Walrus%2C%20a%20transformer-based%20foundation%20model%20developed%20primarily%20for%20fluid-like%20continuum%20dynamics.%20Walrus%20is%20pretrained%20on%20nineteen%20diverse%20scenarios%20spanning%20astrophysics%2C%20geoscience%2C%20rheology%2C%20plasma%20physics%2C%20acoustics%2C%20and%20classical%20fluids.%20Experiments%20show%20that%20Walrus%20outperforms%20prior%20foundation%20models%20on%20both%20short%20and%20long%20term%20prediction%20horizons%20on%20downstream%20tasks%20and%20across%20the%20breadth%20of%20pretraining%20data%2C%20while%20ablation%20studies%20confirm%20the%20value%20of%20our%20contributions%20to%20forecast%20stability%2C%20training%20throughput%2C%20and%20transfer%20performance%20over%20conventional%20approaches.%20Code%20and%20weights%20are%20released%20for%20community%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalrus%253A%2520A%2520Cross-Domain%2520Foundation%2520Model%2520for%2520Continuum%2520Dynamics%26entry.906535625%3DMichael%2520McCabe%2520and%2520Payel%2520Mukhopadhyay%2520and%2520Tanya%2520Marwah%2520and%2520Bruno%2520Regaldo-Saint%2520Blancard%2520and%2520Francois%2520Rozet%2520and%2520Cristiana%2520Diaconu%2520and%2520Lucas%2520Meyer%2520and%2520Kaze%2520W.%2520K.%2520Wong%2520and%2520Hadi%2520Sotoudeh%2520and%2520Alberto%2520Bietti%2520and%2520Irina%2520Espejo%2520and%2520Rio%2520Fear%2520and%2520Siavash%2520Golkar%2520and%2520Tom%2520Hehir%2520and%2520Keiya%2520Hirashima%2520and%2520Geraud%2520Krawezik%2520and%2520Francois%2520Lanusse%2520and%2520Rudy%2520Morel%2520and%2520Ruben%2520Ohana%2520and%2520Liam%2520Parker%2520and%2520Mariel%2520Pettee%2520and%2520Jeff%2520Shen%2520and%2520Kyunghyun%2520Cho%2520and%2520Miles%2520Cranmer%2520and%2520Shirley%2520Ho%26entry.1292438233%3DFoundation%2520models%2520have%2520transformed%2520machine%2520learning%2520for%2520language%2520and%2520vision%252C%2520but%2520achieving%2520comparable%2520impact%2520in%2520physical%2520simulation%2520remains%2520a%2520challenge.%2520Data%2520heterogeneity%2520and%2520unstable%2520long-term%2520dynamics%2520inhibit%2520learning%2520from%2520sufficiently%2520diverse%2520dynamics%252C%2520while%2520varying%2520resolutions%2520and%2520dimensionalities%2520challenge%2520efficient%2520training%2520on%2520modern%2520hardware.%2520Through%2520empirical%2520and%2520theoretical%2520analysis%252C%2520we%2520incorporate%2520new%2520approaches%2520to%2520mitigate%2520these%2520obstacles%252C%2520including%2520a%2520harmonic-analysis-based%2520stabilization%2520method%252C%2520load-balanced%2520distributed%25202D%2520and%25203D%2520training%2520strategies%252C%2520and%2520compute-adaptive%2520tokenization.%2520Using%2520these%2520tools%252C%2520we%2520develop%2520Walrus%252C%2520a%2520transformer-based%2520foundation%2520model%2520developed%2520primarily%2520for%2520fluid-like%2520continuum%2520dynamics.%2520Walrus%2520is%2520pretrained%2520on%2520nineteen%2520diverse%2520scenarios%2520spanning%2520astrophysics%252C%2520geoscience%252C%2520rheology%252C%2520plasma%2520physics%252C%2520acoustics%252C%2520and%2520classical%2520fluids.%2520Experiments%2520show%2520that%2520Walrus%2520outperforms%2520prior%2520foundation%2520models%2520on%2520both%2520short%2520and%2520long%2520term%2520prediction%2520horizons%2520on%2520downstream%2520tasks%2520and%2520across%2520the%2520breadth%2520of%2520pretraining%2520data%252C%2520while%2520ablation%2520studies%2520confirm%2520the%2520value%2520of%2520our%2520contributions%2520to%2520forecast%2520stability%252C%2520training%2520throughput%252C%2520and%2520transfer%2520performance%2520over%2520conventional%2520approaches.%2520Code%2520and%2520weights%2520are%2520released%2520for%2520community%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Walrus%3A%20A%20Cross-Domain%20Foundation%20Model%20for%20Continuum%20Dynamics&entry.906535625=Michael%20McCabe%20and%20Payel%20Mukhopadhyay%20and%20Tanya%20Marwah%20and%20Bruno%20Regaldo-Saint%20Blancard%20and%20Francois%20Rozet%20and%20Cristiana%20Diaconu%20and%20Lucas%20Meyer%20and%20Kaze%20W.%20K.%20Wong%20and%20Hadi%20Sotoudeh%20and%20Alberto%20Bietti%20and%20Irina%20Espejo%20and%20Rio%20Fear%20and%20Siavash%20Golkar%20and%20Tom%20Hehir%20and%20Keiya%20Hirashima%20and%20Geraud%20Krawezik%20and%20Francois%20Lanusse%20and%20Rudy%20Morel%20and%20Ruben%20Ohana%20and%20Liam%20Parker%20and%20Mariel%20Pettee%20and%20Jeff%20Shen%20and%20Kyunghyun%20Cho%20and%20Miles%20Cranmer%20and%20Shirley%20Ho&entry.1292438233=Foundation%20models%20have%20transformed%20machine%20learning%20for%20language%20and%20vision%2C%20but%20achieving%20comparable%20impact%20in%20physical%20simulation%20remains%20a%20challenge.%20Data%20heterogeneity%20and%20unstable%20long-term%20dynamics%20inhibit%20learning%20from%20sufficiently%20diverse%20dynamics%2C%20while%20varying%20resolutions%20and%20dimensionalities%20challenge%20efficient%20training%20on%20modern%20hardware.%20Through%20empirical%20and%20theoretical%20analysis%2C%20we%20incorporate%20new%20approaches%20to%20mitigate%20these%20obstacles%2C%20including%20a%20harmonic-analysis-based%20stabilization%20method%2C%20load-balanced%20distributed%202D%20and%203D%20training%20strategies%2C%20and%20compute-adaptive%20tokenization.%20Using%20these%20tools%2C%20we%20develop%20Walrus%2C%20a%20transformer-based%20foundation%20model%20developed%20primarily%20for%20fluid-like%20continuum%20dynamics.%20Walrus%20is%20pretrained%20on%20nineteen%20diverse%20scenarios%20spanning%20astrophysics%2C%20geoscience%2C%20rheology%2C%20plasma%20physics%2C%20acoustics%2C%20and%20classical%20fluids.%20Experiments%20show%20that%20Walrus%20outperforms%20prior%20foundation%20models%20on%20both%20short%20and%20long%20term%20prediction%20horizons%20on%20downstream%20tasks%20and%20across%20the%20breadth%20of%20pretraining%20data%2C%20while%20ablation%20studies%20confirm%20the%20value%20of%20our%20contributions%20to%20forecast%20stability%2C%20training%20throughput%2C%20and%20transfer%20performance%20over%20conventional%20approaches.%20Code%20and%20weights%20are%20released%20for%20community%20use.&entry.1838667208=http%3A//arxiv.org/abs/2511.15684v1&entry.124074799=Read"},
{"title": "S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning", "author": "Jiangwen Dong and Zehui Lin and Wanyu Lin and Mingjin Zhang", "abstract": "Large Language Models (LLMs) have achieved impressive performance in complex reasoning problems. Their effectiveness highly depends on the specific nature of the task, especially the required domain knowledge. Existing approaches, such as mixture-of-experts, typically operate at the task level; they are too coarse to effectively solve the heterogeneous problems involving multiple subjects. This work proposes a novel framework that performs fine-grained analysis at subject level equipped with a designated multi-agent collaboration strategy for addressing heterogeneous problem reasoning. Specifically, given an input query, we first employ a Graph Neural Network to identify the relevant subjects and infer their interdependencies to generate an \\textit{Subject-based Directed Acyclic Graph} (S-DAG), where nodes represent subjects and edges encode information flow. Then we profile the LLM models by assigning each model a subject-specific expertise score, and select the top-performing one for matching corresponding subject of the S-DAG. Such subject-model matching enables graph-structured multi-agent collaboration where information flows from the starting model to the ending model over S-DAG. We curate and release multi-subject subsets of standard benchmarks (MMLU-Pro, GPQA, MedMCQA) to better reflect complex, real-world reasoning tasks. Extensive experiments show that our approach significantly outperforms existing task-level model selection and multi-agent collaboration baselines in accuracy and efficiency. These results highlight the effectiveness of subject-aware reasoning and structured collaboration in addressing complex and multi-subject problems.", "link": "http://arxiv.org/abs/2511.06727v2", "date": "2025-11-19", "relevancy": 2.0757, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-DAG%3A%20A%20Subject-Based%20Directed%20Acyclic%20Graph%20for%20Multi-Agent%20Heterogeneous%20Reasoning&body=Title%3A%20S-DAG%3A%20A%20Subject-Based%20Directed%20Acyclic%20Graph%20for%20Multi-Agent%20Heterogeneous%20Reasoning%0AAuthor%3A%20Jiangwen%20Dong%20and%20Zehui%20Lin%20and%20Wanyu%20Lin%20and%20Mingjin%20Zhang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20complex%20reasoning%20problems.%20Their%20effectiveness%20highly%20depends%20on%20the%20specific%20nature%20of%20the%20task%2C%20especially%20the%20required%20domain%20knowledge.%20Existing%20approaches%2C%20such%20as%20mixture-of-experts%2C%20typically%20operate%20at%20the%20task%20level%3B%20they%20are%20too%20coarse%20to%20effectively%20solve%20the%20heterogeneous%20problems%20involving%20multiple%20subjects.%20This%20work%20proposes%20a%20novel%20framework%20that%20performs%20fine-grained%20analysis%20at%20subject%20level%20equipped%20with%20a%20designated%20multi-agent%20collaboration%20strategy%20for%20addressing%20heterogeneous%20problem%20reasoning.%20Specifically%2C%20given%20an%20input%20query%2C%20we%20first%20employ%20a%20Graph%20Neural%20Network%20to%20identify%20the%20relevant%20subjects%20and%20infer%20their%20interdependencies%20to%20generate%20an%20%5Ctextit%7BSubject-based%20Directed%20Acyclic%20Graph%7D%20%28S-DAG%29%2C%20where%20nodes%20represent%20subjects%20and%20edges%20encode%20information%20flow.%20Then%20we%20profile%20the%20LLM%20models%20by%20assigning%20each%20model%20a%20subject-specific%20expertise%20score%2C%20and%20select%20the%20top-performing%20one%20for%20matching%20corresponding%20subject%20of%20the%20S-DAG.%20Such%20subject-model%20matching%20enables%20graph-structured%20multi-agent%20collaboration%20where%20information%20flows%20from%20the%20starting%20model%20to%20the%20ending%20model%20over%20S-DAG.%20We%20curate%20and%20release%20multi-subject%20subsets%20of%20standard%20benchmarks%20%28MMLU-Pro%2C%20GPQA%2C%20MedMCQA%29%20to%20better%20reflect%20complex%2C%20real-world%20reasoning%20tasks.%20Extensive%20experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20task-level%20model%20selection%20and%20multi-agent%20collaboration%20baselines%20in%20accuracy%20and%20efficiency.%20These%20results%20highlight%20the%20effectiveness%20of%20subject-aware%20reasoning%20and%20structured%20collaboration%20in%20addressing%20complex%20and%20multi-subject%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-DAG%253A%2520A%2520Subject-Based%2520Directed%2520Acyclic%2520Graph%2520for%2520Multi-Agent%2520Heterogeneous%2520Reasoning%26entry.906535625%3DJiangwen%2520Dong%2520and%2520Zehui%2520Lin%2520and%2520Wanyu%2520Lin%2520and%2520Mingjin%2520Zhang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520in%2520complex%2520reasoning%2520problems.%2520Their%2520effectiveness%2520highly%2520depends%2520on%2520the%2520specific%2520nature%2520of%2520the%2520task%252C%2520especially%2520the%2520required%2520domain%2520knowledge.%2520Existing%2520approaches%252C%2520such%2520as%2520mixture-of-experts%252C%2520typically%2520operate%2520at%2520the%2520task%2520level%253B%2520they%2520are%2520too%2520coarse%2520to%2520effectively%2520solve%2520the%2520heterogeneous%2520problems%2520involving%2520multiple%2520subjects.%2520This%2520work%2520proposes%2520a%2520novel%2520framework%2520that%2520performs%2520fine-grained%2520analysis%2520at%2520subject%2520level%2520equipped%2520with%2520a%2520designated%2520multi-agent%2520collaboration%2520strategy%2520for%2520addressing%2520heterogeneous%2520problem%2520reasoning.%2520Specifically%252C%2520given%2520an%2520input%2520query%252C%2520we%2520first%2520employ%2520a%2520Graph%2520Neural%2520Network%2520to%2520identify%2520the%2520relevant%2520subjects%2520and%2520infer%2520their%2520interdependencies%2520to%2520generate%2520an%2520%255Ctextit%257BSubject-based%2520Directed%2520Acyclic%2520Graph%257D%2520%2528S-DAG%2529%252C%2520where%2520nodes%2520represent%2520subjects%2520and%2520edges%2520encode%2520information%2520flow.%2520Then%2520we%2520profile%2520the%2520LLM%2520models%2520by%2520assigning%2520each%2520model%2520a%2520subject-specific%2520expertise%2520score%252C%2520and%2520select%2520the%2520top-performing%2520one%2520for%2520matching%2520corresponding%2520subject%2520of%2520the%2520S-DAG.%2520Such%2520subject-model%2520matching%2520enables%2520graph-structured%2520multi-agent%2520collaboration%2520where%2520information%2520flows%2520from%2520the%2520starting%2520model%2520to%2520the%2520ending%2520model%2520over%2520S-DAG.%2520We%2520curate%2520and%2520release%2520multi-subject%2520subsets%2520of%2520standard%2520benchmarks%2520%2528MMLU-Pro%252C%2520GPQA%252C%2520MedMCQA%2529%2520to%2520better%2520reflect%2520complex%252C%2520real-world%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520task-level%2520model%2520selection%2520and%2520multi-agent%2520collaboration%2520baselines%2520in%2520accuracy%2520and%2520efficiency.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%2520subject-aware%2520reasoning%2520and%2520structured%2520collaboration%2520in%2520addressing%2520complex%2520and%2520multi-subject%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-DAG%3A%20A%20Subject-Based%20Directed%20Acyclic%20Graph%20for%20Multi-Agent%20Heterogeneous%20Reasoning&entry.906535625=Jiangwen%20Dong%20and%20Zehui%20Lin%20and%20Wanyu%20Lin%20and%20Mingjin%20Zhang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20complex%20reasoning%20problems.%20Their%20effectiveness%20highly%20depends%20on%20the%20specific%20nature%20of%20the%20task%2C%20especially%20the%20required%20domain%20knowledge.%20Existing%20approaches%2C%20such%20as%20mixture-of-experts%2C%20typically%20operate%20at%20the%20task%20level%3B%20they%20are%20too%20coarse%20to%20effectively%20solve%20the%20heterogeneous%20problems%20involving%20multiple%20subjects.%20This%20work%20proposes%20a%20novel%20framework%20that%20performs%20fine-grained%20analysis%20at%20subject%20level%20equipped%20with%20a%20designated%20multi-agent%20collaboration%20strategy%20for%20addressing%20heterogeneous%20problem%20reasoning.%20Specifically%2C%20given%20an%20input%20query%2C%20we%20first%20employ%20a%20Graph%20Neural%20Network%20to%20identify%20the%20relevant%20subjects%20and%20infer%20their%20interdependencies%20to%20generate%20an%20%5Ctextit%7BSubject-based%20Directed%20Acyclic%20Graph%7D%20%28S-DAG%29%2C%20where%20nodes%20represent%20subjects%20and%20edges%20encode%20information%20flow.%20Then%20we%20profile%20the%20LLM%20models%20by%20assigning%20each%20model%20a%20subject-specific%20expertise%20score%2C%20and%20select%20the%20top-performing%20one%20for%20matching%20corresponding%20subject%20of%20the%20S-DAG.%20Such%20subject-model%20matching%20enables%20graph-structured%20multi-agent%20collaboration%20where%20information%20flows%20from%20the%20starting%20model%20to%20the%20ending%20model%20over%20S-DAG.%20We%20curate%20and%20release%20multi-subject%20subsets%20of%20standard%20benchmarks%20%28MMLU-Pro%2C%20GPQA%2C%20MedMCQA%29%20to%20better%20reflect%20complex%2C%20real-world%20reasoning%20tasks.%20Extensive%20experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20task-level%20model%20selection%20and%20multi-agent%20collaboration%20baselines%20in%20accuracy%20and%20efficiency.%20These%20results%20highlight%20the%20effectiveness%20of%20subject-aware%20reasoning%20and%20structured%20collaboration%20in%20addressing%20complex%20and%20multi-subject%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2511.06727v2&entry.124074799=Read"},
{"title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI", "author": "Naomi Simumba and Nils Lehmann and Paolo Fraccaro and Hamed Alemohammad and Geeth De Mel and Salman Khan and Manil Maskey and Nicolas Longepe and Xiao Xiang Zhu and Hannah Kerner and Juan Bernabe-Moreno and Alexander Lacoste", "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.", "link": "http://arxiv.org/abs/2511.15658v1", "date": "2025-11-19", "relevancy": 2.0741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEO-Bench-2%3A%20From%20Performance%20to%20Capability%2C%20Rethinking%20Evaluation%20in%20Geospatial%20AI&body=Title%3A%20GEO-Bench-2%3A%20From%20Performance%20to%20Capability%2C%20Rethinking%20Evaluation%20in%20Geospatial%20AI%0AAuthor%3A%20Naomi%20Simumba%20and%20Nils%20Lehmann%20and%20Paolo%20Fraccaro%20and%20Hamed%20Alemohammad%20and%20Geeth%20De%20Mel%20and%20Salman%20Khan%20and%20Manil%20Maskey%20and%20Nicolas%20Longepe%20and%20Xiao%20Xiang%20Zhu%20and%20Hannah%20Kerner%20and%20Juan%20Bernabe-Moreno%20and%20Alexander%20Lacoste%0AAbstract%3A%20Geospatial%20Foundation%20Models%20%28GeoFMs%29%20are%20transforming%20Earth%20Observation%20%28EO%29%2C%20but%20evaluation%20lacks%20standardized%20protocols.%20GEO-Bench-2%20addresses%20this%20with%20a%20comprehensive%20framework%20spanning%20classification%2C%20segmentation%2C%20regression%2C%20object%20detection%2C%20and%20instance%20segmentation%20across%2019%20permissively-licensed%20datasets.%20We%20introduce%20%27%27capability%27%27%20groups%20to%20rank%20models%20on%20datasets%20that%20share%20common%20characteristics%20%28e.g.%2C%20resolution%2C%20bands%2C%20temporality%29.%20This%20enables%20users%20to%20identify%20which%20models%20excel%20in%20each%20capability%20and%20determine%20which%20areas%20need%20improvement%20in%20future%20work.%20To%20support%20both%20fair%20comparison%20and%20methodological%20innovation%2C%20we%20define%20a%20prescriptive%20yet%20flexible%20evaluation%20protocol.%20This%20not%20only%20ensures%20consistency%20in%20benchmarking%20but%20also%20facilitates%20research%20into%20model%20adaptation%20strategies%2C%20a%20key%20and%20open%20challenge%20in%20advancing%20GeoFMs%20for%20downstream%20tasks.%0A%20%20Our%20experiments%20show%20that%20no%20single%20model%20dominates%20across%20all%20tasks%2C%20confirming%20the%20specificity%20of%20the%20choices%20made%20during%20architecture%20design%20and%20pretraining.%20While%20models%20pretrained%20on%20natural%20images%20%28ConvNext%20ImageNet%2C%20DINO%20V3%29%20excel%20on%20high-resolution%20tasks%2C%20EO-specific%20models%20%28TerraMind%2C%20Prithvi%2C%20and%20Clay%29%20outperform%20them%20on%20multispectral%20applications%20such%20as%20agriculture%20and%20disaster%20response.%20These%20findings%20demonstrate%20that%20optimal%20model%20choice%20depends%20on%20task%20requirements%2C%20data%20modalities%2C%20and%20constraints.%20This%20shows%20that%20the%20goal%20of%20a%20single%20GeoFM%20model%20that%20performs%20well%20across%20all%20tasks%20remains%20open%20for%20future%20research.%20GEO-Bench-2%20enables%20informed%2C%20reproducible%20GeoFM%20evaluation%20tailored%20to%20specific%20use%20cases.%20Code%2C%20data%2C%20and%20leaderboard%20for%20GEO-Bench-2%20are%20publicly%20released%20under%20a%20permissive%20license.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEO-Bench-2%253A%2520From%2520Performance%2520to%2520Capability%252C%2520Rethinking%2520Evaluation%2520in%2520Geospatial%2520AI%26entry.906535625%3DNaomi%2520Simumba%2520and%2520Nils%2520Lehmann%2520and%2520Paolo%2520Fraccaro%2520and%2520Hamed%2520Alemohammad%2520and%2520Geeth%2520De%2520Mel%2520and%2520Salman%2520Khan%2520and%2520Manil%2520Maskey%2520and%2520Nicolas%2520Longepe%2520and%2520Xiao%2520Xiang%2520Zhu%2520and%2520Hannah%2520Kerner%2520and%2520Juan%2520Bernabe-Moreno%2520and%2520Alexander%2520Lacoste%26entry.1292438233%3DGeospatial%2520Foundation%2520Models%2520%2528GeoFMs%2529%2520are%2520transforming%2520Earth%2520Observation%2520%2528EO%2529%252C%2520but%2520evaluation%2520lacks%2520standardized%2520protocols.%2520GEO-Bench-2%2520addresses%2520this%2520with%2520a%2520comprehensive%2520framework%2520spanning%2520classification%252C%2520segmentation%252C%2520regression%252C%2520object%2520detection%252C%2520and%2520instance%2520segmentation%2520across%252019%2520permissively-licensed%2520datasets.%2520We%2520introduce%2520%2527%2527capability%2527%2527%2520groups%2520to%2520rank%2520models%2520on%2520datasets%2520that%2520share%2520common%2520characteristics%2520%2528e.g.%252C%2520resolution%252C%2520bands%252C%2520temporality%2529.%2520This%2520enables%2520users%2520to%2520identify%2520which%2520models%2520excel%2520in%2520each%2520capability%2520and%2520determine%2520which%2520areas%2520need%2520improvement%2520in%2520future%2520work.%2520To%2520support%2520both%2520fair%2520comparison%2520and%2520methodological%2520innovation%252C%2520we%2520define%2520a%2520prescriptive%2520yet%2520flexible%2520evaluation%2520protocol.%2520This%2520not%2520only%2520ensures%2520consistency%2520in%2520benchmarking%2520but%2520also%2520facilitates%2520research%2520into%2520model%2520adaptation%2520strategies%252C%2520a%2520key%2520and%2520open%2520challenge%2520in%2520advancing%2520GeoFMs%2520for%2520downstream%2520tasks.%250A%2520%2520Our%2520experiments%2520show%2520that%2520no%2520single%2520model%2520dominates%2520across%2520all%2520tasks%252C%2520confirming%2520the%2520specificity%2520of%2520the%2520choices%2520made%2520during%2520architecture%2520design%2520and%2520pretraining.%2520While%2520models%2520pretrained%2520on%2520natural%2520images%2520%2528ConvNext%2520ImageNet%252C%2520DINO%2520V3%2529%2520excel%2520on%2520high-resolution%2520tasks%252C%2520EO-specific%2520models%2520%2528TerraMind%252C%2520Prithvi%252C%2520and%2520Clay%2529%2520outperform%2520them%2520on%2520multispectral%2520applications%2520such%2520as%2520agriculture%2520and%2520disaster%2520response.%2520These%2520findings%2520demonstrate%2520that%2520optimal%2520model%2520choice%2520depends%2520on%2520task%2520requirements%252C%2520data%2520modalities%252C%2520and%2520constraints.%2520This%2520shows%2520that%2520the%2520goal%2520of%2520a%2520single%2520GeoFM%2520model%2520that%2520performs%2520well%2520across%2520all%2520tasks%2520remains%2520open%2520for%2520future%2520research.%2520GEO-Bench-2%2520enables%2520informed%252C%2520reproducible%2520GeoFM%2520evaluation%2520tailored%2520to%2520specific%2520use%2520cases.%2520Code%252C%2520data%252C%2520and%2520leaderboard%2520for%2520GEO-Bench-2%2520are%2520publicly%2520released%2520under%2520a%2520permissive%2520license.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEO-Bench-2%3A%20From%20Performance%20to%20Capability%2C%20Rethinking%20Evaluation%20in%20Geospatial%20AI&entry.906535625=Naomi%20Simumba%20and%20Nils%20Lehmann%20and%20Paolo%20Fraccaro%20and%20Hamed%20Alemohammad%20and%20Geeth%20De%20Mel%20and%20Salman%20Khan%20and%20Manil%20Maskey%20and%20Nicolas%20Longepe%20and%20Xiao%20Xiang%20Zhu%20and%20Hannah%20Kerner%20and%20Juan%20Bernabe-Moreno%20and%20Alexander%20Lacoste&entry.1292438233=Geospatial%20Foundation%20Models%20%28GeoFMs%29%20are%20transforming%20Earth%20Observation%20%28EO%29%2C%20but%20evaluation%20lacks%20standardized%20protocols.%20GEO-Bench-2%20addresses%20this%20with%20a%20comprehensive%20framework%20spanning%20classification%2C%20segmentation%2C%20regression%2C%20object%20detection%2C%20and%20instance%20segmentation%20across%2019%20permissively-licensed%20datasets.%20We%20introduce%20%27%27capability%27%27%20groups%20to%20rank%20models%20on%20datasets%20that%20share%20common%20characteristics%20%28e.g.%2C%20resolution%2C%20bands%2C%20temporality%29.%20This%20enables%20users%20to%20identify%20which%20models%20excel%20in%20each%20capability%20and%20determine%20which%20areas%20need%20improvement%20in%20future%20work.%20To%20support%20both%20fair%20comparison%20and%20methodological%20innovation%2C%20we%20define%20a%20prescriptive%20yet%20flexible%20evaluation%20protocol.%20This%20not%20only%20ensures%20consistency%20in%20benchmarking%20but%20also%20facilitates%20research%20into%20model%20adaptation%20strategies%2C%20a%20key%20and%20open%20challenge%20in%20advancing%20GeoFMs%20for%20downstream%20tasks.%0A%20%20Our%20experiments%20show%20that%20no%20single%20model%20dominates%20across%20all%20tasks%2C%20confirming%20the%20specificity%20of%20the%20choices%20made%20during%20architecture%20design%20and%20pretraining.%20While%20models%20pretrained%20on%20natural%20images%20%28ConvNext%20ImageNet%2C%20DINO%20V3%29%20excel%20on%20high-resolution%20tasks%2C%20EO-specific%20models%20%28TerraMind%2C%20Prithvi%2C%20and%20Clay%29%20outperform%20them%20on%20multispectral%20applications%20such%20as%20agriculture%20and%20disaster%20response.%20These%20findings%20demonstrate%20that%20optimal%20model%20choice%20depends%20on%20task%20requirements%2C%20data%20modalities%2C%20and%20constraints.%20This%20shows%20that%20the%20goal%20of%20a%20single%20GeoFM%20model%20that%20performs%20well%20across%20all%20tasks%20remains%20open%20for%20future%20research.%20GEO-Bench-2%20enables%20informed%2C%20reproducible%20GeoFM%20evaluation%20tailored%20to%20specific%20use%20cases.%20Code%2C%20data%2C%20and%20leaderboard%20for%20GEO-Bench-2%20are%20publicly%20released%20under%20a%20permissive%20license.&entry.1838667208=http%3A//arxiv.org/abs/2511.15658v1&entry.124074799=Read"},
{"title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models", "author": "Piercosma Bisconti and Matteo Prandi and Federico Pierucci and Francesco Giarrusso and Marcantonio Bracale and Marcello Galisai and Vincenzo Suriani and Olga Sorokoletova and Federico Sartore and Daniele Nardi", "abstract": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.", "link": "http://arxiv.org/abs/2511.15304v1", "date": "2025-11-19", "relevancy": 2.0667, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.43}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models&body=Title%3A%20Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models%0AAuthor%3A%20Piercosma%20Bisconti%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Francesco%20Giarrusso%20and%20Marcantonio%20Bracale%20and%20Marcello%20Galisai%20and%20Vincenzo%20Suriani%20and%20Olga%20Sorokoletova%20and%20Federico%20Sartore%20and%20Daniele%20Nardi%0AAbstract%3A%20We%20present%20evidence%20that%20adversarial%20poetry%20functions%20as%20a%20universal%20single-turn%20jailbreak%20technique%20for%20large%20language%20models%20%28LLMs%29.%20Across%2025%20frontier%20proprietary%20and%20open-weight%20models%2C%20curated%20poetic%20prompts%20yielded%20high%20attack-success%20rates%20%28ASR%29%2C%20with%20some%20providers%20exceeding%2090%25.%20Mapping%20prompts%20to%20MLCommons%20and%20EU%20CoP%20risk%20taxonomies%20shows%20that%20poetic%20attacks%20transfer%20across%20CBRN%2C%20manipulation%2C%20cyber-offence%2C%20and%20loss-of-control%20domains.%20Converting%201%2C200%20MLCommons%20harmful%20prompts%20into%20verse%20via%20a%20standardized%20meta-prompt%20produced%20ASRs%20up%20to%2018%20times%20higher%20than%20their%20prose%20baselines.%20Outputs%20are%20evaluated%20using%20an%20ensemble%20of%20open-weight%20judge%20models%20and%20a%20human-validated%20stratified%20subset%20%28with%20double-annotations%20to%20measure%20agreement%29.%20Disagreements%20were%20manually%20resolved.%20Poetic%20framing%20achieved%20an%20average%20jailbreak%20success%20rate%20of%2062%25%20for%20hand-crafted%20poems%20and%20approximately%2043%25%20for%20meta-prompt%20conversions%20%28compared%20to%20non-poetic%20baselines%29%2C%20substantially%20outperforming%20non-poetic%20baselines%20and%20revealing%20a%20systematic%20vulnerability%20across%20model%20families%20and%20safety%20training%20approaches.%20These%20findings%20demonstrate%20that%20stylistic%20variation%20alone%20can%20circumvent%20contemporary%20safety%20mechanisms%2C%20suggesting%20fundamental%20limitations%20in%20current%20alignment%20methods%20and%20evaluation%20protocols.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Poetry%2520as%2520a%2520Universal%2520Single-Turn%2520Jailbreak%2520Mechanism%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DPiercosma%2520Bisconti%2520and%2520Matteo%2520Prandi%2520and%2520Federico%2520Pierucci%2520and%2520Francesco%2520Giarrusso%2520and%2520Marcantonio%2520Bracale%2520and%2520Marcello%2520Galisai%2520and%2520Vincenzo%2520Suriani%2520and%2520Olga%2520Sorokoletova%2520and%2520Federico%2520Sartore%2520and%2520Daniele%2520Nardi%26entry.1292438233%3DWe%2520present%2520evidence%2520that%2520adversarial%2520poetry%2520functions%2520as%2520a%2520universal%2520single-turn%2520jailbreak%2520technique%2520for%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Across%252025%2520frontier%2520proprietary%2520and%2520open-weight%2520models%252C%2520curated%2520poetic%2520prompts%2520yielded%2520high%2520attack-success%2520rates%2520%2528ASR%2529%252C%2520with%2520some%2520providers%2520exceeding%252090%2525.%2520Mapping%2520prompts%2520to%2520MLCommons%2520and%2520EU%2520CoP%2520risk%2520taxonomies%2520shows%2520that%2520poetic%2520attacks%2520transfer%2520across%2520CBRN%252C%2520manipulation%252C%2520cyber-offence%252C%2520and%2520loss-of-control%2520domains.%2520Converting%25201%252C200%2520MLCommons%2520harmful%2520prompts%2520into%2520verse%2520via%2520a%2520standardized%2520meta-prompt%2520produced%2520ASRs%2520up%2520to%252018%2520times%2520higher%2520than%2520their%2520prose%2520baselines.%2520Outputs%2520are%2520evaluated%2520using%2520an%2520ensemble%2520of%2520open-weight%2520judge%2520models%2520and%2520a%2520human-validated%2520stratified%2520subset%2520%2528with%2520double-annotations%2520to%2520measure%2520agreement%2529.%2520Disagreements%2520were%2520manually%2520resolved.%2520Poetic%2520framing%2520achieved%2520an%2520average%2520jailbreak%2520success%2520rate%2520of%252062%2525%2520for%2520hand-crafted%2520poems%2520and%2520approximately%252043%2525%2520for%2520meta-prompt%2520conversions%2520%2528compared%2520to%2520non-poetic%2520baselines%2529%252C%2520substantially%2520outperforming%2520non-poetic%2520baselines%2520and%2520revealing%2520a%2520systematic%2520vulnerability%2520across%2520model%2520families%2520and%2520safety%2520training%2520approaches.%2520These%2520findings%2520demonstrate%2520that%2520stylistic%2520variation%2520alone%2520can%2520circumvent%2520contemporary%2520safety%2520mechanisms%252C%2520suggesting%2520fundamental%2520limitations%2520in%2520current%2520alignment%2520methods%2520and%2520evaluation%2520protocols.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Poetry%20as%20a%20Universal%20Single-Turn%20Jailbreak%20Mechanism%20in%20Large%20Language%20Models&entry.906535625=Piercosma%20Bisconti%20and%20Matteo%20Prandi%20and%20Federico%20Pierucci%20and%20Francesco%20Giarrusso%20and%20Marcantonio%20Bracale%20and%20Marcello%20Galisai%20and%20Vincenzo%20Suriani%20and%20Olga%20Sorokoletova%20and%20Federico%20Sartore%20and%20Daniele%20Nardi&entry.1292438233=We%20present%20evidence%20that%20adversarial%20poetry%20functions%20as%20a%20universal%20single-turn%20jailbreak%20technique%20for%20large%20language%20models%20%28LLMs%29.%20Across%2025%20frontier%20proprietary%20and%20open-weight%20models%2C%20curated%20poetic%20prompts%20yielded%20high%20attack-success%20rates%20%28ASR%29%2C%20with%20some%20providers%20exceeding%2090%25.%20Mapping%20prompts%20to%20MLCommons%20and%20EU%20CoP%20risk%20taxonomies%20shows%20that%20poetic%20attacks%20transfer%20across%20CBRN%2C%20manipulation%2C%20cyber-offence%2C%20and%20loss-of-control%20domains.%20Converting%201%2C200%20MLCommons%20harmful%20prompts%20into%20verse%20via%20a%20standardized%20meta-prompt%20produced%20ASRs%20up%20to%2018%20times%20higher%20than%20their%20prose%20baselines.%20Outputs%20are%20evaluated%20using%20an%20ensemble%20of%20open-weight%20judge%20models%20and%20a%20human-validated%20stratified%20subset%20%28with%20double-annotations%20to%20measure%20agreement%29.%20Disagreements%20were%20manually%20resolved.%20Poetic%20framing%20achieved%20an%20average%20jailbreak%20success%20rate%20of%2062%25%20for%20hand-crafted%20poems%20and%20approximately%2043%25%20for%20meta-prompt%20conversions%20%28compared%20to%20non-poetic%20baselines%29%2C%20substantially%20outperforming%20non-poetic%20baselines%20and%20revealing%20a%20systematic%20vulnerability%20across%20model%20families%20and%20safety%20training%20approaches.%20These%20findings%20demonstrate%20that%20stylistic%20variation%20alone%20can%20circumvent%20contemporary%20safety%20mechanisms%2C%20suggesting%20fundamental%20limitations%20in%20current%20alignment%20methods%20and%20evaluation%20protocols.&entry.1838667208=http%3A//arxiv.org/abs/2511.15304v1&entry.124074799=Read"},
{"title": "Controlling False Positives in Image Segmentation via Conformal Prediction", "author": "Luca Mossina and Corentin Friedrich", "abstract": "Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.", "link": "http://arxiv.org/abs/2511.15406v1", "date": "2025-11-19", "relevancy": 2.0615, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5174}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5147}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20False%20Positives%20in%20Image%20Segmentation%20via%20Conformal%20Prediction&body=Title%3A%20Controlling%20False%20Positives%20in%20Image%20Segmentation%20via%20Conformal%20Prediction%0AAuthor%3A%20Luca%20Mossina%20and%20Corentin%20Friedrich%0AAbstract%3A%20Reliable%20semantic%20segmentation%20is%20essential%20for%20clinical%20decision%20making%2C%20yet%20deep%20models%20rarely%20provide%20explicit%20statistical%20guarantees%20on%20their%20errors.%20We%20introduce%20a%20simple%20post-hoc%20framework%20that%20constructs%20confidence%20masks%20with%20distribution-free%2C%20image-level%20control%20of%20false-positive%20predictions.%20Given%20any%20pretrained%20segmentation%20model%2C%20we%20define%20a%20nested%20family%20of%20shrunken%20masks%20obtained%20either%20by%20increasing%20the%20score%20threshold%20or%20by%20applying%20morphological%20erosion.%20A%20labeled%20calibration%20set%20is%20used%20to%20select%20a%20single%20shrink%20parameter%20via%20conformal%20prediction%2C%20ensuring%20that%2C%20for%20new%20images%20that%20are%20exchangeable%20with%20the%20calibration%20data%2C%20the%20proportion%20of%20false%20positives%20retained%20in%20the%20confidence%20mask%20stays%20below%20a%20user-specified%20tolerance%20with%20high%20probability.%20The%20method%20is%20model-agnostic%2C%20requires%20no%20retraining%2C%20and%20provides%20finite-sample%20guarantees%20regardless%20of%20the%20underlying%20predictor.%20Experiments%20on%20a%20polyp-segmentation%20benchmark%20demonstrate%20target-level%20empirical%20validity.%20Our%20framework%20enables%20practical%2C%20risk-aware%20segmentation%20in%20settings%20where%20over-segmentation%20can%20have%20clinical%20consequences.%20Code%20at%20https%3A//github.com/deel-ai-papers/conseco.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520False%2520Positives%2520in%2520Image%2520Segmentation%2520via%2520Conformal%2520Prediction%26entry.906535625%3DLuca%2520Mossina%2520and%2520Corentin%2520Friedrich%26entry.1292438233%3DReliable%2520semantic%2520segmentation%2520is%2520essential%2520for%2520clinical%2520decision%2520making%252C%2520yet%2520deep%2520models%2520rarely%2520provide%2520explicit%2520statistical%2520guarantees%2520on%2520their%2520errors.%2520We%2520introduce%2520a%2520simple%2520post-hoc%2520framework%2520that%2520constructs%2520confidence%2520masks%2520with%2520distribution-free%252C%2520image-level%2520control%2520of%2520false-positive%2520predictions.%2520Given%2520any%2520pretrained%2520segmentation%2520model%252C%2520we%2520define%2520a%2520nested%2520family%2520of%2520shrunken%2520masks%2520obtained%2520either%2520by%2520increasing%2520the%2520score%2520threshold%2520or%2520by%2520applying%2520morphological%2520erosion.%2520A%2520labeled%2520calibration%2520set%2520is%2520used%2520to%2520select%2520a%2520single%2520shrink%2520parameter%2520via%2520conformal%2520prediction%252C%2520ensuring%2520that%252C%2520for%2520new%2520images%2520that%2520are%2520exchangeable%2520with%2520the%2520calibration%2520data%252C%2520the%2520proportion%2520of%2520false%2520positives%2520retained%2520in%2520the%2520confidence%2520mask%2520stays%2520below%2520a%2520user-specified%2520tolerance%2520with%2520high%2520probability.%2520The%2520method%2520is%2520model-agnostic%252C%2520requires%2520no%2520retraining%252C%2520and%2520provides%2520finite-sample%2520guarantees%2520regardless%2520of%2520the%2520underlying%2520predictor.%2520Experiments%2520on%2520a%2520polyp-segmentation%2520benchmark%2520demonstrate%2520target-level%2520empirical%2520validity.%2520Our%2520framework%2520enables%2520practical%252C%2520risk-aware%2520segmentation%2520in%2520settings%2520where%2520over-segmentation%2520can%2520have%2520clinical%2520consequences.%2520Code%2520at%2520https%253A//github.com/deel-ai-papers/conseco.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20False%20Positives%20in%20Image%20Segmentation%20via%20Conformal%20Prediction&entry.906535625=Luca%20Mossina%20and%20Corentin%20Friedrich&entry.1292438233=Reliable%20semantic%20segmentation%20is%20essential%20for%20clinical%20decision%20making%2C%20yet%20deep%20models%20rarely%20provide%20explicit%20statistical%20guarantees%20on%20their%20errors.%20We%20introduce%20a%20simple%20post-hoc%20framework%20that%20constructs%20confidence%20masks%20with%20distribution-free%2C%20image-level%20control%20of%20false-positive%20predictions.%20Given%20any%20pretrained%20segmentation%20model%2C%20we%20define%20a%20nested%20family%20of%20shrunken%20masks%20obtained%20either%20by%20increasing%20the%20score%20threshold%20or%20by%20applying%20morphological%20erosion.%20A%20labeled%20calibration%20set%20is%20used%20to%20select%20a%20single%20shrink%20parameter%20via%20conformal%20prediction%2C%20ensuring%20that%2C%20for%20new%20images%20that%20are%20exchangeable%20with%20the%20calibration%20data%2C%20the%20proportion%20of%20false%20positives%20retained%20in%20the%20confidence%20mask%20stays%20below%20a%20user-specified%20tolerance%20with%20high%20probability.%20The%20method%20is%20model-agnostic%2C%20requires%20no%20retraining%2C%20and%20provides%20finite-sample%20guarantees%20regardless%20of%20the%20underlying%20predictor.%20Experiments%20on%20a%20polyp-segmentation%20benchmark%20demonstrate%20target-level%20empirical%20validity.%20Our%20framework%20enables%20practical%2C%20risk-aware%20segmentation%20in%20settings%20where%20over-segmentation%20can%20have%20clinical%20consequences.%20Code%20at%20https%3A//github.com/deel-ai-papers/conseco.&entry.1838667208=http%3A//arxiv.org/abs/2511.15406v1&entry.124074799=Read"},
{"title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving", "author": "Ferran Agullo and Joan Oliveras and Chen Wang and Alberto Gutierrez-Torre and Olivier Tardieu and Alaa Youssef and Jordi Torres and Josep Ll. Berral", "abstract": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.", "link": "http://arxiv.org/abs/2508.08343v3", "date": "2025-11-19", "relevancy": 2.0489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5067}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Data-driven%20ML%20Approach%20for%20Maximizing%20Performance%20in%20LLM-Adapter%20Serving&body=Title%3A%20A%20Data-driven%20ML%20Approach%20for%20Maximizing%20Performance%20in%20LLM-Adapter%20Serving%0AAuthor%3A%20Ferran%20Agullo%20and%20Joan%20Oliveras%20and%20Chen%20Wang%20and%20Alberto%20Gutierrez-Torre%20and%20Olivier%20Tardieu%20and%20Alaa%20Youssef%20and%20Jordi%20Torres%20and%20Josep%20Ll.%20Berral%0AAbstract%3A%20With%20the%20rapid%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20LLM-adapters%20have%20become%20increasingly%20common%2C%20providing%20lightweight%20specialization%20of%20large-scale%20models.%20Serving%20hundreds%20or%20thousands%20of%20these%20adapters%20on%20a%20single%20GPU%20allows%20request%20aggregation%2C%20increasing%20throughput%2C%20but%20may%20also%20cause%20request%20starvation%20if%20GPU%20memory%20limits%20are%20exceeded.%20To%20address%20this%20issue%2C%20this%20study%20focuses%20on%20determining%20the%20joint%20configuration%20of%20concurrent%20and%20parallel%20adapters%20that%20maximizes%20GPU%20throughput%20without%20inducing%20starvation%2C%20given%20heterogeneous%20adapter%20and%20traffic%20properties.%20We%20propose%20a%20data-driven%20ML%20approach%20leveraging%20interpretable%20models%20to%20tackle%20this%20caching%20problem%20and%20introduce%20the%20first%20Digital%20Twin%20capable%20of%20reproducing%20an%20LLM-adapter%20serving%20system%2C%20enabling%20efficient%20training%20data%20generation.%20Experiments%20with%20the%20vLLM%20framework%20and%20LoRA%20adapters%20show%20that%20the%20Digital%20Twin%20reproduces%20throughput%20within%205.1%25%20of%20real%20results%2C%20while%20the%20ML%20approach%20predicts%20optimal%20numbers%20of%20concurrent%20and%20parallel%20adapters%20with%20an%20error%20of%20at%20most%207.2%25%20under%20heterogeneous%2C%20real-world%20workloads.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/FerranAgulloLopez/GPULLMAdapterOptimization.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08343v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Data-driven%2520ML%2520Approach%2520for%2520Maximizing%2520Performance%2520in%2520LLM-Adapter%2520Serving%26entry.906535625%3DFerran%2520Agullo%2520and%2520Joan%2520Oliveras%2520and%2520Chen%2520Wang%2520and%2520Alberto%2520Gutierrez-Torre%2520and%2520Olivier%2520Tardieu%2520and%2520Alaa%2520Youssef%2520and%2520Jordi%2520Torres%2520and%2520Josep%2520Ll.%2520Berral%26entry.1292438233%3DWith%2520the%2520rapid%2520adoption%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520LLM-adapters%2520have%2520become%2520increasingly%2520common%252C%2520providing%2520lightweight%2520specialization%2520of%2520large-scale%2520models.%2520Serving%2520hundreds%2520or%2520thousands%2520of%2520these%2520adapters%2520on%2520a%2520single%2520GPU%2520allows%2520request%2520aggregation%252C%2520increasing%2520throughput%252C%2520but%2520may%2520also%2520cause%2520request%2520starvation%2520if%2520GPU%2520memory%2520limits%2520are%2520exceeded.%2520To%2520address%2520this%2520issue%252C%2520this%2520study%2520focuses%2520on%2520determining%2520the%2520joint%2520configuration%2520of%2520concurrent%2520and%2520parallel%2520adapters%2520that%2520maximizes%2520GPU%2520throughput%2520without%2520inducing%2520starvation%252C%2520given%2520heterogeneous%2520adapter%2520and%2520traffic%2520properties.%2520We%2520propose%2520a%2520data-driven%2520ML%2520approach%2520leveraging%2520interpretable%2520models%2520to%2520tackle%2520this%2520caching%2520problem%2520and%2520introduce%2520the%2520first%2520Digital%2520Twin%2520capable%2520of%2520reproducing%2520an%2520LLM-adapter%2520serving%2520system%252C%2520enabling%2520efficient%2520training%2520data%2520generation.%2520Experiments%2520with%2520the%2520vLLM%2520framework%2520and%2520LoRA%2520adapters%2520show%2520that%2520the%2520Digital%2520Twin%2520reproduces%2520throughput%2520within%25205.1%2525%2520of%2520real%2520results%252C%2520while%2520the%2520ML%2520approach%2520predicts%2520optimal%2520numbers%2520of%2520concurrent%2520and%2520parallel%2520adapters%2520with%2520an%2520error%2520of%2520at%2520most%25207.2%2525%2520under%2520heterogeneous%252C%2520real-world%2520workloads.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/FerranAgulloLopez/GPULLMAdapterOptimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08343v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-driven%20ML%20Approach%20for%20Maximizing%20Performance%20in%20LLM-Adapter%20Serving&entry.906535625=Ferran%20Agullo%20and%20Joan%20Oliveras%20and%20Chen%20Wang%20and%20Alberto%20Gutierrez-Torre%20and%20Olivier%20Tardieu%20and%20Alaa%20Youssef%20and%20Jordi%20Torres%20and%20Josep%20Ll.%20Berral&entry.1292438233=With%20the%20rapid%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20LLM-adapters%20have%20become%20increasingly%20common%2C%20providing%20lightweight%20specialization%20of%20large-scale%20models.%20Serving%20hundreds%20or%20thousands%20of%20these%20adapters%20on%20a%20single%20GPU%20allows%20request%20aggregation%2C%20increasing%20throughput%2C%20but%20may%20also%20cause%20request%20starvation%20if%20GPU%20memory%20limits%20are%20exceeded.%20To%20address%20this%20issue%2C%20this%20study%20focuses%20on%20determining%20the%20joint%20configuration%20of%20concurrent%20and%20parallel%20adapters%20that%20maximizes%20GPU%20throughput%20without%20inducing%20starvation%2C%20given%20heterogeneous%20adapter%20and%20traffic%20properties.%20We%20propose%20a%20data-driven%20ML%20approach%20leveraging%20interpretable%20models%20to%20tackle%20this%20caching%20problem%20and%20introduce%20the%20first%20Digital%20Twin%20capable%20of%20reproducing%20an%20LLM-adapter%20serving%20system%2C%20enabling%20efficient%20training%20data%20generation.%20Experiments%20with%20the%20vLLM%20framework%20and%20LoRA%20adapters%20show%20that%20the%20Digital%20Twin%20reproduces%20throughput%20within%205.1%25%20of%20real%20results%2C%20while%20the%20ML%20approach%20predicts%20optimal%20numbers%20of%20concurrent%20and%20parallel%20adapters%20with%20an%20error%20of%20at%20most%207.2%25%20under%20heterogeneous%2C%20real-world%20workloads.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/FerranAgulloLopez/GPULLMAdapterOptimization.&entry.1838667208=http%3A//arxiv.org/abs/2508.08343v3&entry.124074799=Read"},
{"title": "Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations", "author": "Laurin Lux and Alexander H. Berger and Maria Romeo Tricas and Richard Rosen and Alaa E. Fayed and Sobha Sivaprasada and Linus Kreitner and Jonas Weidner and Martin J. Menten and Daniel Rueckert and Johannes C. Paetzold", "abstract": "Interpretability is crucial to enhance trust in machine learning models for medical diagnostics. However, most state-of-the-art image classifiers based on neural networks are not interpretable. As a result, clinicians often resort to known biomarkers for diagnosis, although biomarker-based classification typically performs worse than large neural networks. This work proposes a method that surpasses the performance of established machine learning models while simultaneously improving prediction interpretability for diabetic retinopathy staging from optical coherence tomography angiography (OCTA) images. Our method is based on a novel biology-informed heterogeneous graph representation that models retinal vessel segments, intercapillary areas, and the foveal avascular zone (FAZ) in a human-interpretable way. This graph representation allows us to frame diabetic retinopathy staging as a graph-level classification task, which we solve using an efficient graph neural network. We benchmark our method against well-established baselines, including classical biomarker-based classifiers, convolutional neural networks (CNNs), and vision transformers. Our model outperforms all baselines on two datasets. Crucially, we use our biology-informed graph to provide explanations of unprecedented detail. Our approach surpasses existing methods in precisely localizing and identifying critical vessels or intercapillary areas. In addition, we give informative and human-interpretable attributions to critical characteristics. Our work contributes to the development of clinical decision-support tools in ophthalmology.", "link": "http://arxiv.org/abs/2502.16697v2", "date": "2025-11-19", "relevancy": 2.0452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5112}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Retinal%20Disease%20Prediction%20Using%20Biology-Informed%20Heterogeneous%20Graph%20Representations&body=Title%3A%20Interpretable%20Retinal%20Disease%20Prediction%20Using%20Biology-Informed%20Heterogeneous%20Graph%20Representations%0AAuthor%3A%20Laurin%20Lux%20and%20Alexander%20H.%20Berger%20and%20Maria%20Romeo%20Tricas%20and%20Richard%20Rosen%20and%20Alaa%20E.%20Fayed%20and%20Sobha%20Sivaprasada%20and%20Linus%20Kreitner%20and%20Jonas%20Weidner%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20Interpretability%20is%20crucial%20to%20enhance%20trust%20in%20machine%20learning%20models%20for%20medical%20diagnostics.%20However%2C%20most%20state-of-the-art%20image%20classifiers%20based%20on%20neural%20networks%20are%20not%20interpretable.%20As%20a%20result%2C%20clinicians%20often%20resort%20to%20known%20biomarkers%20for%20diagnosis%2C%20although%20biomarker-based%20classification%20typically%20performs%20worse%20than%20large%20neural%20networks.%20This%20work%20proposes%20a%20method%20that%20surpasses%20the%20performance%20of%20established%20machine%20learning%20models%20while%20simultaneously%20improving%20prediction%20interpretability%20for%20diabetic%20retinopathy%20staging%20from%20optical%20coherence%20tomography%20angiography%20%28OCTA%29%20images.%20Our%20method%20is%20based%20on%20a%20novel%20biology-informed%20heterogeneous%20graph%20representation%20that%20models%20retinal%20vessel%20segments%2C%20intercapillary%20areas%2C%20and%20the%20foveal%20avascular%20zone%20%28FAZ%29%20in%20a%20human-interpretable%20way.%20This%20graph%20representation%20allows%20us%20to%20frame%20diabetic%20retinopathy%20staging%20as%20a%20graph-level%20classification%20task%2C%20which%20we%20solve%20using%20an%20efficient%20graph%20neural%20network.%20We%20benchmark%20our%20method%20against%20well-established%20baselines%2C%20including%20classical%20biomarker-based%20classifiers%2C%20convolutional%20neural%20networks%20%28CNNs%29%2C%20and%20vision%20transformers.%20Our%20model%20outperforms%20all%20baselines%20on%20two%20datasets.%20Crucially%2C%20we%20use%20our%20biology-informed%20graph%20to%20provide%20explanations%20of%20unprecedented%20detail.%20Our%20approach%20surpasses%20existing%20methods%20in%20precisely%20localizing%20and%20identifying%20critical%20vessels%20or%20intercapillary%20areas.%20In%20addition%2C%20we%20give%20informative%20and%20human-interpretable%20attributions%20to%20critical%20characteristics.%20Our%20work%20contributes%20to%20the%20development%20of%20clinical%20decision-support%20tools%20in%20ophthalmology.%0ALink%3A%20http%3A//arxiv.org/abs/2502.16697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Retinal%2520Disease%2520Prediction%2520Using%2520Biology-Informed%2520Heterogeneous%2520Graph%2520Representations%26entry.906535625%3DLaurin%2520Lux%2520and%2520Alexander%2520H.%2520Berger%2520and%2520Maria%2520Romeo%2520Tricas%2520and%2520Richard%2520Rosen%2520and%2520Alaa%2520E.%2520Fayed%2520and%2520Sobha%2520Sivaprasada%2520and%2520Linus%2520Kreitner%2520and%2520Jonas%2520Weidner%2520and%2520Martin%2520J.%2520Menten%2520and%2520Daniel%2520Rueckert%2520and%2520Johannes%2520C.%2520Paetzold%26entry.1292438233%3DInterpretability%2520is%2520crucial%2520to%2520enhance%2520trust%2520in%2520machine%2520learning%2520models%2520for%2520medical%2520diagnostics.%2520However%252C%2520most%2520state-of-the-art%2520image%2520classifiers%2520based%2520on%2520neural%2520networks%2520are%2520not%2520interpretable.%2520As%2520a%2520result%252C%2520clinicians%2520often%2520resort%2520to%2520known%2520biomarkers%2520for%2520diagnosis%252C%2520although%2520biomarker-based%2520classification%2520typically%2520performs%2520worse%2520than%2520large%2520neural%2520networks.%2520This%2520work%2520proposes%2520a%2520method%2520that%2520surpasses%2520the%2520performance%2520of%2520established%2520machine%2520learning%2520models%2520while%2520simultaneously%2520improving%2520prediction%2520interpretability%2520for%2520diabetic%2520retinopathy%2520staging%2520from%2520optical%2520coherence%2520tomography%2520angiography%2520%2528OCTA%2529%2520images.%2520Our%2520method%2520is%2520based%2520on%2520a%2520novel%2520biology-informed%2520heterogeneous%2520graph%2520representation%2520that%2520models%2520retinal%2520vessel%2520segments%252C%2520intercapillary%2520areas%252C%2520and%2520the%2520foveal%2520avascular%2520zone%2520%2528FAZ%2529%2520in%2520a%2520human-interpretable%2520way.%2520This%2520graph%2520representation%2520allows%2520us%2520to%2520frame%2520diabetic%2520retinopathy%2520staging%2520as%2520a%2520graph-level%2520classification%2520task%252C%2520which%2520we%2520solve%2520using%2520an%2520efficient%2520graph%2520neural%2520network.%2520We%2520benchmark%2520our%2520method%2520against%2520well-established%2520baselines%252C%2520including%2520classical%2520biomarker-based%2520classifiers%252C%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520and%2520vision%2520transformers.%2520Our%2520model%2520outperforms%2520all%2520baselines%2520on%2520two%2520datasets.%2520Crucially%252C%2520we%2520use%2520our%2520biology-informed%2520graph%2520to%2520provide%2520explanations%2520of%2520unprecedented%2520detail.%2520Our%2520approach%2520surpasses%2520existing%2520methods%2520in%2520precisely%2520localizing%2520and%2520identifying%2520critical%2520vessels%2520or%2520intercapillary%2520areas.%2520In%2520addition%252C%2520we%2520give%2520informative%2520and%2520human-interpretable%2520attributions%2520to%2520critical%2520characteristics.%2520Our%2520work%2520contributes%2520to%2520the%2520development%2520of%2520clinical%2520decision-support%2520tools%2520in%2520ophthalmology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Retinal%20Disease%20Prediction%20Using%20Biology-Informed%20Heterogeneous%20Graph%20Representations&entry.906535625=Laurin%20Lux%20and%20Alexander%20H.%20Berger%20and%20Maria%20Romeo%20Tricas%20and%20Richard%20Rosen%20and%20Alaa%20E.%20Fayed%20and%20Sobha%20Sivaprasada%20and%20Linus%20Kreitner%20and%20Jonas%20Weidner%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert%20and%20Johannes%20C.%20Paetzold&entry.1292438233=Interpretability%20is%20crucial%20to%20enhance%20trust%20in%20machine%20learning%20models%20for%20medical%20diagnostics.%20However%2C%20most%20state-of-the-art%20image%20classifiers%20based%20on%20neural%20networks%20are%20not%20interpretable.%20As%20a%20result%2C%20clinicians%20often%20resort%20to%20known%20biomarkers%20for%20diagnosis%2C%20although%20biomarker-based%20classification%20typically%20performs%20worse%20than%20large%20neural%20networks.%20This%20work%20proposes%20a%20method%20that%20surpasses%20the%20performance%20of%20established%20machine%20learning%20models%20while%20simultaneously%20improving%20prediction%20interpretability%20for%20diabetic%20retinopathy%20staging%20from%20optical%20coherence%20tomography%20angiography%20%28OCTA%29%20images.%20Our%20method%20is%20based%20on%20a%20novel%20biology-informed%20heterogeneous%20graph%20representation%20that%20models%20retinal%20vessel%20segments%2C%20intercapillary%20areas%2C%20and%20the%20foveal%20avascular%20zone%20%28FAZ%29%20in%20a%20human-interpretable%20way.%20This%20graph%20representation%20allows%20us%20to%20frame%20diabetic%20retinopathy%20staging%20as%20a%20graph-level%20classification%20task%2C%20which%20we%20solve%20using%20an%20efficient%20graph%20neural%20network.%20We%20benchmark%20our%20method%20against%20well-established%20baselines%2C%20including%20classical%20biomarker-based%20classifiers%2C%20convolutional%20neural%20networks%20%28CNNs%29%2C%20and%20vision%20transformers.%20Our%20model%20outperforms%20all%20baselines%20on%20two%20datasets.%20Crucially%2C%20we%20use%20our%20biology-informed%20graph%20to%20provide%20explanations%20of%20unprecedented%20detail.%20Our%20approach%20surpasses%20existing%20methods%20in%20precisely%20localizing%20and%20identifying%20critical%20vessels%20or%20intercapillary%20areas.%20In%20addition%2C%20we%20give%20informative%20and%20human-interpretable%20attributions%20to%20critical%20characteristics.%20Our%20work%20contributes%20to%20the%20development%20of%20clinical%20decision-support%20tools%20in%20ophthalmology.&entry.1838667208=http%3A//arxiv.org/abs/2502.16697v2&entry.124074799=Read"},
{"title": "TrackStudio: An Integrated Toolkit for Markerless Tracking", "author": "Hristo Dimitrov and Giulia Dominijanni and Viktorija Pavalkyte and Tamar R. Makin", "abstract": "Markerless motion tracking has advanced rapidly in the past 10 years and currently offers powerful opportunities for behavioural, clinical, and biomechanical research. While several specialised toolkits provide high performance for specific tasks, using existing tools still requires substantial technical expertise. There remains a gap in accessible, integrated solutions that deliver sufficient tracking for non-experts across diverse settings.\n  TrackStudio was developed to address this gap by combining established open-source tools into a single, modular, GUI-based pipeline that works out of the box. It provides automatic 2D and 3D tracking, calibration, preprocessing, feature extraction, and visualisation without requiring any programming skills. We supply a user guide with practical advice for video acquisition, synchronisation, and setup, alongside documentation of common pitfalls and how to avoid them.\n  To validate the toolkit, we tested its performance across three environments using either low-cost webcams or high-resolution cameras, including challenging conditions for body position, lightning, and space and obstructions. Across 76 participants, average inter-frame correlations exceeded 0.98 and average triangulation errors remained low (<13.6mm for hand tracking), demonstrating stable and consistent tracking. We further show that the same pipeline can be extended beyond hand tracking to other body and face regions. TrackStudio provides a practical, accessible route into markerless tracking for researchers or laypeople who need reliable performance without specialist expertise.", "link": "http://arxiv.org/abs/2511.07624v2", "date": "2025-11-19", "relevancy": 2.0443, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5149}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5139}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackStudio%3A%20An%20Integrated%20Toolkit%20for%20Markerless%20Tracking&body=Title%3A%20TrackStudio%3A%20An%20Integrated%20Toolkit%20for%20Markerless%20Tracking%0AAuthor%3A%20Hristo%20Dimitrov%20and%20Giulia%20Dominijanni%20and%20Viktorija%20Pavalkyte%20and%20Tamar%20R.%20Makin%0AAbstract%3A%20Markerless%20motion%20tracking%20has%20advanced%20rapidly%20in%20the%20past%2010%20years%20and%20currently%20offers%20powerful%20opportunities%20for%20behavioural%2C%20clinical%2C%20and%20biomechanical%20research.%20While%20several%20specialised%20toolkits%20provide%20high%20performance%20for%20specific%20tasks%2C%20using%20existing%20tools%20still%20requires%20substantial%20technical%20expertise.%20There%20remains%20a%20gap%20in%20accessible%2C%20integrated%20solutions%20that%20deliver%20sufficient%20tracking%20for%20non-experts%20across%20diverse%20settings.%0A%20%20TrackStudio%20was%20developed%20to%20address%20this%20gap%20by%20combining%20established%20open-source%20tools%20into%20a%20single%2C%20modular%2C%20GUI-based%20pipeline%20that%20works%20out%20of%20the%20box.%20It%20provides%20automatic%202D%20and%203D%20tracking%2C%20calibration%2C%20preprocessing%2C%20feature%20extraction%2C%20and%20visualisation%20without%20requiring%20any%20programming%20skills.%20We%20supply%20a%20user%20guide%20with%20practical%20advice%20for%20video%20acquisition%2C%20synchronisation%2C%20and%20setup%2C%20alongside%20documentation%20of%20common%20pitfalls%20and%20how%20to%20avoid%20them.%0A%20%20To%20validate%20the%20toolkit%2C%20we%20tested%20its%20performance%20across%20three%20environments%20using%20either%20low-cost%20webcams%20or%20high-resolution%20cameras%2C%20including%20challenging%20conditions%20for%20body%20position%2C%20lightning%2C%20and%20space%20and%20obstructions.%20Across%2076%20participants%2C%20average%20inter-frame%20correlations%20exceeded%200.98%20and%20average%20triangulation%20errors%20remained%20low%20%28%3C13.6mm%20for%20hand%20tracking%29%2C%20demonstrating%20stable%20and%20consistent%20tracking.%20We%20further%20show%20that%20the%20same%20pipeline%20can%20be%20extended%20beyond%20hand%20tracking%20to%20other%20body%20and%20face%20regions.%20TrackStudio%20provides%20a%20practical%2C%20accessible%20route%20into%20markerless%20tracking%20for%20researchers%20or%20laypeople%20who%20need%20reliable%20performance%20without%20specialist%20expertise.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackStudio%253A%2520An%2520Integrated%2520Toolkit%2520for%2520Markerless%2520Tracking%26entry.906535625%3DHristo%2520Dimitrov%2520and%2520Giulia%2520Dominijanni%2520and%2520Viktorija%2520Pavalkyte%2520and%2520Tamar%2520R.%2520Makin%26entry.1292438233%3DMarkerless%2520motion%2520tracking%2520has%2520advanced%2520rapidly%2520in%2520the%2520past%252010%2520years%2520and%2520currently%2520offers%2520powerful%2520opportunities%2520for%2520behavioural%252C%2520clinical%252C%2520and%2520biomechanical%2520research.%2520While%2520several%2520specialised%2520toolkits%2520provide%2520high%2520performance%2520for%2520specific%2520tasks%252C%2520using%2520existing%2520tools%2520still%2520requires%2520substantial%2520technical%2520expertise.%2520There%2520remains%2520a%2520gap%2520in%2520accessible%252C%2520integrated%2520solutions%2520that%2520deliver%2520sufficient%2520tracking%2520for%2520non-experts%2520across%2520diverse%2520settings.%250A%2520%2520TrackStudio%2520was%2520developed%2520to%2520address%2520this%2520gap%2520by%2520combining%2520established%2520open-source%2520tools%2520into%2520a%2520single%252C%2520modular%252C%2520GUI-based%2520pipeline%2520that%2520works%2520out%2520of%2520the%2520box.%2520It%2520provides%2520automatic%25202D%2520and%25203D%2520tracking%252C%2520calibration%252C%2520preprocessing%252C%2520feature%2520extraction%252C%2520and%2520visualisation%2520without%2520requiring%2520any%2520programming%2520skills.%2520We%2520supply%2520a%2520user%2520guide%2520with%2520practical%2520advice%2520for%2520video%2520acquisition%252C%2520synchronisation%252C%2520and%2520setup%252C%2520alongside%2520documentation%2520of%2520common%2520pitfalls%2520and%2520how%2520to%2520avoid%2520them.%250A%2520%2520To%2520validate%2520the%2520toolkit%252C%2520we%2520tested%2520its%2520performance%2520across%2520three%2520environments%2520using%2520either%2520low-cost%2520webcams%2520or%2520high-resolution%2520cameras%252C%2520including%2520challenging%2520conditions%2520for%2520body%2520position%252C%2520lightning%252C%2520and%2520space%2520and%2520obstructions.%2520Across%252076%2520participants%252C%2520average%2520inter-frame%2520correlations%2520exceeded%25200.98%2520and%2520average%2520triangulation%2520errors%2520remained%2520low%2520%2528%253C13.6mm%2520for%2520hand%2520tracking%2529%252C%2520demonstrating%2520stable%2520and%2520consistent%2520tracking.%2520We%2520further%2520show%2520that%2520the%2520same%2520pipeline%2520can%2520be%2520extended%2520beyond%2520hand%2520tracking%2520to%2520other%2520body%2520and%2520face%2520regions.%2520TrackStudio%2520provides%2520a%2520practical%252C%2520accessible%2520route%2520into%2520markerless%2520tracking%2520for%2520researchers%2520or%2520laypeople%2520who%2520need%2520reliable%2520performance%2520without%2520specialist%2520expertise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackStudio%3A%20An%20Integrated%20Toolkit%20for%20Markerless%20Tracking&entry.906535625=Hristo%20Dimitrov%20and%20Giulia%20Dominijanni%20and%20Viktorija%20Pavalkyte%20and%20Tamar%20R.%20Makin&entry.1292438233=Markerless%20motion%20tracking%20has%20advanced%20rapidly%20in%20the%20past%2010%20years%20and%20currently%20offers%20powerful%20opportunities%20for%20behavioural%2C%20clinical%2C%20and%20biomechanical%20research.%20While%20several%20specialised%20toolkits%20provide%20high%20performance%20for%20specific%20tasks%2C%20using%20existing%20tools%20still%20requires%20substantial%20technical%20expertise.%20There%20remains%20a%20gap%20in%20accessible%2C%20integrated%20solutions%20that%20deliver%20sufficient%20tracking%20for%20non-experts%20across%20diverse%20settings.%0A%20%20TrackStudio%20was%20developed%20to%20address%20this%20gap%20by%20combining%20established%20open-source%20tools%20into%20a%20single%2C%20modular%2C%20GUI-based%20pipeline%20that%20works%20out%20of%20the%20box.%20It%20provides%20automatic%202D%20and%203D%20tracking%2C%20calibration%2C%20preprocessing%2C%20feature%20extraction%2C%20and%20visualisation%20without%20requiring%20any%20programming%20skills.%20We%20supply%20a%20user%20guide%20with%20practical%20advice%20for%20video%20acquisition%2C%20synchronisation%2C%20and%20setup%2C%20alongside%20documentation%20of%20common%20pitfalls%20and%20how%20to%20avoid%20them.%0A%20%20To%20validate%20the%20toolkit%2C%20we%20tested%20its%20performance%20across%20three%20environments%20using%20either%20low-cost%20webcams%20or%20high-resolution%20cameras%2C%20including%20challenging%20conditions%20for%20body%20position%2C%20lightning%2C%20and%20space%20and%20obstructions.%20Across%2076%20participants%2C%20average%20inter-frame%20correlations%20exceeded%200.98%20and%20average%20triangulation%20errors%20remained%20low%20%28%3C13.6mm%20for%20hand%20tracking%29%2C%20demonstrating%20stable%20and%20consistent%20tracking.%20We%20further%20show%20that%20the%20same%20pipeline%20can%20be%20extended%20beyond%20hand%20tracking%20to%20other%20body%20and%20face%20regions.%20TrackStudio%20provides%20a%20practical%2C%20accessible%20route%20into%20markerless%20tracking%20for%20researchers%20or%20laypeople%20who%20need%20reliable%20performance%20without%20specialist%20expertise.&entry.1838667208=http%3A//arxiv.org/abs/2511.07624v2&entry.124074799=Read"},
{"title": "Distributed Event-Based Learning via ADMM", "author": "Guner Dilsad Er and Sebastian Trimpe and Michael Muehlebach", "abstract": "We consider a distributed learning problem, where agents minimize a global objective function by exchanging information over a network. Our approach has two distinct features: (i) It substantially reduces communication by triggering communication only when necessary, and (ii) it is agnostic to the data-distribution among the different agents. We therefore guarantee convergence even if the local data-distributions of the agents are arbitrarily distinct. We analyze the convergence rate of the algorithm both in convex and nonconvex settings and derive accelerated convergence rates for the convex case. We also characterize the effect of communication failures and demonstrate that our algorithm is robust to these. The article concludes by presenting numerical results from distributed learning tasks on the MNIST and CIFAR-10 datasets. The experiments underline communication savings of 35% or more due to the event-based communication strategy, show resilience towards heterogeneous data-distributions, and highlight that our approach outperforms common baselines such as FedAvg, FedProx, SCAFFOLD and FedADMM.", "link": "http://arxiv.org/abs/2405.10618v3", "date": "2025-11-19", "relevancy": 2.0311, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5109}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Event-Based%20Learning%20via%20ADMM&body=Title%3A%20Distributed%20Event-Based%20Learning%20via%20ADMM%0AAuthor%3A%20Guner%20Dilsad%20Er%20and%20Sebastian%20Trimpe%20and%20Michael%20Muehlebach%0AAbstract%3A%20We%20consider%20a%20distributed%20learning%20problem%2C%20where%20agents%20minimize%20a%20global%20objective%20function%20by%20exchanging%20information%20over%20a%20network.%20Our%20approach%20has%20two%20distinct%20features%3A%20%28i%29%20It%20substantially%20reduces%20communication%20by%20triggering%20communication%20only%20when%20necessary%2C%20and%20%28ii%29%20it%20is%20agnostic%20to%20the%20data-distribution%20among%20the%20different%20agents.%20We%20therefore%20guarantee%20convergence%20even%20if%20the%20local%20data-distributions%20of%20the%20agents%20are%20arbitrarily%20distinct.%20We%20analyze%20the%20convergence%20rate%20of%20the%20algorithm%20both%20in%20convex%20and%20nonconvex%20settings%20and%20derive%20accelerated%20convergence%20rates%20for%20the%20convex%20case.%20We%20also%20characterize%20the%20effect%20of%20communication%20failures%20and%20demonstrate%20that%20our%20algorithm%20is%20robust%20to%20these.%20The%20article%20concludes%20by%20presenting%20numerical%20results%20from%20distributed%20learning%20tasks%20on%20the%20MNIST%20and%20CIFAR-10%20datasets.%20The%20experiments%20underline%20communication%20savings%20of%2035%25%20or%20more%20due%20to%20the%20event-based%20communication%20strategy%2C%20show%20resilience%20towards%20heterogeneous%20data-distributions%2C%20and%20highlight%20that%20our%20approach%20outperforms%20common%20baselines%20such%20as%20FedAvg%2C%20FedProx%2C%20SCAFFOLD%20and%20FedADMM.%0ALink%3A%20http%3A//arxiv.org/abs/2405.10618v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Event-Based%2520Learning%2520via%2520ADMM%26entry.906535625%3DGuner%2520Dilsad%2520Er%2520and%2520Sebastian%2520Trimpe%2520and%2520Michael%2520Muehlebach%26entry.1292438233%3DWe%2520consider%2520a%2520distributed%2520learning%2520problem%252C%2520where%2520agents%2520minimize%2520a%2520global%2520objective%2520function%2520by%2520exchanging%2520information%2520over%2520a%2520network.%2520Our%2520approach%2520has%2520two%2520distinct%2520features%253A%2520%2528i%2529%2520It%2520substantially%2520reduces%2520communication%2520by%2520triggering%2520communication%2520only%2520when%2520necessary%252C%2520and%2520%2528ii%2529%2520it%2520is%2520agnostic%2520to%2520the%2520data-distribution%2520among%2520the%2520different%2520agents.%2520We%2520therefore%2520guarantee%2520convergence%2520even%2520if%2520the%2520local%2520data-distributions%2520of%2520the%2520agents%2520are%2520arbitrarily%2520distinct.%2520We%2520analyze%2520the%2520convergence%2520rate%2520of%2520the%2520algorithm%2520both%2520in%2520convex%2520and%2520nonconvex%2520settings%2520and%2520derive%2520accelerated%2520convergence%2520rates%2520for%2520the%2520convex%2520case.%2520We%2520also%2520characterize%2520the%2520effect%2520of%2520communication%2520failures%2520and%2520demonstrate%2520that%2520our%2520algorithm%2520is%2520robust%2520to%2520these.%2520The%2520article%2520concludes%2520by%2520presenting%2520numerical%2520results%2520from%2520distributed%2520learning%2520tasks%2520on%2520the%2520MNIST%2520and%2520CIFAR-10%2520datasets.%2520The%2520experiments%2520underline%2520communication%2520savings%2520of%252035%2525%2520or%2520more%2520due%2520to%2520the%2520event-based%2520communication%2520strategy%252C%2520show%2520resilience%2520towards%2520heterogeneous%2520data-distributions%252C%2520and%2520highlight%2520that%2520our%2520approach%2520outperforms%2520common%2520baselines%2520such%2520as%2520FedAvg%252C%2520FedProx%252C%2520SCAFFOLD%2520and%2520FedADMM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10618v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Event-Based%20Learning%20via%20ADMM&entry.906535625=Guner%20Dilsad%20Er%20and%20Sebastian%20Trimpe%20and%20Michael%20Muehlebach&entry.1292438233=We%20consider%20a%20distributed%20learning%20problem%2C%20where%20agents%20minimize%20a%20global%20objective%20function%20by%20exchanging%20information%20over%20a%20network.%20Our%20approach%20has%20two%20distinct%20features%3A%20%28i%29%20It%20substantially%20reduces%20communication%20by%20triggering%20communication%20only%20when%20necessary%2C%20and%20%28ii%29%20it%20is%20agnostic%20to%20the%20data-distribution%20among%20the%20different%20agents.%20We%20therefore%20guarantee%20convergence%20even%20if%20the%20local%20data-distributions%20of%20the%20agents%20are%20arbitrarily%20distinct.%20We%20analyze%20the%20convergence%20rate%20of%20the%20algorithm%20both%20in%20convex%20and%20nonconvex%20settings%20and%20derive%20accelerated%20convergence%20rates%20for%20the%20convex%20case.%20We%20also%20characterize%20the%20effect%20of%20communication%20failures%20and%20demonstrate%20that%20our%20algorithm%20is%20robust%20to%20these.%20The%20article%20concludes%20by%20presenting%20numerical%20results%20from%20distributed%20learning%20tasks%20on%20the%20MNIST%20and%20CIFAR-10%20datasets.%20The%20experiments%20underline%20communication%20savings%20of%2035%25%20or%20more%20due%20to%20the%20event-based%20communication%20strategy%2C%20show%20resilience%20towards%20heterogeneous%20data-distributions%2C%20and%20highlight%20that%20our%20approach%20outperforms%20common%20baselines%20such%20as%20FedAvg%2C%20FedProx%2C%20SCAFFOLD%20and%20FedADMM.&entry.1838667208=http%3A//arxiv.org/abs/2405.10618v3&entry.124074799=Read"},
{"title": "Joint Semantic-Channel Coding and Modulation for Token Communications", "author": "Jingkai Ying and Zhijin Qin and Yulong Feng and Liejun Wang and Xiaoming Tao", "abstract": "In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.", "link": "http://arxiv.org/abs/2511.15699v1", "date": "2025-11-19", "relevancy": 2.0303, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5205}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5103}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Semantic-Channel%20Coding%20and%20Modulation%20for%20Token%20Communications&body=Title%3A%20Joint%20Semantic-Channel%20Coding%20and%20Modulation%20for%20Token%20Communications%0AAuthor%3A%20Jingkai%20Ying%20and%20Zhijin%20Qin%20and%20Yulong%20Feng%20and%20Liejun%20Wang%20and%20Xiaoming%20Tao%0AAbstract%3A%20In%20recent%20years%2C%20the%20Transformer%20architecture%20has%20achieved%20outstanding%20performance%20across%20a%20wide%20range%20of%20tasks%20and%20modalities.%20Token%20is%20the%20unified%20input%20and%20output%20representation%20in%20Transformer-based%20models%2C%20which%20has%20become%20a%20fundamental%20information%20unit.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20token%20communication%2C%20studying%20how%20to%20transmit%20tokens%20efficiently%20and%20reliably.%20Point%20cloud%2C%20a%20prevailing%20three-dimensional%20format%20which%20exhibits%20a%20more%20complex%20spatial%20structure%20compared%20to%20image%20or%20video%2C%20is%20chosen%20to%20be%20the%20information%20source.%20We%20utilize%20the%20set%20abstraction%20method%20to%20obtain%20point%20tokens.%20Subsequently%2C%20to%20get%20a%20more%20informative%20and%20transmission-friendly%20representation%20based%20on%20tokens%2C%20we%20propose%20a%20joint%20semantic-channel%20and%20modulation%20%28JSCCM%29%20scheme%20for%20the%20token%20encoder%2C%20mapping%20point%20tokens%20to%20standard%20digital%20constellation%20points%20%28modulated%20tokens%29.%20Specifically%2C%20the%20JSCCM%20consists%20of%20two%20parallel%20Point%20Transformer-based%20encoders%20and%20a%20differential%20modulator%20which%20combines%20the%20Gumel-softmax%20and%20soft%20quantization%20methods.%20Besides%2C%20the%20rate%20allocator%20and%20channel%20adapter%20are%20developed%2C%20facilitating%20adaptive%20generation%20of%20high-quality%20modulated%20tokens%20conditioned%20on%20both%20semantic%20information%20and%20channel%20conditions.%20Extensive%20simulations%20demonstrate%20that%20the%20proposed%20method%20outperforms%20both%20joint%20semantic-channel%20coding%20and%20traditional%20separate%20coding%2C%20achieving%20over%201dB%20gain%20in%20reconstruction%20and%20more%20than%206x%20compression%20ratio%20in%20modulated%20symbols.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Semantic-Channel%2520Coding%2520and%2520Modulation%2520for%2520Token%2520Communications%26entry.906535625%3DJingkai%2520Ying%2520and%2520Zhijin%2520Qin%2520and%2520Yulong%2520Feng%2520and%2520Liejun%2520Wang%2520and%2520Xiaoming%2520Tao%26entry.1292438233%3DIn%2520recent%2520years%252C%2520the%2520Transformer%2520architecture%2520has%2520achieved%2520outstanding%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520modalities.%2520Token%2520is%2520the%2520unified%2520input%2520and%2520output%2520representation%2520in%2520Transformer-based%2520models%252C%2520which%2520has%2520become%2520a%2520fundamental%2520information%2520unit.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520problem%2520of%2520token%2520communication%252C%2520studying%2520how%2520to%2520transmit%2520tokens%2520efficiently%2520and%2520reliably.%2520Point%2520cloud%252C%2520a%2520prevailing%2520three-dimensional%2520format%2520which%2520exhibits%2520a%2520more%2520complex%2520spatial%2520structure%2520compared%2520to%2520image%2520or%2520video%252C%2520is%2520chosen%2520to%2520be%2520the%2520information%2520source.%2520We%2520utilize%2520the%2520set%2520abstraction%2520method%2520to%2520obtain%2520point%2520tokens.%2520Subsequently%252C%2520to%2520get%2520a%2520more%2520informative%2520and%2520transmission-friendly%2520representation%2520based%2520on%2520tokens%252C%2520we%2520propose%2520a%2520joint%2520semantic-channel%2520and%2520modulation%2520%2528JSCCM%2529%2520scheme%2520for%2520the%2520token%2520encoder%252C%2520mapping%2520point%2520tokens%2520to%2520standard%2520digital%2520constellation%2520points%2520%2528modulated%2520tokens%2529.%2520Specifically%252C%2520the%2520JSCCM%2520consists%2520of%2520two%2520parallel%2520Point%2520Transformer-based%2520encoders%2520and%2520a%2520differential%2520modulator%2520which%2520combines%2520the%2520Gumel-softmax%2520and%2520soft%2520quantization%2520methods.%2520Besides%252C%2520the%2520rate%2520allocator%2520and%2520channel%2520adapter%2520are%2520developed%252C%2520facilitating%2520adaptive%2520generation%2520of%2520high-quality%2520modulated%2520tokens%2520conditioned%2520on%2520both%2520semantic%2520information%2520and%2520channel%2520conditions.%2520Extensive%2520simulations%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520both%2520joint%2520semantic-channel%2520coding%2520and%2520traditional%2520separate%2520coding%252C%2520achieving%2520over%25201dB%2520gain%2520in%2520reconstruction%2520and%2520more%2520than%25206x%2520compression%2520ratio%2520in%2520modulated%2520symbols.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Semantic-Channel%20Coding%20and%20Modulation%20for%20Token%20Communications&entry.906535625=Jingkai%20Ying%20and%20Zhijin%20Qin%20and%20Yulong%20Feng%20and%20Liejun%20Wang%20and%20Xiaoming%20Tao&entry.1292438233=In%20recent%20years%2C%20the%20Transformer%20architecture%20has%20achieved%20outstanding%20performance%20across%20a%20wide%20range%20of%20tasks%20and%20modalities.%20Token%20is%20the%20unified%20input%20and%20output%20representation%20in%20Transformer-based%20models%2C%20which%20has%20become%20a%20fundamental%20information%20unit.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20token%20communication%2C%20studying%20how%20to%20transmit%20tokens%20efficiently%20and%20reliably.%20Point%20cloud%2C%20a%20prevailing%20three-dimensional%20format%20which%20exhibits%20a%20more%20complex%20spatial%20structure%20compared%20to%20image%20or%20video%2C%20is%20chosen%20to%20be%20the%20information%20source.%20We%20utilize%20the%20set%20abstraction%20method%20to%20obtain%20point%20tokens.%20Subsequently%2C%20to%20get%20a%20more%20informative%20and%20transmission-friendly%20representation%20based%20on%20tokens%2C%20we%20propose%20a%20joint%20semantic-channel%20and%20modulation%20%28JSCCM%29%20scheme%20for%20the%20token%20encoder%2C%20mapping%20point%20tokens%20to%20standard%20digital%20constellation%20points%20%28modulated%20tokens%29.%20Specifically%2C%20the%20JSCCM%20consists%20of%20two%20parallel%20Point%20Transformer-based%20encoders%20and%20a%20differential%20modulator%20which%20combines%20the%20Gumel-softmax%20and%20soft%20quantization%20methods.%20Besides%2C%20the%20rate%20allocator%20and%20channel%20adapter%20are%20developed%2C%20facilitating%20adaptive%20generation%20of%20high-quality%20modulated%20tokens%20conditioned%20on%20both%20semantic%20information%20and%20channel%20conditions.%20Extensive%20simulations%20demonstrate%20that%20the%20proposed%20method%20outperforms%20both%20joint%20semantic-channel%20coding%20and%20traditional%20separate%20coding%2C%20achieving%20over%201dB%20gain%20in%20reconstruction%20and%20more%20than%206x%20compression%20ratio%20in%20modulated%20symbols.&entry.1838667208=http%3A//arxiv.org/abs/2511.15699v1&entry.124074799=Read"},
{"title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees", "author": "Scott Pesme and Giacomo Meanti and Michael Arbel and Julien Mairal", "abstract": "Denoiser models have become powerful tools for inverse problems, enabling the use of pretrained networks to approximate the score of a smoothed prior distribution. These models are often used in heuristic iterative schemes aimed at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal operator of the negative log-prior plays a central role. In practice, this operator is intractable, and practitioners plug in a pretrained denoiser as a surrogate-despite the lack of general theoretical justification for this substitution. In this work, we show that a simple algorithm, closely related to several used in practice, provably converges to the proximal operator under a log-concavity assumption on the prior $p$. We show that this algorithm can be interpreted as a gradient descent on smoothed proximal objectives. Our analysis thus provides a theoretical foundation for a class of empirically successful but previously heuristic methods.", "link": "http://arxiv.org/abs/2507.15397v3", "date": "2025-11-19", "relevancy": 2.0302, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5241}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4982}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees&body=Title%3A%20MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees%0AAuthor%3A%20Scott%20Pesme%20and%20Giacomo%20Meanti%20and%20Michael%20Arbel%20and%20Julien%20Mairal%0AAbstract%3A%20Denoiser%20models%20have%20become%20powerful%20tools%20for%20inverse%20problems%2C%20enabling%20the%20use%20of%20pretrained%20networks%20to%20approximate%20the%20score%20of%20a%20smoothed%20prior%20distribution.%20These%20models%20are%20often%20used%20in%20heuristic%20iterative%20schemes%20aimed%20at%20solving%20Maximum%20a%20Posteriori%20%28MAP%29%20optimisation%20problems%2C%20where%20the%20proximal%20operator%20of%20the%20negative%20log-prior%20plays%20a%20central%20role.%20In%20practice%2C%20this%20operator%20is%20intractable%2C%20and%20practitioners%20plug%20in%20a%20pretrained%20denoiser%20as%20a%20surrogate-despite%20the%20lack%20of%20general%20theoretical%20justification%20for%20this%20substitution.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20algorithm%2C%20closely%20related%20to%20several%20used%20in%20practice%2C%20provably%20converges%20to%20the%20proximal%20operator%20under%20a%20log-concavity%20assumption%20on%20the%20prior%20%24p%24.%20We%20show%20that%20this%20algorithm%20can%20be%20interpreted%20as%20a%20gradient%20descent%20on%20smoothed%20proximal%20objectives.%20Our%20analysis%20thus%20provides%20a%20theoretical%20foundation%20for%20a%20class%20of%20empirically%20successful%20but%20previously%20heuristic%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2507.15397v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP%2520Estimation%2520with%2520Denoisers%253A%2520Convergence%2520Rates%2520and%2520Guarantees%26entry.906535625%3DScott%2520Pesme%2520and%2520Giacomo%2520Meanti%2520and%2520Michael%2520Arbel%2520and%2520Julien%2520Mairal%26entry.1292438233%3DDenoiser%2520models%2520have%2520become%2520powerful%2520tools%2520for%2520inverse%2520problems%252C%2520enabling%2520the%2520use%2520of%2520pretrained%2520networks%2520to%2520approximate%2520the%2520score%2520of%2520a%2520smoothed%2520prior%2520distribution.%2520These%2520models%2520are%2520often%2520used%2520in%2520heuristic%2520iterative%2520schemes%2520aimed%2520at%2520solving%2520Maximum%2520a%2520Posteriori%2520%2528MAP%2529%2520optimisation%2520problems%252C%2520where%2520the%2520proximal%2520operator%2520of%2520the%2520negative%2520log-prior%2520plays%2520a%2520central%2520role.%2520In%2520practice%252C%2520this%2520operator%2520is%2520intractable%252C%2520and%2520practitioners%2520plug%2520in%2520a%2520pretrained%2520denoiser%2520as%2520a%2520surrogate-despite%2520the%2520lack%2520of%2520general%2520theoretical%2520justification%2520for%2520this%2520substitution.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520a%2520simple%2520algorithm%252C%2520closely%2520related%2520to%2520several%2520used%2520in%2520practice%252C%2520provably%2520converges%2520to%2520the%2520proximal%2520operator%2520under%2520a%2520log-concavity%2520assumption%2520on%2520the%2520prior%2520%2524p%2524.%2520We%2520show%2520that%2520this%2520algorithm%2520can%2520be%2520interpreted%2520as%2520a%2520gradient%2520descent%2520on%2520smoothed%2520proximal%2520objectives.%2520Our%2520analysis%2520thus%2520provides%2520a%2520theoretical%2520foundation%2520for%2520a%2520class%2520of%2520empirically%2520successful%2520but%2520previously%2520heuristic%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15397v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees&entry.906535625=Scott%20Pesme%20and%20Giacomo%20Meanti%20and%20Michael%20Arbel%20and%20Julien%20Mairal&entry.1292438233=Denoiser%20models%20have%20become%20powerful%20tools%20for%20inverse%20problems%2C%20enabling%20the%20use%20of%20pretrained%20networks%20to%20approximate%20the%20score%20of%20a%20smoothed%20prior%20distribution.%20These%20models%20are%20often%20used%20in%20heuristic%20iterative%20schemes%20aimed%20at%20solving%20Maximum%20a%20Posteriori%20%28MAP%29%20optimisation%20problems%2C%20where%20the%20proximal%20operator%20of%20the%20negative%20log-prior%20plays%20a%20central%20role.%20In%20practice%2C%20this%20operator%20is%20intractable%2C%20and%20practitioners%20plug%20in%20a%20pretrained%20denoiser%20as%20a%20surrogate-despite%20the%20lack%20of%20general%20theoretical%20justification%20for%20this%20substitution.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20algorithm%2C%20closely%20related%20to%20several%20used%20in%20practice%2C%20provably%20converges%20to%20the%20proximal%20operator%20under%20a%20log-concavity%20assumption%20on%20the%20prior%20%24p%24.%20We%20show%20that%20this%20algorithm%20can%20be%20interpreted%20as%20a%20gradient%20descent%20on%20smoothed%20proximal%20objectives.%20Our%20analysis%20thus%20provides%20a%20theoretical%20foundation%20for%20a%20class%20of%20empirically%20successful%20but%20previously%20heuristic%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2507.15397v3&entry.124074799=Read"},
{"title": "Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography", "author": "Shourov Joarder and Tushar Talukder Showrav and Md. Kamrul Hasan", "abstract": "Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.", "link": "http://arxiv.org/abs/2511.15640v1", "date": "2025-11-19", "relevancy": 2.0258, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5644}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Stage%20Residual-Aware%20Unsupervised%20Deep%20Learning%20Framework%20for%20Consistent%20Ultrasound%20Strain%20Elastography&body=Title%3A%20Multi-Stage%20Residual-Aware%20Unsupervised%20Deep%20Learning%20Framework%20for%20Consistent%20Ultrasound%20Strain%20Elastography%0AAuthor%3A%20Shourov%20Joarder%20and%20Tushar%20Talukder%20Showrav%20and%20Md.%20Kamrul%20Hasan%0AAbstract%3A%20Ultrasound%20Strain%20Elastography%20%28USE%29%20is%20a%20powerful%20non-invasive%20imaging%20technique%20for%20assessing%20tissue%20mechanical%20properties%2C%20offering%20crucial%20diagnostic%20value%20across%20diverse%20clinical%20applications.%20However%2C%20its%20clinical%20application%20remains%20limited%20by%20tissue%20decorrelation%20noise%2C%20scarcity%20of%20ground%20truth%2C%20and%20inconsistent%20strain%20estimation%20under%20different%20deformation%20conditions.%20Overcoming%20these%20barriers%2C%20we%20propose%20MUSSE-Net%2C%20a%20residual-aware%2C%20multi-stage%20unsupervised%20sequential%20deep%20learning%20framework%20designed%20for%20robust%20and%20consistent%20strain%20estimation.%20At%20its%20backbone%20lies%20our%20proposed%20USSE-Net%2C%20an%20end-to-end%20multi-stream%20encoder-decoder%20architecture%20that%20parallelly%20processes%20pre-%20and%20post-deformation%20RF%20sequences%20to%20estimate%20displacement%20fields%20and%20axial%20strains.%20The%20novel%20architecture%20incorporates%20Context-Aware%20Complementary%20Feature%20Fusion%20%28CACFF%29-based%20encoder%20with%20Tri-Cross%20Attention%20%28TCA%29%20bottleneck%20with%20a%20Cross-Attentive%20Fusion%20%28CAF%29-based%20sequential%20decoder.%20To%20ensure%20temporal%20coherence%20and%20strain%20stability%20across%20varying%20deformation%20levels%2C%20this%20architecture%20leverages%20a%20tailored%20consistency%20loss.%20Finally%2C%20with%20the%20MUSSE-Net%20framework%2C%20a%20secondary%20residual%20refinement%20stage%20further%20enhances%20accuracy%20and%20suppresses%20noise.%20Extensive%20validation%20on%20simulation%2C%20in%20vivo%2C%20and%20private%20clinical%20datasets%20from%20Bangladesh%20University%20of%20Engineering%20and%20Technology%20%28BUET%29%20medical%20center%2C%20demonstrates%20MUSSE-Net%27s%20outperformed%20existing%20unsupervised%20approaches.%20On%20MUSSE-Net%20achieves%20state-of-the-art%20performance%20with%20a%20target%20SNR%20of%2024.54%2C%20background%20SNR%20of%20132.76%2C%20CNR%20of%2059.81%2C%20and%20elastographic%20SNR%20of%209.73%20on%20simulation%20data.%20In%20particular%2C%20on%20the%20BUET%20dataset%2C%20MUSSE-Net%20produces%20strain%20maps%20with%20enhanced%20lesion-to-background%20contrast%20and%20significant%20noise%20suppression%20yielding%20clinically%20interpretable%20strain%20patterns.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Stage%2520Residual-Aware%2520Unsupervised%2520Deep%2520Learning%2520Framework%2520for%2520Consistent%2520Ultrasound%2520Strain%2520Elastography%26entry.906535625%3DShourov%2520Joarder%2520and%2520Tushar%2520Talukder%2520Showrav%2520and%2520Md.%2520Kamrul%2520Hasan%26entry.1292438233%3DUltrasound%2520Strain%2520Elastography%2520%2528USE%2529%2520is%2520a%2520powerful%2520non-invasive%2520imaging%2520technique%2520for%2520assessing%2520tissue%2520mechanical%2520properties%252C%2520offering%2520crucial%2520diagnostic%2520value%2520across%2520diverse%2520clinical%2520applications.%2520However%252C%2520its%2520clinical%2520application%2520remains%2520limited%2520by%2520tissue%2520decorrelation%2520noise%252C%2520scarcity%2520of%2520ground%2520truth%252C%2520and%2520inconsistent%2520strain%2520estimation%2520under%2520different%2520deformation%2520conditions.%2520Overcoming%2520these%2520barriers%252C%2520we%2520propose%2520MUSSE-Net%252C%2520a%2520residual-aware%252C%2520multi-stage%2520unsupervised%2520sequential%2520deep%2520learning%2520framework%2520designed%2520for%2520robust%2520and%2520consistent%2520strain%2520estimation.%2520At%2520its%2520backbone%2520lies%2520our%2520proposed%2520USSE-Net%252C%2520an%2520end-to-end%2520multi-stream%2520encoder-decoder%2520architecture%2520that%2520parallelly%2520processes%2520pre-%2520and%2520post-deformation%2520RF%2520sequences%2520to%2520estimate%2520displacement%2520fields%2520and%2520axial%2520strains.%2520The%2520novel%2520architecture%2520incorporates%2520Context-Aware%2520Complementary%2520Feature%2520Fusion%2520%2528CACFF%2529-based%2520encoder%2520with%2520Tri-Cross%2520Attention%2520%2528TCA%2529%2520bottleneck%2520with%2520a%2520Cross-Attentive%2520Fusion%2520%2528CAF%2529-based%2520sequential%2520decoder.%2520To%2520ensure%2520temporal%2520coherence%2520and%2520strain%2520stability%2520across%2520varying%2520deformation%2520levels%252C%2520this%2520architecture%2520leverages%2520a%2520tailored%2520consistency%2520loss.%2520Finally%252C%2520with%2520the%2520MUSSE-Net%2520framework%252C%2520a%2520secondary%2520residual%2520refinement%2520stage%2520further%2520enhances%2520accuracy%2520and%2520suppresses%2520noise.%2520Extensive%2520validation%2520on%2520simulation%252C%2520in%2520vivo%252C%2520and%2520private%2520clinical%2520datasets%2520from%2520Bangladesh%2520University%2520of%2520Engineering%2520and%2520Technology%2520%2528BUET%2529%2520medical%2520center%252C%2520demonstrates%2520MUSSE-Net%2527s%2520outperformed%2520existing%2520unsupervised%2520approaches.%2520On%2520MUSSE-Net%2520achieves%2520state-of-the-art%2520performance%2520with%2520a%2520target%2520SNR%2520of%252024.54%252C%2520background%2520SNR%2520of%2520132.76%252C%2520CNR%2520of%252059.81%252C%2520and%2520elastographic%2520SNR%2520of%25209.73%2520on%2520simulation%2520data.%2520In%2520particular%252C%2520on%2520the%2520BUET%2520dataset%252C%2520MUSSE-Net%2520produces%2520strain%2520maps%2520with%2520enhanced%2520lesion-to-background%2520contrast%2520and%2520significant%2520noise%2520suppression%2520yielding%2520clinically%2520interpretable%2520strain%2520patterns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Stage%20Residual-Aware%20Unsupervised%20Deep%20Learning%20Framework%20for%20Consistent%20Ultrasound%20Strain%20Elastography&entry.906535625=Shourov%20Joarder%20and%20Tushar%20Talukder%20Showrav%20and%20Md.%20Kamrul%20Hasan&entry.1292438233=Ultrasound%20Strain%20Elastography%20%28USE%29%20is%20a%20powerful%20non-invasive%20imaging%20technique%20for%20assessing%20tissue%20mechanical%20properties%2C%20offering%20crucial%20diagnostic%20value%20across%20diverse%20clinical%20applications.%20However%2C%20its%20clinical%20application%20remains%20limited%20by%20tissue%20decorrelation%20noise%2C%20scarcity%20of%20ground%20truth%2C%20and%20inconsistent%20strain%20estimation%20under%20different%20deformation%20conditions.%20Overcoming%20these%20barriers%2C%20we%20propose%20MUSSE-Net%2C%20a%20residual-aware%2C%20multi-stage%20unsupervised%20sequential%20deep%20learning%20framework%20designed%20for%20robust%20and%20consistent%20strain%20estimation.%20At%20its%20backbone%20lies%20our%20proposed%20USSE-Net%2C%20an%20end-to-end%20multi-stream%20encoder-decoder%20architecture%20that%20parallelly%20processes%20pre-%20and%20post-deformation%20RF%20sequences%20to%20estimate%20displacement%20fields%20and%20axial%20strains.%20The%20novel%20architecture%20incorporates%20Context-Aware%20Complementary%20Feature%20Fusion%20%28CACFF%29-based%20encoder%20with%20Tri-Cross%20Attention%20%28TCA%29%20bottleneck%20with%20a%20Cross-Attentive%20Fusion%20%28CAF%29-based%20sequential%20decoder.%20To%20ensure%20temporal%20coherence%20and%20strain%20stability%20across%20varying%20deformation%20levels%2C%20this%20architecture%20leverages%20a%20tailored%20consistency%20loss.%20Finally%2C%20with%20the%20MUSSE-Net%20framework%2C%20a%20secondary%20residual%20refinement%20stage%20further%20enhances%20accuracy%20and%20suppresses%20noise.%20Extensive%20validation%20on%20simulation%2C%20in%20vivo%2C%20and%20private%20clinical%20datasets%20from%20Bangladesh%20University%20of%20Engineering%20and%20Technology%20%28BUET%29%20medical%20center%2C%20demonstrates%20MUSSE-Net%27s%20outperformed%20existing%20unsupervised%20approaches.%20On%20MUSSE-Net%20achieves%20state-of-the-art%20performance%20with%20a%20target%20SNR%20of%2024.54%2C%20background%20SNR%20of%20132.76%2C%20CNR%20of%2059.81%2C%20and%20elastographic%20SNR%20of%209.73%20on%20simulation%20data.%20In%20particular%2C%20on%20the%20BUET%20dataset%2C%20MUSSE-Net%20produces%20strain%20maps%20with%20enhanced%20lesion-to-background%20contrast%20and%20significant%20noise%20suppression%20yielding%20clinically%20interpretable%20strain%20patterns.&entry.1838667208=http%3A//arxiv.org/abs/2511.15640v1&entry.124074799=Read"},
{"title": "Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents", "author": "Trevor McInroe", "abstract": "We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.", "link": "http://arxiv.org/abs/2511.15378v1", "date": "2025-11-19", "relevancy": 2.025, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5304}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5156}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Terra%20Nova%3A%20A%20Comprehensive%20Challenge%20Environment%20for%20Intelligent%20Agents&body=Title%3A%20Terra%20Nova%3A%20A%20Comprehensive%20Challenge%20Environment%20for%20Intelligent%20Agents%0AAuthor%3A%20Trevor%20McInroe%0AAbstract%3A%20We%20introduce%20Terra%20Nova%2C%20a%20new%20comprehensive%20challenge%20environment%20%28CCE%29%20for%20reinforcement%20learning%20%28RL%29%20research%20inspired%20by%20Civilization%20V.%20A%20CCE%20is%20a%20single%20environment%20in%20which%20multiple%20canonical%20RL%20challenges%20%28e.g.%2C%20partial%20observability%2C%20credit%20assignment%2C%20representation%20learning%2C%20enormous%20action%20spaces%2C%20etc.%29%20arise%20simultaneously.%20Mastery%20therefore%20demands%20integrated%2C%20long-horizon%20understanding%20across%20many%20interacting%20variables.%20We%20emphasize%20that%20this%20definition%20excludes%20challenges%20that%20only%20aggregate%20unrelated%20tasks%20in%20independent%2C%20parallel%20streams%20%28e.g.%2C%20learning%20to%20play%20all%20Atari%20games%20at%20once%29.%20These%20aggregated%20multitask%20benchmarks%20primarily%20asses%20whether%20an%20agent%20can%20catalog%20and%20switch%20among%20unrelated%20policies%20rather%20than%20test%20an%20agent%27s%20ability%20to%20perform%20deep%20reasoning%20across%20many%20interacting%20challenges.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerra%2520Nova%253A%2520A%2520Comprehensive%2520Challenge%2520Environment%2520for%2520Intelligent%2520Agents%26entry.906535625%3DTrevor%2520McInroe%26entry.1292438233%3DWe%2520introduce%2520Terra%2520Nova%252C%2520a%2520new%2520comprehensive%2520challenge%2520environment%2520%2528CCE%2529%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520research%2520inspired%2520by%2520Civilization%2520V.%2520A%2520CCE%2520is%2520a%2520single%2520environment%2520in%2520which%2520multiple%2520canonical%2520RL%2520challenges%2520%2528e.g.%252C%2520partial%2520observability%252C%2520credit%2520assignment%252C%2520representation%2520learning%252C%2520enormous%2520action%2520spaces%252C%2520etc.%2529%2520arise%2520simultaneously.%2520Mastery%2520therefore%2520demands%2520integrated%252C%2520long-horizon%2520understanding%2520across%2520many%2520interacting%2520variables.%2520We%2520emphasize%2520that%2520this%2520definition%2520excludes%2520challenges%2520that%2520only%2520aggregate%2520unrelated%2520tasks%2520in%2520independent%252C%2520parallel%2520streams%2520%2528e.g.%252C%2520learning%2520to%2520play%2520all%2520Atari%2520games%2520at%2520once%2529.%2520These%2520aggregated%2520multitask%2520benchmarks%2520primarily%2520asses%2520whether%2520an%2520agent%2520can%2520catalog%2520and%2520switch%2520among%2520unrelated%2520policies%2520rather%2520than%2520test%2520an%2520agent%2527s%2520ability%2520to%2520perform%2520deep%2520reasoning%2520across%2520many%2520interacting%2520challenges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Terra%20Nova%3A%20A%20Comprehensive%20Challenge%20Environment%20for%20Intelligent%20Agents&entry.906535625=Trevor%20McInroe&entry.1292438233=We%20introduce%20Terra%20Nova%2C%20a%20new%20comprehensive%20challenge%20environment%20%28CCE%29%20for%20reinforcement%20learning%20%28RL%29%20research%20inspired%20by%20Civilization%20V.%20A%20CCE%20is%20a%20single%20environment%20in%20which%20multiple%20canonical%20RL%20challenges%20%28e.g.%2C%20partial%20observability%2C%20credit%20assignment%2C%20representation%20learning%2C%20enormous%20action%20spaces%2C%20etc.%29%20arise%20simultaneously.%20Mastery%20therefore%20demands%20integrated%2C%20long-horizon%20understanding%20across%20many%20interacting%20variables.%20We%20emphasize%20that%20this%20definition%20excludes%20challenges%20that%20only%20aggregate%20unrelated%20tasks%20in%20independent%2C%20parallel%20streams%20%28e.g.%2C%20learning%20to%20play%20all%20Atari%20games%20at%20once%29.%20These%20aggregated%20multitask%20benchmarks%20primarily%20asses%20whether%20an%20agent%20can%20catalog%20and%20switch%20among%20unrelated%20policies%20rather%20than%20test%20an%20agent%27s%20ability%20to%20perform%20deep%20reasoning%20across%20many%20interacting%20challenges.&entry.1838667208=http%3A//arxiv.org/abs/2511.15378v1&entry.124074799=Read"},
{"title": "Hyperspectral Image Classification using Spectral-Spatial Mixer Network", "author": "Mohammed Q. Alkhatib", "abstract": "This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet", "link": "http://arxiv.org/abs/2511.15692v1", "date": "2025-11-19", "relevancy": 2.0215, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Image%20Classification%20using%20Spectral-Spatial%20Mixer%20Network&body=Title%3A%20Hyperspectral%20Image%20Classification%20using%20Spectral-Spatial%20Mixer%20Network%0AAuthor%3A%20Mohammed%20Q.%20Alkhatib%0AAbstract%3A%20This%20paper%20introduces%20SS-MixNet%2C%20a%20lightweight%20and%20effective%20deep%20learning%20model%20for%20hyperspectral%20image%20%28HSI%29%20classification.%20The%20architecture%20integrates%203D%20convolutional%20layers%20for%20local%20spectral-spatial%20feature%20extraction%20with%20two%20parallel%20MLP-style%20mixer%20blocks%20that%20capture%20long-range%20dependencies%20in%20spectral%20and%20spatial%20dimensions.%20A%20depthwise%20convolution-based%20attention%20mechanism%20is%20employed%20to%20enhance%20discriminative%20capability%20with%20minimal%20computational%20overhead.%20The%20model%20is%20evaluated%20on%20the%20QUH-Tangdaowan%20and%20QUH-Qingyun%20datasets%20using%20only%201%25%20of%20labeled%20data%20for%20training%20and%20validation.%20SS-MixNet%20achieves%20the%20highest%20performance%20among%20compared%20methods%2C%20including%202D-CNN%2C%203D-CNN%2C%20IP-SWIN%2C%20SimPoolFormer%2C%20and%20HybridKAN%2C%20reaching%2095.68%25%20and%2093.86%25%20overall%20accuracy%20on%20the%20Tangdaowan%20and%20Qingyun%20datasets%2C%20respectively.%20The%20results%2C%20supported%20by%20quantitative%20metrics%20and%20classification%20maps%2C%20confirm%20the%20model%27s%20effectiveness%20in%20delivering%20accurate%20and%20robust%20predictions%20with%20limited%20supervision.%20The%20code%20will%20be%20made%20publicly%20available%20at%3A%20https%3A//github.com/mqalkhatib/SS-MixNet%0ALink%3A%20http%3A//arxiv.org/abs/2511.15692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Image%2520Classification%2520using%2520Spectral-Spatial%2520Mixer%2520Network%26entry.906535625%3DMohammed%2520Q.%2520Alkhatib%26entry.1292438233%3DThis%2520paper%2520introduces%2520SS-MixNet%252C%2520a%2520lightweight%2520and%2520effective%2520deep%2520learning%2520model%2520for%2520hyperspectral%2520image%2520%2528HSI%2529%2520classification.%2520The%2520architecture%2520integrates%25203D%2520convolutional%2520layers%2520for%2520local%2520spectral-spatial%2520feature%2520extraction%2520with%2520two%2520parallel%2520MLP-style%2520mixer%2520blocks%2520that%2520capture%2520long-range%2520dependencies%2520in%2520spectral%2520and%2520spatial%2520dimensions.%2520A%2520depthwise%2520convolution-based%2520attention%2520mechanism%2520is%2520employed%2520to%2520enhance%2520discriminative%2520capability%2520with%2520minimal%2520computational%2520overhead.%2520The%2520model%2520is%2520evaluated%2520on%2520the%2520QUH-Tangdaowan%2520and%2520QUH-Qingyun%2520datasets%2520using%2520only%25201%2525%2520of%2520labeled%2520data%2520for%2520training%2520and%2520validation.%2520SS-MixNet%2520achieves%2520the%2520highest%2520performance%2520among%2520compared%2520methods%252C%2520including%25202D-CNN%252C%25203D-CNN%252C%2520IP-SWIN%252C%2520SimPoolFormer%252C%2520and%2520HybridKAN%252C%2520reaching%252095.68%2525%2520and%252093.86%2525%2520overall%2520accuracy%2520on%2520the%2520Tangdaowan%2520and%2520Qingyun%2520datasets%252C%2520respectively.%2520The%2520results%252C%2520supported%2520by%2520quantitative%2520metrics%2520and%2520classification%2520maps%252C%2520confirm%2520the%2520model%2527s%2520effectiveness%2520in%2520delivering%2520accurate%2520and%2520robust%2520predictions%2520with%2520limited%2520supervision.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%253A%2520https%253A//github.com/mqalkhatib/SS-MixNet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Image%20Classification%20using%20Spectral-Spatial%20Mixer%20Network&entry.906535625=Mohammed%20Q.%20Alkhatib&entry.1292438233=This%20paper%20introduces%20SS-MixNet%2C%20a%20lightweight%20and%20effective%20deep%20learning%20model%20for%20hyperspectral%20image%20%28HSI%29%20classification.%20The%20architecture%20integrates%203D%20convolutional%20layers%20for%20local%20spectral-spatial%20feature%20extraction%20with%20two%20parallel%20MLP-style%20mixer%20blocks%20that%20capture%20long-range%20dependencies%20in%20spectral%20and%20spatial%20dimensions.%20A%20depthwise%20convolution-based%20attention%20mechanism%20is%20employed%20to%20enhance%20discriminative%20capability%20with%20minimal%20computational%20overhead.%20The%20model%20is%20evaluated%20on%20the%20QUH-Tangdaowan%20and%20QUH-Qingyun%20datasets%20using%20only%201%25%20of%20labeled%20data%20for%20training%20and%20validation.%20SS-MixNet%20achieves%20the%20highest%20performance%20among%20compared%20methods%2C%20including%202D-CNN%2C%203D-CNN%2C%20IP-SWIN%2C%20SimPoolFormer%2C%20and%20HybridKAN%2C%20reaching%2095.68%25%20and%2093.86%25%20overall%20accuracy%20on%20the%20Tangdaowan%20and%20Qingyun%20datasets%2C%20respectively.%20The%20results%2C%20supported%20by%20quantitative%20metrics%20and%20classification%20maps%2C%20confirm%20the%20model%27s%20effectiveness%20in%20delivering%20accurate%20and%20robust%20predictions%20with%20limited%20supervision.%20The%20code%20will%20be%20made%20publicly%20available%20at%3A%20https%3A//github.com/mqalkhatib/SS-MixNet&entry.1838667208=http%3A//arxiv.org/abs/2511.15692v1&entry.124074799=Read"},
{"title": "MessIRve: A Large-Scale Spanish Information Retrieval Dataset", "author": "Francisco Valentini and Viviana Cotik and Dami\u00e1n Furman and Ivan Bercovich and Edgar Altszyler and Juan Manuel P\u00e9rez", "abstract": "Information retrieval (IR) is the task of finding relevant documents in response to a user query. Although Spanish is the second most spoken native language, there are few Spanish IR datasets, which limits the development of information access tools for Spanish speakers. We introduce MessIRve, a large-scale Spanish IR dataset with almost 700,000 queries from Google's autocomplete API and relevant documents sourced from Wikipedia. MessIRve's queries reflect diverse Spanish-speaking regions, unlike other datasets that are translated from English or do not consider dialectal variations. The large size of the dataset allows it to cover a wide variety of topics, unlike smaller datasets. We provide a comprehensive description of the dataset, comparisons with existing datasets, and baseline evaluations of prominent IR models. Our contributions aim to advance Spanish IR research and improve information access for Spanish speakers.", "link": "http://arxiv.org/abs/2409.05994v2", "date": "2025-11-19", "relevancy": 1.56, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MessIRve%3A%20A%20Large-Scale%20Spanish%20Information%20Retrieval%20Dataset&body=Title%3A%20MessIRve%3A%20A%20Large-Scale%20Spanish%20Information%20Retrieval%20Dataset%0AAuthor%3A%20Francisco%20Valentini%20and%20Viviana%20Cotik%20and%20Dami%C3%A1n%20Furman%20and%20Ivan%20Bercovich%20and%20Edgar%20Altszyler%20and%20Juan%20Manuel%20P%C3%A9rez%0AAbstract%3A%20Information%20retrieval%20%28IR%29%20is%20the%20task%20of%20finding%20relevant%20documents%20in%20response%20to%20a%20user%20query.%20Although%20Spanish%20is%20the%20second%20most%20spoken%20native%20language%2C%20there%20are%20few%20Spanish%20IR%20datasets%2C%20which%20limits%20the%20development%20of%20information%20access%20tools%20for%20Spanish%20speakers.%20We%20introduce%20MessIRve%2C%20a%20large-scale%20Spanish%20IR%20dataset%20with%20almost%20700%2C000%20queries%20from%20Google%27s%20autocomplete%20API%20and%20relevant%20documents%20sourced%20from%20Wikipedia.%20MessIRve%27s%20queries%20reflect%20diverse%20Spanish-speaking%20regions%2C%20unlike%20other%20datasets%20that%20are%20translated%20from%20English%20or%20do%20not%20consider%20dialectal%20variations.%20The%20large%20size%20of%20the%20dataset%20allows%20it%20to%20cover%20a%20wide%20variety%20of%20topics%2C%20unlike%20smaller%20datasets.%20We%20provide%20a%20comprehensive%20description%20of%20the%20dataset%2C%20comparisons%20with%20existing%20datasets%2C%20and%20baseline%20evaluations%20of%20prominent%20IR%20models.%20Our%20contributions%20aim%20to%20advance%20Spanish%20IR%20research%20and%20improve%20information%20access%20for%20Spanish%20speakers.%0ALink%3A%20http%3A//arxiv.org/abs/2409.05994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMessIRve%253A%2520A%2520Large-Scale%2520Spanish%2520Information%2520Retrieval%2520Dataset%26entry.906535625%3DFrancisco%2520Valentini%2520and%2520Viviana%2520Cotik%2520and%2520Dami%25C3%25A1n%2520Furman%2520and%2520Ivan%2520Bercovich%2520and%2520Edgar%2520Altszyler%2520and%2520Juan%2520Manuel%2520P%25C3%25A9rez%26entry.1292438233%3DInformation%2520retrieval%2520%2528IR%2529%2520is%2520the%2520task%2520of%2520finding%2520relevant%2520documents%2520in%2520response%2520to%2520a%2520user%2520query.%2520Although%2520Spanish%2520is%2520the%2520second%2520most%2520spoken%2520native%2520language%252C%2520there%2520are%2520few%2520Spanish%2520IR%2520datasets%252C%2520which%2520limits%2520the%2520development%2520of%2520information%2520access%2520tools%2520for%2520Spanish%2520speakers.%2520We%2520introduce%2520MessIRve%252C%2520a%2520large-scale%2520Spanish%2520IR%2520dataset%2520with%2520almost%2520700%252C000%2520queries%2520from%2520Google%2527s%2520autocomplete%2520API%2520and%2520relevant%2520documents%2520sourced%2520from%2520Wikipedia.%2520MessIRve%2527s%2520queries%2520reflect%2520diverse%2520Spanish-speaking%2520regions%252C%2520unlike%2520other%2520datasets%2520that%2520are%2520translated%2520from%2520English%2520or%2520do%2520not%2520consider%2520dialectal%2520variations.%2520The%2520large%2520size%2520of%2520the%2520dataset%2520allows%2520it%2520to%2520cover%2520a%2520wide%2520variety%2520of%2520topics%252C%2520unlike%2520smaller%2520datasets.%2520We%2520provide%2520a%2520comprehensive%2520description%2520of%2520the%2520dataset%252C%2520comparisons%2520with%2520existing%2520datasets%252C%2520and%2520baseline%2520evaluations%2520of%2520prominent%2520IR%2520models.%2520Our%2520contributions%2520aim%2520to%2520advance%2520Spanish%2520IR%2520research%2520and%2520improve%2520information%2520access%2520for%2520Spanish%2520speakers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MessIRve%3A%20A%20Large-Scale%20Spanish%20Information%20Retrieval%20Dataset&entry.906535625=Francisco%20Valentini%20and%20Viviana%20Cotik%20and%20Dami%C3%A1n%20Furman%20and%20Ivan%20Bercovich%20and%20Edgar%20Altszyler%20and%20Juan%20Manuel%20P%C3%A9rez&entry.1292438233=Information%20retrieval%20%28IR%29%20is%20the%20task%20of%20finding%20relevant%20documents%20in%20response%20to%20a%20user%20query.%20Although%20Spanish%20is%20the%20second%20most%20spoken%20native%20language%2C%20there%20are%20few%20Spanish%20IR%20datasets%2C%20which%20limits%20the%20development%20of%20information%20access%20tools%20for%20Spanish%20speakers.%20We%20introduce%20MessIRve%2C%20a%20large-scale%20Spanish%20IR%20dataset%20with%20almost%20700%2C000%20queries%20from%20Google%27s%20autocomplete%20API%20and%20relevant%20documents%20sourced%20from%20Wikipedia.%20MessIRve%27s%20queries%20reflect%20diverse%20Spanish-speaking%20regions%2C%20unlike%20other%20datasets%20that%20are%20translated%20from%20English%20or%20do%20not%20consider%20dialectal%20variations.%20The%20large%20size%20of%20the%20dataset%20allows%20it%20to%20cover%20a%20wide%20variety%20of%20topics%2C%20unlike%20smaller%20datasets.%20We%20provide%20a%20comprehensive%20description%20of%20the%20dataset%2C%20comparisons%20with%20existing%20datasets%2C%20and%20baseline%20evaluations%20of%20prominent%20IR%20models.%20Our%20contributions%20aim%20to%20advance%20Spanish%20IR%20research%20and%20improve%20information%20access%20for%20Spanish%20speakers.&entry.1838667208=http%3A//arxiv.org/abs/2409.05994v2&entry.124074799=Read"},
{"title": "Tokenisation over Bounded Alphabets is Hard", "author": "Violeta Kastreva and Philip Whittington and Dennis Komm and Tiago Pimentel", "abstract": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.", "link": "http://arxiv.org/abs/2511.15709v1", "date": "2025-11-19", "relevancy": 1.6009, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4512}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3723}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenisation%20over%20Bounded%20Alphabets%20is%20Hard&body=Title%3A%20Tokenisation%20over%20Bounded%20Alphabets%20is%20Hard%0AAuthor%3A%20Violeta%20Kastreva%20and%20Philip%20Whittington%20and%20Dennis%20Komm%20and%20Tiago%20Pimentel%0AAbstract%3A%20Recent%20works%20have%20shown%20that%20tokenisation%20is%20NP-complete.%20However%2C%20these%20works%20assume%20tokenisation%20is%20applied%20to%20inputs%20with%20unboundedly%20large%20alphabets%20--%20an%20unrealistic%20assumption%2C%20given%20that%20in%20practice%20tokenisers%20operate%20over%20fixed-size%20alphabets%2C%20such%20as%20bytes%20or%20Unicode%20characters.%20We%20close%20this%20gap%20by%20analysing%20tokenisation%20over%20bounded%20%24n%24-ary%20alphabets%2C%20considering%20two%20natural%20variants%3A%20bottom-up%20tokenisation%20and%20direct%20tokenisation%2C%20where%20we%20must%2C%20respectively%2C%20select%20a%20sequence%20of%20merge%20operations%20or%20a%20vocabulary%20whose%20application%20optimally%20compresses%20a%20dataset.%20First%2C%20we%20note%20that%20proving%20hardness%20results%20for%20an%20%24n%24-ary%20alphabet%20proves%20the%20same%20results%20for%20alphabets%20of%20any%20larger%20size.%20We%20then%20prove%20that%20even%20with%20binary%20alphabets%2C%20both%20variants%20are%20not%20only%20NP-complete%2C%20but%20admit%20no%20polynomial-time%20approximation%20scheme%20%28unless%20P%3DNP%29.%20We%20further%20show%20that%20direct%20tokenisation%20remains%20NP-complete%20even%20when%20applied%20to%20unary%20alphabets.%20While%20unary%20alphabets%20may%20not%20be%20practically%20useful%2C%20this%20result%20establishes%20that%20the%20computational%20intractability%20of%20tokenisation%20is%20not%20an%20artifact%20of%20large%20alphabets%20or%20complex%20constructions%2C%20but%20a%20fundamental%20barrier.%20Overall%2C%20our%20results%20explain%20why%20practical%20algorithms%20such%20as%20BPE%20and%20UnigramLM%20are%20heuristic%2C%20and%20points%20toward%20approximation%20algorithms%20being%20an%20important%20path%20going%20forward%20for%20tokenisation%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenisation%2520over%2520Bounded%2520Alphabets%2520is%2520Hard%26entry.906535625%3DVioleta%2520Kastreva%2520and%2520Philip%2520Whittington%2520and%2520Dennis%2520Komm%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3DRecent%2520works%2520have%2520shown%2520that%2520tokenisation%2520is%2520NP-complete.%2520However%252C%2520these%2520works%2520assume%2520tokenisation%2520is%2520applied%2520to%2520inputs%2520with%2520unboundedly%2520large%2520alphabets%2520--%2520an%2520unrealistic%2520assumption%252C%2520given%2520that%2520in%2520practice%2520tokenisers%2520operate%2520over%2520fixed-size%2520alphabets%252C%2520such%2520as%2520bytes%2520or%2520Unicode%2520characters.%2520We%2520close%2520this%2520gap%2520by%2520analysing%2520tokenisation%2520over%2520bounded%2520%2524n%2524-ary%2520alphabets%252C%2520considering%2520two%2520natural%2520variants%253A%2520bottom-up%2520tokenisation%2520and%2520direct%2520tokenisation%252C%2520where%2520we%2520must%252C%2520respectively%252C%2520select%2520a%2520sequence%2520of%2520merge%2520operations%2520or%2520a%2520vocabulary%2520whose%2520application%2520optimally%2520compresses%2520a%2520dataset.%2520First%252C%2520we%2520note%2520that%2520proving%2520hardness%2520results%2520for%2520an%2520%2524n%2524-ary%2520alphabet%2520proves%2520the%2520same%2520results%2520for%2520alphabets%2520of%2520any%2520larger%2520size.%2520We%2520then%2520prove%2520that%2520even%2520with%2520binary%2520alphabets%252C%2520both%2520variants%2520are%2520not%2520only%2520NP-complete%252C%2520but%2520admit%2520no%2520polynomial-time%2520approximation%2520scheme%2520%2528unless%2520P%253DNP%2529.%2520We%2520further%2520show%2520that%2520direct%2520tokenisation%2520remains%2520NP-complete%2520even%2520when%2520applied%2520to%2520unary%2520alphabets.%2520While%2520unary%2520alphabets%2520may%2520not%2520be%2520practically%2520useful%252C%2520this%2520result%2520establishes%2520that%2520the%2520computational%2520intractability%2520of%2520tokenisation%2520is%2520not%2520an%2520artifact%2520of%2520large%2520alphabets%2520or%2520complex%2520constructions%252C%2520but%2520a%2520fundamental%2520barrier.%2520Overall%252C%2520our%2520results%2520explain%2520why%2520practical%2520algorithms%2520such%2520as%2520BPE%2520and%2520UnigramLM%2520are%2520heuristic%252C%2520and%2520points%2520toward%2520approximation%2520algorithms%2520being%2520an%2520important%2520path%2520going%2520forward%2520for%2520tokenisation%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenisation%20over%20Bounded%20Alphabets%20is%20Hard&entry.906535625=Violeta%20Kastreva%20and%20Philip%20Whittington%20and%20Dennis%20Komm%20and%20Tiago%20Pimentel&entry.1292438233=Recent%20works%20have%20shown%20that%20tokenisation%20is%20NP-complete.%20However%2C%20these%20works%20assume%20tokenisation%20is%20applied%20to%20inputs%20with%20unboundedly%20large%20alphabets%20--%20an%20unrealistic%20assumption%2C%20given%20that%20in%20practice%20tokenisers%20operate%20over%20fixed-size%20alphabets%2C%20such%20as%20bytes%20or%20Unicode%20characters.%20We%20close%20this%20gap%20by%20analysing%20tokenisation%20over%20bounded%20%24n%24-ary%20alphabets%2C%20considering%20two%20natural%20variants%3A%20bottom-up%20tokenisation%20and%20direct%20tokenisation%2C%20where%20we%20must%2C%20respectively%2C%20select%20a%20sequence%20of%20merge%20operations%20or%20a%20vocabulary%20whose%20application%20optimally%20compresses%20a%20dataset.%20First%2C%20we%20note%20that%20proving%20hardness%20results%20for%20an%20%24n%24-ary%20alphabet%20proves%20the%20same%20results%20for%20alphabets%20of%20any%20larger%20size.%20We%20then%20prove%20that%20even%20with%20binary%20alphabets%2C%20both%20variants%20are%20not%20only%20NP-complete%2C%20but%20admit%20no%20polynomial-time%20approximation%20scheme%20%28unless%20P%3DNP%29.%20We%20further%20show%20that%20direct%20tokenisation%20remains%20NP-complete%20even%20when%20applied%20to%20unary%20alphabets.%20While%20unary%20alphabets%20may%20not%20be%20practically%20useful%2C%20this%20result%20establishes%20that%20the%20computational%20intractability%20of%20tokenisation%20is%20not%20an%20artifact%20of%20large%20alphabets%20or%20complex%20constructions%2C%20but%20a%20fundamental%20barrier.%20Overall%2C%20our%20results%20explain%20why%20practical%20algorithms%20such%20as%20BPE%20and%20UnigramLM%20are%20heuristic%2C%20and%20points%20toward%20approximation%20algorithms%20being%20an%20important%20path%20going%20forward%20for%20tokenisation%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.15709v1&entry.124074799=Read"},
{"title": "PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles", "author": "Yinan Yu and Samuel Scheidegger", "abstract": "Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.", "link": "http://arxiv.org/abs/2511.15522v1", "date": "2025-11-19", "relevancy": 1.998, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4997}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCARNN-DCBF%3A%20Minimal-Intervention%20Geofence%20Enforcement%20for%20Ground%20Vehicles&body=Title%3A%20PCARNN-DCBF%3A%20Minimal-Intervention%20Geofence%20Enforcement%20for%20Ground%20Vehicles%0AAuthor%3A%20Yinan%20Yu%20and%20Samuel%20Scheidegger%0AAbstract%3A%20Runtime%20geofencing%20for%20ground%20vehicles%20is%20rapidly%20emerging%20as%20a%20critical%20technology%20for%20enforcing%20Operational%20Design%20Domains%20%28ODDs%29.%20However%2C%20existing%20solutions%20struggle%20to%20reconcile%20high-fidelity%20learning%20with%20the%20structural%20requirements%20of%20verifiable%20control.%20We%20address%20this%20by%20introducing%20PCARNN-DCBF%2C%20a%20novel%20pipeline%20integrating%20a%20Physics-encoded%20Control-Affine%20Residual%20Neural%20Network%20with%20a%20preview-based%20Discrete%20Control%20Barrier%20Function.%20Unlike%20generic%20learned%20models%2C%20PCARNN%20explicitly%20preserves%20the%20control-affine%20structure%20of%20vehicle%20dynamics%2C%20ensuring%20the%20linearity%20required%20for%20reliable%20optimization.%20This%20enables%20the%20DCBF%20to%20enforce%20polygonal%20keep-in%20constraints%20via%20a%20real-time%20Quadratic%20Program%20%28QP%29%20that%20handles%20high%20relative%20degree%20and%20mitigates%20actuator%20saturation.%20Experiments%20in%20CARLA%20across%20electric%20and%20combustion%20platforms%20demonstrate%20that%20this%20structure-preserving%20approach%20significantly%20outperforms%20analytical%20and%20unstructured%20neural%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCARNN-DCBF%253A%2520Minimal-Intervention%2520Geofence%2520Enforcement%2520for%2520Ground%2520Vehicles%26entry.906535625%3DYinan%2520Yu%2520and%2520Samuel%2520Scheidegger%26entry.1292438233%3DRuntime%2520geofencing%2520for%2520ground%2520vehicles%2520is%2520rapidly%2520emerging%2520as%2520a%2520critical%2520technology%2520for%2520enforcing%2520Operational%2520Design%2520Domains%2520%2528ODDs%2529.%2520However%252C%2520existing%2520solutions%2520struggle%2520to%2520reconcile%2520high-fidelity%2520learning%2520with%2520the%2520structural%2520requirements%2520of%2520verifiable%2520control.%2520We%2520address%2520this%2520by%2520introducing%2520PCARNN-DCBF%252C%2520a%2520novel%2520pipeline%2520integrating%2520a%2520Physics-encoded%2520Control-Affine%2520Residual%2520Neural%2520Network%2520with%2520a%2520preview-based%2520Discrete%2520Control%2520Barrier%2520Function.%2520Unlike%2520generic%2520learned%2520models%252C%2520PCARNN%2520explicitly%2520preserves%2520the%2520control-affine%2520structure%2520of%2520vehicle%2520dynamics%252C%2520ensuring%2520the%2520linearity%2520required%2520for%2520reliable%2520optimization.%2520This%2520enables%2520the%2520DCBF%2520to%2520enforce%2520polygonal%2520keep-in%2520constraints%2520via%2520a%2520real-time%2520Quadratic%2520Program%2520%2528QP%2529%2520that%2520handles%2520high%2520relative%2520degree%2520and%2520mitigates%2520actuator%2520saturation.%2520Experiments%2520in%2520CARLA%2520across%2520electric%2520and%2520combustion%2520platforms%2520demonstrate%2520that%2520this%2520structure-preserving%2520approach%2520significantly%2520outperforms%2520analytical%2520and%2520unstructured%2520neural%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCARNN-DCBF%3A%20Minimal-Intervention%20Geofence%20Enforcement%20for%20Ground%20Vehicles&entry.906535625=Yinan%20Yu%20and%20Samuel%20Scheidegger&entry.1292438233=Runtime%20geofencing%20for%20ground%20vehicles%20is%20rapidly%20emerging%20as%20a%20critical%20technology%20for%20enforcing%20Operational%20Design%20Domains%20%28ODDs%29.%20However%2C%20existing%20solutions%20struggle%20to%20reconcile%20high-fidelity%20learning%20with%20the%20structural%20requirements%20of%20verifiable%20control.%20We%20address%20this%20by%20introducing%20PCARNN-DCBF%2C%20a%20novel%20pipeline%20integrating%20a%20Physics-encoded%20Control-Affine%20Residual%20Neural%20Network%20with%20a%20preview-based%20Discrete%20Control%20Barrier%20Function.%20Unlike%20generic%20learned%20models%2C%20PCARNN%20explicitly%20preserves%20the%20control-affine%20structure%20of%20vehicle%20dynamics%2C%20ensuring%20the%20linearity%20required%20for%20reliable%20optimization.%20This%20enables%20the%20DCBF%20to%20enforce%20polygonal%20keep-in%20constraints%20via%20a%20real-time%20Quadratic%20Program%20%28QP%29%20that%20handles%20high%20relative%20degree%20and%20mitigates%20actuator%20saturation.%20Experiments%20in%20CARLA%20across%20electric%20and%20combustion%20platforms%20demonstrate%20that%20this%20structure-preserving%20approach%20significantly%20outperforms%20analytical%20and%20unstructured%20neural%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.15522v1&entry.124074799=Read"},
{"title": "Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis", "author": "Aran Nayebi", "abstract": "We formalize AI alignment as a multi-objective optimization problem called $\\langle M,N,\\varepsilon,\u03b4\\rangle$-agreement, in which a set of $N$ agents (including humans) must reach approximate ($\\varepsilon$) agreement across $M$ candidate objectives, with probability at least $1-\u03b4$. Analyzing communication complexity, we prove an information-theoretic lower bound showing that once either $M$ or $N$ is large enough, no amount of computational power or rationality can avoid intrinsic alignment overheads. This establishes rigorous limits to alignment *itself*, not merely to particular methods, clarifying a \"No-Free-Lunch\" principle: encoding \"all human values\" is inherently intractable and must be managed through consensus-driven reduction or prioritization of objectives. Complementing this impossibility result, we construct explicit algorithms as achievability certificates for alignment under both unbounded and bounded rationality with noisy communication. Even in these best-case regimes, our bounded-agent and sampling analysis shows that with large task spaces ($D$) and finite samples, *reward hacking is globally inevitable*: rare high-loss states are systematically under-covered, implying scalable oversight must target safety-critical slices rather than uniform coverage. Together, these results identify fundamental complexity barriers -- tasks ($M$), agents ($N$), and state-space size ($D$) -- and offer principles for more scalable human-AI collaboration.", "link": "http://arxiv.org/abs/2502.05934v3", "date": "2025-11-19", "relevancy": 1.3572, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Barriers%20and%20Practical%20Pathways%20for%20Human-AI%20Alignment%3A%20An%20Agreement-Based%20Complexity%20Analysis&body=Title%3A%20Intrinsic%20Barriers%20and%20Practical%20Pathways%20for%20Human-AI%20Alignment%3A%20An%20Agreement-Based%20Complexity%20Analysis%0AAuthor%3A%20Aran%20Nayebi%0AAbstract%3A%20We%20formalize%20AI%20alignment%20as%20a%20multi-objective%20optimization%20problem%20called%20%24%5Clangle%20M%2CN%2C%5Cvarepsilon%2C%CE%B4%5Crangle%24-agreement%2C%20in%20which%20a%20set%20of%20%24N%24%20agents%20%28including%20humans%29%20must%20reach%20approximate%20%28%24%5Cvarepsilon%24%29%20agreement%20across%20%24M%24%20candidate%20objectives%2C%20with%20probability%20at%20least%20%241-%CE%B4%24.%20Analyzing%20communication%20complexity%2C%20we%20prove%20an%20information-theoretic%20lower%20bound%20showing%20that%20once%20either%20%24M%24%20or%20%24N%24%20is%20large%20enough%2C%20no%20amount%20of%20computational%20power%20or%20rationality%20can%20avoid%20intrinsic%20alignment%20overheads.%20This%20establishes%20rigorous%20limits%20to%20alignment%20%2Aitself%2A%2C%20not%20merely%20to%20particular%20methods%2C%20clarifying%20a%20%22No-Free-Lunch%22%20principle%3A%20encoding%20%22all%20human%20values%22%20is%20inherently%20intractable%20and%20must%20be%20managed%20through%20consensus-driven%20reduction%20or%20prioritization%20of%20objectives.%20Complementing%20this%20impossibility%20result%2C%20we%20construct%20explicit%20algorithms%20as%20achievability%20certificates%20for%20alignment%20under%20both%20unbounded%20and%20bounded%20rationality%20with%20noisy%20communication.%20Even%20in%20these%20best-case%20regimes%2C%20our%20bounded-agent%20and%20sampling%20analysis%20shows%20that%20with%20large%20task%20spaces%20%28%24D%24%29%20and%20finite%20samples%2C%20%2Areward%20hacking%20is%20globally%20inevitable%2A%3A%20rare%20high-loss%20states%20are%20systematically%20under-covered%2C%20implying%20scalable%20oversight%20must%20target%20safety-critical%20slices%20rather%20than%20uniform%20coverage.%20Together%2C%20these%20results%20identify%20fundamental%20complexity%20barriers%20--%20tasks%20%28%24M%24%29%2C%20agents%20%28%24N%24%29%2C%20and%20state-space%20size%20%28%24D%24%29%20--%20and%20offer%20principles%20for%20more%20scalable%20human-AI%20collaboration.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05934v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Barriers%2520and%2520Practical%2520Pathways%2520for%2520Human-AI%2520Alignment%253A%2520An%2520Agreement-Based%2520Complexity%2520Analysis%26entry.906535625%3DAran%2520Nayebi%26entry.1292438233%3DWe%2520formalize%2520AI%2520alignment%2520as%2520a%2520multi-objective%2520optimization%2520problem%2520called%2520%2524%255Clangle%2520M%252CN%252C%255Cvarepsilon%252C%25CE%25B4%255Crangle%2524-agreement%252C%2520in%2520which%2520a%2520set%2520of%2520%2524N%2524%2520agents%2520%2528including%2520humans%2529%2520must%2520reach%2520approximate%2520%2528%2524%255Cvarepsilon%2524%2529%2520agreement%2520across%2520%2524M%2524%2520candidate%2520objectives%252C%2520with%2520probability%2520at%2520least%2520%25241-%25CE%25B4%2524.%2520Analyzing%2520communication%2520complexity%252C%2520we%2520prove%2520an%2520information-theoretic%2520lower%2520bound%2520showing%2520that%2520once%2520either%2520%2524M%2524%2520or%2520%2524N%2524%2520is%2520large%2520enough%252C%2520no%2520amount%2520of%2520computational%2520power%2520or%2520rationality%2520can%2520avoid%2520intrinsic%2520alignment%2520overheads.%2520This%2520establishes%2520rigorous%2520limits%2520to%2520alignment%2520%252Aitself%252A%252C%2520not%2520merely%2520to%2520particular%2520methods%252C%2520clarifying%2520a%2520%2522No-Free-Lunch%2522%2520principle%253A%2520encoding%2520%2522all%2520human%2520values%2522%2520is%2520inherently%2520intractable%2520and%2520must%2520be%2520managed%2520through%2520consensus-driven%2520reduction%2520or%2520prioritization%2520of%2520objectives.%2520Complementing%2520this%2520impossibility%2520result%252C%2520we%2520construct%2520explicit%2520algorithms%2520as%2520achievability%2520certificates%2520for%2520alignment%2520under%2520both%2520unbounded%2520and%2520bounded%2520rationality%2520with%2520noisy%2520communication.%2520Even%2520in%2520these%2520best-case%2520regimes%252C%2520our%2520bounded-agent%2520and%2520sampling%2520analysis%2520shows%2520that%2520with%2520large%2520task%2520spaces%2520%2528%2524D%2524%2529%2520and%2520finite%2520samples%252C%2520%252Areward%2520hacking%2520is%2520globally%2520inevitable%252A%253A%2520rare%2520high-loss%2520states%2520are%2520systematically%2520under-covered%252C%2520implying%2520scalable%2520oversight%2520must%2520target%2520safety-critical%2520slices%2520rather%2520than%2520uniform%2520coverage.%2520Together%252C%2520these%2520results%2520identify%2520fundamental%2520complexity%2520barriers%2520--%2520tasks%2520%2528%2524M%2524%2529%252C%2520agents%2520%2528%2524N%2524%2529%252C%2520and%2520state-space%2520size%2520%2528%2524D%2524%2529%2520--%2520and%2520offer%2520principles%2520for%2520more%2520scalable%2520human-AI%2520collaboration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05934v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Barriers%20and%20Practical%20Pathways%20for%20Human-AI%20Alignment%3A%20An%20Agreement-Based%20Complexity%20Analysis&entry.906535625=Aran%20Nayebi&entry.1292438233=We%20formalize%20AI%20alignment%20as%20a%20multi-objective%20optimization%20problem%20called%20%24%5Clangle%20M%2CN%2C%5Cvarepsilon%2C%CE%B4%5Crangle%24-agreement%2C%20in%20which%20a%20set%20of%20%24N%24%20agents%20%28including%20humans%29%20must%20reach%20approximate%20%28%24%5Cvarepsilon%24%29%20agreement%20across%20%24M%24%20candidate%20objectives%2C%20with%20probability%20at%20least%20%241-%CE%B4%24.%20Analyzing%20communication%20complexity%2C%20we%20prove%20an%20information-theoretic%20lower%20bound%20showing%20that%20once%20either%20%24M%24%20or%20%24N%24%20is%20large%20enough%2C%20no%20amount%20of%20computational%20power%20or%20rationality%20can%20avoid%20intrinsic%20alignment%20overheads.%20This%20establishes%20rigorous%20limits%20to%20alignment%20%2Aitself%2A%2C%20not%20merely%20to%20particular%20methods%2C%20clarifying%20a%20%22No-Free-Lunch%22%20principle%3A%20encoding%20%22all%20human%20values%22%20is%20inherently%20intractable%20and%20must%20be%20managed%20through%20consensus-driven%20reduction%20or%20prioritization%20of%20objectives.%20Complementing%20this%20impossibility%20result%2C%20we%20construct%20explicit%20algorithms%20as%20achievability%20certificates%20for%20alignment%20under%20both%20unbounded%20and%20bounded%20rationality%20with%20noisy%20communication.%20Even%20in%20these%20best-case%20regimes%2C%20our%20bounded-agent%20and%20sampling%20analysis%20shows%20that%20with%20large%20task%20spaces%20%28%24D%24%29%20and%20finite%20samples%2C%20%2Areward%20hacking%20is%20globally%20inevitable%2A%3A%20rare%20high-loss%20states%20are%20systematically%20under-covered%2C%20implying%20scalable%20oversight%20must%20target%20safety-critical%20slices%20rather%20than%20uniform%20coverage.%20Together%2C%20these%20results%20identify%20fundamental%20complexity%20barriers%20--%20tasks%20%28%24M%24%29%2C%20agents%20%28%24N%24%29%2C%20and%20state-space%20size%20%28%24D%24%29%20--%20and%20offer%20principles%20for%20more%20scalable%20human-AI%20collaboration.&entry.1838667208=http%3A//arxiv.org/abs/2502.05934v3&entry.124074799=Read"},
{"title": "Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports", "author": "Chenchen Kuai and Zihao Li and Braden Rosen and Stephanie Paal and Navid Jafari and Jean-Louis Briaud and Yunlong Zhang and Youssef M. A. Hashash and Yang Zhou", "abstract": "Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.", "link": "http://arxiv.org/abs/2511.14010v2", "date": "2025-11-19", "relevancy": 1.0199, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Grounded%20Agentic%20Large%20Language%20Models%20for%20Multi-Hazard%20Understanding%20from%20Reconnaissance%20Reports&body=Title%3A%20Knowledge-Grounded%20Agentic%20Large%20Language%20Models%20for%20Multi-Hazard%20Understanding%20from%20Reconnaissance%20Reports%0AAuthor%3A%20Chenchen%20Kuai%20and%20Zihao%20Li%20and%20Braden%20Rosen%20and%20Stephanie%20Paal%20and%20Navid%20Jafari%20and%20Jean-Louis%20Briaud%20and%20Yunlong%20Zhang%20and%20Youssef%20M.%20A.%20Hashash%20and%20Yang%20Zhou%0AAbstract%3A%20Post-disaster%20reconnaissance%20reports%20contain%20critical%20evidence%20for%20understanding%20multi-hazard%20interactions%2C%20yet%20their%20unstructured%20narratives%20make%20systematic%20knowledge%20transfer%20difficult.%20Large%20language%20models%20%28LLMs%29%20offer%20new%20potential%20for%20analyzing%20these%20reports%2C%20but%20often%20generate%20unreliable%20or%20hallucinated%20outputs%20when%20domain%20grounding%20is%20absent.%20This%20study%20introduces%20the%20Mixture-of-Retrieval%20Agentic%20RAG%20%28MoRA-RAG%29%2C%20a%20knowledge-grounded%20LLM%20framework%20that%20transforms%20reconnaissance%20reports%20into%20a%20structured%20foundation%20for%20multi-hazard%20reasoning.%20The%20framework%20integrates%20a%20Mixture-of-Retrieval%20mechanism%20that%20dynamically%20routes%20queries%20across%20hazard-specific%20databases%20while%20using%20agentic%20chunking%20to%20preserve%20contextual%20coherence%20during%20retrieval.%20It%20also%20includes%20a%20verification%20loop%20that%20assesses%20evidence%20sufficiency%2C%20refines%20queries%2C%20and%20initiates%20targeted%20searches%20when%20information%20remains%20incomplete.%20We%20construct%20HazardRecQA%20by%20deriving%20question-answer%20pairs%20from%20GEER%20reconnaissance%20reports%2C%20which%20document%2090%20global%20events%20across%20seven%20major%20hazard%20types.%20MoRA-RAG%20achieves%20up%20to%2094.5%20percent%20accuracy%2C%20outperforming%20zero-shot%20LLMs%20by%2030%20percent%20and%20state-of-the-art%20RAG%20systems%20by%2010%20percent%2C%20while%20reducing%20hallucinations%20across%20diverse%20LLM%20architectures.%20MoRA-RAG%20also%20enables%20open-weight%20LLMs%20to%20achieve%20performance%20comparable%20to%20proprietary%20models.%20It%20establishes%20a%20new%20paradigm%20for%20transforming%20post-disaster%20documentation%20into%20actionable%2C%20trustworthy%20intelligence%20for%20hazard%20resilience.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Grounded%2520Agentic%2520Large%2520Language%2520Models%2520for%2520Multi-Hazard%2520Understanding%2520from%2520Reconnaissance%2520Reports%26entry.906535625%3DChenchen%2520Kuai%2520and%2520Zihao%2520Li%2520and%2520Braden%2520Rosen%2520and%2520Stephanie%2520Paal%2520and%2520Navid%2520Jafari%2520and%2520Jean-Louis%2520Briaud%2520and%2520Yunlong%2520Zhang%2520and%2520Youssef%2520M.%2520A.%2520Hashash%2520and%2520Yang%2520Zhou%26entry.1292438233%3DPost-disaster%2520reconnaissance%2520reports%2520contain%2520critical%2520evidence%2520for%2520understanding%2520multi-hazard%2520interactions%252C%2520yet%2520their%2520unstructured%2520narratives%2520make%2520systematic%2520knowledge%2520transfer%2520difficult.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520new%2520potential%2520for%2520analyzing%2520these%2520reports%252C%2520but%2520often%2520generate%2520unreliable%2520or%2520hallucinated%2520outputs%2520when%2520domain%2520grounding%2520is%2520absent.%2520This%2520study%2520introduces%2520the%2520Mixture-of-Retrieval%2520Agentic%2520RAG%2520%2528MoRA-RAG%2529%252C%2520a%2520knowledge-grounded%2520LLM%2520framework%2520that%2520transforms%2520reconnaissance%2520reports%2520into%2520a%2520structured%2520foundation%2520for%2520multi-hazard%2520reasoning.%2520The%2520framework%2520integrates%2520a%2520Mixture-of-Retrieval%2520mechanism%2520that%2520dynamically%2520routes%2520queries%2520across%2520hazard-specific%2520databases%2520while%2520using%2520agentic%2520chunking%2520to%2520preserve%2520contextual%2520coherence%2520during%2520retrieval.%2520It%2520also%2520includes%2520a%2520verification%2520loop%2520that%2520assesses%2520evidence%2520sufficiency%252C%2520refines%2520queries%252C%2520and%2520initiates%2520targeted%2520searches%2520when%2520information%2520remains%2520incomplete.%2520We%2520construct%2520HazardRecQA%2520by%2520deriving%2520question-answer%2520pairs%2520from%2520GEER%2520reconnaissance%2520reports%252C%2520which%2520document%252090%2520global%2520events%2520across%2520seven%2520major%2520hazard%2520types.%2520MoRA-RAG%2520achieves%2520up%2520to%252094.5%2520percent%2520accuracy%252C%2520outperforming%2520zero-shot%2520LLMs%2520by%252030%2520percent%2520and%2520state-of-the-art%2520RAG%2520systems%2520by%252010%2520percent%252C%2520while%2520reducing%2520hallucinations%2520across%2520diverse%2520LLM%2520architectures.%2520MoRA-RAG%2520also%2520enables%2520open-weight%2520LLMs%2520to%2520achieve%2520performance%2520comparable%2520to%2520proprietary%2520models.%2520It%2520establishes%2520a%2520new%2520paradigm%2520for%2520transforming%2520post-disaster%2520documentation%2520into%2520actionable%252C%2520trustworthy%2520intelligence%2520for%2520hazard%2520resilience.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Grounded%20Agentic%20Large%20Language%20Models%20for%20Multi-Hazard%20Understanding%20from%20Reconnaissance%20Reports&entry.906535625=Chenchen%20Kuai%20and%20Zihao%20Li%20and%20Braden%20Rosen%20and%20Stephanie%20Paal%20and%20Navid%20Jafari%20and%20Jean-Louis%20Briaud%20and%20Yunlong%20Zhang%20and%20Youssef%20M.%20A.%20Hashash%20and%20Yang%20Zhou&entry.1292438233=Post-disaster%20reconnaissance%20reports%20contain%20critical%20evidence%20for%20understanding%20multi-hazard%20interactions%2C%20yet%20their%20unstructured%20narratives%20make%20systematic%20knowledge%20transfer%20difficult.%20Large%20language%20models%20%28LLMs%29%20offer%20new%20potential%20for%20analyzing%20these%20reports%2C%20but%20often%20generate%20unreliable%20or%20hallucinated%20outputs%20when%20domain%20grounding%20is%20absent.%20This%20study%20introduces%20the%20Mixture-of-Retrieval%20Agentic%20RAG%20%28MoRA-RAG%29%2C%20a%20knowledge-grounded%20LLM%20framework%20that%20transforms%20reconnaissance%20reports%20into%20a%20structured%20foundation%20for%20multi-hazard%20reasoning.%20The%20framework%20integrates%20a%20Mixture-of-Retrieval%20mechanism%20that%20dynamically%20routes%20queries%20across%20hazard-specific%20databases%20while%20using%20agentic%20chunking%20to%20preserve%20contextual%20coherence%20during%20retrieval.%20It%20also%20includes%20a%20verification%20loop%20that%20assesses%20evidence%20sufficiency%2C%20refines%20queries%2C%20and%20initiates%20targeted%20searches%20when%20information%20remains%20incomplete.%20We%20construct%20HazardRecQA%20by%20deriving%20question-answer%20pairs%20from%20GEER%20reconnaissance%20reports%2C%20which%20document%2090%20global%20events%20across%20seven%20major%20hazard%20types.%20MoRA-RAG%20achieves%20up%20to%2094.5%20percent%20accuracy%2C%20outperforming%20zero-shot%20LLMs%20by%2030%20percent%20and%20state-of-the-art%20RAG%20systems%20by%2010%20percent%2C%20while%20reducing%20hallucinations%20across%20diverse%20LLM%20architectures.%20MoRA-RAG%20also%20enables%20open-weight%20LLMs%20to%20achieve%20performance%20comparable%20to%20proprietary%20models.%20It%20establishes%20a%20new%20paradigm%20for%20transforming%20post-disaster%20documentation%20into%20actionable%2C%20trustworthy%20intelligence%20for%20hazard%20resilience.&entry.1838667208=http%3A//arxiv.org/abs/2511.14010v2&entry.124074799=Read"},
{"title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography", "author": "Sai Puppala and Ismail Hossain and Jahangir Alam and Sajedul Talukder", "abstract": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.", "link": "http://arxiv.org/abs/2511.15614v1", "date": "2025-11-19", "relevancy": 1.9886, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimus-Q%3A%20Utilizing%20Federated%20Learning%20in%20Adaptive%20Robots%20for%20Intelligent%20Nuclear%20Power%20Plant%20Operations%20through%20Quantum%20Cryptography&body=Title%3A%20Optimus-Q%3A%20Utilizing%20Federated%20Learning%20in%20Adaptive%20Robots%20for%20Intelligent%20Nuclear%20Power%20Plant%20Operations%20through%20Quantum%20Cryptography%0AAuthor%3A%20Sai%20Puppala%20and%20Ismail%20Hossain%20and%20Jahangir%20Alam%20and%20Sajedul%20Talukder%0AAbstract%3A%20The%20integration%20of%20advanced%20robotics%20in%20nuclear%20power%20plants%20%28NPPs%29%20presents%20a%20transformative%20opportunity%20to%20enhance%20safety%2C%20efficiency%2C%20and%20environmental%20monitoring%20in%20high-stakes%20environments.%20Our%20paper%20introduces%20the%20Optimus-Q%20robot%2C%20a%20sophisticated%20system%20designed%20to%20autonomously%20monitor%20air%20quality%20and%20detect%20contamination%20while%20leveraging%20adaptive%20learning%20techniques%20and%20secure%20quantum%20communication.%20Equipped%20with%20advanced%20infrared%20sensors%2C%20the%20Optimus-Q%20robot%20continuously%20streams%20real-time%20environmental%20data%20to%20predict%20hazardous%20gas%20emissions%2C%20including%20carbon%20dioxide%20%28CO%24_2%24%29%2C%20carbon%20monoxide%20%28CO%29%2C%20and%20methane%20%28CH%24_4%24%29.%20Utilizing%20a%20federated%20learning%20approach%2C%20the%20robot%20collaborates%20with%20other%20systems%20across%20various%20NPPs%20to%20improve%20its%20predictive%20capabilities%20without%20compromising%20data%20privacy.%20Additionally%2C%20the%20implementation%20of%20Quantum%20Key%20Distribution%20%28QKD%29%20ensures%20secure%20data%20transmission%2C%20safeguarding%20sensitive%20operational%20information.%20Our%20methodology%20combines%20systematic%20navigation%20patterns%20with%20machine%20learning%20algorithms%20to%20facilitate%20efficient%20coverage%20of%20designated%20areas%2C%20thereby%20optimizing%20contamination%20monitoring%20processes.%20Through%20simulations%20and%20real-world%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20Optimus-Q%20robot%20in%20enhancing%20operational%20safety%20and%20responsiveness%20in%20nuclear%20facilities.%20This%20research%20underscores%20the%20potential%20of%20integrating%20robotics%2C%20machine%20learning%2C%20and%20quantum%20technologies%20to%20revolutionize%20monitoring%20systems%20in%20hazardous%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimus-Q%253A%2520Utilizing%2520Federated%2520Learning%2520in%2520Adaptive%2520Robots%2520for%2520Intelligent%2520Nuclear%2520Power%2520Plant%2520Operations%2520through%2520Quantum%2520Cryptography%26entry.906535625%3DSai%2520Puppala%2520and%2520Ismail%2520Hossain%2520and%2520Jahangir%2520Alam%2520and%2520Sajedul%2520Talukder%26entry.1292438233%3DThe%2520integration%2520of%2520advanced%2520robotics%2520in%2520nuclear%2520power%2520plants%2520%2528NPPs%2529%2520presents%2520a%2520transformative%2520opportunity%2520to%2520enhance%2520safety%252C%2520efficiency%252C%2520and%2520environmental%2520monitoring%2520in%2520high-stakes%2520environments.%2520Our%2520paper%2520introduces%2520the%2520Optimus-Q%2520robot%252C%2520a%2520sophisticated%2520system%2520designed%2520to%2520autonomously%2520monitor%2520air%2520quality%2520and%2520detect%2520contamination%2520while%2520leveraging%2520adaptive%2520learning%2520techniques%2520and%2520secure%2520quantum%2520communication.%2520Equipped%2520with%2520advanced%2520infrared%2520sensors%252C%2520the%2520Optimus-Q%2520robot%2520continuously%2520streams%2520real-time%2520environmental%2520data%2520to%2520predict%2520hazardous%2520gas%2520emissions%252C%2520including%2520carbon%2520dioxide%2520%2528CO%2524_2%2524%2529%252C%2520carbon%2520monoxide%2520%2528CO%2529%252C%2520and%2520methane%2520%2528CH%2524_4%2524%2529.%2520Utilizing%2520a%2520federated%2520learning%2520approach%252C%2520the%2520robot%2520collaborates%2520with%2520other%2520systems%2520across%2520various%2520NPPs%2520to%2520improve%2520its%2520predictive%2520capabilities%2520without%2520compromising%2520data%2520privacy.%2520Additionally%252C%2520the%2520implementation%2520of%2520Quantum%2520Key%2520Distribution%2520%2528QKD%2529%2520ensures%2520secure%2520data%2520transmission%252C%2520safeguarding%2520sensitive%2520operational%2520information.%2520Our%2520methodology%2520combines%2520systematic%2520navigation%2520patterns%2520with%2520machine%2520learning%2520algorithms%2520to%2520facilitate%2520efficient%2520coverage%2520of%2520designated%2520areas%252C%2520thereby%2520optimizing%2520contamination%2520monitoring%2520processes.%2520Through%2520simulations%2520and%2520real-world%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520Optimus-Q%2520robot%2520in%2520enhancing%2520operational%2520safety%2520and%2520responsiveness%2520in%2520nuclear%2520facilities.%2520This%2520research%2520underscores%2520the%2520potential%2520of%2520integrating%2520robotics%252C%2520machine%2520learning%252C%2520and%2520quantum%2520technologies%2520to%2520revolutionize%2520monitoring%2520systems%2520in%2520hazardous%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimus-Q%3A%20Utilizing%20Federated%20Learning%20in%20Adaptive%20Robots%20for%20Intelligent%20Nuclear%20Power%20Plant%20Operations%20through%20Quantum%20Cryptography&entry.906535625=Sai%20Puppala%20and%20Ismail%20Hossain%20and%20Jahangir%20Alam%20and%20Sajedul%20Talukder&entry.1292438233=The%20integration%20of%20advanced%20robotics%20in%20nuclear%20power%20plants%20%28NPPs%29%20presents%20a%20transformative%20opportunity%20to%20enhance%20safety%2C%20efficiency%2C%20and%20environmental%20monitoring%20in%20high-stakes%20environments.%20Our%20paper%20introduces%20the%20Optimus-Q%20robot%2C%20a%20sophisticated%20system%20designed%20to%20autonomously%20monitor%20air%20quality%20and%20detect%20contamination%20while%20leveraging%20adaptive%20learning%20techniques%20and%20secure%20quantum%20communication.%20Equipped%20with%20advanced%20infrared%20sensors%2C%20the%20Optimus-Q%20robot%20continuously%20streams%20real-time%20environmental%20data%20to%20predict%20hazardous%20gas%20emissions%2C%20including%20carbon%20dioxide%20%28CO%24_2%24%29%2C%20carbon%20monoxide%20%28CO%29%2C%20and%20methane%20%28CH%24_4%24%29.%20Utilizing%20a%20federated%20learning%20approach%2C%20the%20robot%20collaborates%20with%20other%20systems%20across%20various%20NPPs%20to%20improve%20its%20predictive%20capabilities%20without%20compromising%20data%20privacy.%20Additionally%2C%20the%20implementation%20of%20Quantum%20Key%20Distribution%20%28QKD%29%20ensures%20secure%20data%20transmission%2C%20safeguarding%20sensitive%20operational%20information.%20Our%20methodology%20combines%20systematic%20navigation%20patterns%20with%20machine%20learning%20algorithms%20to%20facilitate%20efficient%20coverage%20of%20designated%20areas%2C%20thereby%20optimizing%20contamination%20monitoring%20processes.%20Through%20simulations%20and%20real-world%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20Optimus-Q%20robot%20in%20enhancing%20operational%20safety%20and%20responsiveness%20in%20nuclear%20facilities.%20This%20research%20underscores%20the%20potential%20of%20integrating%20robotics%2C%20machine%20learning%2C%20and%20quantum%20technologies%20to%20revolutionize%20monitoring%20systems%20in%20hazardous%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.15614v1&entry.124074799=Read"},
{"title": "Multimodal Evaluation of Russian-language Architectures", "author": "Artem Chervyakov and Ulyana Isaeva and Anton Emelyanov and Artem Safin and Maria Tikhonova and Alexander Kharitonov and Yulia Lyakh and Petr Surovtsev and Denis Shevelev Vildan Saburov and Vasily Konovalov and Elisei Rykov and Ivan Sviridov and Amina Miftakhova and Ilseyar Alimova and Alexander Panchenko and Alexander Kapitanov and Alena Fenogenova", "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.", "link": "http://arxiv.org/abs/2511.15552v1", "date": "2025-11-19", "relevancy": 2.0144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Evaluation%20of%20Russian-language%20Architectures&body=Title%3A%20Multimodal%20Evaluation%20of%20Russian-language%20Architectures%0AAuthor%3A%20Artem%20Chervyakov%20and%20Ulyana%20Isaeva%20and%20Anton%20Emelyanov%20and%20Artem%20Safin%20and%20Maria%20Tikhonova%20and%20Alexander%20Kharitonov%20and%20Yulia%20Lyakh%20and%20Petr%20Surovtsev%20and%20Denis%20Shevelev%20Vildan%20Saburov%20and%20Vasily%20Konovalov%20and%20Elisei%20Rykov%20and%20Ivan%20Sviridov%20and%20Amina%20Miftakhova%20and%20Ilseyar%20Alimova%20and%20Alexander%20Panchenko%20and%20Alexander%20Kapitanov%20and%20Alena%20Fenogenova%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20currently%20at%20the%20center%20of%20research%20attention%2C%20showing%20rapid%20progress%20in%20scale%20and%20capabilities%2C%20yet%20their%20intelligence%2C%20limitations%2C%20and%20risks%20remain%20insufficiently%20understood.%20To%20address%20these%20issues%2C%20particularly%20in%20the%20context%20of%20the%20Russian%20language%2C%20where%20no%20multimodal%20benchmarks%20currently%20exist%2C%20we%20introduce%20Mera%20Multi%2C%20an%20open%20multimodal%20evaluation%20framework%20for%20Russian-spoken%20architectures.%20The%20benchmark%20is%20instruction-based%20and%20encompasses%20default%20text%2C%20image%2C%20audio%2C%20and%20video%20modalities%2C%20comprising%2018%20newly%20constructed%20evaluation%20tasks%20for%20both%20general-purpose%20models%20and%20modality-specific%20architectures%20%28image-to-text%2C%20video-to-text%2C%20and%20audio-to-text%29.%20Our%20contributions%20include%3A%20%28i%29%20a%20universal%20taxonomy%20of%20multimodal%20abilities%3B%20%28ii%29%2018%20datasets%20created%20entirely%20from%20scratch%20with%20attention%20to%20Russian%20cultural%20and%20linguistic%20specificity%2C%20unified%20prompts%2C%20and%20metrics%3B%20%28iii%29%20baseline%20results%20for%20both%20closed-source%20and%20open-source%20models%3B%20%28iv%29%20a%20methodology%20for%20preventing%20benchmark%20leakage%2C%20including%20watermarking%20and%20licenses%20for%20private%20sets.%20While%20our%20current%20focus%20is%20on%20Russian%2C%20the%20proposed%20benchmark%20provides%20a%20replicable%20methodology%20for%20constructing%20multimodal%20benchmarks%20in%20typologically%20diverse%20languages%2C%20particularly%20within%20the%20Slavic%20language%20family.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Evaluation%2520of%2520Russian-language%2520Architectures%26entry.906535625%3DArtem%2520Chervyakov%2520and%2520Ulyana%2520Isaeva%2520and%2520Anton%2520Emelyanov%2520and%2520Artem%2520Safin%2520and%2520Maria%2520Tikhonova%2520and%2520Alexander%2520Kharitonov%2520and%2520Yulia%2520Lyakh%2520and%2520Petr%2520Surovtsev%2520and%2520Denis%2520Shevelev%2520Vildan%2520Saburov%2520and%2520Vasily%2520Konovalov%2520and%2520Elisei%2520Rykov%2520and%2520Ivan%2520Sviridov%2520and%2520Amina%2520Miftakhova%2520and%2520Ilseyar%2520Alimova%2520and%2520Alexander%2520Panchenko%2520and%2520Alexander%2520Kapitanov%2520and%2520Alena%2520Fenogenova%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520currently%2520at%2520the%2520center%2520of%2520research%2520attention%252C%2520showing%2520rapid%2520progress%2520in%2520scale%2520and%2520capabilities%252C%2520yet%2520their%2520intelligence%252C%2520limitations%252C%2520and%2520risks%2520remain%2520insufficiently%2520understood.%2520To%2520address%2520these%2520issues%252C%2520particularly%2520in%2520the%2520context%2520of%2520the%2520Russian%2520language%252C%2520where%2520no%2520multimodal%2520benchmarks%2520currently%2520exist%252C%2520we%2520introduce%2520Mera%2520Multi%252C%2520an%2520open%2520multimodal%2520evaluation%2520framework%2520for%2520Russian-spoken%2520architectures.%2520The%2520benchmark%2520is%2520instruction-based%2520and%2520encompasses%2520default%2520text%252C%2520image%252C%2520audio%252C%2520and%2520video%2520modalities%252C%2520comprising%252018%2520newly%2520constructed%2520evaluation%2520tasks%2520for%2520both%2520general-purpose%2520models%2520and%2520modality-specific%2520architectures%2520%2528image-to-text%252C%2520video-to-text%252C%2520and%2520audio-to-text%2529.%2520Our%2520contributions%2520include%253A%2520%2528i%2529%2520a%2520universal%2520taxonomy%2520of%2520multimodal%2520abilities%253B%2520%2528ii%2529%252018%2520datasets%2520created%2520entirely%2520from%2520scratch%2520with%2520attention%2520to%2520Russian%2520cultural%2520and%2520linguistic%2520specificity%252C%2520unified%2520prompts%252C%2520and%2520metrics%253B%2520%2528iii%2529%2520baseline%2520results%2520for%2520both%2520closed-source%2520and%2520open-source%2520models%253B%2520%2528iv%2529%2520a%2520methodology%2520for%2520preventing%2520benchmark%2520leakage%252C%2520including%2520watermarking%2520and%2520licenses%2520for%2520private%2520sets.%2520While%2520our%2520current%2520focus%2520is%2520on%2520Russian%252C%2520the%2520proposed%2520benchmark%2520provides%2520a%2520replicable%2520methodology%2520for%2520constructing%2520multimodal%2520benchmarks%2520in%2520typologically%2520diverse%2520languages%252C%2520particularly%2520within%2520the%2520Slavic%2520language%2520family.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Evaluation%20of%20Russian-language%20Architectures&entry.906535625=Artem%20Chervyakov%20and%20Ulyana%20Isaeva%20and%20Anton%20Emelyanov%20and%20Artem%20Safin%20and%20Maria%20Tikhonova%20and%20Alexander%20Kharitonov%20and%20Yulia%20Lyakh%20and%20Petr%20Surovtsev%20and%20Denis%20Shevelev%20Vildan%20Saburov%20and%20Vasily%20Konovalov%20and%20Elisei%20Rykov%20and%20Ivan%20Sviridov%20and%20Amina%20Miftakhova%20and%20Ilseyar%20Alimova%20and%20Alexander%20Panchenko%20and%20Alexander%20Kapitanov%20and%20Alena%20Fenogenova&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20currently%20at%20the%20center%20of%20research%20attention%2C%20showing%20rapid%20progress%20in%20scale%20and%20capabilities%2C%20yet%20their%20intelligence%2C%20limitations%2C%20and%20risks%20remain%20insufficiently%20understood.%20To%20address%20these%20issues%2C%20particularly%20in%20the%20context%20of%20the%20Russian%20language%2C%20where%20no%20multimodal%20benchmarks%20currently%20exist%2C%20we%20introduce%20Mera%20Multi%2C%20an%20open%20multimodal%20evaluation%20framework%20for%20Russian-spoken%20architectures.%20The%20benchmark%20is%20instruction-based%20and%20encompasses%20default%20text%2C%20image%2C%20audio%2C%20and%20video%20modalities%2C%20comprising%2018%20newly%20constructed%20evaluation%20tasks%20for%20both%20general-purpose%20models%20and%20modality-specific%20architectures%20%28image-to-text%2C%20video-to-text%2C%20and%20audio-to-text%29.%20Our%20contributions%20include%3A%20%28i%29%20a%20universal%20taxonomy%20of%20multimodal%20abilities%3B%20%28ii%29%2018%20datasets%20created%20entirely%20from%20scratch%20with%20attention%20to%20Russian%20cultural%20and%20linguistic%20specificity%2C%20unified%20prompts%2C%20and%20metrics%3B%20%28iii%29%20baseline%20results%20for%20both%20closed-source%20and%20open-source%20models%3B%20%28iv%29%20a%20methodology%20for%20preventing%20benchmark%20leakage%2C%20including%20watermarking%20and%20licenses%20for%20private%20sets.%20While%20our%20current%20focus%20is%20on%20Russian%2C%20the%20proposed%20benchmark%20provides%20a%20replicable%20methodology%20for%20constructing%20multimodal%20benchmarks%20in%20typologically%20diverse%20languages%2C%20particularly%20within%20the%20Slavic%20language%20family.&entry.1838667208=http%3A//arxiv.org/abs/2511.15552v1&entry.124074799=Read"},
{"title": "MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement", "author": "Xinyue Yu and Youqing Fang and Pingyu Wu and Guoyang Ye and Wenbo Zhou and Weiming Zhang and Song Xiao", "abstract": "Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.", "link": "http://arxiv.org/abs/2511.12074v2", "date": "2025-11-19", "relevancy": 1.9612, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4968}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4893}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MF-Speech%3A%20Achieving%20Fine-Grained%20and%20Compositional%20Control%20in%20Speech%20Generation%20via%20Factor%20Disentanglement&body=Title%3A%20MF-Speech%3A%20Achieving%20Fine-Grained%20and%20Compositional%20Control%20in%20Speech%20Generation%20via%20Factor%20Disentanglement%0AAuthor%3A%20Xinyue%20Yu%20and%20Youqing%20Fang%20and%20Pingyu%20Wu%20and%20Guoyang%20Ye%20and%20Wenbo%20Zhou%20and%20Weiming%20Zhang%20and%20Song%20Xiao%0AAbstract%3A%20Generating%20expressive%20and%20controllable%20human%20speech%20is%20one%20of%20the%20core%20goals%20of%20generative%20artificial%20intelligence%2C%20but%20its%20progress%20has%20long%20been%20constrained%20by%20two%20fundamental%20challenges%3A%20the%20deep%20entanglement%20of%20speech%20factors%20and%20the%20coarse%20granularity%20of%20existing%20control%20mechanisms.%20To%20overcome%20these%20challenges%2C%20we%20have%20proposed%20a%20novel%20framework%20called%20MF-Speech%2C%20which%20consists%20of%20two%20core%20components%3A%20MF-SpeechEncoder%20and%20MF-SpeechGenerator.%20MF-SpeechEncoder%20acts%20as%20a%20factor%20purifier%2C%20adopting%20a%20multi-objective%20optimization%20strategy%20to%20decompose%20the%20original%20speech%20signal%20into%20highly%20pure%20and%20independent%20representations%20of%20content%2C%20timbre%2C%20and%20emotion.%20Subsequently%2C%20MF-SpeechGenerator%20functions%20as%20a%20conductor%2C%20achieving%20precise%2C%20composable%20and%20fine-grained%20control%20over%20these%20factors%20through%20dynamic%20fusion%20and%20Hierarchical%20Style%20Adaptive%20Normalization%20%28HSAN%29.%20Experiments%20demonstrate%20that%20in%20the%20highly%20challenging%20multi-factor%20compositional%20speech%20generation%20task%2C%20MF-Speech%20significantly%20outperforms%20current%20state-of-the-art%20methods%2C%20achieving%20a%20lower%20word%20error%20rate%20%28WER%3D4.67%25%29%2C%20superior%20style%20control%20%28SECS%3D0.5685%2C%20Corr%3D0.68%29%2C%20and%20the%20highest%20subjective%20evaluation%20scores%28nMOS%3D3.96%2C%20sMOS_emotion%3D3.86%2C%20sMOS_style%3D3.78%29.%20Furthermore%2C%20the%20learned%20discrete%20factors%20exhibit%20strong%20transferability%2C%20demonstrating%20their%20significant%20potential%20as%20a%20general-purpose%20speech%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMF-Speech%253A%2520Achieving%2520Fine-Grained%2520and%2520Compositional%2520Control%2520in%2520Speech%2520Generation%2520via%2520Factor%2520Disentanglement%26entry.906535625%3DXinyue%2520Yu%2520and%2520Youqing%2520Fang%2520and%2520Pingyu%2520Wu%2520and%2520Guoyang%2520Ye%2520and%2520Wenbo%2520Zhou%2520and%2520Weiming%2520Zhang%2520and%2520Song%2520Xiao%26entry.1292438233%3DGenerating%2520expressive%2520and%2520controllable%2520human%2520speech%2520is%2520one%2520of%2520the%2520core%2520goals%2520of%2520generative%2520artificial%2520intelligence%252C%2520but%2520its%2520progress%2520has%2520long%2520been%2520constrained%2520by%2520two%2520fundamental%2520challenges%253A%2520the%2520deep%2520entanglement%2520of%2520speech%2520factors%2520and%2520the%2520coarse%2520granularity%2520of%2520existing%2520control%2520mechanisms.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520have%2520proposed%2520a%2520novel%2520framework%2520called%2520MF-Speech%252C%2520which%2520consists%2520of%2520two%2520core%2520components%253A%2520MF-SpeechEncoder%2520and%2520MF-SpeechGenerator.%2520MF-SpeechEncoder%2520acts%2520as%2520a%2520factor%2520purifier%252C%2520adopting%2520a%2520multi-objective%2520optimization%2520strategy%2520to%2520decompose%2520the%2520original%2520speech%2520signal%2520into%2520highly%2520pure%2520and%2520independent%2520representations%2520of%2520content%252C%2520timbre%252C%2520and%2520emotion.%2520Subsequently%252C%2520MF-SpeechGenerator%2520functions%2520as%2520a%2520conductor%252C%2520achieving%2520precise%252C%2520composable%2520and%2520fine-grained%2520control%2520over%2520these%2520factors%2520through%2520dynamic%2520fusion%2520and%2520Hierarchical%2520Style%2520Adaptive%2520Normalization%2520%2528HSAN%2529.%2520Experiments%2520demonstrate%2520that%2520in%2520the%2520highly%2520challenging%2520multi-factor%2520compositional%2520speech%2520generation%2520task%252C%2520MF-Speech%2520significantly%2520outperforms%2520current%2520state-of-the-art%2520methods%252C%2520achieving%2520a%2520lower%2520word%2520error%2520rate%2520%2528WER%253D4.67%2525%2529%252C%2520superior%2520style%2520control%2520%2528SECS%253D0.5685%252C%2520Corr%253D0.68%2529%252C%2520and%2520the%2520highest%2520subjective%2520evaluation%2520scores%2528nMOS%253D3.96%252C%2520sMOS_emotion%253D3.86%252C%2520sMOS_style%253D3.78%2529.%2520Furthermore%252C%2520the%2520learned%2520discrete%2520factors%2520exhibit%2520strong%2520transferability%252C%2520demonstrating%2520their%2520significant%2520potential%2520as%2520a%2520general-purpose%2520speech%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MF-Speech%3A%20Achieving%20Fine-Grained%20and%20Compositional%20Control%20in%20Speech%20Generation%20via%20Factor%20Disentanglement&entry.906535625=Xinyue%20Yu%20and%20Youqing%20Fang%20and%20Pingyu%20Wu%20and%20Guoyang%20Ye%20and%20Wenbo%20Zhou%20and%20Weiming%20Zhang%20and%20Song%20Xiao&entry.1292438233=Generating%20expressive%20and%20controllable%20human%20speech%20is%20one%20of%20the%20core%20goals%20of%20generative%20artificial%20intelligence%2C%20but%20its%20progress%20has%20long%20been%20constrained%20by%20two%20fundamental%20challenges%3A%20the%20deep%20entanglement%20of%20speech%20factors%20and%20the%20coarse%20granularity%20of%20existing%20control%20mechanisms.%20To%20overcome%20these%20challenges%2C%20we%20have%20proposed%20a%20novel%20framework%20called%20MF-Speech%2C%20which%20consists%20of%20two%20core%20components%3A%20MF-SpeechEncoder%20and%20MF-SpeechGenerator.%20MF-SpeechEncoder%20acts%20as%20a%20factor%20purifier%2C%20adopting%20a%20multi-objective%20optimization%20strategy%20to%20decompose%20the%20original%20speech%20signal%20into%20highly%20pure%20and%20independent%20representations%20of%20content%2C%20timbre%2C%20and%20emotion.%20Subsequently%2C%20MF-SpeechGenerator%20functions%20as%20a%20conductor%2C%20achieving%20precise%2C%20composable%20and%20fine-grained%20control%20over%20these%20factors%20through%20dynamic%20fusion%20and%20Hierarchical%20Style%20Adaptive%20Normalization%20%28HSAN%29.%20Experiments%20demonstrate%20that%20in%20the%20highly%20challenging%20multi-factor%20compositional%20speech%20generation%20task%2C%20MF-Speech%20significantly%20outperforms%20current%20state-of-the-art%20methods%2C%20achieving%20a%20lower%20word%20error%20rate%20%28WER%3D4.67%25%29%2C%20superior%20style%20control%20%28SECS%3D0.5685%2C%20Corr%3D0.68%29%2C%20and%20the%20highest%20subjective%20evaluation%20scores%28nMOS%3D3.96%2C%20sMOS_emotion%3D3.86%2C%20sMOS_style%3D3.78%29.%20Furthermore%2C%20the%20learned%20discrete%20factors%20exhibit%20strong%20transferability%2C%20demonstrating%20their%20significant%20potential%20as%20a%20general-purpose%20speech%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2511.12074v2&entry.124074799=Read"},
{"title": "Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts", "author": "Sebasti\u00e1n Andr\u00e9s Cajas Ord\u00f3\u00f1ez and Luis Fernando Torres Torres and Mackenzie J. Meni and Carlos Andr\u00e9s Duran Paredes and Eric Arazo and Cristian Bosch and Ricardo Simon Carbajo and Yuan Lai and Leo Anthony Celi", "abstract": "Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.", "link": "http://arxiv.org/abs/2511.11743v2", "date": "2025-11-19", "relevancy": 1.5679, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5202}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Makes%20It%20Stable%3A%20Curiosity-Driven%20Quantized%20Mixture-of-Experts&body=Title%3A%20Uncertainty%20Makes%20It%20Stable%3A%20Curiosity-Driven%20Quantized%20Mixture-of-Experts%0AAuthor%3A%20Sebasti%C3%A1n%20Andr%C3%A9s%20Cajas%20Ord%C3%B3%C3%B1ez%20and%20Luis%20Fernando%20Torres%20Torres%20and%20Mackenzie%20J.%20Meni%20and%20Carlos%20Andr%C3%A9s%20Duran%20Paredes%20and%20Eric%20Arazo%20and%20Cristian%20Bosch%20and%20Ricardo%20Simon%20Carbajo%20and%20Yuan%20Lai%20and%20Leo%20Anthony%20Celi%0AAbstract%3A%20Deploying%20deep%20neural%20networks%20on%20resource-constrained%20devices%20faces%20two%20critical%20challenges%3A%20maintaining%20accuracy%20under%20aggressive%20quantization%20while%20ensuring%20predictable%20inference%20latency.%20We%20present%20a%20curiosity-driven%20quantized%20Mixture-of-Experts%20framework%20that%20addresses%20both%20through%20Bayesian%20epistemic%20uncertainty-based%20routing%20across%20heterogeneous%20experts%20%28BitNet%20ternary%2C%201-16%20bit%20BitLinear%2C%20post-training%20quantization%29.%20Evaluated%20on%20audio%20classification%20benchmarks%20%28ESC-50%2C%20Quinn%2C%20UrbanSound8K%29%2C%20our%204-bit%20quantization%20maintains%2099.9%20percent%20of%2016-bit%20accuracy%20%280.858%20vs%200.859%20F1%29%20with%204x%20compression%20and%2041%20percent%20energy%20savings%20versus%208-bit.%20Crucially%2C%20curiosity-driven%20routing%20reduces%20MoE%20latency%20variance%20by%2082%20percent%20%28p%20%3D%200.008%2C%20Levene%27s%20test%29%20from%20230%20ms%20to%2029%20ms%20standard%20deviation%2C%20enabling%20stable%20inference%20for%20battery-constrained%20devices.%20Statistical%20analysis%20confirms%204-bit/8-bit%20achieve%20practical%20equivalence%20with%20full%20precision%20%28p%20%3E%200.05%29%2C%20while%20MoE%20architectures%20introduce%2011%20percent%20latency%20overhead%20%28p%20%3C%200.001%29%20without%20accuracy%20gains.%20At%20scale%2C%20deployment%20emissions%20dominate%20training%20by%2010000x%20for%20models%20serving%20more%20than%201%2C000%20inferences%2C%20making%20inference%20efficiency%20critical.%20Our%20information-theoretic%20routing%20demonstrates%20that%20adaptive%20quantization%20yields%20accurate%20%280.858%20F1%2C%201.2M%20params%29%2C%20energy-efficient%20%283.87%20F1/mJ%29%2C%20and%20predictable%20edge%20models%2C%20with%20simple%204-bit%20quantized%20architectures%20outperforming%20complex%20MoE%20for%20most%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Makes%2520It%2520Stable%253A%2520Curiosity-Driven%2520Quantized%2520Mixture-of-Experts%26entry.906535625%3DSebasti%25C3%25A1n%2520Andr%25C3%25A9s%2520Cajas%2520Ord%25C3%25B3%25C3%25B1ez%2520and%2520Luis%2520Fernando%2520Torres%2520Torres%2520and%2520Mackenzie%2520J.%2520Meni%2520and%2520Carlos%2520Andr%25C3%25A9s%2520Duran%2520Paredes%2520and%2520Eric%2520Arazo%2520and%2520Cristian%2520Bosch%2520and%2520Ricardo%2520Simon%2520Carbajo%2520and%2520Yuan%2520Lai%2520and%2520Leo%2520Anthony%2520Celi%26entry.1292438233%3DDeploying%2520deep%2520neural%2520networks%2520on%2520resource-constrained%2520devices%2520faces%2520two%2520critical%2520challenges%253A%2520maintaining%2520accuracy%2520under%2520aggressive%2520quantization%2520while%2520ensuring%2520predictable%2520inference%2520latency.%2520We%2520present%2520a%2520curiosity-driven%2520quantized%2520Mixture-of-Experts%2520framework%2520that%2520addresses%2520both%2520through%2520Bayesian%2520epistemic%2520uncertainty-based%2520routing%2520across%2520heterogeneous%2520experts%2520%2528BitNet%2520ternary%252C%25201-16%2520bit%2520BitLinear%252C%2520post-training%2520quantization%2529.%2520Evaluated%2520on%2520audio%2520classification%2520benchmarks%2520%2528ESC-50%252C%2520Quinn%252C%2520UrbanSound8K%2529%252C%2520our%25204-bit%2520quantization%2520maintains%252099.9%2520percent%2520of%252016-bit%2520accuracy%2520%25280.858%2520vs%25200.859%2520F1%2529%2520with%25204x%2520compression%2520and%252041%2520percent%2520energy%2520savings%2520versus%25208-bit.%2520Crucially%252C%2520curiosity-driven%2520routing%2520reduces%2520MoE%2520latency%2520variance%2520by%252082%2520percent%2520%2528p%2520%253D%25200.008%252C%2520Levene%2527s%2520test%2529%2520from%2520230%2520ms%2520to%252029%2520ms%2520standard%2520deviation%252C%2520enabling%2520stable%2520inference%2520for%2520battery-constrained%2520devices.%2520Statistical%2520analysis%2520confirms%25204-bit/8-bit%2520achieve%2520practical%2520equivalence%2520with%2520full%2520precision%2520%2528p%2520%253E%25200.05%2529%252C%2520while%2520MoE%2520architectures%2520introduce%252011%2520percent%2520latency%2520overhead%2520%2528p%2520%253C%25200.001%2529%2520without%2520accuracy%2520gains.%2520At%2520scale%252C%2520deployment%2520emissions%2520dominate%2520training%2520by%252010000x%2520for%2520models%2520serving%2520more%2520than%25201%252C000%2520inferences%252C%2520making%2520inference%2520efficiency%2520critical.%2520Our%2520information-theoretic%2520routing%2520demonstrates%2520that%2520adaptive%2520quantization%2520yields%2520accurate%2520%25280.858%2520F1%252C%25201.2M%2520params%2529%252C%2520energy-efficient%2520%25283.87%2520F1/mJ%2529%252C%2520and%2520predictable%2520edge%2520models%252C%2520with%2520simple%25204-bit%2520quantized%2520architectures%2520outperforming%2520complex%2520MoE%2520for%2520most%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Makes%20It%20Stable%3A%20Curiosity-Driven%20Quantized%20Mixture-of-Experts&entry.906535625=Sebasti%C3%A1n%20Andr%C3%A9s%20Cajas%20Ord%C3%B3%C3%B1ez%20and%20Luis%20Fernando%20Torres%20Torres%20and%20Mackenzie%20J.%20Meni%20and%20Carlos%20Andr%C3%A9s%20Duran%20Paredes%20and%20Eric%20Arazo%20and%20Cristian%20Bosch%20and%20Ricardo%20Simon%20Carbajo%20and%20Yuan%20Lai%20and%20Leo%20Anthony%20Celi&entry.1292438233=Deploying%20deep%20neural%20networks%20on%20resource-constrained%20devices%20faces%20two%20critical%20challenges%3A%20maintaining%20accuracy%20under%20aggressive%20quantization%20while%20ensuring%20predictable%20inference%20latency.%20We%20present%20a%20curiosity-driven%20quantized%20Mixture-of-Experts%20framework%20that%20addresses%20both%20through%20Bayesian%20epistemic%20uncertainty-based%20routing%20across%20heterogeneous%20experts%20%28BitNet%20ternary%2C%201-16%20bit%20BitLinear%2C%20post-training%20quantization%29.%20Evaluated%20on%20audio%20classification%20benchmarks%20%28ESC-50%2C%20Quinn%2C%20UrbanSound8K%29%2C%20our%204-bit%20quantization%20maintains%2099.9%20percent%20of%2016-bit%20accuracy%20%280.858%20vs%200.859%20F1%29%20with%204x%20compression%20and%2041%20percent%20energy%20savings%20versus%208-bit.%20Crucially%2C%20curiosity-driven%20routing%20reduces%20MoE%20latency%20variance%20by%2082%20percent%20%28p%20%3D%200.008%2C%20Levene%27s%20test%29%20from%20230%20ms%20to%2029%20ms%20standard%20deviation%2C%20enabling%20stable%20inference%20for%20battery-constrained%20devices.%20Statistical%20analysis%20confirms%204-bit/8-bit%20achieve%20practical%20equivalence%20with%20full%20precision%20%28p%20%3E%200.05%29%2C%20while%20MoE%20architectures%20introduce%2011%20percent%20latency%20overhead%20%28p%20%3C%200.001%29%20without%20accuracy%20gains.%20At%20scale%2C%20deployment%20emissions%20dominate%20training%20by%2010000x%20for%20models%20serving%20more%20than%201%2C000%20inferences%2C%20making%20inference%20efficiency%20critical.%20Our%20information-theoretic%20routing%20demonstrates%20that%20adaptive%20quantization%20yields%20accurate%20%280.858%20F1%2C%201.2M%20params%29%2C%20energy-efficient%20%283.87%20F1/mJ%29%2C%20and%20predictable%20edge%20models%2C%20with%20simple%204-bit%20quantized%20architectures%20outperforming%20complex%20MoE%20for%20most%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2511.11743v2&entry.124074799=Read"},
{"title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "author": "Sirui Chen and Mengshi Zhao and Lei Xu and Yuying Zhao and Beier Zhu and Hanwang Zhang and Shengjie Zhao and Chaochao Lu", "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "link": "http://arxiv.org/abs/2511.15392v1", "date": "2025-11-19", "relevancy": 1.5258, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5197}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5151}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEPO%3A%20Dual-Efficiency%20Preference%20Optimization%20for%20LLM%20Agents&body=Title%3A%20DEPO%3A%20Dual-Efficiency%20Preference%20Optimization%20for%20LLM%20Agents%0AAuthor%3A%20Sirui%20Chen%20and%20Mengshi%20Zhao%20and%20Lei%20Xu%20and%20Yuying%20Zhao%20and%20Beier%20Zhu%20and%20Hanwang%20Zhang%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20improved%20their%20reasoning%20and%20decision-making%20abilities%20when%20deployed%20as%20agents.%20Richer%20reasoning%2C%20however%2C%20often%20comes%20at%20the%20cost%20of%20longer%20chain%20of%20thought%20%28CoT%29%2C%20hampering%20interaction%20efficiency%20in%20real-world%20scenarios.%20Nevertheless%2C%20there%20still%20lacks%20systematic%20definition%20of%20LLM%20agent%20efficiency%2C%20hindering%20targeted%20improvements.%20To%20this%20end%2C%20we%20introduce%20dual-efficiency%2C%20comprising%20%28i%29%20step-level%20efficiency%2C%20which%20minimizes%20tokens%20per%20step%2C%20and%20%28ii%29%20trajectory-level%20efficiency%2C%20which%20minimizes%20the%20number%20of%20steps%20to%20complete%20a%20task.%20Building%20on%20this%20definition%2C%20we%20propose%20DEPO%2C%20a%20dual-efficiency%20preference%20optimization%20method%20that%20jointly%20rewards%20succinct%20responses%20and%20fewer%20action%20steps.%20Experiments%20on%20WebShop%20and%20BabyAI%20show%20that%20DEPO%20cuts%20token%20usage%20by%20up%20to%2060.9%25%20and%20steps%20by%20up%20to%2026.9%25%2C%20while%20achieving%20up%20to%20a%2029.3%25%20improvement%20in%20performance.%20DEPO%20also%20generalizes%20to%20three%20out-of-domain%20math%20benchmarks%20and%20retains%20its%20efficiency%20gains%20when%20trained%20on%20only%2025%25%20of%20the%20data.%20Our%20project%20page%20is%20at%20https%3A//opencausalab.github.io/DEPO.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEPO%253A%2520Dual-Efficiency%2520Preference%2520Optimization%2520for%2520LLM%2520Agents%26entry.906535625%3DSirui%2520Chen%2520and%2520Mengshi%2520Zhao%2520and%2520Lei%2520Xu%2520and%2520Yuying%2520Zhao%2520and%2520Beier%2520Zhu%2520and%2520Hanwang%2520Zhang%2520and%2520Shengjie%2520Zhao%2520and%2520Chaochao%2520Lu%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520greatly%2520improved%2520their%2520reasoning%2520and%2520decision-making%2520abilities%2520when%2520deployed%2520as%2520agents.%2520Richer%2520reasoning%252C%2520however%252C%2520often%2520comes%2520at%2520the%2520cost%2520of%2520longer%2520chain%2520of%2520thought%2520%2528CoT%2529%252C%2520hampering%2520interaction%2520efficiency%2520in%2520real-world%2520scenarios.%2520Nevertheless%252C%2520there%2520still%2520lacks%2520systematic%2520definition%2520of%2520LLM%2520agent%2520efficiency%252C%2520hindering%2520targeted%2520improvements.%2520To%2520this%2520end%252C%2520we%2520introduce%2520dual-efficiency%252C%2520comprising%2520%2528i%2529%2520step-level%2520efficiency%252C%2520which%2520minimizes%2520tokens%2520per%2520step%252C%2520and%2520%2528ii%2529%2520trajectory-level%2520efficiency%252C%2520which%2520minimizes%2520the%2520number%2520of%2520steps%2520to%2520complete%2520a%2520task.%2520Building%2520on%2520this%2520definition%252C%2520we%2520propose%2520DEPO%252C%2520a%2520dual-efficiency%2520preference%2520optimization%2520method%2520that%2520jointly%2520rewards%2520succinct%2520responses%2520and%2520fewer%2520action%2520steps.%2520Experiments%2520on%2520WebShop%2520and%2520BabyAI%2520show%2520that%2520DEPO%2520cuts%2520token%2520usage%2520by%2520up%2520to%252060.9%2525%2520and%2520steps%2520by%2520up%2520to%252026.9%2525%252C%2520while%2520achieving%2520up%2520to%2520a%252029.3%2525%2520improvement%2520in%2520performance.%2520DEPO%2520also%2520generalizes%2520to%2520three%2520out-of-domain%2520math%2520benchmarks%2520and%2520retains%2520its%2520efficiency%2520gains%2520when%2520trained%2520on%2520only%252025%2525%2520of%2520the%2520data.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//opencausalab.github.io/DEPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEPO%3A%20Dual-Efficiency%20Preference%20Optimization%20for%20LLM%20Agents&entry.906535625=Sirui%20Chen%20and%20Mengshi%20Zhao%20and%20Lei%20Xu%20and%20Yuying%20Zhao%20and%20Beier%20Zhu%20and%20Hanwang%20Zhang%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20improved%20their%20reasoning%20and%20decision-making%20abilities%20when%20deployed%20as%20agents.%20Richer%20reasoning%2C%20however%2C%20often%20comes%20at%20the%20cost%20of%20longer%20chain%20of%20thought%20%28CoT%29%2C%20hampering%20interaction%20efficiency%20in%20real-world%20scenarios.%20Nevertheless%2C%20there%20still%20lacks%20systematic%20definition%20of%20LLM%20agent%20efficiency%2C%20hindering%20targeted%20improvements.%20To%20this%20end%2C%20we%20introduce%20dual-efficiency%2C%20comprising%20%28i%29%20step-level%20efficiency%2C%20which%20minimizes%20tokens%20per%20step%2C%20and%20%28ii%29%20trajectory-level%20efficiency%2C%20which%20minimizes%20the%20number%20of%20steps%20to%20complete%20a%20task.%20Building%20on%20this%20definition%2C%20we%20propose%20DEPO%2C%20a%20dual-efficiency%20preference%20optimization%20method%20that%20jointly%20rewards%20succinct%20responses%20and%20fewer%20action%20steps.%20Experiments%20on%20WebShop%20and%20BabyAI%20show%20that%20DEPO%20cuts%20token%20usage%20by%20up%20to%2060.9%25%20and%20steps%20by%20up%20to%2026.9%25%2C%20while%20achieving%20up%20to%20a%2029.3%25%20improvement%20in%20performance.%20DEPO%20also%20generalizes%20to%20three%20out-of-domain%20math%20benchmarks%20and%20retains%20its%20efficiency%20gains%20when%20trained%20on%20only%2025%25%20of%20the%20data.%20Our%20project%20page%20is%20at%20https%3A//opencausalab.github.io/DEPO.&entry.1838667208=http%3A//arxiv.org/abs/2511.15392v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


