<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250717.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models", "author": "Yudong Jin and Sida Peng and Xuan Wang and Tao Xie and Zhen Xu and Yifan Yang and Yujun Shen and Hujun Bao and Xiaowei Zhou", "abstract": "  This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .\n", "link": "http://arxiv.org/abs/2507.13344v1", "date": "2025-07-17", "relevancy": 3.4767, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7279}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffuman4D%3A%204D%20Consistent%20Human%20View%20Synthesis%20from%20Sparse-View%20Videos%0A%20%20with%20Spatio-Temporal%20Diffusion%20Models&body=Title%3A%20Diffuman4D%3A%204D%20Consistent%20Human%20View%20Synthesis%20from%20Sparse-View%20Videos%0A%20%20with%20Spatio-Temporal%20Diffusion%20Models%0AAuthor%3A%20Yudong%20Jin%20and%20Sida%20Peng%20and%20Xuan%20Wang%20and%20Tao%20Xie%20and%20Zhen%20Xu%20and%20Yifan%20Yang%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20high-fidelity%20view%20synthesis%20of%20humans%0Awith%20sparse-view%20videos%20as%20input.%20Previous%20methods%20solve%20the%20issue%20of%0Ainsufficient%20observation%20by%20leveraging%204D%20diffusion%20models%20to%20generate%20videos%0Aat%20novel%20viewpoints.%20However%2C%20the%20generated%20videos%20from%20these%20models%20often%20lack%0Aspatio-temporal%20consistency%2C%20thus%20degrading%20view%20synthesis%20quality.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20sliding%20iterative%20denoising%20process%20to%20enhance%20the%0Aspatio-temporal%20consistency%20of%20the%204D%20diffusion%20model.%20Specifically%2C%20we%20define%0Aa%20latent%20grid%20in%20which%20each%20latent%20encodes%20the%20image%2C%20camera%20pose%2C%20and%20human%0Apose%20for%20a%20certain%20viewpoint%20and%20timestamp%2C%20then%20alternately%20denoising%20the%0Alatent%20grid%20along%20spatial%20and%20temporal%20dimensions%20with%20a%20sliding%20window%2C%20and%0Afinally%20decode%20the%20videos%20at%20target%20viewpoints%20from%20the%20corresponding%20denoised%0Alatents.%20Through%20the%20iterative%20sliding%2C%20information%20flows%20sufficiently%20across%0Athe%20latent%20grid%2C%20allowing%20the%20diffusion%20model%20to%20obtain%20a%20large%20receptive%20field%0Aand%20thus%20enhance%20the%204D%20consistency%20of%20the%20output%2C%20while%20making%20the%20GPU%20memory%0Aconsumption%20affordable.%20The%20experiments%20on%20the%20DNA-Rendering%20and%20ActorsHQ%0Adatasets%20demonstrate%20that%20our%20method%20is%20able%20to%20synthesize%20high-quality%20and%0Aconsistent%20novel-view%20videos%20and%20significantly%20outperforms%20the%20existing%0Aapproaches.%20See%20our%20project%20page%20for%20interactive%20demos%20and%20video%20results%3A%0Ahttps%3A//diffuman4d.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuman4D%253A%25204D%2520Consistent%2520Human%2520View%2520Synthesis%2520from%2520Sparse-View%2520Videos%250A%2520%2520with%2520Spatio-Temporal%2520Diffusion%2520Models%26entry.906535625%3DYudong%2520Jin%2520and%2520Sida%2520Peng%2520and%2520Xuan%2520Wang%2520and%2520Tao%2520Xie%2520and%2520Zhen%2520Xu%2520and%2520Yifan%2520Yang%2520and%2520Yujun%2520Shen%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520high-fidelity%2520view%2520synthesis%2520of%2520humans%250Awith%2520sparse-view%2520videos%2520as%2520input.%2520Previous%2520methods%2520solve%2520the%2520issue%2520of%250Ainsufficient%2520observation%2520by%2520leveraging%25204D%2520diffusion%2520models%2520to%2520generate%2520videos%250Aat%2520novel%2520viewpoints.%2520However%252C%2520the%2520generated%2520videos%2520from%2520these%2520models%2520often%2520lack%250Aspatio-temporal%2520consistency%252C%2520thus%2520degrading%2520view%2520synthesis%2520quality.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520sliding%2520iterative%2520denoising%2520process%2520to%2520enhance%2520the%250Aspatio-temporal%2520consistency%2520of%2520the%25204D%2520diffusion%2520model.%2520Specifically%252C%2520we%2520define%250Aa%2520latent%2520grid%2520in%2520which%2520each%2520latent%2520encodes%2520the%2520image%252C%2520camera%2520pose%252C%2520and%2520human%250Apose%2520for%2520a%2520certain%2520viewpoint%2520and%2520timestamp%252C%2520then%2520alternately%2520denoising%2520the%250Alatent%2520grid%2520along%2520spatial%2520and%2520temporal%2520dimensions%2520with%2520a%2520sliding%2520window%252C%2520and%250Afinally%2520decode%2520the%2520videos%2520at%2520target%2520viewpoints%2520from%2520the%2520corresponding%2520denoised%250Alatents.%2520Through%2520the%2520iterative%2520sliding%252C%2520information%2520flows%2520sufficiently%2520across%250Athe%2520latent%2520grid%252C%2520allowing%2520the%2520diffusion%2520model%2520to%2520obtain%2520a%2520large%2520receptive%2520field%250Aand%2520thus%2520enhance%2520the%25204D%2520consistency%2520of%2520the%2520output%252C%2520while%2520making%2520the%2520GPU%2520memory%250Aconsumption%2520affordable.%2520The%2520experiments%2520on%2520the%2520DNA-Rendering%2520and%2520ActorsHQ%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520is%2520able%2520to%2520synthesize%2520high-quality%2520and%250Aconsistent%2520novel-view%2520videos%2520and%2520significantly%2520outperforms%2520the%2520existing%250Aapproaches.%2520See%2520our%2520project%2520page%2520for%2520interactive%2520demos%2520and%2520video%2520results%253A%250Ahttps%253A//diffuman4d.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffuman4D%3A%204D%20Consistent%20Human%20View%20Synthesis%20from%20Sparse-View%20Videos%0A%20%20with%20Spatio-Temporal%20Diffusion%20Models&entry.906535625=Yudong%20Jin%20and%20Sida%20Peng%20and%20Xuan%20Wang%20and%20Tao%20Xie%20and%20Zhen%20Xu%20and%20Yifan%20Yang%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20high-fidelity%20view%20synthesis%20of%20humans%0Awith%20sparse-view%20videos%20as%20input.%20Previous%20methods%20solve%20the%20issue%20of%0Ainsufficient%20observation%20by%20leveraging%204D%20diffusion%20models%20to%20generate%20videos%0Aat%20novel%20viewpoints.%20However%2C%20the%20generated%20videos%20from%20these%20models%20often%20lack%0Aspatio-temporal%20consistency%2C%20thus%20degrading%20view%20synthesis%20quality.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20sliding%20iterative%20denoising%20process%20to%20enhance%20the%0Aspatio-temporal%20consistency%20of%20the%204D%20diffusion%20model.%20Specifically%2C%20we%20define%0Aa%20latent%20grid%20in%20which%20each%20latent%20encodes%20the%20image%2C%20camera%20pose%2C%20and%20human%0Apose%20for%20a%20certain%20viewpoint%20and%20timestamp%2C%20then%20alternately%20denoising%20the%0Alatent%20grid%20along%20spatial%20and%20temporal%20dimensions%20with%20a%20sliding%20window%2C%20and%0Afinally%20decode%20the%20videos%20at%20target%20viewpoints%20from%20the%20corresponding%20denoised%0Alatents.%20Through%20the%20iterative%20sliding%2C%20information%20flows%20sufficiently%20across%0Athe%20latent%20grid%2C%20allowing%20the%20diffusion%20model%20to%20obtain%20a%20large%20receptive%20field%0Aand%20thus%20enhance%20the%204D%20consistency%20of%20the%20output%2C%20while%20making%20the%20GPU%20memory%0Aconsumption%20affordable.%20The%20experiments%20on%20the%20DNA-Rendering%20and%20ActorsHQ%0Adatasets%20demonstrate%20that%20our%20method%20is%20able%20to%20synthesize%20high-quality%20and%0Aconsistent%20novel-view%20videos%20and%20significantly%20outperforms%20the%20existing%0Aapproaches.%20See%20our%20project%20page%20for%20interactive%20demos%20and%20video%20results%3A%0Ahttps%3A//diffuman4d.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13344v1&entry.124074799=Read"},
{"title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning", "author": "Senqiao Yang and Junyi Li and Xin Lai and Bei Yu and Hengshuang Zhao and Jiaya Jia", "abstract": "  Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.\n", "link": "http://arxiv.org/abs/2507.13348v1", "date": "2025-07-17", "relevancy": 3.0486, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionThink%3A%20Smart%20and%20Efficient%20Vision%20Language%20Model%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20VisionThink%3A%20Smart%20and%20Efficient%20Vision%20Language%20Model%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Senqiao%20Yang%20and%20Junyi%20Li%20and%20Xin%20Lai%20and%20Bei%20Yu%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20improved%0Aperformance%20by%20increasing%20the%20number%20of%20visual%20tokens%2C%20which%20are%20often%0Asignificantly%20longer%20than%20text%20tokens.%20However%2C%20we%20observe%20that%20most%20real-world%0Ascenarios%20do%20not%20require%20such%20an%20extensive%20number%20of%20visual%20tokens.%20While%20the%0Aperformance%20drops%20significantly%20in%20a%20small%20subset%20of%20OCR-related%20tasks%2C%20models%0Astill%20perform%20accurately%20in%20most%20other%20general%20VQA%20tasks%20with%20only%201/4%0Aresolution.%20Therefore%2C%20we%20propose%20to%20dynamically%20process%20distinct%20samples%20with%0Adifferent%20resolutions%2C%20and%20present%20a%20new%20paradigm%20for%20visual%20token%20compression%2C%0Anamely%2C%20VisionThink.%20It%20starts%20with%20a%20downsampled%20image%20and%20smartly%20decides%0Awhether%20it%20is%20sufficient%20for%20problem%20solving.%20Otherwise%2C%20the%20model%20could%20output%0Aa%20special%20token%20to%20request%20the%20higher-resolution%20image.%20Compared%20to%20existing%0AEfficient%20VLM%20methods%20that%20compress%20tokens%20using%20fixed%20pruning%20ratios%20or%0Athresholds%2C%20VisionThink%20autonomously%20decides%20whether%20to%20compress%20tokens%20case%20by%0Acase.%20As%20a%20result%2C%20it%20demonstrates%20strong%20fine-grained%20visual%20understanding%0Acapability%20on%20OCR-related%20tasks%2C%20and%20meanwhile%20saves%20substantial%20visual%20tokens%0Aon%20simpler%20tasks.%20We%20adopt%20reinforcement%20learning%20and%20propose%20the%20LLM-as-Judge%0Astrategy%20to%20successfully%20apply%20RL%20to%20general%20VQA%20tasks.%20Moreover%2C%20we%20carefully%0Adesign%20a%20reward%20function%20and%20penalty%20mechanism%20to%20achieve%20a%20stable%20and%0Areasonable%20image%20resize%20call%20ratio.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%2C%20efficiency%2C%20and%20effectiveness%20of%20our%20method.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/dvlab-research/VisionThink.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionThink%253A%2520Smart%2520and%2520Efficient%2520Vision%2520Language%2520Model%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSenqiao%2520Yang%2520and%2520Junyi%2520Li%2520and%2520Xin%2520Lai%2520and%2520Bei%2520Yu%2520and%2520Hengshuang%2520Zhao%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520improved%250Aperformance%2520by%2520increasing%2520the%2520number%2520of%2520visual%2520tokens%252C%2520which%2520are%2520often%250Asignificantly%2520longer%2520than%2520text%2520tokens.%2520However%252C%2520we%2520observe%2520that%2520most%2520real-world%250Ascenarios%2520do%2520not%2520require%2520such%2520an%2520extensive%2520number%2520of%2520visual%2520tokens.%2520While%2520the%250Aperformance%2520drops%2520significantly%2520in%2520a%2520small%2520subset%2520of%2520OCR-related%2520tasks%252C%2520models%250Astill%2520perform%2520accurately%2520in%2520most%2520other%2520general%2520VQA%2520tasks%2520with%2520only%25201/4%250Aresolution.%2520Therefore%252C%2520we%2520propose%2520to%2520dynamically%2520process%2520distinct%2520samples%2520with%250Adifferent%2520resolutions%252C%2520and%2520present%2520a%2520new%2520paradigm%2520for%2520visual%2520token%2520compression%252C%250Anamely%252C%2520VisionThink.%2520It%2520starts%2520with%2520a%2520downsampled%2520image%2520and%2520smartly%2520decides%250Awhether%2520it%2520is%2520sufficient%2520for%2520problem%2520solving.%2520Otherwise%252C%2520the%2520model%2520could%2520output%250Aa%2520special%2520token%2520to%2520request%2520the%2520higher-resolution%2520image.%2520Compared%2520to%2520existing%250AEfficient%2520VLM%2520methods%2520that%2520compress%2520tokens%2520using%2520fixed%2520pruning%2520ratios%2520or%250Athresholds%252C%2520VisionThink%2520autonomously%2520decides%2520whether%2520to%2520compress%2520tokens%2520case%2520by%250Acase.%2520As%2520a%2520result%252C%2520it%2520demonstrates%2520strong%2520fine-grained%2520visual%2520understanding%250Acapability%2520on%2520OCR-related%2520tasks%252C%2520and%2520meanwhile%2520saves%2520substantial%2520visual%2520tokens%250Aon%2520simpler%2520tasks.%2520We%2520adopt%2520reinforcement%2520learning%2520and%2520propose%2520the%2520LLM-as-Judge%250Astrategy%2520to%2520successfully%2520apply%2520RL%2520to%2520general%2520VQA%2520tasks.%2520Moreover%252C%2520we%2520carefully%250Adesign%2520a%2520reward%2520function%2520and%2520penalty%2520mechanism%2520to%2520achieve%2520a%2520stable%2520and%250Areasonable%2520image%2520resize%2520call%2520ratio.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%252C%2520efficiency%252C%2520and%2520effectiveness%2520of%2520our%2520method.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/dvlab-research/VisionThink.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionThink%3A%20Smart%20and%20Efficient%20Vision%20Language%20Model%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Senqiao%20Yang%20and%20Junyi%20Li%20and%20Xin%20Lai%20and%20Bei%20Yu%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia&entry.1292438233=%20%20Recent%20advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20improved%0Aperformance%20by%20increasing%20the%20number%20of%20visual%20tokens%2C%20which%20are%20often%0Asignificantly%20longer%20than%20text%20tokens.%20However%2C%20we%20observe%20that%20most%20real-world%0Ascenarios%20do%20not%20require%20such%20an%20extensive%20number%20of%20visual%20tokens.%20While%20the%0Aperformance%20drops%20significantly%20in%20a%20small%20subset%20of%20OCR-related%20tasks%2C%20models%0Astill%20perform%20accurately%20in%20most%20other%20general%20VQA%20tasks%20with%20only%201/4%0Aresolution.%20Therefore%2C%20we%20propose%20to%20dynamically%20process%20distinct%20samples%20with%0Adifferent%20resolutions%2C%20and%20present%20a%20new%20paradigm%20for%20visual%20token%20compression%2C%0Anamely%2C%20VisionThink.%20It%20starts%20with%20a%20downsampled%20image%20and%20smartly%20decides%0Awhether%20it%20is%20sufficient%20for%20problem%20solving.%20Otherwise%2C%20the%20model%20could%20output%0Aa%20special%20token%20to%20request%20the%20higher-resolution%20image.%20Compared%20to%20existing%0AEfficient%20VLM%20methods%20that%20compress%20tokens%20using%20fixed%20pruning%20ratios%20or%0Athresholds%2C%20VisionThink%20autonomously%20decides%20whether%20to%20compress%20tokens%20case%20by%0Acase.%20As%20a%20result%2C%20it%20demonstrates%20strong%20fine-grained%20visual%20understanding%0Acapability%20on%20OCR-related%20tasks%2C%20and%20meanwhile%20saves%20substantial%20visual%20tokens%0Aon%20simpler%20tasks.%20We%20adopt%20reinforcement%20learning%20and%20propose%20the%20LLM-as-Judge%0Astrategy%20to%20successfully%20apply%20RL%20to%20general%20VQA%20tasks.%20Moreover%2C%20we%20carefully%0Adesign%20a%20reward%20function%20and%20penalty%20mechanism%20to%20achieve%20a%20stable%20and%0Areasonable%20image%20resize%20call%20ratio.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%2C%20efficiency%2C%20and%20effectiveness%20of%20our%20method.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/dvlab-research/VisionThink.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13348v1&entry.124074799=Read"},
{"title": "Leveraging Pre-Trained Visual Models for AI-Generated Video Detection", "author": "Keerthi Veeramachaneni and Praveen Tirupattur and Amrit Singh Bedi and Mubarak Shah", "abstract": "  Recent advances in Generative AI (GenAI) have led to significant improvements\nin the quality of generated visual content. As AI-generated visual content\nbecomes increasingly indistinguishable from real content, the challenge of\ndetecting the generated content becomes critical in combating misinformation,\nensuring privacy, and preventing security threats. Although there has been\nsubstantial progress in detecting AI-generated images, current methods for\nvideo detection are largely focused on deepfakes, which primarily involve human\nfaces. However, the field of video generation has advanced beyond DeepFakes,\ncreating an urgent need for methods capable of detecting AI-generated videos\nwith generic content. To address this gap, we propose a novel approach that\nleverages pre-trained visual models to distinguish between real and generated\nvideos. The features extracted from these pre-trained models, which have been\ntrained on extensive real visual content, contain inherent signals that can\nhelp distinguish real from generated videos. Using these extracted features, we\nachieve high detection performance without requiring additional model training,\nand we further improve performance by training a simple linear classification\nlayer on top of the extracted features. We validated our method on a dataset we\ncompiled (VID-AID), which includes around 10,000 AI-generated videos produced\nby 9 different text-to-video models, along with 4,000 real videos, totaling\nover 7 hours of video content. Our evaluation shows that our approach achieves\nhigh detection accuracy, above 90% on average, underscoring its effectiveness.\nUpon acceptance, we plan to publicly release the code, the pre-trained models,\nand our dataset to support ongoing research in this critical area.\n", "link": "http://arxiv.org/abs/2507.13224v1", "date": "2025-07-17", "relevancy": 2.942, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6255}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5726}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Pre-Trained%20Visual%20Models%20for%20AI-Generated%20Video%20Detection&body=Title%3A%20Leveraging%20Pre-Trained%20Visual%20Models%20for%20AI-Generated%20Video%20Detection%0AAuthor%3A%20Keerthi%20Veeramachaneni%20and%20Praveen%20Tirupattur%20and%20Amrit%20Singh%20Bedi%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Recent%20advances%20in%20Generative%20AI%20%28GenAI%29%20have%20led%20to%20significant%20improvements%0Ain%20the%20quality%20of%20generated%20visual%20content.%20As%20AI-generated%20visual%20content%0Abecomes%20increasingly%20indistinguishable%20from%20real%20content%2C%20the%20challenge%20of%0Adetecting%20the%20generated%20content%20becomes%20critical%20in%20combating%20misinformation%2C%0Aensuring%20privacy%2C%20and%20preventing%20security%20threats.%20Although%20there%20has%20been%0Asubstantial%20progress%20in%20detecting%20AI-generated%20images%2C%20current%20methods%20for%0Avideo%20detection%20are%20largely%20focused%20on%20deepfakes%2C%20which%20primarily%20involve%20human%0Afaces.%20However%2C%20the%20field%20of%20video%20generation%20has%20advanced%20beyond%20DeepFakes%2C%0Acreating%20an%20urgent%20need%20for%20methods%20capable%20of%20detecting%20AI-generated%20videos%0Awith%20generic%20content.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%20approach%20that%0Aleverages%20pre-trained%20visual%20models%20to%20distinguish%20between%20real%20and%20generated%0Avideos.%20The%20features%20extracted%20from%20these%20pre-trained%20models%2C%20which%20have%20been%0Atrained%20on%20extensive%20real%20visual%20content%2C%20contain%20inherent%20signals%20that%20can%0Ahelp%20distinguish%20real%20from%20generated%20videos.%20Using%20these%20extracted%20features%2C%20we%0Aachieve%20high%20detection%20performance%20without%20requiring%20additional%20model%20training%2C%0Aand%20we%20further%20improve%20performance%20by%20training%20a%20simple%20linear%20classification%0Alayer%20on%20top%20of%20the%20extracted%20features.%20We%20validated%20our%20method%20on%20a%20dataset%20we%0Acompiled%20%28VID-AID%29%2C%20which%20includes%20around%2010%2C000%20AI-generated%20videos%20produced%0Aby%209%20different%20text-to-video%20models%2C%20along%20with%204%2C000%20real%20videos%2C%20totaling%0Aover%207%20hours%20of%20video%20content.%20Our%20evaluation%20shows%20that%20our%20approach%20achieves%0Ahigh%20detection%20accuracy%2C%20above%2090%25%20on%20average%2C%20underscoring%20its%20effectiveness.%0AUpon%20acceptance%2C%20we%20plan%20to%20publicly%20release%20the%20code%2C%20the%20pre-trained%20models%2C%0Aand%20our%20dataset%20to%20support%20ongoing%20research%20in%20this%20critical%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Pre-Trained%2520Visual%2520Models%2520for%2520AI-Generated%2520Video%2520Detection%26entry.906535625%3DKeerthi%2520Veeramachaneni%2520and%2520Praveen%2520Tirupattur%2520and%2520Amrit%2520Singh%2520Bedi%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Generative%2520AI%2520%2528GenAI%2529%2520have%2520led%2520to%2520significant%2520improvements%250Ain%2520the%2520quality%2520of%2520generated%2520visual%2520content.%2520As%2520AI-generated%2520visual%2520content%250Abecomes%2520increasingly%2520indistinguishable%2520from%2520real%2520content%252C%2520the%2520challenge%2520of%250Adetecting%2520the%2520generated%2520content%2520becomes%2520critical%2520in%2520combating%2520misinformation%252C%250Aensuring%2520privacy%252C%2520and%2520preventing%2520security%2520threats.%2520Although%2520there%2520has%2520been%250Asubstantial%2520progress%2520in%2520detecting%2520AI-generated%2520images%252C%2520current%2520methods%2520for%250Avideo%2520detection%2520are%2520largely%2520focused%2520on%2520deepfakes%252C%2520which%2520primarily%2520involve%2520human%250Afaces.%2520However%252C%2520the%2520field%2520of%2520video%2520generation%2520has%2520advanced%2520beyond%2520DeepFakes%252C%250Acreating%2520an%2520urgent%2520need%2520for%2520methods%2520capable%2520of%2520detecting%2520AI-generated%2520videos%250Awith%2520generic%2520content.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%250Aleverages%2520pre-trained%2520visual%2520models%2520to%2520distinguish%2520between%2520real%2520and%2520generated%250Avideos.%2520The%2520features%2520extracted%2520from%2520these%2520pre-trained%2520models%252C%2520which%2520have%2520been%250Atrained%2520on%2520extensive%2520real%2520visual%2520content%252C%2520contain%2520inherent%2520signals%2520that%2520can%250Ahelp%2520distinguish%2520real%2520from%2520generated%2520videos.%2520Using%2520these%2520extracted%2520features%252C%2520we%250Aachieve%2520high%2520detection%2520performance%2520without%2520requiring%2520additional%2520model%2520training%252C%250Aand%2520we%2520further%2520improve%2520performance%2520by%2520training%2520a%2520simple%2520linear%2520classification%250Alayer%2520on%2520top%2520of%2520the%2520extracted%2520features.%2520We%2520validated%2520our%2520method%2520on%2520a%2520dataset%2520we%250Acompiled%2520%2528VID-AID%2529%252C%2520which%2520includes%2520around%252010%252C000%2520AI-generated%2520videos%2520produced%250Aby%25209%2520different%2520text-to-video%2520models%252C%2520along%2520with%25204%252C000%2520real%2520videos%252C%2520totaling%250Aover%25207%2520hours%2520of%2520video%2520content.%2520Our%2520evaluation%2520shows%2520that%2520our%2520approach%2520achieves%250Ahigh%2520detection%2520accuracy%252C%2520above%252090%2525%2520on%2520average%252C%2520underscoring%2520its%2520effectiveness.%250AUpon%2520acceptance%252C%2520we%2520plan%2520to%2520publicly%2520release%2520the%2520code%252C%2520the%2520pre-trained%2520models%252C%250Aand%2520our%2520dataset%2520to%2520support%2520ongoing%2520research%2520in%2520this%2520critical%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Pre-Trained%20Visual%20Models%20for%20AI-Generated%20Video%20Detection&entry.906535625=Keerthi%20Veeramachaneni%20and%20Praveen%20Tirupattur%20and%20Amrit%20Singh%20Bedi%20and%20Mubarak%20Shah&entry.1292438233=%20%20Recent%20advances%20in%20Generative%20AI%20%28GenAI%29%20have%20led%20to%20significant%20improvements%0Ain%20the%20quality%20of%20generated%20visual%20content.%20As%20AI-generated%20visual%20content%0Abecomes%20increasingly%20indistinguishable%20from%20real%20content%2C%20the%20challenge%20of%0Adetecting%20the%20generated%20content%20becomes%20critical%20in%20combating%20misinformation%2C%0Aensuring%20privacy%2C%20and%20preventing%20security%20threats.%20Although%20there%20has%20been%0Asubstantial%20progress%20in%20detecting%20AI-generated%20images%2C%20current%20methods%20for%0Avideo%20detection%20are%20largely%20focused%20on%20deepfakes%2C%20which%20primarily%20involve%20human%0Afaces.%20However%2C%20the%20field%20of%20video%20generation%20has%20advanced%20beyond%20DeepFakes%2C%0Acreating%20an%20urgent%20need%20for%20methods%20capable%20of%20detecting%20AI-generated%20videos%0Awith%20generic%20content.%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%20approach%20that%0Aleverages%20pre-trained%20visual%20models%20to%20distinguish%20between%20real%20and%20generated%0Avideos.%20The%20features%20extracted%20from%20these%20pre-trained%20models%2C%20which%20have%20been%0Atrained%20on%20extensive%20real%20visual%20content%2C%20contain%20inherent%20signals%20that%20can%0Ahelp%20distinguish%20real%20from%20generated%20videos.%20Using%20these%20extracted%20features%2C%20we%0Aachieve%20high%20detection%20performance%20without%20requiring%20additional%20model%20training%2C%0Aand%20we%20further%20improve%20performance%20by%20training%20a%20simple%20linear%20classification%0Alayer%20on%20top%20of%20the%20extracted%20features.%20We%20validated%20our%20method%20on%20a%20dataset%20we%0Acompiled%20%28VID-AID%29%2C%20which%20includes%20around%2010%2C000%20AI-generated%20videos%20produced%0Aby%209%20different%20text-to-video%20models%2C%20along%20with%204%2C000%20real%20videos%2C%20totaling%0Aover%207%20hours%20of%20video%20content.%20Our%20evaluation%20shows%20that%20our%20approach%20achieves%0Ahigh%20detection%20accuracy%2C%20above%2090%25%20on%20average%2C%20underscoring%20its%20effectiveness.%0AUpon%20acceptance%2C%20we%20plan%20to%20publicly%20release%20the%20code%2C%20the%20pre-trained%20models%2C%0Aand%20our%20dataset%20to%20support%20ongoing%20research%20in%20this%20critical%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13224v1&entry.124074799=Read"},
{"title": "Aligning Information Capacity Between Vision and Language via\n  Dense-to-Sparse Feature Distillation for Image-Text Matching", "author": "Yang Liu and Wentao Feng and Zhuoyao Liu and Shudong Huang and Jiancheng Lv", "abstract": "  Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2503.14953v2", "date": "2025-07-17", "relevancy": 2.8929, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Information%20Capacity%20Between%20Vision%20and%20Language%20via%0A%20%20Dense-to-Sparse%20Feature%20Distillation%20for%20Image-Text%20Matching&body=Title%3A%20Aligning%20Information%20Capacity%20Between%20Vision%20and%20Language%20via%0A%20%20Dense-to-Sparse%20Feature%20Distillation%20for%20Image-Text%20Matching%0AAuthor%3A%20Yang%20Liu%20and%20Wentao%20Feng%20and%20Zhuoyao%20Liu%20and%20Shudong%20Huang%20and%20Jiancheng%20Lv%0AAbstract%3A%20%20%20Enabling%20Visual%20Semantic%20Models%20to%20effectively%20handle%20multi-view%20description%0Amatching%20has%20been%20a%20longstanding%20challenge.%20Existing%20methods%20typically%20learn%20a%0Aset%20of%20embeddings%20to%20find%20the%20optimal%20match%20for%20each%20view%27s%20text%20and%20compute%0Asimilarity.%20However%2C%20the%20visual%20and%20text%20embeddings%20learned%20through%20these%0Aapproaches%20have%20limited%20information%20capacity%20and%20are%20prone%20to%20interference%20from%0Alocally%20similar%20negative%20samples.%20To%20address%20this%20issue%2C%20we%20argue%20that%20the%0Ainformation%20capacity%20of%20embeddings%20is%20crucial%20and%20propose%20Dense-to-Sparse%0AFeature%20Distilled%20Visual%20Semantic%20Embedding%20%28D2S-VSE%29%2C%20which%20enhances%20the%0Ainformation%20capacity%20of%20sparse%20text%20by%20leveraging%20dense%20text%20distillation.%0ASpecifically%2C%20D2S-VSE%20is%20a%20two-stage%20framework.%20In%20the%20pre-training%20stage%2C%20we%0Aalign%20images%20with%20dense%20text%20to%20enhance%20the%20information%20capacity%20of%20visual%0Asemantic%20embeddings.%20In%20the%20fine-tuning%20stage%2C%20we%20optimize%20two%20tasks%0Asimultaneously%2C%20distilling%20dense%20text%20embeddings%20to%20sparse%20text%20embeddings%0Awhile%20aligning%20images%20and%20sparse%20texts%2C%20enhancing%20the%20information%20capacity%20of%0Asparse%20text%20embeddings.%20Our%20proposed%20D2S-VSE%20model%20is%20extensively%20evaluated%20on%0Athe%20large-scale%20MS-COCO%20and%20Flickr30K%20datasets%2C%20demonstrating%20its%20superiority%0Aover%20recent%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Information%2520Capacity%2520Between%2520Vision%2520and%2520Language%2520via%250A%2520%2520Dense-to-Sparse%2520Feature%2520Distillation%2520for%2520Image-Text%2520Matching%26entry.906535625%3DYang%2520Liu%2520and%2520Wentao%2520Feng%2520and%2520Zhuoyao%2520Liu%2520and%2520Shudong%2520Huang%2520and%2520Jiancheng%2520Lv%26entry.1292438233%3D%2520%2520Enabling%2520Visual%2520Semantic%2520Models%2520to%2520effectively%2520handle%2520multi-view%2520description%250Amatching%2520has%2520been%2520a%2520longstanding%2520challenge.%2520Existing%2520methods%2520typically%2520learn%2520a%250Aset%2520of%2520embeddings%2520to%2520find%2520the%2520optimal%2520match%2520for%2520each%2520view%2527s%2520text%2520and%2520compute%250Asimilarity.%2520However%252C%2520the%2520visual%2520and%2520text%2520embeddings%2520learned%2520through%2520these%250Aapproaches%2520have%2520limited%2520information%2520capacity%2520and%2520are%2520prone%2520to%2520interference%2520from%250Alocally%2520similar%2520negative%2520samples.%2520To%2520address%2520this%2520issue%252C%2520we%2520argue%2520that%2520the%250Ainformation%2520capacity%2520of%2520embeddings%2520is%2520crucial%2520and%2520propose%2520Dense-to-Sparse%250AFeature%2520Distilled%2520Visual%2520Semantic%2520Embedding%2520%2528D2S-VSE%2529%252C%2520which%2520enhances%2520the%250Ainformation%2520capacity%2520of%2520sparse%2520text%2520by%2520leveraging%2520dense%2520text%2520distillation.%250ASpecifically%252C%2520D2S-VSE%2520is%2520a%2520two-stage%2520framework.%2520In%2520the%2520pre-training%2520stage%252C%2520we%250Aalign%2520images%2520with%2520dense%2520text%2520to%2520enhance%2520the%2520information%2520capacity%2520of%2520visual%250Asemantic%2520embeddings.%2520In%2520the%2520fine-tuning%2520stage%252C%2520we%2520optimize%2520two%2520tasks%250Asimultaneously%252C%2520distilling%2520dense%2520text%2520embeddings%2520to%2520sparse%2520text%2520embeddings%250Awhile%2520aligning%2520images%2520and%2520sparse%2520texts%252C%2520enhancing%2520the%2520information%2520capacity%2520of%250Asparse%2520text%2520embeddings.%2520Our%2520proposed%2520D2S-VSE%2520model%2520is%2520extensively%2520evaluated%2520on%250Athe%2520large-scale%2520MS-COCO%2520and%2520Flickr30K%2520datasets%252C%2520demonstrating%2520its%2520superiority%250Aover%2520recent%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Information%20Capacity%20Between%20Vision%20and%20Language%20via%0A%20%20Dense-to-Sparse%20Feature%20Distillation%20for%20Image-Text%20Matching&entry.906535625=Yang%20Liu%20and%20Wentao%20Feng%20and%20Zhuoyao%20Liu%20and%20Shudong%20Huang%20and%20Jiancheng%20Lv&entry.1292438233=%20%20Enabling%20Visual%20Semantic%20Models%20to%20effectively%20handle%20multi-view%20description%0Amatching%20has%20been%20a%20longstanding%20challenge.%20Existing%20methods%20typically%20learn%20a%0Aset%20of%20embeddings%20to%20find%20the%20optimal%20match%20for%20each%20view%27s%20text%20and%20compute%0Asimilarity.%20However%2C%20the%20visual%20and%20text%20embeddings%20learned%20through%20these%0Aapproaches%20have%20limited%20information%20capacity%20and%20are%20prone%20to%20interference%20from%0Alocally%20similar%20negative%20samples.%20To%20address%20this%20issue%2C%20we%20argue%20that%20the%0Ainformation%20capacity%20of%20embeddings%20is%20crucial%20and%20propose%20Dense-to-Sparse%0AFeature%20Distilled%20Visual%20Semantic%20Embedding%20%28D2S-VSE%29%2C%20which%20enhances%20the%0Ainformation%20capacity%20of%20sparse%20text%20by%20leveraging%20dense%20text%20distillation.%0ASpecifically%2C%20D2S-VSE%20is%20a%20two-stage%20framework.%20In%20the%20pre-training%20stage%2C%20we%0Aalign%20images%20with%20dense%20text%20to%20enhance%20the%20information%20capacity%20of%20visual%0Asemantic%20embeddings.%20In%20the%20fine-tuning%20stage%2C%20we%20optimize%20two%20tasks%0Asimultaneously%2C%20distilling%20dense%20text%20embeddings%20to%20sparse%20text%20embeddings%0Awhile%20aligning%20images%20and%20sparse%20texts%2C%20enhancing%20the%20information%20capacity%20of%0Asparse%20text%20embeddings.%20Our%20proposed%20D2S-VSE%20model%20is%20extensively%20evaluated%20on%0Athe%20large-scale%20MS-COCO%20and%20Flickr30K%20datasets%2C%20demonstrating%20its%20superiority%0Aover%20recent%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14953v2&entry.124074799=Read"},
{"title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal\n  Conversations", "author": "Yiming Lei and Zhizheng Yang and Zeming Liu and Haitao Leng and Shaoguo Liu and Tingting Gao and Qingjie Liu and Yunhong Wang", "abstract": "  Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.\n", "link": "http://arxiv.org/abs/2505.23121v2", "date": "2025-07-17", "relevancy": 2.871, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextQFormer%3A%20A%20New%20Context%20Modeling%20Method%20for%20Multi-Turn%20Multi-Modal%0A%20%20Conversations&body=Title%3A%20ContextQFormer%3A%20A%20New%20Context%20Modeling%20Method%20for%20Multi-Turn%20Multi-Modal%0A%20%20Conversations%0AAuthor%3A%20Yiming%20Lei%20and%20Zhizheng%20Yang%20and%20Zeming%20Liu%20and%20Haitao%20Leng%20and%20Shaoguo%20Liu%20and%20Tingting%20Gao%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20have%20demonstrated%20remarkable%20zero-shot%0Aabilities%20and%20powerful%20image-understanding%20capabilities.%20However%2C%20the%20existing%0Aopen-source%20multi-modal%20models%20suffer%20from%20the%20weak%20capability%20of%20multi-turn%0Ainteraction%2C%20especially%20for%20long%20contexts.%20To%20address%20the%20issue%2C%20we%20first%0Aintroduce%20a%20context%20modeling%20module%2C%20termed%20ContextQFormer%2C%20which%20utilizes%20a%0Amemory%20block%20to%20enhance%20the%20presentation%20of%20contextual%20information.%0AFurthermore%2C%20to%20facilitate%20further%20research%2C%20we%20carefully%20build%20a%20new%0Amulti-turn%20multi-modal%20dialogue%20dataset%20%28TMDialog%29%20for%20pre-training%2C%0Ainstruction-tuning%2C%20and%20evaluation%2C%20which%20will%20be%20open-sourced%20lately.%20Compared%0Awith%20other%20multi-modal%20dialogue%20datasets%2C%20TMDialog%20contains%20longer%0Aconversations%2C%20which%20supports%20the%20research%20of%20multi-turn%20multi-modal%20dialogue.%0AIn%20addition%2C%20ContextQFormer%20is%20compared%20with%20three%20baselines%20on%20TMDialog%20and%0Aexperimental%20results%20illustrate%20that%20ContextQFormer%20achieves%20an%20improvement%20of%0A2%25-4%25%20in%20available%20rate%20over%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23121v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextQFormer%253A%2520A%2520New%2520Context%2520Modeling%2520Method%2520for%2520Multi-Turn%2520Multi-Modal%250A%2520%2520Conversations%26entry.906535625%3DYiming%2520Lei%2520and%2520Zhizheng%2520Yang%2520and%2520Zeming%2520Liu%2520and%2520Haitao%2520Leng%2520and%2520Shaoguo%2520Liu%2520and%2520Tingting%2520Gao%2520and%2520Qingjie%2520Liu%2520and%2520Yunhong%2520Wang%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520zero-shot%250Aabilities%2520and%2520powerful%2520image-understanding%2520capabilities.%2520However%252C%2520the%2520existing%250Aopen-source%2520multi-modal%2520models%2520suffer%2520from%2520the%2520weak%2520capability%2520of%2520multi-turn%250Ainteraction%252C%2520especially%2520for%2520long%2520contexts.%2520To%2520address%2520the%2520issue%252C%2520we%2520first%250Aintroduce%2520a%2520context%2520modeling%2520module%252C%2520termed%2520ContextQFormer%252C%2520which%2520utilizes%2520a%250Amemory%2520block%2520to%2520enhance%2520the%2520presentation%2520of%2520contextual%2520information.%250AFurthermore%252C%2520to%2520facilitate%2520further%2520research%252C%2520we%2520carefully%2520build%2520a%2520new%250Amulti-turn%2520multi-modal%2520dialogue%2520dataset%2520%2528TMDialog%2529%2520for%2520pre-training%252C%250Ainstruction-tuning%252C%2520and%2520evaluation%252C%2520which%2520will%2520be%2520open-sourced%2520lately.%2520Compared%250Awith%2520other%2520multi-modal%2520dialogue%2520datasets%252C%2520TMDialog%2520contains%2520longer%250Aconversations%252C%2520which%2520supports%2520the%2520research%2520of%2520multi-turn%2520multi-modal%2520dialogue.%250AIn%2520addition%252C%2520ContextQFormer%2520is%2520compared%2520with%2520three%2520baselines%2520on%2520TMDialog%2520and%250Aexperimental%2520results%2520illustrate%2520that%2520ContextQFormer%2520achieves%2520an%2520improvement%2520of%250A2%2525-4%2525%2520in%2520available%2520rate%2520over%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23121v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextQFormer%3A%20A%20New%20Context%20Modeling%20Method%20for%20Multi-Turn%20Multi-Modal%0A%20%20Conversations&entry.906535625=Yiming%20Lei%20and%20Zhizheng%20Yang%20and%20Zeming%20Liu%20and%20Haitao%20Leng%20and%20Shaoguo%20Liu%20and%20Tingting%20Gao%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20have%20demonstrated%20remarkable%20zero-shot%0Aabilities%20and%20powerful%20image-understanding%20capabilities.%20However%2C%20the%20existing%0Aopen-source%20multi-modal%20models%20suffer%20from%20the%20weak%20capability%20of%20multi-turn%0Ainteraction%2C%20especially%20for%20long%20contexts.%20To%20address%20the%20issue%2C%20we%20first%0Aintroduce%20a%20context%20modeling%20module%2C%20termed%20ContextQFormer%2C%20which%20utilizes%20a%0Amemory%20block%20to%20enhance%20the%20presentation%20of%20contextual%20information.%0AFurthermore%2C%20to%20facilitate%20further%20research%2C%20we%20carefully%20build%20a%20new%0Amulti-turn%20multi-modal%20dialogue%20dataset%20%28TMDialog%29%20for%20pre-training%2C%0Ainstruction-tuning%2C%20and%20evaluation%2C%20which%20will%20be%20open-sourced%20lately.%20Compared%0Awith%20other%20multi-modal%20dialogue%20datasets%2C%20TMDialog%20contains%20longer%0Aconversations%2C%20which%20supports%20the%20research%20of%20multi-turn%20multi-modal%20dialogue.%0AIn%20addition%2C%20ContextQFormer%20is%20compared%20with%20three%20baselines%20on%20TMDialog%20and%0Aexperimental%20results%20illustrate%20that%20ContextQFormer%20achieves%20an%20improvement%20of%0A2%25-4%25%20in%20available%20rate%20over%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23121v2&entry.124074799=Read"},
{"title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation", "author": "Junhong Min and Youngpil Jeon and Jimin Kim and Minyong Choi", "abstract": "  The pursuit of a generalizable stereo matching model, capable of performing\nacross varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization. On\nthe other hand, global matching architectures, while theoretically more robust,\nhave been historically rendered infeasible by prohibitive computational and\nmemory costs. We resolve this dilemma with $S^2M^2$: a global matching\narchitecture that achieves both state-of-the-art accuracy and high efficiency\nwithout relying on cost volume filtering or deep refinement stacks. Our design\nintegrates a multi-resolution transformer for robust long-range correspondence,\ntrained with a novel loss function that concentrates probability on feasible\nmatches. This approach enables a more robust joint estimation of disparity,\nocclusion, and confidence. $S^2M^2$ establishes a new state of the art on the\nMiddlebury v3 and ETH3D benchmarks, significantly outperforming prior methods\nacross most metrics while reconstructing high-quality details with competitive\nefficiency.\n", "link": "http://arxiv.org/abs/2507.13229v1", "date": "2025-07-17", "relevancy": 2.8062, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5631}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.563}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation&body=Title%3A%20%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation%0AAuthor%3A%20Junhong%20Min%20and%20Youngpil%20Jeon%20and%20Jimin%20Kim%20and%20Minyong%20Choi%0AAbstract%3A%20%20%20The%20pursuit%20of%20a%20generalizable%20stereo%20matching%20model%2C%20capable%20of%20performing%0Aacross%20varying%20resolutions%20and%20disparity%20ranges%20without%20dataset-specific%0Afine-tuning%2C%20has%20revealed%20a%20fundamental%20trade-off.%20Iterative%20local%20search%0Amethods%20achieve%20high%20scores%20on%20constrained%20benchmarks%2C%20but%20their%20core%20mechanism%0Ainherently%20limits%20the%20global%20consistency%20required%20for%20true%20generalization.%20On%0Athe%20other%20hand%2C%20global%20matching%20architectures%2C%20while%20theoretically%20more%20robust%2C%0Ahave%20been%20historically%20rendered%20infeasible%20by%20prohibitive%20computational%20and%0Amemory%20costs.%20We%20resolve%20this%20dilemma%20with%20%24S%5E2M%5E2%24%3A%20a%20global%20matching%0Aarchitecture%20that%20achieves%20both%20state-of-the-art%20accuracy%20and%20high%20efficiency%0Awithout%20relying%20on%20cost%20volume%20filtering%20or%20deep%20refinement%20stacks.%20Our%20design%0Aintegrates%20a%20multi-resolution%20transformer%20for%20robust%20long-range%20correspondence%2C%0Atrained%20with%20a%20novel%20loss%20function%20that%20concentrates%20probability%20on%20feasible%0Amatches.%20This%20approach%20enables%20a%20more%20robust%20joint%20estimation%20of%20disparity%2C%0Aocclusion%2C%20and%20confidence.%20%24S%5E2M%5E2%24%20establishes%20a%20new%20state%20of%20the%20art%20on%20the%0AMiddlebury%20v3%20and%20ETH3D%20benchmarks%2C%20significantly%20outperforming%20prior%20methods%0Aacross%20most%20metrics%20while%20reconstructing%20high-quality%20details%20with%20competitive%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524S%255E2M%255E2%2524%253A%2520Scalable%2520Stereo%2520Matching%2520Model%2520for%2520Reliable%2520Depth%2520Estimation%26entry.906535625%3DJunhong%2520Min%2520and%2520Youngpil%2520Jeon%2520and%2520Jimin%2520Kim%2520and%2520Minyong%2520Choi%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520a%2520generalizable%2520stereo%2520matching%2520model%252C%2520capable%2520of%2520performing%250Aacross%2520varying%2520resolutions%2520and%2520disparity%2520ranges%2520without%2520dataset-specific%250Afine-tuning%252C%2520has%2520revealed%2520a%2520fundamental%2520trade-off.%2520Iterative%2520local%2520search%250Amethods%2520achieve%2520high%2520scores%2520on%2520constrained%2520benchmarks%252C%2520but%2520their%2520core%2520mechanism%250Ainherently%2520limits%2520the%2520global%2520consistency%2520required%2520for%2520true%2520generalization.%2520On%250Athe%2520other%2520hand%252C%2520global%2520matching%2520architectures%252C%2520while%2520theoretically%2520more%2520robust%252C%250Ahave%2520been%2520historically%2520rendered%2520infeasible%2520by%2520prohibitive%2520computational%2520and%250Amemory%2520costs.%2520We%2520resolve%2520this%2520dilemma%2520with%2520%2524S%255E2M%255E2%2524%253A%2520a%2520global%2520matching%250Aarchitecture%2520that%2520achieves%2520both%2520state-of-the-art%2520accuracy%2520and%2520high%2520efficiency%250Awithout%2520relying%2520on%2520cost%2520volume%2520filtering%2520or%2520deep%2520refinement%2520stacks.%2520Our%2520design%250Aintegrates%2520a%2520multi-resolution%2520transformer%2520for%2520robust%2520long-range%2520correspondence%252C%250Atrained%2520with%2520a%2520novel%2520loss%2520function%2520that%2520concentrates%2520probability%2520on%2520feasible%250Amatches.%2520This%2520approach%2520enables%2520a%2520more%2520robust%2520joint%2520estimation%2520of%2520disparity%252C%250Aocclusion%252C%2520and%2520confidence.%2520%2524S%255E2M%255E2%2524%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520the%250AMiddlebury%2520v3%2520and%2520ETH3D%2520benchmarks%252C%2520significantly%2520outperforming%2520prior%2520methods%250Aacross%2520most%2520metrics%2520while%2520reconstructing%2520high-quality%2520details%2520with%2520competitive%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation&entry.906535625=Junhong%20Min%20and%20Youngpil%20Jeon%20and%20Jimin%20Kim%20and%20Minyong%20Choi&entry.1292438233=%20%20The%20pursuit%20of%20a%20generalizable%20stereo%20matching%20model%2C%20capable%20of%20performing%0Aacross%20varying%20resolutions%20and%20disparity%20ranges%20without%20dataset-specific%0Afine-tuning%2C%20has%20revealed%20a%20fundamental%20trade-off.%20Iterative%20local%20search%0Amethods%20achieve%20high%20scores%20on%20constrained%20benchmarks%2C%20but%20their%20core%20mechanism%0Ainherently%20limits%20the%20global%20consistency%20required%20for%20true%20generalization.%20On%0Athe%20other%20hand%2C%20global%20matching%20architectures%2C%20while%20theoretically%20more%20robust%2C%0Ahave%20been%20historically%20rendered%20infeasible%20by%20prohibitive%20computational%20and%0Amemory%20costs.%20We%20resolve%20this%20dilemma%20with%20%24S%5E2M%5E2%24%3A%20a%20global%20matching%0Aarchitecture%20that%20achieves%20both%20state-of-the-art%20accuracy%20and%20high%20efficiency%0Awithout%20relying%20on%20cost%20volume%20filtering%20or%20deep%20refinement%20stacks.%20Our%20design%0Aintegrates%20a%20multi-resolution%20transformer%20for%20robust%20long-range%20correspondence%2C%0Atrained%20with%20a%20novel%20loss%20function%20that%20concentrates%20probability%20on%20feasible%0Amatches.%20This%20approach%20enables%20a%20more%20robust%20joint%20estimation%20of%20disparity%2C%0Aocclusion%2C%20and%20confidence.%20%24S%5E2M%5E2%24%20establishes%20a%20new%20state%20of%20the%20art%20on%20the%0AMiddlebury%20v3%20and%20ETH3D%20benchmarks%2C%20significantly%20outperforming%20prior%20methods%0Aacross%20most%20metrics%20while%20reconstructing%20high-quality%20details%20with%20competitive%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13229v1&entry.124074799=Read"},
{"title": "3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via\n  Keypoint-Guided Point Clustering", "author": "Zi Wang and Katsuya Hotta and Koichiro Kamide and Yawen Zou and Chao Zhang and Jun Yu", "abstract": "  High-resolution 3D point clouds are highly effective for detecting subtle\nstructural anomalies in industrial inspection. However, their dense and\nirregular nature imposes significant challenges, including high computational\ncost, sensitivity to spatial misalignment, and difficulty in capturing\nlocalized structural differences. This paper introduces a registration-based\nanomaly detection framework that combines multi-prototype alignment with\ncluster-wise discrepancy analysis to enable precise 3D anomaly localization.\nSpecifically, each test sample is first registered to multiple normal\nprototypes to enable direct structural comparison. To evaluate anomalies at a\nlocal level, clustering is performed over the point cloud, and similarity is\ncomputed between features from the test sample and the prototypes within each\ncluster. Rather than selecting cluster centroids randomly, a keypoint-guided\nstrategy is employed, where geometrically informative points are chosen as\ncentroids. This ensures that clusters are centered on feature-rich regions,\nenabling more meaningful and stable distance-based comparisons. Extensive\nexperiments on the Real3D-AD benchmark demonstrate that the proposed method\nachieves state-of-the-art performance in both object-level and point-level\nanomaly detection, even using only raw features.\n", "link": "http://arxiv.org/abs/2507.13110v1", "date": "2025-07-17", "relevancy": 2.7489, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6205}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DKeyAD%3A%20High-Resolution%203D%20Point%20Cloud%20Anomaly%20Detection%20via%0A%20%20Keypoint-Guided%20Point%20Clustering&body=Title%3A%203DKeyAD%3A%20High-Resolution%203D%20Point%20Cloud%20Anomaly%20Detection%20via%0A%20%20Keypoint-Guided%20Point%20Clustering%0AAuthor%3A%20Zi%20Wang%20and%20Katsuya%20Hotta%20and%20Koichiro%20Kamide%20and%20Yawen%20Zou%20and%20Chao%20Zhang%20and%20Jun%20Yu%0AAbstract%3A%20%20%20High-resolution%203D%20point%20clouds%20are%20highly%20effective%20for%20detecting%20subtle%0Astructural%20anomalies%20in%20industrial%20inspection.%20However%2C%20their%20dense%20and%0Airregular%20nature%20imposes%20significant%20challenges%2C%20including%20high%20computational%0Acost%2C%20sensitivity%20to%20spatial%20misalignment%2C%20and%20difficulty%20in%20capturing%0Alocalized%20structural%20differences.%20This%20paper%20introduces%20a%20registration-based%0Aanomaly%20detection%20framework%20that%20combines%20multi-prototype%20alignment%20with%0Acluster-wise%20discrepancy%20analysis%20to%20enable%20precise%203D%20anomaly%20localization.%0ASpecifically%2C%20each%20test%20sample%20is%20first%20registered%20to%20multiple%20normal%0Aprototypes%20to%20enable%20direct%20structural%20comparison.%20To%20evaluate%20anomalies%20at%20a%0Alocal%20level%2C%20clustering%20is%20performed%20over%20the%20point%20cloud%2C%20and%20similarity%20is%0Acomputed%20between%20features%20from%20the%20test%20sample%20and%20the%20prototypes%20within%20each%0Acluster.%20Rather%20than%20selecting%20cluster%20centroids%20randomly%2C%20a%20keypoint-guided%0Astrategy%20is%20employed%2C%20where%20geometrically%20informative%20points%20are%20chosen%20as%0Acentroids.%20This%20ensures%20that%20clusters%20are%20centered%20on%20feature-rich%20regions%2C%0Aenabling%20more%20meaningful%20and%20stable%20distance-based%20comparisons.%20Extensive%0Aexperiments%20on%20the%20Real3D-AD%20benchmark%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance%20in%20both%20object-level%20and%20point-level%0Aanomaly%20detection%2C%20even%20using%20only%20raw%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DKeyAD%253A%2520High-Resolution%25203D%2520Point%2520Cloud%2520Anomaly%2520Detection%2520via%250A%2520%2520Keypoint-Guided%2520Point%2520Clustering%26entry.906535625%3DZi%2520Wang%2520and%2520Katsuya%2520Hotta%2520and%2520Koichiro%2520Kamide%2520and%2520Yawen%2520Zou%2520and%2520Chao%2520Zhang%2520and%2520Jun%2520Yu%26entry.1292438233%3D%2520%2520High-resolution%25203D%2520point%2520clouds%2520are%2520highly%2520effective%2520for%2520detecting%2520subtle%250Astructural%2520anomalies%2520in%2520industrial%2520inspection.%2520However%252C%2520their%2520dense%2520and%250Airregular%2520nature%2520imposes%2520significant%2520challenges%252C%2520including%2520high%2520computational%250Acost%252C%2520sensitivity%2520to%2520spatial%2520misalignment%252C%2520and%2520difficulty%2520in%2520capturing%250Alocalized%2520structural%2520differences.%2520This%2520paper%2520introduces%2520a%2520registration-based%250Aanomaly%2520detection%2520framework%2520that%2520combines%2520multi-prototype%2520alignment%2520with%250Acluster-wise%2520discrepancy%2520analysis%2520to%2520enable%2520precise%25203D%2520anomaly%2520localization.%250ASpecifically%252C%2520each%2520test%2520sample%2520is%2520first%2520registered%2520to%2520multiple%2520normal%250Aprototypes%2520to%2520enable%2520direct%2520structural%2520comparison.%2520To%2520evaluate%2520anomalies%2520at%2520a%250Alocal%2520level%252C%2520clustering%2520is%2520performed%2520over%2520the%2520point%2520cloud%252C%2520and%2520similarity%2520is%250Acomputed%2520between%2520features%2520from%2520the%2520test%2520sample%2520and%2520the%2520prototypes%2520within%2520each%250Acluster.%2520Rather%2520than%2520selecting%2520cluster%2520centroids%2520randomly%252C%2520a%2520keypoint-guided%250Astrategy%2520is%2520employed%252C%2520where%2520geometrically%2520informative%2520points%2520are%2520chosen%2520as%250Acentroids.%2520This%2520ensures%2520that%2520clusters%2520are%2520centered%2520on%2520feature-rich%2520regions%252C%250Aenabling%2520more%2520meaningful%2520and%2520stable%2520distance-based%2520comparisons.%2520Extensive%250Aexperiments%2520on%2520the%2520Real3D-AD%2520benchmark%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aachieves%2520state-of-the-art%2520performance%2520in%2520both%2520object-level%2520and%2520point-level%250Aanomaly%2520detection%252C%2520even%2520using%2520only%2520raw%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DKeyAD%3A%20High-Resolution%203D%20Point%20Cloud%20Anomaly%20Detection%20via%0A%20%20Keypoint-Guided%20Point%20Clustering&entry.906535625=Zi%20Wang%20and%20Katsuya%20Hotta%20and%20Koichiro%20Kamide%20and%20Yawen%20Zou%20and%20Chao%20Zhang%20and%20Jun%20Yu&entry.1292438233=%20%20High-resolution%203D%20point%20clouds%20are%20highly%20effective%20for%20detecting%20subtle%0Astructural%20anomalies%20in%20industrial%20inspection.%20However%2C%20their%20dense%20and%0Airregular%20nature%20imposes%20significant%20challenges%2C%20including%20high%20computational%0Acost%2C%20sensitivity%20to%20spatial%20misalignment%2C%20and%20difficulty%20in%20capturing%0Alocalized%20structural%20differences.%20This%20paper%20introduces%20a%20registration-based%0Aanomaly%20detection%20framework%20that%20combines%20multi-prototype%20alignment%20with%0Acluster-wise%20discrepancy%20analysis%20to%20enable%20precise%203D%20anomaly%20localization.%0ASpecifically%2C%20each%20test%20sample%20is%20first%20registered%20to%20multiple%20normal%0Aprototypes%20to%20enable%20direct%20structural%20comparison.%20To%20evaluate%20anomalies%20at%20a%0Alocal%20level%2C%20clustering%20is%20performed%20over%20the%20point%20cloud%2C%20and%20similarity%20is%0Acomputed%20between%20features%20from%20the%20test%20sample%20and%20the%20prototypes%20within%20each%0Acluster.%20Rather%20than%20selecting%20cluster%20centroids%20randomly%2C%20a%20keypoint-guided%0Astrategy%20is%20employed%2C%20where%20geometrically%20informative%20points%20are%20chosen%20as%0Acentroids.%20This%20ensures%20that%20clusters%20are%20centered%20on%20feature-rich%20regions%2C%0Aenabling%20more%20meaningful%20and%20stable%20distance-based%20comparisons.%20Extensive%0Aexperiments%20on%20the%20Real3D-AD%20benchmark%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance%20in%20both%20object-level%20and%20point-level%0Aanomaly%20detection%2C%20even%20using%20only%20raw%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13110v1&entry.124074799=Read"},
{"title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It", "author": "Yulu Qin and Dheeraj Varghese and Adam Dahlgren Lindstr\u00f6m and Lucia Donatelli and Kanishka Misra and Najoung Kim", "abstract": "  Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.\n", "link": "http://arxiv.org/abs/2507.13328v1", "date": "2025-07-17", "relevancy": 2.6792, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-and-Language%20Training%20Helps%20Deploy%20Taxonomic%20Knowledge%20but%20Does%0A%20%20Not%20Fundamentally%20Alter%20It&body=Title%3A%20Vision-and-Language%20Training%20Helps%20Deploy%20Taxonomic%20Knowledge%20but%20Does%0A%20%20Not%20Fundamentally%20Alter%20It%0AAuthor%3A%20Yulu%20Qin%20and%20Dheeraj%20Varghese%20and%20Adam%20Dahlgren%20Lindstr%C3%B6m%20and%20Lucia%20Donatelli%20and%20Kanishka%20Misra%20and%20Najoung%20Kim%0AAbstract%3A%20%20%20Does%20vision-and-language%20%28VL%29%20training%20change%20the%20linguistic%20representations%0Aof%20language%20models%20in%20meaningful%20ways%3F%20Most%20results%20in%20the%20literature%20have%0Ashown%20inconsistent%20or%20marginal%20differences%2C%20both%20behaviorally%20and%0Arepresentationally.%20In%20this%20work%2C%20we%20start%20from%20the%20hypothesis%20that%20the%20domain%0Ain%20which%20VL%20training%20could%20have%20a%20significant%20effect%20is%20lexical-conceptual%0Aknowledge%2C%20in%20particular%20its%20taxonomic%20organization.%20Through%20comparing%20minimal%0Apairs%20of%20text-only%20LMs%20and%20their%20VL-trained%20counterparts%2C%20we%20first%20show%20that%0Athe%20VL%20models%20often%20outperform%20their%20text-only%20counterparts%20on%20a%20text-only%0Aquestion-answering%20task%20that%20requires%20taxonomic%20understanding%20of%20concepts%0Amentioned%20in%20the%20questions.%20Using%20an%20array%20of%20targeted%20behavioral%20and%0Arepresentational%20analyses%2C%20we%20show%20that%20the%20LMs%20and%20VLMs%20do%20not%20differ%0Asignificantly%20in%20terms%20of%20their%20taxonomic%20knowledge%20itself%2C%20but%20they%20differ%20in%0Ahow%20they%20represent%20questions%20that%20contain%20concepts%20in%20a%20taxonomic%20relation%20vs.%0Aa%20non-taxonomic%20relation.%20This%20implies%20that%20the%20taxonomic%20knowledge%20itself%20does%0Anot%20change%20substantially%20through%20additional%20VL%20training%2C%20but%20VL%20training%20does%0Aimprove%20the%20deployment%20of%20this%20knowledge%20in%20the%20context%20of%20a%20specific%20task%2C%0Aeven%20when%20the%20presentation%20of%20the%20task%20is%20purely%20linguistic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-and-Language%2520Training%2520Helps%2520Deploy%2520Taxonomic%2520Knowledge%2520but%2520Does%250A%2520%2520Not%2520Fundamentally%2520Alter%2520It%26entry.906535625%3DYulu%2520Qin%2520and%2520Dheeraj%2520Varghese%2520and%2520Adam%2520Dahlgren%2520Lindstr%25C3%25B6m%2520and%2520Lucia%2520Donatelli%2520and%2520Kanishka%2520Misra%2520and%2520Najoung%2520Kim%26entry.1292438233%3D%2520%2520Does%2520vision-and-language%2520%2528VL%2529%2520training%2520change%2520the%2520linguistic%2520representations%250Aof%2520language%2520models%2520in%2520meaningful%2520ways%253F%2520Most%2520results%2520in%2520the%2520literature%2520have%250Ashown%2520inconsistent%2520or%2520marginal%2520differences%252C%2520both%2520behaviorally%2520and%250Arepresentationally.%2520In%2520this%2520work%252C%2520we%2520start%2520from%2520the%2520hypothesis%2520that%2520the%2520domain%250Ain%2520which%2520VL%2520training%2520could%2520have%2520a%2520significant%2520effect%2520is%2520lexical-conceptual%250Aknowledge%252C%2520in%2520particular%2520its%2520taxonomic%2520organization.%2520Through%2520comparing%2520minimal%250Apairs%2520of%2520text-only%2520LMs%2520and%2520their%2520VL-trained%2520counterparts%252C%2520we%2520first%2520show%2520that%250Athe%2520VL%2520models%2520often%2520outperform%2520their%2520text-only%2520counterparts%2520on%2520a%2520text-only%250Aquestion-answering%2520task%2520that%2520requires%2520taxonomic%2520understanding%2520of%2520concepts%250Amentioned%2520in%2520the%2520questions.%2520Using%2520an%2520array%2520of%2520targeted%2520behavioral%2520and%250Arepresentational%2520analyses%252C%2520we%2520show%2520that%2520the%2520LMs%2520and%2520VLMs%2520do%2520not%2520differ%250Asignificantly%2520in%2520terms%2520of%2520their%2520taxonomic%2520knowledge%2520itself%252C%2520but%2520they%2520differ%2520in%250Ahow%2520they%2520represent%2520questions%2520that%2520contain%2520concepts%2520in%2520a%2520taxonomic%2520relation%2520vs.%250Aa%2520non-taxonomic%2520relation.%2520This%2520implies%2520that%2520the%2520taxonomic%2520knowledge%2520itself%2520does%250Anot%2520change%2520substantially%2520through%2520additional%2520VL%2520training%252C%2520but%2520VL%2520training%2520does%250Aimprove%2520the%2520deployment%2520of%2520this%2520knowledge%2520in%2520the%2520context%2520of%2520a%2520specific%2520task%252C%250Aeven%2520when%2520the%2520presentation%2520of%2520the%2520task%2520is%2520purely%2520linguistic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-and-Language%20Training%20Helps%20Deploy%20Taxonomic%20Knowledge%20but%20Does%0A%20%20Not%20Fundamentally%20Alter%20It&entry.906535625=Yulu%20Qin%20and%20Dheeraj%20Varghese%20and%20Adam%20Dahlgren%20Lindstr%C3%B6m%20and%20Lucia%20Donatelli%20and%20Kanishka%20Misra%20and%20Najoung%20Kim&entry.1292438233=%20%20Does%20vision-and-language%20%28VL%29%20training%20change%20the%20linguistic%20representations%0Aof%20language%20models%20in%20meaningful%20ways%3F%20Most%20results%20in%20the%20literature%20have%0Ashown%20inconsistent%20or%20marginal%20differences%2C%20both%20behaviorally%20and%0Arepresentationally.%20In%20this%20work%2C%20we%20start%20from%20the%20hypothesis%20that%20the%20domain%0Ain%20which%20VL%20training%20could%20have%20a%20significant%20effect%20is%20lexical-conceptual%0Aknowledge%2C%20in%20particular%20its%20taxonomic%20organization.%20Through%20comparing%20minimal%0Apairs%20of%20text-only%20LMs%20and%20their%20VL-trained%20counterparts%2C%20we%20first%20show%20that%0Athe%20VL%20models%20often%20outperform%20their%20text-only%20counterparts%20on%20a%20text-only%0Aquestion-answering%20task%20that%20requires%20taxonomic%20understanding%20of%20concepts%0Amentioned%20in%20the%20questions.%20Using%20an%20array%20of%20targeted%20behavioral%20and%0Arepresentational%20analyses%2C%20we%20show%20that%20the%20LMs%20and%20VLMs%20do%20not%20differ%0Asignificantly%20in%20terms%20of%20their%20taxonomic%20knowledge%20itself%2C%20but%20they%20differ%20in%0Ahow%20they%20represent%20questions%20that%20contain%20concepts%20in%20a%20taxonomic%20relation%20vs.%0Aa%20non-taxonomic%20relation.%20This%20implies%20that%20the%20taxonomic%20knowledge%20itself%20does%0Anot%20change%20substantially%20through%20additional%20VL%20training%2C%20but%20VL%20training%20does%0Aimprove%20the%20deployment%20of%20this%20knowledge%20in%20the%20context%20of%20a%20specific%20task%2C%0Aeven%20when%20the%20presentation%20of%20the%20task%20is%20purely%20linguistic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13328v1&entry.124074799=Read"},
{"title": "Teach Old SAEs New Domain Tricks with Boosting", "author": "Nikita Koriagin and Yaroslav Aksenov and Daniil Laptev and Gleb Gerasimov and Nikita Balagansky and Daniil Gavrilov", "abstract": "  Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.\n", "link": "http://arxiv.org/abs/2507.12990v1", "date": "2025-07-17", "relevancy": 2.649, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teach%20Old%20SAEs%20New%20Domain%20Tricks%20with%20Boosting&body=Title%3A%20Teach%20Old%20SAEs%20New%20Domain%20Tricks%20with%20Boosting%0AAuthor%3A%20Nikita%20Koriagin%20and%20Yaroslav%20Aksenov%20and%20Daniil%20Laptev%20and%20Gleb%20Gerasimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20have%20emerged%20as%20powerful%20tools%20for%20interpreting%20the%0Ainternal%20representations%20of%20Large%20Language%20Models%2C%20yet%20they%20often%20fail%20to%0Acapture%20domain-specific%20features%20not%20prevalent%20in%20their%20training%20corpora.%20This%0Apaper%20introduces%20a%20residual%20learning%20approach%20that%20addresses%20this%20feature%0Ablindness%20without%20requiring%20complete%20retraining.%20We%20propose%20training%20a%0Asecondary%20SAE%20specifically%20to%20model%20the%20reconstruction%20error%20of%20a%20pretrained%0ASAE%20on%20domain-specific%20texts%2C%20effectively%20capturing%20features%20missed%20by%20the%0Aprimary%20model.%20By%20summing%20the%20outputs%20of%20both%20models%20during%20inference%2C%20we%0Ademonstrate%20significant%20improvements%20in%20both%20LLM%20cross-entropy%20and%20explained%0Avariance%20metrics%20across%20multiple%20specialized%20domains.%20Our%20experiments%20show%20that%0Athis%20method%20efficiently%20incorporates%20new%20domain%20knowledge%20into%20existing%20SAEs%0Awhile%20maintaining%20their%20performance%20on%20general%20tasks.%20This%20approach%20enables%0Aresearchers%20to%20selectively%20enhance%20SAE%20interpretability%20for%20specific%20domains%20of%0Ainterest%2C%20opening%20new%20possibilities%20for%20targeted%20mechanistic%20interpretability%0Aof%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeach%2520Old%2520SAEs%2520New%2520Domain%2520Tricks%2520with%2520Boosting%26entry.906535625%3DNikita%2520Koriagin%2520and%2520Yaroslav%2520Aksenov%2520and%2520Daniil%2520Laptev%2520and%2520Gleb%2520Gerasimov%2520and%2520Nikita%2520Balagansky%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520interpreting%2520the%250Ainternal%2520representations%2520of%2520Large%2520Language%2520Models%252C%2520yet%2520they%2520often%2520fail%2520to%250Acapture%2520domain-specific%2520features%2520not%2520prevalent%2520in%2520their%2520training%2520corpora.%2520This%250Apaper%2520introduces%2520a%2520residual%2520learning%2520approach%2520that%2520addresses%2520this%2520feature%250Ablindness%2520without%2520requiring%2520complete%2520retraining.%2520We%2520propose%2520training%2520a%250Asecondary%2520SAE%2520specifically%2520to%2520model%2520the%2520reconstruction%2520error%2520of%2520a%2520pretrained%250ASAE%2520on%2520domain-specific%2520texts%252C%2520effectively%2520capturing%2520features%2520missed%2520by%2520the%250Aprimary%2520model.%2520By%2520summing%2520the%2520outputs%2520of%2520both%2520models%2520during%2520inference%252C%2520we%250Ademonstrate%2520significant%2520improvements%2520in%2520both%2520LLM%2520cross-entropy%2520and%2520explained%250Avariance%2520metrics%2520across%2520multiple%2520specialized%2520domains.%2520Our%2520experiments%2520show%2520that%250Athis%2520method%2520efficiently%2520incorporates%2520new%2520domain%2520knowledge%2520into%2520existing%2520SAEs%250Awhile%2520maintaining%2520their%2520performance%2520on%2520general%2520tasks.%2520This%2520approach%2520enables%250Aresearchers%2520to%2520selectively%2520enhance%2520SAE%2520interpretability%2520for%2520specific%2520domains%2520of%250Ainterest%252C%2520opening%2520new%2520possibilities%2520for%2520targeted%2520mechanistic%2520interpretability%250Aof%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teach%20Old%20SAEs%20New%20Domain%20Tricks%20with%20Boosting&entry.906535625=Nikita%20Koriagin%20and%20Yaroslav%20Aksenov%20and%20Daniil%20Laptev%20and%20Gleb%20Gerasimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20Sparse%20Autoencoders%20have%20emerged%20as%20powerful%20tools%20for%20interpreting%20the%0Ainternal%20representations%20of%20Large%20Language%20Models%2C%20yet%20they%20often%20fail%20to%0Acapture%20domain-specific%20features%20not%20prevalent%20in%20their%20training%20corpora.%20This%0Apaper%20introduces%20a%20residual%20learning%20approach%20that%20addresses%20this%20feature%0Ablindness%20without%20requiring%20complete%20retraining.%20We%20propose%20training%20a%0Asecondary%20SAE%20specifically%20to%20model%20the%20reconstruction%20error%20of%20a%20pretrained%0ASAE%20on%20domain-specific%20texts%2C%20effectively%20capturing%20features%20missed%20by%20the%0Aprimary%20model.%20By%20summing%20the%20outputs%20of%20both%20models%20during%20inference%2C%20we%0Ademonstrate%20significant%20improvements%20in%20both%20LLM%20cross-entropy%20and%20explained%0Avariance%20metrics%20across%20multiple%20specialized%20domains.%20Our%20experiments%20show%20that%0Athis%20method%20efficiently%20incorporates%20new%20domain%20knowledge%20into%20existing%20SAEs%0Awhile%20maintaining%20their%20performance%20on%20general%20tasks.%20This%20approach%20enables%0Aresearchers%20to%20selectively%20enhance%20SAE%20interpretability%20for%20specific%20domains%20of%0Ainterest%2C%20opening%20new%20possibilities%20for%20targeted%20mechanistic%20interpretability%0Aof%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12990v1&entry.124074799=Read"},
{"title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with\n  On-Generator Training", "author": "Adithyavairavan Murali and Balakumar Sundaralingam and Yu-Wei Chao and Wentao Yuan and Jun Yamada and Mark Carlson and Fabio Ramos and Stan Birchfield and Dieter Fox and Clemens Eppner", "abstract": "  Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.\n", "link": "http://arxiv.org/abs/2507.13097v1", "date": "2025-07-17", "relevancy": 2.6305, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7536}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5924}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraspGen%3A%20A%20Diffusion-based%20Framework%20for%206-DOF%20Grasping%20with%0A%20%20On-Generator%20Training&body=Title%3A%20GraspGen%3A%20A%20Diffusion-based%20Framework%20for%206-DOF%20Grasping%20with%0A%20%20On-Generator%20Training%0AAuthor%3A%20Adithyavairavan%20Murali%20and%20Balakumar%20Sundaralingam%20and%20Yu-Wei%20Chao%20and%20Wentao%20Yuan%20and%20Jun%20Yamada%20and%20Mark%20Carlson%20and%20Fabio%20Ramos%20and%20Stan%20Birchfield%20and%20Dieter%20Fox%20and%20Clemens%20Eppner%0AAbstract%3A%20%20%20Grasping%20is%20a%20fundamental%20robot%20skill%2C%20yet%20despite%20significant%20research%0Aadvancements%2C%20learning-based%206-DOF%20grasping%20approaches%20are%20still%20not%20turnkey%0Aand%20struggle%20to%20generalize%20across%20different%20embodiments%20and%20in-the-wild%0Asettings.%20We%20build%20upon%20the%20recent%20success%20on%20modeling%20the%20object-centric%20grasp%0Ageneration%20process%20as%20an%20iterative%20diffusion%20process.%20Our%20proposed%20framework%2C%0AGraspGen%2C%20consists%20of%20a%20DiffusionTransformer%20architecture%20that%20enhances%20grasp%0Ageneration%2C%20paired%20with%20an%20efficient%20discriminator%20to%20score%20and%20filter%20sampled%0Agrasps.%20We%20introduce%20a%20novel%20and%20performant%20on-generator%20training%20recipe%20for%0Athe%20discriminator.%20To%20scale%20GraspGen%20to%20both%20objects%20and%20grippers%2C%20we%20release%20a%0Anew%20simulated%20dataset%20consisting%20of%20over%2053%20million%20grasps.%20We%20demonstrate%20that%0AGraspGen%20outperforms%20prior%20methods%20in%20simulations%20with%20singulated%20objects%0Aacross%20different%20grippers%2C%20achieves%20state-of-the-art%20performance%20on%20the%0AFetchBench%20grasping%20benchmark%2C%20and%20performs%20well%20on%20a%20real%20robot%20with%20noisy%0Avisual%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraspGen%253A%2520A%2520Diffusion-based%2520Framework%2520for%25206-DOF%2520Grasping%2520with%250A%2520%2520On-Generator%2520Training%26entry.906535625%3DAdithyavairavan%2520Murali%2520and%2520Balakumar%2520Sundaralingam%2520and%2520Yu-Wei%2520Chao%2520and%2520Wentao%2520Yuan%2520and%2520Jun%2520Yamada%2520and%2520Mark%2520Carlson%2520and%2520Fabio%2520Ramos%2520and%2520Stan%2520Birchfield%2520and%2520Dieter%2520Fox%2520and%2520Clemens%2520Eppner%26entry.1292438233%3D%2520%2520Grasping%2520is%2520a%2520fundamental%2520robot%2520skill%252C%2520yet%2520despite%2520significant%2520research%250Aadvancements%252C%2520learning-based%25206-DOF%2520grasping%2520approaches%2520are%2520still%2520not%2520turnkey%250Aand%2520struggle%2520to%2520generalize%2520across%2520different%2520embodiments%2520and%2520in-the-wild%250Asettings.%2520We%2520build%2520upon%2520the%2520recent%2520success%2520on%2520modeling%2520the%2520object-centric%2520grasp%250Ageneration%2520process%2520as%2520an%2520iterative%2520diffusion%2520process.%2520Our%2520proposed%2520framework%252C%250AGraspGen%252C%2520consists%2520of%2520a%2520DiffusionTransformer%2520architecture%2520that%2520enhances%2520grasp%250Ageneration%252C%2520paired%2520with%2520an%2520efficient%2520discriminator%2520to%2520score%2520and%2520filter%2520sampled%250Agrasps.%2520We%2520introduce%2520a%2520novel%2520and%2520performant%2520on-generator%2520training%2520recipe%2520for%250Athe%2520discriminator.%2520To%2520scale%2520GraspGen%2520to%2520both%2520objects%2520and%2520grippers%252C%2520we%2520release%2520a%250Anew%2520simulated%2520dataset%2520consisting%2520of%2520over%252053%2520million%2520grasps.%2520We%2520demonstrate%2520that%250AGraspGen%2520outperforms%2520prior%2520methods%2520in%2520simulations%2520with%2520singulated%2520objects%250Aacross%2520different%2520grippers%252C%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250AFetchBench%2520grasping%2520benchmark%252C%2520and%2520performs%2520well%2520on%2520a%2520real%2520robot%2520with%2520noisy%250Avisual%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraspGen%3A%20A%20Diffusion-based%20Framework%20for%206-DOF%20Grasping%20with%0A%20%20On-Generator%20Training&entry.906535625=Adithyavairavan%20Murali%20and%20Balakumar%20Sundaralingam%20and%20Yu-Wei%20Chao%20and%20Wentao%20Yuan%20and%20Jun%20Yamada%20and%20Mark%20Carlson%20and%20Fabio%20Ramos%20and%20Stan%20Birchfield%20and%20Dieter%20Fox%20and%20Clemens%20Eppner&entry.1292438233=%20%20Grasping%20is%20a%20fundamental%20robot%20skill%2C%20yet%20despite%20significant%20research%0Aadvancements%2C%20learning-based%206-DOF%20grasping%20approaches%20are%20still%20not%20turnkey%0Aand%20struggle%20to%20generalize%20across%20different%20embodiments%20and%20in-the-wild%0Asettings.%20We%20build%20upon%20the%20recent%20success%20on%20modeling%20the%20object-centric%20grasp%0Ageneration%20process%20as%20an%20iterative%20diffusion%20process.%20Our%20proposed%20framework%2C%0AGraspGen%2C%20consists%20of%20a%20DiffusionTransformer%20architecture%20that%20enhances%20grasp%0Ageneration%2C%20paired%20with%20an%20efficient%20discriminator%20to%20score%20and%20filter%20sampled%0Agrasps.%20We%20introduce%20a%20novel%20and%20performant%20on-generator%20training%20recipe%20for%0Athe%20discriminator.%20To%20scale%20GraspGen%20to%20both%20objects%20and%20grippers%2C%20we%20release%20a%0Anew%20simulated%20dataset%20consisting%20of%20over%2053%20million%20grasps.%20We%20demonstrate%20that%0AGraspGen%20outperforms%20prior%20methods%20in%20simulations%20with%20singulated%20objects%0Aacross%20different%20grippers%2C%20achieves%20state-of-the-art%20performance%20on%20the%0AFetchBench%20grasping%20benchmark%2C%20and%20performs%20well%20on%20a%20real%20robot%20with%20noisy%0Avisual%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13097v1&entry.124074799=Read"},
{"title": "FashionPose: Text to Pose to Relight Image Generation for Personalized\n  Fashion Visualization", "author": "Chuancheng Shi and Yixiang Chen and Burong Lei and Jichao Chen", "abstract": "  Realistic and controllable garment visualization is critical for fashion\ne-commerce, where users expect personalized previews under diverse poses and\nlighting conditions. Existing methods often rely on predefined poses, limiting\nsemantic flexibility and illumination adaptability. To address this, we\nintroduce FashionPose, the first unified text-to-pose-to-relighting generation\nframework. Given a natural language description, our method first predicts a 2D\nhuman pose, then employs a diffusion model to generate high-fidelity person\nimages, and finally applies a lightweight relighting module, all guided by the\nsame textual input. By replacing explicit pose annotations with text-driven\nconditioning, FashionPose enables accurate pose alignment, faithful garment\nrendering, and flexible lighting control. Experiments demonstrate fine-grained\npose synthesis and efficient, consistent relighting, providing a practical\nsolution for personalized virtual fashion display.\n", "link": "http://arxiv.org/abs/2507.13311v1", "date": "2025-07-17", "relevancy": 2.618, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6689}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6534}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FashionPose%3A%20Text%20to%20Pose%20to%20Relight%20Image%20Generation%20for%20Personalized%0A%20%20Fashion%20Visualization&body=Title%3A%20FashionPose%3A%20Text%20to%20Pose%20to%20Relight%20Image%20Generation%20for%20Personalized%0A%20%20Fashion%20Visualization%0AAuthor%3A%20Chuancheng%20Shi%20and%20Yixiang%20Chen%20and%20Burong%20Lei%20and%20Jichao%20Chen%0AAbstract%3A%20%20%20Realistic%20and%20controllable%20garment%20visualization%20is%20critical%20for%20fashion%0Ae-commerce%2C%20where%20users%20expect%20personalized%20previews%20under%20diverse%20poses%20and%0Alighting%20conditions.%20Existing%20methods%20often%20rely%20on%20predefined%20poses%2C%20limiting%0Asemantic%20flexibility%20and%20illumination%20adaptability.%20To%20address%20this%2C%20we%0Aintroduce%20FashionPose%2C%20the%20first%20unified%20text-to-pose-to-relighting%20generation%0Aframework.%20Given%20a%20natural%20language%20description%2C%20our%20method%20first%20predicts%20a%202D%0Ahuman%20pose%2C%20then%20employs%20a%20diffusion%20model%20to%20generate%20high-fidelity%20person%0Aimages%2C%20and%20finally%20applies%20a%20lightweight%20relighting%20module%2C%20all%20guided%20by%20the%0Asame%20textual%20input.%20By%20replacing%20explicit%20pose%20annotations%20with%20text-driven%0Aconditioning%2C%20FashionPose%20enables%20accurate%20pose%20alignment%2C%20faithful%20garment%0Arendering%2C%20and%20flexible%20lighting%20control.%20Experiments%20demonstrate%20fine-grained%0Apose%20synthesis%20and%20efficient%2C%20consistent%20relighting%2C%20providing%20a%20practical%0Asolution%20for%20personalized%20virtual%20fashion%20display.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionPose%253A%2520Text%2520to%2520Pose%2520to%2520Relight%2520Image%2520Generation%2520for%2520Personalized%250A%2520%2520Fashion%2520Visualization%26entry.906535625%3DChuancheng%2520Shi%2520and%2520Yixiang%2520Chen%2520and%2520Burong%2520Lei%2520and%2520Jichao%2520Chen%26entry.1292438233%3D%2520%2520Realistic%2520and%2520controllable%2520garment%2520visualization%2520is%2520critical%2520for%2520fashion%250Ae-commerce%252C%2520where%2520users%2520expect%2520personalized%2520previews%2520under%2520diverse%2520poses%2520and%250Alighting%2520conditions.%2520Existing%2520methods%2520often%2520rely%2520on%2520predefined%2520poses%252C%2520limiting%250Asemantic%2520flexibility%2520and%2520illumination%2520adaptability.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520FashionPose%252C%2520the%2520first%2520unified%2520text-to-pose-to-relighting%2520generation%250Aframework.%2520Given%2520a%2520natural%2520language%2520description%252C%2520our%2520method%2520first%2520predicts%2520a%25202D%250Ahuman%2520pose%252C%2520then%2520employs%2520a%2520diffusion%2520model%2520to%2520generate%2520high-fidelity%2520person%250Aimages%252C%2520and%2520finally%2520applies%2520a%2520lightweight%2520relighting%2520module%252C%2520all%2520guided%2520by%2520the%250Asame%2520textual%2520input.%2520By%2520replacing%2520explicit%2520pose%2520annotations%2520with%2520text-driven%250Aconditioning%252C%2520FashionPose%2520enables%2520accurate%2520pose%2520alignment%252C%2520faithful%2520garment%250Arendering%252C%2520and%2520flexible%2520lighting%2520control.%2520Experiments%2520demonstrate%2520fine-grained%250Apose%2520synthesis%2520and%2520efficient%252C%2520consistent%2520relighting%252C%2520providing%2520a%2520practical%250Asolution%2520for%2520personalized%2520virtual%2520fashion%2520display.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionPose%3A%20Text%20to%20Pose%20to%20Relight%20Image%20Generation%20for%20Personalized%0A%20%20Fashion%20Visualization&entry.906535625=Chuancheng%20Shi%20and%20Yixiang%20Chen%20and%20Burong%20Lei%20and%20Jichao%20Chen&entry.1292438233=%20%20Realistic%20and%20controllable%20garment%20visualization%20is%20critical%20for%20fashion%0Ae-commerce%2C%20where%20users%20expect%20personalized%20previews%20under%20diverse%20poses%20and%0Alighting%20conditions.%20Existing%20methods%20often%20rely%20on%20predefined%20poses%2C%20limiting%0Asemantic%20flexibility%20and%20illumination%20adaptability.%20To%20address%20this%2C%20we%0Aintroduce%20FashionPose%2C%20the%20first%20unified%20text-to-pose-to-relighting%20generation%0Aframework.%20Given%20a%20natural%20language%20description%2C%20our%20method%20first%20predicts%20a%202D%0Ahuman%20pose%2C%20then%20employs%20a%20diffusion%20model%20to%20generate%20high-fidelity%20person%0Aimages%2C%20and%20finally%20applies%20a%20lightweight%20relighting%20module%2C%20all%20guided%20by%20the%0Asame%20textual%20input.%20By%20replacing%20explicit%20pose%20annotations%20with%20text-driven%0Aconditioning%2C%20FashionPose%20enables%20accurate%20pose%20alignment%2C%20faithful%20garment%0Arendering%2C%20and%20flexible%20lighting%20control.%20Experiments%20demonstrate%20fine-grained%0Apose%20synthesis%20and%20efficient%2C%20consistent%20relighting%2C%20providing%20a%20practical%0Asolution%20for%20personalized%20virtual%20fashion%20display.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13311v1&entry.124074799=Read"},
{"title": "Boosting Team Modeling through Tempo-Relational Representation Learning", "author": "Vincenzo Marco De Luca and Giovanna Varni and Andrea Passerini", "abstract": "  Team modeling remains a fundamental challenge at the intersection of\nArtificial Intelligence and the Social Sciences. Social Science research\nemphasizes the need to jointly model dynamics and relations, while practical\napplications demand unified models capable of inferring multiple team\nconstructs simultaneously, providing interpretable insights and actionable\nrecommendations to enhance team performance. However, existing works do not\nmeet these practical demands. To bridge this gap, we present TRENN, a novel\ntempo-relational architecture that integrates: (i) an automatic temporal graph\nextractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct\nprediction, and (iv) two complementary explainability modules. TRENN jointly\ncaptures relational and temporal team dynamics, providing a solid foundation\nfor MT-TRENN, which extends TReNN by replacing the decoder with a multi-task\nhead, enabling the model to learn shared Social Embeddings and simultaneously\npredict multiple team constructs, including Emergent Leadership, Leadership\nStyle, and Teamwork components. Experimental results demonstrate that our\napproach significantly outperforms approaches that rely exclusively on temporal\nor relational information. Additionally, experimental evaluation has shown that\nthe explainability modules integrated in MT-TRENN yield interpretable insights\nand actionable suggestions to support team improvement. These capabilities make\nour approach particularly well-suited for Human-Centered AI applications, such\nas intelligent decision-support systems in high-stakes collaborative\nenvironments.\n", "link": "http://arxiv.org/abs/2507.13305v1", "date": "2025-07-17", "relevancy": 2.5834, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Team%20Modeling%20through%20Tempo-Relational%20Representation%20Learning&body=Title%3A%20Boosting%20Team%20Modeling%20through%20Tempo-Relational%20Representation%20Learning%0AAuthor%3A%20Vincenzo%20Marco%20De%20Luca%20and%20Giovanna%20Varni%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20Team%20modeling%20remains%20a%20fundamental%20challenge%20at%20the%20intersection%20of%0AArtificial%20Intelligence%20and%20the%20Social%20Sciences.%20Social%20Science%20research%0Aemphasizes%20the%20need%20to%20jointly%20model%20dynamics%20and%20relations%2C%20while%20practical%0Aapplications%20demand%20unified%20models%20capable%20of%20inferring%20multiple%20team%0Aconstructs%20simultaneously%2C%20providing%20interpretable%20insights%20and%20actionable%0Arecommendations%20to%20enhance%20team%20performance.%20However%2C%20existing%20works%20do%20not%0Ameet%20these%20practical%20demands.%20To%20bridge%20this%20gap%2C%20we%20present%20TRENN%2C%20a%20novel%0Atempo-relational%20architecture%20that%20integrates%3A%20%28i%29%20an%20automatic%20temporal%20graph%0Aextractor%2C%20%28ii%29%20a%20tempo-relational%20encoder%2C%20%28iii%29%20a%20decoder%20for%20team%20construct%0Aprediction%2C%20and%20%28iv%29%20two%20complementary%20explainability%20modules.%20TRENN%20jointly%0Acaptures%20relational%20and%20temporal%20team%20dynamics%2C%20providing%20a%20solid%20foundation%0Afor%20MT-TRENN%2C%20which%20extends%20TReNN%20by%20replacing%20the%20decoder%20with%20a%20multi-task%0Ahead%2C%20enabling%20the%20model%20to%20learn%20shared%20Social%20Embeddings%20and%20simultaneously%0Apredict%20multiple%20team%20constructs%2C%20including%20Emergent%20Leadership%2C%20Leadership%0AStyle%2C%20and%20Teamwork%20components.%20Experimental%20results%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20approaches%20that%20rely%20exclusively%20on%20temporal%0Aor%20relational%20information.%20Additionally%2C%20experimental%20evaluation%20has%20shown%20that%0Athe%20explainability%20modules%20integrated%20in%20MT-TRENN%20yield%20interpretable%20insights%0Aand%20actionable%20suggestions%20to%20support%20team%20improvement.%20These%20capabilities%20make%0Aour%20approach%20particularly%20well-suited%20for%20Human-Centered%20AI%20applications%2C%20such%0Aas%20intelligent%20decision-support%20systems%20in%20high-stakes%20collaborative%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Team%2520Modeling%2520through%2520Tempo-Relational%2520Representation%2520Learning%26entry.906535625%3DVincenzo%2520Marco%2520De%2520Luca%2520and%2520Giovanna%2520Varni%2520and%2520Andrea%2520Passerini%26entry.1292438233%3D%2520%2520Team%2520modeling%2520remains%2520a%2520fundamental%2520challenge%2520at%2520the%2520intersection%2520of%250AArtificial%2520Intelligence%2520and%2520the%2520Social%2520Sciences.%2520Social%2520Science%2520research%250Aemphasizes%2520the%2520need%2520to%2520jointly%2520model%2520dynamics%2520and%2520relations%252C%2520while%2520practical%250Aapplications%2520demand%2520unified%2520models%2520capable%2520of%2520inferring%2520multiple%2520team%250Aconstructs%2520simultaneously%252C%2520providing%2520interpretable%2520insights%2520and%2520actionable%250Arecommendations%2520to%2520enhance%2520team%2520performance.%2520However%252C%2520existing%2520works%2520do%2520not%250Ameet%2520these%2520practical%2520demands.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520TRENN%252C%2520a%2520novel%250Atempo-relational%2520architecture%2520that%2520integrates%253A%2520%2528i%2529%2520an%2520automatic%2520temporal%2520graph%250Aextractor%252C%2520%2528ii%2529%2520a%2520tempo-relational%2520encoder%252C%2520%2528iii%2529%2520a%2520decoder%2520for%2520team%2520construct%250Aprediction%252C%2520and%2520%2528iv%2529%2520two%2520complementary%2520explainability%2520modules.%2520TRENN%2520jointly%250Acaptures%2520relational%2520and%2520temporal%2520team%2520dynamics%252C%2520providing%2520a%2520solid%2520foundation%250Afor%2520MT-TRENN%252C%2520which%2520extends%2520TReNN%2520by%2520replacing%2520the%2520decoder%2520with%2520a%2520multi-task%250Ahead%252C%2520enabling%2520the%2520model%2520to%2520learn%2520shared%2520Social%2520Embeddings%2520and%2520simultaneously%250Apredict%2520multiple%2520team%2520constructs%252C%2520including%2520Emergent%2520Leadership%252C%2520Leadership%250AStyle%252C%2520and%2520Teamwork%2520components.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520outperforms%2520approaches%2520that%2520rely%2520exclusively%2520on%2520temporal%250Aor%2520relational%2520information.%2520Additionally%252C%2520experimental%2520evaluation%2520has%2520shown%2520that%250Athe%2520explainability%2520modules%2520integrated%2520in%2520MT-TRENN%2520yield%2520interpretable%2520insights%250Aand%2520actionable%2520suggestions%2520to%2520support%2520team%2520improvement.%2520These%2520capabilities%2520make%250Aour%2520approach%2520particularly%2520well-suited%2520for%2520Human-Centered%2520AI%2520applications%252C%2520such%250Aas%2520intelligent%2520decision-support%2520systems%2520in%2520high-stakes%2520collaborative%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Team%20Modeling%20through%20Tempo-Relational%20Representation%20Learning&entry.906535625=Vincenzo%20Marco%20De%20Luca%20and%20Giovanna%20Varni%20and%20Andrea%20Passerini&entry.1292438233=%20%20Team%20modeling%20remains%20a%20fundamental%20challenge%20at%20the%20intersection%20of%0AArtificial%20Intelligence%20and%20the%20Social%20Sciences.%20Social%20Science%20research%0Aemphasizes%20the%20need%20to%20jointly%20model%20dynamics%20and%20relations%2C%20while%20practical%0Aapplications%20demand%20unified%20models%20capable%20of%20inferring%20multiple%20team%0Aconstructs%20simultaneously%2C%20providing%20interpretable%20insights%20and%20actionable%0Arecommendations%20to%20enhance%20team%20performance.%20However%2C%20existing%20works%20do%20not%0Ameet%20these%20practical%20demands.%20To%20bridge%20this%20gap%2C%20we%20present%20TRENN%2C%20a%20novel%0Atempo-relational%20architecture%20that%20integrates%3A%20%28i%29%20an%20automatic%20temporal%20graph%0Aextractor%2C%20%28ii%29%20a%20tempo-relational%20encoder%2C%20%28iii%29%20a%20decoder%20for%20team%20construct%0Aprediction%2C%20and%20%28iv%29%20two%20complementary%20explainability%20modules.%20TRENN%20jointly%0Acaptures%20relational%20and%20temporal%20team%20dynamics%2C%20providing%20a%20solid%20foundation%0Afor%20MT-TRENN%2C%20which%20extends%20TReNN%20by%20replacing%20the%20decoder%20with%20a%20multi-task%0Ahead%2C%20enabling%20the%20model%20to%20learn%20shared%20Social%20Embeddings%20and%20simultaneously%0Apredict%20multiple%20team%20constructs%2C%20including%20Emergent%20Leadership%2C%20Leadership%0AStyle%2C%20and%20Teamwork%20components.%20Experimental%20results%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20approaches%20that%20rely%20exclusively%20on%20temporal%0Aor%20relational%20information.%20Additionally%2C%20experimental%20evaluation%20has%20shown%20that%0Athe%20explainability%20modules%20integrated%20in%20MT-TRENN%20yield%20interpretable%20insights%0Aand%20actionable%20suggestions%20to%20support%20team%20improvement.%20These%20capabilities%20make%0Aour%20approach%20particularly%20well-suited%20for%20Human-Centered%20AI%20applications%2C%20such%0Aas%20intelligent%20decision-support%20systems%20in%20high-stakes%20collaborative%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13305v1&entry.124074799=Read"},
{"title": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles", "author": "Yebowen Hu and Xiaoyang Wang and Wenlin Yao and Yiming Lu and Daoan Zhang and Hassan Foroosh and Dong Yu and Fei Liu", "abstract": "  LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital.\n", "link": "http://arxiv.org/abs/2410.01772v2", "date": "2025-07-17", "relevancy": 2.5748, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeFine%3A%20Decision-Making%20with%20Analogical%20Reasoning%20over%20Factor%20Profiles&body=Title%3A%20DeFine%3A%20Decision-Making%20with%20Analogical%20Reasoning%20over%20Factor%20Profiles%0AAuthor%3A%20Yebowen%20Hu%20and%20Xiaoyang%20Wang%20and%20Wenlin%20Yao%20and%20Yiming%20Lu%20and%20Daoan%20Zhang%20and%20Hassan%20Foroosh%20and%20Dong%20Yu%20and%20Fei%20Liu%0AAbstract%3A%20%20%20LLMs%20are%20ideal%20for%20decision-making%20thanks%20to%20their%20ability%20to%20reason%20over%0Along%20contexts.%20However%2C%20challenges%20arise%20when%20processing%20speech%20transcripts%0Athat%20describe%20complex%20scenarios%2C%20as%20they%20are%20verbose%20and%20include%20repetition%2C%0Ahedging%2C%20and%20vagueness.%20E.g.%2C%20during%20a%20company%27s%20earnings%20call%2C%20an%20executive%0Amight%20project%20a%20positive%20revenue%20outlook%20to%20reassure%20investors%2C%20despite%0Auncertainty%20regarding%20future%20earnings.%20It%20is%20crucial%20for%20LLMs%20to%20incorporate%0Athis%20uncertainty%20systematically%20when%20making%20decisions.%20In%20this%20paper%2C%20we%0Aintroduce%20%5Ctextsc%7BDeFine%7D%2C%20a%20modular%20framework%20that%20constructs%20probabilistic%0Afactor%20profiles%20from%20complex%20scenarios.%20It%20then%20integrates%20these%20profiles%20with%0Aanalogical%20reasoning%2C%20leveraging%20insights%20from%20similar%20past%20experiences%20to%0Aguide%20LLMs%20in%20making%20critical%20decisions%20in%20new%20situations.%20Our%20framework%0Aseparates%20the%20tasks%20of%20quantifying%20uncertainty%20and%20incorporating%20it%20into%20LLM%0Adecision-making.%20This%20approach%20is%20particularly%20useful%20in%20areas%20such%20as%0Aconsulting%20and%20financial%20deliberation%2C%20where%20making%20decisions%20under%20uncertainty%0Ais%20vital.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeFine%253A%2520Decision-Making%2520with%2520Analogical%2520Reasoning%2520over%2520Factor%2520Profiles%26entry.906535625%3DYebowen%2520Hu%2520and%2520Xiaoyang%2520Wang%2520and%2520Wenlin%2520Yao%2520and%2520Yiming%2520Lu%2520and%2520Daoan%2520Zhang%2520and%2520Hassan%2520Foroosh%2520and%2520Dong%2520Yu%2520and%2520Fei%2520Liu%26entry.1292438233%3D%2520%2520LLMs%2520are%2520ideal%2520for%2520decision-making%2520thanks%2520to%2520their%2520ability%2520to%2520reason%2520over%250Along%2520contexts.%2520However%252C%2520challenges%2520arise%2520when%2520processing%2520speech%2520transcripts%250Athat%2520describe%2520complex%2520scenarios%252C%2520as%2520they%2520are%2520verbose%2520and%2520include%2520repetition%252C%250Ahedging%252C%2520and%2520vagueness.%2520E.g.%252C%2520during%2520a%2520company%2527s%2520earnings%2520call%252C%2520an%2520executive%250Amight%2520project%2520a%2520positive%2520revenue%2520outlook%2520to%2520reassure%2520investors%252C%2520despite%250Auncertainty%2520regarding%2520future%2520earnings.%2520It%2520is%2520crucial%2520for%2520LLMs%2520to%2520incorporate%250Athis%2520uncertainty%2520systematically%2520when%2520making%2520decisions.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520%255Ctextsc%257BDeFine%257D%252C%2520a%2520modular%2520framework%2520that%2520constructs%2520probabilistic%250Afactor%2520profiles%2520from%2520complex%2520scenarios.%2520It%2520then%2520integrates%2520these%2520profiles%2520with%250Aanalogical%2520reasoning%252C%2520leveraging%2520insights%2520from%2520similar%2520past%2520experiences%2520to%250Aguide%2520LLMs%2520in%2520making%2520critical%2520decisions%2520in%2520new%2520situations.%2520Our%2520framework%250Aseparates%2520the%2520tasks%2520of%2520quantifying%2520uncertainty%2520and%2520incorporating%2520it%2520into%2520LLM%250Adecision-making.%2520This%2520approach%2520is%2520particularly%2520useful%2520in%2520areas%2520such%2520as%250Aconsulting%2520and%2520financial%2520deliberation%252C%2520where%2520making%2520decisions%2520under%2520uncertainty%250Ais%2520vital.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeFine%3A%20Decision-Making%20with%20Analogical%20Reasoning%20over%20Factor%20Profiles&entry.906535625=Yebowen%20Hu%20and%20Xiaoyang%20Wang%20and%20Wenlin%20Yao%20and%20Yiming%20Lu%20and%20Daoan%20Zhang%20and%20Hassan%20Foroosh%20and%20Dong%20Yu%20and%20Fei%20Liu&entry.1292438233=%20%20LLMs%20are%20ideal%20for%20decision-making%20thanks%20to%20their%20ability%20to%20reason%20over%0Along%20contexts.%20However%2C%20challenges%20arise%20when%20processing%20speech%20transcripts%0Athat%20describe%20complex%20scenarios%2C%20as%20they%20are%20verbose%20and%20include%20repetition%2C%0Ahedging%2C%20and%20vagueness.%20E.g.%2C%20during%20a%20company%27s%20earnings%20call%2C%20an%20executive%0Amight%20project%20a%20positive%20revenue%20outlook%20to%20reassure%20investors%2C%20despite%0Auncertainty%20regarding%20future%20earnings.%20It%20is%20crucial%20for%20LLMs%20to%20incorporate%0Athis%20uncertainty%20systematically%20when%20making%20decisions.%20In%20this%20paper%2C%20we%0Aintroduce%20%5Ctextsc%7BDeFine%7D%2C%20a%20modular%20framework%20that%20constructs%20probabilistic%0Afactor%20profiles%20from%20complex%20scenarios.%20It%20then%20integrates%20these%20profiles%20with%0Aanalogical%20reasoning%2C%20leveraging%20insights%20from%20similar%20past%20experiences%20to%0Aguide%20LLMs%20in%20making%20critical%20decisions%20in%20new%20situations.%20Our%20framework%0Aseparates%20the%20tasks%20of%20quantifying%20uncertainty%20and%20incorporating%20it%20into%20LLM%0Adecision-making.%20This%20approach%20is%20particularly%20useful%20in%20areas%20such%20as%0Aconsulting%20and%20financial%20deliberation%2C%20where%20making%20decisions%20under%20uncertainty%0Ais%20vital.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01772v2&entry.124074799=Read"},
{"title": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution", "author": "Ritik Shah and Marco F. Duarte", "abstract": "  High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.\n", "link": "http://arxiv.org/abs/2507.13339v1", "date": "2025-07-17", "relevancy": 2.5705, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5137}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution&body=Title%3A%20SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution%0AAuthor%3A%20Ritik%20Shah%20and%20Marco%20F.%20Duarte%0AAbstract%3A%20%20%20High-spatial-resolution%20hyperspectral%20images%20%28HSI%29%20are%20essential%20for%0Aapplications%20such%20as%20remote%20sensing%20and%20medical%20imaging%2C%20yet%20HSI%20sensors%0Ainherently%20trade%20spatial%20detail%20for%20spectral%20richness.%20Fusing%0Ahigh-spatial-resolution%20multispectral%20images%20%28HR-MSI%29%20with%0Alow-spatial-resolution%20hyperspectral%20images%20%28LR-HSI%29%20is%20a%20promising%20route%20to%0Arecover%20fine%20spatial%20structures%20without%20sacrificing%20spectral%20fidelity.%20Most%0Astate-of-the-art%20methods%20for%20HSI-MSI%20fusion%20demand%20point%20spread%20function%20%28PSF%29%0Acalibration%20or%20ground%20truth%20high%20resolution%20HSI%20%28HR-HSI%29%2C%20both%20of%20which%20are%0Aimpractical%20to%20obtain%20in%20real%20world%20settings.%20We%20present%20SpectraLift%2C%20a%20fully%0Aself-supervised%20framework%20that%20fuses%20LR-HSI%20and%20HR-MSI%20inputs%20using%20only%20the%0AMSI%27s%20Spectral%20Response%20Function%20%28SRF%29.%20SpectraLift%20trains%20a%20lightweight%0Aper-pixel%20multi-layer%20perceptron%20%28MLP%29%20network%20using%20%28%24i%24%29~a%20synthetic%0Alow-spatial-resolution%20multispectral%20image%20%28LR-MSI%29%20obtained%20by%20applying%20the%0ASRF%20to%20the%20LR-HSI%20as%20input%2C%20%28%24ii%24%29~the%20LR-HSI%20as%20the%20output%2C%20and%20%28%24iii%24%29~an%0A%24%5Cell_1%24%20spectral%20reconstruction%20loss%20between%20the%20estimated%20and%20true%20LR-HSI%20as%0Athe%20optimization%20objective.%20At%20inference%2C%20SpectraLift%20uses%20the%20trained%20network%0Ato%20map%20the%20HR-MSI%20pixel-wise%20into%20a%20HR-HSI%20estimate.%20SpectraLift%20converges%20in%0Aminutes%2C%20is%20agnostic%20to%20spatial%20blur%20and%20resolution%2C%20and%20outperforms%0Astate-of-the-art%20methods%20on%20PSNR%2C%20SAM%2C%20SSIM%2C%20and%20RMSE%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectraLift%253A%2520Physics-Guided%2520Spectral-Inversion%2520Network%2520for%250A%2520%2520Self-Supervised%2520Hyperspectral%2520Image%2520Super-Resolution%26entry.906535625%3DRitik%2520Shah%2520and%2520Marco%2520F.%2520Duarte%26entry.1292438233%3D%2520%2520High-spatial-resolution%2520hyperspectral%2520images%2520%2528HSI%2529%2520are%2520essential%2520for%250Aapplications%2520such%2520as%2520remote%2520sensing%2520and%2520medical%2520imaging%252C%2520yet%2520HSI%2520sensors%250Ainherently%2520trade%2520spatial%2520detail%2520for%2520spectral%2520richness.%2520Fusing%250Ahigh-spatial-resolution%2520multispectral%2520images%2520%2528HR-MSI%2529%2520with%250Alow-spatial-resolution%2520hyperspectral%2520images%2520%2528LR-HSI%2529%2520is%2520a%2520promising%2520route%2520to%250Arecover%2520fine%2520spatial%2520structures%2520without%2520sacrificing%2520spectral%2520fidelity.%2520Most%250Astate-of-the-art%2520methods%2520for%2520HSI-MSI%2520fusion%2520demand%2520point%2520spread%2520function%2520%2528PSF%2529%250Acalibration%2520or%2520ground%2520truth%2520high%2520resolution%2520HSI%2520%2528HR-HSI%2529%252C%2520both%2520of%2520which%2520are%250Aimpractical%2520to%2520obtain%2520in%2520real%2520world%2520settings.%2520We%2520present%2520SpectraLift%252C%2520a%2520fully%250Aself-supervised%2520framework%2520that%2520fuses%2520LR-HSI%2520and%2520HR-MSI%2520inputs%2520using%2520only%2520the%250AMSI%2527s%2520Spectral%2520Response%2520Function%2520%2528SRF%2529.%2520SpectraLift%2520trains%2520a%2520lightweight%250Aper-pixel%2520multi-layer%2520perceptron%2520%2528MLP%2529%2520network%2520using%2520%2528%2524i%2524%2529~a%2520synthetic%250Alow-spatial-resolution%2520multispectral%2520image%2520%2528LR-MSI%2529%2520obtained%2520by%2520applying%2520the%250ASRF%2520to%2520the%2520LR-HSI%2520as%2520input%252C%2520%2528%2524ii%2524%2529~the%2520LR-HSI%2520as%2520the%2520output%252C%2520and%2520%2528%2524iii%2524%2529~an%250A%2524%255Cell_1%2524%2520spectral%2520reconstruction%2520loss%2520between%2520the%2520estimated%2520and%2520true%2520LR-HSI%2520as%250Athe%2520optimization%2520objective.%2520At%2520inference%252C%2520SpectraLift%2520uses%2520the%2520trained%2520network%250Ato%2520map%2520the%2520HR-MSI%2520pixel-wise%2520into%2520a%2520HR-HSI%2520estimate.%2520SpectraLift%2520converges%2520in%250Aminutes%252C%2520is%2520agnostic%2520to%2520spatial%2520blur%2520and%2520resolution%252C%2520and%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520PSNR%252C%2520SAM%252C%2520SSIM%252C%2520and%2520RMSE%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution&entry.906535625=Ritik%20Shah%20and%20Marco%20F.%20Duarte&entry.1292438233=%20%20High-spatial-resolution%20hyperspectral%20images%20%28HSI%29%20are%20essential%20for%0Aapplications%20such%20as%20remote%20sensing%20and%20medical%20imaging%2C%20yet%20HSI%20sensors%0Ainherently%20trade%20spatial%20detail%20for%20spectral%20richness.%20Fusing%0Ahigh-spatial-resolution%20multispectral%20images%20%28HR-MSI%29%20with%0Alow-spatial-resolution%20hyperspectral%20images%20%28LR-HSI%29%20is%20a%20promising%20route%20to%0Arecover%20fine%20spatial%20structures%20without%20sacrificing%20spectral%20fidelity.%20Most%0Astate-of-the-art%20methods%20for%20HSI-MSI%20fusion%20demand%20point%20spread%20function%20%28PSF%29%0Acalibration%20or%20ground%20truth%20high%20resolution%20HSI%20%28HR-HSI%29%2C%20both%20of%20which%20are%0Aimpractical%20to%20obtain%20in%20real%20world%20settings.%20We%20present%20SpectraLift%2C%20a%20fully%0Aself-supervised%20framework%20that%20fuses%20LR-HSI%20and%20HR-MSI%20inputs%20using%20only%20the%0AMSI%27s%20Spectral%20Response%20Function%20%28SRF%29.%20SpectraLift%20trains%20a%20lightweight%0Aper-pixel%20multi-layer%20perceptron%20%28MLP%29%20network%20using%20%28%24i%24%29~a%20synthetic%0Alow-spatial-resolution%20multispectral%20image%20%28LR-MSI%29%20obtained%20by%20applying%20the%0ASRF%20to%20the%20LR-HSI%20as%20input%2C%20%28%24ii%24%29~the%20LR-HSI%20as%20the%20output%2C%20and%20%28%24iii%24%29~an%0A%24%5Cell_1%24%20spectral%20reconstruction%20loss%20between%20the%20estimated%20and%20true%20LR-HSI%20as%0Athe%20optimization%20objective.%20At%20inference%2C%20SpectraLift%20uses%20the%20trained%20network%0Ato%20map%20the%20HR-MSI%20pixel-wise%20into%20a%20HR-HSI%20estimate.%20SpectraLift%20converges%20in%0Aminutes%2C%20is%20agnostic%20to%20spatial%20blur%20and%20resolution%2C%20and%20outperforms%0Astate-of-the-art%20methods%20on%20PSNR%2C%20SAM%2C%20SSIM%2C%20and%20RMSE%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13339v1&entry.124074799=Read"},
{"title": "Advancing Complex Wide-Area Scene Understanding with Hierarchical\n  Coresets Selection", "author": "Jingyao Wang and Yiming Chen and Lingyu Si and Changwen Zheng", "abstract": "  Scene understanding is one of the core tasks in computer vision, aiming to\nextract semantic information from images to identify objects, scene categories,\nand their interrelationships. Although advancements in Vision-Language Models\n(VLMs) have driven progress in this field, existing VLMs still face challenges\nin adaptation to unseen complex wide-area scenes. To address the challenges,\nthis paper proposes a Hierarchical Coresets Selection (HCS) mechanism to\nadvance the adaptation of VLMs in complex wide-area scene understanding. It\nprogressively refines the selected regions based on the proposed theoretically\nguaranteed importance function, which considers utility, representativeness,\nrobustness, and synergy. Without requiring additional fine-tuning, HCS enables\nVLMs to achieve rapid understandings of unseen scenes at any scale using\nminimal interpretable regions while mitigating insufficient feature density.\nHCS is a plug-and-play method that is compatible with any VLM. Experiments\ndemonstrate that HCS achieves superior performance and universality in various\ntasks.\n", "link": "http://arxiv.org/abs/2507.13061v1", "date": "2025-07-17", "relevancy": 2.5681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6607}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Complex%20Wide-Area%20Scene%20Understanding%20with%20Hierarchical%0A%20%20Coresets%20Selection&body=Title%3A%20Advancing%20Complex%20Wide-Area%20Scene%20Understanding%20with%20Hierarchical%0A%20%20Coresets%20Selection%0AAuthor%3A%20Jingyao%20Wang%20and%20Yiming%20Chen%20and%20Lingyu%20Si%20and%20Changwen%20Zheng%0AAbstract%3A%20%20%20Scene%20understanding%20is%20one%20of%20the%20core%20tasks%20in%20computer%20vision%2C%20aiming%20to%0Aextract%20semantic%20information%20from%20images%20to%20identify%20objects%2C%20scene%20categories%2C%0Aand%20their%20interrelationships.%20Although%20advancements%20in%20Vision-Language%20Models%0A%28VLMs%29%20have%20driven%20progress%20in%20this%20field%2C%20existing%20VLMs%20still%20face%20challenges%0Ain%20adaptation%20to%20unseen%20complex%20wide-area%20scenes.%20To%20address%20the%20challenges%2C%0Athis%20paper%20proposes%20a%20Hierarchical%20Coresets%20Selection%20%28HCS%29%20mechanism%20to%0Aadvance%20the%20adaptation%20of%20VLMs%20in%20complex%20wide-area%20scene%20understanding.%20It%0Aprogressively%20refines%20the%20selected%20regions%20based%20on%20the%20proposed%20theoretically%0Aguaranteed%20importance%20function%2C%20which%20considers%20utility%2C%20representativeness%2C%0Arobustness%2C%20and%20synergy.%20Without%20requiring%20additional%20fine-tuning%2C%20HCS%20enables%0AVLMs%20to%20achieve%20rapid%20understandings%20of%20unseen%20scenes%20at%20any%20scale%20using%0Aminimal%20interpretable%20regions%20while%20mitigating%20insufficient%20feature%20density.%0AHCS%20is%20a%20plug-and-play%20method%20that%20is%20compatible%20with%20any%20VLM.%20Experiments%0Ademonstrate%20that%20HCS%20achieves%20superior%20performance%20and%20universality%20in%20various%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Complex%2520Wide-Area%2520Scene%2520Understanding%2520with%2520Hierarchical%250A%2520%2520Coresets%2520Selection%26entry.906535625%3DJingyao%2520Wang%2520and%2520Yiming%2520Chen%2520and%2520Lingyu%2520Si%2520and%2520Changwen%2520Zheng%26entry.1292438233%3D%2520%2520Scene%2520understanding%2520is%2520one%2520of%2520the%2520core%2520tasks%2520in%2520computer%2520vision%252C%2520aiming%2520to%250Aextract%2520semantic%2520information%2520from%2520images%2520to%2520identify%2520objects%252C%2520scene%2520categories%252C%250Aand%2520their%2520interrelationships.%2520Although%2520advancements%2520in%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520have%2520driven%2520progress%2520in%2520this%2520field%252C%2520existing%2520VLMs%2520still%2520face%2520challenges%250Ain%2520adaptation%2520to%2520unseen%2520complex%2520wide-area%2520scenes.%2520To%2520address%2520the%2520challenges%252C%250Athis%2520paper%2520proposes%2520a%2520Hierarchical%2520Coresets%2520Selection%2520%2528HCS%2529%2520mechanism%2520to%250Aadvance%2520the%2520adaptation%2520of%2520VLMs%2520in%2520complex%2520wide-area%2520scene%2520understanding.%2520It%250Aprogressively%2520refines%2520the%2520selected%2520regions%2520based%2520on%2520the%2520proposed%2520theoretically%250Aguaranteed%2520importance%2520function%252C%2520which%2520considers%2520utility%252C%2520representativeness%252C%250Arobustness%252C%2520and%2520synergy.%2520Without%2520requiring%2520additional%2520fine-tuning%252C%2520HCS%2520enables%250AVLMs%2520to%2520achieve%2520rapid%2520understandings%2520of%2520unseen%2520scenes%2520at%2520any%2520scale%2520using%250Aminimal%2520interpretable%2520regions%2520while%2520mitigating%2520insufficient%2520feature%2520density.%250AHCS%2520is%2520a%2520plug-and-play%2520method%2520that%2520is%2520compatible%2520with%2520any%2520VLM.%2520Experiments%250Ademonstrate%2520that%2520HCS%2520achieves%2520superior%2520performance%2520and%2520universality%2520in%2520various%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Complex%20Wide-Area%20Scene%20Understanding%20with%20Hierarchical%0A%20%20Coresets%20Selection&entry.906535625=Jingyao%20Wang%20and%20Yiming%20Chen%20and%20Lingyu%20Si%20and%20Changwen%20Zheng&entry.1292438233=%20%20Scene%20understanding%20is%20one%20of%20the%20core%20tasks%20in%20computer%20vision%2C%20aiming%20to%0Aextract%20semantic%20information%20from%20images%20to%20identify%20objects%2C%20scene%20categories%2C%0Aand%20their%20interrelationships.%20Although%20advancements%20in%20Vision-Language%20Models%0A%28VLMs%29%20have%20driven%20progress%20in%20this%20field%2C%20existing%20VLMs%20still%20face%20challenges%0Ain%20adaptation%20to%20unseen%20complex%20wide-area%20scenes.%20To%20address%20the%20challenges%2C%0Athis%20paper%20proposes%20a%20Hierarchical%20Coresets%20Selection%20%28HCS%29%20mechanism%20to%0Aadvance%20the%20adaptation%20of%20VLMs%20in%20complex%20wide-area%20scene%20understanding.%20It%0Aprogressively%20refines%20the%20selected%20regions%20based%20on%20the%20proposed%20theoretically%0Aguaranteed%20importance%20function%2C%20which%20considers%20utility%2C%20representativeness%2C%0Arobustness%2C%20and%20synergy.%20Without%20requiring%20additional%20fine-tuning%2C%20HCS%20enables%0AVLMs%20to%20achieve%20rapid%20understandings%20of%20unseen%20scenes%20at%20any%20scale%20using%0Aminimal%20interpretable%20regions%20while%20mitigating%20insufficient%20feature%20density.%0AHCS%20is%20a%20plug-and-play%20method%20that%20is%20compatible%20with%20any%20VLM.%20Experiments%0Ademonstrate%20that%20HCS%20achieves%20superior%20performance%20and%20universality%20in%20various%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13061v1&entry.124074799=Read"},
{"title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities", "author": "Hao Sun and Mihaela van der Schaar", "abstract": "  In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.\n", "link": "http://arxiv.org/abs/2507.13158v1", "date": "2025-07-17", "relevancy": 2.5378, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Reinforcement%20Learning%20Meets%20Large%20Language%20Model%20Post-Training%3A%0A%20%20Basics%2C%20Advances%2C%20and%20Opportunities&body=Title%3A%20Inverse%20Reinforcement%20Learning%20Meets%20Large%20Language%20Model%20Post-Training%3A%0A%20%20Basics%2C%20Advances%2C%20and%20Opportunities%0AAuthor%3A%20Hao%20Sun%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20In%20the%20era%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20alignment%20has%20emerged%20as%20a%0Afundamental%20yet%20challenging%20problem%20in%20the%20pursuit%20of%20more%20reliable%2C%0Acontrollable%2C%20and%20capable%20machine%20intelligence.%20The%20recent%20success%20of%20reasoning%0Amodels%20and%20conversational%20AI%20systems%20has%20underscored%20the%20critical%20role%20of%0Areinforcement%20learning%20%28RL%29%20in%20enhancing%20these%20systems%2C%20driving%20increased%0Aresearch%20interest%20at%20the%20intersection%20of%20RL%20and%20LLM%20alignment.%20This%20paper%0Aprovides%20a%20comprehensive%20review%20of%20recent%20advances%20in%20LLM%20alignment%20through%20the%0Alens%20of%20inverse%20reinforcement%20learning%20%28IRL%29%2C%20emphasizing%20the%20distinctions%0Abetween%20RL%20techniques%20employed%20in%20LLM%20alignment%20and%20those%20in%20conventional%20RL%0Atasks.%20In%20particular%2C%20we%20highlight%20the%20necessity%20of%20constructing%20neural%20reward%0Amodels%20from%20human%20data%20and%20discuss%20the%20formal%20and%20practical%20implications%20of%0Athis%20paradigm%20shift.%20We%20begin%20by%20introducing%20fundamental%20concepts%20in%20RL%20to%0Aprovide%20a%20foundation%20for%20readers%20unfamiliar%20with%20the%20field.%20We%20then%20examine%0Arecent%20advances%20in%20this%20research%20agenda%2C%20discussing%20key%20challenges%20and%0Aopportunities%20in%20conducting%20IRL%20for%20LLM%20alignment.%20Beyond%20methodological%0Aconsiderations%2C%20we%20explore%20practical%20aspects%2C%20including%20datasets%2C%20benchmarks%2C%0Aevaluation%20metrics%2C%20infrastructure%2C%20and%20computationally%20efficient%20training%20and%0Ainference%20techniques.%20Finally%2C%20we%20draw%20insights%20from%20the%20literature%20on%0Asparse-reward%20RL%20to%20identify%20open%20questions%20and%20potential%20research%20directions.%0ABy%20synthesizing%20findings%20from%20diverse%20studies%2C%20we%20aim%20to%20provide%20a%20structured%0Aand%20critical%20overview%20of%20the%20field%2C%20highlight%20unresolved%20challenges%2C%20and%0Aoutline%20promising%20future%20directions%20for%20improving%20LLM%20alignment%20through%20RL%20and%0AIRL%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Reinforcement%2520Learning%2520Meets%2520Large%2520Language%2520Model%2520Post-Training%253A%250A%2520%2520Basics%252C%2520Advances%252C%2520and%2520Opportunities%26entry.906535625%3DHao%2520Sun%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520alignment%2520has%2520emerged%2520as%2520a%250Afundamental%2520yet%2520challenging%2520problem%2520in%2520the%2520pursuit%2520of%2520more%2520reliable%252C%250Acontrollable%252C%2520and%2520capable%2520machine%2520intelligence.%2520The%2520recent%2520success%2520of%2520reasoning%250Amodels%2520and%2520conversational%2520AI%2520systems%2520has%2520underscored%2520the%2520critical%2520role%2520of%250Areinforcement%2520learning%2520%2528RL%2529%2520in%2520enhancing%2520these%2520systems%252C%2520driving%2520increased%250Aresearch%2520interest%2520at%2520the%2520intersection%2520of%2520RL%2520and%2520LLM%2520alignment.%2520This%2520paper%250Aprovides%2520a%2520comprehensive%2520review%2520of%2520recent%2520advances%2520in%2520LLM%2520alignment%2520through%2520the%250Alens%2520of%2520inverse%2520reinforcement%2520learning%2520%2528IRL%2529%252C%2520emphasizing%2520the%2520distinctions%250Abetween%2520RL%2520techniques%2520employed%2520in%2520LLM%2520alignment%2520and%2520those%2520in%2520conventional%2520RL%250Atasks.%2520In%2520particular%252C%2520we%2520highlight%2520the%2520necessity%2520of%2520constructing%2520neural%2520reward%250Amodels%2520from%2520human%2520data%2520and%2520discuss%2520the%2520formal%2520and%2520practical%2520implications%2520of%250Athis%2520paradigm%2520shift.%2520We%2520begin%2520by%2520introducing%2520fundamental%2520concepts%2520in%2520RL%2520to%250Aprovide%2520a%2520foundation%2520for%2520readers%2520unfamiliar%2520with%2520the%2520field.%2520We%2520then%2520examine%250Arecent%2520advances%2520in%2520this%2520research%2520agenda%252C%2520discussing%2520key%2520challenges%2520and%250Aopportunities%2520in%2520conducting%2520IRL%2520for%2520LLM%2520alignment.%2520Beyond%2520methodological%250Aconsiderations%252C%2520we%2520explore%2520practical%2520aspects%252C%2520including%2520datasets%252C%2520benchmarks%252C%250Aevaluation%2520metrics%252C%2520infrastructure%252C%2520and%2520computationally%2520efficient%2520training%2520and%250Ainference%2520techniques.%2520Finally%252C%2520we%2520draw%2520insights%2520from%2520the%2520literature%2520on%250Asparse-reward%2520RL%2520to%2520identify%2520open%2520questions%2520and%2520potential%2520research%2520directions.%250ABy%2520synthesizing%2520findings%2520from%2520diverse%2520studies%252C%2520we%2520aim%2520to%2520provide%2520a%2520structured%250Aand%2520critical%2520overview%2520of%2520the%2520field%252C%2520highlight%2520unresolved%2520challenges%252C%2520and%250Aoutline%2520promising%2520future%2520directions%2520for%2520improving%2520LLM%2520alignment%2520through%2520RL%2520and%250AIRL%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Reinforcement%20Learning%20Meets%20Large%20Language%20Model%20Post-Training%3A%0A%20%20Basics%2C%20Advances%2C%20and%20Opportunities&entry.906535625=Hao%20Sun%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20In%20the%20era%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20alignment%20has%20emerged%20as%20a%0Afundamental%20yet%20challenging%20problem%20in%20the%20pursuit%20of%20more%20reliable%2C%0Acontrollable%2C%20and%20capable%20machine%20intelligence.%20The%20recent%20success%20of%20reasoning%0Amodels%20and%20conversational%20AI%20systems%20has%20underscored%20the%20critical%20role%20of%0Areinforcement%20learning%20%28RL%29%20in%20enhancing%20these%20systems%2C%20driving%20increased%0Aresearch%20interest%20at%20the%20intersection%20of%20RL%20and%20LLM%20alignment.%20This%20paper%0Aprovides%20a%20comprehensive%20review%20of%20recent%20advances%20in%20LLM%20alignment%20through%20the%0Alens%20of%20inverse%20reinforcement%20learning%20%28IRL%29%2C%20emphasizing%20the%20distinctions%0Abetween%20RL%20techniques%20employed%20in%20LLM%20alignment%20and%20those%20in%20conventional%20RL%0Atasks.%20In%20particular%2C%20we%20highlight%20the%20necessity%20of%20constructing%20neural%20reward%0Amodels%20from%20human%20data%20and%20discuss%20the%20formal%20and%20practical%20implications%20of%0Athis%20paradigm%20shift.%20We%20begin%20by%20introducing%20fundamental%20concepts%20in%20RL%20to%0Aprovide%20a%20foundation%20for%20readers%20unfamiliar%20with%20the%20field.%20We%20then%20examine%0Arecent%20advances%20in%20this%20research%20agenda%2C%20discussing%20key%20challenges%20and%0Aopportunities%20in%20conducting%20IRL%20for%20LLM%20alignment.%20Beyond%20methodological%0Aconsiderations%2C%20we%20explore%20practical%20aspects%2C%20including%20datasets%2C%20benchmarks%2C%0Aevaluation%20metrics%2C%20infrastructure%2C%20and%20computationally%20efficient%20training%20and%0Ainference%20techniques.%20Finally%2C%20we%20draw%20insights%20from%20the%20literature%20on%0Asparse-reward%20RL%20to%20identify%20open%20questions%20and%20potential%20research%20directions.%0ABy%20synthesizing%20findings%20from%20diverse%20studies%2C%20we%20aim%20to%20provide%20a%20structured%0Aand%20critical%20overview%20of%20the%20field%2C%20highlight%20unresolved%20challenges%2C%20and%0Aoutline%20promising%20future%20directions%20for%20improving%20LLM%20alignment%20through%20RL%20and%0AIRL%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13158v1&entry.124074799=Read"},
{"title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation\n  Model", "author": "Maulana Bisyir Azhari and David Hyunchul Shim", "abstract": "  Learning-based monocular visual odometry (VO) poses robustness,\ngeneralization, and efficiency challenges in robotics. Recent advances in\nvisual foundation models, such as DINOv2, have improved robustness and\ngeneralization in various vision tasks, yet their integration in VO remains\nlimited due to coarse feature granularity. In this paper, we present DINO-VO, a\nfeature-based VO system leveraging DINOv2 visual foundation model for its\nsparse feature matching. To address the integration challenge, we propose a\nsalient keypoints detector tailored to DINOv2's coarse features. Furthermore,\nwe complement DINOv2's robust-semantic features with fine-grained geometric\nfeatures, resulting in more localizable representations. Finally, a\ntransformer-based matcher and differentiable pose estimation layer enable\nprecise camera motion estimation by learning good matches. Against prior\ndetector-descriptor networks like SuperPoint, DINO-VO demonstrates greater\nrobustness in challenging environments. Furthermore, we show superior accuracy\nand generalization of the proposed feature descriptors against standalone\nDINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on\nthe TartanAir and KITTI datasets and is competitive on EuRoC dataset, while\nrunning efficiently at 72 FPS with less than 1GB of memory usage on a single\nGPU. Moreover, it performs competitively against Visual SLAM systems on outdoor\ndriving scenarios, showcasing its generalization capabilities.\n", "link": "http://arxiv.org/abs/2507.13145v1", "date": "2025-07-17", "relevancy": 2.5363, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6431}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-VO%3A%20A%20Feature-based%20Visual%20Odometry%20Leveraging%20a%20Visual%20Foundation%0A%20%20Model&body=Title%3A%20DINO-VO%3A%20A%20Feature-based%20Visual%20Odometry%20Leveraging%20a%20Visual%20Foundation%0A%20%20Model%0AAuthor%3A%20Maulana%20Bisyir%20Azhari%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20%20%20Learning-based%20monocular%20visual%20odometry%20%28VO%29%20poses%20robustness%2C%0Ageneralization%2C%20and%20efficiency%20challenges%20in%20robotics.%20Recent%20advances%20in%0Avisual%20foundation%20models%2C%20such%20as%20DINOv2%2C%20have%20improved%20robustness%20and%0Ageneralization%20in%20various%20vision%20tasks%2C%20yet%20their%20integration%20in%20VO%20remains%0Alimited%20due%20to%20coarse%20feature%20granularity.%20In%20this%20paper%2C%20we%20present%20DINO-VO%2C%20a%0Afeature-based%20VO%20system%20leveraging%20DINOv2%20visual%20foundation%20model%20for%20its%0Asparse%20feature%20matching.%20To%20address%20the%20integration%20challenge%2C%20we%20propose%20a%0Asalient%20keypoints%20detector%20tailored%20to%20DINOv2%27s%20coarse%20features.%20Furthermore%2C%0Awe%20complement%20DINOv2%27s%20robust-semantic%20features%20with%20fine-grained%20geometric%0Afeatures%2C%20resulting%20in%20more%20localizable%20representations.%20Finally%2C%20a%0Atransformer-based%20matcher%20and%20differentiable%20pose%20estimation%20layer%20enable%0Aprecise%20camera%20motion%20estimation%20by%20learning%20good%20matches.%20Against%20prior%0Adetector-descriptor%20networks%20like%20SuperPoint%2C%20DINO-VO%20demonstrates%20greater%0Arobustness%20in%20challenging%20environments.%20Furthermore%2C%20we%20show%20superior%20accuracy%0Aand%20generalization%20of%20the%20proposed%20feature%20descriptors%20against%20standalone%0ADINOv2%20coarse%20features.%20DINO-VO%20outperforms%20prior%20frame-to-frame%20VO%20methods%20on%0Athe%20TartanAir%20and%20KITTI%20datasets%20and%20is%20competitive%20on%20EuRoC%20dataset%2C%20while%0Arunning%20efficiently%20at%2072%20FPS%20with%20less%20than%201GB%20of%20memory%20usage%20on%20a%20single%0AGPU.%20Moreover%2C%20it%20performs%20competitively%20against%20Visual%20SLAM%20systems%20on%20outdoor%0Adriving%20scenarios%2C%20showcasing%20its%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-VO%253A%2520A%2520Feature-based%2520Visual%2520Odometry%2520Leveraging%2520a%2520Visual%2520Foundation%250A%2520%2520Model%26entry.906535625%3DMaulana%2520Bisyir%2520Azhari%2520and%2520David%2520Hyunchul%2520Shim%26entry.1292438233%3D%2520%2520Learning-based%2520monocular%2520visual%2520odometry%2520%2528VO%2529%2520poses%2520robustness%252C%250Ageneralization%252C%2520and%2520efficiency%2520challenges%2520in%2520robotics.%2520Recent%2520advances%2520in%250Avisual%2520foundation%2520models%252C%2520such%2520as%2520DINOv2%252C%2520have%2520improved%2520robustness%2520and%250Ageneralization%2520in%2520various%2520vision%2520tasks%252C%2520yet%2520their%2520integration%2520in%2520VO%2520remains%250Alimited%2520due%2520to%2520coarse%2520feature%2520granularity.%2520In%2520this%2520paper%252C%2520we%2520present%2520DINO-VO%252C%2520a%250Afeature-based%2520VO%2520system%2520leveraging%2520DINOv2%2520visual%2520foundation%2520model%2520for%2520its%250Asparse%2520feature%2520matching.%2520To%2520address%2520the%2520integration%2520challenge%252C%2520we%2520propose%2520a%250Asalient%2520keypoints%2520detector%2520tailored%2520to%2520DINOv2%2527s%2520coarse%2520features.%2520Furthermore%252C%250Awe%2520complement%2520DINOv2%2527s%2520robust-semantic%2520features%2520with%2520fine-grained%2520geometric%250Afeatures%252C%2520resulting%2520in%2520more%2520localizable%2520representations.%2520Finally%252C%2520a%250Atransformer-based%2520matcher%2520and%2520differentiable%2520pose%2520estimation%2520layer%2520enable%250Aprecise%2520camera%2520motion%2520estimation%2520by%2520learning%2520good%2520matches.%2520Against%2520prior%250Adetector-descriptor%2520networks%2520like%2520SuperPoint%252C%2520DINO-VO%2520demonstrates%2520greater%250Arobustness%2520in%2520challenging%2520environments.%2520Furthermore%252C%2520we%2520show%2520superior%2520accuracy%250Aand%2520generalization%2520of%2520the%2520proposed%2520feature%2520descriptors%2520against%2520standalone%250ADINOv2%2520coarse%2520features.%2520DINO-VO%2520outperforms%2520prior%2520frame-to-frame%2520VO%2520methods%2520on%250Athe%2520TartanAir%2520and%2520KITTI%2520datasets%2520and%2520is%2520competitive%2520on%2520EuRoC%2520dataset%252C%2520while%250Arunning%2520efficiently%2520at%252072%2520FPS%2520with%2520less%2520than%25201GB%2520of%2520memory%2520usage%2520on%2520a%2520single%250AGPU.%2520Moreover%252C%2520it%2520performs%2520competitively%2520against%2520Visual%2520SLAM%2520systems%2520on%2520outdoor%250Adriving%2520scenarios%252C%2520showcasing%2520its%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-VO%3A%20A%20Feature-based%20Visual%20Odometry%20Leveraging%20a%20Visual%20Foundation%0A%20%20Model&entry.906535625=Maulana%20Bisyir%20Azhari%20and%20David%20Hyunchul%20Shim&entry.1292438233=%20%20Learning-based%20monocular%20visual%20odometry%20%28VO%29%20poses%20robustness%2C%0Ageneralization%2C%20and%20efficiency%20challenges%20in%20robotics.%20Recent%20advances%20in%0Avisual%20foundation%20models%2C%20such%20as%20DINOv2%2C%20have%20improved%20robustness%20and%0Ageneralization%20in%20various%20vision%20tasks%2C%20yet%20their%20integration%20in%20VO%20remains%0Alimited%20due%20to%20coarse%20feature%20granularity.%20In%20this%20paper%2C%20we%20present%20DINO-VO%2C%20a%0Afeature-based%20VO%20system%20leveraging%20DINOv2%20visual%20foundation%20model%20for%20its%0Asparse%20feature%20matching.%20To%20address%20the%20integration%20challenge%2C%20we%20propose%20a%0Asalient%20keypoints%20detector%20tailored%20to%20DINOv2%27s%20coarse%20features.%20Furthermore%2C%0Awe%20complement%20DINOv2%27s%20robust-semantic%20features%20with%20fine-grained%20geometric%0Afeatures%2C%20resulting%20in%20more%20localizable%20representations.%20Finally%2C%20a%0Atransformer-based%20matcher%20and%20differentiable%20pose%20estimation%20layer%20enable%0Aprecise%20camera%20motion%20estimation%20by%20learning%20good%20matches.%20Against%20prior%0Adetector-descriptor%20networks%20like%20SuperPoint%2C%20DINO-VO%20demonstrates%20greater%0Arobustness%20in%20challenging%20environments.%20Furthermore%2C%20we%20show%20superior%20accuracy%0Aand%20generalization%20of%20the%20proposed%20feature%20descriptors%20against%20standalone%0ADINOv2%20coarse%20features.%20DINO-VO%20outperforms%20prior%20frame-to-frame%20VO%20methods%20on%0Athe%20TartanAir%20and%20KITTI%20datasets%20and%20is%20competitive%20on%20EuRoC%20dataset%2C%20while%0Arunning%20efficiently%20at%2072%20FPS%20with%20less%20than%201GB%20of%20memory%20usage%20on%20a%20single%0AGPU.%20Moreover%2C%20it%20performs%20competitively%20against%20Visual%20SLAM%20systems%20on%20outdoor%0Adriving%20scenarios%2C%20showcasing%20its%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13145v1&entry.124074799=Read"},
{"title": "Relation-Aware Slicing in Cross-Domain Alignment", "author": "Dhruv Sarkar and Aprameyo Chakrabartty and Anish Chakrabarty and Swagatam Das", "abstract": "  The Sliced Gromov-Wasserstein (SGW) distance, aiming to relieve the\ncomputational cost of solving a non-convex quadratic program that is the\nGromov-Wasserstein distance, utilizes projecting directions sampled uniformly\nfrom unit hyperspheres. This slicing mechanism incurs unnecessary computational\ncosts due to uninformative directions, which also affects the representative\npower of the distance. However, finding a more appropriate distribution over\nthe projecting directions (slicing distribution) is often an optimization\nproblem in itself that comes with its own computational cost. In addition, with\nmore intricate distributions, the sampling itself may be expensive. As a\nremedy, we propose an optimization-free slicing distribution that provides fast\nsampling for the Monte Carlo approximation. We do so by introducing the\nRelation-Aware Projecting Direction (RAPD), effectively capturing the pairwise\nassociation of each of two pairs of random vectors, each following their\nambient law. This enables us to derive the Relation-Aware Slicing Distribution\n(RASD), a location-scale law corresponding to sampled RAPDs. Finally, we\nintroduce the RASGW distance and its variants, e.g., IWRASGW (Importance\nWeighted RASGW), which overcome the shortcomings experienced by SGW. We\ntheoretically analyze its properties and substantiate its empirical prowess\nusing extensive experiments on various alignment tasks.\n", "link": "http://arxiv.org/abs/2507.13194v1", "date": "2025-07-17", "relevancy": 2.5327, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4916}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relation-Aware%20Slicing%20in%20Cross-Domain%20Alignment&body=Title%3A%20Relation-Aware%20Slicing%20in%20Cross-Domain%20Alignment%0AAuthor%3A%20Dhruv%20Sarkar%20and%20Aprameyo%20Chakrabartty%20and%20Anish%20Chakrabarty%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20The%20Sliced%20Gromov-Wasserstein%20%28SGW%29%20distance%2C%20aiming%20to%20relieve%20the%0Acomputational%20cost%20of%20solving%20a%20non-convex%20quadratic%20program%20that%20is%20the%0AGromov-Wasserstein%20distance%2C%20utilizes%20projecting%20directions%20sampled%20uniformly%0Afrom%20unit%20hyperspheres.%20This%20slicing%20mechanism%20incurs%20unnecessary%20computational%0Acosts%20due%20to%20uninformative%20directions%2C%20which%20also%20affects%20the%20representative%0Apower%20of%20the%20distance.%20However%2C%20finding%20a%20more%20appropriate%20distribution%20over%0Athe%20projecting%20directions%20%28slicing%20distribution%29%20is%20often%20an%20optimization%0Aproblem%20in%20itself%20that%20comes%20with%20its%20own%20computational%20cost.%20In%20addition%2C%20with%0Amore%20intricate%20distributions%2C%20the%20sampling%20itself%20may%20be%20expensive.%20As%20a%0Aremedy%2C%20we%20propose%20an%20optimization-free%20slicing%20distribution%20that%20provides%20fast%0Asampling%20for%20the%20Monte%20Carlo%20approximation.%20We%20do%20so%20by%20introducing%20the%0ARelation-Aware%20Projecting%20Direction%20%28RAPD%29%2C%20effectively%20capturing%20the%20pairwise%0Aassociation%20of%20each%20of%20two%20pairs%20of%20random%20vectors%2C%20each%20following%20their%0Aambient%20law.%20This%20enables%20us%20to%20derive%20the%20Relation-Aware%20Slicing%20Distribution%0A%28RASD%29%2C%20a%20location-scale%20law%20corresponding%20to%20sampled%20RAPDs.%20Finally%2C%20we%0Aintroduce%20the%20RASGW%20distance%20and%20its%20variants%2C%20e.g.%2C%20IWRASGW%20%28Importance%0AWeighted%20RASGW%29%2C%20which%20overcome%20the%20shortcomings%20experienced%20by%20SGW.%20We%0Atheoretically%20analyze%20its%20properties%20and%20substantiate%20its%20empirical%20prowess%0Ausing%20extensive%20experiments%20on%20various%20alignment%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelation-Aware%2520Slicing%2520in%2520Cross-Domain%2520Alignment%26entry.906535625%3DDhruv%2520Sarkar%2520and%2520Aprameyo%2520Chakrabartty%2520and%2520Anish%2520Chakrabarty%2520and%2520Swagatam%2520Das%26entry.1292438233%3D%2520%2520The%2520Sliced%2520Gromov-Wasserstein%2520%2528SGW%2529%2520distance%252C%2520aiming%2520to%2520relieve%2520the%250Acomputational%2520cost%2520of%2520solving%2520a%2520non-convex%2520quadratic%2520program%2520that%2520is%2520the%250AGromov-Wasserstein%2520distance%252C%2520utilizes%2520projecting%2520directions%2520sampled%2520uniformly%250Afrom%2520unit%2520hyperspheres.%2520This%2520slicing%2520mechanism%2520incurs%2520unnecessary%2520computational%250Acosts%2520due%2520to%2520uninformative%2520directions%252C%2520which%2520also%2520affects%2520the%2520representative%250Apower%2520of%2520the%2520distance.%2520However%252C%2520finding%2520a%2520more%2520appropriate%2520distribution%2520over%250Athe%2520projecting%2520directions%2520%2528slicing%2520distribution%2529%2520is%2520often%2520an%2520optimization%250Aproblem%2520in%2520itself%2520that%2520comes%2520with%2520its%2520own%2520computational%2520cost.%2520In%2520addition%252C%2520with%250Amore%2520intricate%2520distributions%252C%2520the%2520sampling%2520itself%2520may%2520be%2520expensive.%2520As%2520a%250Aremedy%252C%2520we%2520propose%2520an%2520optimization-free%2520slicing%2520distribution%2520that%2520provides%2520fast%250Asampling%2520for%2520the%2520Monte%2520Carlo%2520approximation.%2520We%2520do%2520so%2520by%2520introducing%2520the%250ARelation-Aware%2520Projecting%2520Direction%2520%2528RAPD%2529%252C%2520effectively%2520capturing%2520the%2520pairwise%250Aassociation%2520of%2520each%2520of%2520two%2520pairs%2520of%2520random%2520vectors%252C%2520each%2520following%2520their%250Aambient%2520law.%2520This%2520enables%2520us%2520to%2520derive%2520the%2520Relation-Aware%2520Slicing%2520Distribution%250A%2528RASD%2529%252C%2520a%2520location-scale%2520law%2520corresponding%2520to%2520sampled%2520RAPDs.%2520Finally%252C%2520we%250Aintroduce%2520the%2520RASGW%2520distance%2520and%2520its%2520variants%252C%2520e.g.%252C%2520IWRASGW%2520%2528Importance%250AWeighted%2520RASGW%2529%252C%2520which%2520overcome%2520the%2520shortcomings%2520experienced%2520by%2520SGW.%2520We%250Atheoretically%2520analyze%2520its%2520properties%2520and%2520substantiate%2520its%2520empirical%2520prowess%250Ausing%2520extensive%2520experiments%2520on%2520various%2520alignment%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relation-Aware%20Slicing%20in%20Cross-Domain%20Alignment&entry.906535625=Dhruv%20Sarkar%20and%20Aprameyo%20Chakrabartty%20and%20Anish%20Chakrabarty%20and%20Swagatam%20Das&entry.1292438233=%20%20The%20Sliced%20Gromov-Wasserstein%20%28SGW%29%20distance%2C%20aiming%20to%20relieve%20the%0Acomputational%20cost%20of%20solving%20a%20non-convex%20quadratic%20program%20that%20is%20the%0AGromov-Wasserstein%20distance%2C%20utilizes%20projecting%20directions%20sampled%20uniformly%0Afrom%20unit%20hyperspheres.%20This%20slicing%20mechanism%20incurs%20unnecessary%20computational%0Acosts%20due%20to%20uninformative%20directions%2C%20which%20also%20affects%20the%20representative%0Apower%20of%20the%20distance.%20However%2C%20finding%20a%20more%20appropriate%20distribution%20over%0Athe%20projecting%20directions%20%28slicing%20distribution%29%20is%20often%20an%20optimization%0Aproblem%20in%20itself%20that%20comes%20with%20its%20own%20computational%20cost.%20In%20addition%2C%20with%0Amore%20intricate%20distributions%2C%20the%20sampling%20itself%20may%20be%20expensive.%20As%20a%0Aremedy%2C%20we%20propose%20an%20optimization-free%20slicing%20distribution%20that%20provides%20fast%0Asampling%20for%20the%20Monte%20Carlo%20approximation.%20We%20do%20so%20by%20introducing%20the%0ARelation-Aware%20Projecting%20Direction%20%28RAPD%29%2C%20effectively%20capturing%20the%20pairwise%0Aassociation%20of%20each%20of%20two%20pairs%20of%20random%20vectors%2C%20each%20following%20their%0Aambient%20law.%20This%20enables%20us%20to%20derive%20the%20Relation-Aware%20Slicing%20Distribution%0A%28RASD%29%2C%20a%20location-scale%20law%20corresponding%20to%20sampled%20RAPDs.%20Finally%2C%20we%0Aintroduce%20the%20RASGW%20distance%20and%20its%20variants%2C%20e.g.%2C%20IWRASGW%20%28Importance%0AWeighted%20RASGW%29%2C%20which%20overcome%20the%20shortcomings%20experienced%20by%20SGW.%20We%0Atheoretically%20analyze%20its%20properties%20and%20substantiate%20its%20empirical%20prowess%0Ausing%20extensive%20experiments%20on%20various%20alignment%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13194v1&entry.124074799=Read"},
{"title": "SMART: Relation-Aware Learning of Geometric Representations for\n  Knowledge Graphs", "author": "Kossi Amouzouvi and Bowen Song and Andrea Coletta and Luigi Bellomarini and Jens Lehmann and Sahar Vahdati", "abstract": "  Knowledge graph representation learning approaches provide a mapping between\nsymbolic knowledge in the form of triples in a knowledge graph (KG) and their\nfeature vectors. Knowledge graph embedding (KGE) models often represent\nrelations in a KG as geometric transformations. Most state-of-the-art (SOTA)\nKGE models are derived from elementary geometric transformations (EGTs), such\nas translation, scaling, rotation, and reflection, or their combinations. These\ngeometric transformations enable the models to effectively preserve specific\nstructural and relational patterns of the KG. However, the current use of EGTs\nby KGEs remains insufficient without considering relation-specific\ntransformations. Although recent models attempted to address this problem by\nensembling SOTA baseline models in different ways, only a single or composite\nversion of geometric transformations are used by such baselines to represent\nall the relations. In this paper, we propose a framework that evaluates how\nwell each relation fits with different geometric transformations. Based on this\nranking, the model can: (1) assign the best-matching transformation to each\nrelation, or (2) use majority voting to choose one transformation type to apply\nacross all relations. That is, the model learns a single relation-specific EGT\nin low dimensional vector space through an attention mechanism. Furthermore, we\nuse the correlation between relations and EGTs, which are learned in a low\ndimension, for relation embeddings in a high dimensional vector space. The\neffectiveness of our models is demonstrated through comprehensive evaluations\non three benchmark KGs as well as a real-world financial KG, witnessing a\nperformance comparable to leading models\n", "link": "http://arxiv.org/abs/2507.13001v1", "date": "2025-07-17", "relevancy": 2.4921, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5117}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4973}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Relation-Aware%20Learning%20of%20Geometric%20Representations%20for%0A%20%20Knowledge%20Graphs&body=Title%3A%20SMART%3A%20Relation-Aware%20Learning%20of%20Geometric%20Representations%20for%0A%20%20Knowledge%20Graphs%0AAuthor%3A%20Kossi%20Amouzouvi%20and%20Bowen%20Song%20and%20Andrea%20Coletta%20and%20Luigi%20Bellomarini%20and%20Jens%20Lehmann%20and%20Sahar%20Vahdati%0AAbstract%3A%20%20%20Knowledge%20graph%20representation%20learning%20approaches%20provide%20a%20mapping%20between%0Asymbolic%20knowledge%20in%20the%20form%20of%20triples%20in%20a%20knowledge%20graph%20%28KG%29%20and%20their%0Afeature%20vectors.%20Knowledge%20graph%20embedding%20%28KGE%29%20models%20often%20represent%0Arelations%20in%20a%20KG%20as%20geometric%20transformations.%20Most%20state-of-the-art%20%28SOTA%29%0AKGE%20models%20are%20derived%20from%20elementary%20geometric%20transformations%20%28EGTs%29%2C%20such%0Aas%20translation%2C%20scaling%2C%20rotation%2C%20and%20reflection%2C%20or%20their%20combinations.%20These%0Ageometric%20transformations%20enable%20the%20models%20to%20effectively%20preserve%20specific%0Astructural%20and%20relational%20patterns%20of%20the%20KG.%20However%2C%20the%20current%20use%20of%20EGTs%0Aby%20KGEs%20remains%20insufficient%20without%20considering%20relation-specific%0Atransformations.%20Although%20recent%20models%20attempted%20to%20address%20this%20problem%20by%0Aensembling%20SOTA%20baseline%20models%20in%20different%20ways%2C%20only%20a%20single%20or%20composite%0Aversion%20of%20geometric%20transformations%20are%20used%20by%20such%20baselines%20to%20represent%0Aall%20the%20relations.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20that%20evaluates%20how%0Awell%20each%20relation%20fits%20with%20different%20geometric%20transformations.%20Based%20on%20this%0Aranking%2C%20the%20model%20can%3A%20%281%29%20assign%20the%20best-matching%20transformation%20to%20each%0Arelation%2C%20or%20%282%29%20use%20majority%20voting%20to%20choose%20one%20transformation%20type%20to%20apply%0Aacross%20all%20relations.%20That%20is%2C%20the%20model%20learns%20a%20single%20relation-specific%20EGT%0Ain%20low%20dimensional%20vector%20space%20through%20an%20attention%20mechanism.%20Furthermore%2C%20we%0Ause%20the%20correlation%20between%20relations%20and%20EGTs%2C%20which%20are%20learned%20in%20a%20low%0Adimension%2C%20for%20relation%20embeddings%20in%20a%20high%20dimensional%20vector%20space.%20The%0Aeffectiveness%20of%20our%20models%20is%20demonstrated%20through%20comprehensive%20evaluations%0Aon%20three%20benchmark%20KGs%20as%20well%20as%20a%20real-world%20financial%20KG%2C%20witnessing%20a%0Aperformance%20comparable%20to%20leading%20models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Relation-Aware%2520Learning%2520of%2520Geometric%2520Representations%2520for%250A%2520%2520Knowledge%2520Graphs%26entry.906535625%3DKossi%2520Amouzouvi%2520and%2520Bowen%2520Song%2520and%2520Andrea%2520Coletta%2520and%2520Luigi%2520Bellomarini%2520and%2520Jens%2520Lehmann%2520and%2520Sahar%2520Vahdati%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520representation%2520learning%2520approaches%2520provide%2520a%2520mapping%2520between%250Asymbolic%2520knowledge%2520in%2520the%2520form%2520of%2520triples%2520in%2520a%2520knowledge%2520graph%2520%2528KG%2529%2520and%2520their%250Afeature%2520vectors.%2520Knowledge%2520graph%2520embedding%2520%2528KGE%2529%2520models%2520often%2520represent%250Arelations%2520in%2520a%2520KG%2520as%2520geometric%2520transformations.%2520Most%2520state-of-the-art%2520%2528SOTA%2529%250AKGE%2520models%2520are%2520derived%2520from%2520elementary%2520geometric%2520transformations%2520%2528EGTs%2529%252C%2520such%250Aas%2520translation%252C%2520scaling%252C%2520rotation%252C%2520and%2520reflection%252C%2520or%2520their%2520combinations.%2520These%250Ageometric%2520transformations%2520enable%2520the%2520models%2520to%2520effectively%2520preserve%2520specific%250Astructural%2520and%2520relational%2520patterns%2520of%2520the%2520KG.%2520However%252C%2520the%2520current%2520use%2520of%2520EGTs%250Aby%2520KGEs%2520remains%2520insufficient%2520without%2520considering%2520relation-specific%250Atransformations.%2520Although%2520recent%2520models%2520attempted%2520to%2520address%2520this%2520problem%2520by%250Aensembling%2520SOTA%2520baseline%2520models%2520in%2520different%2520ways%252C%2520only%2520a%2520single%2520or%2520composite%250Aversion%2520of%2520geometric%2520transformations%2520are%2520used%2520by%2520such%2520baselines%2520to%2520represent%250Aall%2520the%2520relations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520that%2520evaluates%2520how%250Awell%2520each%2520relation%2520fits%2520with%2520different%2520geometric%2520transformations.%2520Based%2520on%2520this%250Aranking%252C%2520the%2520model%2520can%253A%2520%25281%2529%2520assign%2520the%2520best-matching%2520transformation%2520to%2520each%250Arelation%252C%2520or%2520%25282%2529%2520use%2520majority%2520voting%2520to%2520choose%2520one%2520transformation%2520type%2520to%2520apply%250Aacross%2520all%2520relations.%2520That%2520is%252C%2520the%2520model%2520learns%2520a%2520single%2520relation-specific%2520EGT%250Ain%2520low%2520dimensional%2520vector%2520space%2520through%2520an%2520attention%2520mechanism.%2520Furthermore%252C%2520we%250Ause%2520the%2520correlation%2520between%2520relations%2520and%2520EGTs%252C%2520which%2520are%2520learned%2520in%2520a%2520low%250Adimension%252C%2520for%2520relation%2520embeddings%2520in%2520a%2520high%2520dimensional%2520vector%2520space.%2520The%250Aeffectiveness%2520of%2520our%2520models%2520is%2520demonstrated%2520through%2520comprehensive%2520evaluations%250Aon%2520three%2520benchmark%2520KGs%2520as%2520well%2520as%2520a%2520real-world%2520financial%2520KG%252C%2520witnessing%2520a%250Aperformance%2520comparable%2520to%2520leading%2520models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Relation-Aware%20Learning%20of%20Geometric%20Representations%20for%0A%20%20Knowledge%20Graphs&entry.906535625=Kossi%20Amouzouvi%20and%20Bowen%20Song%20and%20Andrea%20Coletta%20and%20Luigi%20Bellomarini%20and%20Jens%20Lehmann%20and%20Sahar%20Vahdati&entry.1292438233=%20%20Knowledge%20graph%20representation%20learning%20approaches%20provide%20a%20mapping%20between%0Asymbolic%20knowledge%20in%20the%20form%20of%20triples%20in%20a%20knowledge%20graph%20%28KG%29%20and%20their%0Afeature%20vectors.%20Knowledge%20graph%20embedding%20%28KGE%29%20models%20often%20represent%0Arelations%20in%20a%20KG%20as%20geometric%20transformations.%20Most%20state-of-the-art%20%28SOTA%29%0AKGE%20models%20are%20derived%20from%20elementary%20geometric%20transformations%20%28EGTs%29%2C%20such%0Aas%20translation%2C%20scaling%2C%20rotation%2C%20and%20reflection%2C%20or%20their%20combinations.%20These%0Ageometric%20transformations%20enable%20the%20models%20to%20effectively%20preserve%20specific%0Astructural%20and%20relational%20patterns%20of%20the%20KG.%20However%2C%20the%20current%20use%20of%20EGTs%0Aby%20KGEs%20remains%20insufficient%20without%20considering%20relation-specific%0Atransformations.%20Although%20recent%20models%20attempted%20to%20address%20this%20problem%20by%0Aensembling%20SOTA%20baseline%20models%20in%20different%20ways%2C%20only%20a%20single%20or%20composite%0Aversion%20of%20geometric%20transformations%20are%20used%20by%20such%20baselines%20to%20represent%0Aall%20the%20relations.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20that%20evaluates%20how%0Awell%20each%20relation%20fits%20with%20different%20geometric%20transformations.%20Based%20on%20this%0Aranking%2C%20the%20model%20can%3A%20%281%29%20assign%20the%20best-matching%20transformation%20to%20each%0Arelation%2C%20or%20%282%29%20use%20majority%20voting%20to%20choose%20one%20transformation%20type%20to%20apply%0Aacross%20all%20relations.%20That%20is%2C%20the%20model%20learns%20a%20single%20relation-specific%20EGT%0Ain%20low%20dimensional%20vector%20space%20through%20an%20attention%20mechanism.%20Furthermore%2C%20we%0Ause%20the%20correlation%20between%20relations%20and%20EGTs%2C%20which%20are%20learned%20in%20a%20low%0Adimension%2C%20for%20relation%20embeddings%20in%20a%20high%20dimensional%20vector%20space.%20The%0Aeffectiveness%20of%20our%20models%20is%20demonstrated%20through%20comprehensive%20evaluations%0Aon%20three%20benchmark%20KGs%20as%20well%20as%20a%20real-world%20financial%20KG%2C%20witnessing%20a%0Aperformance%20comparable%20to%20leading%20models%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13001v1&entry.124074799=Read"},
{"title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers", "author": "Qiang Wang and Mengchao Wang and Fan Jiang and Yaqi Fan and Yonggang Qi and Mu Xu", "abstract": "  Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.\n", "link": "http://arxiv.org/abs/2507.12956v1", "date": "2025-07-17", "relevancy": 2.4834, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6535}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5978}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FantasyPortrait%3A%20Enhancing%20Multi-Character%20Portrait%20Animation%20with%0A%20%20Expression-Augmented%20Diffusion%20Transformers&body=Title%3A%20FantasyPortrait%3A%20Enhancing%20Multi-Character%20Portrait%20Animation%20with%0A%20%20Expression-Augmented%20Diffusion%20Transformers%0AAuthor%3A%20Qiang%20Wang%20and%20Mengchao%20Wang%20and%20Fan%20Jiang%20and%20Yaqi%20Fan%20and%20Yonggang%20Qi%20and%20Mu%20Xu%0AAbstract%3A%20%20%20Producing%20expressive%20facial%20animations%20from%20static%20images%20is%20a%20challenging%0Atask.%20Prior%20methods%20relying%20on%20explicit%20geometric%20priors%20%28e.g.%2C%20facial%0Alandmarks%20or%203DMM%29%20often%20suffer%20from%20artifacts%20in%20cross%20reenactment%20and%0Astruggle%20to%20capture%20subtle%20emotions.%20Furthermore%2C%20existing%20approaches%20lack%0Asupport%20for%20multi-character%20animation%2C%20as%20driving%20features%20from%20different%0Aindividuals%20frequently%20interfere%20with%20one%20another%2C%20complicating%20the%20task.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20FantasyPortrait%2C%20a%20diffusion%20transformer%0Abased%20framework%20capable%20of%20generating%20high-fidelity%20and%20emotion-rich%20animations%0Afor%20both%20single-%20and%20multi-character%20scenarios.%20Our%20method%20introduces%20an%0Aexpression-augmented%20learning%20strategy%20that%20utilizes%20implicit%20representations%0Ato%20capture%20identity-agnostic%20facial%20dynamics%2C%20enhancing%20the%20model%27s%20ability%20to%0Arender%20fine-grained%20emotions.%20For%20multi-character%20control%2C%20we%20design%20a%20masked%0Across-attention%20mechanism%20that%20ensures%20independent%20yet%20coordinated%20expression%0Ageneration%2C%20effectively%20preventing%20feature%20interference.%20To%20advance%20research%20in%0Athis%20area%2C%20we%20propose%20the%20Multi-Expr%20dataset%20and%20ExprBench%2C%20which%20are%0Aspecifically%20designed%20datasets%20and%20benchmarks%20for%20training%20and%20evaluating%0Amulti-character%20portrait%20animations.%20Extensive%20experiments%20demonstrate%20that%0AFantasyPortrait%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%0Aquantitative%20metrics%20and%20qualitative%20evaluations%2C%20excelling%20particularly%20in%0Achallenging%20cross%20reenactment%20and%20multi-character%20contexts.%20Our%20project%20page%20is%0Ahttps%3A//fantasy-amap.github.io/fantasy-portrait/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFantasyPortrait%253A%2520Enhancing%2520Multi-Character%2520Portrait%2520Animation%2520with%250A%2520%2520Expression-Augmented%2520Diffusion%2520Transformers%26entry.906535625%3DQiang%2520Wang%2520and%2520Mengchao%2520Wang%2520and%2520Fan%2520Jiang%2520and%2520Yaqi%2520Fan%2520and%2520Yonggang%2520Qi%2520and%2520Mu%2520Xu%26entry.1292438233%3D%2520%2520Producing%2520expressive%2520facial%2520animations%2520from%2520static%2520images%2520is%2520a%2520challenging%250Atask.%2520Prior%2520methods%2520relying%2520on%2520explicit%2520geometric%2520priors%2520%2528e.g.%252C%2520facial%250Alandmarks%2520or%25203DMM%2529%2520often%2520suffer%2520from%2520artifacts%2520in%2520cross%2520reenactment%2520and%250Astruggle%2520to%2520capture%2520subtle%2520emotions.%2520Furthermore%252C%2520existing%2520approaches%2520lack%250Asupport%2520for%2520multi-character%2520animation%252C%2520as%2520driving%2520features%2520from%2520different%250Aindividuals%2520frequently%2520interfere%2520with%2520one%2520another%252C%2520complicating%2520the%2520task.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520FantasyPortrait%252C%2520a%2520diffusion%2520transformer%250Abased%2520framework%2520capable%2520of%2520generating%2520high-fidelity%2520and%2520emotion-rich%2520animations%250Afor%2520both%2520single-%2520and%2520multi-character%2520scenarios.%2520Our%2520method%2520introduces%2520an%250Aexpression-augmented%2520learning%2520strategy%2520that%2520utilizes%2520implicit%2520representations%250Ato%2520capture%2520identity-agnostic%2520facial%2520dynamics%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%250Arender%2520fine-grained%2520emotions.%2520For%2520multi-character%2520control%252C%2520we%2520design%2520a%2520masked%250Across-attention%2520mechanism%2520that%2520ensures%2520independent%2520yet%2520coordinated%2520expression%250Ageneration%252C%2520effectively%2520preventing%2520feature%2520interference.%2520To%2520advance%2520research%2520in%250Athis%2520area%252C%2520we%2520propose%2520the%2520Multi-Expr%2520dataset%2520and%2520ExprBench%252C%2520which%2520are%250Aspecifically%2520designed%2520datasets%2520and%2520benchmarks%2520for%2520training%2520and%2520evaluating%250Amulti-character%2520portrait%2520animations.%2520Extensive%2520experiments%2520demonstrate%2520that%250AFantasyPortrait%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%250Aquantitative%2520metrics%2520and%2520qualitative%2520evaluations%252C%2520excelling%2520particularly%2520in%250Achallenging%2520cross%2520reenactment%2520and%2520multi-character%2520contexts.%2520Our%2520project%2520page%2520is%250Ahttps%253A//fantasy-amap.github.io/fantasy-portrait/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FantasyPortrait%3A%20Enhancing%20Multi-Character%20Portrait%20Animation%20with%0A%20%20Expression-Augmented%20Diffusion%20Transformers&entry.906535625=Qiang%20Wang%20and%20Mengchao%20Wang%20and%20Fan%20Jiang%20and%20Yaqi%20Fan%20and%20Yonggang%20Qi%20and%20Mu%20Xu&entry.1292438233=%20%20Producing%20expressive%20facial%20animations%20from%20static%20images%20is%20a%20challenging%0Atask.%20Prior%20methods%20relying%20on%20explicit%20geometric%20priors%20%28e.g.%2C%20facial%0Alandmarks%20or%203DMM%29%20often%20suffer%20from%20artifacts%20in%20cross%20reenactment%20and%0Astruggle%20to%20capture%20subtle%20emotions.%20Furthermore%2C%20existing%20approaches%20lack%0Asupport%20for%20multi-character%20animation%2C%20as%20driving%20features%20from%20different%0Aindividuals%20frequently%20interfere%20with%20one%20another%2C%20complicating%20the%20task.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20FantasyPortrait%2C%20a%20diffusion%20transformer%0Abased%20framework%20capable%20of%20generating%20high-fidelity%20and%20emotion-rich%20animations%0Afor%20both%20single-%20and%20multi-character%20scenarios.%20Our%20method%20introduces%20an%0Aexpression-augmented%20learning%20strategy%20that%20utilizes%20implicit%20representations%0Ato%20capture%20identity-agnostic%20facial%20dynamics%2C%20enhancing%20the%20model%27s%20ability%20to%0Arender%20fine-grained%20emotions.%20For%20multi-character%20control%2C%20we%20design%20a%20masked%0Across-attention%20mechanism%20that%20ensures%20independent%20yet%20coordinated%20expression%0Ageneration%2C%20effectively%20preventing%20feature%20interference.%20To%20advance%20research%20in%0Athis%20area%2C%20we%20propose%20the%20Multi-Expr%20dataset%20and%20ExprBench%2C%20which%20are%0Aspecifically%20designed%20datasets%20and%20benchmarks%20for%20training%20and%20evaluating%0Amulti-character%20portrait%20animations.%20Extensive%20experiments%20demonstrate%20that%0AFantasyPortrait%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%0Aquantitative%20metrics%20and%20qualitative%20evaluations%2C%20excelling%20particularly%20in%0Achallenging%20cross%20reenactment%20and%20multi-character%20contexts.%20Our%20project%20page%20is%0Ahttps%3A//fantasy-amap.github.io/fantasy-portrait/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12956v1&entry.124074799=Read"},
{"title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial and Legged Odometry\n  Fusion SLAM for Dynamic Legged Robotics", "author": "Tingyang Xiao and Xiaolin Zhou and Liu Liu and Wei Sui and Wei Feng and Jiaxiong Qiu and Xinjie Wang and Zhizhong Su", "abstract": "  This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robotics undergoing aggressive and high-frequency\nmotions.By integrating geometric consistency, legged odometry constraints, and\ndual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/HorizonRobotics/geoflow-slam\n", "link": "http://arxiv.org/abs/2503.14247v2", "date": "2025-07-17", "relevancy": 2.4781, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoFlow-SLAM%3A%20A%20Robust%20Tightly-Coupled%20RGBD-Inertial%20and%20Legged%20Odometry%0A%20%20Fusion%20SLAM%20for%20Dynamic%20Legged%20Robotics&body=Title%3A%20GeoFlow-SLAM%3A%20A%20Robust%20Tightly-Coupled%20RGBD-Inertial%20and%20Legged%20Odometry%0A%20%20Fusion%20SLAM%20for%20Dynamic%20Legged%20Robotics%0AAuthor%3A%20Tingyang%20Xiao%20and%20Xiaolin%20Zhou%20and%20Liu%20Liu%20and%20Wei%20Sui%20and%20Wei%20Feng%20and%20Jiaxiong%20Qiu%20and%20Xinjie%20Wang%20and%20Zhizhong%20Su%0AAbstract%3A%20%20%20This%20paper%20presents%20GeoFlow-SLAM%2C%20a%20robust%20and%20effective%20Tightly-Coupled%0ARGBD-inertial%20SLAM%20for%20legged%20robotics%20undergoing%20aggressive%20and%20high-frequency%0Amotions.By%20integrating%20geometric%20consistency%2C%20legged%20odometry%20constraints%2C%20and%0Adual-stream%20optical%20flow%20%28GeoFlow%29%2C%20our%20method%20addresses%20three%20critical%0Achallenges%3Afeature%20matching%20and%20pose%20initialization%20failures%20during%20fast%0Alocomotion%20and%20visual%20feature%20scarcity%20in%20texture-less%20scenes.Specifically%2C%20in%0Arapid%20motion%20scenarios%2C%20feature%20matching%20is%20notably%20enhanced%20by%20leveraging%0Adual-stream%20optical%20flow%2C%20which%20combines%20prior%20map%20points%20and%20poses.%0AAdditionally%2C%20we%20propose%20a%20robust%20pose%20initialization%20method%20for%20fast%0Alocomotion%20and%20IMU%20error%20in%20legged%20robots%2C%20integrating%20IMU/Legged%20odometry%2C%0Ainter-frame%20Perspective-n-Point%20%28PnP%29%2C%20and%20Generalized%20Iterative%20Closest%20Point%0A%28GICP%29.%20Furthermore%2C%20a%20novel%20optimization%20framework%20that%20tightly%20couples%0Adepth-to-map%20and%20GICP%20geometric%20constraints%20is%20first%20introduced%20to%20improve%20the%0Arobustness%20and%20accuracy%20in%20long-duration%2C%20visually%20texture-less%20environments.%0AThe%20proposed%20algorithms%20achieve%20state-of-the-art%20%28SOTA%29%20on%20collected%20legged%0Arobots%20and%20open-source%20datasets.%20To%20further%20promote%20research%20and%20development%2C%0Athe%20open-source%20datasets%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/geoflow-slam%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoFlow-SLAM%253A%2520A%2520Robust%2520Tightly-Coupled%2520RGBD-Inertial%2520and%2520Legged%2520Odometry%250A%2520%2520Fusion%2520SLAM%2520for%2520Dynamic%2520Legged%2520Robotics%26entry.906535625%3DTingyang%2520Xiao%2520and%2520Xiaolin%2520Zhou%2520and%2520Liu%2520Liu%2520and%2520Wei%2520Sui%2520and%2520Wei%2520Feng%2520and%2520Jiaxiong%2520Qiu%2520and%2520Xinjie%2520Wang%2520and%2520Zhizhong%2520Su%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520GeoFlow-SLAM%252C%2520a%2520robust%2520and%2520effective%2520Tightly-Coupled%250ARGBD-inertial%2520SLAM%2520for%2520legged%2520robotics%2520undergoing%2520aggressive%2520and%2520high-frequency%250Amotions.By%2520integrating%2520geometric%2520consistency%252C%2520legged%2520odometry%2520constraints%252C%2520and%250Adual-stream%2520optical%2520flow%2520%2528GeoFlow%2529%252C%2520our%2520method%2520addresses%2520three%2520critical%250Achallenges%253Afeature%2520matching%2520and%2520pose%2520initialization%2520failures%2520during%2520fast%250Alocomotion%2520and%2520visual%2520feature%2520scarcity%2520in%2520texture-less%2520scenes.Specifically%252C%2520in%250Arapid%2520motion%2520scenarios%252C%2520feature%2520matching%2520is%2520notably%2520enhanced%2520by%2520leveraging%250Adual-stream%2520optical%2520flow%252C%2520which%2520combines%2520prior%2520map%2520points%2520and%2520poses.%250AAdditionally%252C%2520we%2520propose%2520a%2520robust%2520pose%2520initialization%2520method%2520for%2520fast%250Alocomotion%2520and%2520IMU%2520error%2520in%2520legged%2520robots%252C%2520integrating%2520IMU/Legged%2520odometry%252C%250Ainter-frame%2520Perspective-n-Point%2520%2528PnP%2529%252C%2520and%2520Generalized%2520Iterative%2520Closest%2520Point%250A%2528GICP%2529.%2520Furthermore%252C%2520a%2520novel%2520optimization%2520framework%2520that%2520tightly%2520couples%250Adepth-to-map%2520and%2520GICP%2520geometric%2520constraints%2520is%2520first%2520introduced%2520to%2520improve%2520the%250Arobustness%2520and%2520accuracy%2520in%2520long-duration%252C%2520visually%2520texture-less%2520environments.%250AThe%2520proposed%2520algorithms%2520achieve%2520state-of-the-art%2520%2528SOTA%2529%2520on%2520collected%2520legged%250Arobots%2520and%2520open-source%2520datasets.%2520To%2520further%2520promote%2520research%2520and%2520development%252C%250Athe%2520open-source%2520datasets%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HorizonRobotics/geoflow-slam%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoFlow-SLAM%3A%20A%20Robust%20Tightly-Coupled%20RGBD-Inertial%20and%20Legged%20Odometry%0A%20%20Fusion%20SLAM%20for%20Dynamic%20Legged%20Robotics&entry.906535625=Tingyang%20Xiao%20and%20Xiaolin%20Zhou%20and%20Liu%20Liu%20and%20Wei%20Sui%20and%20Wei%20Feng%20and%20Jiaxiong%20Qiu%20and%20Xinjie%20Wang%20and%20Zhizhong%20Su&entry.1292438233=%20%20This%20paper%20presents%20GeoFlow-SLAM%2C%20a%20robust%20and%20effective%20Tightly-Coupled%0ARGBD-inertial%20SLAM%20for%20legged%20robotics%20undergoing%20aggressive%20and%20high-frequency%0Amotions.By%20integrating%20geometric%20consistency%2C%20legged%20odometry%20constraints%2C%20and%0Adual-stream%20optical%20flow%20%28GeoFlow%29%2C%20our%20method%20addresses%20three%20critical%0Achallenges%3Afeature%20matching%20and%20pose%20initialization%20failures%20during%20fast%0Alocomotion%20and%20visual%20feature%20scarcity%20in%20texture-less%20scenes.Specifically%2C%20in%0Arapid%20motion%20scenarios%2C%20feature%20matching%20is%20notably%20enhanced%20by%20leveraging%0Adual-stream%20optical%20flow%2C%20which%20combines%20prior%20map%20points%20and%20poses.%0AAdditionally%2C%20we%20propose%20a%20robust%20pose%20initialization%20method%20for%20fast%0Alocomotion%20and%20IMU%20error%20in%20legged%20robots%2C%20integrating%20IMU/Legged%20odometry%2C%0Ainter-frame%20Perspective-n-Point%20%28PnP%29%2C%20and%20Generalized%20Iterative%20Closest%20Point%0A%28GICP%29.%20Furthermore%2C%20a%20novel%20optimization%20framework%20that%20tightly%20couples%0Adepth-to-map%20and%20GICP%20geometric%20constraints%20is%20first%20introduced%20to%20improve%20the%0Arobustness%20and%20accuracy%20in%20long-duration%2C%20visually%20texture-less%20environments.%0AThe%20proposed%20algorithms%20achieve%20state-of-the-art%20%28SOTA%29%20on%20collected%20legged%0Arobots%20and%20open-source%20datasets.%20To%20further%20promote%20research%20and%20development%2C%0Athe%20open-source%20datasets%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/geoflow-slam%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14247v2&entry.124074799=Read"},
{"title": "Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID", "author": "Koray Ulusan and Benjamin Kiefer", "abstract": "  Personalizing Stable Diffusion for professional portrait generation from\namateur photos faces challenges in maintaining facial resemblance. This paper\nevaluates the impact of augmentation strategies on two personalization methods:\nDreamBooth and InstantID. We compare classical augmentations (flipping,\ncropping, color adjustments) with generative augmentation using InstantID's\nsynthetic images to enrich training data. Using SDXL and a new FaceDistance\nmetric based on FaceNet, we quantitatively assess facial similarity. Results\nshow classical augmentations can cause artifacts harming identity retention,\nwhile InstantID improves fidelity when balanced with real images to avoid\noverfitting. A user study with 97 participants confirms high photorealism and\npreferences for InstantID's polished look versus DreamBooth's identity\naccuracy. Our findings inform effective augmentation strategies for\npersonalized text-to-image generation.\n", "link": "http://arxiv.org/abs/2505.03557v2", "date": "2025-07-17", "relevancy": 2.4758, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6312}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6215}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID&body=Title%3A%20Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID%0AAuthor%3A%20Koray%20Ulusan%20and%20Benjamin%20Kiefer%0AAbstract%3A%20%20%20Personalizing%20Stable%20Diffusion%20for%20professional%20portrait%20generation%20from%0Aamateur%20photos%20faces%20challenges%20in%20maintaining%20facial%20resemblance.%20This%20paper%0Aevaluates%20the%20impact%20of%20augmentation%20strategies%20on%20two%20personalization%20methods%3A%0ADreamBooth%20and%20InstantID.%20We%20compare%20classical%20augmentations%20%28flipping%2C%0Acropping%2C%20color%20adjustments%29%20with%20generative%20augmentation%20using%20InstantID%27s%0Asynthetic%20images%20to%20enrich%20training%20data.%20Using%20SDXL%20and%20a%20new%20FaceDistance%0Ametric%20based%20on%20FaceNet%2C%20we%20quantitatively%20assess%20facial%20similarity.%20Results%0Ashow%20classical%20augmentations%20can%20cause%20artifacts%20harming%20identity%20retention%2C%0Awhile%20InstantID%20improves%20fidelity%20when%20balanced%20with%20real%20images%20to%20avoid%0Aoverfitting.%20A%20user%20study%20with%2097%20participants%20confirms%20high%20photorealism%20and%0Apreferences%20for%20InstantID%27s%20polished%20look%20versus%20DreamBooth%27s%20identity%0Aaccuracy.%20Our%20findings%20inform%20effective%20augmentation%20strategies%20for%0Apersonalized%20text-to-image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Synthetic%2520Data%2520via%2520Augmentations%2520for%2520Improved%2520Facial%250A%2520%2520Resemblance%2520in%2520DreamBooth%2520and%2520InstantID%26entry.906535625%3DKoray%2520Ulusan%2520and%2520Benjamin%2520Kiefer%26entry.1292438233%3D%2520%2520Personalizing%2520Stable%2520Diffusion%2520for%2520professional%2520portrait%2520generation%2520from%250Aamateur%2520photos%2520faces%2520challenges%2520in%2520maintaining%2520facial%2520resemblance.%2520This%2520paper%250Aevaluates%2520the%2520impact%2520of%2520augmentation%2520strategies%2520on%2520two%2520personalization%2520methods%253A%250ADreamBooth%2520and%2520InstantID.%2520We%2520compare%2520classical%2520augmentations%2520%2528flipping%252C%250Acropping%252C%2520color%2520adjustments%2529%2520with%2520generative%2520augmentation%2520using%2520InstantID%2527s%250Asynthetic%2520images%2520to%2520enrich%2520training%2520data.%2520Using%2520SDXL%2520and%2520a%2520new%2520FaceDistance%250Ametric%2520based%2520on%2520FaceNet%252C%2520we%2520quantitatively%2520assess%2520facial%2520similarity.%2520Results%250Ashow%2520classical%2520augmentations%2520can%2520cause%2520artifacts%2520harming%2520identity%2520retention%252C%250Awhile%2520InstantID%2520improves%2520fidelity%2520when%2520balanced%2520with%2520real%2520images%2520to%2520avoid%250Aoverfitting.%2520A%2520user%2520study%2520with%252097%2520participants%2520confirms%2520high%2520photorealism%2520and%250Apreferences%2520for%2520InstantID%2527s%2520polished%2520look%2520versus%2520DreamBooth%2527s%2520identity%250Aaccuracy.%2520Our%2520findings%2520inform%2520effective%2520augmentation%2520strategies%2520for%250Apersonalized%2520text-to-image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID&entry.906535625=Koray%20Ulusan%20and%20Benjamin%20Kiefer&entry.1292438233=%20%20Personalizing%20Stable%20Diffusion%20for%20professional%20portrait%20generation%20from%0Aamateur%20photos%20faces%20challenges%20in%20maintaining%20facial%20resemblance.%20This%20paper%0Aevaluates%20the%20impact%20of%20augmentation%20strategies%20on%20two%20personalization%20methods%3A%0ADreamBooth%20and%20InstantID.%20We%20compare%20classical%20augmentations%20%28flipping%2C%0Acropping%2C%20color%20adjustments%29%20with%20generative%20augmentation%20using%20InstantID%27s%0Asynthetic%20images%20to%20enrich%20training%20data.%20Using%20SDXL%20and%20a%20new%20FaceDistance%0Ametric%20based%20on%20FaceNet%2C%20we%20quantitatively%20assess%20facial%20similarity.%20Results%0Ashow%20classical%20augmentations%20can%20cause%20artifacts%20harming%20identity%20retention%2C%0Awhile%20InstantID%20improves%20fidelity%20when%20balanced%20with%20real%20images%20to%20avoid%0Aoverfitting.%20A%20user%20study%20with%2097%20participants%20confirms%20high%20photorealism%20and%0Apreferences%20for%20InstantID%27s%20polished%20look%20versus%20DreamBooth%27s%20identity%0Aaccuracy.%20Our%20findings%20inform%20effective%20augmentation%20strategies%20for%0Apersonalized%20text-to-image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03557v2&entry.124074799=Read"},
{"title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in\n  Dynamic Environments", "author": "Zheng Jia and Shengbin Yue and Wei Chen and Siyuan Wang and Yidong Liu and Yun Song and Zhongyu Wei", "abstract": "  The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research.\n", "link": "http://arxiv.org/abs/2507.04037v2", "date": "2025-07-17", "relevancy": 2.4727, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%0A%20%20Dynamic%20Environments&body=Title%3A%20Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%0A%20%20Dynamic%20Environments%0AAuthor%3A%20Zheng%20Jia%20and%20Shengbin%20Yue%20and%20Wei%20Chen%20and%20Siyuan%20Wang%20and%20Yidong%20Liu%20and%20Yun%20Song%20and%20Zhongyu%20Wei%0AAbstract%3A%20%20%20The%20gap%20between%20static%20benchmarks%20and%20the%20dynamic%20nature%20of%20real-world%20legal%0Apractice%20poses%20a%20key%20barrier%20to%20advancing%20legal%20intelligence.%20To%20this%20end%2C%20we%0Aintroduce%20J1-ENVS%2C%20the%20first%20interactive%20and%20dynamic%20legal%20environment%20tailored%0Afor%20LLM-based%20agents.%20Guided%20by%20legal%20experts%2C%20it%20comprises%20six%20representative%0Ascenarios%20from%20Chinese%20legal%20practices%20across%20three%20levels%20of%20environmental%0Acomplexity.%20We%20further%20introduce%20J1-EVAL%2C%20a%20fine-grained%20evaluation%20framework%2C%0Adesigned%20to%20assess%20both%20task%20performance%20and%20procedural%20compliance%20across%0Avarying%20levels%20of%20legal%20proficiency.%20Extensive%20experiments%20on%2017%20LLM%20agents%0Areveal%20that%2C%20while%20many%20models%20demonstrate%20solid%20legal%20knowledge%2C%20they%20struggle%0Awith%20procedural%20execution%20in%20dynamic%20settings.%20Even%20the%20SOTA%20model%2C%20GPT-4o%2C%0Afalls%20short%20of%2060%25%20overall%20performance.%20These%20findings%20highlight%20persistent%0Achallenges%20in%20achieving%20dynamic%20legal%20intelligence%20and%20offer%20valuable%20insights%0Ato%20guide%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReady%2520Jurist%2520One%253A%2520Benchmarking%2520Language%2520Agents%2520for%2520Legal%2520Intelligence%2520in%250A%2520%2520Dynamic%2520Environments%26entry.906535625%3DZheng%2520Jia%2520and%2520Shengbin%2520Yue%2520and%2520Wei%2520Chen%2520and%2520Siyuan%2520Wang%2520and%2520Yidong%2520Liu%2520and%2520Yun%2520Song%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3D%2520%2520The%2520gap%2520between%2520static%2520benchmarks%2520and%2520the%2520dynamic%2520nature%2520of%2520real-world%2520legal%250Apractice%2520poses%2520a%2520key%2520barrier%2520to%2520advancing%2520legal%2520intelligence.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520J1-ENVS%252C%2520the%2520first%2520interactive%2520and%2520dynamic%2520legal%2520environment%2520tailored%250Afor%2520LLM-based%2520agents.%2520Guided%2520by%2520legal%2520experts%252C%2520it%2520comprises%2520six%2520representative%250Ascenarios%2520from%2520Chinese%2520legal%2520practices%2520across%2520three%2520levels%2520of%2520environmental%250Acomplexity.%2520We%2520further%2520introduce%2520J1-EVAL%252C%2520a%2520fine-grained%2520evaluation%2520framework%252C%250Adesigned%2520to%2520assess%2520both%2520task%2520performance%2520and%2520procedural%2520compliance%2520across%250Avarying%2520levels%2520of%2520legal%2520proficiency.%2520Extensive%2520experiments%2520on%252017%2520LLM%2520agents%250Areveal%2520that%252C%2520while%2520many%2520models%2520demonstrate%2520solid%2520legal%2520knowledge%252C%2520they%2520struggle%250Awith%2520procedural%2520execution%2520in%2520dynamic%2520settings.%2520Even%2520the%2520SOTA%2520model%252C%2520GPT-4o%252C%250Afalls%2520short%2520of%252060%2525%2520overall%2520performance.%2520These%2520findings%2520highlight%2520persistent%250Achallenges%2520in%2520achieving%2520dynamic%2520legal%2520intelligence%2520and%2520offer%2520valuable%2520insights%250Ato%2520guide%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ready%20Jurist%20One%3A%20Benchmarking%20Language%20Agents%20for%20Legal%20Intelligence%20in%0A%20%20Dynamic%20Environments&entry.906535625=Zheng%20Jia%20and%20Shengbin%20Yue%20and%20Wei%20Chen%20and%20Siyuan%20Wang%20and%20Yidong%20Liu%20and%20Yun%20Song%20and%20Zhongyu%20Wei&entry.1292438233=%20%20The%20gap%20between%20static%20benchmarks%20and%20the%20dynamic%20nature%20of%20real-world%20legal%0Apractice%20poses%20a%20key%20barrier%20to%20advancing%20legal%20intelligence.%20To%20this%20end%2C%20we%0Aintroduce%20J1-ENVS%2C%20the%20first%20interactive%20and%20dynamic%20legal%20environment%20tailored%0Afor%20LLM-based%20agents.%20Guided%20by%20legal%20experts%2C%20it%20comprises%20six%20representative%0Ascenarios%20from%20Chinese%20legal%20practices%20across%20three%20levels%20of%20environmental%0Acomplexity.%20We%20further%20introduce%20J1-EVAL%2C%20a%20fine-grained%20evaluation%20framework%2C%0Adesigned%20to%20assess%20both%20task%20performance%20and%20procedural%20compliance%20across%0Avarying%20levels%20of%20legal%20proficiency.%20Extensive%20experiments%20on%2017%20LLM%20agents%0Areveal%20that%2C%20while%20many%20models%20demonstrate%20solid%20legal%20knowledge%2C%20they%20struggle%0Awith%20procedural%20execution%20in%20dynamic%20settings.%20Even%20the%20SOTA%20model%2C%20GPT-4o%2C%0Afalls%20short%20of%2060%25%20overall%20performance.%20These%20findings%20highlight%20persistent%0Achallenges%20in%20achieving%20dynamic%20legal%20intelligence%20and%20offer%20valuable%20insights%0Ato%20guide%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04037v2&entry.124074799=Read"},
{"title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management", "author": "Luis Gasco and Hermenegildo Fabregat and Laura Garc\u00eda-Sardi\u00f1a and Paula Estrella and Daniel Deniz and Alvaro Rodrigo and Rabih Zbib", "abstract": "  Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.\n", "link": "http://arxiv.org/abs/2507.13275v1", "date": "2025-07-17", "relevancy": 2.4593, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overview%20of%20the%20TalentCLEF%202025%3A%20Skill%20and%20Job%20Title%20Intelligence%20for%0A%20%20Human%20Capital%20Management&body=Title%3A%20Overview%20of%20the%20TalentCLEF%202025%3A%20Skill%20and%20Job%20Title%20Intelligence%20for%0A%20%20Human%20Capital%20Management%0AAuthor%3A%20Luis%20Gasco%20and%20Hermenegildo%20Fabregat%20and%20Laura%20Garc%C3%ADa-Sardi%C3%B1a%20and%20Paula%20Estrella%20and%20Daniel%20Deniz%20and%20Alvaro%20Rodrigo%20and%20Rabih%20Zbib%0AAbstract%3A%20%20%20Advances%20in%20natural%20language%20processing%20and%20large%20language%20models%20are%20driving%0Aa%20major%20transformation%20in%20Human%20Capital%20Management%2C%20with%20a%20growing%20interest%20in%0Abuilding%20smart%20systems%20based%20on%20language%20technologies%20for%20talent%20acquisition%2C%0Aupskilling%20strategies%2C%20and%20workforce%20planning.%20However%2C%20the%20adoption%20and%0Aprogress%20of%20these%20technologies%20critically%20depend%20on%20the%20development%20of%20reliable%0Aand%20fair%20models%2C%20properly%20evaluated%20on%20public%20data%20and%20open%20benchmarks%2C%20which%0Ahave%20so%20far%20been%20unavailable%20in%20this%20domain.%0A%20%20To%20address%20this%20gap%2C%20we%20present%20TalentCLEF%202025%2C%20the%20first%20evaluation%0Acampaign%20focused%20on%20skill%20and%20job%20title%20intelligence.%20The%20lab%20consists%20of%20two%0Atasks%3A%20Task%20A%20-%20Multilingual%20Job%20Title%20Matching%2C%20covering%20English%2C%20Spanish%2C%0AGerman%2C%20and%20Chinese%3B%20and%20Task%20B%20-%20Job%20Title-Based%20Skill%20Prediction%2C%20in%20English.%0ABoth%20corpora%20were%20built%20from%20real%20job%20applications%2C%20carefully%20anonymized%2C%20and%0Amanually%20annotated%20to%20reflect%20the%20complexity%20and%20diversity%20of%20real-world%20labor%0Amarket%20data%2C%20including%20linguistic%20variability%20and%20gender-marked%20expressions.%0A%20%20The%20evaluations%20included%20monolingual%20and%20cross-lingual%20scenarios%20and%20covered%0Athe%20evaluation%20of%20gender%20bias.%0A%20%20TalentCLEF%20attracted%2076%20registered%20teams%20with%20more%20than%20280%20submissions.%20Most%0Asystems%20relied%20on%20information%20retrieval%20techniques%20built%20with%20multilingual%0Aencoder-based%20models%20fine-tuned%20with%20contrastive%20learning%2C%20and%20several%20of%20them%0Aincorporated%20large%20language%20models%20for%20data%20augmentation%20or%20re-ranking.%20The%0Aresults%20show%20that%20the%20training%20strategies%20have%20a%20larger%20effect%20than%20the%20size%20of%0Athe%20model%20alone.%20TalentCLEF%20provides%20the%20first%20public%20benchmark%20in%20this%20field%0Aand%20encourages%20the%20development%20of%20robust%2C%20fair%2C%20and%20transferable%20language%0Atechnologies%20for%20the%20labor%20market.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverview%2520of%2520the%2520TalentCLEF%25202025%253A%2520Skill%2520and%2520Job%2520Title%2520Intelligence%2520for%250A%2520%2520Human%2520Capital%2520Management%26entry.906535625%3DLuis%2520Gasco%2520and%2520Hermenegildo%2520Fabregat%2520and%2520Laura%2520Garc%25C3%25ADa-Sardi%25C3%25B1a%2520and%2520Paula%2520Estrella%2520and%2520Daniel%2520Deniz%2520and%2520Alvaro%2520Rodrigo%2520and%2520Rabih%2520Zbib%26entry.1292438233%3D%2520%2520Advances%2520in%2520natural%2520language%2520processing%2520and%2520large%2520language%2520models%2520are%2520driving%250Aa%2520major%2520transformation%2520in%2520Human%2520Capital%2520Management%252C%2520with%2520a%2520growing%2520interest%2520in%250Abuilding%2520smart%2520systems%2520based%2520on%2520language%2520technologies%2520for%2520talent%2520acquisition%252C%250Aupskilling%2520strategies%252C%2520and%2520workforce%2520planning.%2520However%252C%2520the%2520adoption%2520and%250Aprogress%2520of%2520these%2520technologies%2520critically%2520depend%2520on%2520the%2520development%2520of%2520reliable%250Aand%2520fair%2520models%252C%2520properly%2520evaluated%2520on%2520public%2520data%2520and%2520open%2520benchmarks%252C%2520which%250Ahave%2520so%2520far%2520been%2520unavailable%2520in%2520this%2520domain.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520TalentCLEF%25202025%252C%2520the%2520first%2520evaluation%250Acampaign%2520focused%2520on%2520skill%2520and%2520job%2520title%2520intelligence.%2520The%2520lab%2520consists%2520of%2520two%250Atasks%253A%2520Task%2520A%2520-%2520Multilingual%2520Job%2520Title%2520Matching%252C%2520covering%2520English%252C%2520Spanish%252C%250AGerman%252C%2520and%2520Chinese%253B%2520and%2520Task%2520B%2520-%2520Job%2520Title-Based%2520Skill%2520Prediction%252C%2520in%2520English.%250ABoth%2520corpora%2520were%2520built%2520from%2520real%2520job%2520applications%252C%2520carefully%2520anonymized%252C%2520and%250Amanually%2520annotated%2520to%2520reflect%2520the%2520complexity%2520and%2520diversity%2520of%2520real-world%2520labor%250Amarket%2520data%252C%2520including%2520linguistic%2520variability%2520and%2520gender-marked%2520expressions.%250A%2520%2520The%2520evaluations%2520included%2520monolingual%2520and%2520cross-lingual%2520scenarios%2520and%2520covered%250Athe%2520evaluation%2520of%2520gender%2520bias.%250A%2520%2520TalentCLEF%2520attracted%252076%2520registered%2520teams%2520with%2520more%2520than%2520280%2520submissions.%2520Most%250Asystems%2520relied%2520on%2520information%2520retrieval%2520techniques%2520built%2520with%2520multilingual%250Aencoder-based%2520models%2520fine-tuned%2520with%2520contrastive%2520learning%252C%2520and%2520several%2520of%2520them%250Aincorporated%2520large%2520language%2520models%2520for%2520data%2520augmentation%2520or%2520re-ranking.%2520The%250Aresults%2520show%2520that%2520the%2520training%2520strategies%2520have%2520a%2520larger%2520effect%2520than%2520the%2520size%2520of%250Athe%2520model%2520alone.%2520TalentCLEF%2520provides%2520the%2520first%2520public%2520benchmark%2520in%2520this%2520field%250Aand%2520encourages%2520the%2520development%2520of%2520robust%252C%2520fair%252C%2520and%2520transferable%2520language%250Atechnologies%2520for%2520the%2520labor%2520market.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overview%20of%20the%20TalentCLEF%202025%3A%20Skill%20and%20Job%20Title%20Intelligence%20for%0A%20%20Human%20Capital%20Management&entry.906535625=Luis%20Gasco%20and%20Hermenegildo%20Fabregat%20and%20Laura%20Garc%C3%ADa-Sardi%C3%B1a%20and%20Paula%20Estrella%20and%20Daniel%20Deniz%20and%20Alvaro%20Rodrigo%20and%20Rabih%20Zbib&entry.1292438233=%20%20Advances%20in%20natural%20language%20processing%20and%20large%20language%20models%20are%20driving%0Aa%20major%20transformation%20in%20Human%20Capital%20Management%2C%20with%20a%20growing%20interest%20in%0Abuilding%20smart%20systems%20based%20on%20language%20technologies%20for%20talent%20acquisition%2C%0Aupskilling%20strategies%2C%20and%20workforce%20planning.%20However%2C%20the%20adoption%20and%0Aprogress%20of%20these%20technologies%20critically%20depend%20on%20the%20development%20of%20reliable%0Aand%20fair%20models%2C%20properly%20evaluated%20on%20public%20data%20and%20open%20benchmarks%2C%20which%0Ahave%20so%20far%20been%20unavailable%20in%20this%20domain.%0A%20%20To%20address%20this%20gap%2C%20we%20present%20TalentCLEF%202025%2C%20the%20first%20evaluation%0Acampaign%20focused%20on%20skill%20and%20job%20title%20intelligence.%20The%20lab%20consists%20of%20two%0Atasks%3A%20Task%20A%20-%20Multilingual%20Job%20Title%20Matching%2C%20covering%20English%2C%20Spanish%2C%0AGerman%2C%20and%20Chinese%3B%20and%20Task%20B%20-%20Job%20Title-Based%20Skill%20Prediction%2C%20in%20English.%0ABoth%20corpora%20were%20built%20from%20real%20job%20applications%2C%20carefully%20anonymized%2C%20and%0Amanually%20annotated%20to%20reflect%20the%20complexity%20and%20diversity%20of%20real-world%20labor%0Amarket%20data%2C%20including%20linguistic%20variability%20and%20gender-marked%20expressions.%0A%20%20The%20evaluations%20included%20monolingual%20and%20cross-lingual%20scenarios%20and%20covered%0Athe%20evaluation%20of%20gender%20bias.%0A%20%20TalentCLEF%20attracted%2076%20registered%20teams%20with%20more%20than%20280%20submissions.%20Most%0Asystems%20relied%20on%20information%20retrieval%20techniques%20built%20with%20multilingual%0Aencoder-based%20models%20fine-tuned%20with%20contrastive%20learning%2C%20and%20several%20of%20them%0Aincorporated%20large%20language%20models%20for%20data%20augmentation%20or%20re-ranking.%20The%0Aresults%20show%20that%20the%20training%20strategies%20have%20a%20larger%20effect%20than%20the%20size%20of%0Athe%20model%20alone.%20TalentCLEF%20provides%20the%20first%20public%20benchmark%20in%20this%20field%0Aand%20encourages%20the%20development%20of%20robust%2C%20fair%2C%20and%20transferable%20language%0Atechnologies%20for%20the%20labor%20market.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13275v1&entry.124074799=Read"},
{"title": "Voxtral", "author": "Alexander H. Liu and Andy Ehrenberg and Andy Lo and Cl\u00e9ment Denoix and Corentin Barreau and Guillaume Lample and Jean-Malo Delignon and Khyathi Raghavi Chandu and Patrick von Platen and Pavankumar Reddy Muddireddy and Sanchit Gandhi and Soham Ghosh and Srijan Mishra and Thomas Foubert and Abhinav Rastogi and Adam Yang and Albert Q. Jiang and Alexandre Sablayrolles and Am\u00e9lie H\u00e9liou and Am\u00e9lie Martin and Anmol Agarwal and Antoine Roux and Arthur Darcet and Arthur Mensch and Baptiste Bout and Baptiste Rozi\u00e8re and Baudouin De Monicault and Chris Bamford and Christian Wallenwein and Christophe Renaudin and Cl\u00e9mence Lanfranchi and Darius Dabert and Devendra Singh Chaplot and Devon Mizelle and Diego de las Casas and Elliot Chane-Sane and Emilien Fugier and Emma Bou Hanna and Gabrielle Berrada and Gauthier Delerce and Gauthier Guinet and Georgii Novikov and Guillaume Martin and Himanshu Jaju and Jan Ludziejewski and Jason Rute and Jean-Hadrien Chabran and Jessica Chudnovsky and Joachim Studnia and Joep Barmentlo and Jonas Amar and Josselin Somerville Roberts and Julien Denize and Karan Saxena and Karmesh Yadav and Kartik Khandelwal and Kush Jain and L\u00e9lio Renard Lavaud and L\u00e9onard Blier and Lingxiao Zhao and Louis Martin and Lucile Saulnier and Luyu Gao and Marie Pellat and Mathilde Guillaumin and Mathis Felardos and Matthieu Dinot and Maxime Darrin and Maximilian Augustin and Micka\u00ebl Seznec and Neha Gupta and Nikhil Raghuraman and Olivier Duchenne and Patricia Wang and Patryk Saffer and Paul Jacob and Paul Wambergue and Paula Kurylowicz and Philom\u00e8ne Chagniot and Pierre Stock and Pravesh Agrawal and R\u00e9mi Delacourt and Romain Sauvestre and Roman Soletskyi and Sagar Vaze and Sandeep Subramanian and Saurabh Garg and Shashwat Dalal and Siddharth Gandhi and Sumukh Aithal and Szymon Antoniak and Teven Le Scao and Thibault Schueller and Thibaut Lavril and Thomas Robert and Thomas Wang and Timoth\u00e9e Lacroix and Tom Bewley and Valeriia Nemychnikova and Victor Paltz and Virgile Richard and Wen-Ding Li and William Marshall and Xuanyu Zhang and Yihan Wan and Yunhao Tang", "abstract": "  We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.\nVoxtral is trained to comprehend both spoken audio and text documents,\nachieving state-of-the-art performance across a diverse range of audio\nbenchmarks, while preserving strong text capabilities. Voxtral Small\noutperforms a number of closed-source models, while being small enough to run\nlocally. A 32K context window enables the model to handle audio files up to 40\nminutes in duration and long multi-turn conversations. We also contribute three\nbenchmarks for evaluating speech understanding models on knowledge and trivia.\nBoth Voxtral models are released under Apache 2.0 license.\n", "link": "http://arxiv.org/abs/2507.13264v1", "date": "2025-07-17", "relevancy": 2.4565, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxtral&body=Title%3A%20Voxtral%0AAuthor%3A%20Alexander%20H.%20Liu%20and%20Andy%20Ehrenberg%20and%20Andy%20Lo%20and%20Cl%C3%A9ment%20Denoix%20and%20Corentin%20Barreau%20and%20Guillaume%20Lample%20and%20Jean-Malo%20Delignon%20and%20Khyathi%20Raghavi%20Chandu%20and%20Patrick%20von%20Platen%20and%20Pavankumar%20Reddy%20Muddireddy%20and%20Sanchit%20Gandhi%20and%20Soham%20Ghosh%20and%20Srijan%20Mishra%20and%20Thomas%20Foubert%20and%20Abhinav%20Rastogi%20and%20Adam%20Yang%20and%20Albert%20Q.%20Jiang%20and%20Alexandre%20Sablayrolles%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Am%C3%A9lie%20Martin%20and%20Anmol%20Agarwal%20and%20Antoine%20Roux%20and%20Arthur%20Darcet%20and%20Arthur%20Mensch%20and%20Baptiste%20Bout%20and%20Baptiste%20Rozi%C3%A8re%20and%20Baudouin%20De%20Monicault%20and%20Chris%20Bamford%20and%20Christian%20Wallenwein%20and%20Christophe%20Renaudin%20and%20Cl%C3%A9mence%20Lanfranchi%20and%20Darius%20Dabert%20and%20Devendra%20Singh%20Chaplot%20and%20Devon%20Mizelle%20and%20Diego%20de%20las%20Casas%20and%20Elliot%20Chane-Sane%20and%20Emilien%20Fugier%20and%20Emma%20Bou%20Hanna%20and%20Gabrielle%20Berrada%20and%20Gauthier%20Delerce%20and%20Gauthier%20Guinet%20and%20Georgii%20Novikov%20and%20Guillaume%20Martin%20and%20Himanshu%20Jaju%20and%20Jan%20Ludziejewski%20and%20Jason%20Rute%20and%20Jean-Hadrien%20Chabran%20and%20Jessica%20Chudnovsky%20and%20Joachim%20Studnia%20and%20Joep%20Barmentlo%20and%20Jonas%20Amar%20and%20Josselin%20Somerville%20Roberts%20and%20Julien%20Denize%20and%20Karan%20Saxena%20and%20Karmesh%20Yadav%20and%20Kartik%20Khandelwal%20and%20Kush%20Jain%20and%20L%C3%A9lio%20Renard%20Lavaud%20and%20L%C3%A9onard%20Blier%20and%20Lingxiao%20Zhao%20and%20Louis%20Martin%20and%20Lucile%20Saulnier%20and%20Luyu%20Gao%20and%20Marie%20Pellat%20and%20Mathilde%20Guillaumin%20and%20Mathis%20Felardos%20and%20Matthieu%20Dinot%20and%20Maxime%20Darrin%20and%20Maximilian%20Augustin%20and%20Micka%C3%ABl%20Seznec%20and%20Neha%20Gupta%20and%20Nikhil%20Raghuraman%20and%20Olivier%20Duchenne%20and%20Patricia%20Wang%20and%20Patryk%20Saffer%20and%20Paul%20Jacob%20and%20Paul%20Wambergue%20and%20Paula%20Kurylowicz%20and%20Philom%C3%A8ne%20Chagniot%20and%20Pierre%20Stock%20and%20Pravesh%20Agrawal%20and%20R%C3%A9mi%20Delacourt%20and%20Romain%20Sauvestre%20and%20Roman%20Soletskyi%20and%20Sagar%20Vaze%20and%20Sandeep%20Subramanian%20and%20Saurabh%20Garg%20and%20Shashwat%20Dalal%20and%20Siddharth%20Gandhi%20and%20Sumukh%20Aithal%20and%20Szymon%20Antoniak%20and%20Teven%20Le%20Scao%20and%20Thibault%20Schueller%20and%20Thibaut%20Lavril%20and%20Thomas%20Robert%20and%20Thomas%20Wang%20and%20Timoth%C3%A9e%20Lacroix%20and%20Tom%20Bewley%20and%20Valeriia%20Nemychnikova%20and%20Victor%20Paltz%20and%20Virgile%20Richard%20and%20Wen-Ding%20Li%20and%20William%20Marshall%20and%20Xuanyu%20Zhang%20and%20Yihan%20Wan%20and%20Yunhao%20Tang%0AAbstract%3A%20%20%20We%20present%20Voxtral%20Mini%20and%20Voxtral%20Small%2C%20two%20multimodal%20audio%20chat%20models.%0AVoxtral%20is%20trained%20to%20comprehend%20both%20spoken%20audio%20and%20text%20documents%2C%0Aachieving%20state-of-the-art%20performance%20across%20a%20diverse%20range%20of%20audio%0Abenchmarks%2C%20while%20preserving%20strong%20text%20capabilities.%20Voxtral%20Small%0Aoutperforms%20a%20number%20of%20closed-source%20models%2C%20while%20being%20small%20enough%20to%20run%0Alocally.%20A%2032K%20context%20window%20enables%20the%20model%20to%20handle%20audio%20files%20up%20to%2040%0Aminutes%20in%20duration%20and%20long%20multi-turn%20conversations.%20We%20also%20contribute%20three%0Abenchmarks%20for%20evaluating%20speech%20understanding%20models%20on%20knowledge%20and%20trivia.%0ABoth%20Voxtral%20models%20are%20released%20under%20Apache%202.0%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxtral%26entry.906535625%3DAlexander%2520H.%2520Liu%2520and%2520Andy%2520Ehrenberg%2520and%2520Andy%2520Lo%2520and%2520Cl%25C3%25A9ment%2520Denoix%2520and%2520Corentin%2520Barreau%2520and%2520Guillaume%2520Lample%2520and%2520Jean-Malo%2520Delignon%2520and%2520Khyathi%2520Raghavi%2520Chandu%2520and%2520Patrick%2520von%2520Platen%2520and%2520Pavankumar%2520Reddy%2520Muddireddy%2520and%2520Sanchit%2520Gandhi%2520and%2520Soham%2520Ghosh%2520and%2520Srijan%2520Mishra%2520and%2520Thomas%2520Foubert%2520and%2520Abhinav%2520Rastogi%2520and%2520Adam%2520Yang%2520and%2520Albert%2520Q.%2520Jiang%2520and%2520Alexandre%2520Sablayrolles%2520and%2520Am%25C3%25A9lie%2520H%25C3%25A9liou%2520and%2520Am%25C3%25A9lie%2520Martin%2520and%2520Anmol%2520Agarwal%2520and%2520Antoine%2520Roux%2520and%2520Arthur%2520Darcet%2520and%2520Arthur%2520Mensch%2520and%2520Baptiste%2520Bout%2520and%2520Baptiste%2520Rozi%25C3%25A8re%2520and%2520Baudouin%2520De%2520Monicault%2520and%2520Chris%2520Bamford%2520and%2520Christian%2520Wallenwein%2520and%2520Christophe%2520Renaudin%2520and%2520Cl%25C3%25A9mence%2520Lanfranchi%2520and%2520Darius%2520Dabert%2520and%2520Devendra%2520Singh%2520Chaplot%2520and%2520Devon%2520Mizelle%2520and%2520Diego%2520de%2520las%2520Casas%2520and%2520Elliot%2520Chane-Sane%2520and%2520Emilien%2520Fugier%2520and%2520Emma%2520Bou%2520Hanna%2520and%2520Gabrielle%2520Berrada%2520and%2520Gauthier%2520Delerce%2520and%2520Gauthier%2520Guinet%2520and%2520Georgii%2520Novikov%2520and%2520Guillaume%2520Martin%2520and%2520Himanshu%2520Jaju%2520and%2520Jan%2520Ludziejewski%2520and%2520Jason%2520Rute%2520and%2520Jean-Hadrien%2520Chabran%2520and%2520Jessica%2520Chudnovsky%2520and%2520Joachim%2520Studnia%2520and%2520Joep%2520Barmentlo%2520and%2520Jonas%2520Amar%2520and%2520Josselin%2520Somerville%2520Roberts%2520and%2520Julien%2520Denize%2520and%2520Karan%2520Saxena%2520and%2520Karmesh%2520Yadav%2520and%2520Kartik%2520Khandelwal%2520and%2520Kush%2520Jain%2520and%2520L%25C3%25A9lio%2520Renard%2520Lavaud%2520and%2520L%25C3%25A9onard%2520Blier%2520and%2520Lingxiao%2520Zhao%2520and%2520Louis%2520Martin%2520and%2520Lucile%2520Saulnier%2520and%2520Luyu%2520Gao%2520and%2520Marie%2520Pellat%2520and%2520Mathilde%2520Guillaumin%2520and%2520Mathis%2520Felardos%2520and%2520Matthieu%2520Dinot%2520and%2520Maxime%2520Darrin%2520and%2520Maximilian%2520Augustin%2520and%2520Micka%25C3%25ABl%2520Seznec%2520and%2520Neha%2520Gupta%2520and%2520Nikhil%2520Raghuraman%2520and%2520Olivier%2520Duchenne%2520and%2520Patricia%2520Wang%2520and%2520Patryk%2520Saffer%2520and%2520Paul%2520Jacob%2520and%2520Paul%2520Wambergue%2520and%2520Paula%2520Kurylowicz%2520and%2520Philom%25C3%25A8ne%2520Chagniot%2520and%2520Pierre%2520Stock%2520and%2520Pravesh%2520Agrawal%2520and%2520R%25C3%25A9mi%2520Delacourt%2520and%2520Romain%2520Sauvestre%2520and%2520Roman%2520Soletskyi%2520and%2520Sagar%2520Vaze%2520and%2520Sandeep%2520Subramanian%2520and%2520Saurabh%2520Garg%2520and%2520Shashwat%2520Dalal%2520and%2520Siddharth%2520Gandhi%2520and%2520Sumukh%2520Aithal%2520and%2520Szymon%2520Antoniak%2520and%2520Teven%2520Le%2520Scao%2520and%2520Thibault%2520Schueller%2520and%2520Thibaut%2520Lavril%2520and%2520Thomas%2520Robert%2520and%2520Thomas%2520Wang%2520and%2520Timoth%25C3%25A9e%2520Lacroix%2520and%2520Tom%2520Bewley%2520and%2520Valeriia%2520Nemychnikova%2520and%2520Victor%2520Paltz%2520and%2520Virgile%2520Richard%2520and%2520Wen-Ding%2520Li%2520and%2520William%2520Marshall%2520and%2520Xuanyu%2520Zhang%2520and%2520Yihan%2520Wan%2520and%2520Yunhao%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520Voxtral%2520Mini%2520and%2520Voxtral%2520Small%252C%2520two%2520multimodal%2520audio%2520chat%2520models.%250AVoxtral%2520is%2520trained%2520to%2520comprehend%2520both%2520spoken%2520audio%2520and%2520text%2520documents%252C%250Aachieving%2520state-of-the-art%2520performance%2520across%2520a%2520diverse%2520range%2520of%2520audio%250Abenchmarks%252C%2520while%2520preserving%2520strong%2520text%2520capabilities.%2520Voxtral%2520Small%250Aoutperforms%2520a%2520number%2520of%2520closed-source%2520models%252C%2520while%2520being%2520small%2520enough%2520to%2520run%250Alocally.%2520A%252032K%2520context%2520window%2520enables%2520the%2520model%2520to%2520handle%2520audio%2520files%2520up%2520to%252040%250Aminutes%2520in%2520duration%2520and%2520long%2520multi-turn%2520conversations.%2520We%2520also%2520contribute%2520three%250Abenchmarks%2520for%2520evaluating%2520speech%2520understanding%2520models%2520on%2520knowledge%2520and%2520trivia.%250ABoth%2520Voxtral%2520models%2520are%2520released%2520under%2520Apache%25202.0%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxtral&entry.906535625=Alexander%20H.%20Liu%20and%20Andy%20Ehrenberg%20and%20Andy%20Lo%20and%20Cl%C3%A9ment%20Denoix%20and%20Corentin%20Barreau%20and%20Guillaume%20Lample%20and%20Jean-Malo%20Delignon%20and%20Khyathi%20Raghavi%20Chandu%20and%20Patrick%20von%20Platen%20and%20Pavankumar%20Reddy%20Muddireddy%20and%20Sanchit%20Gandhi%20and%20Soham%20Ghosh%20and%20Srijan%20Mishra%20and%20Thomas%20Foubert%20and%20Abhinav%20Rastogi%20and%20Adam%20Yang%20and%20Albert%20Q.%20Jiang%20and%20Alexandre%20Sablayrolles%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Am%C3%A9lie%20Martin%20and%20Anmol%20Agarwal%20and%20Antoine%20Roux%20and%20Arthur%20Darcet%20and%20Arthur%20Mensch%20and%20Baptiste%20Bout%20and%20Baptiste%20Rozi%C3%A8re%20and%20Baudouin%20De%20Monicault%20and%20Chris%20Bamford%20and%20Christian%20Wallenwein%20and%20Christophe%20Renaudin%20and%20Cl%C3%A9mence%20Lanfranchi%20and%20Darius%20Dabert%20and%20Devendra%20Singh%20Chaplot%20and%20Devon%20Mizelle%20and%20Diego%20de%20las%20Casas%20and%20Elliot%20Chane-Sane%20and%20Emilien%20Fugier%20and%20Emma%20Bou%20Hanna%20and%20Gabrielle%20Berrada%20and%20Gauthier%20Delerce%20and%20Gauthier%20Guinet%20and%20Georgii%20Novikov%20and%20Guillaume%20Martin%20and%20Himanshu%20Jaju%20and%20Jan%20Ludziejewski%20and%20Jason%20Rute%20and%20Jean-Hadrien%20Chabran%20and%20Jessica%20Chudnovsky%20and%20Joachim%20Studnia%20and%20Joep%20Barmentlo%20and%20Jonas%20Amar%20and%20Josselin%20Somerville%20Roberts%20and%20Julien%20Denize%20and%20Karan%20Saxena%20and%20Karmesh%20Yadav%20and%20Kartik%20Khandelwal%20and%20Kush%20Jain%20and%20L%C3%A9lio%20Renard%20Lavaud%20and%20L%C3%A9onard%20Blier%20and%20Lingxiao%20Zhao%20and%20Louis%20Martin%20and%20Lucile%20Saulnier%20and%20Luyu%20Gao%20and%20Marie%20Pellat%20and%20Mathilde%20Guillaumin%20and%20Mathis%20Felardos%20and%20Matthieu%20Dinot%20and%20Maxime%20Darrin%20and%20Maximilian%20Augustin%20and%20Micka%C3%ABl%20Seznec%20and%20Neha%20Gupta%20and%20Nikhil%20Raghuraman%20and%20Olivier%20Duchenne%20and%20Patricia%20Wang%20and%20Patryk%20Saffer%20and%20Paul%20Jacob%20and%20Paul%20Wambergue%20and%20Paula%20Kurylowicz%20and%20Philom%C3%A8ne%20Chagniot%20and%20Pierre%20Stock%20and%20Pravesh%20Agrawal%20and%20R%C3%A9mi%20Delacourt%20and%20Romain%20Sauvestre%20and%20Roman%20Soletskyi%20and%20Sagar%20Vaze%20and%20Sandeep%20Subramanian%20and%20Saurabh%20Garg%20and%20Shashwat%20Dalal%20and%20Siddharth%20Gandhi%20and%20Sumukh%20Aithal%20and%20Szymon%20Antoniak%20and%20Teven%20Le%20Scao%20and%20Thibault%20Schueller%20and%20Thibaut%20Lavril%20and%20Thomas%20Robert%20and%20Thomas%20Wang%20and%20Timoth%C3%A9e%20Lacroix%20and%20Tom%20Bewley%20and%20Valeriia%20Nemychnikova%20and%20Victor%20Paltz%20and%20Virgile%20Richard%20and%20Wen-Ding%20Li%20and%20William%20Marshall%20and%20Xuanyu%20Zhang%20and%20Yihan%20Wan%20and%20Yunhao%20Tang&entry.1292438233=%20%20We%20present%20Voxtral%20Mini%20and%20Voxtral%20Small%2C%20two%20multimodal%20audio%20chat%20models.%0AVoxtral%20is%20trained%20to%20comprehend%20both%20spoken%20audio%20and%20text%20documents%2C%0Aachieving%20state-of-the-art%20performance%20across%20a%20diverse%20range%20of%20audio%0Abenchmarks%2C%20while%20preserving%20strong%20text%20capabilities.%20Voxtral%20Small%0Aoutperforms%20a%20number%20of%20closed-source%20models%2C%20while%20being%20small%20enough%20to%20run%0Alocally.%20A%2032K%20context%20window%20enables%20the%20model%20to%20handle%20audio%20files%20up%20to%2040%0Aminutes%20in%20duration%20and%20long%20multi-turn%20conversations.%20We%20also%20contribute%20three%0Abenchmarks%20for%20evaluating%20speech%20understanding%20models%20on%20knowledge%20and%20trivia.%0ABoth%20Voxtral%20models%20are%20released%20under%20Apache%202.0%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13264v1&entry.124074799=Read"},
{"title": "Color Image Set Recognition Based on Quaternionic Grassmannians", "author": "Xiang Xiang Wang and Tin-Yau Tam", "abstract": "  We propose a new method for recognizing color image sets using quaternionic\nGrassmannians, which use the power of quaternions to capture color information\nand represent each color image set as a point on the quaternionic Grassmannian.\nWe provide a direct formula to calculate the shortest distance between two\npoints on the quaternionic Grassmannian, and use this distance to build a new\nclassification framework. Experiments on the ETH-80 benchmark dataset and and\nthe Highway Traffic video dataset show that our method achieves good\nrecognition results. We also discuss some limitations in stability and suggest\nways the method can be improved in the future.\n", "link": "http://arxiv.org/abs/2505.23629v2", "date": "2025-07-17", "relevancy": 2.4291, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4997}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians&body=Title%3A%20Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians%0AAuthor%3A%20Xiang%20Xiang%20Wang%20and%20Tin-Yau%20Tam%0AAbstract%3A%20%20%20We%20propose%20a%20new%20method%20for%20recognizing%20color%20image%20sets%20using%20quaternionic%0AGrassmannians%2C%20which%20use%20the%20power%20of%20quaternions%20to%20capture%20color%20information%0Aand%20represent%20each%20color%20image%20set%20as%20a%20point%20on%20the%20quaternionic%20Grassmannian.%0AWe%20provide%20a%20direct%20formula%20to%20calculate%20the%20shortest%20distance%20between%20two%0Apoints%20on%20the%20quaternionic%20Grassmannian%2C%20and%20use%20this%20distance%20to%20build%20a%20new%0Aclassification%20framework.%20Experiments%20on%20the%20ETH-80%20benchmark%20dataset%20and%20and%0Athe%20Highway%20Traffic%20video%20dataset%20show%20that%20our%20method%20achieves%20good%0Arecognition%20results.%20We%20also%20discuss%20some%20limitations%20in%20stability%20and%20suggest%0Aways%20the%20method%20can%20be%20improved%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Image%2520Set%2520Recognition%2520Based%2520on%2520Quaternionic%2520Grassmannians%26entry.906535625%3DXiang%2520Xiang%2520Wang%2520and%2520Tin-Yau%2520Tam%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520method%2520for%2520recognizing%2520color%2520image%2520sets%2520using%2520quaternionic%250AGrassmannians%252C%2520which%2520use%2520the%2520power%2520of%2520quaternions%2520to%2520capture%2520color%2520information%250Aand%2520represent%2520each%2520color%2520image%2520set%2520as%2520a%2520point%2520on%2520the%2520quaternionic%2520Grassmannian.%250AWe%2520provide%2520a%2520direct%2520formula%2520to%2520calculate%2520the%2520shortest%2520distance%2520between%2520two%250Apoints%2520on%2520the%2520quaternionic%2520Grassmannian%252C%2520and%2520use%2520this%2520distance%2520to%2520build%2520a%2520new%250Aclassification%2520framework.%2520Experiments%2520on%2520the%2520ETH-80%2520benchmark%2520dataset%2520and%2520and%250Athe%2520Highway%2520Traffic%2520video%2520dataset%2520show%2520that%2520our%2520method%2520achieves%2520good%250Arecognition%2520results.%2520We%2520also%2520discuss%2520some%2520limitations%2520in%2520stability%2520and%2520suggest%250Aways%2520the%2520method%2520can%2520be%2520improved%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Image%20Set%20Recognition%20Based%20on%20Quaternionic%20Grassmannians&entry.906535625=Xiang%20Xiang%20Wang%20and%20Tin-Yau%20Tam&entry.1292438233=%20%20We%20propose%20a%20new%20method%20for%20recognizing%20color%20image%20sets%20using%20quaternionic%0AGrassmannians%2C%20which%20use%20the%20power%20of%20quaternions%20to%20capture%20color%20information%0Aand%20represent%20each%20color%20image%20set%20as%20a%20point%20on%20the%20quaternionic%20Grassmannian.%0AWe%20provide%20a%20direct%20formula%20to%20calculate%20the%20shortest%20distance%20between%20two%0Apoints%20on%20the%20quaternionic%20Grassmannian%2C%20and%20use%20this%20distance%20to%20build%20a%20new%0Aclassification%20framework.%20Experiments%20on%20the%20ETH-80%20benchmark%20dataset%20and%20and%0Athe%20Highway%20Traffic%20video%20dataset%20show%20that%20our%20method%20achieves%20good%0Arecognition%20results.%20We%20also%20discuss%20some%20limitations%20in%20stability%20and%20suggest%0Aways%20the%20method%20can%20be%20improved%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23629v2&entry.124074799=Read"},
{"title": "FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient", "author": "ShanBin Liu", "abstract": "  Fairness has emerged as one of the key challenges in federated learning. In\nhorizontal federated settings, data heterogeneity often leads to substantial\nperformance disparities across clients, raising concerns about equitable model\nbehavior. To address this issue, we propose FedGA, a fairness-aware federated\nlearning algorithm. We first employ the Gini coefficient to measure the\nperformance disparity among clients. Based on this, we establish a relationship\nbetween the Gini coefficient $G$ and the update scale of the global model\n${U_s}$, and use this relationship to adaptively determine the timing of\nfairness intervention. Subsequently, we dynamically adjust the aggregation\nweights according to the system's real-time fairness status, enabling the\nglobal model to better incorporate information from clients with relatively\npoor performance.We conduct extensive experiments on the Office-Caltech-10,\nCIFAR-10, and Synthetic datasets. The results show that FedGA effectively\nimproves fairness metrics such as variance and the Gini coefficient, while\nmaintaining strong overall performance, demonstrating the effectiveness of our\napproach.\n", "link": "http://arxiv.org/abs/2507.12983v1", "date": "2025-07-17", "relevancy": 2.3969, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4961}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedGA%3A%20A%20Fair%20Federated%20Learning%20Framework%20Based%20on%20the%20Gini%20Coefficient&body=Title%3A%20FedGA%3A%20A%20Fair%20Federated%20Learning%20Framework%20Based%20on%20the%20Gini%20Coefficient%0AAuthor%3A%20ShanBin%20Liu%0AAbstract%3A%20%20%20Fairness%20has%20emerged%20as%20one%20of%20the%20key%20challenges%20in%20federated%20learning.%20In%0Ahorizontal%20federated%20settings%2C%20data%20heterogeneity%20often%20leads%20to%20substantial%0Aperformance%20disparities%20across%20clients%2C%20raising%20concerns%20about%20equitable%20model%0Abehavior.%20To%20address%20this%20issue%2C%20we%20propose%20FedGA%2C%20a%20fairness-aware%20federated%0Alearning%20algorithm.%20We%20first%20employ%20the%20Gini%20coefficient%20to%20measure%20the%0Aperformance%20disparity%20among%20clients.%20Based%20on%20this%2C%20we%20establish%20a%20relationship%0Abetween%20the%20Gini%20coefficient%20%24G%24%20and%20the%20update%20scale%20of%20the%20global%20model%0A%24%7BU_s%7D%24%2C%20and%20use%20this%20relationship%20to%20adaptively%20determine%20the%20timing%20of%0Afairness%20intervention.%20Subsequently%2C%20we%20dynamically%20adjust%20the%20aggregation%0Aweights%20according%20to%20the%20system%27s%20real-time%20fairness%20status%2C%20enabling%20the%0Aglobal%20model%20to%20better%20incorporate%20information%20from%20clients%20with%20relatively%0Apoor%20performance.We%20conduct%20extensive%20experiments%20on%20the%20Office-Caltech-10%2C%0ACIFAR-10%2C%20and%20Synthetic%20datasets.%20The%20results%20show%20that%20FedGA%20effectively%0Aimproves%20fairness%20metrics%20such%20as%20variance%20and%20the%20Gini%20coefficient%2C%20while%0Amaintaining%20strong%20overall%20performance%2C%20demonstrating%20the%20effectiveness%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedGA%253A%2520A%2520Fair%2520Federated%2520Learning%2520Framework%2520Based%2520on%2520the%2520Gini%2520Coefficient%26entry.906535625%3DShanBin%2520Liu%26entry.1292438233%3D%2520%2520Fairness%2520has%2520emerged%2520as%2520one%2520of%2520the%2520key%2520challenges%2520in%2520federated%2520learning.%2520In%250Ahorizontal%2520federated%2520settings%252C%2520data%2520heterogeneity%2520often%2520leads%2520to%2520substantial%250Aperformance%2520disparities%2520across%2520clients%252C%2520raising%2520concerns%2520about%2520equitable%2520model%250Abehavior.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520FedGA%252C%2520a%2520fairness-aware%2520federated%250Alearning%2520algorithm.%2520We%2520first%2520employ%2520the%2520Gini%2520coefficient%2520to%2520measure%2520the%250Aperformance%2520disparity%2520among%2520clients.%2520Based%2520on%2520this%252C%2520we%2520establish%2520a%2520relationship%250Abetween%2520the%2520Gini%2520coefficient%2520%2524G%2524%2520and%2520the%2520update%2520scale%2520of%2520the%2520global%2520model%250A%2524%257BU_s%257D%2524%252C%2520and%2520use%2520this%2520relationship%2520to%2520adaptively%2520determine%2520the%2520timing%2520of%250Afairness%2520intervention.%2520Subsequently%252C%2520we%2520dynamically%2520adjust%2520the%2520aggregation%250Aweights%2520according%2520to%2520the%2520system%2527s%2520real-time%2520fairness%2520status%252C%2520enabling%2520the%250Aglobal%2520model%2520to%2520better%2520incorporate%2520information%2520from%2520clients%2520with%2520relatively%250Apoor%2520performance.We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520Office-Caltech-10%252C%250ACIFAR-10%252C%2520and%2520Synthetic%2520datasets.%2520The%2520results%2520show%2520that%2520FedGA%2520effectively%250Aimproves%2520fairness%2520metrics%2520such%2520as%2520variance%2520and%2520the%2520Gini%2520coefficient%252C%2520while%250Amaintaining%2520strong%2520overall%2520performance%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedGA%3A%20A%20Fair%20Federated%20Learning%20Framework%20Based%20on%20the%20Gini%20Coefficient&entry.906535625=ShanBin%20Liu&entry.1292438233=%20%20Fairness%20has%20emerged%20as%20one%20of%20the%20key%20challenges%20in%20federated%20learning.%20In%0Ahorizontal%20federated%20settings%2C%20data%20heterogeneity%20often%20leads%20to%20substantial%0Aperformance%20disparities%20across%20clients%2C%20raising%20concerns%20about%20equitable%20model%0Abehavior.%20To%20address%20this%20issue%2C%20we%20propose%20FedGA%2C%20a%20fairness-aware%20federated%0Alearning%20algorithm.%20We%20first%20employ%20the%20Gini%20coefficient%20to%20measure%20the%0Aperformance%20disparity%20among%20clients.%20Based%20on%20this%2C%20we%20establish%20a%20relationship%0Abetween%20the%20Gini%20coefficient%20%24G%24%20and%20the%20update%20scale%20of%20the%20global%20model%0A%24%7BU_s%7D%24%2C%20and%20use%20this%20relationship%20to%20adaptively%20determine%20the%20timing%20of%0Afairness%20intervention.%20Subsequently%2C%20we%20dynamically%20adjust%20the%20aggregation%0Aweights%20according%20to%20the%20system%27s%20real-time%20fairness%20status%2C%20enabling%20the%0Aglobal%20model%20to%20better%20incorporate%20information%20from%20clients%20with%20relatively%0Apoor%20performance.We%20conduct%20extensive%20experiments%20on%20the%20Office-Caltech-10%2C%0ACIFAR-10%2C%20and%20Synthetic%20datasets.%20The%20results%20show%20that%20FedGA%20effectively%0Aimproves%20fairness%20metrics%20such%20as%20variance%20and%20the%20Gini%20coefficient%2C%20while%0Amaintaining%20strong%20overall%20performance%2C%20demonstrating%20the%20effectiveness%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12983v1&entry.124074799=Read"},
{"title": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering", "author": "I-Chun Chen and Hsu-Shen Liu and Wei-Fang Sun and Chen-Hao Chao and Yen-Chang Hsu and Chun-Yi Lee", "abstract": "  Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments.\n", "link": "http://arxiv.org/abs/2410.08589v3", "date": "2025-07-17", "relevancy": 2.3937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4829}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retraining-Free%20Merging%20of%20Sparse%20MoE%20via%20Hierarchical%20Clustering&body=Title%3A%20Retraining-Free%20Merging%20of%20Sparse%20MoE%20via%20Hierarchical%20Clustering%0AAuthor%3A%20I-Chun%20Chen%20and%20Hsu-Shen%20Liu%20and%20Wei-Fang%20Sun%20and%20Chen-Hao%20Chao%20and%20Yen-Chang%20Hsu%20and%20Chun-Yi%20Lee%0AAbstract%3A%20%20%20Sparse%20Mixture-of-Experts%20%28SMoE%29%20models%20represent%20a%20significant%20advancement%0Ain%20large%20language%20model%20%28LLM%29%20development%20through%20their%20efficient%20parameter%0Autilization.%20These%20models%20achieve%20substantial%20performance%20improvements%20at%0Areduced%20inference%20costs.%20However%2C%20the%20deployment%20of%20SMoE%20models%20faces%0Aconstraints%20from%20extensive%20memory%20requirements%20of%20expert%20components%20in%0Aresource-limited%20environments.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20Hierarchical%20Clustering%20for%20Sparsely%20activated%20Mixture%20of%20Experts%0A%28HC-SMoE%29%2C%20a%20task-agnostic%20expert%20merging%20framework%20for%20parameter%20reduction%0Awithout%20retraining.%20HC-SMoE%20introduces%20a%20novel%20hierarchical%20clustering%20approach%0Abased%20on%20expert%20outputs%20to%20ensure%20merging%20robustness%20independent%20of%20routing%0Adecisions.%20The%20proposed%20output-based%20clustering%20method%20enables%20effective%0Acapture%20of%20functional%20relationships%20between%20experts%20for%20large-scale%0Aarchitectures.%20We%20provide%20theoretical%20analysis%20and%20comprehensive%20evaluations%0Aacross%20multiple%20zero-shot%20language%20tasks%20to%20demonstrate%20HC-SMoE%27s%20effectiveness%0Ain%20state-of-the-art%20models%20including%20Qwen%20and%20Mixtral.%20The%20experimental%20results%0Avalidate%20HC-SMoE%27s%20superior%20performance%20and%20practical%20applicability%20for%0Areal-world%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08589v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetraining-Free%2520Merging%2520of%2520Sparse%2520MoE%2520via%2520Hierarchical%2520Clustering%26entry.906535625%3DI-Chun%2520Chen%2520and%2520Hsu-Shen%2520Liu%2520and%2520Wei-Fang%2520Sun%2520and%2520Chen-Hao%2520Chao%2520and%2520Yen-Chang%2520Hsu%2520and%2520Chun-Yi%2520Lee%26entry.1292438233%3D%2520%2520Sparse%2520Mixture-of-Experts%2520%2528SMoE%2529%2520models%2520represent%2520a%2520significant%2520advancement%250Ain%2520large%2520language%2520model%2520%2528LLM%2529%2520development%2520through%2520their%2520efficient%2520parameter%250Autilization.%2520These%2520models%2520achieve%2520substantial%2520performance%2520improvements%2520at%250Areduced%2520inference%2520costs.%2520However%252C%2520the%2520deployment%2520of%2520SMoE%2520models%2520faces%250Aconstraints%2520from%2520extensive%2520memory%2520requirements%2520of%2520expert%2520components%2520in%250Aresource-limited%2520environments.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%250Aintroduces%2520Hierarchical%2520Clustering%2520for%2520Sparsely%2520activated%2520Mixture%2520of%2520Experts%250A%2528HC-SMoE%2529%252C%2520a%2520task-agnostic%2520expert%2520merging%2520framework%2520for%2520parameter%2520reduction%250Awithout%2520retraining.%2520HC-SMoE%2520introduces%2520a%2520novel%2520hierarchical%2520clustering%2520approach%250Abased%2520on%2520expert%2520outputs%2520to%2520ensure%2520merging%2520robustness%2520independent%2520of%2520routing%250Adecisions.%2520The%2520proposed%2520output-based%2520clustering%2520method%2520enables%2520effective%250Acapture%2520of%2520functional%2520relationships%2520between%2520experts%2520for%2520large-scale%250Aarchitectures.%2520We%2520provide%2520theoretical%2520analysis%2520and%2520comprehensive%2520evaluations%250Aacross%2520multiple%2520zero-shot%2520language%2520tasks%2520to%2520demonstrate%2520HC-SMoE%2527s%2520effectiveness%250Ain%2520state-of-the-art%2520models%2520including%2520Qwen%2520and%2520Mixtral.%2520The%2520experimental%2520results%250Avalidate%2520HC-SMoE%2527s%2520superior%2520performance%2520and%2520practical%2520applicability%2520for%250Areal-world%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08589v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retraining-Free%20Merging%20of%20Sparse%20MoE%20via%20Hierarchical%20Clustering&entry.906535625=I-Chun%20Chen%20and%20Hsu-Shen%20Liu%20and%20Wei-Fang%20Sun%20and%20Chen-Hao%20Chao%20and%20Yen-Chang%20Hsu%20and%20Chun-Yi%20Lee&entry.1292438233=%20%20Sparse%20Mixture-of-Experts%20%28SMoE%29%20models%20represent%20a%20significant%20advancement%0Ain%20large%20language%20model%20%28LLM%29%20development%20through%20their%20efficient%20parameter%0Autilization.%20These%20models%20achieve%20substantial%20performance%20improvements%20at%0Areduced%20inference%20costs.%20However%2C%20the%20deployment%20of%20SMoE%20models%20faces%0Aconstraints%20from%20extensive%20memory%20requirements%20of%20expert%20components%20in%0Aresource-limited%20environments.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20Hierarchical%20Clustering%20for%20Sparsely%20activated%20Mixture%20of%20Experts%0A%28HC-SMoE%29%2C%20a%20task-agnostic%20expert%20merging%20framework%20for%20parameter%20reduction%0Awithout%20retraining.%20HC-SMoE%20introduces%20a%20novel%20hierarchical%20clustering%20approach%0Abased%20on%20expert%20outputs%20to%20ensure%20merging%20robustness%20independent%20of%20routing%0Adecisions.%20The%20proposed%20output-based%20clustering%20method%20enables%20effective%0Acapture%20of%20functional%20relationships%20between%20experts%20for%20large-scale%0Aarchitectures.%20We%20provide%20theoretical%20analysis%20and%20comprehensive%20evaluations%0Aacross%20multiple%20zero-shot%20language%20tasks%20to%20demonstrate%20HC-SMoE%27s%20effectiveness%0Ain%20state-of-the-art%20models%20including%20Qwen%20and%20Mixtral.%20The%20experimental%20results%0Avalidate%20HC-SMoE%27s%20superior%20performance%20and%20practical%20applicability%20for%0Areal-world%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08589v3&entry.124074799=Read"},
{"title": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding", "author": "Shihao Wang and Guo Chen and De-an Huang and Zhiqi Li and Minghan Li and Guilin Li and Jose M. Alvarez and Lei Zhang and Zhiding Yu", "abstract": "  Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.\n", "link": "http://arxiv.org/abs/2507.13353v1", "date": "2025-07-17", "relevancy": 2.3671, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6009}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoITG%3A%20Multimodal%20Video%20Understanding%20with%20Instructed%20Temporal%0A%20%20Grounding&body=Title%3A%20VideoITG%3A%20Multimodal%20Video%20Understanding%20with%20Instructed%20Temporal%0A%20%20Grounding%0AAuthor%3A%20Shihao%20Wang%20and%20Guo%20Chen%20and%20De-an%20Huang%20and%20Zhiqi%20Li%20and%20Minghan%20Li%20and%20Guilin%20Li%20and%20Jose%20M.%20Alvarez%20and%20Lei%20Zhang%20and%20Zhiding%20Yu%0AAbstract%3A%20%20%20Recent%20studies%20have%20revealed%20that%20selecting%20informative%20and%20relevant%20video%0Aframes%20can%20significantly%20improve%20the%20performance%20of%20Video%20Large%20Language%20Models%0A%28Video-LLMs%29.%20Current%20methods%2C%20such%20as%20reducing%20inter-frame%20redundancy%2C%0Aemploying%20separate%20models%20for%20image-text%20relevance%20assessment%2C%20or%20utilizing%0Atemporal%20video%20grounding%20for%20event%20localization%2C%20substantially%20adopt%0Aunsupervised%20learning%20paradigms%2C%20whereas%20they%20struggle%20to%20address%20the%20complex%0Ascenarios%20in%20long%20video%20understanding.%20We%20propose%20Instructed%20Temporal%20Grounding%0Afor%20Videos%20%28VideoITG%29%2C%20featuring%20customized%20frame%20sampling%20aligned%20with%20user%0Ainstructions.%20The%20core%20of%20VideoITG%20is%20the%20VidThinker%20pipeline%2C%20an%20automated%0Aannotation%20framework%20that%20explicitly%20mimics%20the%20human%20annotation%20process.%0AFirst%2C%20it%20generates%20detailed%20clip-level%20captions%20conditioned%20on%20the%0Ainstruction%3B%20then%2C%20it%20retrieves%20relevant%20video%20segments%20through%0Ainstruction-guided%20reasoning%3B%20finally%2C%20it%20performs%20fine-grained%20frame%20selection%0Ato%20pinpoint%20the%20most%20informative%20visual%20evidence.%20Leveraging%20VidThinker%2C%20we%0Aconstruct%20the%20VideoITG-40K%20dataset%2C%20containing%2040K%20videos%20and%20500K%20instructed%0Atemporal%20grounding%20annotations.%20We%20then%20design%20a%20plug-and-play%20VideoITG%20model%2C%0Awhich%20takes%20advantage%20of%20visual%20language%20alignment%20and%20reasoning%20capabilities%0Aof%20Video-LLMs%2C%20for%20effective%20frame%20selection%20in%20a%20discriminative%20manner.%0ACoupled%20with%20Video-LLMs%2C%20VideoITG%20achieves%20consistent%20performance%20improvements%0Aacross%20multiple%20multimodal%20video%20understanding%20benchmarks%2C%20showing%20its%0Asuperiority%20and%20great%20potentials%20for%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoITG%253A%2520Multimodal%2520Video%2520Understanding%2520with%2520Instructed%2520Temporal%250A%2520%2520Grounding%26entry.906535625%3DShihao%2520Wang%2520and%2520Guo%2520Chen%2520and%2520De-an%2520Huang%2520and%2520Zhiqi%2520Li%2520and%2520Minghan%2520Li%2520and%2520Guilin%2520Li%2520and%2520Jose%2520M.%2520Alvarez%2520and%2520Lei%2520Zhang%2520and%2520Zhiding%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520revealed%2520that%2520selecting%2520informative%2520and%2520relevant%2520video%250Aframes%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520Video%2520Large%2520Language%2520Models%250A%2528Video-LLMs%2529.%2520Current%2520methods%252C%2520such%2520as%2520reducing%2520inter-frame%2520redundancy%252C%250Aemploying%2520separate%2520models%2520for%2520image-text%2520relevance%2520assessment%252C%2520or%2520utilizing%250Atemporal%2520video%2520grounding%2520for%2520event%2520localization%252C%2520substantially%2520adopt%250Aunsupervised%2520learning%2520paradigms%252C%2520whereas%2520they%2520struggle%2520to%2520address%2520the%2520complex%250Ascenarios%2520in%2520long%2520video%2520understanding.%2520We%2520propose%2520Instructed%2520Temporal%2520Grounding%250Afor%2520Videos%2520%2528VideoITG%2529%252C%2520featuring%2520customized%2520frame%2520sampling%2520aligned%2520with%2520user%250Ainstructions.%2520The%2520core%2520of%2520VideoITG%2520is%2520the%2520VidThinker%2520pipeline%252C%2520an%2520automated%250Aannotation%2520framework%2520that%2520explicitly%2520mimics%2520the%2520human%2520annotation%2520process.%250AFirst%252C%2520it%2520generates%2520detailed%2520clip-level%2520captions%2520conditioned%2520on%2520the%250Ainstruction%253B%2520then%252C%2520it%2520retrieves%2520relevant%2520video%2520segments%2520through%250Ainstruction-guided%2520reasoning%253B%2520finally%252C%2520it%2520performs%2520fine-grained%2520frame%2520selection%250Ato%2520pinpoint%2520the%2520most%2520informative%2520visual%2520evidence.%2520Leveraging%2520VidThinker%252C%2520we%250Aconstruct%2520the%2520VideoITG-40K%2520dataset%252C%2520containing%252040K%2520videos%2520and%2520500K%2520instructed%250Atemporal%2520grounding%2520annotations.%2520We%2520then%2520design%2520a%2520plug-and-play%2520VideoITG%2520model%252C%250Awhich%2520takes%2520advantage%2520of%2520visual%2520language%2520alignment%2520and%2520reasoning%2520capabilities%250Aof%2520Video-LLMs%252C%2520for%2520effective%2520frame%2520selection%2520in%2520a%2520discriminative%2520manner.%250ACoupled%2520with%2520Video-LLMs%252C%2520VideoITG%2520achieves%2520consistent%2520performance%2520improvements%250Aacross%2520multiple%2520multimodal%2520video%2520understanding%2520benchmarks%252C%2520showing%2520its%250Asuperiority%2520and%2520great%2520potentials%2520for%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoITG%3A%20Multimodal%20Video%20Understanding%20with%20Instructed%20Temporal%0A%20%20Grounding&entry.906535625=Shihao%20Wang%20and%20Guo%20Chen%20and%20De-an%20Huang%20and%20Zhiqi%20Li%20and%20Minghan%20Li%20and%20Guilin%20Li%20and%20Jose%20M.%20Alvarez%20and%20Lei%20Zhang%20and%20Zhiding%20Yu&entry.1292438233=%20%20Recent%20studies%20have%20revealed%20that%20selecting%20informative%20and%20relevant%20video%0Aframes%20can%20significantly%20improve%20the%20performance%20of%20Video%20Large%20Language%20Models%0A%28Video-LLMs%29.%20Current%20methods%2C%20such%20as%20reducing%20inter-frame%20redundancy%2C%0Aemploying%20separate%20models%20for%20image-text%20relevance%20assessment%2C%20or%20utilizing%0Atemporal%20video%20grounding%20for%20event%20localization%2C%20substantially%20adopt%0Aunsupervised%20learning%20paradigms%2C%20whereas%20they%20struggle%20to%20address%20the%20complex%0Ascenarios%20in%20long%20video%20understanding.%20We%20propose%20Instructed%20Temporal%20Grounding%0Afor%20Videos%20%28VideoITG%29%2C%20featuring%20customized%20frame%20sampling%20aligned%20with%20user%0Ainstructions.%20The%20core%20of%20VideoITG%20is%20the%20VidThinker%20pipeline%2C%20an%20automated%0Aannotation%20framework%20that%20explicitly%20mimics%20the%20human%20annotation%20process.%0AFirst%2C%20it%20generates%20detailed%20clip-level%20captions%20conditioned%20on%20the%0Ainstruction%3B%20then%2C%20it%20retrieves%20relevant%20video%20segments%20through%0Ainstruction-guided%20reasoning%3B%20finally%2C%20it%20performs%20fine-grained%20frame%20selection%0Ato%20pinpoint%20the%20most%20informative%20visual%20evidence.%20Leveraging%20VidThinker%2C%20we%0Aconstruct%20the%20VideoITG-40K%20dataset%2C%20containing%2040K%20videos%20and%20500K%20instructed%0Atemporal%20grounding%20annotations.%20We%20then%20design%20a%20plug-and-play%20VideoITG%20model%2C%0Awhich%20takes%20advantage%20of%20visual%20language%20alignment%20and%20reasoning%20capabilities%0Aof%20Video-LLMs%2C%20for%20effective%20frame%20selection%20in%20a%20discriminative%20manner.%0ACoupled%20with%20Video-LLMs%2C%20VideoITG%20achieves%20consistent%20performance%20improvements%0Aacross%20multiple%20multimodal%20video%20understanding%20benchmarks%2C%20showing%20its%0Asuperiority%20and%20great%20potentials%20for%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13353v1&entry.124074799=Read"},
{"title": "$\u03c0^3$: Scalable Permutation-Equivariant Visual Geometry Learning", "author": "Yifan Wang and Jianjun Zhou and Haoyi Zhu and Wenzheng Chang and Yang Zhou and Zizun Li and Junyi Chen and Jiangmiao Pang and Chunhua Shen and Tong He", "abstract": "  We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.\n", "link": "http://arxiv.org/abs/2507.13347v1", "date": "2025-07-17", "relevancy": 2.3583, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5987}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5836}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CF%80%5E3%24%3A%20Scalable%20Permutation-Equivariant%20Visual%20Geometry%20Learning&body=Title%3A%20%24%CF%80%5E3%24%3A%20Scalable%20Permutation-Equivariant%20Visual%20Geometry%20Learning%0AAuthor%3A%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Haoyi%20Zhu%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Jiangmiao%20Pang%20and%20Chunhua%20Shen%20and%20Tong%20He%0AAbstract%3A%20%20%20We%20introduce%20%24%5Cpi%5E3%24%2C%20a%20feed-forward%20neural%20network%20that%20offers%20a%20novel%0Aapproach%20to%20visual%20geometry%20reconstruction%2C%20breaking%20the%20reliance%20on%20a%0Aconventional%20fixed%20reference%20view.%20Previous%20methods%20often%20anchor%20their%0Areconstructions%20to%20a%20designated%20viewpoint%2C%20an%20inductive%20bias%20that%20can%20lead%20to%0Ainstability%20and%20failures%20if%20the%20reference%20is%20suboptimal.%20In%20contrast%2C%20%24%5Cpi%5E3%24%0Aemploys%20a%20fully%20permutation-equivariant%20architecture%20to%20predict%0Aaffine-invariant%20camera%20poses%20and%20scale-invariant%20local%20point%20maps%20without%20any%0Areference%20frames.%20This%20design%20makes%20our%20model%20inherently%20robust%20to%20input%0Aordering%20and%20highly%20scalable.%20These%20advantages%20enable%20our%20simple%20and%20bias-free%0Aapproach%20to%20achieve%20state-of-the-art%20performance%20on%20a%20wide%20range%20of%20tasks%2C%0Aincluding%20camera%20pose%20estimation%2C%20monocular/video%20depth%20estimation%2C%20and%20dense%0Apoint%20map%20reconstruction.%20Code%20and%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CF%2580%255E3%2524%253A%2520Scalable%2520Permutation-Equivariant%2520Visual%2520Geometry%2520Learning%26entry.906535625%3DYifan%2520Wang%2520and%2520Jianjun%2520Zhou%2520and%2520Haoyi%2520Zhu%2520and%2520Wenzheng%2520Chang%2520and%2520Yang%2520Zhou%2520and%2520Zizun%2520Li%2520and%2520Junyi%2520Chen%2520and%2520Jiangmiao%2520Pang%2520and%2520Chunhua%2520Shen%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2524%255Cpi%255E3%2524%252C%2520a%2520feed-forward%2520neural%2520network%2520that%2520offers%2520a%2520novel%250Aapproach%2520to%2520visual%2520geometry%2520reconstruction%252C%2520breaking%2520the%2520reliance%2520on%2520a%250Aconventional%2520fixed%2520reference%2520view.%2520Previous%2520methods%2520often%2520anchor%2520their%250Areconstructions%2520to%2520a%2520designated%2520viewpoint%252C%2520an%2520inductive%2520bias%2520that%2520can%2520lead%2520to%250Ainstability%2520and%2520failures%2520if%2520the%2520reference%2520is%2520suboptimal.%2520In%2520contrast%252C%2520%2524%255Cpi%255E3%2524%250Aemploys%2520a%2520fully%2520permutation-equivariant%2520architecture%2520to%2520predict%250Aaffine-invariant%2520camera%2520poses%2520and%2520scale-invariant%2520local%2520point%2520maps%2520without%2520any%250Areference%2520frames.%2520This%2520design%2520makes%2520our%2520model%2520inherently%2520robust%2520to%2520input%250Aordering%2520and%2520highly%2520scalable.%2520These%2520advantages%2520enable%2520our%2520simple%2520and%2520bias-free%250Aapproach%2520to%2520achieve%2520state-of-the-art%2520performance%2520on%2520a%2520wide%2520range%2520of%2520tasks%252C%250Aincluding%2520camera%2520pose%2520estimation%252C%2520monocular/video%2520depth%2520estimation%252C%2520and%2520dense%250Apoint%2520map%2520reconstruction.%2520Code%2520and%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CF%80%5E3%24%3A%20Scalable%20Permutation-Equivariant%20Visual%20Geometry%20Learning&entry.906535625=Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Haoyi%20Zhu%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Jiangmiao%20Pang%20and%20Chunhua%20Shen%20and%20Tong%20He&entry.1292438233=%20%20We%20introduce%20%24%5Cpi%5E3%24%2C%20a%20feed-forward%20neural%20network%20that%20offers%20a%20novel%0Aapproach%20to%20visual%20geometry%20reconstruction%2C%20breaking%20the%20reliance%20on%20a%0Aconventional%20fixed%20reference%20view.%20Previous%20methods%20often%20anchor%20their%0Areconstructions%20to%20a%20designated%20viewpoint%2C%20an%20inductive%20bias%20that%20can%20lead%20to%0Ainstability%20and%20failures%20if%20the%20reference%20is%20suboptimal.%20In%20contrast%2C%20%24%5Cpi%5E3%24%0Aemploys%20a%20fully%20permutation-equivariant%20architecture%20to%20predict%0Aaffine-invariant%20camera%20poses%20and%20scale-invariant%20local%20point%20maps%20without%20any%0Areference%20frames.%20This%20design%20makes%20our%20model%20inherently%20robust%20to%20input%0Aordering%20and%20highly%20scalable.%20These%20advantages%20enable%20our%20simple%20and%20bias-free%0Aapproach%20to%20achieve%20state-of-the-art%20performance%20on%20a%20wide%20range%20of%20tasks%2C%0Aincluding%20camera%20pose%20estimation%2C%20monocular/video%20depth%20estimation%2C%20and%20dense%0Apoint%20map%20reconstruction.%20Code%20and%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13347v1&entry.124074799=Read"},
{"title": "AutoPartGen: Autogressive 3D Part Generation and Discovery", "author": "Minghao Chen and Jianyuan Wang and Roman Shapovalov and Tom Monnier and Hyunyoung Jung and Dilin Wang and Rakesh Ranjan and Iro Laina and Andrea Vedaldi", "abstract": "  We introduce AutoPartGen, a model that generates objects composed of 3D parts\nin an autoregressive manner. This model can take as input an image of an\nobject, 2D masks of the object's parts, or an existing 3D object, and generate\na corresponding compositional 3D reconstruction. Our approach builds upon\n3DShape2VecSet, a recent latent 3D representation with powerful geometric\nexpressiveness. We observe that this latent space exhibits strong compositional\nproperties, making it particularly well-suited for part-based generation tasks.\nSpecifically, AutoPartGen generates object parts autoregressively, predicting\none part at a time while conditioning on previously generated parts and\nadditional inputs, such as 2D images, masks, or 3D objects. This process\ncontinues until the model decides that all parts have been generated, thus\ndetermining automatically the type and number of parts. The resulting parts can\nbe seamlessly assembled into coherent objects or scenes without requiring\nadditional optimization. We evaluate both the overall 3D generation\ncapabilities and the part-level generation quality of AutoPartGen,\ndemonstrating that it achieves state-of-the-art performance in 3D part\ngeneration.\n", "link": "http://arxiv.org/abs/2507.13346v1", "date": "2025-07-17", "relevancy": 2.3402, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6087}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5803}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPartGen%3A%20Autogressive%203D%20Part%20Generation%20and%20Discovery&body=Title%3A%20AutoPartGen%3A%20Autogressive%203D%20Part%20Generation%20and%20Discovery%0AAuthor%3A%20Minghao%20Chen%20and%20Jianyuan%20Wang%20and%20Roman%20Shapovalov%20and%20Tom%20Monnier%20and%20Hyunyoung%20Jung%20and%20Dilin%20Wang%20and%20Rakesh%20Ranjan%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20introduce%20AutoPartGen%2C%20a%20model%20that%20generates%20objects%20composed%20of%203D%20parts%0Ain%20an%20autoregressive%20manner.%20This%20model%20can%20take%20as%20input%20an%20image%20of%20an%0Aobject%2C%202D%20masks%20of%20the%20object%27s%20parts%2C%20or%20an%20existing%203D%20object%2C%20and%20generate%0Aa%20corresponding%20compositional%203D%20reconstruction.%20Our%20approach%20builds%20upon%0A3DShape2VecSet%2C%20a%20recent%20latent%203D%20representation%20with%20powerful%20geometric%0Aexpressiveness.%20We%20observe%20that%20this%20latent%20space%20exhibits%20strong%20compositional%0Aproperties%2C%20making%20it%20particularly%20well-suited%20for%20part-based%20generation%20tasks.%0ASpecifically%2C%20AutoPartGen%20generates%20object%20parts%20autoregressively%2C%20predicting%0Aone%20part%20at%20a%20time%20while%20conditioning%20on%20previously%20generated%20parts%20and%0Aadditional%20inputs%2C%20such%20as%202D%20images%2C%20masks%2C%20or%203D%20objects.%20This%20process%0Acontinues%20until%20the%20model%20decides%20that%20all%20parts%20have%20been%20generated%2C%20thus%0Adetermining%20automatically%20the%20type%20and%20number%20of%20parts.%20The%20resulting%20parts%20can%0Abe%20seamlessly%20assembled%20into%20coherent%20objects%20or%20scenes%20without%20requiring%0Aadditional%20optimization.%20We%20evaluate%20both%20the%20overall%203D%20generation%0Acapabilities%20and%20the%20part-level%20generation%20quality%20of%20AutoPartGen%2C%0Ademonstrating%20that%20it%20achieves%20state-of-the-art%20performance%20in%203D%20part%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPartGen%253A%2520Autogressive%25203D%2520Part%2520Generation%2520and%2520Discovery%26entry.906535625%3DMinghao%2520Chen%2520and%2520Jianyuan%2520Wang%2520and%2520Roman%2520Shapovalov%2520and%2520Tom%2520Monnier%2520and%2520Hyunyoung%2520Jung%2520and%2520Dilin%2520Wang%2520and%2520Rakesh%2520Ranjan%2520and%2520Iro%2520Laina%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520We%2520introduce%2520AutoPartGen%252C%2520a%2520model%2520that%2520generates%2520objects%2520composed%2520of%25203D%2520parts%250Ain%2520an%2520autoregressive%2520manner.%2520This%2520model%2520can%2520take%2520as%2520input%2520an%2520image%2520of%2520an%250Aobject%252C%25202D%2520masks%2520of%2520the%2520object%2527s%2520parts%252C%2520or%2520an%2520existing%25203D%2520object%252C%2520and%2520generate%250Aa%2520corresponding%2520compositional%25203D%2520reconstruction.%2520Our%2520approach%2520builds%2520upon%250A3DShape2VecSet%252C%2520a%2520recent%2520latent%25203D%2520representation%2520with%2520powerful%2520geometric%250Aexpressiveness.%2520We%2520observe%2520that%2520this%2520latent%2520space%2520exhibits%2520strong%2520compositional%250Aproperties%252C%2520making%2520it%2520particularly%2520well-suited%2520for%2520part-based%2520generation%2520tasks.%250ASpecifically%252C%2520AutoPartGen%2520generates%2520object%2520parts%2520autoregressively%252C%2520predicting%250Aone%2520part%2520at%2520a%2520time%2520while%2520conditioning%2520on%2520previously%2520generated%2520parts%2520and%250Aadditional%2520inputs%252C%2520such%2520as%25202D%2520images%252C%2520masks%252C%2520or%25203D%2520objects.%2520This%2520process%250Acontinues%2520until%2520the%2520model%2520decides%2520that%2520all%2520parts%2520have%2520been%2520generated%252C%2520thus%250Adetermining%2520automatically%2520the%2520type%2520and%2520number%2520of%2520parts.%2520The%2520resulting%2520parts%2520can%250Abe%2520seamlessly%2520assembled%2520into%2520coherent%2520objects%2520or%2520scenes%2520without%2520requiring%250Aadditional%2520optimization.%2520We%2520evaluate%2520both%2520the%2520overall%25203D%2520generation%250Acapabilities%2520and%2520the%2520part-level%2520generation%2520quality%2520of%2520AutoPartGen%252C%250Ademonstrating%2520that%2520it%2520achieves%2520state-of-the-art%2520performance%2520in%25203D%2520part%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPartGen%3A%20Autogressive%203D%20Part%20Generation%20and%20Discovery&entry.906535625=Minghao%20Chen%20and%20Jianyuan%20Wang%20and%20Roman%20Shapovalov%20and%20Tom%20Monnier%20and%20Hyunyoung%20Jung%20and%20Dilin%20Wang%20and%20Rakesh%20Ranjan%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20introduce%20AutoPartGen%2C%20a%20model%20that%20generates%20objects%20composed%20of%203D%20parts%0Ain%20an%20autoregressive%20manner.%20This%20model%20can%20take%20as%20input%20an%20image%20of%20an%0Aobject%2C%202D%20masks%20of%20the%20object%27s%20parts%2C%20or%20an%20existing%203D%20object%2C%20and%20generate%0Aa%20corresponding%20compositional%203D%20reconstruction.%20Our%20approach%20builds%20upon%0A3DShape2VecSet%2C%20a%20recent%20latent%203D%20representation%20with%20powerful%20geometric%0Aexpressiveness.%20We%20observe%20that%20this%20latent%20space%20exhibits%20strong%20compositional%0Aproperties%2C%20making%20it%20particularly%20well-suited%20for%20part-based%20generation%20tasks.%0ASpecifically%2C%20AutoPartGen%20generates%20object%20parts%20autoregressively%2C%20predicting%0Aone%20part%20at%20a%20time%20while%20conditioning%20on%20previously%20generated%20parts%20and%0Aadditional%20inputs%2C%20such%20as%202D%20images%2C%20masks%2C%20or%203D%20objects.%20This%20process%0Acontinues%20until%20the%20model%20decides%20that%20all%20parts%20have%20been%20generated%2C%20thus%0Adetermining%20automatically%20the%20type%20and%20number%20of%20parts.%20The%20resulting%20parts%20can%0Abe%20seamlessly%20assembled%20into%20coherent%20objects%20or%20scenes%20without%20requiring%0Aadditional%20optimization.%20We%20evaluate%20both%20the%20overall%203D%20generation%0Acapabilities%20and%20the%20part-level%20generation%20quality%20of%20AutoPartGen%2C%0Ademonstrating%20that%20it%20achieves%20state-of-the-art%20performance%20in%203D%20part%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13346v1&entry.124074799=Read"},
{"title": "Multiple-Frequencies Population-Based Training", "author": "Wa\u00ebl Doulazmi and Auguste Lehuger and Marin Toromanoff and Valentin Charraut and Thibault Buhet and Fabien Moutarde", "abstract": "  Reinforcement Learning's high sensitivity to hyperparameters is a source of\ninstability and inefficiency, creating significant challenges for\npractitioners. Hyperparameter Optimization (HPO) algorithms have been developed\nto address this issue, among them Population-Based Training (PBT) stands out\nfor its ability to generate hyperparameters schedules instead of fixed\nconfigurations. PBT trains a population of agents, each with its own\nhyperparameters, frequently ranking them and replacing the worst performers\nwith mutations of the best agents. These intermediate selection steps can cause\nPBT to focus on short-term improvements, leading it to get stuck in local\noptima and eventually fall behind vanilla Random Search over longer timescales.\nThis paper studies how this greediness issue is connected to the choice of\nevolution frequency, the rate at which the selection is done. We propose\nMultiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm\nthat addresses greediness by employing sub-populations, each evolving at\ndistinct frequencies. MF-PBT introduces a migration process to transfer\ninformation between sub-populations, with an asymmetric design to balance short\nand long-term optimization. Extensive experiments on the Brax suite demonstrate\nthat MF-PBT improves sample efficiency and long-term performance, even without\nactually tuning hyperparameters.\n", "link": "http://arxiv.org/abs/2506.03225v2", "date": "2025-07-17", "relevancy": 2.3245, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.479}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4583}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple-Frequencies%20Population-Based%20Training&body=Title%3A%20Multiple-Frequencies%20Population-Based%20Training%0AAuthor%3A%20Wa%C3%ABl%20Doulazmi%20and%20Auguste%20Lehuger%20and%20Marin%20Toromanoff%20and%20Valentin%20Charraut%20and%20Thibault%20Buhet%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Reinforcement%20Learning%27s%20high%20sensitivity%20to%20hyperparameters%20is%20a%20source%20of%0Ainstability%20and%20inefficiency%2C%20creating%20significant%20challenges%20for%0Apractitioners.%20Hyperparameter%20Optimization%20%28HPO%29%20algorithms%20have%20been%20developed%0Ato%20address%20this%20issue%2C%20among%20them%20Population-Based%20Training%20%28PBT%29%20stands%20out%0Afor%20its%20ability%20to%20generate%20hyperparameters%20schedules%20instead%20of%20fixed%0Aconfigurations.%20PBT%20trains%20a%20population%20of%20agents%2C%20each%20with%20its%20own%0Ahyperparameters%2C%20frequently%20ranking%20them%20and%20replacing%20the%20worst%20performers%0Awith%20mutations%20of%20the%20best%20agents.%20These%20intermediate%20selection%20steps%20can%20cause%0APBT%20to%20focus%20on%20short-term%20improvements%2C%20leading%20it%20to%20get%20stuck%20in%20local%0Aoptima%20and%20eventually%20fall%20behind%20vanilla%20Random%20Search%20over%20longer%20timescales.%0AThis%20paper%20studies%20how%20this%20greediness%20issue%20is%20connected%20to%20the%20choice%20of%0Aevolution%20frequency%2C%20the%20rate%20at%20which%20the%20selection%20is%20done.%20We%20propose%0AMultiple-Frequencies%20Population-Based%20Training%20%28MF-PBT%29%2C%20a%20novel%20HPO%20algorithm%0Athat%20addresses%20greediness%20by%20employing%20sub-populations%2C%20each%20evolving%20at%0Adistinct%20frequencies.%20MF-PBT%20introduces%20a%20migration%20process%20to%20transfer%0Ainformation%20between%20sub-populations%2C%20with%20an%20asymmetric%20design%20to%20balance%20short%0Aand%20long-term%20optimization.%20Extensive%20experiments%20on%20the%20Brax%20suite%20demonstrate%0Athat%20MF-PBT%20improves%20sample%20efficiency%20and%20long-term%20performance%2C%20even%20without%0Aactually%20tuning%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple-Frequencies%2520Population-Based%2520Training%26entry.906535625%3DWa%25C3%25ABl%2520Doulazmi%2520and%2520Auguste%2520Lehuger%2520and%2520Marin%2520Toromanoff%2520and%2520Valentin%2520Charraut%2520and%2520Thibault%2520Buhet%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2527s%2520high%2520sensitivity%2520to%2520hyperparameters%2520is%2520a%2520source%2520of%250Ainstability%2520and%2520inefficiency%252C%2520creating%2520significant%2520challenges%2520for%250Apractitioners.%2520Hyperparameter%2520Optimization%2520%2528HPO%2529%2520algorithms%2520have%2520been%2520developed%250Ato%2520address%2520this%2520issue%252C%2520among%2520them%2520Population-Based%2520Training%2520%2528PBT%2529%2520stands%2520out%250Afor%2520its%2520ability%2520to%2520generate%2520hyperparameters%2520schedules%2520instead%2520of%2520fixed%250Aconfigurations.%2520PBT%2520trains%2520a%2520population%2520of%2520agents%252C%2520each%2520with%2520its%2520own%250Ahyperparameters%252C%2520frequently%2520ranking%2520them%2520and%2520replacing%2520the%2520worst%2520performers%250Awith%2520mutations%2520of%2520the%2520best%2520agents.%2520These%2520intermediate%2520selection%2520steps%2520can%2520cause%250APBT%2520to%2520focus%2520on%2520short-term%2520improvements%252C%2520leading%2520it%2520to%2520get%2520stuck%2520in%2520local%250Aoptima%2520and%2520eventually%2520fall%2520behind%2520vanilla%2520Random%2520Search%2520over%2520longer%2520timescales.%250AThis%2520paper%2520studies%2520how%2520this%2520greediness%2520issue%2520is%2520connected%2520to%2520the%2520choice%2520of%250Aevolution%2520frequency%252C%2520the%2520rate%2520at%2520which%2520the%2520selection%2520is%2520done.%2520We%2520propose%250AMultiple-Frequencies%2520Population-Based%2520Training%2520%2528MF-PBT%2529%252C%2520a%2520novel%2520HPO%2520algorithm%250Athat%2520addresses%2520greediness%2520by%2520employing%2520sub-populations%252C%2520each%2520evolving%2520at%250Adistinct%2520frequencies.%2520MF-PBT%2520introduces%2520a%2520migration%2520process%2520to%2520transfer%250Ainformation%2520between%2520sub-populations%252C%2520with%2520an%2520asymmetric%2520design%2520to%2520balance%2520short%250Aand%2520long-term%2520optimization.%2520Extensive%2520experiments%2520on%2520the%2520Brax%2520suite%2520demonstrate%250Athat%2520MF-PBT%2520improves%2520sample%2520efficiency%2520and%2520long-term%2520performance%252C%2520even%2520without%250Aactually%2520tuning%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple-Frequencies%20Population-Based%20Training&entry.906535625=Wa%C3%ABl%20Doulazmi%20and%20Auguste%20Lehuger%20and%20Marin%20Toromanoff%20and%20Valentin%20Charraut%20and%20Thibault%20Buhet%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Reinforcement%20Learning%27s%20high%20sensitivity%20to%20hyperparameters%20is%20a%20source%20of%0Ainstability%20and%20inefficiency%2C%20creating%20significant%20challenges%20for%0Apractitioners.%20Hyperparameter%20Optimization%20%28HPO%29%20algorithms%20have%20been%20developed%0Ato%20address%20this%20issue%2C%20among%20them%20Population-Based%20Training%20%28PBT%29%20stands%20out%0Afor%20its%20ability%20to%20generate%20hyperparameters%20schedules%20instead%20of%20fixed%0Aconfigurations.%20PBT%20trains%20a%20population%20of%20agents%2C%20each%20with%20its%20own%0Ahyperparameters%2C%20frequently%20ranking%20them%20and%20replacing%20the%20worst%20performers%0Awith%20mutations%20of%20the%20best%20agents.%20These%20intermediate%20selection%20steps%20can%20cause%0APBT%20to%20focus%20on%20short-term%20improvements%2C%20leading%20it%20to%20get%20stuck%20in%20local%0Aoptima%20and%20eventually%20fall%20behind%20vanilla%20Random%20Search%20over%20longer%20timescales.%0AThis%20paper%20studies%20how%20this%20greediness%20issue%20is%20connected%20to%20the%20choice%20of%0Aevolution%20frequency%2C%20the%20rate%20at%20which%20the%20selection%20is%20done.%20We%20propose%0AMultiple-Frequencies%20Population-Based%20Training%20%28MF-PBT%29%2C%20a%20novel%20HPO%20algorithm%0Athat%20addresses%20greediness%20by%20employing%20sub-populations%2C%20each%20evolving%20at%0Adistinct%20frequencies.%20MF-PBT%20introduces%20a%20migration%20process%20to%20transfer%0Ainformation%20between%20sub-populations%2C%20with%20an%20asymmetric%20design%20to%20balance%20short%0Aand%20long-term%20optimization.%20Extensive%20experiments%20on%20the%20Brax%20suite%20demonstrate%0Athat%20MF-PBT%20improves%20sample%20efficiency%20and%20long-term%20performance%2C%20even%20without%0Aactually%20tuning%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03225v2&entry.124074799=Read"},
{"title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements\n  of Virtual Human Characters", "author": "Hendric Voss and Stefan Kopp", "abstract": "  Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK\n", "link": "http://arxiv.org/abs/2507.00792v2", "date": "2025-07-17", "relevancy": 2.3209, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5457}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Inverse%20Kinematics%20for%20Generating%20Multi-Constrained%20Movements%0A%20%20of%20Virtual%20Human%20Characters&body=Title%3A%20Real-Time%20Inverse%20Kinematics%20for%20Generating%20Multi-Constrained%20Movements%0A%20%20of%20Virtual%20Human%20Characters%0AAuthor%3A%20Hendric%20Voss%20and%20Stefan%20Kopp%0AAbstract%3A%20%20%20Generating%20accurate%20and%20realistic%20virtual%20human%20movements%20in%20real-time%20is%20of%0Ahigh%20importance%20for%20a%20variety%20of%20applications%20in%20computer%20graphics%2C%20interactive%0Avirtual%20environments%2C%20robotics%2C%20and%20biomechanics.%20This%20paper%20introduces%20a%20novel%0Areal-time%20inverse%20kinematics%20%28IK%29%20solver%20specifically%20designed%20for%20realistic%0Ahuman-like%20movement%20generation.%20Leveraging%20the%20automatic%20differentiation%20and%0Ajust-in-time%20compilation%20of%20TensorFlow%2C%20the%20proposed%20solver%20efficiently%20handles%0Acomplex%20articulated%20human%20skeletons%20with%20high%20degrees%20of%20freedom.%20By%20treating%0Aforward%20and%20inverse%20kinematics%20as%20differentiable%20operations%2C%20our%20method%0Aeffectively%20addresses%20common%20challenges%20such%20as%20error%20accumulation%20and%0Acomplicated%20joint%20limits%20in%20multi-constrained%20problems%2C%20which%20are%20critical%20for%0Arealistic%20human%20motion%20modeling.%20We%20demonstrate%20the%20solver%27s%20effectiveness%20on%0Athe%20SMPLX%20human%20skeleton%20model%2C%20evaluating%20its%20performance%20against%20widely%20used%0Aiterative-based%20IK%20algorithms%2C%20like%20Cyclic%20Coordinate%20Descent%20%28CCD%29%2C%20FABRIK%2C%0Aand%20the%20nonlinear%20optimization%20algorithm%20IPOPT.%20Our%20experiments%20cover%20both%0Asimple%20end-effector%20tasks%20and%20sophisticated%2C%20multi-constrained%20problems%20with%0Arealistic%20joint%20limits.%20Results%20indicate%20that%20our%20IK%20solver%20achieves%20real-time%0Aperformance%2C%20exhibiting%20rapid%20convergence%2C%20minimal%20computational%20overhead%20per%0Aiteration%2C%20and%20improved%20success%20rates%20compared%20to%20existing%20methods.%20The%20project%0Acode%20is%20available%20at%20https%3A//github.com/hvoss-techfak/TF-JAX-IK%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Inverse%2520Kinematics%2520for%2520Generating%2520Multi-Constrained%2520Movements%250A%2520%2520of%2520Virtual%2520Human%2520Characters%26entry.906535625%3DHendric%2520Voss%2520and%2520Stefan%2520Kopp%26entry.1292438233%3D%2520%2520Generating%2520accurate%2520and%2520realistic%2520virtual%2520human%2520movements%2520in%2520real-time%2520is%2520of%250Ahigh%2520importance%2520for%2520a%2520variety%2520of%2520applications%2520in%2520computer%2520graphics%252C%2520interactive%250Avirtual%2520environments%252C%2520robotics%252C%2520and%2520biomechanics.%2520This%2520paper%2520introduces%2520a%2520novel%250Areal-time%2520inverse%2520kinematics%2520%2528IK%2529%2520solver%2520specifically%2520designed%2520for%2520realistic%250Ahuman-like%2520movement%2520generation.%2520Leveraging%2520the%2520automatic%2520differentiation%2520and%250Ajust-in-time%2520compilation%2520of%2520TensorFlow%252C%2520the%2520proposed%2520solver%2520efficiently%2520handles%250Acomplex%2520articulated%2520human%2520skeletons%2520with%2520high%2520degrees%2520of%2520freedom.%2520By%2520treating%250Aforward%2520and%2520inverse%2520kinematics%2520as%2520differentiable%2520operations%252C%2520our%2520method%250Aeffectively%2520addresses%2520common%2520challenges%2520such%2520as%2520error%2520accumulation%2520and%250Acomplicated%2520joint%2520limits%2520in%2520multi-constrained%2520problems%252C%2520which%2520are%2520critical%2520for%250Arealistic%2520human%2520motion%2520modeling.%2520We%2520demonstrate%2520the%2520solver%2527s%2520effectiveness%2520on%250Athe%2520SMPLX%2520human%2520skeleton%2520model%252C%2520evaluating%2520its%2520performance%2520against%2520widely%2520used%250Aiterative-based%2520IK%2520algorithms%252C%2520like%2520Cyclic%2520Coordinate%2520Descent%2520%2528CCD%2529%252C%2520FABRIK%252C%250Aand%2520the%2520nonlinear%2520optimization%2520algorithm%2520IPOPT.%2520Our%2520experiments%2520cover%2520both%250Asimple%2520end-effector%2520tasks%2520and%2520sophisticated%252C%2520multi-constrained%2520problems%2520with%250Arealistic%2520joint%2520limits.%2520Results%2520indicate%2520that%2520our%2520IK%2520solver%2520achieves%2520real-time%250Aperformance%252C%2520exhibiting%2520rapid%2520convergence%252C%2520minimal%2520computational%2520overhead%2520per%250Aiteration%252C%2520and%2520improved%2520success%2520rates%2520compared%2520to%2520existing%2520methods.%2520The%2520project%250Acode%2520is%2520available%2520at%2520https%253A//github.com/hvoss-techfak/TF-JAX-IK%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Inverse%20Kinematics%20for%20Generating%20Multi-Constrained%20Movements%0A%20%20of%20Virtual%20Human%20Characters&entry.906535625=Hendric%20Voss%20and%20Stefan%20Kopp&entry.1292438233=%20%20Generating%20accurate%20and%20realistic%20virtual%20human%20movements%20in%20real-time%20is%20of%0Ahigh%20importance%20for%20a%20variety%20of%20applications%20in%20computer%20graphics%2C%20interactive%0Avirtual%20environments%2C%20robotics%2C%20and%20biomechanics.%20This%20paper%20introduces%20a%20novel%0Areal-time%20inverse%20kinematics%20%28IK%29%20solver%20specifically%20designed%20for%20realistic%0Ahuman-like%20movement%20generation.%20Leveraging%20the%20automatic%20differentiation%20and%0Ajust-in-time%20compilation%20of%20TensorFlow%2C%20the%20proposed%20solver%20efficiently%20handles%0Acomplex%20articulated%20human%20skeletons%20with%20high%20degrees%20of%20freedom.%20By%20treating%0Aforward%20and%20inverse%20kinematics%20as%20differentiable%20operations%2C%20our%20method%0Aeffectively%20addresses%20common%20challenges%20such%20as%20error%20accumulation%20and%0Acomplicated%20joint%20limits%20in%20multi-constrained%20problems%2C%20which%20are%20critical%20for%0Arealistic%20human%20motion%20modeling.%20We%20demonstrate%20the%20solver%27s%20effectiveness%20on%0Athe%20SMPLX%20human%20skeleton%20model%2C%20evaluating%20its%20performance%20against%20widely%20used%0Aiterative-based%20IK%20algorithms%2C%20like%20Cyclic%20Coordinate%20Descent%20%28CCD%29%2C%20FABRIK%2C%0Aand%20the%20nonlinear%20optimization%20algorithm%20IPOPT.%20Our%20experiments%20cover%20both%0Asimple%20end-effector%20tasks%20and%20sophisticated%2C%20multi-constrained%20problems%20with%0Arealistic%20joint%20limits.%20Results%20indicate%20that%20our%20IK%20solver%20achieves%20real-time%0Aperformance%2C%20exhibiting%20rapid%20convergence%2C%20minimal%20computational%20overhead%20per%0Aiteration%2C%20and%20improved%20success%20rates%20compared%20to%20existing%20methods.%20The%20project%0Acode%20is%20available%20at%20https%3A//github.com/hvoss-techfak/TF-JAX-IK%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00792v2&entry.124074799=Read"},
{"title": "From Variability To Accuracy: Conditional Bernoulli Diffusion Models\n  with Consensus-Driven Correction for Thin Structure Segmentation", "author": "Jinseo An and Min Jin Lee and Kyu Won Shim and Helen Hong", "abstract": "  Accurate segmentation of orbital bones in facial computed tomography (CT)\nimages is essential for the creation of customized implants for reconstruction\nof defected orbital bones, particularly challenging due to the ambiguous\nboundaries and thin structures such as the orbital medial wall and orbital\nfloor. In these ambiguous regions, existing segmentation approaches often\noutput disconnected or under-segmented results. We propose a novel framework\nthat corrects segmentation results by leveraging consensus from multiple\ndiffusion model outputs. Our approach employs a conditional Bernoulli diffusion\nmodel trained on diverse annotation patterns per image to generate multiple\nplausible segmentations, followed by a consensus-driven correction that\nincorporates position proximity, consensus level, and gradient direction\nsimilarity to correct challenging regions. Experimental results demonstrate\nthat our method outperforms existing methods, significantly improving recall in\nambiguous regions while preserving the continuity of thin structures.\nFurthermore, our method automates the manual process of segmentation result\ncorrection and can be applied to image-guided surgical planning and surgery.\n", "link": "http://arxiv.org/abs/2507.12985v1", "date": "2025-07-17", "relevancy": 2.2998, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5804}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5804}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Variability%20To%20Accuracy%3A%20Conditional%20Bernoulli%20Diffusion%20Models%0A%20%20with%20Consensus-Driven%20Correction%20for%20Thin%20Structure%20Segmentation&body=Title%3A%20From%20Variability%20To%20Accuracy%3A%20Conditional%20Bernoulli%20Diffusion%20Models%0A%20%20with%20Consensus-Driven%20Correction%20for%20Thin%20Structure%20Segmentation%0AAuthor%3A%20Jinseo%20An%20and%20Min%20Jin%20Lee%20and%20Kyu%20Won%20Shim%20and%20Helen%20Hong%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20orbital%20bones%20in%20facial%20computed%20tomography%20%28CT%29%0Aimages%20is%20essential%20for%20the%20creation%20of%20customized%20implants%20for%20reconstruction%0Aof%20defected%20orbital%20bones%2C%20particularly%20challenging%20due%20to%20the%20ambiguous%0Aboundaries%20and%20thin%20structures%20such%20as%20the%20orbital%20medial%20wall%20and%20orbital%0Afloor.%20In%20these%20ambiguous%20regions%2C%20existing%20segmentation%20approaches%20often%0Aoutput%20disconnected%20or%20under-segmented%20results.%20We%20propose%20a%20novel%20framework%0Athat%20corrects%20segmentation%20results%20by%20leveraging%20consensus%20from%20multiple%0Adiffusion%20model%20outputs.%20Our%20approach%20employs%20a%20conditional%20Bernoulli%20diffusion%0Amodel%20trained%20on%20diverse%20annotation%20patterns%20per%20image%20to%20generate%20multiple%0Aplausible%20segmentations%2C%20followed%20by%20a%20consensus-driven%20correction%20that%0Aincorporates%20position%20proximity%2C%20consensus%20level%2C%20and%20gradient%20direction%0Asimilarity%20to%20correct%20challenging%20regions.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20outperforms%20existing%20methods%2C%20significantly%20improving%20recall%20in%0Aambiguous%20regions%20while%20preserving%20the%20continuity%20of%20thin%20structures.%0AFurthermore%2C%20our%20method%20automates%20the%20manual%20process%20of%20segmentation%20result%0Acorrection%20and%20can%20be%20applied%20to%20image-guided%20surgical%20planning%20and%20surgery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Variability%2520To%2520Accuracy%253A%2520Conditional%2520Bernoulli%2520Diffusion%2520Models%250A%2520%2520with%2520Consensus-Driven%2520Correction%2520for%2520Thin%2520Structure%2520Segmentation%26entry.906535625%3DJinseo%2520An%2520and%2520Min%2520Jin%2520Lee%2520and%2520Kyu%2520Won%2520Shim%2520and%2520Helen%2520Hong%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520orbital%2520bones%2520in%2520facial%2520computed%2520tomography%2520%2528CT%2529%250Aimages%2520is%2520essential%2520for%2520the%2520creation%2520of%2520customized%2520implants%2520for%2520reconstruction%250Aof%2520defected%2520orbital%2520bones%252C%2520particularly%2520challenging%2520due%2520to%2520the%2520ambiguous%250Aboundaries%2520and%2520thin%2520structures%2520such%2520as%2520the%2520orbital%2520medial%2520wall%2520and%2520orbital%250Afloor.%2520In%2520these%2520ambiguous%2520regions%252C%2520existing%2520segmentation%2520approaches%2520often%250Aoutput%2520disconnected%2520or%2520under-segmented%2520results.%2520We%2520propose%2520a%2520novel%2520framework%250Athat%2520corrects%2520segmentation%2520results%2520by%2520leveraging%2520consensus%2520from%2520multiple%250Adiffusion%2520model%2520outputs.%2520Our%2520approach%2520employs%2520a%2520conditional%2520Bernoulli%2520diffusion%250Amodel%2520trained%2520on%2520diverse%2520annotation%2520patterns%2520per%2520image%2520to%2520generate%2520multiple%250Aplausible%2520segmentations%252C%2520followed%2520by%2520a%2520consensus-driven%2520correction%2520that%250Aincorporates%2520position%2520proximity%252C%2520consensus%2520level%252C%2520and%2520gradient%2520direction%250Asimilarity%2520to%2520correct%2520challenging%2520regions.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520existing%2520methods%252C%2520significantly%2520improving%2520recall%2520in%250Aambiguous%2520regions%2520while%2520preserving%2520the%2520continuity%2520of%2520thin%2520structures.%250AFurthermore%252C%2520our%2520method%2520automates%2520the%2520manual%2520process%2520of%2520segmentation%2520result%250Acorrection%2520and%2520can%2520be%2520applied%2520to%2520image-guided%2520surgical%2520planning%2520and%2520surgery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Variability%20To%20Accuracy%3A%20Conditional%20Bernoulli%20Diffusion%20Models%0A%20%20with%20Consensus-Driven%20Correction%20for%20Thin%20Structure%20Segmentation&entry.906535625=Jinseo%20An%20and%20Min%20Jin%20Lee%20and%20Kyu%20Won%20Shim%20and%20Helen%20Hong&entry.1292438233=%20%20Accurate%20segmentation%20of%20orbital%20bones%20in%20facial%20computed%20tomography%20%28CT%29%0Aimages%20is%20essential%20for%20the%20creation%20of%20customized%20implants%20for%20reconstruction%0Aof%20defected%20orbital%20bones%2C%20particularly%20challenging%20due%20to%20the%20ambiguous%0Aboundaries%20and%20thin%20structures%20such%20as%20the%20orbital%20medial%20wall%20and%20orbital%0Afloor.%20In%20these%20ambiguous%20regions%2C%20existing%20segmentation%20approaches%20often%0Aoutput%20disconnected%20or%20under-segmented%20results.%20We%20propose%20a%20novel%20framework%0Athat%20corrects%20segmentation%20results%20by%20leveraging%20consensus%20from%20multiple%0Adiffusion%20model%20outputs.%20Our%20approach%20employs%20a%20conditional%20Bernoulli%20diffusion%0Amodel%20trained%20on%20diverse%20annotation%20patterns%20per%20image%20to%20generate%20multiple%0Aplausible%20segmentations%2C%20followed%20by%20a%20consensus-driven%20correction%20that%0Aincorporates%20position%20proximity%2C%20consensus%20level%2C%20and%20gradient%20direction%0Asimilarity%20to%20correct%20challenging%20regions.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20outperforms%20existing%20methods%2C%20significantly%20improving%20recall%20in%0Aambiguous%20regions%20while%20preserving%20the%20continuity%20of%20thin%20structures.%0AFurthermore%2C%20our%20method%20automates%20the%20manual%20process%20of%20segmentation%20result%0Acorrection%20and%20can%20be%20applied%20to%20image-guided%20surgical%20planning%20and%20surgery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12985v1&entry.124074799=Read"},
{"title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models", "author": "Junjie Wu and Tsz Ting Chung and Kai Chen and Dit-Yan Yeung", "abstract": "  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, we design a unified\nframework to measure the object and relation hallucination in LVLMs\nsimultaneously. The core idea of our framework is to evaluate hallucinations\nvia (object, relation, object) triplets extracted from LVLMs' responses, making\nit easily generalizable to different vision-language tasks. Based on our\nframework, we further introduce Tri-HE, a novel Triplet-level Hallucination\nEvaluation benchmark which can be used to study both object and relation\nhallucination at the same time. With comprehensive evaluations on Tri-HE, we\nobserve that the relation hallucination issue is even more serious than object\nhallucination among existing LVLMs, highlighting a previously neglected problem\ntowards reliable LVLMs. Moreover, based on our findings, we design a simple\ntraining-free approach that effectively mitigates hallucinations for LVLMs. Our\ndataset and code for the reproduction of our experiments are available publicly\nat https://github.com/wujunjie1998/Tri-HE.\n", "link": "http://arxiv.org/abs/2410.23114v4", "date": "2025-07-17", "relevancy": 2.2947, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Triplet-Level%20Hallucination%20Evaluation%20for%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Unified%20Triplet-Level%20Hallucination%20Evaluation%20for%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Junjie%20Wu%20and%20Tsz%20Ting%20Chung%20and%20Kai%20Chen%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Despite%20the%20outstanding%20performance%20in%20vision-language%20reasoning%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20might%20generate%20hallucinated%20contents%20that%20do%20not%0Aexist%20in%20the%20given%20image.%20Most%20existing%20LVLM%20hallucination%20benchmarks%20are%0Aconstrained%20to%20evaluate%20the%20object-related%20hallucinations.%20However%2C%20the%0Apotential%20hallucination%20on%20the%20relations%20between%20two%20objects%2C%20i.e.%2C%20relation%0Ahallucination%2C%20still%20lacks%20investigation.%20To%20remedy%20that%2C%20we%20design%20a%20unified%0Aframework%20to%20measure%20the%20object%20and%20relation%20hallucination%20in%20LVLMs%0Asimultaneously.%20The%20core%20idea%20of%20our%20framework%20is%20to%20evaluate%20hallucinations%0Avia%20%28object%2C%20relation%2C%20object%29%20triplets%20extracted%20from%20LVLMs%27%20responses%2C%20making%0Ait%20easily%20generalizable%20to%20different%20vision-language%20tasks.%20Based%20on%20our%0Aframework%2C%20we%20further%20introduce%20Tri-HE%2C%20a%20novel%20Triplet-level%20Hallucination%0AEvaluation%20benchmark%20which%20can%20be%20used%20to%20study%20both%20object%20and%20relation%0Ahallucination%20at%20the%20same%20time.%20With%20comprehensive%20evaluations%20on%20Tri-HE%2C%20we%0Aobserve%20that%20the%20relation%20hallucination%20issue%20is%20even%20more%20serious%20than%20object%0Ahallucination%20among%20existing%20LVLMs%2C%20highlighting%20a%20previously%20neglected%20problem%0Atowards%20reliable%20LVLMs.%20Moreover%2C%20based%20on%20our%20findings%2C%20we%20design%20a%20simple%0Atraining-free%20approach%20that%20effectively%20mitigates%20hallucinations%20for%20LVLMs.%20Our%0Adataset%20and%20code%20for%20the%20reproduction%20of%20our%20experiments%20are%20available%20publicly%0Aat%20https%3A//github.com/wujunjie1998/Tri-HE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23114v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Triplet-Level%2520Hallucination%2520Evaluation%2520for%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DJunjie%2520Wu%2520and%2520Tsz%2520Ting%2520Chung%2520and%2520Kai%2520Chen%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Despite%2520the%2520outstanding%2520performance%2520in%2520vision-language%2520reasoning%252C%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520might%2520generate%2520hallucinated%2520contents%2520that%2520do%2520not%250Aexist%2520in%2520the%2520given%2520image.%2520Most%2520existing%2520LVLM%2520hallucination%2520benchmarks%2520are%250Aconstrained%2520to%2520evaluate%2520the%2520object-related%2520hallucinations.%2520However%252C%2520the%250Apotential%2520hallucination%2520on%2520the%2520relations%2520between%2520two%2520objects%252C%2520i.e.%252C%2520relation%250Ahallucination%252C%2520still%2520lacks%2520investigation.%2520To%2520remedy%2520that%252C%2520we%2520design%2520a%2520unified%250Aframework%2520to%2520measure%2520the%2520object%2520and%2520relation%2520hallucination%2520in%2520LVLMs%250Asimultaneously.%2520The%2520core%2520idea%2520of%2520our%2520framework%2520is%2520to%2520evaluate%2520hallucinations%250Avia%2520%2528object%252C%2520relation%252C%2520object%2529%2520triplets%2520extracted%2520from%2520LVLMs%2527%2520responses%252C%2520making%250Ait%2520easily%2520generalizable%2520to%2520different%2520vision-language%2520tasks.%2520Based%2520on%2520our%250Aframework%252C%2520we%2520further%2520introduce%2520Tri-HE%252C%2520a%2520novel%2520Triplet-level%2520Hallucination%250AEvaluation%2520benchmark%2520which%2520can%2520be%2520used%2520to%2520study%2520both%2520object%2520and%2520relation%250Ahallucination%2520at%2520the%2520same%2520time.%2520With%2520comprehensive%2520evaluations%2520on%2520Tri-HE%252C%2520we%250Aobserve%2520that%2520the%2520relation%2520hallucination%2520issue%2520is%2520even%2520more%2520serious%2520than%2520object%250Ahallucination%2520among%2520existing%2520LVLMs%252C%2520highlighting%2520a%2520previously%2520neglected%2520problem%250Atowards%2520reliable%2520LVLMs.%2520Moreover%252C%2520based%2520on%2520our%2520findings%252C%2520we%2520design%2520a%2520simple%250Atraining-free%2520approach%2520that%2520effectively%2520mitigates%2520hallucinations%2520for%2520LVLMs.%2520Our%250Adataset%2520and%2520code%2520for%2520the%2520reproduction%2520of%2520our%2520experiments%2520are%2520available%2520publicly%250Aat%2520https%253A//github.com/wujunjie1998/Tri-HE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23114v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Triplet-Level%20Hallucination%20Evaluation%20for%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Junjie%20Wu%20and%20Tsz%20Ting%20Chung%20and%20Kai%20Chen%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Despite%20the%20outstanding%20performance%20in%20vision-language%20reasoning%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20might%20generate%20hallucinated%20contents%20that%20do%20not%0Aexist%20in%20the%20given%20image.%20Most%20existing%20LVLM%20hallucination%20benchmarks%20are%0Aconstrained%20to%20evaluate%20the%20object-related%20hallucinations.%20However%2C%20the%0Apotential%20hallucination%20on%20the%20relations%20between%20two%20objects%2C%20i.e.%2C%20relation%0Ahallucination%2C%20still%20lacks%20investigation.%20To%20remedy%20that%2C%20we%20design%20a%20unified%0Aframework%20to%20measure%20the%20object%20and%20relation%20hallucination%20in%20LVLMs%0Asimultaneously.%20The%20core%20idea%20of%20our%20framework%20is%20to%20evaluate%20hallucinations%0Avia%20%28object%2C%20relation%2C%20object%29%20triplets%20extracted%20from%20LVLMs%27%20responses%2C%20making%0Ait%20easily%20generalizable%20to%20different%20vision-language%20tasks.%20Based%20on%20our%0Aframework%2C%20we%20further%20introduce%20Tri-HE%2C%20a%20novel%20Triplet-level%20Hallucination%0AEvaluation%20benchmark%20which%20can%20be%20used%20to%20study%20both%20object%20and%20relation%0Ahallucination%20at%20the%20same%20time.%20With%20comprehensive%20evaluations%20on%20Tri-HE%2C%20we%0Aobserve%20that%20the%20relation%20hallucination%20issue%20is%20even%20more%20serious%20than%20object%0Ahallucination%20among%20existing%20LVLMs%2C%20highlighting%20a%20previously%20neglected%20problem%0Atowards%20reliable%20LVLMs.%20Moreover%2C%20based%20on%20our%20findings%2C%20we%20design%20a%20simple%0Atraining-free%20approach%20that%20effectively%20mitigates%20hallucinations%20for%20LVLMs.%20Our%0Adataset%20and%20code%20for%20the%20reproduction%20of%20our%20experiments%20are%20available%20publicly%0Aat%20https%3A//github.com/wujunjie1998/Tri-HE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23114v4&entry.124074799=Read"},
{"title": "RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model\n  for Spectral Reconstruction", "author": "Keli Deng and Jie Nie and Yuntao Qian", "abstract": "  Spectral reconstruction (SR) is a crucial problem in image processing that\nrequires reconstructing hyperspectral images (HSIs) from the corresponding RGB\nimages. A key difficulty in SR is estimating the unobservable feature, which\nencapsulates significant spectral information not captured by RGB imaging\nsensors. The solution lies in effectively constructing the spectral-spatial\njoint distribution conditioned on the RGB image to complement the unobservable\nfeature. Since HSIs share a similar spatial structure with the corresponding\nRGB images, it is rational to capitalize on the rich spatial knowledge in RGB\npre-trained models for spectral-spatial joint distribution learning. To this\nend, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an\nunobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding\nspatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can\nfocus on modeling spectral structure. Moreover, separating the unobservable\nfeature from the HSI reduces the redundant spectral information and empowers\nthe ULDM to learn the joint distribution in a compact latent space.\nSpecifically, we propose a two-stage pipeline consisting of spectral structure\nrepresentation learning and spectral-spatial joint distribution learning to\ntransform the RGB-LDM into the ULDM. In the first stage, a spectral\nunobservable feature autoencoder (SpeUAE) is trained to extract and compress\nthe unobservable feature into a 3D manifold aligned with RGB space. In the\nsecond stage, the spectral and spatial structures are sequentially encoded by\nthe SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the\ndistribution of the coded unobservable feature with guidance from the\ncorresponding RGB images. Experimental results on SR and downstream relighting\ntasks demonstrate that our proposed method achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2507.12967v1", "date": "2025-07-17", "relevancy": 2.2876, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6212}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5775}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB%20Pre-Training%20Enhanced%20Unobservable%20Feature%20Latent%20Diffusion%20Model%0A%20%20for%20Spectral%20Reconstruction&body=Title%3A%20RGB%20Pre-Training%20Enhanced%20Unobservable%20Feature%20Latent%20Diffusion%20Model%0A%20%20for%20Spectral%20Reconstruction%0AAuthor%3A%20Keli%20Deng%20and%20Jie%20Nie%20and%20Yuntao%20Qian%0AAbstract%3A%20%20%20Spectral%20reconstruction%20%28SR%29%20is%20a%20crucial%20problem%20in%20image%20processing%20that%0Arequires%20reconstructing%20hyperspectral%20images%20%28HSIs%29%20from%20the%20corresponding%20RGB%0Aimages.%20A%20key%20difficulty%20in%20SR%20is%20estimating%20the%20unobservable%20feature%2C%20which%0Aencapsulates%20significant%20spectral%20information%20not%20captured%20by%20RGB%20imaging%0Asensors.%20The%20solution%20lies%20in%20effectively%20constructing%20the%20spectral-spatial%0Ajoint%20distribution%20conditioned%20on%20the%20RGB%20image%20to%20complement%20the%20unobservable%0Afeature.%20Since%20HSIs%20share%20a%20similar%20spatial%20structure%20with%20the%20corresponding%0ARGB%20images%2C%20it%20is%20rational%20to%20capitalize%20on%20the%20rich%20spatial%20knowledge%20in%20RGB%0Apre-trained%20models%20for%20spectral-spatial%20joint%20distribution%20learning.%20To%20this%0Aend%2C%20we%20extend%20the%20RGB%20pre-trained%20latent%20diffusion%20model%20%28RGB-LDM%29%20to%20an%0Aunobservable%20feature%20LDM%20%28ULDM%29%20for%20SR.%20As%20the%20RGB-LDM%20and%20its%20corresponding%0Aspatial%20autoencoder%20%28SpaAE%29%20already%20excel%20in%20spatial%20knowledge%2C%20the%20ULDM%20can%0Afocus%20on%20modeling%20spectral%20structure.%20Moreover%2C%20separating%20the%20unobservable%0Afeature%20from%20the%20HSI%20reduces%20the%20redundant%20spectral%20information%20and%20empowers%0Athe%20ULDM%20to%20learn%20the%20joint%20distribution%20in%20a%20compact%20latent%20space.%0ASpecifically%2C%20we%20propose%20a%20two-stage%20pipeline%20consisting%20of%20spectral%20structure%0Arepresentation%20learning%20and%20spectral-spatial%20joint%20distribution%20learning%20to%0Atransform%20the%20RGB-LDM%20into%20the%20ULDM.%20In%20the%20first%20stage%2C%20a%20spectral%0Aunobservable%20feature%20autoencoder%20%28SpeUAE%29%20is%20trained%20to%20extract%20and%20compress%0Athe%20unobservable%20feature%20into%20a%203D%20manifold%20aligned%20with%20RGB%20space.%20In%20the%0Asecond%20stage%2C%20the%20spectral%20and%20spatial%20structures%20are%20sequentially%20encoded%20by%0Athe%20SpeUAE%20and%20the%20SpaAE%2C%20respectively.%20The%20ULDM%20is%20then%20acquired%20to%20model%20the%0Adistribution%20of%20the%20coded%20unobservable%20feature%20with%20guidance%20from%20the%0Acorresponding%20RGB%20images.%20Experimental%20results%20on%20SR%20and%20downstream%20relighting%0Atasks%20demonstrate%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB%2520Pre-Training%2520Enhanced%2520Unobservable%2520Feature%2520Latent%2520Diffusion%2520Model%250A%2520%2520for%2520Spectral%2520Reconstruction%26entry.906535625%3DKeli%2520Deng%2520and%2520Jie%2520Nie%2520and%2520Yuntao%2520Qian%26entry.1292438233%3D%2520%2520Spectral%2520reconstruction%2520%2528SR%2529%2520is%2520a%2520crucial%2520problem%2520in%2520image%2520processing%2520that%250Arequires%2520reconstructing%2520hyperspectral%2520images%2520%2528HSIs%2529%2520from%2520the%2520corresponding%2520RGB%250Aimages.%2520A%2520key%2520difficulty%2520in%2520SR%2520is%2520estimating%2520the%2520unobservable%2520feature%252C%2520which%250Aencapsulates%2520significant%2520spectral%2520information%2520not%2520captured%2520by%2520RGB%2520imaging%250Asensors.%2520The%2520solution%2520lies%2520in%2520effectively%2520constructing%2520the%2520spectral-spatial%250Ajoint%2520distribution%2520conditioned%2520on%2520the%2520RGB%2520image%2520to%2520complement%2520the%2520unobservable%250Afeature.%2520Since%2520HSIs%2520share%2520a%2520similar%2520spatial%2520structure%2520with%2520the%2520corresponding%250ARGB%2520images%252C%2520it%2520is%2520rational%2520to%2520capitalize%2520on%2520the%2520rich%2520spatial%2520knowledge%2520in%2520RGB%250Apre-trained%2520models%2520for%2520spectral-spatial%2520joint%2520distribution%2520learning.%2520To%2520this%250Aend%252C%2520we%2520extend%2520the%2520RGB%2520pre-trained%2520latent%2520diffusion%2520model%2520%2528RGB-LDM%2529%2520to%2520an%250Aunobservable%2520feature%2520LDM%2520%2528ULDM%2529%2520for%2520SR.%2520As%2520the%2520RGB-LDM%2520and%2520its%2520corresponding%250Aspatial%2520autoencoder%2520%2528SpaAE%2529%2520already%2520excel%2520in%2520spatial%2520knowledge%252C%2520the%2520ULDM%2520can%250Afocus%2520on%2520modeling%2520spectral%2520structure.%2520Moreover%252C%2520separating%2520the%2520unobservable%250Afeature%2520from%2520the%2520HSI%2520reduces%2520the%2520redundant%2520spectral%2520information%2520and%2520empowers%250Athe%2520ULDM%2520to%2520learn%2520the%2520joint%2520distribution%2520in%2520a%2520compact%2520latent%2520space.%250ASpecifically%252C%2520we%2520propose%2520a%2520two-stage%2520pipeline%2520consisting%2520of%2520spectral%2520structure%250Arepresentation%2520learning%2520and%2520spectral-spatial%2520joint%2520distribution%2520learning%2520to%250Atransform%2520the%2520RGB-LDM%2520into%2520the%2520ULDM.%2520In%2520the%2520first%2520stage%252C%2520a%2520spectral%250Aunobservable%2520feature%2520autoencoder%2520%2528SpeUAE%2529%2520is%2520trained%2520to%2520extract%2520and%2520compress%250Athe%2520unobservable%2520feature%2520into%2520a%25203D%2520manifold%2520aligned%2520with%2520RGB%2520space.%2520In%2520the%250Asecond%2520stage%252C%2520the%2520spectral%2520and%2520spatial%2520structures%2520are%2520sequentially%2520encoded%2520by%250Athe%2520SpeUAE%2520and%2520the%2520SpaAE%252C%2520respectively.%2520The%2520ULDM%2520is%2520then%2520acquired%2520to%2520model%2520the%250Adistribution%2520of%2520the%2520coded%2520unobservable%2520feature%2520with%2520guidance%2520from%2520the%250Acorresponding%2520RGB%2520images.%2520Experimental%2520results%2520on%2520SR%2520and%2520downstream%2520relighting%250Atasks%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB%20Pre-Training%20Enhanced%20Unobservable%20Feature%20Latent%20Diffusion%20Model%0A%20%20for%20Spectral%20Reconstruction&entry.906535625=Keli%20Deng%20and%20Jie%20Nie%20and%20Yuntao%20Qian&entry.1292438233=%20%20Spectral%20reconstruction%20%28SR%29%20is%20a%20crucial%20problem%20in%20image%20processing%20that%0Arequires%20reconstructing%20hyperspectral%20images%20%28HSIs%29%20from%20the%20corresponding%20RGB%0Aimages.%20A%20key%20difficulty%20in%20SR%20is%20estimating%20the%20unobservable%20feature%2C%20which%0Aencapsulates%20significant%20spectral%20information%20not%20captured%20by%20RGB%20imaging%0Asensors.%20The%20solution%20lies%20in%20effectively%20constructing%20the%20spectral-spatial%0Ajoint%20distribution%20conditioned%20on%20the%20RGB%20image%20to%20complement%20the%20unobservable%0Afeature.%20Since%20HSIs%20share%20a%20similar%20spatial%20structure%20with%20the%20corresponding%0ARGB%20images%2C%20it%20is%20rational%20to%20capitalize%20on%20the%20rich%20spatial%20knowledge%20in%20RGB%0Apre-trained%20models%20for%20spectral-spatial%20joint%20distribution%20learning.%20To%20this%0Aend%2C%20we%20extend%20the%20RGB%20pre-trained%20latent%20diffusion%20model%20%28RGB-LDM%29%20to%20an%0Aunobservable%20feature%20LDM%20%28ULDM%29%20for%20SR.%20As%20the%20RGB-LDM%20and%20its%20corresponding%0Aspatial%20autoencoder%20%28SpaAE%29%20already%20excel%20in%20spatial%20knowledge%2C%20the%20ULDM%20can%0Afocus%20on%20modeling%20spectral%20structure.%20Moreover%2C%20separating%20the%20unobservable%0Afeature%20from%20the%20HSI%20reduces%20the%20redundant%20spectral%20information%20and%20empowers%0Athe%20ULDM%20to%20learn%20the%20joint%20distribution%20in%20a%20compact%20latent%20space.%0ASpecifically%2C%20we%20propose%20a%20two-stage%20pipeline%20consisting%20of%20spectral%20structure%0Arepresentation%20learning%20and%20spectral-spatial%20joint%20distribution%20learning%20to%0Atransform%20the%20RGB-LDM%20into%20the%20ULDM.%20In%20the%20first%20stage%2C%20a%20spectral%0Aunobservable%20feature%20autoencoder%20%28SpeUAE%29%20is%20trained%20to%20extract%20and%20compress%0Athe%20unobservable%20feature%20into%20a%203D%20manifold%20aligned%20with%20RGB%20space.%20In%20the%0Asecond%20stage%2C%20the%20spectral%20and%20spatial%20structures%20are%20sequentially%20encoded%20by%0Athe%20SpeUAE%20and%20the%20SpaAE%2C%20respectively.%20The%20ULDM%20is%20then%20acquired%20to%20model%20the%0Adistribution%20of%20the%20coded%20unobservable%20feature%20with%20guidance%20from%20the%0Acorresponding%20RGB%20images.%20Experimental%20results%20on%20SR%20and%20downstream%20relighting%0Atasks%20demonstrate%20that%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12967v1&entry.124074799=Read"},
{"title": "4D-MISR: A unified model for low-dose super-resolution imaging via\n  feature fusion", "author": "Zifei Wang and Zian Mao and Xiaoya He and Xi Huang and Haoran Zhang and Chun Cheng and Shufen Chu and Tingzheng Hou and Xiaoqin Zeng and Yujun Xie", "abstract": "  While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.\n", "link": "http://arxiv.org/abs/2507.09953v3", "date": "2025-07-17", "relevancy": 2.2875, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion&body=Title%3A%204D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion%0AAuthor%3A%20Zifei%20Wang%20and%20Zian%20Mao%20and%20Xiaoya%20He%20and%20Xi%20Huang%20and%20Haoran%20Zhang%20and%20Chun%20Cheng%20and%20Shufen%20Chu%20and%20Tingzheng%20Hou%20and%20Xiaoqin%20Zeng%20and%20Yujun%20Xie%0AAbstract%3A%20%20%20While%20electron%20microscopy%20offers%20crucial%20atomic-resolution%20insights%20into%0Astructure-property%20relationships%2C%20radiation%20damage%20severely%20limits%20its%20use%20on%0Abeam-sensitive%20materials%20like%20proteins%20and%202D%20materials.%20To%20overcome%20this%0Achallenge%2C%20we%20push%20beyond%20the%20electron%20dose%20limits%20of%20conventional%20electron%0Amicroscopy%20by%20adapting%20principles%20from%20multi-image%20super-resolution%20%28MISR%29%20that%0Ahave%20been%20widely%20used%20in%20remote%20sensing.%20Our%20method%20fuses%20multiple%0Alow-resolution%2C%20sub-pixel-shifted%20views%20and%20enhances%20the%20reconstruction%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20that%20integrates%20features%20from%20synthetic%2C%0Amulti-angle%20observations.%20We%20developed%20a%20dual-path%2C%20attention-guided%20network%0Afor%204D-STEM%20that%20achieves%20atomic-scale%20super-resolution%20from%20ultra-low-dose%0Adata.%20This%20provides%20robust%20atomic-scale%20visualization%20across%20amorphous%2C%0Asemi-crystalline%2C%20and%20crystalline%20beam-sensitive%20specimens.%20Systematic%0Aevaluations%20on%20representative%20materials%20demonstrate%20comparable%20spatial%0Aresolution%20to%20conventional%20ptychography%20under%20ultra-low-dose%20conditions.%20Our%0Awork%20expands%20the%20capabilities%20of%204D-STEM%2C%20offering%20a%20new%20and%20generalizable%0Amethod%20for%20the%20structural%20analysis%20of%20radiation-vulnerable%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09953v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-MISR%253A%2520A%2520unified%2520model%2520for%2520low-dose%2520super-resolution%2520imaging%2520via%250A%2520%2520feature%2520fusion%26entry.906535625%3DZifei%2520Wang%2520and%2520Zian%2520Mao%2520and%2520Xiaoya%2520He%2520and%2520Xi%2520Huang%2520and%2520Haoran%2520Zhang%2520and%2520Chun%2520Cheng%2520and%2520Shufen%2520Chu%2520and%2520Tingzheng%2520Hou%2520and%2520Xiaoqin%2520Zeng%2520and%2520Yujun%2520Xie%26entry.1292438233%3D%2520%2520While%2520electron%2520microscopy%2520offers%2520crucial%2520atomic-resolution%2520insights%2520into%250Astructure-property%2520relationships%252C%2520radiation%2520damage%2520severely%2520limits%2520its%2520use%2520on%250Abeam-sensitive%2520materials%2520like%2520proteins%2520and%25202D%2520materials.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520push%2520beyond%2520the%2520electron%2520dose%2520limits%2520of%2520conventional%2520electron%250Amicroscopy%2520by%2520adapting%2520principles%2520from%2520multi-image%2520super-resolution%2520%2528MISR%2529%2520that%250Ahave%2520been%2520widely%2520used%2520in%2520remote%2520sensing.%2520Our%2520method%2520fuses%2520multiple%250Alow-resolution%252C%2520sub-pixel-shifted%2520views%2520and%2520enhances%2520the%2520reconstruction%2520with%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520that%2520integrates%2520features%2520from%2520synthetic%252C%250Amulti-angle%2520observations.%2520We%2520developed%2520a%2520dual-path%252C%2520attention-guided%2520network%250Afor%25204D-STEM%2520that%2520achieves%2520atomic-scale%2520super-resolution%2520from%2520ultra-low-dose%250Adata.%2520This%2520provides%2520robust%2520atomic-scale%2520visualization%2520across%2520amorphous%252C%250Asemi-crystalline%252C%2520and%2520crystalline%2520beam-sensitive%2520specimens.%2520Systematic%250Aevaluations%2520on%2520representative%2520materials%2520demonstrate%2520comparable%2520spatial%250Aresolution%2520to%2520conventional%2520ptychography%2520under%2520ultra-low-dose%2520conditions.%2520Our%250Awork%2520expands%2520the%2520capabilities%2520of%25204D-STEM%252C%2520offering%2520a%2520new%2520and%2520generalizable%250Amethod%2520for%2520the%2520structural%2520analysis%2520of%2520radiation-vulnerable%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09953v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion&entry.906535625=Zifei%20Wang%20and%20Zian%20Mao%20and%20Xiaoya%20He%20and%20Xi%20Huang%20and%20Haoran%20Zhang%20and%20Chun%20Cheng%20and%20Shufen%20Chu%20and%20Tingzheng%20Hou%20and%20Xiaoqin%20Zeng%20and%20Yujun%20Xie&entry.1292438233=%20%20While%20electron%20microscopy%20offers%20crucial%20atomic-resolution%20insights%20into%0Astructure-property%20relationships%2C%20radiation%20damage%20severely%20limits%20its%20use%20on%0Abeam-sensitive%20materials%20like%20proteins%20and%202D%20materials.%20To%20overcome%20this%0Achallenge%2C%20we%20push%20beyond%20the%20electron%20dose%20limits%20of%20conventional%20electron%0Amicroscopy%20by%20adapting%20principles%20from%20multi-image%20super-resolution%20%28MISR%29%20that%0Ahave%20been%20widely%20used%20in%20remote%20sensing.%20Our%20method%20fuses%20multiple%0Alow-resolution%2C%20sub-pixel-shifted%20views%20and%20enhances%20the%20reconstruction%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20that%20integrates%20features%20from%20synthetic%2C%0Amulti-angle%20observations.%20We%20developed%20a%20dual-path%2C%20attention-guided%20network%0Afor%204D-STEM%20that%20achieves%20atomic-scale%20super-resolution%20from%20ultra-low-dose%0Adata.%20This%20provides%20robust%20atomic-scale%20visualization%20across%20amorphous%2C%0Asemi-crystalline%2C%20and%20crystalline%20beam-sensitive%20specimens.%20Systematic%0Aevaluations%20on%20representative%20materials%20demonstrate%20comparable%20spatial%0Aresolution%20to%20conventional%20ptychography%20under%20ultra-low-dose%20conditions.%20Our%0Awork%20expands%20the%20capabilities%20of%204D-STEM%2C%20offering%20a%20new%20and%20generalizable%0Amethod%20for%20the%20structural%20analysis%20of%20radiation-vulnerable%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09953v3&entry.124074799=Read"},
{"title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models", "author": "Xiangyu Dong and Haoran Zhao and Jiang Gao and Haozhou Li and Xiaoguang Ma and Yaoming Zhou and Fuhai Chen and Juan Liu", "abstract": "  Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.\n", "link": "http://arxiv.org/abs/2507.13152v1", "date": "2025-07-17", "relevancy": 2.2788, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE-VLN%3A%20A%20Self-Evolving%20Vision-Language%20Navigation%20Framework%20Based%20on%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20SE-VLN%3A%20A%20Self-Evolving%20Vision-Language%20Navigation%20Framework%20Based%20on%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Xiangyu%20Dong%20and%20Haoran%20Zhao%20and%20Jiang%20Gao%20and%20Haozhou%20Li%20and%20Xiaoguang%20Ma%20and%20Yaoming%20Zhou%20and%20Fuhai%20Chen%20and%20Juan%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20navigation%20%28VLN%29%20were%20mainly%20attributed%20to%0Aemerging%20large%20language%20models%20%28LLMs%29.%20These%20methods%20exhibited%20excellent%0Ageneralization%20capabilities%20in%20instruction%20understanding%20and%20task%20reasoning.%0AHowever%2C%20they%20were%20constrained%20by%20the%20fixed%20knowledge%20bases%20and%20reasoning%0Aabilities%20of%20LLMs%2C%20preventing%20fully%20incorporating%20experiential%20knowledge%20and%0Athus%20resulting%20in%20a%20lack%20of%20efficient%20evolutionary%20capacity.%20To%20address%20this%2C%0Awe%20drew%20inspiration%20from%20the%20evolution%20capabilities%20of%20natural%20agents%2C%20and%0Aproposed%20a%20self-evolving%20VLN%20framework%20%28SE-VLN%29%20to%20endow%20VLN%20agents%20with%20the%0Aability%20to%20continuously%20evolve%20during%20testing.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Awas%20the%20first%20time%20that%20an%20multimodal%20LLM-powered%20self-evolving%20VLN%20framework%0Awas%20proposed.%20Specifically%2C%20SE-VLN%20comprised%20three%20core%20modules%2C%20i.e.%2C%20a%0Ahierarchical%20memory%20module%20to%20transfer%20successful%20and%20failure%20cases%20into%0Areusable%20knowledge%2C%20a%20retrieval-augmented%20thought-based%20reasoning%20module%20to%0Aretrieve%20experience%20and%20enable%20multi-step%20decision-making%2C%20and%20a%20reflection%0Amodule%20to%20realize%20continual%20evolution.%20Comprehensive%20tests%20illustrated%20that%20the%0ASE-VLN%20achieved%20navigation%20success%20rates%20of%2057%25%20and%2035.2%25%20in%20unseen%0Aenvironments%2C%20representing%20absolute%20performance%20improvements%20of%2023.9%25%20and%2015.0%25%0Aover%20current%20state-of-the-art%20methods%20on%20R2R%20and%20REVERSE%20datasets%2C%0Arespectively.%20Moreover%2C%20the%20SE-VLN%20showed%20performance%20improvement%20with%0Aincreasing%20experience%20repository%2C%20elucidating%20its%20great%20potential%20as%20a%0Aself-evolving%20agent%20framework%20for%20VLN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE-VLN%253A%2520A%2520Self-Evolving%2520Vision-Language%2520Navigation%2520Framework%2520Based%2520on%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DXiangyu%2520Dong%2520and%2520Haoran%2520Zhao%2520and%2520Jiang%2520Gao%2520and%2520Haozhou%2520Li%2520and%2520Xiaoguang%2520Ma%2520and%2520Yaoming%2520Zhou%2520and%2520Fuhai%2520Chen%2520and%2520Juan%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520navigation%2520%2528VLN%2529%2520were%2520mainly%2520attributed%2520to%250Aemerging%2520large%2520language%2520models%2520%2528LLMs%2529.%2520These%2520methods%2520exhibited%2520excellent%250Ageneralization%2520capabilities%2520in%2520instruction%2520understanding%2520and%2520task%2520reasoning.%250AHowever%252C%2520they%2520were%2520constrained%2520by%2520the%2520fixed%2520knowledge%2520bases%2520and%2520reasoning%250Aabilities%2520of%2520LLMs%252C%2520preventing%2520fully%2520incorporating%2520experiential%2520knowledge%2520and%250Athus%2520resulting%2520in%2520a%2520lack%2520of%2520efficient%2520evolutionary%2520capacity.%2520To%2520address%2520this%252C%250Awe%2520drew%2520inspiration%2520from%2520the%2520evolution%2520capabilities%2520of%2520natural%2520agents%252C%2520and%250Aproposed%2520a%2520self-evolving%2520VLN%2520framework%2520%2528SE-VLN%2529%2520to%2520endow%2520VLN%2520agents%2520with%2520the%250Aability%2520to%2520continuously%2520evolve%2520during%2520testing.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520it%250Awas%2520the%2520first%2520time%2520that%2520an%2520multimodal%2520LLM-powered%2520self-evolving%2520VLN%2520framework%250Awas%2520proposed.%2520Specifically%252C%2520SE-VLN%2520comprised%2520three%2520core%2520modules%252C%2520i.e.%252C%2520a%250Ahierarchical%2520memory%2520module%2520to%2520transfer%2520successful%2520and%2520failure%2520cases%2520into%250Areusable%2520knowledge%252C%2520a%2520retrieval-augmented%2520thought-based%2520reasoning%2520module%2520to%250Aretrieve%2520experience%2520and%2520enable%2520multi-step%2520decision-making%252C%2520and%2520a%2520reflection%250Amodule%2520to%2520realize%2520continual%2520evolution.%2520Comprehensive%2520tests%2520illustrated%2520that%2520the%250ASE-VLN%2520achieved%2520navigation%2520success%2520rates%2520of%252057%2525%2520and%252035.2%2525%2520in%2520unseen%250Aenvironments%252C%2520representing%2520absolute%2520performance%2520improvements%2520of%252023.9%2525%2520and%252015.0%2525%250Aover%2520current%2520state-of-the-art%2520methods%2520on%2520R2R%2520and%2520REVERSE%2520datasets%252C%250Arespectively.%2520Moreover%252C%2520the%2520SE-VLN%2520showed%2520performance%2520improvement%2520with%250Aincreasing%2520experience%2520repository%252C%2520elucidating%2520its%2520great%2520potential%2520as%2520a%250Aself-evolving%2520agent%2520framework%2520for%2520VLN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE-VLN%3A%20A%20Self-Evolving%20Vision-Language%20Navigation%20Framework%20Based%20on%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Xiangyu%20Dong%20and%20Haoran%20Zhao%20and%20Jiang%20Gao%20and%20Haozhou%20Li%20and%20Xiaoguang%20Ma%20and%20Yaoming%20Zhou%20and%20Fuhai%20Chen%20and%20Juan%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20navigation%20%28VLN%29%20were%20mainly%20attributed%20to%0Aemerging%20large%20language%20models%20%28LLMs%29.%20These%20methods%20exhibited%20excellent%0Ageneralization%20capabilities%20in%20instruction%20understanding%20and%20task%20reasoning.%0AHowever%2C%20they%20were%20constrained%20by%20the%20fixed%20knowledge%20bases%20and%20reasoning%0Aabilities%20of%20LLMs%2C%20preventing%20fully%20incorporating%20experiential%20knowledge%20and%0Athus%20resulting%20in%20a%20lack%20of%20efficient%20evolutionary%20capacity.%20To%20address%20this%2C%0Awe%20drew%20inspiration%20from%20the%20evolution%20capabilities%20of%20natural%20agents%2C%20and%0Aproposed%20a%20self-evolving%20VLN%20framework%20%28SE-VLN%29%20to%20endow%20VLN%20agents%20with%20the%0Aability%20to%20continuously%20evolve%20during%20testing.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Awas%20the%20first%20time%20that%20an%20multimodal%20LLM-powered%20self-evolving%20VLN%20framework%0Awas%20proposed.%20Specifically%2C%20SE-VLN%20comprised%20three%20core%20modules%2C%20i.e.%2C%20a%0Ahierarchical%20memory%20module%20to%20transfer%20successful%20and%20failure%20cases%20into%0Areusable%20knowledge%2C%20a%20retrieval-augmented%20thought-based%20reasoning%20module%20to%0Aretrieve%20experience%20and%20enable%20multi-step%20decision-making%2C%20and%20a%20reflection%0Amodule%20to%20realize%20continual%20evolution.%20Comprehensive%20tests%20illustrated%20that%20the%0ASE-VLN%20achieved%20navigation%20success%20rates%20of%2057%25%20and%2035.2%25%20in%20unseen%0Aenvironments%2C%20representing%20absolute%20performance%20improvements%20of%2023.9%25%20and%2015.0%25%0Aover%20current%20state-of-the-art%20methods%20on%20R2R%20and%20REVERSE%20datasets%2C%0Arespectively.%20Moreover%2C%20the%20SE-VLN%20showed%20performance%20improvement%20with%0Aincreasing%20experience%20repository%2C%20elucidating%20its%20great%20potential%20as%20a%0Aself-evolving%20agent%20framework%20for%20VLN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13152v1&entry.124074799=Read"},
{"title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain\n  Environments under Data Sharing constraints", "author": "Youssef Tawfilis and Hossam Amer and Minar El-Aasser and Tallal Elshabrawy", "abstract": "  Federated Learning has gained increasing attention for its ability to enable\nmultiple nodes to collaboratively train machine learning models without sharing\ntheir raw data. At the same time, Generative AI -- particularly Generative\nAdversarial Networks (GANs) -- have achieved remarkable success across a wide\nrange of domains, such as healthcare, security, and Image Generation. However,\ntraining generative models typically requires large datasets and significant\ncomputational resources, which are often unavailable in real-world settings.\nAcquiring such resources can be costly and inefficient, especially when many\nunderutilized devices -- such as IoT devices and edge devices -- with varying\ncapabilities remain idle. Moreover, obtaining large datasets is challenging due\nto privacy concerns and copyright restrictions, as most devices are unwilling\nto share their data. To address these challenges, we propose a novel approach\nfor decentralized GAN training that enables the utilization of distributed data\nand underutilized, low-capability devices while not sharing data in its raw\nform. Our approach is designed to tackle key challenges in decentralized\nenvironments, combining KLD-weighted Clustered Federated Learning to address\nthe issues of data heterogeneity and multi-domain datasets, with Heterogeneous\nU-Shaped split learning to tackle the challenge of device heterogeneity under\nstrict data sharing constraints -- ensuring that no labels or raw data, whether\nreal or synthetic, are ever shared between nodes. Experimental results shows\nthat our approach demonstrates consistent and significant improvements across\nkey performance metrics, where it achieves 1.1x -- 2.2x higher image generation\nscores, an average 10% boost in classification metrics (up to 50% in\nmulti-domain non-IID settings), in much lower latency compared to several\nbenchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.\n", "link": "http://arxiv.org/abs/2507.12979v1", "date": "2025-07-17", "relevancy": 2.2745, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5776}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5746}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%0A%20%20Environments%20under%20Data%20Sharing%20constraints&body=Title%3A%20A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%0A%20%20Environments%20under%20Data%20Sharing%20constraints%0AAuthor%3A%20Youssef%20Tawfilis%20and%20Hossam%20Amer%20and%20Minar%20El-Aasser%20and%20Tallal%20Elshabrawy%0AAbstract%3A%20%20%20Federated%20Learning%20has%20gained%20increasing%20attention%20for%20its%20ability%20to%20enable%0Amultiple%20nodes%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%0Atheir%20raw%20data.%20At%20the%20same%20time%2C%20Generative%20AI%20--%20particularly%20Generative%0AAdversarial%20Networks%20%28GANs%29%20--%20have%20achieved%20remarkable%20success%20across%20a%20wide%0Arange%20of%20domains%2C%20such%20as%20healthcare%2C%20security%2C%20and%20Image%20Generation.%20However%2C%0Atraining%20generative%20models%20typically%20requires%20large%20datasets%20and%20significant%0Acomputational%20resources%2C%20which%20are%20often%20unavailable%20in%20real-world%20settings.%0AAcquiring%20such%20resources%20can%20be%20costly%20and%20inefficient%2C%20especially%20when%20many%0Aunderutilized%20devices%20--%20such%20as%20IoT%20devices%20and%20edge%20devices%20--%20with%20varying%0Acapabilities%20remain%20idle.%20Moreover%2C%20obtaining%20large%20datasets%20is%20challenging%20due%0Ato%20privacy%20concerns%20and%20copyright%20restrictions%2C%20as%20most%20devices%20are%20unwilling%0Ato%20share%20their%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%0Afor%20decentralized%20GAN%20training%20that%20enables%20the%20utilization%20of%20distributed%20data%0Aand%20underutilized%2C%20low-capability%20devices%20while%20not%20sharing%20data%20in%20its%20raw%0Aform.%20Our%20approach%20is%20designed%20to%20tackle%20key%20challenges%20in%20decentralized%0Aenvironments%2C%20combining%20KLD-weighted%20Clustered%20Federated%20Learning%20to%20address%0Athe%20issues%20of%20data%20heterogeneity%20and%20multi-domain%20datasets%2C%20with%20Heterogeneous%0AU-Shaped%20split%20learning%20to%20tackle%20the%20challenge%20of%20device%20heterogeneity%20under%0Astrict%20data%20sharing%20constraints%20--%20ensuring%20that%20no%20labels%20or%20raw%20data%2C%20whether%0Areal%20or%20synthetic%2C%20are%20ever%20shared%20between%20nodes.%20Experimental%20results%20shows%0Athat%20our%20approach%20demonstrates%20consistent%20and%20significant%20improvements%20across%0Akey%20performance%20metrics%2C%20where%20it%20achieves%201.1x%20--%202.2x%20higher%20image%20generation%0Ascores%2C%20an%20average%2010%25%20boost%20in%20classification%20metrics%20%28up%20to%2050%25%20in%0Amulti-domain%20non-IID%20settings%29%2C%20in%20much%20lower%20latency%20compared%20to%20several%0Abenchmarks.%20Find%20our%20code%20at%20https%3A//github.com/youssefga28/HuSCF-GAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Distributed%2520Generative%2520AI%2520Approach%2520for%2520Heterogeneous%2520Multi-Domain%250A%2520%2520Environments%2520under%2520Data%2520Sharing%2520constraints%26entry.906535625%3DYoussef%2520Tawfilis%2520and%2520Hossam%2520Amer%2520and%2520Minar%2520El-Aasser%2520and%2520Tallal%2520Elshabrawy%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520has%2520gained%2520increasing%2520attention%2520for%2520its%2520ability%2520to%2520enable%250Amultiple%2520nodes%2520to%2520collaboratively%2520train%2520machine%2520learning%2520models%2520without%2520sharing%250Atheir%2520raw%2520data.%2520At%2520the%2520same%2520time%252C%2520Generative%2520AI%2520--%2520particularly%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%2520--%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520wide%250Arange%2520of%2520domains%252C%2520such%2520as%2520healthcare%252C%2520security%252C%2520and%2520Image%2520Generation.%2520However%252C%250Atraining%2520generative%2520models%2520typically%2520requires%2520large%2520datasets%2520and%2520significant%250Acomputational%2520resources%252C%2520which%2520are%2520often%2520unavailable%2520in%2520real-world%2520settings.%250AAcquiring%2520such%2520resources%2520can%2520be%2520costly%2520and%2520inefficient%252C%2520especially%2520when%2520many%250Aunderutilized%2520devices%2520--%2520such%2520as%2520IoT%2520devices%2520and%2520edge%2520devices%2520--%2520with%2520varying%250Acapabilities%2520remain%2520idle.%2520Moreover%252C%2520obtaining%2520large%2520datasets%2520is%2520challenging%2520due%250Ato%2520privacy%2520concerns%2520and%2520copyright%2520restrictions%252C%2520as%2520most%2520devices%2520are%2520unwilling%250Ato%2520share%2520their%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%250Afor%2520decentralized%2520GAN%2520training%2520that%2520enables%2520the%2520utilization%2520of%2520distributed%2520data%250Aand%2520underutilized%252C%2520low-capability%2520devices%2520while%2520not%2520sharing%2520data%2520in%2520its%2520raw%250Aform.%2520Our%2520approach%2520is%2520designed%2520to%2520tackle%2520key%2520challenges%2520in%2520decentralized%250Aenvironments%252C%2520combining%2520KLD-weighted%2520Clustered%2520Federated%2520Learning%2520to%2520address%250Athe%2520issues%2520of%2520data%2520heterogeneity%2520and%2520multi-domain%2520datasets%252C%2520with%2520Heterogeneous%250AU-Shaped%2520split%2520learning%2520to%2520tackle%2520the%2520challenge%2520of%2520device%2520heterogeneity%2520under%250Astrict%2520data%2520sharing%2520constraints%2520--%2520ensuring%2520that%2520no%2520labels%2520or%2520raw%2520data%252C%2520whether%250Areal%2520or%2520synthetic%252C%2520are%2520ever%2520shared%2520between%2520nodes.%2520Experimental%2520results%2520shows%250Athat%2520our%2520approach%2520demonstrates%2520consistent%2520and%2520significant%2520improvements%2520across%250Akey%2520performance%2520metrics%252C%2520where%2520it%2520achieves%25201.1x%2520--%25202.2x%2520higher%2520image%2520generation%250Ascores%252C%2520an%2520average%252010%2525%2520boost%2520in%2520classification%2520metrics%2520%2528up%2520to%252050%2525%2520in%250Amulti-domain%2520non-IID%2520settings%2529%252C%2520in%2520much%2520lower%2520latency%2520compared%2520to%2520several%250Abenchmarks.%2520Find%2520our%2520code%2520at%2520https%253A//github.com/youssefga28/HuSCF-GAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Distributed%20Generative%20AI%20Approach%20for%20Heterogeneous%20Multi-Domain%0A%20%20Environments%20under%20Data%20Sharing%20constraints&entry.906535625=Youssef%20Tawfilis%20and%20Hossam%20Amer%20and%20Minar%20El-Aasser%20and%20Tallal%20Elshabrawy&entry.1292438233=%20%20Federated%20Learning%20has%20gained%20increasing%20attention%20for%20its%20ability%20to%20enable%0Amultiple%20nodes%20to%20collaboratively%20train%20machine%20learning%20models%20without%20sharing%0Atheir%20raw%20data.%20At%20the%20same%20time%2C%20Generative%20AI%20--%20particularly%20Generative%0AAdversarial%20Networks%20%28GANs%29%20--%20have%20achieved%20remarkable%20success%20across%20a%20wide%0Arange%20of%20domains%2C%20such%20as%20healthcare%2C%20security%2C%20and%20Image%20Generation.%20However%2C%0Atraining%20generative%20models%20typically%20requires%20large%20datasets%20and%20significant%0Acomputational%20resources%2C%20which%20are%20often%20unavailable%20in%20real-world%20settings.%0AAcquiring%20such%20resources%20can%20be%20costly%20and%20inefficient%2C%20especially%20when%20many%0Aunderutilized%20devices%20--%20such%20as%20IoT%20devices%20and%20edge%20devices%20--%20with%20varying%0Acapabilities%20remain%20idle.%20Moreover%2C%20obtaining%20large%20datasets%20is%20challenging%20due%0Ato%20privacy%20concerns%20and%20copyright%20restrictions%2C%20as%20most%20devices%20are%20unwilling%0Ato%20share%20their%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%0Afor%20decentralized%20GAN%20training%20that%20enables%20the%20utilization%20of%20distributed%20data%0Aand%20underutilized%2C%20low-capability%20devices%20while%20not%20sharing%20data%20in%20its%20raw%0Aform.%20Our%20approach%20is%20designed%20to%20tackle%20key%20challenges%20in%20decentralized%0Aenvironments%2C%20combining%20KLD-weighted%20Clustered%20Federated%20Learning%20to%20address%0Athe%20issues%20of%20data%20heterogeneity%20and%20multi-domain%20datasets%2C%20with%20Heterogeneous%0AU-Shaped%20split%20learning%20to%20tackle%20the%20challenge%20of%20device%20heterogeneity%20under%0Astrict%20data%20sharing%20constraints%20--%20ensuring%20that%20no%20labels%20or%20raw%20data%2C%20whether%0Areal%20or%20synthetic%2C%20are%20ever%20shared%20between%20nodes.%20Experimental%20results%20shows%0Athat%20our%20approach%20demonstrates%20consistent%20and%20significant%20improvements%20across%0Akey%20performance%20metrics%2C%20where%20it%20achieves%201.1x%20--%202.2x%20higher%20image%20generation%0Ascores%2C%20an%20average%2010%25%20boost%20in%20classification%20metrics%20%28up%20to%2050%25%20in%0Amulti-domain%20non-IID%20settings%29%2C%20in%20much%20lower%20latency%20compared%20to%20several%0Abenchmarks.%20Find%20our%20code%20at%20https%3A//github.com/youssefga28/HuSCF-GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12979v1&entry.124074799=Read"},
{"title": "Beyond Fully Supervised Pixel Annotations: Scribble-Driven\n  Weakly-Supervised Framework for Image Manipulation Localization", "author": "Songlin Li and Guofeng Yu and Zhiqing Guo and Yunfeng Diao and Dan Ma and Gaobo Yang and Liejun Wang", "abstract": "  Deep learning-based image manipulation localization (IML) methods have\nachieved remarkable performance in recent years, but typically rely on\nlarge-scale pixel-level annotated datasets. To address the challenge of\nacquiring high-quality annotations, some recent weakly supervised methods\nutilize image-level labels to segment manipulated regions. However, the\nperformance is still limited due to insufficient supervision signals. In this\nstudy, we explore a form of weak supervision that improves the annotation\nefficiency and detection performance, namely scribble annotation supervision.\nWe re-annotated mainstream IML datasets with scribble labels and propose the\nfirst scribble-based IML (Sc-IML) dataset. Additionally, we propose the first\nscribble-based weakly supervised IML framework. Specifically, we employ\nself-supervised training with a structural consistency loss to encourage the\nmodel to produce consistent predictions under multi-scale and augmented inputs.\nIn addition, we propose a prior-aware feature modulation module (PFMM) that\nadaptively integrates prior information from both manipulated and authentic\nregions for dynamic feature adjustment, further enhancing feature\ndiscriminability and prediction consistency in complex scenes. We also propose\na gated adaptive fusion module (GAFM) that utilizes gating mechanisms to\nregulate information flow during feature fusion, guiding the model toward\nemphasizing potential tampered regions. Finally, we propose a confidence-aware\nentropy minimization loss (${\\mathcal{L}}_{ {CEM }}$). This loss dynamically\nregularizes predictions in weakly annotated or unlabeled regions based on model\nuncertainty, effectively suppressing unreliable predictions. Experimental\nresults show that our method outperforms existing fully supervised approaches\nin terms of average performance both in-distribution and out-of-distribution.\n", "link": "http://arxiv.org/abs/2507.13018v1", "date": "2025-07-17", "relevancy": 2.2655, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5789}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Fully%20Supervised%20Pixel%20Annotations%3A%20Scribble-Driven%0A%20%20Weakly-Supervised%20Framework%20for%20Image%20Manipulation%20Localization&body=Title%3A%20Beyond%20Fully%20Supervised%20Pixel%20Annotations%3A%20Scribble-Driven%0A%20%20Weakly-Supervised%20Framework%20for%20Image%20Manipulation%20Localization%0AAuthor%3A%20Songlin%20Li%20and%20Guofeng%20Yu%20and%20Zhiqing%20Guo%20and%20Yunfeng%20Diao%20and%20Dan%20Ma%20and%20Gaobo%20Yang%20and%20Liejun%20Wang%0AAbstract%3A%20%20%20Deep%20learning-based%20image%20manipulation%20localization%20%28IML%29%20methods%20have%0Aachieved%20remarkable%20performance%20in%20recent%20years%2C%20but%20typically%20rely%20on%0Alarge-scale%20pixel-level%20annotated%20datasets.%20To%20address%20the%20challenge%20of%0Aacquiring%20high-quality%20annotations%2C%20some%20recent%20weakly%20supervised%20methods%0Autilize%20image-level%20labels%20to%20segment%20manipulated%20regions.%20However%2C%20the%0Aperformance%20is%20still%20limited%20due%20to%20insufficient%20supervision%20signals.%20In%20this%0Astudy%2C%20we%20explore%20a%20form%20of%20weak%20supervision%20that%20improves%20the%20annotation%0Aefficiency%20and%20detection%20performance%2C%20namely%20scribble%20annotation%20supervision.%0AWe%20re-annotated%20mainstream%20IML%20datasets%20with%20scribble%20labels%20and%20propose%20the%0Afirst%20scribble-based%20IML%20%28Sc-IML%29%20dataset.%20Additionally%2C%20we%20propose%20the%20first%0Ascribble-based%20weakly%20supervised%20IML%20framework.%20Specifically%2C%20we%20employ%0Aself-supervised%20training%20with%20a%20structural%20consistency%20loss%20to%20encourage%20the%0Amodel%20to%20produce%20consistent%20predictions%20under%20multi-scale%20and%20augmented%20inputs.%0AIn%20addition%2C%20we%20propose%20a%20prior-aware%20feature%20modulation%20module%20%28PFMM%29%20that%0Aadaptively%20integrates%20prior%20information%20from%20both%20manipulated%20and%20authentic%0Aregions%20for%20dynamic%20feature%20adjustment%2C%20further%20enhancing%20feature%0Adiscriminability%20and%20prediction%20consistency%20in%20complex%20scenes.%20We%20also%20propose%0Aa%20gated%20adaptive%20fusion%20module%20%28GAFM%29%20that%20utilizes%20gating%20mechanisms%20to%0Aregulate%20information%20flow%20during%20feature%20fusion%2C%20guiding%20the%20model%20toward%0Aemphasizing%20potential%20tampered%20regions.%20Finally%2C%20we%20propose%20a%20confidence-aware%0Aentropy%20minimization%20loss%20%28%24%7B%5Cmathcal%7BL%7D%7D_%7B%20%7BCEM%20%7D%7D%24%29.%20This%20loss%20dynamically%0Aregularizes%20predictions%20in%20weakly%20annotated%20or%20unlabeled%20regions%20based%20on%20model%0Auncertainty%2C%20effectively%20suppressing%20unreliable%20predictions.%20Experimental%0Aresults%20show%20that%20our%20method%20outperforms%20existing%20fully%20supervised%20approaches%0Ain%20terms%20of%20average%20performance%20both%20in-distribution%20and%20out-of-distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Fully%2520Supervised%2520Pixel%2520Annotations%253A%2520Scribble-Driven%250A%2520%2520Weakly-Supervised%2520Framework%2520for%2520Image%2520Manipulation%2520Localization%26entry.906535625%3DSonglin%2520Li%2520and%2520Guofeng%2520Yu%2520and%2520Zhiqing%2520Guo%2520and%2520Yunfeng%2520Diao%2520and%2520Dan%2520Ma%2520and%2520Gaobo%2520Yang%2520and%2520Liejun%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520image%2520manipulation%2520localization%2520%2528IML%2529%2520methods%2520have%250Aachieved%2520remarkable%2520performance%2520in%2520recent%2520years%252C%2520but%2520typically%2520rely%2520on%250Alarge-scale%2520pixel-level%2520annotated%2520datasets.%2520To%2520address%2520the%2520challenge%2520of%250Aacquiring%2520high-quality%2520annotations%252C%2520some%2520recent%2520weakly%2520supervised%2520methods%250Autilize%2520image-level%2520labels%2520to%2520segment%2520manipulated%2520regions.%2520However%252C%2520the%250Aperformance%2520is%2520still%2520limited%2520due%2520to%2520insufficient%2520supervision%2520signals.%2520In%2520this%250Astudy%252C%2520we%2520explore%2520a%2520form%2520of%2520weak%2520supervision%2520that%2520improves%2520the%2520annotation%250Aefficiency%2520and%2520detection%2520performance%252C%2520namely%2520scribble%2520annotation%2520supervision.%250AWe%2520re-annotated%2520mainstream%2520IML%2520datasets%2520with%2520scribble%2520labels%2520and%2520propose%2520the%250Afirst%2520scribble-based%2520IML%2520%2528Sc-IML%2529%2520dataset.%2520Additionally%252C%2520we%2520propose%2520the%2520first%250Ascribble-based%2520weakly%2520supervised%2520IML%2520framework.%2520Specifically%252C%2520we%2520employ%250Aself-supervised%2520training%2520with%2520a%2520structural%2520consistency%2520loss%2520to%2520encourage%2520the%250Amodel%2520to%2520produce%2520consistent%2520predictions%2520under%2520multi-scale%2520and%2520augmented%2520inputs.%250AIn%2520addition%252C%2520we%2520propose%2520a%2520prior-aware%2520feature%2520modulation%2520module%2520%2528PFMM%2529%2520that%250Aadaptively%2520integrates%2520prior%2520information%2520from%2520both%2520manipulated%2520and%2520authentic%250Aregions%2520for%2520dynamic%2520feature%2520adjustment%252C%2520further%2520enhancing%2520feature%250Adiscriminability%2520and%2520prediction%2520consistency%2520in%2520complex%2520scenes.%2520We%2520also%2520propose%250Aa%2520gated%2520adaptive%2520fusion%2520module%2520%2528GAFM%2529%2520that%2520utilizes%2520gating%2520mechanisms%2520to%250Aregulate%2520information%2520flow%2520during%2520feature%2520fusion%252C%2520guiding%2520the%2520model%2520toward%250Aemphasizing%2520potential%2520tampered%2520regions.%2520Finally%252C%2520we%2520propose%2520a%2520confidence-aware%250Aentropy%2520minimization%2520loss%2520%2528%2524%257B%255Cmathcal%257BL%257D%257D_%257B%2520%257BCEM%2520%257D%257D%2524%2529.%2520This%2520loss%2520dynamically%250Aregularizes%2520predictions%2520in%2520weakly%2520annotated%2520or%2520unlabeled%2520regions%2520based%2520on%2520model%250Auncertainty%252C%2520effectively%2520suppressing%2520unreliable%2520predictions.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520fully%2520supervised%2520approaches%250Ain%2520terms%2520of%2520average%2520performance%2520both%2520in-distribution%2520and%2520out-of-distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Fully%20Supervised%20Pixel%20Annotations%3A%20Scribble-Driven%0A%20%20Weakly-Supervised%20Framework%20for%20Image%20Manipulation%20Localization&entry.906535625=Songlin%20Li%20and%20Guofeng%20Yu%20and%20Zhiqing%20Guo%20and%20Yunfeng%20Diao%20and%20Dan%20Ma%20and%20Gaobo%20Yang%20and%20Liejun%20Wang&entry.1292438233=%20%20Deep%20learning-based%20image%20manipulation%20localization%20%28IML%29%20methods%20have%0Aachieved%20remarkable%20performance%20in%20recent%20years%2C%20but%20typically%20rely%20on%0Alarge-scale%20pixel-level%20annotated%20datasets.%20To%20address%20the%20challenge%20of%0Aacquiring%20high-quality%20annotations%2C%20some%20recent%20weakly%20supervised%20methods%0Autilize%20image-level%20labels%20to%20segment%20manipulated%20regions.%20However%2C%20the%0Aperformance%20is%20still%20limited%20due%20to%20insufficient%20supervision%20signals.%20In%20this%0Astudy%2C%20we%20explore%20a%20form%20of%20weak%20supervision%20that%20improves%20the%20annotation%0Aefficiency%20and%20detection%20performance%2C%20namely%20scribble%20annotation%20supervision.%0AWe%20re-annotated%20mainstream%20IML%20datasets%20with%20scribble%20labels%20and%20propose%20the%0Afirst%20scribble-based%20IML%20%28Sc-IML%29%20dataset.%20Additionally%2C%20we%20propose%20the%20first%0Ascribble-based%20weakly%20supervised%20IML%20framework.%20Specifically%2C%20we%20employ%0Aself-supervised%20training%20with%20a%20structural%20consistency%20loss%20to%20encourage%20the%0Amodel%20to%20produce%20consistent%20predictions%20under%20multi-scale%20and%20augmented%20inputs.%0AIn%20addition%2C%20we%20propose%20a%20prior-aware%20feature%20modulation%20module%20%28PFMM%29%20that%0Aadaptively%20integrates%20prior%20information%20from%20both%20manipulated%20and%20authentic%0Aregions%20for%20dynamic%20feature%20adjustment%2C%20further%20enhancing%20feature%0Adiscriminability%20and%20prediction%20consistency%20in%20complex%20scenes.%20We%20also%20propose%0Aa%20gated%20adaptive%20fusion%20module%20%28GAFM%29%20that%20utilizes%20gating%20mechanisms%20to%0Aregulate%20information%20flow%20during%20feature%20fusion%2C%20guiding%20the%20model%20toward%0Aemphasizing%20potential%20tampered%20regions.%20Finally%2C%20we%20propose%20a%20confidence-aware%0Aentropy%20minimization%20loss%20%28%24%7B%5Cmathcal%7BL%7D%7D_%7B%20%7BCEM%20%7D%7D%24%29.%20This%20loss%20dynamically%0Aregularizes%20predictions%20in%20weakly%20annotated%20or%20unlabeled%20regions%20based%20on%20model%0Auncertainty%2C%20effectively%20suppressing%20unreliable%20predictions.%20Experimental%0Aresults%20show%20that%20our%20method%20outperforms%20existing%20fully%20supervised%20approaches%0Ain%20terms%20of%20average%20performance%20both%20in-distribution%20and%20out-of-distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13018v1&entry.124074799=Read"},
{"title": "fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting", "author": "Alicia Durrer and Florentin Bieder and Paul Friedrich and Bjoern Menze and Philippe C. Cattin and Florian Kofler", "abstract": "  Healthy tissue inpainting has significant applications, including the\ngeneration of pseudo-healthy baselines for tumor growth models and the\nfacilitation of image registration. In previous editions of the BraTS Local\nSynthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion\nprobabilistic models (DDPMs) demonstrated qualitatively convincing results but\nsuffered from low sampling speed. To mitigate this limitation, we adapted a 2D\nimage generation approach, combining DDPMs with generative adversarial networks\n(GANs) and employing a variance-preserving noise schedule, for the task of 3D\ninpainting. Our experiments showed that the variance-preserving noise schedule\nand the selected reconstruction losses can be effectively utilized for\nhigh-quality 3D inpainting in a few time steps without requiring adversarial\ntraining. We applied our findings to a different architecture, a 3D wavelet\ndiffusion model (WDM3D) that does not include a GAN component. The resulting\nmodel, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a\nPSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these\nscores using only two time steps, completing the 3D inpainting process in 1.81\ns per image. When compared to other DDPMs used for healthy brain tissue\ninpainting, our model is up to 800 x faster while still achieving superior\nperformance metrics. Our proposed method, fastWDM3D, represents a promising\napproach for fast and accurate healthy tissue inpainting. Our code is available\nat https://github.com/AliciaDurrer/fastWDM3D.\n", "link": "http://arxiv.org/abs/2507.13146v1", "date": "2025-07-17", "relevancy": 2.2601, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5778}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5627}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20fastWDM3D%3A%20Fast%20and%20Accurate%203D%20Healthy%20Tissue%20Inpainting&body=Title%3A%20fastWDM3D%3A%20Fast%20and%20Accurate%203D%20Healthy%20Tissue%20Inpainting%0AAuthor%3A%20Alicia%20Durrer%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Bjoern%20Menze%20and%20Philippe%20C.%20Cattin%20and%20Florian%20Kofler%0AAbstract%3A%20%20%20Healthy%20tissue%20inpainting%20has%20significant%20applications%2C%20including%20the%0Ageneration%20of%20pseudo-healthy%20baselines%20for%20tumor%20growth%20models%20and%20the%0Afacilitation%20of%20image%20registration.%20In%20previous%20editions%20of%20the%20BraTS%20Local%0ASynthesis%20of%20Healthy%20Brain%20Tissue%20via%20Inpainting%20Challenge%2C%20denoising%20diffusion%0Aprobabilistic%20models%20%28DDPMs%29%20demonstrated%20qualitatively%20convincing%20results%20but%0Asuffered%20from%20low%20sampling%20speed.%20To%20mitigate%20this%20limitation%2C%20we%20adapted%20a%202D%0Aimage%20generation%20approach%2C%20combining%20DDPMs%20with%20generative%20adversarial%20networks%0A%28GANs%29%20and%20employing%20a%20variance-preserving%20noise%20schedule%2C%20for%20the%20task%20of%203D%0Ainpainting.%20Our%20experiments%20showed%20that%20the%20variance-preserving%20noise%20schedule%0Aand%20the%20selected%20reconstruction%20losses%20can%20be%20effectively%20utilized%20for%0Ahigh-quality%203D%20inpainting%20in%20a%20few%20time%20steps%20without%20requiring%20adversarial%0Atraining.%20We%20applied%20our%20findings%20to%20a%20different%20architecture%2C%20a%203D%20wavelet%0Adiffusion%20model%20%28WDM3D%29%20that%20does%20not%20include%20a%20GAN%20component.%20The%20resulting%0Amodel%2C%20denoted%20as%20fastWDM3D%2C%20obtained%20a%20SSIM%20of%200.8571%2C%20a%20MSE%20of%200.0079%2C%20and%20a%0APSNR%20of%2022.26%20on%20the%20BraTS%20inpainting%20test%20set.%20Remarkably%2C%20it%20achieved%20these%0Ascores%20using%20only%20two%20time%20steps%2C%20completing%20the%203D%20inpainting%20process%20in%201.81%0As%20per%20image.%20When%20compared%20to%20other%20DDPMs%20used%20for%20healthy%20brain%20tissue%0Ainpainting%2C%20our%20model%20is%20up%20to%20800%20x%20faster%20while%20still%20achieving%20superior%0Aperformance%20metrics.%20Our%20proposed%20method%2C%20fastWDM3D%2C%20represents%20a%20promising%0Aapproach%20for%20fast%20and%20accurate%20healthy%20tissue%20inpainting.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/AliciaDurrer/fastWDM3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DfastWDM3D%253A%2520Fast%2520and%2520Accurate%25203D%2520Healthy%2520Tissue%2520Inpainting%26entry.906535625%3DAlicia%2520Durrer%2520and%2520Florentin%2520Bieder%2520and%2520Paul%2520Friedrich%2520and%2520Bjoern%2520Menze%2520and%2520Philippe%2520C.%2520Cattin%2520and%2520Florian%2520Kofler%26entry.1292438233%3D%2520%2520Healthy%2520tissue%2520inpainting%2520has%2520significant%2520applications%252C%2520including%2520the%250Ageneration%2520of%2520pseudo-healthy%2520baselines%2520for%2520tumor%2520growth%2520models%2520and%2520the%250Afacilitation%2520of%2520image%2520registration.%2520In%2520previous%2520editions%2520of%2520the%2520BraTS%2520Local%250ASynthesis%2520of%2520Healthy%2520Brain%2520Tissue%2520via%2520Inpainting%2520Challenge%252C%2520denoising%2520diffusion%250Aprobabilistic%2520models%2520%2528DDPMs%2529%2520demonstrated%2520qualitatively%2520convincing%2520results%2520but%250Asuffered%2520from%2520low%2520sampling%2520speed.%2520To%2520mitigate%2520this%2520limitation%252C%2520we%2520adapted%2520a%25202D%250Aimage%2520generation%2520approach%252C%2520combining%2520DDPMs%2520with%2520generative%2520adversarial%2520networks%250A%2528GANs%2529%2520and%2520employing%2520a%2520variance-preserving%2520noise%2520schedule%252C%2520for%2520the%2520task%2520of%25203D%250Ainpainting.%2520Our%2520experiments%2520showed%2520that%2520the%2520variance-preserving%2520noise%2520schedule%250Aand%2520the%2520selected%2520reconstruction%2520losses%2520can%2520be%2520effectively%2520utilized%2520for%250Ahigh-quality%25203D%2520inpainting%2520in%2520a%2520few%2520time%2520steps%2520without%2520requiring%2520adversarial%250Atraining.%2520We%2520applied%2520our%2520findings%2520to%2520a%2520different%2520architecture%252C%2520a%25203D%2520wavelet%250Adiffusion%2520model%2520%2528WDM3D%2529%2520that%2520does%2520not%2520include%2520a%2520GAN%2520component.%2520The%2520resulting%250Amodel%252C%2520denoted%2520as%2520fastWDM3D%252C%2520obtained%2520a%2520SSIM%2520of%25200.8571%252C%2520a%2520MSE%2520of%25200.0079%252C%2520and%2520a%250APSNR%2520of%252022.26%2520on%2520the%2520BraTS%2520inpainting%2520test%2520set.%2520Remarkably%252C%2520it%2520achieved%2520these%250Ascores%2520using%2520only%2520two%2520time%2520steps%252C%2520completing%2520the%25203D%2520inpainting%2520process%2520in%25201.81%250As%2520per%2520image.%2520When%2520compared%2520to%2520other%2520DDPMs%2520used%2520for%2520healthy%2520brain%2520tissue%250Ainpainting%252C%2520our%2520model%2520is%2520up%2520to%2520800%2520x%2520faster%2520while%2520still%2520achieving%2520superior%250Aperformance%2520metrics.%2520Our%2520proposed%2520method%252C%2520fastWDM3D%252C%2520represents%2520a%2520promising%250Aapproach%2520for%2520fast%2520and%2520accurate%2520healthy%2520tissue%2520inpainting.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/AliciaDurrer/fastWDM3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=fastWDM3D%3A%20Fast%20and%20Accurate%203D%20Healthy%20Tissue%20Inpainting&entry.906535625=Alicia%20Durrer%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Bjoern%20Menze%20and%20Philippe%20C.%20Cattin%20and%20Florian%20Kofler&entry.1292438233=%20%20Healthy%20tissue%20inpainting%20has%20significant%20applications%2C%20including%20the%0Ageneration%20of%20pseudo-healthy%20baselines%20for%20tumor%20growth%20models%20and%20the%0Afacilitation%20of%20image%20registration.%20In%20previous%20editions%20of%20the%20BraTS%20Local%0ASynthesis%20of%20Healthy%20Brain%20Tissue%20via%20Inpainting%20Challenge%2C%20denoising%20diffusion%0Aprobabilistic%20models%20%28DDPMs%29%20demonstrated%20qualitatively%20convincing%20results%20but%0Asuffered%20from%20low%20sampling%20speed.%20To%20mitigate%20this%20limitation%2C%20we%20adapted%20a%202D%0Aimage%20generation%20approach%2C%20combining%20DDPMs%20with%20generative%20adversarial%20networks%0A%28GANs%29%20and%20employing%20a%20variance-preserving%20noise%20schedule%2C%20for%20the%20task%20of%203D%0Ainpainting.%20Our%20experiments%20showed%20that%20the%20variance-preserving%20noise%20schedule%0Aand%20the%20selected%20reconstruction%20losses%20can%20be%20effectively%20utilized%20for%0Ahigh-quality%203D%20inpainting%20in%20a%20few%20time%20steps%20without%20requiring%20adversarial%0Atraining.%20We%20applied%20our%20findings%20to%20a%20different%20architecture%2C%20a%203D%20wavelet%0Adiffusion%20model%20%28WDM3D%29%20that%20does%20not%20include%20a%20GAN%20component.%20The%20resulting%0Amodel%2C%20denoted%20as%20fastWDM3D%2C%20obtained%20a%20SSIM%20of%200.8571%2C%20a%20MSE%20of%200.0079%2C%20and%20a%0APSNR%20of%2022.26%20on%20the%20BraTS%20inpainting%20test%20set.%20Remarkably%2C%20it%20achieved%20these%0Ascores%20using%20only%20two%20time%20steps%2C%20completing%20the%203D%20inpainting%20process%20in%201.81%0As%20per%20image.%20When%20compared%20to%20other%20DDPMs%20used%20for%20healthy%20brain%20tissue%0Ainpainting%2C%20our%20model%20is%20up%20to%20800%20x%20faster%20while%20still%20achieving%20superior%0Aperformance%20metrics.%20Our%20proposed%20method%2C%20fastWDM3D%2C%20represents%20a%20promising%0Aapproach%20for%20fast%20and%20accurate%20healthy%20tissue%20inpainting.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/AliciaDurrer/fastWDM3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13146v1&entry.124074799=Read"},
{"title": "Decoupled PROB: Decoupled Query Initialization Tasks and\n  Objectness-Class Learning for Open World Object Detection", "author": "Riku Inoue and Masamitsu Tsuchiya and Yuji Yasui", "abstract": "  Open World Object Detection (OWOD) is a challenging computer vision task that\nextends standard object detection by (1) detecting and classifying unknown\nobjects without supervision, and (2) incrementally learning new object classes\nwithout forgetting previously learned ones. The absence of ground truths for\nunknown objects makes OWOD tasks particularly challenging. Many methods have\naddressed this by using pseudo-labels for unknown objects. The recently\nproposed Probabilistic Objectness transformer-based open-world detector (PROB)\nis a state-of-the-art model that does not require pseudo-labels for unknown\nobjects, as it predicts probabilistic objectness. However, this method faces\nissues with learning conflicts between objectness and class predictions.\n  To address this issue and further enhance performance, we propose a novel\nmodel, Decoupled PROB. Decoupled PROB introduces Early Termination of\nObjectness Prediction (ETOP) to stop objectness predictions at appropriate\nlayers in the decoder, resolving the learning conflicts between class and\nobjectness predictions in PROB. Additionally, we introduce Task-Decoupled Query\nInitialization (TDQI), which efficiently extracts features of known and unknown\nobjects, thereby improving performance. TDQI is a query initialization method\nthat combines query selection and learnable queries, and it is a module that\ncan be easily integrated into existing DETR-based OWOD models. Extensive\nexperiments on OWOD benchmarks demonstrate that Decoupled PROB surpasses all\nexisting methods across several metrics, significantly improving performance.\n", "link": "http://arxiv.org/abs/2507.13085v1", "date": "2025-07-17", "relevancy": 2.2572, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20PROB%3A%20Decoupled%20Query%20Initialization%20Tasks%20and%0A%20%20Objectness-Class%20Learning%20for%20Open%20World%20Object%20Detection&body=Title%3A%20Decoupled%20PROB%3A%20Decoupled%20Query%20Initialization%20Tasks%20and%0A%20%20Objectness-Class%20Learning%20for%20Open%20World%20Object%20Detection%0AAuthor%3A%20Riku%20Inoue%20and%20Masamitsu%20Tsuchiya%20and%20Yuji%20Yasui%0AAbstract%3A%20%20%20Open%20World%20Object%20Detection%20%28OWOD%29%20is%20a%20challenging%20computer%20vision%20task%20that%0Aextends%20standard%20object%20detection%20by%20%281%29%20detecting%20and%20classifying%20unknown%0Aobjects%20without%20supervision%2C%20and%20%282%29%20incrementally%20learning%20new%20object%20classes%0Awithout%20forgetting%20previously%20learned%20ones.%20The%20absence%20of%20ground%20truths%20for%0Aunknown%20objects%20makes%20OWOD%20tasks%20particularly%20challenging.%20Many%20methods%20have%0Aaddressed%20this%20by%20using%20pseudo-labels%20for%20unknown%20objects.%20The%20recently%0Aproposed%20Probabilistic%20Objectness%20transformer-based%20open-world%20detector%20%28PROB%29%0Ais%20a%20state-of-the-art%20model%20that%20does%20not%20require%20pseudo-labels%20for%20unknown%0Aobjects%2C%20as%20it%20predicts%20probabilistic%20objectness.%20However%2C%20this%20method%20faces%0Aissues%20with%20learning%20conflicts%20between%20objectness%20and%20class%20predictions.%0A%20%20To%20address%20this%20issue%20and%20further%20enhance%20performance%2C%20we%20propose%20a%20novel%0Amodel%2C%20Decoupled%20PROB.%20Decoupled%20PROB%20introduces%20Early%20Termination%20of%0AObjectness%20Prediction%20%28ETOP%29%20to%20stop%20objectness%20predictions%20at%20appropriate%0Alayers%20in%20the%20decoder%2C%20resolving%20the%20learning%20conflicts%20between%20class%20and%0Aobjectness%20predictions%20in%20PROB.%20Additionally%2C%20we%20introduce%20Task-Decoupled%20Query%0AInitialization%20%28TDQI%29%2C%20which%20efficiently%20extracts%20features%20of%20known%20and%20unknown%0Aobjects%2C%20thereby%20improving%20performance.%20TDQI%20is%20a%20query%20initialization%20method%0Athat%20combines%20query%20selection%20and%20learnable%20queries%2C%20and%20it%20is%20a%20module%20that%0Acan%20be%20easily%20integrated%20into%20existing%20DETR-based%20OWOD%20models.%20Extensive%0Aexperiments%20on%20OWOD%20benchmarks%20demonstrate%20that%20Decoupled%20PROB%20surpasses%20all%0Aexisting%20methods%20across%20several%20metrics%2C%20significantly%20improving%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520PROB%253A%2520Decoupled%2520Query%2520Initialization%2520Tasks%2520and%250A%2520%2520Objectness-Class%2520Learning%2520for%2520Open%2520World%2520Object%2520Detection%26entry.906535625%3DRiku%2520Inoue%2520and%2520Masamitsu%2520Tsuchiya%2520and%2520Yuji%2520Yasui%26entry.1292438233%3D%2520%2520Open%2520World%2520Object%2520Detection%2520%2528OWOD%2529%2520is%2520a%2520challenging%2520computer%2520vision%2520task%2520that%250Aextends%2520standard%2520object%2520detection%2520by%2520%25281%2529%2520detecting%2520and%2520classifying%2520unknown%250Aobjects%2520without%2520supervision%252C%2520and%2520%25282%2529%2520incrementally%2520learning%2520new%2520object%2520classes%250Awithout%2520forgetting%2520previously%2520learned%2520ones.%2520The%2520absence%2520of%2520ground%2520truths%2520for%250Aunknown%2520objects%2520makes%2520OWOD%2520tasks%2520particularly%2520challenging.%2520Many%2520methods%2520have%250Aaddressed%2520this%2520by%2520using%2520pseudo-labels%2520for%2520unknown%2520objects.%2520The%2520recently%250Aproposed%2520Probabilistic%2520Objectness%2520transformer-based%2520open-world%2520detector%2520%2528PROB%2529%250Ais%2520a%2520state-of-the-art%2520model%2520that%2520does%2520not%2520require%2520pseudo-labels%2520for%2520unknown%250Aobjects%252C%2520as%2520it%2520predicts%2520probabilistic%2520objectness.%2520However%252C%2520this%2520method%2520faces%250Aissues%2520with%2520learning%2520conflicts%2520between%2520objectness%2520and%2520class%2520predictions.%250A%2520%2520To%2520address%2520this%2520issue%2520and%2520further%2520enhance%2520performance%252C%2520we%2520propose%2520a%2520novel%250Amodel%252C%2520Decoupled%2520PROB.%2520Decoupled%2520PROB%2520introduces%2520Early%2520Termination%2520of%250AObjectness%2520Prediction%2520%2528ETOP%2529%2520to%2520stop%2520objectness%2520predictions%2520at%2520appropriate%250Alayers%2520in%2520the%2520decoder%252C%2520resolving%2520the%2520learning%2520conflicts%2520between%2520class%2520and%250Aobjectness%2520predictions%2520in%2520PROB.%2520Additionally%252C%2520we%2520introduce%2520Task-Decoupled%2520Query%250AInitialization%2520%2528TDQI%2529%252C%2520which%2520efficiently%2520extracts%2520features%2520of%2520known%2520and%2520unknown%250Aobjects%252C%2520thereby%2520improving%2520performance.%2520TDQI%2520is%2520a%2520query%2520initialization%2520method%250Athat%2520combines%2520query%2520selection%2520and%2520learnable%2520queries%252C%2520and%2520it%2520is%2520a%2520module%2520that%250Acan%2520be%2520easily%2520integrated%2520into%2520existing%2520DETR-based%2520OWOD%2520models.%2520Extensive%250Aexperiments%2520on%2520OWOD%2520benchmarks%2520demonstrate%2520that%2520Decoupled%2520PROB%2520surpasses%2520all%250Aexisting%2520methods%2520across%2520several%2520metrics%252C%2520significantly%2520improving%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20PROB%3A%20Decoupled%20Query%20Initialization%20Tasks%20and%0A%20%20Objectness-Class%20Learning%20for%20Open%20World%20Object%20Detection&entry.906535625=Riku%20Inoue%20and%20Masamitsu%20Tsuchiya%20and%20Yuji%20Yasui&entry.1292438233=%20%20Open%20World%20Object%20Detection%20%28OWOD%29%20is%20a%20challenging%20computer%20vision%20task%20that%0Aextends%20standard%20object%20detection%20by%20%281%29%20detecting%20and%20classifying%20unknown%0Aobjects%20without%20supervision%2C%20and%20%282%29%20incrementally%20learning%20new%20object%20classes%0Awithout%20forgetting%20previously%20learned%20ones.%20The%20absence%20of%20ground%20truths%20for%0Aunknown%20objects%20makes%20OWOD%20tasks%20particularly%20challenging.%20Many%20methods%20have%0Aaddressed%20this%20by%20using%20pseudo-labels%20for%20unknown%20objects.%20The%20recently%0Aproposed%20Probabilistic%20Objectness%20transformer-based%20open-world%20detector%20%28PROB%29%0Ais%20a%20state-of-the-art%20model%20that%20does%20not%20require%20pseudo-labels%20for%20unknown%0Aobjects%2C%20as%20it%20predicts%20probabilistic%20objectness.%20However%2C%20this%20method%20faces%0Aissues%20with%20learning%20conflicts%20between%20objectness%20and%20class%20predictions.%0A%20%20To%20address%20this%20issue%20and%20further%20enhance%20performance%2C%20we%20propose%20a%20novel%0Amodel%2C%20Decoupled%20PROB.%20Decoupled%20PROB%20introduces%20Early%20Termination%20of%0AObjectness%20Prediction%20%28ETOP%29%20to%20stop%20objectness%20predictions%20at%20appropriate%0Alayers%20in%20the%20decoder%2C%20resolving%20the%20learning%20conflicts%20between%20class%20and%0Aobjectness%20predictions%20in%20PROB.%20Additionally%2C%20we%20introduce%20Task-Decoupled%20Query%0AInitialization%20%28TDQI%29%2C%20which%20efficiently%20extracts%20features%20of%20known%20and%20unknown%0Aobjects%2C%20thereby%20improving%20performance.%20TDQI%20is%20a%20query%20initialization%20method%0Athat%20combines%20query%20selection%20and%20learnable%20queries%2C%20and%20it%20is%20a%20module%20that%0Acan%20be%20easily%20integrated%20into%20existing%20DETR-based%20OWOD%20models.%20Extensive%0Aexperiments%20on%20OWOD%20benchmarks%20demonstrate%20that%20Decoupled%20PROB%20surpasses%20all%0Aexisting%20methods%20across%20several%20metrics%2C%20significantly%20improving%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13085v1&entry.124074799=Read"},
{"title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos", "author": "Ruihan Yang and Qinxi Yu and Yecheng Wu and Rui Yan and Borui Li and An-Chieh Cheng and Xueyan Zou and Yunhao Fang and Hongxu Yin and Sifei Liu and Song Han and Yao Lu and Xiaolong Wang", "abstract": "  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n", "link": "http://arxiv.org/abs/2507.12440v2", "date": "2025-07-17", "relevancy": 2.2559, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5846}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5641}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos&body=Title%3A%20EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos%0AAuthor%3A%20Ruihan%20Yang%20and%20Qinxi%20Yu%20and%20Yecheng%20Wu%20and%20Rui%20Yan%20and%20Borui%20Li%20and%20An-Chieh%20Cheng%20and%20Xueyan%20Zou%20and%20Yunhao%20Fang%20and%20Hongxu%20Yin%20and%20Sifei%20Liu%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Real%20robot%20data%20collection%20for%20imitation%20learning%20has%20led%20to%20significant%0Aadvancements%20in%20robotic%20manipulation.%20However%2C%20the%20requirement%20for%20robot%0Ahardware%20in%20the%20process%20fundamentally%20constrains%20the%20scale%20of%20the%20data.%20In%20this%0Apaper%2C%20we%20explore%20training%20Vision-Language-Action%20%28VLA%29%20models%20using%20egocentric%0Ahuman%20videos.%20The%20benefit%20of%20using%20human%20videos%20is%20not%20only%20for%20their%20scale%20but%0Amore%20importantly%20for%20the%20richness%20of%20scenes%20and%20tasks.%20With%20a%20VLA%20trained%20on%0Ahuman%20video%20that%20predicts%20human%20wrist%20and%20hand%20actions%2C%20we%20can%20perform%20Inverse%0AKinematics%20and%20retargeting%20to%20convert%20the%20human%20actions%20to%20robot%20actions.%20We%0Afine-tune%20the%20model%20using%20a%20few%20robot%20manipulation%20demonstrations%20to%20obtain%20the%0Arobot%20policy%2C%20namely%20EgoVLA.%20We%20propose%20a%20simulation%20benchmark%20called%20Ego%0AHumanoid%20Manipulation%20Benchmark%2C%20where%20we%20design%20diverse%20bimanual%20manipulation%0Atasks%20with%20demonstrations.%20We%20fine-tune%20and%20evaluate%20EgoVLA%20with%20Ego%20Humanoid%0AManipulation%20Benchmark%20and%20show%20significant%20improvements%20over%20baselines%20and%0Aablate%20the%20importance%20of%20human%20data.%20Videos%20can%20be%20found%20on%20our%20website%3A%0Ahttps%3A//rchalyang.github.io/EgoVLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoVLA%253A%2520Learning%2520Vision-Language-Action%2520Models%2520from%2520Egocentric%2520Human%250A%2520%2520Videos%26entry.906535625%3DRuihan%2520Yang%2520and%2520Qinxi%2520Yu%2520and%2520Yecheng%2520Wu%2520and%2520Rui%2520Yan%2520and%2520Borui%2520Li%2520and%2520An-Chieh%2520Cheng%2520and%2520Xueyan%2520Zou%2520and%2520Yunhao%2520Fang%2520and%2520Hongxu%2520Yin%2520and%2520Sifei%2520Liu%2520and%2520Song%2520Han%2520and%2520Yao%2520Lu%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Real%2520robot%2520data%2520collection%2520for%2520imitation%2520learning%2520has%2520led%2520to%2520significant%250Aadvancements%2520in%2520robotic%2520manipulation.%2520However%252C%2520the%2520requirement%2520for%2520robot%250Ahardware%2520in%2520the%2520process%2520fundamentally%2520constrains%2520the%2520scale%2520of%2520the%2520data.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520training%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520using%2520egocentric%250Ahuman%2520videos.%2520The%2520benefit%2520of%2520using%2520human%2520videos%2520is%2520not%2520only%2520for%2520their%2520scale%2520but%250Amore%2520importantly%2520for%2520the%2520richness%2520of%2520scenes%2520and%2520tasks.%2520With%2520a%2520VLA%2520trained%2520on%250Ahuman%2520video%2520that%2520predicts%2520human%2520wrist%2520and%2520hand%2520actions%252C%2520we%2520can%2520perform%2520Inverse%250AKinematics%2520and%2520retargeting%2520to%2520convert%2520the%2520human%2520actions%2520to%2520robot%2520actions.%2520We%250Afine-tune%2520the%2520model%2520using%2520a%2520few%2520robot%2520manipulation%2520demonstrations%2520to%2520obtain%2520the%250Arobot%2520policy%252C%2520namely%2520EgoVLA.%2520We%2520propose%2520a%2520simulation%2520benchmark%2520called%2520Ego%250AHumanoid%2520Manipulation%2520Benchmark%252C%2520where%2520we%2520design%2520diverse%2520bimanual%2520manipulation%250Atasks%2520with%2520demonstrations.%2520We%2520fine-tune%2520and%2520evaluate%2520EgoVLA%2520with%2520Ego%2520Humanoid%250AManipulation%2520Benchmark%2520and%2520show%2520significant%2520improvements%2520over%2520baselines%2520and%250Aablate%2520the%2520importance%2520of%2520human%2520data.%2520Videos%2520can%2520be%2520found%2520on%2520our%2520website%253A%250Ahttps%253A//rchalyang.github.io/EgoVLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos&entry.906535625=Ruihan%20Yang%20and%20Qinxi%20Yu%20and%20Yecheng%20Wu%20and%20Rui%20Yan%20and%20Borui%20Li%20and%20An-Chieh%20Cheng%20and%20Xueyan%20Zou%20and%20Yunhao%20Fang%20and%20Hongxu%20Yin%20and%20Sifei%20Liu%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Real%20robot%20data%20collection%20for%20imitation%20learning%20has%20led%20to%20significant%0Aadvancements%20in%20robotic%20manipulation.%20However%2C%20the%20requirement%20for%20robot%0Ahardware%20in%20the%20process%20fundamentally%20constrains%20the%20scale%20of%20the%20data.%20In%20this%0Apaper%2C%20we%20explore%20training%20Vision-Language-Action%20%28VLA%29%20models%20using%20egocentric%0Ahuman%20videos.%20The%20benefit%20of%20using%20human%20videos%20is%20not%20only%20for%20their%20scale%20but%0Amore%20importantly%20for%20the%20richness%20of%20scenes%20and%20tasks.%20With%20a%20VLA%20trained%20on%0Ahuman%20video%20that%20predicts%20human%20wrist%20and%20hand%20actions%2C%20we%20can%20perform%20Inverse%0AKinematics%20and%20retargeting%20to%20convert%20the%20human%20actions%20to%20robot%20actions.%20We%0Afine-tune%20the%20model%20using%20a%20few%20robot%20manipulation%20demonstrations%20to%20obtain%20the%0Arobot%20policy%2C%20namely%20EgoVLA.%20We%20propose%20a%20simulation%20benchmark%20called%20Ego%0AHumanoid%20Manipulation%20Benchmark%2C%20where%20we%20design%20diverse%20bimanual%20manipulation%0Atasks%20with%20demonstrations.%20We%20fine-tune%20and%20evaluate%20EgoVLA%20with%20Ego%20Humanoid%0AManipulation%20Benchmark%20and%20show%20significant%20improvements%20over%20baselines%20and%0Aablate%20the%20importance%20of%20human%20data.%20Videos%20can%20be%20found%20on%20our%20website%3A%0Ahttps%3A//rchalyang.github.io/EgoVLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12440v2&entry.124074799=Read"},
{"title": "Salvaging the Overlooked: Leveraging Class-Aware Contrastive Learning\n  for Multi-Class Anomaly Detection", "author": "Lei Fan and Junjie Huang and Donglin Di and Anyang Su and Tianyou Song and Maurice Pagnucco and Yang Song", "abstract": "  For anomaly detection (AD), early approaches often train separate models for\nindividual classes, yielding high performance but posing challenges in\nscalability and resource management. Recent efforts have shifted toward\ntraining a single model capable of handling multiple classes. However, directly\nextending early AD methods to multi-class settings often results in degraded\nperformance. In this paper, we investigate this performance degradation\nobserved in reconstruction-based methods, identifying the key issue:\ninter-class confusion. This confusion emerges when a model trained in\nmulti-class scenarios incorrectly reconstructs samples from one class as those\nof another, thereby exacerbating reconstruction errors. To this end, we propose\na simple yet effective modification, called class-aware contrastive learning\n(CCL). By explicitly leveraging raw object category information (\\eg carpet or\nwood) as supervised signals, we introduce local CL to refine multiscale dense\nfeatures, and global CL to obtain more compact feature representations of\nnormal patterns, thereby effectively adapting the models to multi-class\nsettings. Experiments across five datasets validate the effectiveness of our\napproach, demonstrating significant improvements and superior performance\ncompared to state-of-the-art methods. Notably, ablation studies indicate that\npseudo-class labels can achieve comparable performance.\n", "link": "http://arxiv.org/abs/2412.04769v2", "date": "2025-07-17", "relevancy": 2.2514, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Salvaging%20the%20Overlooked%3A%20Leveraging%20Class-Aware%20Contrastive%20Learning%0A%20%20for%20Multi-Class%20Anomaly%20Detection&body=Title%3A%20Salvaging%20the%20Overlooked%3A%20Leveraging%20Class-Aware%20Contrastive%20Learning%0A%20%20for%20Multi-Class%20Anomaly%20Detection%0AAuthor%3A%20Lei%20Fan%20and%20Junjie%20Huang%20and%20Donglin%20Di%20and%20Anyang%20Su%20and%20Tianyou%20Song%20and%20Maurice%20Pagnucco%20and%20Yang%20Song%0AAbstract%3A%20%20%20For%20anomaly%20detection%20%28AD%29%2C%20early%20approaches%20often%20train%20separate%20models%20for%0Aindividual%20classes%2C%20yielding%20high%20performance%20but%20posing%20challenges%20in%0Ascalability%20and%20resource%20management.%20Recent%20efforts%20have%20shifted%20toward%0Atraining%20a%20single%20model%20capable%20of%20handling%20multiple%20classes.%20However%2C%20directly%0Aextending%20early%20AD%20methods%20to%20multi-class%20settings%20often%20results%20in%20degraded%0Aperformance.%20In%20this%20paper%2C%20we%20investigate%20this%20performance%20degradation%0Aobserved%20in%20reconstruction-based%20methods%2C%20identifying%20the%20key%20issue%3A%0Ainter-class%20confusion.%20This%20confusion%20emerges%20when%20a%20model%20trained%20in%0Amulti-class%20scenarios%20incorrectly%20reconstructs%20samples%20from%20one%20class%20as%20those%0Aof%20another%2C%20thereby%20exacerbating%20reconstruction%20errors.%20To%20this%20end%2C%20we%20propose%0Aa%20simple%20yet%20effective%20modification%2C%20called%20class-aware%20contrastive%20learning%0A%28CCL%29.%20By%20explicitly%20leveraging%20raw%20object%20category%20information%20%28%5Ceg%20carpet%20or%0Awood%29%20as%20supervised%20signals%2C%20we%20introduce%20local%20CL%20to%20refine%20multiscale%20dense%0Afeatures%2C%20and%20global%20CL%20to%20obtain%20more%20compact%20feature%20representations%20of%0Anormal%20patterns%2C%20thereby%20effectively%20adapting%20the%20models%20to%20multi-class%0Asettings.%20Experiments%20across%20five%20datasets%20validate%20the%20effectiveness%20of%20our%0Aapproach%2C%20demonstrating%20significant%20improvements%20and%20superior%20performance%0Acompared%20to%20state-of-the-art%20methods.%20Notably%2C%20ablation%20studies%20indicate%20that%0Apseudo-class%20labels%20can%20achieve%20comparable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSalvaging%2520the%2520Overlooked%253A%2520Leveraging%2520Class-Aware%2520Contrastive%2520Learning%250A%2520%2520for%2520Multi-Class%2520Anomaly%2520Detection%26entry.906535625%3DLei%2520Fan%2520and%2520Junjie%2520Huang%2520and%2520Donglin%2520Di%2520and%2520Anyang%2520Su%2520and%2520Tianyou%2520Song%2520and%2520Maurice%2520Pagnucco%2520and%2520Yang%2520Song%26entry.1292438233%3D%2520%2520For%2520anomaly%2520detection%2520%2528AD%2529%252C%2520early%2520approaches%2520often%2520train%2520separate%2520models%2520for%250Aindividual%2520classes%252C%2520yielding%2520high%2520performance%2520but%2520posing%2520challenges%2520in%250Ascalability%2520and%2520resource%2520management.%2520Recent%2520efforts%2520have%2520shifted%2520toward%250Atraining%2520a%2520single%2520model%2520capable%2520of%2520handling%2520multiple%2520classes.%2520However%252C%2520directly%250Aextending%2520early%2520AD%2520methods%2520to%2520multi-class%2520settings%2520often%2520results%2520in%2520degraded%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520performance%2520degradation%250Aobserved%2520in%2520reconstruction-based%2520methods%252C%2520identifying%2520the%2520key%2520issue%253A%250Ainter-class%2520confusion.%2520This%2520confusion%2520emerges%2520when%2520a%2520model%2520trained%2520in%250Amulti-class%2520scenarios%2520incorrectly%2520reconstructs%2520samples%2520from%2520one%2520class%2520as%2520those%250Aof%2520another%252C%2520thereby%2520exacerbating%2520reconstruction%2520errors.%2520To%2520this%2520end%252C%2520we%2520propose%250Aa%2520simple%2520yet%2520effective%2520modification%252C%2520called%2520class-aware%2520contrastive%2520learning%250A%2528CCL%2529.%2520By%2520explicitly%2520leveraging%2520raw%2520object%2520category%2520information%2520%2528%255Ceg%2520carpet%2520or%250Awood%2529%2520as%2520supervised%2520signals%252C%2520we%2520introduce%2520local%2520CL%2520to%2520refine%2520multiscale%2520dense%250Afeatures%252C%2520and%2520global%2520CL%2520to%2520obtain%2520more%2520compact%2520feature%2520representations%2520of%250Anormal%2520patterns%252C%2520thereby%2520effectively%2520adapting%2520the%2520models%2520to%2520multi-class%250Asettings.%2520Experiments%2520across%2520five%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%250Aapproach%252C%2520demonstrating%2520significant%2520improvements%2520and%2520superior%2520performance%250Acompared%2520to%2520state-of-the-art%2520methods.%2520Notably%252C%2520ablation%2520studies%2520indicate%2520that%250Apseudo-class%2520labels%2520can%2520achieve%2520comparable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Salvaging%20the%20Overlooked%3A%20Leveraging%20Class-Aware%20Contrastive%20Learning%0A%20%20for%20Multi-Class%20Anomaly%20Detection&entry.906535625=Lei%20Fan%20and%20Junjie%20Huang%20and%20Donglin%20Di%20and%20Anyang%20Su%20and%20Tianyou%20Song%20and%20Maurice%20Pagnucco%20and%20Yang%20Song&entry.1292438233=%20%20For%20anomaly%20detection%20%28AD%29%2C%20early%20approaches%20often%20train%20separate%20models%20for%0Aindividual%20classes%2C%20yielding%20high%20performance%20but%20posing%20challenges%20in%0Ascalability%20and%20resource%20management.%20Recent%20efforts%20have%20shifted%20toward%0Atraining%20a%20single%20model%20capable%20of%20handling%20multiple%20classes.%20However%2C%20directly%0Aextending%20early%20AD%20methods%20to%20multi-class%20settings%20often%20results%20in%20degraded%0Aperformance.%20In%20this%20paper%2C%20we%20investigate%20this%20performance%20degradation%0Aobserved%20in%20reconstruction-based%20methods%2C%20identifying%20the%20key%20issue%3A%0Ainter-class%20confusion.%20This%20confusion%20emerges%20when%20a%20model%20trained%20in%0Amulti-class%20scenarios%20incorrectly%20reconstructs%20samples%20from%20one%20class%20as%20those%0Aof%20another%2C%20thereby%20exacerbating%20reconstruction%20errors.%20To%20this%20end%2C%20we%20propose%0Aa%20simple%20yet%20effective%20modification%2C%20called%20class-aware%20contrastive%20learning%0A%28CCL%29.%20By%20explicitly%20leveraging%20raw%20object%20category%20information%20%28%5Ceg%20carpet%20or%0Awood%29%20as%20supervised%20signals%2C%20we%20introduce%20local%20CL%20to%20refine%20multiscale%20dense%0Afeatures%2C%20and%20global%20CL%20to%20obtain%20more%20compact%20feature%20representations%20of%0Anormal%20patterns%2C%20thereby%20effectively%20adapting%20the%20models%20to%20multi-class%0Asettings.%20Experiments%20across%20five%20datasets%20validate%20the%20effectiveness%20of%20our%0Aapproach%2C%20demonstrating%20significant%20improvements%20and%20superior%20performance%0Acompared%20to%20state-of-the-art%20methods.%20Notably%2C%20ablation%20studies%20indicate%20that%0Apseudo-class%20labels%20can%20achieve%20comparable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04769v2&entry.124074799=Read"},
{"title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models", "author": "Ashray Gupta and Rohan Joseph and Sunny Rai", "abstract": "  Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.\n", "link": "http://arxiv.org/abs/2507.13238v1", "date": "2025-07-17", "relevancy": 2.2384, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HATS%3A%20Hindi%20Analogy%20Test%20Set%20for%20Evaluating%20Reasoning%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20HATS%3A%20Hindi%20Analogy%20Test%20Set%20for%20Evaluating%20Reasoning%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Ashray%20Gupta%20and%20Rohan%20Joseph%20and%20Sunny%20Rai%0AAbstract%3A%20%20%20Analogies%20test%20a%20model%27s%20ability%20to%20infer%20implicit%20relationships%20between%0Aconcepts%2C%20making%20them%20a%20key%20benchmark%20for%20evaluating%20reasoning%20capabilities.%0AWhile%20large%20language%20models%20%28LLMs%29%20are%20widely%20evaluated%20for%20reasoning%20in%0AEnglish%2C%20their%20abilities%20in%20Indic%20languages%20remain%20understudied%2C%20limiting%20our%0Aunderstanding%20of%20whether%20these%20models%20generalize%20across%20languages.%20To%20address%0Athis%20gap%2C%20we%20introduce%20a%20new%20Hindi%20Analogy%20Test%20Set%20%28HATS%29%2C%20comprising%20405%0Amultiple-choice%20questions%20sourced%20from%20Indian%20government%20exams.%20We%20benchmark%0Astate-of-the-art%20multilingual%20LLMs%20using%20various%20prompting%20strategies%20and%0Aintroduce%20a%20grounded%20Chain%20of%20Thought%20approach%20that%20leverages%20cognitive%0Atheories%20of%20analogical%20reasoning.%20This%20approach%20improves%20model%20performance%20on%0AHindi%20analogy%20questions.%20Our%20experiments%20show%20that%20models%20perform%20best%20with%0AEnglish%20prompts%2C%20irrespective%20of%20the%20prompting%20strategy.%20Our%20test%20set%20addresses%0Athe%20lack%20of%20a%20critical%20resource%20to%20evaluate%20LLM%20reasoning%20capabilities%20in%0AHindi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHATS%253A%2520Hindi%2520Analogy%2520Test%2520Set%2520for%2520Evaluating%2520Reasoning%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DAshray%2520Gupta%2520and%2520Rohan%2520Joseph%2520and%2520Sunny%2520Rai%26entry.1292438233%3D%2520%2520Analogies%2520test%2520a%2520model%2527s%2520ability%2520to%2520infer%2520implicit%2520relationships%2520between%250Aconcepts%252C%2520making%2520them%2520a%2520key%2520benchmark%2520for%2520evaluating%2520reasoning%2520capabilities.%250AWhile%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520widely%2520evaluated%2520for%2520reasoning%2520in%250AEnglish%252C%2520their%2520abilities%2520in%2520Indic%2520languages%2520remain%2520understudied%252C%2520limiting%2520our%250Aunderstanding%2520of%2520whether%2520these%2520models%2520generalize%2520across%2520languages.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520new%2520Hindi%2520Analogy%2520Test%2520Set%2520%2528HATS%2529%252C%2520comprising%2520405%250Amultiple-choice%2520questions%2520sourced%2520from%2520Indian%2520government%2520exams.%2520We%2520benchmark%250Astate-of-the-art%2520multilingual%2520LLMs%2520using%2520various%2520prompting%2520strategies%2520and%250Aintroduce%2520a%2520grounded%2520Chain%2520of%2520Thought%2520approach%2520that%2520leverages%2520cognitive%250Atheories%2520of%2520analogical%2520reasoning.%2520This%2520approach%2520improves%2520model%2520performance%2520on%250AHindi%2520analogy%2520questions.%2520Our%2520experiments%2520show%2520that%2520models%2520perform%2520best%2520with%250AEnglish%2520prompts%252C%2520irrespective%2520of%2520the%2520prompting%2520strategy.%2520Our%2520test%2520set%2520addresses%250Athe%2520lack%2520of%2520a%2520critical%2520resource%2520to%2520evaluate%2520LLM%2520reasoning%2520capabilities%2520in%250AHindi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HATS%3A%20Hindi%20Analogy%20Test%20Set%20for%20Evaluating%20Reasoning%20in%20Large%20Language%0A%20%20Models&entry.906535625=Ashray%20Gupta%20and%20Rohan%20Joseph%20and%20Sunny%20Rai&entry.1292438233=%20%20Analogies%20test%20a%20model%27s%20ability%20to%20infer%20implicit%20relationships%20between%0Aconcepts%2C%20making%20them%20a%20key%20benchmark%20for%20evaluating%20reasoning%20capabilities.%0AWhile%20large%20language%20models%20%28LLMs%29%20are%20widely%20evaluated%20for%20reasoning%20in%0AEnglish%2C%20their%20abilities%20in%20Indic%20languages%20remain%20understudied%2C%20limiting%20our%0Aunderstanding%20of%20whether%20these%20models%20generalize%20across%20languages.%20To%20address%0Athis%20gap%2C%20we%20introduce%20a%20new%20Hindi%20Analogy%20Test%20Set%20%28HATS%29%2C%20comprising%20405%0Amultiple-choice%20questions%20sourced%20from%20Indian%20government%20exams.%20We%20benchmark%0Astate-of-the-art%20multilingual%20LLMs%20using%20various%20prompting%20strategies%20and%0Aintroduce%20a%20grounded%20Chain%20of%20Thought%20approach%20that%20leverages%20cognitive%0Atheories%20of%20analogical%20reasoning.%20This%20approach%20improves%20model%20performance%20on%0AHindi%20analogy%20questions.%20Our%20experiments%20show%20that%20models%20perform%20best%20with%0AEnglish%20prompts%2C%20irrespective%20of%20the%20prompting%20strategy.%20Our%20test%20set%20addresses%0Athe%20lack%20of%20a%20critical%20resource%20to%20evaluate%20LLM%20reasoning%20capabilities%20in%0AHindi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13238v1&entry.124074799=Read"},
{"title": "BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop\n  Closure", "author": "Haoxin Cai and Shenghai Yuan and Xinyi Li and Junfeng Guo and Jianqi Liu", "abstract": "  This work introduces BEV-LIO(LC), a novel LiDAR-Inertial Odometry (LIO)\nframework that combines Bird's Eye View (BEV) image representations of LiDAR\ndata with geometry-based point cloud registration and incorporates loop closure\n(LC) through BEV image features. By normalizing point density, we project LiDAR\npoint clouds into BEV images, thereby enabling efficient feature extraction and\nmatching. A lightweight convolutional neural network (CNN) based feature\nextractor is employed to extract distinctive local and global descriptors from\nthe BEV images. Local descriptors are used to match BEV images with FAST\nkeypoints for reprojection error construction, while global descriptors\nfacilitate loop closure detection. Reprojection error minimization is then\nintegrated with point-to-plane registration within an iterated Extended Kalman\nFilter (iEKF). In the back-end, global descriptors are used to create a\nKD-tree-indexed keyframe database for accurate loop closure detection. When a\nloop closure is detected, Random Sample Consensus (RANSAC) computes a coarse\ntransform from BEV image matching, which serves as the initial estimate for\nIterative Closest Point (ICP). The refined transform is subsequently\nincorporated into a factor graph along with odometry factors, improving the\nglobal consistency of localization. Extensive experiments conducted in various\nscenarios with different LiDAR types demonstrate that BEV-LIO(LC) outperforms\nstate-of-the-art methods, achieving competitive localization accuracy. Our code\nand video can be found at https://github.com/HxCa1/BEV-LIO-LC.\n", "link": "http://arxiv.org/abs/2502.19242v2", "date": "2025-07-17", "relevancy": 2.2337, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5753}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5505}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure&body=Title%3A%20BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure%0AAuthor%3A%20Haoxin%20Cai%20and%20Shenghai%20Yuan%20and%20Xinyi%20Li%20and%20Junfeng%20Guo%20and%20Jianqi%20Liu%0AAbstract%3A%20%20%20This%20work%20introduces%20BEV-LIO%28LC%29%2C%20a%20novel%20LiDAR-Inertial%20Odometry%20%28LIO%29%0Aframework%20that%20combines%20Bird%27s%20Eye%20View%20%28BEV%29%20image%20representations%20of%20LiDAR%0Adata%20with%20geometry-based%20point%20cloud%20registration%20and%20incorporates%20loop%20closure%0A%28LC%29%20through%20BEV%20image%20features.%20By%20normalizing%20point%20density%2C%20we%20project%20LiDAR%0Apoint%20clouds%20into%20BEV%20images%2C%20thereby%20enabling%20efficient%20feature%20extraction%20and%0Amatching.%20A%20lightweight%20convolutional%20neural%20network%20%28CNN%29%20based%20feature%0Aextractor%20is%20employed%20to%20extract%20distinctive%20local%20and%20global%20descriptors%20from%0Athe%20BEV%20images.%20Local%20descriptors%20are%20used%20to%20match%20BEV%20images%20with%20FAST%0Akeypoints%20for%20reprojection%20error%20construction%2C%20while%20global%20descriptors%0Afacilitate%20loop%20closure%20detection.%20Reprojection%20error%20minimization%20is%20then%0Aintegrated%20with%20point-to-plane%20registration%20within%20an%20iterated%20Extended%20Kalman%0AFilter%20%28iEKF%29.%20In%20the%20back-end%2C%20global%20descriptors%20are%20used%20to%20create%20a%0AKD-tree-indexed%20keyframe%20database%20for%20accurate%20loop%20closure%20detection.%20When%20a%0Aloop%20closure%20is%20detected%2C%20Random%20Sample%20Consensus%20%28RANSAC%29%20computes%20a%20coarse%0Atransform%20from%20BEV%20image%20matching%2C%20which%20serves%20as%20the%20initial%20estimate%20for%0AIterative%20Closest%20Point%20%28ICP%29.%20The%20refined%20transform%20is%20subsequently%0Aincorporated%20into%20a%20factor%20graph%20along%20with%20odometry%20factors%2C%20improving%20the%0Aglobal%20consistency%20of%20localization.%20Extensive%20experiments%20conducted%20in%20various%0Ascenarios%20with%20different%20LiDAR%20types%20demonstrate%20that%20BEV-LIO%28LC%29%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20competitive%20localization%20accuracy.%20Our%20code%0Aand%20video%20can%20be%20found%20at%20https%3A//github.com/HxCa1/BEV-LIO-LC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEV-LIO%2528LC%2529%253A%2520BEV%2520Image%2520Assisted%2520LiDAR-Inertial%2520Odometry%2520with%2520Loop%250A%2520%2520Closure%26entry.906535625%3DHaoxin%2520Cai%2520and%2520Shenghai%2520Yuan%2520and%2520Xinyi%2520Li%2520and%2520Junfeng%2520Guo%2520and%2520Jianqi%2520Liu%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520BEV-LIO%2528LC%2529%252C%2520a%2520novel%2520LiDAR-Inertial%2520Odometry%2520%2528LIO%2529%250Aframework%2520that%2520combines%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520image%2520representations%2520of%2520LiDAR%250Adata%2520with%2520geometry-based%2520point%2520cloud%2520registration%2520and%2520incorporates%2520loop%2520closure%250A%2528LC%2529%2520through%2520BEV%2520image%2520features.%2520By%2520normalizing%2520point%2520density%252C%2520we%2520project%2520LiDAR%250Apoint%2520clouds%2520into%2520BEV%2520images%252C%2520thereby%2520enabling%2520efficient%2520feature%2520extraction%2520and%250Amatching.%2520A%2520lightweight%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520based%2520feature%250Aextractor%2520is%2520employed%2520to%2520extract%2520distinctive%2520local%2520and%2520global%2520descriptors%2520from%250Athe%2520BEV%2520images.%2520Local%2520descriptors%2520are%2520used%2520to%2520match%2520BEV%2520images%2520with%2520FAST%250Akeypoints%2520for%2520reprojection%2520error%2520construction%252C%2520while%2520global%2520descriptors%250Afacilitate%2520loop%2520closure%2520detection.%2520Reprojection%2520error%2520minimization%2520is%2520then%250Aintegrated%2520with%2520point-to-plane%2520registration%2520within%2520an%2520iterated%2520Extended%2520Kalman%250AFilter%2520%2528iEKF%2529.%2520In%2520the%2520back-end%252C%2520global%2520descriptors%2520are%2520used%2520to%2520create%2520a%250AKD-tree-indexed%2520keyframe%2520database%2520for%2520accurate%2520loop%2520closure%2520detection.%2520When%2520a%250Aloop%2520closure%2520is%2520detected%252C%2520Random%2520Sample%2520Consensus%2520%2528RANSAC%2529%2520computes%2520a%2520coarse%250Atransform%2520from%2520BEV%2520image%2520matching%252C%2520which%2520serves%2520as%2520the%2520initial%2520estimate%2520for%250AIterative%2520Closest%2520Point%2520%2528ICP%2529.%2520The%2520refined%2520transform%2520is%2520subsequently%250Aincorporated%2520into%2520a%2520factor%2520graph%2520along%2520with%2520odometry%2520factors%252C%2520improving%2520the%250Aglobal%2520consistency%2520of%2520localization.%2520Extensive%2520experiments%2520conducted%2520in%2520various%250Ascenarios%2520with%2520different%2520LiDAR%2520types%2520demonstrate%2520that%2520BEV-LIO%2528LC%2529%2520outperforms%250Astate-of-the-art%2520methods%252C%2520achieving%2520competitive%2520localization%2520accuracy.%2520Our%2520code%250Aand%2520video%2520can%2520be%2520found%2520at%2520https%253A//github.com/HxCa1/BEV-LIO-LC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure&entry.906535625=Haoxin%20Cai%20and%20Shenghai%20Yuan%20and%20Xinyi%20Li%20and%20Junfeng%20Guo%20and%20Jianqi%20Liu&entry.1292438233=%20%20This%20work%20introduces%20BEV-LIO%28LC%29%2C%20a%20novel%20LiDAR-Inertial%20Odometry%20%28LIO%29%0Aframework%20that%20combines%20Bird%27s%20Eye%20View%20%28BEV%29%20image%20representations%20of%20LiDAR%0Adata%20with%20geometry-based%20point%20cloud%20registration%20and%20incorporates%20loop%20closure%0A%28LC%29%20through%20BEV%20image%20features.%20By%20normalizing%20point%20density%2C%20we%20project%20LiDAR%0Apoint%20clouds%20into%20BEV%20images%2C%20thereby%20enabling%20efficient%20feature%20extraction%20and%0Amatching.%20A%20lightweight%20convolutional%20neural%20network%20%28CNN%29%20based%20feature%0Aextractor%20is%20employed%20to%20extract%20distinctive%20local%20and%20global%20descriptors%20from%0Athe%20BEV%20images.%20Local%20descriptors%20are%20used%20to%20match%20BEV%20images%20with%20FAST%0Akeypoints%20for%20reprojection%20error%20construction%2C%20while%20global%20descriptors%0Afacilitate%20loop%20closure%20detection.%20Reprojection%20error%20minimization%20is%20then%0Aintegrated%20with%20point-to-plane%20registration%20within%20an%20iterated%20Extended%20Kalman%0AFilter%20%28iEKF%29.%20In%20the%20back-end%2C%20global%20descriptors%20are%20used%20to%20create%20a%0AKD-tree-indexed%20keyframe%20database%20for%20accurate%20loop%20closure%20detection.%20When%20a%0Aloop%20closure%20is%20detected%2C%20Random%20Sample%20Consensus%20%28RANSAC%29%20computes%20a%20coarse%0Atransform%20from%20BEV%20image%20matching%2C%20which%20serves%20as%20the%20initial%20estimate%20for%0AIterative%20Closest%20Point%20%28ICP%29.%20The%20refined%20transform%20is%20subsequently%0Aincorporated%20into%20a%20factor%20graph%20along%20with%20odometry%20factors%2C%20improving%20the%0Aglobal%20consistency%20of%20localization.%20Extensive%20experiments%20conducted%20in%20various%0Ascenarios%20with%20different%20LiDAR%20types%20demonstrate%20that%20BEV-LIO%28LC%29%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20competitive%20localization%20accuracy.%20Our%20code%0Aand%20video%20can%20be%20found%20at%20https%3A//github.com/HxCa1/BEV-LIO-LC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19242v2&entry.124074799=Read"},
{"title": "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with\n  Equivariant Neural Networks", "author": "Sneh Pandya and Purvik Patel and Brian D. Nord and Mike Walmsley and Aleksandra \u0106iprijanovi\u0107", "abstract": "  Modern neural networks (NNs) often do not generalize well in the presence of\na \"covariate shift\"; that is, in situations where the training and test data\ndistributions differ, but the conditional distribution of classification labels\nremains unchanged. In such cases, NN generalization can be reduced to a problem\nof learning more domain-invariant features. Domain adaptation (DA) methods\ninclude a range of techniques aimed at achieving this; however, these methods\nhave struggled with the need for extensive hyperparameter tuning, which then\nincurs significant computational costs. In this work, we introduce SIDDA, an\nout-of-the-box DA training algorithm built upon the Sinkhorn divergence, that\ncan achieve effective domain alignment with minimal hyperparameter tuning and\ncomputational overhead. We demonstrate the efficacy of our method on multiple\nsimulated and real datasets of varying complexity, including simple shapes,\nhandwritten digits, and real astronomical observations. SIDDA is compatible\nwith a variety of NN architectures, and it works particularly well in improving\nclassification accuracy and model calibration when paired with equivariant\nneural networks (ENNs). We find that SIDDA enhances the generalization\ncapabilities of NNs, achieving up to a $\\approx40\\%$ improvement in\nclassification accuracy on unlabeled target data. We also study the efficacy of\nDA on ENNs with respect to the varying group orders of the dihedral group\n$D_N$, and find that the model performance improves as the degree of\nequivariance increases. Finally, we find that SIDDA enhances model calibration\non both source and target data--achieving over an order of magnitude\nimprovement in the ECE and Brier score. SIDDA's versatility, combined with its\nautomated approach to domain alignment, has the potential to advance\nmulti-dataset studies by enabling the development of highly generalizable\nmodels.\n", "link": "http://arxiv.org/abs/2501.14048v2", "date": "2025-07-17", "relevancy": 2.2279, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5783}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5427}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIDDA%3A%20SInkhorn%20Dynamic%20Domain%20Adaptation%20for%20Image%20Classification%20with%0A%20%20Equivariant%20Neural%20Networks&body=Title%3A%20SIDDA%3A%20SInkhorn%20Dynamic%20Domain%20Adaptation%20for%20Image%20Classification%20with%0A%20%20Equivariant%20Neural%20Networks%0AAuthor%3A%20Sneh%20Pandya%20and%20Purvik%20Patel%20and%20Brian%20D.%20Nord%20and%20Mike%20Walmsley%20and%20Aleksandra%20%C4%86iprijanovi%C4%87%0AAbstract%3A%20%20%20Modern%20neural%20networks%20%28NNs%29%20often%20do%20not%20generalize%20well%20in%20the%20presence%20of%0Aa%20%22covariate%20shift%22%3B%20that%20is%2C%20in%20situations%20where%20the%20training%20and%20test%20data%0Adistributions%20differ%2C%20but%20the%20conditional%20distribution%20of%20classification%20labels%0Aremains%20unchanged.%20In%20such%20cases%2C%20NN%20generalization%20can%20be%20reduced%20to%20a%20problem%0Aof%20learning%20more%20domain-invariant%20features.%20Domain%20adaptation%20%28DA%29%20methods%0Ainclude%20a%20range%20of%20techniques%20aimed%20at%20achieving%20this%3B%20however%2C%20these%20methods%0Ahave%20struggled%20with%20the%20need%20for%20extensive%20hyperparameter%20tuning%2C%20which%20then%0Aincurs%20significant%20computational%20costs.%20In%20this%20work%2C%20we%20introduce%20SIDDA%2C%20an%0Aout-of-the-box%20DA%20training%20algorithm%20built%20upon%20the%20Sinkhorn%20divergence%2C%20that%0Acan%20achieve%20effective%20domain%20alignment%20with%20minimal%20hyperparameter%20tuning%20and%0Acomputational%20overhead.%20We%20demonstrate%20the%20efficacy%20of%20our%20method%20on%20multiple%0Asimulated%20and%20real%20datasets%20of%20varying%20complexity%2C%20including%20simple%20shapes%2C%0Ahandwritten%20digits%2C%20and%20real%20astronomical%20observations.%20SIDDA%20is%20compatible%0Awith%20a%20variety%20of%20NN%20architectures%2C%20and%20it%20works%20particularly%20well%20in%20improving%0Aclassification%20accuracy%20and%20model%20calibration%20when%20paired%20with%20equivariant%0Aneural%20networks%20%28ENNs%29.%20We%20find%20that%20SIDDA%20enhances%20the%20generalization%0Acapabilities%20of%20NNs%2C%20achieving%20up%20to%20a%20%24%5Capprox40%5C%25%24%20improvement%20in%0Aclassification%20accuracy%20on%20unlabeled%20target%20data.%20We%20also%20study%20the%20efficacy%20of%0ADA%20on%20ENNs%20with%20respect%20to%20the%20varying%20group%20orders%20of%20the%20dihedral%20group%0A%24D_N%24%2C%20and%20find%20that%20the%20model%20performance%20improves%20as%20the%20degree%20of%0Aequivariance%20increases.%20Finally%2C%20we%20find%20that%20SIDDA%20enhances%20model%20calibration%0Aon%20both%20source%20and%20target%20data--achieving%20over%20an%20order%20of%20magnitude%0Aimprovement%20in%20the%20ECE%20and%20Brier%20score.%20SIDDA%27s%20versatility%2C%20combined%20with%20its%0Aautomated%20approach%20to%20domain%20alignment%2C%20has%20the%20potential%20to%20advance%0Amulti-dataset%20studies%20by%20enabling%20the%20development%20of%20highly%20generalizable%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIDDA%253A%2520SInkhorn%2520Dynamic%2520Domain%2520Adaptation%2520for%2520Image%2520Classification%2520with%250A%2520%2520Equivariant%2520Neural%2520Networks%26entry.906535625%3DSneh%2520Pandya%2520and%2520Purvik%2520Patel%2520and%2520Brian%2520D.%2520Nord%2520and%2520Mike%2520Walmsley%2520and%2520Aleksandra%2520%25C4%2586iprijanovi%25C4%2587%26entry.1292438233%3D%2520%2520Modern%2520neural%2520networks%2520%2528NNs%2529%2520often%2520do%2520not%2520generalize%2520well%2520in%2520the%2520presence%2520of%250Aa%2520%2522covariate%2520shift%2522%253B%2520that%2520is%252C%2520in%2520situations%2520where%2520the%2520training%2520and%2520test%2520data%250Adistributions%2520differ%252C%2520but%2520the%2520conditional%2520distribution%2520of%2520classification%2520labels%250Aremains%2520unchanged.%2520In%2520such%2520cases%252C%2520NN%2520generalization%2520can%2520be%2520reduced%2520to%2520a%2520problem%250Aof%2520learning%2520more%2520domain-invariant%2520features.%2520Domain%2520adaptation%2520%2528DA%2529%2520methods%250Ainclude%2520a%2520range%2520of%2520techniques%2520aimed%2520at%2520achieving%2520this%253B%2520however%252C%2520these%2520methods%250Ahave%2520struggled%2520with%2520the%2520need%2520for%2520extensive%2520hyperparameter%2520tuning%252C%2520which%2520then%250Aincurs%2520significant%2520computational%2520costs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SIDDA%252C%2520an%250Aout-of-the-box%2520DA%2520training%2520algorithm%2520built%2520upon%2520the%2520Sinkhorn%2520divergence%252C%2520that%250Acan%2520achieve%2520effective%2520domain%2520alignment%2520with%2520minimal%2520hyperparameter%2520tuning%2520and%250Acomputational%2520overhead.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520method%2520on%2520multiple%250Asimulated%2520and%2520real%2520datasets%2520of%2520varying%2520complexity%252C%2520including%2520simple%2520shapes%252C%250Ahandwritten%2520digits%252C%2520and%2520real%2520astronomical%2520observations.%2520SIDDA%2520is%2520compatible%250Awith%2520a%2520variety%2520of%2520NN%2520architectures%252C%2520and%2520it%2520works%2520particularly%2520well%2520in%2520improving%250Aclassification%2520accuracy%2520and%2520model%2520calibration%2520when%2520paired%2520with%2520equivariant%250Aneural%2520networks%2520%2528ENNs%2529.%2520We%2520find%2520that%2520SIDDA%2520enhances%2520the%2520generalization%250Acapabilities%2520of%2520NNs%252C%2520achieving%2520up%2520to%2520a%2520%2524%255Capprox40%255C%2525%2524%2520improvement%2520in%250Aclassification%2520accuracy%2520on%2520unlabeled%2520target%2520data.%2520We%2520also%2520study%2520the%2520efficacy%2520of%250ADA%2520on%2520ENNs%2520with%2520respect%2520to%2520the%2520varying%2520group%2520orders%2520of%2520the%2520dihedral%2520group%250A%2524D_N%2524%252C%2520and%2520find%2520that%2520the%2520model%2520performance%2520improves%2520as%2520the%2520degree%2520of%250Aequivariance%2520increases.%2520Finally%252C%2520we%2520find%2520that%2520SIDDA%2520enhances%2520model%2520calibration%250Aon%2520both%2520source%2520and%2520target%2520data--achieving%2520over%2520an%2520order%2520of%2520magnitude%250Aimprovement%2520in%2520the%2520ECE%2520and%2520Brier%2520score.%2520SIDDA%2527s%2520versatility%252C%2520combined%2520with%2520its%250Aautomated%2520approach%2520to%2520domain%2520alignment%252C%2520has%2520the%2520potential%2520to%2520advance%250Amulti-dataset%2520studies%2520by%2520enabling%2520the%2520development%2520of%2520highly%2520generalizable%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIDDA%3A%20SInkhorn%20Dynamic%20Domain%20Adaptation%20for%20Image%20Classification%20with%0A%20%20Equivariant%20Neural%20Networks&entry.906535625=Sneh%20Pandya%20and%20Purvik%20Patel%20and%20Brian%20D.%20Nord%20and%20Mike%20Walmsley%20and%20Aleksandra%20%C4%86iprijanovi%C4%87&entry.1292438233=%20%20Modern%20neural%20networks%20%28NNs%29%20often%20do%20not%20generalize%20well%20in%20the%20presence%20of%0Aa%20%22covariate%20shift%22%3B%20that%20is%2C%20in%20situations%20where%20the%20training%20and%20test%20data%0Adistributions%20differ%2C%20but%20the%20conditional%20distribution%20of%20classification%20labels%0Aremains%20unchanged.%20In%20such%20cases%2C%20NN%20generalization%20can%20be%20reduced%20to%20a%20problem%0Aof%20learning%20more%20domain-invariant%20features.%20Domain%20adaptation%20%28DA%29%20methods%0Ainclude%20a%20range%20of%20techniques%20aimed%20at%20achieving%20this%3B%20however%2C%20these%20methods%0Ahave%20struggled%20with%20the%20need%20for%20extensive%20hyperparameter%20tuning%2C%20which%20then%0Aincurs%20significant%20computational%20costs.%20In%20this%20work%2C%20we%20introduce%20SIDDA%2C%20an%0Aout-of-the-box%20DA%20training%20algorithm%20built%20upon%20the%20Sinkhorn%20divergence%2C%20that%0Acan%20achieve%20effective%20domain%20alignment%20with%20minimal%20hyperparameter%20tuning%20and%0Acomputational%20overhead.%20We%20demonstrate%20the%20efficacy%20of%20our%20method%20on%20multiple%0Asimulated%20and%20real%20datasets%20of%20varying%20complexity%2C%20including%20simple%20shapes%2C%0Ahandwritten%20digits%2C%20and%20real%20astronomical%20observations.%20SIDDA%20is%20compatible%0Awith%20a%20variety%20of%20NN%20architectures%2C%20and%20it%20works%20particularly%20well%20in%20improving%0Aclassification%20accuracy%20and%20model%20calibration%20when%20paired%20with%20equivariant%0Aneural%20networks%20%28ENNs%29.%20We%20find%20that%20SIDDA%20enhances%20the%20generalization%0Acapabilities%20of%20NNs%2C%20achieving%20up%20to%20a%20%24%5Capprox40%5C%25%24%20improvement%20in%0Aclassification%20accuracy%20on%20unlabeled%20target%20data.%20We%20also%20study%20the%20efficacy%20of%0ADA%20on%20ENNs%20with%20respect%20to%20the%20varying%20group%20orders%20of%20the%20dihedral%20group%0A%24D_N%24%2C%20and%20find%20that%20the%20model%20performance%20improves%20as%20the%20degree%20of%0Aequivariance%20increases.%20Finally%2C%20we%20find%20that%20SIDDA%20enhances%20model%20calibration%0Aon%20both%20source%20and%20target%20data--achieving%20over%20an%20order%20of%20magnitude%0Aimprovement%20in%20the%20ECE%20and%20Brier%20score.%20SIDDA%27s%20versatility%2C%20combined%20with%20its%0Aautomated%20approach%20to%20domain%20alignment%2C%20has%20the%20potential%20to%20advance%0Amulti-dataset%20studies%20by%20enabling%20the%20development%20of%20highly%20generalizable%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14048v2&entry.124074799=Read"},
{"title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform\n  Midjourney for Construction Worker Detection", "author": "Hongyang Zhao and Tianyu Liang and Sina Davari and Daeho Kim", "abstract": "  While recent advancements in deep neural networks (DNNs) have substantially\nenhanced visual AI's capabilities, the challenge of inadequate data diversity\nand volume remains, particularly in construction domain. This study presents a\nnovel image synthesis methodology tailored for construction worker detection,\nleveraging the generative-AI platform Midjourney. The approach entails\ngenerating a collection of 12,000 synthetic images by formulating 3000\ndifferent prompts, with an emphasis on image realism and diversity. These\nimages, after manual labeling, serve as a dataset for DNN training. Evaluation\non a real construction image dataset yielded promising results, with the model\nattaining average precisions (APs) of 0.937 and 0.642 at\nintersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively.\nNotably, the model demonstrated near-perfect performance on the synthetic\ndataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds.\nThese findings reveal both the potential and weakness of generative AI in\naddressing DNN training data scarcity.\n", "link": "http://arxiv.org/abs/2507.13221v1", "date": "2025-07-17", "relevancy": 2.2162, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5586}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5536}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Reality%3A%20Leveraging%20the%20Generative%20AI-Powered%20Platform%0A%20%20Midjourney%20for%20Construction%20Worker%20Detection&body=Title%3A%20Synthesizing%20Reality%3A%20Leveraging%20the%20Generative%20AI-Powered%20Platform%0A%20%20Midjourney%20for%20Construction%20Worker%20Detection%0AAuthor%3A%20Hongyang%20Zhao%20and%20Tianyu%20Liang%20and%20Sina%20Davari%20and%20Daeho%20Kim%0AAbstract%3A%20%20%20While%20recent%20advancements%20in%20deep%20neural%20networks%20%28DNNs%29%20have%20substantially%0Aenhanced%20visual%20AI%27s%20capabilities%2C%20the%20challenge%20of%20inadequate%20data%20diversity%0Aand%20volume%20remains%2C%20particularly%20in%20construction%20domain.%20This%20study%20presents%20a%0Anovel%20image%20synthesis%20methodology%20tailored%20for%20construction%20worker%20detection%2C%0Aleveraging%20the%20generative-AI%20platform%20Midjourney.%20The%20approach%20entails%0Agenerating%20a%20collection%20of%2012%2C000%20synthetic%20images%20by%20formulating%203000%0Adifferent%20prompts%2C%20with%20an%20emphasis%20on%20image%20realism%20and%20diversity.%20These%0Aimages%2C%20after%20manual%20labeling%2C%20serve%20as%20a%20dataset%20for%20DNN%20training.%20Evaluation%0Aon%20a%20real%20construction%20image%20dataset%20yielded%20promising%20results%2C%20with%20the%20model%0Aattaining%20average%20precisions%20%28APs%29%20of%200.937%20and%200.642%20at%0Aintersection-over-union%20%28IoU%29%20thresholds%20of%200.5%20and%200.5%20to%200.95%2C%20respectively.%0ANotably%2C%20the%20model%20demonstrated%20near-perfect%20performance%20on%20the%20synthetic%0Adataset%2C%20achieving%20APs%20of%200.994%20and%200.919%20at%20the%20two%20mentioned%20thresholds.%0AThese%20findings%20reveal%20both%20the%20potential%20and%20weakness%20of%20generative%20AI%20in%0Aaddressing%20DNN%20training%20data%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520Reality%253A%2520Leveraging%2520the%2520Generative%2520AI-Powered%2520Platform%250A%2520%2520Midjourney%2520for%2520Construction%2520Worker%2520Detection%26entry.906535625%3DHongyang%2520Zhao%2520and%2520Tianyu%2520Liang%2520and%2520Sina%2520Davari%2520and%2520Daeho%2520Kim%26entry.1292438233%3D%2520%2520While%2520recent%2520advancements%2520in%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520substantially%250Aenhanced%2520visual%2520AI%2527s%2520capabilities%252C%2520the%2520challenge%2520of%2520inadequate%2520data%2520diversity%250Aand%2520volume%2520remains%252C%2520particularly%2520in%2520construction%2520domain.%2520This%2520study%2520presents%2520a%250Anovel%2520image%2520synthesis%2520methodology%2520tailored%2520for%2520construction%2520worker%2520detection%252C%250Aleveraging%2520the%2520generative-AI%2520platform%2520Midjourney.%2520The%2520approach%2520entails%250Agenerating%2520a%2520collection%2520of%252012%252C000%2520synthetic%2520images%2520by%2520formulating%25203000%250Adifferent%2520prompts%252C%2520with%2520an%2520emphasis%2520on%2520image%2520realism%2520and%2520diversity.%2520These%250Aimages%252C%2520after%2520manual%2520labeling%252C%2520serve%2520as%2520a%2520dataset%2520for%2520DNN%2520training.%2520Evaluation%250Aon%2520a%2520real%2520construction%2520image%2520dataset%2520yielded%2520promising%2520results%252C%2520with%2520the%2520model%250Aattaining%2520average%2520precisions%2520%2528APs%2529%2520of%25200.937%2520and%25200.642%2520at%250Aintersection-over-union%2520%2528IoU%2529%2520thresholds%2520of%25200.5%2520and%25200.5%2520to%25200.95%252C%2520respectively.%250ANotably%252C%2520the%2520model%2520demonstrated%2520near-perfect%2520performance%2520on%2520the%2520synthetic%250Adataset%252C%2520achieving%2520APs%2520of%25200.994%2520and%25200.919%2520at%2520the%2520two%2520mentioned%2520thresholds.%250AThese%2520findings%2520reveal%2520both%2520the%2520potential%2520and%2520weakness%2520of%2520generative%2520AI%2520in%250Aaddressing%2520DNN%2520training%2520data%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Reality%3A%20Leveraging%20the%20Generative%20AI-Powered%20Platform%0A%20%20Midjourney%20for%20Construction%20Worker%20Detection&entry.906535625=Hongyang%20Zhao%20and%20Tianyu%20Liang%20and%20Sina%20Davari%20and%20Daeho%20Kim&entry.1292438233=%20%20While%20recent%20advancements%20in%20deep%20neural%20networks%20%28DNNs%29%20have%20substantially%0Aenhanced%20visual%20AI%27s%20capabilities%2C%20the%20challenge%20of%20inadequate%20data%20diversity%0Aand%20volume%20remains%2C%20particularly%20in%20construction%20domain.%20This%20study%20presents%20a%0Anovel%20image%20synthesis%20methodology%20tailored%20for%20construction%20worker%20detection%2C%0Aleveraging%20the%20generative-AI%20platform%20Midjourney.%20The%20approach%20entails%0Agenerating%20a%20collection%20of%2012%2C000%20synthetic%20images%20by%20formulating%203000%0Adifferent%20prompts%2C%20with%20an%20emphasis%20on%20image%20realism%20and%20diversity.%20These%0Aimages%2C%20after%20manual%20labeling%2C%20serve%20as%20a%20dataset%20for%20DNN%20training.%20Evaluation%0Aon%20a%20real%20construction%20image%20dataset%20yielded%20promising%20results%2C%20with%20the%20model%0Aattaining%20average%20precisions%20%28APs%29%20of%200.937%20and%200.642%20at%0Aintersection-over-union%20%28IoU%29%20thresholds%20of%200.5%20and%200.5%20to%200.95%2C%20respectively.%0ANotably%2C%20the%20model%20demonstrated%20near-perfect%20performance%20on%20the%20synthetic%0Adataset%2C%20achieving%20APs%20of%200.994%20and%200.919%20at%20the%20two%20mentioned%20thresholds.%0AThese%20findings%20reveal%20both%20the%20potential%20and%20weakness%20of%20generative%20AI%20in%0Aaddressing%20DNN%20training%20data%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13221v1&entry.124074799=Read"},
{"title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data\n  for Neonatal Bronchopulmonary Dysplasia", "author": "Rachit Saluja and Arzu Kovanlikaya and Candace Chien and Lauren Kathryn Blatt and Jeffrey M. Perlman and Stefan Worgall and Mert R. Sabuncu and Jonathan P. Dyke", "abstract": "  Bronchopulmonary dysplasia (BPD) is a common complication among preterm\nneonates, with portable X-ray imaging serving as the standard diagnostic\nmodality in neonatal intensive care units (NICUs). However, lung magnetic\nresonance imaging (MRI) offers a non-invasive alternative that avoids sedation\nand radiation while providing detailed insights into the underlying mechanisms\nof BPD. Leveraging high-resolution 3D MRI data, advanced image processing and\nsemantic segmentation algorithms can be developed to assist clinicians in\nidentifying the etiology of BPD. In this dataset, we present MRI scans paired\nwith corresponding semantic segmentations of the lungs and trachea for 40\nneonates, the majority of whom are diagnosed with BPD. The imaging data consist\nof free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as\nthe StarVIBE series. Additionally, we provide comprehensive clinical data and\nbaseline segmentation models, validated against clinical assessments, to\nsupport further research and development in neonatal lung imaging.\n", "link": "http://arxiv.org/abs/2506.23305v2", "date": "2025-07-17", "relevancy": 2.2143, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4463}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BPD-Neo%3A%20An%20MRI%20Dataset%20for%20Lung-Trachea%20Segmentation%20with%20Clinical%20Data%0A%20%20for%20Neonatal%20Bronchopulmonary%20Dysplasia&body=Title%3A%20BPD-Neo%3A%20An%20MRI%20Dataset%20for%20Lung-Trachea%20Segmentation%20with%20Clinical%20Data%0A%20%20for%20Neonatal%20Bronchopulmonary%20Dysplasia%0AAuthor%3A%20Rachit%20Saluja%20and%20Arzu%20Kovanlikaya%20and%20Candace%20Chien%20and%20Lauren%20Kathryn%20Blatt%20and%20Jeffrey%20M.%20Perlman%20and%20Stefan%20Worgall%20and%20Mert%20R.%20Sabuncu%20and%20Jonathan%20P.%20Dyke%0AAbstract%3A%20%20%20Bronchopulmonary%20dysplasia%20%28BPD%29%20is%20a%20common%20complication%20among%20preterm%0Aneonates%2C%20with%20portable%20X-ray%20imaging%20serving%20as%20the%20standard%20diagnostic%0Amodality%20in%20neonatal%20intensive%20care%20units%20%28NICUs%29.%20However%2C%20lung%20magnetic%0Aresonance%20imaging%20%28MRI%29%20offers%20a%20non-invasive%20alternative%20that%20avoids%20sedation%0Aand%20radiation%20while%20providing%20detailed%20insights%20into%20the%20underlying%20mechanisms%0Aof%20BPD.%20Leveraging%20high-resolution%203D%20MRI%20data%2C%20advanced%20image%20processing%20and%0Asemantic%20segmentation%20algorithms%20can%20be%20developed%20to%20assist%20clinicians%20in%0Aidentifying%20the%20etiology%20of%20BPD.%20In%20this%20dataset%2C%20we%20present%20MRI%20scans%20paired%0Awith%20corresponding%20semantic%20segmentations%20of%20the%20lungs%20and%20trachea%20for%2040%0Aneonates%2C%20the%20majority%20of%20whom%20are%20diagnosed%20with%20BPD.%20The%20imaging%20data%20consist%0Aof%20free-breathing%203D%20stack-of-stars%20radial%20gradient%20echo%20acquisitions%2C%20known%20as%0Athe%20StarVIBE%20series.%20Additionally%2C%20we%20provide%20comprehensive%20clinical%20data%20and%0Abaseline%20segmentation%20models%2C%20validated%20against%20clinical%20assessments%2C%20to%0Asupport%20further%20research%20and%20development%20in%20neonatal%20lung%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBPD-Neo%253A%2520An%2520MRI%2520Dataset%2520for%2520Lung-Trachea%2520Segmentation%2520with%2520Clinical%2520Data%250A%2520%2520for%2520Neonatal%2520Bronchopulmonary%2520Dysplasia%26entry.906535625%3DRachit%2520Saluja%2520and%2520Arzu%2520Kovanlikaya%2520and%2520Candace%2520Chien%2520and%2520Lauren%2520Kathryn%2520Blatt%2520and%2520Jeffrey%2520M.%2520Perlman%2520and%2520Stefan%2520Worgall%2520and%2520Mert%2520R.%2520Sabuncu%2520and%2520Jonathan%2520P.%2520Dyke%26entry.1292438233%3D%2520%2520Bronchopulmonary%2520dysplasia%2520%2528BPD%2529%2520is%2520a%2520common%2520complication%2520among%2520preterm%250Aneonates%252C%2520with%2520portable%2520X-ray%2520imaging%2520serving%2520as%2520the%2520standard%2520diagnostic%250Amodality%2520in%2520neonatal%2520intensive%2520care%2520units%2520%2528NICUs%2529.%2520However%252C%2520lung%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529%2520offers%2520a%2520non-invasive%2520alternative%2520that%2520avoids%2520sedation%250Aand%2520radiation%2520while%2520providing%2520detailed%2520insights%2520into%2520the%2520underlying%2520mechanisms%250Aof%2520BPD.%2520Leveraging%2520high-resolution%25203D%2520MRI%2520data%252C%2520advanced%2520image%2520processing%2520and%250Asemantic%2520segmentation%2520algorithms%2520can%2520be%2520developed%2520to%2520assist%2520clinicians%2520in%250Aidentifying%2520the%2520etiology%2520of%2520BPD.%2520In%2520this%2520dataset%252C%2520we%2520present%2520MRI%2520scans%2520paired%250Awith%2520corresponding%2520semantic%2520segmentations%2520of%2520the%2520lungs%2520and%2520trachea%2520for%252040%250Aneonates%252C%2520the%2520majority%2520of%2520whom%2520are%2520diagnosed%2520with%2520BPD.%2520The%2520imaging%2520data%2520consist%250Aof%2520free-breathing%25203D%2520stack-of-stars%2520radial%2520gradient%2520echo%2520acquisitions%252C%2520known%2520as%250Athe%2520StarVIBE%2520series.%2520Additionally%252C%2520we%2520provide%2520comprehensive%2520clinical%2520data%2520and%250Abaseline%2520segmentation%2520models%252C%2520validated%2520against%2520clinical%2520assessments%252C%2520to%250Asupport%2520further%2520research%2520and%2520development%2520in%2520neonatal%2520lung%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BPD-Neo%3A%20An%20MRI%20Dataset%20for%20Lung-Trachea%20Segmentation%20with%20Clinical%20Data%0A%20%20for%20Neonatal%20Bronchopulmonary%20Dysplasia&entry.906535625=Rachit%20Saluja%20and%20Arzu%20Kovanlikaya%20and%20Candace%20Chien%20and%20Lauren%20Kathryn%20Blatt%20and%20Jeffrey%20M.%20Perlman%20and%20Stefan%20Worgall%20and%20Mert%20R.%20Sabuncu%20and%20Jonathan%20P.%20Dyke&entry.1292438233=%20%20Bronchopulmonary%20dysplasia%20%28BPD%29%20is%20a%20common%20complication%20among%20preterm%0Aneonates%2C%20with%20portable%20X-ray%20imaging%20serving%20as%20the%20standard%20diagnostic%0Amodality%20in%20neonatal%20intensive%20care%20units%20%28NICUs%29.%20However%2C%20lung%20magnetic%0Aresonance%20imaging%20%28MRI%29%20offers%20a%20non-invasive%20alternative%20that%20avoids%20sedation%0Aand%20radiation%20while%20providing%20detailed%20insights%20into%20the%20underlying%20mechanisms%0Aof%20BPD.%20Leveraging%20high-resolution%203D%20MRI%20data%2C%20advanced%20image%20processing%20and%0Asemantic%20segmentation%20algorithms%20can%20be%20developed%20to%20assist%20clinicians%20in%0Aidentifying%20the%20etiology%20of%20BPD.%20In%20this%20dataset%2C%20we%20present%20MRI%20scans%20paired%0Awith%20corresponding%20semantic%20segmentations%20of%20the%20lungs%20and%20trachea%20for%2040%0Aneonates%2C%20the%20majority%20of%20whom%20are%20diagnosed%20with%20BPD.%20The%20imaging%20data%20consist%0Aof%20free-breathing%203D%20stack-of-stars%20radial%20gradient%20echo%20acquisitions%2C%20known%20as%0Athe%20StarVIBE%20series.%20Additionally%2C%20we%20provide%20comprehensive%20clinical%20data%20and%0Abaseline%20segmentation%20models%2C%20validated%20against%20clinical%20assessments%2C%20to%0Asupport%20further%20research%20and%20development%20in%20neonatal%20lung%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23305v2&entry.124074799=Read"},
{"title": "Characterizing Dynamical Stability of Stochastic Gradient Descent in\n  Overparameterized Learning", "author": "Dennis Chemnitz and Maximilian Engel", "abstract": "  For overparameterized optimization tasks, such as those found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent that depends on the local dynamics\naround a global minimum and rigorously prove that the sign of this Lyapunov\nexponent determines whether SGD can accumulate at the respective global\nminimum.\n", "link": "http://arxiv.org/abs/2407.20209v3", "date": "2025-07-17", "relevancy": 2.2092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4381}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&body=Title%3A%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning%0AAuthor%3A%20Dennis%20Chemnitz%20and%20Maximilian%20Engel%0AAbstract%3A%20%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20those%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20that%20depends%20on%20the%20local%20dynamics%0Aaround%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%20Lyapunov%0Aexponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%20global%0Aminimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20209v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Dynamical%2520Stability%2520of%2520Stochastic%2520Gradient%2520Descent%2520in%250A%2520%2520Overparameterized%2520Learning%26entry.906535625%3DDennis%2520Chemnitz%2520and%2520Maximilian%2520Engel%26entry.1292438233%3D%2520%2520For%2520overparameterized%2520optimization%2520tasks%252C%2520such%2520as%2520those%2520found%2520in%2520modern%250Amachine%2520learning%252C%2520global%2520minima%2520are%2520generally%2520not%2520unique.%2520In%2520order%2520to%250Aunderstand%2520generalization%2520in%2520these%2520settings%252C%2520it%2520is%2520vital%2520to%2520study%2520to%2520which%250Aminimum%2520an%2520optimization%2520algorithm%2520converges.%2520The%2520possibility%2520of%2520having%2520minima%250Athat%2520are%2520unstable%2520under%2520the%2520dynamics%2520imposed%2520by%2520the%2520optimization%2520algorithm%250Alimits%2520the%2520potential%2520minima%2520that%2520the%2520algorithm%2520can%2520find.%2520In%2520this%2520paper%252C%2520we%250Acharacterize%2520the%2520global%2520minima%2520that%2520are%2520dynamically%2520stable/unstable%2520for%2520both%250Adeterministic%2520and%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520In%2520particular%252C%2520we%250Aintroduce%2520a%2520characteristic%2520Lyapunov%2520exponent%2520that%2520depends%2520on%2520the%2520local%2520dynamics%250Aaround%2520a%2520global%2520minimum%2520and%2520rigorously%2520prove%2520that%2520the%2520sign%2520of%2520this%2520Lyapunov%250Aexponent%2520determines%2520whether%2520SGD%2520can%2520accumulate%2520at%2520the%2520respective%2520global%250Aminimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20209v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&entry.906535625=Dennis%20Chemnitz%20and%20Maximilian%20Engel&entry.1292438233=%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20those%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20that%20depends%20on%20the%20local%20dynamics%0Aaround%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%20Lyapunov%0Aexponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%20global%0Aminimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20209v3&entry.124074799=Read"},
{"title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept\n  Learning", "author": "Xiaohan Guo and Yusong Cai and Zejia Liu and Zhengning Wang and Lili Pan and Hongliang Li", "abstract": "  Enabling large-scale generative models to continuously learn new visual\nconcepts is essential for personalizing pre-trained models to meet individual\nuser preferences. Existing approaches for continual visual concept learning are\nconstrained by two fundamental challenges: catastrophic forgetting and\nparameter expansion. In this paper, we propose Redundancy-Removal Mixture of\nExperts (R^2MoE), a parameter-efficient framework for lifelong visual concept\nlearning that effectively learns new concepts while incurring minimal parameter\noverhead. Our framework includes three key innovative contributions: First, we\npropose a mixture-of-experts framework with a routing distillation mechanism\nthat enables experts to acquire concept-specific knowledge while preserving the\ngating network's routing capability, thereby effectively mitigating\ncatastrophic forgetting. Second, we propose a strategy for eliminating\nredundant layer-wise experts that reduces the number of expert parameters by\nfully utilizing previously learned experts. Third, we employ a hierarchical\nlocal attention-guided inference approach to mitigate interference between\ngenerated visual concepts. Extensive experiments have demonstrated that our\nmethod generates images with superior conceptual fidelity compared to the\nstate-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in\nforgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset.\nOur code is available at {https://github.com/learninginvision/R2MoE}\n", "link": "http://arxiv.org/abs/2507.13107v1", "date": "2025-07-17", "relevancy": 2.2083, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5699}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5417}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R%5E2MoE%3A%20Redundancy-Removal%20Mixture%20of%20Experts%20for%20Lifelong%20Concept%0A%20%20Learning&body=Title%3A%20R%5E2MoE%3A%20Redundancy-Removal%20Mixture%20of%20Experts%20for%20Lifelong%20Concept%0A%20%20Learning%0AAuthor%3A%20Xiaohan%20Guo%20and%20Yusong%20Cai%20and%20Zejia%20Liu%20and%20Zhengning%20Wang%20and%20Lili%20Pan%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20Enabling%20large-scale%20generative%20models%20to%20continuously%20learn%20new%20visual%0Aconcepts%20is%20essential%20for%20personalizing%20pre-trained%20models%20to%20meet%20individual%0Auser%20preferences.%20Existing%20approaches%20for%20continual%20visual%20concept%20learning%20are%0Aconstrained%20by%20two%20fundamental%20challenges%3A%20catastrophic%20forgetting%20and%0Aparameter%20expansion.%20In%20this%20paper%2C%20we%20propose%20Redundancy-Removal%20Mixture%20of%0AExperts%20%28R%5E2MoE%29%2C%20a%20parameter-efficient%20framework%20for%20lifelong%20visual%20concept%0Alearning%20that%20effectively%20learns%20new%20concepts%20while%20incurring%20minimal%20parameter%0Aoverhead.%20Our%20framework%20includes%20three%20key%20innovative%20contributions%3A%20First%2C%20we%0Apropose%20a%20mixture-of-experts%20framework%20with%20a%20routing%20distillation%20mechanism%0Athat%20enables%20experts%20to%20acquire%20concept-specific%20knowledge%20while%20preserving%20the%0Agating%20network%27s%20routing%20capability%2C%20thereby%20effectively%20mitigating%0Acatastrophic%20forgetting.%20Second%2C%20we%20propose%20a%20strategy%20for%20eliminating%0Aredundant%20layer-wise%20experts%20that%20reduces%20the%20number%20of%20expert%20parameters%20by%0Afully%20utilizing%20previously%20learned%20experts.%20Third%2C%20we%20employ%20a%20hierarchical%0Alocal%20attention-guided%20inference%20approach%20to%20mitigate%20interference%20between%0Agenerated%20visual%20concepts.%20Extensive%20experiments%20have%20demonstrated%20that%20our%0Amethod%20generates%20images%20with%20superior%20conceptual%20fidelity%20compared%20to%20the%0Astate-of-the-art%20%28SOTA%29%20method%2C%20achieving%20an%20impressive%2087.8%5C%25%20reduction%20in%0Aforgetting%20rates%20and%2063.3%5C%25%20fewer%20parameters%20on%20the%20CustomConcept%20101%20dataset.%0AOur%20code%20is%20available%20at%20%7Bhttps%3A//github.com/learninginvision/R2MoE%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR%255E2MoE%253A%2520Redundancy-Removal%2520Mixture%2520of%2520Experts%2520for%2520Lifelong%2520Concept%250A%2520%2520Learning%26entry.906535625%3DXiaohan%2520Guo%2520and%2520Yusong%2520Cai%2520and%2520Zejia%2520Liu%2520and%2520Zhengning%2520Wang%2520and%2520Lili%2520Pan%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520Enabling%2520large-scale%2520generative%2520models%2520to%2520continuously%2520learn%2520new%2520visual%250Aconcepts%2520is%2520essential%2520for%2520personalizing%2520pre-trained%2520models%2520to%2520meet%2520individual%250Auser%2520preferences.%2520Existing%2520approaches%2520for%2520continual%2520visual%2520concept%2520learning%2520are%250Aconstrained%2520by%2520two%2520fundamental%2520challenges%253A%2520catastrophic%2520forgetting%2520and%250Aparameter%2520expansion.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Redundancy-Removal%2520Mixture%2520of%250AExperts%2520%2528R%255E2MoE%2529%252C%2520a%2520parameter-efficient%2520framework%2520for%2520lifelong%2520visual%2520concept%250Alearning%2520that%2520effectively%2520learns%2520new%2520concepts%2520while%2520incurring%2520minimal%2520parameter%250Aoverhead.%2520Our%2520framework%2520includes%2520three%2520key%2520innovative%2520contributions%253A%2520First%252C%2520we%250Apropose%2520a%2520mixture-of-experts%2520framework%2520with%2520a%2520routing%2520distillation%2520mechanism%250Athat%2520enables%2520experts%2520to%2520acquire%2520concept-specific%2520knowledge%2520while%2520preserving%2520the%250Agating%2520network%2527s%2520routing%2520capability%252C%2520thereby%2520effectively%2520mitigating%250Acatastrophic%2520forgetting.%2520Second%252C%2520we%2520propose%2520a%2520strategy%2520for%2520eliminating%250Aredundant%2520layer-wise%2520experts%2520that%2520reduces%2520the%2520number%2520of%2520expert%2520parameters%2520by%250Afully%2520utilizing%2520previously%2520learned%2520experts.%2520Third%252C%2520we%2520employ%2520a%2520hierarchical%250Alocal%2520attention-guided%2520inference%2520approach%2520to%2520mitigate%2520interference%2520between%250Agenerated%2520visual%2520concepts.%2520Extensive%2520experiments%2520have%2520demonstrated%2520that%2520our%250Amethod%2520generates%2520images%2520with%2520superior%2520conceptual%2520fidelity%2520compared%2520to%2520the%250Astate-of-the-art%2520%2528SOTA%2529%2520method%252C%2520achieving%2520an%2520impressive%252087.8%255C%2525%2520reduction%2520in%250Aforgetting%2520rates%2520and%252063.3%255C%2525%2520fewer%2520parameters%2520on%2520the%2520CustomConcept%2520101%2520dataset.%250AOur%2520code%2520is%2520available%2520at%2520%257Bhttps%253A//github.com/learninginvision/R2MoE%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R%5E2MoE%3A%20Redundancy-Removal%20Mixture%20of%20Experts%20for%20Lifelong%20Concept%0A%20%20Learning&entry.906535625=Xiaohan%20Guo%20and%20Yusong%20Cai%20and%20Zejia%20Liu%20and%20Zhengning%20Wang%20and%20Lili%20Pan%20and%20Hongliang%20Li&entry.1292438233=%20%20Enabling%20large-scale%20generative%20models%20to%20continuously%20learn%20new%20visual%0Aconcepts%20is%20essential%20for%20personalizing%20pre-trained%20models%20to%20meet%20individual%0Auser%20preferences.%20Existing%20approaches%20for%20continual%20visual%20concept%20learning%20are%0Aconstrained%20by%20two%20fundamental%20challenges%3A%20catastrophic%20forgetting%20and%0Aparameter%20expansion.%20In%20this%20paper%2C%20we%20propose%20Redundancy-Removal%20Mixture%20of%0AExperts%20%28R%5E2MoE%29%2C%20a%20parameter-efficient%20framework%20for%20lifelong%20visual%20concept%0Alearning%20that%20effectively%20learns%20new%20concepts%20while%20incurring%20minimal%20parameter%0Aoverhead.%20Our%20framework%20includes%20three%20key%20innovative%20contributions%3A%20First%2C%20we%0Apropose%20a%20mixture-of-experts%20framework%20with%20a%20routing%20distillation%20mechanism%0Athat%20enables%20experts%20to%20acquire%20concept-specific%20knowledge%20while%20preserving%20the%0Agating%20network%27s%20routing%20capability%2C%20thereby%20effectively%20mitigating%0Acatastrophic%20forgetting.%20Second%2C%20we%20propose%20a%20strategy%20for%20eliminating%0Aredundant%20layer-wise%20experts%20that%20reduces%20the%20number%20of%20expert%20parameters%20by%0Afully%20utilizing%20previously%20learned%20experts.%20Third%2C%20we%20employ%20a%20hierarchical%0Alocal%20attention-guided%20inference%20approach%20to%20mitigate%20interference%20between%0Agenerated%20visual%20concepts.%20Extensive%20experiments%20have%20demonstrated%20that%20our%0Amethod%20generates%20images%20with%20superior%20conceptual%20fidelity%20compared%20to%20the%0Astate-of-the-art%20%28SOTA%29%20method%2C%20achieving%20an%20impressive%2087.8%5C%25%20reduction%20in%0Aforgetting%20rates%20and%2063.3%5C%25%20fewer%20parameters%20on%20the%20CustomConcept%20101%20dataset.%0AOur%20code%20is%20available%20at%20%7Bhttps%3A//github.com/learninginvision/R2MoE%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13107v1&entry.124074799=Read"},
{"title": "A Progressive Image Restoration Network for High-order Degradation\n  Imaging in Remote Sensing", "author": "Yujie Feng and Yin Yang and Xiaohong Fan and Zhengpeng Zhang and Lijing Bu and Jianping Zhang", "abstract": "  Recently, deep learning methods have gained remarkable achievements in the\nfield of image restoration for remote sensing (RS). However, most existing RS\nimage restoration methods focus mainly on conventional first-order degradation\nmodels, which may not effectively capture the imaging mechanisms of remote\nsensing images. Furthermore, many RS image restoration approaches that use deep\nlearning are often criticized for their lacks of architecture transparency and\nmodel interpretability. To address these problems, we propose a novel\nprogressive restoration network for high-order degradation imaging (HDI-PRNet),\nto progressively restore different image degradation. HDI-PRNet is developed\nbased on the theoretical framework of degradation imaging, also Markov\nproperties of the high-order degradation process and Maximum a posteriori (MAP)\nestimation, offering the benefit of mathematical interpretability within the\nunfolding network. The framework is composed of three main components: a module\nfor image denoising that relies on proximal mapping prior learning, a module\nfor image deblurring that integrates Neumann series expansion with dual-domain\ndegradation learning, and a module for super-resolution. Extensive experiments\ndemonstrate that our method achieves superior performance on both synthetic and\nreal remote sensing images.\n", "link": "http://arxiv.org/abs/2412.07195v2", "date": "2025-07-17", "relevancy": 2.2064, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5571}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.554}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Progressive%20Image%20Restoration%20Network%20for%20High-order%20Degradation%0A%20%20Imaging%20in%20Remote%20Sensing&body=Title%3A%20A%20Progressive%20Image%20Restoration%20Network%20for%20High-order%20Degradation%0A%20%20Imaging%20in%20Remote%20Sensing%0AAuthor%3A%20Yujie%20Feng%20and%20Yin%20Yang%20and%20Xiaohong%20Fan%20and%20Zhengpeng%20Zhang%20and%20Lijing%20Bu%20and%20Jianping%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20deep%20learning%20methods%20have%20gained%20remarkable%20achievements%20in%20the%0Afield%20of%20image%20restoration%20for%20remote%20sensing%20%28RS%29.%20However%2C%20most%20existing%20RS%0Aimage%20restoration%20methods%20focus%20mainly%20on%20conventional%20first-order%20degradation%0Amodels%2C%20which%20may%20not%20effectively%20capture%20the%20imaging%20mechanisms%20of%20remote%0Asensing%20images.%20Furthermore%2C%20many%20RS%20image%20restoration%20approaches%20that%20use%20deep%0Alearning%20are%20often%20criticized%20for%20their%20lacks%20of%20architecture%20transparency%20and%0Amodel%20interpretability.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aprogressive%20restoration%20network%20for%20high-order%20degradation%20imaging%20%28HDI-PRNet%29%2C%0Ato%20progressively%20restore%20different%20image%20degradation.%20HDI-PRNet%20is%20developed%0Abased%20on%20the%20theoretical%20framework%20of%20degradation%20imaging%2C%20also%20Markov%0Aproperties%20of%20the%20high-order%20degradation%20process%20and%20Maximum%20a%20posteriori%20%28MAP%29%0Aestimation%2C%20offering%20the%20benefit%20of%20mathematical%20interpretability%20within%20the%0Aunfolding%20network.%20The%20framework%20is%20composed%20of%20three%20main%20components%3A%20a%20module%0Afor%20image%20denoising%20that%20relies%20on%20proximal%20mapping%20prior%20learning%2C%20a%20module%0Afor%20image%20deblurring%20that%20integrates%20Neumann%20series%20expansion%20with%20dual-domain%0Adegradation%20learning%2C%20and%20a%20module%20for%20super-resolution.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20superior%20performance%20on%20both%20synthetic%20and%0Areal%20remote%20sensing%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Progressive%2520Image%2520Restoration%2520Network%2520for%2520High-order%2520Degradation%250A%2520%2520Imaging%2520in%2520Remote%2520Sensing%26entry.906535625%3DYujie%2520Feng%2520and%2520Yin%2520Yang%2520and%2520Xiaohong%2520Fan%2520and%2520Zhengpeng%2520Zhang%2520and%2520Lijing%2520Bu%2520and%2520Jianping%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520learning%2520methods%2520have%2520gained%2520remarkable%2520achievements%2520in%2520the%250Afield%2520of%2520image%2520restoration%2520for%2520remote%2520sensing%2520%2528RS%2529.%2520However%252C%2520most%2520existing%2520RS%250Aimage%2520restoration%2520methods%2520focus%2520mainly%2520on%2520conventional%2520first-order%2520degradation%250Amodels%252C%2520which%2520may%2520not%2520effectively%2520capture%2520the%2520imaging%2520mechanisms%2520of%2520remote%250Asensing%2520images.%2520Furthermore%252C%2520many%2520RS%2520image%2520restoration%2520approaches%2520that%2520use%2520deep%250Alearning%2520are%2520often%2520criticized%2520for%2520their%2520lacks%2520of%2520architecture%2520transparency%2520and%250Amodel%2520interpretability.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%250Aprogressive%2520restoration%2520network%2520for%2520high-order%2520degradation%2520imaging%2520%2528HDI-PRNet%2529%252C%250Ato%2520progressively%2520restore%2520different%2520image%2520degradation.%2520HDI-PRNet%2520is%2520developed%250Abased%2520on%2520the%2520theoretical%2520framework%2520of%2520degradation%2520imaging%252C%2520also%2520Markov%250Aproperties%2520of%2520the%2520high-order%2520degradation%2520process%2520and%2520Maximum%2520a%2520posteriori%2520%2528MAP%2529%250Aestimation%252C%2520offering%2520the%2520benefit%2520of%2520mathematical%2520interpretability%2520within%2520the%250Aunfolding%2520network.%2520The%2520framework%2520is%2520composed%2520of%2520three%2520main%2520components%253A%2520a%2520module%250Afor%2520image%2520denoising%2520that%2520relies%2520on%2520proximal%2520mapping%2520prior%2520learning%252C%2520a%2520module%250Afor%2520image%2520deblurring%2520that%2520integrates%2520Neumann%2520series%2520expansion%2520with%2520dual-domain%250Adegradation%2520learning%252C%2520and%2520a%2520module%2520for%2520super-resolution.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%2520on%2520both%2520synthetic%2520and%250Areal%2520remote%2520sensing%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Progressive%20Image%20Restoration%20Network%20for%20High-order%20Degradation%0A%20%20Imaging%20in%20Remote%20Sensing&entry.906535625=Yujie%20Feng%20and%20Yin%20Yang%20and%20Xiaohong%20Fan%20and%20Zhengpeng%20Zhang%20and%20Lijing%20Bu%20and%20Jianping%20Zhang&entry.1292438233=%20%20Recently%2C%20deep%20learning%20methods%20have%20gained%20remarkable%20achievements%20in%20the%0Afield%20of%20image%20restoration%20for%20remote%20sensing%20%28RS%29.%20However%2C%20most%20existing%20RS%0Aimage%20restoration%20methods%20focus%20mainly%20on%20conventional%20first-order%20degradation%0Amodels%2C%20which%20may%20not%20effectively%20capture%20the%20imaging%20mechanisms%20of%20remote%0Asensing%20images.%20Furthermore%2C%20many%20RS%20image%20restoration%20approaches%20that%20use%20deep%0Alearning%20are%20often%20criticized%20for%20their%20lacks%20of%20architecture%20transparency%20and%0Amodel%20interpretability.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aprogressive%20restoration%20network%20for%20high-order%20degradation%20imaging%20%28HDI-PRNet%29%2C%0Ato%20progressively%20restore%20different%20image%20degradation.%20HDI-PRNet%20is%20developed%0Abased%20on%20the%20theoretical%20framework%20of%20degradation%20imaging%2C%20also%20Markov%0Aproperties%20of%20the%20high-order%20degradation%20process%20and%20Maximum%20a%20posteriori%20%28MAP%29%0Aestimation%2C%20offering%20the%20benefit%20of%20mathematical%20interpretability%20within%20the%0Aunfolding%20network.%20The%20framework%20is%20composed%20of%20three%20main%20components%3A%20a%20module%0Afor%20image%20denoising%20that%20relies%20on%20proximal%20mapping%20prior%20learning%2C%20a%20module%0Afor%20image%20deblurring%20that%20integrates%20Neumann%20series%20expansion%20with%20dual-domain%0Adegradation%20learning%2C%20and%20a%20module%20for%20super-resolution.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20superior%20performance%20on%20both%20synthetic%20and%0Areal%20remote%20sensing%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07195v2&entry.124074799=Read"},
{"title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World\n  Models", "author": "Arian Mousakhan and Sudhanshu Mittal and Silvio Galesso and Karim Farid and Thomas Brox", "abstract": "  Existing world models for autonomous driving struggle with long-horizon\ngeneration and generalization to challenging scenarios. In this work, we\ndevelop a model using simple design choices, and without additional supervision\nor sensors, such as maps, depth, or multiple cameras. We show that our model\nyields state-of-the-art performance, despite having only 469M parameters and\nbeing trained on 280h of video data. It particularly stands out in difficult\nscenarios like turning maneuvers and urban traffic. We test whether discrete\ntoken models possibly have advantages over continuous models based on flow\nmatching. To this end, we set up a hybrid tokenizer that is compatible with\nboth approaches and allows for a side-by-side comparison. Our study concludes\nin favor of the continuous autoregressive model, which is less brittle on\nindividual design choices and more powerful than the model built on discrete\ntokens. Code, models and qualitative results are publicly available at\nhttps://lmb-freiburg.github.io/orbis.github.io/.\n", "link": "http://arxiv.org/abs/2507.13162v1", "date": "2025-07-17", "relevancy": 2.2061, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5825}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orbis%3A%20Overcoming%20Challenges%20of%20Long-Horizon%20Prediction%20in%20Driving%20World%0A%20%20Models&body=Title%3A%20Orbis%3A%20Overcoming%20Challenges%20of%20Long-Horizon%20Prediction%20in%20Driving%20World%0A%20%20Models%0AAuthor%3A%20Arian%20Mousakhan%20and%20Sudhanshu%20Mittal%20and%20Silvio%20Galesso%20and%20Karim%20Farid%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Existing%20world%20models%20for%20autonomous%20driving%20struggle%20with%20long-horizon%0Ageneration%20and%20generalization%20to%20challenging%20scenarios.%20In%20this%20work%2C%20we%0Adevelop%20a%20model%20using%20simple%20design%20choices%2C%20and%20without%20additional%20supervision%0Aor%20sensors%2C%20such%20as%20maps%2C%20depth%2C%20or%20multiple%20cameras.%20We%20show%20that%20our%20model%0Ayields%20state-of-the-art%20performance%2C%20despite%20having%20only%20469M%20parameters%20and%0Abeing%20trained%20on%20280h%20of%20video%20data.%20It%20particularly%20stands%20out%20in%20difficult%0Ascenarios%20like%20turning%20maneuvers%20and%20urban%20traffic.%20We%20test%20whether%20discrete%0Atoken%20models%20possibly%20have%20advantages%20over%20continuous%20models%20based%20on%20flow%0Amatching.%20To%20this%20end%2C%20we%20set%20up%20a%20hybrid%20tokenizer%20that%20is%20compatible%20with%0Aboth%20approaches%20and%20allows%20for%20a%20side-by-side%20comparison.%20Our%20study%20concludes%0Ain%20favor%20of%20the%20continuous%20autoregressive%20model%2C%20which%20is%20less%20brittle%20on%0Aindividual%20design%20choices%20and%20more%20powerful%20than%20the%20model%20built%20on%20discrete%0Atokens.%20Code%2C%20models%20and%20qualitative%20results%20are%20publicly%20available%20at%0Ahttps%3A//lmb-freiburg.github.io/orbis.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrbis%253A%2520Overcoming%2520Challenges%2520of%2520Long-Horizon%2520Prediction%2520in%2520Driving%2520World%250A%2520%2520Models%26entry.906535625%3DArian%2520Mousakhan%2520and%2520Sudhanshu%2520Mittal%2520and%2520Silvio%2520Galesso%2520and%2520Karim%2520Farid%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Existing%2520world%2520models%2520for%2520autonomous%2520driving%2520struggle%2520with%2520long-horizon%250Ageneration%2520and%2520generalization%2520to%2520challenging%2520scenarios.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520a%2520model%2520using%2520simple%2520design%2520choices%252C%2520and%2520without%2520additional%2520supervision%250Aor%2520sensors%252C%2520such%2520as%2520maps%252C%2520depth%252C%2520or%2520multiple%2520cameras.%2520We%2520show%2520that%2520our%2520model%250Ayields%2520state-of-the-art%2520performance%252C%2520despite%2520having%2520only%2520469M%2520parameters%2520and%250Abeing%2520trained%2520on%2520280h%2520of%2520video%2520data.%2520It%2520particularly%2520stands%2520out%2520in%2520difficult%250Ascenarios%2520like%2520turning%2520maneuvers%2520and%2520urban%2520traffic.%2520We%2520test%2520whether%2520discrete%250Atoken%2520models%2520possibly%2520have%2520advantages%2520over%2520continuous%2520models%2520based%2520on%2520flow%250Amatching.%2520To%2520this%2520end%252C%2520we%2520set%2520up%2520a%2520hybrid%2520tokenizer%2520that%2520is%2520compatible%2520with%250Aboth%2520approaches%2520and%2520allows%2520for%2520a%2520side-by-side%2520comparison.%2520Our%2520study%2520concludes%250Ain%2520favor%2520of%2520the%2520continuous%2520autoregressive%2520model%252C%2520which%2520is%2520less%2520brittle%2520on%250Aindividual%2520design%2520choices%2520and%2520more%2520powerful%2520than%2520the%2520model%2520built%2520on%2520discrete%250Atokens.%2520Code%252C%2520models%2520and%2520qualitative%2520results%2520are%2520publicly%2520available%2520at%250Ahttps%253A//lmb-freiburg.github.io/orbis.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orbis%3A%20Overcoming%20Challenges%20of%20Long-Horizon%20Prediction%20in%20Driving%20World%0A%20%20Models&entry.906535625=Arian%20Mousakhan%20and%20Sudhanshu%20Mittal%20and%20Silvio%20Galesso%20and%20Karim%20Farid%20and%20Thomas%20Brox&entry.1292438233=%20%20Existing%20world%20models%20for%20autonomous%20driving%20struggle%20with%20long-horizon%0Ageneration%20and%20generalization%20to%20challenging%20scenarios.%20In%20this%20work%2C%20we%0Adevelop%20a%20model%20using%20simple%20design%20choices%2C%20and%20without%20additional%20supervision%0Aor%20sensors%2C%20such%20as%20maps%2C%20depth%2C%20or%20multiple%20cameras.%20We%20show%20that%20our%20model%0Ayields%20state-of-the-art%20performance%2C%20despite%20having%20only%20469M%20parameters%20and%0Abeing%20trained%20on%20280h%20of%20video%20data.%20It%20particularly%20stands%20out%20in%20difficult%0Ascenarios%20like%20turning%20maneuvers%20and%20urban%20traffic.%20We%20test%20whether%20discrete%0Atoken%20models%20possibly%20have%20advantages%20over%20continuous%20models%20based%20on%20flow%0Amatching.%20To%20this%20end%2C%20we%20set%20up%20a%20hybrid%20tokenizer%20that%20is%20compatible%20with%0Aboth%20approaches%20and%20allows%20for%20a%20side-by-side%20comparison.%20Our%20study%20concludes%0Ain%20favor%20of%20the%20continuous%20autoregressive%20model%2C%20which%20is%20less%20brittle%20on%0Aindividual%20design%20choices%20and%20more%20powerful%20than%20the%20model%20built%20on%20discrete%0Atokens.%20Code%2C%20models%20and%20qualitative%20results%20are%20publicly%20available%20at%0Ahttps%3A//lmb-freiburg.github.io/orbis.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13162v1&entry.124074799=Read"},
{"title": "Robustness Requirement Coverage using a Situation Coverage Approach for\n  Vision-based AI Systems", "author": "Sepeedeh Shahbeigi and Nawshin Mannan Proma and Victoria Hodge and Richard Hawkins and Boda Li and Valentina Donzella", "abstract": "  AI-based robots and vehicles are expected to operate safely in complex and\ndynamic environments, even in the presence of component degradation. In such\nsystems, perception relies on sensors such as cameras to capture environmental\ndata, which is then processed by AI models to support decision-making. However,\ndegradation in sensor performance directly impacts input data quality and can\nimpair AI inference. Specifying safety requirements for all possible sensor\ndegradation scenarios leads to unmanageable complexity and inevitable gaps. In\nthis position paper, we present a novel framework that integrates camera noise\nfactor identification with situation coverage analysis to systematically elicit\nrobustness-related safety requirements for AI-based perception systems. We\nfocus specifically on camera degradation in the automotive domain. Building on\nan existing framework for identifying degradation modes, we propose involving\ndomain, sensor, and safety experts, and incorporating Operational Design Domain\nspecifications to extend the degradation model by incorporating noise factors\nrelevant to AI performance. Situation coverage analysis is then applied to\nidentify representative operational contexts. This work marks an initial step\ntoward integrating noise factor analysis and situational coverage to support\nprincipled formulation and completeness assessment of robustness requirements\nfor camera-based AI perception.\n", "link": "http://arxiv.org/abs/2507.12986v1", "date": "2025-07-17", "relevancy": 2.1934, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5604}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20Requirement%20Coverage%20using%20a%20Situation%20Coverage%20Approach%20for%0A%20%20Vision-based%20AI%20Systems&body=Title%3A%20Robustness%20Requirement%20Coverage%20using%20a%20Situation%20Coverage%20Approach%20for%0A%20%20Vision-based%20AI%20Systems%0AAuthor%3A%20Sepeedeh%20Shahbeigi%20and%20Nawshin%20Mannan%20Proma%20and%20Victoria%20Hodge%20and%20Richard%20Hawkins%20and%20Boda%20Li%20and%20Valentina%20Donzella%0AAbstract%3A%20%20%20AI-based%20robots%20and%20vehicles%20are%20expected%20to%20operate%20safely%20in%20complex%20and%0Adynamic%20environments%2C%20even%20in%20the%20presence%20of%20component%20degradation.%20In%20such%0Asystems%2C%20perception%20relies%20on%20sensors%20such%20as%20cameras%20to%20capture%20environmental%0Adata%2C%20which%20is%20then%20processed%20by%20AI%20models%20to%20support%20decision-making.%20However%2C%0Adegradation%20in%20sensor%20performance%20directly%20impacts%20input%20data%20quality%20and%20can%0Aimpair%20AI%20inference.%20Specifying%20safety%20requirements%20for%20all%20possible%20sensor%0Adegradation%20scenarios%20leads%20to%20unmanageable%20complexity%20and%20inevitable%20gaps.%20In%0Athis%20position%20paper%2C%20we%20present%20a%20novel%20framework%20that%20integrates%20camera%20noise%0Afactor%20identification%20with%20situation%20coverage%20analysis%20to%20systematically%20elicit%0Arobustness-related%20safety%20requirements%20for%20AI-based%20perception%20systems.%20We%0Afocus%20specifically%20on%20camera%20degradation%20in%20the%20automotive%20domain.%20Building%20on%0Aan%20existing%20framework%20for%20identifying%20degradation%20modes%2C%20we%20propose%20involving%0Adomain%2C%20sensor%2C%20and%20safety%20experts%2C%20and%20incorporating%20Operational%20Design%20Domain%0Aspecifications%20to%20extend%20the%20degradation%20model%20by%20incorporating%20noise%20factors%0Arelevant%20to%20AI%20performance.%20Situation%20coverage%20analysis%20is%20then%20applied%20to%0Aidentify%20representative%20operational%20contexts.%20This%20work%20marks%20an%20initial%20step%0Atoward%20integrating%20noise%20factor%20analysis%20and%20situational%20coverage%20to%20support%0Aprincipled%20formulation%20and%20completeness%20assessment%20of%20robustness%20requirements%0Afor%20camera-based%20AI%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520Requirement%2520Coverage%2520using%2520a%2520Situation%2520Coverage%2520Approach%2520for%250A%2520%2520Vision-based%2520AI%2520Systems%26entry.906535625%3DSepeedeh%2520Shahbeigi%2520and%2520Nawshin%2520Mannan%2520Proma%2520and%2520Victoria%2520Hodge%2520and%2520Richard%2520Hawkins%2520and%2520Boda%2520Li%2520and%2520Valentina%2520Donzella%26entry.1292438233%3D%2520%2520AI-based%2520robots%2520and%2520vehicles%2520are%2520expected%2520to%2520operate%2520safely%2520in%2520complex%2520and%250Adynamic%2520environments%252C%2520even%2520in%2520the%2520presence%2520of%2520component%2520degradation.%2520In%2520such%250Asystems%252C%2520perception%2520relies%2520on%2520sensors%2520such%2520as%2520cameras%2520to%2520capture%2520environmental%250Adata%252C%2520which%2520is%2520then%2520processed%2520by%2520AI%2520models%2520to%2520support%2520decision-making.%2520However%252C%250Adegradation%2520in%2520sensor%2520performance%2520directly%2520impacts%2520input%2520data%2520quality%2520and%2520can%250Aimpair%2520AI%2520inference.%2520Specifying%2520safety%2520requirements%2520for%2520all%2520possible%2520sensor%250Adegradation%2520scenarios%2520leads%2520to%2520unmanageable%2520complexity%2520and%2520inevitable%2520gaps.%2520In%250Athis%2520position%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%2520that%2520integrates%2520camera%2520noise%250Afactor%2520identification%2520with%2520situation%2520coverage%2520analysis%2520to%2520systematically%2520elicit%250Arobustness-related%2520safety%2520requirements%2520for%2520AI-based%2520perception%2520systems.%2520We%250Afocus%2520specifically%2520on%2520camera%2520degradation%2520in%2520the%2520automotive%2520domain.%2520Building%2520on%250Aan%2520existing%2520framework%2520for%2520identifying%2520degradation%2520modes%252C%2520we%2520propose%2520involving%250Adomain%252C%2520sensor%252C%2520and%2520safety%2520experts%252C%2520and%2520incorporating%2520Operational%2520Design%2520Domain%250Aspecifications%2520to%2520extend%2520the%2520degradation%2520model%2520by%2520incorporating%2520noise%2520factors%250Arelevant%2520to%2520AI%2520performance.%2520Situation%2520coverage%2520analysis%2520is%2520then%2520applied%2520to%250Aidentify%2520representative%2520operational%2520contexts.%2520This%2520work%2520marks%2520an%2520initial%2520step%250Atoward%2520integrating%2520noise%2520factor%2520analysis%2520and%2520situational%2520coverage%2520to%2520support%250Aprincipled%2520formulation%2520and%2520completeness%2520assessment%2520of%2520robustness%2520requirements%250Afor%2520camera-based%2520AI%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20Requirement%20Coverage%20using%20a%20Situation%20Coverage%20Approach%20for%0A%20%20Vision-based%20AI%20Systems&entry.906535625=Sepeedeh%20Shahbeigi%20and%20Nawshin%20Mannan%20Proma%20and%20Victoria%20Hodge%20and%20Richard%20Hawkins%20and%20Boda%20Li%20and%20Valentina%20Donzella&entry.1292438233=%20%20AI-based%20robots%20and%20vehicles%20are%20expected%20to%20operate%20safely%20in%20complex%20and%0Adynamic%20environments%2C%20even%20in%20the%20presence%20of%20component%20degradation.%20In%20such%0Asystems%2C%20perception%20relies%20on%20sensors%20such%20as%20cameras%20to%20capture%20environmental%0Adata%2C%20which%20is%20then%20processed%20by%20AI%20models%20to%20support%20decision-making.%20However%2C%0Adegradation%20in%20sensor%20performance%20directly%20impacts%20input%20data%20quality%20and%20can%0Aimpair%20AI%20inference.%20Specifying%20safety%20requirements%20for%20all%20possible%20sensor%0Adegradation%20scenarios%20leads%20to%20unmanageable%20complexity%20and%20inevitable%20gaps.%20In%0Athis%20position%20paper%2C%20we%20present%20a%20novel%20framework%20that%20integrates%20camera%20noise%0Afactor%20identification%20with%20situation%20coverage%20analysis%20to%20systematically%20elicit%0Arobustness-related%20safety%20requirements%20for%20AI-based%20perception%20systems.%20We%0Afocus%20specifically%20on%20camera%20degradation%20in%20the%20automotive%20domain.%20Building%20on%0Aan%20existing%20framework%20for%20identifying%20degradation%20modes%2C%20we%20propose%20involving%0Adomain%2C%20sensor%2C%20and%20safety%20experts%2C%20and%20incorporating%20Operational%20Design%20Domain%0Aspecifications%20to%20extend%20the%20degradation%20model%20by%20incorporating%20noise%20factors%0Arelevant%20to%20AI%20performance.%20Situation%20coverage%20analysis%20is%20then%20applied%20to%0Aidentify%20representative%20operational%20contexts.%20This%20work%20marks%20an%20initial%20step%0Atoward%20integrating%20noise%20factor%20analysis%20and%20situational%20coverage%20to%20support%0Aprincipled%20formulation%20and%20completeness%20assessment%20of%20robustness%20requirements%0Afor%20camera-based%20AI%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12986v1&entry.124074799=Read"},
{"title": "SeaS: Few-shot Industrial Anomaly Image Generation with Separation and\n  Sharing Fine-tuning", "author": "Zhewei Dai and Shilei Zeng and Haotian Liu and Xurui Li and Feng Xue and Yu Zhou", "abstract": "  We introduce SeaS, a unified industrial generative model for automatically\ncreating diverse anomalies, authentic normal products, and precise anomaly\nmasks. While extensive research exists, most efforts either focus on specific\ntasks, i.e., anomalies or normal products only, or require separate models for\neach anomaly type. Consequently, prior methods either offer limited generative\ncapability or depend on a vast array of anomaly-specific models. We demonstrate\nthat U-Net's differentiated learning ability captures the distinct visual\ntraits of slightly-varied normal products and diverse anomalies, enabling us to\nconstruct a unified model for all tasks. Specifically, we first introduce an\nUnbalanced Abnormal (UA) Text Prompt, comprising one normal token and multiple\nanomaly tokens. More importantly, our Decoupled Anomaly Alignment (DA) loss\ndecouples anomaly attributes and binds them to distinct anomaly tokens of UA,\nenabling SeaS to create unseen anomalies by recombining these attributes.\nFurthermore, our Normal-image Alignment (NA) loss aligns the normal token to\nnormal patterns, making generated normal products globally consistent and\nlocally varied. Finally, SeaS produces accurate anomaly masks by fusing\ndiscriminative U-Net features with high-resolution VAE features. SeaS sets a\nnew benchmark for industrial generation, significantly enhancing downstream\napplications, with average improvements of $+8.66\\%$ pixel-level AP for\nsynthesis-based AD approaches, $+1.10\\%$ image-level AP for unsupervised AD\nmethods, and $+12.79\\%$ IoU for supervised segmentation models. Code is\navailable at\n\\href{https://github.com/HUST-SLOW/SeaS}{https://github.com/HUST-SLOW/SeaS}.\n", "link": "http://arxiv.org/abs/2410.14987v3", "date": "2025-07-17", "relevancy": 2.1759, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5596}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.545}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeaS%3A%20Few-shot%20Industrial%20Anomaly%20Image%20Generation%20with%20Separation%20and%0A%20%20Sharing%20Fine-tuning&body=Title%3A%20SeaS%3A%20Few-shot%20Industrial%20Anomaly%20Image%20Generation%20with%20Separation%20and%0A%20%20Sharing%20Fine-tuning%0AAuthor%3A%20Zhewei%20Dai%20and%20Shilei%20Zeng%20and%20Haotian%20Liu%20and%20Xurui%20Li%20and%20Feng%20Xue%20and%20Yu%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20SeaS%2C%20a%20unified%20industrial%20generative%20model%20for%20automatically%0Acreating%20diverse%20anomalies%2C%20authentic%20normal%20products%2C%20and%20precise%20anomaly%0Amasks.%20While%20extensive%20research%20exists%2C%20most%20efforts%20either%20focus%20on%20specific%0Atasks%2C%20i.e.%2C%20anomalies%20or%20normal%20products%20only%2C%20or%20require%20separate%20models%20for%0Aeach%20anomaly%20type.%20Consequently%2C%20prior%20methods%20either%20offer%20limited%20generative%0Acapability%20or%20depend%20on%20a%20vast%20array%20of%20anomaly-specific%20models.%20We%20demonstrate%0Athat%20U-Net%27s%20differentiated%20learning%20ability%20captures%20the%20distinct%20visual%0Atraits%20of%20slightly-varied%20normal%20products%20and%20diverse%20anomalies%2C%20enabling%20us%20to%0Aconstruct%20a%20unified%20model%20for%20all%20tasks.%20Specifically%2C%20we%20first%20introduce%20an%0AUnbalanced%20Abnormal%20%28UA%29%20Text%20Prompt%2C%20comprising%20one%20normal%20token%20and%20multiple%0Aanomaly%20tokens.%20More%20importantly%2C%20our%20Decoupled%20Anomaly%20Alignment%20%28DA%29%20loss%0Adecouples%20anomaly%20attributes%20and%20binds%20them%20to%20distinct%20anomaly%20tokens%20of%20UA%2C%0Aenabling%20SeaS%20to%20create%20unseen%20anomalies%20by%20recombining%20these%20attributes.%0AFurthermore%2C%20our%20Normal-image%20Alignment%20%28NA%29%20loss%20aligns%20the%20normal%20token%20to%0Anormal%20patterns%2C%20making%20generated%20normal%20products%20globally%20consistent%20and%0Alocally%20varied.%20Finally%2C%20SeaS%20produces%20accurate%20anomaly%20masks%20by%20fusing%0Adiscriminative%20U-Net%20features%20with%20high-resolution%20VAE%20features.%20SeaS%20sets%20a%0Anew%20benchmark%20for%20industrial%20generation%2C%20significantly%20enhancing%20downstream%0Aapplications%2C%20with%20average%20improvements%20of%20%24%2B8.66%5C%25%24%20pixel-level%20AP%20for%0Asynthesis-based%20AD%20approaches%2C%20%24%2B1.10%5C%25%24%20image-level%20AP%20for%20unsupervised%20AD%0Amethods%2C%20and%20%24%2B12.79%5C%25%24%20IoU%20for%20supervised%20segmentation%20models.%20Code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/HUST-SLOW/SeaS%7D%7Bhttps%3A//github.com/HUST-SLOW/SeaS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14987v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeaS%253A%2520Few-shot%2520Industrial%2520Anomaly%2520Image%2520Generation%2520with%2520Separation%2520and%250A%2520%2520Sharing%2520Fine-tuning%26entry.906535625%3DZhewei%2520Dai%2520and%2520Shilei%2520Zeng%2520and%2520Haotian%2520Liu%2520and%2520Xurui%2520Li%2520and%2520Feng%2520Xue%2520and%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520SeaS%252C%2520a%2520unified%2520industrial%2520generative%2520model%2520for%2520automatically%250Acreating%2520diverse%2520anomalies%252C%2520authentic%2520normal%2520products%252C%2520and%2520precise%2520anomaly%250Amasks.%2520While%2520extensive%2520research%2520exists%252C%2520most%2520efforts%2520either%2520focus%2520on%2520specific%250Atasks%252C%2520i.e.%252C%2520anomalies%2520or%2520normal%2520products%2520only%252C%2520or%2520require%2520separate%2520models%2520for%250Aeach%2520anomaly%2520type.%2520Consequently%252C%2520prior%2520methods%2520either%2520offer%2520limited%2520generative%250Acapability%2520or%2520depend%2520on%2520a%2520vast%2520array%2520of%2520anomaly-specific%2520models.%2520We%2520demonstrate%250Athat%2520U-Net%2527s%2520differentiated%2520learning%2520ability%2520captures%2520the%2520distinct%2520visual%250Atraits%2520of%2520slightly-varied%2520normal%2520products%2520and%2520diverse%2520anomalies%252C%2520enabling%2520us%2520to%250Aconstruct%2520a%2520unified%2520model%2520for%2520all%2520tasks.%2520Specifically%252C%2520we%2520first%2520introduce%2520an%250AUnbalanced%2520Abnormal%2520%2528UA%2529%2520Text%2520Prompt%252C%2520comprising%2520one%2520normal%2520token%2520and%2520multiple%250Aanomaly%2520tokens.%2520More%2520importantly%252C%2520our%2520Decoupled%2520Anomaly%2520Alignment%2520%2528DA%2529%2520loss%250Adecouples%2520anomaly%2520attributes%2520and%2520binds%2520them%2520to%2520distinct%2520anomaly%2520tokens%2520of%2520UA%252C%250Aenabling%2520SeaS%2520to%2520create%2520unseen%2520anomalies%2520by%2520recombining%2520these%2520attributes.%250AFurthermore%252C%2520our%2520Normal-image%2520Alignment%2520%2528NA%2529%2520loss%2520aligns%2520the%2520normal%2520token%2520to%250Anormal%2520patterns%252C%2520making%2520generated%2520normal%2520products%2520globally%2520consistent%2520and%250Alocally%2520varied.%2520Finally%252C%2520SeaS%2520produces%2520accurate%2520anomaly%2520masks%2520by%2520fusing%250Adiscriminative%2520U-Net%2520features%2520with%2520high-resolution%2520VAE%2520features.%2520SeaS%2520sets%2520a%250Anew%2520benchmark%2520for%2520industrial%2520generation%252C%2520significantly%2520enhancing%2520downstream%250Aapplications%252C%2520with%2520average%2520improvements%2520of%2520%2524%252B8.66%255C%2525%2524%2520pixel-level%2520AP%2520for%250Asynthesis-based%2520AD%2520approaches%252C%2520%2524%252B1.10%255C%2525%2524%2520image-level%2520AP%2520for%2520unsupervised%2520AD%250Amethods%252C%2520and%2520%2524%252B12.79%255C%2525%2524%2520IoU%2520for%2520supervised%2520segmentation%2520models.%2520Code%2520is%250Aavailable%2520at%250A%255Chref%257Bhttps%253A//github.com/HUST-SLOW/SeaS%257D%257Bhttps%253A//github.com/HUST-SLOW/SeaS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14987v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeaS%3A%20Few-shot%20Industrial%20Anomaly%20Image%20Generation%20with%20Separation%20and%0A%20%20Sharing%20Fine-tuning&entry.906535625=Zhewei%20Dai%20and%20Shilei%20Zeng%20and%20Haotian%20Liu%20and%20Xurui%20Li%20and%20Feng%20Xue%20and%20Yu%20Zhou&entry.1292438233=%20%20We%20introduce%20SeaS%2C%20a%20unified%20industrial%20generative%20model%20for%20automatically%0Acreating%20diverse%20anomalies%2C%20authentic%20normal%20products%2C%20and%20precise%20anomaly%0Amasks.%20While%20extensive%20research%20exists%2C%20most%20efforts%20either%20focus%20on%20specific%0Atasks%2C%20i.e.%2C%20anomalies%20or%20normal%20products%20only%2C%20or%20require%20separate%20models%20for%0Aeach%20anomaly%20type.%20Consequently%2C%20prior%20methods%20either%20offer%20limited%20generative%0Acapability%20or%20depend%20on%20a%20vast%20array%20of%20anomaly-specific%20models.%20We%20demonstrate%0Athat%20U-Net%27s%20differentiated%20learning%20ability%20captures%20the%20distinct%20visual%0Atraits%20of%20slightly-varied%20normal%20products%20and%20diverse%20anomalies%2C%20enabling%20us%20to%0Aconstruct%20a%20unified%20model%20for%20all%20tasks.%20Specifically%2C%20we%20first%20introduce%20an%0AUnbalanced%20Abnormal%20%28UA%29%20Text%20Prompt%2C%20comprising%20one%20normal%20token%20and%20multiple%0Aanomaly%20tokens.%20More%20importantly%2C%20our%20Decoupled%20Anomaly%20Alignment%20%28DA%29%20loss%0Adecouples%20anomaly%20attributes%20and%20binds%20them%20to%20distinct%20anomaly%20tokens%20of%20UA%2C%0Aenabling%20SeaS%20to%20create%20unseen%20anomalies%20by%20recombining%20these%20attributes.%0AFurthermore%2C%20our%20Normal-image%20Alignment%20%28NA%29%20loss%20aligns%20the%20normal%20token%20to%0Anormal%20patterns%2C%20making%20generated%20normal%20products%20globally%20consistent%20and%0Alocally%20varied.%20Finally%2C%20SeaS%20produces%20accurate%20anomaly%20masks%20by%20fusing%0Adiscriminative%20U-Net%20features%20with%20high-resolution%20VAE%20features.%20SeaS%20sets%20a%0Anew%20benchmark%20for%20industrial%20generation%2C%20significantly%20enhancing%20downstream%0Aapplications%2C%20with%20average%20improvements%20of%20%24%2B8.66%5C%25%24%20pixel-level%20AP%20for%0Asynthesis-based%20AD%20approaches%2C%20%24%2B1.10%5C%25%24%20image-level%20AP%20for%20unsupervised%20AD%0Amethods%2C%20and%20%24%2B12.79%5C%25%24%20IoU%20for%20supervised%20segmentation%20models.%20Code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/HUST-SLOW/SeaS%7D%7Bhttps%3A//github.com/HUST-SLOW/SeaS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14987v3&entry.124074799=Read"},
{"title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in\n  Industrial Domains", "author": "Antonio Finocchiaro and Alessandro Sebastiano Catinello and Michele Mazzamuto and Rosario Leonardi and Antonino Furnari and Giovanni Maria Farinella", "abstract": "  Hand-object interaction detection remains an open challenge in real-time\napplications, where intuitive user experiences depend on fast and accurate\ndetection of interactions with surrounding objects. We propose an efficient\napproach for detecting hand-objects interactions from streaming egocentric\nvision that operates in real time. Our approach consists of an action\nrecognition module and an object detection module for identifying active\nobjects upon confirmed interaction. Our Mamba model with EfficientNetV2 as\nbackbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark\nat 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.\nWe implement our models in a cascaded architecture where the action recognition\nand object detection modules operate sequentially. When the action recognition\npredicts a contact state, it activates the object detection module, which in\nturn performs inference on the relevant frame to detect and classify the active\nobject.\n", "link": "http://arxiv.org/abs/2507.13326v1", "date": "2025-07-17", "relevancy": 2.1705, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5485}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5431}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-Time%20System%20for%20Egocentric%20Hand-Object%20Interaction%20Detection%20in%0A%20%20Industrial%20Domains&body=Title%3A%20A%20Real-Time%20System%20for%20Egocentric%20Hand-Object%20Interaction%20Detection%20in%0A%20%20Industrial%20Domains%0AAuthor%3A%20Antonio%20Finocchiaro%20and%20Alessandro%20Sebastiano%20Catinello%20and%20Michele%20Mazzamuto%20and%20Rosario%20Leonardi%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20%20%20Hand-object%20interaction%20detection%20remains%20an%20open%20challenge%20in%20real-time%0Aapplications%2C%20where%20intuitive%20user%20experiences%20depend%20on%20fast%20and%20accurate%0Adetection%20of%20interactions%20with%20surrounding%20objects.%20We%20propose%20an%20efficient%0Aapproach%20for%20detecting%20hand-objects%20interactions%20from%20streaming%20egocentric%0Avision%20that%20operates%20in%20real%20time.%20Our%20approach%20consists%20of%20an%20action%0Arecognition%20module%20and%20an%20object%20detection%20module%20for%20identifying%20active%0Aobjects%20upon%20confirmed%20interaction.%20Our%20Mamba%20model%20with%20EfficientNetV2%20as%0Abackbone%20for%20action%20recognition%20achieves%2038.52%25%20p-AP%20on%20the%20ENIGMA-51%20benchmark%0Aat%2030fps%2C%20while%20our%20fine-tuned%20YOLOWorld%20reaches%2085.13%25%20AP%20for%20hand%20and%20object.%0AWe%20implement%20our%20models%20in%20a%20cascaded%20architecture%20where%20the%20action%20recognition%0Aand%20object%20detection%20modules%20operate%20sequentially.%20When%20the%20action%20recognition%0Apredicts%20a%20contact%20state%2C%20it%20activates%20the%20object%20detection%20module%2C%20which%20in%0Aturn%20performs%20inference%20on%20the%20relevant%20frame%20to%20detect%20and%20classify%20the%20active%0Aobject.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-Time%2520System%2520for%2520Egocentric%2520Hand-Object%2520Interaction%2520Detection%2520in%250A%2520%2520Industrial%2520Domains%26entry.906535625%3DAntonio%2520Finocchiaro%2520and%2520Alessandro%2520Sebastiano%2520Catinello%2520and%2520Michele%2520Mazzamuto%2520and%2520Rosario%2520Leonardi%2520and%2520Antonino%2520Furnari%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3D%2520%2520Hand-object%2520interaction%2520detection%2520remains%2520an%2520open%2520challenge%2520in%2520real-time%250Aapplications%252C%2520where%2520intuitive%2520user%2520experiences%2520depend%2520on%2520fast%2520and%2520accurate%250Adetection%2520of%2520interactions%2520with%2520surrounding%2520objects.%2520We%2520propose%2520an%2520efficient%250Aapproach%2520for%2520detecting%2520hand-objects%2520interactions%2520from%2520streaming%2520egocentric%250Avision%2520that%2520operates%2520in%2520real%2520time.%2520Our%2520approach%2520consists%2520of%2520an%2520action%250Arecognition%2520module%2520and%2520an%2520object%2520detection%2520module%2520for%2520identifying%2520active%250Aobjects%2520upon%2520confirmed%2520interaction.%2520Our%2520Mamba%2520model%2520with%2520EfficientNetV2%2520as%250Abackbone%2520for%2520action%2520recognition%2520achieves%252038.52%2525%2520p-AP%2520on%2520the%2520ENIGMA-51%2520benchmark%250Aat%252030fps%252C%2520while%2520our%2520fine-tuned%2520YOLOWorld%2520reaches%252085.13%2525%2520AP%2520for%2520hand%2520and%2520object.%250AWe%2520implement%2520our%2520models%2520in%2520a%2520cascaded%2520architecture%2520where%2520the%2520action%2520recognition%250Aand%2520object%2520detection%2520modules%2520operate%2520sequentially.%2520When%2520the%2520action%2520recognition%250Apredicts%2520a%2520contact%2520state%252C%2520it%2520activates%2520the%2520object%2520detection%2520module%252C%2520which%2520in%250Aturn%2520performs%2520inference%2520on%2520the%2520relevant%2520frame%2520to%2520detect%2520and%2520classify%2520the%2520active%250Aobject.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-Time%20System%20for%20Egocentric%20Hand-Object%20Interaction%20Detection%20in%0A%20%20Industrial%20Domains&entry.906535625=Antonio%20Finocchiaro%20and%20Alessandro%20Sebastiano%20Catinello%20and%20Michele%20Mazzamuto%20and%20Rosario%20Leonardi%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=%20%20Hand-object%20interaction%20detection%20remains%20an%20open%20challenge%20in%20real-time%0Aapplications%2C%20where%20intuitive%20user%20experiences%20depend%20on%20fast%20and%20accurate%0Adetection%20of%20interactions%20with%20surrounding%20objects.%20We%20propose%20an%20efficient%0Aapproach%20for%20detecting%20hand-objects%20interactions%20from%20streaming%20egocentric%0Avision%20that%20operates%20in%20real%20time.%20Our%20approach%20consists%20of%20an%20action%0Arecognition%20module%20and%20an%20object%20detection%20module%20for%20identifying%20active%0Aobjects%20upon%20confirmed%20interaction.%20Our%20Mamba%20model%20with%20EfficientNetV2%20as%0Abackbone%20for%20action%20recognition%20achieves%2038.52%25%20p-AP%20on%20the%20ENIGMA-51%20benchmark%0Aat%2030fps%2C%20while%20our%20fine-tuned%20YOLOWorld%20reaches%2085.13%25%20AP%20for%20hand%20and%20object.%0AWe%20implement%20our%20models%20in%20a%20cascaded%20architecture%20where%20the%20action%20recognition%0Aand%20object%20detection%20modules%20operate%20sequentially.%20When%20the%20action%20recognition%0Apredicts%20a%20contact%20state%2C%20it%20activates%20the%20object%20detection%20module%2C%20which%20in%0Aturn%20performs%20inference%20on%20the%20relevant%20frame%20to%20detect%20and%20classify%20the%20active%0Aobject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13326v1&entry.124074799=Read"},
{"title": "GLAD: Generalizable Tuning for Vision-Language Models", "author": "Yuqi Peng and Pengfei Wang and Jianzhuang Liu and Shifeng Chen", "abstract": "  Pre-trained vision-language models, such as CLIP, show impressive zero-shot\nrecognition ability and can be easily transferred to specific downstream tasks\nvia prompt tuning, even with limited training data. However, existing prompt\ntuning methods face two main challenges: (1) In few-shot scenarios, data\nscarcity often leads to overfitting, making the model sensitive to changes in\nthe input domain. (2) To mitigate overfitting, these methods typically rely on\ncomplex task-specific model architectures and sensitive hyperparameter tuning,\nseverely restricting their general applicability. To address these issues, we\npropose a simpler and more general framework called GLAD (Generalizable LoRA\ntuning with RegulArized GraDient). We show that merely applying LoRA achieves\nperformance in downstream tasks comparable to current state-of-the-art\nprompt-based methods. While LoRA is effective and easy to use, it remains\nsusceptible to overfitting in few-shot learning scenarios. To mitigate this\nrisk, we introduce a gradient-based regularization technique. This technique\neffectively steers the optimization trajectory, encouraging the model to find a\nmore stable parameter region that is robust to variations in data distribution.\nThrough extensive experiments conducted on 15 benchmark datasets, we\ndemonstrate that GLAD outperforms previous tuning approaches in terms of\nbase-to-novel class generalization, image domain generalization, and\ncross-dataset generalization. The code will be publicly available.\n", "link": "http://arxiv.org/abs/2507.13089v1", "date": "2025-07-17", "relevancy": 2.1624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5202}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLAD%3A%20Generalizable%20Tuning%20for%20Vision-Language%20Models&body=Title%3A%20GLAD%3A%20Generalizable%20Tuning%20for%20Vision-Language%20Models%0AAuthor%3A%20Yuqi%20Peng%20and%20Pengfei%20Wang%20and%20Jianzhuang%20Liu%20and%20Shifeng%20Chen%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20show%20impressive%20zero-shot%0Arecognition%20ability%20and%20can%20be%20easily%20transferred%20to%20specific%20downstream%20tasks%0Avia%20prompt%20tuning%2C%20even%20with%20limited%20training%20data.%20However%2C%20existing%20prompt%0Atuning%20methods%20face%20two%20main%20challenges%3A%20%281%29%20In%20few-shot%20scenarios%2C%20data%0Ascarcity%20often%20leads%20to%20overfitting%2C%20making%20the%20model%20sensitive%20to%20changes%20in%0Athe%20input%20domain.%20%282%29%20To%20mitigate%20overfitting%2C%20these%20methods%20typically%20rely%20on%0Acomplex%20task-specific%20model%20architectures%20and%20sensitive%20hyperparameter%20tuning%2C%0Aseverely%20restricting%20their%20general%20applicability.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20simpler%20and%20more%20general%20framework%20called%20GLAD%20%28Generalizable%20LoRA%0Atuning%20with%20RegulArized%20GraDient%29.%20We%20show%20that%20merely%20applying%20LoRA%20achieves%0Aperformance%20in%20downstream%20tasks%20comparable%20to%20current%20state-of-the-art%0Aprompt-based%20methods.%20While%20LoRA%20is%20effective%20and%20easy%20to%20use%2C%20it%20remains%0Asusceptible%20to%20overfitting%20in%20few-shot%20learning%20scenarios.%20To%20mitigate%20this%0Arisk%2C%20we%20introduce%20a%20gradient-based%20regularization%20technique.%20This%20technique%0Aeffectively%20steers%20the%20optimization%20trajectory%2C%20encouraging%20the%20model%20to%20find%20a%0Amore%20stable%20parameter%20region%20that%20is%20robust%20to%20variations%20in%20data%20distribution.%0AThrough%20extensive%20experiments%20conducted%20on%2015%20benchmark%20datasets%2C%20we%0Ademonstrate%20that%20GLAD%20outperforms%20previous%20tuning%20approaches%20in%20terms%20of%0Abase-to-novel%20class%20generalization%2C%20image%20domain%20generalization%2C%20and%0Across-dataset%20generalization.%20The%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLAD%253A%2520Generalizable%2520Tuning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DYuqi%2520Peng%2520and%2520Pengfei%2520Wang%2520and%2520Jianzhuang%2520Liu%2520and%2520Shifeng%2520Chen%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520show%2520impressive%2520zero-shot%250Arecognition%2520ability%2520and%2520can%2520be%2520easily%2520transferred%2520to%2520specific%2520downstream%2520tasks%250Avia%2520prompt%2520tuning%252C%2520even%2520with%2520limited%2520training%2520data.%2520However%252C%2520existing%2520prompt%250Atuning%2520methods%2520face%2520two%2520main%2520challenges%253A%2520%25281%2529%2520In%2520few-shot%2520scenarios%252C%2520data%250Ascarcity%2520often%2520leads%2520to%2520overfitting%252C%2520making%2520the%2520model%2520sensitive%2520to%2520changes%2520in%250Athe%2520input%2520domain.%2520%25282%2529%2520To%2520mitigate%2520overfitting%252C%2520these%2520methods%2520typically%2520rely%2520on%250Acomplex%2520task-specific%2520model%2520architectures%2520and%2520sensitive%2520hyperparameter%2520tuning%252C%250Aseverely%2520restricting%2520their%2520general%2520applicability.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520simpler%2520and%2520more%2520general%2520framework%2520called%2520GLAD%2520%2528Generalizable%2520LoRA%250Atuning%2520with%2520RegulArized%2520GraDient%2529.%2520We%2520show%2520that%2520merely%2520applying%2520LoRA%2520achieves%250Aperformance%2520in%2520downstream%2520tasks%2520comparable%2520to%2520current%2520state-of-the-art%250Aprompt-based%2520methods.%2520While%2520LoRA%2520is%2520effective%2520and%2520easy%2520to%2520use%252C%2520it%2520remains%250Asusceptible%2520to%2520overfitting%2520in%2520few-shot%2520learning%2520scenarios.%2520To%2520mitigate%2520this%250Arisk%252C%2520we%2520introduce%2520a%2520gradient-based%2520regularization%2520technique.%2520This%2520technique%250Aeffectively%2520steers%2520the%2520optimization%2520trajectory%252C%2520encouraging%2520the%2520model%2520to%2520find%2520a%250Amore%2520stable%2520parameter%2520region%2520that%2520is%2520robust%2520to%2520variations%2520in%2520data%2520distribution.%250AThrough%2520extensive%2520experiments%2520conducted%2520on%252015%2520benchmark%2520datasets%252C%2520we%250Ademonstrate%2520that%2520GLAD%2520outperforms%2520previous%2520tuning%2520approaches%2520in%2520terms%2520of%250Abase-to-novel%2520class%2520generalization%252C%2520image%2520domain%2520generalization%252C%2520and%250Across-dataset%2520generalization.%2520The%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLAD%3A%20Generalizable%20Tuning%20for%20Vision-Language%20Models&entry.906535625=Yuqi%20Peng%20and%20Pengfei%20Wang%20and%20Jianzhuang%20Liu%20and%20Shifeng%20Chen&entry.1292438233=%20%20Pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20show%20impressive%20zero-shot%0Arecognition%20ability%20and%20can%20be%20easily%20transferred%20to%20specific%20downstream%20tasks%0Avia%20prompt%20tuning%2C%20even%20with%20limited%20training%20data.%20However%2C%20existing%20prompt%0Atuning%20methods%20face%20two%20main%20challenges%3A%20%281%29%20In%20few-shot%20scenarios%2C%20data%0Ascarcity%20often%20leads%20to%20overfitting%2C%20making%20the%20model%20sensitive%20to%20changes%20in%0Athe%20input%20domain.%20%282%29%20To%20mitigate%20overfitting%2C%20these%20methods%20typically%20rely%20on%0Acomplex%20task-specific%20model%20architectures%20and%20sensitive%20hyperparameter%20tuning%2C%0Aseverely%20restricting%20their%20general%20applicability.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20simpler%20and%20more%20general%20framework%20called%20GLAD%20%28Generalizable%20LoRA%0Atuning%20with%20RegulArized%20GraDient%29.%20We%20show%20that%20merely%20applying%20LoRA%20achieves%0Aperformance%20in%20downstream%20tasks%20comparable%20to%20current%20state-of-the-art%0Aprompt-based%20methods.%20While%20LoRA%20is%20effective%20and%20easy%20to%20use%2C%20it%20remains%0Asusceptible%20to%20overfitting%20in%20few-shot%20learning%20scenarios.%20To%20mitigate%20this%0Arisk%2C%20we%20introduce%20a%20gradient-based%20regularization%20technique.%20This%20technique%0Aeffectively%20steers%20the%20optimization%20trajectory%2C%20encouraging%20the%20model%20to%20find%20a%0Amore%20stable%20parameter%20region%20that%20is%20robust%20to%20variations%20in%20data%20distribution.%0AThrough%20extensive%20experiments%20conducted%20on%2015%20benchmark%20datasets%2C%20we%0Ademonstrate%20that%20GLAD%20outperforms%20previous%20tuning%20approaches%20in%20terms%20of%0Abase-to-novel%20class%20generalization%2C%20image%20domain%20generalization%2C%20and%0Across-dataset%20generalization.%20The%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13089v1&entry.124074799=Read"},
{"title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced\n  AI Applications with Retrieval Augmented Generation and Knowledge Graphs", "author": "Irene Siragusa and Salvatore Contino and Massimo La Ciura and Rosario Alicata and Roberto Pirrone", "abstract": "  The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. In addition, the recent increase in Vision Language\nModels (VLM) leads to the need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding medical scans. This\npaper illustrates the entire workflow for building the MedPix 2.0 data set.\nStarting with the well-known multimodal data set\nMedPix\\textsuperscript{\\textregistered}, mainly used by physicians, nurses, and\nhealthcare students for Continuing Medical Education purposes, a semi-automatic\npipeline was developed to extract visual and textual data followed by a manual\ncuring procedure in which noisy samples were removed, thus creating a MongoDB\ndatabase. Along with the data set, we developed a Graphical User Interface\naimed at navigating efficiently the MongoDB instance and obtaining the raw data\nthat can be easily used for training and/or fine-tuning VLMs. To enforce this\npoint, in this work, we first recall DR-Minerva, a Retrieve Augmented\nGeneration-based VLM model trained upon MedPix 2.0. DR-Minerva predicts the\nbody part and the modality used to scan its input image. We also propose the\nextension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B,\nand leverages MedPix 2.0. The resulting architecture can be queried in a\nend-to-end manner, as a medical decision support system. MedPix 2.0 is\navailable on GitHub.\n", "link": "http://arxiv.org/abs/2407.02994v5", "date": "2025-07-17", "relevancy": 2.1608, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5644}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs&body=Title%3A%20MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs%0AAuthor%3A%20Irene%20Siragusa%20and%20Salvatore%20Contino%20and%20Massimo%20La%20Ciura%20and%20Rosario%20Alicata%20and%20Roberto%20Pirrone%0AAbstract%3A%20%20%20The%20increasing%20interest%20in%20developing%20Artificial%20Intelligence%20applications%20in%0Athe%20medical%20domain%2C%20suffers%20from%20the%20lack%20of%20high-quality%20data%20set%2C%20mainly%20due%0Ato%20privacy-related%20issues.%20In%20addition%2C%20the%20recent%20increase%20in%20Vision%20Language%0AModels%20%28VLM%29%20leads%20to%20the%20need%20for%20multimodal%20medical%20data%20sets%2C%20where%20clinical%0Areports%20and%20findings%20are%20attached%20to%20the%20corresponding%20medical%20scans.%20This%0Apaper%20illustrates%20the%20entire%20workflow%20for%20building%20the%20MedPix%202.0%20data%20set.%0AStarting%20with%20the%20well-known%20multimodal%20data%20set%0AMedPix%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20mainly%20used%20by%20physicians%2C%20nurses%2C%20and%0Ahealthcare%20students%20for%20Continuing%20Medical%20Education%20purposes%2C%20a%20semi-automatic%0Apipeline%20was%20developed%20to%20extract%20visual%20and%20textual%20data%20followed%20by%20a%20manual%0Acuring%20procedure%20in%20which%20noisy%20samples%20were%20removed%2C%20thus%20creating%20a%20MongoDB%0Adatabase.%20Along%20with%20the%20data%20set%2C%20we%20developed%20a%20Graphical%20User%20Interface%0Aaimed%20at%20navigating%20efficiently%20the%20MongoDB%20instance%20and%20obtaining%20the%20raw%20data%0Athat%20can%20be%20easily%20used%20for%20training%20and/or%20fine-tuning%20VLMs.%20To%20enforce%20this%0Apoint%2C%20in%20this%20work%2C%20we%20first%20recall%20DR-Minerva%2C%20a%20Retrieve%20Augmented%0AGeneration-based%20VLM%20model%20trained%20upon%20MedPix%202.0.%20DR-Minerva%20predicts%20the%0Abody%20part%20and%20the%20modality%20used%20to%20scan%20its%20input%20image.%20We%20also%20propose%20the%0Aextension%20of%20DR-Minerva%20with%20a%20Knowledge%20Graph%20that%20uses%20Llama%203.1%20Instruct%208B%2C%0Aand%20leverages%20MedPix%202.0.%20The%20resulting%20architecture%20can%20be%20queried%20in%20a%0Aend-to-end%20manner%2C%20as%20a%20medical%20decision%20support%20system.%20MedPix%202.0%20is%0Aavailable%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02994v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedPix%25202.0%253A%2520A%2520Comprehensive%2520Multimodal%2520Biomedical%2520Data%2520set%2520for%2520Advanced%250A%2520%2520AI%2520Applications%2520with%2520Retrieval%2520Augmented%2520Generation%2520and%2520Knowledge%2520Graphs%26entry.906535625%3DIrene%2520Siragusa%2520and%2520Salvatore%2520Contino%2520and%2520Massimo%2520La%2520Ciura%2520and%2520Rosario%2520Alicata%2520and%2520Roberto%2520Pirrone%26entry.1292438233%3D%2520%2520The%2520increasing%2520interest%2520in%2520developing%2520Artificial%2520Intelligence%2520applications%2520in%250Athe%2520medical%2520domain%252C%2520suffers%2520from%2520the%2520lack%2520of%2520high-quality%2520data%2520set%252C%2520mainly%2520due%250Ato%2520privacy-related%2520issues.%2520In%2520addition%252C%2520the%2520recent%2520increase%2520in%2520Vision%2520Language%250AModels%2520%2528VLM%2529%2520leads%2520to%2520the%2520need%2520for%2520multimodal%2520medical%2520data%2520sets%252C%2520where%2520clinical%250Areports%2520and%2520findings%2520are%2520attached%2520to%2520the%2520corresponding%2520medical%2520scans.%2520This%250Apaper%2520illustrates%2520the%2520entire%2520workflow%2520for%2520building%2520the%2520MedPix%25202.0%2520data%2520set.%250AStarting%2520with%2520the%2520well-known%2520multimodal%2520data%2520set%250AMedPix%255Ctextsuperscript%257B%255Ctextregistered%257D%252C%2520mainly%2520used%2520by%2520physicians%252C%2520nurses%252C%2520and%250Ahealthcare%2520students%2520for%2520Continuing%2520Medical%2520Education%2520purposes%252C%2520a%2520semi-automatic%250Apipeline%2520was%2520developed%2520to%2520extract%2520visual%2520and%2520textual%2520data%2520followed%2520by%2520a%2520manual%250Acuring%2520procedure%2520in%2520which%2520noisy%2520samples%2520were%2520removed%252C%2520thus%2520creating%2520a%2520MongoDB%250Adatabase.%2520Along%2520with%2520the%2520data%2520set%252C%2520we%2520developed%2520a%2520Graphical%2520User%2520Interface%250Aaimed%2520at%2520navigating%2520efficiently%2520the%2520MongoDB%2520instance%2520and%2520obtaining%2520the%2520raw%2520data%250Athat%2520can%2520be%2520easily%2520used%2520for%2520training%2520and/or%2520fine-tuning%2520VLMs.%2520To%2520enforce%2520this%250Apoint%252C%2520in%2520this%2520work%252C%2520we%2520first%2520recall%2520DR-Minerva%252C%2520a%2520Retrieve%2520Augmented%250AGeneration-based%2520VLM%2520model%2520trained%2520upon%2520MedPix%25202.0.%2520DR-Minerva%2520predicts%2520the%250Abody%2520part%2520and%2520the%2520modality%2520used%2520to%2520scan%2520its%2520input%2520image.%2520We%2520also%2520propose%2520the%250Aextension%2520of%2520DR-Minerva%2520with%2520a%2520Knowledge%2520Graph%2520that%2520uses%2520Llama%25203.1%2520Instruct%25208B%252C%250Aand%2520leverages%2520MedPix%25202.0.%2520The%2520resulting%2520architecture%2520can%2520be%2520queried%2520in%2520a%250Aend-to-end%2520manner%252C%2520as%2520a%2520medical%2520decision%2520support%2520system.%2520MedPix%25202.0%2520is%250Aavailable%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02994v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs&entry.906535625=Irene%20Siragusa%20and%20Salvatore%20Contino%20and%20Massimo%20La%20Ciura%20and%20Rosario%20Alicata%20and%20Roberto%20Pirrone&entry.1292438233=%20%20The%20increasing%20interest%20in%20developing%20Artificial%20Intelligence%20applications%20in%0Athe%20medical%20domain%2C%20suffers%20from%20the%20lack%20of%20high-quality%20data%20set%2C%20mainly%20due%0Ato%20privacy-related%20issues.%20In%20addition%2C%20the%20recent%20increase%20in%20Vision%20Language%0AModels%20%28VLM%29%20leads%20to%20the%20need%20for%20multimodal%20medical%20data%20sets%2C%20where%20clinical%0Areports%20and%20findings%20are%20attached%20to%20the%20corresponding%20medical%20scans.%20This%0Apaper%20illustrates%20the%20entire%20workflow%20for%20building%20the%20MedPix%202.0%20data%20set.%0AStarting%20with%20the%20well-known%20multimodal%20data%20set%0AMedPix%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20mainly%20used%20by%20physicians%2C%20nurses%2C%20and%0Ahealthcare%20students%20for%20Continuing%20Medical%20Education%20purposes%2C%20a%20semi-automatic%0Apipeline%20was%20developed%20to%20extract%20visual%20and%20textual%20data%20followed%20by%20a%20manual%0Acuring%20procedure%20in%20which%20noisy%20samples%20were%20removed%2C%20thus%20creating%20a%20MongoDB%0Adatabase.%20Along%20with%20the%20data%20set%2C%20we%20developed%20a%20Graphical%20User%20Interface%0Aaimed%20at%20navigating%20efficiently%20the%20MongoDB%20instance%20and%20obtaining%20the%20raw%20data%0Athat%20can%20be%20easily%20used%20for%20training%20and/or%20fine-tuning%20VLMs.%20To%20enforce%20this%0Apoint%2C%20in%20this%20work%2C%20we%20first%20recall%20DR-Minerva%2C%20a%20Retrieve%20Augmented%0AGeneration-based%20VLM%20model%20trained%20upon%20MedPix%202.0.%20DR-Minerva%20predicts%20the%0Abody%20part%20and%20the%20modality%20used%20to%20scan%20its%20input%20image.%20We%20also%20propose%20the%0Aextension%20of%20DR-Minerva%20with%20a%20Knowledge%20Graph%20that%20uses%20Llama%203.1%20Instruct%208B%2C%0Aand%20leverages%20MedPix%202.0.%20The%20resulting%20architecture%20can%20be%20queried%20in%20a%0Aend-to-end%20manner%2C%20as%20a%20medical%20decision%20support%20system.%20MedPix%202.0%20is%0Aavailable%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02994v5&entry.124074799=Read"},
{"title": "Prompt-driven Transferable Adversarial Attack on Person\n  Re-Identification with Attribute-aware Textual Inversion", "author": "Yuan Bian and Min Liu and Yunqi Yi and Xueping Wang and Yaonan Wang", "abstract": "  Person re-identification (re-id) models are vital in security surveillance\nsystems, requiring transferable adversarial attacks to explore the\nvulnerabilities of them. Recently, vision-language models (VLM) based attacks\nhave shown superior transferability by attacking generalized image and textual\nfeatures of VLM, but they lack comprehensive feature disruption due to the\noveremphasis on discriminative semantics in integral representation. In this\npaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel\nmethod that leverages VLM's image-text alignment capability to explicitly\ndisrupt fine-grained semantic features of pedestrian images by destroying\nattribute-specific textual embeddings. To obtain personalized textual\ndescriptions for individual attributes, textual inversion networks are designed\nto map pedestrian images to pseudo tokens that represent semantic embeddings,\ntrained in the contrastive learning manner with images and a predefined prompt\ntemplate that explicitly describes the pedestrian attributes. Inverted benign\nand adversarial fine-grained textual semantics facilitate attacker in\neffectively conducting thorough disruptions, enhancing the transferability of\nadversarial examples. Extensive experiments show that AP-Attack achieves\nstate-of-the-art transferability, significantly outperforming previous methods\nby 22.9% on mean Drop Rate in cross-model&dataset attack scenarios.\n", "link": "http://arxiv.org/abs/2502.19697v3", "date": "2025-07-17", "relevancy": 2.156, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5492}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-driven%20Transferable%20Adversarial%20Attack%20on%20Person%0A%20%20Re-Identification%20with%20Attribute-aware%20Textual%20Inversion&body=Title%3A%20Prompt-driven%20Transferable%20Adversarial%20Attack%20on%20Person%0A%20%20Re-Identification%20with%20Attribute-aware%20Textual%20Inversion%0AAuthor%3A%20Yuan%20Bian%20and%20Min%20Liu%20and%20Yunqi%20Yi%20and%20Xueping%20Wang%20and%20Yaonan%20Wang%0AAbstract%3A%20%20%20Person%20re-identification%20%28re-id%29%20models%20are%20vital%20in%20security%20surveillance%0Asystems%2C%20requiring%20transferable%20adversarial%20attacks%20to%20explore%20the%0Avulnerabilities%20of%20them.%20Recently%2C%20vision-language%20models%20%28VLM%29%20based%20attacks%0Ahave%20shown%20superior%20transferability%20by%20attacking%20generalized%20image%20and%20textual%0Afeatures%20of%20VLM%2C%20but%20they%20lack%20comprehensive%20feature%20disruption%20due%20to%20the%0Aoveremphasis%20on%20discriminative%20semantics%20in%20integral%20representation.%20In%20this%0Apaper%2C%20we%20introduce%20the%20Attribute-aware%20Prompt%20Attack%20%28AP-Attack%29%2C%20a%20novel%0Amethod%20that%20leverages%20VLM%27s%20image-text%20alignment%20capability%20to%20explicitly%0Adisrupt%20fine-grained%20semantic%20features%20of%20pedestrian%20images%20by%20destroying%0Aattribute-specific%20textual%20embeddings.%20To%20obtain%20personalized%20textual%0Adescriptions%20for%20individual%20attributes%2C%20textual%20inversion%20networks%20are%20designed%0Ato%20map%20pedestrian%20images%20to%20pseudo%20tokens%20that%20represent%20semantic%20embeddings%2C%0Atrained%20in%20the%20contrastive%20learning%20manner%20with%20images%20and%20a%20predefined%20prompt%0Atemplate%20that%20explicitly%20describes%20the%20pedestrian%20attributes.%20Inverted%20benign%0Aand%20adversarial%20fine-grained%20textual%20semantics%20facilitate%20attacker%20in%0Aeffectively%20conducting%20thorough%20disruptions%2C%20enhancing%20the%20transferability%20of%0Aadversarial%20examples.%20Extensive%20experiments%20show%20that%20AP-Attack%20achieves%0Astate-of-the-art%20transferability%2C%20significantly%20outperforming%20previous%20methods%0Aby%2022.9%25%20on%20mean%20Drop%20Rate%20in%20cross-model%26dataset%20attack%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19697v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-driven%2520Transferable%2520Adversarial%2520Attack%2520on%2520Person%250A%2520%2520Re-Identification%2520with%2520Attribute-aware%2520Textual%2520Inversion%26entry.906535625%3DYuan%2520Bian%2520and%2520Min%2520Liu%2520and%2520Yunqi%2520Yi%2520and%2520Xueping%2520Wang%2520and%2520Yaonan%2520Wang%26entry.1292438233%3D%2520%2520Person%2520re-identification%2520%2528re-id%2529%2520models%2520are%2520vital%2520in%2520security%2520surveillance%250Asystems%252C%2520requiring%2520transferable%2520adversarial%2520attacks%2520to%2520explore%2520the%250Avulnerabilities%2520of%2520them.%2520Recently%252C%2520vision-language%2520models%2520%2528VLM%2529%2520based%2520attacks%250Ahave%2520shown%2520superior%2520transferability%2520by%2520attacking%2520generalized%2520image%2520and%2520textual%250Afeatures%2520of%2520VLM%252C%2520but%2520they%2520lack%2520comprehensive%2520feature%2520disruption%2520due%2520to%2520the%250Aoveremphasis%2520on%2520discriminative%2520semantics%2520in%2520integral%2520representation.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%2520Attribute-aware%2520Prompt%2520Attack%2520%2528AP-Attack%2529%252C%2520a%2520novel%250Amethod%2520that%2520leverages%2520VLM%2527s%2520image-text%2520alignment%2520capability%2520to%2520explicitly%250Adisrupt%2520fine-grained%2520semantic%2520features%2520of%2520pedestrian%2520images%2520by%2520destroying%250Aattribute-specific%2520textual%2520embeddings.%2520To%2520obtain%2520personalized%2520textual%250Adescriptions%2520for%2520individual%2520attributes%252C%2520textual%2520inversion%2520networks%2520are%2520designed%250Ato%2520map%2520pedestrian%2520images%2520to%2520pseudo%2520tokens%2520that%2520represent%2520semantic%2520embeddings%252C%250Atrained%2520in%2520the%2520contrastive%2520learning%2520manner%2520with%2520images%2520and%2520a%2520predefined%2520prompt%250Atemplate%2520that%2520explicitly%2520describes%2520the%2520pedestrian%2520attributes.%2520Inverted%2520benign%250Aand%2520adversarial%2520fine-grained%2520textual%2520semantics%2520facilitate%2520attacker%2520in%250Aeffectively%2520conducting%2520thorough%2520disruptions%252C%2520enhancing%2520the%2520transferability%2520of%250Aadversarial%2520examples.%2520Extensive%2520experiments%2520show%2520that%2520AP-Attack%2520achieves%250Astate-of-the-art%2520transferability%252C%2520significantly%2520outperforming%2520previous%2520methods%250Aby%252022.9%2525%2520on%2520mean%2520Drop%2520Rate%2520in%2520cross-model%2526dataset%2520attack%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19697v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-driven%20Transferable%20Adversarial%20Attack%20on%20Person%0A%20%20Re-Identification%20with%20Attribute-aware%20Textual%20Inversion&entry.906535625=Yuan%20Bian%20and%20Min%20Liu%20and%20Yunqi%20Yi%20and%20Xueping%20Wang%20and%20Yaonan%20Wang&entry.1292438233=%20%20Person%20re-identification%20%28re-id%29%20models%20are%20vital%20in%20security%20surveillance%0Asystems%2C%20requiring%20transferable%20adversarial%20attacks%20to%20explore%20the%0Avulnerabilities%20of%20them.%20Recently%2C%20vision-language%20models%20%28VLM%29%20based%20attacks%0Ahave%20shown%20superior%20transferability%20by%20attacking%20generalized%20image%20and%20textual%0Afeatures%20of%20VLM%2C%20but%20they%20lack%20comprehensive%20feature%20disruption%20due%20to%20the%0Aoveremphasis%20on%20discriminative%20semantics%20in%20integral%20representation.%20In%20this%0Apaper%2C%20we%20introduce%20the%20Attribute-aware%20Prompt%20Attack%20%28AP-Attack%29%2C%20a%20novel%0Amethod%20that%20leverages%20VLM%27s%20image-text%20alignment%20capability%20to%20explicitly%0Adisrupt%20fine-grained%20semantic%20features%20of%20pedestrian%20images%20by%20destroying%0Aattribute-specific%20textual%20embeddings.%20To%20obtain%20personalized%20textual%0Adescriptions%20for%20individual%20attributes%2C%20textual%20inversion%20networks%20are%20designed%0Ato%20map%20pedestrian%20images%20to%20pseudo%20tokens%20that%20represent%20semantic%20embeddings%2C%0Atrained%20in%20the%20contrastive%20learning%20manner%20with%20images%20and%20a%20predefined%20prompt%0Atemplate%20that%20explicitly%20describes%20the%20pedestrian%20attributes.%20Inverted%20benign%0Aand%20adversarial%20fine-grained%20textual%20semantics%20facilitate%20attacker%20in%0Aeffectively%20conducting%20thorough%20disruptions%2C%20enhancing%20the%20transferability%20of%0Aadversarial%20examples.%20Extensive%20experiments%20show%20that%20AP-Attack%20achieves%0Astate-of-the-art%20transferability%2C%20significantly%20outperforming%20previous%20methods%0Aby%2022.9%25%20on%20mean%20Drop%20Rate%20in%20cross-model%26dataset%20attack%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19697v3&entry.124074799=Read"},
{"title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by\n  Approximately Orthogonal Fine-Tuning Strategy", "author": "Yiting Yang and Hao Luo and Yuan Sun and Qingsen Yan and Haokui Zhang and Wei Dong and Guoqing Wang and Peng Wang and Yang Yang and Hengtao Shen", "abstract": "  A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained\nVision Transformers (ViT) involves freezing the majority of the backbone\nparameters and solely learning low-rank adaptation weight matrices to\naccommodate downstream tasks. These low-rank matrices are commonly derived\nthrough the multiplication structure of down-projection and up-projection\nmatrices, exemplified by methods such as LoRA and Adapter. In this work, we\nobserve an approximate orthogonality among any two row or column vectors within\nany weight matrix of the backbone parameters; however, this property is absent\nin the vectors of the down/up-projection matrices. Approximate orthogonality\nimplies a reduction in the upper bound of the model's generalization error,\nsignifying that the model possesses enhanced generalization capability. If the\nfine-tuned down/up-projection matrices were to exhibit this same property as\nthe pre-trained backbone matrices, could the generalization capability of\nfine-tuned ViTs be further augmented? To address this question, we propose an\nApproximately Orthogonal Fine-Tuning (AOFT) strategy for representing the\nlow-rank weight matrices. This strategy employs a single learnable vector to\ngenerate a set of approximately orthogonal vectors, which form the\ndown/up-projection matrices, thereby aligning the properties of these matrices\nwith those of the backbone. Extensive experimental results demonstrate that our\nmethod achieves competitive performance across a range of downstream image\nclassification tasks, confirming the efficacy of the enhanced generalization\ncapability embedded in the down/up-projection matrices.\n", "link": "http://arxiv.org/abs/2507.13260v1", "date": "2025-07-17", "relevancy": 2.1535, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5534}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5302}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptation%20of%20Pre-trained%20Vision%20Transformer%20underpinned%20by%0A%20%20Approximately%20Orthogonal%20Fine-Tuning%20Strategy&body=Title%3A%20Efficient%20Adaptation%20of%20Pre-trained%20Vision%20Transformer%20underpinned%20by%0A%20%20Approximately%20Orthogonal%20Fine-Tuning%20Strategy%0AAuthor%3A%20Yiting%20Yang%20and%20Hao%20Luo%20and%20Yuan%20Sun%20and%20Qingsen%20Yan%20and%20Haokui%20Zhang%20and%20Wei%20Dong%20and%20Guoqing%20Wang%20and%20Peng%20Wang%20and%20Yang%20Yang%20and%20Hengtao%20Shen%0AAbstract%3A%20%20%20A%20prevalent%20approach%20in%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20of%20pre-trained%0AVision%20Transformers%20%28ViT%29%20involves%20freezing%20the%20majority%20of%20the%20backbone%0Aparameters%20and%20solely%20learning%20low-rank%20adaptation%20weight%20matrices%20to%0Aaccommodate%20downstream%20tasks.%20These%20low-rank%20matrices%20are%20commonly%20derived%0Athrough%20the%20multiplication%20structure%20of%20down-projection%20and%20up-projection%0Amatrices%2C%20exemplified%20by%20methods%20such%20as%20LoRA%20and%20Adapter.%20In%20this%20work%2C%20we%0Aobserve%20an%20approximate%20orthogonality%20among%20any%20two%20row%20or%20column%20vectors%20within%0Aany%20weight%20matrix%20of%20the%20backbone%20parameters%3B%20however%2C%20this%20property%20is%20absent%0Ain%20the%20vectors%20of%20the%20down/up-projection%20matrices.%20Approximate%20orthogonality%0Aimplies%20a%20reduction%20in%20the%20upper%20bound%20of%20the%20model%27s%20generalization%20error%2C%0Asignifying%20that%20the%20model%20possesses%20enhanced%20generalization%20capability.%20If%20the%0Afine-tuned%20down/up-projection%20matrices%20were%20to%20exhibit%20this%20same%20property%20as%0Athe%20pre-trained%20backbone%20matrices%2C%20could%20the%20generalization%20capability%20of%0Afine-tuned%20ViTs%20be%20further%20augmented%3F%20To%20address%20this%20question%2C%20we%20propose%20an%0AApproximately%20Orthogonal%20Fine-Tuning%20%28AOFT%29%20strategy%20for%20representing%20the%0Alow-rank%20weight%20matrices.%20This%20strategy%20employs%20a%20single%20learnable%20vector%20to%0Agenerate%20a%20set%20of%20approximately%20orthogonal%20vectors%2C%20which%20form%20the%0Adown/up-projection%20matrices%2C%20thereby%20aligning%20the%20properties%20of%20these%20matrices%0Awith%20those%20of%20the%20backbone.%20Extensive%20experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20performance%20across%20a%20range%20of%20downstream%20image%0Aclassification%20tasks%2C%20confirming%20the%20efficacy%20of%20the%20enhanced%20generalization%0Acapability%20embedded%20in%20the%20down/up-projection%20matrices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptation%2520of%2520Pre-trained%2520Vision%2520Transformer%2520underpinned%2520by%250A%2520%2520Approximately%2520Orthogonal%2520Fine-Tuning%2520Strategy%26entry.906535625%3DYiting%2520Yang%2520and%2520Hao%2520Luo%2520and%2520Yuan%2520Sun%2520and%2520Qingsen%2520Yan%2520and%2520Haokui%2520Zhang%2520and%2520Wei%2520Dong%2520and%2520Guoqing%2520Wang%2520and%2520Peng%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Hengtao%2520Shen%26entry.1292438233%3D%2520%2520A%2520prevalent%2520approach%2520in%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520of%2520pre-trained%250AVision%2520Transformers%2520%2528ViT%2529%2520involves%2520freezing%2520the%2520majority%2520of%2520the%2520backbone%250Aparameters%2520and%2520solely%2520learning%2520low-rank%2520adaptation%2520weight%2520matrices%2520to%250Aaccommodate%2520downstream%2520tasks.%2520These%2520low-rank%2520matrices%2520are%2520commonly%2520derived%250Athrough%2520the%2520multiplication%2520structure%2520of%2520down-projection%2520and%2520up-projection%250Amatrices%252C%2520exemplified%2520by%2520methods%2520such%2520as%2520LoRA%2520and%2520Adapter.%2520In%2520this%2520work%252C%2520we%250Aobserve%2520an%2520approximate%2520orthogonality%2520among%2520any%2520two%2520row%2520or%2520column%2520vectors%2520within%250Aany%2520weight%2520matrix%2520of%2520the%2520backbone%2520parameters%253B%2520however%252C%2520this%2520property%2520is%2520absent%250Ain%2520the%2520vectors%2520of%2520the%2520down/up-projection%2520matrices.%2520Approximate%2520orthogonality%250Aimplies%2520a%2520reduction%2520in%2520the%2520upper%2520bound%2520of%2520the%2520model%2527s%2520generalization%2520error%252C%250Asignifying%2520that%2520the%2520model%2520possesses%2520enhanced%2520generalization%2520capability.%2520If%2520the%250Afine-tuned%2520down/up-projection%2520matrices%2520were%2520to%2520exhibit%2520this%2520same%2520property%2520as%250Athe%2520pre-trained%2520backbone%2520matrices%252C%2520could%2520the%2520generalization%2520capability%2520of%250Afine-tuned%2520ViTs%2520be%2520further%2520augmented%253F%2520To%2520address%2520this%2520question%252C%2520we%2520propose%2520an%250AApproximately%2520Orthogonal%2520Fine-Tuning%2520%2528AOFT%2529%2520strategy%2520for%2520representing%2520the%250Alow-rank%2520weight%2520matrices.%2520This%2520strategy%2520employs%2520a%2520single%2520learnable%2520vector%2520to%250Agenerate%2520a%2520set%2520of%2520approximately%2520orthogonal%2520vectors%252C%2520which%2520form%2520the%250Adown/up-projection%2520matrices%252C%2520thereby%2520aligning%2520the%2520properties%2520of%2520these%2520matrices%250Awith%2520those%2520of%2520the%2520backbone.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520competitive%2520performance%2520across%2520a%2520range%2520of%2520downstream%2520image%250Aclassification%2520tasks%252C%2520confirming%2520the%2520efficacy%2520of%2520the%2520enhanced%2520generalization%250Acapability%2520embedded%2520in%2520the%2520down/up-projection%2520matrices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptation%20of%20Pre-trained%20Vision%20Transformer%20underpinned%20by%0A%20%20Approximately%20Orthogonal%20Fine-Tuning%20Strategy&entry.906535625=Yiting%20Yang%20and%20Hao%20Luo%20and%20Yuan%20Sun%20and%20Qingsen%20Yan%20and%20Haokui%20Zhang%20and%20Wei%20Dong%20and%20Guoqing%20Wang%20and%20Peng%20Wang%20and%20Yang%20Yang%20and%20Hengtao%20Shen&entry.1292438233=%20%20A%20prevalent%20approach%20in%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20of%20pre-trained%0AVision%20Transformers%20%28ViT%29%20involves%20freezing%20the%20majority%20of%20the%20backbone%0Aparameters%20and%20solely%20learning%20low-rank%20adaptation%20weight%20matrices%20to%0Aaccommodate%20downstream%20tasks.%20These%20low-rank%20matrices%20are%20commonly%20derived%0Athrough%20the%20multiplication%20structure%20of%20down-projection%20and%20up-projection%0Amatrices%2C%20exemplified%20by%20methods%20such%20as%20LoRA%20and%20Adapter.%20In%20this%20work%2C%20we%0Aobserve%20an%20approximate%20orthogonality%20among%20any%20two%20row%20or%20column%20vectors%20within%0Aany%20weight%20matrix%20of%20the%20backbone%20parameters%3B%20however%2C%20this%20property%20is%20absent%0Ain%20the%20vectors%20of%20the%20down/up-projection%20matrices.%20Approximate%20orthogonality%0Aimplies%20a%20reduction%20in%20the%20upper%20bound%20of%20the%20model%27s%20generalization%20error%2C%0Asignifying%20that%20the%20model%20possesses%20enhanced%20generalization%20capability.%20If%20the%0Afine-tuned%20down/up-projection%20matrices%20were%20to%20exhibit%20this%20same%20property%20as%0Athe%20pre-trained%20backbone%20matrices%2C%20could%20the%20generalization%20capability%20of%0Afine-tuned%20ViTs%20be%20further%20augmented%3F%20To%20address%20this%20question%2C%20we%20propose%20an%0AApproximately%20Orthogonal%20Fine-Tuning%20%28AOFT%29%20strategy%20for%20representing%20the%0Alow-rank%20weight%20matrices.%20This%20strategy%20employs%20a%20single%20learnable%20vector%20to%0Agenerate%20a%20set%20of%20approximately%20orthogonal%20vectors%2C%20which%20form%20the%0Adown/up-projection%20matrices%2C%20thereby%20aligning%20the%20properties%20of%20these%20matrices%0Awith%20those%20of%20the%20backbone.%20Extensive%20experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20performance%20across%20a%20range%20of%20downstream%20image%0Aclassification%20tasks%2C%20confirming%20the%20efficacy%20of%20the%20enhanced%20generalization%0Acapability%20embedded%20in%20the%20down/up-projection%20matrices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13260v1&entry.124074799=Read"},
{"title": "Leveraging Language Prior for Infrared Small Target Detection", "author": "Pranav Singh and Pravendra Singh", "abstract": "  IRSTD (InfraRed Small Target Detection) detects small targets in infrared\nblurry backgrounds and is essential for various applications. The detection\ntask is challenging due to the small size of the targets and their sparse\ndistribution in infrared small target datasets. Although existing IRSTD methods\nand datasets have led to significant advancements, they are limited by their\nreliance solely on the image modality. Recent advances in deep learning and\nlarge vision-language models have shown remarkable performance in various\nvisual recognition tasks. In this work, we propose a novel multimodal IRSTD\nframework that incorporates language priors to guide small target detection. We\nleverage language-guided attention weights derived from the language prior to\nenhance the model's ability for IRSTD, presenting a novel approach that\ncombines textual information with image data to improve IRSTD capabilities.\nUtilizing the state-of-the-art GPT-4 vision model, we generate text\ndescriptions that provide the locations of small targets in infrared images,\nemploying careful prompt engineering to ensure improved accuracy. Due to the\nabsence of multimodal IR datasets, existing IRSTD methods rely solely on image\ndata. To address this shortcoming, we have curated a multimodal infrared\ndataset that includes both image and text modalities for small target\ndetection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We\nvalidate the effectiveness of our approach through extensive experiments and\ncomprehensive ablation studies. The results demonstrate significant\nimprovements over the state-of-the-art method, with relative percentage\ndifferences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the\nNUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset\nof the LangIR dataset, respectively.\n", "link": "http://arxiv.org/abs/2507.13113v1", "date": "2025-07-17", "relevancy": 2.1483, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Language%20Prior%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Leveraging%20Language%20Prior%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Pranav%20Singh%20and%20Pravendra%20Singh%0AAbstract%3A%20%20%20IRSTD%20%28InfraRed%20Small%20Target%20Detection%29%20detects%20small%20targets%20in%20infrared%0Ablurry%20backgrounds%20and%20is%20essential%20for%20various%20applications.%20The%20detection%0Atask%20is%20challenging%20due%20to%20the%20small%20size%20of%20the%20targets%20and%20their%20sparse%0Adistribution%20in%20infrared%20small%20target%20datasets.%20Although%20existing%20IRSTD%20methods%0Aand%20datasets%20have%20led%20to%20significant%20advancements%2C%20they%20are%20limited%20by%20their%0Areliance%20solely%20on%20the%20image%20modality.%20Recent%20advances%20in%20deep%20learning%20and%0Alarge%20vision-language%20models%20have%20shown%20remarkable%20performance%20in%20various%0Avisual%20recognition%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20IRSTD%0Aframework%20that%20incorporates%20language%20priors%20to%20guide%20small%20target%20detection.%20We%0Aleverage%20language-guided%20attention%20weights%20derived%20from%20the%20language%20prior%20to%0Aenhance%20the%20model%27s%20ability%20for%20IRSTD%2C%20presenting%20a%20novel%20approach%20that%0Acombines%20textual%20information%20with%20image%20data%20to%20improve%20IRSTD%20capabilities.%0AUtilizing%20the%20state-of-the-art%20GPT-4%20vision%20model%2C%20we%20generate%20text%0Adescriptions%20that%20provide%20the%20locations%20of%20small%20targets%20in%20infrared%20images%2C%0Aemploying%20careful%20prompt%20engineering%20to%20ensure%20improved%20accuracy.%20Due%20to%20the%0Aabsence%20of%20multimodal%20IR%20datasets%2C%20existing%20IRSTD%20methods%20rely%20solely%20on%20image%0Adata.%20To%20address%20this%20shortcoming%2C%20we%20have%20curated%20a%20multimodal%20infrared%0Adataset%20that%20includes%20both%20image%20and%20text%20modalities%20for%20small%20target%0Adetection%2C%20expanding%20upon%20the%20popular%20IRSTD-1k%20and%20NUDT-SIRST%20datasets.%20We%0Avalidate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20experiments%20and%0Acomprehensive%20ablation%20studies.%20The%20results%20demonstrate%20significant%0Aimprovements%20over%20the%20state-of-the-art%20method%2C%20with%20relative%20percentage%0Adifferences%20of%209.74%25%2C%2013.02%25%2C%201.25%25%2C%20and%2067.87%25%20in%20IoU%2C%20nIoU%2C%20Pd%2C%20and%20Fa%20on%20the%0ANUAA-SIRST%20subset%2C%20and%204.41%25%2C%202.04%25%2C%202.01%25%2C%20and%20113.43%25%20on%20the%20IRSTD-1k%20subset%0Aof%20the%20LangIR%20dataset%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Language%2520Prior%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DPranav%2520Singh%2520and%2520Pravendra%2520Singh%26entry.1292438233%3D%2520%2520IRSTD%2520%2528InfraRed%2520Small%2520Target%2520Detection%2529%2520detects%2520small%2520targets%2520in%2520infrared%250Ablurry%2520backgrounds%2520and%2520is%2520essential%2520for%2520various%2520applications.%2520The%2520detection%250Atask%2520is%2520challenging%2520due%2520to%2520the%2520small%2520size%2520of%2520the%2520targets%2520and%2520their%2520sparse%250Adistribution%2520in%2520infrared%2520small%2520target%2520datasets.%2520Although%2520existing%2520IRSTD%2520methods%250Aand%2520datasets%2520have%2520led%2520to%2520significant%2520advancements%252C%2520they%2520are%2520limited%2520by%2520their%250Areliance%2520solely%2520on%2520the%2520image%2520modality.%2520Recent%2520advances%2520in%2520deep%2520learning%2520and%250Alarge%2520vision-language%2520models%2520have%2520shown%2520remarkable%2520performance%2520in%2520various%250Avisual%2520recognition%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520IRSTD%250Aframework%2520that%2520incorporates%2520language%2520priors%2520to%2520guide%2520small%2520target%2520detection.%2520We%250Aleverage%2520language-guided%2520attention%2520weights%2520derived%2520from%2520the%2520language%2520prior%2520to%250Aenhance%2520the%2520model%2527s%2520ability%2520for%2520IRSTD%252C%2520presenting%2520a%2520novel%2520approach%2520that%250Acombines%2520textual%2520information%2520with%2520image%2520data%2520to%2520improve%2520IRSTD%2520capabilities.%250AUtilizing%2520the%2520state-of-the-art%2520GPT-4%2520vision%2520model%252C%2520we%2520generate%2520text%250Adescriptions%2520that%2520provide%2520the%2520locations%2520of%2520small%2520targets%2520in%2520infrared%2520images%252C%250Aemploying%2520careful%2520prompt%2520engineering%2520to%2520ensure%2520improved%2520accuracy.%2520Due%2520to%2520the%250Aabsence%2520of%2520multimodal%2520IR%2520datasets%252C%2520existing%2520IRSTD%2520methods%2520rely%2520solely%2520on%2520image%250Adata.%2520To%2520address%2520this%2520shortcoming%252C%2520we%2520have%2520curated%2520a%2520multimodal%2520infrared%250Adataset%2520that%2520includes%2520both%2520image%2520and%2520text%2520modalities%2520for%2520small%2520target%250Adetection%252C%2520expanding%2520upon%2520the%2520popular%2520IRSTD-1k%2520and%2520NUDT-SIRST%2520datasets.%2520We%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520extensive%2520experiments%2520and%250Acomprehensive%2520ablation%2520studies.%2520The%2520results%2520demonstrate%2520significant%250Aimprovements%2520over%2520the%2520state-of-the-art%2520method%252C%2520with%2520relative%2520percentage%250Adifferences%2520of%25209.74%2525%252C%252013.02%2525%252C%25201.25%2525%252C%2520and%252067.87%2525%2520in%2520IoU%252C%2520nIoU%252C%2520Pd%252C%2520and%2520Fa%2520on%2520the%250ANUAA-SIRST%2520subset%252C%2520and%25204.41%2525%252C%25202.04%2525%252C%25202.01%2525%252C%2520and%2520113.43%2525%2520on%2520the%2520IRSTD-1k%2520subset%250Aof%2520the%2520LangIR%2520dataset%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Language%20Prior%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Pranav%20Singh%20and%20Pravendra%20Singh&entry.1292438233=%20%20IRSTD%20%28InfraRed%20Small%20Target%20Detection%29%20detects%20small%20targets%20in%20infrared%0Ablurry%20backgrounds%20and%20is%20essential%20for%20various%20applications.%20The%20detection%0Atask%20is%20challenging%20due%20to%20the%20small%20size%20of%20the%20targets%20and%20their%20sparse%0Adistribution%20in%20infrared%20small%20target%20datasets.%20Although%20existing%20IRSTD%20methods%0Aand%20datasets%20have%20led%20to%20significant%20advancements%2C%20they%20are%20limited%20by%20their%0Areliance%20solely%20on%20the%20image%20modality.%20Recent%20advances%20in%20deep%20learning%20and%0Alarge%20vision-language%20models%20have%20shown%20remarkable%20performance%20in%20various%0Avisual%20recognition%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20IRSTD%0Aframework%20that%20incorporates%20language%20priors%20to%20guide%20small%20target%20detection.%20We%0Aleverage%20language-guided%20attention%20weights%20derived%20from%20the%20language%20prior%20to%0Aenhance%20the%20model%27s%20ability%20for%20IRSTD%2C%20presenting%20a%20novel%20approach%20that%0Acombines%20textual%20information%20with%20image%20data%20to%20improve%20IRSTD%20capabilities.%0AUtilizing%20the%20state-of-the-art%20GPT-4%20vision%20model%2C%20we%20generate%20text%0Adescriptions%20that%20provide%20the%20locations%20of%20small%20targets%20in%20infrared%20images%2C%0Aemploying%20careful%20prompt%20engineering%20to%20ensure%20improved%20accuracy.%20Due%20to%20the%0Aabsence%20of%20multimodal%20IR%20datasets%2C%20existing%20IRSTD%20methods%20rely%20solely%20on%20image%0Adata.%20To%20address%20this%20shortcoming%2C%20we%20have%20curated%20a%20multimodal%20infrared%0Adataset%20that%20includes%20both%20image%20and%20text%20modalities%20for%20small%20target%0Adetection%2C%20expanding%20upon%20the%20popular%20IRSTD-1k%20and%20NUDT-SIRST%20datasets.%20We%0Avalidate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20experiments%20and%0Acomprehensive%20ablation%20studies.%20The%20results%20demonstrate%20significant%0Aimprovements%20over%20the%20state-of-the-art%20method%2C%20with%20relative%20percentage%0Adifferences%20of%209.74%25%2C%2013.02%25%2C%201.25%25%2C%20and%2067.87%25%20in%20IoU%2C%20nIoU%2C%20Pd%2C%20and%20Fa%20on%20the%0ANUAA-SIRST%20subset%2C%20and%204.41%25%2C%202.04%25%2C%202.01%25%2C%20and%20113.43%25%20on%20the%20IRSTD-1k%20subset%0Aof%20the%20LangIR%20dataset%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13113v1&entry.124074799=Read"},
{"title": "Fast Bilateral Teleoperation and Imitation Learning Using Sensorless\n  Force Control via Accurate Dynamics Model", "author": "Koki Yamane and Yunhan Li and Masashi Konosu and Koki Inami and Junji Oaki and Sho Sakaino and Toshiaki Tsuji", "abstract": "  In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.\n", "link": "http://arxiv.org/abs/2507.06174v3", "date": "2025-07-17", "relevancy": 2.1225, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5301}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&body=Title%3A%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model%0AAuthor%3A%20Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06174v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Bilateral%2520Teleoperation%2520and%2520Imitation%2520Learning%2520Using%2520Sensorless%250A%2520%2520Force%2520Control%2520via%2520Accurate%2520Dynamics%2520Model%26entry.906535625%3DKoki%2520Yamane%2520and%2520Yunhan%2520Li%2520and%2520Masashi%2520Konosu%2520and%2520Koki%2520Inami%2520and%2520Junji%2520Oaki%2520and%2520Sho%2520Sakaino%2520and%2520Toshiaki%2520Tsuji%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520advancement%2520of%2520imitation%2520learning%2520has%2520led%2520to%2520increased%250Ainterest%2520in%2520teleoperating%2520low-cost%2520manipulators%2520to%2520collect%2520demonstration%2520data.%250AHowever%252C%2520most%2520existing%2520systems%2520rely%2520on%2520unilateral%2520control%252C%2520which%2520only%2520transmits%250Atarget%2520position%2520values.%2520While%2520this%2520approach%2520is%2520easy%2520to%2520implement%2520and%2520suitable%250Afor%2520slow%252C%2520non-contact%2520tasks%252C%2520it%2520struggles%2520with%2520fast%2520or%2520contact-rich%2520operations%250Adue%2520to%2520the%2520absence%2520of%2520force%2520feedback.%2520This%2520work%2520demonstrates%2520that%2520fast%250Ateleoperation%2520with%2520force%2520feedback%2520is%2520feasible%2520even%2520with%2520force-sensorless%252C%250Alow-cost%2520manipulators%2520by%2520leveraging%25204-channel%2520bilateral%2520control.%2520Based%2520on%250Aaccurately%2520identified%2520manipulator%2520dynamics%252C%2520our%2520method%2520integrates%2520nonlinear%250Aterms%2520compensation%252C%2520velocity%2520and%2520external%2520force%2520estimation%252C%2520and%2520variable%2520gain%250Acorresponding%2520to%2520inertial%2520variation.%2520Furthermore%252C%2520using%2520data%2520collected%2520by%250A4-channel%2520bilateral%2520control%252C%2520we%2520show%2520that%2520incorporating%2520force%2520information%2520into%250Aboth%2520the%2520input%2520and%2520output%2520of%2520learned%2520policies%2520improves%2520performance%2520in%2520imitation%250Alearning.%2520These%2520results%2520highlight%2520the%2520practical%2520effectiveness%2520of%2520our%2520system%2520for%250Ahigh-fidelity%2520teleoperation%2520and%2520data%2520collection%2520on%2520affordable%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06174v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&entry.906535625=Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji&entry.1292438233=%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06174v3&entry.124074799=Read"},
{"title": "VITA: Vision-to-Action Flow Matching Policy", "author": "Dechen Gao and Boqi Zhao and Andrew Lee and Ian Chuang and Hanchu Zhou and Hang Wang and Zhe Zhao and Junshan Zhang and Iman Soltani", "abstract": "  We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks.\n", "link": "http://arxiv.org/abs/2507.13231v1", "date": "2025-07-17", "relevancy": 2.1151, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5367}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5233}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy&body=Title%3A%20VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy%0AAuthor%3A%20Dechen%20Gao%20and%20Boqi%20Zhao%20and%20Andrew%20Lee%20and%20Ian%20Chuang%20and%20Hanchu%20Zhou%20and%20Hang%20Wang%20and%20Zhe%20Zhao%20and%20Junshan%20Zhang%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20We%20present%20VITA%2C%20a%20Vision-To-Action%20flow%20matching%20policy%20that%20evolves%20latent%0Avisual%20representations%20into%20latent%20actions%20for%20visuomotor%20control.%20Traditional%0Aflow%20matching%20and%20diffusion%20policies%20sample%20from%20standard%20source%20distributions%0A%28e.g.%2C%20Gaussian%20noise%29%20and%20require%20additional%20conditioning%20mechanisms%20like%0Across-attention%20to%20condition%20action%20generation%20on%20visual%20information%2C%20creating%0Atime%20and%20space%20overheads.%20VITA%20proposes%20a%20novel%20paradigm%20that%20treats%20latent%0Aimages%20as%20the%20flow%20source%2C%20learning%20an%20inherent%20mapping%20from%20vision%20to%20action%0Awhile%20eliminating%20separate%20conditioning%20modules%20and%20preserving%20generative%0Amodeling%20capabilities.%20Learning%20flows%20between%20fundamentally%20different%0Amodalities%20like%20vision%20and%20action%20is%20challenging%20due%20to%20sparse%20action%20data%0Alacking%20semantic%20structures%20and%20dimensional%20mismatches%20between%20high-dimensional%0Avisual%20representations%20and%20raw%20actions.%20We%20address%20this%20by%20creating%20a%0Astructured%20action%20latent%20space%20via%20an%20autoencoder%20as%20the%20flow%20matching%20target%2C%0Aup-sampling%20raw%20actions%20to%20match%20visual%20representation%20shapes.%20Crucially%2C%20we%0Asupervise%20flow%20matching%20with%20both%20encoder%20targets%20and%20final%20action%20outputs%0Athrough%20flow%20latent%20decoding%2C%20which%20backpropagates%20action%20reconstruction%20loss%0Athrough%20sequential%20flow%20matching%20ODE%20solving%20steps%20for%20effective%20end-to-end%0Alearning.%20Implemented%20as%20simple%20MLP%20layers%2C%20VITA%20is%20evaluated%20on%20challenging%0Abi-manual%20manipulation%20tasks%20on%20the%20ALOHA%20platform%2C%20including%205%20simulation%20and%0A2%20real-world%20tasks.%20Despite%20its%20simplicity%2C%20MLP-only%20VITA%20outperforms%20or%0Amatches%20state-of-the-art%20generative%20policies%20while%20reducing%20inference%20latency%0Aby%2050-130%25%20compared%20to%20conventional%20flow%20matching%20policies%20requiring%20different%0Aconditioning%20mechanisms%20or%20complex%20architectures.%20To%20our%20knowledge%2C%20VITA%20is%20the%0Afirst%20MLP-only%20flow%20matching%20policy%20capable%20of%20solving%20complex%20bi-manual%0Amanipulation%20tasks%20like%20those%20in%20ALOHA%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA%253A%2520Vision-to-Action%2520Flow%2520Matching%2520Policy%26entry.906535625%3DDechen%2520Gao%2520and%2520Boqi%2520Zhao%2520and%2520Andrew%2520Lee%2520and%2520Ian%2520Chuang%2520and%2520Hanchu%2520Zhou%2520and%2520Hang%2520Wang%2520and%2520Zhe%2520Zhao%2520and%2520Junshan%2520Zhang%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520We%2520present%2520VITA%252C%2520a%2520Vision-To-Action%2520flow%2520matching%2520policy%2520that%2520evolves%2520latent%250Avisual%2520representations%2520into%2520latent%2520actions%2520for%2520visuomotor%2520control.%2520Traditional%250Aflow%2520matching%2520and%2520diffusion%2520policies%2520sample%2520from%2520standard%2520source%2520distributions%250A%2528e.g.%252C%2520Gaussian%2520noise%2529%2520and%2520require%2520additional%2520conditioning%2520mechanisms%2520like%250Across-attention%2520to%2520condition%2520action%2520generation%2520on%2520visual%2520information%252C%2520creating%250Atime%2520and%2520space%2520overheads.%2520VITA%2520proposes%2520a%2520novel%2520paradigm%2520that%2520treats%2520latent%250Aimages%2520as%2520the%2520flow%2520source%252C%2520learning%2520an%2520inherent%2520mapping%2520from%2520vision%2520to%2520action%250Awhile%2520eliminating%2520separate%2520conditioning%2520modules%2520and%2520preserving%2520generative%250Amodeling%2520capabilities.%2520Learning%2520flows%2520between%2520fundamentally%2520different%250Amodalities%2520like%2520vision%2520and%2520action%2520is%2520challenging%2520due%2520to%2520sparse%2520action%2520data%250Alacking%2520semantic%2520structures%2520and%2520dimensional%2520mismatches%2520between%2520high-dimensional%250Avisual%2520representations%2520and%2520raw%2520actions.%2520We%2520address%2520this%2520by%2520creating%2520a%250Astructured%2520action%2520latent%2520space%2520via%2520an%2520autoencoder%2520as%2520the%2520flow%2520matching%2520target%252C%250Aup-sampling%2520raw%2520actions%2520to%2520match%2520visual%2520representation%2520shapes.%2520Crucially%252C%2520we%250Asupervise%2520flow%2520matching%2520with%2520both%2520encoder%2520targets%2520and%2520final%2520action%2520outputs%250Athrough%2520flow%2520latent%2520decoding%252C%2520which%2520backpropagates%2520action%2520reconstruction%2520loss%250Athrough%2520sequential%2520flow%2520matching%2520ODE%2520solving%2520steps%2520for%2520effective%2520end-to-end%250Alearning.%2520Implemented%2520as%2520simple%2520MLP%2520layers%252C%2520VITA%2520is%2520evaluated%2520on%2520challenging%250Abi-manual%2520manipulation%2520tasks%2520on%2520the%2520ALOHA%2520platform%252C%2520including%25205%2520simulation%2520and%250A2%2520real-world%2520tasks.%2520Despite%2520its%2520simplicity%252C%2520MLP-only%2520VITA%2520outperforms%2520or%250Amatches%2520state-of-the-art%2520generative%2520policies%2520while%2520reducing%2520inference%2520latency%250Aby%252050-130%2525%2520compared%2520to%2520conventional%2520flow%2520matching%2520policies%2520requiring%2520different%250Aconditioning%2520mechanisms%2520or%2520complex%2520architectures.%2520To%2520our%2520knowledge%252C%2520VITA%2520is%2520the%250Afirst%2520MLP-only%2520flow%2520matching%2520policy%2520capable%2520of%2520solving%2520complex%2520bi-manual%250Amanipulation%2520tasks%2520like%2520those%2520in%2520ALOHA%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy&entry.906535625=Dechen%20Gao%20and%20Boqi%20Zhao%20and%20Andrew%20Lee%20and%20Ian%20Chuang%20and%20Hanchu%20Zhou%20and%20Hang%20Wang%20and%20Zhe%20Zhao%20and%20Junshan%20Zhang%20and%20Iman%20Soltani&entry.1292438233=%20%20We%20present%20VITA%2C%20a%20Vision-To-Action%20flow%20matching%20policy%20that%20evolves%20latent%0Avisual%20representations%20into%20latent%20actions%20for%20visuomotor%20control.%20Traditional%0Aflow%20matching%20and%20diffusion%20policies%20sample%20from%20standard%20source%20distributions%0A%28e.g.%2C%20Gaussian%20noise%29%20and%20require%20additional%20conditioning%20mechanisms%20like%0Across-attention%20to%20condition%20action%20generation%20on%20visual%20information%2C%20creating%0Atime%20and%20space%20overheads.%20VITA%20proposes%20a%20novel%20paradigm%20that%20treats%20latent%0Aimages%20as%20the%20flow%20source%2C%20learning%20an%20inherent%20mapping%20from%20vision%20to%20action%0Awhile%20eliminating%20separate%20conditioning%20modules%20and%20preserving%20generative%0Amodeling%20capabilities.%20Learning%20flows%20between%20fundamentally%20different%0Amodalities%20like%20vision%20and%20action%20is%20challenging%20due%20to%20sparse%20action%20data%0Alacking%20semantic%20structures%20and%20dimensional%20mismatches%20between%20high-dimensional%0Avisual%20representations%20and%20raw%20actions.%20We%20address%20this%20by%20creating%20a%0Astructured%20action%20latent%20space%20via%20an%20autoencoder%20as%20the%20flow%20matching%20target%2C%0Aup-sampling%20raw%20actions%20to%20match%20visual%20representation%20shapes.%20Crucially%2C%20we%0Asupervise%20flow%20matching%20with%20both%20encoder%20targets%20and%20final%20action%20outputs%0Athrough%20flow%20latent%20decoding%2C%20which%20backpropagates%20action%20reconstruction%20loss%0Athrough%20sequential%20flow%20matching%20ODE%20solving%20steps%20for%20effective%20end-to-end%0Alearning.%20Implemented%20as%20simple%20MLP%20layers%2C%20VITA%20is%20evaluated%20on%20challenging%0Abi-manual%20manipulation%20tasks%20on%20the%20ALOHA%20platform%2C%20including%205%20simulation%20and%0A2%20real-world%20tasks.%20Despite%20its%20simplicity%2C%20MLP-only%20VITA%20outperforms%20or%0Amatches%20state-of-the-art%20generative%20policies%20while%20reducing%20inference%20latency%0Aby%2050-130%25%20compared%20to%20conventional%20flow%20matching%20policies%20requiring%20different%0Aconditioning%20mechanisms%20or%20complex%20architectures.%20To%20our%20knowledge%2C%20VITA%20is%20the%0Afirst%20MLP-only%20flow%20matching%20policy%20capable%20of%20solving%20complex%20bi-manual%0Amanipulation%20tasks%20like%20those%20in%20ALOHA%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13231v1&entry.124074799=Read"},
{"title": "DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation", "author": "Ekta Balkrishna Gavas and Chinmay Hegde and Nasir Memon and Sudipta Banerjee", "abstract": "  Accurate age verification can protect underage users from unauthorized access\nto online platforms and e-commerce sites that provide age-restricted services.\nHowever, accurate age estimation can be confounded by several factors,\nincluding facial makeup that can induce changes to alter perceived identity and\nage to fool both humans and machines. In this work, we propose DiffClean which\nerases makeup traces using a text-guided diffusion model to defend against\nmakeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by\n4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines\non digitally simulated and real makeup images.\n", "link": "http://arxiv.org/abs/2507.13292v1", "date": "2025-07-17", "relevancy": 2.1102, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5334}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5278}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffClean%3A%20Diffusion-based%20Makeup%20Removal%20for%20Accurate%20Age%20Estimation&body=Title%3A%20DiffClean%3A%20Diffusion-based%20Makeup%20Removal%20for%20Accurate%20Age%20Estimation%0AAuthor%3A%20Ekta%20Balkrishna%20Gavas%20and%20Chinmay%20Hegde%20and%20Nasir%20Memon%20and%20Sudipta%20Banerjee%0AAbstract%3A%20%20%20Accurate%20age%20verification%20can%20protect%20underage%20users%20from%20unauthorized%20access%0Ato%20online%20platforms%20and%20e-commerce%20sites%20that%20provide%20age-restricted%20services.%0AHowever%2C%20accurate%20age%20estimation%20can%20be%20confounded%20by%20several%20factors%2C%0Aincluding%20facial%20makeup%20that%20can%20induce%20changes%20to%20alter%20perceived%20identity%20and%0Aage%20to%20fool%20both%20humans%20and%20machines.%20In%20this%20work%2C%20we%20propose%20DiffClean%20which%0Aerases%20makeup%20traces%20using%20a%20text-guided%20diffusion%20model%20to%20defend%20against%0Amakeup%20attacks.%20DiffClean%20improves%20age%20estimation%20%28minor%20vs.%20adult%20accuracy%20by%0A4.8%25%29%20and%20face%20verification%20%28TMR%20by%208.9%25%20at%20FMR%3D0.01%25%29%20over%20competing%20baselines%0Aon%20digitally%20simulated%20and%20real%20makeup%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffClean%253A%2520Diffusion-based%2520Makeup%2520Removal%2520for%2520Accurate%2520Age%2520Estimation%26entry.906535625%3DEkta%2520Balkrishna%2520Gavas%2520and%2520Chinmay%2520Hegde%2520and%2520Nasir%2520Memon%2520and%2520Sudipta%2520Banerjee%26entry.1292438233%3D%2520%2520Accurate%2520age%2520verification%2520can%2520protect%2520underage%2520users%2520from%2520unauthorized%2520access%250Ato%2520online%2520platforms%2520and%2520e-commerce%2520sites%2520that%2520provide%2520age-restricted%2520services.%250AHowever%252C%2520accurate%2520age%2520estimation%2520can%2520be%2520confounded%2520by%2520several%2520factors%252C%250Aincluding%2520facial%2520makeup%2520that%2520can%2520induce%2520changes%2520to%2520alter%2520perceived%2520identity%2520and%250Aage%2520to%2520fool%2520both%2520humans%2520and%2520machines.%2520In%2520this%2520work%252C%2520we%2520propose%2520DiffClean%2520which%250Aerases%2520makeup%2520traces%2520using%2520a%2520text-guided%2520diffusion%2520model%2520to%2520defend%2520against%250Amakeup%2520attacks.%2520DiffClean%2520improves%2520age%2520estimation%2520%2528minor%2520vs.%2520adult%2520accuracy%2520by%250A4.8%2525%2529%2520and%2520face%2520verification%2520%2528TMR%2520by%25208.9%2525%2520at%2520FMR%253D0.01%2525%2529%2520over%2520competing%2520baselines%250Aon%2520digitally%2520simulated%2520and%2520real%2520makeup%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffClean%3A%20Diffusion-based%20Makeup%20Removal%20for%20Accurate%20Age%20Estimation&entry.906535625=Ekta%20Balkrishna%20Gavas%20and%20Chinmay%20Hegde%20and%20Nasir%20Memon%20and%20Sudipta%20Banerjee&entry.1292438233=%20%20Accurate%20age%20verification%20can%20protect%20underage%20users%20from%20unauthorized%20access%0Ato%20online%20platforms%20and%20e-commerce%20sites%20that%20provide%20age-restricted%20services.%0AHowever%2C%20accurate%20age%20estimation%20can%20be%20confounded%20by%20several%20factors%2C%0Aincluding%20facial%20makeup%20that%20can%20induce%20changes%20to%20alter%20perceived%20identity%20and%0Aage%20to%20fool%20both%20humans%20and%20machines.%20In%20this%20work%2C%20we%20propose%20DiffClean%20which%0Aerases%20makeup%20traces%20using%20a%20text-guided%20diffusion%20model%20to%20defend%20against%0Amakeup%20attacks.%20DiffClean%20improves%20age%20estimation%20%28minor%20vs.%20adult%20accuracy%20by%0A4.8%25%29%20and%20face%20verification%20%28TMR%20by%208.9%25%20at%20FMR%3D0.01%25%29%20over%20competing%20baselines%0Aon%20digitally%20simulated%20and%20real%20makeup%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13292v1&entry.124074799=Read"},
{"title": "(Almost) Free Modality Stitching of Foundation Models", "author": "Jaisidh Singh and Diganta Misra and Boris Knyazev and Antonio Orvieto", "abstract": "  Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\ntext model. This stitching process is performed by training a connector module\nthat aims to align the representation spaces of these uni-modal models towards\na multi-modal objective. However, given the complexity of training such\nconnectors on large scale web-based datasets coupled with the ever-increasing\nnumber of available pretrained uni-modal models, the task of uni-modal models\nselection and subsequent connector module training becomes computationally\ndemanding. To address this under-studied critical problem, we propose\nHypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal\nuni-modal model selection and connector training by leveraging hypernetworks.\nSpecifically, our framework utilizes the parameter prediction capability of a\nhypernetwork to obtain jointly trained connector modules for $N \\times M$\ncombinations of uni-modal models. In our experiments, Hyma reduces the cost of\nsearching for the best performing uni-modal model pair by $10\\times$, while\nmatching the ranking and trained connector performance obtained via grid search\nacross a suite of diverse multi-modal benchmarks.\n", "link": "http://arxiv.org/abs/2507.10015v3", "date": "2025-07-17", "relevancy": 2.1094, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%28Almost%29%20Free%20Modality%20Stitching%20of%20Foundation%20Models&body=Title%3A%20%28Almost%29%20Free%20Modality%20Stitching%20of%20Foundation%20Models%0AAuthor%3A%20Jaisidh%20Singh%20and%20Diganta%20Misra%20and%20Boris%20Knyazev%20and%20Antonio%20Orvieto%0AAbstract%3A%20%20%20Foundation%20multi-modal%20models%20are%20often%20designed%20by%20stitching%20of%20multiple%0Aexisting%20pretrained%20uni-modal%20models%3A%20for%20example%2C%20an%20image%20classifier%20with%20an%0Atext%20model.%20This%20stitching%20process%20is%20performed%20by%20training%20a%20connector%20module%0Athat%20aims%20to%20align%20the%20representation%20spaces%20of%20these%20uni-modal%20models%20towards%0Aa%20multi-modal%20objective.%20However%2C%20given%20the%20complexity%20of%20training%20such%0Aconnectors%20on%20large%20scale%20web-based%20datasets%20coupled%20with%20the%20ever-increasing%0Anumber%20of%20available%20pretrained%20uni-modal%20models%2C%20the%20task%20of%20uni-modal%20models%0Aselection%20and%20subsequent%20connector%20module%20training%20becomes%20computationally%0Ademanding.%20To%20address%20this%20under-studied%20critical%20problem%2C%20we%20propose%0AHypernetwork%20Model%20Alignment%20%28Hyma%29%2C%20a%20novel%20all-in-one%20solution%20for%20optimal%0Auni-modal%20model%20selection%20and%20connector%20training%20by%20leveraging%20hypernetworks.%0ASpecifically%2C%20our%20framework%20utilizes%20the%20parameter%20prediction%20capability%20of%20a%0Ahypernetwork%20to%20obtain%20jointly%20trained%20connector%20modules%20for%20%24N%20%5Ctimes%20M%24%0Acombinations%20of%20uni-modal%20models.%20In%20our%20experiments%2C%20Hyma%20reduces%20the%20cost%20of%0Asearching%20for%20the%20best%20performing%20uni-modal%20model%20pair%20by%20%2410%5Ctimes%24%2C%20while%0Amatching%20the%20ranking%20and%20trained%20connector%20performance%20obtained%20via%20grid%20search%0Aacross%20a%20suite%20of%20diverse%20multi-modal%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10015v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2528Almost%2529%2520Free%2520Modality%2520Stitching%2520of%2520Foundation%2520Models%26entry.906535625%3DJaisidh%2520Singh%2520and%2520Diganta%2520Misra%2520and%2520Boris%2520Knyazev%2520and%2520Antonio%2520Orvieto%26entry.1292438233%3D%2520%2520Foundation%2520multi-modal%2520models%2520are%2520often%2520designed%2520by%2520stitching%2520of%2520multiple%250Aexisting%2520pretrained%2520uni-modal%2520models%253A%2520for%2520example%252C%2520an%2520image%2520classifier%2520with%2520an%250Atext%2520model.%2520This%2520stitching%2520process%2520is%2520performed%2520by%2520training%2520a%2520connector%2520module%250Athat%2520aims%2520to%2520align%2520the%2520representation%2520spaces%2520of%2520these%2520uni-modal%2520models%2520towards%250Aa%2520multi-modal%2520objective.%2520However%252C%2520given%2520the%2520complexity%2520of%2520training%2520such%250Aconnectors%2520on%2520large%2520scale%2520web-based%2520datasets%2520coupled%2520with%2520the%2520ever-increasing%250Anumber%2520of%2520available%2520pretrained%2520uni-modal%2520models%252C%2520the%2520task%2520of%2520uni-modal%2520models%250Aselection%2520and%2520subsequent%2520connector%2520module%2520training%2520becomes%2520computationally%250Ademanding.%2520To%2520address%2520this%2520under-studied%2520critical%2520problem%252C%2520we%2520propose%250AHypernetwork%2520Model%2520Alignment%2520%2528Hyma%2529%252C%2520a%2520novel%2520all-in-one%2520solution%2520for%2520optimal%250Auni-modal%2520model%2520selection%2520and%2520connector%2520training%2520by%2520leveraging%2520hypernetworks.%250ASpecifically%252C%2520our%2520framework%2520utilizes%2520the%2520parameter%2520prediction%2520capability%2520of%2520a%250Ahypernetwork%2520to%2520obtain%2520jointly%2520trained%2520connector%2520modules%2520for%2520%2524N%2520%255Ctimes%2520M%2524%250Acombinations%2520of%2520uni-modal%2520models.%2520In%2520our%2520experiments%252C%2520Hyma%2520reduces%2520the%2520cost%2520of%250Asearching%2520for%2520the%2520best%2520performing%2520uni-modal%2520model%2520pair%2520by%2520%252410%255Ctimes%2524%252C%2520while%250Amatching%2520the%2520ranking%2520and%2520trained%2520connector%2520performance%2520obtained%2520via%2520grid%2520search%250Aacross%2520a%2520suite%2520of%2520diverse%2520multi-modal%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10015v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%28Almost%29%20Free%20Modality%20Stitching%20of%20Foundation%20Models&entry.906535625=Jaisidh%20Singh%20and%20Diganta%20Misra%20and%20Boris%20Knyazev%20and%20Antonio%20Orvieto&entry.1292438233=%20%20Foundation%20multi-modal%20models%20are%20often%20designed%20by%20stitching%20of%20multiple%0Aexisting%20pretrained%20uni-modal%20models%3A%20for%20example%2C%20an%20image%20classifier%20with%20an%0Atext%20model.%20This%20stitching%20process%20is%20performed%20by%20training%20a%20connector%20module%0Athat%20aims%20to%20align%20the%20representation%20spaces%20of%20these%20uni-modal%20models%20towards%0Aa%20multi-modal%20objective.%20However%2C%20given%20the%20complexity%20of%20training%20such%0Aconnectors%20on%20large%20scale%20web-based%20datasets%20coupled%20with%20the%20ever-increasing%0Anumber%20of%20available%20pretrained%20uni-modal%20models%2C%20the%20task%20of%20uni-modal%20models%0Aselection%20and%20subsequent%20connector%20module%20training%20becomes%20computationally%0Ademanding.%20To%20address%20this%20under-studied%20critical%20problem%2C%20we%20propose%0AHypernetwork%20Model%20Alignment%20%28Hyma%29%2C%20a%20novel%20all-in-one%20solution%20for%20optimal%0Auni-modal%20model%20selection%20and%20connector%20training%20by%20leveraging%20hypernetworks.%0ASpecifically%2C%20our%20framework%20utilizes%20the%20parameter%20prediction%20capability%20of%20a%0Ahypernetwork%20to%20obtain%20jointly%20trained%20connector%20modules%20for%20%24N%20%5Ctimes%20M%24%0Acombinations%20of%20uni-modal%20models.%20In%20our%20experiments%2C%20Hyma%20reduces%20the%20cost%20of%0Asearching%20for%20the%20best%20performing%20uni-modal%20model%20pair%20by%20%2410%5Ctimes%24%2C%20while%0Amatching%20the%20ranking%20and%20trained%20connector%20performance%20obtained%20via%20grid%20search%0Aacross%20a%20suite%20of%20diverse%20multi-modal%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10015v3&entry.124074799=Read"},
{"title": "The late-stage training dynamics of (stochastic) subgradient descent on\n  homogeneous neural networks", "author": "Sholom Schechtman and Nicolas Schreuder", "abstract": "  We analyze the implicit bias of constant step stochastic subgradient descent\n(SGD). We consider the setting of binary classification with homogeneous neural\nnetworks - a large class of deep neural networks with ReLU-type activation\nfunctions such as MLPs and CNNs without biases. We interpret the dynamics of\nnormalized SGD iterates as an Euler-like discretization of a conservative field\nflow that is naturally associated to the normalized classification margin.\nOwing to this interpretation, we show that normalized SGD iterates converge to\nthe set of critical points of the normalized margin at late-stage training\n(i.e., assuming that the data is correctly classified with positive normalized\nmargin). Up to our knowledge, this is the first extension of the analysis of\nLyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth\nand stochastic setting. Our main result applies to binary classification with\nexponential or logistic losses. We additionally discuss extensions to more\ngeneral settings.\n", "link": "http://arxiv.org/abs/2502.05668v3", "date": "2025-07-17", "relevancy": 2.1009, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5737}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20late-stage%20training%20dynamics%20of%20%28stochastic%29%20subgradient%20descent%20on%0A%20%20homogeneous%20neural%20networks&body=Title%3A%20The%20late-stage%20training%20dynamics%20of%20%28stochastic%29%20subgradient%20descent%20on%0A%20%20homogeneous%20neural%20networks%0AAuthor%3A%20Sholom%20Schechtman%20and%20Nicolas%20Schreuder%0AAbstract%3A%20%20%20We%20analyze%20the%20implicit%20bias%20of%20constant%20step%20stochastic%20subgradient%20descent%0A%28SGD%29.%20We%20consider%20the%20setting%20of%20binary%20classification%20with%20homogeneous%20neural%0Anetworks%20-%20a%20large%20class%20of%20deep%20neural%20networks%20with%20ReLU-type%20activation%0Afunctions%20such%20as%20MLPs%20and%20CNNs%20without%20biases.%20We%20interpret%20the%20dynamics%20of%0Anormalized%20SGD%20iterates%20as%20an%20Euler-like%20discretization%20of%20a%20conservative%20field%0Aflow%20that%20is%20naturally%20associated%20to%20the%20normalized%20classification%20margin.%0AOwing%20to%20this%20interpretation%2C%20we%20show%20that%20normalized%20SGD%20iterates%20converge%20to%0Athe%20set%20of%20critical%20points%20of%20the%20normalized%20margin%20at%20late-stage%20training%0A%28i.e.%2C%20assuming%20that%20the%20data%20is%20correctly%20classified%20with%20positive%20normalized%0Amargin%29.%20Up%20to%20our%20knowledge%2C%20this%20is%20the%20first%20extension%20of%20the%20analysis%20of%0ALyu%20and%20Li%20%282020%29%20on%20the%20discrete%20dynamics%20of%20gradient%20descent%20to%20the%20nonsmooth%0Aand%20stochastic%20setting.%20Our%20main%20result%20applies%20to%20binary%20classification%20with%0Aexponential%20or%20logistic%20losses.%20We%20additionally%20discuss%20extensions%20to%20more%0Ageneral%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520late-stage%2520training%2520dynamics%2520of%2520%2528stochastic%2529%2520subgradient%2520descent%2520on%250A%2520%2520homogeneous%2520neural%2520networks%26entry.906535625%3DSholom%2520Schechtman%2520and%2520Nicolas%2520Schreuder%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520implicit%2520bias%2520of%2520constant%2520step%2520stochastic%2520subgradient%2520descent%250A%2528SGD%2529.%2520We%2520consider%2520the%2520setting%2520of%2520binary%2520classification%2520with%2520homogeneous%2520neural%250Anetworks%2520-%2520a%2520large%2520class%2520of%2520deep%2520neural%2520networks%2520with%2520ReLU-type%2520activation%250Afunctions%2520such%2520as%2520MLPs%2520and%2520CNNs%2520without%2520biases.%2520We%2520interpret%2520the%2520dynamics%2520of%250Anormalized%2520SGD%2520iterates%2520as%2520an%2520Euler-like%2520discretization%2520of%2520a%2520conservative%2520field%250Aflow%2520that%2520is%2520naturally%2520associated%2520to%2520the%2520normalized%2520classification%2520margin.%250AOwing%2520to%2520this%2520interpretation%252C%2520we%2520show%2520that%2520normalized%2520SGD%2520iterates%2520converge%2520to%250Athe%2520set%2520of%2520critical%2520points%2520of%2520the%2520normalized%2520margin%2520at%2520late-stage%2520training%250A%2528i.e.%252C%2520assuming%2520that%2520the%2520data%2520is%2520correctly%2520classified%2520with%2520positive%2520normalized%250Amargin%2529.%2520Up%2520to%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520extension%2520of%2520the%2520analysis%2520of%250ALyu%2520and%2520Li%2520%25282020%2529%2520on%2520the%2520discrete%2520dynamics%2520of%2520gradient%2520descent%2520to%2520the%2520nonsmooth%250Aand%2520stochastic%2520setting.%2520Our%2520main%2520result%2520applies%2520to%2520binary%2520classification%2520with%250Aexponential%2520or%2520logistic%2520losses.%2520We%2520additionally%2520discuss%2520extensions%2520to%2520more%250Ageneral%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20late-stage%20training%20dynamics%20of%20%28stochastic%29%20subgradient%20descent%20on%0A%20%20homogeneous%20neural%20networks&entry.906535625=Sholom%20Schechtman%20and%20Nicolas%20Schreuder&entry.1292438233=%20%20We%20analyze%20the%20implicit%20bias%20of%20constant%20step%20stochastic%20subgradient%20descent%0A%28SGD%29.%20We%20consider%20the%20setting%20of%20binary%20classification%20with%20homogeneous%20neural%0Anetworks%20-%20a%20large%20class%20of%20deep%20neural%20networks%20with%20ReLU-type%20activation%0Afunctions%20such%20as%20MLPs%20and%20CNNs%20without%20biases.%20We%20interpret%20the%20dynamics%20of%0Anormalized%20SGD%20iterates%20as%20an%20Euler-like%20discretization%20of%20a%20conservative%20field%0Aflow%20that%20is%20naturally%20associated%20to%20the%20normalized%20classification%20margin.%0AOwing%20to%20this%20interpretation%2C%20we%20show%20that%20normalized%20SGD%20iterates%20converge%20to%0Athe%20set%20of%20critical%20points%20of%20the%20normalized%20margin%20at%20late-stage%20training%0A%28i.e.%2C%20assuming%20that%20the%20data%20is%20correctly%20classified%20with%20positive%20normalized%0Amargin%29.%20Up%20to%20our%20knowledge%2C%20this%20is%20the%20first%20extension%20of%20the%20analysis%20of%0ALyu%20and%20Li%20%282020%29%20on%20the%20discrete%20dynamics%20of%20gradient%20descent%20to%20the%20nonsmooth%0Aand%20stochastic%20setting.%20Our%20main%20result%20applies%20to%20binary%20classification%20with%0Aexponential%20or%20logistic%20losses.%20We%20additionally%20discuss%20extensions%20to%20more%0Ageneral%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05668v3&entry.124074799=Read"},
{"title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond\n  Competitive Programming", "author": "Gal Beniamini and Yuval Dor and Alon Vinnikov and Shir Granot Peled and Or Weinstein and Or Sharir and Noam Wies and Tomer Nussbaum and Ido Ben Shaul and Tomer Zekharya and Yoav Levine and Shai Shalev-Shwartz and Amnon Shashua", "abstract": "  Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework.\n", "link": "http://arxiv.org/abs/2507.13337v1", "date": "2025-07-17", "relevancy": 2.0869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FormulaOne%3A%20Measuring%20the%20Depth%20of%20Algorithmic%20Reasoning%20Beyond%0A%20%20Competitive%20Programming&body=Title%3A%20FormulaOne%3A%20Measuring%20the%20Depth%20of%20Algorithmic%20Reasoning%20Beyond%0A%20%20Competitive%20Programming%0AAuthor%3A%20Gal%20Beniamini%20and%20Yuval%20Dor%20and%20Alon%20Vinnikov%20and%20Shir%20Granot%20Peled%20and%20Or%20Weinstein%20and%20Or%20Sharir%20and%20Noam%20Wies%20and%20Tomer%20Nussbaum%20and%20Ido%20Ben%20Shaul%20and%20Tomer%20Zekharya%20and%20Yoav%20Levine%20and%20Shai%20Shalev-Shwartz%20and%20Amnon%20Shashua%0AAbstract%3A%20%20%20Frontier%20AI%20models%20demonstrate%20formidable%20breadth%20of%20knowledge.%20But%20how%20close%0Aare%20they%20to%20true%20human%20--%20or%20superhuman%20--%20expertise%3F%20Genuine%20experts%20can%0Atackle%20the%20hardest%20problems%20and%20push%20the%20boundaries%20of%20scientific%0Aunderstanding.%20To%20illuminate%20the%20limits%20of%20frontier%20model%20capabilities%2C%20we%20turn%0Aaway%20from%20contrived%20competitive%20programming%20puzzles%2C%20and%20instead%20focus%20on%0Areal-life%20research%20problems.%0A%20%20We%20construct%20FormulaOne%2C%20a%20benchmark%20that%20lies%20at%20the%20intersection%20of%20graph%0Atheory%2C%20logic%2C%20and%20algorithms%2C%20all%20well%20within%20the%20training%20distribution%20of%0Afrontier%20models.%20Our%20problems%20are%20incredibly%20demanding%2C%20requiring%20an%20array%20of%0Areasoning%20steps.%20The%20dataset%20has%20three%20key%20properties.%20First%2C%20it%20is%20of%0Acommercial%20interest%20and%20relates%20to%20practical%20large-scale%20optimisation%20problems%2C%0Asuch%20as%20those%20arising%20in%20routing%2C%20scheduling%2C%20and%20network%20design.%20Second%2C%20it%20is%0Agenerated%20from%20the%20highly%20expressive%20framework%20of%20Monadic%20Second-Order%20%28MSO%29%0Alogic%20on%20graphs%2C%20paving%20the%20way%20toward%20automatic%20problem%20generation%20at%20scale%3B%0Aideal%20for%20building%20RL%20environments.%20Third%2C%20many%20of%20our%20problems%20are%20intimately%0Arelated%20to%20the%20frontier%20of%20theoretical%20computer%20science%2C%20and%20to%20central%0Aconjectures%20therein%2C%20such%20as%20the%20Strong%20Exponential%20Time%20Hypothesis%20%28SETH%29.%20As%0Asuch%2C%20any%20significant%20algorithmic%20progress%20on%20our%20dataset%2C%20beyond%20known%0Aresults%2C%20could%20carry%20profound%20theoretical%20implications.%0A%20%20Remarkably%2C%20state-of-the-art%20models%20like%20OpenAI%27s%20o3%20fail%20entirely%20on%0AFormulaOne%2C%20solving%20less%20than%201%25%20of%20the%20questions%2C%20even%20when%20given%2010%20attempts%0Aand%20explanatory%20fewshot%20examples%20--%20highlighting%20how%20far%20they%20remain%20from%0Aexpert-level%20understanding%20in%20some%20domains.%20To%20support%20further%20research%2C%20we%0Aadditionally%20curate%20FormulaOne-Warmup%2C%20offering%20a%20set%20of%20simpler%20tasks%2C%20from%0Athe%20same%20distribution.%20We%20release%20the%20full%20corpus%20along%20with%20a%20comprehensive%0Aevaluation%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFormulaOne%253A%2520Measuring%2520the%2520Depth%2520of%2520Algorithmic%2520Reasoning%2520Beyond%250A%2520%2520Competitive%2520Programming%26entry.906535625%3DGal%2520Beniamini%2520and%2520Yuval%2520Dor%2520and%2520Alon%2520Vinnikov%2520and%2520Shir%2520Granot%2520Peled%2520and%2520Or%2520Weinstein%2520and%2520Or%2520Sharir%2520and%2520Noam%2520Wies%2520and%2520Tomer%2520Nussbaum%2520and%2520Ido%2520Ben%2520Shaul%2520and%2520Tomer%2520Zekharya%2520and%2520Yoav%2520Levine%2520and%2520Shai%2520Shalev-Shwartz%2520and%2520Amnon%2520Shashua%26entry.1292438233%3D%2520%2520Frontier%2520AI%2520models%2520demonstrate%2520formidable%2520breadth%2520of%2520knowledge.%2520But%2520how%2520close%250Aare%2520they%2520to%2520true%2520human%2520--%2520or%2520superhuman%2520--%2520expertise%253F%2520Genuine%2520experts%2520can%250Atackle%2520the%2520hardest%2520problems%2520and%2520push%2520the%2520boundaries%2520of%2520scientific%250Aunderstanding.%2520To%2520illuminate%2520the%2520limits%2520of%2520frontier%2520model%2520capabilities%252C%2520we%2520turn%250Aaway%2520from%2520contrived%2520competitive%2520programming%2520puzzles%252C%2520and%2520instead%2520focus%2520on%250Areal-life%2520research%2520problems.%250A%2520%2520We%2520construct%2520FormulaOne%252C%2520a%2520benchmark%2520that%2520lies%2520at%2520the%2520intersection%2520of%2520graph%250Atheory%252C%2520logic%252C%2520and%2520algorithms%252C%2520all%2520well%2520within%2520the%2520training%2520distribution%2520of%250Afrontier%2520models.%2520Our%2520problems%2520are%2520incredibly%2520demanding%252C%2520requiring%2520an%2520array%2520of%250Areasoning%2520steps.%2520The%2520dataset%2520has%2520three%2520key%2520properties.%2520First%252C%2520it%2520is%2520of%250Acommercial%2520interest%2520and%2520relates%2520to%2520practical%2520large-scale%2520optimisation%2520problems%252C%250Asuch%2520as%2520those%2520arising%2520in%2520routing%252C%2520scheduling%252C%2520and%2520network%2520design.%2520Second%252C%2520it%2520is%250Agenerated%2520from%2520the%2520highly%2520expressive%2520framework%2520of%2520Monadic%2520Second-Order%2520%2528MSO%2529%250Alogic%2520on%2520graphs%252C%2520paving%2520the%2520way%2520toward%2520automatic%2520problem%2520generation%2520at%2520scale%253B%250Aideal%2520for%2520building%2520RL%2520environments.%2520Third%252C%2520many%2520of%2520our%2520problems%2520are%2520intimately%250Arelated%2520to%2520the%2520frontier%2520of%2520theoretical%2520computer%2520science%252C%2520and%2520to%2520central%250Aconjectures%2520therein%252C%2520such%2520as%2520the%2520Strong%2520Exponential%2520Time%2520Hypothesis%2520%2528SETH%2529.%2520As%250Asuch%252C%2520any%2520significant%2520algorithmic%2520progress%2520on%2520our%2520dataset%252C%2520beyond%2520known%250Aresults%252C%2520could%2520carry%2520profound%2520theoretical%2520implications.%250A%2520%2520Remarkably%252C%2520state-of-the-art%2520models%2520like%2520OpenAI%2527s%2520o3%2520fail%2520entirely%2520on%250AFormulaOne%252C%2520solving%2520less%2520than%25201%2525%2520of%2520the%2520questions%252C%2520even%2520when%2520given%252010%2520attempts%250Aand%2520explanatory%2520fewshot%2520examples%2520--%2520highlighting%2520how%2520far%2520they%2520remain%2520from%250Aexpert-level%2520understanding%2520in%2520some%2520domains.%2520To%2520support%2520further%2520research%252C%2520we%250Aadditionally%2520curate%2520FormulaOne-Warmup%252C%2520offering%2520a%2520set%2520of%2520simpler%2520tasks%252C%2520from%250Athe%2520same%2520distribution.%2520We%2520release%2520the%2520full%2520corpus%2520along%2520with%2520a%2520comprehensive%250Aevaluation%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FormulaOne%3A%20Measuring%20the%20Depth%20of%20Algorithmic%20Reasoning%20Beyond%0A%20%20Competitive%20Programming&entry.906535625=Gal%20Beniamini%20and%20Yuval%20Dor%20and%20Alon%20Vinnikov%20and%20Shir%20Granot%20Peled%20and%20Or%20Weinstein%20and%20Or%20Sharir%20and%20Noam%20Wies%20and%20Tomer%20Nussbaum%20and%20Ido%20Ben%20Shaul%20and%20Tomer%20Zekharya%20and%20Yoav%20Levine%20and%20Shai%20Shalev-Shwartz%20and%20Amnon%20Shashua&entry.1292438233=%20%20Frontier%20AI%20models%20demonstrate%20formidable%20breadth%20of%20knowledge.%20But%20how%20close%0Aare%20they%20to%20true%20human%20--%20or%20superhuman%20--%20expertise%3F%20Genuine%20experts%20can%0Atackle%20the%20hardest%20problems%20and%20push%20the%20boundaries%20of%20scientific%0Aunderstanding.%20To%20illuminate%20the%20limits%20of%20frontier%20model%20capabilities%2C%20we%20turn%0Aaway%20from%20contrived%20competitive%20programming%20puzzles%2C%20and%20instead%20focus%20on%0Areal-life%20research%20problems.%0A%20%20We%20construct%20FormulaOne%2C%20a%20benchmark%20that%20lies%20at%20the%20intersection%20of%20graph%0Atheory%2C%20logic%2C%20and%20algorithms%2C%20all%20well%20within%20the%20training%20distribution%20of%0Afrontier%20models.%20Our%20problems%20are%20incredibly%20demanding%2C%20requiring%20an%20array%20of%0Areasoning%20steps.%20The%20dataset%20has%20three%20key%20properties.%20First%2C%20it%20is%20of%0Acommercial%20interest%20and%20relates%20to%20practical%20large-scale%20optimisation%20problems%2C%0Asuch%20as%20those%20arising%20in%20routing%2C%20scheduling%2C%20and%20network%20design.%20Second%2C%20it%20is%0Agenerated%20from%20the%20highly%20expressive%20framework%20of%20Monadic%20Second-Order%20%28MSO%29%0Alogic%20on%20graphs%2C%20paving%20the%20way%20toward%20automatic%20problem%20generation%20at%20scale%3B%0Aideal%20for%20building%20RL%20environments.%20Third%2C%20many%20of%20our%20problems%20are%20intimately%0Arelated%20to%20the%20frontier%20of%20theoretical%20computer%20science%2C%20and%20to%20central%0Aconjectures%20therein%2C%20such%20as%20the%20Strong%20Exponential%20Time%20Hypothesis%20%28SETH%29.%20As%0Asuch%2C%20any%20significant%20algorithmic%20progress%20on%20our%20dataset%2C%20beyond%20known%0Aresults%2C%20could%20carry%20profound%20theoretical%20implications.%0A%20%20Remarkably%2C%20state-of-the-art%20models%20like%20OpenAI%27s%20o3%20fail%20entirely%20on%0AFormulaOne%2C%20solving%20less%20than%201%25%20of%20the%20questions%2C%20even%20when%20given%2010%20attempts%0Aand%20explanatory%20fewshot%20examples%20--%20highlighting%20how%20far%20they%20remain%20from%0Aexpert-level%20understanding%20in%20some%20domains.%20To%20support%20further%20research%2C%20we%0Aadditionally%20curate%20FormulaOne-Warmup%2C%20offering%20a%20set%20of%20simpler%20tasks%2C%20from%0Athe%20same%20distribution.%20We%20release%20the%20full%20corpus%20along%20with%20a%20comprehensive%0Aevaluation%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13337v1&entry.124074799=Read"},
{"title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI", "author": "Vincenzo Dentamaro and Felice Franchini and Giuseppe Pirlo and Irina Voiculescu", "abstract": "  Robust XAI techniques should ideally be simultaneously deterministic, model\nagnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM\nAGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability\ntechnique, with guaranteed convergency. MUPAX measure theoretic formulation\ngives principled feature importance attribution through structured perturbation\nanalysis that discovers inherent input patterns and eliminates spurious\nrelationships. We evaluate MUPAX on an extensive range of data modalities and\ntasks: audio classification (1D), image classification (2D), volumetric medical\nimage analysis (3D), and anatomical landmark detection, demonstrating dimension\nagnostic effectiveness. The rigorous convergence guarantees extend to any loss\nfunction and arbitrary dimensions, making MUPAX applicable to virtually any\nproblem context for AI. By contrast with other XAI methods that typically\ndecrease performance when masking, MUPAX not only preserves but actually\nenhances model accuracy by capturing only the most important patterns of the\noriginal data. Extensive benchmarking against the state of the XAI art\ndemonstrates MUPAX ability to generate precise, consistent and understandable\nexplanations, a crucial step towards explainable and trustworthy AI systems.\nThe source code will be released upon publication.\n", "link": "http://arxiv.org/abs/2507.13090v1", "date": "2025-07-17", "relevancy": 2.0823, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUPAX%3A%20Multidimensional%20Problem%20Agnostic%20eXplainable%20AI&body=Title%3A%20MUPAX%3A%20Multidimensional%20Problem%20Agnostic%20eXplainable%20AI%0AAuthor%3A%20Vincenzo%20Dentamaro%20and%20Felice%20Franchini%20and%20Giuseppe%20Pirlo%20and%20Irina%20Voiculescu%0AAbstract%3A%20%20%20Robust%20XAI%20techniques%20should%20ideally%20be%20simultaneously%20deterministic%2C%20model%0Aagnostic%2C%20and%20guaranteed%20to%20converge.%20We%20propose%20MULTIDIMENSIONAL%20PROBLEM%0AAGNOSTIC%20EXPLAINABLE%20AI%20%28MUPAX%29%2C%20a%20deterministic%2C%20model%20agnostic%20explainability%0Atechnique%2C%20with%20guaranteed%20convergency.%20MUPAX%20measure%20theoretic%20formulation%0Agives%20principled%20feature%20importance%20attribution%20through%20structured%20perturbation%0Aanalysis%20that%20discovers%20inherent%20input%20patterns%20and%20eliminates%20spurious%0Arelationships.%20We%20evaluate%20MUPAX%20on%20an%20extensive%20range%20of%20data%20modalities%20and%0Atasks%3A%20audio%20classification%20%281D%29%2C%20image%20classification%20%282D%29%2C%20volumetric%20medical%0Aimage%20analysis%20%283D%29%2C%20and%20anatomical%20landmark%20detection%2C%20demonstrating%20dimension%0Aagnostic%20effectiveness.%20The%20rigorous%20convergence%20guarantees%20extend%20to%20any%20loss%0Afunction%20and%20arbitrary%20dimensions%2C%20making%20MUPAX%20applicable%20to%20virtually%20any%0Aproblem%20context%20for%20AI.%20By%20contrast%20with%20other%20XAI%20methods%20that%20typically%0Adecrease%20performance%20when%20masking%2C%20MUPAX%20not%20only%20preserves%20but%20actually%0Aenhances%20model%20accuracy%20by%20capturing%20only%20the%20most%20important%20patterns%20of%20the%0Aoriginal%20data.%20Extensive%20benchmarking%20against%20the%20state%20of%20the%20XAI%20art%0Ademonstrates%20MUPAX%20ability%20to%20generate%20precise%2C%20consistent%20and%20understandable%0Aexplanations%2C%20a%20crucial%20step%20towards%20explainable%20and%20trustworthy%20AI%20systems.%0AThe%20source%20code%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUPAX%253A%2520Multidimensional%2520Problem%2520Agnostic%2520eXplainable%2520AI%26entry.906535625%3DVincenzo%2520Dentamaro%2520and%2520Felice%2520Franchini%2520and%2520Giuseppe%2520Pirlo%2520and%2520Irina%2520Voiculescu%26entry.1292438233%3D%2520%2520Robust%2520XAI%2520techniques%2520should%2520ideally%2520be%2520simultaneously%2520deterministic%252C%2520model%250Aagnostic%252C%2520and%2520guaranteed%2520to%2520converge.%2520We%2520propose%2520MULTIDIMENSIONAL%2520PROBLEM%250AAGNOSTIC%2520EXPLAINABLE%2520AI%2520%2528MUPAX%2529%252C%2520a%2520deterministic%252C%2520model%2520agnostic%2520explainability%250Atechnique%252C%2520with%2520guaranteed%2520convergency.%2520MUPAX%2520measure%2520theoretic%2520formulation%250Agives%2520principled%2520feature%2520importance%2520attribution%2520through%2520structured%2520perturbation%250Aanalysis%2520that%2520discovers%2520inherent%2520input%2520patterns%2520and%2520eliminates%2520spurious%250Arelationships.%2520We%2520evaluate%2520MUPAX%2520on%2520an%2520extensive%2520range%2520of%2520data%2520modalities%2520and%250Atasks%253A%2520audio%2520classification%2520%25281D%2529%252C%2520image%2520classification%2520%25282D%2529%252C%2520volumetric%2520medical%250Aimage%2520analysis%2520%25283D%2529%252C%2520and%2520anatomical%2520landmark%2520detection%252C%2520demonstrating%2520dimension%250Aagnostic%2520effectiveness.%2520The%2520rigorous%2520convergence%2520guarantees%2520extend%2520to%2520any%2520loss%250Afunction%2520and%2520arbitrary%2520dimensions%252C%2520making%2520MUPAX%2520applicable%2520to%2520virtually%2520any%250Aproblem%2520context%2520for%2520AI.%2520By%2520contrast%2520with%2520other%2520XAI%2520methods%2520that%2520typically%250Adecrease%2520performance%2520when%2520masking%252C%2520MUPAX%2520not%2520only%2520preserves%2520but%2520actually%250Aenhances%2520model%2520accuracy%2520by%2520capturing%2520only%2520the%2520most%2520important%2520patterns%2520of%2520the%250Aoriginal%2520data.%2520Extensive%2520benchmarking%2520against%2520the%2520state%2520of%2520the%2520XAI%2520art%250Ademonstrates%2520MUPAX%2520ability%2520to%2520generate%2520precise%252C%2520consistent%2520and%2520understandable%250Aexplanations%252C%2520a%2520crucial%2520step%2520towards%2520explainable%2520and%2520trustworthy%2520AI%2520systems.%250AThe%2520source%2520code%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUPAX%3A%20Multidimensional%20Problem%20Agnostic%20eXplainable%20AI&entry.906535625=Vincenzo%20Dentamaro%20and%20Felice%20Franchini%20and%20Giuseppe%20Pirlo%20and%20Irina%20Voiculescu&entry.1292438233=%20%20Robust%20XAI%20techniques%20should%20ideally%20be%20simultaneously%20deterministic%2C%20model%0Aagnostic%2C%20and%20guaranteed%20to%20converge.%20We%20propose%20MULTIDIMENSIONAL%20PROBLEM%0AAGNOSTIC%20EXPLAINABLE%20AI%20%28MUPAX%29%2C%20a%20deterministic%2C%20model%20agnostic%20explainability%0Atechnique%2C%20with%20guaranteed%20convergency.%20MUPAX%20measure%20theoretic%20formulation%0Agives%20principled%20feature%20importance%20attribution%20through%20structured%20perturbation%0Aanalysis%20that%20discovers%20inherent%20input%20patterns%20and%20eliminates%20spurious%0Arelationships.%20We%20evaluate%20MUPAX%20on%20an%20extensive%20range%20of%20data%20modalities%20and%0Atasks%3A%20audio%20classification%20%281D%29%2C%20image%20classification%20%282D%29%2C%20volumetric%20medical%0Aimage%20analysis%20%283D%29%2C%20and%20anatomical%20landmark%20detection%2C%20demonstrating%20dimension%0Aagnostic%20effectiveness.%20The%20rigorous%20convergence%20guarantees%20extend%20to%20any%20loss%0Afunction%20and%20arbitrary%20dimensions%2C%20making%20MUPAX%20applicable%20to%20virtually%20any%0Aproblem%20context%20for%20AI.%20By%20contrast%20with%20other%20XAI%20methods%20that%20typically%0Adecrease%20performance%20when%20masking%2C%20MUPAX%20not%20only%20preserves%20but%20actually%0Aenhances%20model%20accuracy%20by%20capturing%20only%20the%20most%20important%20patterns%20of%20the%0Aoriginal%20data.%20Extensive%20benchmarking%20against%20the%20state%20of%20the%20XAI%20art%0Ademonstrates%20MUPAX%20ability%20to%20generate%20precise%2C%20consistent%20and%20understandable%0Aexplanations%2C%20a%20crucial%20step%20towards%20explainable%20and%20trustworthy%20AI%20systems.%0AThe%20source%20code%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13090v1&entry.124074799=Read"},
{"title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research", "author": "Yilun Zhao and Weiyuan Chen and Zhijian Xu and Manasi Patwardhan and Yixin Liu and Chengye Wang and Lovekesh Vig and Arman Cohan", "abstract": "  We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.\n", "link": "http://arxiv.org/abs/2507.13300v1", "date": "2025-07-17", "relevancy": 2.0704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AbGen%3A%20Evaluating%20Large%20Language%20Models%20in%20Ablation%20Study%20Design%20and%0A%20%20Evaluation%20for%20Scientific%20Research&body=Title%3A%20AbGen%3A%20Evaluating%20Large%20Language%20Models%20in%20Ablation%20Study%20Design%20and%0A%20%20Evaluation%20for%20Scientific%20Research%0AAuthor%3A%20Yilun%20Zhao%20and%20Weiyuan%20Chen%20and%20Zhijian%20Xu%20and%20Manasi%20Patwardhan%20and%20Yixin%20Liu%20and%20Chengye%20Wang%20and%20Lovekesh%20Vig%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20We%20introduce%20AbGen%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20the%20capabilities%0Aof%20LLMs%20in%20designing%20ablation%20studies%20for%20scientific%20research.%20AbGen%20consists%0Aof%201%2C500%20expert-annotated%20examples%20derived%20from%20807%20NLP%20papers.%20In%20this%0Abenchmark%2C%20LLMs%20are%20tasked%20with%20generating%20detailed%20ablation%20study%20designs%20for%0Aa%20specified%20module%20or%20process%20based%20on%20the%20given%20research%20context.%20Our%0Aevaluation%20of%20leading%20LLMs%2C%20such%20as%20DeepSeek-R1-0528%20and%20o4-mini%2C%20highlights%20a%0Asignificant%20performance%20gap%20between%20these%20models%20and%20human%20experts%20in%20terms%20of%0Athe%20importance%2C%20faithfulness%2C%20and%20soundness%20of%20the%20ablation%20study%20designs.%0AMoreover%2C%20we%20demonstrate%20that%20current%20automated%20evaluation%20methods%20are%20not%0Areliable%20for%20our%20task%2C%20as%20they%20show%20a%20significant%20discrepancy%20when%20compared%20to%0Ahuman%20assessment.%20To%20better%20investigate%20this%2C%20we%20develop%20AbGen-Eval%2C%20a%0Ameta-evaluation%20benchmark%20designed%20to%20assess%20the%20reliability%20of%20commonly%20used%0Aautomated%20evaluation%20systems%20in%20measuring%20LLM%20performance%20on%20our%20task.%20We%0Ainvestigate%20various%20LLM-as-Judge%20systems%20on%20AbGen-Eval%2C%20providing%20insights%20for%0Afuture%20research%20on%20developing%20more%20effective%20and%20reliable%20LLM-based%20evaluation%0Asystems%20for%20complex%20scientific%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbGen%253A%2520Evaluating%2520Large%2520Language%2520Models%2520in%2520Ablation%2520Study%2520Design%2520and%250A%2520%2520Evaluation%2520for%2520Scientific%2520Research%26entry.906535625%3DYilun%2520Zhao%2520and%2520Weiyuan%2520Chen%2520and%2520Zhijian%2520Xu%2520and%2520Manasi%2520Patwardhan%2520and%2520Yixin%2520Liu%2520and%2520Chengye%2520Wang%2520and%2520Lovekesh%2520Vig%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520We%2520introduce%2520AbGen%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520capabilities%250Aof%2520LLMs%2520in%2520designing%2520ablation%2520studies%2520for%2520scientific%2520research.%2520AbGen%2520consists%250Aof%25201%252C500%2520expert-annotated%2520examples%2520derived%2520from%2520807%2520NLP%2520papers.%2520In%2520this%250Abenchmark%252C%2520LLMs%2520are%2520tasked%2520with%2520generating%2520detailed%2520ablation%2520study%2520designs%2520for%250Aa%2520specified%2520module%2520or%2520process%2520based%2520on%2520the%2520given%2520research%2520context.%2520Our%250Aevaluation%2520of%2520leading%2520LLMs%252C%2520such%2520as%2520DeepSeek-R1-0528%2520and%2520o4-mini%252C%2520highlights%2520a%250Asignificant%2520performance%2520gap%2520between%2520these%2520models%2520and%2520human%2520experts%2520in%2520terms%2520of%250Athe%2520importance%252C%2520faithfulness%252C%2520and%2520soundness%2520of%2520the%2520ablation%2520study%2520designs.%250AMoreover%252C%2520we%2520demonstrate%2520that%2520current%2520automated%2520evaluation%2520methods%2520are%2520not%250Areliable%2520for%2520our%2520task%252C%2520as%2520they%2520show%2520a%2520significant%2520discrepancy%2520when%2520compared%2520to%250Ahuman%2520assessment.%2520To%2520better%2520investigate%2520this%252C%2520we%2520develop%2520AbGen-Eval%252C%2520a%250Ameta-evaluation%2520benchmark%2520designed%2520to%2520assess%2520the%2520reliability%2520of%2520commonly%2520used%250Aautomated%2520evaluation%2520systems%2520in%2520measuring%2520LLM%2520performance%2520on%2520our%2520task.%2520We%250Ainvestigate%2520various%2520LLM-as-Judge%2520systems%2520on%2520AbGen-Eval%252C%2520providing%2520insights%2520for%250Afuture%2520research%2520on%2520developing%2520more%2520effective%2520and%2520reliable%2520LLM-based%2520evaluation%250Asystems%2520for%2520complex%2520scientific%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbGen%3A%20Evaluating%20Large%20Language%20Models%20in%20Ablation%20Study%20Design%20and%0A%20%20Evaluation%20for%20Scientific%20Research&entry.906535625=Yilun%20Zhao%20and%20Weiyuan%20Chen%20and%20Zhijian%20Xu%20and%20Manasi%20Patwardhan%20and%20Yixin%20Liu%20and%20Chengye%20Wang%20and%20Lovekesh%20Vig%20and%20Arman%20Cohan&entry.1292438233=%20%20We%20introduce%20AbGen%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20the%20capabilities%0Aof%20LLMs%20in%20designing%20ablation%20studies%20for%20scientific%20research.%20AbGen%20consists%0Aof%201%2C500%20expert-annotated%20examples%20derived%20from%20807%20NLP%20papers.%20In%20this%0Abenchmark%2C%20LLMs%20are%20tasked%20with%20generating%20detailed%20ablation%20study%20designs%20for%0Aa%20specified%20module%20or%20process%20based%20on%20the%20given%20research%20context.%20Our%0Aevaluation%20of%20leading%20LLMs%2C%20such%20as%20DeepSeek-R1-0528%20and%20o4-mini%2C%20highlights%20a%0Asignificant%20performance%20gap%20between%20these%20models%20and%20human%20experts%20in%20terms%20of%0Athe%20importance%2C%20faithfulness%2C%20and%20soundness%20of%20the%20ablation%20study%20designs.%0AMoreover%2C%20we%20demonstrate%20that%20current%20automated%20evaluation%20methods%20are%20not%0Areliable%20for%20our%20task%2C%20as%20they%20show%20a%20significant%20discrepancy%20when%20compared%20to%0Ahuman%20assessment.%20To%20better%20investigate%20this%2C%20we%20develop%20AbGen-Eval%2C%20a%0Ameta-evaluation%20benchmark%20designed%20to%20assess%20the%20reliability%20of%20commonly%20used%0Aautomated%20evaluation%20systems%20in%20measuring%20LLM%20performance%20on%20our%20task.%20We%0Ainvestigate%20various%20LLM-as-Judge%20systems%20on%20AbGen-Eval%2C%20providing%20insights%20for%0Afuture%20research%20on%20developing%20more%20effective%20and%20reliable%20LLM-based%20evaluation%0Asystems%20for%20complex%20scientific%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13300v1&entry.124074799=Read"},
{"title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication", "author": "Tianyu Song and Feng Li and Yuan Bi and Angelos Karlas and Amir Yousefi and Daniela Branzan and Zhongliang Jiang and Ulrich Eck and Nassir Navab", "abstract": "  The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.\n", "link": "http://arxiv.org/abs/2507.13052v1", "date": "2025-07-17", "relevancy": 2.0682, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5414}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5138}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Virtual%20Sonographer%20%28IVS%29%3A%20Enhancing%20Physician-Robot-Patient%0A%20%20Communication&body=Title%3A%20Intelligent%20Virtual%20Sonographer%20%28IVS%29%3A%20Enhancing%20Physician-Robot-Patient%0A%20%20Communication%0AAuthor%3A%20Tianyu%20Song%20and%20Feng%20Li%20and%20Yuan%20Bi%20and%20Angelos%20Karlas%20and%20Amir%20Yousefi%20and%20Daniela%20Branzan%20and%20Zhongliang%20Jiang%20and%20Ulrich%20Eck%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20The%20advancement%20and%20maturity%20of%20large%20language%20models%20%28LLMs%29%20and%20robotics%0Ahave%20unlocked%20vast%20potential%20for%20human-computer%20interaction%2C%20particularly%20in%0Athe%20field%20of%20robotic%20ultrasound.%20While%20existing%20research%20primarily%20focuses%20on%0Aeither%20patient-robot%20or%20physician-robot%20interaction%2C%20the%20role%20of%20an%20intelligent%0Avirtual%20sonographer%20%28IVS%29%20bridging%20physician-robot-patient%20communication%0Aremains%20underexplored.%20This%20work%20introduces%20a%20conversational%20virtual%20agent%20in%0AExtended%20Reality%20%28XR%29%20that%20facilitates%20real-time%20interaction%20between%0Aphysicians%2C%20a%20robotic%20ultrasound%20system%28RUS%29%2C%20and%20patients.%20The%20IVS%20agent%0Acommunicates%20with%20physicians%20in%20a%20professional%20manner%20while%20offering%20empathetic%0Aexplanations%20and%20reassurance%20to%20patients.%20Furthermore%2C%20it%20actively%20controls%20the%0ARUS%20by%20executing%20physician%20commands%20and%20transparently%20relays%20these%20actions%20to%0Athe%20patient.%20By%20integrating%20LLM-powered%20dialogue%20with%20speech-to-text%2C%0Atext-to-speech%2C%20and%20robotic%20control%2C%20our%20system%20enhances%20the%20efficiency%2C%0Aclarity%2C%20and%20accessibility%20of%20robotic%20ultrasound%20acquisition.%20This%20work%0Aconstitutes%20a%20first%20step%20toward%20understanding%20how%20IVS%20can%20bridge%20communication%0Agaps%20in%20physician-robot-patient%20interaction%2C%20providing%20more%20control%20and%0Atherefore%20trust%20into%20physician-robot%20interaction%20while%20improving%20patient%0Aexperience%20and%20acceptance%20of%20robotic%20ultrasound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Virtual%2520Sonographer%2520%2528IVS%2529%253A%2520Enhancing%2520Physician-Robot-Patient%250A%2520%2520Communication%26entry.906535625%3DTianyu%2520Song%2520and%2520Feng%2520Li%2520and%2520Yuan%2520Bi%2520and%2520Angelos%2520Karlas%2520and%2520Amir%2520Yousefi%2520and%2520Daniela%2520Branzan%2520and%2520Zhongliang%2520Jiang%2520and%2520Ulrich%2520Eck%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520The%2520advancement%2520and%2520maturity%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520robotics%250Ahave%2520unlocked%2520vast%2520potential%2520for%2520human-computer%2520interaction%252C%2520particularly%2520in%250Athe%2520field%2520of%2520robotic%2520ultrasound.%2520While%2520existing%2520research%2520primarily%2520focuses%2520on%250Aeither%2520patient-robot%2520or%2520physician-robot%2520interaction%252C%2520the%2520role%2520of%2520an%2520intelligent%250Avirtual%2520sonographer%2520%2528IVS%2529%2520bridging%2520physician-robot-patient%2520communication%250Aremains%2520underexplored.%2520This%2520work%2520introduces%2520a%2520conversational%2520virtual%2520agent%2520in%250AExtended%2520Reality%2520%2528XR%2529%2520that%2520facilitates%2520real-time%2520interaction%2520between%250Aphysicians%252C%2520a%2520robotic%2520ultrasound%2520system%2528RUS%2529%252C%2520and%2520patients.%2520The%2520IVS%2520agent%250Acommunicates%2520with%2520physicians%2520in%2520a%2520professional%2520manner%2520while%2520offering%2520empathetic%250Aexplanations%2520and%2520reassurance%2520to%2520patients.%2520Furthermore%252C%2520it%2520actively%2520controls%2520the%250ARUS%2520by%2520executing%2520physician%2520commands%2520and%2520transparently%2520relays%2520these%2520actions%2520to%250Athe%2520patient.%2520By%2520integrating%2520LLM-powered%2520dialogue%2520with%2520speech-to-text%252C%250Atext-to-speech%252C%2520and%2520robotic%2520control%252C%2520our%2520system%2520enhances%2520the%2520efficiency%252C%250Aclarity%252C%2520and%2520accessibility%2520of%2520robotic%2520ultrasound%2520acquisition.%2520This%2520work%250Aconstitutes%2520a%2520first%2520step%2520toward%2520understanding%2520how%2520IVS%2520can%2520bridge%2520communication%250Agaps%2520in%2520physician-robot-patient%2520interaction%252C%2520providing%2520more%2520control%2520and%250Atherefore%2520trust%2520into%2520physician-robot%2520interaction%2520while%2520improving%2520patient%250Aexperience%2520and%2520acceptance%2520of%2520robotic%2520ultrasound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Virtual%20Sonographer%20%28IVS%29%3A%20Enhancing%20Physician-Robot-Patient%0A%20%20Communication&entry.906535625=Tianyu%20Song%20and%20Feng%20Li%20and%20Yuan%20Bi%20and%20Angelos%20Karlas%20and%20Amir%20Yousefi%20and%20Daniela%20Branzan%20and%20Zhongliang%20Jiang%20and%20Ulrich%20Eck%20and%20Nassir%20Navab&entry.1292438233=%20%20The%20advancement%20and%20maturity%20of%20large%20language%20models%20%28LLMs%29%20and%20robotics%0Ahave%20unlocked%20vast%20potential%20for%20human-computer%20interaction%2C%20particularly%20in%0Athe%20field%20of%20robotic%20ultrasound.%20While%20existing%20research%20primarily%20focuses%20on%0Aeither%20patient-robot%20or%20physician-robot%20interaction%2C%20the%20role%20of%20an%20intelligent%0Avirtual%20sonographer%20%28IVS%29%20bridging%20physician-robot-patient%20communication%0Aremains%20underexplored.%20This%20work%20introduces%20a%20conversational%20virtual%20agent%20in%0AExtended%20Reality%20%28XR%29%20that%20facilitates%20real-time%20interaction%20between%0Aphysicians%2C%20a%20robotic%20ultrasound%20system%28RUS%29%2C%20and%20patients.%20The%20IVS%20agent%0Acommunicates%20with%20physicians%20in%20a%20professional%20manner%20while%20offering%20empathetic%0Aexplanations%20and%20reassurance%20to%20patients.%20Furthermore%2C%20it%20actively%20controls%20the%0ARUS%20by%20executing%20physician%20commands%20and%20transparently%20relays%20these%20actions%20to%0Athe%20patient.%20By%20integrating%20LLM-powered%20dialogue%20with%20speech-to-text%2C%0Atext-to-speech%2C%20and%20robotic%20control%2C%20our%20system%20enhances%20the%20efficiency%2C%0Aclarity%2C%20and%20accessibility%20of%20robotic%20ultrasound%20acquisition.%20This%20work%0Aconstitutes%20a%20first%20step%20toward%20understanding%20how%20IVS%20can%20bridge%20communication%0Agaps%20in%20physician-robot-patient%20interaction%2C%20providing%20more%20control%20and%0Atherefore%20trust%20into%20physician-robot%20interaction%20while%20improving%20patient%0Aexperience%20and%20acceptance%20of%20robotic%20ultrasound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13052v1&entry.124074799=Read"},
{"title": "Creating a Historical Migration Dataset from Finnish Church Records,\n  1800-1920", "author": "Ari Vesalainen and Jenna Kanerva and Aida Nitsch and Kiia Korsu and Ilari Larkiola and Laura Ruotsalainen and Filip Ginter", "abstract": "  This article presents a large-scale effort to create a structured dataset of\ninternal migration in Finland between 1800 and 1920 using digitized church\nmoving records. These records, maintained by Evangelical-Lutheran parishes,\ndocument the migration of individuals and families and offer a valuable source\nfor studying historical demographic patterns. The dataset includes over six\nmillion entries extracted from approximately 200,000 images of handwritten\nmigration records.\n  The data extraction process was automated using a deep learning pipeline that\nincluded layout analysis, table detection, cell classification, and handwriting\nrecognition. The complete pipeline was applied to all images, resulting in a\nstructured dataset suitable for research.\n  The dataset can be used to study internal migration, urbanization, and family\nmigration, and the spread of disease in preindustrial Finland. A case study\nfrom the Elim\\\"aki parish shows how local migration histories can be\nreconstructed. The work demonstrates how large volumes of handwritten archival\nmaterial can be transformed into structured data to support historical and\ndemographic research.\n", "link": "http://arxiv.org/abs/2506.07960v2", "date": "2025-07-17", "relevancy": 2.0639, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.419}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4109}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20Historical%20Migration%20Dataset%20from%20Finnish%20Church%20Records%2C%0A%20%201800-1920&body=Title%3A%20Creating%20a%20Historical%20Migration%20Dataset%20from%20Finnish%20Church%20Records%2C%0A%20%201800-1920%0AAuthor%3A%20Ari%20Vesalainen%20and%20Jenna%20Kanerva%20and%20Aida%20Nitsch%20and%20Kiia%20Korsu%20and%20Ilari%20Larkiola%20and%20Laura%20Ruotsalainen%20and%20Filip%20Ginter%0AAbstract%3A%20%20%20This%20article%20presents%20a%20large-scale%20effort%20to%20create%20a%20structured%20dataset%20of%0Ainternal%20migration%20in%20Finland%20between%201800%20and%201920%20using%20digitized%20church%0Amoving%20records.%20These%20records%2C%20maintained%20by%20Evangelical-Lutheran%20parishes%2C%0Adocument%20the%20migration%20of%20individuals%20and%20families%20and%20offer%20a%20valuable%20source%0Afor%20studying%20historical%20demographic%20patterns.%20The%20dataset%20includes%20over%20six%0Amillion%20entries%20extracted%20from%20approximately%20200%2C000%20images%20of%20handwritten%0Amigration%20records.%0A%20%20The%20data%20extraction%20process%20was%20automated%20using%20a%20deep%20learning%20pipeline%20that%0Aincluded%20layout%20analysis%2C%20table%20detection%2C%20cell%20classification%2C%20and%20handwriting%0Arecognition.%20The%20complete%20pipeline%20was%20applied%20to%20all%20images%2C%20resulting%20in%20a%0Astructured%20dataset%20suitable%20for%20research.%0A%20%20The%20dataset%20can%20be%20used%20to%20study%20internal%20migration%2C%20urbanization%2C%20and%20family%0Amigration%2C%20and%20the%20spread%20of%20disease%20in%20preindustrial%20Finland.%20A%20case%20study%0Afrom%20the%20Elim%5C%22aki%20parish%20shows%20how%20local%20migration%20histories%20can%20be%0Areconstructed.%20The%20work%20demonstrates%20how%20large%20volumes%20of%20handwritten%20archival%0Amaterial%20can%20be%20transformed%20into%20structured%20data%20to%20support%20historical%20and%0Ademographic%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520a%2520Historical%2520Migration%2520Dataset%2520from%2520Finnish%2520Church%2520Records%252C%250A%2520%25201800-1920%26entry.906535625%3DAri%2520Vesalainen%2520and%2520Jenna%2520Kanerva%2520and%2520Aida%2520Nitsch%2520and%2520Kiia%2520Korsu%2520and%2520Ilari%2520Larkiola%2520and%2520Laura%2520Ruotsalainen%2520and%2520Filip%2520Ginter%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520large-scale%2520effort%2520to%2520create%2520a%2520structured%2520dataset%2520of%250Ainternal%2520migration%2520in%2520Finland%2520between%25201800%2520and%25201920%2520using%2520digitized%2520church%250Amoving%2520records.%2520These%2520records%252C%2520maintained%2520by%2520Evangelical-Lutheran%2520parishes%252C%250Adocument%2520the%2520migration%2520of%2520individuals%2520and%2520families%2520and%2520offer%2520a%2520valuable%2520source%250Afor%2520studying%2520historical%2520demographic%2520patterns.%2520The%2520dataset%2520includes%2520over%2520six%250Amillion%2520entries%2520extracted%2520from%2520approximately%2520200%252C000%2520images%2520of%2520handwritten%250Amigration%2520records.%250A%2520%2520The%2520data%2520extraction%2520process%2520was%2520automated%2520using%2520a%2520deep%2520learning%2520pipeline%2520that%250Aincluded%2520layout%2520analysis%252C%2520table%2520detection%252C%2520cell%2520classification%252C%2520and%2520handwriting%250Arecognition.%2520The%2520complete%2520pipeline%2520was%2520applied%2520to%2520all%2520images%252C%2520resulting%2520in%2520a%250Astructured%2520dataset%2520suitable%2520for%2520research.%250A%2520%2520The%2520dataset%2520can%2520be%2520used%2520to%2520study%2520internal%2520migration%252C%2520urbanization%252C%2520and%2520family%250Amigration%252C%2520and%2520the%2520spread%2520of%2520disease%2520in%2520preindustrial%2520Finland.%2520A%2520case%2520study%250Afrom%2520the%2520Elim%255C%2522aki%2520parish%2520shows%2520how%2520local%2520migration%2520histories%2520can%2520be%250Areconstructed.%2520The%2520work%2520demonstrates%2520how%2520large%2520volumes%2520of%2520handwritten%2520archival%250Amaterial%2520can%2520be%2520transformed%2520into%2520structured%2520data%2520to%2520support%2520historical%2520and%250Ademographic%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20Historical%20Migration%20Dataset%20from%20Finnish%20Church%20Records%2C%0A%20%201800-1920&entry.906535625=Ari%20Vesalainen%20and%20Jenna%20Kanerva%20and%20Aida%20Nitsch%20and%20Kiia%20Korsu%20and%20Ilari%20Larkiola%20and%20Laura%20Ruotsalainen%20and%20Filip%20Ginter&entry.1292438233=%20%20This%20article%20presents%20a%20large-scale%20effort%20to%20create%20a%20structured%20dataset%20of%0Ainternal%20migration%20in%20Finland%20between%201800%20and%201920%20using%20digitized%20church%0Amoving%20records.%20These%20records%2C%20maintained%20by%20Evangelical-Lutheran%20parishes%2C%0Adocument%20the%20migration%20of%20individuals%20and%20families%20and%20offer%20a%20valuable%20source%0Afor%20studying%20historical%20demographic%20patterns.%20The%20dataset%20includes%20over%20six%0Amillion%20entries%20extracted%20from%20approximately%20200%2C000%20images%20of%20handwritten%0Amigration%20records.%0A%20%20The%20data%20extraction%20process%20was%20automated%20using%20a%20deep%20learning%20pipeline%20that%0Aincluded%20layout%20analysis%2C%20table%20detection%2C%20cell%20classification%2C%20and%20handwriting%0Arecognition.%20The%20complete%20pipeline%20was%20applied%20to%20all%20images%2C%20resulting%20in%20a%0Astructured%20dataset%20suitable%20for%20research.%0A%20%20The%20dataset%20can%20be%20used%20to%20study%20internal%20migration%2C%20urbanization%2C%20and%20family%0Amigration%2C%20and%20the%20spread%20of%20disease%20in%20preindustrial%20Finland.%20A%20case%20study%0Afrom%20the%20Elim%5C%22aki%20parish%20shows%20how%20local%20migration%20histories%20can%20be%0Areconstructed.%20The%20work%20demonstrates%20how%20large%20volumes%20of%20handwritten%20archival%0Amaterial%20can%20be%20transformed%20into%20structured%20data%20to%20support%20historical%20and%0Ademographic%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07960v2&entry.124074799=Read"},
{"title": "The Power of Architecture: Deep Dive into Transformer Architectures for\n  Long-Term Time Series Forecasting", "author": "Lefei Shen and Mouxiang Chen and Han Fu and Xiaoxue Ren and Xiaoyun Joy Wang and Jianling Sun and Zhuo Li and Chenghao Liu", "abstract": "  Transformer-based models have recently become dominant in Long-term Time\nSeries Forecasting (LTSF), yet the variations in their architecture, such as\nencoder-only, encoder-decoder, and decoder-only designs, raise a crucial\nquestion: What Transformer architecture works best for LTSF tasks? However,\nexisting models are often tightly coupled with various time-series-specific\ndesigns, making it difficult to isolate the impact of the architecture itself.\nTo address this, we propose a novel taxonomy that disentangles these designs,\nenabling clearer and more unified comparisons of Transformer architectures. Our\ntaxonomy considers key aspects such as attention mechanisms, forecasting\naggregations, forecasting paradigms, and normalization layers. Through\nextensive experiments, we uncover several key insights: bi-directional\nattention with joint-attention is most effective; more complete forecasting\naggregation improves performance; and the direct-mapping paradigm outperforms\nautoregressive approaches. Furthermore, our combined model, utilizing optimal\narchitectural choices, consistently outperforms several existing models,\nreinforcing the validity of our conclusions. We hope these findings offer\nvaluable guidance for future research on Transformer architectural designs in\nLTSF. Our code is available at https://github.com/HALF111/TSF_architecture.\n", "link": "http://arxiv.org/abs/2507.13043v1", "date": "2025-07-17", "relevancy": 2.0609, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5895}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Power%20of%20Architecture%3A%20Deep%20Dive%20into%20Transformer%20Architectures%20for%0A%20%20Long-Term%20Time%20Series%20Forecasting&body=Title%3A%20The%20Power%20of%20Architecture%3A%20Deep%20Dive%20into%20Transformer%20Architectures%20for%0A%20%20Long-Term%20Time%20Series%20Forecasting%0AAuthor%3A%20Lefei%20Shen%20and%20Mouxiang%20Chen%20and%20Han%20Fu%20and%20Xiaoxue%20Ren%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Zhuo%20Li%20and%20Chenghao%20Liu%0AAbstract%3A%20%20%20Transformer-based%20models%20have%20recently%20become%20dominant%20in%20Long-term%20Time%0ASeries%20Forecasting%20%28LTSF%29%2C%20yet%20the%20variations%20in%20their%20architecture%2C%20such%20as%0Aencoder-only%2C%20encoder-decoder%2C%20and%20decoder-only%20designs%2C%20raise%20a%20crucial%0Aquestion%3A%20What%20Transformer%20architecture%20works%20best%20for%20LTSF%20tasks%3F%20However%2C%0Aexisting%20models%20are%20often%20tightly%20coupled%20with%20various%20time-series-specific%0Adesigns%2C%20making%20it%20difficult%20to%20isolate%20the%20impact%20of%20the%20architecture%20itself.%0ATo%20address%20this%2C%20we%20propose%20a%20novel%20taxonomy%20that%20disentangles%20these%20designs%2C%0Aenabling%20clearer%20and%20more%20unified%20comparisons%20of%20Transformer%20architectures.%20Our%0Ataxonomy%20considers%20key%20aspects%20such%20as%20attention%20mechanisms%2C%20forecasting%0Aaggregations%2C%20forecasting%20paradigms%2C%20and%20normalization%20layers.%20Through%0Aextensive%20experiments%2C%20we%20uncover%20several%20key%20insights%3A%20bi-directional%0Aattention%20with%20joint-attention%20is%20most%20effective%3B%20more%20complete%20forecasting%0Aaggregation%20improves%20performance%3B%20and%20the%20direct-mapping%20paradigm%20outperforms%0Aautoregressive%20approaches.%20Furthermore%2C%20our%20combined%20model%2C%20utilizing%20optimal%0Aarchitectural%20choices%2C%20consistently%20outperforms%20several%20existing%20models%2C%0Areinforcing%20the%20validity%20of%20our%20conclusions.%20We%20hope%20these%20findings%20offer%0Avaluable%20guidance%20for%20future%20research%20on%20Transformer%20architectural%20designs%20in%0ALTSF.%20Our%20code%20is%20available%20at%20https%3A//github.com/HALF111/TSF_architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Power%2520of%2520Architecture%253A%2520Deep%2520Dive%2520into%2520Transformer%2520Architectures%2520for%250A%2520%2520Long-Term%2520Time%2520Series%2520Forecasting%26entry.906535625%3DLefei%2520Shen%2520and%2520Mouxiang%2520Chen%2520and%2520Han%2520Fu%2520and%2520Xiaoxue%2520Ren%2520and%2520Xiaoyun%2520Joy%2520Wang%2520and%2520Jianling%2520Sun%2520and%2520Zhuo%2520Li%2520and%2520Chenghao%2520Liu%26entry.1292438233%3D%2520%2520Transformer-based%2520models%2520have%2520recently%2520become%2520dominant%2520in%2520Long-term%2520Time%250ASeries%2520Forecasting%2520%2528LTSF%2529%252C%2520yet%2520the%2520variations%2520in%2520their%2520architecture%252C%2520such%2520as%250Aencoder-only%252C%2520encoder-decoder%252C%2520and%2520decoder-only%2520designs%252C%2520raise%2520a%2520crucial%250Aquestion%253A%2520What%2520Transformer%2520architecture%2520works%2520best%2520for%2520LTSF%2520tasks%253F%2520However%252C%250Aexisting%2520models%2520are%2520often%2520tightly%2520coupled%2520with%2520various%2520time-series-specific%250Adesigns%252C%2520making%2520it%2520difficult%2520to%2520isolate%2520the%2520impact%2520of%2520the%2520architecture%2520itself.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520taxonomy%2520that%2520disentangles%2520these%2520designs%252C%250Aenabling%2520clearer%2520and%2520more%2520unified%2520comparisons%2520of%2520Transformer%2520architectures.%2520Our%250Ataxonomy%2520considers%2520key%2520aspects%2520such%2520as%2520attention%2520mechanisms%252C%2520forecasting%250Aaggregations%252C%2520forecasting%2520paradigms%252C%2520and%2520normalization%2520layers.%2520Through%250Aextensive%2520experiments%252C%2520we%2520uncover%2520several%2520key%2520insights%253A%2520bi-directional%250Aattention%2520with%2520joint-attention%2520is%2520most%2520effective%253B%2520more%2520complete%2520forecasting%250Aaggregation%2520improves%2520performance%253B%2520and%2520the%2520direct-mapping%2520paradigm%2520outperforms%250Aautoregressive%2520approaches.%2520Furthermore%252C%2520our%2520combined%2520model%252C%2520utilizing%2520optimal%250Aarchitectural%2520choices%252C%2520consistently%2520outperforms%2520several%2520existing%2520models%252C%250Areinforcing%2520the%2520validity%2520of%2520our%2520conclusions.%2520We%2520hope%2520these%2520findings%2520offer%250Avaluable%2520guidance%2520for%2520future%2520research%2520on%2520Transformer%2520architectural%2520designs%2520in%250ALTSF.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/HALF111/TSF_architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Power%20of%20Architecture%3A%20Deep%20Dive%20into%20Transformer%20Architectures%20for%0A%20%20Long-Term%20Time%20Series%20Forecasting&entry.906535625=Lefei%20Shen%20and%20Mouxiang%20Chen%20and%20Han%20Fu%20and%20Xiaoxue%20Ren%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Zhuo%20Li%20and%20Chenghao%20Liu&entry.1292438233=%20%20Transformer-based%20models%20have%20recently%20become%20dominant%20in%20Long-term%20Time%0ASeries%20Forecasting%20%28LTSF%29%2C%20yet%20the%20variations%20in%20their%20architecture%2C%20such%20as%0Aencoder-only%2C%20encoder-decoder%2C%20and%20decoder-only%20designs%2C%20raise%20a%20crucial%0Aquestion%3A%20What%20Transformer%20architecture%20works%20best%20for%20LTSF%20tasks%3F%20However%2C%0Aexisting%20models%20are%20often%20tightly%20coupled%20with%20various%20time-series-specific%0Adesigns%2C%20making%20it%20difficult%20to%20isolate%20the%20impact%20of%20the%20architecture%20itself.%0ATo%20address%20this%2C%20we%20propose%20a%20novel%20taxonomy%20that%20disentangles%20these%20designs%2C%0Aenabling%20clearer%20and%20more%20unified%20comparisons%20of%20Transformer%20architectures.%20Our%0Ataxonomy%20considers%20key%20aspects%20such%20as%20attention%20mechanisms%2C%20forecasting%0Aaggregations%2C%20forecasting%20paradigms%2C%20and%20normalization%20layers.%20Through%0Aextensive%20experiments%2C%20we%20uncover%20several%20key%20insights%3A%20bi-directional%0Aattention%20with%20joint-attention%20is%20most%20effective%3B%20more%20complete%20forecasting%0Aaggregation%20improves%20performance%3B%20and%20the%20direct-mapping%20paradigm%20outperforms%0Aautoregressive%20approaches.%20Furthermore%2C%20our%20combined%20model%2C%20utilizing%20optimal%0Aarchitectural%20choices%2C%20consistently%20outperforms%20several%20existing%20models%2C%0Areinforcing%20the%20validity%20of%20our%20conclusions.%20We%20hope%20these%20findings%20offer%0Avaluable%20guidance%20for%20future%20research%20on%20Transformer%20architectural%20designs%20in%0ALTSF.%20Our%20code%20is%20available%20at%20https%3A//github.com/HALF111/TSF_architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13043v1&entry.124074799=Read"},
{"title": "Unsupervised Ground Metric Learning", "author": "Janis Auffenberg and Jonas Bresch and Oleh Melnyk and Gabriele Steidl", "abstract": "  Data classification without access to labeled samples remains a challenging\nproblem. It usually depends on an appropriately chosen distance between\nfeatures, a topic addressed in metric learning. Recently, Huizing, Cantini and\nPeyr\\'e proposed to simultaneously learn optimal transport (OT) cost matrices\nbetween samples and features of the dataset. This leads to the task of finding\npositive eigenvectors of a certain nonlinear function that maps cost matrices\nto OT distances. Having this basic idea in mind, we consider both the\nalgorithmic and the modeling part of unsupervised metric learning. First, we\nexamine appropriate algorithms and their convergence. In particular, we propose\nto use the stochastic random function iteration algorithm and prove that it\nconverges linearly for our setting, although our operators are not\nparacontractive as it was required for convergence so far. Second, we ask the\nnatural question if the OT distance can be replaced by other distances. We show\nhow Mahalanobis-like distances fit into our considerations. Further, we examine\nan approach via graph Laplacians. In contrast to the previous settings, we have\njust to deal with linear functions in the wanted matrices here, so that simple\nalgorithms from linear algebra can be applied.\n", "link": "http://arxiv.org/abs/2507.13094v1", "date": "2025-07-17", "relevancy": 2.0527, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Ground%20Metric%20Learning&body=Title%3A%20Unsupervised%20Ground%20Metric%20Learning%0AAuthor%3A%20Janis%20Auffenberg%20and%20Jonas%20Bresch%20and%20Oleh%20Melnyk%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Data%20classification%20without%20access%20to%20labeled%20samples%20remains%20a%20challenging%0Aproblem.%20It%20usually%20depends%20on%20an%20appropriately%20chosen%20distance%20between%0Afeatures%2C%20a%20topic%20addressed%20in%20metric%20learning.%20Recently%2C%20Huizing%2C%20Cantini%20and%0APeyr%5C%27e%20proposed%20to%20simultaneously%20learn%20optimal%20transport%20%28OT%29%20cost%20matrices%0Abetween%20samples%20and%20features%20of%20the%20dataset.%20This%20leads%20to%20the%20task%20of%20finding%0Apositive%20eigenvectors%20of%20a%20certain%20nonlinear%20function%20that%20maps%20cost%20matrices%0Ato%20OT%20distances.%20Having%20this%20basic%20idea%20in%20mind%2C%20we%20consider%20both%20the%0Aalgorithmic%20and%20the%20modeling%20part%20of%20unsupervised%20metric%20learning.%20First%2C%20we%0Aexamine%20appropriate%20algorithms%20and%20their%20convergence.%20In%20particular%2C%20we%20propose%0Ato%20use%20the%20stochastic%20random%20function%20iteration%20algorithm%20and%20prove%20that%20it%0Aconverges%20linearly%20for%20our%20setting%2C%20although%20our%20operators%20are%20not%0Aparacontractive%20as%20it%20was%20required%20for%20convergence%20so%20far.%20Second%2C%20we%20ask%20the%0Anatural%20question%20if%20the%20OT%20distance%20can%20be%20replaced%20by%20other%20distances.%20We%20show%0Ahow%20Mahalanobis-like%20distances%20fit%20into%20our%20considerations.%20Further%2C%20we%20examine%0Aan%20approach%20via%20graph%20Laplacians.%20In%20contrast%20to%20the%20previous%20settings%2C%20we%20have%0Ajust%20to%20deal%20with%20linear%20functions%20in%20the%20wanted%20matrices%20here%2C%20so%20that%20simple%0Aalgorithms%20from%20linear%20algebra%20can%20be%20applied.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Ground%2520Metric%2520Learning%26entry.906535625%3DJanis%2520Auffenberg%2520and%2520Jonas%2520Bresch%2520and%2520Oleh%2520Melnyk%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Data%2520classification%2520without%2520access%2520to%2520labeled%2520samples%2520remains%2520a%2520challenging%250Aproblem.%2520It%2520usually%2520depends%2520on%2520an%2520appropriately%2520chosen%2520distance%2520between%250Afeatures%252C%2520a%2520topic%2520addressed%2520in%2520metric%2520learning.%2520Recently%252C%2520Huizing%252C%2520Cantini%2520and%250APeyr%255C%2527e%2520proposed%2520to%2520simultaneously%2520learn%2520optimal%2520transport%2520%2528OT%2529%2520cost%2520matrices%250Abetween%2520samples%2520and%2520features%2520of%2520the%2520dataset.%2520This%2520leads%2520to%2520the%2520task%2520of%2520finding%250Apositive%2520eigenvectors%2520of%2520a%2520certain%2520nonlinear%2520function%2520that%2520maps%2520cost%2520matrices%250Ato%2520OT%2520distances.%2520Having%2520this%2520basic%2520idea%2520in%2520mind%252C%2520we%2520consider%2520both%2520the%250Aalgorithmic%2520and%2520the%2520modeling%2520part%2520of%2520unsupervised%2520metric%2520learning.%2520First%252C%2520we%250Aexamine%2520appropriate%2520algorithms%2520and%2520their%2520convergence.%2520In%2520particular%252C%2520we%2520propose%250Ato%2520use%2520the%2520stochastic%2520random%2520function%2520iteration%2520algorithm%2520and%2520prove%2520that%2520it%250Aconverges%2520linearly%2520for%2520our%2520setting%252C%2520although%2520our%2520operators%2520are%2520not%250Aparacontractive%2520as%2520it%2520was%2520required%2520for%2520convergence%2520so%2520far.%2520Second%252C%2520we%2520ask%2520the%250Anatural%2520question%2520if%2520the%2520OT%2520distance%2520can%2520be%2520replaced%2520by%2520other%2520distances.%2520We%2520show%250Ahow%2520Mahalanobis-like%2520distances%2520fit%2520into%2520our%2520considerations.%2520Further%252C%2520we%2520examine%250Aan%2520approach%2520via%2520graph%2520Laplacians.%2520In%2520contrast%2520to%2520the%2520previous%2520settings%252C%2520we%2520have%250Ajust%2520to%2520deal%2520with%2520linear%2520functions%2520in%2520the%2520wanted%2520matrices%2520here%252C%2520so%2520that%2520simple%250Aalgorithms%2520from%2520linear%2520algebra%2520can%2520be%2520applied.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Ground%20Metric%20Learning&entry.906535625=Janis%20Auffenberg%20and%20Jonas%20Bresch%20and%20Oleh%20Melnyk%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Data%20classification%20without%20access%20to%20labeled%20samples%20remains%20a%20challenging%0Aproblem.%20It%20usually%20depends%20on%20an%20appropriately%20chosen%20distance%20between%0Afeatures%2C%20a%20topic%20addressed%20in%20metric%20learning.%20Recently%2C%20Huizing%2C%20Cantini%20and%0APeyr%5C%27e%20proposed%20to%20simultaneously%20learn%20optimal%20transport%20%28OT%29%20cost%20matrices%0Abetween%20samples%20and%20features%20of%20the%20dataset.%20This%20leads%20to%20the%20task%20of%20finding%0Apositive%20eigenvectors%20of%20a%20certain%20nonlinear%20function%20that%20maps%20cost%20matrices%0Ato%20OT%20distances.%20Having%20this%20basic%20idea%20in%20mind%2C%20we%20consider%20both%20the%0Aalgorithmic%20and%20the%20modeling%20part%20of%20unsupervised%20metric%20learning.%20First%2C%20we%0Aexamine%20appropriate%20algorithms%20and%20their%20convergence.%20In%20particular%2C%20we%20propose%0Ato%20use%20the%20stochastic%20random%20function%20iteration%20algorithm%20and%20prove%20that%20it%0Aconverges%20linearly%20for%20our%20setting%2C%20although%20our%20operators%20are%20not%0Aparacontractive%20as%20it%20was%20required%20for%20convergence%20so%20far.%20Second%2C%20we%20ask%20the%0Anatural%20question%20if%20the%20OT%20distance%20can%20be%20replaced%20by%20other%20distances.%20We%20show%0Ahow%20Mahalanobis-like%20distances%20fit%20into%20our%20considerations.%20Further%2C%20we%20examine%0Aan%20approach%20via%20graph%20Laplacians.%20In%20contrast%20to%20the%20previous%20settings%2C%20we%20have%0Ajust%20to%20deal%20with%20linear%20functions%20in%20the%20wanted%20matrices%20here%2C%20so%20that%20simple%0Aalgorithms%20from%20linear%20algebra%20can%20be%20applied.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13094v1&entry.124074799=Read"},
{"title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts", "author": "Marc Brinner and Sina Zarriess", "abstract": "  We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.\n", "link": "http://arxiv.org/abs/2507.13105v1", "date": "2025-07-17", "relevancy": 2.047, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemCSE%3A%20Semantic%20Contrastive%20Sentence%20Embeddings%20Using%20LLM-Generated%0A%20%20Summaries%20For%20Scientific%20Abstracts&body=Title%3A%20SemCSE%3A%20Semantic%20Contrastive%20Sentence%20Embeddings%20Using%20LLM-Generated%0A%20%20Summaries%20For%20Scientific%20Abstracts%0AAuthor%3A%20Marc%20Brinner%20and%20Sina%20Zarriess%0AAbstract%3A%20%20%20We%20introduce%20SemCSE%2C%20an%20unsupervised%20method%20for%20learning%20semantic%20embeddings%0Aof%20scientific%20texts.%20Building%20on%20recent%20advances%20in%20contrastive%20learning%20for%0Atext%20embeddings%2C%20our%20approach%20leverages%20LLM-generated%20summaries%20of%20scientific%0Aabstracts%20to%20train%20a%20model%20that%20positions%20semantically%20related%20summaries%20closer%0Atogether%20in%20the%20embedding%20space.%20This%20resulting%20objective%20ensures%20that%20the%0Amodel%20captures%20the%20true%20semantic%20content%20of%20a%20text%2C%20in%20contrast%20to%20traditional%0Acitation-based%20approaches%20that%20do%20not%20necessarily%20reflect%20semantic%20similarity.%0ATo%20validate%20this%2C%20we%20propose%20a%20novel%20benchmark%20designed%20to%20assess%20a%20model%27s%0Aability%20to%20understand%20and%20encode%20the%20semantic%20content%20of%20scientific%20texts%2C%0Ademonstrating%20that%20our%20method%20enforces%20a%20stronger%20semantic%20separation%20within%0Athe%20embedding%20space.%20Additionally%2C%20we%20evaluate%20SemCSE%20on%20the%20comprehensive%0ASciRepEval%20benchmark%20for%20scientific%20text%20embeddings%2C%20where%20it%20achieves%0Astate-of-the-art%20performance%20among%20models%20of%20its%20size%2C%20thus%20highlighting%20the%0Abenefits%20of%20a%20semantically%20focused%20training%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemCSE%253A%2520Semantic%2520Contrastive%2520Sentence%2520Embeddings%2520Using%2520LLM-Generated%250A%2520%2520Summaries%2520For%2520Scientific%2520Abstracts%26entry.906535625%3DMarc%2520Brinner%2520and%2520Sina%2520Zarriess%26entry.1292438233%3D%2520%2520We%2520introduce%2520SemCSE%252C%2520an%2520unsupervised%2520method%2520for%2520learning%2520semantic%2520embeddings%250Aof%2520scientific%2520texts.%2520Building%2520on%2520recent%2520advances%2520in%2520contrastive%2520learning%2520for%250Atext%2520embeddings%252C%2520our%2520approach%2520leverages%2520LLM-generated%2520summaries%2520of%2520scientific%250Aabstracts%2520to%2520train%2520a%2520model%2520that%2520positions%2520semantically%2520related%2520summaries%2520closer%250Atogether%2520in%2520the%2520embedding%2520space.%2520This%2520resulting%2520objective%2520ensures%2520that%2520the%250Amodel%2520captures%2520the%2520true%2520semantic%2520content%2520of%2520a%2520text%252C%2520in%2520contrast%2520to%2520traditional%250Acitation-based%2520approaches%2520that%2520do%2520not%2520necessarily%2520reflect%2520semantic%2520similarity.%250ATo%2520validate%2520this%252C%2520we%2520propose%2520a%2520novel%2520benchmark%2520designed%2520to%2520assess%2520a%2520model%2527s%250Aability%2520to%2520understand%2520and%2520encode%2520the%2520semantic%2520content%2520of%2520scientific%2520texts%252C%250Ademonstrating%2520that%2520our%2520method%2520enforces%2520a%2520stronger%2520semantic%2520separation%2520within%250Athe%2520embedding%2520space.%2520Additionally%252C%2520we%2520evaluate%2520SemCSE%2520on%2520the%2520comprehensive%250ASciRepEval%2520benchmark%2520for%2520scientific%2520text%2520embeddings%252C%2520where%2520it%2520achieves%250Astate-of-the-art%2520performance%2520among%2520models%2520of%2520its%2520size%252C%2520thus%2520highlighting%2520the%250Abenefits%2520of%2520a%2520semantically%2520focused%2520training%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemCSE%3A%20Semantic%20Contrastive%20Sentence%20Embeddings%20Using%20LLM-Generated%0A%20%20Summaries%20For%20Scientific%20Abstracts&entry.906535625=Marc%20Brinner%20and%20Sina%20Zarriess&entry.1292438233=%20%20We%20introduce%20SemCSE%2C%20an%20unsupervised%20method%20for%20learning%20semantic%20embeddings%0Aof%20scientific%20texts.%20Building%20on%20recent%20advances%20in%20contrastive%20learning%20for%0Atext%20embeddings%2C%20our%20approach%20leverages%20LLM-generated%20summaries%20of%20scientific%0Aabstracts%20to%20train%20a%20model%20that%20positions%20semantically%20related%20summaries%20closer%0Atogether%20in%20the%20embedding%20space.%20This%20resulting%20objective%20ensures%20that%20the%0Amodel%20captures%20the%20true%20semantic%20content%20of%20a%20text%2C%20in%20contrast%20to%20traditional%0Acitation-based%20approaches%20that%20do%20not%20necessarily%20reflect%20semantic%20similarity.%0ATo%20validate%20this%2C%20we%20propose%20a%20novel%20benchmark%20designed%20to%20assess%20a%20model%27s%0Aability%20to%20understand%20and%20encode%20the%20semantic%20content%20of%20scientific%20texts%2C%0Ademonstrating%20that%20our%20method%20enforces%20a%20stronger%20semantic%20separation%20within%0Athe%20embedding%20space.%20Additionally%2C%20we%20evaluate%20SemCSE%20on%20the%20comprehensive%0ASciRepEval%20benchmark%20for%20scientific%20text%20embeddings%2C%20where%20it%20achieves%0Astate-of-the-art%20performance%20among%20models%20of%20its%20size%2C%20thus%20highlighting%20the%0Abenefits%20of%20a%20semantically%20focused%20training%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13105v1&entry.124074799=Read"},
{"title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects\n  in Remote Sensing Images", "author": "Xiaozheng Jiang and Wei Zhang and Xuerui Mao", "abstract": "  Detecting tiny objects in remote sensing (RS) imagery has been a\nlong-standing challenge due to their extremely limited spatial information,\nweak feature representations, and dense distributions across complex\nbackgrounds. Despite numerous efforts devoted, mainstream detectors still\nunderperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a\nmulti-stage feature fusion and enhancement model explicitly tailored for RS\ntiny object detection in various RS scenarios. RS-TinyNet comes with two novel\ndesigns: tiny object saliency modeling and feature integrity reconstruction.\nGuided by these principles, we design three step-wise feature enhancement\nmodules. Among them, the multi-dimensional collaborative attention (MDCA)\nmodule employs multi-dimensional attention to enhance the saliency of tiny\nobjects. Additionally, the auxiliary reversible branch (ARB) and a progressive\nfusion detection head (PFDH) module are introduced to preserve information flow\nand fuse multi-level features to bridge semantic gaps and retain structural\ndetail. Comprehensive experiments on public RS dataset AI-TOD show that our\nRS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and\n6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior\ndetection performance in diverse RS scenarios. These results demonstrate that\nthe proposed multi-stage feature fusion strategy offers an effective and\npractical solution for tiny object detection in complex RS environments.\n", "link": "http://arxiv.org/abs/2507.13120v1", "date": "2025-07-17", "relevancy": 2.0456, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5284}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RS-TinyNet%3A%20Stage-wise%20Feature%20Fusion%20Network%20for%20Detecting%20Tiny%20Objects%0A%20%20in%20Remote%20Sensing%20Images&body=Title%3A%20RS-TinyNet%3A%20Stage-wise%20Feature%20Fusion%20Network%20for%20Detecting%20Tiny%20Objects%0A%20%20in%20Remote%20Sensing%20Images%0AAuthor%3A%20Xiaozheng%20Jiang%20and%20Wei%20Zhang%20and%20Xuerui%20Mao%0AAbstract%3A%20%20%20Detecting%20tiny%20objects%20in%20remote%20sensing%20%28RS%29%20imagery%20has%20been%20a%0Along-standing%20challenge%20due%20to%20their%20extremely%20limited%20spatial%20information%2C%0Aweak%20feature%20representations%2C%20and%20dense%20distributions%20across%20complex%0Abackgrounds.%20Despite%20numerous%20efforts%20devoted%2C%20mainstream%20detectors%20still%0Aunderperform%20in%20such%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RS-TinyNet%2C%20a%0Amulti-stage%20feature%20fusion%20and%20enhancement%20model%20explicitly%20tailored%20for%20RS%0Atiny%20object%20detection%20in%20various%20RS%20scenarios.%20RS-TinyNet%20comes%20with%20two%20novel%0Adesigns%3A%20tiny%20object%20saliency%20modeling%20and%20feature%20integrity%20reconstruction.%0AGuided%20by%20these%20principles%2C%20we%20design%20three%20step-wise%20feature%20enhancement%0Amodules.%20Among%20them%2C%20the%20multi-dimensional%20collaborative%20attention%20%28MDCA%29%0Amodule%20employs%20multi-dimensional%20attention%20to%20enhance%20the%20saliency%20of%20tiny%0Aobjects.%20Additionally%2C%20the%20auxiliary%20reversible%20branch%20%28ARB%29%20and%20a%20progressive%0Afusion%20detection%20head%20%28PFDH%29%20module%20are%20introduced%20to%20preserve%20information%20flow%0Aand%20fuse%20multi-level%20features%20to%20bridge%20semantic%20gaps%20and%20retain%20structural%0Adetail.%20Comprehensive%20experiments%20on%20public%20RS%20dataset%20AI-TOD%20show%20that%20our%0ARS-TinyNet%20surpasses%20existing%20state-of-the-art%20%28SOTA%29%20detectors%20by%204.0%25%20AP%20and%0A6.5%25%20AP75.%20Evaluations%20on%20DIOR%20benchmark%20dataset%20further%20validate%20its%20superior%0Adetection%20performance%20in%20diverse%20RS%20scenarios.%20These%20results%20demonstrate%20that%0Athe%20proposed%20multi-stage%20feature%20fusion%20strategy%20offers%20an%20effective%20and%0Apractical%20solution%20for%20tiny%20object%20detection%20in%20complex%20RS%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRS-TinyNet%253A%2520Stage-wise%2520Feature%2520Fusion%2520Network%2520for%2520Detecting%2520Tiny%2520Objects%250A%2520%2520in%2520Remote%2520Sensing%2520Images%26entry.906535625%3DXiaozheng%2520Jiang%2520and%2520Wei%2520Zhang%2520and%2520Xuerui%2520Mao%26entry.1292438233%3D%2520%2520Detecting%2520tiny%2520objects%2520in%2520remote%2520sensing%2520%2528RS%2529%2520imagery%2520has%2520been%2520a%250Along-standing%2520challenge%2520due%2520to%2520their%2520extremely%2520limited%2520spatial%2520information%252C%250Aweak%2520feature%2520representations%252C%2520and%2520dense%2520distributions%2520across%2520complex%250Abackgrounds.%2520Despite%2520numerous%2520efforts%2520devoted%252C%2520mainstream%2520detectors%2520still%250Aunderperform%2520in%2520such%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520RS-TinyNet%252C%2520a%250Amulti-stage%2520feature%2520fusion%2520and%2520enhancement%2520model%2520explicitly%2520tailored%2520for%2520RS%250Atiny%2520object%2520detection%2520in%2520various%2520RS%2520scenarios.%2520RS-TinyNet%2520comes%2520with%2520two%2520novel%250Adesigns%253A%2520tiny%2520object%2520saliency%2520modeling%2520and%2520feature%2520integrity%2520reconstruction.%250AGuided%2520by%2520these%2520principles%252C%2520we%2520design%2520three%2520step-wise%2520feature%2520enhancement%250Amodules.%2520Among%2520them%252C%2520the%2520multi-dimensional%2520collaborative%2520attention%2520%2528MDCA%2529%250Amodule%2520employs%2520multi-dimensional%2520attention%2520to%2520enhance%2520the%2520saliency%2520of%2520tiny%250Aobjects.%2520Additionally%252C%2520the%2520auxiliary%2520reversible%2520branch%2520%2528ARB%2529%2520and%2520a%2520progressive%250Afusion%2520detection%2520head%2520%2528PFDH%2529%2520module%2520are%2520introduced%2520to%2520preserve%2520information%2520flow%250Aand%2520fuse%2520multi-level%2520features%2520to%2520bridge%2520semantic%2520gaps%2520and%2520retain%2520structural%250Adetail.%2520Comprehensive%2520experiments%2520on%2520public%2520RS%2520dataset%2520AI-TOD%2520show%2520that%2520our%250ARS-TinyNet%2520surpasses%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520detectors%2520by%25204.0%2525%2520AP%2520and%250A6.5%2525%2520AP75.%2520Evaluations%2520on%2520DIOR%2520benchmark%2520dataset%2520further%2520validate%2520its%2520superior%250Adetection%2520performance%2520in%2520diverse%2520RS%2520scenarios.%2520These%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520multi-stage%2520feature%2520fusion%2520strategy%2520offers%2520an%2520effective%2520and%250Apractical%2520solution%2520for%2520tiny%2520object%2520detection%2520in%2520complex%2520RS%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RS-TinyNet%3A%20Stage-wise%20Feature%20Fusion%20Network%20for%20Detecting%20Tiny%20Objects%0A%20%20in%20Remote%20Sensing%20Images&entry.906535625=Xiaozheng%20Jiang%20and%20Wei%20Zhang%20and%20Xuerui%20Mao&entry.1292438233=%20%20Detecting%20tiny%20objects%20in%20remote%20sensing%20%28RS%29%20imagery%20has%20been%20a%0Along-standing%20challenge%20due%20to%20their%20extremely%20limited%20spatial%20information%2C%0Aweak%20feature%20representations%2C%20and%20dense%20distributions%20across%20complex%0Abackgrounds.%20Despite%20numerous%20efforts%20devoted%2C%20mainstream%20detectors%20still%0Aunderperform%20in%20such%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RS-TinyNet%2C%20a%0Amulti-stage%20feature%20fusion%20and%20enhancement%20model%20explicitly%20tailored%20for%20RS%0Atiny%20object%20detection%20in%20various%20RS%20scenarios.%20RS-TinyNet%20comes%20with%20two%20novel%0Adesigns%3A%20tiny%20object%20saliency%20modeling%20and%20feature%20integrity%20reconstruction.%0AGuided%20by%20these%20principles%2C%20we%20design%20three%20step-wise%20feature%20enhancement%0Amodules.%20Among%20them%2C%20the%20multi-dimensional%20collaborative%20attention%20%28MDCA%29%0Amodule%20employs%20multi-dimensional%20attention%20to%20enhance%20the%20saliency%20of%20tiny%0Aobjects.%20Additionally%2C%20the%20auxiliary%20reversible%20branch%20%28ARB%29%20and%20a%20progressive%0Afusion%20detection%20head%20%28PFDH%29%20module%20are%20introduced%20to%20preserve%20information%20flow%0Aand%20fuse%20multi-level%20features%20to%20bridge%20semantic%20gaps%20and%20retain%20structural%0Adetail.%20Comprehensive%20experiments%20on%20public%20RS%20dataset%20AI-TOD%20show%20that%20our%0ARS-TinyNet%20surpasses%20existing%20state-of-the-art%20%28SOTA%29%20detectors%20by%204.0%25%20AP%20and%0A6.5%25%20AP75.%20Evaluations%20on%20DIOR%20benchmark%20dataset%20further%20validate%20its%20superior%0Adetection%20performance%20in%20diverse%20RS%20scenarios.%20These%20results%20demonstrate%20that%0Athe%20proposed%20multi-stage%20feature%20fusion%20strategy%20offers%20an%20effective%20and%0Apractical%20solution%20for%20tiny%20object%20detection%20in%20complex%20RS%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13120v1&entry.124074799=Read"},
{"title": "Bounding the Worst-class Error: A Boosting Approach", "author": "Yuya Saito and Shinnosuke Matsuo and Seiichi Uchida and Daiki Suehiro", "abstract": "  This paper tackles the problem of the worst-class error rate, instead of the\nstandard error rate averaged over all classes. For example, a three-class\nclassification task with class-wise error rates of 10%, 10%, and 40% has a\nworst-class error rate of 40%, whereas the average is 20% under the\nclass-balanced condition. The worst-class error is important in many\napplications. For example, in a medical image classification task, it would not\nbe acceptable for the malignant tumor class to have a 40% error rate, while the\nbenign and healthy classes have a 10% error rates. To avoid overfitting in\nworst-class error minimization using Deep Neural Networks (DNNs), we design a\nproblem formulation for bounding the worst-class error instead of achieving\nzero worst-class error. Moreover, to correctly bound the worst-class error, we\npropose a boosting approach which ensembles DNNs. We give training and\ngeneralization worst-class-error bound. Experimental results show that the\nalgorithm lowers worst-class test error rates while avoiding overfitting to the\ntraining set. This code is available at\nhttps://github.com/saito-yuya/Bounding-the-Worst-class-error-A-Boosting-Approach.\n", "link": "http://arxiv.org/abs/2310.14890v3", "date": "2025-07-17", "relevancy": 2.0397, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5597}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4862}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bounding%20the%20Worst-class%20Error%3A%20A%20Boosting%20Approach&body=Title%3A%20Bounding%20the%20Worst-class%20Error%3A%20A%20Boosting%20Approach%0AAuthor%3A%20Yuya%20Saito%20and%20Shinnosuke%20Matsuo%20and%20Seiichi%20Uchida%20and%20Daiki%20Suehiro%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20problem%20of%20the%20worst-class%20error%20rate%2C%20instead%20of%20the%0Astandard%20error%20rate%20averaged%20over%20all%20classes.%20For%20example%2C%20a%20three-class%0Aclassification%20task%20with%20class-wise%20error%20rates%20of%2010%25%2C%2010%25%2C%20and%2040%25%20has%20a%0Aworst-class%20error%20rate%20of%2040%25%2C%20whereas%20the%20average%20is%2020%25%20under%20the%0Aclass-balanced%20condition.%20The%20worst-class%20error%20is%20important%20in%20many%0Aapplications.%20For%20example%2C%20in%20a%20medical%20image%20classification%20task%2C%20it%20would%20not%0Abe%20acceptable%20for%20the%20malignant%20tumor%20class%20to%20have%20a%2040%25%20error%20rate%2C%20while%20the%0Abenign%20and%20healthy%20classes%20have%20a%2010%25%20error%20rates.%20To%20avoid%20overfitting%20in%0Aworst-class%20error%20minimization%20using%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20we%20design%20a%0Aproblem%20formulation%20for%20bounding%20the%20worst-class%20error%20instead%20of%20achieving%0Azero%20worst-class%20error.%20Moreover%2C%20to%20correctly%20bound%20the%20worst-class%20error%2C%20we%0Apropose%20a%20boosting%20approach%20which%20ensembles%20DNNs.%20We%20give%20training%20and%0Ageneralization%20worst-class-error%20bound.%20Experimental%20results%20show%20that%20the%0Aalgorithm%20lowers%20worst-class%20test%20error%20rates%20while%20avoiding%20overfitting%20to%20the%0Atraining%20set.%20This%20code%20is%20available%20at%0Ahttps%3A//github.com/saito-yuya/Bounding-the-Worst-class-error-A-Boosting-Approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBounding%2520the%2520Worst-class%2520Error%253A%2520A%2520Boosting%2520Approach%26entry.906535625%3DYuya%2520Saito%2520and%2520Shinnosuke%2520Matsuo%2520and%2520Seiichi%2520Uchida%2520and%2520Daiki%2520Suehiro%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520problem%2520of%2520the%2520worst-class%2520error%2520rate%252C%2520instead%2520of%2520the%250Astandard%2520error%2520rate%2520averaged%2520over%2520all%2520classes.%2520For%2520example%252C%2520a%2520three-class%250Aclassification%2520task%2520with%2520class-wise%2520error%2520rates%2520of%252010%2525%252C%252010%2525%252C%2520and%252040%2525%2520has%2520a%250Aworst-class%2520error%2520rate%2520of%252040%2525%252C%2520whereas%2520the%2520average%2520is%252020%2525%2520under%2520the%250Aclass-balanced%2520condition.%2520The%2520worst-class%2520error%2520is%2520important%2520in%2520many%250Aapplications.%2520For%2520example%252C%2520in%2520a%2520medical%2520image%2520classification%2520task%252C%2520it%2520would%2520not%250Abe%2520acceptable%2520for%2520the%2520malignant%2520tumor%2520class%2520to%2520have%2520a%252040%2525%2520error%2520rate%252C%2520while%2520the%250Abenign%2520and%2520healthy%2520classes%2520have%2520a%252010%2525%2520error%2520rates.%2520To%2520avoid%2520overfitting%2520in%250Aworst-class%2520error%2520minimization%2520using%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520we%2520design%2520a%250Aproblem%2520formulation%2520for%2520bounding%2520the%2520worst-class%2520error%2520instead%2520of%2520achieving%250Azero%2520worst-class%2520error.%2520Moreover%252C%2520to%2520correctly%2520bound%2520the%2520worst-class%2520error%252C%2520we%250Apropose%2520a%2520boosting%2520approach%2520which%2520ensembles%2520DNNs.%2520We%2520give%2520training%2520and%250Ageneralization%2520worst-class-error%2520bound.%2520Experimental%2520results%2520show%2520that%2520the%250Aalgorithm%2520lowers%2520worst-class%2520test%2520error%2520rates%2520while%2520avoiding%2520overfitting%2520to%2520the%250Atraining%2520set.%2520This%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/saito-yuya/Bounding-the-Worst-class-error-A-Boosting-Approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounding%20the%20Worst-class%20Error%3A%20A%20Boosting%20Approach&entry.906535625=Yuya%20Saito%20and%20Shinnosuke%20Matsuo%20and%20Seiichi%20Uchida%20and%20Daiki%20Suehiro&entry.1292438233=%20%20This%20paper%20tackles%20the%20problem%20of%20the%20worst-class%20error%20rate%2C%20instead%20of%20the%0Astandard%20error%20rate%20averaged%20over%20all%20classes.%20For%20example%2C%20a%20three-class%0Aclassification%20task%20with%20class-wise%20error%20rates%20of%2010%25%2C%2010%25%2C%20and%2040%25%20has%20a%0Aworst-class%20error%20rate%20of%2040%25%2C%20whereas%20the%20average%20is%2020%25%20under%20the%0Aclass-balanced%20condition.%20The%20worst-class%20error%20is%20important%20in%20many%0Aapplications.%20For%20example%2C%20in%20a%20medical%20image%20classification%20task%2C%20it%20would%20not%0Abe%20acceptable%20for%20the%20malignant%20tumor%20class%20to%20have%20a%2040%25%20error%20rate%2C%20while%20the%0Abenign%20and%20healthy%20classes%20have%20a%2010%25%20error%20rates.%20To%20avoid%20overfitting%20in%0Aworst-class%20error%20minimization%20using%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20we%20design%20a%0Aproblem%20formulation%20for%20bounding%20the%20worst-class%20error%20instead%20of%20achieving%0Azero%20worst-class%20error.%20Moreover%2C%20to%20correctly%20bound%20the%20worst-class%20error%2C%20we%0Apropose%20a%20boosting%20approach%20which%20ensembles%20DNNs.%20We%20give%20training%20and%0Ageneralization%20worst-class-error%20bound.%20Experimental%20results%20show%20that%20the%0Aalgorithm%20lowers%20worst-class%20test%20error%20rates%20while%20avoiding%20overfitting%20to%20the%0Atraining%20set.%20This%20code%20is%20available%20at%0Ahttps%3A//github.com/saito-yuya/Bounding-the-Worst-class-error-A-Boosting-Approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14890v3&entry.124074799=Read"},
{"title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation", "author": "Jiazheng Li and Hong Lu and Kaiyue Wen and Zaiwen Yang and Jiaxuan Gao and Hongzhou Lin and Yi Wu and Jingzhao Zhang", "abstract": "  Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.\n", "link": "http://arxiv.org/abs/2507.13266v1", "date": "2025-07-17", "relevancy": 2.0389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuestA%3A%20Expanding%20Reasoning%20Capacity%20in%20LLMs%20via%20Question%20Augmentation&body=Title%3A%20QuestA%3A%20Expanding%20Reasoning%20Capacity%20in%20LLMs%20via%20Question%20Augmentation%0AAuthor%3A%20Jiazheng%20Li%20and%20Hong%20Lu%20and%20Kaiyue%20Wen%20and%20Zaiwen%20Yang%20and%20Jiaxuan%20Gao%20and%20Hongzhou%20Lin%20and%20Yi%20Wu%20and%20Jingzhao%20Zhang%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20become%20a%20key%20component%20in%20training%20large%0Alanguage%20reasoning%20models%20%28LLMs%29.%20However%2C%20recent%20studies%20questions%20its%0Aeffectiveness%20in%20improving%20multi-step%20reasoning-particularly%20on%20hard%20problems.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20simple%20yet%20effective%20strategy%20via%0AQuestion%20Augmentation%3A%20introduce%20partial%20solutions%20during%20training%20to%20reduce%0Aproblem%20difficulty%20and%20provide%20more%20informative%20learning%20signals.%20Our%20method%2C%0AQuestA%2C%20when%20applied%20during%20RL%20training%20on%20math%20reasoning%20tasks%2C%20not%20only%0Aimproves%20pass%401%20but%20also%20pass%40k-particularly%20on%20problems%20where%20standard%20RL%0Astruggles%20to%20make%20progress.%20This%20enables%20continual%20improvement%20over%20strong%0Aopen-source%20models%20such%20as%20DeepScaleR%20and%20OpenMath%20Nemotron%2C%20further%20enhancing%0Atheir%20reasoning%20capabilities.%20We%20achieve%20new%20state-of-the-art%20results%20on%20math%0Abenchmarks%20using%201.5B-parameter%20models%3A%2067.1%25%20%28%2B5.3%25%29%20on%20AIME24%2C%2059.5%25%20%28%2B10.0%25%29%0Aon%20AIME25%2C%20and%2035.5%25%20%28%2B4.0%25%29%20on%20HMMT25.%20Further%2C%20we%20provide%20theoretical%0Aexplanations%20that%20QuestA%20improves%20sample%20efficiency%2C%20offering%20a%20practical%20and%0Ageneralizable%20pathway%20for%20expanding%20reasoning%20capability%20through%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestA%253A%2520Expanding%2520Reasoning%2520Capacity%2520in%2520LLMs%2520via%2520Question%2520Augmentation%26entry.906535625%3DJiazheng%2520Li%2520and%2520Hong%2520Lu%2520and%2520Kaiyue%2520Wen%2520and%2520Zaiwen%2520Yang%2520and%2520Jiaxuan%2520Gao%2520and%2520Hongzhou%2520Lin%2520and%2520Yi%2520Wu%2520and%2520Jingzhao%2520Zhang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520become%2520a%2520key%2520component%2520in%2520training%2520large%250Alanguage%2520reasoning%2520models%2520%2528LLMs%2529.%2520However%252C%2520recent%2520studies%2520questions%2520its%250Aeffectiveness%2520in%2520improving%2520multi-step%2520reasoning-particularly%2520on%2520hard%2520problems.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520strategy%2520via%250AQuestion%2520Augmentation%253A%2520introduce%2520partial%2520solutions%2520during%2520training%2520to%2520reduce%250Aproblem%2520difficulty%2520and%2520provide%2520more%2520informative%2520learning%2520signals.%2520Our%2520method%252C%250AQuestA%252C%2520when%2520applied%2520during%2520RL%2520training%2520on%2520math%2520reasoning%2520tasks%252C%2520not%2520only%250Aimproves%2520pass%25401%2520but%2520also%2520pass%2540k-particularly%2520on%2520problems%2520where%2520standard%2520RL%250Astruggles%2520to%2520make%2520progress.%2520This%2520enables%2520continual%2520improvement%2520over%2520strong%250Aopen-source%2520models%2520such%2520as%2520DeepScaleR%2520and%2520OpenMath%2520Nemotron%252C%2520further%2520enhancing%250Atheir%2520reasoning%2520capabilities.%2520We%2520achieve%2520new%2520state-of-the-art%2520results%2520on%2520math%250Abenchmarks%2520using%25201.5B-parameter%2520models%253A%252067.1%2525%2520%2528%252B5.3%2525%2529%2520on%2520AIME24%252C%252059.5%2525%2520%2528%252B10.0%2525%2529%250Aon%2520AIME25%252C%2520and%252035.5%2525%2520%2528%252B4.0%2525%2529%2520on%2520HMMT25.%2520Further%252C%2520we%2520provide%2520theoretical%250Aexplanations%2520that%2520QuestA%2520improves%2520sample%2520efficiency%252C%2520offering%2520a%2520practical%2520and%250Ageneralizable%2520pathway%2520for%2520expanding%2520reasoning%2520capability%2520through%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuestA%3A%20Expanding%20Reasoning%20Capacity%20in%20LLMs%20via%20Question%20Augmentation&entry.906535625=Jiazheng%20Li%20and%20Hong%20Lu%20and%20Kaiyue%20Wen%20and%20Zaiwen%20Yang%20and%20Jiaxuan%20Gao%20and%20Hongzhou%20Lin%20and%20Yi%20Wu%20and%20Jingzhao%20Zhang&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20become%20a%20key%20component%20in%20training%20large%0Alanguage%20reasoning%20models%20%28LLMs%29.%20However%2C%20recent%20studies%20questions%20its%0Aeffectiveness%20in%20improving%20multi-step%20reasoning-particularly%20on%20hard%20problems.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20simple%20yet%20effective%20strategy%20via%0AQuestion%20Augmentation%3A%20introduce%20partial%20solutions%20during%20training%20to%20reduce%0Aproblem%20difficulty%20and%20provide%20more%20informative%20learning%20signals.%20Our%20method%2C%0AQuestA%2C%20when%20applied%20during%20RL%20training%20on%20math%20reasoning%20tasks%2C%20not%20only%0Aimproves%20pass%401%20but%20also%20pass%40k-particularly%20on%20problems%20where%20standard%20RL%0Astruggles%20to%20make%20progress.%20This%20enables%20continual%20improvement%20over%20strong%0Aopen-source%20models%20such%20as%20DeepScaleR%20and%20OpenMath%20Nemotron%2C%20further%20enhancing%0Atheir%20reasoning%20capabilities.%20We%20achieve%20new%20state-of-the-art%20results%20on%20math%0Abenchmarks%20using%201.5B-parameter%20models%3A%2067.1%25%20%28%2B5.3%25%29%20on%20AIME24%2C%2059.5%25%20%28%2B10.0%25%29%0Aon%20AIME25%2C%20and%2035.5%25%20%28%2B4.0%25%29%20on%20HMMT25.%20Further%2C%20we%20provide%20theoretical%0Aexplanations%20that%20QuestA%20improves%20sample%20efficiency%2C%20offering%20a%20practical%20and%0Ageneralizable%20pathway%20for%20expanding%20reasoning%20capability%20through%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13266v1&entry.124074799=Read"},
{"title": "Branching Stein Variational Gradient Descent for sampling multimodal\n  distributions", "author": "Isa\u00edas Ba\u00f1ales and Arturo Jaramillo and Joshu\u00e9 Hel\u00ed Ricalde-Guerrero", "abstract": "  We propose a novel particle-based variational inference method designed to\nwork with multimodal distributions. Our approach, referred to as Branched Stein\nVariational Gradient Descent (BSVGD), extends the classical Stein Variational\nGradient Descent (SVGD) algorithm by incorporating a random branching mechanism\nthat encourages the exploration of the state space. In this work, a theoretical\nguarantee for the convergence in distribution is presented, as well as\nnumerical experiments to validate the suitability of our algorithm. Performance\ncomparisons between the BSVGD and the SVGD are presented using the Wasserstein\ndistance between samples and the corresponding computational times.\n", "link": "http://arxiv.org/abs/2506.13916v2", "date": "2025-07-17", "relevancy": 2.0375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branching%20Stein%20Variational%20Gradient%20Descent%20for%20sampling%20multimodal%0A%20%20distributions&body=Title%3A%20Branching%20Stein%20Variational%20Gradient%20Descent%20for%20sampling%20multimodal%0A%20%20distributions%0AAuthor%3A%20Isa%C3%ADas%20Ba%C3%B1ales%20and%20Arturo%20Jaramillo%20and%20Joshu%C3%A9%20Hel%C3%AD%20Ricalde-Guerrero%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20particle-based%20variational%20inference%20method%20designed%20to%0Awork%20with%20multimodal%20distributions.%20Our%20approach%2C%20referred%20to%20as%20Branched%20Stein%0AVariational%20Gradient%20Descent%20%28BSVGD%29%2C%20extends%20the%20classical%20Stein%20Variational%0AGradient%20Descent%20%28SVGD%29%20algorithm%20by%20incorporating%20a%20random%20branching%20mechanism%0Athat%20encourages%20the%20exploration%20of%20the%20state%20space.%20In%20this%20work%2C%20a%20theoretical%0Aguarantee%20for%20the%20convergence%20in%20distribution%20is%20presented%2C%20as%20well%20as%0Anumerical%20experiments%20to%20validate%20the%20suitability%20of%20our%20algorithm.%20Performance%0Acomparisons%20between%20the%20BSVGD%20and%20the%20SVGD%20are%20presented%20using%20the%20Wasserstein%0Adistance%20between%20samples%20and%20the%20corresponding%20computational%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranching%2520Stein%2520Variational%2520Gradient%2520Descent%2520for%2520sampling%2520multimodal%250A%2520%2520distributions%26entry.906535625%3DIsa%25C3%25ADas%2520Ba%25C3%25B1ales%2520and%2520Arturo%2520Jaramillo%2520and%2520Joshu%25C3%25A9%2520Hel%25C3%25AD%2520Ricalde-Guerrero%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520particle-based%2520variational%2520inference%2520method%2520designed%2520to%250Awork%2520with%2520multimodal%2520distributions.%2520Our%2520approach%252C%2520referred%2520to%2520as%2520Branched%2520Stein%250AVariational%2520Gradient%2520Descent%2520%2528BSVGD%2529%252C%2520extends%2520the%2520classical%2520Stein%2520Variational%250AGradient%2520Descent%2520%2528SVGD%2529%2520algorithm%2520by%2520incorporating%2520a%2520random%2520branching%2520mechanism%250Athat%2520encourages%2520the%2520exploration%2520of%2520the%2520state%2520space.%2520In%2520this%2520work%252C%2520a%2520theoretical%250Aguarantee%2520for%2520the%2520convergence%2520in%2520distribution%2520is%2520presented%252C%2520as%2520well%2520as%250Anumerical%2520experiments%2520to%2520validate%2520the%2520suitability%2520of%2520our%2520algorithm.%2520Performance%250Acomparisons%2520between%2520the%2520BSVGD%2520and%2520the%2520SVGD%2520are%2520presented%2520using%2520the%2520Wasserstein%250Adistance%2520between%2520samples%2520and%2520the%2520corresponding%2520computational%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branching%20Stein%20Variational%20Gradient%20Descent%20for%20sampling%20multimodal%0A%20%20distributions&entry.906535625=Isa%C3%ADas%20Ba%C3%B1ales%20and%20Arturo%20Jaramillo%20and%20Joshu%C3%A9%20Hel%C3%AD%20Ricalde-Guerrero&entry.1292438233=%20%20We%20propose%20a%20novel%20particle-based%20variational%20inference%20method%20designed%20to%0Awork%20with%20multimodal%20distributions.%20Our%20approach%2C%20referred%20to%20as%20Branched%20Stein%0AVariational%20Gradient%20Descent%20%28BSVGD%29%2C%20extends%20the%20classical%20Stein%20Variational%0AGradient%20Descent%20%28SVGD%29%20algorithm%20by%20incorporating%20a%20random%20branching%20mechanism%0Athat%20encourages%20the%20exploration%20of%20the%20state%20space.%20In%20this%20work%2C%20a%20theoretical%0Aguarantee%20for%20the%20convergence%20in%20distribution%20is%20presented%2C%20as%20well%20as%0Anumerical%20experiments%20to%20validate%20the%20suitability%20of%20our%20algorithm.%20Performance%0Acomparisons%20between%20the%20BSVGD%20and%20the%20SVGD%20are%20presented%20using%20the%20Wasserstein%0Adistance%20between%20samples%20and%20the%20corresponding%20computational%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13916v2&entry.124074799=Read"},
{"title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained\n  Foundation Models", "author": "Suhas G Hegde and Shilpy Kaur and Aruna Tiwari", "abstract": "  Popular PEFT methods reduce trainable parameter count for fine-tuning by\nparameterizing new low-rank or sparse trainable weights in parallel to the\nfrozen pre-trained weights $W$. However, these weights are trained from\nscratch, and there exists a performance gap between these methods and full\nfine-tuning, especially in low-budget settings. We introduce VectorFit, a new\nway of parameterization that efficiently utilizes the existing knowledge\nembedded in $W$ by adaptively training their singular vectors and biases. We\nshow that utilizing the structural and transformational properties of $W$ in\nthis way can lead to high-rank incremental weight matrices $\\Delta W$,\ncomparable to that of full fine-tuning. VectorFit delivers superior results\nwith \\textbf{9$\\boldsymbol\\times$} fewer trainable parameters than the leading\nPEFT methods. Through comprehensive experiments across 19 datasets covering a\nwide range of language and vision tasks such as natural language understanding\nand generation, question answering, image classification, and image generation,\nwe demonstrate that VectorFit surpasses baselines in terms of performance as a\nfunction of parameter-efficiency.\n", "link": "http://arxiv.org/abs/2503.19530v2", "date": "2025-07-17", "relevancy": 2.0282, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5302}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5092}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VectorFit%20%3A%20Adaptive%20Singular%20%26%20Bias%20Vector%20Fine-Tuning%20of%20Pre-trained%0A%20%20Foundation%20Models&body=Title%3A%20VectorFit%20%3A%20Adaptive%20Singular%20%26%20Bias%20Vector%20Fine-Tuning%20of%20Pre-trained%0A%20%20Foundation%20Models%0AAuthor%3A%20Suhas%20G%20Hegde%20and%20Shilpy%20Kaur%20and%20Aruna%20Tiwari%0AAbstract%3A%20%20%20Popular%20PEFT%20methods%20reduce%20trainable%20parameter%20count%20for%20fine-tuning%20by%0Aparameterizing%20new%20low-rank%20or%20sparse%20trainable%20weights%20in%20parallel%20to%20the%0Afrozen%20pre-trained%20weights%20%24W%24.%20However%2C%20these%20weights%20are%20trained%20from%0Ascratch%2C%20and%20there%20exists%20a%20performance%20gap%20between%20these%20methods%20and%20full%0Afine-tuning%2C%20especially%20in%20low-budget%20settings.%20We%20introduce%20VectorFit%2C%20a%20new%0Away%20of%20parameterization%20that%20efficiently%20utilizes%20the%20existing%20knowledge%0Aembedded%20in%20%24W%24%20by%20adaptively%20training%20their%20singular%20vectors%20and%20biases.%20We%0Ashow%20that%20utilizing%20the%20structural%20and%20transformational%20properties%20of%20%24W%24%20in%0Athis%20way%20can%20lead%20to%20high-rank%20incremental%20weight%20matrices%20%24%5CDelta%20W%24%2C%0Acomparable%20to%20that%20of%20full%20fine-tuning.%20VectorFit%20delivers%20superior%20results%0Awith%20%5Ctextbf%7B9%24%5Cboldsymbol%5Ctimes%24%7D%20fewer%20trainable%20parameters%20than%20the%20leading%0APEFT%20methods.%20Through%20comprehensive%20experiments%20across%2019%20datasets%20covering%20a%0Awide%20range%20of%20language%20and%20vision%20tasks%20such%20as%20natural%20language%20understanding%0Aand%20generation%2C%20question%20answering%2C%20image%20classification%2C%20and%20image%20generation%2C%0Awe%20demonstrate%20that%20VectorFit%20surpasses%20baselines%20in%20terms%20of%20performance%20as%20a%0Afunction%20of%20parameter-efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVectorFit%2520%253A%2520Adaptive%2520Singular%2520%2526%2520Bias%2520Vector%2520Fine-Tuning%2520of%2520Pre-trained%250A%2520%2520Foundation%2520Models%26entry.906535625%3DSuhas%2520G%2520Hegde%2520and%2520Shilpy%2520Kaur%2520and%2520Aruna%2520Tiwari%26entry.1292438233%3D%2520%2520Popular%2520PEFT%2520methods%2520reduce%2520trainable%2520parameter%2520count%2520for%2520fine-tuning%2520by%250Aparameterizing%2520new%2520low-rank%2520or%2520sparse%2520trainable%2520weights%2520in%2520parallel%2520to%2520the%250Afrozen%2520pre-trained%2520weights%2520%2524W%2524.%2520However%252C%2520these%2520weights%2520are%2520trained%2520from%250Ascratch%252C%2520and%2520there%2520exists%2520a%2520performance%2520gap%2520between%2520these%2520methods%2520and%2520full%250Afine-tuning%252C%2520especially%2520in%2520low-budget%2520settings.%2520We%2520introduce%2520VectorFit%252C%2520a%2520new%250Away%2520of%2520parameterization%2520that%2520efficiently%2520utilizes%2520the%2520existing%2520knowledge%250Aembedded%2520in%2520%2524W%2524%2520by%2520adaptively%2520training%2520their%2520singular%2520vectors%2520and%2520biases.%2520We%250Ashow%2520that%2520utilizing%2520the%2520structural%2520and%2520transformational%2520properties%2520of%2520%2524W%2524%2520in%250Athis%2520way%2520can%2520lead%2520to%2520high-rank%2520incremental%2520weight%2520matrices%2520%2524%255CDelta%2520W%2524%252C%250Acomparable%2520to%2520that%2520of%2520full%2520fine-tuning.%2520VectorFit%2520delivers%2520superior%2520results%250Awith%2520%255Ctextbf%257B9%2524%255Cboldsymbol%255Ctimes%2524%257D%2520fewer%2520trainable%2520parameters%2520than%2520the%2520leading%250APEFT%2520methods.%2520Through%2520comprehensive%2520experiments%2520across%252019%2520datasets%2520covering%2520a%250Awide%2520range%2520of%2520language%2520and%2520vision%2520tasks%2520such%2520as%2520natural%2520language%2520understanding%250Aand%2520generation%252C%2520question%2520answering%252C%2520image%2520classification%252C%2520and%2520image%2520generation%252C%250Awe%2520demonstrate%2520that%2520VectorFit%2520surpasses%2520baselines%2520in%2520terms%2520of%2520performance%2520as%2520a%250Afunction%2520of%2520parameter-efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VectorFit%20%3A%20Adaptive%20Singular%20%26%20Bias%20Vector%20Fine-Tuning%20of%20Pre-trained%0A%20%20Foundation%20Models&entry.906535625=Suhas%20G%20Hegde%20and%20Shilpy%20Kaur%20and%20Aruna%20Tiwari&entry.1292438233=%20%20Popular%20PEFT%20methods%20reduce%20trainable%20parameter%20count%20for%20fine-tuning%20by%0Aparameterizing%20new%20low-rank%20or%20sparse%20trainable%20weights%20in%20parallel%20to%20the%0Afrozen%20pre-trained%20weights%20%24W%24.%20However%2C%20these%20weights%20are%20trained%20from%0Ascratch%2C%20and%20there%20exists%20a%20performance%20gap%20between%20these%20methods%20and%20full%0Afine-tuning%2C%20especially%20in%20low-budget%20settings.%20We%20introduce%20VectorFit%2C%20a%20new%0Away%20of%20parameterization%20that%20efficiently%20utilizes%20the%20existing%20knowledge%0Aembedded%20in%20%24W%24%20by%20adaptively%20training%20their%20singular%20vectors%20and%20biases.%20We%0Ashow%20that%20utilizing%20the%20structural%20and%20transformational%20properties%20of%20%24W%24%20in%0Athis%20way%20can%20lead%20to%20high-rank%20incremental%20weight%20matrices%20%24%5CDelta%20W%24%2C%0Acomparable%20to%20that%20of%20full%20fine-tuning.%20VectorFit%20delivers%20superior%20results%0Awith%20%5Ctextbf%7B9%24%5Cboldsymbol%5Ctimes%24%7D%20fewer%20trainable%20parameters%20than%20the%20leading%0APEFT%20methods.%20Through%20comprehensive%20experiments%20across%2019%20datasets%20covering%20a%0Awide%20range%20of%20language%20and%20vision%20tasks%20such%20as%20natural%20language%20understanding%0Aand%20generation%2C%20question%20answering%2C%20image%20classification%2C%20and%20image%20generation%2C%0Awe%20demonstrate%20that%20VectorFit%20surpasses%20baselines%20in%20terms%20of%20performance%20as%20a%0Afunction%20of%20parameter-efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19530v2&entry.124074799=Read"},
{"title": "NGTM: Substructure-based Neural Graph Topic Model for Interpretable\n  Graph Generation", "author": "Yuanxin Zhuang and Dazhong Shen and Ying Sun", "abstract": "  Graph generation plays a pivotal role across numerous domains, including\nmolecular design and knowledge graph construction. Although existing methods\nachieve considerable success in generating realistic graphs, their\ninterpretability remains limited, often obscuring the rationale behind\nstructural decisions. To address this challenge, we propose the Neural Graph\nTopic Model (NGTM), a novel generative framework inspired by topic modeling in\nnatural language processing. NGTM represents graphs as mixtures of latent\ntopics, each defining a distribution over semantically meaningful\nsubstructures, which facilitates explicit interpretability at both local and\nglobal scales. The generation process transparently integrates these topic\ndistributions with a global structural variable, enabling clear semantic\ntracing of each generated graph. Experiments demonstrate that NGTM achieves\ncompetitive generation quality while uniquely enabling fine-grained control and\ninterpretability, allowing users to tune structural features or induce\nbiological properties through topic-level adjustments.\n", "link": "http://arxiv.org/abs/2507.13133v1", "date": "2025-07-17", "relevancy": 2.0251, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5257}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4934}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NGTM%3A%20Substructure-based%20Neural%20Graph%20Topic%20Model%20for%20Interpretable%0A%20%20Graph%20Generation&body=Title%3A%20NGTM%3A%20Substructure-based%20Neural%20Graph%20Topic%20Model%20for%20Interpretable%0A%20%20Graph%20Generation%0AAuthor%3A%20Yuanxin%20Zhuang%20and%20Dazhong%20Shen%20and%20Ying%20Sun%0AAbstract%3A%20%20%20Graph%20generation%20plays%20a%20pivotal%20role%20across%20numerous%20domains%2C%20including%0Amolecular%20design%20and%20knowledge%20graph%20construction.%20Although%20existing%20methods%0Aachieve%20considerable%20success%20in%20generating%20realistic%20graphs%2C%20their%0Ainterpretability%20remains%20limited%2C%20often%20obscuring%20the%20rationale%20behind%0Astructural%20decisions.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Neural%20Graph%0ATopic%20Model%20%28NGTM%29%2C%20a%20novel%20generative%20framework%20inspired%20by%20topic%20modeling%20in%0Anatural%20language%20processing.%20NGTM%20represents%20graphs%20as%20mixtures%20of%20latent%0Atopics%2C%20each%20defining%20a%20distribution%20over%20semantically%20meaningful%0Asubstructures%2C%20which%20facilitates%20explicit%20interpretability%20at%20both%20local%20and%0Aglobal%20scales.%20The%20generation%20process%20transparently%20integrates%20these%20topic%0Adistributions%20with%20a%20global%20structural%20variable%2C%20enabling%20clear%20semantic%0Atracing%20of%20each%20generated%20graph.%20Experiments%20demonstrate%20that%20NGTM%20achieves%0Acompetitive%20generation%20quality%20while%20uniquely%20enabling%20fine-grained%20control%20and%0Ainterpretability%2C%20allowing%20users%20to%20tune%20structural%20features%20or%20induce%0Abiological%20properties%20through%20topic-level%20adjustments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNGTM%253A%2520Substructure-based%2520Neural%2520Graph%2520Topic%2520Model%2520for%2520Interpretable%250A%2520%2520Graph%2520Generation%26entry.906535625%3DYuanxin%2520Zhuang%2520and%2520Dazhong%2520Shen%2520and%2520Ying%2520Sun%26entry.1292438233%3D%2520%2520Graph%2520generation%2520plays%2520a%2520pivotal%2520role%2520across%2520numerous%2520domains%252C%2520including%250Amolecular%2520design%2520and%2520knowledge%2520graph%2520construction.%2520Although%2520existing%2520methods%250Aachieve%2520considerable%2520success%2520in%2520generating%2520realistic%2520graphs%252C%2520their%250Ainterpretability%2520remains%2520limited%252C%2520often%2520obscuring%2520the%2520rationale%2520behind%250Astructural%2520decisions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Neural%2520Graph%250ATopic%2520Model%2520%2528NGTM%2529%252C%2520a%2520novel%2520generative%2520framework%2520inspired%2520by%2520topic%2520modeling%2520in%250Anatural%2520language%2520processing.%2520NGTM%2520represents%2520graphs%2520as%2520mixtures%2520of%2520latent%250Atopics%252C%2520each%2520defining%2520a%2520distribution%2520over%2520semantically%2520meaningful%250Asubstructures%252C%2520which%2520facilitates%2520explicit%2520interpretability%2520at%2520both%2520local%2520and%250Aglobal%2520scales.%2520The%2520generation%2520process%2520transparently%2520integrates%2520these%2520topic%250Adistributions%2520with%2520a%2520global%2520structural%2520variable%252C%2520enabling%2520clear%2520semantic%250Atracing%2520of%2520each%2520generated%2520graph.%2520Experiments%2520demonstrate%2520that%2520NGTM%2520achieves%250Acompetitive%2520generation%2520quality%2520while%2520uniquely%2520enabling%2520fine-grained%2520control%2520and%250Ainterpretability%252C%2520allowing%2520users%2520to%2520tune%2520structural%2520features%2520or%2520induce%250Abiological%2520properties%2520through%2520topic-level%2520adjustments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NGTM%3A%20Substructure-based%20Neural%20Graph%20Topic%20Model%20for%20Interpretable%0A%20%20Graph%20Generation&entry.906535625=Yuanxin%20Zhuang%20and%20Dazhong%20Shen%20and%20Ying%20Sun&entry.1292438233=%20%20Graph%20generation%20plays%20a%20pivotal%20role%20across%20numerous%20domains%2C%20including%0Amolecular%20design%20and%20knowledge%20graph%20construction.%20Although%20existing%20methods%0Aachieve%20considerable%20success%20in%20generating%20realistic%20graphs%2C%20their%0Ainterpretability%20remains%20limited%2C%20often%20obscuring%20the%20rationale%20behind%0Astructural%20decisions.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Neural%20Graph%0ATopic%20Model%20%28NGTM%29%2C%20a%20novel%20generative%20framework%20inspired%20by%20topic%20modeling%20in%0Anatural%20language%20processing.%20NGTM%20represents%20graphs%20as%20mixtures%20of%20latent%0Atopics%2C%20each%20defining%20a%20distribution%20over%20semantically%20meaningful%0Asubstructures%2C%20which%20facilitates%20explicit%20interpretability%20at%20both%20local%20and%0Aglobal%20scales.%20The%20generation%20process%20transparently%20integrates%20these%20topic%0Adistributions%20with%20a%20global%20structural%20variable%2C%20enabling%20clear%20semantic%0Atracing%20of%20each%20generated%20graph.%20Experiments%20demonstrate%20that%20NGTM%20achieves%0Acompetitive%20generation%20quality%20while%20uniquely%20enabling%20fine-grained%20control%20and%0Ainterpretability%2C%20allowing%20users%20to%20tune%20structural%20features%20or%20induce%0Abiological%20properties%20through%20topic-level%20adjustments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13133v1&entry.124074799=Read"},
{"title": "Training Transformers with Enforced Lipschitz Constants", "author": "Laker Newhouse and R. Preston Hess and Franz Cesista and Andrii Zahorodnii and Jeremy Bernstein and Phillip Isola", "abstract": "  Neural networks are often highly sensitive to input and weight perturbations.\nThis sensitivity has been linked to pathologies such as vulnerability to\nadversarial examples, divergent training, and overfitting. To combat these\nproblems, past research has looked at building neural networks entirely from\nLipschitz components. However, these techniques have not matured to the point\nwhere researchers have trained a modern architecture such as a transformer with\na Lipschitz certificate enforced beyond initialization. To explore this gap, we\nbegin by developing and benchmarking novel, computationally-efficient tools for\nmaintaining norm-constrained weight matrices. Applying these tools, we are able\nto train transformer models with Lipschitz bounds enforced throughout training.\nWe find that optimizer dynamics matter: switching from AdamW to Muon improves\nstandard methods -- weight decay and spectral normalization -- allowing models\nto reach equal performance with a lower Lipschitz bound. Inspired by Muon's\nupdate having a fixed spectral norm, we co-design a weight constraint method\nthat improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter\ntransformers. Our 2-Lipschitz transformer on Shakespeare text reaches\nvalidation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz\ntransformer reaches 21% accuracy on internet text. However, to match the\nNanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound\nincreases to 10^264. Nonetheless, our Lipschitz transformers train without\nstability measures such as layer norm, QK norm, and logit tanh softcapping.\n", "link": "http://arxiv.org/abs/2507.13338v1", "date": "2025-07-17", "relevancy": 2.0049, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Transformers%20with%20Enforced%20Lipschitz%20Constants&body=Title%3A%20Training%20Transformers%20with%20Enforced%20Lipschitz%20Constants%0AAuthor%3A%20Laker%20Newhouse%20and%20R.%20Preston%20Hess%20and%20Franz%20Cesista%20and%20Andrii%20Zahorodnii%20and%20Jeremy%20Bernstein%20and%20Phillip%20Isola%0AAbstract%3A%20%20%20Neural%20networks%20are%20often%20highly%20sensitive%20to%20input%20and%20weight%20perturbations.%0AThis%20sensitivity%20has%20been%20linked%20to%20pathologies%20such%20as%20vulnerability%20to%0Aadversarial%20examples%2C%20divergent%20training%2C%20and%20overfitting.%20To%20combat%20these%0Aproblems%2C%20past%20research%20has%20looked%20at%20building%20neural%20networks%20entirely%20from%0ALipschitz%20components.%20However%2C%20these%20techniques%20have%20not%20matured%20to%20the%20point%0Awhere%20researchers%20have%20trained%20a%20modern%20architecture%20such%20as%20a%20transformer%20with%0Aa%20Lipschitz%20certificate%20enforced%20beyond%20initialization.%20To%20explore%20this%20gap%2C%20we%0Abegin%20by%20developing%20and%20benchmarking%20novel%2C%20computationally-efficient%20tools%20for%0Amaintaining%20norm-constrained%20weight%20matrices.%20Applying%20these%20tools%2C%20we%20are%20able%0Ato%20train%20transformer%20models%20with%20Lipschitz%20bounds%20enforced%20throughout%20training.%0AWe%20find%20that%20optimizer%20dynamics%20matter%3A%20switching%20from%20AdamW%20to%20Muon%20improves%0Astandard%20methods%20--%20weight%20decay%20and%20spectral%20normalization%20--%20allowing%20models%0Ato%20reach%20equal%20performance%20with%20a%20lower%20Lipschitz%20bound.%20Inspired%20by%20Muon%27s%0Aupdate%20having%20a%20fixed%20spectral%20norm%2C%20we%20co-design%20a%20weight%20constraint%20method%0Athat%20improves%20the%20Lipschitz%20vs.%20performance%20tradeoff%20on%20MLPs%20and%202M%20parameter%0Atransformers.%20Our%202-Lipschitz%20transformer%20on%20Shakespeare%20text%20reaches%0Avalidation%20accuracy%2060%25.%20Scaling%20to%20145M%20parameters%2C%20our%2010-Lipschitz%0Atransformer%20reaches%2021%25%20accuracy%20on%20internet%20text.%20However%2C%20to%20match%20the%0ANanoGPT%20baseline%20validation%20accuracy%20of%2039.4%25%2C%20our%20Lipschitz%20upper%20bound%0Aincreases%20to%2010%5E264.%20Nonetheless%2C%20our%20Lipschitz%20transformers%20train%20without%0Astability%20measures%20such%20as%20layer%20norm%2C%20QK%20norm%2C%20and%20logit%20tanh%20softcapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Transformers%2520with%2520Enforced%2520Lipschitz%2520Constants%26entry.906535625%3DLaker%2520Newhouse%2520and%2520R.%2520Preston%2520Hess%2520and%2520Franz%2520Cesista%2520and%2520Andrii%2520Zahorodnii%2520and%2520Jeremy%2520Bernstein%2520and%2520Phillip%2520Isola%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520often%2520highly%2520sensitive%2520to%2520input%2520and%2520weight%2520perturbations.%250AThis%2520sensitivity%2520has%2520been%2520linked%2520to%2520pathologies%2520such%2520as%2520vulnerability%2520to%250Aadversarial%2520examples%252C%2520divergent%2520training%252C%2520and%2520overfitting.%2520To%2520combat%2520these%250Aproblems%252C%2520past%2520research%2520has%2520looked%2520at%2520building%2520neural%2520networks%2520entirely%2520from%250ALipschitz%2520components.%2520However%252C%2520these%2520techniques%2520have%2520not%2520matured%2520to%2520the%2520point%250Awhere%2520researchers%2520have%2520trained%2520a%2520modern%2520architecture%2520such%2520as%2520a%2520transformer%2520with%250Aa%2520Lipschitz%2520certificate%2520enforced%2520beyond%2520initialization.%2520To%2520explore%2520this%2520gap%252C%2520we%250Abegin%2520by%2520developing%2520and%2520benchmarking%2520novel%252C%2520computationally-efficient%2520tools%2520for%250Amaintaining%2520norm-constrained%2520weight%2520matrices.%2520Applying%2520these%2520tools%252C%2520we%2520are%2520able%250Ato%2520train%2520transformer%2520models%2520with%2520Lipschitz%2520bounds%2520enforced%2520throughout%2520training.%250AWe%2520find%2520that%2520optimizer%2520dynamics%2520matter%253A%2520switching%2520from%2520AdamW%2520to%2520Muon%2520improves%250Astandard%2520methods%2520--%2520weight%2520decay%2520and%2520spectral%2520normalization%2520--%2520allowing%2520models%250Ato%2520reach%2520equal%2520performance%2520with%2520a%2520lower%2520Lipschitz%2520bound.%2520Inspired%2520by%2520Muon%2527s%250Aupdate%2520having%2520a%2520fixed%2520spectral%2520norm%252C%2520we%2520co-design%2520a%2520weight%2520constraint%2520method%250Athat%2520improves%2520the%2520Lipschitz%2520vs.%2520performance%2520tradeoff%2520on%2520MLPs%2520and%25202M%2520parameter%250Atransformers.%2520Our%25202-Lipschitz%2520transformer%2520on%2520Shakespeare%2520text%2520reaches%250Avalidation%2520accuracy%252060%2525.%2520Scaling%2520to%2520145M%2520parameters%252C%2520our%252010-Lipschitz%250Atransformer%2520reaches%252021%2525%2520accuracy%2520on%2520internet%2520text.%2520However%252C%2520to%2520match%2520the%250ANanoGPT%2520baseline%2520validation%2520accuracy%2520of%252039.4%2525%252C%2520our%2520Lipschitz%2520upper%2520bound%250Aincreases%2520to%252010%255E264.%2520Nonetheless%252C%2520our%2520Lipschitz%2520transformers%2520train%2520without%250Astability%2520measures%2520such%2520as%2520layer%2520norm%252C%2520QK%2520norm%252C%2520and%2520logit%2520tanh%2520softcapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Transformers%20with%20Enforced%20Lipschitz%20Constants&entry.906535625=Laker%20Newhouse%20and%20R.%20Preston%20Hess%20and%20Franz%20Cesista%20and%20Andrii%20Zahorodnii%20and%20Jeremy%20Bernstein%20and%20Phillip%20Isola&entry.1292438233=%20%20Neural%20networks%20are%20often%20highly%20sensitive%20to%20input%20and%20weight%20perturbations.%0AThis%20sensitivity%20has%20been%20linked%20to%20pathologies%20such%20as%20vulnerability%20to%0Aadversarial%20examples%2C%20divergent%20training%2C%20and%20overfitting.%20To%20combat%20these%0Aproblems%2C%20past%20research%20has%20looked%20at%20building%20neural%20networks%20entirely%20from%0ALipschitz%20components.%20However%2C%20these%20techniques%20have%20not%20matured%20to%20the%20point%0Awhere%20researchers%20have%20trained%20a%20modern%20architecture%20such%20as%20a%20transformer%20with%0Aa%20Lipschitz%20certificate%20enforced%20beyond%20initialization.%20To%20explore%20this%20gap%2C%20we%0Abegin%20by%20developing%20and%20benchmarking%20novel%2C%20computationally-efficient%20tools%20for%0Amaintaining%20norm-constrained%20weight%20matrices.%20Applying%20these%20tools%2C%20we%20are%20able%0Ato%20train%20transformer%20models%20with%20Lipschitz%20bounds%20enforced%20throughout%20training.%0AWe%20find%20that%20optimizer%20dynamics%20matter%3A%20switching%20from%20AdamW%20to%20Muon%20improves%0Astandard%20methods%20--%20weight%20decay%20and%20spectral%20normalization%20--%20allowing%20models%0Ato%20reach%20equal%20performance%20with%20a%20lower%20Lipschitz%20bound.%20Inspired%20by%20Muon%27s%0Aupdate%20having%20a%20fixed%20spectral%20norm%2C%20we%20co-design%20a%20weight%20constraint%20method%0Athat%20improves%20the%20Lipschitz%20vs.%20performance%20tradeoff%20on%20MLPs%20and%202M%20parameter%0Atransformers.%20Our%202-Lipschitz%20transformer%20on%20Shakespeare%20text%20reaches%0Avalidation%20accuracy%2060%25.%20Scaling%20to%20145M%20parameters%2C%20our%2010-Lipschitz%0Atransformer%20reaches%2021%25%20accuracy%20on%20internet%20text.%20However%2C%20to%20match%20the%0ANanoGPT%20baseline%20validation%20accuracy%20of%2039.4%25%2C%20our%20Lipschitz%20upper%20bound%0Aincreases%20to%2010%5E264.%20Nonetheless%2C%20our%20Lipschitz%20transformers%20train%20without%0Astability%20measures%20such%20as%20layer%20norm%2C%20QK%20norm%2C%20and%20logit%20tanh%20softcapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13338v1&entry.124074799=Read"},
{"title": "A Spectral Interpretation of Redundancy in a Graph Reservoir", "author": "Anna Bison and Alessandro Sperduti", "abstract": "  Reservoir computing has been successfully applied to graphs as a\npreprocessing method to improve the training efficiency of Graph Neural\nNetworks (GNNs). However, a common issue that arises when repeatedly applying\nlayer operators on graphs is over-smoothing, which consists in the convergence\nof graph signals toward low-frequency components of the graph Laplacian. This\nwork revisits the definition of the reservoir in the Multiresolution Reservoir\nGraph Neural Network (MRGNN), a spectral reservoir model, and proposes a\nvariant based on a Fairing algorithm originally introduced in the field of\nsurface design in computer graphics. This algorithm provides a pass-band\nspectral filter that allows smoothing without shrinkage, and it can be adapted\nto the graph setting through the Laplacian operator. Given its spectral\nformulation, this method naturally connects to GNN architectures for tasks\nwhere smoothing, when properly controlled, can be beneficial,such as graph\nclassification. The core contribution of the paper lies in the theoretical\nanalysis of the algorithm from a random walks perspective. In particular, it\nshows how tuning the spectral coefficients can be interpreted as modulating the\ncontribution of redundant random walks. Exploratory experiments based on the\nMRGNN architecture illustrate the potential of this approach and suggest\npromising directions for future research.\n", "link": "http://arxiv.org/abs/2507.12963v1", "date": "2025-07-17", "relevancy": 2.0013, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5065}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4965}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Spectral%20Interpretation%20of%20Redundancy%20in%20a%20Graph%20Reservoir&body=Title%3A%20A%20Spectral%20Interpretation%20of%20Redundancy%20in%20a%20Graph%20Reservoir%0AAuthor%3A%20Anna%20Bison%20and%20Alessandro%20Sperduti%0AAbstract%3A%20%20%20Reservoir%20computing%20has%20been%20successfully%20applied%20to%20graphs%20as%20a%0Apreprocessing%20method%20to%20improve%20the%20training%20efficiency%20of%20Graph%20Neural%0ANetworks%20%28GNNs%29.%20However%2C%20a%20common%20issue%20that%20arises%20when%20repeatedly%20applying%0Alayer%20operators%20on%20graphs%20is%20over-smoothing%2C%20which%20consists%20in%20the%20convergence%0Aof%20graph%20signals%20toward%20low-frequency%20components%20of%20the%20graph%20Laplacian.%20This%0Awork%20revisits%20the%20definition%20of%20the%20reservoir%20in%20the%20Multiresolution%20Reservoir%0AGraph%20Neural%20Network%20%28MRGNN%29%2C%20a%20spectral%20reservoir%20model%2C%20and%20proposes%20a%0Avariant%20based%20on%20a%20Fairing%20algorithm%20originally%20introduced%20in%20the%20field%20of%0Asurface%20design%20in%20computer%20graphics.%20This%20algorithm%20provides%20a%20pass-band%0Aspectral%20filter%20that%20allows%20smoothing%20without%20shrinkage%2C%20and%20it%20can%20be%20adapted%0Ato%20the%20graph%20setting%20through%20the%20Laplacian%20operator.%20Given%20its%20spectral%0Aformulation%2C%20this%20method%20naturally%20connects%20to%20GNN%20architectures%20for%20tasks%0Awhere%20smoothing%2C%20when%20properly%20controlled%2C%20can%20be%20beneficial%2Csuch%20as%20graph%0Aclassification.%20The%20core%20contribution%20of%20the%20paper%20lies%20in%20the%20theoretical%0Aanalysis%20of%20the%20algorithm%20from%20a%20random%20walks%20perspective.%20In%20particular%2C%20it%0Ashows%20how%20tuning%20the%20spectral%20coefficients%20can%20be%20interpreted%20as%20modulating%20the%0Acontribution%20of%20redundant%20random%20walks.%20Exploratory%20experiments%20based%20on%20the%0AMRGNN%20architecture%20illustrate%20the%20potential%20of%20this%20approach%20and%20suggest%0Apromising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Spectral%2520Interpretation%2520of%2520Redundancy%2520in%2520a%2520Graph%2520Reservoir%26entry.906535625%3DAnna%2520Bison%2520and%2520Alessandro%2520Sperduti%26entry.1292438233%3D%2520%2520Reservoir%2520computing%2520has%2520been%2520successfully%2520applied%2520to%2520graphs%2520as%2520a%250Apreprocessing%2520method%2520to%2520improve%2520the%2520training%2520efficiency%2520of%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529.%2520However%252C%2520a%2520common%2520issue%2520that%2520arises%2520when%2520repeatedly%2520applying%250Alayer%2520operators%2520on%2520graphs%2520is%2520over-smoothing%252C%2520which%2520consists%2520in%2520the%2520convergence%250Aof%2520graph%2520signals%2520toward%2520low-frequency%2520components%2520of%2520the%2520graph%2520Laplacian.%2520This%250Awork%2520revisits%2520the%2520definition%2520of%2520the%2520reservoir%2520in%2520the%2520Multiresolution%2520Reservoir%250AGraph%2520Neural%2520Network%2520%2528MRGNN%2529%252C%2520a%2520spectral%2520reservoir%2520model%252C%2520and%2520proposes%2520a%250Avariant%2520based%2520on%2520a%2520Fairing%2520algorithm%2520originally%2520introduced%2520in%2520the%2520field%2520of%250Asurface%2520design%2520in%2520computer%2520graphics.%2520This%2520algorithm%2520provides%2520a%2520pass-band%250Aspectral%2520filter%2520that%2520allows%2520smoothing%2520without%2520shrinkage%252C%2520and%2520it%2520can%2520be%2520adapted%250Ato%2520the%2520graph%2520setting%2520through%2520the%2520Laplacian%2520operator.%2520Given%2520its%2520spectral%250Aformulation%252C%2520this%2520method%2520naturally%2520connects%2520to%2520GNN%2520architectures%2520for%2520tasks%250Awhere%2520smoothing%252C%2520when%2520properly%2520controlled%252C%2520can%2520be%2520beneficial%252Csuch%2520as%2520graph%250Aclassification.%2520The%2520core%2520contribution%2520of%2520the%2520paper%2520lies%2520in%2520the%2520theoretical%250Aanalysis%2520of%2520the%2520algorithm%2520from%2520a%2520random%2520walks%2520perspective.%2520In%2520particular%252C%2520it%250Ashows%2520how%2520tuning%2520the%2520spectral%2520coefficients%2520can%2520be%2520interpreted%2520as%2520modulating%2520the%250Acontribution%2520of%2520redundant%2520random%2520walks.%2520Exploratory%2520experiments%2520based%2520on%2520the%250AMRGNN%2520architecture%2520illustrate%2520the%2520potential%2520of%2520this%2520approach%2520and%2520suggest%250Apromising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spectral%20Interpretation%20of%20Redundancy%20in%20a%20Graph%20Reservoir&entry.906535625=Anna%20Bison%20and%20Alessandro%20Sperduti&entry.1292438233=%20%20Reservoir%20computing%20has%20been%20successfully%20applied%20to%20graphs%20as%20a%0Apreprocessing%20method%20to%20improve%20the%20training%20efficiency%20of%20Graph%20Neural%0ANetworks%20%28GNNs%29.%20However%2C%20a%20common%20issue%20that%20arises%20when%20repeatedly%20applying%0Alayer%20operators%20on%20graphs%20is%20over-smoothing%2C%20which%20consists%20in%20the%20convergence%0Aof%20graph%20signals%20toward%20low-frequency%20components%20of%20the%20graph%20Laplacian.%20This%0Awork%20revisits%20the%20definition%20of%20the%20reservoir%20in%20the%20Multiresolution%20Reservoir%0AGraph%20Neural%20Network%20%28MRGNN%29%2C%20a%20spectral%20reservoir%20model%2C%20and%20proposes%20a%0Avariant%20based%20on%20a%20Fairing%20algorithm%20originally%20introduced%20in%20the%20field%20of%0Asurface%20design%20in%20computer%20graphics.%20This%20algorithm%20provides%20a%20pass-band%0Aspectral%20filter%20that%20allows%20smoothing%20without%20shrinkage%2C%20and%20it%20can%20be%20adapted%0Ato%20the%20graph%20setting%20through%20the%20Laplacian%20operator.%20Given%20its%20spectral%0Aformulation%2C%20this%20method%20naturally%20connects%20to%20GNN%20architectures%20for%20tasks%0Awhere%20smoothing%2C%20when%20properly%20controlled%2C%20can%20be%20beneficial%2Csuch%20as%20graph%0Aclassification.%20The%20core%20contribution%20of%20the%20paper%20lies%20in%20the%20theoretical%0Aanalysis%20of%20the%20algorithm%20from%20a%20random%20walks%20perspective.%20In%20particular%2C%20it%0Ashows%20how%20tuning%20the%20spectral%20coefficients%20can%20be%20interpreted%20as%20modulating%20the%0Acontribution%20of%20redundant%20random%20walks.%20Exploratory%20experiments%20based%20on%20the%0AMRGNN%20architecture%20illustrate%20the%20potential%20of%20this%20approach%20and%20suggest%0Apromising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12963v1&entry.124074799=Read"},
{"title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations", "author": "Carlos Arriaga and Gonzalo Mart\u00ednez and Eneko Sendin and Javier Conde and Pedro Reviriego", "abstract": "  The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.\n", "link": "http://arxiv.org/abs/2507.13302v1", "date": "2025-07-17", "relevancy": 1.9941, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Generative%20Energy%20Arena%20%28GEA%29%3A%20Incorporating%20Energy%20Awareness%20in%0A%20%20Large%20Language%20Model%20%28LLM%29%20Human%20Evaluations&body=Title%3A%20The%20Generative%20Energy%20Arena%20%28GEA%29%3A%20Incorporating%20Energy%20Awareness%20in%0A%20%20Large%20Language%20Model%20%28LLM%29%20Human%20Evaluations%0AAuthor%3A%20Carlos%20Arriaga%20and%20Gonzalo%20Mart%C3%ADnez%20and%20Eneko%20Sendin%20and%20Javier%20Conde%20and%20Pedro%20Reviriego%0AAbstract%3A%20%20%20The%20evaluation%20of%20large%20language%20models%20is%20a%20complex%20task%2C%20in%20which%20several%0Aapproaches%20have%20been%20proposed.%20The%20most%20common%20is%20the%20use%20of%20automated%0Abenchmarks%20in%20which%20LLMs%20have%20to%20answer%20multiple-choice%20questions%20of%20different%0Atopics.%20However%2C%20this%20method%20has%20certain%20limitations%2C%20being%20the%20most%0Aconcerning%2C%20the%20poor%20correlation%20with%20the%20humans.%20An%20alternative%20approach%2C%20is%0Ato%20have%20humans%20evaluate%20the%20LLMs.%20This%20poses%20scalability%20issues%20as%20there%20is%20a%0Alarge%20and%20growing%20number%20of%20models%20to%20evaluate%20making%20it%20impractical%20%28and%0Acostly%29%20to%20run%20traditional%20studies%20based%20on%20recruiting%20a%20number%20of%20evaluators%0Aand%20having%20them%20rank%20the%20responses%20of%20the%20models.%20An%20alternative%20approach%20is%0Athe%20use%20of%20public%20arenas%2C%20such%20as%20the%20popular%20LM%20arena%2C%20on%20which%20any%20user%20can%0Afreely%20evaluate%20models%20on%20any%20question%20and%20rank%20the%20responses%20of%20two%20models.%0AThe%20results%20are%20then%20elaborated%20into%20a%20model%20ranking.%20An%20increasingly%20important%0Aaspect%20of%20LLMs%20is%20their%20energy%20consumption%20and%2C%20therefore%2C%20evaluating%20how%0Aenergy%20awareness%20influences%20the%20decisions%20of%20humans%20in%20selecting%20a%20model%20is%20of%0Ainterest.%20In%20this%20paper%2C%20we%20present%20GEA%2C%20the%20Generative%20Energy%20Arena%2C%20an%20arena%0Athat%20incorporates%20information%20on%20the%20energy%20consumption%20of%20the%20model%20in%20the%0Aevaluation%20process.%20Preliminary%20results%20obtained%20with%20GEA%20are%20also%20presented%2C%0Ashowing%20that%20for%20most%20questions%2C%20when%20users%20are%20aware%20of%20the%20energy%0Aconsumption%2C%20they%20favor%20smaller%20and%20more%20energy%20efficient%20models.%20This%20suggests%0Athat%20for%20most%20user%20interactions%2C%20the%20extra%20cost%20and%20energy%20incurred%20by%20the%20more%0Acomplex%20and%20top-performing%20models%20do%20not%20provide%20an%20increase%20in%20the%20perceived%0Aquality%20of%20the%20responses%20that%20justifies%20their%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Generative%2520Energy%2520Arena%2520%2528GEA%2529%253A%2520Incorporating%2520Energy%2520Awareness%2520in%250A%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520Human%2520Evaluations%26entry.906535625%3DCarlos%2520Arriaga%2520and%2520Gonzalo%2520Mart%25C3%25ADnez%2520and%2520Eneko%2520Sendin%2520and%2520Javier%2520Conde%2520and%2520Pedro%2520Reviriego%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520large%2520language%2520models%2520is%2520a%2520complex%2520task%252C%2520in%2520which%2520several%250Aapproaches%2520have%2520been%2520proposed.%2520The%2520most%2520common%2520is%2520the%2520use%2520of%2520automated%250Abenchmarks%2520in%2520which%2520LLMs%2520have%2520to%2520answer%2520multiple-choice%2520questions%2520of%2520different%250Atopics.%2520However%252C%2520this%2520method%2520has%2520certain%2520limitations%252C%2520being%2520the%2520most%250Aconcerning%252C%2520the%2520poor%2520correlation%2520with%2520the%2520humans.%2520An%2520alternative%2520approach%252C%2520is%250Ato%2520have%2520humans%2520evaluate%2520the%2520LLMs.%2520This%2520poses%2520scalability%2520issues%2520as%2520there%2520is%2520a%250Alarge%2520and%2520growing%2520number%2520of%2520models%2520to%2520evaluate%2520making%2520it%2520impractical%2520%2528and%250Acostly%2529%2520to%2520run%2520traditional%2520studies%2520based%2520on%2520recruiting%2520a%2520number%2520of%2520evaluators%250Aand%2520having%2520them%2520rank%2520the%2520responses%2520of%2520the%2520models.%2520An%2520alternative%2520approach%2520is%250Athe%2520use%2520of%2520public%2520arenas%252C%2520such%2520as%2520the%2520popular%2520LM%2520arena%252C%2520on%2520which%2520any%2520user%2520can%250Afreely%2520evaluate%2520models%2520on%2520any%2520question%2520and%2520rank%2520the%2520responses%2520of%2520two%2520models.%250AThe%2520results%2520are%2520then%2520elaborated%2520into%2520a%2520model%2520ranking.%2520An%2520increasingly%2520important%250Aaspect%2520of%2520LLMs%2520is%2520their%2520energy%2520consumption%2520and%252C%2520therefore%252C%2520evaluating%2520how%250Aenergy%2520awareness%2520influences%2520the%2520decisions%2520of%2520humans%2520in%2520selecting%2520a%2520model%2520is%2520of%250Ainterest.%2520In%2520this%2520paper%252C%2520we%2520present%2520GEA%252C%2520the%2520Generative%2520Energy%2520Arena%252C%2520an%2520arena%250Athat%2520incorporates%2520information%2520on%2520the%2520energy%2520consumption%2520of%2520the%2520model%2520in%2520the%250Aevaluation%2520process.%2520Preliminary%2520results%2520obtained%2520with%2520GEA%2520are%2520also%2520presented%252C%250Ashowing%2520that%2520for%2520most%2520questions%252C%2520when%2520users%2520are%2520aware%2520of%2520the%2520energy%250Aconsumption%252C%2520they%2520favor%2520smaller%2520and%2520more%2520energy%2520efficient%2520models.%2520This%2520suggests%250Athat%2520for%2520most%2520user%2520interactions%252C%2520the%2520extra%2520cost%2520and%2520energy%2520incurred%2520by%2520the%2520more%250Acomplex%2520and%2520top-performing%2520models%2520do%2520not%2520provide%2520an%2520increase%2520in%2520the%2520perceived%250Aquality%2520of%2520the%2520responses%2520that%2520justifies%2520their%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Generative%20Energy%20Arena%20%28GEA%29%3A%20Incorporating%20Energy%20Awareness%20in%0A%20%20Large%20Language%20Model%20%28LLM%29%20Human%20Evaluations&entry.906535625=Carlos%20Arriaga%20and%20Gonzalo%20Mart%C3%ADnez%20and%20Eneko%20Sendin%20and%20Javier%20Conde%20and%20Pedro%20Reviriego&entry.1292438233=%20%20The%20evaluation%20of%20large%20language%20models%20is%20a%20complex%20task%2C%20in%20which%20several%0Aapproaches%20have%20been%20proposed.%20The%20most%20common%20is%20the%20use%20of%20automated%0Abenchmarks%20in%20which%20LLMs%20have%20to%20answer%20multiple-choice%20questions%20of%20different%0Atopics.%20However%2C%20this%20method%20has%20certain%20limitations%2C%20being%20the%20most%0Aconcerning%2C%20the%20poor%20correlation%20with%20the%20humans.%20An%20alternative%20approach%2C%20is%0Ato%20have%20humans%20evaluate%20the%20LLMs.%20This%20poses%20scalability%20issues%20as%20there%20is%20a%0Alarge%20and%20growing%20number%20of%20models%20to%20evaluate%20making%20it%20impractical%20%28and%0Acostly%29%20to%20run%20traditional%20studies%20based%20on%20recruiting%20a%20number%20of%20evaluators%0Aand%20having%20them%20rank%20the%20responses%20of%20the%20models.%20An%20alternative%20approach%20is%0Athe%20use%20of%20public%20arenas%2C%20such%20as%20the%20popular%20LM%20arena%2C%20on%20which%20any%20user%20can%0Afreely%20evaluate%20models%20on%20any%20question%20and%20rank%20the%20responses%20of%20two%20models.%0AThe%20results%20are%20then%20elaborated%20into%20a%20model%20ranking.%20An%20increasingly%20important%0Aaspect%20of%20LLMs%20is%20their%20energy%20consumption%20and%2C%20therefore%2C%20evaluating%20how%0Aenergy%20awareness%20influences%20the%20decisions%20of%20humans%20in%20selecting%20a%20model%20is%20of%0Ainterest.%20In%20this%20paper%2C%20we%20present%20GEA%2C%20the%20Generative%20Energy%20Arena%2C%20an%20arena%0Athat%20incorporates%20information%20on%20the%20energy%20consumption%20of%20the%20model%20in%20the%0Aevaluation%20process.%20Preliminary%20results%20obtained%20with%20GEA%20are%20also%20presented%2C%0Ashowing%20that%20for%20most%20questions%2C%20when%20users%20are%20aware%20of%20the%20energy%0Aconsumption%2C%20they%20favor%20smaller%20and%20more%20energy%20efficient%20models.%20This%20suggests%0Athat%20for%20most%20user%20interactions%2C%20the%20extra%20cost%20and%20energy%20incurred%20by%20the%20more%0Acomplex%20and%20top-performing%20models%20do%20not%20provide%20an%20increase%20in%20the%20perceived%0Aquality%20of%20the%20responses%20that%20justifies%20their%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13302v1&entry.124074799=Read"},
{"title": "Spectral Bellman Method: Unifying Representation and Exploration in RL", "author": "Ofir Nabati and Bo Dai and Shie Mannor and Guy Tennenholtz", "abstract": "  The effect of representation has been demonstrated in reinforcement learning,\nfrom both theoretical and empirical successes. However, the existing\nrepresentation learning mainly induced from model learning aspects, misaligning\nwith our RL tasks. This work introduces Spectral Bellman Representation, a\nnovel framework derived from the Inherent Bellman Error (IBE) condition, which\naligns with the fundamental structure of Bellman updates across a space of\npossible value functions, therefore, directly towards value-based RL. Our key\ninsight is the discovery of a fundamental spectral relationship: under the\nzero-IBE condition, the transformation of a distribution of value functions by\nthe Bellman operator is intrinsically linked to the feature covariance\nstructure. This spectral connection yields a new, theoretically-grounded\nobjective for learning state-action features that inherently capture this\nBellman-aligned covariance. Our method requires a simple modification to\nexisting algorithms. We demonstrate that our learned representations enable\nstructured exploration, by aligning feature covariance with Bellman dynamics,\nand improve overall performance, particularly in challenging hard-exploration\nand long-horizon credit assignment tasks. Our framework naturally extends to\npowerful multi-step Bellman operators, further broadening its impact. Spectral\nBellman Representation offers a principled and effective path toward learning\nmore powerful and structurally sound representations for value-based\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2507.13181v1", "date": "2025-07-17", "relevancy": 1.9898, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Bellman%20Method%3A%20Unifying%20Representation%20and%20Exploration%20in%20RL&body=Title%3A%20Spectral%20Bellman%20Method%3A%20Unifying%20Representation%20and%20Exploration%20in%20RL%0AAuthor%3A%20Ofir%20Nabati%20and%20Bo%20Dai%20and%20Shie%20Mannor%20and%20Guy%20Tennenholtz%0AAbstract%3A%20%20%20The%20effect%20of%20representation%20has%20been%20demonstrated%20in%20reinforcement%20learning%2C%0Afrom%20both%20theoretical%20and%20empirical%20successes.%20However%2C%20the%20existing%0Arepresentation%20learning%20mainly%20induced%20from%20model%20learning%20aspects%2C%20misaligning%0Awith%20our%20RL%20tasks.%20This%20work%20introduces%20Spectral%20Bellman%20Representation%2C%20a%0Anovel%20framework%20derived%20from%20the%20Inherent%20Bellman%20Error%20%28IBE%29%20condition%2C%20which%0Aaligns%20with%20the%20fundamental%20structure%20of%20Bellman%20updates%20across%20a%20space%20of%0Apossible%20value%20functions%2C%20therefore%2C%20directly%20towards%20value-based%20RL.%20Our%20key%0Ainsight%20is%20the%20discovery%20of%20a%20fundamental%20spectral%20relationship%3A%20under%20the%0Azero-IBE%20condition%2C%20the%20transformation%20of%20a%20distribution%20of%20value%20functions%20by%0Athe%20Bellman%20operator%20is%20intrinsically%20linked%20to%20the%20feature%20covariance%0Astructure.%20This%20spectral%20connection%20yields%20a%20new%2C%20theoretically-grounded%0Aobjective%20for%20learning%20state-action%20features%20that%20inherently%20capture%20this%0ABellman-aligned%20covariance.%20Our%20method%20requires%20a%20simple%20modification%20to%0Aexisting%20algorithms.%20We%20demonstrate%20that%20our%20learned%20representations%20enable%0Astructured%20exploration%2C%20by%20aligning%20feature%20covariance%20with%20Bellman%20dynamics%2C%0Aand%20improve%20overall%20performance%2C%20particularly%20in%20challenging%20hard-exploration%0Aand%20long-horizon%20credit%20assignment%20tasks.%20Our%20framework%20naturally%20extends%20to%0Apowerful%20multi-step%20Bellman%20operators%2C%20further%20broadening%20its%20impact.%20Spectral%0ABellman%20Representation%20offers%20a%20principled%20and%20effective%20path%20toward%20learning%0Amore%20powerful%20and%20structurally%20sound%20representations%20for%20value-based%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Bellman%2520Method%253A%2520Unifying%2520Representation%2520and%2520Exploration%2520in%2520RL%26entry.906535625%3DOfir%2520Nabati%2520and%2520Bo%2520Dai%2520and%2520Shie%2520Mannor%2520and%2520Guy%2520Tennenholtz%26entry.1292438233%3D%2520%2520The%2520effect%2520of%2520representation%2520has%2520been%2520demonstrated%2520in%2520reinforcement%2520learning%252C%250Afrom%2520both%2520theoretical%2520and%2520empirical%2520successes.%2520However%252C%2520the%2520existing%250Arepresentation%2520learning%2520mainly%2520induced%2520from%2520model%2520learning%2520aspects%252C%2520misaligning%250Awith%2520our%2520RL%2520tasks.%2520This%2520work%2520introduces%2520Spectral%2520Bellman%2520Representation%252C%2520a%250Anovel%2520framework%2520derived%2520from%2520the%2520Inherent%2520Bellman%2520Error%2520%2528IBE%2529%2520condition%252C%2520which%250Aaligns%2520with%2520the%2520fundamental%2520structure%2520of%2520Bellman%2520updates%2520across%2520a%2520space%2520of%250Apossible%2520value%2520functions%252C%2520therefore%252C%2520directly%2520towards%2520value-based%2520RL.%2520Our%2520key%250Ainsight%2520is%2520the%2520discovery%2520of%2520a%2520fundamental%2520spectral%2520relationship%253A%2520under%2520the%250Azero-IBE%2520condition%252C%2520the%2520transformation%2520of%2520a%2520distribution%2520of%2520value%2520functions%2520by%250Athe%2520Bellman%2520operator%2520is%2520intrinsically%2520linked%2520to%2520the%2520feature%2520covariance%250Astructure.%2520This%2520spectral%2520connection%2520yields%2520a%2520new%252C%2520theoretically-grounded%250Aobjective%2520for%2520learning%2520state-action%2520features%2520that%2520inherently%2520capture%2520this%250ABellman-aligned%2520covariance.%2520Our%2520method%2520requires%2520a%2520simple%2520modification%2520to%250Aexisting%2520algorithms.%2520We%2520demonstrate%2520that%2520our%2520learned%2520representations%2520enable%250Astructured%2520exploration%252C%2520by%2520aligning%2520feature%2520covariance%2520with%2520Bellman%2520dynamics%252C%250Aand%2520improve%2520overall%2520performance%252C%2520particularly%2520in%2520challenging%2520hard-exploration%250Aand%2520long-horizon%2520credit%2520assignment%2520tasks.%2520Our%2520framework%2520naturally%2520extends%2520to%250Apowerful%2520multi-step%2520Bellman%2520operators%252C%2520further%2520broadening%2520its%2520impact.%2520Spectral%250ABellman%2520Representation%2520offers%2520a%2520principled%2520and%2520effective%2520path%2520toward%2520learning%250Amore%2520powerful%2520and%2520structurally%2520sound%2520representations%2520for%2520value-based%250Areinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Bellman%20Method%3A%20Unifying%20Representation%20and%20Exploration%20in%20RL&entry.906535625=Ofir%20Nabati%20and%20Bo%20Dai%20and%20Shie%20Mannor%20and%20Guy%20Tennenholtz&entry.1292438233=%20%20The%20effect%20of%20representation%20has%20been%20demonstrated%20in%20reinforcement%20learning%2C%0Afrom%20both%20theoretical%20and%20empirical%20successes.%20However%2C%20the%20existing%0Arepresentation%20learning%20mainly%20induced%20from%20model%20learning%20aspects%2C%20misaligning%0Awith%20our%20RL%20tasks.%20This%20work%20introduces%20Spectral%20Bellman%20Representation%2C%20a%0Anovel%20framework%20derived%20from%20the%20Inherent%20Bellman%20Error%20%28IBE%29%20condition%2C%20which%0Aaligns%20with%20the%20fundamental%20structure%20of%20Bellman%20updates%20across%20a%20space%20of%0Apossible%20value%20functions%2C%20therefore%2C%20directly%20towards%20value-based%20RL.%20Our%20key%0Ainsight%20is%20the%20discovery%20of%20a%20fundamental%20spectral%20relationship%3A%20under%20the%0Azero-IBE%20condition%2C%20the%20transformation%20of%20a%20distribution%20of%20value%20functions%20by%0Athe%20Bellman%20operator%20is%20intrinsically%20linked%20to%20the%20feature%20covariance%0Astructure.%20This%20spectral%20connection%20yields%20a%20new%2C%20theoretically-grounded%0Aobjective%20for%20learning%20state-action%20features%20that%20inherently%20capture%20this%0ABellman-aligned%20covariance.%20Our%20method%20requires%20a%20simple%20modification%20to%0Aexisting%20algorithms.%20We%20demonstrate%20that%20our%20learned%20representations%20enable%0Astructured%20exploration%2C%20by%20aligning%20feature%20covariance%20with%20Bellman%20dynamics%2C%0Aand%20improve%20overall%20performance%2C%20particularly%20in%20challenging%20hard-exploration%0Aand%20long-horizon%20credit%20assignment%20tasks.%20Our%20framework%20naturally%20extends%20to%0Apowerful%20multi-step%20Bellman%20operators%2C%20further%20broadening%20its%20impact.%20Spectral%0ABellman%20Representation%20offers%20a%20principled%20and%20effective%20path%20toward%20learning%0Amore%20powerful%20and%20structurally%20sound%20representations%20for%20value-based%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13181v1&entry.124074799=Read"},
{"title": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level\n  Length Control", "author": "Xingyang He and Xiao Ling and Jie Liu", "abstract": "  Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods.\n", "link": "http://arxiv.org/abs/2507.04348v2", "date": "2025-07-17", "relevancy": 1.9774, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.508}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartThinker%3A%20Learning%20to%20Compress%20and%20Preserve%20Reasoning%20by%20Step-Level%0A%20%20Length%20Control&body=Title%3A%20SmartThinker%3A%20Learning%20to%20Compress%20and%20Preserve%20Reasoning%20by%20Step-Level%0A%20%20Length%20Control%0AAuthor%3A%20Xingyang%20He%20and%20Xiao%20Ling%20and%20Jie%20Liu%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20exhibited%20remarkable%20reasoning%0Acapabilities%20through%20inference-time%20scaling%2C%20but%20this%20progress%20has%20also%0Aintroduced%20considerable%20redundancy%20and%20inefficiency%20into%20their%20reasoning%0Aprocesses%2C%20resulting%20in%20substantial%20computational%20waste.%20Previous%20work%20has%0Aattempted%20to%20mitigate%20this%20issue%20by%20penalizing%20the%20overall%20length%20of%20generated%0Asamples%20during%20reinforcement%20learning%20%28RL%29%2C%20with%20the%20goal%20of%20encouraging%20a%20more%0Aconcise%20chains%20of%20thought.%20However%2C%20we%20observe%20that%20such%20global%20length%20penalty%0Aoften%20lead%20to%20excessive%20compression%20of%20critical%20reasoning%20steps%20while%0Apreserving%20unnecessary%20details%20in%20simpler%20ones%2C%20yielding%20a%20suboptimal%20trade-off%0Abetween%20accuracy%20and%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%0ASmartThinker%2C%20a%20two-stage%20learnable%20framework%20designed%20to%20enable%20fine-grained%0Acontrol%20over%20the%20length%20of%20reasoning%20chains%20based%20on%20the%20importance%20of%20each%0Aindividual%20step.%20In%20the%20first%20stage%2C%20SmartThinker%20adapts%20a%20reasoning%20model%20to%20a%0Ashort-form%20reasoning%20mode%20through%20rejection%20sampling%20combined%20with%20supervised%0Afine-tuning%20%28SFT%29.%20In%20the%20second%20stage%2C%20SmartThinker%20applies%20Step-Level%20Length%0AControl%20Policy%20Optimization%20%28SCPO%29%20to%20refine%20the%20model%20output%20distribution%2C%0Awhich%20increases%20the%20proportion%20of%20length%20allocated%20to%20critical%20steps%20while%0Areducing%20redundancy%20in%20less%20important%20ones.%20SCPO%20consists%20of%20four%20core%0Acomponents%3A%20an%20online%20importance%20estimator%2C%20a%20step-level%20length%20control%20reward%0Afunction%2C%20a%20step-level%20generalized%20advantage%20estimation%20%28S-GAE%29%20and%20a%0Adifficulty-adaptive%20clipping%20strategy.%20Working%20in%20concert%2C%20these%20components%0Aenable%20SCPO%20to%20implement%20differentiated%20length%20control%20across%20reasoning%20steps.%0AEmpirical%20results%20across%20multiple%20reasoning%20benchmarks%20and%20various%20backbone%0Amodels%20demonstrate%20that%20SmartThinker%20significantly%20reduces%20redundant%20reasoning%0Awhile%20achieving%20comparable%20or%20even%20superior%20performance%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartThinker%253A%2520Learning%2520to%2520Compress%2520and%2520Preserve%2520Reasoning%2520by%2520Step-Level%250A%2520%2520Length%2520Control%26entry.906535625%3DXingyang%2520He%2520and%2520Xiao%2520Ling%2520and%2520Jie%2520Liu%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520exhibited%2520remarkable%2520reasoning%250Acapabilities%2520through%2520inference-time%2520scaling%252C%2520but%2520this%2520progress%2520has%2520also%250Aintroduced%2520considerable%2520redundancy%2520and%2520inefficiency%2520into%2520their%2520reasoning%250Aprocesses%252C%2520resulting%2520in%2520substantial%2520computational%2520waste.%2520Previous%2520work%2520has%250Aattempted%2520to%2520mitigate%2520this%2520issue%2520by%2520penalizing%2520the%2520overall%2520length%2520of%2520generated%250Asamples%2520during%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520with%2520the%2520goal%2520of%2520encouraging%2520a%2520more%250Aconcise%2520chains%2520of%2520thought.%2520However%252C%2520we%2520observe%2520that%2520such%2520global%2520length%2520penalty%250Aoften%2520lead%2520to%2520excessive%2520compression%2520of%2520critical%2520reasoning%2520steps%2520while%250Apreserving%2520unnecessary%2520details%2520in%2520simpler%2520ones%252C%2520yielding%2520a%2520suboptimal%2520trade-off%250Abetween%2520accuracy%2520and%2520efficiency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250ASmartThinker%252C%2520a%2520two-stage%2520learnable%2520framework%2520designed%2520to%2520enable%2520fine-grained%250Acontrol%2520over%2520the%2520length%2520of%2520reasoning%2520chains%2520based%2520on%2520the%2520importance%2520of%2520each%250Aindividual%2520step.%2520In%2520the%2520first%2520stage%252C%2520SmartThinker%2520adapts%2520a%2520reasoning%2520model%2520to%2520a%250Ashort-form%2520reasoning%2520mode%2520through%2520rejection%2520sampling%2520combined%2520with%2520supervised%250Afine-tuning%2520%2528SFT%2529.%2520In%2520the%2520second%2520stage%252C%2520SmartThinker%2520applies%2520Step-Level%2520Length%250AControl%2520Policy%2520Optimization%2520%2528SCPO%2529%2520to%2520refine%2520the%2520model%2520output%2520distribution%252C%250Awhich%2520increases%2520the%2520proportion%2520of%2520length%2520allocated%2520to%2520critical%2520steps%2520while%250Areducing%2520redundancy%2520in%2520less%2520important%2520ones.%2520SCPO%2520consists%2520of%2520four%2520core%250Acomponents%253A%2520an%2520online%2520importance%2520estimator%252C%2520a%2520step-level%2520length%2520control%2520reward%250Afunction%252C%2520a%2520step-level%2520generalized%2520advantage%2520estimation%2520%2528S-GAE%2529%2520and%2520a%250Adifficulty-adaptive%2520clipping%2520strategy.%2520Working%2520in%2520concert%252C%2520these%2520components%250Aenable%2520SCPO%2520to%2520implement%2520differentiated%2520length%2520control%2520across%2520reasoning%2520steps.%250AEmpirical%2520results%2520across%2520multiple%2520reasoning%2520benchmarks%2520and%2520various%2520backbone%250Amodels%2520demonstrate%2520that%2520SmartThinker%2520significantly%2520reduces%2520redundant%2520reasoning%250Awhile%2520achieving%2520comparable%2520or%2520even%2520superior%2520performance%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartThinker%3A%20Learning%20to%20Compress%20and%20Preserve%20Reasoning%20by%20Step-Level%0A%20%20Length%20Control&entry.906535625=Xingyang%20He%20and%20Xiao%20Ling%20and%20Jie%20Liu&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20exhibited%20remarkable%20reasoning%0Acapabilities%20through%20inference-time%20scaling%2C%20but%20this%20progress%20has%20also%0Aintroduced%20considerable%20redundancy%20and%20inefficiency%20into%20their%20reasoning%0Aprocesses%2C%20resulting%20in%20substantial%20computational%20waste.%20Previous%20work%20has%0Aattempted%20to%20mitigate%20this%20issue%20by%20penalizing%20the%20overall%20length%20of%20generated%0Asamples%20during%20reinforcement%20learning%20%28RL%29%2C%20with%20the%20goal%20of%20encouraging%20a%20more%0Aconcise%20chains%20of%20thought.%20However%2C%20we%20observe%20that%20such%20global%20length%20penalty%0Aoften%20lead%20to%20excessive%20compression%20of%20critical%20reasoning%20steps%20while%0Apreserving%20unnecessary%20details%20in%20simpler%20ones%2C%20yielding%20a%20suboptimal%20trade-off%0Abetween%20accuracy%20and%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%0ASmartThinker%2C%20a%20two-stage%20learnable%20framework%20designed%20to%20enable%20fine-grained%0Acontrol%20over%20the%20length%20of%20reasoning%20chains%20based%20on%20the%20importance%20of%20each%0Aindividual%20step.%20In%20the%20first%20stage%2C%20SmartThinker%20adapts%20a%20reasoning%20model%20to%20a%0Ashort-form%20reasoning%20mode%20through%20rejection%20sampling%20combined%20with%20supervised%0Afine-tuning%20%28SFT%29.%20In%20the%20second%20stage%2C%20SmartThinker%20applies%20Step-Level%20Length%0AControl%20Policy%20Optimization%20%28SCPO%29%20to%20refine%20the%20model%20output%20distribution%2C%0Awhich%20increases%20the%20proportion%20of%20length%20allocated%20to%20critical%20steps%20while%0Areducing%20redundancy%20in%20less%20important%20ones.%20SCPO%20consists%20of%20four%20core%0Acomponents%3A%20an%20online%20importance%20estimator%2C%20a%20step-level%20length%20control%20reward%0Afunction%2C%20a%20step-level%20generalized%20advantage%20estimation%20%28S-GAE%29%20and%20a%0Adifficulty-adaptive%20clipping%20strategy.%20Working%20in%20concert%2C%20these%20components%0Aenable%20SCPO%20to%20implement%20differentiated%20length%20control%20across%20reasoning%20steps.%0AEmpirical%20results%20across%20multiple%20reasoning%20benchmarks%20and%20various%20backbone%0Amodels%20demonstrate%20that%20SmartThinker%20significantly%20reduces%20redundant%20reasoning%0Awhile%20achieving%20comparable%20or%20even%20superior%20performance%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04348v2&entry.124074799=Read"},
{"title": "Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop\n  Mapping with Deep Learning", "author": "Mehmet Ozgur Turkoglu and Selene Ledain and Helge Aasen", "abstract": "  Crop type classification using optical satellite time series remains limited\nin its ability to generalize across seasons, particularly when crop phenology\nshifts due to inter-annual weather variability. This hampers real-world\napplicability in scenarios where current-year labels are unavailable. In\naddition, uncertainty quantification is often overlooked, which reduces the\nreliability of such approaches for operational crop monitoring. Inspired by\necophysiological principles of plant growth, we propose a simple,\nmodel-agnostic Thermal-Time-based Temporal Sampling (T3S) method that replaces\ncalendar time with thermal time. By subsampling time series in this\nbiologically meaningful way, our method highlights key periods within the\ngrowing season while reducing temporal redundancy and noise. We evaluate the\nT3S on a multi-year Sentinel-2 dataset covering the entirety of Switzerland,\nwhich allows us to assess all applied methods on unseen years. Compared to\nstate-of-the-art baselines, our approach yields substantial improvements in\nclassification accuracy and, critically, provides well-calibrated uncertainty\nestimates. Moreover, the T3S method excels in low-data regimes and enables\nsignificantly more accurate early-season classification. With just 10% of the\ntraining labels, it outperforms the current baseline in both accuracy and\nuncertainty calibration, and by the end of June, it achieves a performance\nsimilar to the full-season baseline model.\n", "link": "http://arxiv.org/abs/2506.12885v3", "date": "2025-07-17", "relevancy": 1.9613, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4903}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Agnostic%2C%20Temperature-Informed%20Sampling%20Enhances%20Cross-Year%20Crop%0A%20%20Mapping%20with%20Deep%20Learning&body=Title%3A%20Model-Agnostic%2C%20Temperature-Informed%20Sampling%20Enhances%20Cross-Year%20Crop%0A%20%20Mapping%20with%20Deep%20Learning%0AAuthor%3A%20Mehmet%20Ozgur%20Turkoglu%20and%20Selene%20Ledain%20and%20Helge%20Aasen%0AAbstract%3A%20%20%20Crop%20type%20classification%20using%20optical%20satellite%20time%20series%20remains%20limited%0Ain%20its%20ability%20to%20generalize%20across%20seasons%2C%20particularly%20when%20crop%20phenology%0Ashifts%20due%20to%20inter-annual%20weather%20variability.%20This%20hampers%20real-world%0Aapplicability%20in%20scenarios%20where%20current-year%20labels%20are%20unavailable.%20In%0Aaddition%2C%20uncertainty%20quantification%20is%20often%20overlooked%2C%20which%20reduces%20the%0Areliability%20of%20such%20approaches%20for%20operational%20crop%20monitoring.%20Inspired%20by%0Aecophysiological%20principles%20of%20plant%20growth%2C%20we%20propose%20a%20simple%2C%0Amodel-agnostic%20Thermal-Time-based%20Temporal%20Sampling%20%28T3S%29%20method%20that%20replaces%0Acalendar%20time%20with%20thermal%20time.%20By%20subsampling%20time%20series%20in%20this%0Abiologically%20meaningful%20way%2C%20our%20method%20highlights%20key%20periods%20within%20the%0Agrowing%20season%20while%20reducing%20temporal%20redundancy%20and%20noise.%20We%20evaluate%20the%0AT3S%20on%20a%20multi-year%20Sentinel-2%20dataset%20covering%20the%20entirety%20of%20Switzerland%2C%0Awhich%20allows%20us%20to%20assess%20all%20applied%20methods%20on%20unseen%20years.%20Compared%20to%0Astate-of-the-art%20baselines%2C%20our%20approach%20yields%20substantial%20improvements%20in%0Aclassification%20accuracy%20and%2C%20critically%2C%20provides%20well-calibrated%20uncertainty%0Aestimates.%20Moreover%2C%20the%20T3S%20method%20excels%20in%20low-data%20regimes%20and%20enables%0Asignificantly%20more%20accurate%20early-season%20classification.%20With%20just%2010%25%20of%20the%0Atraining%20labels%2C%20it%20outperforms%20the%20current%20baseline%20in%20both%20accuracy%20and%0Auncertainty%20calibration%2C%20and%20by%20the%20end%20of%20June%2C%20it%20achieves%20a%20performance%0Asimilar%20to%20the%20full-season%20baseline%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Agnostic%252C%2520Temperature-Informed%2520Sampling%2520Enhances%2520Cross-Year%2520Crop%250A%2520%2520Mapping%2520with%2520Deep%2520Learning%26entry.906535625%3DMehmet%2520Ozgur%2520Turkoglu%2520and%2520Selene%2520Ledain%2520and%2520Helge%2520Aasen%26entry.1292438233%3D%2520%2520Crop%2520type%2520classification%2520using%2520optical%2520satellite%2520time%2520series%2520remains%2520limited%250Ain%2520its%2520ability%2520to%2520generalize%2520across%2520seasons%252C%2520particularly%2520when%2520crop%2520phenology%250Ashifts%2520due%2520to%2520inter-annual%2520weather%2520variability.%2520This%2520hampers%2520real-world%250Aapplicability%2520in%2520scenarios%2520where%2520current-year%2520labels%2520are%2520unavailable.%2520In%250Aaddition%252C%2520uncertainty%2520quantification%2520is%2520often%2520overlooked%252C%2520which%2520reduces%2520the%250Areliability%2520of%2520such%2520approaches%2520for%2520operational%2520crop%2520monitoring.%2520Inspired%2520by%250Aecophysiological%2520principles%2520of%2520plant%2520growth%252C%2520we%2520propose%2520a%2520simple%252C%250Amodel-agnostic%2520Thermal-Time-based%2520Temporal%2520Sampling%2520%2528T3S%2529%2520method%2520that%2520replaces%250Acalendar%2520time%2520with%2520thermal%2520time.%2520By%2520subsampling%2520time%2520series%2520in%2520this%250Abiologically%2520meaningful%2520way%252C%2520our%2520method%2520highlights%2520key%2520periods%2520within%2520the%250Agrowing%2520season%2520while%2520reducing%2520temporal%2520redundancy%2520and%2520noise.%2520We%2520evaluate%2520the%250AT3S%2520on%2520a%2520multi-year%2520Sentinel-2%2520dataset%2520covering%2520the%2520entirety%2520of%2520Switzerland%252C%250Awhich%2520allows%2520us%2520to%2520assess%2520all%2520applied%2520methods%2520on%2520unseen%2520years.%2520Compared%2520to%250Astate-of-the-art%2520baselines%252C%2520our%2520approach%2520yields%2520substantial%2520improvements%2520in%250Aclassification%2520accuracy%2520and%252C%2520critically%252C%2520provides%2520well-calibrated%2520uncertainty%250Aestimates.%2520Moreover%252C%2520the%2520T3S%2520method%2520excels%2520in%2520low-data%2520regimes%2520and%2520enables%250Asignificantly%2520more%2520accurate%2520early-season%2520classification.%2520With%2520just%252010%2525%2520of%2520the%250Atraining%2520labels%252C%2520it%2520outperforms%2520the%2520current%2520baseline%2520in%2520both%2520accuracy%2520and%250Auncertainty%2520calibration%252C%2520and%2520by%2520the%2520end%2520of%2520June%252C%2520it%2520achieves%2520a%2520performance%250Asimilar%2520to%2520the%2520full-season%2520baseline%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Agnostic%2C%20Temperature-Informed%20Sampling%20Enhances%20Cross-Year%20Crop%0A%20%20Mapping%20with%20Deep%20Learning&entry.906535625=Mehmet%20Ozgur%20Turkoglu%20and%20Selene%20Ledain%20and%20Helge%20Aasen&entry.1292438233=%20%20Crop%20type%20classification%20using%20optical%20satellite%20time%20series%20remains%20limited%0Ain%20its%20ability%20to%20generalize%20across%20seasons%2C%20particularly%20when%20crop%20phenology%0Ashifts%20due%20to%20inter-annual%20weather%20variability.%20This%20hampers%20real-world%0Aapplicability%20in%20scenarios%20where%20current-year%20labels%20are%20unavailable.%20In%0Aaddition%2C%20uncertainty%20quantification%20is%20often%20overlooked%2C%20which%20reduces%20the%0Areliability%20of%20such%20approaches%20for%20operational%20crop%20monitoring.%20Inspired%20by%0Aecophysiological%20principles%20of%20plant%20growth%2C%20we%20propose%20a%20simple%2C%0Amodel-agnostic%20Thermal-Time-based%20Temporal%20Sampling%20%28T3S%29%20method%20that%20replaces%0Acalendar%20time%20with%20thermal%20time.%20By%20subsampling%20time%20series%20in%20this%0Abiologically%20meaningful%20way%2C%20our%20method%20highlights%20key%20periods%20within%20the%0Agrowing%20season%20while%20reducing%20temporal%20redundancy%20and%20noise.%20We%20evaluate%20the%0AT3S%20on%20a%20multi-year%20Sentinel-2%20dataset%20covering%20the%20entirety%20of%20Switzerland%2C%0Awhich%20allows%20us%20to%20assess%20all%20applied%20methods%20on%20unseen%20years.%20Compared%20to%0Astate-of-the-art%20baselines%2C%20our%20approach%20yields%20substantial%20improvements%20in%0Aclassification%20accuracy%20and%2C%20critically%2C%20provides%20well-calibrated%20uncertainty%0Aestimates.%20Moreover%2C%20the%20T3S%20method%20excels%20in%20low-data%20regimes%20and%20enables%0Asignificantly%20more%20accurate%20early-season%20classification.%20With%20just%2010%25%20of%20the%0Atraining%20labels%2C%20it%20outperforms%20the%20current%20baseline%20in%20both%20accuracy%20and%0Auncertainty%20calibration%2C%20and%20by%20the%20end%20of%20June%2C%20it%20achieves%20a%20performance%0Asimilar%20to%20the%20full-season%20baseline%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12885v3&entry.124074799=Read"},
{"title": "GradNetOT: Learning Optimal Transport Maps with GradNets", "author": "Shreyas Chaudhari and Srinivasa Pranav and Jos\u00e9 M. F. Moura", "abstract": "  Monotone gradient functions play a central role in solving the Monge\nformulation of the optimal transport problem, which arises in modern\napplications ranging from fluid dynamics to robot swarm control. When the\ntransport cost is the squared Euclidean distance, Brenier's theorem guarantees\nthat the unique optimal map is the gradient of a convex function, namely a\nmonotone gradient map, and it satisfies a Monge-Amp\\`ere equation. In\n[arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks\n(mGradNets), neural networks that directly parameterize the space of monotone\ngradient maps. In this work, we leverage mGradNets to directly learn the\noptimal transport mapping by minimizing a training loss function defined using\nthe Monge-Amp\\`ere equation. We empirically show that the structural bias of\nmGradNets facilitates the learning of optimal transport maps and employ our\nmethod for a robot swarm control problem.\n", "link": "http://arxiv.org/abs/2507.13191v1", "date": "2025-07-17", "relevancy": 1.9573, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GradNetOT%3A%20Learning%20Optimal%20Transport%20Maps%20with%20GradNets&body=Title%3A%20GradNetOT%3A%20Learning%20Optimal%20Transport%20Maps%20with%20GradNets%0AAuthor%3A%20Shreyas%20Chaudhari%20and%20Srinivasa%20Pranav%20and%20Jos%C3%A9%20M.%20F.%20Moura%0AAbstract%3A%20%20%20Monotone%20gradient%20functions%20play%20a%20central%20role%20in%20solving%20the%20Monge%0Aformulation%20of%20the%20optimal%20transport%20problem%2C%20which%20arises%20in%20modern%0Aapplications%20ranging%20from%20fluid%20dynamics%20to%20robot%20swarm%20control.%20When%20the%0Atransport%20cost%20is%20the%20squared%20Euclidean%20distance%2C%20Brenier%27s%20theorem%20guarantees%0Athat%20the%20unique%20optimal%20map%20is%20the%20gradient%20of%20a%20convex%20function%2C%20namely%20a%0Amonotone%20gradient%20map%2C%20and%20it%20satisfies%20a%20Monge-Amp%5C%60ere%20equation.%20In%0A%5BarXiv%3A2301.10862%5D%20%5BarXiv%3A2404.07361%5D%2C%20we%20proposed%20Monotone%20Gradient%20Networks%0A%28mGradNets%29%2C%20neural%20networks%20that%20directly%20parameterize%20the%20space%20of%20monotone%0Agradient%20maps.%20In%20this%20work%2C%20we%20leverage%20mGradNets%20to%20directly%20learn%20the%0Aoptimal%20transport%20mapping%20by%20minimizing%20a%20training%20loss%20function%20defined%20using%0Athe%20Monge-Amp%5C%60ere%20equation.%20We%20empirically%20show%20that%20the%20structural%20bias%20of%0AmGradNets%20facilitates%20the%20learning%20of%20optimal%20transport%20maps%20and%20employ%20our%0Amethod%20for%20a%20robot%20swarm%20control%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradNetOT%253A%2520Learning%2520Optimal%2520Transport%2520Maps%2520with%2520GradNets%26entry.906535625%3DShreyas%2520Chaudhari%2520and%2520Srinivasa%2520Pranav%2520and%2520Jos%25C3%25A9%2520M.%2520F.%2520Moura%26entry.1292438233%3D%2520%2520Monotone%2520gradient%2520functions%2520play%2520a%2520central%2520role%2520in%2520solving%2520the%2520Monge%250Aformulation%2520of%2520the%2520optimal%2520transport%2520problem%252C%2520which%2520arises%2520in%2520modern%250Aapplications%2520ranging%2520from%2520fluid%2520dynamics%2520to%2520robot%2520swarm%2520control.%2520When%2520the%250Atransport%2520cost%2520is%2520the%2520squared%2520Euclidean%2520distance%252C%2520Brenier%2527s%2520theorem%2520guarantees%250Athat%2520the%2520unique%2520optimal%2520map%2520is%2520the%2520gradient%2520of%2520a%2520convex%2520function%252C%2520namely%2520a%250Amonotone%2520gradient%2520map%252C%2520and%2520it%2520satisfies%2520a%2520Monge-Amp%255C%2560ere%2520equation.%2520In%250A%255BarXiv%253A2301.10862%255D%2520%255BarXiv%253A2404.07361%255D%252C%2520we%2520proposed%2520Monotone%2520Gradient%2520Networks%250A%2528mGradNets%2529%252C%2520neural%2520networks%2520that%2520directly%2520parameterize%2520the%2520space%2520of%2520monotone%250Agradient%2520maps.%2520In%2520this%2520work%252C%2520we%2520leverage%2520mGradNets%2520to%2520directly%2520learn%2520the%250Aoptimal%2520transport%2520mapping%2520by%2520minimizing%2520a%2520training%2520loss%2520function%2520defined%2520using%250Athe%2520Monge-Amp%255C%2560ere%2520equation.%2520We%2520empirically%2520show%2520that%2520the%2520structural%2520bias%2520of%250AmGradNets%2520facilitates%2520the%2520learning%2520of%2520optimal%2520transport%2520maps%2520and%2520employ%2520our%250Amethod%2520for%2520a%2520robot%2520swarm%2520control%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GradNetOT%3A%20Learning%20Optimal%20Transport%20Maps%20with%20GradNets&entry.906535625=Shreyas%20Chaudhari%20and%20Srinivasa%20Pranav%20and%20Jos%C3%A9%20M.%20F.%20Moura&entry.1292438233=%20%20Monotone%20gradient%20functions%20play%20a%20central%20role%20in%20solving%20the%20Monge%0Aformulation%20of%20the%20optimal%20transport%20problem%2C%20which%20arises%20in%20modern%0Aapplications%20ranging%20from%20fluid%20dynamics%20to%20robot%20swarm%20control.%20When%20the%0Atransport%20cost%20is%20the%20squared%20Euclidean%20distance%2C%20Brenier%27s%20theorem%20guarantees%0Athat%20the%20unique%20optimal%20map%20is%20the%20gradient%20of%20a%20convex%20function%2C%20namely%20a%0Amonotone%20gradient%20map%2C%20and%20it%20satisfies%20a%20Monge-Amp%5C%60ere%20equation.%20In%0A%5BarXiv%3A2301.10862%5D%20%5BarXiv%3A2404.07361%5D%2C%20we%20proposed%20Monotone%20Gradient%20Networks%0A%28mGradNets%29%2C%20neural%20networks%20that%20directly%20parameterize%20the%20space%20of%20monotone%0Agradient%20maps.%20In%20this%20work%2C%20we%20leverage%20mGradNets%20to%20directly%20learn%20the%0Aoptimal%20transport%20mapping%20by%20minimizing%20a%20training%20loss%20function%20defined%20using%0Athe%20Monge-Amp%5C%60ere%20equation.%20We%20empirically%20show%20that%20the%20structural%20bias%20of%0AmGradNets%20facilitates%20the%20learning%20of%20optimal%20transport%20maps%20and%20employ%20our%0Amethod%20for%20a%20robot%20swarm%20control%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13191v1&entry.124074799=Read"},
{"title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust\n  Deepfake Detection against Adversarial Attacks", "author": "Kutub Uddin and Awais Khan and Muhammad Umar Farooq and Khalid Malik", "abstract": "  Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively.\n", "link": "http://arxiv.org/abs/2507.13170v1", "date": "2025-07-17", "relevancy": 1.9571, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4949}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4903}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHIELD%3A%20A%20Secure%20and%20Highly%20Enhanced%20Integrated%20Learning%20for%20Robust%0A%20%20Deepfake%20Detection%20against%20Adversarial%20Attacks&body=Title%3A%20SHIELD%3A%20A%20Secure%20and%20Highly%20Enhanced%20Integrated%20Learning%20for%20Robust%0A%20%20Deepfake%20Detection%20against%20Adversarial%20Attacks%0AAuthor%3A%20Kutub%20Uddin%20and%20Awais%20Khan%20and%20Muhammad%20Umar%20Farooq%20and%20Khalid%20Malik%0AAbstract%3A%20%20%20Audio%20plays%20a%20crucial%20role%20in%20applications%20like%20speaker%20verification%2C%0Avoice-enabled%20smart%20devices%2C%20and%20audio%20conferencing.%20However%2C%20audio%0Amanipulations%2C%20such%20as%20deepfakes%2C%20pose%20significant%20risks%20by%20enabling%20the%20spread%0Aof%20misinformation.%20Our%20empirical%20analysis%20reveals%20that%20existing%20methods%20for%0Adetecting%20deepfake%20audio%20are%20often%20vulnerable%20to%20anti-forensic%20%28AF%29%20attacks%2C%0Aparticularly%20those%20attacked%20using%20generative%20adversarial%20networks.%20In%20this%0Aarticle%2C%20we%20propose%20a%20novel%20collaborative%20learning%20method%20called%20SHIELD%20to%0Adefend%20against%20generative%20AF%20attacks.%20To%20expose%20AF%20signatures%2C%20we%20integrate%20an%0Aauxiliary%20generative%20model%2C%20called%20the%20defense%20%28DF%29%20generative%20model%2C%20which%0Afacilitates%20collaborative%20learning%20by%20combining%20input%20and%20output.%20Furthermore%2C%0Awe%20design%20a%20triplet%20model%20to%20capture%20correlations%20for%20real%20and%20AF%20attacked%0Aaudios%20with%20real-generated%20and%20attacked-generated%20audios%20using%20auxiliary%0Agenerative%20models.%20The%20proposed%20SHIELD%20strengthens%20the%20defense%20against%0Agenerative%20AF%20attacks%20and%20achieves%20robust%20performance%20across%20various%20generative%0Amodels.%20The%20proposed%20AF%20significantly%20reduces%20the%20average%20detection%20accuracy%0Afrom%2095.49%25%20to%2059.77%25%20for%20ASVspoof2019%2C%20from%2099.44%25%20to%2038.45%25%20for%20In-the-Wild%2C%0Aand%20from%2098.41%25%20to%2051.18%25%20for%20HalfTruth%20for%20three%20different%20generative%20models.%0AThe%20proposed%20SHIELD%20mechanism%20is%20robust%20against%20AF%20attacks%20and%20achieves%20an%0Aaverage%20accuracy%20of%2098.13%25%2C%2098.58%25%2C%20and%2099.57%25%20in%20match%2C%20and%2098.78%25%2C%2098.62%25%2C%0Aand%2098.85%25%20in%20mismatch%20settings%20for%20the%20ASVspoof2019%2C%20In-the-Wild%2C%20and%0AHalfTruth%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHIELD%253A%2520A%2520Secure%2520and%2520Highly%2520Enhanced%2520Integrated%2520Learning%2520for%2520Robust%250A%2520%2520Deepfake%2520Detection%2520against%2520Adversarial%2520Attacks%26entry.906535625%3DKutub%2520Uddin%2520and%2520Awais%2520Khan%2520and%2520Muhammad%2520Umar%2520Farooq%2520and%2520Khalid%2520Malik%26entry.1292438233%3D%2520%2520Audio%2520plays%2520a%2520crucial%2520role%2520in%2520applications%2520like%2520speaker%2520verification%252C%250Avoice-enabled%2520smart%2520devices%252C%2520and%2520audio%2520conferencing.%2520However%252C%2520audio%250Amanipulations%252C%2520such%2520as%2520deepfakes%252C%2520pose%2520significant%2520risks%2520by%2520enabling%2520the%2520spread%250Aof%2520misinformation.%2520Our%2520empirical%2520analysis%2520reveals%2520that%2520existing%2520methods%2520for%250Adetecting%2520deepfake%2520audio%2520are%2520often%2520vulnerable%2520to%2520anti-forensic%2520%2528AF%2529%2520attacks%252C%250Aparticularly%2520those%2520attacked%2520using%2520generative%2520adversarial%2520networks.%2520In%2520this%250Aarticle%252C%2520we%2520propose%2520a%2520novel%2520collaborative%2520learning%2520method%2520called%2520SHIELD%2520to%250Adefend%2520against%2520generative%2520AF%2520attacks.%2520To%2520expose%2520AF%2520signatures%252C%2520we%2520integrate%2520an%250Aauxiliary%2520generative%2520model%252C%2520called%2520the%2520defense%2520%2528DF%2529%2520generative%2520model%252C%2520which%250Afacilitates%2520collaborative%2520learning%2520by%2520combining%2520input%2520and%2520output.%2520Furthermore%252C%250Awe%2520design%2520a%2520triplet%2520model%2520to%2520capture%2520correlations%2520for%2520real%2520and%2520AF%2520attacked%250Aaudios%2520with%2520real-generated%2520and%2520attacked-generated%2520audios%2520using%2520auxiliary%250Agenerative%2520models.%2520The%2520proposed%2520SHIELD%2520strengthens%2520the%2520defense%2520against%250Agenerative%2520AF%2520attacks%2520and%2520achieves%2520robust%2520performance%2520across%2520various%2520generative%250Amodels.%2520The%2520proposed%2520AF%2520significantly%2520reduces%2520the%2520average%2520detection%2520accuracy%250Afrom%252095.49%2525%2520to%252059.77%2525%2520for%2520ASVspoof2019%252C%2520from%252099.44%2525%2520to%252038.45%2525%2520for%2520In-the-Wild%252C%250Aand%2520from%252098.41%2525%2520to%252051.18%2525%2520for%2520HalfTruth%2520for%2520three%2520different%2520generative%2520models.%250AThe%2520proposed%2520SHIELD%2520mechanism%2520is%2520robust%2520against%2520AF%2520attacks%2520and%2520achieves%2520an%250Aaverage%2520accuracy%2520of%252098.13%2525%252C%252098.58%2525%252C%2520and%252099.57%2525%2520in%2520match%252C%2520and%252098.78%2525%252C%252098.62%2525%252C%250Aand%252098.85%2525%2520in%2520mismatch%2520settings%2520for%2520the%2520ASVspoof2019%252C%2520In-the-Wild%252C%2520and%250AHalfTruth%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHIELD%3A%20A%20Secure%20and%20Highly%20Enhanced%20Integrated%20Learning%20for%20Robust%0A%20%20Deepfake%20Detection%20against%20Adversarial%20Attacks&entry.906535625=Kutub%20Uddin%20and%20Awais%20Khan%20and%20Muhammad%20Umar%20Farooq%20and%20Khalid%20Malik&entry.1292438233=%20%20Audio%20plays%20a%20crucial%20role%20in%20applications%20like%20speaker%20verification%2C%0Avoice-enabled%20smart%20devices%2C%20and%20audio%20conferencing.%20However%2C%20audio%0Amanipulations%2C%20such%20as%20deepfakes%2C%20pose%20significant%20risks%20by%20enabling%20the%20spread%0Aof%20misinformation.%20Our%20empirical%20analysis%20reveals%20that%20existing%20methods%20for%0Adetecting%20deepfake%20audio%20are%20often%20vulnerable%20to%20anti-forensic%20%28AF%29%20attacks%2C%0Aparticularly%20those%20attacked%20using%20generative%20adversarial%20networks.%20In%20this%0Aarticle%2C%20we%20propose%20a%20novel%20collaborative%20learning%20method%20called%20SHIELD%20to%0Adefend%20against%20generative%20AF%20attacks.%20To%20expose%20AF%20signatures%2C%20we%20integrate%20an%0Aauxiliary%20generative%20model%2C%20called%20the%20defense%20%28DF%29%20generative%20model%2C%20which%0Afacilitates%20collaborative%20learning%20by%20combining%20input%20and%20output.%20Furthermore%2C%0Awe%20design%20a%20triplet%20model%20to%20capture%20correlations%20for%20real%20and%20AF%20attacked%0Aaudios%20with%20real-generated%20and%20attacked-generated%20audios%20using%20auxiliary%0Agenerative%20models.%20The%20proposed%20SHIELD%20strengthens%20the%20defense%20against%0Agenerative%20AF%20attacks%20and%20achieves%20robust%20performance%20across%20various%20generative%0Amodels.%20The%20proposed%20AF%20significantly%20reduces%20the%20average%20detection%20accuracy%0Afrom%2095.49%25%20to%2059.77%25%20for%20ASVspoof2019%2C%20from%2099.44%25%20to%2038.45%25%20for%20In-the-Wild%2C%0Aand%20from%2098.41%25%20to%2051.18%25%20for%20HalfTruth%20for%20three%20different%20generative%20models.%0AThe%20proposed%20SHIELD%20mechanism%20is%20robust%20against%20AF%20attacks%20and%20achieves%20an%0Aaverage%20accuracy%20of%2098.13%25%2C%2098.58%25%2C%20and%2099.57%25%20in%20match%2C%20and%2098.78%25%2C%2098.62%25%2C%0Aand%2098.85%25%20in%20mismatch%20settings%20for%20the%20ASVspoof2019%2C%20In-the-Wild%2C%20and%0AHalfTruth%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13170v1&entry.124074799=Read"},
{"title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information", "author": "Yingya Li and Timothy Miller and Steven Bethard and Guergana Savova", "abstract": "  The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.\n", "link": "http://arxiv.org/abs/2410.12774v2", "date": "2025-07-17", "relevancy": 1.946, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Task%20Groupings%20for%20Multi-Task%20Learning%20Using%20Pointwise%0A%20%20V-Usable%20Information&body=Title%3A%20Identifying%20Task%20Groupings%20for%20Multi-Task%20Learning%20Using%20Pointwise%0A%20%20V-Usable%20Information%0AAuthor%3A%20Yingya%20Li%20and%20Timothy%20Miller%20and%20Steven%20Bethard%20and%20Guergana%20Savova%0AAbstract%3A%20%20%20The%20success%20of%20multi-task%20learning%20can%20depend%20heavily%20on%20which%20tasks%20are%0Agrouped%20together.%20Naively%20grouping%20all%20tasks%20or%20a%20random%20set%20of%20tasks%20can%0Aresult%20in%20negative%20transfer%2C%20with%20the%20multi-task%20models%20performing%20worse%20than%0Asingle-task%20models.%20Though%20many%20efforts%20have%20been%20made%20to%20identify%20task%0Agroupings%20and%20to%20measure%20the%20relatedness%20among%20different%20tasks%2C%20it%20remains%20a%0Achallenging%20research%20topic%20to%20define%20a%20metric%20to%20identify%20the%20best%20task%0Agrouping%20out%20of%20a%20pool%20of%20many%20potential%20task%20combinations.%20We%20propose%20a%20metric%0Aof%20task%20relatedness%20based%20on%20task%20difficulty%20measured%20by%20pointwise%20V-usable%0Ainformation%20%28PVI%29.%20PVI%20is%20a%20recently%20proposed%20metric%20to%20estimate%20how%20much%0Ausable%20information%20a%20dataset%20contains%20given%20a%20model.%20We%20hypothesize%20that%20tasks%0Awith%20not%20statistically%20different%20PVI%20estimates%20are%20similar%20enough%20to%20benefit%0Afrom%20the%20joint%20learning%20process.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20the%20feasibility%20of%20this%20metric%20for%20task%20grouping%20on%2015%20NLP%20datasets%20in%0Athe%20general%2C%20biomedical%2C%20and%20clinical%20domains.%20We%20compare%20the%20results%20of%20the%0Ajoint%20learners%20against%20single%20learners%2C%20existing%20baseline%20methods%2C%20and%20recent%0Alarge%20language%20models%2C%20including%20Llama%202%20and%20GPT-4.%20The%20results%20show%20that%20by%0Agrouping%20tasks%20with%20similar%20PVI%20estimates%2C%20the%20joint%20learners%20yielded%0Acompetitive%20results%20with%20fewer%20total%20parameters%2C%20with%20consistent%20performance%0Aacross%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Task%2520Groupings%2520for%2520Multi-Task%2520Learning%2520Using%2520Pointwise%250A%2520%2520V-Usable%2520Information%26entry.906535625%3DYingya%2520Li%2520and%2520Timothy%2520Miller%2520and%2520Steven%2520Bethard%2520and%2520Guergana%2520Savova%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520multi-task%2520learning%2520can%2520depend%2520heavily%2520on%2520which%2520tasks%2520are%250Agrouped%2520together.%2520Naively%2520grouping%2520all%2520tasks%2520or%2520a%2520random%2520set%2520of%2520tasks%2520can%250Aresult%2520in%2520negative%2520transfer%252C%2520with%2520the%2520multi-task%2520models%2520performing%2520worse%2520than%250Asingle-task%2520models.%2520Though%2520many%2520efforts%2520have%2520been%2520made%2520to%2520identify%2520task%250Agroupings%2520and%2520to%2520measure%2520the%2520relatedness%2520among%2520different%2520tasks%252C%2520it%2520remains%2520a%250Achallenging%2520research%2520topic%2520to%2520define%2520a%2520metric%2520to%2520identify%2520the%2520best%2520task%250Agrouping%2520out%2520of%2520a%2520pool%2520of%2520many%2520potential%2520task%2520combinations.%2520We%2520propose%2520a%2520metric%250Aof%2520task%2520relatedness%2520based%2520on%2520task%2520difficulty%2520measured%2520by%2520pointwise%2520V-usable%250Ainformation%2520%2528PVI%2529.%2520PVI%2520is%2520a%2520recently%2520proposed%2520metric%2520to%2520estimate%2520how%2520much%250Ausable%2520information%2520a%2520dataset%2520contains%2520given%2520a%2520model.%2520We%2520hypothesize%2520that%2520tasks%250Awith%2520not%2520statistically%2520different%2520PVI%2520estimates%2520are%2520similar%2520enough%2520to%2520benefit%250Afrom%2520the%2520joint%2520learning%2520process.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%250Aevaluate%2520the%2520feasibility%2520of%2520this%2520metric%2520for%2520task%2520grouping%2520on%252015%2520NLP%2520datasets%2520in%250Athe%2520general%252C%2520biomedical%252C%2520and%2520clinical%2520domains.%2520We%2520compare%2520the%2520results%2520of%2520the%250Ajoint%2520learners%2520against%2520single%2520learners%252C%2520existing%2520baseline%2520methods%252C%2520and%2520recent%250Alarge%2520language%2520models%252C%2520including%2520Llama%25202%2520and%2520GPT-4.%2520The%2520results%2520show%2520that%2520by%250Agrouping%2520tasks%2520with%2520similar%2520PVI%2520estimates%252C%2520the%2520joint%2520learners%2520yielded%250Acompetitive%2520results%2520with%2520fewer%2520total%2520parameters%252C%2520with%2520consistent%2520performance%250Aacross%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Task%20Groupings%20for%20Multi-Task%20Learning%20Using%20Pointwise%0A%20%20V-Usable%20Information&entry.906535625=Yingya%20Li%20and%20Timothy%20Miller%20and%20Steven%20Bethard%20and%20Guergana%20Savova&entry.1292438233=%20%20The%20success%20of%20multi-task%20learning%20can%20depend%20heavily%20on%20which%20tasks%20are%0Agrouped%20together.%20Naively%20grouping%20all%20tasks%20or%20a%20random%20set%20of%20tasks%20can%0Aresult%20in%20negative%20transfer%2C%20with%20the%20multi-task%20models%20performing%20worse%20than%0Asingle-task%20models.%20Though%20many%20efforts%20have%20been%20made%20to%20identify%20task%0Agroupings%20and%20to%20measure%20the%20relatedness%20among%20different%20tasks%2C%20it%20remains%20a%0Achallenging%20research%20topic%20to%20define%20a%20metric%20to%20identify%20the%20best%20task%0Agrouping%20out%20of%20a%20pool%20of%20many%20potential%20task%20combinations.%20We%20propose%20a%20metric%0Aof%20task%20relatedness%20based%20on%20task%20difficulty%20measured%20by%20pointwise%20V-usable%0Ainformation%20%28PVI%29.%20PVI%20is%20a%20recently%20proposed%20metric%20to%20estimate%20how%20much%0Ausable%20information%20a%20dataset%20contains%20given%20a%20model.%20We%20hypothesize%20that%20tasks%0Awith%20not%20statistically%20different%20PVI%20estimates%20are%20similar%20enough%20to%20benefit%0Afrom%20the%20joint%20learning%20process.%20We%20conduct%20comprehensive%20experiments%20to%0Aevaluate%20the%20feasibility%20of%20this%20metric%20for%20task%20grouping%20on%2015%20NLP%20datasets%20in%0Athe%20general%2C%20biomedical%2C%20and%20clinical%20domains.%20We%20compare%20the%20results%20of%20the%0Ajoint%20learners%20against%20single%20learners%2C%20existing%20baseline%20methods%2C%20and%20recent%0Alarge%20language%20models%2C%20including%20Llama%202%20and%20GPT-4.%20The%20results%20show%20that%20by%0Agrouping%20tasks%20with%20similar%20PVI%20estimates%2C%20the%20joint%20learners%20yielded%0Acompetitive%20results%20with%20fewer%20total%20parameters%2C%20with%20consistent%20performance%0Aacross%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12774v2&entry.124074799=Read"},
{"title": "ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs", "author": "Fahmida Liza Piya and Rahmatollah Beheshti", "abstract": "  Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation.\n", "link": "http://arxiv.org/abs/2504.16394v3", "date": "2025-07-17", "relevancy": 1.945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConTextual%3A%20Improving%20Clinical%20Text%20Summarization%20in%20LLMs%20with%0A%20%20Context-preserving%20Token%20Filtering%20and%20Knowledge%20Graphs&body=Title%3A%20ConTextual%3A%20Improving%20Clinical%20Text%20Summarization%20in%20LLMs%20with%0A%20%20Context-preserving%20Token%20Filtering%20and%20Knowledge%20Graphs%0AAuthor%3A%20Fahmida%20Liza%20Piya%20and%20Rahmatollah%20Beheshti%0AAbstract%3A%20%20%20Unstructured%20clinical%20data%20can%20serve%20as%20a%20unique%20and%20rich%20source%20of%0Ainformation%20that%20can%20meaningfully%20inform%20clinical%20practice.%20Extracting%20the%20most%0Apertinent%20context%20from%20such%20data%20is%20critical%20for%20exploiting%20its%20true%20potential%0Atoward%20optimal%20and%20timely%20decision-making%20in%20patient%20care.%20While%20prior%20research%0Ahas%20explored%20various%20methods%20for%20clinical%20text%20summarization%2C%20most%20prior%0Astudies%20either%20process%20all%20input%20tokens%20uniformly%20or%20rely%20on%20heuristic-based%0Afilters%2C%20which%20can%20overlook%20nuanced%20clinical%20cues%20and%20fail%20to%20prioritize%0Ainformation%20critical%20for%20decision-making.%20In%20this%20study%2C%20we%20propose%20Contextual%2C%0Aa%20novel%20framework%20that%20integrates%20a%20Context-Preserving%20Token%20Filtering%20method%0Awith%20a%20Domain-Specific%20Knowledge%20Graph%20%28KG%29%20for%20contextual%20augmentation.%20By%0Apreserving%20context-specific%20important%20tokens%20and%20enriching%20them%20with%20structured%0Aknowledge%2C%20ConTextual%20improves%20both%20linguistic%20coherence%20and%20clinical%20fidelity.%0AOur%20extensive%20empirical%20evaluations%20on%20two%20public%20benchmark%20datasets%0Ademonstrate%20that%20ConTextual%20consistently%20outperforms%20other%20baselines.%20Our%0Aproposed%20approach%20highlights%20the%20complementary%20role%20of%20token-level%20filtering%0Aand%20structured%20retrieval%20in%20enhancing%20both%20linguistic%20and%20clinical%20integrity%2C%0Aas%20well%20as%20offering%20a%20scalable%20solution%20for%20improving%20precision%20in%20clinical%0Atext%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16394v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConTextual%253A%2520Improving%2520Clinical%2520Text%2520Summarization%2520in%2520LLMs%2520with%250A%2520%2520Context-preserving%2520Token%2520Filtering%2520and%2520Knowledge%2520Graphs%26entry.906535625%3DFahmida%2520Liza%2520Piya%2520and%2520Rahmatollah%2520Beheshti%26entry.1292438233%3D%2520%2520Unstructured%2520clinical%2520data%2520can%2520serve%2520as%2520a%2520unique%2520and%2520rich%2520source%2520of%250Ainformation%2520that%2520can%2520meaningfully%2520inform%2520clinical%2520practice.%2520Extracting%2520the%2520most%250Apertinent%2520context%2520from%2520such%2520data%2520is%2520critical%2520for%2520exploiting%2520its%2520true%2520potential%250Atoward%2520optimal%2520and%2520timely%2520decision-making%2520in%2520patient%2520care.%2520While%2520prior%2520research%250Ahas%2520explored%2520various%2520methods%2520for%2520clinical%2520text%2520summarization%252C%2520most%2520prior%250Astudies%2520either%2520process%2520all%2520input%2520tokens%2520uniformly%2520or%2520rely%2520on%2520heuristic-based%250Afilters%252C%2520which%2520can%2520overlook%2520nuanced%2520clinical%2520cues%2520and%2520fail%2520to%2520prioritize%250Ainformation%2520critical%2520for%2520decision-making.%2520In%2520this%2520study%252C%2520we%2520propose%2520Contextual%252C%250Aa%2520novel%2520framework%2520that%2520integrates%2520a%2520Context-Preserving%2520Token%2520Filtering%2520method%250Awith%2520a%2520Domain-Specific%2520Knowledge%2520Graph%2520%2528KG%2529%2520for%2520contextual%2520augmentation.%2520By%250Apreserving%2520context-specific%2520important%2520tokens%2520and%2520enriching%2520them%2520with%2520structured%250Aknowledge%252C%2520ConTextual%2520improves%2520both%2520linguistic%2520coherence%2520and%2520clinical%2520fidelity.%250AOur%2520extensive%2520empirical%2520evaluations%2520on%2520two%2520public%2520benchmark%2520datasets%250Ademonstrate%2520that%2520ConTextual%2520consistently%2520outperforms%2520other%2520baselines.%2520Our%250Aproposed%2520approach%2520highlights%2520the%2520complementary%2520role%2520of%2520token-level%2520filtering%250Aand%2520structured%2520retrieval%2520in%2520enhancing%2520both%2520linguistic%2520and%2520clinical%2520integrity%252C%250Aas%2520well%2520as%2520offering%2520a%2520scalable%2520solution%2520for%2520improving%2520precision%2520in%2520clinical%250Atext%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16394v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConTextual%3A%20Improving%20Clinical%20Text%20Summarization%20in%20LLMs%20with%0A%20%20Context-preserving%20Token%20Filtering%20and%20Knowledge%20Graphs&entry.906535625=Fahmida%20Liza%20Piya%20and%20Rahmatollah%20Beheshti&entry.1292438233=%20%20Unstructured%20clinical%20data%20can%20serve%20as%20a%20unique%20and%20rich%20source%20of%0Ainformation%20that%20can%20meaningfully%20inform%20clinical%20practice.%20Extracting%20the%20most%0Apertinent%20context%20from%20such%20data%20is%20critical%20for%20exploiting%20its%20true%20potential%0Atoward%20optimal%20and%20timely%20decision-making%20in%20patient%20care.%20While%20prior%20research%0Ahas%20explored%20various%20methods%20for%20clinical%20text%20summarization%2C%20most%20prior%0Astudies%20either%20process%20all%20input%20tokens%20uniformly%20or%20rely%20on%20heuristic-based%0Afilters%2C%20which%20can%20overlook%20nuanced%20clinical%20cues%20and%20fail%20to%20prioritize%0Ainformation%20critical%20for%20decision-making.%20In%20this%20study%2C%20we%20propose%20Contextual%2C%0Aa%20novel%20framework%20that%20integrates%20a%20Context-Preserving%20Token%20Filtering%20method%0Awith%20a%20Domain-Specific%20Knowledge%20Graph%20%28KG%29%20for%20contextual%20augmentation.%20By%0Apreserving%20context-specific%20important%20tokens%20and%20enriching%20them%20with%20structured%0Aknowledge%2C%20ConTextual%20improves%20both%20linguistic%20coherence%20and%20clinical%20fidelity.%0AOur%20extensive%20empirical%20evaluations%20on%20two%20public%20benchmark%20datasets%0Ademonstrate%20that%20ConTextual%20consistently%20outperforms%20other%20baselines.%20Our%0Aproposed%20approach%20highlights%20the%20complementary%20role%20of%20token-level%20filtering%0Aand%20structured%20retrieval%20in%20enhancing%20both%20linguistic%20and%20clinical%20integrity%2C%0Aas%20well%20as%20offering%20a%20scalable%20solution%20for%20improving%20precision%20in%20clinical%0Atext%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16394v3&entry.124074799=Read"},
{"title": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts", "author": "Aaron Councilman and David Fu and Aryan Gupta and Chengxiao Wang and David Grove and Yu-Xiong Wang and Vikram Adve", "abstract": "  In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.\n", "link": "http://arxiv.org/abs/2507.13290v1", "date": "2025-07-17", "relevancy": 0.8403, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.425}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4233}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Formal%20Verification%20of%20LLM-Generated%20Code%20from%20Natural%20Language%0A%20%20Prompts&body=Title%3A%20Towards%20Formal%20Verification%20of%20LLM-Generated%20Code%20from%20Natural%20Language%0A%20%20Prompts%0AAuthor%3A%20Aaron%20Councilman%20and%20David%20Fu%20and%20Aryan%20Gupta%20and%20Chengxiao%20Wang%20and%20David%20Grove%20and%20Yu-Xiong%20Wang%20and%20Vikram%20Adve%0AAbstract%3A%20%20%20In%20the%20past%20few%20years%20LLMs%20have%20emerged%20as%20a%20tool%20that%20can%20aid%20programmers%20by%0Ataking%20natural%20language%20descriptions%20and%20generating%20code%20based%20on%20it.%20However%2C%0ALLMs%20often%20generate%20incorrect%20code%20that%20users%20need%20to%20fix%20and%20the%20literature%0Asuggests%20users%20often%20struggle%20to%20detect%20these%20errors.%20In%20this%20work%20we%20seek%20to%0Aoffer%20formal%20guarantees%20of%20correctness%20to%20LLM%20generated%20code%3B%20such%20guarantees%0Acould%20improve%20the%20experience%20of%20using%20AI%20Code%20Assistants%20and%20potentially%20enable%0Anatural%20language%20programming%20for%20users%20with%20little%20or%20no%20programming%20knowledge.%0ATo%20address%20this%20challenge%20we%20propose%20to%20incorporate%20a%20formal%20query%20language%0Athat%20can%20represent%20a%20user%27s%20intent%20in%20a%20formally%20defined%20but%20natural%0Alanguage-like%20manner%20that%20a%20user%20can%20confirm%20matches%20their%20intent.%20Then%2C%20using%0Asuch%20a%20query%20we%20propose%20to%20verify%20LLM%20generated%20code%20to%20ensure%20it%20matches%20the%0Auser%27s%20intent.%20We%20implement%20these%20ideas%20in%20our%20system%2C%20Astrogator%2C%20for%20the%0AAnsible%20programming%20language%20which%20includes%20such%20a%20formal%20query%20language%2C%20a%0Acalculus%20for%20representing%20the%20behavior%20of%20Ansible%20programs%2C%20and%20a%20symbolic%0Ainterpreter%20which%20is%20used%20for%20the%20verification.%20On%20a%20benchmark%20suite%20of%2021%0Acode-generation%20tasks%2C%20our%20verifier%20is%20able%20to%20verify%20correct%20code%20in%2083%25%20of%0Acases%20and%20identify%20incorrect%20code%20in%2092%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Formal%2520Verification%2520of%2520LLM-Generated%2520Code%2520from%2520Natural%2520Language%250A%2520%2520Prompts%26entry.906535625%3DAaron%2520Councilman%2520and%2520David%2520Fu%2520and%2520Aryan%2520Gupta%2520and%2520Chengxiao%2520Wang%2520and%2520David%2520Grove%2520and%2520Yu-Xiong%2520Wang%2520and%2520Vikram%2520Adve%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520few%2520years%2520LLMs%2520have%2520emerged%2520as%2520a%2520tool%2520that%2520can%2520aid%2520programmers%2520by%250Ataking%2520natural%2520language%2520descriptions%2520and%2520generating%2520code%2520based%2520on%2520it.%2520However%252C%250ALLMs%2520often%2520generate%2520incorrect%2520code%2520that%2520users%2520need%2520to%2520fix%2520and%2520the%2520literature%250Asuggests%2520users%2520often%2520struggle%2520to%2520detect%2520these%2520errors.%2520In%2520this%2520work%2520we%2520seek%2520to%250Aoffer%2520formal%2520guarantees%2520of%2520correctness%2520to%2520LLM%2520generated%2520code%253B%2520such%2520guarantees%250Acould%2520improve%2520the%2520experience%2520of%2520using%2520AI%2520Code%2520Assistants%2520and%2520potentially%2520enable%250Anatural%2520language%2520programming%2520for%2520users%2520with%2520little%2520or%2520no%2520programming%2520knowledge.%250ATo%2520address%2520this%2520challenge%2520we%2520propose%2520to%2520incorporate%2520a%2520formal%2520query%2520language%250Athat%2520can%2520represent%2520a%2520user%2527s%2520intent%2520in%2520a%2520formally%2520defined%2520but%2520natural%250Alanguage-like%2520manner%2520that%2520a%2520user%2520can%2520confirm%2520matches%2520their%2520intent.%2520Then%252C%2520using%250Asuch%2520a%2520query%2520we%2520propose%2520to%2520verify%2520LLM%2520generated%2520code%2520to%2520ensure%2520it%2520matches%2520the%250Auser%2527s%2520intent.%2520We%2520implement%2520these%2520ideas%2520in%2520our%2520system%252C%2520Astrogator%252C%2520for%2520the%250AAnsible%2520programming%2520language%2520which%2520includes%2520such%2520a%2520formal%2520query%2520language%252C%2520a%250Acalculus%2520for%2520representing%2520the%2520behavior%2520of%2520Ansible%2520programs%252C%2520and%2520a%2520symbolic%250Ainterpreter%2520which%2520is%2520used%2520for%2520the%2520verification.%2520On%2520a%2520benchmark%2520suite%2520of%252021%250Acode-generation%2520tasks%252C%2520our%2520verifier%2520is%2520able%2520to%2520verify%2520correct%2520code%2520in%252083%2525%2520of%250Acases%2520and%2520identify%2520incorrect%2520code%2520in%252092%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Formal%20Verification%20of%20LLM-Generated%20Code%20from%20Natural%20Language%0A%20%20Prompts&entry.906535625=Aaron%20Councilman%20and%20David%20Fu%20and%20Aryan%20Gupta%20and%20Chengxiao%20Wang%20and%20David%20Grove%20and%20Yu-Xiong%20Wang%20and%20Vikram%20Adve&entry.1292438233=%20%20In%20the%20past%20few%20years%20LLMs%20have%20emerged%20as%20a%20tool%20that%20can%20aid%20programmers%20by%0Ataking%20natural%20language%20descriptions%20and%20generating%20code%20based%20on%20it.%20However%2C%0ALLMs%20often%20generate%20incorrect%20code%20that%20users%20need%20to%20fix%20and%20the%20literature%0Asuggests%20users%20often%20struggle%20to%20detect%20these%20errors.%20In%20this%20work%20we%20seek%20to%0Aoffer%20formal%20guarantees%20of%20correctness%20to%20LLM%20generated%20code%3B%20such%20guarantees%0Acould%20improve%20the%20experience%20of%20using%20AI%20Code%20Assistants%20and%20potentially%20enable%0Anatural%20language%20programming%20for%20users%20with%20little%20or%20no%20programming%20knowledge.%0ATo%20address%20this%20challenge%20we%20propose%20to%20incorporate%20a%20formal%20query%20language%0Athat%20can%20represent%20a%20user%27s%20intent%20in%20a%20formally%20defined%20but%20natural%0Alanguage-like%20manner%20that%20a%20user%20can%20confirm%20matches%20their%20intent.%20Then%2C%20using%0Asuch%20a%20query%20we%20propose%20to%20verify%20LLM%20generated%20code%20to%20ensure%20it%20matches%20the%0Auser%27s%20intent.%20We%20implement%20these%20ideas%20in%20our%20system%2C%20Astrogator%2C%20for%20the%0AAnsible%20programming%20language%20which%20includes%20such%20a%20formal%20query%20language%2C%20a%0Acalculus%20for%20representing%20the%20behavior%20of%20Ansible%20programs%2C%20and%20a%20symbolic%0Ainterpreter%20which%20is%20used%20for%20the%20verification.%20On%20a%20benchmark%20suite%20of%2021%0Acode-generation%20tasks%2C%20our%20verifier%20is%20able%20to%20verify%20correct%20code%20in%2083%25%20of%0Acases%20and%20identify%20incorrect%20code%20in%2092%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13290v1&entry.124074799=Read"},
{"title": "KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal\n  Fake News Detection", "author": "Peican Zhu and Yubo Jing and Le Cheng and Keke Tang and Yangming Guo", "abstract": "  In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.\n", "link": "http://arxiv.org/abs/2507.09647v2", "date": "2025-07-17", "relevancy": 1.0978, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KEN%3A%20Knowledge%20Augmentation%20and%20Emotion%20Guidance%20Network%20for%20Multimodal%0A%20%20Fake%20News%20Detection&body=Title%3A%20KEN%3A%20Knowledge%20Augmentation%20and%20Emotion%20Guidance%20Network%20for%20Multimodal%0A%20%20Fake%20News%20Detection%0AAuthor%3A%20Peican%20Zhu%20and%20Yubo%20Jing%20and%20Le%20Cheng%20and%20Keke%20Tang%20and%20Yangming%20Guo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rampant%20spread%20of%20misinformation%20on%20social%20media%20has%0Amade%20accurate%20detection%20of%20multimodal%20fake%20news%20a%20critical%20research%20focus.%0AHowever%2C%20previous%20research%20has%20not%20adequately%20understood%20the%20semantics%20of%0Aimages%2C%20and%20models%20struggle%20to%20discern%20news%20authenticity%20with%20limited%20textual%0Ainformation.%20Meanwhile%2C%20treating%20all%20emotional%20types%20of%20news%20uniformly%20without%0Atailored%20approaches%20further%20leads%20to%20performance%20degradation.%20Therefore%2C%20we%0Apropose%20a%20novel%20Knowledge%20Augmentation%20and%20Emotion%20Guidance%20Network%20%28KEN%29.%20On%0Athe%20one%20hand%2C%20we%20effectively%20leverage%20LVLM%27s%20powerful%20semantic%20understanding%0Aand%20extensive%20world%20knowledge.%20For%20images%2C%20the%20generated%20captions%20provide%20a%0Acomprehensive%20understanding%20of%20image%20content%20and%20scenes%2C%20while%20for%20text%2C%20the%0Aretrieved%20evidence%20helps%20break%20the%20information%20silos%20caused%20by%20the%20closed%20and%0Alimited%20text%20and%20context.%20On%20the%20other%20hand%2C%20we%20consider%20inter-class%0Adifferences%20between%20different%20emotional%20types%20of%20news%20through%20balanced%0Alearning%2C%20achieving%20fine-grained%20modeling%20of%20the%20relationship%20between%20emotional%0Atypes%20and%20authenticity.%20Extensive%20experiments%20on%20two%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20KEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKEN%253A%2520Knowledge%2520Augmentation%2520and%2520Emotion%2520Guidance%2520Network%2520for%2520Multimodal%250A%2520%2520Fake%2520News%2520Detection%26entry.906535625%3DPeican%2520Zhu%2520and%2520Yubo%2520Jing%2520and%2520Le%2520Cheng%2520and%2520Keke%2520Tang%2520and%2520Yangming%2520Guo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520rampant%2520spread%2520of%2520misinformation%2520on%2520social%2520media%2520has%250Amade%2520accurate%2520detection%2520of%2520multimodal%2520fake%2520news%2520a%2520critical%2520research%2520focus.%250AHowever%252C%2520previous%2520research%2520has%2520not%2520adequately%2520understood%2520the%2520semantics%2520of%250Aimages%252C%2520and%2520models%2520struggle%2520to%2520discern%2520news%2520authenticity%2520with%2520limited%2520textual%250Ainformation.%2520Meanwhile%252C%2520treating%2520all%2520emotional%2520types%2520of%2520news%2520uniformly%2520without%250Atailored%2520approaches%2520further%2520leads%2520to%2520performance%2520degradation.%2520Therefore%252C%2520we%250Apropose%2520a%2520novel%2520Knowledge%2520Augmentation%2520and%2520Emotion%2520Guidance%2520Network%2520%2528KEN%2529.%2520On%250Athe%2520one%2520hand%252C%2520we%2520effectively%2520leverage%2520LVLM%2527s%2520powerful%2520semantic%2520understanding%250Aand%2520extensive%2520world%2520knowledge.%2520For%2520images%252C%2520the%2520generated%2520captions%2520provide%2520a%250Acomprehensive%2520understanding%2520of%2520image%2520content%2520and%2520scenes%252C%2520while%2520for%2520text%252C%2520the%250Aretrieved%2520evidence%2520helps%2520break%2520the%2520information%2520silos%2520caused%2520by%2520the%2520closed%2520and%250Alimited%2520text%2520and%2520context.%2520On%2520the%2520other%2520hand%252C%2520we%2520consider%2520inter-class%250Adifferences%2520between%2520different%2520emotional%2520types%2520of%2520news%2520through%2520balanced%250Alearning%252C%2520achieving%2520fine-grained%2520modeling%2520of%2520the%2520relationship%2520between%2520emotional%250Atypes%2520and%2520authenticity.%2520Extensive%2520experiments%2520on%2520two%2520real-world%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520KEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KEN%3A%20Knowledge%20Augmentation%20and%20Emotion%20Guidance%20Network%20for%20Multimodal%0A%20%20Fake%20News%20Detection&entry.906535625=Peican%20Zhu%20and%20Yubo%20Jing%20and%20Le%20Cheng%20and%20Keke%20Tang%20and%20Yangming%20Guo&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rampant%20spread%20of%20misinformation%20on%20social%20media%20has%0Amade%20accurate%20detection%20of%20multimodal%20fake%20news%20a%20critical%20research%20focus.%0AHowever%2C%20previous%20research%20has%20not%20adequately%20understood%20the%20semantics%20of%0Aimages%2C%20and%20models%20struggle%20to%20discern%20news%20authenticity%20with%20limited%20textual%0Ainformation.%20Meanwhile%2C%20treating%20all%20emotional%20types%20of%20news%20uniformly%20without%0Atailored%20approaches%20further%20leads%20to%20performance%20degradation.%20Therefore%2C%20we%0Apropose%20a%20novel%20Knowledge%20Augmentation%20and%20Emotion%20Guidance%20Network%20%28KEN%29.%20On%0Athe%20one%20hand%2C%20we%20effectively%20leverage%20LVLM%27s%20powerful%20semantic%20understanding%0Aand%20extensive%20world%20knowledge.%20For%20images%2C%20the%20generated%20captions%20provide%20a%0Acomprehensive%20understanding%20of%20image%20content%20and%20scenes%2C%20while%20for%20text%2C%20the%0Aretrieved%20evidence%20helps%20break%20the%20information%20silos%20caused%20by%20the%20closed%20and%0Alimited%20text%20and%20context.%20On%20the%20other%20hand%2C%20we%20consider%20inter-class%0Adifferences%20between%20different%20emotional%20types%20of%20news%20through%20balanced%0Alearning%2C%20achieving%20fine-grained%20modeling%20of%20the%20relationship%20between%20emotional%0Atypes%20and%20authenticity.%20Extensive%20experiments%20on%20two%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20KEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09647v2&entry.124074799=Read"},
{"title": "Automating Steering for Safe Multimodal Large Language Models", "author": "Lyucheng Wu and Mengru Wang and Ziwen Xu and Tri Cao and Nay Oo and Bryan Hooi and Shumin Deng", "abstract": "  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.\n", "link": "http://arxiv.org/abs/2507.13255v1", "date": "2025-07-17", "relevancy": 1.6113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Steering%20for%20Safe%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Automating%20Steering%20for%20Safe%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Lyucheng%20Wu%20and%20Mengru%20Wang%20and%20Ziwen%20Xu%20and%20Tri%20Cao%20and%20Nay%20Oo%20and%20Bryan%20Hooi%20and%20Shumin%20Deng%0AAbstract%3A%20%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20unlocked%0Apowerful%20cross-modal%20reasoning%20abilities%2C%20but%20also%20raised%20new%20safety%20concerns%2C%0Aparticularly%20when%20faced%20with%20adversarial%20multimodal%20inputs.%20To%20improve%20the%0Asafety%20of%20MLLMs%20during%20inference%2C%20we%20introduce%20a%20modular%20and%20adaptive%0Ainference-time%20intervention%20technology%2C%20AutoSteer%2C%20without%20requiring%20any%0Afine-tuning%20of%20the%20underlying%20model.%20AutoSteer%20incorporates%20three%20core%0Acomponents%3A%20%281%29%20a%20novel%20Safety%20Awareness%20Score%20%28SAS%29%20that%20automatically%0Aidentifies%20the%20most%20safety-relevant%20distinctions%20among%20the%20model%27s%20internal%0Alayers%3B%20%282%29%20an%20adaptive%20safety%20prober%20trained%20to%20estimate%20the%20likelihood%20of%0Atoxic%20outputs%20from%20intermediate%20representations%3B%20and%20%283%29%20a%20lightweight%20Refusal%0AHead%20that%20selectively%20intervenes%20to%20modulate%20generation%20when%20safety%20risks%20are%0Adetected.%20Experiments%20on%20LLaVA-OV%20and%20Chameleon%20across%20diverse%20safety-critical%0Abenchmarks%20demonstrate%20that%20AutoSteer%20significantly%20reduces%20the%20Attack%20Success%0ARate%20%28ASR%29%20for%20textual%2C%20visual%2C%20and%20cross-modal%20threats%2C%20while%20maintaining%0Ageneral%20abilities.%20These%20findings%20position%20AutoSteer%20as%20a%20practical%2C%0Ainterpretable%2C%20and%20effective%20framework%20for%20safer%20deployment%20of%20multimodal%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Steering%2520for%2520Safe%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DLyucheng%2520Wu%2520and%2520Mengru%2520Wang%2520and%2520Ziwen%2520Xu%2520and%2520Tri%2520Cao%2520and%2520Nay%2520Oo%2520and%2520Bryan%2520Hooi%2520and%2520Shumin%2520Deng%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520unlocked%250Apowerful%2520cross-modal%2520reasoning%2520abilities%252C%2520but%2520also%2520raised%2520new%2520safety%2520concerns%252C%250Aparticularly%2520when%2520faced%2520with%2520adversarial%2520multimodal%2520inputs.%2520To%2520improve%2520the%250Asafety%2520of%2520MLLMs%2520during%2520inference%252C%2520we%2520introduce%2520a%2520modular%2520and%2520adaptive%250Ainference-time%2520intervention%2520technology%252C%2520AutoSteer%252C%2520without%2520requiring%2520any%250Afine-tuning%2520of%2520the%2520underlying%2520model.%2520AutoSteer%2520incorporates%2520three%2520core%250Acomponents%253A%2520%25281%2529%2520a%2520novel%2520Safety%2520Awareness%2520Score%2520%2528SAS%2529%2520that%2520automatically%250Aidentifies%2520the%2520most%2520safety-relevant%2520distinctions%2520among%2520the%2520model%2527s%2520internal%250Alayers%253B%2520%25282%2529%2520an%2520adaptive%2520safety%2520prober%2520trained%2520to%2520estimate%2520the%2520likelihood%2520of%250Atoxic%2520outputs%2520from%2520intermediate%2520representations%253B%2520and%2520%25283%2529%2520a%2520lightweight%2520Refusal%250AHead%2520that%2520selectively%2520intervenes%2520to%2520modulate%2520generation%2520when%2520safety%2520risks%2520are%250Adetected.%2520Experiments%2520on%2520LLaVA-OV%2520and%2520Chameleon%2520across%2520diverse%2520safety-critical%250Abenchmarks%2520demonstrate%2520that%2520AutoSteer%2520significantly%2520reduces%2520the%2520Attack%2520Success%250ARate%2520%2528ASR%2529%2520for%2520textual%252C%2520visual%252C%2520and%2520cross-modal%2520threats%252C%2520while%2520maintaining%250Ageneral%2520abilities.%2520These%2520findings%2520position%2520AutoSteer%2520as%2520a%2520practical%252C%250Ainterpretable%252C%2520and%2520effective%2520framework%2520for%2520safer%2520deployment%2520of%2520multimodal%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Steering%20for%20Safe%20Multimodal%20Large%20Language%20Models&entry.906535625=Lyucheng%20Wu%20and%20Mengru%20Wang%20and%20Ziwen%20Xu%20and%20Tri%20Cao%20and%20Nay%20Oo%20and%20Bryan%20Hooi%20and%20Shumin%20Deng&entry.1292438233=%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20unlocked%0Apowerful%20cross-modal%20reasoning%20abilities%2C%20but%20also%20raised%20new%20safety%20concerns%2C%0Aparticularly%20when%20faced%20with%20adversarial%20multimodal%20inputs.%20To%20improve%20the%0Asafety%20of%20MLLMs%20during%20inference%2C%20we%20introduce%20a%20modular%20and%20adaptive%0Ainference-time%20intervention%20technology%2C%20AutoSteer%2C%20without%20requiring%20any%0Afine-tuning%20of%20the%20underlying%20model.%20AutoSteer%20incorporates%20three%20core%0Acomponents%3A%20%281%29%20a%20novel%20Safety%20Awareness%20Score%20%28SAS%29%20that%20automatically%0Aidentifies%20the%20most%20safety-relevant%20distinctions%20among%20the%20model%27s%20internal%0Alayers%3B%20%282%29%20an%20adaptive%20safety%20prober%20trained%20to%20estimate%20the%20likelihood%20of%0Atoxic%20outputs%20from%20intermediate%20representations%3B%20and%20%283%29%20a%20lightweight%20Refusal%0AHead%20that%20selectively%20intervenes%20to%20modulate%20generation%20when%20safety%20risks%20are%0Adetected.%20Experiments%20on%20LLaVA-OV%20and%20Chameleon%20across%20diverse%20safety-critical%0Abenchmarks%20demonstrate%20that%20AutoSteer%20significantly%20reduces%20the%20Attack%20Success%0ARate%20%28ASR%29%20for%20textual%2C%20visual%2C%20and%20cross-modal%20threats%2C%20while%20maintaining%0Ageneral%20abilities.%20These%20findings%20position%20AutoSteer%20as%20a%20practical%2C%0Ainterpretable%2C%20and%20effective%20framework%20for%20safer%20deployment%20of%20multimodal%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13255v1&entry.124074799=Read"},
{"title": "From Roots to Rewards: Dynamic Tree Reasoning with RL", "author": "Ahmed Bahloul and Simon Malberg", "abstract": "  Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.\n", "link": "http://arxiv.org/abs/2507.13142v1", "date": "2025-07-17", "relevancy": 1.5256, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Roots%20to%20Rewards%3A%20Dynamic%20Tree%20Reasoning%20with%20RL&body=Title%3A%20From%20Roots%20to%20Rewards%3A%20Dynamic%20Tree%20Reasoning%20with%20RL%0AAuthor%3A%20Ahmed%20Bahloul%20and%20Simon%20Malberg%0AAbstract%3A%20%20%20Modern%20language%20models%20address%20complex%20questions%20through%20chain-of-thought%0A%28CoT%29%20reasoning%20%28Wei%20et%20al.%2C%202023%29%20and%20retrieval%20augmentation%20%28Lewis%20et%20al.%2C%0A2021%29%2C%20yet%20struggle%20with%20error%20propagation%20and%20knowledge%20integration.%0ATree-structured%20reasoning%20methods%2C%20particularly%20the%20Probabilistic%0ATree-of-Thought%20%28ProbTree%29%28Cao%20et%20al.%2C%202023%29%20framework%2C%20mitigate%20these%20issues%0Aby%20decomposing%20questions%20into%20hierarchical%20structures%20and%20selecting%20answers%0Athrough%20confidence-weighted%20aggregation%20of%20parametric%20and%20retrieved%20knowledge%0A%28Yao%20et%20al.%2C%202023%29.%20However%2C%20ProbTree%27s%20static%20implementation%20introduces%20two%0Akey%20limitations%3A%20%281%29%20the%20reasoning%20tree%20is%20fixed%20during%20the%20initial%0Aconstruction%20phase%2C%20preventing%20dynamic%20adaptation%20to%20intermediate%20results%2C%20and%0A%282%29%20each%20node%20requires%20exhaustive%20evaluation%20of%20all%20possible%20solution%0Astrategies%2C%20creating%20computational%20inefficiency.%20We%20present%20a%20dynamic%0Areinforcement%20learning%20%28Sutton%20and%20Barto%2C%202018%29%20framework%20that%20transforms%0Atree-based%20reasoning%20into%20an%20adaptive%20process.%20Our%20approach%20incrementally%0Aconstructs%20the%20reasoning%20tree%20based%20on%20real-time%20confidence%20estimates%2C%20while%0Alearning%20optimal%20policies%20for%20action%20selection%20%28decomposition%2C%20retrieval%2C%20or%0Aaggregation%29.%20This%20maintains%20ProbTree%27s%20probabilistic%20rigor%20while%20improving%0Aboth%20solution%20quality%20and%20computational%20efficiency%20through%20selective%20expansion%0Aand%20focused%20resource%20allocation.%20The%20work%20establishes%20a%20new%20paradigm%20for%0Atreestructured%20reasoning%20that%20balances%20the%20reliability%20of%20probabilistic%0Aframeworks%20with%20the%20flexibility%20required%20for%20real-world%20question%20answering%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Roots%2520to%2520Rewards%253A%2520Dynamic%2520Tree%2520Reasoning%2520with%2520RL%26entry.906535625%3DAhmed%2520Bahloul%2520and%2520Simon%2520Malberg%26entry.1292438233%3D%2520%2520Modern%2520language%2520models%2520address%2520complex%2520questions%2520through%2520chain-of-thought%250A%2528CoT%2529%2520reasoning%2520%2528Wei%2520et%2520al.%252C%25202023%2529%2520and%2520retrieval%2520augmentation%2520%2528Lewis%2520et%2520al.%252C%250A2021%2529%252C%2520yet%2520struggle%2520with%2520error%2520propagation%2520and%2520knowledge%2520integration.%250ATree-structured%2520reasoning%2520methods%252C%2520particularly%2520the%2520Probabilistic%250ATree-of-Thought%2520%2528ProbTree%2529%2528Cao%2520et%2520al.%252C%25202023%2529%2520framework%252C%2520mitigate%2520these%2520issues%250Aby%2520decomposing%2520questions%2520into%2520hierarchical%2520structures%2520and%2520selecting%2520answers%250Athrough%2520confidence-weighted%2520aggregation%2520of%2520parametric%2520and%2520retrieved%2520knowledge%250A%2528Yao%2520et%2520al.%252C%25202023%2529.%2520However%252C%2520ProbTree%2527s%2520static%2520implementation%2520introduces%2520two%250Akey%2520limitations%253A%2520%25281%2529%2520the%2520reasoning%2520tree%2520is%2520fixed%2520during%2520the%2520initial%250Aconstruction%2520phase%252C%2520preventing%2520dynamic%2520adaptation%2520to%2520intermediate%2520results%252C%2520and%250A%25282%2529%2520each%2520node%2520requires%2520exhaustive%2520evaluation%2520of%2520all%2520possible%2520solution%250Astrategies%252C%2520creating%2520computational%2520inefficiency.%2520We%2520present%2520a%2520dynamic%250Areinforcement%2520learning%2520%2528Sutton%2520and%2520Barto%252C%25202018%2529%2520framework%2520that%2520transforms%250Atree-based%2520reasoning%2520into%2520an%2520adaptive%2520process.%2520Our%2520approach%2520incrementally%250Aconstructs%2520the%2520reasoning%2520tree%2520based%2520on%2520real-time%2520confidence%2520estimates%252C%2520while%250Alearning%2520optimal%2520policies%2520for%2520action%2520selection%2520%2528decomposition%252C%2520retrieval%252C%2520or%250Aaggregation%2529.%2520This%2520maintains%2520ProbTree%2527s%2520probabilistic%2520rigor%2520while%2520improving%250Aboth%2520solution%2520quality%2520and%2520computational%2520efficiency%2520through%2520selective%2520expansion%250Aand%2520focused%2520resource%2520allocation.%2520The%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%250Atreestructured%2520reasoning%2520that%2520balances%2520the%2520reliability%2520of%2520probabilistic%250Aframeworks%2520with%2520the%2520flexibility%2520required%2520for%2520real-world%2520question%2520answering%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Roots%20to%20Rewards%3A%20Dynamic%20Tree%20Reasoning%20with%20RL&entry.906535625=Ahmed%20Bahloul%20and%20Simon%20Malberg&entry.1292438233=%20%20Modern%20language%20models%20address%20complex%20questions%20through%20chain-of-thought%0A%28CoT%29%20reasoning%20%28Wei%20et%20al.%2C%202023%29%20and%20retrieval%20augmentation%20%28Lewis%20et%20al.%2C%0A2021%29%2C%20yet%20struggle%20with%20error%20propagation%20and%20knowledge%20integration.%0ATree-structured%20reasoning%20methods%2C%20particularly%20the%20Probabilistic%0ATree-of-Thought%20%28ProbTree%29%28Cao%20et%20al.%2C%202023%29%20framework%2C%20mitigate%20these%20issues%0Aby%20decomposing%20questions%20into%20hierarchical%20structures%20and%20selecting%20answers%0Athrough%20confidence-weighted%20aggregation%20of%20parametric%20and%20retrieved%20knowledge%0A%28Yao%20et%20al.%2C%202023%29.%20However%2C%20ProbTree%27s%20static%20implementation%20introduces%20two%0Akey%20limitations%3A%20%281%29%20the%20reasoning%20tree%20is%20fixed%20during%20the%20initial%0Aconstruction%20phase%2C%20preventing%20dynamic%20adaptation%20to%20intermediate%20results%2C%20and%0A%282%29%20each%20node%20requires%20exhaustive%20evaluation%20of%20all%20possible%20solution%0Astrategies%2C%20creating%20computational%20inefficiency.%20We%20present%20a%20dynamic%0Areinforcement%20learning%20%28Sutton%20and%20Barto%2C%202018%29%20framework%20that%20transforms%0Atree-based%20reasoning%20into%20an%20adaptive%20process.%20Our%20approach%20incrementally%0Aconstructs%20the%20reasoning%20tree%20based%20on%20real-time%20confidence%20estimates%2C%20while%0Alearning%20optimal%20policies%20for%20action%20selection%20%28decomposition%2C%20retrieval%2C%20or%0Aaggregation%29.%20This%20maintains%20ProbTree%27s%20probabilistic%20rigor%20while%20improving%0Aboth%20solution%20quality%20and%20computational%20efficiency%20through%20selective%20expansion%0Aand%20focused%20resource%20allocation.%20The%20work%20establishes%20a%20new%20paradigm%20for%0Atreestructured%20reasoning%20that%20balances%20the%20reliability%20of%20probabilistic%0Aframeworks%20with%20the%20flexibility%20required%20for%20real-world%20question%20answering%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13142v1&entry.124074799=Read"},
{"title": "Few-shot transfer of tool-use skills using human demonstrations with\n  proximity and tactile sensing", "author": "Marina Y. Aoyama and Sethu Vijayakumar and Tetsuya Narita", "abstract": "  Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.\n", "link": "http://arxiv.org/abs/2507.13200v1", "date": "2025-07-17", "relevancy": 1.6727, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6067}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5766}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20transfer%20of%20tool-use%20skills%20using%20human%20demonstrations%20with%0A%20%20proximity%20and%20tactile%20sensing&body=Title%3A%20Few-shot%20transfer%20of%20tool-use%20skills%20using%20human%20demonstrations%20with%0A%20%20proximity%20and%20tactile%20sensing%0AAuthor%3A%20Marina%20Y.%20Aoyama%20and%20Sethu%20Vijayakumar%20and%20Tetsuya%20Narita%0AAbstract%3A%20%20%20Tools%20extend%20the%20manipulation%20abilities%20of%20robots%2C%20much%20like%20they%20do%20for%0Ahumans.%20Despite%20human%20expertise%20in%20tool%20manipulation%2C%20teaching%20robots%20these%0Askills%20faces%20challenges.%20The%20complexity%20arises%20from%20the%20interplay%20of%20two%0Asimultaneous%20points%20of%20contact%3A%20one%20between%20the%20robot%20and%20the%20tool%2C%20and%20another%0Abetween%20the%20tool%20and%20the%20environment.%20Tactile%20and%20proximity%20sensors%20play%20a%0Acrucial%20role%20in%20identifying%20these%20complex%20contacts.%20However%2C%20learning%20tool%0Amanipulation%20using%20these%20sensors%20remains%20challenging%20due%20to%20limited%20real-world%0Adata%20and%20the%20large%20sim-to-real%20gap.%20To%20address%20this%2C%20we%20propose%20a%20few-shot%0Atool-use%20skill%20transfer%20framework%20using%20multimodal%20sensing.%20The%20framework%0Ainvolves%20pre-training%20the%20base%20policy%20to%20capture%20contact%20states%20common%20in%0Atool-use%20skills%20in%20simulation%20and%20fine-tuning%20it%20with%20human%20demonstrations%0Acollected%20in%20the%20real-world%20target%20domain%20to%20bridge%20the%20domain%20gap.%20We%20validate%0Athat%20this%20framework%20enables%20teaching%20surface-following%20tasks%20using%20tools%20with%0Adiverse%20physical%20and%20geometric%20properties%20with%20a%20small%20number%20of%20demonstrations%0Aon%20the%20Franka%20Emika%20robot%20arm.%20Our%20analysis%20suggests%20that%20the%20robot%20acquires%0Anew%20tool-use%20skills%20by%20transferring%20the%20ability%20to%20recognise%20tool-environment%0Acontact%20relationships%20from%20pre-trained%20to%20fine-tuned%20policies.%20Additionally%2C%0Acombining%20proximity%20and%20tactile%20sensors%20enhances%20the%20identification%20of%20contact%0Astates%20and%20environmental%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520transfer%2520of%2520tool-use%2520skills%2520using%2520human%2520demonstrations%2520with%250A%2520%2520proximity%2520and%2520tactile%2520sensing%26entry.906535625%3DMarina%2520Y.%2520Aoyama%2520and%2520Sethu%2520Vijayakumar%2520and%2520Tetsuya%2520Narita%26entry.1292438233%3D%2520%2520Tools%2520extend%2520the%2520manipulation%2520abilities%2520of%2520robots%252C%2520much%2520like%2520they%2520do%2520for%250Ahumans.%2520Despite%2520human%2520expertise%2520in%2520tool%2520manipulation%252C%2520teaching%2520robots%2520these%250Askills%2520faces%2520challenges.%2520The%2520complexity%2520arises%2520from%2520the%2520interplay%2520of%2520two%250Asimultaneous%2520points%2520of%2520contact%253A%2520one%2520between%2520the%2520robot%2520and%2520the%2520tool%252C%2520and%2520another%250Abetween%2520the%2520tool%2520and%2520the%2520environment.%2520Tactile%2520and%2520proximity%2520sensors%2520play%2520a%250Acrucial%2520role%2520in%2520identifying%2520these%2520complex%2520contacts.%2520However%252C%2520learning%2520tool%250Amanipulation%2520using%2520these%2520sensors%2520remains%2520challenging%2520due%2520to%2520limited%2520real-world%250Adata%2520and%2520the%2520large%2520sim-to-real%2520gap.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520few-shot%250Atool-use%2520skill%2520transfer%2520framework%2520using%2520multimodal%2520sensing.%2520The%2520framework%250Ainvolves%2520pre-training%2520the%2520base%2520policy%2520to%2520capture%2520contact%2520states%2520common%2520in%250Atool-use%2520skills%2520in%2520simulation%2520and%2520fine-tuning%2520it%2520with%2520human%2520demonstrations%250Acollected%2520in%2520the%2520real-world%2520target%2520domain%2520to%2520bridge%2520the%2520domain%2520gap.%2520We%2520validate%250Athat%2520this%2520framework%2520enables%2520teaching%2520surface-following%2520tasks%2520using%2520tools%2520with%250Adiverse%2520physical%2520and%2520geometric%2520properties%2520with%2520a%2520small%2520number%2520of%2520demonstrations%250Aon%2520the%2520Franka%2520Emika%2520robot%2520arm.%2520Our%2520analysis%2520suggests%2520that%2520the%2520robot%2520acquires%250Anew%2520tool-use%2520skills%2520by%2520transferring%2520the%2520ability%2520to%2520recognise%2520tool-environment%250Acontact%2520relationships%2520from%2520pre-trained%2520to%2520fine-tuned%2520policies.%2520Additionally%252C%250Acombining%2520proximity%2520and%2520tactile%2520sensors%2520enhances%2520the%2520identification%2520of%2520contact%250Astates%2520and%2520environmental%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20transfer%20of%20tool-use%20skills%20using%20human%20demonstrations%20with%0A%20%20proximity%20and%20tactile%20sensing&entry.906535625=Marina%20Y.%20Aoyama%20and%20Sethu%20Vijayakumar%20and%20Tetsuya%20Narita&entry.1292438233=%20%20Tools%20extend%20the%20manipulation%20abilities%20of%20robots%2C%20much%20like%20they%20do%20for%0Ahumans.%20Despite%20human%20expertise%20in%20tool%20manipulation%2C%20teaching%20robots%20these%0Askills%20faces%20challenges.%20The%20complexity%20arises%20from%20the%20interplay%20of%20two%0Asimultaneous%20points%20of%20contact%3A%20one%20between%20the%20robot%20and%20the%20tool%2C%20and%20another%0Abetween%20the%20tool%20and%20the%20environment.%20Tactile%20and%20proximity%20sensors%20play%20a%0Acrucial%20role%20in%20identifying%20these%20complex%20contacts.%20However%2C%20learning%20tool%0Amanipulation%20using%20these%20sensors%20remains%20challenging%20due%20to%20limited%20real-world%0Adata%20and%20the%20large%20sim-to-real%20gap.%20To%20address%20this%2C%20we%20propose%20a%20few-shot%0Atool-use%20skill%20transfer%20framework%20using%20multimodal%20sensing.%20The%20framework%0Ainvolves%20pre-training%20the%20base%20policy%20to%20capture%20contact%20states%20common%20in%0Atool-use%20skills%20in%20simulation%20and%20fine-tuning%20it%20with%20human%20demonstrations%0Acollected%20in%20the%20real-world%20target%20domain%20to%20bridge%20the%20domain%20gap.%20We%20validate%0Athat%20this%20framework%20enables%20teaching%20surface-following%20tasks%20using%20tools%20with%0Adiverse%20physical%20and%20geometric%20properties%20with%20a%20small%20number%20of%20demonstrations%0Aon%20the%20Franka%20Emika%20robot%20arm.%20Our%20analysis%20suggests%20that%20the%20robot%20acquires%0Anew%20tool-use%20skills%20by%20transferring%20the%20ability%20to%20recognise%20tool-environment%0Acontact%20relationships%20from%20pre-trained%20to%20fine-tuned%20policies.%20Additionally%2C%0Acombining%20proximity%20and%20tactile%20sensors%20enhances%20the%20identification%20of%20contact%0Astates%20and%20environmental%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13200v1&entry.124074799=Read"},
{"title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence\n  Algorithms Using California Traffic Data", "author": "Junseong Lee and Jaegwan Cho and Yoonju Cho and Seoyoon Choi and Yejin Shin", "abstract": "  The study \"Prediction of Highway Traffic Flow Based on Artificial\nIntelligence Algorithms Using California Traffic Data\" presents a machine\nlearning-based traffic flow prediction model to address global traffic\ncongestion issues. The research utilized 30-second interval traffic data from\nCalifornia Highway 78 over a five-month period from July to November 2022,\nanalyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino\nReal\" in the San Diego area. The study employed Multiple Linear Regression\n(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals\nranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance\nmetrics, the analysis revealed that both MLR and RF models performed optimally\nwith 10-minute data collection intervals. These findings are expected to\ncontribute to future traffic congestion solutions and efficient traffic\nmanagement.\n", "link": "http://arxiv.org/abs/2507.13112v1", "date": "2025-07-17", "relevancy": 1.2868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4607}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4312}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20of%20Highway%20Traffic%20Flow%20Based%20on%20Artificial%20Intelligence%0A%20%20Algorithms%20Using%20California%20Traffic%20Data&body=Title%3A%20Prediction%20of%20Highway%20Traffic%20Flow%20Based%20on%20Artificial%20Intelligence%0A%20%20Algorithms%20Using%20California%20Traffic%20Data%0AAuthor%3A%20Junseong%20Lee%20and%20Jaegwan%20Cho%20and%20Yoonju%20Cho%20and%20Seoyoon%20Choi%20and%20Yejin%20Shin%0AAbstract%3A%20%20%20The%20study%20%22Prediction%20of%20Highway%20Traffic%20Flow%20Based%20on%20Artificial%0AIntelligence%20Algorithms%20Using%20California%20Traffic%20Data%22%20presents%20a%20machine%0Alearning-based%20traffic%20flow%20prediction%20model%20to%20address%20global%20traffic%0Acongestion%20issues.%20The%20research%20utilized%2030-second%20interval%20traffic%20data%20from%0ACalifornia%20Highway%2078%20over%20a%20five-month%20period%20from%20July%20to%20November%202022%2C%0Aanalyzing%20a%207.24%20km%20westbound%20section%20connecting%20%22Melrose%20Dr%22%20and%20%22El-Camino%0AReal%22%20in%20the%20San%20Diego%20area.%20The%20study%20employed%20Multiple%20Linear%20Regression%0A%28MLR%29%20and%20Random%20Forest%20%28RF%29%20algorithms%2C%20analyzing%20data%20collection%20intervals%0Aranging%20from%2030%20seconds%20to%2015%20minutes.%20Using%20R%5E2%2C%20MAE%2C%20and%20RMSE%20as%20performance%0Ametrics%2C%20the%20analysis%20revealed%20that%20both%20MLR%20and%20RF%20models%20performed%20optimally%0Awith%2010-minute%20data%20collection%20intervals.%20These%20findings%20are%20expected%20to%0Acontribute%20to%20future%20traffic%20congestion%20solutions%20and%20efficient%20traffic%0Amanagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520of%2520Highway%2520Traffic%2520Flow%2520Based%2520on%2520Artificial%2520Intelligence%250A%2520%2520Algorithms%2520Using%2520California%2520Traffic%2520Data%26entry.906535625%3DJunseong%2520Lee%2520and%2520Jaegwan%2520Cho%2520and%2520Yoonju%2520Cho%2520and%2520Seoyoon%2520Choi%2520and%2520Yejin%2520Shin%26entry.1292438233%3D%2520%2520The%2520study%2520%2522Prediction%2520of%2520Highway%2520Traffic%2520Flow%2520Based%2520on%2520Artificial%250AIntelligence%2520Algorithms%2520Using%2520California%2520Traffic%2520Data%2522%2520presents%2520a%2520machine%250Alearning-based%2520traffic%2520flow%2520prediction%2520model%2520to%2520address%2520global%2520traffic%250Acongestion%2520issues.%2520The%2520research%2520utilized%252030-second%2520interval%2520traffic%2520data%2520from%250ACalifornia%2520Highway%252078%2520over%2520a%2520five-month%2520period%2520from%2520July%2520to%2520November%25202022%252C%250Aanalyzing%2520a%25207.24%2520km%2520westbound%2520section%2520connecting%2520%2522Melrose%2520Dr%2522%2520and%2520%2522El-Camino%250AReal%2522%2520in%2520the%2520San%2520Diego%2520area.%2520The%2520study%2520employed%2520Multiple%2520Linear%2520Regression%250A%2528MLR%2529%2520and%2520Random%2520Forest%2520%2528RF%2529%2520algorithms%252C%2520analyzing%2520data%2520collection%2520intervals%250Aranging%2520from%252030%2520seconds%2520to%252015%2520minutes.%2520Using%2520R%255E2%252C%2520MAE%252C%2520and%2520RMSE%2520as%2520performance%250Ametrics%252C%2520the%2520analysis%2520revealed%2520that%2520both%2520MLR%2520and%2520RF%2520models%2520performed%2520optimally%250Awith%252010-minute%2520data%2520collection%2520intervals.%2520These%2520findings%2520are%2520expected%2520to%250Acontribute%2520to%2520future%2520traffic%2520congestion%2520solutions%2520and%2520efficient%2520traffic%250Amanagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20Highway%20Traffic%20Flow%20Based%20on%20Artificial%20Intelligence%0A%20%20Algorithms%20Using%20California%20Traffic%20Data&entry.906535625=Junseong%20Lee%20and%20Jaegwan%20Cho%20and%20Yoonju%20Cho%20and%20Seoyoon%20Choi%20and%20Yejin%20Shin&entry.1292438233=%20%20The%20study%20%22Prediction%20of%20Highway%20Traffic%20Flow%20Based%20on%20Artificial%0AIntelligence%20Algorithms%20Using%20California%20Traffic%20Data%22%20presents%20a%20machine%0Alearning-based%20traffic%20flow%20prediction%20model%20to%20address%20global%20traffic%0Acongestion%20issues.%20The%20research%20utilized%2030-second%20interval%20traffic%20data%20from%0ACalifornia%20Highway%2078%20over%20a%20five-month%20period%20from%20July%20to%20November%202022%2C%0Aanalyzing%20a%207.24%20km%20westbound%20section%20connecting%20%22Melrose%20Dr%22%20and%20%22El-Camino%0AReal%22%20in%20the%20San%20Diego%20area.%20The%20study%20employed%20Multiple%20Linear%20Regression%0A%28MLR%29%20and%20Random%20Forest%20%28RF%29%20algorithms%2C%20analyzing%20data%20collection%20intervals%0Aranging%20from%2030%20seconds%20to%2015%20minutes.%20Using%20R%5E2%2C%20MAE%2C%20and%20RMSE%20as%20performance%0Ametrics%2C%20the%20analysis%20revealed%20that%20both%20MLR%20and%20RF%20models%20performed%20optimally%0Awith%2010-minute%20data%20collection%20intervals.%20These%20findings%20are%20expected%20to%0Acontribute%20to%20future%20traffic%20congestion%20solutions%20and%20efficient%20traffic%0Amanagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13112v1&entry.124074799=Read"},
{"title": "Sampling-Based Motion Planning with Discrete Configuration-Space\n  Symmetries", "author": "Thomas Cohn and Russ Tedrake", "abstract": "  When planning motions in a configuration space that has underlying symmetries\n(e.g. when manipulating one or multiple symmetric objects), the ideal planning\nalgorithm should take advantage of those symmetries to produce shorter\ntrajectories. However, finite symmetries lead to complicated changes to the\nunderlying topology of configuration space, preventing the use of standard\nalgorithms. We demonstrate how the key primitives used for sampling-based\nplanning can be efficiently implemented in spaces with finite symmetries. A\nrigorous theoretical analysis, building upon a study of the geometry of the\nconfiguration space, shows improvements in the sample complexity of several\nstandard algorithms. Furthermore, a comprehensive slate of experiments\ndemonstrates the practical improvements in both path length and runtime.\n", "link": "http://arxiv.org/abs/2503.00614v2", "date": "2025-07-17", "relevancy": 1.8856, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4925}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4583}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-Based%20Motion%20Planning%20with%20Discrete%20Configuration-Space%0A%20%20Symmetries&body=Title%3A%20Sampling-Based%20Motion%20Planning%20with%20Discrete%20Configuration-Space%0A%20%20Symmetries%0AAuthor%3A%20Thomas%20Cohn%20and%20Russ%20Tedrake%0AAbstract%3A%20%20%20When%20planning%20motions%20in%20a%20configuration%20space%20that%20has%20underlying%20symmetries%0A%28e.g.%20when%20manipulating%20one%20or%20multiple%20symmetric%20objects%29%2C%20the%20ideal%20planning%0Aalgorithm%20should%20take%20advantage%20of%20those%20symmetries%20to%20produce%20shorter%0Atrajectories.%20However%2C%20finite%20symmetries%20lead%20to%20complicated%20changes%20to%20the%0Aunderlying%20topology%20of%20configuration%20space%2C%20preventing%20the%20use%20of%20standard%0Aalgorithms.%20We%20demonstrate%20how%20the%20key%20primitives%20used%20for%20sampling-based%0Aplanning%20can%20be%20efficiently%20implemented%20in%20spaces%20with%20finite%20symmetries.%20A%0Arigorous%20theoretical%20analysis%2C%20building%20upon%20a%20study%20of%20the%20geometry%20of%20the%0Aconfiguration%20space%2C%20shows%20improvements%20in%20the%20sample%20complexity%20of%20several%0Astandard%20algorithms.%20Furthermore%2C%20a%20comprehensive%20slate%20of%20experiments%0Ademonstrates%20the%20practical%20improvements%20in%20both%20path%20length%20and%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-Based%2520Motion%2520Planning%2520with%2520Discrete%2520Configuration-Space%250A%2520%2520Symmetries%26entry.906535625%3DThomas%2520Cohn%2520and%2520Russ%2520Tedrake%26entry.1292438233%3D%2520%2520When%2520planning%2520motions%2520in%2520a%2520configuration%2520space%2520that%2520has%2520underlying%2520symmetries%250A%2528e.g.%2520when%2520manipulating%2520one%2520or%2520multiple%2520symmetric%2520objects%2529%252C%2520the%2520ideal%2520planning%250Aalgorithm%2520should%2520take%2520advantage%2520of%2520those%2520symmetries%2520to%2520produce%2520shorter%250Atrajectories.%2520However%252C%2520finite%2520symmetries%2520lead%2520to%2520complicated%2520changes%2520to%2520the%250Aunderlying%2520topology%2520of%2520configuration%2520space%252C%2520preventing%2520the%2520use%2520of%2520standard%250Aalgorithms.%2520We%2520demonstrate%2520how%2520the%2520key%2520primitives%2520used%2520for%2520sampling-based%250Aplanning%2520can%2520be%2520efficiently%2520implemented%2520in%2520spaces%2520with%2520finite%2520symmetries.%2520A%250Arigorous%2520theoretical%2520analysis%252C%2520building%2520upon%2520a%2520study%2520of%2520the%2520geometry%2520of%2520the%250Aconfiguration%2520space%252C%2520shows%2520improvements%2520in%2520the%2520sample%2520complexity%2520of%2520several%250Astandard%2520algorithms.%2520Furthermore%252C%2520a%2520comprehensive%2520slate%2520of%2520experiments%250Ademonstrates%2520the%2520practical%2520improvements%2520in%2520both%2520path%2520length%2520and%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-Based%20Motion%20Planning%20with%20Discrete%20Configuration-Space%0A%20%20Symmetries&entry.906535625=Thomas%20Cohn%20and%20Russ%20Tedrake&entry.1292438233=%20%20When%20planning%20motions%20in%20a%20configuration%20space%20that%20has%20underlying%20symmetries%0A%28e.g.%20when%20manipulating%20one%20or%20multiple%20symmetric%20objects%29%2C%20the%20ideal%20planning%0Aalgorithm%20should%20take%20advantage%20of%20those%20symmetries%20to%20produce%20shorter%0Atrajectories.%20However%2C%20finite%20symmetries%20lead%20to%20complicated%20changes%20to%20the%0Aunderlying%20topology%20of%20configuration%20space%2C%20preventing%20the%20use%20of%20standard%0Aalgorithms.%20We%20demonstrate%20how%20the%20key%20primitives%20used%20for%20sampling-based%0Aplanning%20can%20be%20efficiently%20implemented%20in%20spaces%20with%20finite%20symmetries.%20A%0Arigorous%20theoretical%20analysis%2C%20building%20upon%20a%20study%20of%20the%20geometry%20of%20the%0Aconfiguration%20space%2C%20shows%20improvements%20in%20the%20sample%20complexity%20of%20several%0Astandard%20algorithms.%20Furthermore%2C%20a%20comprehensive%20slate%20of%20experiments%0Ademonstrates%20the%20practical%20improvements%20in%20both%20path%20length%20and%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00614v2&entry.124074799=Read"},
{"title": "Channel-wise Motion Features for Efficient Motion Segmentation", "author": "Riku Inoue and Masamitsu Tsuchiya and Yuji Yasui", "abstract": "  For safety-critical robotics applications such as autonomous driving, it is\nimportant to detect all required objects accurately in real-time. Motion\nsegmentation offers a solution by identifying dynamic objects from the scene in\na class-agnostic manner. Recently, various motion segmentation models have been\nproposed, most of which jointly use subnetworks to estimate Depth, Pose,\nOptical Flow, and Scene Flow. As a result, the overall computational cost of\nthe model increases, hindering real-time performance.\n  In this paper, we propose a novel cost-volume-based motion feature\nrepresentation, Channel-wise Motion Features. By extracting depth features of\neach instance in the feature map and capturing the scene's 3D motion\ninformation, it offers enhanced efficiency. The only subnetwork used to build\nChannel-wise Motion Features is the Pose Network, and no others are required.\nOur method not only achieves about 4 times the FPS of state-of-the-art models\nin the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also\ndemonstrates equivalent accuracy while reducing the parameters to about 25$\\%$.\n", "link": "http://arxiv.org/abs/2507.13082v1", "date": "2025-07-17", "relevancy": 1.6571, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5663}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel-wise%20Motion%20Features%20for%20Efficient%20Motion%20Segmentation&body=Title%3A%20Channel-wise%20Motion%20Features%20for%20Efficient%20Motion%20Segmentation%0AAuthor%3A%20Riku%20Inoue%20and%20Masamitsu%20Tsuchiya%20and%20Yuji%20Yasui%0AAbstract%3A%20%20%20For%20safety-critical%20robotics%20applications%20such%20as%20autonomous%20driving%2C%20it%20is%0Aimportant%20to%20detect%20all%20required%20objects%20accurately%20in%20real-time.%20Motion%0Asegmentation%20offers%20a%20solution%20by%20identifying%20dynamic%20objects%20from%20the%20scene%20in%0Aa%20class-agnostic%20manner.%20Recently%2C%20various%20motion%20segmentation%20models%20have%20been%0Aproposed%2C%20most%20of%20which%20jointly%20use%20subnetworks%20to%20estimate%20Depth%2C%20Pose%2C%0AOptical%20Flow%2C%20and%20Scene%20Flow.%20As%20a%20result%2C%20the%20overall%20computational%20cost%20of%0Athe%20model%20increases%2C%20hindering%20real-time%20performance.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cost-volume-based%20motion%20feature%0Arepresentation%2C%20Channel-wise%20Motion%20Features.%20By%20extracting%20depth%20features%20of%0Aeach%20instance%20in%20the%20feature%20map%20and%20capturing%20the%20scene%27s%203D%20motion%0Ainformation%2C%20it%20offers%20enhanced%20efficiency.%20The%20only%20subnetwork%20used%20to%20build%0AChannel-wise%20Motion%20Features%20is%20the%20Pose%20Network%2C%20and%20no%20others%20are%20required.%0AOur%20method%20not%20only%20achieves%20about%204%20times%20the%20FPS%20of%20state-of-the-art%20models%0Ain%20the%20KITTI%20Dataset%20and%20Cityscapes%20of%20the%20VCAS-Motion%20Dataset%2C%20but%20also%0Ademonstrates%20equivalent%20accuracy%20while%20reducing%20the%20parameters%20to%20about%2025%24%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel-wise%2520Motion%2520Features%2520for%2520Efficient%2520Motion%2520Segmentation%26entry.906535625%3DRiku%2520Inoue%2520and%2520Masamitsu%2520Tsuchiya%2520and%2520Yuji%2520Yasui%26entry.1292438233%3D%2520%2520For%2520safety-critical%2520robotics%2520applications%2520such%2520as%2520autonomous%2520driving%252C%2520it%2520is%250Aimportant%2520to%2520detect%2520all%2520required%2520objects%2520accurately%2520in%2520real-time.%2520Motion%250Asegmentation%2520offers%2520a%2520solution%2520by%2520identifying%2520dynamic%2520objects%2520from%2520the%2520scene%2520in%250Aa%2520class-agnostic%2520manner.%2520Recently%252C%2520various%2520motion%2520segmentation%2520models%2520have%2520been%250Aproposed%252C%2520most%2520of%2520which%2520jointly%2520use%2520subnetworks%2520to%2520estimate%2520Depth%252C%2520Pose%252C%250AOptical%2520Flow%252C%2520and%2520Scene%2520Flow.%2520As%2520a%2520result%252C%2520the%2520overall%2520computational%2520cost%2520of%250Athe%2520model%2520increases%252C%2520hindering%2520real-time%2520performance.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520cost-volume-based%2520motion%2520feature%250Arepresentation%252C%2520Channel-wise%2520Motion%2520Features.%2520By%2520extracting%2520depth%2520features%2520of%250Aeach%2520instance%2520in%2520the%2520feature%2520map%2520and%2520capturing%2520the%2520scene%2527s%25203D%2520motion%250Ainformation%252C%2520it%2520offers%2520enhanced%2520efficiency.%2520The%2520only%2520subnetwork%2520used%2520to%2520build%250AChannel-wise%2520Motion%2520Features%2520is%2520the%2520Pose%2520Network%252C%2520and%2520no%2520others%2520are%2520required.%250AOur%2520method%2520not%2520only%2520achieves%2520about%25204%2520times%2520the%2520FPS%2520of%2520state-of-the-art%2520models%250Ain%2520the%2520KITTI%2520Dataset%2520and%2520Cityscapes%2520of%2520the%2520VCAS-Motion%2520Dataset%252C%2520but%2520also%250Ademonstrates%2520equivalent%2520accuracy%2520while%2520reducing%2520the%2520parameters%2520to%2520about%252025%2524%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel-wise%20Motion%20Features%20for%20Efficient%20Motion%20Segmentation&entry.906535625=Riku%20Inoue%20and%20Masamitsu%20Tsuchiya%20and%20Yuji%20Yasui&entry.1292438233=%20%20For%20safety-critical%20robotics%20applications%20such%20as%20autonomous%20driving%2C%20it%20is%0Aimportant%20to%20detect%20all%20required%20objects%20accurately%20in%20real-time.%20Motion%0Asegmentation%20offers%20a%20solution%20by%20identifying%20dynamic%20objects%20from%20the%20scene%20in%0Aa%20class-agnostic%20manner.%20Recently%2C%20various%20motion%20segmentation%20models%20have%20been%0Aproposed%2C%20most%20of%20which%20jointly%20use%20subnetworks%20to%20estimate%20Depth%2C%20Pose%2C%0AOptical%20Flow%2C%20and%20Scene%20Flow.%20As%20a%20result%2C%20the%20overall%20computational%20cost%20of%0Athe%20model%20increases%2C%20hindering%20real-time%20performance.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20cost-volume-based%20motion%20feature%0Arepresentation%2C%20Channel-wise%20Motion%20Features.%20By%20extracting%20depth%20features%20of%0Aeach%20instance%20in%20the%20feature%20map%20and%20capturing%20the%20scene%27s%203D%20motion%0Ainformation%2C%20it%20offers%20enhanced%20efficiency.%20The%20only%20subnetwork%20used%20to%20build%0AChannel-wise%20Motion%20Features%20is%20the%20Pose%20Network%2C%20and%20no%20others%20are%20required.%0AOur%20method%20not%20only%20achieves%20about%204%20times%20the%20FPS%20of%20state-of-the-art%20models%0Ain%20the%20KITTI%20Dataset%20and%20Cityscapes%20of%20the%20VCAS-Motion%20Dataset%2C%20but%20also%0Ademonstrates%20equivalent%20accuracy%20while%20reducing%20the%20parameters%20to%20about%2025%24%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13082v1&entry.124074799=Read"},
{"title": "An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and\n  Mapping", "author": "Masoud Dayani Najafabadi and Mohammad Reza Ahmadzadeh", "abstract": "  Compared to regular cameras, Dynamic Vision Sensors or Event Cameras can\noutput compact visual data based on a change in the intensity in each pixel\nlocation asynchronously. In this paper, we study the application of current\nimage-based SLAM techniques to these novel sensors. To this end, the\ninformation in adaptively selected event windows is processed to form\nmotion-compensated images. These images are then used to reconstruct the scene\nand estimate the 6-DOF pose of the camera. We also propose an inertial version\nof the event-only pipeline to assess its capabilities. We compare the results\nof different configurations of the proposed algorithm against the ground truth\nfor sequences of two publicly available event datasets. We also compare the\nresults of the proposed event-inertial pipeline with the state-of-the-art and\nshow it can produce comparable or more accurate results provided the map\nestimate is reliable.\n", "link": "http://arxiv.org/abs/2301.00618v4", "date": "2025-07-17", "relevancy": 1.7548, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping&body=Title%3A%20An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping%0AAuthor%3A%20Masoud%20Dayani%20Najafabadi%20and%20Mohammad%20Reza%20Ahmadzadeh%0AAbstract%3A%20%20%20Compared%20to%20regular%20cameras%2C%20Dynamic%20Vision%20Sensors%20or%20Event%20Cameras%20can%0Aoutput%20compact%20visual%20data%20based%20on%20a%20change%20in%20the%20intensity%20in%20each%20pixel%0Alocation%20asynchronously.%20In%20this%20paper%2C%20we%20study%20the%20application%20of%20current%0Aimage-based%20SLAM%20techniques%20to%20these%20novel%20sensors.%20To%20this%20end%2C%20the%0Ainformation%20in%20adaptively%20selected%20event%20windows%20is%20processed%20to%20form%0Amotion-compensated%20images.%20These%20images%20are%20then%20used%20to%20reconstruct%20the%20scene%0Aand%20estimate%20the%206-DOF%20pose%20of%20the%20camera.%20We%20also%20propose%20an%20inertial%20version%0Aof%20the%20event-only%20pipeline%20to%20assess%20its%20capabilities.%20We%20compare%20the%20results%0Aof%20different%20configurations%20of%20the%20proposed%20algorithm%20against%20the%20ground%20truth%0Afor%20sequences%20of%20two%20publicly%20available%20event%20datasets.%20We%20also%20compare%20the%0Aresults%20of%20the%20proposed%20event-inertial%20pipeline%20with%20the%20state-of-the-art%20and%0Ashow%20it%20can%20produce%20comparable%20or%20more%20accurate%20results%20provided%20the%20map%0Aestimate%20is%20reliable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.00618v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Event-based%2520Algorithm%2520for%2520Simultaneous%25206-DOF%2520Camera%2520Pose%2520Tracking%2520and%250A%2520%2520Mapping%26entry.906535625%3DMasoud%2520Dayani%2520Najafabadi%2520and%2520Mohammad%2520Reza%2520Ahmadzadeh%26entry.1292438233%3D%2520%2520Compared%2520to%2520regular%2520cameras%252C%2520Dynamic%2520Vision%2520Sensors%2520or%2520Event%2520Cameras%2520can%250Aoutput%2520compact%2520visual%2520data%2520based%2520on%2520a%2520change%2520in%2520the%2520intensity%2520in%2520each%2520pixel%250Alocation%2520asynchronously.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520application%2520of%2520current%250Aimage-based%2520SLAM%2520techniques%2520to%2520these%2520novel%2520sensors.%2520To%2520this%2520end%252C%2520the%250Ainformation%2520in%2520adaptively%2520selected%2520event%2520windows%2520is%2520processed%2520to%2520form%250Amotion-compensated%2520images.%2520These%2520images%2520are%2520then%2520used%2520to%2520reconstruct%2520the%2520scene%250Aand%2520estimate%2520the%25206-DOF%2520pose%2520of%2520the%2520camera.%2520We%2520also%2520propose%2520an%2520inertial%2520version%250Aof%2520the%2520event-only%2520pipeline%2520to%2520assess%2520its%2520capabilities.%2520We%2520compare%2520the%2520results%250Aof%2520different%2520configurations%2520of%2520the%2520proposed%2520algorithm%2520against%2520the%2520ground%2520truth%250Afor%2520sequences%2520of%2520two%2520publicly%2520available%2520event%2520datasets.%2520We%2520also%2520compare%2520the%250Aresults%2520of%2520the%2520proposed%2520event-inertial%2520pipeline%2520with%2520the%2520state-of-the-art%2520and%250Ashow%2520it%2520can%2520produce%2520comparable%2520or%2520more%2520accurate%2520results%2520provided%2520the%2520map%250Aestimate%2520is%2520reliable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.00618v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Event-based%20Algorithm%20for%20Simultaneous%206-DOF%20Camera%20Pose%20Tracking%20and%0A%20%20Mapping&entry.906535625=Masoud%20Dayani%20Najafabadi%20and%20Mohammad%20Reza%20Ahmadzadeh&entry.1292438233=%20%20Compared%20to%20regular%20cameras%2C%20Dynamic%20Vision%20Sensors%20or%20Event%20Cameras%20can%0Aoutput%20compact%20visual%20data%20based%20on%20a%20change%20in%20the%20intensity%20in%20each%20pixel%0Alocation%20asynchronously.%20In%20this%20paper%2C%20we%20study%20the%20application%20of%20current%0Aimage-based%20SLAM%20techniques%20to%20these%20novel%20sensors.%20To%20this%20end%2C%20the%0Ainformation%20in%20adaptively%20selected%20event%20windows%20is%20processed%20to%20form%0Amotion-compensated%20images.%20These%20images%20are%20then%20used%20to%20reconstruct%20the%20scene%0Aand%20estimate%20the%206-DOF%20pose%20of%20the%20camera.%20We%20also%20propose%20an%20inertial%20version%0Aof%20the%20event-only%20pipeline%20to%20assess%20its%20capabilities.%20We%20compare%20the%20results%0Aof%20different%20configurations%20of%20the%20proposed%20algorithm%20against%20the%20ground%20truth%0Afor%20sequences%20of%20two%20publicly%20available%20event%20datasets.%20We%20also%20compare%20the%0Aresults%20of%20the%20proposed%20event-inertial%20pipeline%20with%20the%20state-of-the-art%20and%0Ashow%20it%20can%20produce%20comparable%20or%20more%20accurate%20results%20provided%20the%20map%0Aestimate%20is%20reliable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.00618v4&entry.124074799=Read"},
{"title": "Do you know what q-means?", "author": "Arjan Cornelissen and Joao F. Doriguello and Alessandro Luongo and Ewin Tang", "abstract": "  Clustering is one of the most important tools for analysis of large datasets,\nand perhaps the most popular clustering algorithm is Lloyd's algorithm for\n$k$-means. This algorithm takes $n$ vectors\n$V=[v_1,\\dots,v_n]\\in\\mathbb{R}^{d\\times n}$ and outputs $k$ centroids\n$c_1,\\dots,c_k\\in\\mathbb{R}^d$; these partition the vectors into clusters based\non which centroid is closest to a particular vector. We present a classical\n$\\varepsilon$-$k$-means algorithm that performs an approximate version of one\niteration of Lloyd's algorithm with time complexity\n$\\tilde{O}\\big(\\frac{\\|V\\|_F^2}{n}\\frac{k^{2}d}{\\varepsilon^2}(k +\n\\log{n})\\big)$, exponentially improving the dependence on the data size $n$ and\nmatching that of the \"$q$-means\" quantum algorithm originally proposed by\nKerenidis, Landman, Luongo, and Prakash (NeurIPS'19). Moreover, we propose an\nimproved $q$-means quantum algorithm with time complexity\n$\\tilde{O}\\big(\\frac{\\|V\\|_F}{\\sqrt{n}}\\frac{k^{3/2}d}{\\varepsilon}(\\sqrt{k}+\\sqrt{d})(\\sqrt{k}\n+ \\log{n})\\big)$ that quadratically improves the runtime of our classical\n$\\varepsilon$-$k$-means algorithm in several parameters. Our quantum algorithm\ndoes not rely on quantum linear algebra primitives of prior work, but instead\nonly uses QRAM to prepare simple states based on the current iteration's\nclusters and multivariate quantum amplitude estimation. Finally, we provide\nclassical and quantum query lower bounds, showing that our algorithms are\noptimal in most parameters.\n", "link": "http://arxiv.org/abs/2308.09701v3", "date": "2025-07-17", "relevancy": 1.1767, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.394}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20you%20know%20what%20q-means%3F&body=Title%3A%20Do%20you%20know%20what%20q-means%3F%0AAuthor%3A%20Arjan%20Cornelissen%20and%20Joao%20F.%20Doriguello%20and%20Alessandro%20Luongo%20and%20Ewin%20Tang%0AAbstract%3A%20%20%20Clustering%20is%20one%20of%20the%20most%20important%20tools%20for%20analysis%20of%20large%20datasets%2C%0Aand%20perhaps%20the%20most%20popular%20clustering%20algorithm%20is%20Lloyd%27s%20algorithm%20for%0A%24k%24-means.%20This%20algorithm%20takes%20%24n%24%20vectors%0A%24V%3D%5Bv_1%2C%5Cdots%2Cv_n%5D%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20n%7D%24%20and%20outputs%20%24k%24%20centroids%0A%24c_1%2C%5Cdots%2Cc_k%5Cin%5Cmathbb%7BR%7D%5Ed%24%3B%20these%20partition%20the%20vectors%20into%20clusters%20based%0Aon%20which%20centroid%20is%20closest%20to%20a%20particular%20vector.%20We%20present%20a%20classical%0A%24%5Cvarepsilon%24-%24k%24-means%20algorithm%20that%20performs%20an%20approximate%20version%20of%20one%0Aiteration%20of%20Lloyd%27s%20algorithm%20with%20time%20complexity%0A%24%5Ctilde%7BO%7D%5Cbig%28%5Cfrac%7B%5C%7CV%5C%7C_F%5E2%7D%7Bn%7D%5Cfrac%7Bk%5E%7B2%7Dd%7D%7B%5Cvarepsilon%5E2%7D%28k%20%2B%0A%5Clog%7Bn%7D%29%5Cbig%29%24%2C%20exponentially%20improving%20the%20dependence%20on%20the%20data%20size%20%24n%24%20and%0Amatching%20that%20of%20the%20%22%24q%24-means%22%20quantum%20algorithm%20originally%20proposed%20by%0AKerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%20%28NeurIPS%2719%29.%20Moreover%2C%20we%20propose%20an%0Aimproved%20%24q%24-means%20quantum%20algorithm%20with%20time%20complexity%0A%24%5Ctilde%7BO%7D%5Cbig%28%5Cfrac%7B%5C%7CV%5C%7C_F%7D%7B%5Csqrt%7Bn%7D%7D%5Cfrac%7Bk%5E%7B3/2%7Dd%7D%7B%5Cvarepsilon%7D%28%5Csqrt%7Bk%7D%2B%5Csqrt%7Bd%7D%29%28%5Csqrt%7Bk%7D%0A%2B%20%5Clog%7Bn%7D%29%5Cbig%29%24%20that%20quadratically%20improves%20the%20runtime%20of%20our%20classical%0A%24%5Cvarepsilon%24-%24k%24-means%20algorithm%20in%20several%20parameters.%20Our%20quantum%20algorithm%0Adoes%20not%20rely%20on%20quantum%20linear%20algebra%20primitives%20of%20prior%20work%2C%20but%20instead%0Aonly%20uses%20QRAM%20to%20prepare%20simple%20states%20based%20on%20the%20current%20iteration%27s%0Aclusters%20and%20multivariate%20quantum%20amplitude%20estimation.%20Finally%2C%20we%20provide%0Aclassical%20and%20quantum%20query%20lower%20bounds%2C%20showing%20that%20our%20algorithms%20are%0Aoptimal%20in%20most%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09701v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520you%2520know%2520what%2520q-means%253F%26entry.906535625%3DArjan%2520Cornelissen%2520and%2520Joao%2520F.%2520Doriguello%2520and%2520Alessandro%2520Luongo%2520and%2520Ewin%2520Tang%26entry.1292438233%3D%2520%2520Clustering%2520is%2520one%2520of%2520the%2520most%2520important%2520tools%2520for%2520analysis%2520of%2520large%2520datasets%252C%250Aand%2520perhaps%2520the%2520most%2520popular%2520clustering%2520algorithm%2520is%2520Lloyd%2527s%2520algorithm%2520for%250A%2524k%2524-means.%2520This%2520algorithm%2520takes%2520%2524n%2524%2520vectors%250A%2524V%253D%255Bv_1%252C%255Cdots%252Cv_n%255D%255Cin%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520n%257D%2524%2520and%2520outputs%2520%2524k%2524%2520centroids%250A%2524c_1%252C%255Cdots%252Cc_k%255Cin%255Cmathbb%257BR%257D%255Ed%2524%253B%2520these%2520partition%2520the%2520vectors%2520into%2520clusters%2520based%250Aon%2520which%2520centroid%2520is%2520closest%2520to%2520a%2520particular%2520vector.%2520We%2520present%2520a%2520classical%250A%2524%255Cvarepsilon%2524-%2524k%2524-means%2520algorithm%2520that%2520performs%2520an%2520approximate%2520version%2520of%2520one%250Aiteration%2520of%2520Lloyd%2527s%2520algorithm%2520with%2520time%2520complexity%250A%2524%255Ctilde%257BO%257D%255Cbig%2528%255Cfrac%257B%255C%257CV%255C%257C_F%255E2%257D%257Bn%257D%255Cfrac%257Bk%255E%257B2%257Dd%257D%257B%255Cvarepsilon%255E2%257D%2528k%2520%252B%250A%255Clog%257Bn%257D%2529%255Cbig%2529%2524%252C%2520exponentially%2520improving%2520the%2520dependence%2520on%2520the%2520data%2520size%2520%2524n%2524%2520and%250Amatching%2520that%2520of%2520the%2520%2522%2524q%2524-means%2522%2520quantum%2520algorithm%2520originally%2520proposed%2520by%250AKerenidis%252C%2520Landman%252C%2520Luongo%252C%2520and%2520Prakash%2520%2528NeurIPS%252719%2529.%2520Moreover%252C%2520we%2520propose%2520an%250Aimproved%2520%2524q%2524-means%2520quantum%2520algorithm%2520with%2520time%2520complexity%250A%2524%255Ctilde%257BO%257D%255Cbig%2528%255Cfrac%257B%255C%257CV%255C%257C_F%257D%257B%255Csqrt%257Bn%257D%257D%255Cfrac%257Bk%255E%257B3/2%257Dd%257D%257B%255Cvarepsilon%257D%2528%255Csqrt%257Bk%257D%252B%255Csqrt%257Bd%257D%2529%2528%255Csqrt%257Bk%257D%250A%252B%2520%255Clog%257Bn%257D%2529%255Cbig%2529%2524%2520that%2520quadratically%2520improves%2520the%2520runtime%2520of%2520our%2520classical%250A%2524%255Cvarepsilon%2524-%2524k%2524-means%2520algorithm%2520in%2520several%2520parameters.%2520Our%2520quantum%2520algorithm%250Adoes%2520not%2520rely%2520on%2520quantum%2520linear%2520algebra%2520primitives%2520of%2520prior%2520work%252C%2520but%2520instead%250Aonly%2520uses%2520QRAM%2520to%2520prepare%2520simple%2520states%2520based%2520on%2520the%2520current%2520iteration%2527s%250Aclusters%2520and%2520multivariate%2520quantum%2520amplitude%2520estimation.%2520Finally%252C%2520we%2520provide%250Aclassical%2520and%2520quantum%2520query%2520lower%2520bounds%252C%2520showing%2520that%2520our%2520algorithms%2520are%250Aoptimal%2520in%2520most%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09701v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20you%20know%20what%20q-means%3F&entry.906535625=Arjan%20Cornelissen%20and%20Joao%20F.%20Doriguello%20and%20Alessandro%20Luongo%20and%20Ewin%20Tang&entry.1292438233=%20%20Clustering%20is%20one%20of%20the%20most%20important%20tools%20for%20analysis%20of%20large%20datasets%2C%0Aand%20perhaps%20the%20most%20popular%20clustering%20algorithm%20is%20Lloyd%27s%20algorithm%20for%0A%24k%24-means.%20This%20algorithm%20takes%20%24n%24%20vectors%0A%24V%3D%5Bv_1%2C%5Cdots%2Cv_n%5D%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20n%7D%24%20and%20outputs%20%24k%24%20centroids%0A%24c_1%2C%5Cdots%2Cc_k%5Cin%5Cmathbb%7BR%7D%5Ed%24%3B%20these%20partition%20the%20vectors%20into%20clusters%20based%0Aon%20which%20centroid%20is%20closest%20to%20a%20particular%20vector.%20We%20present%20a%20classical%0A%24%5Cvarepsilon%24-%24k%24-means%20algorithm%20that%20performs%20an%20approximate%20version%20of%20one%0Aiteration%20of%20Lloyd%27s%20algorithm%20with%20time%20complexity%0A%24%5Ctilde%7BO%7D%5Cbig%28%5Cfrac%7B%5C%7CV%5C%7C_F%5E2%7D%7Bn%7D%5Cfrac%7Bk%5E%7B2%7Dd%7D%7B%5Cvarepsilon%5E2%7D%28k%20%2B%0A%5Clog%7Bn%7D%29%5Cbig%29%24%2C%20exponentially%20improving%20the%20dependence%20on%20the%20data%20size%20%24n%24%20and%0Amatching%20that%20of%20the%20%22%24q%24-means%22%20quantum%20algorithm%20originally%20proposed%20by%0AKerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%20%28NeurIPS%2719%29.%20Moreover%2C%20we%20propose%20an%0Aimproved%20%24q%24-means%20quantum%20algorithm%20with%20time%20complexity%0A%24%5Ctilde%7BO%7D%5Cbig%28%5Cfrac%7B%5C%7CV%5C%7C_F%7D%7B%5Csqrt%7Bn%7D%7D%5Cfrac%7Bk%5E%7B3/2%7Dd%7D%7B%5Cvarepsilon%7D%28%5Csqrt%7Bk%7D%2B%5Csqrt%7Bd%7D%29%28%5Csqrt%7Bk%7D%0A%2B%20%5Clog%7Bn%7D%29%5Cbig%29%24%20that%20quadratically%20improves%20the%20runtime%20of%20our%20classical%0A%24%5Cvarepsilon%24-%24k%24-means%20algorithm%20in%20several%20parameters.%20Our%20quantum%20algorithm%0Adoes%20not%20rely%20on%20quantum%20linear%20algebra%20primitives%20of%20prior%20work%2C%20but%20instead%0Aonly%20uses%20QRAM%20to%20prepare%20simple%20states%20based%20on%20the%20current%20iteration%27s%0Aclusters%20and%20multivariate%20quantum%20amplitude%20estimation.%20Finally%2C%20we%20provide%0Aclassical%20and%20quantum%20query%20lower%20bounds%2C%20showing%20that%20our%20algorithms%20are%0Aoptimal%20in%20most%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09701v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


